throw new SemanticException("The table of " + tsop.getName() + " " + tsop.getIdentifier() + " is null, which is not expected.");	}	String alias = tsop.getConf().getAlias();	aliases.add(alias);	Path p = table.getPath();	ContentSummary resultCs = null;	try {	FileSystem fs = table.getPath().getFileSystem(pCtx.getConf());	resultCs = fs.getContentSummary(p);	} catch (IOException e) {	
encounter a error while querying content summary of table from filesystem cannot guess if commonjoinoperator will optimize 

Long es = aliasToSize.get(alias);	if(es == null) {	es = new Long(0);	}	es += size;	aliasToSize.put(alias, es);	}	posToAliases.put(pos, aliases);	}	if (!isAbleToGuess) {	
cannot guess if commonjoinoperator will optimize 

if (!bigTableCandidates.contains(i)) {	continue;	}	Set<String> aliases = posToAliases.get(i);	long aliasKnownSize = Utilities.sumOf(aliasToSize, aliases);	if (!CommonJoinTaskDispatcher.cannotConvert(aliasKnownSize, aliasTotalKnownInputSize, ThresholdOfSmallTblSizeSum)) {	mayConvert = true;	}	}	if (mayConvert) {	
may be converted to mapjoin by commonjoinresolver 

CorrelationNodeProcCtx corrCtx = new CorrelationNodeProcCtx(pCtx);	Map<Rule, NodeProcessor> opRules = new LinkedHashMap<Rule, NodeProcessor>();	opRules.put(new RuleRegExp("R1", ReduceSinkOperator.getOperatorName() + "%"), new CorrelationNodeProc());	Dispatcher disp = new DefaultRuleDispatcher(getDefaultProc(), opRules, corrCtx);	GraphWalker ogw = new DefaultGraphWalker(disp);	List<Node> topNodes = new ArrayList<Node>();	topNodes.addAll(pCtx.getTopOps().values());	ogw.startWalking(topNodes, null);	abort = corrCtx.isAbort();	if (abort) {	
abort reasons are 

GraphWalker ogw = new DefaultGraphWalker(disp);	List<Node> topNodes = new ArrayList<Node>();	topNodes.addAll(pCtx.getTopOps().values());	ogw.startWalking(topNodes, null);	abort = corrCtx.isAbort();	if (abort) {	for (String reason : corrCtx.getAbortReasons()) {	LOG.info("-- " + reason);	}	} else {	
begain query plan transformation based on intra query correlations correlation s to be applied 

private LinkedHashSet<ReduceSinkOperator> findCorrelatedReduceSinkOperators( Operator<? extends OperatorDesc> child, List<ExprNodeDesc> childKeyCols, List<ExprNodeDesc> childPartitionCols, String childRSOrder, Operator<? extends OperatorDesc> current, IntraQueryCorrelation correlation) throws SemanticException {	
now detecting operator 

private LinkedHashSet<ReduceSinkOperator> findCorrelatedReduceSinkOperators( Operator<? extends OperatorDesc> child, List<ExprNodeDesc> childKeyCols, List<ExprNodeDesc> childPartitionCols, String childRSOrder, Operator<? extends OperatorDesc> current, IntraQueryCorrelation correlation) throws SemanticException {	LinkedHashSet<ReduceSinkOperator> correlatedReduceSinkOperators = new LinkedHashSet<ReduceSinkOperator>();	if (skipedJoinOperators.contains(current)) {	
may be converted to mapjoin by commonjoinresolver correlation optimizer will not detect correlations involved in this operator 

private LinkedHashSet<ReduceSinkOperator> findCorrelatedReduceSinkOperators( Operator<? extends OperatorDesc> child, List<ExprNodeDesc> childKeyCols, List<ExprNodeDesc> childPartitionCols, String childRSOrder, Operator<? extends OperatorDesc> current, IntraQueryCorrelation correlation) throws SemanticException {	LinkedHashSet<ReduceSinkOperator> correlatedReduceSinkOperators = new LinkedHashSet<ReduceSinkOperator>();	if (skipedJoinOperators.contains(current)) {	return correlatedReduceSinkOperators;	}	if ((current.getParentOperators() == null) || (current.getParentOperators().isEmpty())) {	return correlatedReduceSinkOperators;	}	if (current instanceof PTFOperator) {	
currently correlation optimizer does not support ptf operator 

List<ExprNodeDesc> rsKeyCols = rsop.getConf().getKeyCols();	List<ExprNodeDesc> rsPartitionCols = rsop.getConf().getPartitionCols();	boolean isCorrelated = sameKeys(rsKeyCols, backtrackedKeyCols) && sameOrder(rsop.getConf().getOrder(), childRSOrder) && sameKeys(backtrackedPartitionCols, rsPartitionCols) && correlation.adjustNumReducers(rsop.getConf().getNumReducers());	GroupByOperator cGBY = CorrelationUtilities.getSingleChild(rsop, GroupByOperator.class);	if (cGBY != null) {	if (CorrelationUtilities.hasGroupingSet(rsop) || cGBY.getConf().isGroupingSetsPresent()) {	isCorrelated = false;	}	}	if (isCorrelated) {	
operator is correlated 

JoinOperator joinOp = (JoinOperator) childOperator;	JoinCondDesc[] joinConds = joinOp.getConf().getConds();	List<Operator<? extends OperatorDesc>> rsOps = joinOp.getParentOperators();	LinkedHashSet<ReduceSinkOperator> correlatedRsOps = new LinkedHashSet<ReduceSinkOperator>();	analyzeReduceSinkOperatorsOfJoinOperator(joinConds, rsOps, current, correlatedRsOps);	correlatedReduceSinkOperators.addAll(correlatedRsOps);	} else {	correlatedReduceSinkOperators.add(rsop);	}	} else {	
operator is not correlated 

LinkedHashSet<ReduceSinkOperator> correlatedRsOps = new LinkedHashSet<ReduceSinkOperator>();	analyzeReduceSinkOperatorsOfJoinOperator(joinConds, rsOps, current, correlatedRsOps);	correlatedReduceSinkOperators.addAll(correlatedRsOps);	} else {	correlatedReduceSinkOperators.add(rsop);	}	} else {	correlatedReduceSinkOperators.clear();	}	} else {	
reducesinkoperator does not have columnexprmap 

GroupByOperator cGBY = CorrelationUtilities.getSingleChild(op, GroupByOperator.class);	if (cGBY != null) {	if (CorrelationUtilities.hasGroupingSet(op) || cGBY.getConf().isGroupingSetsPresent()) {	shouldDetect = false;	}	}	if (shouldDetect) {	LinkedHashSet<ReduceSinkOperator> newReduceSinkOperators = new LinkedHashSet<ReduceSinkOperator>();	String sortOrder = op.getConf().getOrder();	for (Operator<? extends OperatorDesc> parent : op.getParentOperators()) {	
operator start detecting correlation from this operator 

public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx ctx, Object... nodeOutputs) throws SemanticException {	CorrelationNodeProcCtx corrCtx = (CorrelationNodeProcCtx) ctx;	ReduceSinkOperator op = (ReduceSinkOperator) nd;	if (corrCtx.isWalked(op)) {	return null;	}	
walk to operator 

}	}	for (ReduceSinkOperator rsop : topReduceSinkOperators) {	LinkedHashSet<ReduceSinkOperator> thisBottomReduceSinkOperators = exploitJobFlowCorrelation(rsop, corrCtx, correlation);	if (thisBottomReduceSinkOperators.size() == 0) {	thisBottomReduceSinkOperators.add(rsop);	}	bottomReduceSinkOperators.addAll(thisBottomReduceSinkOperators);	}	if (!topReduceSinkOperators.containsAll(bottomReduceSinkOperators)) {	
has job flow correlation 

private NodeProcessor getDefaultProc() {	return new NodeProcessor() {	public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx ctx, Object... nodeOutputs) throws SemanticException {	Operator<? extends OperatorDesc> op = (Operator<? extends OperatorDesc>) nd;	
walk to operator no actual work to do 

========================= hive sample_3020 =========================

private void authorizeDropMultiPartition(HiveMultiPartitionAuthorizationProviderBase authorizer, final PreDropPartitionEvent context) throws AuthorizationException, HiveException {	Iterator<Partition> partitionIterator = context.getPartitionIterator();	final TableWrapper table = new TableWrapper(context.getTable());	final Iterator<org.apache.hadoop.hive.ql.metadata.Partition> qlPartitionIterator = Iterators.transform(partitionIterator, new Function<Partition, org.apache.hadoop.hive.ql.metadata.Partition>() {	public org.apache.hadoop.hive.ql.metadata.Partition apply(Partition partition) {	try {	return new PartitionWrapper(table, partition);	} catch (Exception exception) {	
could not construct partition object for 

========================= hive sample_3165 =========================

String currentQueryId = conf.get(HiveConf.ConfVars.HIVEQUERYID.varname);	if (!currentQueryId.equals(queryId)) {	if (!tableContainerMap.isEmpty()) {	synchronized (tableContainerMap) {	if (!currentQueryId.equals(queryId) && !tableContainerMap.isEmpty()) {	for (MapJoinTableContainer tableContainer: tableContainerMap.values()) {	tableContainer.clear();	}	tableContainerMap.clear();	if (LOG.isDebugEnabled()) {	
cleaned up small table cache for query 

public static void cache(Path path, MapJoinTableContainer tableContainer) {	if (tableContainerMap.putIfAbsent(path, tableContainer) == null && LOG.isDebugEnabled()) {	
cached small table file for query 

public static MapJoinTableContainer get(Path path) {	MapJoinTableContainer tableContainer = tableContainerMap.get(path);	if (tableContainer != null && LOG.isDebugEnabled()) {	
loaded small table file from cache for query 

========================= hive sample_4589 =========================

List<String> rs = runStatementOnDriver("select a, b, INPUT__FILE__NAME from " + Table.NONACIDNONBUCKET + " order by a, b, INPUT__FILE__NAME", confForTez);	String expected0[][] = {	{"1\t2", AbstractFileMergeOperator.UNION_SUDBIR_PREFIX + "1/000000_0"}, {"3\t4", AbstractFileMergeOperator.UNION_SUDBIR_PREFIX + "1/000000_0"}, {"5\t6", AbstractFileMergeOperator.UNION_SUDBIR_PREFIX + "1/000000_0"}, {"5\t6", AbstractFileMergeOperator.UNION_SUDBIR_PREFIX + "2/000000_0"}, {"7\t8", AbstractFileMergeOperator.UNION_SUDBIR_PREFIX + "2/000000_0"}, {"9\t10", AbstractFileMergeOperator.UNION_SUDBIR_PREFIX + "2/000000_0"}, };	Assert.assertEquals("Unexpected row count after ctas", expected0.length, rs.size());	for(int i = 0; i < expected0.length; i++) {	Assert.assertTrue("Actual line " + i + " bc: " + rs.get(i), rs.get(i).startsWith(expected0[i][0]));	Assert.assertTrue("Actual line(file) " + i + " bc: " + rs.get(i), rs.get(i).endsWith(expected0[i][1]));	}	runStatementOnDriver("alter table " + Table.NONACIDNONBUCKET + " SET TBLPROPERTIES ('transactional'='true')", confForTez);	rs = runStatementOnDriver("select ROW__ID, a, b, INPUT__FILE__NAME from " + Table.NONACIDNONBUCKET + " order by ROW__ID", confForTez);	
after ctas 

String expected[][] = {	{"{\"transactionid\":0,\"bucketid\":536870912,\"rowid\":0}\t1\t2", AbstractFileMergeOperator.UNION_SUDBIR_PREFIX + "1/000000_0"}, {"{\"transactionid\":0,\"bucketid\":536870912,\"rowid\":1}\t3\t4", AbstractFileMergeOperator.UNION_SUDBIR_PREFIX + "1/000000_0"}, {"{\"transactionid\":0,\"bucketid\":536870912,\"rowid\":2}\t5\t6", AbstractFileMergeOperator.UNION_SUDBIR_PREFIX + "1/000000_0"}, {"{\"transactionid\":0,\"bucketid\":536870912,\"rowid\":3}\t9\t10", AbstractFileMergeOperator.UNION_SUDBIR_PREFIX + "2/000000_0"}, {"{\"transactionid\":0,\"bucketid\":536870912,\"rowid\":4}\t7\t8", AbstractFileMergeOperator.UNION_SUDBIR_PREFIX + "2/000000_0"}, {"{\"transactionid\":0,\"bucketid\":536870912,\"rowid\":5}\t5\t6", AbstractFileMergeOperator.UNION_SUDBIR_PREFIX + "2/000000_0"}, };	Assert.assertEquals("Unexpected row count after ctas", expected.length, rs.size());	for(int i = 0; i < expected.length; i++) {	Assert.assertTrue("Actual line " + i + " bc: " + rs.get(i), rs.get(i).startsWith(expected[i][0]));	Assert.assertTrue("Actual line(file) " + i + " bc: " + rs.get(i), rs.get(i).endsWith(expected[i][1]));	}	runStatementOnDriver("update " + Table.NONACIDNONBUCKET + " set a = 70, b  = 80 where a = 7", confForTez);	runStatementOnDriver("delete from " + Table.NONACIDNONBUCKET + " where a = 5", confForTez);	rs = runStatementOnDriver("select ROW__ID, a, b, INPUT__FILE__NAME from " + Table.NONACIDNONBUCKET + " order by ROW__ID", confForTez);	
after update delete 

expectedDelDelta[i] = null;	}	}	}	for(int i = 0; i < expectedDelDelta.length; i++) {	Assert.assertNull("at " + i + " " + expectedDelDelta[i] + " not found on disk", expectedDelDelta[i]);	}	runStatementOnDriver("alter table " + Table.NONACIDNONBUCKET + " compact 'minor'", confForTez);	TestTxnCommands2.runWorker(hiveConf);	rs = runStatementOnDriver("select ROW__ID, a, b, INPUT__FILE__NAME from " + Table.NONACIDNONBUCKET + " order by ROW__ID", confForTez);	
after compact minor 

break;	}	}	}	for(int i = 0; i < expectedDelDelta2.length; i++) {	Assert.assertNull("at " + i + " " + expectedDelDelta2[i] + " not found on disk", expectedDelDelta2[i]);	}	runStatementOnDriver("alter table " + Table.NONACIDNONBUCKET + " compact 'major'", confForTez);	TestTxnCommands2.runWorker(hiveConf);	rs = runStatementOnDriver("select ROW__ID, a, b, INPUT__FILE__NAME from " + Table.NONACIDNONBUCKET + " order by ROW__ID", confForTez);	
after compact major 

Path to = new Path(TEST_WAREHOUSE_DIR + "/" +  Table.NONACIDPART+ "/p=1/" + p.getName());	while(limit-- > 0 && !fs.rename(p, to)) {	Thread.sleep(200);	}	if(limit <= 0) {	throw new IllegalStateException("Could not rename " + p + " to " + to);	}	}	This is what we expect on disk ekoifman:warehouse ekoifman$ tree nonacidpart/ nonacidpart/ └── p=1 ├── 000000_0 ├── HIVE_UNION_SUBDIR__1 │   └── 000000_0 ├── HIVE_UNION_SUBDIR_2 │   └── 000000_0 └── HIVE_UNION_SUBDIR_3 └── 000000_0 4 directories, 4 files runStatementOnDriver("alter table " + Table.NONACIDPART + " SET TBLPROPERTIES ('transactional'='true')", confForTez);	rs = runStatementOnDriver("select ROW__ID, a, b, p, INPUT__FILE__NAME from " + Table.NONACIDPART + " order by ROW__ID", confForTez);	
after acid conversion 

{"{\"transactionid\":0,\"bucketid\":536870912,\"rowid\":0}\t100\t110\t1", "nonacidpart/p=1/000000_0"}, {"{\"transactionid\":0,\"bucketid\":536870912,\"rowid\":1}\t1\t2\t1", "nonacidpart/p=1/" + AbstractFileMergeOperator.UNION_SUDBIR_PREFIX + "1/000000_0"}, {"{\"transactionid\":0,\"bucketid\":536870912,\"rowid\":2}\t3\t4\t1", "nonacidpart/p=1/" + AbstractFileMergeOperator.UNION_SUDBIR_PREFIX + "1/000000_0"}, {"{\"transactionid\":0,\"bucketid\":536870912,\"rowid\":3}\t9\t10\t1", "nonacidpart/p=1/" + AbstractFileMergeOperator.UNION_SUDBIR_PREFIX + "2/000000_0"}, {"{\"transactionid\":0,\"bucketid\":536870912,\"rowid\":4}\t7\t8\t1", "nonacidpart/p=1/" + AbstractFileMergeOperator.UNION_SUDBIR_PREFIX + "2/000000_0"}, {"{\"transactionid\":0,\"bucketid\":536870912,\"rowid\":5}\t5\t6\t1", "nonacidpart/p=1/" + AbstractFileMergeOperator.UNION_SUDBIR_PREFIX + "3/000000_0"}	};	Assert.assertEquals("Wrong row count", expected.length, rs.size());	for(int i = 0; i < expected.length; i++) {	Assert.assertTrue("Actual line " + i + " bc: " + rs.get(i), rs.get(i).startsWith(expected[i][0]));	Assert.assertTrue("Actual line(file) " + i + " bc: " + rs.get(i), rs.get(i).endsWith(expected[i][1]));	}	runStatementOnDriver("alter table " + Table.NONACIDPART + " partition (p=1) compact 'major'", confForTez);	TestTxnCommands2.runWorker(hiveConf);	rs = runStatementOnDriver("select ROW__ID, a, b, p, INPUT__FILE__NAME from " + Table.NONACIDPART + " order by ROW__ID", confForTez);	
after major compaction 

public void testCtasTezUnion() throws Exception {	HiveConf confForTez = new HiveConf(hiveConf);	confForTez.setBoolVar(HiveConf.ConfVars.HIVE_EXPLAIN_USER, false);	setupTez(confForTez);	List<String> rs0 = runStatementOnDriver("explain create table " + Table.ACIDNOBUCKET + " stored as ORC TBLPROPERTIES('transactional'='true') as " + "select a, b from " + Table.ACIDTBL + " where a <= 5 union all select a, b from " + Table.NONACIDORCTBL + " where a >= 5", confForTez);	
explain ctas 

public void testCtasTezUnion() throws Exception {	HiveConf confForTez = new HiveConf(hiveConf);	confForTez.setBoolVar(HiveConf.ConfVars.HIVE_EXPLAIN_USER, false);	setupTez(confForTez);	List<String> rs0 = runStatementOnDriver("explain create table " + Table.ACIDNOBUCKET + " stored as ORC TBLPROPERTIES('transactional'='true') as " + "select a, b from " + Table.ACIDTBL + " where a <= 5 union all select a, b from " + Table.NONACIDORCTBL + " where a >= 5", confForTez);	for (String s : rs0) {	LOG.warn(s);	}	runStatementOnDriver("create table " + Table.ACIDNOBUCKET + " stored as ORC TBLPROPERTIES('transactional'='true') as " + "select a, b from " + Table.ACIDTBL + " where a <= 5 union all select a, b from " + Table.NONACIDORCTBL + " where a >= 5", confForTez);	List<String> rs = runStatementOnDriver("select ROW__ID, a, b, INPUT__FILE__NAME from " + Table.ACIDNOBUCKET + " order by ROW__ID", confForTez);	
after ctas 

String expected[][] = {	{"{\"transactionid\":18,\"bucketid\":536870913,\"rowid\":0}\t1\t2", "/delta_0000018_0000018_0001/bucket_00000"}, {"{\"transactionid\":18,\"bucketid\":536870913,\"rowid\":1}\t3\t4", "/delta_0000018_0000018_0001/bucket_00000"}, {"{\"transactionid\":18,\"bucketid\":536870913,\"rowid\":2}\t5\t6", "/delta_0000018_0000018_0001/bucket_00000"}, {"{\"transactionid\":18,\"bucketid\":536870914,\"rowid\":0}\t9\t10", "/delta_0000018_0000018_0002/bucket_00000"}, {"{\"transactionid\":18,\"bucketid\":536870914,\"rowid\":1}\t7\t8", "/delta_0000018_0000018_0002/bucket_00000"}, {"{\"transactionid\":18,\"bucketid\":536870914,\"rowid\":2}\t5\t6", "/delta_0000018_0000018_0002/bucket_00000"}, };	Assert.assertEquals("Unexpected row count after ctas", expected.length, rs.size());	for(int i = 0; i < expected.length; i++) {	Assert.assertTrue("Actual line " + i + " bc: " + rs.get(i), rs.get(i).startsWith(expected[i][0]));	Assert.assertTrue("Actual line(file) " + i + " bc: " + rs.get(i), rs.get(i).endsWith(expected[i][1]));	}	runStatementOnDriver("update " + Table.ACIDNOBUCKET + " set a = 70, b  = 80 where a = 7", confForTez);	runStatementOnDriver("delete from " + Table.ACIDNOBUCKET + " where a = 5", confForTez);	rs = runStatementOnDriver("select ROW__ID, a, b, INPUT__FILE__NAME from " + Table.ACIDNOBUCKET + " order by ROW__ID", confForTez);	
after update delete 

expectedDelDelta[i] = null;	}	}	}	for(int i = 0; i < expectedDelDelta.length; i++) {	Assert.assertNull("at " + i + " " + expectedDelDelta[i] + " not found on disk", expectedDelDelta[i]);	}	runStatementOnDriver("alter table " + Table.ACIDNOBUCKET + " compact 'minor'", confForTez);	TestTxnCommands2.runWorker(hiveConf);	rs = runStatementOnDriver("select ROW__ID, a, b, INPUT__FILE__NAME from " + Table.ACIDNOBUCKET + " order by ROW__ID", confForTez);	
after compact minor 

break;	}	}	}	for(int i = 0; i < expectedDelDelta2.length; i++) {	Assert.assertNull("at " + i + " " + expectedDelDelta2[i] + " not found on disk", expectedDelDelta2[i]);	}	runStatementOnDriver("alter table " + Table.ACIDNOBUCKET + " compact 'major'", confForTez);	TestTxnCommands2.runWorker(hiveConf);	rs = runStatementOnDriver("select ROW__ID, a, b, INPUT__FILE__NAME from " + Table.ACIDNOBUCKET + " order by ROW__ID", confForTez);	
after compact major 

public void testInsertWithRemoveUnion() throws Exception {	int[][] values = {{1,2},{3,4},{5,6},{7,8},{9,10}};	HiveConf confForTez = new HiveConf(hiveConf);	setupTez(confForTez);	runStatementOnDriver("drop table if exists T", confForTez);	runStatementOnDriver("create table T (a int, b int) stored as ORC  TBLPROPERTIES ('transactional'='false')", confForTez);	ekoifman:apache-hive-3.0.0-SNAPSHOT-bin ekoifman$ tree  ~/dev/hiverwgit/itests/hive-unit/target/tmp/org.apache.hadoop.hive.ql.TestAcidOnTez-1505502329802/warehouse/t/.hive-staging_hive_2017-09-15_12-07-33_224_7717909516029836949-1/ └── -ext-10000 ├── HIVE_UNION_SUBDIR_1 │   └── 000000_0 ├── HIVE_UNION_SUBDIR_2 │   └── 000000_0 └── HIVE_UNION_SUBDIR_3 └── 000000_0 4 directories, 3 files runStatementOnDriver("insert into T(a,b) select a, b from " + Table.ACIDTBL + " where a between 1 and 3 group by a, b union all select a, b from " + Table.ACIDTBL + " where a between 5 and 7 union all select a, b from " + Table.ACIDTBL + " where a >= 9", confForTez);	List<String> rs = runStatementOnDriver("select a, b, INPUT__FILE__NAME from T order by a, b, INPUT__FILE__NAME", confForTez);	
before converting to acid 

for(int i = 0; i < expected.length; i++) {	Assert.assertTrue("Actual line " + i + " bc: " + rs.get(i), rs.get(i).startsWith(expected[i][0]));	Assert.assertTrue("Actual line(file) " + i + " bc: " + rs.get(i), rs.get(i).endsWith(expected[i][1]));	}	runStatementOnDriver("alter table T SET TBLPROPERTIES ('transactional'='true')", confForTez);	rs = runStatementOnDriver("select a,b from T order by a, b", confForTez);	Assert.assertEquals("After to Acid conversion", TestTxnCommands2.stringifyValues(values), rs);	runStatementOnDriver("alter table T compact 'major'", confForTez);	TestTxnCommands2.runWorker(hiveConf);	rs = runStatementOnDriver("select ROW__ID, a, b, INPUT__FILE__NAME from T order by ROW__ID", confForTez);	
after compact major of t 

public void testAcidInsertWithRemoveUnion() throws Exception {	HiveConf confForTez = new HiveConf(hiveConf);	setupTez(confForTez);	int[][] values = {{1,2},{3,4},{5,6},{7,8},{9,10}};	runStatementOnDriver("drop table if exists T", confForTez);	runStatementOnDriver("create table T (a int, b int) stored as ORC  TBLPROPERTIES ('transactional'='true')", confForTez);	ekoifman:apache-hive-3.0.0-SNAPSHOT-bin ekoifman$ tree  ~/dev/hiverwgit/itests/hive-unit/target/tmp/org.apache.hadoop.hive.ql.TestAcidOnTez-1505500035574/warehouse/t/.hive-staging_hive_2017-09-15_11-28-33_960_9111484239090506828-1/ └── -ext-10000 ├── HIVE_UNION_SUBDIR_1 │   └── 000000_0 │       ├── _orc_acid_version │       └── delta_0000019_0000019_0001 │           └── bucket_00000 ├── HIVE_UNION_SUBDIR_2 │   └── 000000_0 │       ├── _orc_acid_version │       └── delta_0000019_0000019_0002 │           └── bucket_00000 └── HIVE_UNION_SUBDIR_3 └── 000000_0 ├── _orc_acid_version └── delta_0000019_0000019_0003 └── bucket_00000 10 directories, 6 files     */ runStatementOnDriver("insert into T(a,b) select a, b from " + Table.ACIDTBL + " where a between 1 and 3 union all select a, b from " + Table.ACIDTBL + " where a between 5 and 7 union all select a, b from " + Table.ACIDTBL + " where a >= 9", confForTez);	List<String> rs = runStatementOnDriver("select ROW__ID, a, b, INPUT__FILE__NAME from T order by a, b", confForTez);	
reading acid table t 

public void testBucketedAcidInsertWithRemoveUnion() throws Exception {	HiveConf confForTez = new HiveConf(hiveConf);	setupTez(confForTez);	int[][] values = {{1,2},{2,4},{5,6},{6,8},{9,10}};	runStatementOnDriver("delete from " + Table.ACIDTBL, confForTez);	runStatementOnDriver("insert into " + Table.ACIDTBL + TestTxnCommands2.makeValuesClause(values));	runStatementOnDriver("drop table if exists T", confForTez);	With bucketed target table Union All is not removed ekoifman:apache-hive-3.0.0-SNAPSHOT-bin ekoifman$ tree  ~/dev/hiverwgit/itests/hive-unit/target/tmp/org.apache.hadoop.hive.ql.TestAcidOnTez-1505510130462/warehouse/t/.hive-staging_hive_2017-09-15_14-16-32_422_4626314315862498838-1/ └── -ext-10000 ├── 000000_0 │   ├── _orc_acid_version │   └── delta_0000021_0000021_0000 │       └── bucket_00000 └── 000001_0 ├── _orc_acid_version └── delta_0000021_0000021_0000 └── bucket_00001 5 directories, 4 files runStatementOnDriver("create table T (a int, b int) clustered by (a) into 2 buckets stored as ORC  TBLPROPERTIES ('transactional'='true')", confForTez);	runStatementOnDriver("insert into T(a,b) select a, b from " + Table.ACIDTBL + " where a between 1 and 3 union all select a, b from " + Table.ACIDTBL + " where a between 5 and 7 union all select a, b from " + Table.ACIDTBL + " where a >= 9", confForTez);	List<String> rs = runStatementOnDriver("select ROW__ID, a, b, INPUT__FILE__NAME from T order by a, b", confForTez);	
reading bucketed acid table t 

========================= hive sample_253 =========================

public void init() {	try {	HiveConf conf = new HiveConf();	conf.set(HiveConf.ConfVars.METASTORE_FILTER_HOOK.varname, DefaultMetaStoreFilterHookImpl.class.getName());	init(Hive.get(conf));	} catch (HiveException e) {	
problem connecting to the metastore when initializing the view registry 

public void init(Hive db) {	dummy = db.getConf().get(HiveConf.ConfVars.HIVE_SERVER2_MATERIALIZED_VIEWS_REGISTRY_IMPL.varname) .equals("DUMMY");	if (dummy) {	initialized.set(true);	
using dummy materialized views registry 

public void run() {	try {	for (String dbName : db.getAllDatabases()) {	for (Table mv : db.getAllMaterializedViewObjects(dbName)) {	addMaterializedView(db.getConf(), mv, OpType.LOAD);	}	}	initialized.set(true);	
materialized views registry has been initialized 

public void run() {	try {	for (String dbName : db.getAllDatabases()) {	for (Table mv : db.getAllMaterializedViewObjects(dbName)) {	addMaterializedView(db.getConf(), mv, OpType.LOAD);	}	}	initialized.set(true);	} catch (HiveException e) {	
problem connecting to the metastore when initializing the view registry 

ConcurrentMap<String, RelOptMaterialization> cq = new ConcurrentHashMap<String, RelOptMaterialization>();	if (!dummy) {	final ConcurrentMap<String, RelOptMaterialization> prevCq = materializedViews.putIfAbsent( materializedViewTable.getDbName(), cq);	if (prevCq != null) {	cq = prevCq;	}	}	final String viewQuery = materializedViewTable.getViewExpandedText();	final RelNode viewScan = createMaterializedViewScan(conf, materializedViewTable);	if (viewScan == null) {	
materialized view ignored error creating view replacement 

cq = prevCq;	}	}	final String viewQuery = materializedViewTable.getViewExpandedText();	final RelNode viewScan = createMaterializedViewScan(conf, materializedViewTable);	if (viewScan == null) {	return null;	}	final RelNode queryRel = parseQuery(conf, viewQuery);	if (queryRel == null) {	
materialized view ignored error parsing original query 

if (queryRel == null) {	return null;	}	RelOptMaterialization materialization = new RelOptMaterialization(viewScan, queryRel, null, viewScan.getTable().getQualifiedName());	if (opType == OpType.CREATE) {	cq.put(materializedViewTable.getTableName(), materialization);	} else {	cq.putIfAbsent(materializedViewTable.getTableName(), materialization);	}	if (LOG.isDebugEnabled()) {	
created materialized view for rewriting 

========================= hive sample_5009 =========================

public void run() {	try {	
cmclearer started 

FileSystem fs = cmroot.getFileSystem(conf);	FileStatus[] files = fs.listStatus(cmroot);	for (FileStatus file : files) {	long modifiedTime = file.getModificationTime();	if (now - modifiedTime > secRetain*1000) {	try {	if (fs.getXAttrs(file.getPath()).containsKey(REMAIN_IN_TRASH_TAG)) {	boolean succ = Trash.moveToAppropriateTrash(fs, file.getPath(), conf);	if (succ) {	if (LOG.isDebugEnabled()) {	
move to trash 

for (FileStatus file : files) {	long modifiedTime = file.getModificationTime();	if (now - modifiedTime > secRetain*1000) {	try {	if (fs.getXAttrs(file.getPath()).containsKey(REMAIN_IN_TRASH_TAG)) {	boolean succ = Trash.moveToAppropriateTrash(fs, file.getPath(), conf);	if (succ) {	if (LOG.isDebugEnabled()) {	}	} else {	
fail to move to trash 

boolean succ = Trash.moveToAppropriateTrash(fs, file.getPath(), conf);	if (succ) {	if (LOG.isDebugEnabled()) {	}	} else {	}	} else {	boolean succ = fs.delete(file.getPath(), false);	if (succ) {	if (LOG.isDebugEnabled()) {	
remove 

if (LOG.isDebugEnabled()) {	}	} else {	}	} else {	boolean succ = fs.delete(file.getPath(), false);	if (succ) {	if (LOG.isDebugEnabled()) {	}	} else {	
fail to remove 

}	} else {	boolean succ = fs.delete(file.getPath(), false);	if (succ) {	if (LOG.isDebugEnabled()) {	}	} else {	}	}	} catch (UnsupportedOperationException e) {	
error getting xattr for 

if (LOG.isDebugEnabled()) {	}	} else {	}	}	} catch (UnsupportedOperationException e) {	}	}	}	} catch (IOException e) {	
exception when clearing cmroot 

========================= hive sample_1949 =========================

public ExecutionController() throws IOException {	String executionContextConfigurationFile = System.getProperty(CONF_PROPERTY, "").trim();	Preconditions.checkArgument(!executionContextConfigurationFile.isEmpty(), CONF_PROPERTY + " is required");	
reading configuration from file 

public ExecutionController() throws IOException {	String executionContextConfigurationFile = System.getProperty(CONF_PROPERTY, "").trim();	Preconditions.checkArgument(!executionContextConfigurationFile.isEmpty(), CONF_PROPERTY + " is required");	mExecutionContextConfiguration = ExecutionContextConfiguration.withContext( Context.fromFile(executionContextConfigurationFile) );	
executioncontext is 

}	});	mTestQueue = new ArrayBlockingQueue<Test>(5);	mGlobalLogDir = new File(mExecutionContextConfiguration.getGlobalLogDirectory());	mTestExecutor = new TestExecutor(mExecutionContextConfiguration, mExecutionContextProvider, mTestQueue, new PTest.Builder());	mTestExecutor.setName("TestExecutor");	mTestExecutor.setDaemon(true);	mTestExecutor.start();	Runtime.getRuntime().addShutdownHook(new Thread() {	public void run() {	
shutdown hook called 

mGlobalLogDir = new File(mExecutionContextConfiguration.getGlobalLogDirectory());	mTestExecutor = new TestExecutor(mExecutionContextConfiguration, mExecutionContextProvider, mTestQueue, new PTest.Builder());	mTestExecutor.setName("TestExecutor");	mTestExecutor.setDaemon(true);	mTestExecutor.start();	Runtime.getRuntime().addShutdownHook(new Thread() {	public void run() {	try {	mTestExecutor.shutdown();	} catch (Exception e) {	
error shutting down testexecutor 

mTestExecutor.start();	Runtime.getRuntime().addShutdownHook(new Thread() {	public void run() {	try {	mTestExecutor.shutdown();	} catch (Exception e) {	}	try {	mExecutionContextProvider.close();	} catch (Exception e) {	
error shutting down executioncontextprovider 

public @ResponseBody TestStartResponse testStart(@RequestBody TestStartRequest startRequest, BindingResult result) {	
startrequest 

public @ResponseBody TestStartResponse testStart(@RequestBody TestStartRequest startRequest, BindingResult result) {	TestStartResponse startResponse = doStartTest(startRequest, result);	
startresponse 

========================= hive sample_5635 =========================

private HiveClientCache(final int timeout, final int initialCapacity, final int maxCapacity, final boolean enableStats) {	this.timeout = timeout;	this.enableStats = enableStats;	LOG.info("Initializing cache: eviction-timeout=" + timeout + " initial-capacity=" + initialCapacity + " maximum-capacity=" + maxCapacity);	CacheBuilder builder = CacheBuilder.newBuilder() .initialCapacity(initialCapacity) .maximumSize(maxCapacity) .expireAfterAccess(timeout, TimeUnit.SECONDS) .removalListener(createRemovalListener());	try {	java.lang.reflect.Method m = builder.getClass().getMethod("recordStats", null);	m.invoke(builder, null);	} catch (NoSuchMethodException e) {	
using a version of guava stats collection is enabled by default 

private HiveClientCache(final int timeout, final int initialCapacity, final int maxCapacity, final boolean enableStats) {	this.timeout = timeout;	this.enableStats = enableStats;	LOG.info("Initializing cache: eviction-timeout=" + timeout + " initial-capacity=" + initialCapacity + " maximum-capacity=" + maxCapacity);	CacheBuilder builder = CacheBuilder.newBuilder() .initialCapacity(initialCapacity) .maximumSize(maxCapacity) .expireAfterAccess(timeout, TimeUnit.SECONDS) .removalListener(createRemovalListener());	try {	java.lang.reflect.Method m = builder.getClass().getMethod("recordStats", null);	m.invoke(builder, null);	} catch (NoSuchMethodException e) {	} catch (Exception e) {	
unable to invoke recordstats method 

private RemovalListener<HiveClientCacheKey, ICacheableMetaStoreClient> createRemovalListener() {	RemovalListener<HiveClientCacheKey, ICacheableMetaStoreClient> listener = new RemovalListener<HiveClientCacheKey, ICacheableMetaStoreClient>() {	public void onRemoval(RemovalNotification<HiveClientCacheKey, ICacheableMetaStoreClient> notification) {	ICacheableMetaStoreClient hiveMetaStoreClient = notification.getValue();	if (hiveMetaStoreClient != null) {	if (LOG.isDebugEnabled()) {	
evicting client 

private void createShutdownHook() {	Thread cleanupHiveClientShutdownThread = new Thread() {	public void run() {	
cleaning up hive client cache in shutdown hook 

public synchronized void acquire() {	users.incrementAndGet();	if (users.get() > 1) {	
unexpected increment of user count beyond one 

private void release() {	if (users.get() > 0) {	users.decrementAndGet();	} else {	
unexpected attempt to decrement user count of zero 

public synchronized void setExpiredFromCache() {	if (users.get() != 0) {	
evicted client has non zero user count 

public void tearDown() {	try {	if (!isClosed) {	super.close();	}	isClosed = true;	} catch (Exception e) {	
error closing hive metastore client ignored 

========================= hive sample_711 =========================

protected void openInternal(String[] additionalFilesNotFromConf, boolean isAsync, LogHelper console, HiveResources resources) throws IOException, LoginException, URISyntaxException, TezException {	String confQueueName = conf.get(TezConfiguration.TEZ_QUEUE_NAME);	if (queueName != null && !queueName.equals(confQueueName)) {	
resetting a queue name that was already set was now 

protected void openInternal(String[] additionalFilesNotFromConf, boolean isAsync, LogHelper console, HiveResources resources) throws IOException, LoginException, URISyntaxException, TezException {	String confQueueName = conf.get(TezConfiguration.TEZ_QUEUE_NAME);	if (queueName != null && !queueName.equals(confQueueName)) {	}	this.queueName = confQueueName;	this.doAsEnabled = conf.getBoolVar(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS);	final boolean llapMode = "llap".equalsIgnoreCase(HiveConf.getVar( conf, HiveConf.ConfVars.HIVE_EXECUTION_MODE));	UserGroupInformation ugi = Utils.getUGI();	user = ugi.getShortUserName();	
user of session id is 

if (queueName != null && !queueName.equals(confQueueName)) {	}	this.queueName = confQueueName;	this.doAsEnabled = conf.getBoolVar(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS);	final boolean llapMode = "llap".equalsIgnoreCase(HiveConf.getVar( conf, HiveConf.ConfVars.HIVE_EXECUTION_MODE));	UserGroupInformation ugi = Utils.getUGI();	user = ugi.getShortUserName();	tezScratchDir = createTezDir(sessionId, null);	if (resources != null) {	this.resources = resources;	
setting resources to 

this.doAsEnabled = conf.getBoolVar(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS);	final boolean llapMode = "llap".equalsIgnoreCase(HiveConf.getVar( conf, HiveConf.ConfVars.HIVE_EXECUTION_MODE));	UserGroupInformation ugi = Utils.getUGI();	user = ugi.getShortUserName();	tezScratchDir = createTezDir(sessionId, null);	if (resources != null) {	this.resources = resources;	} else {	this.resources = new HiveResources(createTezDir(sessionId, "resources"));	ensureLocalResources(conf, additionalFilesNotFromConf);	
created new resources 

} else {	servicePluginsDescriptor = ServicePluginsDescriptor.create(true);	}	if (HiveConf.getBoolVar(conf, ConfVars.HIVE_PREWARM_ENABLED)) {	int n = HiveConf.getIntVar(conf, ConfVars.HIVE_PREWARM_NUM_CONTAINERS);	n = Math.max(tezConfig.getInt( TezConfiguration.TEZ_AM_SESSION_MIN_HELD_CONTAINERS, TezConfiguration.TEZ_AM_SESSION_MIN_HELD_CONTAINERS_DEFAULT), n);	tezConfig.setInt(TezConfiguration.TEZ_AM_SESSION_MIN_HELD_CONTAINERS, n);	}	setupSessionAcls(tezConfig, conf);	final TezClient session = TezClient.newBuilder("HIVE-" + sessionId, tezConfig) .setIsSession(true).setLocalResources(commonLocalResources) .setCredentials(llapCredentials).setServicePluginDescriptor(servicePluginsDescriptor) .build();	
opening new tez session id scratch dir 

TezJobMonitor.initShutdownHook();	if (!isAsync) {	startSessionAndContainers(session, conf, commonLocalResources, tezConfig, false);	this.session = session;	} else {	FutureTask<TezClient> sessionFuture = new FutureTask<>(new Callable<TezClient>() {	public TezClient call() throws Exception {	try {	return startSessionAndContainers(session, conf, commonLocalResources, tezConfig, true);	} catch (Throwable t) {	
failed to start tez session 

if (isInHs2) {	coordinator = LlapCoordinator.getInstance();	if (coordinator == null) {	throw new IOException("LLAP coordinator not initialized; cannot get LLAP tokens");	}	token = coordinator.getLocalTokenClient(conf, user).createToken(null, null, false);	} else {	token = new LlapTokenClient(conf).getDelegationToken(null);	}	if (LOG.isInfoEnabled()) {	
obtained a llap token 

private TezClient startSessionAndContainers(TezClient session, HiveConf conf, Map<String, LocalResource> commonLocalResources, TezConfiguration tezConfig, boolean isOnThread) throws TezException, IOException {	session.start();	boolean isSuccessful = false;	try {	if (HiveConf.getBoolVar(conf, ConfVars.HIVE_PREWARM_ENABLED)) {	int n = HiveConf.getIntVar(conf, ConfVars.HIVE_PREWARM_NUM_CONTAINERS);	
prewarming containers id scratch dir 

session.start();	boolean isSuccessful = false;	try {	if (HiveConf.getBoolVar(conf, ConfVars.HIVE_PREWARM_ENABLED)) {	int n = HiveConf.getIntVar(conf, ConfVars.HIVE_PREWARM_NUM_CONTAINERS);	PreWarmVertex prewarmVertex = utils.createPreWarmVertex( tezConfig, n, commonLocalResources);	try {	session.preWarm(prewarmVertex);	} catch (IOException ie) {	if (!isOnThread && ie.getMessage().contains("Interrupted while waiting")) {	
hive prewarm threw an exception 

private static void closeAndIgnoreExceptions(TezClient session) {	try {	session.stop();	} catch (SessionNotRunning nr) {	} catch (IOException | TezException ex) {	
failed to close tez session after failure to initialize 

protected final void cleanupDagResources() throws IOException {	
attemting to clean up resources for 

private LocalResource createJarLocalResource(String localJarPath) throws IOException, LoginException, IllegalArgumentException {	FileStatus destDirStatus = utils.getHiveJarDirectory(conf);	assert destDirStatus != null;	Path destDirPath = destDirStatus.getPath();	Path localFile = new Path(localJarPath);	String sha = getSha(localFile);	String destFileName = localFile.getName();	destFileName = FilenameUtils.removeExtension(destFileName) + "-" + sha + FilenameUtils.EXTENSION_SEPARATOR + FilenameUtils.getExtension(destFileName);	if (LOG.isDebugEnabled()) {	
the destination file name for is 

FileStatus fileStatus = localFs.getFileStatus(localFile);	String key = getKey(fileStatus);	String sha256 = shaCache.getIfPresent(key);	if (sha256 == null) {	FSDataInputStream is = null;	try {	is = localFs.open(localFile);	long start = System.currentTimeMillis();	sha256 = DigestUtils.sha256Hex(is);	long end = System.currentTimeMillis();	
computed sha for file of length in ms 

public Path replaceHiveResources(HiveResources resources, boolean isAsync) {	Path dir = null;	if (this.resources != null) {	dir = this.resources.dagResourcesDir;	if (!isAsync) {	try {	dir.getFileSystem(conf).delete(dir, true);	} catch (Exception ex) {	
failed to delete the old resources directory ignoring 

========================= hive sample_4024 =========================

transformations.add(new GroupByOptimizer());	}	transformations.add(new ColumnPruner());	if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVECOUNTDISTINCTOPTIMIZER) && (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVE_IN_TEST) || isTezExecEngine)) {	transformations.add(new CountDistinctRewriteProc());	}	if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVE_OPTIMIZE_SKEWJOIN_COMPILETIME)) {	if (!isTezExecEngine) {	transformations.add(new SkewJoinOptimizer());	} else {	
skew join is currently not supported in tez disabling the skew join optimization 

========================= hive sample_2959 =========================

public static int getNumBitVectorsForNDVEstimation(Configuration conf) throws Exception {	int numBitVectors;	float percentageError = HiveConf.getFloatVar(conf, HiveConf.ConfVars.HIVE_STATS_NDV_ERROR);	if (percentageError < 0.0) {	throw new Exception("hive.stats.ndv.error can't be negative");	} else if (percentageError <= 2.4) {	numBitVectors = 1024;	
lowest error achievable is but error requested is 

public static int getNumBitVectorsForNDVEstimation(Configuration conf) throws Exception {	int numBitVectors;	float percentageError = HiveConf.getFloatVar(conf, HiveConf.ConfVars.HIVE_STATS_NDV_ERROR);	if (percentageError < 0.0) {	throw new Exception("hive.stats.ndv.error can't be negative");	} else if (percentageError <= 2.4) {	numBitVectors = 1024;	
choosing bit vectors 

public static int getNumBitVectorsForNDVEstimation(Configuration conf) throws Exception {	int numBitVectors;	float percentageError = HiveConf.getFloatVar(conf, HiveConf.ConfVars.HIVE_STATS_NDV_ERROR);	if (percentageError < 0.0) {	throw new Exception("hive.stats.ndv.error can't be negative");	} else if (percentageError <= 2.4) {	numBitVectors = 1024;	} else if (percentageError <= 3.4 ) {	numBitVectors = 1024;	
error requested is 

public static int getNumBitVectorsForNDVEstimation(Configuration conf) throws Exception {	int numBitVectors;	float percentageError = HiveConf.getFloatVar(conf, HiveConf.ConfVars.HIVE_STATS_NDV_ERROR);	if (percentageError < 0.0) {	throw new Exception("hive.stats.ndv.error can't be negative");	} else if (percentageError <= 2.4) {	numBitVectors = 1024;	} else if (percentageError <= 3.4 ) {	numBitVectors = 1024;	
choosing bit vectors 

int numBitVectors;	float percentageError = HiveConf.getFloatVar(conf, HiveConf.ConfVars.HIVE_STATS_NDV_ERROR);	if (percentageError < 0.0) {	throw new Exception("hive.stats.ndv.error can't be negative");	} else if (percentageError <= 2.4) {	numBitVectors = 1024;	} else if (percentageError <= 3.4 ) {	numBitVectors = 1024;	} else if (percentageError <= 4.8) {	numBitVectors = 512;	
error requested is 

int numBitVectors;	float percentageError = HiveConf.getFloatVar(conf, HiveConf.ConfVars.HIVE_STATS_NDV_ERROR);	if (percentageError < 0.0) {	throw new Exception("hive.stats.ndv.error can't be negative");	} else if (percentageError <= 2.4) {	numBitVectors = 1024;	} else if (percentageError <= 3.4 ) {	numBitVectors = 1024;	} else if (percentageError <= 4.8) {	numBitVectors = 512;	
choosing bit vectors 

if (percentageError < 0.0) {	throw new Exception("hive.stats.ndv.error can't be negative");	} else if (percentageError <= 2.4) {	numBitVectors = 1024;	} else if (percentageError <= 3.4 ) {	numBitVectors = 1024;	} else if (percentageError <= 4.8) {	numBitVectors = 512;	} else if (percentageError <= 6.8) {	numBitVectors = 256;	
error requested is 

if (percentageError < 0.0) {	throw new Exception("hive.stats.ndv.error can't be negative");	} else if (percentageError <= 2.4) {	numBitVectors = 1024;	} else if (percentageError <= 3.4 ) {	numBitVectors = 1024;	} else if (percentageError <= 4.8) {	numBitVectors = 512;	} else if (percentageError <= 6.8) {	numBitVectors = 256;	
choosing bit vectors 

} else if (percentageError <= 2.4) {	numBitVectors = 1024;	} else if (percentageError <= 3.4 ) {	numBitVectors = 1024;	} else if (percentageError <= 4.8) {	numBitVectors = 512;	} else if (percentageError <= 6.8) {	numBitVectors = 256;	} else if (percentageError <= 9.7) {	numBitVectors = 128;	
error requested is 

} else if (percentageError <= 2.4) {	numBitVectors = 1024;	} else if (percentageError <= 3.4 ) {	numBitVectors = 1024;	} else if (percentageError <= 4.8) {	numBitVectors = 512;	} else if (percentageError <= 6.8) {	numBitVectors = 256;	} else if (percentageError <= 9.7) {	numBitVectors = 128;	
choosing bit vectors 

} else if (percentageError <= 3.4 ) {	numBitVectors = 1024;	} else if (percentageError <= 4.8) {	numBitVectors = 512;	} else if (percentageError <= 6.8) {	numBitVectors = 256;	} else if (percentageError <= 9.7) {	numBitVectors = 128;	} else if (percentageError <= 13.8) {	numBitVectors = 64;	
error requested is 

} else if (percentageError <= 3.4 ) {	numBitVectors = 1024;	} else if (percentageError <= 4.8) {	numBitVectors = 512;	} else if (percentageError <= 6.8) {	numBitVectors = 256;	} else if (percentageError <= 9.7) {	numBitVectors = 128;	} else if (percentageError <= 13.8) {	numBitVectors = 64;	
choosing bit vectors 

} else if (percentageError <= 4.8) {	numBitVectors = 512;	} else if (percentageError <= 6.8) {	numBitVectors = 256;	} else if (percentageError <= 9.7) {	numBitVectors = 128;	} else if (percentageError <= 13.8) {	numBitVectors = 64;	} else if (percentageError <= 19.6) {	numBitVectors = 32;	
error requested is 

} else if (percentageError <= 4.8) {	numBitVectors = 512;	} else if (percentageError <= 6.8) {	numBitVectors = 256;	} else if (percentageError <= 9.7) {	numBitVectors = 128;	} else if (percentageError <= 13.8) {	numBitVectors = 64;	} else if (percentageError <= 19.6) {	numBitVectors = 32;	
choosing bit vectors 

} else if (percentageError <= 6.8) {	numBitVectors = 256;	} else if (percentageError <= 9.7) {	numBitVectors = 128;	} else if (percentageError <= 13.8) {	numBitVectors = 64;	} else if (percentageError <= 19.6) {	numBitVectors = 32;	} else if (percentageError <= 28.2) {	numBitVectors = 16;	
error requested is 

} else if (percentageError <= 6.8) {	numBitVectors = 256;	} else if (percentageError <= 9.7) {	numBitVectors = 128;	} else if (percentageError <= 13.8) {	numBitVectors = 64;	} else if (percentageError <= 19.6) {	numBitVectors = 32;	} else if (percentageError <= 28.2) {	numBitVectors = 16;	
choosing bit vectors 

} else if (percentageError <= 9.7) {	numBitVectors = 128;	} else if (percentageError <= 13.8) {	numBitVectors = 64;	} else if (percentageError <= 19.6) {	numBitVectors = 32;	} else if (percentageError <= 28.2) {	numBitVectors = 16;	} else if (percentageError <= 40.9) {	numBitVectors = 8;	
error requested is 

} else if (percentageError <= 9.7) {	numBitVectors = 128;	} else if (percentageError <= 13.8) {	numBitVectors = 64;	} else if (percentageError <= 19.6) {	numBitVectors = 32;	} else if (percentageError <= 28.2) {	numBitVectors = 16;	} else if (percentageError <= 40.9) {	numBitVectors = 8;	
choosing bit vectors 

} else if (percentageError <= 13.8) {	numBitVectors = 64;	} else if (percentageError <= 19.6) {	numBitVectors = 32;	} else if (percentageError <= 28.2) {	numBitVectors = 16;	} else if (percentageError <= 40.9) {	numBitVectors = 8;	} else if (percentageError <= 61.0) {	numBitVectors = 4;	
error requested is 

} else if (percentageError <= 13.8) {	numBitVectors = 64;	} else if (percentageError <= 19.6) {	numBitVectors = 32;	} else if (percentageError <= 28.2) {	numBitVectors = 16;	} else if (percentageError <= 40.9) {	numBitVectors = 8;	} else if (percentageError <= 61.0) {	numBitVectors = 4;	
choosing bit vectors 

} else if (percentageError <= 19.6) {	numBitVectors = 32;	} else if (percentageError <= 28.2) {	numBitVectors = 16;	} else if (percentageError <= 40.9) {	numBitVectors = 8;	} else if (percentageError <= 61.0) {	numBitVectors = 4;	} else {	numBitVectors = 2;	
error requested is 

} else if (percentageError <= 19.6) {	numBitVectors = 32;	} else if (percentageError <= 28.2) {	numBitVectors = 16;	} else if (percentageError <= 40.9) {	numBitVectors = 8;	} else if (percentageError <= 61.0) {	numBitVectors = 4;	} else {	numBitVectors = 2;	
choosing bit vectors 

========================= hive sample_1363 =========================

columns = new String[columnNames.size()];	types = new PrimitiveTypeInfo[columnNames.size()];	for (int i = 0; i < columnTypes.size(); ++i) {	columns[i] = columnNames.get(i);	types[i] = columnTypes.get(i);	inspectors.add(PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(types[i]));	}	inspector = ObjectInspectorFactory.getStandardStructObjectInspector(columnNames, inspectors);	}	if (LOG.isDebugEnabled()) {	
druidserde initialized with t columns t types 

========================= hive sample_24 =========================

public void serviceStart() throws Exception {	
using shuffleport 

DataInputBuffer dib = new DataInputBuffer();	byte[] tokenBytes = request.getCredentialsBinary().toByteArray();	dib.reset(tokenBytes, tokenBytes.length);	credentials.readTokenStorageStream(dib);	Token<JobTokenIdentifier> jobToken = TokenCache.getSessionToken(credentials);	LlapNodeId amNodeId = LlapNodeId.getInstance(request.getAmHost(), request.getAmPort());	QueryFragmentInfo fragmentInfo = queryTracker.registerFragment( queryIdentifier, qIdProto.getApplicationIdString(), dagId, vertex.getDagName(), vertex.getHiveQueryId(), dagIdentifier, vertex.getVertexName(), request.getFragmentNumber(), request.getAttemptNumber(), vertex.getUser(), vertex, jobToken, fragmentIdString, tokenInfo, amNodeId);	String[] localDirs = fragmentInfo.getLocalDirs();	Preconditions.checkNotNull(localDirs);	if (LOG.isDebugEnabled()) {	
dirs are 

Preconditions.checkNotNull(localDirs);	if (LOG.isDebugEnabled()) {	}	Configuration callableConf = new Configuration(getConfig());	UserGroupInformation fsTaskUgi = fsUgiFactory == null ? null : fsUgiFactory.createUgi();	boolean isGuaranteed = request.hasIsGuaranteed() && request.getIsGuaranteed();	WmFragmentCounters wmCounters = new WmFragmentCounters( FragmentCountersMap.getCountersForFragment(fragmentId));	TaskRunnerCallable callable = new TaskRunnerCallable(request, fragmentInfo, callableConf, new ExecutionContextImpl(localAddress.get().getHostName()), env, credentials, memoryPerExecutor, amReporter, confParams, metrics, killedTaskHandler, this, tezHadoopShim, attemptId, vertex, initialEvent, fsTaskUgi, completionListener, socketFactory, isGuaranteed, wmCounters);	submissionState = executorService.schedule(callable);	if (LOG.isInfoEnabled()) {	
submissionstate for 

public SourceStateUpdatedResponseProto sourceStateUpdated( SourceStateUpdatedRequestProto request) throws IOException {	
processing state update 

public QueryCompleteResponseProto queryComplete( QueryCompleteRequestProto request) throws IOException {	QueryIdentifier queryIdentifier = new QueryIdentifier(request.getQueryIdentifier().getApplicationIdString(), request.getQueryIdentifier().getDagIndex());	
processing querycomplete notification for 

public QueryCompleteResponseProto queryComplete( QueryCompleteRequestProto request) throws IOException {	QueryIdentifier queryIdentifier = new QueryIdentifier(request.getQueryIdentifier().getApplicationIdString(), request.getQueryIdentifier().getDagIndex());	QueryInfo queryInfo = queryTracker.queryComplete(queryIdentifier, request.getDeleteDelay(), false);	if (queryInfo != null) {	List<QueryFragmentInfo> knownFragments = queryInfo.getRegisteredFragments();	LOG.info("DBG: Pending fragment count for completed query {} = {}", queryIdentifier, knownFragments.size());	for (QueryFragmentInfo fragmentInfo : knownFragments) {	
issuing killfragment for completed query 

public TerminateFragmentResponseProto terminateFragment( TerminateFragmentRequestProto request) throws IOException {	String fragmentId = request.getFragmentIdentifierString();	
dbg received terminatefragment request for 

public UpdateFragmentResponseProto updateFragment( UpdateFragmentRequestProto request) throws IOException {	String fragmentId = request.getFragmentIdentifierString();	boolean isGuaranteed = request.hasIsGuaranteed() && request.getIsGuaranteed();	
dbg received updatefragment request for 

public void queryFailed(QueryIdentifier queryIdentifier) {	
processing query failed notification for 

public void queryFailed(QueryIdentifier queryIdentifier) {	List<QueryFragmentInfo> knownFragments;	knownFragments = queryTracker.getRegisteredFragments(queryIdentifier);	LOG.info("DBG: Pending fragment count for failed query {} = {}", queryIdentifier, knownFragments.size());	for (QueryFragmentInfo fragmentInfo : knownFragments) {	
dbg issuing killfragment for failed query 

========================= hive sample_2220 =========================

public void initialize(Configuration job, Properties tbl) throws SerDeException {	cachedObjectInspector = ObjectInspectorFactory .getReflectionObjectInspector(S3LogStruct.class, ObjectInspectorFactory.ObjectInspectorOptions.JAVA);	
initialized 

========================= hive sample_1119 =========================

out = new ObjectOutputStream(new BufferedOutputStream(fsout));	out.writeInt(conf.getTargetInfos().size());	for (SparkPartitionPruningSinkDesc.DPPTargetInfo info : conf.getTargetInfos()) {	out.writeUTF(info.columnName);	}	buffer.writeTo(out);	} catch (Exception e) {	try {	fs.delete(path, false);	} catch (Exception ex) {	
exception happened while trying to clean partial file 

}	buffer.writeTo(out);	} catch (Exception e) {	try {	fs.delete(path, false);	} catch (Exception ex) {	}	throw e;	} finally {	if (out != null) {	
flushed to file 

========================= hive sample_3574 =========================

boolean useSsl = hiveConf.getBoolVar(ConfVars.HIVE_SERVER2_USE_SSL);	String schemeName = useSsl ? "https" : "http";	if (useSsl) {	String keyStorePath = hiveConf.getVar(ConfVars.HIVE_SERVER2_SSL_KEYSTORE_PATH).trim();	String keyStorePassword = ShimLoader.getHadoopShims().getPassword(hiveConf, HiveConf.ConfVars.HIVE_SERVER2_SSL_KEYSTORE_PASSWORD.varname);	if (keyStorePath.isEmpty()) {	throw new IllegalArgumentException(ConfVars.HIVE_SERVER2_SSL_KEYSTORE_PATH.varname + " Not configured for SSL connection");	}	SslContextFactory sslContextFactory = new SslContextFactory();	String[] excludedProtocols = hiveConf.getVar(ConfVars.HIVE_SSL_PROTOCOL_BLACKLIST).split(",");	
http server ssl adding excluded protocols 

hiveAuthFactory = new HiveAuthFactory(hiveConf);	TProcessor processor = new TCLIService.Processor<Iface>(this);	TProtocolFactory protocolFactory = new TBinaryProtocol.Factory();	UserGroupInformation serviceUGI = cliService.getServiceUGI();	UserGroupInformation httpUGI = cliService.getHttpUGI();	String authType = hiveConf.getVar(ConfVars.HIVE_SERVER2_AUTHENTICATION);	TServlet thriftHttpServlet = new ThriftHttpServlet(processor, protocolFactory, authType, serviceUGI, httpUGI, hiveAuthFactory);	final ServletContextHandler context = new ServletContextHandler( ServletContextHandler.SESSIONS);	context.setContextPath("/");	if (hiveConf.getBoolean(ConfVars.HIVE_SERVER2_XSRF_FILTER_ENABLED.varname, false)){	
xsrf filter enabled 

TProcessor processor = new TCLIService.Processor<Iface>(this);	TProtocolFactory protocolFactory = new TBinaryProtocol.Factory();	UserGroupInformation serviceUGI = cliService.getServiceUGI();	UserGroupInformation httpUGI = cliService.getHttpUGI();	String authType = hiveConf.getVar(ConfVars.HIVE_SERVER2_AUTHENTICATION);	TServlet thriftHttpServlet = new ThriftHttpServlet(processor, protocolFactory, authType, serviceUGI, httpUGI, hiveAuthFactory);	final ServletContextHandler context = new ServletContextHandler( ServletContextHandler.SESSIONS);	context.setContextPath("/");	if (hiveConf.getBoolean(ConfVars.HIVE_SERVER2_XSRF_FILTER_ENABLED.varname, false)){	} else {	
xsrf filter disabled 

httpServer.setHandler(gzipHandler);	} else {	httpServer.setHandler(context);	}	context.addServlet(new ServletHolder(thriftHttpServlet), httpPath);	httpServer.start();	String msg = "Started " + ThriftHttpCLIService.class.getSimpleName() + " in " + schemeName + " mode on port " + portNum + " path=" + httpPath + " with " + minWorkerThreads + "..." + maxWorkerThreads + " worker threads";	LOG.info(msg);	httpServer.join();	} catch (Throwable t) {	
error starting could not start 

========================= hive sample_2365 =========================

public VectorReduceSinkEmptyKeyOperator(CompilationOpContext ctx, OperatorDesc conf, VectorizationContext vContext, VectorDesc vectorDesc) throws HiveException {	super(ctx, conf, vContext, vectorDesc);	
vectorreducesinkemptykeyoperator constructor vectorreducesinkinfo 

public void process(Object row, int tag) throws HiveException {	try {	VectorizedRowBatch batch = (VectorizedRowBatch) row;	batchCounter++;	if (batch.size == 0) {	if (LOG.isDebugEnabled()) {	
batch empty 

========================= hive sample_4198 =========================

static JavaDataModel getModelForSystem() {	String props = null;	try {	props = System.getProperty("sun.arch.data.model");	} catch (Exception e) {	
failed to determine java data model defaulting to 

========================= hive sample_1230 =========================

public int run() {	int result = 0;	boolean error = false;	List<String> messages = Lists.newArrayList();	Map<String, Long> elapsedTimes = Maps.newTreeMap();	try {	
running tests with 

phase.execute();	} finally {	long elapsedTime = TimeUnit.MINUTES.convert((System.currentTimeMillis() - start), TimeUnit.MILLISECONDS);	elapsedTimes.put(phase.getClass().getSimpleName(), elapsedTime);	}	}	if(!mFailedTests.isEmpty()) {	throw new TestsFailedException(mFailedTests.size() + " tests failed");	}	} catch(Throwable throwable) {	
test run exited with an unexpected error 

hostExecutor.shutdownNow();	if(hostExecutor.isBad()) {	mExecutionContext.addBadHost(hostExecutor.getHost());	}	}	mSshCommandExecutor.shutdownNow();	mRsyncCommandExecutor.shutdownNow();	mExecutor.shutdownNow();	SortedSet<String> failedTests = new TreeSet<String>(mFailedTests);	if(failedTests.isEmpty()) {	
d failed tests 

if(hostExecutor.isBad()) {	mExecutionContext.addBadHost(hostExecutor.getHost());	}	}	mSshCommandExecutor.shutdownNow();	mRsyncCommandExecutor.shutdownNow();	mExecutor.shutdownNow();	SortedSet<String> failedTests = new TreeSet<String>(mFailedTests);	if(failedTests.isEmpty()) {	} else {	
d failed tests 

mSshCommandExecutor.shutdownNow();	mRsyncCommandExecutor.shutdownNow();	mExecutor.shutdownNow();	SortedSet<String> failedTests = new TreeSet<String>(mFailedTests);	if(failedTests.isEmpty()) {	} else {	}	for(String failingTestName : failedTests) {	mLogger.warn(failingTestName);	}	
executed tests 

mRsyncCommandExecutor.shutdownNow();	mExecutor.shutdownNow();	SortedSet<String> failedTests = new TreeSet<String>(mFailedTests);	if(failedTests.isEmpty()) {	} else {	}	for(String failingTestName : failedTests) {	mLogger.warn(failingTestName);	}	for(Map.Entry<String, Long> entry : elapsedTimes.entrySet()) {	
perf phase s took d minutes 

private void publishJiraComment(boolean error, List<String> messages, SortedSet<String> failedTests, Set<String> addedTests) {	if(mConfiguration.getJiraName().isEmpty()) {	
skipping jira comment as name is empty 

private void publishJiraComment(boolean error, List<String> messages, SortedSet<String> failedTests, Set<String> addedTests) {	if(mConfiguration.getJiraName().isEmpty()) {	return;	}	if(mConfiguration.getJiraUrl().isEmpty()) {	
skipping jira comment as url is empty 

private void publishJiraComment(boolean error, List<String> messages, SortedSet<String> failedTests, Set<String> addedTests) {	if(mConfiguration.getJiraName().isEmpty()) {	return;	}	if(mConfiguration.getJiraUrl().isEmpty()) {	return;	}	if(mConfiguration.getJiraUser().isEmpty()) {	
skipping jira comment as user is empty 

if(mConfiguration.getJiraName().isEmpty()) {	return;	}	if(mConfiguration.getJiraUrl().isEmpty()) {	return;	}	if(mConfiguration.getJiraUser().isEmpty()) {	return;	}	if(mConfiguration.getJiraPassword().isEmpty()) {	
skipping jira comment as password is empty 

}	if(mConfiguration.getJiraUrl().isEmpty()) {	return;	}	if(mConfiguration.getJiraUser().isEmpty()) {	return;	}	if(mConfiguration.getJiraPassword().isEmpty()) {	return;	}	
added tests 

public static void main(String[] args) throws Exception {	
args 

========================= hive sample_5623 =========================

private void terminateRequest() {	if (closed || requestInfo == null) {	
no current request to terminate 

private void terminateRequest() {	if (closed || requestInfo == null) {	return;	}	TerminateFragmentRequestProto.Builder builder = TerminateFragmentRequestProto.newBuilder();	builder.setQueryIdentifier(requestInfo.queryIdentifierProto);	builder.setFragmentIdentifierString(requestInfo.taskAttemptId);	final String taskAttemptId = requestInfo.taskAttemptId;	communicator.sendTerminateFragment(builder.build(), requestInfo.hostname, requestInfo.port, new LlapProtocolClientProxy.ExecuteRequestCallback<TerminateFragmentResponseProto>() {	public void setResponse(TerminateFragmentResponseProto response) {	
received terminate response for 

private void retrySubmission() {	attemptNum++;	long retryDelay = determineRetryDelay();	
queueing fragment for resubmission attempt delay 

private void registerClient() {	SharedUmbilicalServer umbilicalServer = SharedUmbilicalServer.getInstance(conf);	LlapTaskUmbilicalExternalClient prevVal = umbilicalServer.umbilicalProtocol.registeredClients.putIfAbsent(requestInfo.taskAttemptId, this);	if (prevVal != null) {	
unexpected fragment is already registered 

long currentTime = System.currentTimeMillis();	List<LlapTaskUmbilicalExternalClient> timedOutTasks = new ArrayList<LlapTaskUmbilicalExternalClient>();	for (Map.Entry<String, LlapTaskUmbilicalExternalClient> entry : umbilicalImpl.registeredClients.entrySet()) {	LlapTaskUmbilicalExternalClient client = entry.getValue();	if (client.isTimedOut(currentTime)) {	timedOutTasks.add(client);	}	}	for (LlapTaskUmbilicalExternalClient timedOutTask : timedOutTasks) {	String taskAttemptId = timedOutTask.requestInfo.taskAttemptId;	
running taskattemptid timed out 

LOG.debug("Received heartbeat from container, request=" + request);	}	TezHeartbeatResponse response = new TezHeartbeatResponse();	response.setLastRequestId(request.getRequestId());	TezTaskAttemptID taskAttemptId = request.getCurrentTaskAttemptID();	String taskAttemptIdString = taskAttemptId.toString();	updateHeartbeatInfo(taskAttemptIdString);	List<TezEvent> tezEvents = null;	LlapTaskUmbilicalExternalClient client = registeredClients.get(taskAttemptIdString);	if (client == null) {	
unexpected heartbeat from 

response.setNextFromEventId(0);	response.setNextPreRoutedEventId(0);	response.setEvents(tezEvents);	List<TezEvent> inEvents = request.getEvents();	if (LOG.isDebugEnabled()) {	LOG.debug("Heartbeat from " + taskAttemptIdString + " events: " + (inEvents != null ? inEvents.size() : -1));	}	for (TezEvent tezEvent : ListUtils.emptyIfNull(inEvents)) {	EventType eventType = tezEvent.getEventType();	switch (eventType) {	
task completed event for 

response.setEvents(tezEvents);	List<TezEvent> inEvents = request.getEvents();	if (LOG.isDebugEnabled()) {	LOG.debug("Heartbeat from " + taskAttemptIdString + " events: " + (inEvents != null ? inEvents.size() : -1));	}	for (TezEvent tezEvent : ListUtils.emptyIfNull(inEvents)) {	EventType eventType = tezEvent.getEventType();	switch (eventType) {	shouldUnregisterClient = true;	break;	
task failed event for 

if (LOG.isDebugEnabled()) {	LOG.debug("Heartbeat from " + taskAttemptIdString + " events: " + (inEvents != null ? inEvents.size() : -1));	}	for (TezEvent tezEvent : ListUtils.emptyIfNull(inEvents)) {	EventType eventType = tezEvent.getEventType();	switch (eventType) {	shouldUnregisterClient = true;	break;	shouldUnregisterClient = true;	break;	
task update event for 

LOG.debug("Heartbeat from " + taskAttemptIdString + " events: " + (inEvents != null ? inEvents.size() : -1));	}	for (TezEvent tezEvent : ListUtils.emptyIfNull(inEvents)) {	EventType eventType = tezEvent.getEventType();	switch (eventType) {	shouldUnregisterClient = true;	break;	shouldUnregisterClient = true;	break;	break;	
unhandled event type 

}	}	if (shouldUnregisterClient) {	client.unregisterClient();	}	try {	if (client.responder != null) {	client.responder.heartbeat(request);	}	} catch (Exception err) {	
error during responder execution 

public void nodeHeartbeat( Text hostname, Text uniqueId, int port, TezAttemptArray aw) throws IOException {	if (LOG.isDebugEnabled()) {	
node heartbeat from 

public void taskKilled(TezTaskAttemptID taskAttemptId) throws IOException {	String taskAttemptIdString = taskAttemptId.toString();	LlapTaskUmbilicalExternalClient client = registeredClients.get(taskAttemptIdString);	if (client != null) {	if (client.requestInfo.state == RequestState.PENDING) {	
received task kill for which is still in pending state retry submission 

public void taskKilled(TezTaskAttemptID taskAttemptId) throws IOException {	String taskAttemptIdString = taskAttemptId.toString();	LlapTaskUmbilicalExternalClient client = registeredClients.get(taskAttemptIdString);	if (client != null) {	if (client.requestInfo.state == RequestState.PENDING) {	client.retrySubmission();	} else {	try {	
task killed 

if (client != null) {	if (client.requestInfo.state == RequestState.PENDING) {	client.retrySubmission();	} else {	try {	client.unregisterClient();	if (client.responder != null) {	client.responder.taskKilled(taskAttemptId);	}	} catch (Exception err) {	
error during responder execution 

} else {	try {	client.unregisterClient();	if (client.responder != null) {	client.responder.taskKilled(taskAttemptId);	}	} catch (Exception err) {	}	}	} else {	
received task killed notification for task which is not currently being tracked 

private void updateHeartbeatInfo(String taskAttemptId) {	int updateCount = 0;	LlapTaskUmbilicalExternalClient registeredClient = registeredClients.get(taskAttemptId);	if (registeredClient != null) {	registeredClient.setLastHeartbeat(System.currentTimeMillis());	updateCount++;	}	if (updateCount == 0) {	
no tasks found for heartbeat from taskattemptid 

TezTaskAttemptID ta = TezTaskAttemptID.fromString(registeredClient.requestInfo.taskAttemptId);	if (attempts.contains(ta)) {	registeredClient.setLastHeartbeat(System.currentTimeMillis());	updateCount++;	} else {	error += (registeredClient.requestInfo.taskAttemptId + ", ");	}	}	}	if (!error.isEmpty()) {	
the tasks we expected to be on the node are not there 

registeredClient.setLastHeartbeat(System.currentTimeMillis());	updateCount++;	} else {	error += (registeredClient.requestInfo.taskAttemptId + ", ");	}	}	}	if (!error.isEmpty()) {	}	if (updateCount == 0) {	
no tasks found for heartbeat from hostname port 

========================= hive sample_642 =========================

accumuloSerDeParameters = new AccumuloSerDeParameters(conf, properties, getClass().getName());	final LazySerDeParameters serDeParams = accumuloSerDeParameters.getSerDeParameters();	final List<ColumnMapping> mappings = accumuloSerDeParameters.getColumnMappings();	final List<TypeInfo> columnTypes = accumuloSerDeParameters.getHiveColumnTypes();	final AccumuloRowIdFactory factory = accumuloSerDeParameters.getRowIdFactory();	ArrayList<ObjectInspector> columnObjectInspectors = getColumnObjectInspectors(columnTypes, serDeParams, mappings, factory);	cachedObjectInspector = LazyObjectInspectorFactory.getLazySimpleStructObjectInspector( serDeParams.getColumnNames(), columnObjectInspectors, serDeParams.getSeparators()[0], serDeParams.getNullSequence(), serDeParams.isLastColumnTakesRest(), serDeParams.isEscaped(), serDeParams.getEscapeChar());	cachedRow = new LazyAccumuloRow((LazySimpleStructObjectInspector) cachedObjectInspector);	serializer = new AccumuloRowSerializer(accumuloSerDeParameters.getRowIdOffset(), accumuloSerDeParameters.getSerDeParameters(), accumuloSerDeParameters.getColumnMappings(), accumuloSerDeParameters.getTableVisibilityLabel(), accumuloSerDeParameters.getRowIdFactory());	if (log.isInfoEnabled()) {	
initialized with type 

========================= hive sample_212 =========================

public void handle(Context withinContext) throws Exception {	
processing add partition message 

public void handle(Context withinContext) throws Exception {	AddPartitionMessage apm = deserializer.getAddPartitionMessage(event.getMessage());	org.apache.hadoop.hive.metastore.api.Table tobj = apm.getTableObj();	if (tobj == null) {	
event was a add ptn event with no table listed 

org.apache.hadoop.hive.metastore.api.Table tobj = apm.getTableObj();	if (tobj == null) {	return;	}	final Table qlMdTable = new Table(tobj);	if (!Utils.shouldReplicate(withinContext.replicationSpec, qlMdTable, withinContext.hiveConf)) {	return;	}	Iterable<org.apache.hadoop.hive.metastore.api.Partition> ptns = apm.getPartitionObjs();	if ((ptns == null) || (!ptns.iterator().hasNext())) {	
event was an add ptn event with no partitions 

========================= hive sample_3466 =========================

public static List<ColumnStatisticsObj> aggrPartitionStats( Map<ColumnStatsAggregator, List<ColStatsObjWithSourceInfo>> colStatsMap, final List<String> partNames, final boolean areAllPartsFound, final boolean useDensityFunctionForNDVEstimation, final double ndvTuner) throws MetaException {	List<ColumnStatisticsObj> aggrColStatObjs = new ArrayList<ColumnStatisticsObj>();	int numProcessors = Runtime.getRuntime().availableProcessors();	final ExecutorService pool = Executors.newFixedThreadPool(Math.min(colStatsMap.size(), numProcessors), new ThreadFactoryBuilder().setDaemon(true).setNameFormat("aggr-col-stats-%d").build());	final List<Future<ColumnStatisticsObj>> futures = Lists.newLinkedList();	
aggregating column stats threads used 

if (future.get() != null) {	aggrColStatObjs.add(future.get());	}	} catch (InterruptedException | ExecutionException e) {	LOG.debug(e.getMessage());	pool.shutdownNow();	throw new MetaException(e.toString());	}	}	}	
time for aggr col stats in seconds threads used 

if (doNotUpdateStats){	return false;	}	}	boolean updated = false;	if (forceRecompute || params == null || !containsAllFastStats(params)) {	if (params == null) {	params = new HashMap<>();	}	if (!newDir) {	
updating table stats fast for 

return false;	}	}	boolean updated = false;	if (forceRecompute || params == null || !containsAllFastStats(params)) {	if (params == null) {	params = new HashMap<>();	}	if (!newDir) {	populateQuickStats(fileStatus, params);	
updated size of table to 

public static boolean updatePartitionStatsFast(PartitionSpecProxy.PartitionIterator part, Warehouse wh, boolean madeDir, boolean forceRecompute, EnvironmentContext environmentContext) throws MetaException {	Map<String,String> params = part.getParameters();	boolean updated = false;	if (forceRecompute || params == null || !containsAllFastStats(params)) {	if (params == null) {	params = new HashMap<>();	}	if (!madeDir) {	
updating partition stats fast for 

public static boolean updatePartitionStatsFast(PartitionSpecProxy.PartitionIterator part, Warehouse wh, boolean madeDir, boolean forceRecompute, EnvironmentContext environmentContext) throws MetaException {	Map<String,String> params = part.getParameters();	boolean updated = false;	if (forceRecompute || params == null || !containsAllFastStats(params)) {	if (params == null) {	params = new HashMap<>();	}	if (!madeDir) {	FileStatus[] fileStatus = wh.getFileStatusesForLocation(part.getLocation());	populateQuickStats(fileStatus, params);	
updated size to 

List<T> listeners = new ArrayList<T>();	if (StringUtils.isBlank(listenerImplList)) {	return listeners;	}	String[] listenerImpls = listenerImplList.split(",");	for (String listenerImpl : listenerImpls) {	try {	T listener = (T) Class.forName( listenerImpl.trim(), true, JavaUtils.getClassLoader()).getConstructor( Configuration.class).newInstance(conf);	listeners.add(listener);	} catch (InvocationTargetException ie) {	
got invocationtargetexception 

return listeners;	}	String[] listenerImpls = listenerImplList.split(",");	for (String listenerImpl : listenerImpls) {	try {	T listener = (T) Class.forName( listenerImpl.trim(), true, JavaUtils.getClassLoader()).getConstructor( Configuration.class).newInstance(conf);	listeners.add(listener);	} catch (InvocationTargetException ie) {	throw new MetaException("Failed to instantiate listener named: "+ listenerImpl + ", reason: " + ie.getCause());	} catch (Exception e) {	
got exception 

public static void mergeColStats(ColumnStatistics csNew, ColumnStatistics csOld) throws InvalidObjectException {	List<ColumnStatisticsObj> list = new ArrayList<>();	if (csNew.getStatsObj().size() != csOld.getStatsObjSize()) {	
new columnstats size is but old columnstats size is 

public static Map<String, String> getMetaStoreSaslProperties(Configuration conf, boolean useSSL) {	String hadoopRpcProtectionVal = conf.get(CommonConfigurationKeysPublic.HADOOP_RPC_PROTECTION);	String hadoopRpcProtectionAuth = SaslRpcServer.QualityOfProtection.AUTHENTICATION.toString();	if (useSSL && hadoopRpcProtectionVal != null && !hadoopRpcProtectionVal.equals(hadoopRpcProtectionAuth)) {	
overriding value of setting it from to because ssl is enabled 

private static URL urlFromPathString(String onestr) {	URL oneurl = null;	try {	if (onestr.startsWith("file:/")) {	oneurl = new URL(onestr);	} else {	oneurl = new File(onestr).toURL();	}	} catch (Exception err) {	
bad url ignoring path 

if (first) {	first = false;	} else {	ddl.append(", ");	}	ddl.append(ColumnType.typeToThriftType(col.getType()));	ddl.append(' ');	ddl.append(col.getName());	}	ddl.append("}");	
ddl 

public static void startMetaStore(final int port, final HadoopThriftAuthBridge bridge, Configuration hiveConf) throws Exception{	if (hiveConf == null) {	hiveConf = MetastoreConf.newMetastoreConf();	}	final Configuration finalHiveConf = hiveConf;	Thread thread = new Thread(new Runnable() {	public void run() {	try {	HiveMetaStore.startMetaStore(port, bridge, finalHiveConf);	} catch (Throwable e) {	
metastore thrift server threw an exception 

socket.close();	return;	} catch (Exception e) {	if (retries++ > 60) {	exc = e;	break;	}	Thread.sleep(1000);	}	}	
unable to connect to metastore server 

socket.close();	return;	} catch (Exception e) {	if (retries++ > 60) {	exc = e;	break;	}	Thread.sleep(1000);	}	}	
printing all thread stack traces for debugging before throwing exception 

========================= hive sample_1908 =========================

public static GenericUDAFResolver getGenericUDAFResolver(String functionName) throws SemanticException {	if (LOG.isDebugEnabled()) {	
looking up genericudaf 

} catch (UDFArgumentException err) {	throw new IllegalArgumentException(err);	}	if (genericUDF instanceof SettableUDF) {	try {	TypeInfo typeInfo = ((SettableUDF)genericUDF).getTypeInfo();	if (typeInfo != null) {	((SettableUDF)clonedUDF).setTypeInfo(typeInfo);	}	} catch (UDFArgumentException err) {	
unable to add settable data to udf 

========================= hive sample_3870 =========================

throw new SemanticException("Cannot replicate an event renaming a table across" + " databases into a db level load " + oldDbName + "->" + newDbName);	} else {	oldDbName = context.dbName;	newDbName = context.dbName;	}	}	String oldName = StatsUtils.getFullyQualifiedTableName(oldDbName, msg.getTableObjBefore().getTableName());	String newName = StatsUtils.getFullyQualifiedTableName(newDbName, msg.getTableObjAfter().getTableName());	AlterTableDesc renameTableDesc = new AlterTableDesc( oldName, newName, false, context.eventOnlyReplicationSpec());	Task<DDLWork> renameTableTask = TaskFactory.get( new DDLWork(readEntitySet, writeEntitySet, renameTableDesc), context.hiveConf);	
added rename table task 

========================= hive sample_3489 =========================

if (dynamicPartitioningUsed) {	if (!customDynamicLocationUsed) {	src = new Path(getPartitionRootLocation(jobInfo.getLocation(), jobInfo.getTableInfo().getTable() .getPartitionKeysSize()));	} else {	src = new Path(getCustomPartitionRootLocation(jobInfo, jobContext.getConfiguration()));	}	} else {	src = new Path(jobInfo.getLocation());	}	FileSystem fs = src.getFileSystem(jobContext.getConfiguration());	
job failed try cleaning up temporary directory 

private void applyGroupAndPerms(FileSystem fs, Path dir, FsPermission permission, String group, boolean recursive) throws IOException {	if(LOG.isDebugEnabled()) {	
applygroupandperms perms group recursive 

private void moveTaskOutputs(FileSystem fs, Path file, Path srcDir, Path destDir, final boolean dryRun, boolean immutable ) throws IOException {	if(LOG.isDebugEnabled()) {	
movetaskoutputs from to dry immutable 

}	if (file.getName().equals(TEMP_DIR_NAME) || file.getName().equals(LOGS_DIR_NAME) || file.getName().equals(SUCCEEDED_FILE_NAME)) {	return;	}	final Path finalOutputPath = getFinalPath(fs, file, srcDir, destDir, immutable);	FileStatus fileStatus = fs.getFileStatus(file);	if (!fileStatus.isDir()) {	if (dryRun){	if (immutable){	if(LOG.isDebugEnabled()) {	
testing if moving file to would cause a problem 

if (dryRun){	if (immutable){	if(LOG.isDebugEnabled()) {	}	if (fs.exists(finalOutputPath)) {	throw new HCatException(ErrorType.ERROR_MOVE_FAILED, "Data already exists in " + finalOutputPath + ", duplicate publish not possible.");	}	}	} else {	if(LOG.isDebugEnabled()) {	
moving file to 

}	} else {	if (!dryRun) {	if (dynamicPartitioningUsed) {	final Path parentDir = finalOutputPath.getParent();	Path placeholder = new Path(parentDir, "_placeholder" + String.valueOf(Math.random()));	if (fs.mkdirs(parentDir)) {	fs.create(placeholder).close();	}	if (LOG.isDebugEnabled()) {	
moving directory to 

} else {	filetype = "";	}	int counter = 1;	for (; fs.exists(itemDest) && counter < maxAppendAttempts; counter++) {	itemDest = new Path(dest, name + (APPEND_SUFFIX + counter) + filetype);	}	if (counter == maxAppendAttempts){	throw new HCatException(ErrorType.ERROR_MOVE_FAILED, "Could not find a unique destination path for move: file = " + file + " , src = " + src + ", dest = " + dest);	} else if (counter > APPEND_COUNTER_WARN_THRESHOLD) {	
append job used filename clash counter which is greater than warning limit please compact this table so that performance is not impacted please see hive for details 

moveTaskOutputs(fs, src, src, tblPath, true, true);	moveTaskOutputs(fs, src, src, tblPath, false, true);	if (!src.equals(tblPath)){	fs.delete(src, true);	}	} else {	moveCustomLocationTaskOutputs(fs, table, hiveConf);	}	try {	updateTableSchema(client, table, jobInfo.getOutputSchema());	
har is being used the table has new partitions 

for (Partition p : partitionsToAdd){	Path ptnPath = new Path(harProcessor.getParentFSPath(new Path(p.getSd().getLocation())));	if (fs.exists(ptnPath)){	fs.delete(ptnPath,true);	}	}	throw e;	}	}else{	updateTableSchema(client, table, jobInfo.getOutputSchema());	
har not is not being used the table has new partitions 

private void cancelDelegationTokens(JobContext context) throws IOException{	
cancelling delegation token for the job 

try {	HiveConf hiveConf = HCatUtil .getHiveConf(context.getConfiguration());	client = HCatUtil.getHiveMetastoreClient(hiveConf);	String tokenStrForm = client.getTokenStrForm();	String hCatKeyTokenSignature = context.getConfiguration().get( HCatConstants.HCAT_KEY_TOKEN_SIGNATURE);	if (tokenStrForm != null && hCatKeyTokenSignature != null) {	LOG.info("FileOutputCommitterContainer::cancelDelegationTokens(): " + "Cancelling token fetched for HCAT_KEY_TOKEN_SIGNATURE == (" + hCatKeyTokenSignature + ").");	client.cancelDelegationToken(tokenStrForm);	}	else {	
fileoutputcommittercontainer canceldelegationtokens could not find tokenstrform or hcat key token signature skipping token cancellation 

client = HCatUtil.getHiveMetastoreClient(hiveConf);	String tokenStrForm = client.getTokenStrForm();	String hCatKeyTokenSignature = context.getConfiguration().get( HCatConstants.HCAT_KEY_TOKEN_SIGNATURE);	if (tokenStrForm != null && hCatKeyTokenSignature != null) {	LOG.info("FileOutputCommitterContainer::cancelDelegationTokens(): " + "Cancelling token fetched for HCAT_KEY_TOKEN_SIGNATURE == (" + hCatKeyTokenSignature + ").");	client.cancelDelegationToken(tokenStrForm);	}	else {	}	} catch (MetaException e) {	
metaexception while cancelling delegation token 

String tokenStrForm = client.getTokenStrForm();	String hCatKeyTokenSignature = context.getConfiguration().get( HCatConstants.HCAT_KEY_TOKEN_SIGNATURE);	if (tokenStrForm != null && hCatKeyTokenSignature != null) {	LOG.info("FileOutputCommitterContainer::cancelDelegationTokens(): " + "Cancelling token fetched for HCAT_KEY_TOKEN_SIGNATURE == (" + hCatKeyTokenSignature + ").");	client.cancelDelegationToken(tokenStrForm);	}	else {	}	} catch (MetaException e) {	} catch (TException e) {	
texception while cancelling delegation token 

========================= hive sample_750 =========================

public void finishOuter(VectorizedRowBatch batch, int allMatchCount, int equalKeySeriesCount, boolean atLeastOneNonMatch, boolean inputSelectedInUse, int inputLogicalSize, int spillCount, int hashMapResultCount) throws IOException, HiveException {	if (spillCount > 0) {	spillHashMapBatch(batch, (VectorMapJoinHashTableResult[]) hashMapResults, spills, spillHashMapResultIndices, spillCount);	}	int noMatchCount = 0;	if (spillCount > 0) {	int nonSpillCount = subtractFromInputSelected( inputSelectedInUse, inputLogicalSize, spills, spillCount, nonSpills);	if (LOG.isDebugEnabled()) {	
finishouter spillcount nonspills 

if (spillCount > 0) {	int nonSpillCount = subtractFromInputSelected( inputSelectedInUse, inputLogicalSize, spills, spillCount, nonSpills);	if (LOG.isDebugEnabled()) {	}	if (bigTableValueExpressions != null) {	doValueExpr(batch, nonSpills, nonSpillCount);	}	if (atLeastOneNonMatch) {	noMatchCount = subtract(nonSpills, nonSpillCount, allMatchs, allMatchCount, noMatchs);	if (LOG.isDebugEnabled()) {	
finishouter spillcount nomatchs 

int duplicateCount = equalKeySeriesDuplicateCounts[i];	if (isSingleValue) {	numSel = generateHashMapResultSingleValue( batch, hashMapResult, allMatchs, allMatchesIndex, duplicateCount, numSel);	} else {	generateHashMapResultMultiValue( batch, hashMapResult, allMatchs, allMatchesIndex, duplicateCount);	}	}	batch.size = numSel;	batch.selectedInUse = true;	if (LOG.isDebugEnabled()) {	
finishouter allmatchcount batch selected 

if (LOG.isDebugEnabled()) {	}	} else {	batch.size = 0;	}	if (noMatchCount > 0) {	if (batch.size > 0) {	generateOuterNulls(batch, noMatchs, noMatchCount);	int mergeCount = sortMerge( noMatchs, noMatchCount, batch.selected, batch.size, merged);	if (LOG.isDebugEnabled()) {	
finishouter nomatchcount batch size merged 

========================= hive sample_4120 =========================

public synchronized void onTaskEnd(SparkListenerTaskEnd taskEnd) {	int stageId = taskEnd.stageId();	Integer jobId = stageIdToJobId.get(stageId);	if (jobId == null) {	
can not find job id for stage 

========================= hive sample_4570 =========================

public void testUpdateSomeColumnsUsed() throws HiveAuthzPluginException, HiveAccessControlException, CommandNeedRetryException {	reset(mockedAuthorizer);	int status = driver.compile("update " + acidTableName + " set i = 5 where j = 3");	assertEquals(0, status);	Pair<List<HivePrivilegeObject>, List<HivePrivilegeObject>> io = getHivePrivilegeObjectInputs();	List<HivePrivilegeObject> outputs = io.getRight();	HivePrivilegeObject tableObj = outputs.get(0);	
got privilege object 

public void testUpdateSomeColumnsUsedExprInSet() throws HiveAuthzPluginException, HiveAccessControlException, CommandNeedRetryException {	reset(mockedAuthorizer);	int status = driver.compile("update " + acidTableName + " set i = 5, j = k where j = 3");	assertEquals(0, status);	Pair<List<HivePrivilegeObject>, List<HivePrivilegeObject>> io = getHivePrivilegeObjectInputs();	List<HivePrivilegeObject> outputs = io.getRight();	HivePrivilegeObject tableObj = outputs.get(0);	
got privilege object 

========================= hive sample_245 =========================

ObjectInspectorObject obj = new ObjectInspectorObject( ObjectInspectorUtils.copyToStandardObject(parameter, inputOI, ObjectInspectorCopyOption.JAVA), copiedOI);	if (averageAggregation.uniqueObjects.contains(obj)) {	return;	}	averageAggregation.uniqueObjects.add(obj);	}	doIterate(averageAggregation, inputOI, parameter);	} catch (NumberFormatException e) {	if (!warned) {	warned = true;	
ignoring similar exceptions 

========================= hive sample_4823 =========================

public void onSuccess(Void result) {	
exited 

public void onFailure(Throwable t) {	if (t instanceof CancellationException) {	
was cancelled 

public void onFailure(Throwable t) {	if (t instanceof CancellationException) {	} else {	
exited with error 

========================= hive sample_615 =========================

Map<String, String> props = tsOp.getConf().getOpProps();	if (props != null) {	Properties target = aliasPartnDesc.getProperties();	target.putAll(props);	}	plan.getAliasToPartnInfo().put(alias_id, aliasPartnDesc);	long sizeNeeded = Integer.MAX_VALUE;	int fileLimit = -1;	if (parseCtx.getGlobalLimitCtx().isEnable()) {	if (isAcidTable) {	
skip global limit optimization for acid table 

long sizeNeeded = Integer.MAX_VALUE;	int fileLimit = -1;	if (parseCtx.getGlobalLimitCtx().isEnable()) {	if (isAcidTable) {	parseCtx.getGlobalLimitCtx().disableOpt();	} else {	long sizePerRow = HiveConf.getLongVar(parseCtx.getConf(), HiveConf.ConfVars.HIVELIMITMAXROWSIZE);	sizeNeeded = (parseCtx.getGlobalLimitCtx().getGlobalOffset() + parseCtx.getGlobalLimitCtx().getGlobalLimit()) * sizePerRow;	fileLimit = HiveConf.getIntVar(parseCtx.getConf(), HiveConf.ConfVars.HIVELIMITOPTLIMITFILE);	if (sizePerRow <= 0 || fileLimit <= 0) {	
skip optimization to reduce input size of limit 

if (parseCtx.getGlobalLimitCtx().isEnable()) {	if (isAcidTable) {	parseCtx.getGlobalLimitCtx().disableOpt();	} else {	long sizePerRow = HiveConf.getLongVar(parseCtx.getConf(), HiveConf.ConfVars.HIVELIMITMAXROWSIZE);	sizeNeeded = (parseCtx.getGlobalLimitCtx().getGlobalOffset() + parseCtx.getGlobalLimitCtx().getGlobalLimit()) * sizePerRow;	fileLimit = HiveConf.getIntVar(parseCtx.getConf(), HiveConf.ConfVars.HIVELIMITOPTLIMITFILE);	if (sizePerRow <= 0 || fileLimit <= 0) {	parseCtx.getGlobalLimitCtx().disableOpt();	} else if (parts.isEmpty()) {	
empty input skip limit optimization 

if (isAcidTable) {	parseCtx.getGlobalLimitCtx().disableOpt();	} else {	long sizePerRow = HiveConf.getLongVar(parseCtx.getConf(), HiveConf.ConfVars.HIVELIMITMAXROWSIZE);	sizeNeeded = (parseCtx.getGlobalLimitCtx().getGlobalOffset() + parseCtx.getGlobalLimitCtx().getGlobalLimit()) * sizePerRow;	fileLimit = HiveConf.getIntVar(parseCtx.getConf(), HiveConf.ConfVars.HIVELIMITOPTLIMITFILE);	if (sizePerRow <= 0 || fileLimit <= 0) {	parseCtx.getGlobalLimitCtx().disableOpt();	} else if (parts.isEmpty()) {	} else {	
try to reduce input size for limit sizeneeded file limit 

paths = ListBucketingPruner.prune(parseCtx, part, listBucketingPruner);	} else {	if (parseCtx.getGlobalLimitCtx().isEnable()) {	if (isFirstPart) {	long sizeLeft = sizeNeeded;	ArrayList<Path> retPathList = new ArrayList<Path>();	SamplePruner.LimitPruneRetStatus status = SamplePruner.limitPrune(part, sizeLeft, fileLimit, retPathList);	if (status.equals(SamplePruner.LimitPruneRetStatus.NoFile)) {	continue;	} else if (status.equals(SamplePruner.LimitPruneRetStatus.NotQualify)) {	
use full input first files are more than bytes 

if (props != null) {	Properties target = tblDesc.getProperties();	target.putAll(props);	}	for (Path p : paths) {	if (p == null) {	continue;	}	String path = p.toString();	if (LOG.isDebugEnabled()) {	
adding of table 

Iterator<Path> iterPath = partDir.iterator();	Iterator<PartitionDesc> iterPartnDesc = partDesc.iterator();	if (!local) {	while (iterPath.hasNext()) {	assert iterPartnDesc.hasNext();	Path path = iterPath.next();	PartitionDesc prtDesc = iterPartnDesc.next();	plan.addPathToAlias(path,alias_id);	plan.addPathToPartitionInfo(path, prtDesc);	if (LOG.isDebugEnabled()) {	
information added for path 

public static void createMRWorkForMergingFiles(FileSinkOperator fsInput, Path finalName, DependencyCollectionTask dependencyTask, List<Task<MoveWork>> mvTasks, HiveConf conf, Task<? extends Serializable> currTask, LineageState lineageState) throws SemanticException {	FileSinkDesc fsInputDesc = fsInput.getConf();	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
creating merge work from with write id into 

private static Path getTableLocationPath(final HiveConf hconf, final TableDesc tableDesc) {	Table table = null;	try {	Hive hive = Hive.get(hconf);	table = hive.getTable(tableDesc.getTableName());	} catch (HiveException e) {	
unable to get the table location path for 

} catch (HiveException e) {	throw new RuntimeException("unexpected; table should be present already..: " + tableName, e);	}	} else if (mvWork.getLoadFileWork() != null) {	statsWork = new BasicStatsWork(mvWork.getLoadFileWork());	truncate = true;	if (mvWork.getLoadFileWork().getCtasCreateTableDesc() != null) {	try {	table = mvWork.getLoadFileWork().getCtasCreateTableDesc().toTable(hconf);	} catch (HiveException e) {	
can t pre create table for ctas 

String tableName = mvWork.getLoadFileWork().getCreateViewDesc().getViewName();	try {	table = Hive.get().getTable(tableName);	} catch (HiveException e) {	throw new RuntimeException("unexpected; MV should be present already..: " + tableName, e);	}	} else {	try {	table = mvWork.getLoadFileWork().getCreateViewDesc().toTable(hconf);	} catch (HiveException e) {	
can t pre create table for mv 

inputDirstr.add(inputDir.toString());	final Class<? extends InputFormat> internalIFClass;	if (tblDesc.getInputFileFormatClass().equals(RCFileInputFormat.class)) {	internalIFClass = RCFileBlockMergeInputFormat.class;	} else if (tblDesc.getInputFileFormatClass().equals(OrcInputFormat.class)) {	internalIFClass = OrcFileStripeMergeInputFormat.class;	} else {	throw new SemanticException("createMergeTask called on a table with file" + " format other than RCFile or ORCFile");	}	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
creating mergefilework from to 

private static ConditionalTask createCondTask(HiveConf conf, Task<? extends Serializable> currTask, MoveWork mvWork, Serializable mergeWork, Path condInputPath, Path condOutputPath, Task<MoveWork> moveTaskToLink, DependencyCollectionTask dependencyTask, LineageState lineageState) {	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
creating conditional merge task for 

boolean isMmTable = fileSinkDesc.isMmTable();	if (chDir) {	dest = fileSinkDesc.getMergeInputDirName();	if (!isMmTable) {	Context baseCtx = parseCtx.getContext();	Path tmpDir = baseCtx.getTempDirForFinalJobPath(fileSinkDesc.getDestPath());	if (fileSinkDesc.isLinkedFileSink()) {	for (FileSinkDesc fsConf : fileSinkDesc.getLinkedFileSinkDesc()) {	fsConf.setDirName(new Path(tmpDir, fsConf.getDirName().getName()));	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
createmovetask setting tmpdir for linkedfilesink chdir dest was 

Path tmpDir = baseCtx.getTempDirForFinalJobPath(fileSinkDesc.getDestPath());	if (fileSinkDesc.isLinkedFileSink()) {	for (FileSinkDesc fsConf : fileSinkDesc.getLinkedFileSinkDesc()) {	fsConf.setDirName(new Path(tmpDir, fsConf.getDirName().getName()));	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	}	}	} else {	fileSinkDesc.setDirName(tmpDir);	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
createmovetask setting tmpdir chdir dest was 

========================= hive sample_3108 =========================

public void configureInputJobProperties(TableDesc tableDesc, Map<String, String> jobProperties) {	try {	
adding properties to input job conf 

public void configureInputJobCredentials(TableDesc tableDesc, Map<String, String> jobSecrets) {	try {	
adding secrets to input job conf 

public void configureTableJobProperties(TableDesc tableDesc, Map<String, String> jobProperties) {	try {	
adding properties to input job conf 

========================= hive sample_1135 =========================

DefaultExecutor executor = new DefaultExecutor();	executor.setExitValues(null);	int nbytes = appConf.getInt(AppConfig.EXEC_MAX_BYTES_NAME, -1);	ByteArrayOutputStream outStream = new MaxByteArrayOutputStream(nbytes);	ByteArrayOutputStream errStream = new MaxByteArrayOutputStream(nbytes);	executor.setStreamHandler(new PumpStreamHandler(outStream, errStream));	int timeout = appConf.getInt(AppConfig.EXEC_TIMEOUT_NAME, 0);	ExecuteWatchdog watchdog = new ExecuteWatchdog(timeout);	executor.setWatchdog(watchdog);	CommandLine cmd = makeCommandLine(program, args);	
running 

========================= hive sample_814 =========================

}	Rpc.MessageType replyType;	Object replyPayload;	try {	replyPayload = handler.invoke(this, ctx, msg);	if (replyPayload == null) {	replyPayload = new Rpc.NullMessage();	}	replyType = Rpc.MessageType.REPLY;	} catch (InvocationTargetException ite) {	
s error in rpc handler 

private void handleError(ChannelHandlerContext ctx, Object msg, OutstandingRpc rpc) throws Exception {	if (msg instanceof String) {	
received error message 

private void handleError(ChannelHandlerContext ctx, Object msg, OutstandingRpc rpc) throws Exception {	if (msg instanceof String) {	rpc.future.setFailure(new RpcException((String) msg));	} else {	String error = String.format("Received error with unexpected payload (%s).", msg != null ? msg.getClass().getName() : null);	
s s 

public final void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) {	
s closing channel due to exception in pipeline 

public final void channelInactive(ChannelHandlerContext ctx) throws Exception {	if (rpcs.size() > 0) {	
closing rpc channel with outstanding rpcs 

========================= hive sample_547 =========================

public int process(Hive db, Table tbl) throws Exception {	
executing stats no job task 

public void run() {	Map<String, String> parameters = partish.getPartParameters();	try {	long numRows = 0;	long rawDataSize = 0;	long fileSize = 0;	long numFiles = 0;	
aggregating stats for 

public void run() {	Map<String, String> parameters = partish.getPartParameters();	try {	long numRows = 0;	long rawDataSize = 0;	long fileSize = 0;	long numFiles = 0;	FileStatus[] fileList = HiveStatsUtils.getFileStatusRecurse(dir, -1, fs);	for (FileStatus file : fileList) {	
computing stats for 

}	}	List<FooterStatCollector> scs = Lists.newArrayList();	for (Partish partish : partishes) {	scs.add(new FooterStatCollector(jc, partish));	}	for (FooterStatCollector sc : scs) {	sc.init(conf, console);	threadPool.execute(sc);	}	
stats collection waiting for threadpool to shutdown 

}	List<FooterStatCollector> scs = Lists.newArrayList();	for (Partish partish : partishes) {	scs.add(new FooterStatCollector(jc, partish));	}	for (FooterStatCollector sc : scs) {	sc.init(conf, console);	threadPool.execute(sc);	}	shutdownAndAwaitTermination(threadPool);	
stats collection threadpool shutdown successful 

private int updatePartitions(Hive db, List<FooterStatCollector> scs, Table table) throws InvalidOperationException, HiveException {	String tableFullName = table.getFullyQualifiedName();	if (scs.isEmpty()) {	return 0;	}	if (work.isStatsReliable()) {	for (FooterStatCollector statsCollection : scs) {	if (statsCollection.result == null) {	
stats requested to be reliable empty stats found 

}	List<FooterStatCollector> validColectors = Lists.newArrayList();	for (FooterStatCollector statsCollection : scs) {	if (statsCollection.isValid()) {	validColectors.add(statsCollection);	}	}	EnvironmentContext environmentContext = new EnvironmentContext();	environmentContext.putToProperties(StatsSetupConst.DO_NOT_UPDATE_STATS, StatsSetupConst.TRUE);	ImmutableListMultimap<String, FooterStatCollector> collectorsByTable = Multimaps.index(validColectors, FooterStatCollector.SIMPLE_NAME_FUNCTION);	
collectors size 

List<FooterStatCollector> validColectors = Lists.newArrayList();	for (FooterStatCollector statsCollection : scs) {	if (statsCollection.isValid()) {	validColectors.add(statsCollection);	}	}	EnvironmentContext environmentContext = new EnvironmentContext();	environmentContext.putToProperties(StatsSetupConst.DO_NOT_UPDATE_STATS, StatsSetupConst.TRUE);	ImmutableListMultimap<String, FooterStatCollector> collectorsByTable = Multimaps.index(validColectors, FooterStatCollector.SIMPLE_NAME_FUNCTION);	if (collectorsByTable.keySet().size() < 1) {	
collectors are empty 

if (statsCollection.isValid()) {	validColectors.add(statsCollection);	}	}	EnvironmentContext environmentContext = new EnvironmentContext();	environmentContext.putToProperties(StatsSetupConst.DO_NOT_UPDATE_STATS, StatsSetupConst.TRUE);	ImmutableListMultimap<String, FooterStatCollector> collectorsByTable = Multimaps.index(validColectors, FooterStatCollector.SIMPLE_NAME_FUNCTION);	if (collectorsByTable.keySet().size() < 1) {	}	assert (collectorsByTable.keySet().size() <= 1);	
updating stats for 

if (collectorsByTable.keySet().size() < 1) {	}	assert (collectorsByTable.keySet().size() <= 1);	for (String partName : collectorsByTable.keySet()) {	ImmutableList<FooterStatCollector> values = collectorsByTable.get(partName);	if (values == null) {	throw new RuntimeException("very intresting");	}	if (values.get(0).result instanceof Table) {	db.alterTable(tableFullName, (Table) values.get(0).result, environmentContext);	
updated stats for 

ImmutableList<FooterStatCollector> values = collectorsByTable.get(partName);	if (values == null) {	throw new RuntimeException("very intresting");	}	if (values.get(0).result instanceof Table) {	db.alterTable(tableFullName, (Table) values.get(0).result, environmentContext);	} else {	if (values.get(0).result instanceof Partition) {	List<Partition> results = Lists.transform(values, FooterStatCollector.EXTRACT_RESULT_FUNCTION);	db.alterPartitions(tableFullName, results, environmentContext);	
bulk updated partitions of 

db.alterTable(tableFullName, (Table) values.get(0).result, environmentContext);	} else {	if (values.get(0).result instanceof Partition) {	List<Partition> results = Lists.transform(values, FooterStatCollector.EXTRACT_RESULT_FUNCTION);	db.alterPartitions(tableFullName, results, environmentContext);	} else {	throw new RuntimeException("inconsistent");	}	}	}	
updated stats for 

private void shutdownAndAwaitTermination(ExecutorService threadPool) {	threadPool.shutdown();	try {	while (!threadPool.awaitTermination(10, TimeUnit.SECONDS)) {	
waiting for all stats tasks to finish 

private void shutdownAndAwaitTermination(ExecutorService threadPool) {	threadPool.shutdown();	try {	while (!threadPool.awaitTermination(10, TimeUnit.SECONDS)) {	}	threadPool.shutdownNow();	if (!threadPool.awaitTermination(100, TimeUnit.SECONDS)) {	
stats collection thread pool did not terminate 

========================= hive sample_5033 =========================

Executor threadPool = Executors.newFixedThreadPool(4);	threadPool.execute(rdmTask1);	threadPool.execute(rdmTask2);	threadPool.execute(rdmTask3);	threadPool.execute(evictionTask);	try {	cdlIn.await();	cdlOut.countDown();	long result1 = rdmTask1.get(), result2 = rdmTask2.get(), result3 = rdmTask3.get();	int evictions = evictionTask.get();	
mtt test task task task evictions 

========================= hive sample_2125 =========================

public void run() {	StreamingConnection conn = null;	try {	conn = endPoint.newConnection(true, "UT_" + Thread.currentThread().getName());	RecordWriter writer = new DelimitedInputWriter(cols, ",", endPoint);	for (int i = 0; i < batches; i++) {	long start = System.currentTimeMillis();	
starting batch 

========================= hive sample_906 =========================

} catch(NumberFormatException e) {	}	for (int numRetries = 0;;) {	try {	openTransport();	client = new TCLIService.Client(new TBinaryProtocol(transport));	openSession();	executeInitSql();	break;	} catch (Exception e) {	
failed to connect to 

jdbcUriString = connParams.getJdbcUriString();	host = Utils.getCanonicalHostName(connParams.getHost());	port = connParams.getPort();	} else {	errMsg = warnMsg;	++numRetries;	}	if (numRetries >= maxRetries) {	throw new SQLException(errMsg + e.getMessage(), " 08S01", e);	} else {	
retrying of 

for(String sql : sqlList) {	boolean hasResult = st.execute(sql);	if (hasResult) {	ResultSet rs = st.getResultSet();	while (rs.next()) {	System.out.println(rs.getString(1));	}	}	}	} catch(Exception e) {	
failed to execute initial sql 

if (line.startsWith("#") || line.startsWith("--")) {	continue;	} else {	line = line.concat(" ");	sb.append(line);	}	}	}	initSqlList = getInitSql(sb.toString());	} catch(IOException e) {	
failed to read initial sql file 

public long getRetryInterval() {	return 0;	}	});	} else {	httpClientBuilder = HttpClientBuilder.create();	}	httpClientBuilder.setRetryHandler(new HttpRequestRetryHandler() {	public boolean retryRequest(IOException exception, int executionCount, HttpContext context) {	if (executionCount > 1) {	
retry attempts to connect to server exceeded 

});	} else {	httpClientBuilder = HttpClientBuilder.create();	}	httpClientBuilder.setRetryHandler(new HttpRequestRetryHandler() {	public boolean retryRequest(IOException exception, int executionCount, HttpContext context) {	if (executionCount > 1) {	return false;	}	if (exception instanceof org.apache.http.NoHttpResponseException) {	
could not connect to the server retrying one more time 

if (!supportedProtocols.contains(openResp.getServerProtocolVersion())) {	throw new TException("Unsupported Hive2 protocol");	}	protocol = openResp.getServerProtocolVersion();	sessHandle = openResp.getSessionHandle();	String serverFetchSize = openResp.getConfiguration().get("hive.server2.thrift.resultset.default.fetch.size");	if (serverFetchSize != null) {	fetchSize = Integer.parseInt(serverFetchSize);	}	} catch (TException e) {	
error opening session 

for (Entry<Object, Object> e : clientInfo.entrySet()) {	if (e.getKey() == null || e.getValue() == null) continue;	map.put(e.getKey().toString(), e.getValue().toString());	}	}	req.setConfiguration(map);	try {	TSetClientInfoResp openResp = client.SetClientInfo(req);	Utils.verifySuccess(openResp.getStatus());	} catch (TException | SQLException e) {	
error sending client info 

========================= hive sample_1162 =========================

separator = altSep;	}	}	String columnProperty = tbl.getProperty(COLUMNS);	if (columnProperty == null || columnProperty.length() == 0) {	cachedObjectInspector = ObjectInspectorFactory .getReflectionObjectInspector(ColumnSet.class, ObjectInspectorFactory.ObjectInspectorOptions.JAVA);	} else {	columnNames = Arrays.asList(columnProperty.split(","));	cachedObjectInspector = MetadataListStructObjectInspector .getInstance(columnNames,Lists.newArrayList(Splitter.on('\0').split(tbl.getProperty(COLUMNS_COMMENTS))));	}	
initialized with columnnames 

========================= hive sample_455 =========================

llaps = getLlapServices(false).iterator();	clientInstance = llaps.next();	}	ByteString tokenBytes = null;	boolean hasRefreshed = false;	while (true) {	try {	tokenBytes = getTokenBytes(appId);	break;	} catch (IOException | ServiceException ex) {	
cannot get a token trying a different instance 

if (hasRefreshed) {	throw new RuntimeException("Cannot find any LLAPs to get the token from");	}	llaps = getLlapServices(true).iterator();	hasRefreshed = true;	}	clientInstance = llaps.next();	}	Token<LlapTokenIdentifier> token = extractToken(tokenBytes);	if (LOG.isInfoEnabled()) {	
obtained a llap delegation token from 

========================= hive sample_639 =========================

private synchronized boolean setStateAndResult(JobState jobState, T result) {	if (this.jobState == JobState.STARTED) {	this.jobState = jobState;	this.returnResult = result;	return true;	} else {	
failed to set job state to due to job state expected state is 

========================= hive sample_793 =========================

numPartitions = calcNumPartitions(memoryThreshold, estimatedTableSize, minNumParts, minWbSize);	writeBufferSize = (int)(estimatedTableSize / numPartitions);	} else {	numPartitions = nwayConf.getNumberOfPartitions();	if (nwayConf.getLoadedContainerList().size() == 0) {	writeBufferSize = (int)(estimatedTableSize / numPartitions);	} else {	while (memoryThreshold < numPartitions * minWbSize) {	long memFreed = nwayConf.spill();	if (memFreed == 0) {	
available memory is not enough to create hybridhashtablecontainers consistently 

} else {	numPartitions = nwayConf.getNumberOfPartitions();	if (nwayConf.getLoadedContainerList().size() == 0) {	writeBufferSize = (int)(estimatedTableSize / numPartitions);	} else {	while (memoryThreshold < numPartitions * minWbSize) {	long memFreed = nwayConf.spill();	if (memFreed == 0) {	break;	} else {	
total available memory was 

long memFreed = nwayConf.spill();	if (memFreed == 0) {	break;	} else {	memoryThreshold += memFreed;	}	}	writeBufferSize = (int)(memoryThreshold / numPartitions);	}	}	
total available memory is 

break;	} else {	memoryThreshold += memFreed;	}	}	writeBufferSize = (int)(memoryThreshold / numPartitions);	}	}	writeBufferSize = Integer.bitCount(writeBufferSize) == 1 ? writeBufferSize : Integer.highestOneBit(writeBufferSize);	writeBufferSize = writeBufferSize < minWbSize ? minWbSize : Math.min(maxWbSize / numPartitions, writeBufferSize);	
write buffer size 

writeBufferSize = Integer.bitCount(writeBufferSize) == 1 ? writeBufferSize : Integer.highestOneBit(writeBufferSize);	writeBufferSize = writeBufferSize < minWbSize ? minWbSize : Math.min(maxWbSize / numPartitions, writeBufferSize);	memoryUsed = 0;	if (useBloomFilter) {	if (newKeyCount <= BLOOM_FILTER_MAX_SIZE) {	this.bloom1 = new BloomFilter(newKeyCount);	} else {	double fpp = calcFPP(newKeyCount);	assert fpp < 1 : "Too many keys! BloomFilter False Positive Probability is 1!";	if (fpp >= 0.5) {	
bloomfilter fpp is greater than 

writeBufferSize = writeBufferSize < minWbSize ? minWbSize : Math.min(maxWbSize / numPartitions, writeBufferSize);	memoryUsed = 0;	if (useBloomFilter) {	if (newKeyCount <= BLOOM_FILTER_MAX_SIZE) {	this.bloom1 = new BloomFilter(newKeyCount);	} else {	double fpp = calcFPP(newKeyCount);	assert fpp < 1 : "Too many keys! BloomFilter False Positive Probability is 1!";	if (fpp >= 0.5) {	}	
bloomfilter is using fpp 

if (useBloomFilter) {	if (newKeyCount <= BLOOM_FILTER_MAX_SIZE) {	this.bloom1 = new BloomFilter(newKeyCount);	} else {	double fpp = calcFPP(newKeyCount);	assert fpp < 1 : "Too many keys! BloomFilter False Positive Probability is 1!";	if (fpp >= 0.5) {	}	this.bloom1 = new BloomFilter(newKeyCount, fpp);	}	
using a bloom filter d keys of size d bytes 

int numPartitionsSpilledOnCreation = 0;	int initialCapacity = Math.max(newKeyCount / numPartitions, threshold / numPartitions);	float probePercentage = (float) 8 if (probePercentage == 1) {	probePercentage = probePercent;	}	int maxCapacity = (int)(memoryThreshold * probePercentage);	for (int i = 0; i < numPartitions; i++) {	if (this.nwayConf == null || nwayConf.getLoadedContainerList().size() == 0) {	if (i == 0) {	hashPartitions[i] = new HashPartition(initialCapacity, loadFactor, writeBufferSize, maxCapacity, true, spillLocalDirs);	
each new partition will require memory 

hashPartitions[i] = new HashPartition(initialCapacity, loadFactor, writeBufferSize, maxCapacity, true, spillLocalDirs);	}	}	if (isHashMapSpilledOnCreation(i)) {	numPartitionsSpilledOnCreation++;	numPartitionsSpilled++;	this.setSpill(true);	if (this.nwayConf != null && this.nwayConf.getNextSpillPartition() == numPartitions - 1) {	this.nwayConf.setNextSpillPartition(i - 1);	}	
hash partition is spilled on creation 

}	if (isHashMapSpilledOnCreation(i)) {	numPartitionsSpilledOnCreation++;	numPartitionsSpilled++;	this.setSpill(true);	if (this.nwayConf != null && this.nwayConf.getNextSpillPartition() == numPartitions - 1) {	this.nwayConf.setNextSpillPartition(i - 1);	}	} else {	memoryUsed += hashPartitions[i].hashMap.memorySize();	
hash partition is created in memory total memory usage so far 

numPartitionsSpilled++;	this.setSpill(true);	if (this.nwayConf != null && this.nwayConf.getNextSpillPartition() == numPartitions - 1) {	this.nwayConf.setNextSpillPartition(i - 1);	}	} else {	memoryUsed += hashPartitions[i].hashMap.memorySize();	}	}	if (writeBufferSize * (numPartitions - numPartitionsSpilledOnCreation) > memoryThreshold) {	
there is not enough memory to allocate hash partitions 

if (this.nwayConf != null && this.nwayConf.getNextSpillPartition() == numPartitions - 1) {	this.nwayConf.setNextSpillPartition(i - 1);	}	} else {	memoryUsed += hashPartitions[i].hashMap.memorySize();	}	}	if (writeBufferSize * (numPartitions - numPartitionsSpilledOnCreation) > memoryThreshold) {	}	assert numPartitionsSpilledOnCreation != numPartitions : "All partitions are directly spilled!" + " It is not supported now.";	
number of partitions created 

if (this.nwayConf != null && this.nwayConf.getNextSpillPartition() == numPartitions - 1) {	this.nwayConf.setNextSpillPartition(i - 1);	}	} else {	memoryUsed += hashPartitions[i].hashMap.memorySize();	}	}	if (writeBufferSize * (numPartitions - numPartitionsSpilledOnCreation) > memoryThreshold) {	}	assert numPartitionsSpilledOnCreation != numPartitions : "All partitions are directly spilled!" + " It is not supported now.";	
number of partitions spilled directly to disk on creation 

HashPartition hashPartition = hashPartitions[partitionId];	if (bloom1 != null) {	bloom1.addLong(keyHash);	}	if (isOnDisk(partitionId) || isHashMapSpilledOnCreation(partitionId)) {	putToSidefile = true;	} else {	if (!lastPartitionInMem && (hashPartition.size() == 0 || (totalInMemRowCount & (this.memoryCheckFrequency - 1)) == 0)) {	if (isMemoryFull()) {	if ((numPartitionsSpilled == hashPartitions.length - 1) ) {	
this last partition in memory won t be spilled 

lastPartitionInMem = true;	} else {	if (nwayConf == null) {	int biggest = biggestPartition();	spillPartition(biggest);	this.setSpill(true);	if (partitionId == biggest) {	putToSidefile = true;	}	} else {	
n way spilling spill tail partition from previously loaded small tables 

this.setSpill(true);	if (partitionId == biggest) {	putToSidefile = true;	}	} else {	int biggest = nwayConf.getNextSpillPartition();	memoryThreshold += nwayConf.spill();	if (biggest != 0 && partitionId == biggest) {	putToSidefile = true;	}	
memory threshold has been increased to 

public long spillPartition(int partitionId) throws IOException {	HashPartition partition = hashPartitions[partitionId];	int inMemRowCount = partition.hashMap.getNumValues();	if (inMemRowCount == 0) {	
trying to spill an empty hash partition it may be due to hive auto convert join noconditionaltask size being set too low 

public long spillPartition(int partitionId) throws IOException {	HashPartition partition = hashPartitions[partitionId];	int inMemRowCount = partition.hashMap.getNumValues();	if (inMemRowCount == 0) {	}	File file = FileUtils.createLocalDirsTempFile( spillLocalDirs, "partition-" + partitionId + "-", null, false);	OutputStream outputStream = new FileOutputStream(file, false);	com.esotericsoftware.kryo.io.Output output = new com.esotericsoftware.kryo.io.Output(outputStream);	Kryo kryo = SerializationUtilities.borrowKryo();	try {	
trying to spill hash partition 

Kryo kryo = SerializationUtilities.borrowKryo();	try {	kryo.writeObject(output, partition.hashMap);	output.close();	outputStream.close();	} finally {	SerializationUtilities.releaseKryo(kryo);	}	partition.hashMapLocalPath = file.toPath();	partition.hashMapOnDisk = true;	
spilling hash partition rows mem size 

Kryo kryo = SerializationUtilities.borrowKryo();	try {	kryo.writeObject(output, partition.hashMap);	output.close();	outputStream.close();	} finally {	SerializationUtilities.releaseKryo(kryo);	}	partition.hashMapLocalPath = file.toPath();	partition.hashMapOnDisk = true;	
memory usage before spilling 

kryo.writeObject(output, partition.hashMap);	output.close();	outputStream.close();	} finally {	SerializationUtilities.releaseKryo(kryo);	}	partition.hashMapLocalPath = file.toPath();	partition.hashMapOnDisk = true;	long memFreed = partition.hashMap.memorySize();	memoryUsed -= memFreed;	
memory usage after spilling 

public static int calcNumPartitions(long memoryThreshold, long dataSize, int minNumParts, int minWbSize) throws IOException {	int numPartitions = minNumParts;	if (memoryThreshold < minNumParts * minWbSize) {	
available memory is not enough to create a hybridhashtablecontainer 

public void clear() {	for (int i = 0; i < hashPartitions.length; i++) {	HashPartition hp = hashPartitions[i];	if (hp != null) {	
going to clear hash partition 

public void dumpStats() {	int numPartitionsInMem = 0;	int numPartitionsOnDisk = 0;	for (HashPartition hp : hashPartitions) {	if (hp.isHashMapOnDisk()) {	numPartitionsOnDisk++;	} else {	numPartitionsInMem++;	}	}	
in memory partitions have been processed successfully partitions in memory have been processed partitions have been spilled to disk and will be processed next 

public void setSerde(MapJoinObjectSerDeContext keyCtx, MapJoinObjectSerDeContext valCtx) throws SerDeException {	AbstractSerDe keySerde = keyCtx.getSerDe(), valSerde = valCtx.getSerDe();	if (writeHelper == null) {	
initializing container with and 

========================= hive sample_4049 =========================

public static void checkFileAccess(FileSystem fs, FileStatus stat, FsAction action, String user, List<String> groups) throws IOException, AccessControlException {	if (groups == null) {	groups = emptyGroups;	}	String superGroupName = getSuperGroupName(fs.getConf());	if (userBelongsToSuperGroup(superGroupName, groups)) {	
user user belongs to super group supergroupname permission granted for action 

========================= hive sample_1461 =========================

} else {	partitionValidationPattern = null;	}	MetricRegistry registry = Metrics.getRegistry();	if (registry != null) {	directSqlErrors = Metrics.getOrCreateCounter(MetricsConstants.DIRECTSQL_ERRORS);	}	if (!isInitialized) {	throw new RuntimeException( "Unable to create persistence manager. Check dss.log for details");	} else {	
initialized objectstore 

long retryInterval = MetastoreConf.getTimeVar(conf, ConfVars.HMSHANDLERINTERVAL, TimeUnit.MILLISECONDS);	int numTries = retryLimit;	while (numTries > 0){	try {	initializeHelper(dsProps);	return;	} catch (Exception e){	numTries--;	boolean retriable = isRetriableException(e);	if ((numTries > 0) && retriable){	
retriable exception while instantiating objectstore retrying tries left 

try {	initializeHelper(dsProps);	return;	} catch (Exception e){	numTries--;	boolean retriable = isRetriableException(e);	if ((numTries > 0) && retriable){	try {	Thread.sleep(retryInterval);	} catch (InterruptedException ie) {	
interrupted while sleeping before retrying 

numTries--;	boolean retriable = isRetriableException(e);	if ((numTries > 0) && retriable){	try {	Thread.sleep(retryInterval);	} catch (InterruptedException ie) {	Thread.currentThread().interrupt();	}	} else {	if (retriable){	
exception retry limit reached not retrying any longer 

boolean retriable = isRetriableException(e);	if ((numTries > 0) && retriable){	try {	Thread.sleep(retryInterval);	} catch (InterruptedException ie) {	Thread.currentThread().interrupt();	}	} else {	if (retriable){	} else {	
non retriable exception during objectstore initialize 

private void initializeHelper(Properties dsProps) {	
objectstore initialize called 

private void initializeHelper(Properties dsProps) {	prop = dsProps;	pm = getPersistenceManager();	try {	String productName = MetaStoreDirectSql.getProductName(pm);	sqlGenerator = new SQLGenerator(DatabaseProduct.determineDatabaseProduct(productName), conf);	} catch (SQLException e) {	
error trying to figure out the database product 

isInitialized = pm != null;	if (isInitialized) {	dbType = determineDatabaseProduct();	expressionProxy = createExpressionProxy(conf);	if (MetastoreConf.getBoolVar(getConf(), ConfVars.TRY_DIRECT_SQL)) {	String schema = prop.getProperty("javax.jdo.mapping.Schema");	schema = org.apache.commons.lang.StringUtils.defaultIfBlank(schema, null);	directSql = new MetaStoreDirectSql(pm, conf, schema);	}	}	
rawstore with persistencemanager created in the thread with id 

private DatabaseProduct determineDatabaseProduct() {	try {	return DatabaseProduct.determineDatabaseProduct(getProductName(pm));	} catch (SQLException e) {	
cannot determine database product assuming other 

private static String getProductName(PersistenceManager pm) {	JDOConnection jdoConn = pm.getDataStoreConnection();	try {	return ((Connection)jdoConn.getNativeConnection()).getMetaData().getDatabaseProductName();	} catch (Throwable t) {	
error retrieving product name 

private static PartitionExpressionProxy createExpressionProxy(Configuration conf) {	String className = MetastoreConf.getVar(conf, ConfVars.EXPRESSION_PROXY_CLASS);	try {	Class<? extends PartitionExpressionProxy> clazz = JavaUtils.getClass(className, PartitionExpressionProxy.class);	return JavaUtils.newInstance(clazz, new Class<?>[0], new Object[0]);	} catch (MetaException e) {	
error loading partitionexpressionproxy 

private static void configureSSL(Configuration conf) {	String sslPropString = MetastoreConf.getVar(conf, ConfVars.DBACCESS_SSL_PROPS);	if (org.apache.commons.lang.StringUtils.isNotEmpty(sslPropString)) {	
metastore setting ssl properties of the connection to backed db 

private static void configureSSL(Configuration conf) {	String sslPropString = MetastoreConf.getVar(conf, ConfVars.DBACCESS_SSL_PROPS);	if (org.apache.commons.lang.StringUtils.isNotEmpty(sslPropString)) {	for (String sslProp : sslPropString.split(",")) {	String[] pair = sslProp.trim().split("=");	if (pair != null && pair.length == 2) {	System.setProperty(pair[0].trim(), pair[1].trim());	} else {	
invalid metastore property value for 

private static Properties getDataSourceProps(Configuration conf) {	Properties prop = new Properties();	correctAutoStartMechanism(conf);	for (ConfVars var : MetastoreConf.dataNucleusAndJdoConfs) {	String confVal = MetastoreConf.getAsString(conf, var);	String varName = var.getVarname();	Object prevVal = prop.setProperty(varName, confVal);	if (MetastoreConf.isPrintable(varName)) {	
overriding value from jpox properties with 

String confVal = MetastoreConf.getAsString(conf, var);	String varName = var.getVarname();	Object prevVal = prop.setProperty(varName, confVal);	if (MetastoreConf.isPrintable(varName)) {	}	}	for (Map.Entry<String, String> e : conf) {	if (e.getKey().startsWith("datanucleus.") || e.getKey().startsWith("javax.jdo.")) {	Object prevVal = prop.setProperty(e.getKey(), e.getValue());	if (LOG.isDebugEnabled() && MetastoreConf.isPrintable(e.getKey())) {	
overriding value from jpox properties with 

private static void correctAutoStartMechanism(Configuration conf) {	final String autoStartKey = "datanucleus.autoStartMechanismMode";	final String autoStartIgnore = "ignored";	String currentAutoStartVal = conf.get(autoStartKey);	if (!autoStartIgnore.equalsIgnoreCase(currentAutoStartVal)) {	
is set to unsupported value setting it to value 

pmf = JDOHelper.getPersistenceManagerFactory(prop);	} else {	try {	DataSource ds = dsp.create(conf);	Map<Object, Object> dsProperties = new HashMap<>();	dsProperties.putAll(prop);	dsProperties.put("datanucleus.ConnectionFactory", ds);	dsProperties.put("javax.jdo.PersistenceManagerFactoryClass", "org.datanucleus.api.jdo.JDOPersistenceManagerFactory");	pmf = JDOHelper.getPersistenceManagerFactory(dsProperties);	} catch (SQLException e) {	
could not create persistencemanagerfactory using connection pool properties will fall back 

if (dsc != null) {	String objTypes = MetastoreConf.getVar(conf, ConfVars.CACHE_PINOBJTYPES);	LOG.info("Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=\"{}\"", objTypes);	if (org.apache.commons.lang.StringUtils.isNotEmpty(objTypes)) {	String[] typeTokens = objTypes.toLowerCase().split(",");	for (String type : typeTokens) {	type = type.trim();	if (PINCLASSMAP.containsKey(type)) {	dsc.pinAll(true, PINCLASSMAP.get(type));	} else {	
is not one of the pinnable object types 

String[] typeTokens = objTypes.toLowerCase().split(",");	for (String type : typeTokens) {	type = type.trim();	if (PINCLASSMAP.containsKey(type)) {	dsc.pinAll(true, PINCLASSMAP.get(type));	} else {	}	}	}	} else {	
persistencemanagerfactory returned null datastorecache object unable to initialize object pin types defined by hive metastore cache pinobjtypes 

public void shutdown() {	
rawstore with persistencemanager will be shutdown 

public boolean commitTransaction() {	if (TXN_STATUS.ROLLBACK == transactionStatus) {	debugLog("Commit transaction: rollback");	return false;	}	if (openTrasactionCalls <= 0) {	RuntimeException e = new RuntimeException("commitTransaction was called but openTransactionCalls = " + openTrasactionCalls + ". This probably indicates that there are unbalanced " + "calls to openTransaction/commitTransaction");	
unbalanced calls to open commit transaction 

if (TXN_STATUS.ROLLBACK == transactionStatus) {	debugLog("Commit transaction: rollback");	return false;	}	if (openTrasactionCalls <= 0) {	RuntimeException e = new RuntimeException("commitTransaction was called but openTransactionCalls = " + openTrasactionCalls + ". This probably indicates that there are unbalanced " + "calls to openTransaction/commitTransaction");	throw e;	}	if (!currentTransaction.isActive()) {	RuntimeException e = new RuntimeException("commitTransaction was called but openTransactionCalls = " + openTrasactionCalls + ". This probably indicates that there are unbalanced " + "calls to openTransaction/commitTransaction");	
unbalanced calls to open commit transaction 

public Database getDatabase(String name) throws NoSuchObjectException {	MetaException ex = null;	Database db = null;	try {	db = getDatabaseInternal(name);	} catch (MetaException e) {	ex = e;	}	if (db == null) {	
failed to get database returning nosuchobjectexception 

public boolean dropDatabase(String dbname) throws NoSuchObjectException, MetaException {	boolean success = false;	
dropping database along with all tables 

query.declareParameters("java.lang.String typeName");	query.setUnique(true);	MType type = (MType) query.execute(typeName.trim());	pm.retrieve(type);	if (type != null) {	pm.deletePersistent(type);	}	success = commitTransaction();	} catch (JDOObjectNotFoundException e) {	success = commitTransaction();	
type not found 

if (CollectionUtils.isNotEmpty(partGrants)) {	pm.deletePersistentAll(partGrants);	}	List<MPartitionColumnPrivilege> partColGrants = listTableAllPartitionColumnGrants(dbName, tableName);	if (CollectionUtils.isNotEmpty(partColGrants)) {	pm.deletePersistentAll(partColGrants);	}	try {	deleteTableColumnStatistics(dbName, tableName, null);	} catch (NoSuchObjectException e) {	
found no table level column statistics associated with db table record to delete 

private MTable convertToMTable(Table tbl) throws InvalidObjectException, MetaException {	if (tbl == null) {	return null;	}	MDatabase mdb = null;	try {	mdb = getMDatabase(tbl.getDbName());	} catch (NoSuchObjectException e) {	
could not convert to mtable 

}	List<MPartitionColumnPrivilege> partColumnGrants = listPartitionAllColumnGrants( part.getTable().getDatabase().getName(), part.getTable().getTableName(), Lists.newArrayList(partName));	if (CollectionUtils.isNotEmpty(partColumnGrants)) {	pm.deletePersistentAll(partColumnGrants);	}	String dbName = part.getTable().getDatabase().getName();	String tableName = part.getTable().getTableName();	try {	deletePartitionColumnStatistics(dbName, tableName, partName, part.getValues(), null);	} catch (NoSuchObjectException e) {	
no column statistics records found to delete 

public List<String> listPartitionNames(String dbName, String tableName, short max) throws MetaException {	List<String> pns = null;	boolean success = false;	try {	openTransaction();	
executing getpartitionnames 

String partKey = "/" + key.getName() + "=";	if (pkeys.size() == 1 && (pkeys.get(0).getName().matches(key.getName()))) {	buffer.append("partitionName.substring(partitionName.indexOf(\"") .append(key.getName()).append("=\") + ").append(key.getName().length() + 1) .append(")");	} else if ((pkeys.get(0).getName().matches(key.getName()))) {	buffer.append("partitionName.substring(partitionName.indexOf(\"") .append(key.getName()).append("=\") + ").append(key.getName().length() + 1).append(", ") .append("partitionName.indexOf(\"/\")") .append(")");	} else if ((pkeys.get(pkeys.size() - 1).getName().matches(key.getName()))) {	buffer.append("partitionName.substring(partitionName.indexOf(\"") .append(partKey).append("\") + ").append(partKey.length()) .append(")");	} else {	buffer.append("partitionName.substring(partitionName.indexOf(\"") .append(partKey).append("\") + ").append(partKey.length()).append(", ") .append("partitionName.indexOf(\"/\", partitionName.indexOf(\"").append(partKey) .append("\") + 1))");	}	
query for key is 

public PartitionValuesResponse listPartitionValues(String dbName, String tableName, List<FieldSchema> cols, boolean applyDistinct, String filter, boolean ascending, List<FieldSchema> order, long maxParts) throws MetaException {	dbName = dbName.toLowerCase().trim();	tableName = tableName.toLowerCase().trim();	try {	if (filter == null || filter.isEmpty()) {	PartitionValuesResponse response = getDistinctValuesForPartitionsNoTxn(dbName, tableName, cols, applyDistinct, ascending, maxParts);	
number of records fetched 

public PartitionValuesResponse listPartitionValues(String dbName, String tableName, List<FieldSchema> cols, boolean applyDistinct, String filter, boolean ascending, List<FieldSchema> order, long maxParts) throws MetaException {	dbName = dbName.toLowerCase().trim();	tableName = tableName.toLowerCase().trim();	try {	if (filter == null || filter.isEmpty()) {	PartitionValuesResponse response = getDistinctValuesForPartitionsNoTxn(dbName, tableName, cols, applyDistinct, ascending, maxParts);	return response;	} else {	PartitionValuesResponse response = extractPartitionNamesByFilter(dbName, tableName, filter, cols, ascending, applyDistinct, maxParts);	if (response != null && response.getPartitionValues() != null) {	
number of records fetched with filter 

if (filter == null || filter.isEmpty()) {	PartitionValuesResponse response = getDistinctValuesForPartitionsNoTxn(dbName, tableName, cols, applyDistinct, ascending, maxParts);	return response;	} else {	PartitionValuesResponse response = extractPartitionNamesByFilter(dbName, tableName, filter, cols, ascending, applyDistinct, maxParts);	if (response != null && response.getPartitionValues() != null) {	}	return response;	}	} catch (Exception t) {	
exception in orm 

private PartitionValuesResponse extractPartitionNamesByFilter(String dbName, String tableName, String filter, List<FieldSchema> cols, boolean ascending, boolean applyDistinct, long maxParts) throws MetaException, NoSuchObjectException {	
database table filter cols 

private PartitionValuesResponse extractPartitionNamesByFilter(String dbName, String tableName, String filter, List<FieldSchema> cols, boolean ascending, boolean applyDistinct, long maxParts) throws MetaException, NoSuchObjectException {	List<String> partitionNames = null;	List<Partition> partitions = null;	Table tbl = getTable(dbName, tableName);	try {	partitionNames = getPartitionNamesByFilter(dbName, tableName, filter, ascending, maxParts);	} catch (MetaException e) {	
querying by partition names failed trying out with partition objects filter 

}	}	if (partitionNames == null && partitions == null) {	throw new MetaException("Cannot obtain list of partitions by filter:\"" + filter + "\" for " + dbName + ":" + tableName);	}	if (!ascending) {	Collections.sort(partitionNames, Collections.reverseOrder());	}	PartitionValuesResponse response = new PartitionValuesResponse();	response.setPartitionValues(new ArrayList<PartitionValuesRow>(partitionNames.size()));	
converting responses to partition values for items 

private List<String> getPartitionNamesByFilter(String dbName, String tableName, String filter, boolean ascending, long maxParts) throws MetaException {	boolean success = false;	List<String> partNames = new ArrayList<String>();	try {	openTransaction();	
executing getpartitionnamesbyfilter 

MTable mtable = getMTable(dbName, tableName);	if( mtable == null ) {	return partNames;	}	Map<String, Object> params = new HashMap<String, Object>();	String queryFilterString = makeQueryFilterString(dbName, mtable, filter, params);	Query query = pm.newQuery( "select partitionName from org.apache.hadoop.hive.metastore.model.MPartition " + "where " + queryFilterString);	if (maxParts >= 0) {	query.setRange(0, maxParts);	}	
filter specified is jdoql filter is 

MTable mtable = getMTable(dbName, tableName);	if( mtable == null ) {	return partNames;	}	Map<String, Object> params = new HashMap<String, Object>();	String queryFilterString = makeQueryFilterString(dbName, mtable, filter, params);	Query query = pm.newQuery( "select partitionName from org.apache.hadoop.hive.metastore.model.MPartition " + "where " + queryFilterString);	if (maxParts >= 0) {	query.setRange(0, maxParts);	}	
parms is 

String parameterDeclaration = makeParameterDeclarationStringObj(params);	query.declareParameters(parameterDeclaration);	if (ascending) {	query.setOrdering("partitionName ascending");	} else {	query.setOrdering("partitionName descending");	}	query.setResult("partitionName");	Collection<String> names = (Collection<String>) query.executeWithMap(params);	partNames = new ArrayList<String>(names);	
done executing query for getpartitionnamesbyfilter 

query.declareParameters(parameterDeclaration);	if (ascending) {	query.setOrdering("partitionName ascending");	} else {	query.setOrdering("partitionName descending");	}	query.setResult("partitionName");	Collection<String> names = (Collection<String>) query.executeWithMap(params);	partNames = new ArrayList<String>(names);	success = commitTransaction();	
done retrieving all objects for getpartitionnamesbyfilter size 

}	StringBuilder partValuesSelect = new StringBuilder(256);	if (applyDistinct) {	partValuesSelect.append("DISTINCT ");	}	List<FieldSchema> partitionKeys = getTable(dbName, tableName).getPartitionKeys();	for (FieldSchema key : cols) {	partValuesSelect.append(extractPartitionKey(key, partitionKeys)).append(", ");	}	partValuesSelect.setLength(partValuesSelect.length() - 2);	
columns to be selected from partitions 

public List<Partition> listPartitionsPsWithAuth(String db_name, String tbl_name, List<String> part_vals, short max_parts, String userName, List<String> groupNames) throws MetaException, InvalidObjectException, NoSuchObjectException {	List<Partition> partitions = new ArrayList<>();	boolean success = false;	QueryWrapper queryWrapper = new QueryWrapper();	try {	openTransaction();	
executing listpartitionnamespswithauth 

public List<String> listPartitionNamesPs(String dbName, String tableName, List<String> part_vals, short max_parts) throws MetaException, NoSuchObjectException {	List<String> partitionNames = new ArrayList<>();	boolean success = false;	QueryWrapper queryWrapper = new QueryWrapper();	try {	openTransaction();	
executing listpartitionnamesps 

private List<MPartition> listMPartitions(String dbName, String tableName, int max, QueryWrapper queryWrapper) {	boolean success = false;	List<MPartition> mparts = null;	try {	openTransaction();	
executing listmpartitions 

openTransaction();	dbName = normalizeIdentifier(dbName);	tableName = normalizeIdentifier(tableName);	Query query = queryWrapper.query = pm.newQuery(MPartition.class, "table.tableName == t1 && table.database.name == t2");	query.declareParameters("java.lang.String t1, java.lang.String t2");	query.setOrdering("partitionName ascending");	if (max > 0) {	query.setRange(0, max);	}	mparts = (List<MPartition>) query.execute(tableName, dbName);	
done executing query for listmpartitions 

tableName = normalizeIdentifier(tableName);	Query query = queryWrapper.query = pm.newQuery(MPartition.class, "table.tableName == t1 && table.database.name == t2");	query.declareParameters("java.lang.String t1, java.lang.String t2");	query.setOrdering("partitionName ascending");	if (max > 0) {	query.setRange(0, max);	}	mparts = (List<MPartition>) query.execute(tableName, dbName);	pm.retrieveAll(mparts);	success = commitTransaction();	
done retrieving all objects for listmpartitions 

return null;	}	Query query = pm.newQuery(MPartition.class, jdoFilter);	if (maxParts >= 0) {	query.setRange(0, maxParts);	}	String parameterDeclaration = makeParameterDeclarationStringObj(params);	query.declareParameters(parameterDeclaration);	query.setOrdering("partitionName ascending");	List<MPartition> mparts = (List<MPartition>) query.executeWithMap(params);	
done executing query for getpartitionsviaormfilter 

}	Query query = pm.newQuery(MPartition.class, jdoFilter);	if (maxParts >= 0) {	query.setRange(0, maxParts);	}	String parameterDeclaration = makeParameterDeclarationStringObj(params);	query.declareParameters(parameterDeclaration);	query.setOrdering("partitionName ascending");	List<MPartition> mparts = (List<MPartition>) query.executeWithMap(params);	pm.retrieveAll(mparts);	
done retrieving all objects for getpartitionsviaormfilter 

private void dropPartitionsNoTxn(String dbName, String tblName, List<String> partNames) {	ObjectPair<Query, Map<String, String>> queryWithParams = getPartQueryWithParams(dbName, tblName, partNames);	Query query = queryWithParams.getFirst();	query.setClass(MPartition.class);	long deleted = query.deletePersistentAll(queryWithParams.getSecond());	
deleted partition from store 

n++;	String part = itr.next();	params.put(pn, part);	sb.append("partitionName == ").append(pn);	sb.append(" || ");	}	sb.setLength(sb.length() - 4);	sb.append(')');	Query query = pm.newQuery();	query.setFilter(sb.toString());	
jdoql filter is 

private void handleDirectSqlError(Exception ex) throws MetaException, NoSuchObjectException {	String message = null;	try {	message = generateShorterMessage(ex);	} catch (Throwable t) {	message = ex.toString() + "; error building a better message: " + t.getMessage();	}	LOG.warn(message);	if (LOG.isDebugEnabled()) {	
full directsql callstack for debugging note this is not an error 

JDOException rollbackEx = null;	try {	rollbackTransaction();	} catch (JDOException jex) {	rollbackEx = jex;	}	if (rollbackEx != null) {	if (currentTransaction != null && currentTransaction.isActive()) {	throw rollbackEx;	}	
ignoring exception rollback succeeded 

private T commit() {	success = commitTransaction();	if (doTrace) {	double time = ((System.nanoTime() - start) / 1000000.0);	String result = describeResult();	String retrieveType = doUseDirectSql ? "SQL" : "ORM";	
retrieved using in ms 

queryBuilder.append("table.tableName == t1 && table.database.name == t2");	params.put("t1", table.getTableName());	params.put("t2", table.getDbName());	} else {	queryBuilder.append("database.name == dbName");	params.put("dbName", dbName);	}	tree.generateJDOFilterFragment(getConf(), table, params, queryBuilder);	if (queryBuilder.hasError()) {	assert !isValidatedFilter;	
jdo filter pushdown cannot be used 

public List<String> listTableNamesByFilter(String dbName, String filter, short maxTables) throws MetaException {	boolean success = false;	Query query = null;	List<String> tableNames = new ArrayList<>();	try {	openTransaction();	
executing listtablenamesbyfilter 

dbName = normalizeIdentifier(dbName);	Map<String, Object> params = new HashMap<>();	String queryFilterString = makeQueryFilterString(dbName, null, filter, params);	query = pm.newQuery(MTable.class);	query.declareImports("import java.lang.String");	query.setResult("tableName");	query.setResultClass(java.lang.String.class);	if (maxTables >= 0) {	query.setRange(0, maxTables);	}	
filter specified is jdoql filter is 

String queryFilterString = makeQueryFilterString(dbName, null, filter, params);	query = pm.newQuery(MTable.class);	query.declareImports("import java.lang.String");	query.setResult("tableName");	query.setResultClass(java.lang.String.class);	if (maxTables >= 0) {	query.setRange(0, maxTables);	}	if (LOG.isDebugEnabled()) {	for (Entry<String, Object> entry : params.entrySet()) {	
key value class 

}	if (LOG.isDebugEnabled()) {	for (Entry<String, Object> entry : params.entrySet()) {	}	}	String parameterDeclaration = makeParameterDeclarationStringObj(params);	query.declareParameters(parameterDeclaration);	query.setFilter(queryFilterString);	Collection<String> names = (Collection<String>)query.executeWithMap(params);	tableNames = new ArrayList<>(new HashSet<>(names));	
done executing query for listtablenamesbyfilter 

if (LOG.isDebugEnabled()) {	for (Entry<String, Object> entry : params.entrySet()) {	}	}	String parameterDeclaration = makeParameterDeclarationStringObj(params);	query.declareParameters(parameterDeclaration);	query.setFilter(queryFilterString);	Collection<String> names = (Collection<String>)query.executeWithMap(params);	tableNames = new ArrayList<>(new HashSet<>(names));	success = commitTransaction();	
done retrieving all objects for listtablenamesbyfilter 

public List<String> listPartitionNamesByFilter(String dbName, String tableName, String filter, short maxParts) throws MetaException {	boolean success = false;	Query query = null;	List<String> partNames = new ArrayList<>();	try {	openTransaction();	
executing listmpartitionnamesbyfilter 

MTable mtable = getMTable(dbName, tableName);	if (mtable == null) {	return partNames;	}	Map<String, Object> params = new HashMap<>();	String queryFilterString = makeQueryFilterString(dbName, mtable, filter, params);	query = pm.newQuery("select partitionName from org.apache.hadoop.hive.metastore.model.MPartition " + "where " + queryFilterString);	if (maxParts >= 0) {	query.setRange(0, maxParts);	}	
filter specified is jdoql filter is 

MTable mtable = getMTable(dbName, tableName);	if (mtable == null) {	return partNames;	}	Map<String, Object> params = new HashMap<>();	String queryFilterString = makeQueryFilterString(dbName, mtable, filter, params);	query = pm.newQuery("select partitionName from org.apache.hadoop.hive.metastore.model.MPartition " + "where " + queryFilterString);	if (maxParts >= 0) {	query.setRange(0, maxParts);	}	
parms is 

query = pm.newQuery("select partitionName from org.apache.hadoop.hive.metastore.model.MPartition " + "where " + queryFilterString);	if (maxParts >= 0) {	query.setRange(0, maxParts);	}	String parameterDeclaration = makeParameterDeclarationStringObj(params);	query.declareParameters(parameterDeclaration);	query.setOrdering("partitionName ascending");	query.setResult("partitionName");	Collection<String> names = (Collection<String>) query.executeWithMap(params);	partNames = new ArrayList<>(names);	
done executing query for listmpartitionnamesbyfilter 

if (maxParts >= 0) {	query.setRange(0, maxParts);	}	String parameterDeclaration = makeParameterDeclarationStringObj(params);	query.declareParameters(parameterDeclaration);	query.setOrdering("partitionName ascending");	query.setResult("partitionName");	Collection<String> names = (Collection<String>) query.executeWithMap(params);	partNames = new ArrayList<>(names);	success = commitTransaction();	
done retrieving all objects for listmpartitionnamesbyfilter 

private void removeUnusedColumnDescriptor(MColumnDescriptor oldCD) {	if (oldCD == null) {	return;	}	boolean success = false;	QueryWrapper queryWrapper = new QueryWrapper();	try {	openTransaction();	
execute removeunusedcolumndescriptor 

try {	openTransaction();	Query query = pm.newQuery("select count(1) from " + "org.apache.hadoop.hive.metastore.model.MStorageDescriptor where (this.cd == inCD)");	query.declareParameters("MColumnDescriptor inCD");	long count = ((Long)query.execute(oldCD)).longValue();	if (count == 0) {	pm.retrieve(oldCD);	pm.deletePersistent(oldCD);	}	success = commitTransaction();	
successfully deleted a cd in removeunusedcolumndescriptor 

private String createDbGuidAndPersist() throws MetaException {	boolean success = false;	Query query = null;	try {	openTransaction();	MMetastoreDBProperties prop = new MMetastoreDBProperties();	prop.setPropertykey("guid");	final String guid = UUID.randomUUID().toString();	
attempting to add a guid for the metastore db 

try {	openTransaction();	MMetastoreDBProperties prop = new MMetastoreDBProperties();	prop.setPropertykey("guid");	final String guid = UUID.randomUUID().toString();	prop.setPropertyValue(guid);	prop.setDescription("Metastore DB GUID generated on " + LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss.SSS")));	pm.makePersistent(prop);	success = commitTransaction();	if (success) {	
metastore db guid created successfully 

prop.setPropertykey("guid");	final String guid = UUID.randomUUID().toString();	prop.setPropertyValue(guid);	prop.setDescription("Metastore DB GUID generated on " + LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss.SSS")));	pm.makePersistent(prop);	success = commitTransaction();	if (success) {	return guid;	}	} catch (Exception e) {	
metastore db guid creation failed 

boolean success = false;	Query query = null;	try {	openTransaction();	query = pm.newQuery(MMetastoreDBProperties.class, "this.propertyKey == key");	query.declareParameters("java.lang.String key");	Collection<MMetastoreDBProperties> names = (Collection<MMetastoreDBProperties>) query.execute("guid");	List<String> uuids = new ArrayList<>();	for (Iterator<MMetastoreDBProperties> i = names.iterator(); i.hasNext();) {	String uuid = i.next().getPropertyValue();	
found guid 

List<String> uuids = new ArrayList<>();	for (Iterator<MMetastoreDBProperties> i = names.iterator(); i.hasNext();) {	String uuid = i.next().getPropertyValue();	uuids.add(uuid);	}	success = commitTransaction();	if(uuids.size() > 1) {	throw new MetaException("Multiple uuids found");	}	if(!uuids.isEmpty()) {	
returning guid of metastore db 

success = commitTransaction();	if(uuids.size() > 1) {	throw new MetaException("Multiple uuids found");	}	if(!uuids.isEmpty()) {	return uuids.get(0);	}	} finally {	rollbackAndCleanup(success, query);	}	
guid for metastore db not found 

public List<Index> getIndexes(String dbName, String origTableName, int max) throws MetaException {	boolean success = false;	Query query = null;	try {	
executing getindexes 

origTableName = normalizeIdentifier(origTableName);	query = pm.newQuery(MIndex.class, "origTable.tableName == t1 && origTable.database.name == t2");	query.declareParameters("java.lang.String t1, java.lang.String t2");	List<MIndex> mIndexes = (List<MIndex>) query.execute(origTableName, dbName);	pm.retrieveAll(mIndexes);	List<Index> indexes = new ArrayList<>(mIndexes.size());	for (MIndex mIdx : mIndexes) {	indexes.add(this.convertToIndex(mIdx));	}	success = commitTransaction();	
done retrieving all objects for getindexes 

public List<String> listIndexNames(String dbName, String origTableName, short max) throws MetaException {	List<String> pns = new ArrayList<>();	boolean success = false;	Query query = null;	try {	openTransaction();	
executing listindexnames 

public List<MRoleMap> listMRoles(String principalName, PrincipalType principalType) {	boolean success = false;	Query query = null;	List<MRoleMap> mRoleMember = new ArrayList<>();	try {	
executing listroles 

List<MRoleMap> mRoleMember = new ArrayList<>();	try {	openTransaction();	query = pm.newQuery(MRoleMap.class, "principalName == t1 && principalType == t2");	query.declareParameters("java.lang.String t1, java.lang.String t2");	query.setUnique(false);	List<MRoleMap> mRoles = (List<MRoleMap>) query.executeWithArray(principalName, principalType.toString());	pm.retrieveAll(mRoles);	success = commitTransaction();	mRoleMember.addAll(mRoles);	
done retrieving all objects for listroles 

private List<MRoleMap> listMSecurityPrincipalMembershipRole(final String roleName, final PrincipalType principalType, QueryWrapper queryWrapper) {	boolean success = false;	List<MRoleMap> mRoleMemebership = null;	try {	
executing listmsecurityprincipalmembershiprole 

private List<MRoleMap> listMSecurityPrincipalMembershipRole(final String roleName, final PrincipalType principalType, QueryWrapper queryWrapper) {	boolean success = false;	List<MRoleMap> mRoleMemebership = null;	try {	openTransaction();	Query query = queryWrapper.query = pm.newQuery(MRoleMap.class, "principalName == t1 && principalType == t2");	query.declareParameters("java.lang.String t1, java.lang.String t2");	mRoleMemebership = (List<MRoleMap>) query.execute(roleName, principalType.toString());	pm.retrieveAll(mRoleMemebership);	success = commitTransaction();	
done retrieving all objects for listmsecurityprincipalmembershiprole 

public List<String> listRoleNames() {	boolean success = false;	Query query = null;	try {	openTransaction();	
executing listallrolenames 

public List<MRoleMap> listMRoleMembers(String roleName) {	boolean success = false;	Query query = null;	List<MRoleMap> mRoleMemeberList = new ArrayList<>();	try {	
executing listrolemembers 

List<MRoleMap> mRoleMemeberList = new ArrayList<>();	try {	openTransaction();	query = pm.newQuery(MRoleMap.class, "role.roleName == t1");	query.declareParameters("java.lang.String t1");	query.setUnique(false);	List<MRoleMap> mRoles = (List<MRoleMap>) query.execute(roleName);	pm.retrieveAll(mRoles);	success = commitTransaction();	mRoleMemeberList.addAll(mRoles);	
done retrieving all objects for listrolemembers 

public List<MDBPrivilege> listPrincipalMDBGrants(String principalName, PrincipalType principalType, String dbName) {	boolean success = false;	Query query = null;	List<MDBPrivilege> mSecurityDBList = new ArrayList<>();	dbName = normalizeIdentifier(dbName);	try {	
executing listprincipaldbgrants 

List<MDBPrivilege> mSecurityDBList = new ArrayList<>();	dbName = normalizeIdentifier(dbName);	try {	openTransaction();	query = pm.newQuery(MDBPrivilege.class, "principalName == t1 && principalType == t2 && database.name == t3");	query.declareParameters("java.lang.String t1, java.lang.String t2, java.lang.String t3");	List<MDBPrivilege> mPrivs = (List<MDBPrivilege>) query.executeWithArray(principalName, principalType.toString(), dbName);	pm.retrieveAll(mPrivs);	success = commitTransaction();	mSecurityDBList.addAll(mPrivs);	
done retrieving all objects for listprincipaldbgrants 

private List<MDBPrivilege> listPrincipalAllDBGrant(String principalName, PrincipalType principalType, QueryWrapper queryWrapper) {	boolean success = false;	Query query = null;	List<MDBPrivilege> mSecurityDBList = null;	try {	
executing listprincipalalldbgrant 

if (principalName != null && principalType != null) {	query = queryWrapper.query = pm.newQuery(MDBPrivilege.class, "principalName == t1 && principalType == t2");	query.declareParameters("java.lang.String t1, java.lang.String t2");	mSecurityDBList = (List<MDBPrivilege>) query.execute(principalName, principalType.toString());	} else {	query = queryWrapper.query = pm.newQuery(MDBPrivilege.class);	mSecurityDBList = (List<MDBPrivilege>) query.execute();	}	pm.retrieveAll(mSecurityDBList);	success = commitTransaction();	
done retrieving all objects for listprincipalalldbgrant 

public List<MTablePrivilege> listAllTableGrants(String dbName, String tableName) {	boolean success = false;	Query query = null;	tableName = normalizeIdentifier(tableName);	dbName = normalizeIdentifier(dbName);	List<MTablePrivilege> mSecurityTabList = new ArrayList<>();	tableName = normalizeIdentifier(tableName);	dbName = normalizeIdentifier(dbName);	try {	
executing listalltablegrants 

dbName = normalizeIdentifier(dbName);	List<MTablePrivilege> mSecurityTabList = new ArrayList<>();	tableName = normalizeIdentifier(tableName);	dbName = normalizeIdentifier(dbName);	try {	openTransaction();	String queryStr = "table.tableName == t1 && table.database.name == t2";	query = pm.newQuery(MTablePrivilege.class, queryStr);	query.declareParameters("java.lang.String t1, java.lang.String t2");	List<MTablePrivilege> mPrivs  = (List<MTablePrivilege>) query.executeWithArray(tableName, dbName);	
done executing query for listalltablegrants 

dbName = normalizeIdentifier(dbName);	try {	openTransaction();	String queryStr = "table.tableName == t1 && table.database.name == t2";	query = pm.newQuery(MTablePrivilege.class, queryStr);	query.declareParameters("java.lang.String t1, java.lang.String t2");	List<MTablePrivilege> mPrivs  = (List<MTablePrivilege>) query.executeWithArray(tableName, dbName);	pm.retrieveAll(mPrivs);	success = commitTransaction();	mSecurityTabList.addAll(mPrivs);	
done retrieving all objects for listalltablegrants 

public List<MPartitionPrivilege> listTableAllPartitionGrants(String dbName, String tableName) {	tableName = normalizeIdentifier(tableName);	dbName = normalizeIdentifier(dbName);	boolean success = false;	Query query = null;	List<MPartitionPrivilege> mSecurityTabPartList = new ArrayList<>();	try {	
executing listtableallpartitiongrants 

List<MPartitionPrivilege> mSecurityTabPartList = new ArrayList<>();	try {	openTransaction();	String queryStr = "partition.table.tableName == t1 && partition.table.database.name == t2";	query = pm.newQuery(MPartitionPrivilege.class, queryStr);	query.declareParameters("java.lang.String t1, java.lang.String t2");	List<MPartitionPrivilege> mPrivs = (List<MPartitionPrivilege>) query.executeWithArray(tableName, dbName);	pm.retrieveAll(mPrivs);	success = commitTransaction();	mSecurityTabPartList.addAll(mPrivs);	
done retrieving all objects for listtableallpartitiongrants 

public List<MTableColumnPrivilege> listTableAllColumnGrants(String dbName, String tableName) {	boolean success = false;	Query query = null;	List<MTableColumnPrivilege> mTblColPrivilegeList = new ArrayList<>();	tableName = normalizeIdentifier(tableName);	dbName = normalizeIdentifier(dbName);	try {	
executing listtableallcolumngrants 

dbName = normalizeIdentifier(dbName);	try {	openTransaction();	String queryStr = "table.tableName == t1 && table.database.name == t2";	query = pm.newQuery(MTableColumnPrivilege.class, queryStr);	query.declareParameters("java.lang.String t1, java.lang.String t2");	List<MTableColumnPrivilege> mPrivs = (List<MTableColumnPrivilege>) query.executeWithArray(tableName, dbName);	pm.retrieveAll(mPrivs);	success = commitTransaction();	mTblColPrivilegeList.addAll(mPrivs);	
done retrieving all objects for listtableallcolumngrants 

public List<MPartitionColumnPrivilege> listTableAllPartitionColumnGrants(String dbName, String tableName) {	boolean success = false;	Query query = null;	tableName = normalizeIdentifier(tableName);	dbName = normalizeIdentifier(dbName);	List<MPartitionColumnPrivilege> mSecurityColList = new ArrayList<>();	try {	
executing listtableallpartitioncolumngrants 

List<MPartitionColumnPrivilege> mSecurityColList = new ArrayList<>();	try {	openTransaction();	String queryStr = "partition.table.tableName == t1 && partition.table.database.name == t2";	query = pm.newQuery(MPartitionColumnPrivilege.class, queryStr);	query.declareParameters("java.lang.String t1, java.lang.String t2");	List<MPartitionColumnPrivilege> mPrivs = (List<MPartitionColumnPrivilege>) query.executeWithArray(tableName, dbName);	pm.retrieveAll(mPrivs);	success = commitTransaction();	mSecurityColList.addAll(mPrivs);	
done retrieving all objects for listtableallpartitioncolumngrants 

public List<MPartitionColumnPrivilege> listPartitionAllColumnGrants(String dbName, String tableName, List<String> partNames) {	boolean success = false;	tableName = normalizeIdentifier(tableName);	dbName = normalizeIdentifier(dbName);	List<MPartitionColumnPrivilege> mSecurityColList = null;	try {	openTransaction();	
executing listpartitionallcolumngrants 

public List<MPartitionColumnPrivilege> listPartitionAllColumnGrants(String dbName, String tableName, List<String> partNames) {	boolean success = false;	tableName = normalizeIdentifier(tableName);	dbName = normalizeIdentifier(dbName);	List<MPartitionColumnPrivilege> mSecurityColList = null;	try {	openTransaction();	mSecurityColList = queryByPartitionNames( dbName, tableName, partNames, MPartitionColumnPrivilege.class, "partition.table.tableName", "partition.table.database.name", "partition.partitionName");	
done executing query for listpartitionallcolumngrants 

public List<MPartitionColumnPrivilege> listPartitionAllColumnGrants(String dbName, String tableName, List<String> partNames) {	boolean success = false;	tableName = normalizeIdentifier(tableName);	dbName = normalizeIdentifier(dbName);	List<MPartitionColumnPrivilege> mSecurityColList = null;	try {	openTransaction();	mSecurityColList = queryByPartitionNames( dbName, tableName, partNames, MPartitionColumnPrivilege.class, "partition.table.tableName", "partition.table.database.name", "partition.partitionName");	pm.retrieveAll(mSecurityColList);	success = commitTransaction();	
done retrieving all objects for listpartitionallcolumngrants 

private List<MDBPrivilege> listDatabaseGrants(String dbName, QueryWrapper queryWrapper) {	dbName = normalizeIdentifier(dbName);	boolean success = false;	try {	
executing listdatabasegrants 

private List<MDBPrivilege> listDatabaseGrants(String dbName, QueryWrapper queryWrapper) {	dbName = normalizeIdentifier(dbName);	boolean success = false;	try {	openTransaction();	Query query = queryWrapper.query = pm.newQuery(MDBPrivilege.class, "database.name == t1");	query.declareParameters("java.lang.String t1");	List<MDBPrivilege> mSecurityDBList = (List<MDBPrivilege>) query.executeWithArray(dbName);	pm.retrieveAll(mSecurityDBList);	success = commitTransaction();	
done retrieving all objects for listdatabasegrants 

private List<MPartitionPrivilege> listPartitionGrants(String dbName, String tableName, List<String> partNames) {	tableName = normalizeIdentifier(tableName);	dbName = normalizeIdentifier(dbName);	boolean success = false;	List<MPartitionPrivilege> mSecurityTabPartList = null;	try {	openTransaction();	
executing listpartitiongrants 

private List<MPartitionPrivilege> listPartitionGrants(String dbName, String tableName, List<String> partNames) {	tableName = normalizeIdentifier(tableName);	dbName = normalizeIdentifier(dbName);	boolean success = false;	List<MPartitionPrivilege> mSecurityTabPartList = null;	try {	openTransaction();	mSecurityTabPartList = queryByPartitionNames( dbName, tableName, partNames, MPartitionPrivilege.class, "partition.table.tableName", "partition.table.database.name", "partition.partitionName");	
done executing query for listpartitiongrants 

private List<MPartitionPrivilege> listPartitionGrants(String dbName, String tableName, List<String> partNames) {	tableName = normalizeIdentifier(tableName);	dbName = normalizeIdentifier(dbName);	boolean success = false;	List<MPartitionPrivilege> mSecurityTabPartList = null;	try {	openTransaction();	mSecurityTabPartList = queryByPartitionNames( dbName, tableName, partNames, MPartitionPrivilege.class, "partition.table.tableName", "partition.table.database.name", "partition.partitionName");	pm.retrieveAll(mSecurityTabPartList);	success = commitTransaction();	
done retrieving all objects for listpartitiongrants 

public List<MTablePrivilege> listAllMTableGrants( String principalName, PrincipalType principalType, String dbName, String tableName) {	tableName = normalizeIdentifier(tableName);	dbName = normalizeIdentifier(dbName);	boolean success = false;	Query query = null;	List<MTablePrivilege> mSecurityTabPartList = new ArrayList<>();	try {	openTransaction();	
executing listalltablegrants 

Query query = null;	List<MTablePrivilege> mSecurityTabPartList = new ArrayList<>();	try {	openTransaction();	query = pm.newQuery(MTablePrivilege.class, "principalName == t1 && principalType == t2 && table.tableName == t3 && table.database.name == t4");	query .declareParameters("java.lang.String t1, java.lang.String t2, java.lang.String t3, java.lang.String t4");	List<MTablePrivilege> mPrivs = (List<MTablePrivilege>) query.executeWithArray(principalName, principalType.toString(), tableName, dbName);	pm.retrieveAll(mPrivs);	success = commitTransaction();	mSecurityTabPartList.addAll(mPrivs);	
done retrieving all objects for listalltablegrants 

public List<MPartitionPrivilege> listPrincipalMPartitionGrants( String principalName, PrincipalType principalType, String dbName, String tableName, String partName) {	boolean success = false;	Query query = null;	tableName = normalizeIdentifier(tableName);	dbName = normalizeIdentifier(dbName);	List<MPartitionPrivilege> mSecurityTabPartList = new ArrayList<>();	try {	
executing listprincipalpartitiongrants 

dbName = normalizeIdentifier(dbName);	List<MPartitionPrivilege> mSecurityTabPartList = new ArrayList<>();	try {	openTransaction();	query = pm.newQuery(MPartitionPrivilege.class, "principalName == t1 && principalType == t2 && partition.table.tableName == t3 " + "&& partition.table.database.name == t4 && partition.partitionName == t5");	query .declareParameters("java.lang.String t1, java.lang.String t2, java.lang.String t3, java.lang.String t4, " + "java.lang.String t5");	List<MPartitionPrivilege> mPrivs = (List<MPartitionPrivilege>) query.executeWithArray(principalName, principalType.toString(), tableName, dbName, partName);	pm.retrieveAll(mPrivs);	success = commitTransaction();	mSecurityTabPartList.addAll(mPrivs);	
done retrieving all objects for listprincipalpartitiongrants 

public List<MTableColumnPrivilege> listPrincipalMTableColumnGrants( String principalName, PrincipalType principalType, String dbName, String tableName, String columnName) {	boolean success = false;	Query query = null;	tableName = normalizeIdentifier(tableName);	dbName = normalizeIdentifier(dbName);	columnName = normalizeIdentifier(columnName);	List<MTableColumnPrivilege> mSecurityColList = new ArrayList<>();	try {	
executing listprincipaltablecolumngrants 

List<MTableColumnPrivilege> mSecurityColList = new ArrayList<>();	try {	openTransaction();	String queryStr = "principalName == t1 && principalType == t2 && " + "table.tableName == t3 && table.database.name == t4 &&  columnName == t5 ";	query = pm.newQuery(MTableColumnPrivilege.class, queryStr);	query.declareParameters("java.lang.String t1, java.lang.String t2, java.lang.String t3, " + "java.lang.String t4, java.lang.String t5");	List<MTableColumnPrivilege> mPrivs = (List<MTableColumnPrivilege>) query.executeWithArray(principalName, principalType.toString(), tableName, dbName, columnName);	pm.retrieveAll(mPrivs);	success = commitTransaction();	mSecurityColList.addAll(mPrivs);	
done retrieving all objects for listprincipaltablecolumngrants 

public List<MPartitionColumnPrivilege> listPrincipalMPartitionColumnGrants( String principalName, PrincipalType principalType, String dbName, String tableName, String partitionName, String columnName) {	boolean success = false;	Query query = null;	tableName = normalizeIdentifier(tableName);	dbName = normalizeIdentifier(dbName);	columnName = normalizeIdentifier(columnName);	List<MPartitionColumnPrivilege> mSecurityColList = new ArrayList<>();	try {	
executing listprincipalpartitioncolumngrants 

columnName = normalizeIdentifier(columnName);	List<MPartitionColumnPrivilege> mSecurityColList = new ArrayList<>();	try {	openTransaction();	query = pm.newQuery( MPartitionColumnPrivilege.class, "principalName == t1 && principalType == t2 && partition.table.tableName == t3 " + "&& partition.table.database.name == t4 && partition.partitionName == t5 && columnName == t6");	query.declareParameters("java.lang.String t1, java.lang.String t2, java.lang.String t3, " + "java.lang.String t4, java.lang.String t5, java.lang.String t6");	List<MPartitionColumnPrivilege> mPrivs = (List<MPartitionColumnPrivilege>) query.executeWithArray(principalName, principalType.toString(), tableName, dbName, partitionName, columnName);	pm.retrieveAll(mPrivs);	success = commitTransaction();	mSecurityColList.addAll(mPrivs);	
done retrieving all objects for listprincipalpartitioncolumngrants 

public List<HiveObjectPrivilege> listPrincipalPartitionColumnGrantsAll( String principalName, PrincipalType principalType) {	boolean success = false;	Query query = null;	try {	openTransaction();	
executing listprincipalpartitioncolumngrantsall 

openTransaction();	List<MPartitionColumnPrivilege> mSecurityTabPartList;	if (principalName != null && principalType != null) {	query = pm.newQuery(MPartitionColumnPrivilege.class, "principalName == t1 && principalType == t2");	query.declareParameters("java.lang.String t1, java.lang.String t2");	mSecurityTabPartList = (List<MPartitionColumnPrivilege>) query.executeWithArray(principalName, principalType.toString());	} else {	query = pm.newQuery(MPartitionColumnPrivilege.class);	mSecurityTabPartList = (List<MPartitionColumnPrivilege>) query.execute();	}	
done executing query for listprincipalpartitioncolumngrantsall 

query = pm.newQuery(MPartitionColumnPrivilege.class, "principalName == t1 && principalType == t2");	query.declareParameters("java.lang.String t1, java.lang.String t2");	mSecurityTabPartList = (List<MPartitionColumnPrivilege>) query.executeWithArray(principalName, principalType.toString());	} else {	query = pm.newQuery(MPartitionColumnPrivilege.class);	mSecurityTabPartList = (List<MPartitionColumnPrivilege>) query.execute();	}	pm.retrieveAll(mSecurityTabPartList);	List<HiveObjectPrivilege> result = convertPartCols(mSecurityTabPartList);	success = commitTransaction();	
done retrieving all objects for listprincipalpartitioncolumngrantsall 

public List<HiveObjectPrivilege> listPartitionColumnGrantsAll(String dbName, String tableName, String partitionName, String columnName) {	boolean success = false;	Query query = null;	try {	openTransaction();	
executing listpartitioncolumngrantsall 

public List<HiveObjectPrivilege> listPartitionColumnGrantsAll(String dbName, String tableName, String partitionName, String columnName) {	boolean success = false;	Query query = null;	try {	openTransaction();	query = pm.newQuery(MPartitionColumnPrivilege.class, "partition.table.tableName == t3 && partition.table.database.name == t4 && " + "partition.partitionName == t5 && columnName == t6");	query .declareParameters("java.lang.String t3, java.lang.String t4, java.lang.String t5, java.lang.String t6");	List<MPartitionColumnPrivilege> mSecurityTabPartList = (List<MPartitionColumnPrivilege>) query.executeWithArray(tableName, dbName, partitionName, columnName);	
done executing query for listpartitioncolumngrantsall 

boolean success = false;	Query query = null;	try {	openTransaction();	query = pm.newQuery(MPartitionColumnPrivilege.class, "partition.table.tableName == t3 && partition.table.database.name == t4 && " + "partition.partitionName == t5 && columnName == t6");	query .declareParameters("java.lang.String t3, java.lang.String t4, java.lang.String t5, java.lang.String t6");	List<MPartitionColumnPrivilege> mSecurityTabPartList = (List<MPartitionColumnPrivilege>) query.executeWithArray(tableName, dbName, partitionName, columnName);	pm.retrieveAll(mSecurityTabPartList);	List<HiveObjectPrivilege> result = convertPartCols(mSecurityTabPartList);	success = commitTransaction();	
done retrieving all objects for listpartitioncolumngrantsall 

private List<MTablePrivilege> listPrincipalAllTableGrants( String principalName, PrincipalType principalType, QueryWrapper queryWrapper) {	boolean success = false;	List<MTablePrivilege> mSecurityTabPartList = null;	try {	
executing listprincipalalltablegrants 

private List<MTablePrivilege> listPrincipalAllTableGrants( String principalName, PrincipalType principalType, QueryWrapper queryWrapper) {	boolean success = false;	List<MTablePrivilege> mSecurityTabPartList = null;	try {	openTransaction();	Query query = queryWrapper.query = pm.newQuery(MTablePrivilege.class, "principalName == t1 && principalType == t2");	query.declareParameters("java.lang.String t1, java.lang.String t2");	mSecurityTabPartList = (List<MTablePrivilege>) query.execute( principalName, principalType.toString());	pm.retrieveAll(mSecurityTabPartList);	success = commitTransaction();	
done retrieving all objects for listprincipalalltablegrants 

public List<HiveObjectPrivilege> listPrincipalTableGrantsAll(String principalName, PrincipalType principalType) {	boolean success = false;	Query query = null;	try {	openTransaction();	
executing listprincipalalltablegrants 

openTransaction();	List<MTablePrivilege> mSecurityTabPartList;	if (principalName != null && principalType != null) {	query = pm.newQuery(MTablePrivilege.class, "principalName == t1 && principalType == t2");	query.declareParameters("java.lang.String t1, java.lang.String t2");	mSecurityTabPartList = (List<MTablePrivilege>) query.execute(principalName, principalType.toString());	} else {	query = pm.newQuery(MTablePrivilege.class);	mSecurityTabPartList = (List<MTablePrivilege>) query.execute();	}	
done executing query for listprincipalalltablegrants 

query = pm.newQuery(MTablePrivilege.class, "principalName == t1 && principalType == t2");	query.declareParameters("java.lang.String t1, java.lang.String t2");	mSecurityTabPartList = (List<MTablePrivilege>) query.execute(principalName, principalType.toString());	} else {	query = pm.newQuery(MTablePrivilege.class);	mSecurityTabPartList = (List<MTablePrivilege>) query.execute();	}	pm.retrieveAll(mSecurityTabPartList);	List<HiveObjectPrivilege> result = convertTable(mSecurityTabPartList);	success = commitTransaction();	
done retrieving all objects for listprincipalalltablegrants 

public List<HiveObjectPrivilege> listTableGrantsAll(String dbName, String tableName) {	boolean success = false;	Query query = null;	dbName = normalizeIdentifier(dbName);	tableName = normalizeIdentifier(tableName);	try {	openTransaction();	
executing listtablegrantsall 

public List<HiveObjectPrivilege> listTableGrantsAll(String dbName, String tableName) {	boolean success = false;	Query query = null;	dbName = normalizeIdentifier(dbName);	tableName = normalizeIdentifier(tableName);	try {	openTransaction();	query = pm.newQuery(MTablePrivilege.class, "table.tableName == t1 && table.database.name == t2");	query.declareParameters("java.lang.String t1, java.lang.String t2");	List<MTablePrivilege> mSecurityTabPartList = (List<MTablePrivilege>) query.executeWithArray(tableName, dbName);	
done executing query for listtablegrantsall 

dbName = normalizeIdentifier(dbName);	tableName = normalizeIdentifier(tableName);	try {	openTransaction();	query = pm.newQuery(MTablePrivilege.class, "table.tableName == t1 && table.database.name == t2");	query.declareParameters("java.lang.String t1, java.lang.String t2");	List<MTablePrivilege> mSecurityTabPartList = (List<MTablePrivilege>) query.executeWithArray(tableName, dbName);	pm.retrieveAll(mSecurityTabPartList);	List<HiveObjectPrivilege> result = convertTable(mSecurityTabPartList);	success = commitTransaction();	
done retrieving all objects for listprincipalalltablegrants 

private List<MPartitionPrivilege> listPrincipalAllPartitionGrants(String principalName, PrincipalType principalType, QueryWrapper queryWrapper) {	boolean success = false;	List<MPartitionPrivilege> mSecurityTabPartList = null;	try {	openTransaction();	
executing listprincipalallpartitiongrants 

private List<MPartitionPrivilege> listPrincipalAllPartitionGrants(String principalName, PrincipalType principalType, QueryWrapper queryWrapper) {	boolean success = false;	List<MPartitionPrivilege> mSecurityTabPartList = null;	try {	openTransaction();	Query query = queryWrapper.query = pm.newQuery(MPartitionPrivilege.class, "principalName == t1 && principalType == t2");	query.declareParameters("java.lang.String t1, java.lang.String t2");	mSecurityTabPartList = (List<MPartitionPrivilege>) query.execute(principalName, principalType.toString());	pm.retrieveAll(mSecurityTabPartList);	success = commitTransaction();	
done retrieving all objects for listprincipalallpartitiongrants 

public List<HiveObjectPrivilege> listPrincipalPartitionGrantsAll(String principalName, PrincipalType principalType) {	boolean success = false;	Query query = null;	try {	openTransaction();	
executing listprincipalpartitiongrantsall 

openTransaction();	List<MPartitionPrivilege> mSecurityTabPartList;	if (principalName != null && principalType != null) {	query = pm.newQuery(MPartitionPrivilege.class, "principalName == t1 && principalType == t2");	query.declareParameters("java.lang.String t1, java.lang.String t2");	mSecurityTabPartList = (List<MPartitionPrivilege>) query.execute(principalName, principalType.toString());	} else {	query = pm.newQuery(MPartitionPrivilege.class);	mSecurityTabPartList = (List<MPartitionPrivilege>) query.execute();	}	
done executing query for listprincipalpartitiongrantsall 

query = pm.newQuery(MPartitionPrivilege.class, "principalName == t1 && principalType == t2");	query.declareParameters("java.lang.String t1, java.lang.String t2");	mSecurityTabPartList = (List<MPartitionPrivilege>) query.execute(principalName, principalType.toString());	} else {	query = pm.newQuery(MPartitionPrivilege.class);	mSecurityTabPartList = (List<MPartitionPrivilege>) query.execute();	}	pm.retrieveAll(mSecurityTabPartList);	List<HiveObjectPrivilege> result = convertPartition(mSecurityTabPartList);	success = commitTransaction();	
done retrieving all objects for listprincipalpartitiongrantsall 

public List<HiveObjectPrivilege> listPartitionGrantsAll(String dbName, String tableName, String partitionName) {	boolean success = false;	Query query = null;	try {	openTransaction();	
executing listprincipalpartitiongrantsall 

public List<HiveObjectPrivilege> listPartitionGrantsAll(String dbName, String tableName, String partitionName) {	boolean success = false;	Query query = null;	try {	openTransaction();	query = pm.newQuery(MPartitionPrivilege.class, "partition.table.tableName == t3 && partition.table.database.name == t4 && " + "partition.partitionName == t5");	query.declareParameters("java.lang.String t3, java.lang.String t4, java.lang.String t5");	List<MPartitionPrivilege> mSecurityTabPartList = (List<MPartitionPrivilege>) query.executeWithArray(tableName, dbName, partitionName);	
done executing query for listprincipalpartitiongrantsall 

boolean success = false;	Query query = null;	try {	openTransaction();	query = pm.newQuery(MPartitionPrivilege.class, "partition.table.tableName == t3 && partition.table.database.name == t4 && " + "partition.partitionName == t5");	query.declareParameters("java.lang.String t3, java.lang.String t4, java.lang.String t5");	List<MPartitionPrivilege> mSecurityTabPartList = (List<MPartitionPrivilege>) query.executeWithArray(tableName, dbName, partitionName);	pm.retrieveAll(mSecurityTabPartList);	List<HiveObjectPrivilege> result = convertPartition(mSecurityTabPartList);	success = commitTransaction();	
done retrieving all objects for listprincipalpartitiongrantsall 

private List<MTableColumnPrivilege> listPrincipalAllTableColumnGrants(String principalName, PrincipalType principalType, QueryWrapper queryWrapper) {	boolean success = false;	List<MTableColumnPrivilege> mSecurityColumnList = null;	try {	
executing listprincipalalltablecolumngrants 

private List<MTableColumnPrivilege> listPrincipalAllTableColumnGrants(String principalName, PrincipalType principalType, QueryWrapper queryWrapper) {	boolean success = false;	List<MTableColumnPrivilege> mSecurityColumnList = null;	try {	openTransaction();	Query query = queryWrapper.query = pm.newQuery(MTableColumnPrivilege.class, "principalName == t1 && principalType == t2");	query.declareParameters("java.lang.String t1, java.lang.String t2");	mSecurityColumnList = (List<MTableColumnPrivilege>) query.execute(principalName, principalType.toString());	pm.retrieveAll(mSecurityColumnList);	success = commitTransaction();	
done retrieving all objects for listprincipalalltablecolumngrants 

public List<HiveObjectPrivilege> listPrincipalTableColumnGrantsAll(String principalName, PrincipalType principalType) {	boolean success = false;	Query query = null;	try {	openTransaction();	
executing listprincipaltablecolumngrantsall 

openTransaction();	List<MTableColumnPrivilege> mSecurityTabPartList;	if (principalName != null && principalType != null) {	query = pm.newQuery(MTableColumnPrivilege.class, "principalName == t1 && principalType == t2");	query.declareParameters("java.lang.String t1, java.lang.String t2");	mSecurityTabPartList = (List<MTableColumnPrivilege>) query.execute(principalName, principalType.toString());	} else {	query = pm.newQuery(MTableColumnPrivilege.class);	mSecurityTabPartList = (List<MTableColumnPrivilege>) query.execute();	}	
done executing query for listprincipaltablecolumngrantsall 

query = pm.newQuery(MTableColumnPrivilege.class, "principalName == t1 && principalType == t2");	query.declareParameters("java.lang.String t1, java.lang.String t2");	mSecurityTabPartList = (List<MTableColumnPrivilege>) query.execute(principalName, principalType.toString());	} else {	query = pm.newQuery(MTableColumnPrivilege.class);	mSecurityTabPartList = (List<MTableColumnPrivilege>) query.execute();	}	pm.retrieveAll(mSecurityTabPartList);	List<HiveObjectPrivilege> result = convertTableCols(mSecurityTabPartList);	success = commitTransaction();	
done retrieving all objects for listprincipaltablecolumngrantsall 

public List<HiveObjectPrivilege> listTableColumnGrantsAll(String dbName, String tableName, String columnName) {	boolean success = false;	Query query = null;	dbName = normalizeIdentifier(dbName);	tableName = normalizeIdentifier(tableName);	try {	openTransaction();	
executing listprincipaltablecolumngrantsall 

public List<HiveObjectPrivilege> listTableColumnGrantsAll(String dbName, String tableName, String columnName) {	boolean success = false;	Query query = null;	dbName = normalizeIdentifier(dbName);	tableName = normalizeIdentifier(tableName);	try {	openTransaction();	query = pm.newQuery(MTableColumnPrivilege.class, "table.tableName == t3 && table.database.name == t4 &&  columnName == t5");	query.declareParameters("java.lang.String t3, java.lang.String t4, java.lang.String t5");	List<MTableColumnPrivilege> mSecurityTabPartList = (List<MTableColumnPrivilege>) query.executeWithArray(tableName, dbName, columnName);	
done executing query for listprincipaltablecolumngrantsall 

dbName = normalizeIdentifier(dbName);	tableName = normalizeIdentifier(tableName);	try {	openTransaction();	query = pm.newQuery(MTableColumnPrivilege.class, "table.tableName == t3 && table.database.name == t4 &&  columnName == t5");	query.declareParameters("java.lang.String t3, java.lang.String t4, java.lang.String t5");	List<MTableColumnPrivilege> mSecurityTabPartList = (List<MTableColumnPrivilege>) query.executeWithArray(tableName, dbName, columnName);	pm.retrieveAll(mSecurityTabPartList);	List<HiveObjectPrivilege> result = convertTableCols(mSecurityTabPartList);	success = commitTransaction();	
done retrieving all objects for listprincipaltablecolumngrantsall 

private List<MPartitionColumnPrivilege> listPrincipalAllPartitionColumnGrants( String principalName, PrincipalType principalType, QueryWrapper queryWrapper) {	boolean success = false;	List<MPartitionColumnPrivilege> mSecurityColumnList = null;	try {	
executing listprincipalalltablecolumngrants 

private List<MPartitionColumnPrivilege> listPrincipalAllPartitionColumnGrants( String principalName, PrincipalType principalType, QueryWrapper queryWrapper) {	boolean success = false;	List<MPartitionColumnPrivilege> mSecurityColumnList = null;	try {	openTransaction();	Query query = queryWrapper.query = pm.newQuery(MPartitionColumnPrivilege.class, "principalName == t1 && principalType == t2");	query.declareParameters("java.lang.String t1, java.lang.String t2");	mSecurityColumnList = (List<MPartitionColumnPrivilege>) query.execute(principalName, principalType.toString());	pm.retrieveAll(mSecurityColumnList);	success = commitTransaction();	
done retrieving all objects for listprincipalalltablecolumngrants 

public boolean isPartitionMarkedForEvent(String dbName, String tblName, Map<String, String> partName, PartitionEventType evtType) throws UnknownTableException, MetaException, InvalidPartitionException, UnknownPartitionException {	boolean success = false;	Query query = null;	try {	
begin executing ispartitionmarkedforevent 

openTransaction();	query = pm.newQuery(MPartitionEvent.class, "dbName == t1 && tblName == t2 && partName == t3 && eventType == t4");	query .declareParameters("java.lang.String t1, java.lang.String t2, java.lang.String t3, int t4");	Table tbl = getTable(dbName, tblName);	if (null == tbl) {	throw new UnknownTableException("Table: " + tblName + " is not found.");	}	Collection<MPartitionEvent> partEvents = (Collection<MPartitionEvent>) query.executeWithArray(dbName, tblName, getPartitionStr(tbl, partName), evtType.getValue());	pm.retrieveAll(partEvents);	success = commitTransaction();	
done executing ispartitionmarkedforevent 

public Table markPartitionForEvent(String dbName, String tblName, Map<String,String> partName, PartitionEventType evtType) throws MetaException, UnknownTableException, InvalidPartitionException, UnknownPartitionException {	
begin executing markpartitionforevent 

boolean success = false;	Table tbl = null;	try{	openTransaction();	tbl = getTable(dbName, tblName);	if(null == tbl) {	throw new UnknownTableException("Table: "+ tblName + " is not found.");	}	pm.makePersistent(new MPartitionEvent(dbName,tblName,getPartitionStr(tbl, partName), evtType.getValue()));	success = commitTransaction();	
done executing markpartitionforevent 

LOG.info("Updating partition level column statistics for db=" + dbName + " tableName=" + tableName + " partName=" + partName + " colName=" + colName);	boolean foundCol = false;	List<FieldSchema> colList = partition.getSd().getCols();	for (FieldSchema col : colList) {	if (col.getName().equals(mStatsObj.getColName())) {	foundCol = true;	break;	}	}	if (!foundCol) {	
column for which stats gathering is requested doesn t exist 

query.setFilter(filter);	query.declareParameters(paramStr);	result = (List<MTableColumnStatistics>) query.executeWithArray(params);	pm.retrieveAll(result);	if (result.size() > colNames.size()) {	throw new MetaException("Unexpected " + result.size() + " statistics for " + colNames.size() + " columns");	}	committed = commitTransaction();	return result;	} catch (Exception ex) {	
error retrieving statistics via jdo 

private List<MPartitionColumnStatistics> getMPartitionColumnStatistics( Table table, List<String> partNames, List<String> colNames, QueryWrapper queryWrapper) throws NoSuchObjectException, MetaException {	boolean committed = false;	try {	openTransaction();	try {	validateTableCols(table, colNames);	} catch (MetaException me) {	
the table does not have the same column definition as its partition 

}	filter += ")";	query.setFilter(filter);	query.declareParameters(paramStr);	query.setOrdering("partitionName ascending");	List<MPartitionColumnStatistics> result = (List<MPartitionColumnStatistics>) query.executeWithArray(params);	pm.retrieveAll(result);	committed = commitTransaction();	return result;	} catch (Exception ex) {	
error retrieving statistics via jdo 

public long cleanupEvents() {	boolean commited = false;	Query query = null;	long delCnt;	
begin executing cleanupevents 

Long expiryTime = MetastoreConf.getTimeVar(getConf(), ConfVars.EVENT_EXPIRY_DURATION, TimeUnit.MILLISECONDS);	Long curTime = System.currentTimeMillis();	try {	openTransaction();	query = pm.newQuery(MPartitionEvent.class, "curTime - eventTime > expiryTime");	query.declareParameters("java.lang.Long curTime, java.lang.Long expiryTime");	delCnt = query.deletePersistentAll(curTime, expiryTime);	commited = commitTransaction();	} finally {	rollbackAndCleanup(commited, query);	
done executing cleanupevents 

public boolean addToken(String tokenId, String delegationToken) {	
begin executing addtoken 

token = getTokenFrom(tokenId);	if (token == null) {	pm.makePersistent(new MDelegationToken(tokenId, delegationToken));	}	committed = commitTransaction();	} finally {	if(!committed) {	rollbackTransaction();	}	}	
done executing addtoken with status 

public boolean removeToken(String tokenId) {	
begin executing removetoken 

token = getTokenFrom(tokenId);	if (null != token) {	pm.deletePersistent(token);	}	committed = commitTransaction();	} finally {	if(!committed) {	rollbackTransaction();	}	}	
done executing removetoken with status 

public String getToken(String tokenId) {	
begin executing gettoken 

token = getTokenFrom(tokenId);	if (null != token) {	pm.retrieve(token);	}	committed = commitTransaction();	} finally {	if(!committed) {	rollbackTransaction();	}	}	
done executing gettoken with status 

public List<String> getAllTokenIdentifiers() {	
begin executing getalltokenidentifiers 

openTransaction();	query = pm.newQuery(MDelegationToken.class);	List<MDelegationToken> tokens = (List<MDelegationToken>) query.execute();	pm.retrieveAll(tokens);	committed = commitTransaction();	for (MDelegationToken token : tokens) {	tokenIdents.add(token.getTokenIdentifier());	}	return tokenIdents;	} finally {	
done executing getalltokenidentifers with status 

public int addMasterKey(String key) throws MetaException{	
begin executing addmasterkey 

MMasterKey masterKey = new MMasterKey(key);	try{	openTransaction();	pm.makePersistent(masterKey);	committed = commitTransaction();	} finally {	if(!committed) {	rollbackTransaction();	}	}	
done executing addmasterkey with status 

public void updateMasterKey(Integer id, String key) throws NoSuchObjectException, MetaException {	
begin executing updatemasterkey 

query.declareParameters("java.lang.Integer id");	query.setUnique(true);	masterKey = (MMasterKey) query.execute(id);	if (null != masterKey) {	masterKey.setMasterKey(key);	}	committed = commitTransaction();	} finally {	rollbackAndCleanup(committed, query);	}	
done executing updatemasterkey with status 

public boolean removeMasterKey(Integer id) {	
begin executing removemasterkey 

query.declareParameters("java.lang.Integer id");	query.setUnique(true);	masterKey = (MMasterKey) query.execute(id);	if (null != masterKey) {	pm.deletePersistent(masterKey);	}	success = commitTransaction();	} finally {	rollbackAndCleanup(success, query);	}	
done executing removemasterkey with status 

public String[] getMasterKeys() {	
begin executing getmasterkeys 

query = pm.newQuery(MMasterKey.class);	keys = (List<MMasterKey>) query.execute();	pm.retrieveAll(keys);	committed = commitTransaction();	String[] masterKeys = new String[keys.size()];	for (int i = 0; i < keys.size(); i++) {	masterKeys[i] = keys.get(i).getMasterKey();	}	return masterKeys;	} finally {	
done executing getmasterkeys with status 

return;	}	boolean strictValidation = MetastoreConf.getBoolVar(getConf(), ConfVars.SCHEMA_VERIFICATION);	String dbSchemaVer = getMetaStoreSchemaVersion();	IMetaStoreSchemaInfo metastoreSchemaInfo = MetaStoreSchemaInfoFactory.get(getConf());	String hiveSchemaVer = metastoreSchemaInfo.getHiveSchemaVersion();	if (dbSchemaVer == null) {	if (strictValidation) {	throw new MetaException("Version information not found in metastore.");	} else {	
version information not found in metastore is not enabled so recording the schema version 

IMetaStoreSchemaInfo metastoreSchemaInfo = MetaStoreSchemaInfoFactory.get(getConf());	String hiveSchemaVer = metastoreSchemaInfo.getHiveSchemaVersion();	if (dbSchemaVer == null) {	if (strictValidation) {	throw new MetaException("Version information not found in metastore.");	} else {	setMetaStoreSchemaVersion(hiveSchemaVer, "Set by MetaStore " + USER + "@" + HOSTNAME);	}	} else {	if (metastoreSchemaInfo.isVersionCompatible(hiveSchemaVer, dbSchemaVer)) {	
found expected hms version of 

throw new MetaException("Version information not found in metastore.");	} else {	setMetaStoreSchemaVersion(hiveSchemaVer, "Set by MetaStore " + USER + "@" + HOSTNAME);	}	} else {	if (metastoreSchemaInfo.isVersionCompatible(hiveSchemaVer, dbSchemaVer)) {	} else {	if (strictValidation) {	throw new MetaException("Hive Schema version " + hiveSchemaVer + " does not match metastore's schema version " + dbSchemaVer + " Metastore is not upgraded or corrupt");	} else {	
version information found in metastore differs from expected schema version schema verification is disabled 

public void setMetaStoreSchemaVersion(String schemaVersion, String comment) throws MetaException {	MVersionTable mSchemaVer;	boolean commited = false;	boolean recordVersion = MetastoreConf.getBoolVar(getConf(), ConfVars.SCHEMA_VERIFICATION_RECORD_VERSION);	if (!recordVersion) {	LOG.warn("setMetaStoreSchemaVersion called but recording version is disabled: " + "version = {}, comment = {}", schemaVersion, comment);	return;	}	
setting metastore schema version in db to 

private MFunction convertToMFunction(Function func) throws InvalidObjectException {	if (func == null) {	return null;	}	MDatabase mdb = null;	try {	mdb = getMDatabase(func.getDbName());	} catch (NoSuchObjectException e) {	
database does not exist 

public void run() throws MetaException {	while (true) {	try {	command.process();	break;	} catch (Exception e) {	
attempting to acquire the db log notification lock out of retries 

needToPersistId = false;	}	entry.setEventId(mNotificationNextId.getNextEventId());	mNotificationNextId.incrementEventId();	if (needToPersistId) {	pm.makePersistent(mNotificationNextId);	}	pm.makePersistent(translateThriftToDb(entry));	commited = commitTransaction();	} catch (Exception e) {	
couldnot get lock for update 

clearClr(clr);	}	}	Field classLoaderResolverMap = AbstractNucleusContext.class.getDeclaredField( "classLoaderResolverMap");	classLoaderResolverMap.setAccessible(true);	Map<String,ClassLoaderResolver> loaderMap = (Map<String, ClassLoaderResolver>) classLoaderResolverMap.get(nc);	for (ClassLoaderResolver clr : loaderMap.values()){	clearClr(clr);	}	classLoaderResolverMap.set(nc, new HashMap<String, ClassLoaderResolver>());	
removed cached classloaders from datanucleus nucleuscontext 

}	}	Field classLoaderResolverMap = AbstractNucleusContext.class.getDeclaredField( "classLoaderResolverMap");	classLoaderResolverMap.setAccessible(true);	Map<String,ClassLoaderResolver> loaderMap = (Map<String, ClassLoaderResolver>) classLoaderResolverMap.get(nc);	for (ClassLoaderResolver clr : loaderMap.values()){	clearClr(clr);	}	classLoaderResolverMap.set(nc, new HashMap<String, ClassLoaderResolver>());	} catch (Exception e) {	
failed to remove cached classloaders from datanucleus nucleuscontext 

private static void clearClr(ClassLoaderResolver clr) throws Exception {	if (clr != null){	if (clr instanceof ClassLoaderResolverImpl){	ClassLoaderResolverImpl clri = (ClassLoaderResolverImpl) clr;	long resourcesCleared = clearFieldMap(clri,"resources");	long loadedClassesCleared = clearFieldMap(clri,"loadedClasses");	long unloadedClassesCleared = clearFieldMap(clri, "unloadedClasses");	
cleared classloaderresolverimpl 

========================= hive sample_1898 =========================

testBatches.add(batch);	if(batch.isParallel()) {	parallelWorkQueue.add(batch);	} else {	isolatedWorkQueue.add(batch);	}	}	logger.info("ParallelWorkQueueSize={}, IsolatedWorkQueueSize={}", parallelWorkQueue.size(), isolatedWorkQueue.size());	if (logger.isDebugEnabled()) {	for (TestBatch testBatch : parallelWorkQueue) {	
pbatch 

parallelWorkQueue.add(batch);	} else {	isolatedWorkQueue.add(batch);	}	}	logger.info("ParallelWorkQueueSize={}, IsolatedWorkQueueSize={}", parallelWorkQueue.size(), isolatedWorkQueue.size());	if (logger.isDebugEnabled()) {	for (TestBatch testBatch : parallelWorkQueue) {	}	for (TestBatch testBatch : isolatedWorkQueue) {	
ibatch 

messageBuilder.append(Joiner.on(",").join(tests));	messageBuilder.append("]");	}	}	failedTests.add(messageBuilder.toString());	}	}	}	} finally {	long elapsed = System.currentTimeMillis() - start;	
perf exec phase minutes 

private void replaceBadHosts(int expectedNumHosts) throws Exception {	Set<Host> goodHosts = Sets.newHashSet();	for(HostExecutor hostExecutor : ImmutableList.copyOf(hostExecutors)) {	if(hostExecutor.isBad()) {	
removing host during execution phase 

hostExecutors.remove(hostExecutor);	} else {	goodHosts.add(hostExecutor.getHost());	}	}	long start = System.currentTimeMillis();	while(hostExecutors.size() < expectedNumHosts) {	if(System.currentTimeMillis() - start > FOUR_HOURS) {	throw new RuntimeException("Waited over fours for hosts, still have only " + hostExecutors.size() + " hosts out of an expected " + expectedNumHosts);	}	
only hosts out of an expected attempting to replace bad hosts 

}	TimeUnit.MINUTES.sleep(1);	executionContext.replaceBadHosts();	for(Host host : executionContext.getHosts()) {	if(!goodHosts.contains(host)) {	HostExecutor hostExecutor = hostExecutorBuilder.build(host);	initalizeHost(hostExecutor);	if(hostExecutor.isBad()) {	executionContext.addBadHost(hostExecutor.getHost());	} else {	
adding new host during execution phase 

========================= hive sample_5588 =========================

String colType = null;	String colName = null;	boolean doAllPartitionContainStats = partNames.size() == colStatsWithSourceInfo.size();	NumDistinctValueEstimator ndvEstimator = null;	for (ColStatsObjWithSourceInfo csp : colStatsWithSourceInfo) {	ColumnStatisticsObj cso = csp.getColStatsObj();	if (statsObj == null) {	colName = cso.getColName();	colType = cso.getColType();	statsObj = ColumnStatsAggregatorFactory.newColumnStaticsObj(colName, colType, cso.getStatsData().getSetField());	
doallpartitioncontainstats for column is 

} else if (estimation > higherBound) {	estimation = higherBound;	}	} else {	estimation = (long) (lowerBound + (higherBound - lowerBound) * ndvTuner);	}	aggregateData.setNumDVs(estimation);	}	columnStatisticsData.setDecimalStats(aggregateData);	} else {	
start extrapolation for 

ColumnStatisticsData csd = new ColumnStatisticsData();	csd.setDecimalStats(aggregateData);	adjustedStatsMap.put(pseudoPartName.toString(), csd);	if (useDensityFunctionForNDVEstimation) {	densityAvgSum += (MetaStoreUtils.decimalToDouble(aggregateData.getHighValue()) - MetaStoreUtils .decimalToDouble(aggregateData.getLowValue())) / aggregateData.getNumDVs();	}	}	}	extrapolate(columnStatisticsData, partNames.size(), colStatsWithSourceInfo.size(), adjustedIndexMap, adjustedStatsMap, densityAvgSum / adjustedStatsMap.size());	}	
ndv estimatation for is of partitions requested of partitions found 

========================= hive sample_1932 =========================

boolean combinable;	if (left instanceof HiveJoin) {	HiveJoin hj = (HiveJoin) left;	leftCondition = hj.getCondition();	leftJoinInputs = ImmutableList.of(Pair.of(0, 1));	leftJoinTypes = ImmutableList.of(hj.getJoinType());	leftJoinFilters = ImmutableList.of(hj.getJoinFilter());	try {	combinable = isCombinableJoin(join, hj);	} catch (CalciteSemanticException e) {	
failed to merge join join 

}	} else {	HiveMultiJoin hmj = (HiveMultiJoin) left;	leftCondition = hmj.getCondition();	leftJoinInputs = hmj.getJoinInputs();	leftJoinTypes = hmj.getJoinTypes();	leftJoinFilters = hmj.getJoinFilters();	try {	combinable = isCombinableJoin(join, hmj);	} catch (CalciteSemanticException e) {	
failed to merge join multijoin 

final List<RelDataTypeField> systemFieldList = ImmutableList.of();	List<List<RexNode>> joinKeyExprs = new ArrayList<List<RexNode>>();	List<Integer> filterNulls = new ArrayList<Integer>();	for (int i=0; i<newInputs.size(); i++) {	joinKeyExprs.add(new ArrayList<RexNode>());	}	RexNode filters;	try {	filters = HiveRelOptUtil.splitHiveJoinCondition(systemFieldList, newInputs, join.getCondition(), joinKeyExprs, filterNulls, null);	} catch (CalciteSemanticException e) {	
failed to merge joins 

========================= hive sample_2867 =========================

}	}	Iterator<Map.Entry<Long, LlapDataBuffer>> matches = cache.subMap( absOffset, currentNotCached.getEnd() + baseOffset) .entrySet().iterator();	long cacheEnd = -1;	while (matches.hasNext()) {	assert currentNotCached != null;	Map.Entry<Long, LlapDataBuffer> e = matches.next();	LlapDataBuffer buffer = e.getValue();	long requestedLength = currentNotCached.getLength();	if (LlapIoImpl.LOCKING_LOGGER.isTraceEnabled()) {	
locking during get 

public long[] putFileData(Object fileKey, DiskRange[] ranges, MemoryBuffer[] buffers, long baseOffset, Priority priority, LowLevelCacheCounters qfCounters) {	long[] result = null;	assert buffers.length == ranges.length;	FileCache<ConcurrentSkipListMap<Long, LlapDataBuffer>> subCache = FileCache.getOrAddFileSubCache(cache, fileKey, CACHE_CTOR);	try {	for (int i = 0; i < ranges.length; ++i) {	LlapDataBuffer buffer = (LlapDataBuffer)buffers[i];	if (LlapIoImpl.LOCKING_LOGGER.isTraceEnabled()) {	
locking at put time 

while (true) {	LlapDataBuffer oldVal = subCache.getCache().putIfAbsent(offset, buffer);	if (oldVal == null) {	cachePolicy.cache(buffer, priority);	if (qfCounters != null) {	qfCounters.recordAllocBytes(buffer.byteBuffer.remaining(), buffer.allocSize);	}	break;	}	if (LlapIoImpl.CACHE_LOGGER.isTraceEnabled()) {	
trying to cache when the chunk is already cached for base old new 

if (oldVal == null) {	cachePolicy.cache(buffer, priority);	if (qfCounters != null) {	qfCounters.recordAllocBytes(buffer.byteBuffer.remaining(), buffer.allocSize);	}	break;	}	if (LlapIoImpl.CACHE_LOGGER.isTraceEnabled()) {	}	if (LlapIoImpl.LOCKING_LOGGER.isTraceEnabled()) {	
locking due to cache collision 

}	if (LlapIoImpl.CACHE_LOGGER.isTraceEnabled()) {	}	if (LlapIoImpl.LOCKING_LOGGER.isTraceEnabled()) {	}	if (lockBuffer(oldVal, true)) {	if (oldVal.declaredCachedLength != buffer.declaredCachedLength) {	throw new RuntimeException("Found a block with different length at the same offset: " + oldVal.declaredCachedLength + " vs " + buffer.declaredCachedLength + " @" + offset + " (base " + baseOffset + ")");	}	if (LlapIoImpl.LOCKING_LOGGER.isTraceEnabled()) {	
unlocking due to cache collision with 

private void unlockBuffer(LlapDataBuffer buffer, boolean handleLastDecRef) {	boolean isLastDecref = (buffer.decRef() == 0);	if (handleLastDecRef && isLastDecref) {	if (buffer.declaredCachedLength != LlapDataBuffer.UNKNOWN_CACHED_LENGTH) {	cachePolicy.notifyUnlock(buffer);	} else {	if (LlapIoImpl.CACHE_LOGGER.isTraceEnabled()) {	
deallocating that was not cached 

========================= hive sample_2164 =========================

public static byte[] decodeIfNeeded(byte[] recv) {	boolean arrayByteBase64 = Base64.isArrayByteBase64(recv);	if (LOG.isDebugEnabled() && arrayByteBase64) {	
data only contains alphabets only so try to decode the data 

========================= hive sample_5475 =========================

try {	FileSystem fs = getDataLocation().getFileSystem(SessionState.getSessionConf());	String pathPattern = getDataLocation().toString();	if (getBucketCount() > 0) {	pathPattern = pathPattern + "/*";	}	LOG.info("Path pattern = " + pathPattern);	FileStatus srcs[] = fs.globStatus(new Path(pathPattern), FileUtils.HIDDEN_FILES_PATH_FILTER);	Arrays.sort(srcs);	for (FileStatus src : srcs) {	
got file 

private List<FieldSchema> getColsInternal(boolean forMs) {	try {	String serializationLib = tPartition.getSd().getSerdeInfo().getSerializationLib();	if (Table.hasMetastoreBasedSchema(SessionState.getSessionConf(), serializationLib)) {	return tPartition.getSd().getCols();	} else if (forMs && !Table.shouldStoreFieldsInMetastore( SessionState.getSessionConf(), serializationLib, table.getParameters())) {	return Hive.getFieldsFromDeserializerForMsStorage(table, getDeserializer());	}	return HiveMetaStoreUtils.getFieldsFromDeserializer(table.getTableName(), getDeserializer());	} catch (Exception e) {	
unable to get cols from serde 

public List<String> getSkewedColNames() {	
sd is 

========================= hive sample_5025 =========================

private static Rpc createRpc(RpcConfiguration config, SaslHandler saslHandler, SocketChannel client, EventExecutorGroup egroup) throws IOException {	LogLevel logLevel = LogLevel.TRACE;	if (config.getRpcChannelLogLevel() != null) {	try {	logLevel = LogLevel.valueOf(config.getRpcChannelLogLevel());	} catch (Exception e) {	
invalid log level reverting to default 

public <T> Future<T> call(final Object msg, Class<T> retType) {	Preconditions.checkArgument(msg != null);	Preconditions.checkState(channel.isActive(), "RPC channel is closed.");	try {	final long id = rpcId.getAndIncrement();	final Promise<T> promise = createPromise();	final ChannelFutureListener listener = new ChannelFutureListener() {	public void operationComplete(ChannelFuture cf) {	if (!cf.isSuccess() && !promise.isDone()) {	
failed to send rpc closing connection 

try {	channel.close().sync();	} catch (InterruptedException ie) {	Thread.interrupted();	} finally {	synchronized (listeners) {	for (Listener l : listeners) {	try {	l.rpcClosed(this);	} catch (Exception e) {	
error caught in rpc listener invocation 

========================= hive sample_551 =========================

return;	}	setResponseHeader(response);	String dataJson = request.getReader().lines().collect(Collectors.joining());	ObjectMapper objectMapper = new ObjectMapper();	try {	ConfLoggers confLoggers = objectMapper.readValue(dataJson, ConfLoggers.class);	configureLogger(confLoggers);	} catch (IOException e) {	response.setStatus(HttpServletResponse.SC_BAD_REQUEST);	
error configuring via conflog endpoint 

========================= hive sample_1436 =========================

public abstract RelOptCost getDefaultCost();	public abstract RelOptCost getAggregateCost(HiveAggregate aggregate);	public abstract RelOptCost getScanCost(HiveTableScan ts, RelMetadataQuery mq);	public RelOptCost getJoinCost(HiveJoin join) {	JoinAlgorithm joinAlgorithm = null;	RelOptCost minJoinCost = null;	if (LOG.isTraceEnabled()) {	
join algorithm selection for 

JoinAlgorithm joinAlgorithm = null;	RelOptCost minJoinCost = null;	if (LOG.isTraceEnabled()) {	}	for (JoinAlgorithm possibleAlgorithm : this.joinAlgorithms) {	if (!possibleAlgorithm.isExecutable(join)) {	continue;	}	RelOptCost joinCost = possibleAlgorithm.getCost(join);	if (LOG.isTraceEnabled()) {	
cost 

}	RelOptCost joinCost = possibleAlgorithm.getCost(join);	if (LOG.isTraceEnabled()) {	}	if (minJoinCost == null || joinCost.isLt(minJoinCost) ) {	joinAlgorithm = possibleAlgorithm;	minJoinCost = joinCost;	}	}	if (LOG.isTraceEnabled()) {	
selected 

========================= hive sample_2830 =========================

public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx, Object... nodeOutputs) throws SemanticException {	FilterOperator filterOp = (FilterOperator) nd;	ExprNodeDesc predicate = filterOp.getConf().getPredicate();	ExprNodeDesc newPredicate = generateInClause(predicate);	if (newPredicate != null) {	if (LOG.isDebugEnabled()) {	
generated new predicate with in clause 

========================= hive sample_2975 =========================

public void open() {	if (!isOpen) {	isOpen = true;	this.timerContext = timer.time();	CodahaleMetrics.this.incrementCounter(MetricsConstant.ACTIVE_CALLS + name);	} else {	
scope named is not closed cannot be opened 

public void close() {	if (isOpen) {	timerContext.close();	CodahaleMetrics.this.decrementCounter(MetricsConstant.ACTIVE_CALLS + name);	} else {	
scope named is not open cannot be closed 

private void addGaugeInternal(String name, Gauge gauge) {	try {	gaugesLock.lock();	gauges.put(name, gauge);	if (metricRegistry.getGauges().containsKey(name)) {	
a gauge with name already exists the old gauge will be overwritten but this is not recommended 

private void initReporting() {	if (!(initCodahaleMetricsReporterClasses() || initMetricsReporter())) {	
unable to initialize metrics reporting 

private void initReporting() {	if (!(initCodahaleMetricsReporterClasses() || initMetricsReporter())) {	}	if (reporters.isEmpty()) {	
no reporters configured for codahale metrics 

private boolean initCodahaleMetricsReporterClasses() {	List<String> reporterClasses = Lists.newArrayList(Splitter.on(",").trimResults(). omitEmptyStrings().split(conf.getVar(HiveConf.ConfVars.HIVE_CODAHALE_METRICS_REPORTER_CLASSES)));	if (reporterClasses.isEmpty()) {	return false;	}	for (String reporterClass : reporterClasses) {	Class name = null;	try {	name = conf.getClassByName(reporterClass);	} catch (ClassNotFoundException e) {	
unable to instantiate metrics reporter class from conf hive codahale metrics reporter classes 

name = conf.getClassByName(reporterClass);	} catch (ClassNotFoundException e) {	throw new IllegalArgumentException(e);	}	try {	Constructor constructor = name.getConstructor(MetricRegistry.class, HiveConf.class);	CodahaleReporter reporter = (CodahaleReporter) constructor.newInstance(metricRegistry, conf);	reporter.start();	reporters.add(reporter);	} catch (NoSuchMethodException | InstantiationException | IllegalAccessException | InvocationTargetException e) {	
unable to instantiate using constructor metricregistry hiveconf for reporter from conf hive codahale metrics reporter classes 

private boolean initMetricsReporter() {	List<String> metricsReporterNames = Lists.newArrayList(Splitter.on(",").trimResults(). omitEmptyStrings().split(conf.getVar(HiveConf.ConfVars.HIVE_METRICS_REPORTER)));	if (metricsReporterNames.isEmpty()) {	return false;	}	MetricsReporting reporter = null;	for (String metricsReportingName : metricsReporterNames) {	try {	reporter = MetricsReporting.valueOf(metricsReportingName.trim().toUpperCase());	} catch (IllegalArgumentException e) {	
invalid reporter name 

CodahaleReporter codahaleReporter = null;	switch (reporter) {	case CONSOLE: codahaleReporter = new ConsoleMetricsReporter(metricRegistry, conf);	break;	case JMX: codahaleReporter = new JmxMetricsReporter(metricRegistry, conf);	break;	case JSON_FILE: codahaleReporter = new JsonFileMetricsReporter(metricRegistry, conf);	break;	case HADOOP2: codahaleReporter = new Metrics2Reporter(metricRegistry, conf);	break;	
unhandled reporter provided 

========================= hive sample_1338 =========================

public void generateIndexQuery(List<Index> indexes, ExprNodeDesc predicate, ParseContext pctx, HiveIndexQueryContext queryContext) {	Map<Index, ExprNodeDesc> indexPredicates  = decomposePredicate( predicate, indexes, queryContext);	if (indexPredicates == null) {	
no decomposed predicate found 

String tmpFile = pctx.getContext().getMRTmpPath().toUri().toString();	qlCommand.append( "\"" + tmpFile + "\" ");	qlCommand.append("SELECT bucketname AS `_bucketname` , COLLECT_SET(offset) AS `_offsets` FROM ");	qlCommand.append("(SELECT `_bucketname` AS bucketname , `_offset` AS offset FROM ");	BitmapQuery head = iqs.get(0);	for ( i = 1; i < iqs.size(); i++) {	head = new BitmapOuterQuery("oind"+i, head, iqs.get(i));	}	qlCommand.append(head.toString());	qlCommand.append(" WHERE NOT EWAH_BITMAP_EMPTY(" + head.getAlias() + ".`_bitmaps`) ) tmp_index GROUP BY bucketname");	
generating tasks for re entrant ql query 

========================= hive sample_4643 =========================

public void close(Reporter reporter) throws IOException {	
closing the record writer output stream for 

========================= hive sample_2797 =========================

private <T extends Serializable> JobHandle.Listener<T> newListener() {	JobHandle.Listener<T> listener = mock(JobHandle.Listener.class);	answerWhen(listener, "cancelled").onJobCancelled(Mockito.<JobHandle<T>>any());	answerWhen(listener, "queued").onJobQueued(Mockito.<JobHandle<T>>any());	answerWhen(listener, "started").onJobStarted(Mockito.<JobHandle<T>>any());	answerWhen(listener, "succeeded").onJobSucceeded( Mockito.<JobHandle<T>>any(), Mockito.<T>any());	answerWhen(listener, "job started").onSparkJobStarted( Mockito.<JobHandle<T>>any(), Mockito.anyInt());	Mockito.doAnswer(new Answer<Void>() {	public Void answer(InvocationOnMock invocation) throws Throwable {	JobHandleImpl arg = ((JobHandleImpl)invocation.getArguments()[0]);	
job failed 

protected <T extends Serializable> Listener<T> answerWhen( Listener<T> listener, final String logStr) {	return Mockito.doAnswer(new Answer<Void>() {	public Void answer(InvocationOnMock invocation) throws Throwable {	JobHandleImpl arg = ((JobHandleImpl)invocation.getArguments()[0]);	
job 

========================= hive sample_526 =========================

public ColumnMapping getColumnMappingForHiveColumn(List<String> hiveColumns, String hiveColumnName) {	Preconditions.checkNotNull(hiveColumns);	Preconditions.checkNotNull(hiveColumnName);	Preconditions.checkArgument(columnMappings.size() <= hiveColumns.size(), "Expected equal number of column mappings and Hive columns, " + columnMappings + ", " + hiveColumns);	int hiveColumnOffset = 0;	for (; hiveColumnOffset < hiveColumns.size() && hiveColumnOffset < columnMappings.size(); hiveColumnOffset++) {	if (hiveColumns.get(hiveColumnOffset).equals(hiveColumnName)) {	return columnMappings.get(hiveColumnOffset);	}	}	
could not find offset for hive column with name with columns 

========================= hive sample_199 =========================

private void cleanRawStore() {	Long threadId = this.getId();	RawStore threadLocalRawStore = threadRawStoreMap.get(threadId);	if (threadLocalRawStore != null) {	
rawstore for the thread will be closed now 

public void cacheThreadLocalRawStore() {	Long threadId = this.getId();	RawStore threadLocalRawStore = HiveMetaStore.HMSHandler.getRawStore();	if (threadLocalRawStore != null && !threadRawStoreMap.containsKey(threadId)) {	
adding rawstore for the thread to threadrawstoremap for future cleanup 

========================= hive sample_2328 =========================

for (String propName : System.getProperties().stringPropertyNames()) {	for (String allowedPrefix : allowedPrefixes) {	if (propName.startsWith(allowedPrefix)) {	command.add( String.format( "-D%s=%s", propName, System.getProperty(propName) ) );	}	}	}	this.properties .forEach((key, value) -> command.add(String.format("-D%s=%s", key, value)));	command.addAll(Lists.newArrayList("io.druid.cli.Main", "server", getNodeType()));	processBuilder.command(command);	
creating forking druid node with 

public void start() throws IOException {	synchronized (started) {	if (started == false) {	druidProcess = processBuilder.start();	started = true;	}	
started 

public void close() throws IOException {	synchronized (started) {	if (druidProcess != null && druidProcess.isAlive()) {	druidProcess.destroy();	}	try {	
waiting for 

public void close() throws IOException {	synchronized (started) {	if (druidProcess != null && druidProcess.isAlive()) {	druidProcess.destroy();	}	try {	if (druidProcess.waitFor(5000, TimeUnit.MILLISECONDS)) {	
shutdown completed for node s 

public void close() throws IOException {	synchronized (started) {	if (druidProcess != null && druidProcess.isAlive()) {	druidProcess.destroy();	}	try {	if (druidProcess.waitFor(5000, TimeUnit.MILLISECONDS)) {	} else {	
waiting to shutdown node s exhausted shutting down forcibly 

========================= hive sample_463 =========================

public void run() {	TxnStore.MutexAPI.LockHandle handle = null;	try {	handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.CompactionHistory.name());	long startTime = System.currentTimeMillis();	txnHandler.purgeCompactionHistory();	
history reaper reaper ran for seconds 

public void run() {	TxnStore.MutexAPI.LockHandle handle = null;	try {	handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.CompactionHistory.name());	long startTime = System.currentTimeMillis();	txnHandler.purgeCompactionHistory();	} catch(Throwable t) {	
serious error in 

========================= hive sample_1857 =========================

public LlapBaseInputFormat() {}	public RecordReader<NullWritable, V> getRecordReader(InputSplit split, JobConf job, Reporter reporter) throws IOException {	LlapInputSplit llapSplit = (LlapInputSplit) split;	HiveConf.setVar(job, HiveConf.ConfVars.LLAP_ZK_REGISTRY_USER, llapSplit.getLlapUser());	SubmitWorkInfo submitWorkInfo = SubmitWorkInfo.fromBytes(llapSplit.getPlanBytes());	LlapServiceInstance serviceInstance = getServiceInstance(job, llapSplit);	String host = serviceInstance.getHost();	int llapSubmitPort = serviceInstance.getRpcPort();	
found service instance for host with rpc port and outputformat port 

llapToken = new Token<LlapTokenIdentifier>();	llapToken.readFields(in);	}	LlapRecordReaderTaskUmbilicalExternalResponder umbilicalResponder = new LlapRecordReaderTaskUmbilicalExternalResponder();	LlapTaskUmbilicalExternalClient llapClient = new LlapTaskUmbilicalExternalClient(job, submitWorkInfo.getTokenIdentifier(), submitWorkInfo.getToken(), umbilicalResponder, llapToken);	int attemptNum = 0;	TaskAttemptID taskAttemptId = TaskAttemptID.forName(job.get(MRJobConfig.TASK_ATTEMPT_ID));	if (taskAttemptId != null) {	attemptNum = taskAttemptId.getId();	if (LOG.isDebugEnabled()) {	
setting attempt number to from task attempt id in conf 

int attemptNum = 0;	TaskAttemptID taskAttemptId = TaskAttemptID.forName(job.get(MRJobConfig.TASK_ATTEMPT_ID));	if (taskAttemptId != null) {	attemptNum = taskAttemptId.getId();	if (LOG.isDebugEnabled()) {	}	}	SubmitWorkRequestProto request = constructSubmitWorkRequestProto( submitWorkInfo, llapSplit.getSplitNum(), attemptNum, llapClient.getAddress(), submitWorkInfo.getToken(), llapSplit.getFragmentBytes(), llapSplit.getFragmentBytesSignature(), job);	llapClient.submitWork(request, host, llapSubmitPort);	Socket socket = new Socket(host, serviceInstance.getOutputFormatPort());	
socket connected 

Socket socket = new Socket(host, serviceInstance.getOutputFormatPort());	SignableVertexSpec vertex = SignableVertexSpec.parseFrom(submitWorkInfo.getVertexBinary());	String fragmentId = Converters.createTaskAttemptId(vertex.getQueryIdentifier(), vertex.getVertexIndex(), request.getFragmentNumber(), request.getAttemptNumber()).toString();	OutputStream socketStream = socket.getOutputStream();	LlapOutputSocketInitMessage.Builder builder = LlapOutputSocketInitMessage.newBuilder().setFragmentId(fragmentId);	if (llapSplit.getTokenBytes() != null) {	builder.setToken(ByteString.copyFrom(llapSplit.getTokenBytes()));	}	builder.build().writeDelimitedTo(socketStream);	socketStream.flush();	
registered id 

if (query == null) query = job.get(QUERY_KEY);	if (user == null) user = job.get(USER_KEY);	if (pwd == null) pwd = job.get(PWD_KEY);	String database = job.get(DB_KEY);	if (url == null || query == null) {	throw new IllegalStateException();	}	String handleId = job.get(HANDLE_ID);	if (handleId == null) {	handleId = UUID.randomUUID().toString();	
handle id not specified generated handle id 

}	ResultSet res = stmt.executeQuery(sql);	while (res.next()) {	DataInput in = new DataInputStream(res.getBinaryStream(1));	InputSplitWithLocationInfo is = new LlapInputSplit();	is.readFields(in);	ins.add(is);	}	res.close();	} catch (Exception e) {	
closing connection due to error 

public static void close(String handleId) throws IOException {	List<Connection> handleConnections;	synchronized (lock) {	handleConnections = connectionMap.remove(handleId);	}	if (handleConnections != null) {	
closing connections for handle id 

public static void close(String handleId) throws IOException {	List<Connection> handleConnections;	synchronized (lock) {	handleConnections = connectionMap.remove(handleId);	}	if (handleConnections != null) {	for (Connection conn : handleConnections) {	try {	conn.close();	} catch (Exception err) {	
error while closing connection for 

handleConnections = connectionMap.remove(handleId);	}	if (handleConnections != null) {	for (Connection conn : handleConnections) {	try {	conn.close();	} catch (Exception err) {	}	}	} else {	
no connection found for handle id 

public static void closeAll() {	
closing all handles 

public static void closeAll() {	for (String handleId : connectionMap.keySet()) {	try {	close(handleId);	} catch (Exception err) {	
error closing handle id 

private LlapServiceInstance getServiceInstance(JobConf job, LlapInputSplit llapSplit) throws IOException {	LlapRegistryService registryService = LlapRegistryService.getClient(job);	String host = llapSplit.getLocations()[0];	LlapServiceInstance serviceInstance = getServiceInstanceForHost(registryService, host);	if (serviceInstance == null) {	
no service instances found for in registry 

private LlapServiceInstance getServiceInstanceForHost(LlapRegistryService registryService, String host) throws IOException {	InetAddress address = InetAddress.getByName(host);	LlapServiceInstanceSet instanceSet = registryService.getInstances();	LlapServiceInstance serviceInstance = null;	String name = address.getHostName();	
searching service instance by hostname 

private LlapServiceInstance getServiceInstanceForHost(LlapRegistryService registryService, String host) throws IOException {	InetAddress address = InetAddress.getByName(host);	LlapServiceInstanceSet instanceSet = registryService.getInstances();	LlapServiceInstance serviceInstance = null;	String name = address.getHostName();	serviceInstance = selectServiceInstance(instanceSet.getByHost(name));	if (serviceInstance != null) {	return serviceInstance;	}	name = address.getCanonicalHostName();	
searching service instance by canonical hostname 

serviceInstance = selectServiceInstance(instanceSet.getByHost(name));	if (serviceInstance != null) {	return serviceInstance;	}	name = address.getCanonicalHostName();	serviceInstance = selectServiceInstance(instanceSet.getByHost(name));	if (serviceInstance != null) {	return serviceInstance;	}	name = address.getHostAddress();	
searching service instance by address 

private LlapServiceInstance getServiceInstanceRandom(LlapRegistryService registryService) throws IOException {	LlapServiceInstanceSet instanceSet = registryService.getInstances();	LlapServiceInstance serviceInstance = null;	
finding random live service instance 

private LlapServiceInstance selectServiceInstance(Set<LlapServiceInstance> serviceInstances) {	if (serviceInstances == null || serviceInstances.isEmpty()) {	return null;	}	for (LlapServiceInstance serviceInstance : serviceInstances) {	return serviceInstance;	}	
no live service instances were found 

private SubmitWorkRequestProto constructSubmitWorkRequestProto(SubmitWorkInfo submitWorkInfo, int taskNum, int attemptNum, InetSocketAddress address, Token<JobTokenIdentifier> token, byte[] fragmentBytes, byte[] fragmentBytesSignature, JobConf job) throws IOException {	ApplicationId appId = submitWorkInfo.getFakeAppId();	String user = System.getenv(ApplicationConstants.Environment.USER.name());	
setting user in submitworkrequest to 

public void submissionFailed(String fragmentId, Throwable throwable) {	try {	sendOrQueueEvent(ReaderEvent.errorEvent( "Received submission failed event for fragment ID " + fragmentId + ": " + throwable.toString()));	} catch (Exception err) {	
error during heartbeat responder 

for (TezEvent tezEvent : ListUtils.emptyIfNull(inEvents)) {	EventType eventType = tezEvent.getEventType();	try {	switch (eventType) {	case TASK_ATTEMPT_COMPLETED_EVENT: sendOrQueueEvent(ReaderEvent.doneEvent());	break;	case TASK_ATTEMPT_FAILED_EVENT: TaskAttemptFailedEvent taskFailedEvent = (TaskAttemptFailedEvent) tezEvent.getEvent();	sendOrQueueEvent(ReaderEvent.errorEvent(taskFailedEvent.getDiagnostics()));	break;	case TASK_STATUS_UPDATE_EVENT: break;	
unhandled event type 

switch (eventType) {	case TASK_ATTEMPT_COMPLETED_EVENT: sendOrQueueEvent(ReaderEvent.doneEvent());	break;	case TASK_ATTEMPT_FAILED_EVENT: TaskAttemptFailedEvent taskFailedEvent = (TaskAttemptFailedEvent) tezEvent.getEvent();	sendOrQueueEvent(ReaderEvent.errorEvent(taskFailedEvent.getDiagnostics()));	break;	case TASK_STATUS_UPDATE_EVENT: break;	break;	}	} catch (Exception err) {	
error during heartbeat responder 

public void taskKilled(TezTaskAttemptID taskAttemptId) {	try {	sendOrQueueEvent(ReaderEvent.errorEvent( "Received task killed event for task ID " + taskAttemptId));	} catch (Exception err) {	
error during heartbeat responder 

public void heartbeatTimeout(String taskAttemptId) {	try {	sendOrQueueEvent(ReaderEvent.errorEvent( "Timed out waiting for heartbeat for task ID " + taskAttemptId));	} catch (Exception err) {	
error during heartbeat responder 

public synchronized void setRecordReader(LlapBaseRecordReader recordReader) {	this.recordReader = recordReader;	if (recordReader == null) {	return;	}	while (!queuedEvents.isEmpty()) {	ReaderEvent readerEvent = queuedEvents.poll();	
sending queued event to record reader 

protected synchronized void sendOrQueueEvent(ReaderEvent readerEvent) {	LlapBaseRecordReader<?> recordReader = getRecordReader();	if (recordReader != null) {	recordReader.handleEvent(readerEvent);	} else {	if (LOG.isDebugEnabled()) {	
no registered record reader queueing event with message 

========================= hive sample_658 =========================

this.writeBinary(output, value, (PrimitiveObjectInspector) oi);	} else {	this.writeString(output, value, (PrimitiveObjectInspector) oi);	}	return;	case LIST: char separator = (char) serDeParams.getSeparators()[level];	ListObjectInspector loi = (ListObjectInspector) oi;	List<?> list = loi.getList(value);	ObjectInspector eoi = loi.getListElementObjectInspector();	if (list == null) {	
no objects found when serializing list 

}	}	return;	case MAP: char sep = (char) serDeParams.getSeparators()[level];	char keyValueSeparator = (char) serDeParams.getSeparators()[level + 1];	MapObjectInspector moi = (MapObjectInspector) oi;	ObjectInspector koi = moi.getMapKeyObjectInspector();	ObjectInspector voi = moi.getMapValueObjectInspector();	Map<?,?> map = moi.getMap(value);	if (map == null) {	
no object found when serializing map 

output.write(keyValueSeparator);	writeWithLevel(voi, entry.getValue(), output, mapping, level + 2);	}	}	return;	case STRUCT: sep = (char) serDeParams.getSeparators()[level];	StructObjectInspector soi = (StructObjectInspector) oi;	List<? extends StructField> fields = soi.getAllStructFieldRefs();	list = soi.getStructFieldsDataAsList(value);	if (list == null) {	
no object found when serializing struct 

========================= hive sample_208 =========================

private String downloadResource(URI srcUri, String subDir, boolean convertToUnix) throws IOException, URISyntaxException {	
converting to local 

========================= hive sample_2810 =========================

}	List<?> uriList = Arrays.asList(metastoreUris);	Collections.shuffle(uriList);	metastoreUris = (URI[]) uriList.toArray();	} catch (IllegalArgumentException e) {	throw (e);	} catch (Exception e) {	MetaStoreUtils.logAndThrowMetaException(e);	}	} else {	
not getting uris from conf 

}	} else {	throw new MetaException("MetaStoreURIs not found in conf file");	}	String HADOOP_PROXY_USER = "HADOOP_PROXY_USER";	String proxyUser = System.getenv(HADOOP_PROXY_USER);	if (proxyUser == null) {	proxyUser = System.getProperty(HADOOP_PROXY_USER);	}	if(proxyUser != null) {	
is set using delegation token for hivemetastore connection 

open();	return null;	}	});	String delegationTokenPropString = "DelegationTokenForHiveMetaStoreServer";	String delegationTokenStr = getDelegationToken(proxyUser, proxyUser);	SecurityUtils.setTokenStr(UserGroupInformation.getCurrentUser(), delegationTokenStr, delegationTokenPropString);	MetastoreConf.setVar(this.conf, ConfVars.TOKEN_SIGNATURE, delegationTokenPropString);	close();	} catch (Exception e) {	
error while setting delegation token for 

public boolean isCompatibleWith(Configuration conf) {	Map<String, String> currentMetaVarsCopy = currentMetaVars;	if (currentMetaVarsCopy == null) {	return false;	}	boolean compatible = true;	for (ConfVars oneVar : MetastoreConf.metaVars) {	String oldVar = currentMetaVarsCopy.get(oneVar.getVarname());	String newVar = MetastoreConf.getAsString(conf, oneVar);	if (oldVar == null || (oneVar.isCaseSensitive() ? !oldVar.equals(newVar) : !oldVar.equalsIgnoreCase(newVar))) {	
mestastore configuration changed from to 

private void open() throws MetaException {	isConnected = false;	TTransportException tte = null;	boolean useSSL = MetastoreConf.getBoolVar(conf, ConfVars.USE_SSL);	boolean useSasl = MetastoreConf.getBoolVar(conf, ConfVars.USE_THRIFT_SASL);	boolean useFramedTransport = MetastoreConf.getBoolVar(conf, ConfVars.USE_THRIFT_FRAMED_TRANSPORT);	boolean useCompactProtocol = MetastoreConf.getBoolVar(conf, ConfVars.USE_THRIFT_COMPACT_PROTOCOL);	int clientSocketTimeout = (int) MetastoreConf.getTimeVar(conf, ConfVars.CLIENT_SOCKET_TIMEOUT, TimeUnit.MILLISECONDS);	for (int attempt = 0; !isConnected && attempt < retries; ++attempt) {	for (URI store : metastoreUris) {	
trying to connect to metastore with uri 

for (URI store : metastoreUris) {	try {	if (useSSL) {	try {	String trustStorePath = MetastoreConf.getVar(conf, ConfVars.SSL_TRUSTSTORE_PATH).trim();	if (trustStorePath.isEmpty()) {	throw new IllegalArgumentException(ConfVars.SSL_TRUSTSTORE_PATH.toString() + " Not configured for SSL connection");	}	String trustStorePassword = MetastoreConf.getPassword(conf, MetastoreConf.ConfVars.SSL_TRUSTSTORE_PASSWORD);	transport = SecurityUtils.getSSLSocket(store.getHost(), store.getPort(), clientSocketTimeout, trustStorePath, trustStorePassword );	
opened an ssl connection to metastore current connections 

}	} else {	transport = new TSocket(store.getHost(), store.getPort(), clientSocketTimeout);	}	if (useSasl) {	try {	HadoopThriftAuthBridge.Client authBridge = HadoopThriftAuthBridge.getBridge().createClient();	String tokenSig = MetastoreConf.getVar(conf, ConfVars.TOKEN_SIGNATURE);	tokenStrForm = SecurityUtils.getTokenStrForm(tokenSig);	if(tokenStrForm != null) {	
hmsc open found delegation token creating digest based thrift connection 

transport = new TSocket(store.getHost(), store.getPort(), clientSocketTimeout);	}	if (useSasl) {	try {	HadoopThriftAuthBridge.Client authBridge = HadoopThriftAuthBridge.getBridge().createClient();	String tokenSig = MetastoreConf.getVar(conf, ConfVars.TOKEN_SIGNATURE);	tokenStrForm = SecurityUtils.getTokenStrForm(tokenSig);	if(tokenStrForm != null) {	transport = authBridge.createClientTransport(null, store.getHost(), "DIGEST", tokenStrForm, transport, MetaStoreUtils.getMetaStoreSaslProperties(conf, useSSL));	} else {	
hmsc open could not find delegation token creating kerberos based thrift connection 

HadoopThriftAuthBridge.Client authBridge = HadoopThriftAuthBridge.getBridge().createClient();	String tokenSig = MetastoreConf.getVar(conf, ConfVars.TOKEN_SIGNATURE);	tokenStrForm = SecurityUtils.getTokenStrForm(tokenSig);	if(tokenStrForm != null) {	transport = authBridge.createClientTransport(null, store.getHost(), "DIGEST", tokenStrForm, transport, MetaStoreUtils.getMetaStoreSaslProperties(conf, useSSL));	} else {	String principalConfig = MetastoreConf.getVar(conf, ConfVars.KERBEROS_PRINCIPAL);	transport = authBridge.createClientTransport( principalConfig, store.getHost(), "KERBEROS", null, transport, MetaStoreUtils.getMetaStoreSaslProperties(conf, useSSL));	}	} catch (IOException ioe) {	
couldn t create client transport 

final TProtocol protocol;	if (useCompactProtocol) {	protocol = new TCompactProtocol(transport);	} else {	protocol = new TBinaryProtocol(transport);	}	client = new ThriftHiveMetastore.Client(protocol);	try {	if (!transport.isOpen()) {	transport.open();	
opened a connection to metastore current connections 

}	client = new ThriftHiveMetastore.Client(protocol);	try {	if (!transport.isOpen()) {	transport.open();	}	isConnected = true;	} catch (TTransportException e) {	tte = e;	if (LOG.isDebugEnabled()) {	
failed to connect to the metastore server 

client = new ThriftHiveMetastore.Client(protocol);	try {	if (!transport.isOpen()) {	transport.open();	}	isConnected = true;	} catch (TTransportException e) {	tte = e;	if (LOG.isDebugEnabled()) {	} else {	
failed to connect to the metastore server 

tte = e;	if (LOG.isDebugEnabled()) {	} else {	}	}	if (isConnected && !useSasl && MetastoreConf.getBoolVar(conf, ConfVars.EXECUTE_SET_UGI)){	try {	UserGroupInformation ugi = SecurityUtils.getUGI();	client.set_ugi(ugi.getUserName(), Arrays.asList(ugi.getGroupNames()));	} catch (LoginException e) {	
failed to do login set ugi is not successful continuing without it 

if (LOG.isDebugEnabled()) {	} else {	}	}	if (isConnected && !useSasl && MetastoreConf.getBoolVar(conf, ConfVars.EXECUTE_SET_UGI)){	try {	UserGroupInformation ugi = SecurityUtils.getUGI();	client.set_ugi(ugi.getUserName(), Arrays.asList(ugi.getGroupNames()));	} catch (LoginException e) {	} catch (IOException e) {	
failed to find ugi of client set ugi is not successful continuing without it 

} else {	}	}	if (isConnected && !useSasl && MetastoreConf.getBoolVar(conf, ConfVars.EXECUTE_SET_UGI)){	try {	UserGroupInformation ugi = SecurityUtils.getUGI();	client.set_ugi(ugi.getUserName(), Arrays.asList(ugi.getGroupNames()));	} catch (LoginException e) {	} catch (IOException e) {	} catch (TException e) {	
set ugi not successful likely cause new client talking to old server continuing without it 

if (isConnected && !useSasl && MetastoreConf.getBoolVar(conf, ConfVars.EXECUTE_SET_UGI)){	try {	UserGroupInformation ugi = SecurityUtils.getUGI();	client.set_ugi(ugi.getUserName(), Arrays.asList(ugi.getGroupNames()));	} catch (LoginException e) {	} catch (IOException e) {	} catch (TException e) {	}	}	} catch (MetaException e) {	
unable to connect to metastore with uri in attempt 

}	}	} catch (MetaException e) {	}	if (isConnected) {	break;	}	}	if (!isConnected && retryDelaySeconds > 0) {	try {	
waiting seconds before next connection attempt 

if (!isConnected && retryDelaySeconds > 0) {	try {	Thread.sleep(retryDelaySeconds * 1000);	} catch (InterruptedException ignore) {}	}	}	if (!isConnected) {	throw new MetaException("Could not connect to meta store using any of the URIs provided." + " Most recent failure: " + StringUtils.stringifyException(tte));	}	snapshotActiveConf();	
connected to metastore 

public void close() {	isConnected = false;	currentMetaVars = null;	try {	if (null != client) {	client.shutdown();	}	} catch (TException e) {	
unable to shutdown metastore client will try closing transport directly 

isConnected = false;	currentMetaVars = null;	try {	if (null != client) {	client.shutdown();	}	} catch (TException e) {	}	if ((transport != null) && transport.isOpen()) {	transport.close();	
closed a connection to metastore current connections 

if (hook != null) {	hook.commitCreateTable(tbl);	}	success = true;	}	finally {	if (!success && (hook != null)) {	try {	hook.rollbackCreateTable(tbl);	} catch (Exception e){	
create rollback failed with 

dpe.setExpr(partExpr.getSecond());	dpe.setPartArchiveLevel(partExpr.getFirst());	exprs.add(dpe);	}	rps.setExprs(exprs);	DropPartitionsRequest req = new DropPartitionsRequest(dbName, tblName, rps);	req.setDeleteData(options.deleteData);	req.setNeedResult(options.returnResults);	req.setIfExists(options.ifExists);	if (options.purgeData) {	
dropped partitions will be purged 

public void flushCache() {	try {	client.flushCache();	} catch (TException e) {	
got error flushing the cache 

public OpenTxnsResponse openTxns(String user, int numTxns) throws TException {	String hostname = null;	try {	hostname = InetAddress.getLocalHost().getHostName();	} catch (UnknownHostException e) {	
unable to resolve my host name 

public NotificationEventResponse getNextNotification(long lastEventId, int maxEvents, NotificationFilter filter) throws TException {	NotificationEventRequest rqst = new NotificationEventRequest(lastEventId);	rqst.setMaxEvents(maxEvents);	NotificationEventResponse rsp = client.get_next_notification(rqst);	
got back events 

public AggrStats getAggrColStatsFor(String dbName, String tblName, List<String> colNames, List<String> partNames) throws NoSuchObjectException, MetaException, TException {	if (colNames.isEmpty() || partNames.isEmpty()) {	
columns is empty or partnames is empty short circuiting stats eval on client side 

========================= hive sample_1945 =========================

private IMetaStoreClient getMetaStoreClient(boolean retryInCaseOfTokenExpiration) throws HiveSQLException {	try {	return Hive.get(getHiveConf()).getMSC();	} catch (HiveException e) {	throw new HiveSQLException("Failed to get metastore connection: " + e, e);	} catch(MetaException e1) {	if (hmsDelegationTokenStr != null && retryInCaseOfTokenExpiration) {	
retrying failed metastore connection 

========================= hive sample_2390 =========================

public void init(String[] args) {	initLogger();	conf = loadConfig(args);	conf.startCleanup();	
loaded conf 

public AppConfig loadConfig(String[] args) {	AppConfig cf = new AppConfig();	try {	GenericOptionsParser parser = new GenericOptionsParser(cf, args);	if (parser.getRemainingArgs().length > 0) usage();	} catch (IOException e) {	
unable to parse options 

public void run() {	int port = conf.getInt(AppConfig.PORT, DEFAULT_PORT);	try {	checkEnv();	runServer(port);	port =  ArrayUtils.isEmpty(server.getConnectors()) ? -1 : ((ServerConnector)(server.getConnectors()[0])).getLocalPort();	System.out.println("templeton: listening on port " + port);	
templeton listening on port 

public void run() {	int port = conf.getInt(AppConfig.PORT, DEFAULT_PORT);	try {	checkEnv();	runServer(port);	port =  ArrayUtils.isEmpty(server.getConnectors()) ? -1 : ((ServerConnector)(server.getConnectors()[0])).getLocalPort();	System.out.println("templeton: listening on port " + port);	} catch (Exception e) {	System.err.println("templeton: Server failed to start: " + e.getMessage());	
server failed to start 

root.addFilter(fHolder, "/" + SERVLET_PATH + "/v1/pig/*", dispatches);	root.addFilter(fHolder, "/" + SERVLET_PATH + "/v1/hive/*", dispatches);	root.addFilter(fHolder, "/" + SERVLET_PATH + "/v1/sqoop/*", dispatches);	root.addFilter(fHolder, "/" + SERVLET_PATH + "/v1/queue/*", dispatches);	root.addFilter(fHolder, "/" + SERVLET_PATH + "/v1/jobs/*", dispatches);	root.addFilter(fHolder, "/" + SERVLET_PATH + "/v1/mapreduce/*", dispatches);	root.addFilter(fHolder, "/" + SERVLET_PATH + "/v1/status/*", dispatches);	root.addFilter(fHolder, "/" + SERVLET_PATH + "/v1/version/*", dispatches);	if (conf.getBoolean(AppConfig.XSRF_FILTER_ENABLED, false)){	root.addFilter(makeXSRFFilter(), "/" + SERVLET_PATH + "/*", dispatches);	
xsrf filter enabled 

root.addFilter(fHolder, "/" + SERVLET_PATH + "/v1/hive/*", dispatches);	root.addFilter(fHolder, "/" + SERVLET_PATH + "/v1/sqoop/*", dispatches);	root.addFilter(fHolder, "/" + SERVLET_PATH + "/v1/queue/*", dispatches);	root.addFilter(fHolder, "/" + SERVLET_PATH + "/v1/jobs/*", dispatches);	root.addFilter(fHolder, "/" + SERVLET_PATH + "/v1/mapreduce/*", dispatches);	root.addFilter(fHolder, "/" + SERVLET_PATH + "/v1/status/*", dispatches);	root.addFilter(fHolder, "/" + SERVLET_PATH + "/v1/version/*", dispatches);	if (conf.getBoolean(AppConfig.XSRF_FILTER_ENABLED, false)){	root.addFilter(makeXSRFFilter(), "/" + SERVLET_PATH + "/*", dispatches);	} else {	
xsrf filter disabled 

static String getUserName(HttpServletRequest request) {	if(!UserGroupInformation.isSecurityEnabled() && "POST".equalsIgnoreCase(request.getMethod())) {	String userName = request.getParameter(PseudoAuthenticator.USER_NAME);	if(userName != null) {	
is sent as form parameter which is deprecated as of hive should send it in the query string 

========================= hive sample_822 =========================

LOG.debug("File=" + INPUT_FILE_NAME);	PigServer server = createPigServer(true);	int queryNumber = 1;	logAndRegister(server, "A = load '" + INPUT_FILE_NAME + "' as (id:int, char5:chararray, varchar10:chararray, dec52:bigdecimal);", queryNumber++);	logAndRegister(server, "store A into '" + tblName + "' using " + HCatStorer.class.getName() + "();", queryNumber++);	logAndRegister(server, "B = load '" + tblName + "' using " + HCatLoader.class.getName() + "();", queryNumber);	CommandProcessorResponse cpr = driver.run("select * from " + tblName);	LOG.debug("cpr.respCode=" + cpr.getResponseCode() + " cpr.errMsg=" + cpr.getErrorMessage());	List l = new ArrayList();	driver.getResults(l);	
dumping rows via sql from 

static void dumpFile(String fileName) throws Exception {	File file = new File(fileName);	BufferedReader reader = new BufferedReader(new FileReader(file));	String line = null;	
dumping raw file 

========================= hive sample_988 =========================

if (createFunctionDesc.isTemp()) {	return createTemporaryFunction(createFunctionDesc);	} else {	try {	if (createFunctionDesc.getReplicationSpec().isInReplicationScope()) {	String[] qualifiedNameParts = FunctionUtils.getQualifiedFunctionNameParts( createFunctionDesc.getFunctionName());	String dbName = qualifiedNameParts[0];	String funcName = qualifiedNameParts[1];	Map<String, String> dbProps = Hive.get().getDatabase(dbName).getParameters();	if (!createFunctionDesc.getReplicationSpec().allowEventReplacementInto(dbProps)) {	
functiontask create function is skipped as database is newer than update 

String dbName = qualifiedNameParts[0];	String funcName = qualifiedNameParts[1];	Map<String, String> dbProps = Hive.get().getDatabase(dbName).getParameters();	if (!createFunctionDesc.getReplicationSpec().allowEventReplacementInto(dbProps)) {	return 0;	}	}	return createPermanentFunction(Hive.get(conf), createFunctionDesc);	} catch (Exception e) {	setException(e);	
failed to create function 

if (dropFunctionDesc.isTemp()) {	return dropTemporaryFunction(dropFunctionDesc);	} else {	try {	if (dropFunctionDesc.getReplicationSpec().isInReplicationScope()) {	String[] qualifiedNameParts = FunctionUtils.getQualifiedFunctionNameParts( dropFunctionDesc.getFunctionName());	String dbName = qualifiedNameParts[0];	String funcName = qualifiedNameParts[1];	Map<String, String> dbProps = Hive.get().getDatabase(dbName).getParameters();	if (!dropFunctionDesc.getReplicationSpec().allowEventReplacementInto(dbProps)) {	
functiontask drop function is skipped as database is newer than update 

String dbName = qualifiedNameParts[0];	String funcName = qualifiedNameParts[1];	Map<String, String> dbProps = Hive.get().getDatabase(dbName).getParameters();	if (!dropFunctionDesc.getReplicationSpec().allowEventReplacementInto(dbProps)) {	return 0;	}	}	return dropPermanentFunction(Hive.get(conf), dropFunctionDesc);	} catch (Exception e) {	setException(e);	
failed to drop function 

setException(e);	return 1;	}	}	}	if (work.getReloadFunctionDesc() != null) {	try {	Hive.get().reloadFunctions();	} catch (Exception e) {	setException(e);	
failed to reload functions 

addFunctionResources(resources);	Class<?> udfClass = getUdfClass(createFunctionDesc);	FunctionInfo registered = FunctionRegistry.registerTemporaryUDF( createFunctionDesc.getFunctionName(), udfClass, resources);	if (registered != null) {	return 0;	}	console.printError("FAILED: Class " + createFunctionDesc.getClassName() + " does not implement UDF, GenericUDF, or UDAF");	return 1;	} catch (HiveException e) {	console.printError("FAILED: " + e.toString());	
create function 

if (registered != null) {	return 0;	}	console.printError("FAILED: Class " + createFunctionDesc.getClassName() + " does not implement UDF, GenericUDF, or UDAF");	return 1;	} catch (HiveException e) {	console.printError("FAILED: " + e.toString());	return 1;	} catch (ClassNotFoundException e) {	console.printError("FAILED: Class " + createFunctionDesc.getClassName() + " not found");	
create function 

private int dropMacro(DropMacroDesc dropMacroDesc) {	try {	FunctionRegistry.unregisterTemporaryUDF(dropMacroDesc.getMacroName());	return 0;	} catch (HiveException e) {	
drop macro 

private int dropPermanentFunction(Hive db, DropFunctionDesc dropFunctionDesc) {	try {	String[] qualifiedNameParts = FunctionUtils.getQualifiedFunctionNameParts( dropFunctionDesc.getFunctionName());	String dbName = qualifiedNameParts[0];	String funcName = qualifiedNameParts[1];	String registeredName = FunctionUtils.qualifyFunctionName(funcName, dbName);	FunctionRegistry.unregisterPermanentFunction(registeredName);	db.dropFunction(dbName, funcName);	return 0;	} catch (Exception e) {	
drop function 

private int dropTemporaryFunction(DropFunctionDesc dropFunctionDesc) {	try {	FunctionRegistry.unregisterTemporaryUDF(dropFunctionDesc.getFunctionName());	return 0;	} catch (HiveException e) {	
drop function 

}	for (ResourceUri res : resources) {	String resUri = res.getUri();	if (ResourceDownloader.isFileUri(resUri)) {	throw new HiveException("Hive warehouse is non-local, but " + res.getUri() + " specifies file on local filesystem. " + "Resources on non-local warehouse should specify a non-local scheme/path");	}	}	} catch (HiveException e) {	throw e;	} catch (Exception e) {	
exception caught in checklocalfunctionresources 

========================= hive sample_4614 =========================

private void printDebugOutput(String functionName, AggregationBuffer agg) {	BooleanStatsAgg myagg = (BooleanStatsAgg) agg;	LOG.debug(functionName);	
count of true values 

private void printDebugOutput(String functionName, AggregationBuffer agg) {	BooleanStatsAgg myagg = (BooleanStatsAgg) agg;	LOG.debug(functionName);	
count of false values 

private void printDebugOutput(String functionName, AggregationBuffer agg) {	BooleanStatsAgg myagg = (BooleanStatsAgg) agg;	LOG.debug(functionName);	
count of null values 

========================= hive sample_4751 =========================

while (elementByteEnd <= arrayByteEnd) {	if (elementByteEnd == arrayByteEnd || bytes[elementByteEnd] == itemSeparator) {	if (keyStart == null || mapSize + 1 == keyStart.length) {	enlargeArrays();	}	keyStart[mapSize] = elementByteBegin;	keyEnd[mapSize] = (keyValueSeparatorPosition == -1 ? elementByteEnd : keyValueSeparatorPosition);	valueLength[mapSize] = elementByteEnd - (keyEnd[mapSize] + 1);	LazyPrimitive<?, ?> lazyKey = uncheckedGetKey(mapSize);	if (lazyKey == null) {	
skipped empty entry or entry with empty key in the representation of column with map type 

========================= hive sample_5510 =========================

if (!paths.isEmpty()) {	Iterator<URI> pathIterator = Iterators.transform(paths.iterator(), new Function<Path, URI>() {	public URI apply(Path path) {	return path.toUri();	}	});	Set<URI> uris = new HashSet<URI>();	Iterators.addAll(uris, pathIterator);	if (LOG.isDebugEnabled()) {	for (URI uri: uris) {	
marking uri as needing credentials 

public GroupInputEdge createEdge(VertexGroup group, JobConf vConf, Vertex w, TezEdgeProperty edgeProp, BaseWork work, TezWork tezWork) throws IOException {	Class mergeInputClass;	
creating edge between and 

}	logLevel = sb.toString();	if (HiveConf.getIntVar(conf, HiveConf.ConfVars.HIVETEZCONTAINERSIZE) > 0) {	if (javaOpts != null) {	return javaOpts + " " + logLevel;	} else  {	return logLevel;	}	} else {	if (javaOpts != null && !javaOpts.isEmpty()) {	
will be ignored because is not set 

if (mergeJoinWork.getMainWork() instanceof MapWork) {	List<BaseWork> mapWorkList = mergeJoinWork.getBaseWorkList();	MapWork mapWork = (MapWork) (mergeJoinWork.getMainWork());	Vertex mergeVx = createVertex( conf, mapWork, fs, mrScratchDir, ctx, vertexType, localResources);	conf.setClass("mapred.input.format.class", HiveInputFormat.class, InputFormat.class);	conf.setBoolean("mapreduce.tez.input.initializer.serialize.event.payload", false);	for (int i = 0; i < mapWorkList.size(); i++) {	mapWork = (MapWork) (mapWorkList.get(i));	conf.set(TEZ_MERGE_CURRENT_MERGE_FILE_PREFIX, mapWork.getName());	conf.set(Utilities.INPUT_NAME, mapWork.getName());	
going through each work and adding multimrinput 

private Vertex createVertex(JobConf conf, MapWork mapWork, FileSystem fs, Path mrScratchDir, Context ctx, VertexType vertexType, Map<String, LocalResource> localResources) throws Exception {	Utilities.cacheMapWork(conf, mapWork, mrScratchDir);	Utilities.createTmpDirs(conf, mapWork);	Vertex map = null;	boolean groupSplitsInInputInitializer;	DataSourceDescriptor dataSource;	int numTasks = -1;	Class inputFormatClass = conf.getClass("mapred.input.format.class", InputFormat.class);	boolean vertexHasCustomInput = VertexType.isCustomInputType(vertexType);	
vertex has custom input 

private LocalResource createLocalResource(FileSystem remoteFs, Path file, LocalResourceType type, LocalResourceVisibility visibility) {	FileStatus fstat = null;	try {	fstat = remoteFs.getFileStatus(file);	} catch (IOException e) {	e.printStackTrace();	}	URL resourceURL = ConverterUtils.getYarnUrlFromPath(file);	long resourceSize = fstat.getLen();	long resourceModificationTime = fstat.getModificationTime();	
resource modification time for 

for (String skipFile : skipFiles) {	if (StringUtils.isBlank(skipFile)) continue;	skipFileSet.add(new Path(skipFile));	}	}	for (String file : files) {	if (!StringUtils.isNotBlank(file)) {	continue;	}	if (skipFileSet != null && skipFileSet.contains(new Path(file))) {	
skipping vertex resource that already exists in the session 

public LocalResource localizeResource( Path src, Path dest, LocalResourceType type, Configuration conf) throws IOException {	FileSystem destFS = dest.getFileSystem(conf);	FileSystem srcFs = FileSystem.getLocal(conf);	if (src != null && !checkPreExisting(srcFs, src, dest, conf)) {	String srcStr = src.toString();	
localizing resource because it does not exist to dest 

try {	destFS.copyFromLocalFile(false, false, src, dest);	synchronized (notifier) {	notifier.notifyAll();	}	copyNotifiers.remove(srcStr, notifier);	} catch (IOException e) {	if ("Exception while contacting value generator".equals(e.getMessage())) {	throw new IOException("copyFromLocalFile failed due to HDFS KMS failure", e);	}	
looks like another thread or process is writing the same file 

notifier.notifyAll();	}	copyNotifiers.remove(srcStr, notifier);	} catch (IOException e) {	if ("Exception while contacting value generator".equals(e.getMessage())) {	throw new IOException("copyFromLocalFile failed due to HDFS KMS failure", e);	}	int waitAttempts = HiveConf.getIntVar( conf, ConfVars.HIVE_LOCALIZE_RESOURCE_NUM_WAIT_ATTEMPTS);	long sleepInterval = HiveConf.getTimeVar( conf, HiveConf.ConfVars.HIVE_LOCALIZE_RESOURCE_WAIT_INTERVAL, TimeUnit.MILLISECONDS);	if (!checkOrWaitForTheFile( srcFs, src, dest, conf, notifierOld, waitAttempts, sleepInterval, true)) {	
could not find the jar that was being uploaded 

public boolean checkOrWaitForTheFile(FileSystem srcFs, Path src, Path dest, Configuration conf, Object notifier, int waitAttempts, long sleepInterval, boolean doLog) throws IOException {	for (int i = 0; i < waitAttempts; i++) {	if (checkPreExisting(srcFs, src, dest, conf)) return true;	if (doLog && i == 0) {	
waiting for the file attempts with ms interval 

String userName = System.getProperty("user.name");	try {	ugi = Utils.getUGI();	userName = ugi.getShortUserName();	} catch (LoginException e) {	throw new IOException(e);	}	scratchDir = new Path(scratchDir, userName);	Path tezDir = getTezDir(scratchDir);	FileSystem fs = tezDir.getFileSystem(conf);	
tezdir path set for user 

float tezMaxReserveFraction = conf.getFloatVar(ConfVars.TEZ_TASK_SCALE_MEMORY_RESERVE_FRACTION_MAX);	Resource resource = getContainerResource(conf);	long containerSize = (long) resource.getMemory() * 1024 * 1024;	String javaOpts = getContainerJavaOpts(conf);	long xmx = parseRightmostXmx(javaOpts);	if (xmx <= 0) {	xmx = (long) (tezHeapFraction * containerSize);	}	long actualMemToBeAllocated = (long) (tezMinReserveFraction * xmx);	if (actualMemToBeAllocated < memoryRequested) {	
the actual amount of memory to be allocated is less than the amount of requested memory for map join conversion 

Resource resource = getContainerResource(conf);	long containerSize = (long) resource.getMemory() * 1024 * 1024;	String javaOpts = getContainerJavaOpts(conf);	long xmx = parseRightmostXmx(javaOpts);	if (xmx <= 0) {	xmx = (long) (tezHeapFraction * containerSize);	}	long actualMemToBeAllocated = (long) (tezMinReserveFraction * xmx);	if (actualMemToBeAllocated < memoryRequested) {	float frac = (float) memoryRequested / xmx;	
fraction after calculation 

long xmx = parseRightmostXmx(javaOpts);	if (xmx <= 0) {	xmx = (long) (tezHeapFraction * containerSize);	}	long actualMemToBeAllocated = (long) (tezMinReserveFraction * xmx);	if (actualMemToBeAllocated < memoryRequested) {	float frac = (float) memoryRequested / xmx;	if (frac <= tezMinReserveFraction) {	return tezMinReserveFraction;	} else if (frac > tezMinReserveFraction && frac < tezMaxReserveFraction) {	
will adjust tez setting to to allocate more memory 

========================= hive sample_3997 =========================

public boolean next(RecordIdentifier identifier, Text text) throws IOException {	if (is == null) {	if (filesToRead.empty()) return false;	Path p = filesToRead.pop();	
reading records from 

========================= hive sample_2410 =========================

public void process(Object row, int tag) throws HiveException {	try {	VectorizedRowBatch batch = (VectorizedRowBatch) row;	batchCounter++;	if (batch.size == 0) {	if (LOG.isDebugEnabled()) {	
batch empty 

========================= hive sample_4194 =========================

private void collectFileSinkDescs(Operator<?> leaf, Set<FileSinkDesc> acidSinks) {	if(leaf instanceof FileSinkOperator) {	FileSinkDesc fsd = ((FileSinkOperator) leaf).getConf();	if(fsd.getWriteType() != AcidUtils.Operation.NOT_ACID) {	if(acidSinks.add(fsd)) {	if(LOG.isDebugEnabled()) {	
found acid sink 

========================= hive sample_2943 =========================

while ((line = in.readLine()) != null) {	logger.info(line);	if (errLogs != null) {	if (numErrLogLines++ < MAX_ERR_LOG_LINES_FOR_RPC) {	errLogs.add(line);	}	}	}	} catch (IOException e) {	if (callback.isAlive()) {	
i o error in redirector thread 

logger.info(line);	if (errLogs != null) {	if (numErrLogLines++ < MAX_ERR_LOG_LINES_FOR_RPC) {	errLogs.add(line);	}	}	}	} catch (IOException e) {	if (callback.isAlive()) {	} else {	
i o error in redirector thread while stopping the remote driver 

if (numErrLogLines++ < MAX_ERR_LOG_LINES_FOR_RPC) {	errLogs.add(line);	}	}	}	} catch (IOException e) {	if (callback.isAlive()) {	} else {	}	} catch (Exception e) {	
error in redirector thread 

========================= hive sample_1396 =========================

private void readPageV1(DataPageV1 page) {	ValuesReader rlReader = page.getRlEncoding().getValuesReader(descriptor, REPETITION_LEVEL);	ValuesReader dlReader = page.getDlEncoding().getValuesReader(descriptor, DEFINITION_LEVEL);	this.repetitionLevelColumn = new ValuesReaderIntIterator(rlReader);	this.definitionLevelColumn = new ValuesReaderIntIterator(dlReader);	try {	byte[] bytes = page.getBytes().toByteArray();	
page size bytes and records 

private void readPageV1(DataPageV1 page) {	ValuesReader rlReader = page.getRlEncoding().getValuesReader(descriptor, REPETITION_LEVEL);	ValuesReader dlReader = page.getDlEncoding().getValuesReader(descriptor, DEFINITION_LEVEL);	this.repetitionLevelColumn = new ValuesReaderIntIterator(rlReader);	this.definitionLevelColumn = new ValuesReaderIntIterator(dlReader);	try {	byte[] bytes = page.getBytes().toByteArray();	
reading repetition levels at 

private void readPageV1(DataPageV1 page) {	ValuesReader rlReader = page.getRlEncoding().getValuesReader(descriptor, REPETITION_LEVEL);	ValuesReader dlReader = page.getDlEncoding().getValuesReader(descriptor, DEFINITION_LEVEL);	this.repetitionLevelColumn = new ValuesReaderIntIterator(rlReader);	this.definitionLevelColumn = new ValuesReaderIntIterator(dlReader);	try {	byte[] bytes = page.getBytes().toByteArray();	rlReader.initFromPage(pageValueCount, bytes, 0);	int next = rlReader.getNextOffset();	
reading definition levels at 

ValuesReader rlReader = page.getRlEncoding().getValuesReader(descriptor, REPETITION_LEVEL);	ValuesReader dlReader = page.getDlEncoding().getValuesReader(descriptor, DEFINITION_LEVEL);	this.repetitionLevelColumn = new ValuesReaderIntIterator(rlReader);	this.definitionLevelColumn = new ValuesReaderIntIterator(dlReader);	try {	byte[] bytes = page.getBytes().toByteArray();	rlReader.initFromPage(pageValueCount, bytes, 0);	int next = rlReader.getNextOffset();	dlReader.initFromPage(pageValueCount, bytes, next);	next = dlReader.getNextOffset();	
reading data at 

private void readPageV2(DataPageV2 page) {	this.pageValueCount = page.getValueCount();	this.repetitionLevelColumn = newRLEIterator(descriptor.getMaxRepetitionLevel(), page.getRepetitionLevels());	this.definitionLevelColumn = newRLEIterator(descriptor.getMaxDefinitionLevel(), page.getDefinitionLevels());	try {	
page data size bytes and records 

========================= hive sample_3714 =========================

protected Table resolveTable(CompactionInfo ci) throws MetaException {	try {	return rs.getTable(ci.dbname, ci.tableName);	} catch (MetaException e) {	
unable to find table 

protected Partition resolvePartition(CompactionInfo ci) throws Exception {	if (ci.partName != null) {	List<Partition> parts;	try {	parts = rs.getPartitionsByNames(ci.dbname, ci.tableName, Collections.singletonList(ci.partName));	if (parts == null || parts.size() == 0) {	return null;	}	} catch (Exception e) {	
unable to find partition 

List<Partition> parts;	try {	parts = rs.getPartitionsByNames(ci.dbname, ci.tableName, Collections.singletonList(ci.partName));	if (parts == null || parts.size() == 0) {	return null;	}	} catch (Exception e) {	throw e;	}	if (parts.size() != 1) {	
does not refer to a single partition 

protected String findUserToRunAs(String location, Table t) throws IOException, InterruptedException {	
determining who to run the job as 

protected String findUserToRunAs(String location, Table t) throws IOException, InterruptedException {	final Path p = new Path(location);	final FileSystem fs = p.getFileSystem(conf);	try {	FileStatus stat = fs.getFileStatus(p);	
running job as 

protected String findUserToRunAs(String location, Table t) throws IOException, InterruptedException {	final Path p = new Path(location);	final FileSystem fs = p.getFileSystem(conf);	try {	FileStatus stat = fs.getFileStatus(p);	return stat.getOwner();	} catch (AccessControlException e) {	
unable to stat file as current user trying as table owner 

public Object run() throws Exception {	FileSystem proxyFs = p.getFileSystem(conf);	FileStatus stat = proxyFs.getFileStatus(p);	wrapper.add(stat.getOwner());	return null;	}	});	try {	FileSystem.closeAllForUGI(ugi);	} catch (IOException exception) {	
could not clean up file system handles for ugi 

FileStatus stat = proxyFs.getFileStatus(p);	wrapper.add(stat.getOwner());	return null;	}	});	try {	FileSystem.closeAllForUGI(ugi);	} catch (IOException exception) {	}	if (wrapper.size() == 1) {	
running job as 

}	});	try {	FileSystem.closeAllForUGI(ugi);	} catch (IOException exception) {	}	if (wrapper.size() == 1) {	return wrapper.get(0);	}	}	
unable to stat file as either current user or table owner giving up 

========================= hive sample_2806 =========================

private void setupOutput() throws IOException {	if (parentFile == null) {	while (true) {	parentFile = File.createTempFile("hive-resultcache", "");	if (parentFile.delete() && parentFile.mkdir()) {	parentFile.deleteOnExit();	break;	}	if (LOG.isDebugEnabled()) {	
retry creating tmp result cache directory 

if (parentFile.delete() && parentFile.mkdir()) {	parentFile.deleteOnExit();	break;	}	if (LOG.isDebugEnabled()) {	}	}	}	if (tmpFile == null || input != null) {	tmpFile = File.createTempFile("ResultCache", ".tmp", parentFile);	
resultcache created temp file 

========================= hive sample_4602 =========================

public static void cleanUpScratchDir(HiveConf hiveConf) {	if (hiveConf.getBoolVar(HiveConf.ConfVars.HIVE_START_CLEANUP_SCRATCHDIR)) {	String hiveScratchDir = hiveConf.get(HiveConf.ConfVars.SCRATCHDIR.varname);	try {	Path jobScratchDir = new Path(hiveScratchDir);	
cleaning scratchdir 

public static void cleanUpScratchDir(HiveConf hiveConf) {	if (hiveConf.getBoolVar(HiveConf.ConfVars.HIVE_START_CLEANUP_SCRATCHDIR)) {	String hiveScratchDir = hiveConf.get(HiveConf.ConfVars.SCRATCHDIR.varname);	try {	Path jobScratchDir = new Path(hiveScratchDir);	FileSystem fileSystem = jobScratchDir.getFileSystem(hiveConf);	fileSystem.delete(jobScratchDir, true);	}	catch (Throwable e) {	
unable to delete scratchdir 

public static String hostname() {	try {	return InetAddress.getLocalHost().getHostName();	} catch (UnknownHostException e) {	
unable to resolve my host name 

========================= hive sample_1400 =========================

public static TezSessionState getSession(TezSessionState session, HiveConf conf, MappingInput input, boolean isUnmanagedLlapMode, WmContext wmContext) throws Exception {	Set<String> desiredCounters = new HashSet<>();	WorkloadManager wm = WorkloadManager.getInstance();	if (wm == null) {	
using unmanaged session wm is not initialized 

public static TezSessionState getSession(TezSessionState session, HiveConf conf, MappingInput input, boolean isUnmanagedLlapMode, WmContext wmContext) throws Exception {	Set<String> desiredCounters = new HashSet<>();	WorkloadManager wm = WorkloadManager.getInstance();	if (wm == null) {	return getUnmanagedSession(session, conf, desiredCounters, isUnmanagedLlapMode, wmContext);	}	if (!wm.isManaged(input)) {	
using unmanaged session no mapping for 

public static TezSessionState getSession(TezSessionState session, HiveConf conf, MappingInput input, boolean isUnmanagedLlapMode, WmContext wmContext) throws Exception {	Set<String> desiredCounters = new HashSet<>();	WorkloadManager wm = WorkloadManager.getInstance();	if (wm == null) {	return getUnmanagedSession(session, conf, desiredCounters, isUnmanagedLlapMode, wmContext);	}	if (!wm.isManaged(input)) {	return getUnmanagedSession(session, conf, desiredCounters, isUnmanagedLlapMode, wmContext);	}	try {	
getting a wm session for 

}	if (!wm.isManaged(input)) {	return getUnmanagedSession(session, conf, desiredCounters, isUnmanagedLlapMode, wmContext);	}	try {	WmTezSession result = wm.getSession(session, input, conf, wmContext);	result.setWmContext(wmContext);	wm.updateTriggers(result);	return result;	} catch (WorkloadManager.NoPoolMappingException ex) {	
nopoolmappingexception thrown getting an un managed session 

========================= hive sample_3981 =========================

sb.append(filter(colMap.familyName)).append("_") .append(filter(colMap.qualifierPrefix) + i).append(StringUtils.COMMA_STR);	} else {	sb.append(filter(colMap.familyName)).append("_").append("col" + i) .append(StringUtils.COMMA_STR);	}	} else {	sb.append(filter(colMap.familyName)).append("_").append(filter(colMap.qualifierName)) .append(StringUtils.COMMA_STR);	}	}	trim(sb);	if (LOG.isDebugEnabled()) {	
generated columns 

throw new SerDeException(serdeConstants.SERIALIZATION_CLASS + " property not provided for column family [" + colMap.familyName + "] and qualifier [" + qualifierName + "]");	}	}	StringBuilder generatedStruct = new StringBuilder();	generateColumnStruct(serType, serClassName, schemaLiteral, colMap, generatedStruct);	sb.append(generatedStruct);	}	}	trim(sb);	if (LOG.isDebugEnabled()) {	
generated column types 

========================= hive sample_588 =========================

response = metaStoreClient.lock(request);	} catch (TException e) {	throw new LockException("Unable to acquire lock for tables: [" + join(tables) + "]", e);	}	if (response != null) {	LockState state = response.getState();	if (state == LockState.NOT_ACQUIRED || state == LockState.ABORT) {	break;	}	if (state == LockState.ACQUIRED) {	
acquired lock 

private void internalRelease() {	try {	if (lockId != null && transactionId == null) {	metaStoreClient.unlock(lockId);	
released lock 

private void internalRelease() {	try {	if (lockId != null && transactionId == null) {	metaStoreClient.unlock(lockId);	lockId = null;	}	} catch (TException e) {	
lock failed 

private void initiateHeartbeat() {	int heartbeatPeriod = getHeartbeatPeriod();	
heartbeat period s 

========================= hive sample_971 =========================

private void setupOutput(String spillLocalDirs) throws IOException, HiveException {	FileOutputStream fos = null;	try {	if (parentDir == null) {	parentDir = FileUtils.createLocalDirsTempFile(spillLocalDirs, "object-container", "", true);	}	if (tmpFile == null || input != null) {	tmpFile = File.createTempFile("ObjectContainer", ".tmp", parentDir);	
objectcontainer created temp file 

========================= hive sample_4069 =========================

ImmutableBitSet.Builder keys = ImmutableBitSet.builder();	for (String pkColName : pki.getColNames().values()) {	int pkPos;	for (pkPos = 0; pkPos < rowType.getFieldNames().size(); pkPos++) {	String colName = rowType.getFieldNames().get(pkPos);	if (pkColName.equals(colName)) {	break;	}	}	if (pkPos == rowType.getFieldNames().size()) {	
column for primary key definition not found 

ImmutableBitSet.Builder keys = ImmutableBitSet.builder();	for (UniqueConstraintCol ukCol : ukCols) {	int ukPos;	for (ukPos = 0; ukPos < rowType.getFieldNames().size(); ukPos++) {	String colName = rowType.getFieldNames().get(ukPos);	if (ukCol.colName.equals(colName)) {	break;	}	}	if (ukPos == rowType.getFieldNames().size()) {	
column for unique constraint definition not found 

parentFullyQualifiedName = parentTableName;	}	List<String> parentTableQualifiedName = Lists.newArrayList(parentFullyQualifiedName);	Table parentTab = null;	try {	parentTab = Hive.get().getTable(parentDatabaseName, parentTableName);	} catch (HiveException e) {	throw new RuntimeException(e);	}	if (parentTab == null) {	
table for primary key not found databasename tablename 

}	}	int pkPos;	for (pkPos = 0; pkPos < parentTab.getAllCols().size(); pkPos++) {	String pkColName = parentTab.getAllCols().get(pkPos).getName();	if (pkColName.equals(fkCol.parentColName)) {	break;	}	}	if (fkPos == rowType.getFieldNames().size() || pkPos == parentTab.getAllCols().size()) {	
column for foreign key definition not found 

String logMsg = "Collecting stats failed.";	LOG.error(logMsg, e);	throw new RuntimeException(logMsg, e);	}	}	if (hiveColStats != null && hiveColStats.size() == nonPartColNamesThatRqrStats.size()) {	for (int i = 0; i < hiveColStats.size(); i++) {	hiveColStatsMap.put(nonPartColIndxsThatRqrStats.get(i), hiveColStats.get(i));	colStatsCached.put(hiveColStats.get(i).getColumnName(), hiveColStats.get(i));	if (LOG.isDebugEnabled()) {	
stats for column in table stored in cache 

}	}	}	if (colNamesFailedStats.isEmpty() && !partColNamesThatRqrStats.isEmpty()) {	ColStatistics cStats = null;	for (int i = 0; i < partColNamesThatRqrStats.size(); i++) {	cStats = StatsUtils.getColStatsForPartCol(hivePartitionColsMap.get(partColIndxsThatRqrStats.get(i)), new PartitionIterable(partitionList.getNotDeniedPartns()), hiveConf);	hiveColStatsMap.put(partColIndxsThatRqrStats.get(i), cStats);	colStatsCached.put(cStats.getColumnName(), cStats);	if (LOG.isDebugEnabled()) {	
stats for column in table stored in cache 

========================= hive sample_2851 =========================

TServerSocket thriftServerSocket = TSSLTransportFactory.getServerSocket(portNum, 0, serverAddress.getAddress(), params);	if (thriftServerSocket.getServerSocket() instanceof SSLServerSocket) {	List<String> sslVersionBlacklistLocal = new ArrayList<>();	for (String sslVersion : sslVersionBlacklist) {	sslVersionBlacklistLocal.add(sslVersion.trim().toLowerCase());	}	SSLServerSocket sslServerSocket = (SSLServerSocket) thriftServerSocket.getServerSocket();	List<String> enabledProtocols = new ArrayList<>();	for (String protocol : sslServerSocket.getEnabledProtocols()) {	if (sslVersionBlacklistLocal.contains(protocol.toLowerCase())) {	
disabling ssl protocol 

}	SSLServerSocket sslServerSocket = (SSLServerSocket) thriftServerSocket.getServerSocket();	List<String> enabledProtocols = new ArrayList<>();	for (String protocol : sslServerSocket.getEnabledProtocols()) {	if (sslVersionBlacklistLocal.contains(protocol.toLowerCase())) {	} else {	enabledProtocols.add(protocol);	}	}	sslServerSocket.setEnabledProtocols(enabledProtocols.toArray(new String[0]));	
ssl server socket enabled protocols 

========================= hive sample_1904 =========================

Map<TableDesc, StructObjectInspector> convertedOI = getConvertedOI(tableNameToConf);	for (Map.Entry<Path, ArrayList<String>> entry : conf.getPathToAliases().entrySet()) {	Path onefile = entry.getKey();	List<String> aliases = entry.getValue();	PartitionDesc partDesc = conf.getPathToPartitionInfo().get(onefile);	TableDesc tableDesc = partDesc.getTableDesc();	Configuration newConf = tableNameToConf.get(tableDesc.getTableName());	for (String alias : aliases) {	Operator<? extends OperatorDesc> op = conf.getAliasToWork().get(alias);	if (LOG.isDebugEnabled()) {	
adding alias to work list for file 

for (Map<Operator<?>, MapOpCtx> contexts : opCtxMap.values()) {	for (MapOpCtx context : contexts.values()) {	if (!children.contains(context.op)) {	continue;	}	StructObjectInspector prev = childrenOpToOI.put(context.op, context.rowObjectInspector);	if (prev != null && !prev.equals(context.rowObjectInspector)) {	throw new HiveException("Conflict on row inspector for " + context.alias);	}	if (LOG.isDebugEnabled()) {	
dump 

Map<Operator<?>, MapOpCtx> contexts = opCtxMap.get(nominalPath);	if (LOG.isInfoEnabled()) {	StringBuilder builder = new StringBuilder();	for (MapOpCtx context : contexts.values()) {	if (builder.length() > 0) {	builder.append(", ");	}	builder.append(context.alias);	}	if (LOG.isDebugEnabled()) {	
processing alias es for file 

protected final void rowsForwarded(int childrenDone, int rows) {	numRows += rows;	if (LOG.isInfoEnabled()) {	while (numRows >= cntr) {	cntr = logEveryNRows == 0 ? cntr * 10 : numRows + logEveryNRows;	if (cntr < 0 || numRows < 0) {	cntr = 1;	numRows = 0;	}	
records read 

========================= hive sample_3893 =========================

try {	DUMP_HEAP_METHOD.invoke(hotspotMBean, new Object[]{fileName, Boolean.valueOf(live)});	} catch (RuntimeException re) {	LOG.error(re.getMessage());	throw re;	} catch (Exception exp) {	LOG.error(exp.getMessage());	throw new RuntimeException(exp);	}	} else {	
cannot find method dumpheap in com sun management hotspotdiagnosticmxbean 

========================= hive sample_5044 =========================

if (key.startsWith(Utils.BOOTSTRAP_DUMP_STATE_KEY_PREFIX) || key.equals(ReplicationSpec.KEY.CURR_STATE_ID.toString())) {	continue;	}	newDbProps.put(key, entry.getValue());	}	alterDbDesc = new AlterDatabaseDesc(actualDbName, newDbProps, context.eventOnlyReplicationSpec());	} else {	alterDbDesc = new AlterDatabaseDesc(actualDbName, new PrincipalDesc(newDb.getOwnerName(), newDb.getOwnerType()), context.eventOnlyReplicationSpec());	}	Task<DDLWork> alterDbTask = TaskFactory.get( new DDLWork(readEntitySet, writeEntitySet, alterDbDesc), context.hiveConf);	
added alter database task 

========================= hive sample_3485 =========================

private String createConditionString(String filterXml, Map<String, String> columnMap) {	if ((filterXml == null) || (filterXml.trim().isEmpty())) {	return EMPTY_STRING;	}	try (XMLDecoder decoder = new XMLDecoder(new ByteArrayInputStream(filterXml.getBytes("UTF-8")))) {	Object object = decoder.readObject();	if (!(object instanceof ExprNodeDesc)) {	
deserialized filter expression is not of the expected type 

try (XMLDecoder decoder = new XMLDecoder(new ByteArrayInputStream(filterXml.getBytes("UTF-8")))) {	Object object = decoder.readObject();	if (!(object instanceof ExprNodeDesc)) {	throw new RuntimeException("Deserialized filter expression is not of the expected type");	}	ExprNodeDesc conditionNode = (ExprNodeDesc) object;	walkTreeAndTranslateColumnNames(conditionNode, columnMap);	return conditionNode.getExprString();	}	catch (Exception e) {	
error during condition build 

========================= hive sample_1142 =========================

public MttTestCallableResult call() throws Exception {	
thread starts 

dest[0] = null;	a.allocateMultiple(dest, allocSize);	LlapAllocatorBuffer buf = dest[0];	assertTrue(buf.incRef() > 0);	allocs.add(buf);	++result.successes;	buf.decRef();	} catch (AllocatorOutOfMemoryException ex) {	++result.ooms;	} catch (Throwable ex) {	
Failed 

} catch (AllocatorOutOfMemoryException ex) {	++result.ooms;	} catch (Throwable ex) {	throw new Exception(ex);	}	}	for (MemoryBuffer buf : allocs) {	try {	a.deallocate(buf);	} catch (Throwable ex) {	
Failed 

cdlIn.await();	} catch (InterruptedException e) {	throw new RuntimeException(e);	}	cdlOut.countDown();	for (int i = 0; i < allocTasks.length; ++i) {	try {	Object result = allocTasks[i].get();	LOG.info("" + result);	} catch (Throwable tt) {	
test callable failed 

t = tt;	}	}	}	dumpTask.cancel(true);	if (defragTask != null) {	defragStopped.set(true);	try {	defragTask.get();	} catch (Throwable tt) {	
defragmentation thread failed 

} catch (Throwable tt) {	if (t == null) {	a.dumpTestLog();	t = tt;	}	}	}	if (t != null) {	throw new RuntimeException("One of the errors", t);	}	
allocator state after all the tasks 

public static FutureTask<Void> createAllocatorDumpTask(final BuddyAllocator a) {	return new FutureTask<Void>(new Callable<Void>() {	public Void call() throws Exception {	int logs = 40000;	while ((--logs) >= 0) {	
allocator state mtt 

private static void runCustomDiscard(BuddyAllocator a, int[] sizes, int[] dealloc, int size) {	LlapAllocatorBuffer[] initial = prepareCustomFragmentedAllocator(a, sizes, dealloc, true);	LlapAllocatorBuffer after = allocate(a, 1, size, initial.length + 1)[0];	
after 

private static void runZebraDiscard( BuddyAllocator a, int baseSize, int pairCount, int allocs) {	LlapAllocatorBuffer[] initial = prepareZebraFragmentedAllocator(a, baseSize, pairCount, true);	int allocFraction = allocs * 2;	int bigAllocSize = pairCount * 2 * baseSize / allocFraction;	LlapAllocatorBuffer[] after = allocate(a, allocs, bigAllocSize, 1 + initial.length);	
after 

public static LlapAllocatorBuffer[] prepareZebraFragmentedAllocator( BuddyAllocator a, int baseSize, int pairCount, boolean doIncRef) {	LlapAllocatorBuffer[] initial = allocate(a, pairCount * 2, baseSize, 1, doIncRef);	for (int i = 0; i < pairCount; ++i) {	a.deallocate(initial[i << 1]);	initial[i << 1] = null;	}	
before 

private void runSimple1to2Discard(BuddyAllocator a, int baseSize) {	LlapAllocatorBuffer[] initial = prepareSimpleFragmentedAllocator(a, baseSize, true);	LlapAllocatorBuffer[] after = allocate(a, 1, baseSize * 2, 1 + initial.length);	
after 

public static LlapAllocatorBuffer[] prepareSimpleFragmentedAllocator( BuddyAllocator a, int baseSize, boolean doIncRef) {	LlapAllocatorBuffer[] initial = allocate(a, 4, baseSize, 1, doIncRef);	checkInitialValues(initial, 0, 2);	a.deallocate(initial[1]);	a.deallocate(initial[3]);	
before 

private void runSmallBlockersDiscard(BuddyAllocator a, int baseSize, boolean deallocOneFirst, boolean deallocOneSecond) {	LlapAllocatorBuffer[] initial = prepareAllocatorWithSmallFragments( a, baseSize, deallocOneFirst, deallocOneSecond, true);	int bigAllocSize = baseSize * 4;	LlapAllocatorBuffer[] after = allocate(a, 1, bigAllocSize, 1 + initial.length);	
after 

tmp = allocate(a, 2, baseSize, offset + 1);	System.arraycopy(tmp, 0, initial, offset, 2);	if (deallocOneFirst) {	a.deallocate(initial[1]);	}	if (deallocOneSecond) {	a.deallocate(initial[5]);	}	a.deallocate(initial[0]);	a.deallocate(initial[3]);	
before 

public static LlapAllocatorBuffer[] allocate( BuddyAllocator a, int count, int size, int baseValue, boolean doIncRef) {	LlapAllocatorBuffer[] allocs = new LlapAllocatorBuffer[count];	try {	a.allocateMultiple(allocs, size);	} catch (AllocatorOutOfMemoryException ex) {	
failed to allocate of 

public static LlapAllocatorBuffer[] prepareCustomFragmentedAllocator( BuddyAllocator a, int[] sizes, int[] dealloc, boolean doIncRef) {	LlapAllocatorBuffer[] initial = new LlapAllocatorBuffer[sizes.length];	for (int i = 0; i < sizes.length; ++i) {	initial[i] = allocate(a, 1, sizes[i], i + 1, doIncRef)[0];	}	for (int i = 0; i < dealloc.length; ++i) {	a.deallocate(initial[dealloc[i]]);	initial[dealloc[i]] = null;	}	
before 

========================= hive sample_2129 =========================

try {	Oid mechOid = KerberosUtil.getOidInstance("GSS_KRB5_MECH_OID");	GSSManager manager = GSSManager.getInstance();	GSSName serverName = manager.createName("HTTP@" + server, GSSName.NT_HOSTBASED_SERVICE);	GSSContext gssContext = manager.createContext(serverName.canonicalize(mechOid), mechOid, null, GSSContext.DEFAULT_LIFETIME);	gssContext.requestMutualAuth(true);	gssContext.requestCredDeleg(true);	byte[] inToken = new byte[0];	byte[] outToken = gssContext.initSecContext(inToken, 0, inToken.length);	gssContext.dispose();	
got valid challenge for host 

========================= hive sample_12 =========================

file = new Path(args[i]);	}	}	if (file == null) {	LOG.error(usage);	System.exit(-1);	}	LOG.info("count = {}", count);	LOG.info("create = {}", create);	LOG.info("file = {}", file);	
finished 

========================= hive sample_662 =========================

public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx, Object... nodeOutputs) throws SemanticException {	FilterOperator filterOp = (FilterOperator) nd;	ExprNodeDesc predicate = filterOp.getConf().getPredicate();	ExprNodeDesc newPredicate = generateInClauses(predicate);	if (newPredicate != null) {	if (LOG.isDebugEnabled()) {	
generated new predicate with in clause 

public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx, Object... nodeOutputs) throws SemanticException {	ExprNodeGenericFuncDesc fd = getInExprNode((ExprNodeDesc)nd);	BEGIN : Early terminations for Partition Column Separator if (fd == null) {	if (LOG.isDebugEnabled()) {	
partition columns not separated for is not in operator 

public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx, Object... nodeOutputs) throws SemanticException {	ExprNodeGenericFuncDesc fd = getInExprNode((ExprNodeDesc)nd);	BEGIN : Early terminations for Partition Column Separator if (fd == null) {	if (LOG.isDebugEnabled()) {	}	return null;	}	List<ExprNodeDesc> children = fd.getChildren();	if (!(children.get(0) instanceof ExprNodeGenericFuncDesc) || (!(((ExprNodeGenericFuncDesc) children.get(0)).getGenericUDF() instanceof GenericUDFStruct))) {	if (LOG.isDebugEnabled()) {	
partition columns not separated for children size child expression 

return null;	}	List<ExprNodeDesc> children = fd.getChildren();	if (!(children.get(0) instanceof ExprNodeGenericFuncDesc) || (!(((ExprNodeGenericFuncDesc) children.get(0)).getGenericUDF() instanceof GenericUDFStruct))) {	if (LOG.isDebugEnabled()) {	}	return null;	}	if (!hasAtleastOneSubExprWithPartColOrVirtualColWithOneTableAlias(children.get(0))) {	if (LOG.isDebugEnabled()) {	
partition columns not separated for there are no expression containing partition columns in struct fields 

}	return null;	}	if (!hasAtleastOneSubExprWithPartColOrVirtualColWithOneTableAlias(children.get(0))) {	if (LOG.isDebugEnabled()) {	}	return null;	}	if (hasAllSubExprWithConstOrPartColOrVirtualColWithOneTableAlias(children.get(0))) {	if (LOG.isDebugEnabled()) {	
partition columns not separated for all fields are expressions containing constants or only partition columns coming from same table 

========================= hive sample_2980 =========================

public List<String> getColumnNames(Configuration conf) throws HiveJdbcDatabaseAccessException {	Connection conn = null;	PreparedStatement ps = null;	ResultSet rs = null;	try {	initializeDatabaseConnection(conf);	String sql = JdbcStorageConfigManager.getQueryToExecute(conf);	String metadataQuery = addLimitToQuery(sql, 1);	
query to execute is 

rs = ps.executeQuery();	ResultSetMetaData metadata = rs.getMetaData();	int numColumns = metadata.getColumnCount();	List<String> columnNames = new ArrayList<String>(numColumns);	for (int i = 0; i < numColumns; i++) {	columnNames.add(metadata.getColumnName(i + 1));	}	return columnNames;	}	catch (Exception e) {	
error while trying to get column names 

public int getTotalNumberOfRecords(Configuration conf) throws HiveJdbcDatabaseAccessException {	Connection conn = null;	PreparedStatement ps = null;	ResultSet rs = null;	try {	initializeDatabaseConnection(conf);	String sql = JdbcStorageConfigManager.getQueryToExecute(conf);	String countQuery = "SELECT COUNT(*) FROM (" + sql + ") tmptable";	
query to execute is 

initializeDatabaseConnection(conf);	String sql = JdbcStorageConfigManager.getQueryToExecute(conf);	String countQuery = "SELECT COUNT(*) FROM (" + sql + ") tmptable";	conn = dbcpDataSource.getConnection();	ps = conn.prepareStatement(countQuery);	rs = ps.executeQuery();	if (rs.next()) {	return rs.getInt(1);	}	else {	
the count query did not return any results 

return rs.getInt(1);	}	else {	throw new HiveJdbcDatabaseAccessException("Count query did not return any results.");	}	}	catch (HiveJdbcDatabaseAccessException he) {	throw he;	}	catch (Exception e) {	
caught exception while trying to get the number of records 

protected void cleanupResources(Connection conn, PreparedStatement ps, ResultSet rs) {	try {	if (rs != null) {	rs.close();	}	} catch (SQLException e) {	
caught exception during resultset cleanup 

if (rs != null) {	rs.close();	}	} catch (SQLException e) {	}	try {	if (ps != null) {	ps.close();	}	} catch (SQLException e) {	
caught exception during statement cleanup 

if (ps != null) {	ps.close();	}	} catch (SQLException e) {	}	try {	if (conn != null) {	conn.close();	}	} catch (SQLException e) {	
caught exception during connection cleanup 

protected Properties getConnectionPoolProperties(Configuration conf) throws Exception {	Properties dbProperties = getDefaultDBCPProperties();	Map<String, String> userProperties = conf.getValByRegex(DBCP_CONFIG_PREFIX + "\\.*");	if ((userProperties != null) && (!userProperties.isEmpty())) {	for (Entry<String, String> entry : userProperties.entrySet()) {	dbProperties.put(entry.getKey().replaceFirst(DBCP_CONFIG_PREFIX + "\\.", ""), entry.getValue());	}	}	Credentials credentials = UserGroupInformation.getCurrentUser().getCredentials();	if (credentials.getSecretKey(DBCP_PWD) != null) {	
found token in credentials 

========================= hive sample_1139 =========================

private long[] getMapJoinConversionInfo( JoinOperator joinOp, OptimizeSparkProcContext context) {	Set<Integer> bigTableCandidateSet = MapJoinProcessor.getBigTableCandidates(joinOp.getConf().getConds());	long maxSize = context.getConf().getLongVar( HiveConf.ConfVars.HIVECONVERTJOINNOCONDITIONALTASKTHRESHOLD);	int bigTablePosition = -1;	Statistics bigInputStat = null;	long totalSize = 0;	int pos = 0;	boolean bigTableFound = false;	boolean useTsStats = context.getConf().getBoolean(HiveConf.ConfVars.SPARK_USE_TS_STATS_FOR_MAPJOIN.varname, false);	if (useTsStats) {	
checking map join optimization for operator using ts stats 

int bigTablePosition = -1;	Statistics bigInputStat = null;	long totalSize = 0;	int pos = 0;	boolean bigTableFound = false;	boolean useTsStats = context.getConf().getBoolean(HiveConf.ConfVars.SPARK_USE_TS_STATS_FOR_MAPJOIN.varname, false);	if (useTsStats) {	for (Operator<? extends OperatorDesc> parentOp : joinOp.getParentOperators()) {	if (isBigTableBranch(parentOp)) {	if (bigTablePosition < 0 && bigTableCandidateSet.contains(pos) && !containUnionWithoutRS(parentOp.getParentOperators().get(0))) {	
found a big table branch with parent operator and position 

boolean bigTableFound = false;	boolean useTsStats = context.getConf().getBoolean(HiveConf.ConfVars.SPARK_USE_TS_STATS_FOR_MAPJOIN.varname, false);	if (useTsStats) {	for (Operator<? extends OperatorDesc> parentOp : joinOp.getParentOperators()) {	if (isBigTableBranch(parentOp)) {	if (bigTablePosition < 0 && bigTableCandidateSet.contains(pos) && !containUnionWithoutRS(parentOp.getParentOperators().get(0))) {	bigTablePosition = pos;	bigTableFound = true;	bigInputStat = new Statistics(0, Long.MAX_VALUE);	} else {	
cannot enable map join optimization for operator 

if (currInputStat == null) {	currInputStat = root.getStatistics().clone();	} else {	currInputStat.addBasicStats(root.getStatistics());	}	}	} else {	currInputStat = parentOp.getStatistics();	}	if (currInputStat == null) {	
couldn t get statistics from 

for (Operator<?> parentOp : parentBigTableOp.getParentOperators()) {	Set<SparkPartitionPruningSinkOperator> partitionPruningSinkOps = new HashSet<>();	for (Operator<?> childOp : parentOp.getChildOperators()) {	SparkPartitionPruningSinkOperator partitionPruningSinkOp = findPartitionPruningSinkOperator(childOp);	if (partitionPruningSinkOp != null) {	partitionPruningSinkOps.add(partitionPruningSinkOp);	}	}	for (SparkPartitionPruningSinkOperator partitionPruningSinkOp : partitionPruningSinkOps) {	OperatorUtils.removeBranch(partitionPruningSinkOp);	
disabling dynamic pruning for need to be removed together with reduce sink 

========================= hive sample_3104 =========================

skipRecursion = true;	switch (ast.getToken().getType()) {	case HiveParser.TOK_SELECTDI: qb.countSelDi();	case HiveParser.TOK_SELECT: qb.countSel();	qbp.setSelExprForClause(ctx_1.dest, ast);	int posn = 0;	if (((ASTNode) ast.getChild(0)).getToken().getType() == HiveParser.QUERY_HINT) {	ParseDriver pd = new ParseDriver();	String queryHintStr = ast.getChild(0).getText();	if (LOG.isDebugEnabled()) {	
query hint 

try {	table = this.getTableObjectByName(tableName);	} catch (HiveException ex) {	throw new SemanticException(ex);	}	try {	Partition parMetaData = db.getPartition(table, partition, false);	if (parMetaData != null) {	phase1Result = false;	skipRecursion = true;	
partition already exists so insert into overwrite skipped for partition 

throw new SemanticException(ex);	}	try {	Partition parMetaData = db.getPartition(table, partition, false);	if (parMetaData != null) {	phase1Result = false;	skipRecursion = true;	break;	}	} catch (HiveException e) {	
error while getting metadata 

Set<String> targetColumns = new HashSet<String>();	targetColumns.addAll(targetColNames);	if(targetColNames.size() != targetColumns.size()) {	throw new SemanticException(generateErrorMessage(tabColName, "Duplicate column name detected in " + fullTableName + " table schema specification"));	}	Table targetTable = null;	try {	targetTable = db.getTable(fullTableName, false);	}	catch (HiveException ex) {	
error processing hiveparser tok destination 

private void getMetaData(QB qb, ReadEntity parentInput) throws HiveException {	
get metadata for source tables 

}	}	tab.setTableSpec(ts);	qb.getParseInfo().addTableSpec(alias, ts);	}	ReadEntity parentViewInfo = PlanUtils.getParentViewInfo(getAliasId(alias, qb), viewAliasToInput);	if (!PlanUtils.isValuesTempTable(alias)) {	PlanUtils.addInput(inputs, new ReadEntity(tab, parentViewInfo, parentViewInfo == null),mergeIsDirect);	}	}	
get metadata for subqueries 

QBExpr qbexpr = qb.getSubqForAlias(alias);	getMetaData(qbexpr, newParentInput);	if (wasView) {	viewsExpanded.remove(viewsExpanded.size() - 1);	} else if (wasCTE) {	ctesExpanded.remove(ctesExpanded.size() - 1);	}	}	RowFormatParams rowFormatParams = new RowFormatParams();	StorageFormat storageFormat = new StorageFormat(conf);	
get metadata for destination tables 

ExprNodeConstantDesc c = (ExprNodeConstantDesc) filterCond;	if (Boolean.TRUE.equals(c.getValue())) {	return input;	}	if (ExprNodeDescUtils.isNullConstant(c)) {	filterCond = new ExprNodeConstantDesc(TypeInfoFactory.booleanTypeInfo, false);	}	}	Operator output = putOpInsertMap(OperatorFactory.getAndMakeChild( new FilterDesc(filterCond, false), new RowSchema( inputRR.getColumnInfos()), input), inputRR);	if (LOG.isDebugEnabled()) {	
created filter plan for row schema 

List<ExprNodeDesc> preds = new ArrayList<ExprNodeDesc>();	preds.add(f.getConf().getPredicate());	preds.add(filterPred);	f.getConf().setPredicate(ExprNodeDescUtils.mergePredicates(preds));	return input;	}	FilterDesc filterDesc = new FilterDesc(filterPred, false);	filterDesc.setGenerated(true);	Operator output = putOpInsertMap(OperatorFactory.getAndMakeChild(filterDesc, new RowSchema(inputRR.getColumnInfos()), input), inputRR);	if (LOG.isDebugEnabled()) {	
created filter plan for row schema 

private Operator<?> genSelectPlan(String dest, QB qb, Operator<?> input, Operator<?> inputForSelectStar) throws SemanticException {	ASTNode selExprList = qb.getParseInfo().getSelForClause(dest);	Operator<?> op = genSelectPlan(dest, selExprList, qb, input, inputForSelectStar, false);	if (LOG.isDebugEnabled()) {	
created select plan for clause 

private Operator<?> genSelectPlan(String dest, ASTNode selExprList, QB qb, Operator<?> input, Operator<?> inputForSelectStar, boolean outerLV) throws SemanticException {	if (LOG.isDebugEnabled()) {	
tree 

break;	case HiveParser.TOK_TABALIAS: assert (selExprChild.getChildCount() == 1);	udtfTableAlias = unescapeIdentifier(selExprChild.getChild(0) .getText());	qb.addAlias(udtfTableAlias);	unparseTranslator.addIdentifierTranslation((ASTNode) selExprChild .getChild(0));	break;	default: assert (false);	}	}	if (LOG.isDebugEnabled()) {	
udtf table alias is 

break;	case HiveParser.TOK_TABALIAS: assert (selExprChild.getChildCount() == 1);	udtfTableAlias = unescapeIdentifier(selExprChild.getChild(0) .getText());	qb.addAlias(udtfTableAlias);	unparseTranslator.addIdentifierTranslation((ASTNode) selExprChild .getChild(0));	break;	default: assert (false);	}	}	if (LOG.isDebugEnabled()) {	
udtf col aliases are 

}	Operator output = putOpInsertMap(OperatorFactory.getAndMakeChild( new SelectDesc(col_list, columnNames, selectStar), new RowSchema( out_rwsch.getColumnInfos()), input), out_rwsch);	output.setColumnExprMap(colExprMap);	if (isInTransform) {	output = genScriptPlan(trfm, qb, output);	}	if (isUDTF) {	output = genUDTFPlan(genericUDTF, udtfTableAlias, udtfColAliases, qb, output, outerLV);	}	if (LOG.isDebugEnabled()) {	
created select plan row schema 

}	enforceBucketing = true;	}	if (enforceBucketing) {	int maxReducers = conf.getIntVar(HiveConf.ConfVars.MAXREDUCERS);	if (conf.getIntVar(HiveConf.ConfVars.HADOOPNUMREDUCERS) > 0) {	maxReducers = conf.getIntVar(HiveConf.ConfVars.HADOOPNUMREDUCERS);	}	int numBuckets = dest_tab.getNumBuckets();	if (numBuckets > maxReducers) {	
numbuckets is and maxreducers is 

dest_path = new Path(dest_tab.getPath(), dpCtx.getSPPath());	}	boolean isNonNativeTable = dest_tab.isNonNative();	isMmTable = AcidUtils.isInsertOnlyTable(dest_tab.getParameters());	if (isNonNativeTable || isMmTable) {	queryTmpdir = dest_path;	} else {	queryTmpdir = ctx.getTempDirForFinalJobPath(dest_path);	}	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
create filesink w dest table specifying from 

destTableIsTransactional = AcidUtils.isTransactionalTable(dest_tab);	destTableIsFullAcid = AcidUtils.isAcidTable(dest_tab);	checkExternalTable(dest_tab);	Path tabPath = dest_tab.getPath();	Path partPath = dest_part.getDataLocation();	checkImmutableTable(qb, dest_tab, partPath, true);	dest_path = new Path(tabPath.toUri().getScheme(), tabPath.toUri() .getAuthority(), partPath.toUri().getPath());	isMmTable = AcidUtils.isInsertOnlyTable(dest_tab.getParameters());	queryTmpdir = isMmTable ? dest_path : ctx.getTempDirForFinalJobPath(dest_path);	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
create filesink w dest partition specifying from 

destTableIsTemporary = false;	}	if (isLocal) {	assert !isMmTable;	queryTmpdir = ctx.getMRTmpPath();	} else {	try {	Path qPath = FileUtils.makeQualified(dest_path, conf);	queryTmpdir = isMmTable ? qPath : ctx.getTempDirForFinalJobPath(qPath);	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
setting query directory from 

tableDesc.setWriter(fileSinkDesc);	}	if (SessionState.get().isHiveServerQuery() && null != table_desc && table_desc.getSerdeClassName().equalsIgnoreCase(ThriftJDBCBinarySerDe.class.getName()) && HiveConf.getBoolVar(conf,HiveConf.ConfVars.HIVE_SERVER2_THRIFT_RESULTSET_SERIALIZE_IN_TASKS)) {	fileSinkDesc.setIsUsingThriftJDBCBinarySerDe(true);	} else {	fileSinkDesc.setIsUsingThriftJDBCBinarySerDe(false);	}	Operator output = putOpInsertMap(OperatorFactory.getAndMakeChild( fileSinkDesc, fsRS, input), inputRR);	handleLineage(ltd, output);	if (LOG.isDebugEnabled()) {	
created filesink plan for clause dest path row schema 

fileSinkDesc.setMaterialization(destTableIsMaterialization);	if (lbCtx != null) {	lbCtx.processRowSkewedIndex(fsRS);	lbCtx.calculateSkewedValueSubDirList();	}	fileSinkDesc.setLbCtx(lbCtx);	fileSinkDesc.setStatsAggPrefix(fileSinkDesc.getDirName().toString());	if (!destTableIsMaterialization && HiveConf.getVar(conf, HIVESTATSDBCLASS).equalsIgnoreCase(StatDB.fs.name())) {	String statsTmpLoc = ctx.getTempDirForInterimJobPath(dest_path).toString();	fileSinkDesc.setStatsTmpDir(statsTmpLoc);	
set stats collection dir 

private void checkImmutableTable(QB qb, Table dest_tab, Path dest_path, boolean isPart) throws SemanticException {	if (!dest_tab.isImmutable() || !qb.getParseInfo().isInsertIntoTable( dest_tab.getDbName(), dest_tab.getTableName())) {	return;	}	try {	FileSystem fs = dest_path.getFileSystem(conf);	if (! org.apache.hadoop.hive.metastore.utils.FileUtils.isDirEmpty(fs,dest_path)){	
attempted write into an immutable table 

private void checkImmutableTable(QB qb, Table dest_tab, Path dest_path, boolean isPart) throws SemanticException {	if (!dest_tab.isImmutable() || !qb.getParseInfo().isInsertIntoTable( dest_tab.getDbName(), dest_tab.getTableName())) {	return;	}	try {	FileSystem fs = dest_path.getFileSystem(conf);	if (! org.apache.hadoop.hive.metastore.utils.FileUtils.isDirEmpty(fs,dest_path)){	throw new SemanticException( ErrorMsg.INSERT_INTO_IMMUTABLE_TABLE.getMsg(dest_tab.getTableName()));	}	} catch (IOException ioe) {	
error while trying to determine if immutable table partition has any data 

private void genAutoColumnStatsGatheringPipeline(QB qb, TableDesc table_desc, Map<String, String> partSpec, Operator curr, boolean isInsertInto) throws SemanticException {	String tableName = table_desc.getTableName();	Table table = null;	try {	table = db.getTable(tableName);	} catch (HiveException e) {	throw new SemanticException(e.getMessage());	}	
generate an operator pipeline to autogather column stats for table in query 

private void checkAcidConstraints(QB qb, TableDesc tableDesc, Table table) throws SemanticException {	
modifying config values for acid write 

private Operator genLimitPlan(String dest, QB qb, Operator input, int offset, int limit) throws SemanticException {	RowResolver inputRR = opParseCtx.get(input).getRowResolver();	LimitDesc limitDesc = new LimitDesc(offset, limit);	globalLimitCtx.setLastReduceLimitDesc(limitDesc);	Operator limitMap = putOpInsertMap(OperatorFactory.getAndMakeChild( limitDesc, new RowSchema(inputRR.getColumnInfos()), input), inputRR);	if (LOG.isDebugEnabled()) {	
created limitoperator plan for clause row schema 

if (!qbp.getDestToSortBy().isEmpty()) {	throw new SemanticException(ErrorMsg.UDTF_NO_SORT_BY.getMsg());	}	if (!qbp.getDestToClusterBy().isEmpty()) {	throw new SemanticException(ErrorMsg.UDTF_NO_CLUSTER_BY.getMsg());	}	if (!qbp.getAliasToLateralViews().isEmpty()) {	throw new SemanticException(ErrorMsg.UDTF_LATERAL_VIEW.getMsg());	}	if (LOG.isDebugEnabled()) {	
table alias col aliases 

}	JoinCondDesc[] joinCondns = new JoinCondDesc[join.getJoinCond().length];	for (int i = 0; i < join.getJoinCond().length; i++) {	JoinCond condn = join.getJoinCond()[i];	joinCondns[i] = new JoinCondDesc(condn);	}	JoinDesc desc = new JoinDesc(exprMap, outputColumnNames, join.getNoOuterJoin(), joinCondns, filterMap, joinKeys, null);	desc.setReversedExprs(reversedExprs);	desc.setFilterMap(join.getFilterMap());	if (join.getPostJoinFilters().size() != 0 && (!join.getNoOuterJoin() || !join.getNoSemiJoin() || HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_PUSH_RESIDUAL_INNER))) {	
generate join with post filtering conditions 

}	joinTree.setBaseSrc(baseSrc.toArray(new String[0]));	joinTree.setLeftAliases(leftAliases.toArray(new String[0]));	joinTree.setRightAliases(rightAliases.toArray(new String[0]));	JoinCond[] condn = new JoinCond[preserved.size()];	for (int i = 0; i < condn.length; i++) {	condn[i] = new JoinCond(preserved.get(i));	}	joinTree.setJoinCond(condn);	if ((qb.getParseInfo().getHints() != null) && !(conf.getVar(HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals("tez"))) {	
streamtable hint honored 

return result;	}	}	int curIdx = 0;	while(curIdx < args.getChildCount()) {	curIdx = parseSingleSemiJoinHint(args, curIdx, result);	}	}	}	if (LOG.isDebugEnabled()) {	
semijoin hint parsed 

String target = args.getChild(curIdx++).getText();	if (StringUtils.isNumeric(target)) {	throw new SemanticException("User provided bloom filter entries when target alias is " + "expected. target: " + target);	}	Integer number = null;	if (numEntriesLeft > 3) {	try {	number = Integer.parseInt(args.getChild(curIdx).getText());	curIdx++;	} catch (NumberFormatException e) {	
number format exception when parsing 

private Operator genBodyPlan(QB qb, Operator input, Map<String, Operator> aliasToOpInfo) throws SemanticException {	QBParseInfo qbp = qb.getParseInfo();	TreeSet<String> ks = new TreeSet<String>(qbp.getClauseNames());	Map<String, Operator<? extends OperatorDesc>> inputs = createInputForDests(qb, input, ks);	Operator curr = input;	List<List<String>> commonGroupByDestGroups = null;	if (conf.getBoolVar(HiveConf.ConfVars.HIVEMULTIGROUPBYSINGLEREDUCER)) {	try {	commonGroupByDestGroups = getCommonGroupByDestGroups(qb, inputs);	} catch (SemanticException e) {	
failed to group clauses by common spray keys 

} else {	curr = genGroupByPlanMapAggr2MR(dest, qb, curr);	}	} else if (conf.getBoolVar(HiveConf.ConfVars.HIVEGROUPBYSKEW)) {	curr = genGroupByPlan2MR(dest, qb, curr);	} else {	curr = genGroupByPlan1MR(dest, qb, curr);	}	}	if (LOG.isDebugEnabled()) {	
rr before gb after gb 

if (LOG.isDebugEnabled()) {	}	curr = genPostGroupByBodyPlan(curr, dest, qb, aliasToOpInfo, gbySource);	}	} else {	curr = genGroupByPlan1ReduceMultiGBY(commonGroupByDestGroup, qb, input, aliasToOpInfo);	}	}	}	if (LOG.isDebugEnabled()) {	
created body plan for query block 

Table tab = qb.getMetaData().getSrcForAlias(alias);	RowResolver rwsch;	TableScanOperator top = topOps.get(alias_id);	Map<String, String> properties = qb.getTabPropsForAlias(alias);	if (top == null) {	rwsch = new RowResolver();	try {	if (properties != null) {	for (Entry<String, String> prop : properties.entrySet()) {	if (tab.getSerdeParam(prop.getKey()) != null) {	
serde property in input query overrides stored serde property 

List<? extends StructField> fields = rowObjectInspector .getAllStructFieldRefs();	for (int i = 0; i < fields.size(); i++) {	ColumnInfo colInfo = new ColumnInfo(fields.get(i).getFieldName(), TypeInfoUtils.getTypeInfoFromObjectInspector(fields.get(i) .getFieldObjectInspector()), alias, false);	colInfo.setSkewedCol((isSkewedCol(alias, qb, fields.get(i) .getFieldName())) ? true : false);	rwsch.put(alias, fields.get(i).getFieldName(), colInfo);	}	} catch (SerDeException e) {	throw new RuntimeException(e);	}	for (FieldSchema part_col : tab.getPartCols()) {	
adding partition col 

break;	}	if (((ASTNode) sampleExprs.get(i).getChild(0)).getText() .equalsIgnoreCase(tabBucketCols.get(j))) {	colFound = true;	}	}	colsEqual = (colsEqual && colFound);	}	ts.setInputPruning((sampleExprs == null || sampleExprs.size() == 0 || colsEqual));	if ((sampleExprs == null || sampleExprs.size() == 0 || colsEqual) && (num == den || (den % numBuckets == 0 || numBuckets % den == 0))) {	
no need for sample filter 

}	colsEqual = (colsEqual && colFound);	}	ts.setInputPruning((sampleExprs == null || sampleExprs.size() == 0 || colsEqual));	if ((sampleExprs == null || sampleExprs.size() == 0 || colsEqual) && (num == den || (den % numBuckets == 0 || numBuckets % den == 0))) {	ExprNodeDesc samplePredicate = genSamplePredicate(ts, tabBucketCols, colsEqual, alias, rwsch, qb.getMetaData(), null);	FilterDesc filterDesc = new FilterDesc( samplePredicate, true, new SampleDesc(ts.getNumerator(), ts.getDenominator(), tabBucketCols, true));	filterDesc.setGenerated(true);	op = OperatorFactory.getAndMakeChild(filterDesc, new RowSchema(rwsch.getColumnInfos()), top);	} else {	
need sample filter 

if (!unsample) {	int numBuckets = tab.getNumBuckets();	if (numBuckets > 0) {	TableSample tsSample = new TableSample(1, numBuckets);	tsSample.setInputPruning(true);	qb.getParseInfo().setTabSample(alias, tsSample);	ExprNodeDesc samplePred = genSamplePredicate(tsSample, tab .getBucketCols(), true, alias, rwsch, qb.getMetaData(), null);	FilterDesc filterDesc = new FilterDesc(samplePred, true, new SampleDesc(tsSample.getNumerator(), tsSample .getDenominator(), tab.getBucketCols(), true));	filterDesc.setGenerated(true);	op = OperatorFactory.getAndMakeChild(filterDesc, new RowSchema(rwsch.getColumnInfos()), top);	
no need for sample filter 

qb.getParseInfo().setTabSample(alias, tsSample);	ExprNodeDesc samplePred = genSamplePredicate(tsSample, tab .getBucketCols(), true, alias, rwsch, qb.getMetaData(), null);	FilterDesc filterDesc = new FilterDesc(samplePred, true, new SampleDesc(tsSample.getNumerator(), tsSample .getDenominator(), tab.getBucketCols(), true));	filterDesc.setGenerated(true);	op = OperatorFactory.getAndMakeChild(filterDesc, new RowSchema(rwsch.getColumnInfos()), top);	} else {	int freq = conf.getIntVar(HiveConf.ConfVars.HIVETESTMODESAMPLEFREQ);	TableSample tsSample = new TableSample(1, freq);	tsSample.setInputPruning(false);	qb.getParseInfo().setTabSample(alias, tsSample);	
need sample filter 

ExprNodeDesc samplePred = genSamplePredicate(tsSample, null, false, alias, rwsch, qb.getMetaData(), randFunc);	FilterDesc filterDesc = new FilterDesc(samplePred, true);	filterDesc.setGenerated(true);	op = OperatorFactory.getAndMakeChild(filterDesc, new RowSchema(rwsch.getColumnInfos()), top);	}	}	}	}	Operator output = putOpInsertMap(op, rwsch);	if (LOG.isDebugEnabled()) {	
created table plan for 

private void setupStats(TableScanDesc tsDesc, QBParseInfo qbp, Table tab, String alias, RowResolver rwsch) throws SemanticException {	if (!qbp.isAnalyzeCommand() && qbp.getAnalyzeRewrite() == null) {	tsDesc.setGatherStats(false);	} else {	if (HiveConf.getVar(conf, HIVESTATSDBCLASS).equalsIgnoreCase(StatDB.fs.name())) {	String statsTmpLoc = ctx.getTempDirForInterimJobPath(tab.getPath()).toString();	
set stats collection dir 

Map<String, String> partSpec = tblSpec.getPartSpec();	if (partSpec != null) {	cols.addAll(partSpec.keySet());	tsDesc.setPartColumns(cols);	} else {	throw new SemanticException(ErrorMsg.NEED_PARTITION_SPECIFICATION.getMsg());	}	List<Partition> partitions = qbp.getTableSpec().partitions;	if (partitions != null) {	for (Partition partn : partitions) {	
xxx adding part 

}	}	pushJoinFilters(qb, qb.getQbJoinTree(), aliasToOpInfo);	srcOpInfo = genJoinPlan(qb, aliasToOpInfo);	} else {	srcOpInfo = aliasToOpInfo.values().iterator().next();	srcOpInfo = lastPTFOp != null ? lastPTFOp : srcOpInfo;	}	Operator bodyOpInfo = genBodyPlan(qb, srcOpInfo, aliasToOpInfo);	if (LOG.isDebugEnabled()) {	
created plan for query block 

String replacementText = null;	Table table = null;	try {	if (!tabNameToTabObject.containsKey(tabIdName)) {	table = db.getTable(tabIdName, true);	tabNameToTabObject.put(tabIdName, table);	} else {	table = tabNameToTabObject.get(tabIdName);	}	} catch (HiveException e) {	
table is not found in walkastmarktabref 

public void validate() throws SemanticException {	
validation start 

checkAcidTxnManager(tbl);	}	}	for (WriteEntity writeEntity : getOutputs()) {	WriteEntity.Type type = writeEntity.getType();	if (type == WriteEntity.Type.PARTITION || type == WriteEntity.Type.DUMMYPARTITION) {	String conflictingArchive;	try {	Partition usedp = writeEntity.getPartition();	Table tbl = usedp.getTable();	
validated 

conflictingArchive = ArchiveUtils .conflictingArchiveNameOrNull(db, tbl, usedp.getSpec());	} catch (HiveException e) {	throw new SemanticException(e);	}	if (conflictingArchive != null) {	String message = String.format("Insert conflict with existing archive: %s", conflictingArchive);	throw new SemanticException(message);	}	}	if (type != WriteEntity.Type.TABLE && type != WriteEntity.Type.PARTITION) {	
not validating writeentity because entity is neither table nor partition 

if (p != null) {	tbl = p.getTable();	} else {	tbl = inputPartition.getTable();	}	} catch (HiveException e) {	throw new SemanticException(e);	}	}	else {	
not a partition 

}	} catch (ClassNotFoundException e) {	LOG.warn("Could not verify InputFormat=" + storageFormat.getInputFormat() + " or OutputFormat=" + storageFormat.getOutputFormat() + "  for " + qualifiedTableName);	return retValue;	}	if(sortCols != null && !sortCols.isEmpty()) {	return retValue;	}	retValue.put(hive_metastoreConstants.TABLE_IS_TRANSACTIONAL, "true");	retValue.put(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES, TransactionalValidationListener.DEFAULT_TRANSACTIONAL_PROPERTY);	
automatically chose to make acid 

========================= hive sample_3554 =========================

public static PartitionExpressionProxy createExpressionProxy(Configuration conf) {	String className = MetastoreConf.getVar(conf, ConfVars.EXPRESSION_PROXY_CLASS);	try {	Class<? extends PartitionExpressionProxy> clazz = JavaUtils.getClass(className, PartitionExpressionProxy.class);	return JavaUtils.newInstance( clazz, new Class<?>[0], new Object[0]);	} catch (MetaException e) {	if (e.getMessage().matches(".* class not found")) {	return new DefaultPartitionExpressionProxy();	}	
error loading partitionexpressionproxy 

private static ExpressionTree makeExpressionTree(String filter) throws MetaException {	if (filter == null || filter.isEmpty()) {	return ExpressionTree.EMPTY_TREE;	}	
filter specified is 

private static ExpressionTree makeExpressionTree(String filter) throws MetaException {	if (filter == null || filter.isEmpty()) {	return ExpressionTree.EMPTY_TREE;	}	ExpressionTree tree = null;	try {	tree = getFilterParser(filter).tree;	} catch (MetaException ex) {	
unable to make the expression tree from expression string 

========================= hive sample_1990 =========================

static ReaderWithOffsets create(LineRecordReader sourceReader) {	if (isCompressedMethod == null) return new PassThruOffsetReader(sourceReader);	Boolean isCompressed = null;	try {	isCompressed = (Boolean)isCompressedMethod.invoke(sourceReader);	} catch (IllegalAccessException | IllegalArgumentException | InvocationTargetException e) {	
cannot check the reader for compression offsets not supported 

static ReaderWithOffsets create(LineRecordReader sourceReader) {	if (isCompressedMethod == null) return new PassThruOffsetReader(sourceReader);	Boolean isCompressed = null;	try {	isCompressed = (Boolean)isCompressedMethod.invoke(sourceReader);	} catch (IllegalAccessException | IllegalArgumentException | InvocationTargetException e) {	return new PassThruOffsetReader(sourceReader);	}	if (isCompressed) {	
reader is compressed offsets not supported 

========================= hive sample_2200 =========================

public void close(TaskAttemptContext context) throws IOException, InterruptedException {	Reporter reporter = InternalUtil.createReporter(context);	for (RecordWriter<? super WritableComparable<?>, ? super Writable> bwriter : baseDynamicWriters .values()) {	bwriter.close(reporter);	}	TaskCommitContextRegistry.getInstance().register(context, new TaskCommitContextRegistry.TaskCommitterProxy() {	public void abortTask(TaskAttemptContext context) throws IOException {	for (Map.Entry<String, OutputJobInfo> outputJobInfoEntry : dynamicOutputJobInfo.entrySet()) {	String dynKey = outputJobInfoEntry.getKey();	OutputJobInfo outputJobInfo = outputJobInfoEntry.getValue();	
aborting task attempt for 

for (Map.Entry<String, OutputJobInfo> outputJobInfoEntry : dynamicOutputJobInfo.entrySet()) {	String dynKey = outputJobInfoEntry.getKey();	OutputJobInfo outputJobInfo = outputJobInfoEntry.getValue();	baseDynamicCommitters.get(dynKey) .abortTask(dynamicContexts.get(dynKey));	}	}	public void commitTask(TaskAttemptContext context) throws IOException {	for (Map.Entry<String, OutputJobInfo> outputJobInfoEntry : dynamicOutputJobInfo.entrySet()) {	String dynKey = outputJobInfoEntry.getKey();	OutputJobInfo outputJobInfo = outputJobInfoEntry.getValue();	
committing task attempt for 

public void commitTask(TaskAttemptContext context) throws IOException {	for (Map.Entry<String, OutputJobInfo> outputJobInfoEntry : dynamicOutputJobInfo.entrySet()) {	String dynKey = outputJobInfoEntry.getKey();	OutputJobInfo outputJobInfo = outputJobInfoEntry.getValue();	TaskAttemptContext dynContext = dynamicContexts.get(dynKey);	OutputCommitter dynCommitter = baseDynamicCommitters.get(dynKey);	if (dynCommitter.needsTaskCommit(dynContext)) {	dynCommitter.commitTask(dynContext);	}	else {	
skipping committask for 

========================= hive sample_770 =========================

ArrayList<Integer> argList = new ArrayList<Integer>();	for (Integer arg : aggCall.getArgList()) {	argList.add(arg);	positions.add(arg);	}	argListList.add(argList);	argListSets.add(argList);	}	Util.permAssert(argListSets.size() > 0, "containsDistinctCall lied");	if (numCountDistinct > 1 && numCountDistinct == aggregate.getAggCallList().size() && aggregate.getGroupSet().isEmpty()) {	
trigger countdistinct rewrite numcountdistinct is 

========================= hive sample_2892 =========================

public static MapredContext init(boolean isMap, JobConf jobConf) {	MapredContext context = HiveConf.getVar(jobConf, ConfVars.HIVE_EXECUTION_ENGINE).equals("tez") ? new TezContext(isMap, jobConf) : new MapredContext(isMap, jobConf);	contexts.set(context);	if (logger.isDebugEnabled()) {	
mapredcontext initialized 

private void closeAll() {	for (Closeable eval : udfs) {	try {	eval.close();	} catch (IOException e) {	
hit error while closing udf 

========================= hive sample_3850 =========================

try {	db = metastoreClient.getDatabase(hivePrivObject.getDbname());	} catch (Exception e) {	throwGetObjErr(e, hivePrivObject);	}	if(db.getOwnerType() == PrincipalType.USER){	return userName.equals(db.getOwnerName());	} else if(db.getOwnerType() == PrincipalType.ROLE){	return curRoles.contains(db.getOwnerName());	} else {	
owner of database is of unsupported type 

public static RequiredPrivileges getPrivilegesFromFS(Path filePath, HiveConf conf, String userName) throws HiveAuthzPluginException {	RequiredPrivileges availPrivs = new RequiredPrivileges();	FileSystem fs;	try {	fs = FileSystem.get(filePath.toUri(), conf);	FileStatus[] fileMatches = fs.globStatus(filePath);	if ((fileMatches != null ) && (fileMatches.length > 1)){	
checking fs privileges for multiple files that matched 

if ((fileMatches != null ) && (fileMatches.length > 1)){	addPrivilegesFromFS(userName, availPrivs, fs, fileMatches, true);	} else {	FileStatus fileStatus = FileUtils.getFileStatusOrNull(fs, filePath);	boolean pickParent = (fileStatus == null);	if (pickParent){	fileStatus = FileUtils.getPathOrParentThatExists(fs, filePath.getParent());	}	Path path = fileStatus.getPath();	if (pickParent){	
checking fs privileges for parent path for nonexistent 

} else {	FileStatus fileStatus = FileUtils.getFileStatusOrNull(fs, filePath);	boolean pickParent = (fileStatus == null);	if (pickParent){	fileStatus = FileUtils.getPathOrParentThatExists(fs, filePath.getParent());	}	Path path = fileStatus.getPath();	if (pickParent){	addPrivilegesFromFS(userName, availPrivs, fs, fileStatus, false);	} else {	
checking fs privileges for path itself originally specified as 

private static Set<SQLPrivTypeGrant> getPrivilegesFromFS( String userName, FileSystem fs, FileStatus fileStatus, boolean recurse) throws Exception {	Set<SQLPrivTypeGrant> privs = new HashSet<SQLPrivTypeGrant>();	
checking fs privileges of user for recursively without recursion 

========================= hive sample_3153 =========================

if(name != null) {	executedTests.add(name);	if(failedOrErrored) {	failedTests.add(name);	}	}	}	}	});	} catch (Exception e) {	
error parsing file 

}	}	}	});	} catch (Exception e) {	} finally {	if(stream != null) {	try {	stream.close();	} catch (IOException e) {	
error closing file 

========================= hive sample_5592 =========================

}	if (evictor == null) return false;	long evicted = evictor.evictSomeBlocks(remainingToReserve);	if (evicted == 0) {	if (!waitForEviction) {	result = false;	break;	}	++badCallCount;	if (badCallCount == nextLog) {	
cannot evict blocks for calls cache full 

========================= hive sample_2168 =========================

}	outputObjInspector = ObjectInspectorFactory .getStandardStructObjectInspector(columnNames, outputFieldOIs);	outputRow = new ArrayList<Object>(columns);	for (int c = 0; c < columns; c++) {	outputRow.add(null);	}	needsTransform = new boolean[parents];	for (int p = 0; p < parents; p++) {	needsTransform[p] = (inputObjInspectors[p] != outputObjInspector);	if (LOG.isInfoEnabled() && needsTransform[p]) {	
union operator needs to transform row from parent from to 

========================= hive sample_4546 =========================

needCommonSetup = false;	}	if (needHashTableSetup) {	hashMap = (VectorMapJoinBytesHashMap) vectorMapJoinHashTable;	needHashTableSetup = false;	}	batchCounter++;	final int inputLogicalSize = batch.size;	if (inputLogicalSize == 0) {	if (LOG.isDebugEnabled()) {	
batch empty 

}	boolean someRowsFilteredOut =  false;	if (bigTableFilterExpressions.length > 0) {	for (VectorExpression ve : bigTableFilterExpressions) {	ve.evaluate(batch);	}	someRowsFilteredOut = (batch.size != inputLogicalSize);	if (LOG.isDebugEnabled()) {	if (batch.selectedInUse) {	if (inputSelectedInUse) {	
inputselected filtered batch selected 

boolean someRowsFilteredOut =  false;	if (bigTableFilterExpressions.length > 0) {	for (VectorExpression ve : bigTableFilterExpressions) {	ve.evaluate(batch);	}	someRowsFilteredOut = (batch.size != inputLogicalSize);	if (LOG.isDebugEnabled()) {	if (batch.selectedInUse) {	if (inputSelectedInUse) {	} else {	
inputlogicalsize filtered batch selected 

joinResult = JoinUtil.JoinResult.NOMATCH;	} else if (!joinColVector.noNulls && joinColVector.isNull[0]) {	joinResult = JoinUtil.JoinResult.NOMATCH;	} else {	byte[] keyBytes = vector[0];	int keyStart = start[0];	int keyLength = length[0];	joinResult = hashMap.lookup(keyBytes, keyStart, keyLength, hashMapResults[0]);	}	if (LOG.isDebugEnabled()) {	
batch repeated joinresult 

byte[] keyBytes = vector[0];	int keyStart = start[0];	int keyLength = length[0];	joinResult = hashMap.lookup(keyBytes, keyStart, keyLength, hashMapResults[0]);	}	if (LOG.isDebugEnabled()) {	}	finishOuterRepeated(batch, joinResult, hashMapResults[0], someRowsFilteredOut, inputSelectedInUse, inputLogicalSize);	} else {	if (LOG.isDebugEnabled()) {	
batch non repeated 

switch (saveJoinResult) {	case MATCH: hashMapResultCount++;	equalKeySeriesCount++;	break;	case SPILL: hashMapResultCount++;	break;	case NOMATCH: break;	}	}	if (LOG.isDebugEnabled()) {	
batch allmatchs equalkeyserieshashmapresultindices equalkeyseriesallmatchindices equalkeyseriesissinglevalue equalkeyseriesduplicatecounts atleastonenonmatch inputselectedinuse inputlogicalsize spills spillhashmapresultindices hashmapresults 

========================= hive sample_4116 =========================

trace.logTreeReaderNextVector(idx);	ColumnVector cv = cvb.cols[idx];	cv.noNulls = true;	cv.reset();	cv.ensureSize(batchSize, false);	reader.nextVector(cv, null, batchSize);	}	downstreamConsumer.consumeData(cvb);	counters.incrCounter(LlapIOCounters.ROWS_EMITTED, batchSize);	}	
done with decode 

========================= hive sample_2187 =========================

public void testDatabaseOps() throws MetaException, InvalidObjectException, NoSuchObjectException {	Database db1 = new Database(DB1, "description", "locationurl", null);	Database db2 = new Database(DB2, "description", "locationurl", null);	objectStore.createDatabase(db1);	objectStore.createDatabase(db2);	List<String> databases = objectStore.getAllDatabases();	
databases 

========================= hive sample_1528 =========================

public DirSearch getInstance(HiveConf conf, String principal, String password) throws AuthenticationException {	try {	DirContext ctx = createDirContext(conf, principal, password);	return new LdapSearch(conf, ctx);	} catch (NamingException e) {	
could not connect to the ldap server authentication failed for 

private static DirContext createDirContext(HiveConf conf, String principal, String password) throws NamingException {	Hashtable<String, Object> env = new Hashtable<String, Object>();	String ldapUrl = conf.getVar(HiveConf.ConfVars.HIVE_SERVER2_PLAIN_LDAP_URL);	env.put(Context.INITIAL_CONTEXT_FACTORY, "com.sun.jndi.ldap.LdapCtxFactory");	env.put(Context.PROVIDER_URL, ldapUrl);	env.put(Context.SECURITY_AUTHENTICATION, "simple");	env.put(Context.SECURITY_CREDENTIALS, password);	env.put(Context.SECURITY_PRINCIPAL, principal);	
connecting using principal to ldap url 

========================= hive sample_2304 =========================

protected int execute(DriverContext driverContext) {	try {	TableExport.Paths exportPaths = new TableExport.Paths(work.getAstRepresentationForErrorMsg(), work.getExportRootDir(), conf, false);	Hive db = getHive();	
exporting data to 

protected int execute(DriverContext driverContext) {	try {	TableExport.Paths exportPaths = new TableExport.Paths(work.getAstRepresentationForErrorMsg(), work.getExportRootDir(), conf, false);	Hive db = getHive();	TableExport tableExport = new TableExport( exportPaths, work.getTableSpec(), work.getReplicationSpec(), db, null, conf );	if (!tableExport.write()) {	throw new SemanticException(ErrorMsg.EXIM_FOR_NON_NATIVE.getMsg());	}	} catch (Exception e) {	
failed 

========================= hive sample_3903 =========================

public static IMetaStoreSchemaInfo get(Configuration conf) {	String hiveHome = System.getenv("HIVE_HOME");	if (hiveHome == null) {	
hive home is not set using current directory instead 

public static IMetaStoreSchemaInfo get(Configuration conf, String hiveHome, String dbType) {	String className = MetastoreConf.getVar(conf, MetastoreConf.ConfVars.SCHEMA_INFO_CLASS);	Class<?> clasz;	try {	clasz = conf.getClassByName(className);	} catch (ClassNotFoundException e) {	
unable to load class 

clasz = conf.getClassByName(className);	} catch (ClassNotFoundException e) {	throw new IllegalArgumentException(e);	}	Constructor<?> constructor;	try {	constructor = clasz.getConstructor(String.class, String.class);	constructor.setAccessible(true);	return (IMetaStoreSchemaInfo) constructor.newInstance(hiveHome, dbType);	} catch (NoSuchMethodException | InstantiationException | IllegalAccessException | IllegalArgumentException | InvocationTargetException e) {	
unable to create instance of class 

========================= hive sample_1894 =========================

public static void processSkewJoin(JoinOperator joinOp, Task<? extends Serializable> currTask, ReduceWork reduceWork, ParseContext parseCtx) throws SemanticException {	SparkWork currentWork = ((SparkTask) currTask).getWork();	if (currentWork.getChildren(reduceWork).size() > 0) {	
skip runtime skew join as the reducework has child work and hasn t been split 

private static void setMemUsage(MapJoinOperator mapJoinOp, Task<? extends Serializable> task, ParseContext parseContext) {	MapJoinResolver.LocalMapJoinProcCtx context = new MapJoinResolver.LocalMapJoinProcCtx(task, parseContext);	try {	new LocalMapJoinProcFactory.LocalMapJoinProcessor().hasGroupBy(mapJoinOp, context);	} catch (Exception e) {	
error setting memory usage 

========================= hive sample_3000 =========================

Schema fileSchema = recordWritable.getFileSchema();	UID recordReaderId = recordWritable.getRecordReaderID();	if(!noEncodingNeeded.contains(recordReaderId)) {	SchemaReEncoder reEncoder = null;	if(reEncoderCache.containsKey(recordReaderId)) {	reEncoder = reEncoderCache.get(recordReaderId);	} else if (!r.getSchema().equals(readerSchema)) {	reEncoder = new SchemaReEncoder(r.getSchema(), readerSchema);	reEncoderCache.put(recordReaderId, reEncoder);	} else{	
adding new valid rrid 

if(reEncoderCache.containsKey(recordReaderId)) {	reEncoder = reEncoderCache.get(recordReaderId);	} else if (!r.getSchema().equals(readerSchema)) {	reEncoder = new SchemaReEncoder(r.getSchema(), readerSchema);	reEncoderCache.put(recordReaderId, reEncoder);	} else{	noEncodingNeeded.add(recordReaderId);	}	if(reEncoder != null) {	if (!warnedOnce) {	
received different schemas have to re encode size id 

========================= hive sample_5300 =========================

public void testRW() throws Exception {	Configuration conf = new Configuration();	for (Pair<Properties, HCatRecord> e : getData()) {	Properties tblProps = e.first;	HCatRecord r = e.second;	HCatRecordSerDe hrsd = new HCatRecordSerDe();	SerDeUtils.initializeSerDe(hrsd, conf, tblProps, null);	JsonSerDe jsde = new JsonSerDe();	SerDeUtils.initializeSerDe(jsde, conf, tblProps, null);	
orig 

public void testRW() throws Exception {	Configuration conf = new Configuration();	for (Pair<Properties, HCatRecord> e : getData()) {	Properties tblProps = e.first;	HCatRecord r = e.second;	HCatRecordSerDe hrsd = new HCatRecordSerDe();	SerDeUtils.initializeSerDe(hrsd, conf, tblProps, null);	JsonSerDe jsde = new JsonSerDe();	SerDeUtils.initializeSerDe(jsde, conf, tblProps, null);	Writable s = hrsd.serialize(r, hrsd.getObjectInspector());	
one 

HCatRecordSerDe hrsd = new HCatRecordSerDe();	SerDeUtils.initializeSerDe(hrsd, conf, tblProps, null);	JsonSerDe jsde = new JsonSerDe();	SerDeUtils.initializeSerDe(jsde, conf, tblProps, null);	Writable s = hrsd.serialize(r, hrsd.getObjectInspector());	Object o1 = hrsd.deserialize(s);	StringBuilder msg = new StringBuilder();	boolean isEqual = HCatDataCheckUtil.recordsEqual(r, (HCatRecord) o1);	assertTrue(msg.toString(), isEqual);	Writable s2 = jsde.serialize(o1, hrsd.getObjectInspector());	
two 

SerDeUtils.initializeSerDe(hrsd, conf, tblProps, null);	JsonSerDe jsde = new JsonSerDe();	SerDeUtils.initializeSerDe(jsde, conf, tblProps, null);	Writable s = hrsd.serialize(r, hrsd.getObjectInspector());	Object o1 = hrsd.deserialize(s);	StringBuilder msg = new StringBuilder();	boolean isEqual = HCatDataCheckUtil.recordsEqual(r, (HCatRecord) o1);	assertTrue(msg.toString(), isEqual);	Writable s2 = jsde.serialize(o1, hrsd.getObjectInspector());	Object o2 = jsde.deserialize(s2);	
deserialized two 

Properties tblProps = e.first;	HCatRecord r = e.second;	Properties internalTblProps = new Properties();	for (Map.Entry pe : tblProps.entrySet()) {	if (!pe.getKey().equals(serdeConstants.LIST_COLUMNS)) {	internalTblProps.put(pe.getKey(), pe.getValue());	} else {	internalTblProps.put(pe.getKey(), getInternalNames((String) pe.getValue()));	}	}	
orig tbl props 

Properties tblProps = e.first;	HCatRecord r = e.second;	Properties internalTblProps = new Properties();	for (Map.Entry pe : tblProps.entrySet()) {	if (!pe.getKey().equals(serdeConstants.LIST_COLUMNS)) {	internalTblProps.put(pe.getKey(), pe.getValue());	} else {	internalTblProps.put(pe.getKey(), getInternalNames((String) pe.getValue()));	}	}	
modif tbl props 

if (!pe.getKey().equals(serdeConstants.LIST_COLUMNS)) {	internalTblProps.put(pe.getKey(), pe.getValue());	} else {	internalTblProps.put(pe.getKey(), getInternalNames((String) pe.getValue()));	}	}	JsonSerDe wjsd = new JsonSerDe();	SerDeUtils.initializeSerDe(wjsd, conf, internalTblProps, null);	JsonSerDe rjsd = new JsonSerDe();	SerDeUtils.initializeSerDe(rjsd, conf, tblProps, null);	
orig 

internalTblProps.put(pe.getKey(), pe.getValue());	} else {	internalTblProps.put(pe.getKey(), getInternalNames((String) pe.getValue()));	}	}	JsonSerDe wjsd = new JsonSerDe();	SerDeUtils.initializeSerDe(wjsd, conf, internalTblProps, null);	JsonSerDe rjsd = new JsonSerDe();	SerDeUtils.initializeSerDe(rjsd, conf, tblProps, null);	Writable s = wjsd.serialize(r, wjsd.getObjectInspector());	
one 

} else {	internalTblProps.put(pe.getKey(), getInternalNames((String) pe.getValue()));	}	}	JsonSerDe wjsd = new JsonSerDe();	SerDeUtils.initializeSerDe(wjsd, conf, internalTblProps, null);	JsonSerDe rjsd = new JsonSerDe();	SerDeUtils.initializeSerDe(rjsd, conf, tblProps, null);	Writable s = wjsd.serialize(r, wjsd.getObjectInspector());	Object o1 = wjsd.deserialize(s);	
deserialized one 

internalTblProps.put(pe.getKey(), getInternalNames((String) pe.getValue()));	}	}	JsonSerDe wjsd = new JsonSerDe();	SerDeUtils.initializeSerDe(wjsd, conf, internalTblProps, null);	JsonSerDe rjsd = new JsonSerDe();	SerDeUtils.initializeSerDe(rjsd, conf, tblProps, null);	Writable s = wjsd.serialize(r, wjsd.getObjectInspector());	Object o1 = wjsd.deserialize(s);	Object o2 = rjsd.deserialize(s);	
deserialized two 

========================= hive sample_674 =========================

private List<QFileTestBatch> createQFileTestBatches(String driver, String queryFilesProperty, File directory, int batchSize, boolean isParallel, Set<String> excluded, Set<String> included, Set<String> isolated) {	
create batches for 

for(File test : checkNotNull(directory.listFiles(), directory.getAbsolutePath())) {	String testName = test.getName();	if(test.isFile() && testName.endsWith(".q") && (included.isEmpty() || included.contains(testName))) {	qFileTestNames.add(testName);	}	}	List<QFileTestBatch> testBatches = Lists.newArrayList();	List<String> testBatch = Lists.newArrayList();	for(final String test : qFileTestNames) {	if(excluded.contains(test)) {	
exlcuding test 

String testName = test.getName();	if(test.isFile() && testName.endsWith(".q") && (included.isEmpty() || included.contains(testName))) {	qFileTestNames.add(testName);	}	}	List<QFileTestBatch> testBatches = Lists.newArrayList();	List<String> testBatch = Lists.newArrayList();	for(final String test : qFileTestNames) {	if(excluded.contains(test)) {	} else if(isolated.contains(test)) {	
executing isolated test 

Map<String, String> propFiles = context.getSubProperties("qFileTests.propertyFiles.");	Map<String, Properties> propertyMap = new HashMap<String, Properties>();	for (String propFile : propFiles.keySet()) {	Properties properties = new Properties();	String path = sourceDirectory + File.separator + propFiles.get(propFile);	FileInputStream fis = null;	try {	fis = new FileInputStream(path);	properties.load(fis);	} catch (IOException e) {	
error processing qtest property file 

throw new IllegalArgumentException("Error processing Qtest property file: " + path);	} finally {	try {	if (fis != null) {	fis.close();	}	} catch (IOException e) {	}	}	propertyMap.put(propFile, properties);	
loaded qtest property file 

========================= hive sample_5615 =========================

}	int size = map.size();	LazyBinaryUtils.writeVInt(byteStream, size);	int b = 0;	byte nullByte = 0;	for (Map.Entry<?, ?> entry : map.entrySet()) {	if (null != entry.getKey()) {	nullByte |= 1 << (b % 8);	} else if (warnedOnceNullMapKey != null) {	if (!warnedOnceNullMapKey.value) {	
null map key encountered ignoring similar problems 

========================= hive sample_5245 =========================

return result;	}	public static class CreateHelper {	private DiskRangeList tail = null, head;	public DiskRangeList getTail() {	return tail;	}	public void addOrMerge(long offset, long end, boolean doMerge, boolean doLogNew) {	if (doMerge && tail != null && tail.merge(offset, end)) return;	if (doLogNew) {	
creating new range last range which can include some previous adds was 

========================= hive sample_1206 =========================

public void onFailure(Throwable t) {	
workload management fatal error 

private void runWmThread() {	while (true) {	EventState currentEvents = null;	currentLock.lock();	try {	while (!hasChanges) {	try {	hasChangesCondition.await(1, TimeUnit.SECONDS);	} catch (InterruptedException e) {	
wm thread was interrupted and will now exit 

return;	}	}	hasChanges = false;	currentEvents = current;	current = (currentEvents == one) ? two : one;	} finally {	currentLock.unlock();	}	try {	
processing current events 

currentEvents = current;	current = (currentEvents == one) ? two : one;	} finally {	currentLock.unlock();	}	try {	processCurrentEvents(currentEvents, syncWork);	scheduleWork(syncWork);	updateSessionTriggerProvidersOnMasterThread();	} catch (InterruptedException ex) {	
wm thread was interrupted and will now exit 

} finally {	currentLock.unlock();	}	try {	processCurrentEvents(currentEvents, syncWork);	scheduleWork(syncWork);	updateSessionTriggerProvidersOnMasterThread();	} catch (InterruptedException ex) {	return;	} catch (Exception | AssertionError ex) {	
wm thread encountered an error but will attempt to continue 

private void scheduleWork(WmThreadSyncWork context) {	for (KillQueryContext killCtx : context.toKillQuery.values()) {	final WmTezSession toKill = killCtx.session;	final String reason = killCtx.reason;	
killing query for 

private void scheduleWork(WmThreadSyncWork context) {	for (KillQueryContext killCtx : context.toKillQuery.values()) {	final WmTezSession toKill = killCtx.session;	final String reason = killCtx.reason;	workPool.submit(() -> {	String queryId = toKill.getQueryId();	KillQuery kq = toKill.getKillQuery();	try {	if (kq != null && queryId != null) {	WmEvent wmEvent = new WmEvent(WmEvent.EventType.KILL);	
invoking killquery for 

String queryId = toKill.getQueryId();	KillQuery kq = toKill.getKillQuery();	try {	if (kq != null && queryId != null) {	WmEvent wmEvent = new WmEvent(WmEvent.EventType.KILL);	try {	kq.killQuery(queryId, reason);	addKillQueryResult(toKill, true);	killCtx.killSessionFuture.set(true);	wmEvent.endEvent(toKill);	
killed 

try {	if (kq != null && queryId != null) {	WmEvent wmEvent = new WmEvent(WmEvent.EventType.KILL);	try {	kq.killQuery(queryId, reason);	addKillQueryResult(toKill, true);	killCtx.killSessionFuture.set(true);	wmEvent.endEvent(toKill);	return;	} catch (HiveException ex) {	
failed to kill will try to restart am instead 

WmEvent wmEvent = new WmEvent(WmEvent.EventType.KILL);	try {	kq.killQuery(queryId, reason);	addKillQueryResult(toKill, true);	killCtx.killSessionFuture.set(true);	wmEvent.endEvent(toKill);	return;	} catch (HiveException ex) {	}	} else {	
will queue restart for queryid killquery 

} else {	}	} finally {	toKill.setQueryId(null);	}	addKillQueryResult(toKill, false);	});	}	context.toKillQuery.clear();	for (final WmTezSession toRestart : context.toRestartInUse) {	
replacing with a new session 

}	context.toKillQuery.clear();	for (final WmTezSession toRestart : context.toRestartInUse) {	toRestart.setQueryId(null);	workPool.submit(() -> {	try {	WmEvent wmEvent = new WmEvent(WmEvent.EventType.RESTART);	tezAmPool.replaceSession(toRestart);	wmEvent.endEvent(toRestart);	} catch (Exception ex) {	
failed to restart an old session ignoring 

try {	WmEvent wmEvent = new WmEvent(WmEvent.EventType.RESTART);	tezAmPool.replaceSession(toRestart);	wmEvent.endEvent(toRestart);	} catch (Exception ex) {	}	});	}	context.toRestartInUse.clear();	for (final WmTezSession toDestroy : context.toDestroyNoRestart) {	
closing without restart 

});	}	context.toRestartInUse.clear();	for (final WmTezSession toDestroy : context.toDestroyNoRestart) {	workPool.submit(() -> {	try {	WmEvent wmEvent = new WmEvent(WmEvent.EventType.DESTROY);	toDestroy.close(false);	wmEvent.endEvent(toDestroy);	} catch (Exception ex) {	
failed to close an old session ignoring 

try {	WmEvent wmEvent = new WmEvent(WmEvent.EventType.DESTROY);	toDestroy.close(false);	wmEvent.endEvent(toDestroy);	} catch (Exception ex) {	}	});	}	context.toDestroyNoRestart.clear();	for (final Path path : context.pathsToDelete) {	
deleting 

} catch (Exception ex) {	}	});	}	context.toDestroyNoRestart.clear();	for (final Path path : context.pathsToDelete) {	workPool.submit(() -> {	try {	path.getFileSystem(conf).delete(path, true);	} catch (Exception ex) {	
failed to delete an old path ignoring 

private void processCurrentEvents(EventState e, WmThreadSyncWork syncWork) throws Exception {	HashSet<String> poolsToRedistribute = new HashSet<>();	for (SessionInitContext sw : e.initResults) {	handleInitResultOnMasterThread(sw, syncWork, poolsToRedistribute);	}	e.initResults.clear();	for (Map.Entry<WmTezSession, Boolean> entry : e.killQueryResults.entrySet()) {	WmTezSession killQuerySession = entry.getKey();	boolean killResult = entry.getValue();	
processing killquery for success failure 

HashSet<String> poolsToRedistribute = new HashSet<>();	for (SessionInitContext sw : e.initResults) {	handleInitResultOnMasterThread(sw, syncWork, poolsToRedistribute);	}	e.initResults.clear();	for (Map.Entry<WmTezSession, Boolean> entry : e.killQueryResults.entrySet()) {	WmTezSession killQuerySession = entry.getKey();	boolean killResult = entry.getValue();	KillQueryContext killCtx = killQueryInProgress.get(killQuerySession);	if (killCtx == null) {	
internal error cannot find the context for killing 

boolean killResult = entry.getValue();	KillQueryContext killCtx = killQueryInProgress.get(killQuerySession);	if (killCtx == null) {	continue;	}	killCtx.handleKillQueryCallback(!killResult);	}	e.killQueryResults.clear();	for (WmTezSession sessionToDestroy : e.toDestroy) {	if (e.toReturn.remove(sessionToDestroy)) {	
the session was both destroyed and returned by the user destroying 

KillQueryContext killCtx = killQueryInProgress.get(killQuerySession);	if (killCtx == null) {	continue;	}	killCtx.handleKillQueryCallback(!killResult);	}	e.killQueryResults.clear();	for (WmTezSession sessionToDestroy : e.toDestroy) {	if (e.toReturn.remove(sessionToDestroy)) {	}	
destroying 

for (WmTezSession sessionToDestroy : e.toDestroy) {	if (e.toReturn.remove(sessionToDestroy)) {	}	RemoveSessionResult rr = handleReturnedInUseSessionOnMasterThread( e, sessionToDestroy, poolsToRedistribute, false);	if (rr == RemoveSessionResult.OK || rr == RemoveSessionResult.NOT_FOUND) {	syncWork.toRestartInUse.add(sessionToDestroy);	}	}	e.toDestroy.clear();	for (WmTezSession sessionToReturn: e.toReturn) {	
returning 

}	break;	case NOT_FOUND: syncWork.toRestartInUse.add(sessionToReturn);	break;	case IGNORE: break;	default: throw new AssertionError("Unknown state " + rr);	}	}	e.toReturn.clear();	for (Map.Entry<WmTezSession, SettableFuture<WmTezSession>> entry : e.toReopen.entrySet()) {	
reopening 

}	}	e.toReturn.clear();	for (Map.Entry<WmTezSession, SettableFuture<WmTezSession>> entry : e.toReopen.entrySet()) {	handeReopenRequestOnMasterThread( e, entry.getKey(), entry.getValue(), poolsToRedistribute, syncWork);	}	e.toReopen.clear();	for (Map.Entry<WmTezSession, Integer> entry : e.updateErrors.entrySet()) {	WmTezSession sessionWithUpdateError = entry.getKey();	int failedEndpointVersion = entry.getValue();	
update failed for 

}	e.toReopen.clear();	for (Map.Entry<WmTezSession, Integer> entry : e.updateErrors.entrySet()) {	WmTezSession sessionWithUpdateError = entry.getKey();	int failedEndpointVersion = entry.getValue();	handleUpdateErrorOnMasterThread( sessionWithUpdateError, failedEndpointVersion, e.toReuse, syncWork, poolsToRedistribute);	}	e.updateErrors.clear();	boolean hasRequeues = false;	if (e.resourcePlanToApply != null || e.doClearResourcePlan) {	
applying new resource plan 

}	e.resourcePlanToApply = null;	e.doClearResourcePlan = false;	Map<WmTezSession, WmEvent> recordMoveEvents = new HashMap<>();	for (MoveSession moveSession : e.moveSessions) {	handleMoveSessionOnMasterThread(moveSession, syncWork, poolsToRedistribute, e.toReuse, recordMoveEvents);	}	e.moveSessions.clear();	GetRequest req;	while ((req = e.getRequests.pollFirst()) != null) {	
processing a new get request from 

}	e.toReuse.clear();	Iterator<KillQueryContext> iter = killQueryInProgress.values().iterator();	while (iter.hasNext()) {	KillQueryContext ctx = iter.next();	KillQueryResult kr = ctx.process();	switch (kr) {	case IN_PROGRESS: continue;	case OK: {	iter.remove();	
kill query succeeded returning to the pool 

if (ctx.session.getWmContext() != null && ctx.session.getWmContext().isQueryCompleted()) {	ctx.session.resolveReturnFuture();	}	wmEvent.endEvent(ctx.session);	}	break;	}	case RESTART_REQUIRED: {	iter.remove();	ctx.killSessionFuture.set(true);	
kill query failed restarting 

case RESTART_REQUIRED: {	iter.remove();	ctx.killSessionFuture.set(true);	syncWork.toRestartInUse.add(ctx.session);	break;	}	default: throw new AssertionError("Unknown state " + kr);	}	}	if (e.hasClusterStateChanged) {	
processing a cluster state change 

}	default: throw new AssertionError("Unknown state " + kr);	}	}	if (e.hasClusterStateChanged) {	poolsToRedistribute.addAll(pools.keySet());	e.hasClusterStateChanged = false;	}	for (String poolName : poolsToRedistribute) {	if (LOG.isDebugEnabled()) {	
processing changes for pool 

poolsToRedistribute.addAll(pools.keySet());	e.hasClusterStateChanged = false;	}	for (String poolName : poolsToRedistribute) {	if (LOG.isDebugEnabled()) {	}	processPoolChangesOnMasterThread(poolName, hasRequeues, syncWork);	}	for (KillQueryContext killCtx : syncWork.toKillQuery.values()) {	if (killQueryInProgress.put(killCtx.session, killCtx) != null) {	
one query killed several times internal error 

private void handleMoveSessionOnMasterThread(final MoveSession moveSession, final WmThreadSyncWork syncWork, final HashSet<String> poolsToRedistribute, final Map<WmTezSession, GetRequest> toReuse, final Map<WmTezSession, WmEvent> recordMoveEvents) {	String destPoolName = moveSession.destPool;	
handling move session event 

WmEvent moveEvent = new WmEvent(WmEvent.EventType.MOVE);	RemoveSessionResult rr = checkAndRemoveSessionFromItsPool( moveSession.srcSession, poolsToRedistribute, true);	if (rr == RemoveSessionResult.OK) {	if (capacityAvailable(destPoolName)) {	Boolean added = checkAndAddSessionToAnotherPool( moveSession.srcSession, destPoolName, poolsToRedistribute);	if (added != null && added) {	moveSession.future.set(true);	recordMoveEvents.put(moveSession.srcSession, moveEvent);	return;	} else {	
failed to move session session is not added to destination 

recordMoveEvents.put(moveSession.srcSession, moveEvent);	return;	} else {	}	} else {	WmTezSession session = moveSession.srcSession;	KillQueryContext killQueryContext = new KillQueryContext(session, "Destination pool " + destPoolName + " is full. Killing query.");	resetAndQueueKill(syncWork.toKillQuery, killQueryContext, toReuse);	}	} else {	
failed to move session session is not removed from its pool 

} else {	}	} else {	WmTezSession session = moveSession.srcSession;	KillQueryContext killQueryContext = new KillQueryContext(session, "Destination pool " + destPoolName + " is full. Killing query.");	resetAndQueueKill(syncWork.toKillQuery, killQueryContext, toReuse);	}	} else {	}	} else {	
validation failed for move session invalid move or session pool got removed 

}	assert sw.state == SessionInitState.DONE;	session = sw.session;	sw.session = null;	} finally {	sw.lock.unlock();	}	LOG.info("Processing " + ((session == null) ? "failed" : "successful") + " initialization result for pool " + sw.poolName);	PoolState pool = pools.get(sw.poolName);	if (pool == null || !pool.initializingSessions.remove(sw)) {	
cannot remove initializing session from the pool internal error 

}	LOG.info("Processing " + ((session == null) ? "failed" : "successful") + " initialization result for pool " + sw.poolName);	PoolState pool = pools.get(sw.poolName);	if (pool == null || !pool.initializingSessions.remove(sw)) {	}	poolsToRedistribute.add(sw.poolName);	if (session != null) {	if (pool != null) {	pool.sessions.add(session);	} else {	
cannot add new session to the pool because it was removed unexpectedly internal error 

private RemoveSessionResult handleReturnedInUseSessionOnMasterThread( EventState e, WmTezSession session, HashSet<String> poolsToRedistribute, boolean isReturn) {	if (e.updateErrors.remove(session) != null) {	
ignoring an update error for a session being destroyed or returned 

private void handeReopenRequestOnMasterThread(EventState e, WmTezSession session, SettableFuture<WmTezSession> future, HashSet<String> poolsToRedistribute, WmThreadSyncWork syncWork) throws Exception {	if (e.updateErrors.remove(session) != null) {	
ignoring an update error for a session being reopened 

private void handleUpdateErrorOnMasterThread(WmTezSession session, int failedEndpointVersion, IdentityHashMap<WmTezSession, GetRequest> toReuse, WmThreadSyncWork syncWork, HashSet<String> poolsToRedistribute) {	Ref<Integer> endpointVersion = new Ref<>(-1);	AmPluginInfo info = session.getAmPluginInfo(endpointVersion);	if (info != null && endpointVersion.value > failedEndpointVersion) {	
ignoring an update error endpoint information has been updated to 

private void applyNewResourcePlanOnMasterThread( EventState e, WmThreadSyncWork syncWork, HashSet<String> poolsToRedistribute) {	int totalQueryParallelism = 0;	WMFullResourcePlan plan = e.resourcePlanToApply;	if (plan == null) {	
disabling workload management because the resource plan has been removed 

parent.finalFractionRemaining -= fraction;	}	PoolState state = oldPools == null ? null : oldPools.remove(fullName);	if (state == null) {	state = new PoolState(fullName, qp, fraction, pool.getSchedulingPolicy());	} else {	state.update(qp, fraction, syncWork, e, pool.getSchedulingPolicy());	poolsToRedistribute.add(fullName);	}	state.setTriggers(new LinkedList<Trigger>());	
adding hive pool 

Map<String, Trigger> triggers = new HashMap<>();	for (WMTrigger trigger : plan.getTriggers()) {	ExecutionTrigger execTrigger = ExecutionTrigger.fromWMTrigger(trigger);	triggers.put(trigger.getTriggerName(), execTrigger);	}	for (WMPoolTrigger poolTrigger : plan.getPoolTriggers()) {	PoolState pool = pools.get(poolTrigger.getPool());	Trigger trigger = triggers.get(poolTrigger.getTrigger());	pool.triggers.add(trigger);	poolsToRedistribute.add(pool.fullName);	
adding pool trigger 

Trigger trigger = triggers.get(poolTrigger.getTrigger());	pool.triggers.add(trigger);	poolsToRedistribute.add(pool.fullName);	}	}	if (oldPools != null && !oldPools.isEmpty()) {	for (PoolState oldPool : oldPools.values()) {	oldPool.destroy(syncWork, e.getRequests, e.toReuse);	}	}	
updating with total query parallelism 

private static int transferSessionsToDestroy(Collection<WmTezSession> source, List<WmTezSession> toDestroy, int deltaSessions) {	if (deltaSessions >= 0) return deltaSessions;	int toTransfer = Math.min(-deltaSessions, source.size());	Iterator<WmTezSession> iter = source.iterator();	for (int i = 0; i < toTransfer; ++i) {	WmTezSession session = iter.next();	
will destroy instead of restarting 

private void processPoolChangesOnMasterThread( String poolName, boolean hasRequeues, WmThreadSyncWork syncWork) throws Exception {	PoolState pool = pools.get(poolName);	if (pool == null) return;	int queriesToStart = Math.min(pool.queue.size(), pool.queryParallelism - pool.getTotalActiveSessions());	if (queriesToStart > 0) {	
starting queries in pool 

}	String poolName = session.getPoolName();	if (poolName != null) {	poolsToRedistribute.add(poolName);	PoolState pool = pools.get(poolName);	session.clearWm();	if (pool != null && pool.sessions.remove(session)) {	return RemoveSessionResult.OK;	}	}	
session was not in the pool internal error 

private Boolean checkAndAddSessionToAnotherPool( WmTezSession session, String destPoolName, Set<String> poolsToRedistribute) {	if (session.isIrrelevantForWm()) {	
unexpected during add session to another pool if remove failed this should not have been called 

if (session.isIrrelevantForWm()) {	return false;	}	PoolState destPool = pools.get(destPoolName);	if (destPool != null && destPool.sessions.add(session)) {	session.setPoolName(destPoolName);	updateTriggers(session);	poolsToRedistribute.add(destPoolName);	return true;	}	
session was not added to pool 

public ListenableFuture<Boolean> updateResourcePlanAsync(WMFullResourcePlan plan) {	SettableFuture<Boolean> applyRpFuture = SettableFuture.create();	currentLock.lock();	try {	if (current.resourcePlanToApply != null) {	
several resource plans are being applied at the same time using the latest 

} else {	future = current.dumpStateFuture = SettableFuture.create();	notifyWmThreadUnderLock();	}	} finally {	currentLock.unlock();	}	try {	return future.get();	} catch (InterruptedException | ExecutionException e) {	
error getting description 

public TezSessionState reopen(TezSessionState session) throws Exception {	WmTezSession wmTezSession = ensureOwnedSession(session);	HiveConf sessionConf = wmTezSession.getConf();	if (sessionConf == null) {	
session configuration is null for 

private WmTezSession checkSessionForReuse(TezSessionState session) throws Exception {	if (session == null) return null;	WmTezSession result = null;	if (session instanceof WmTezSession) {	result = (WmTezSession) session;	if (result.isOwnedBy(this)) {	return result;	}	
attempting to reuse a session not belonging to us 

if (session == null) return null;	WmTezSession result = null;	if (session instanceof WmTezSession) {	result = (WmTezSession) session;	if (result.isOwnedBy(this)) {	return result;	}	result.returnToSessionManager();	return null;	}	
attempting to reuse a non wm session for workload management 

private WmTezSession createSession(HiveConf conf) {	WmTezSession session = createSessionObject(TezSessionState.makeSessionId(), conf);	session.setQueueName(yarnQueue);	session.setDefault();	
created new interactive session object 

public void update(int queryParallelism, double fraction, WmThreadSyncWork syncWork, EventState e, String schedulingPolicy) {	this.finalFraction = this.finalFractionRemaining = fraction;	this.queryParallelism = queryParallelism;	try {	this.schedulingPolicy = MetaStoreUtils.parseSchedulingPolicy(schedulingPolicy);	} catch (IllegalArgumentException ex) {	
unknown scheduling policy using fair 

public void onSuccess(WmTezSession session) {	SessionInitState oldState;	SettableFuture<WmTezSession> future = null;	lock.lock();	try {	oldState = state;	switch (oldState) {	case GETTING: {	
received a session from am pool 

}	}	} finally {	lock.unlock();	}	if (doRetry) {	try {	start();	return;	} catch (Exception e) {	
failed to retry propagating original error the new error is 

try {	start();	return;	} catch (Exception e) {	} finally {	discardSessionOnFailure(session);	}	}	if (!wasCanceled) {	if (LOG.isDebugEnabled()) {	
queueing the initialization failure with 

public void discardSessionOnFailure(WmTezSession session) {	if (session == null) return;	session.clearWm();	session.setQueryId(null);	try {	tezAmPool.replaceSession(session);	} catch (Exception e) {	
failed to restart a failed session 

this.cancelReason = cancelReason;	if (state == SessionInitState.DONE) {	WmTezSession result = this.session;	this.session = null;	if (pathToDelete != null) {	toDelete.add(pathToDelete);	}	return result;	} else {	if (state == SessionInitState.CANCELED) {	
duplicate call to extract 

private void handleUserCallback(boolean hasFailed) {	if (isUserDone) {	
duplicate user call for a session being killed ignoring 

========================= hive sample_4004 =========================

break;	case MR: mr = ShimLoader.getHadoopShims().getMiniMrCluster(hiveConf, 2, uriString, 1);	break;	default: throw new IllegalArgumentException("Unsupported cluster type " + mr);	}	mr.setupConfiguration(getHiveConf());	baseFsDir = new Path(new Path(fs.getUri()), "/base");	} else {	fs = FileSystem.getLocal(hiveConf);	baseFsDir = new Path("file: if (cleanupLocalDirOnStartup) {	
attempting to cleanup basefsdir while setting up 

========================= hive sample_418 =========================

queryContext.setIndexInputFormat(HiveCompactIndexInputFormat.class.getName());	StringBuilder qlCommand = new StringBuilder("INSERT OVERWRITE DIRECTORY ");	String tmpFile = pctx.getContext().getMRTmpPath().toUri().toString();	queryContext.setIndexIntermediateFile(tmpFile);	qlCommand.append( "\"" + tmpFile + "\" ");	qlCommand.append("SELECT `_bucketname` ,  `_offsets` FROM ");	qlCommand.append(HiveUtils.unparseIdentifier(index.getIndexTableName()));	qlCommand.append(" WHERE ");	String predicateString = decomposedPredicate.pushedPredicate.getExprString();	qlCommand.append(predicateString);	
generating tasks for re entrant ql query 

HiveConf queryConf = new HiveConf(pctx.getConf(), CompactIndexHandler.class);	HiveConf.setBoolVar(queryConf, HiveConf.ConfVars.COMPRESSRESULT, false);	Driver driver = new Driver(queryConf, pctx.getQueryState().getLineageState());	driver.compile(qlCommand.toString(), false);	if (pctx.getConf().getBoolVar(ConfVars.HIVE_INDEX_COMPACT_BINARY_SEARCH) && useSorted) {	MapWork work = null;	String originalInputFormat = null;	for (Task task : driver.getPlan().getRootTasks()) {	if (task.getWork() instanceof MapredWork) {	if (work != null) {	
tried to use a binary search on a compact index but there were an unexpected number of root level map reduce tasks in the reentrant query plan 

originalInputFormat = inputFormat;	if (inputFormat == null) {	inputFormat = HiveConf.getVar(pctx.getConf(), HiveConf.ConfVars.HIVEINPUTFORMAT);	}	try {	if (!HiveInputFormat.class.isAssignableFrom(JavaUtils.loadClass(inputFormat))) {	work = null;	break;	}	} catch (ClassNotFoundException e) {	
map reduce work s input format class was not found cannot use the fact the compact index is sorted 

}	} catch (ClassNotFoundException e) {	work = null;	break;	}	work.setInputFormatSorted(true);	}	}	if (work != null) {	if (!findIndexColumnFilter(work.getAliasToWork().values())) {	
could not locate the index column s filter operator and expr node cannot use the fact the compact index is sorted 

========================= hive sample_4638 =========================

public void analyzeInternal(ASTNode ast) throws SemanticException {	if (ast.getToken().getType() == HiveParser.TOK_CREATEMACRO) {	
analyzing create macro 

public void analyzeInternal(ASTNode ast) throws SemanticException {	if (ast.getToken().getType() == HiveParser.TOK_CREATEMACRO) {	analyzeCreateMacro(ast);	}	if (ast.getToken().getType() == HiveParser.TOK_DROPMACRO) {	
analyzing drop macro 

========================= hive sample_3407 =========================

protected void drop_table_with_environment_context(String dbname, String name, boolean deleteData, EnvironmentContext envContext) throws MetaException, TException, NoSuchObjectException, UnsupportedOperationException {	org.apache.hadoop.hive.metastore.api.Table table = getTempTable(dbname, name);	if (table != null) {	try {	deleteTempTableColumnStatsForTable(dbname, name);	} catch (NoSuchObjectException err){	
object not found in metastore 

} else {	if (haveTableColumnsChanged(oldt, newt)) {	shouldDeleteColStats = true;	}	getTempTablesForDatabase(dbname).put(tbl_name, newTable);	}	if (shouldDeleteColStats) {	try {	deleteTempTableColumnStatsForTable(dbname, tbl_name);	} catch (NoSuchObjectException err){	
object not found in metastore 

throw new MetaException( "Could not find temp table entry for " + StatsUtils.getFullyQualifiedTableName(dbName, tableName));	}	if (deleteData && !MetaStoreUtils.isExternalTable(table)) {	try {	boolean ifPurge = false;	if (envContext != null){	ifPurge = Boolean.parseBoolean(envContext.getProperties().get("ifPurge"));	}	getWh().deleteDir(tablePath, true, ifPurge);	} catch (Exception err) {	
failed to delete temp table directory 

public static Map<String, Map<String, Table>> getTempTables() {	SessionState ss = SessionState.get();	if (ss == null) {	
no current sessionstate skipping temp tables 

private Map<String, ColumnStatisticsObj> getTempTableColumnStatsForTable(String dbName, String tableName) {	SessionState ss = SessionState.get();	if (ss == null) {	
no current sessionstate skipping temp tables 

========================= hive sample_5029 =========================

public TemporaryHashSinkOperator(CompilationOpContext ctx, MapJoinDesc desc) {	super(ctx);	conf = new HashTableSinkDesc(desc);	assert conf.getHashtableMemoryUsage() != 0;	if (conf.getHashtableMemoryUsage() == 0) {	
hash table memory usage not set in map join operator non staged load may fail 

========================= hive sample_3954 =========================

boolean isUnmanagedPool = false;	String poolPath = null;	if (poolTarget.getType() == HiveParser.TOK_UNMANAGED) {	isUnmanagedPool = true;	} else {	poolPath = poolPath(ast.getChild(1));	}	WMNullablePool poolChanges = null;	for (int i = 2; i < ast.getChildCount(); ++i) {	Tree child = ast.getChild(i);	
todo 

descTblDesc.setExt(descOptions == HiveParser.KW_EXTENDED);	if (!colPath.equalsIgnoreCase(tableName) && descTblDesc.isFormatted()) {	showColStats = true;	}	}	inputs.add(new ReadEntity(getTable(tableName)));	Task ddlTask = TaskFactory.get(new DDLWork(getInputs(), getOutputs(), descTblDesc), conf);	rootTasks.add(ddlTask);	String schema = DescTableDesc.getSchema(showColStats);	setFetchTask(createFetchTask(schema));	
analyzedescribetable done 

private void addTablePartsOutputs(Table table, List<Map<String, String>> partSpecs, boolean throwIfNonExistent, boolean allowMany, ASTNode ast, WriteEntity.WriteType writeType) throws SemanticException {	Iterator<Map<String, String>> i;	int index;	for (i = partSpecs.iterator(), index = 1; i.hasNext(); ++index) {	Map<String, String> partSpec = i.next();	List<Partition> parts = null;	if (allowMany) {	try {	parts = db.getPartitions(table, partSpec);	} catch (HiveException e) {	
got hiveexception during obtaining list of partitions 

throw new SemanticException(e.getMessage(), e);	}	} else {	parts = new ArrayList<Partition>();	try {	Partition p = db.getPartition(table, partSpec, false);	if (p != null) {	parts.add(p);	}	} catch (HiveException e) {	
wrong specification 

========================= hive sample_3543 =========================

public void readEncodedColumns(int stripeIx, StripeInformation stripe, OrcProto.RowIndex[] indexes, List<OrcProto.ColumnEncoding> encodings, List<OrcProto.Stream> streamList, boolean[] included, boolean[] rgs, Consumer<OrcEncodedColumnBatch> consumer) throws IOException {	long stripeOffset = stripe.getOffset();	long offset = 0;	boolean[] hasNull = RecordReaderUtils.findPresentStreamsByColumn(streamList, types);	if (isTracingEnabled) {	
the following columns have present streams 

boolean[] hasNull = RecordReaderUtils.findPresentStreamsByColumn(streamList, types);	if (isTracingEnabled) {	}	ColumnReadContext[] colCtxs = new ColumnReadContext[included.length];	int colRgIx = -1;	for (int i = 1; i < included.length; ++i) {	if (!included[i]) continue;	ColumnEncoding enc = encodings.get(i);	colCtxs[i] = new ColumnReadContext(i, enc, indexes[i], ++colRgIx);	if (isTracingEnabled) {	
creating context 

}	CreateHelper listToRead = new CreateHelper();	boolean hasIndexOnlyCols = false;	for (OrcProto.Stream stream : streamList) {	long length = stream.getLength();	int colIx = stream.getColumn();	OrcProto.Stream.Kind streamKind = stream.getKind();	if (!included[colIx] || StreamName.getArea(streamKind) != StreamName.Area.DATA) {	hasIndexOnlyCols = hasIndexOnlyCols || included[colIx];	if (isTracingEnabled) {	
skipping stream for column at 

}	trace.logSkipStream(colIx, streamKind, offset, length);	offset += length;	continue;	}	ColumnReadContext ctx = colCtxs[colIx];	assert ctx != null;	int indexIx = RecordReaderUtils.getIndexPosition(ctx.encoding.getKind(), types.get(colIx).getKind(), streamKind, isCompressed, hasNull[colIx]);	ctx.addStream(offset, stream, indexIx);	if (isTracingEnabled) {	
adding stream for column at index position 

ColumnReadContext ctx = colCtxs[colIx];	assert ctx != null;	int indexIx = RecordReaderUtils.getIndexPosition(ctx.encoding.getKind(), types.get(colIx).getKind(), streamKind, isCompressed, hasNull[colIx]);	ctx.addStream(offset, stream, indexIx);	if (isTracingEnabled) {	}	if (rgs == null || RecordReaderUtils.isDictionary(streamKind, encodings.get(colIx))) {	trace.logAddStream(colIx, streamKind, offset, length, indexIx, true);	RecordReaderUtils.addEntireStreamToRanges(offset, length, listToRead, true);	if (isTracingEnabled) {	
will read whole stream added to 

offset += length;	}	boolean hasFileId = this.fileKey != null;	if (listToRead.get() == null) {	if (hasIndexOnlyCols && (rgs == null)) {	OrcEncodedColumnBatch ecb = POOLS.ecbPool.take();	ecb.init(fileKey, stripeIx, OrcEncodedColumnBatch.ALL_RGS, included.length);	try {	consumer.consumeData(ecb);	} catch (InterruptedException e) {	
io thread interrupted while queueing data 

if (listToRead.get() == null) {	if (hasIndexOnlyCols && (rgs == null)) {	OrcEncodedColumnBatch ecb = POOLS.ecbPool.take();	ecb.init(fileKey, stripeIx, OrcEncodedColumnBatch.ALL_RGS, included.length);	try {	consumer.consumeData(ecb);	} catch (InterruptedException e) {	throw new IOException(e);	}	} else {	
nothing to read for stripe 

boolean isLastRg = rgIx == rgCount - 1;	OrcEncodedColumnBatch ecb = POOLS.ecbPool.take();	trace.logStartRg(rgIx);	boolean hasErrorForEcb = true;	try {	ecb.init(fileKey, stripeIx, rgIx, included.length);	for (int colIx = 0; colIx < colCtxs.length; ++colIx) {	ColumnReadContext ctx = colCtxs[colIx];	if (ctx == null) continue;	if (isTracingEnabled) {	
ctx rgix islastrg rgcount 

}	OrcProto.RowIndexEntry index = ctx.rowIndex.getEntry(rgIx), nextIndex = isLastRg ? null : ctx.rowIndex.getEntry(rgIx + 1);	ecb.initOrcColumn(ctx.colIx);	trace.logStartCol(ctx.colIx);	for (int streamIx = 0; streamIx < ctx.streamCount; ++streamIx) {	StreamContext sctx = ctx.streams[streamIx];	ColumnStreamData cb = null;	try {	if (RecordReaderUtils.isDictionary(sctx.kind, ctx.encoding)) {	if (isTracingEnabled) {	
getting stripe level stream for column rg at 

cb = createRgColumnStreamData(rgIx, isLastRg, ctx.colIx, sctx, cOffset, endCOffset, isCompressed, unlockUntilCOffset);	boolean isStartOfStream = sctx.bufferIter == null;	DiskRangeList lastCached = readEncodedStream(stripeOffset, (isStartOfStream ? iter : sctx.bufferIter), cOffset, endCOffset, cb, unlockUntilCOffset, sctx.offset, toRelease);	if (lastCached != null) {	sctx.bufferIter = iter = lastCached;	}	}	ecb.setStreamData(ctx.colIx, sctx.kind.getNumber(), cb);	} catch (Exception ex) {	DiskRangeList drl = toRead == null ? null : toRead.next;	
error getting stream for column rg at toread 

}	hasErrorForEcb = false;	} finally {	if (hasErrorForEcb) {	releaseEcbRefCountsOnError(ecb);	}	}	try {	consumer.consumeData(ecb);	} catch (InterruptedException e) {	
io thread interrupted while queueing data 

}	}	try {	consumer.consumeData(ecb);	} catch (InterruptedException e) {	releaseEcbRefCountsOnError(ecb);	throw new IOException(e);	}	}	if (isTracingEnabled) {	
disk ranges after preparing all the data 

try {	for (int colIx = 0; colIx < colCtxs.length; ++colIx) {	ColumnReadContext ctx = colCtxs[colIx];	if (ctx == null) continue;	for (int streamIx = 0; streamIx < ctx.streamCount; ++streamIx) {	StreamContext sctx = ctx.streams[streamIx];	if (sctx == null || sctx.stripeLevelStream == null) continue;	if (0 != sctx.stripeLevelStream.decRef()) continue;	for (MemoryBuffer buf : sctx.stripeLevelStream.getCacheBuffers()) {	if (LOG.isTraceEnabled()) {	
unlocking at the end of processing 

if (LOG.isTraceEnabled()) {	}	cacheWrapper.releaseBuffer(buf);	}	}	}	releaseInitialRefcounts(toRead.next);	releaseBuffers(toRelease.keySet(), true);	} catch (Throwable t) {	if (!hasError) throw new IOException(t);	
error during the cleanup after another error ignoring 

private DiskRangeList.MutateHelper getDataFromCacheAndDisk(DiskRangeList listToRead, long stripeOffset, boolean hasFileId, IdentityHashMap<ByteBuffer, Boolean> toRelease) throws IOException {	DiskRangeList.MutateHelper toRead = new DiskRangeList.MutateHelper(listToRead);	if (LOG.isInfoEnabled()) {	
resulting disk ranges to read file 

private DiskRangeList.MutateHelper getDataFromCacheAndDisk(DiskRangeList listToRead, long stripeOffset, boolean hasFileId, IdentityHashMap<ByteBuffer, Boolean> toRelease) throws IOException {	DiskRangeList.MutateHelper toRead = new DiskRangeList.MutateHelper(listToRead);	if (LOG.isInfoEnabled()) {	}	BooleanRef isAllInCache = new BooleanRef();	if (hasFileId) {	cacheWrapper.getFileData(fileKey, toRead.next, stripeOffset, CC_FACTORY, isAllInCache);	if (LOG.isInfoEnabled()) {	
disk ranges after cache found everything file base offset 

private void releaseEcbRefCountsOnError(OrcEncodedColumnBatch ecb) {	try {	if (isTracingEnabled) {	
unlocking the batch not sent to consumer on error 

ColumnStreamData[] datas = ecb.getColumnData(colIx);	for (ColumnStreamData data : datas) {	if (data == null || data.decRef() != 0) continue;	for (MemoryBuffer buf : data.getCacheBuffers()) {	if (buf == null) continue;	cacheWrapper.releaseBuffer(buf);	}	}	}	} catch (Throwable t) {	
error during the cleanup of an error ignoring 

private ColumnStreamData createRgColumnStreamData(int rgIx, boolean isLastRg, int colIx, StreamContext sctx, long cOffset, long endCOffset, boolean isCompressed, long unlockUntilCOffset) {	ColumnStreamData cb = POOLS.csdPool.take();	cb.incRef();	if (isTracingEnabled) {	
getting data for column last rg stream at index position un compressed 

List<ProcCacheChunk> toDecompress = null;	List<IncompleteCb> badEstimates = null;	List<ByteBuffer> toReleaseCopies = null;	if (isCompressed) {	toReleaseCopies = new ArrayList<>();	toDecompress = new ArrayList<>();	badEstimates = new ArrayList<>();	}	DiskRangeList current = findExactPosition(start, cOffset);	if (isTracingEnabled) {	
starting read for at 

badEstimates = new ArrayList<>();	}	DiskRangeList current = findExactPosition(start, cOffset);	if (isTracingEnabled) {	}	trace.logStartRead(current);	CacheChunk lastUncompressed = null;	try {	lastUncompressed = isCompressed ? prepareRangesForCompressedRead(cOffset, endCOffset, streamOffset, unlockUntilCOffset, current, csd, toRelease, toReleaseCopies, toDecompress, badEstimates) : prepareRangesForUncompressedRead( cOffset, endCOffset, streamOffset, unlockUntilCOffset, current, csd);	} catch (Exception ex) {	
failed un compressed read coffset endcoffset streamoffset unlockuntilcoffset ranges passed in ranges passed to prepare 

}	cacheWrapper.getAllocator().allocateMultiple(targetBuffers, bufferSize, cacheWrapper.getDataBufferFactory());	for (ProcCacheChunk chunk : toDecompress) {	ByteBuffer dest = chunk.getBuffer().getByteBufferRaw();	if (chunk.isOriginalDataCompressed) {	decompressChunk(chunk.originalData, codec, dest);	} else {	copyUncompressedChunk(chunk.originalData, dest);	}	if (isTracingEnabled) {	
locking due to reuse after decompression 

if (cOffset > current.getOffset()) {	current = current.split(cOffset).next;	}	long currentOffset = cOffset;	CacheChunk lastUncompressed = null;	while (true) {	DiskRangeList next = null;	if (current instanceof CacheChunk) {	CacheChunk cc = (CacheChunk)current;	if (isTracingEnabled) {	
locking due to reuse 

while (true) {	DiskRangeList next = null;	if (current instanceof CacheChunk) {	CacheChunk cc = (CacheChunk)current;	if (isTracingEnabled) {	}	cacheWrapper.reuseBuffer(cc.getBuffer());	columnStreamData.getCacheBuffers().add(cc.getBuffer());	currentOffset = cc.getEnd();	if (isTracingEnabled) {	
adding an already uncompressed buffer 

if (isTracingEnabled) {	}	ponderReleaseInitialRefcount(unlockUntilCOffset, streamOffset, cc);	lastUncompressed = cc;	next = current.next;	if (next != null && (endCOffset >= 0 && currentOffset < endCOffset) && next.getOffset() >= endCOffset) {	throw new IOException("Expected data at " + currentOffset + " (reading until " + endCOffset + "), but the next buffer starts at " + next.getOffset());	}	} else if (current instanceof IncompleteCb)  {	if (isTracingEnabled) {	
cannot read 

private CacheChunk prepareRangesForUncompressedRead(long cOffset, long endCOffset, long streamOffset, long unlockUntilCOffset, DiskRangeList current, ColumnStreamData columnStreamData) throws IOException {	long currentOffset = cOffset;	CacheChunk lastUncompressed = null;	boolean isFirst = true;	while (true) {	DiskRangeList next = null;	assert current instanceof CacheChunk;	lastUncompressed = (CacheChunk)current;	if (isTracingEnabled) {	
locking due to reuse 

if (isTracingEnabled) {	}	cacheWrapper.reuseBuffer(lastUncompressed.getBuffer());	if (isFirst) {	columnStreamData.setIndexBaseOffset((int)(lastUncompressed.getOffset() - streamOffset));	isFirst = false;	}	columnStreamData.getCacheBuffers().add(lastUncompressed.getBuffer());	currentOffset = lastUncompressed.getEnd();	if (isTracingEnabled) {	
adding an uncompressed buffer 

private DiskRangeList preReadUncompressedStream(long baseOffset, DiskRangeList start, long streamOffset, long streamEnd, Kind kind) throws IOException {	if (streamOffset == streamEnd) return null;	List<UncompressedCacheChunk> toCache = null;	DiskRangeList current = findIntersectingPosition(start, streamOffset, streamEnd);	if (isTracingEnabled) {	
starting pre read for at 

if (noMoreDataForPart && hasEntirePartTo < partEnd && candidateCached != null) {	lastUncompressed = copyAndReplaceCandidateToNonCached( candidateCached, partOffset, hasEntirePartTo, cacheWrapper, singleAlloc);	candidateCached = null;	}	current = next;	if (noMoreDataForPart) break;	if (current.getEnd() > partEnd) {	current = current.split(partEnd);	}	if (isTracingEnabled) {	
processing uncompressed file data at 

private static void decompressChunk( ByteBuffer src, CompressionCodec codec, ByteBuffer dest) throws IOException {	int startPos = dest.position(), startLim = dest.limit();	int startSrcPos = src.position(), startSrcLim = src.limit();	if (LOG.isTraceEnabled()) {	
decompressing bytes to dest buffer pos limit 

codec.decompress(src, dest);	dest.position(startPos);	int newLim = dest.limit();	if (newLim > startLim) {	throw new AssertionError("After codec, buffer [" + startPos + ", " + startLim + ") became [" + dest.position() + ", " + newLim + ")");	}	if (dest.remaining() > 0) return;	dest.limit(startLim);	src.position(startSrcPos);	src.limit(startSrcLim);	
the codec has produced bytes for bytes at pos data hash 

dest.limit(startLim);	src.position(startSrcPos);	src.limit(startSrcLim);	ByteBuffer srcHeap = ByteBuffer.allocate(src.remaining()), destHeap = ByteBuffer.allocate(dest.remaining());	int destHeapPos = destHeap.position();	srcHeap.put(src);	srcHeap.position(startSrcPos);	codec.decompress(srcHeap, destHeap);	destHeap.position(destHeapPos);	int newLen = destHeap.remaining();	
fell back to jdk decompressor with memcopy got bytes 

private void ponderReleaseInitialRefcount( long unlockUntilCOffset, long streamStartOffset, CacheChunk cc) {	if (cc.getEnd() > unlockUntilCOffset) return;	assert cc.getBuffer() != null;	try {	releaseInitialRefcount(cc, false);	} catch (AssertionError e) {	
bug releasing initial refcount stream start unlocking until from 

}	DiskRangeList prev = cc.prev;	while (true) {	if ((prev == null) || (prev.getEnd() <= streamStartOffset)) break;	if (prev.getClass() != CacheChunk.class) break;	CacheChunk prevCc = (CacheChunk)prev;	if (prevCc.buffer == null) break;	try {	releaseInitialRefcount(prevCc, true);	} catch (AssertionError e) {	
bug releasing initial refcount stream start unlocking until from and backtracked to 

private void releaseInitialRefcount(CacheChunk cc, boolean isBacktracking) {	if (isTracingEnabled) {	
unlocking for the fetching thread backtracking 

assert collisionMask.length >= (toDecompress.size() >>> 6);	long maskVal = -1;	for (int i = 0; i < toDecompress.size(); ++i) {	if ((i & 63) == 0) {	maskVal = collisionMask[i >>> 6];	}	if ((maskVal & 1) == 1) {	CacheChunk replacedChunk = toDecompress.get(i);	MemoryBuffer replacementBuffer = targetBuffers[i];	if (isTracingEnabled) {	
discarding data due to cache collision replaced with 

b2 = bytes[2];	}	int chunkLength = (b2 << 15) | (b1 << 7) | (b0 >> 1);	if (chunkLength > bufferSize) {	throw new IllegalArgumentException("Buffer size too small. size = " + bufferSize + " needed = " + chunkLength);	}	int consumedLength = chunkLength + OutStream.HEADER_SIZE;	long cbEndOffset = cbStartOffset + consumedLength;	boolean isUncompressed = ((b0 & 0x01) == 1);	if (isTracingEnabled) {	
found cb at chunk length total not compressed 

if (current.getEnd() < cbEndOffset && !current.hasContiguousNext()) {	badEstimates.add(addIncompleteCompressionBuffer( cbStartOffset, current, 0, isTracingEnabled, trace));	return null;	}	ByteBuffer copy = allocateBuffer(chunkLength, compressed.isDirect());	toReleaseCopies.add(copy);	int remaining = chunkLength - compressed.remaining();	int originalPos = compressed.position();	copy.put(compressed);	if (isTracingEnabled) {	
removing partial cb from ranges after copying its contents 

}	remaining -= compressed.remaining();	copy.put(compressed);	if (toRelease.remove(compressed)) {	releaseBuffer(compressed, true);	}	DiskRangeList tmp = next;	next = next.hasContiguousNext() ? next.next : null;	if (next != null) {	if (isTracingEnabled) {	
removing partial cb from ranges after copying its contents 

if (!(current instanceof BufferChunk)) {	throw new IOException( "Trying to extend compressed block into uncompressed block " + current);	}	BufferChunk currentBc = (BufferChunk) current;	ix = readLengthBytes(currentBc.getChunk(), result, ix);	if (ix == 3) return currentBc;	DiskRangeList tmp = current;	current = current.hasContiguousNext() ? current.next : null;	if (current != null) {	if (isTracingEnabled) {	
removing partial cb from ranges after copying its contents 

private void releaseBuffer(ByteBuffer bb, boolean isFromDataReader) {	if (isTracingEnabled) {	
releasing the buffer 

dataReader.releaseBuffer(bb);	return;	}	Field localCf = cleanerField;	if (!bb.isDirect() || localCf == null) return;	try {	Cleaner cleaner = (Cleaner) localCf.get(bb);	if (cleaner != null) {	cleaner.clean();	} else {	
unable to clean a buffer using cleaner no cleaner 

}	Field localCf = cleanerField;	if (!bb.isDirect() || localCf == null) return;	try {	Cleaner cleaner = (Cleaner) localCf.get(bb);	if (cleaner != null) {	cleaner.clean();	} else {	}	} catch (Exception e) {	
unable to clean direct buffers using cleaner 

private static IncompleteCb addIncompleteCompressionBuffer(long cbStartOffset, DiskRangeList target, int extraChunkCountToLog, boolean isTracingEnabled, IoTrace trace) {	IncompleteCb icb = new IncompleteCb(cbStartOffset, target.getEnd());	if (isTracingEnabled) {	
replacing and previous chunks with in the buffers 

private ProcCacheChunk addOneCompressionBlockByteBuffer(ByteBuffer fullCompressionBlock, boolean isUncompressed, long cbStartOffset, long cbEndOffset, int lastChunkLength, BufferChunk lastChunk, List<ProcCacheChunk> toDecompress, List<MemoryBuffer> cacheBuffers, boolean doTrace) {	MemoryBuffer futureAlloc = cacheWrapper.getDataBufferFactory().create();	cacheBuffers.add(futureAlloc);	ProcCacheChunk cc = new ProcCacheChunk(cbStartOffset, cbEndOffset, !isUncompressed, fullCompressionBlock, futureAlloc, cacheBuffers.size() - 1);	toDecompress.add(cc);	if (isTracingEnabled) {	
adjusting to consume compressed bytes 

ProcCacheChunk cc = new ProcCacheChunk(cbStartOffset, cbEndOffset, !isUncompressed, fullCompressionBlock, futureAlloc, cacheBuffers.size() - 1);	toDecompress.add(cc);	if (isTracingEnabled) {	}	if (doTrace) {	trace.logCompositeOrcCb(lastChunkLength, lastChunk.getChunk().remaining(), cc);	}	lastChunk.getChunk().position(lastChunk.getChunk().position() + lastChunkLength);	if (lastChunk.getChunk().remaining() <= 0) {	if (isTracingEnabled) {	
replacing with in the buffers 

if (doTrace) {	trace.logCompositeOrcCb(lastChunkLength, lastChunk.getChunk().remaining(), cc);	}	lastChunk.getChunk().position(lastChunk.getChunk().position() + lastChunkLength);	if (lastChunk.getChunk().remaining() <= 0) {	if (isTracingEnabled) {	}	lastChunk.replaceSelfWith(cc);	} else {	if (isTracingEnabled) {	
adding before in the buffers 

public void readIndexStreams(OrcIndex index, StripeInformation stripe, List<OrcProto.Stream> streams, boolean[] included, boolean[] sargColumns) throws IOException {	long stripeOffset = stripe.getOffset();	DiskRangeList indexRanges = planIndexReading( fileSchema, streams, true, included, sargColumns, version, index.getBloomFilterKinds());	if (indexRanges == null) {	if (LOG.isDebugEnabled()) {	
nothing to read for stripe 

if (LOG.isDebugEnabled()) {	}	return;	}	ReadContext[] colCtxs = new ReadContext[included.length];	int colRgIx = -1;	for (int i = 0; i < included.length; ++i) {	if (!included[i] && (sargColumns == null || !sargColumns[i])) continue;	colCtxs[i] = new ReadContext(i, ++colRgIx);	if (isTracingEnabled) {	
creating context 

}	long offset = 0;	for (OrcProto.Stream stream : streams) {	long length = stream.getLength();	int colIx = stream.getColumn();	OrcProto.Stream.Kind streamKind = stream.getKind();	if ((StreamName.getArea(streamKind) == StreamName.Area.INDEX) && ((sargColumns != null && sargColumns[colIx]) || (included[colIx] && streamKind == Kind.ROW_INDEX))) {	trace.logAddStream(colIx, streamKind, offset, length, -1, true);	colCtxs[colIx].addStream(offset, stream, -1);	if (isTracingEnabled) {	
adding stream for column at 

DiskRangeList iter = preReadUncompressedStreams(stripeOffset, colCtxs, toRead, toRelease);	boolean hasError = true;	try {	for (int colIx = 0; colIx < colCtxs.length; ++colIx) {	ReadContext ctx = colCtxs[colIx];	if (ctx == null) continue;	for (int streamIx = 0; streamIx < ctx.streamCount; ++streamIx) {	StreamContext sctx = ctx.streams[streamIx];	try {	if (isTracingEnabled) {	
getting index stream for column at 

iter = lastCached;	}	if (isTracingEnabled) {	traceLogBuffersUsedToParse(csd);	}	CodedInputStream cis = CodedInputStream.newInstance( new IndexStream(csd.getCacheBuffers(), sctx.length));	cis.setSizeLimit(InStream.PROTOBUF_MESSAGE_MAX_LIMIT);	switch (sctx.kind) {	case ROW_INDEX: OrcProto.RowIndex tmp = index.getRowGroupIndex()[colIx] = OrcProto.RowIndex.parseFrom(cis);	if (isTracingEnabled) {	
index is 

case BLOOM_FILTER: case BLOOM_FILTER_UTF8: index.getBloomFilterIndex()[colIx] = OrcProto.BloomFilterIndex.parseFrom(cis);	break;	default: throw new AssertionError("Unexpected index stream type " + sctx.kind);	}	for (MemoryBuffer buf : csd.getCacheBuffers()) {	if (buf == null) continue;	cacheWrapper.releaseBuffer(buf);	}	} catch (Exception ex) {	DiskRangeList drl = toRead == null ? null : toRead.next;	
error getting stream for column at toread 

if (buf == null) continue;	cacheWrapper.releaseBuffer(buf);	}	} catch (Exception ex) {	DiskRangeList drl = toRead == null ? null : toRead.next;	throw (ex instanceof IOException) ? (IOException)ex : new IOException(ex);	}	}	}	if (isTracingEnabled) {	
disk ranges after preparing all the data 

}	if (isTracingEnabled) {	}	hasError = false;	} finally {	try {	releaseInitialRefcounts(toRead.next);	releaseBuffers(toRelease.keySet(), true);	} catch (Throwable t) {	if (!hasError) throw new IOException(t);	
error during the cleanup after another error ignoring 

if (newIter != null) {	iter = newIter;	}	}	}	if (toRelease != null) {	releaseBuffers(toRelease.keySet(), true);	toRelease.clear();	}	if (LOG.isInfoEnabled()) {	
disk ranges after pre read file base offset 

hasError = false;	} finally {	if (hasError) {	try {	releaseInitialRefcounts(toRead.next);	if (toRelease != null) {	releaseBuffers(toRelease.keySet(), true);	toRelease.clear();	}	} catch (Throwable t) {	
error during the cleanup after another error ignoring 

========================= hive sample_3685 =========================

public static IDriver instantiateDriver(MiniCluster cluster) {	HiveConf hiveConf = new HiveConf(HCatDataCheckUtil.class);	for (Entry e : cluster.getProperties().entrySet()) {	hiveConf.set(e.getKey().toString(), e.getValue().toString());	}	hiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");	hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");	hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");	
hive conf 

========================= hive sample_675 =========================

public Object getStructFieldData(Object data, StructField f) {	if (data == null) {	return null;	}	int fieldID = f.getFieldID();	if (LOG.isDebugEnabled()) {	
getting struct field data for field on data 

return null;	}	int fieldID = f.getFieldID();	if (LOG.isDebugEnabled()) {	}	if (data instanceof LazyStruct) {	LazyStruct row = (LazyStruct) data;	Object rowField = row.getField(fieldID);	if (rowField instanceof LazyStruct) {	if (LOG.isDebugEnabled() && rowField != null) {	
deserializing struct 

} else if (rowField instanceof LazyMap) {	LazyMap lazyMap = (LazyMap) rowField;	for (Entry<Object, Object> entry : lazyMap.getMap().entrySet()) {	Object _key = entry.getKey();	Object _value = entry.getValue();	if (_value instanceof LazyStruct) {	lazyMap.getMap().put(_key, deserializeStruct(_value, f.getFieldName()));	}	}	if (LOG.isDebugEnabled()) {	
returning a lazy map for field 

Object _value = entry.getValue();	if (_value instanceof LazyStruct) {	lazyMap.getMap().put(_key, deserializeStruct(_value, f.getFieldName()));	}	}	if (LOG.isDebugEnabled()) {	}	return lazyMap;	} else {	if (LOG.isDebugEnabled()) {	
returning for field 

}	rs = schemaRetriever.retrieveReaderSchema(data);	if (rs == null) {	throw new IllegalStateException( "A valid reader schema could not be retrieved either directly or from the schema retriever for field [" + fieldName + "]");	}	ws = schemaRetriever.retrieveWriterSchema(data);	if (ws == null) {	throw new IllegalStateException( "Null writer schema retrieved from schemaRetriever for field [" + fieldName + "]");	}	if (LOG.isDebugEnabled()) {	
retrieved writer schema 

}	rs = schemaRetriever.retrieveReaderSchema(data);	if (rs == null) {	throw new IllegalStateException( "A valid reader schema could not be retrieved either directly or from the schema retriever for field [" + fieldName + "]");	}	ws = schemaRetriever.retrieveWriterSchema(data);	if (ws == null) {	throw new IllegalStateException( "Null writer schema retrieved from schemaRetriever for field [" + fieldName + "]");	}	if (LOG.isDebugEnabled()) {	
retrieved reader schema 

========================= hive sample_5301 =========================

public long getEstimatedSize(InputSplit inputSplit) throws IOException {	long colProjSize = inputSplit.getLength();	if (inputSplit instanceof ColumnarSplit) {	colProjSize = ((ColumnarSplit) inputSplit).getColumnarProjectionSize();	if (LOG.isDebugEnabled()) {	
estimated column projection size 

long colProjSize = inputSplit.getLength();	if (inputSplit instanceof ColumnarSplit) {	colProjSize = ((ColumnarSplit) inputSplit).getColumnarProjectionSize();	if (LOG.isDebugEnabled()) {	}	} else if (inputSplit instanceof HiveInputFormat.HiveInputSplit) {	InputSplit innerSplit = ((HiveInputFormat.HiveInputSplit) inputSplit).getInputSplit();	if (innerSplit instanceof ColumnarSplit) {	colProjSize = ((ColumnarSplit) innerSplit).getColumnarProjectionSize();	if (LOG.isDebugEnabled()) {	
estimated column projection size 

========================= hive sample_4005 =========================

opRules.put(new RuleRegExp("R7", UDTFOperator.getOperatorName() + "%"), OpProcFactory.getUDTFProc());	opRules.put(new RuleRegExp("R8", LateralViewForwardOperator.getOperatorName() + "%"), OpProcFactory.getLVFProc());	opRules.put(new RuleRegExp("R9", LateralViewJoinOperator.getOperatorName() + "%"), OpProcFactory.getLVJProc());	opRules.put(new RuleRegExp("R10", ReduceSinkOperator.getOperatorName() + "%"), OpProcFactory.getRSProc());	Dispatcher disp = new DefaultRuleDispatcher(OpProcFactory.getDefaultProc(), opRules, opWalkerInfo);	GraphWalker ogw = new DefaultGraphWalker(disp);	ArrayList<Node> topNodes = new ArrayList<Node>();	topNodes.addAll(pGraphContext.getTopOps().values());	ogw.startWalking(topNodes, null);	if (LOG.isDebugEnabled()) {	
after ppd 

========================= hive sample_3390 =========================

return createTableLike(db, crtTblLike);	}	DropTableDesc dropTbl = work.getDropTblDesc();	if (dropTbl != null) {	dropTableOrPartitions(db, dropTbl);	return 0;	}	AlterTableDesc alterTbl = work.getAlterTblDesc();	if (alterTbl != null) {	if (!allowOperationInReplicationScope(db, alterTbl.getOldName(), null, alterTbl.getReplicationSpec())) {	
ddltask alter table is skipped as table is newer than update 

private int handleWorkloadManagementServiceChange(WorkloadManager wm, TezSessionPoolManager pm, boolean isActivate, WMFullResourcePlan appliedRp) throws HiveException {	String name = null;	if (isActivate) {	name = appliedRp.getPlan().getName();	
activating a new resource plan 

private int handleWorkloadManagementServiceChange(WorkloadManager wm, TezSessionPoolManager pm, boolean isActivate, WMFullResourcePlan appliedRp) throws HiveException {	String name = null;	if (isActivate) {	name = appliedRp.getPlan().getName();	} else {	
disabling workload management 

name = appliedRp.getPlan().getName();	} else {	}	if (wm != null) {	ListenableFuture<Boolean> future = wm.updateResourcePlanAsync(appliedRp);	boolean isOk = false;	try {	future.get();	isOk = true;	if (isActivate) {	
successfully activated resource plan 

} else {	}	if (wm != null) {	ListenableFuture<Boolean> future = wm.updateResourcePlanAsync(appliedRp);	boolean isOk = false;	try {	future.get();	isOk = true;	if (isActivate) {	} else {	
successfully disabled workload management 

future.get();	isOk = true;	if (isActivate) {	} else {	}	} catch (InterruptedException | ExecutionException e) {	throw new HiveException(e);	} finally {	if (!isOk) {	if (isActivate) {	
failed to activate resource plan 

isOk = true;	if (isActivate) {	} else {	}	} catch (InterruptedException | ExecutionException e) {	throw new HiveException(e);	} finally {	if (!isOk) {	if (isActivate) {	} else {	
failed to disable workload management 

} finally {	if (!isOk) {	if (isActivate) {	} else {	}	}	}	}	if (pm != null) {	pm.updateTriggers(appliedRp);	
updated tez session pool manager with active resource plan 

private void failed(Throwable e) {	while (e.getCause() != null && e.getClass() == RuntimeException.class) {	e = e.getCause();	}	setException(e);	
Failed 

private int alterDatabase(Hive db, AlterDatabaseDesc alterDbDesc) throws HiveException {	String dbName = alterDbDesc.getDatabaseName();	Database database = db.getDatabase(dbName);	if (database == null) {	throw new HiveException(ErrorMsg.DATABASE_NOT_EXISTS, dbName);	}	Map<String, String> params = database.getParameters();	if ((null != alterDbDesc.getReplicationSpec()) && !alterDbDesc.getReplicationSpec().allowEventReplacementInto(params)) {	
ddltask alter database is skipped as database is newer than update 

case ALTER_OWNER: database.setOwnerName(alterDbDesc.getOwnerPrincipal().getName());	database.setOwnerType(alterDbDesc.getOwnerPrincipal().getType());	break;	case ALTER_LOCATION: try {	String newLocation = alterDbDesc.getLocation();	URI locationURI = new URI(newLocation);	if (   !locationURI.isAbsolute() || StringUtils.isBlank(locationURI.getScheme())) {	throw new HiveException(ErrorMsg.BAD_LOCATION_VALUE, newLocation);	}	if (newLocation.equals(database.getLocationUri())) {	
alterdatabase skipped no change in location 

default: console.printError("Unsupported Alter command");	return 1;	}	if (!updateModifiedParameters(idx.getParameters(), conf)) {	return 1;	}	try {	db.alterIndex(baseTableName, indexName, idx);	} catch (InvalidOperationException e) {	console.printError("Invalid alter operation: " + e.getMessage());	
alter index 

private int renamePartition(Hive db, RenamePartitionDesc renamePartitionDesc) throws HiveException {	String tableName = renamePartitionDesc.getTableName();	LinkedHashMap<String, String> oldPartSpec = renamePartitionDesc.getOldPartSpec();	if (!allowOperationInReplicationScope(db, tableName, oldPartSpec, renamePartitionDesc.getReplicationSpec())) {	if (LOG.isDebugEnabled()) {	
ddltask rename partition is skipped as table partition is newer than update 

private int renamePartition(Hive db, RenamePartitionDesc renamePartitionDesc) throws HiveException {	String tableName = renamePartitionDesc.getTableName();	LinkedHashMap<String, String> oldPartSpec = renamePartitionDesc.getOldPartSpec();	if (!allowOperationInReplicationScope(db, tableName, oldPartSpec, renamePartitionDesc.getReplicationSpec())) {	if (LOG.isDebugEnabled()) {	}	return 0;	}	String names[] = Utilities.getDbTableName(tableName);	if (Utils.isBootstrapDumpInProgress(db, names[0])) {	
ddltask rename partition not allowed as bootstrap dump in progress 

if (batchSize == 0) {	batchSize = partsNotInMs.size();	}	try {	createPartitionsInBatches(db, repairOutput, partsNotInMs, table, batchSize, decayingFactor, maxRetries);	} catch (Exception e) {	throw new HiveException(e);	}	}	} catch (HiveException e) {	
failed to run metacheck 

}	try {	createPartitionsInBatches(db, repairOutput, partsNotInMs, table, batchSize, decayingFactor, maxRetries);	} catch (Exception e) {	throw new HiveException(e);	}	}	} catch (HiveException e) {	return 1;	} catch (IOException e) {	
failed to run metacheck 

firstWritten |= writeMsckResult(result.getPartitionsNotOnFs(), "Partitions missing from filesystem:", resultOut, firstWritten);	for (String rout : repairOutput) {	if (firstWritten) {	resultOut.write(terminator);	} else {	firstWritten = true;	}	resultOut.write(rout);	}	} catch (IOException e) {	
failed to save metacheck output 

}	resultOut.write(rout);	}	} catch (IOException e) {	return 1;	} finally {	if (resultOut != null) {	try {	resultOut.close();	} catch (IOException e) {	
failed to close output file 

createTab_stmt.add(LIST_PARTITIONS, tbl_partitions);	createTab_stmt.add(SORT_BUCKET, tbl_sort_bucket);	createTab_stmt.add(SKEWED_INFO, tbl_skewedinfo);	createTab_stmt.add(ROW_FORMAT, tbl_row_format);	if (needsLocation) {	createTab_stmt.add(TBL_LOCATION, tbl_location);	}	createTab_stmt.add(TBL_PROPERTIES, tbl_properties);	outStream.write(createTab_stmt.render().getBytes(StandardCharsets.UTF_8));	} catch (IOException e) {	
show create table 

boolean isOutputPadded = !SessionState.get().isHiveServerQuery();	DataOutputStream outStream = getOutputStream(showIndexes.getResFile());	try {	if (showIndexes.isFormatted()) {	outStream.write(MetaDataFormatUtils.getIndexColumnsHeader().getBytes(StandardCharsets.UTF_8));	}	for (Index index : indexes) {	outStream.write(MetaDataFormatUtils.getIndexInformation(index, isOutputPadded).getBytes(StandardCharsets.UTF_8));	}	} catch (FileNotFoundException e) {	
show indexes 

try {	if (showIndexes.isFormatted()) {	outStream.write(MetaDataFormatUtils.getIndexColumnsHeader().getBytes(StandardCharsets.UTF_8));	}	for (Index index : indexes) {	outStream.write(MetaDataFormatUtils.getIndexInformation(index, isOutputPadded).getBytes(StandardCharsets.UTF_8));	}	} catch (FileNotFoundException e) {	throw new HiveException(e.toString());	} catch (IOException e) {	
show indexes 

private int showDatabases(Hive db, ShowDatabasesDesc showDatabasesDesc) throws HiveException {	List<String> databases = null;	if (showDatabasesDesc.getPattern() != null) {	
pattern 

private int showDatabases(Hive db, ShowDatabasesDesc showDatabasesDesc) throws HiveException {	List<String> databases = null;	if (showDatabasesDesc.getPattern() != null) {	databases = db.getDatabasesByPattern(showDatabasesDesc.getPattern());	} else {	databases = db.getAllDatabases();	}	
results 

private int showTablesOrViews(Hive db, ShowTablesDesc showDesc) throws HiveException {	List<String> tablesOrViews = null;	String dbName      = showDesc.getDbName();	String pattern     = showDesc.getPattern();	String resultsFile = showDesc.getResFile();	TableType type     = showDesc.getType();	if (!db.databaseExists(dbName)) {	throw new HiveException(ErrorMsg.DATABASE_NOT_EXISTS, dbName);	}	
pattern 

private int showTablesOrViews(Hive db, ShowTablesDesc showDesc) throws HiveException {	List<String> tablesOrViews = null;	String dbName      = showDesc.getDbName();	String pattern     = showDesc.getPattern();	String resultsFile = showDesc.getResFile();	TableType type     = showDesc.getType();	if (!db.databaseExists(dbName)) {	throw new HiveException(ErrorMsg.DATABASE_NOT_EXISTS, dbName);	}	tablesOrViews = db.getTablesByType(dbName, pattern, type);	
results 

private int showFunctions(Hive db, ShowFunctionsDesc showFuncs) throws HiveException {	Set<String> funcs = null;	if (showFuncs.getPattern() != null) {	
pattern 

private int showFunctions(Hive db, ShowFunctionsDesc showFuncs) throws HiveException {	Set<String> funcs = null;	if (showFuncs.getPattern() != null) {	if (showFuncs.getIsLikePattern()) {	funcs = FunctionRegistry.getFunctionNamesByLikePattern(showFuncs.getPattern());	} else {	console.printInfo("SHOW FUNCTIONS is deprecated, please use SHOW FUNCTIONS LIKE instead.");	funcs = FunctionRegistry.getFunctionNames(showFuncs.getPattern());	}	
results 

DataOutputStream outStream = getOutputStream(showFuncs.getResFile());	try {	SortedSet<String> sortedFuncs = new TreeSet<String>(funcs);	sortedFuncs.removeAll(serdeConstants.PrimitiveTypes);	Iterator<String> iterFuncs = sortedFuncs.iterator();	while (iterFuncs.hasNext()) {	outStream.writeBytes(iterFuncs.next());	outStream.write(terminator);	}	} catch (FileNotFoundException e) {	
show function 

SortedSet<String> sortedFuncs = new TreeSet<String>(funcs);	sortedFuncs.removeAll(serdeConstants.PrimitiveTypes);	Iterator<String> iterFuncs = sortedFuncs.iterator();	while (iterFuncs.hasNext()) {	outStream.writeBytes(iterFuncs.next());	outStream.write(terminator);	}	} catch (FileNotFoundException e) {	return 1;	} catch (IOException e) {	
show function 

outStream.writeBytes("LOCK_TIME:" + lockData.getLockTime());	outStream.write(terminator);	outStream.writeBytes("LOCK_MODE:" + lockData.getLockMode());	outStream.write(terminator);	outStream.writeBytes("LOCK_QUERYSTRING:" + lockData.getQueryStr());	}	}	outStream.write(terminator);	}	} catch (FileNotFoundException e) {	
show function 

outStream.writeBytes("LOCK_MODE:" + lockData.getLockMode());	outStream.write(terminator);	outStream.writeBytes("LOCK_QUERYSTRING:" + lockData.getQueryStr());	}	}	outStream.write(terminator);	}	} catch (FileNotFoundException e) {	return 1;	} catch (IOException e) {	
show function 

valList.add(partVal);	}	String partName = FileUtils.makePartName(keyList, valList);	rqst.setPartname(partName);	}	ShowLocksResponse rsp = lockMgr.getLocks(rqst);	DataOutputStream os = getOutputStream(showLocks.getResFile());	try {	dumpLockInfo(os, rsp);	} catch (FileNotFoundException e) {	
show function 

String partName = FileUtils.makePartName(keyList, valList);	rqst.setPartname(partName);	}	ShowLocksResponse rsp = lockMgr.getLocks(rqst);	DataOutputStream os = getOutputStream(showLocks.getResFile());	try {	dumpLockInfo(os, rsp);	} catch (FileNotFoundException e) {	return 1;	} catch (IOException e) {	
show function 

os.write(separator);	os.writeBytes(e.isSetStart() ? Long.toString(e.getStart()) : noVal);	os.write(separator);	os.writeBytes(e.isSetEndTime() ? Long.toString(e.getEndTime() - e.getStart()) : noVal);	os.write(separator);	os.writeBytes(e.isSetHadoopJobId() ?  e.getHadoopJobId() : noVal);	os.write(terminator);	}	}	} catch (IOException e) {	
show compactions 

os.writeBytes(Long.toString(txn.getStartedTime()));	os.write(separator);	os.writeBytes(Long.toString(txn.getLastHeartbeatTime()));	os.write(separator);	os.writeBytes(txn.getUser());	os.write(separator);	os.writeBytes(txn.getHostname());	os.write(terminator);	}	} catch (IOException e) {	
show transactions 

private int killQuery(Hive db, KillQueryDesc desc) throws HiveException {	SessionState sessionState = SessionState.get();	for (String queryId : desc.getQueryIds()) {	sessionState.getKillQuery().killQuery(queryId, "User invoked KILL QUERY");	}	
kill query called 

outStream.writeBytes("Function type:" + functionInfo.getFunctionType() + "\n");	FunctionResource[] resources = functionInfo.getResources();	if (resources != null) {	for (FunctionResource resource : resources) {	outStream.writeBytes("Resource:" + resource.getResourceURI() + "\n");	}	}	}	}	} catch (FileNotFoundException e) {	
describe function 

if (resources != null) {	for (FunctionResource resource : resources) {	outStream.writeBytes("Resource:" + resource.getResourceURI() + "\n");	}	}	}	}	} catch (FileNotFoundException e) {	return 1;	} catch (IOException e) {	
describe function 

Map<String, String> part = showTblStatus.getPartSpec();	Partition par = null;	if (part != null) {	Table tbl = db.getTable(showTblStatus.getDbName(), showTblStatus.getPattern());	par = db.getPartition(tbl, part, false);	if (par == null) {	throw new HiveException("Partition " + part + " for table " + showTblStatus.getPattern() + " does not exist.");	}	tbls.add(tbl);	} else {	
pattern 

tbls.add(tbl);	} else {	List<String> tblStr = db.getTablesForDb(showTblStatus.getDbName(), showTblStatus.getPattern());	SortedSet<String> sortedTbls = new TreeSet<String>(tblStr);	Iterator<String> iterTbls = sortedTbls.iterator();	while (iterTbls.hasNext()) {	String tblName = iterTbls.next();	Table tbl = db.getTable(showTblStatus.getDbName(), tblName);	tbls.add(tbl);	}	
results 

private int showTableProperties(Hive db, ShowTblPropertiesDesc showTblPrpt) throws HiveException {	String tableName = showTblPrpt.getTableName();	Table tbl = db.getTable(tableName, false);	try {	if (tbl == null) {	String errMsg = "Table " + tableName + " does not exist";	writeToFile(errMsg, showTblPrpt.getResFile());	return 0;	}	
ddltask show properties for 

builder.append(propertyValue);	}	}	else {	Map<String, String> properties = new TreeMap<String, String>(tbl.getParameters());	for (Entry<String, String> entry : properties.entrySet()) {	appendNonNull(builder, entry.getKey(), true);	appendNonNull(builder, entry.getValue());	}	}	
ddltask written data for showing properties of 

}	else {	Map<String, String> properties = new TreeMap<String, String>(tbl.getParameters());	for (Entry<String, String> entry : properties.entrySet()) {	appendNonNull(builder, entry.getKey(), true);	appendNonNull(builder, entry.getValue());	}	}	writeToFile(builder.toString(), showTblPrpt.getResFile());	} catch (FileNotFoundException e) {	
show table properties 

Map<String, String> properties = new TreeMap<String, String>(tbl.getParameters());	for (Entry<String, String> entry : properties.entrySet()) {	appendNonNull(builder, entry.getKey(), true);	appendNonNull(builder, entry.getValue());	}	}	writeToFile(builder.toString(), showTblPrpt.getResFile());	} catch (FileNotFoundException e) {	return 1;	} catch (IOException e) {	
show table properties 

Partition part = null;	if (descTbl.getPartSpec() != null) {	part = db.getPartition(tbl, descTbl.getPartSpec(), false);	if (part == null) {	throw new HiveException(ErrorMsg.INVALID_PARTITION, StringUtils.join(descTbl.getPartSpec().keySet(), ','), tableName);	}	tbl = part.getTable();	}	DataOutputStream outStream = getOutputStream(descTbl.getResFile());	try {	
ddltask got data for 

NotNullConstraint nnInfo = null;	if (descTbl.isExt() || descTbl.isFormatted()) {	pkInfo = db.getPrimaryKeys(tbl.getDbName(), tbl.getTableName());	fkInfo = db.getForeignKeys(tbl.getDbName(), tbl.getTableName());	ukInfo = db.getUniqueConstraints(tbl.getDbName(), tbl.getTableName());	nnInfo = db.getNotNullConstraints(tbl.getDbName(), tbl.getTableName());	}	fixDecimalColumnTypeName(cols);	boolean isOutputPadded = !SessionState.get().isHiveServerQuery();	formatter.describeTable(outStream, colPath, tableName, tbl, part, cols, descTbl.isFormatted(), descTbl.isExt(), isOutputPadded, colStats, pkInfo, fkInfo, ukInfo, nnInfo);	
ddltask written data for 

private int alterTable(Hive db, AlterTableDesc alterTbl) throws HiveException {	if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.RENAME) {	String names[] = Utilities.getDbTableName(alterTbl.getOldName());	if (Utils.isBootstrapDumpInProgress(db, names[0])) {	
ddltask rename table not allowed as bootstrap dump in progress 

}	}	try {	if (allPartitions == null) {	db.alterTable(alterTbl.getOldName(), tbl, alterTbl.getIsCascade(), alterTbl.getEnvironmentContext());	} else {	db.alterPartitions(Warehouse.getQualifiedName(tbl.getTTable()), allPartitions, alterTbl.getEnvironmentContext());	}	addConstraints(db, alterTbl);	} catch (InvalidOperationException e) {	
alter table 

static boolean addIfAbsentByName(WriteEntity newWriteEntity, Set<WriteEntity> outputs) {	for(WriteEntity writeEntity : outputs) {	if(writeEntity.getName().equalsIgnoreCase(newWriteEntity.getName())) {	
ignoring request to add because is present 

if (isOrcSchemaEvolution) {	final List<FieldSchema> existingCols = sd.getCols();	final List<FieldSchema> replaceCols = alterTbl.getNewCols();	if (replaceCols.size() < existingCols.size()) {	throw new HiveException(ErrorMsg.REPLACE_CANNOT_DROP_COLUMNS, alterTbl.getOldName());	}	}	boolean partitioned = tbl.isPartitioned();	boolean droppingColumns = alterTbl.getNewCols().size() < sd.getCols().size();	if (ParquetHiveSerDe.isParquetTable(tbl) && isSchemaEvolutionEnabled(tbl) && !alterTbl.getIsCascade() && droppingColumns && partitioned) {	
cannot drop columns from a partitioned parquet table without the cascade option 

private static void ensureDelete(FileSystem fs, Path path, String what) throws IOException {	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
deleting 

if (tbl.getPartitionKeys().size() > 0) {	PartitionIterable parts = new PartitionIterable(db, tbl, null, HiveConf.getIntVar(conf, ConfVars.METASTORE_BATCH_RETRIEVE_MAX));	Iterator<Partition> partIter = parts.iterator();	while (partIter.hasNext()) {	Partition part = partIter.next();	checkMmLb(part);	Path src = part.getDataLocation(), tgt = new Path(src, mmDir);	srcs.add(src);	tgts.add(tgt);	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
will move to 

tgts.add(tgt);	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	}	}	} else {	checkMmLb(tbl);	Path src = tbl.getDataLocation(), tgt = new Path(src, mmDir);	srcs.add(src);	tgts.add(tgt);	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
will move to 

private int addConstraints(Hive db, AlterTableDesc alterTbl) throws SemanticException, HiveException {	try {	if (alterTbl.getPrimaryKeyCols() != null && !alterTbl.getPrimaryKeyCols().isEmpty()) {	db.addPrimaryKey(alterTbl.getPrimaryKeyCols());	}	if (alterTbl.getForeignKeyCols() != null && !alterTbl.getForeignKeyCols().isEmpty()) {	try {	db.addForeignKey(alterTbl.getForeignKeyCols());	} catch (HiveException e) {	if (e.getCause() instanceof InvalidObjectException && alterTbl.getReplicationSpec()!= null && alterTbl.getReplicationSpec().isInReplicationScope()) {	
invalidobjectexception 

}	ReplicationSpec replicationSpec = dropTbl.getReplicationSpec();	if ((tbl!= null) && replicationSpec.isInReplicationScope()){	if (!replicationSpec.allowEventReplacementInto(tbl.getParameters())){	if (tbl.isPartitioned()){	PartitionIterable partitions = new PartitionIterable(db,tbl,null, conf.getIntVar(HiveConf.ConfVars.METASTORE_BATCH_RETRIEVE_MAX));	for (Partition p : Iterables.filter(partitions, replicationSpec.allowEventReplacementInto())){	db.dropPartition(tbl.getDbName(),tbl.getTableName(),p.getValues(),true);	}	}	
ddltask drop table is skipped as table is newer than update 

public static void validateSerDe(String serdeName, HiveConf conf) throws HiveException {	try {	Deserializer d = ReflectionUtil.newInstance(conf.getClassByName(serdeName). asSubclass(Deserializer.class), conf);	if (d != null) {	
found class for 

throw new HiveException(ErrorMsg.DATABASE_NOT_EXISTS, dbName);	}	SessionState.get().setCurrentDatabase(dbName);	Database database = db.getDatabase(dbName);	assert(database != null);	Map<String, String> dbParams = database.getParameters();	if (dbParams != null) {	for (HiveConf.ConfVars var: HiveConf.dbVars) {	String newValue = dbParams.get(var.varname);	if (newValue != null) {	
changing from to 

private int createTable(Hive db, CreateTableDesc crtTbl) throws HiveException {	Table tbl = crtTbl.toTable(conf);	List<SQLPrimaryKey> primaryKeys = crtTbl.getPrimaryKeys();	List<SQLForeignKey> foreignKeys = crtTbl.getForeignKeys();	List<SQLUniqueConstraint> uniqueConstraints = crtTbl.getUniqueConstraints();	List<SQLNotNullConstraint> notNullConstraints = crtTbl.getNotNullConstraints();	
creating table on 

List<SQLPrimaryKey> primaryKeys = crtTbl.getPrimaryKeys();	List<SQLForeignKey> foreignKeys = crtTbl.getForeignKeys();	List<SQLUniqueConstraint> uniqueConstraints = crtTbl.getUniqueConstraints();	List<SQLNotNullConstraint> notNullConstraints = crtTbl.getNotNullConstraints();	if (crtTbl.getReplicationSpec().isInReplicationScope() && (!crtTbl.getReplaceMode())){	Table existingTable = db.getTable(tbl.getDbName(), tbl.getTableName(), false);	if (existingTable != null){	if (crtTbl.getReplicationSpec().allowEventReplacementInto(existingTable.getParameters())){	crtTbl.setReplaceMode(true);	} else {	
ddltask create table is skipped as table is newer than update 

tbl.getTTable().getParameters().putAll(crtTbl.getTblProps());	}	tbl.setTableType(TableType.MANAGED_TABLE);	if (crtTbl.isExternal()) {	tbl.setProperty("EXTERNAL", "TRUE");	tbl.setTableType(TableType.EXTERNAL_TABLE);	}	tbl.setFields(oldtbl.getCols());	tbl.setPartCols(oldtbl.getPartCols());	if (crtTbl.getDefaultSerName() == null) {	
default to lazysimpleserde for table 

}	if (crtTbl.getTblProps() != null) {	params.putAll(crtTbl.getTblProps());	}	if (crtTbl.isUserStorageFormat()) {	tbl.setInputFormatClass(crtTbl.getDefaultInputFormat());	tbl.setOutputFormatClass(crtTbl.getDefaultOutputFormat());	tbl.getTTable().getSd().setInputFormat( tbl.getInputFormatClass().getName());	tbl.getTTable().getSd().setOutputFormat( tbl.getOutputFormatClass().getName());	if (crtTbl.getDefaultSerName() == null) {	
default to lazysimpleserde for like table 

private int createView(Hive db, CreateViewDesc crtView) throws HiveException {	Table oldview = db.getTable(crtView.getViewName(), false);	if (oldview != null) {	if (crtView.getReplicationSpec().isInReplicationScope()) {	if (crtView.getReplicationSpec().allowEventReplacementInto(oldview.getParameters())){	crtView.setReplace(true);	} else {	
ddltask create view is skipped as view is newer than update 

========================= hive sample_3847 =========================

protected void initializeOp(Configuration hconf) throws HiveException {	super.initializeOp(hconf);	try {	numRows = 0;	cntr = 1;	logEveryNRows = HiveConf.getLongVar(hconf, HiveConf.ConfVars.HIVE_LOG_N_RECORDS);	statsMap.put(getCounterName(Counter.RECORDS_OUT_INTERMEDIATE, hconf), recordCounter);	List<ExprNodeDesc> keys = conf.getKeyCols();	if (LOG.isDebugEnabled()) {	
keys size is 

protected void initializeOp(Configuration hconf) throws HiveException {	super.initializeOp(hconf);	try {	numRows = 0;	cntr = 1;	logEveryNRows = HiveConf.getLongVar(hconf, HiveConf.ConfVars.HIVE_LOG_N_RECORDS);	statsMap.put(getCounterName(Counter.RECORDS_OUT_INTERMEDIATE, hconf), recordCounter);	List<ExprNodeDesc> keys = conf.getKeyCols();	if (LOG.isDebugEnabled()) {	for (ExprNodeDesc k : keys) {	
key exprnodedesc 

public void process(Object row, int tag) throws HiveException {	try {	ObjectInspector rowInspector = inputObjInspectors[tag];	if (firstRow) {	firstRow = false;	if (LOG.isInfoEnabled()) {	
keys are num distributions 

keyHashCode = random.nextInt();	} else {	Object[] bucketFieldValues = new Object[partitionEval.length];	for(int i = 0; i < partitionEval.length; i++) {	bucketFieldValues[i] = partitionEval[i].evaluate(row);	}	keyHashCode = ObjectInspectorUtils.getBucketHashCode(bucketFieldValues, partitionObjectInspectors);	}	int hashCode = buckNum < 0 ? keyHashCode : keyHashCode * 31 + buckNum;	if (LOG.isTraceEnabled()) {	
going to return hash code 

if (null != out) {	numRows++;	runTimeNumRows++;	if (LOG.isTraceEnabled()) {	if (numRows == cntr) {	cntr = logEveryNRows == 0 ? cntr * 10 : numRows + logEveryNRows;	if (cntr < 0 || numRows < 0) {	cntr = 0;	numRows = 1;	}	
records written 

protected void closeOp(boolean abort) throws HiveException {	if (!abort && reducerHash != null) {	reducerHash.flush();	}	super.closeOp(abort);	out = null;	random = null;	reducerHash = null;	if (LOG.isTraceEnabled()) {	
records written 

========================= hive sample_3902 =========================

} else {	assert(1 == qlPtns.size());	dataPath = new Path(withinContext.eventRoot, qlPtns.get(0).getName());	}	try (BufferedWriter fileListWriter = writer(withinContext, dataPath)) {	for (String file : files) {	fileListWriter.write(file + "\n");	}	}	}	
processing insert message 

========================= hive sample_3451 =========================

runStatementOnDriver("insert into " + Table.NONACIDORCTBL + "(a,b) values(1,2),(1,3)");	runStatementOnDriver("insert into " + Table.NONACIDORCTBL + "(a,b) values(0,12),(0,13),(1,4),(1,5)");	runStatementOnDriver("insert into " + Table.NONACIDORCTBL + "(a,b) values(1,6)");	runStatementOnDriver("alter table " + Table.NONACIDORCTBL + " SET TBLPROPERTIES ('transactional'='true', 'transactional_properties'='default')");	List<String> rs1 = runStatementOnDriver("describe "+ Table.NONACIDORCTBL);	runStatementOnDriver("insert into " + Table.NONACIDORCTBL + "(a,b) values(0,15),(1,16)");	runStatementOnDriver("update " + Table.NONACIDORCTBL + " set b = 120 where a = 0 and b = 12");	runStatementOnDriver("insert into " + Table.NONACIDORCTBL + "(a,b) values(0,17)");	runStatementOnDriver("delete from " + Table.NONACIDORCTBL + " where a = 1 and b = 3");	List<String> rs = runStatementOnDriver("select ROW__ID, a, b, INPUT__FILE__NAME from " +  Table.NONACIDORCTBL + " order by a,b");	
before compact 

String[][] expected = {	{"{\"transactionid\":0,\"bucketid\":536870912,\"rowid\":0}\t0\t13",  "bucket_00000"}, {"{\"transactionid\":20,\"bucketid\":536870912,\"rowid\":0}\t0\t15", "bucket_00000"}, {"{\"transactionid\":22,\"bucketid\":536870912,\"rowid\":0}\t0\t17", "bucket_00000"}, {"{\"transactionid\":21,\"bucketid\":536870912,\"rowid\":0}\t0\t120", "bucket_00000"}, {"{\"transactionid\":0,\"bucketid\":536936448,\"rowid\":1}\t1\t2",   "bucket_00001"}, {"{\"transactionid\":0,\"bucketid\":536936448,\"rowid\":3}\t1\t4",   "bucket_00001"}, {"{\"transactionid\":0,\"bucketid\":536936448,\"rowid\":2}\t1\t5",   "bucket_00001"}, {"{\"transactionid\":0,\"bucketid\":536936448,\"rowid\":4}\t1\t6",   "bucket_00001"}, {"{\"transactionid\":20,\"bucketid\":536936448,\"rowid\":0}\t1\t16", "bucket_00001"}	};	Assert.assertEquals("Unexpected row count before compaction", expected.length, rs.size());	for(int i = 0; i < expected.length; i++) {	Assert.assertTrue("Actual line " + i + " bc: " + rs.get(i) + "; expected " + expected[i][0], rs.get(i).startsWith(expected[i][0]));	}	runStatementOnDriver("alter table "+ TestTxnCommands2.Table.NONACIDORCTBL +" compact 'major'");	TestTxnCommands2.runWorker(hiveConf);	rs = runStatementOnDriver("select ROW__ID, a, b, INPUT__FILE__NAME from " + Table.NONACIDORCTBL + " order by a,b");	
after compact 

protected List<String> runStatementOnDriver(String stmt) throws Exception {	
runstatementondriver 

========================= hive sample_2539 =========================

File jarFile1 = new File(jarFileName1);	try {	org.apache.commons.io.FileUtils.touch(jarFile1);	Set<String> jars = FileUtils.getJarFilesByPath(tmpDir.getAbsolutePath(), conf);	Assert.assertEquals(Sets.newHashSet("file: jars = FileUtils.getJarFilesByPath("/folder/not/exist", conf);	Assert.assertTrue(jars.isEmpty());	File jarFile2 = new File(jarFileName2);	org.apache.commons.io.FileUtils.touch(jarFile2);	String newPath = "file: jars = FileUtils.getJarFilesByPath(newPath, conf);	Assert.assertEquals(Sets.newHashSet("file: } catch (IOException e) {	
failed to copy file to reloading folder 

========================= hive sample_1315 =========================

if (useMinMax) {	min = hashMap.min();	max = hashMap.max();	}	needHashTableSetup = false;	}	batchCounter++;	final int inputLogicalSize = batch.size;	if (inputLogicalSize == 0) {	if (LOG.isDebugEnabled()) {	
batch empty 

}	boolean someRowsFilteredOut =  false;	if (bigTableFilterExpressions.length > 0) {	for (VectorExpression ve : bigTableFilterExpressions) {	ve.evaluate(batch);	}	someRowsFilteredOut = (batch.size != inputLogicalSize);	if (LOG.isDebugEnabled()) {	if (batch.selectedInUse) {	if (inputSelectedInUse) {	
inputselected filtered batch selected 

boolean someRowsFilteredOut =  false;	if (bigTableFilterExpressions.length > 0) {	for (VectorExpression ve : bigTableFilterExpressions) {	ve.evaluate(batch);	}	someRowsFilteredOut = (batch.size != inputLogicalSize);	if (LOG.isDebugEnabled()) {	if (batch.selectedInUse) {	if (inputSelectedInUse) {	} else {	
inputlogicalsize filtered batch selected 

joinResult = JoinUtil.JoinResult.NOMATCH;	} else {	long key = vector[0];	if (useMinMax && (key < min || key > max)) {	joinResult = JoinUtil.JoinResult.NOMATCH;	} else {	joinResult = hashMap.lookup(key, hashMapResults[0]);	}	}	if (LOG.isDebugEnabled()) {	
batch repeated joinresult 

joinResult = JoinUtil.JoinResult.NOMATCH;	} else {	joinResult = hashMap.lookup(key, hashMapResults[0]);	}	}	if (LOG.isDebugEnabled()) {	}	finishOuterRepeated(batch, joinResult, hashMapResults[0], someRowsFilteredOut, inputSelectedInUse, inputLogicalSize);	} else {	if (LOG.isDebugEnabled()) {	
batch non repeated 

switch (saveJoinResult) {	case MATCH: hashMapResultCount++;	equalKeySeriesCount++;	break;	case SPILL: hashMapResultCount++;	break;	case NOMATCH: break;	}	}	if (LOG.isDebugEnabled()) {	
batch allmatchs equalkeyserieshashmapresultindices equalkeyseriesallmatchindices equalkeyseriesissinglevalue equalkeyseriesduplicatecounts atleastonenonmatch inputselectedinuse inputlogicalsize spills spillhashmapresultindices hashmapresults 

========================= hive sample_4117 =========================

public void checkOutputSpecs(JobContext context) throws IOException, InterruptedException {	for (String alias : getOutputFormatAliases(context)) {	
calling checkoutputspecs for alias 

public MultiRecordWriter(TaskAttemptContext context) throws IOException, InterruptedException {	baseRecordWriters = new LinkedHashMap<String, BaseRecordWriterContainer>();	String[] aliases = getOutputFormatAliases(context);	for (String alias : aliases) {	
creating record writer for alias 

public void close(TaskAttemptContext context) throws IOException, InterruptedException {	for (Entry<String, BaseRecordWriterContainer> entry : baseRecordWriters.entrySet()) {	BaseRecordWriterContainer baseRWContainer = entry.getValue();	
closing record writer for alias 

public MultiOutputCommitter(TaskAttemptContext context) throws IOException, InterruptedException {	outputCommitters = new LinkedHashMap<String, MultiOutputFormat.BaseOutputCommitterContainer>();	String[] aliases = getOutputFormatAliases(context);	for (String alias : aliases) {	
creating output committer for alias 

public void setupJob(JobContext jobContext) throws IOException {	for (String alias : outputCommitters.keySet()) {	
calling setupjob for alias 

public void setupTask(TaskAttemptContext taskContext) throws IOException {	for (String alias : outputCommitters.keySet()) {	
calling setuptask for alias 

public void commitTask(TaskAttemptContext taskContext) throws IOException {	for (String alias : outputCommitters.keySet()) {	BaseOutputCommitterContainer outputContainer = outputCommitters.get(alias);	OutputCommitter baseCommitter = outputContainer.getBaseCommitter();	TaskAttemptContext committerContext = outputContainer.getContext();	if (baseCommitter.needsTaskCommit(committerContext)) {	
calling committask for alias 

public void abortTask(TaskAttemptContext taskContext) throws IOException {	for (String alias : outputCommitters.keySet()) {	
calling aborttask for alias 

public void commitJob(JobContext jobContext) throws IOException {	for (String alias : outputCommitters.keySet()) {	
calling commitjob for alias 

public void abortJob(JobContext jobContext, State state) throws IOException {	for (String alias : outputCommitters.keySet()) {	
calling abortjob for alias 

========================= hive sample_767 =========================

public static void appendReadColumns( Configuration conf, List<Integer> ids, List<String> names, List<String> groupPaths) {	if (ids.size() != names.size()) {	
read column counts do not match ids names 

========================= hive sample_5213 =========================

String guidAttr = conf.getVar(HiveConf.ConfVars.HIVE_SERVER2_PLAIN_LDAP_GUIDKEY);	if (StringUtils.isNotBlank(defaultBaseDn)) {	result.add(guidAttr + "=%s," + defaultBaseDn);	}	} else {	String[] patterns = patternsString.split(":");	for (String pattern : patterns) {	if (pattern.contains(",") && pattern.contains("=")) {	result.add(pattern);	} else {	
unexpected format for ignoring 

========================= hive sample_2313 =========================

eval = new ExprNodeEvaluator[colList.size()];	for (int i = 0; i < colList.size(); i++) {	assert (colList.get(i) != null);	eval[i] = ExprNodeEvaluatorFactory.get(colList.get(i), hconf);	}	if (HiveConf.getBoolVar(hconf, HiveConf.ConfVars.HIVEEXPREVALUATIONCACHE)) {	eval = ExprNodeEvaluatorFactory.toCachedEvals(eval);	}	output = new Object[eval.length];	if (LOG.isInfoEnabled()) {	
select 

========================= hive sample_3851 =========================

public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx, Object... nodeOutputs) throws SemanticException {	BucketingSortingCtx bctx = (BucketingSortingCtx)procCtx;	FileSinkOperator fop = (FileSinkOperator)nd;	if (fop.getConf().isMmTable()) {	
currently inferring buckets is not going to work for mm tables by design 

========================= hive sample_3017 =========================

private Task<? extends Serializable> tasksForAddPartition(Table table, AddPartitionDesc addPartitionDesc) throws MetaException, IOException, HiveException {	AddPartitionDesc.OnePartitionDesc partSpec = addPartitionDesc.getPartition(0);	Path sourceWarehousePartitionLocation = new Path(partSpec.getLocation());	Path replicaWarehousePartitionLocation = locationOnReplicaWarehouse(table, partSpec);	partSpec.setLocation(replicaWarehousePartitionLocation.toString());	
adding dependent copywork addpart movework for partition with source location 

private TaskTracker forExistingTable(AddPartitionDesc lastPartitionReplicated) throws Exception {	boolean encounteredTheLastReplicatedPartition = (lastPartitionReplicated == null);	Map<String, String> lastReplicatedPartSpec = null;	if (!encounteredTheLastReplicatedPartition) {	lastReplicatedPartSpec = lastPartitionReplicated.getPartition(0).getPartSpec();	
start processing from partition info spec 

========================= hive sample_3914 =========================

public List<Path> getAliasBucketFiles(String refTableInputFile, String refTableAlias, String alias) {	List<String> pathStr=aliasBucketMapping.get(alias).get(refTableInputFile);	List<Path> paths = new ArrayList<Path>();	if(pathStr!=null) {	for (String p : pathStr) {	
loading file for 

========================= hive sample_4101 =========================

if (work.isUserLevelExplain()) {	JsonParser jsonParser = JsonParserFactory.getParser(conf);	work.getConfig().setFormatted(true);	JSONObject jsonPlan = getJSONPlan(out, work);	if (work.getCboInfo() != null) {	jsonPlan.put("cboInfo", work.getCboInfo());	}	try {	jsonParser.print(jsonPlan, out);	} catch (Exception e) {	
running explain user level has problem falling back to normal explain 

work.getConfig().setFormatted(false);	work.getConfig().setUserLevelExplain(false);	jsonPlan = getJSONPlan(out, work);	}	} else {	JSONObject jsonPlan = getJSONPlan(out, work);	if (work.isFormatted()) {	JsonParser jsonParser = JsonParserFactory.getParser(conf);	if (jsonParser != null) {	jsonParser.print(jsonPlan, null);	
jsonplan is augmented to 

========================= hive sample_4551 =========================

public OrcColumnVectorProducer(MetadataCache metadataCache, LowLevelCache lowLevelCache, BufferUsageManager bufferManager, Configuration conf, LlapDaemonCacheMetrics cacheMetrics, LlapDaemonIOMetrics ioMetrics, FixedSizedObjectPool<IoTrace> tracePool) {	
initializing orc column vector producer 

========================= hive sample_2190 =========================

hiveUDF = SqlFunctionConverter.getHiveUDF(call.getOperator(), call.getType(), call.operands.size());	if (hiveUDF != null && !FunctionRegistry.isConsistentWithinQuery(hiveUDF)) {	return null;	}	} finally {	if (hiveUDF != null) {	try {	hiveUDF.close();	} catch (IOException  e) {	if (LOG.isDebugEnabled()) {	
exception in closing 

========================= hive sample_2893 =========================

private void removeDynamicPartitionPruningSink(List<String> removedMapWorkList, SparkWork sparkWork) {	List<BaseWork> allWorks = sparkWork.getAllWork();	for (BaseWork baseWork : allWorks) {	Set<Operator<?>> rootOperators = baseWork.getAllRootOperators();	for (Operator root : rootOperators) {	List<Operator<?>> pruningList = new ArrayList<>();	SparkUtilities.collectOp(pruningList, root, SparkPartitionPruningSinkOperator.class);	for (Operator pruneSinkOp : pruningList) {	SparkPartitionPruningSinkOperator sparkPruneSinkOp = (SparkPartitionPruningSinkOperator) pruneSinkOp;	if (removedMapWorkList.contains(sparkPruneSinkOp.getConf().getTargetMapWork().getName())) {	
ready to remove the sparkprunesinkop which target work is because the mapwork is equals to other map work and has been deleted 

========================= hive sample_3105 =========================

}	} else {	}	i++;	}	ctx.setExplainConfig(config);	ctx.setExplainPlan(true);	ASTNode input = (ASTNode) ast.getChild(0);	if (config.getAnalyze() == AnalyzeState.RUNNING) {	String query = ctx.getTokenRewriteStream().toString(input.getTokenStartIndex(), input.getTokenStopIndex());	
explain analyze running phase for query 

}	config.setOpIdToRuntimeNumRows(aggregateStats(config.getExplainRootPath()));	} catch (IOException e1) {	throw new SemanticException(e1);	} catch (CommandNeedRetryException e) {	throw new SemanticException(e);	}	ctx.resetOpContext();	ctx.resetStream();	TaskFactory.resetId();	
explain analyze analyzing phase for query 

========================= hive sample_3555 =========================

batch.size = 1;	expr.evaluate(batch);	if (like.get()) {	positive++;	} else {	negative++;	}	assertEquals(String.format("Checking '%s' against '%s'", input, pattern), like.get(), (batch.size != 0));	}	}	
randomized testing ran d positive tests and d negative tests 

========================= hive sample_2651 =========================

private static int determineAllocSize(BufferUsageManager bufferManager, Configuration conf) {	long allocSize = HiveConf.getSizeVar(conf, ConfVars.LLAP_IO_ENCODE_ALLOC_SIZE);	int maxAllocSize = bufferManager.getAllocator().getMaxAllocation();	if (allocSize > maxAllocSize) {	
encode allocation size is being capped to the maximum allocation size 

public void stop() {	
encoded reader is being stopped 

public void discardData() {	
discarding disk data if any wasn t cached 

public void discardData() {	for (CacheStripeData stripe : stripes) {	if (stripe.colStreams == null || stripe.colStreams.isEmpty()) continue;	for (List<CacheStreamData> streams : stripe.colStreams.values()) {	for (CacheStreamData cos : streams) {	for (MemoryBuffer buffer : cos.data) {	if (LlapIoImpl.CACHE_LOGGER.isTraceEnabled()) {	
deallocating 

public OutputReceiver createDataStream(StreamName name) throws IOException {	OutputReceiver or = streams.get(name);	if (or != null) return or;	if (isNeeded(name)) {	if (LlapIoImpl.LOG.isTraceEnabled()) {	
creating cache receiver for 

CacheOutputReceiver cor = new CacheOutputReceiver(bufferManager, bufferFactory, name);	or = cor;	List<CacheOutputReceiver> list = colStreams.get(name.getColumn());	if (list == null) {	list = new ArrayList<>();	colStreams.put(name.getColumn(), list);	}	list.add(cor);	} else {	if (LlapIoImpl.LOG.isTraceEnabled()) {	
creating null receiver for 

}	}	currentStripe.rowCount = si.getNumberOfRows();	for (Map.Entry<Integer, List<CacheOutputReceiver>> e : colStreams.entrySet()) {	int colIx = e.getKey();	List<CacheOutputReceiver> streams = e.getValue();	List<CacheStreamData> data = new ArrayList<>(streams.size());	for (CacheOutputReceiver receiver : streams) {	List<MemoryBuffer> buffers = receiver.buffers;	if (buffers == null) {	
buffers are null for 

for (CacheOutputReceiver receiver : streams) {	List<MemoryBuffer> buffers = receiver.buffers;	if (buffers == null) {	}	data.add(new CacheStreamData(receiver.suppressed, receiver.name, buffers == null ? new ArrayList<MemoryBuffer>() : new ArrayList<>(buffers)));	receiver.clear();	}	if (doesSourceHaveIncludes) {	int newColIx = getSparseOrcIndexFromDenseDest(colIx);	if (LlapIoImpl.LOG.isTraceEnabled()) {	
mapping the orc writer column to 

public void output(ByteBuffer buffer) throws IOException {	if (LlapIoImpl.LOG.isTraceEnabled()) {	
receiving a buffer of size 

protected Void performDataRead() throws IOException, InterruptedException {	boolean isOk = false;	try {	try {	long startTime = counters.startTimeCounter();	
processing data for 

}	cachedData = null;	}	}	if (isFromCache == null) return null;	if (!isFromCache) {	if (!processOneFileSplit(split, startTime, Ref.from(0), null)) return null;	}	recordReaderTime(startTime);	if (LlapIoImpl.LOG.isTraceEnabled()) {	
done processing 

}	}	if (isFromCache == null) return null;	if (!isFromCache) {	if (!processOneFileSplit(split, startTime, Ref.from(0), null)) return null;	}	recordReaderTime(startTime);	if (LlapIoImpl.LOG.isTraceEnabled()) {	}	} catch (Throwable e) {	
exception while processing 

public Boolean readFileWithCache(long startTime) throws IOException, InterruptedException {	if (fileKey == null) return false;	BooleanRef gotAllData = new BooleanRef();	long endOfSplit = split.getStart() + split.getLength();	this.cachedData = cache.getFileData(fileKey, split.getStart(), endOfSplit, writerIncludes, CC_FACTORY, counters, gotAllData);	if (cachedData == null) {	if (LlapIoImpl.CACHE_LOGGER.isTraceEnabled()) {	
no data for the split found in cache 

long start = slice.getKnownTornStart();	long len = slice.getLastStart() - start;	FileSplit sliceSplit = new FileSplit(split.getPath(), start, len, hosts, inMemoryHosts);	if (!processOneFileSplit(sliceSplit, startTime, stripeIx, slice)) return null;	}	boolean isUnfortunate = false;	if (uncachedSuffixStart == endOfSplit) {	long size =  split.getPath().getFileSystem( daemonConf).getFileStatus(split.getPath()).getLen();	isUnfortunate = size > endOfSplit;	if (isUnfortunate) {	
one row mismatch at the end of split at file size is 

public boolean processOneFileSplit(FileSplit split, long startTime, Ref<Integer> stripeIxRef, StripeData slice) throws IOException, InterruptedException {	
processing one split 

public boolean processOneFileSplit(FileSplit split, long startTime, Ref<Integer> stripeIxRef, StripeData slice) throws IOException, InterruptedException {	if (LlapIoImpl.CACHE_LOGGER.isTraceEnabled()) {	
cache data for the split is 

if ((cacheEncodings[colIx] != null) != (slice.getData()[colIx] != null)) {	throw new AssertionError("Inconsistent cache slice " + slice);	}	if (cacheEncodings[colIx] != null) {	splitIncludes[colIx] = false;	} else {	hasAllData = false;	}	}	if (LlapIoImpl.CACHE_LOGGER.isTraceEnabled()) {	
includes accounting for cached data before after 

boolean hasAllData = diskData == null;	if (!hasAllData) {	sliceToCache = createSliceToCache(diskData, cacheData);	metadata.setEncodings(combineCacheAndWriterEncodings(cacheEncodings, diskData.encodings));	metadata.setRowCount(diskData.rowCount);	} else {	metadata.setEncodings(Lists.newArrayList(cacheEncodings));	metadata.setRowCount(cacheRowCount);	}	if (LlapIoImpl.LOG.isTraceEnabled()) {	
derived stripe metadata for this split is 

ecb.initColumn(colIx, OrcEncodedColumnBatch.MAX_DATA_STREAMS);	if (!hasAllData && splitIncludes[colIx]) {	List<CacheWriter.CacheStreamData> streams = diskData.colStreams.get(colIx);	LlapSerDeDataBuffer[][] newCacheDataForCol = createArrayToCache(sliceToCache, colIx, streams);	if (streams == null) continue;	Iterator<CacheWriter.CacheStreamData> iter = streams.iterator();	while (iter.hasNext()) {	CacheWriter.CacheStreamData stream = iter.next();	if (stream.isSuppressed) {	if (LlapIoImpl.LOG.isTraceEnabled()) {	
removing a suppressed stream 

}	} else {	processColumnCacheData(cacheBuffers, ecb, colIx);	}	}	if (processStop()) {	recordReaderTime(startTime);	return false;	}	if (LlapIoImpl.CACHE_LOGGER.isTraceEnabled()) {	
data to cache from the read 

}	ColumnEncoding[] cacheEncodings = cacheData == null ? null : cacheData.getEncodings();	LlapSerDeDataBuffer[][][] cacheBuffers = cacheData == null ? null : cacheData.getData();	if (cacheData != null) {	validateCacheAndDisk(cacheData, diskData.getRowCount(), -1, diskData);	}	SerDeStripeMetadata metadata = new SerDeStripeMetadata(stripeIx);	metadata.setEncodings(Arrays.asList(cacheEncodings == null ? new ColumnEncoding[splitIncludes.length] : cacheEncodings));	metadata.setRowCount(diskData.getRowCount());	if (LlapIoImpl.LOG.isTraceEnabled()) {	
derived stripe metadata for this split is 

consumer.setStripeMetadata(metadata);	OrcEncodedColumnBatch ecb = ECB_POOL.take();	ecb.init(fileKey, metadata.getStripeIx(), OrcEncodedColumnBatch.ALL_RGS, writerIncludes.length);	int vectorsIx = 0;	for (int colIx = 0; colIx < writerIncludes.length; ++colIx) {	if (!writerIncludes[colIx]) continue;	if (splitIncludes[colIx]) {	if (colIx != 0 ) {	List<ColumnVector> vectors = diskData.getVectors(vectorsIx++);	if (LlapIoImpl.LOG.isTraceEnabled()) {	
processing vectors for column 

for (int colIx = 0; colIx < splitIncludes.length; ++colIx) {	if (!splitIncludes[colIx]) continue;	List<CacheWriter.CacheStreamData> streams = diskData.colStreams.get(colIx);	LlapSerDeDataBuffer[][] newCacheDataForCol = createArrayToCache(sliceToCache, colIx, streams);	if (streams == null) continue;	Iterator<CacheWriter.CacheStreamData> iter = streams.iterator();	while (iter.hasNext()) {	CacheWriter.CacheStreamData stream = iter.next();	if (stream.isSuppressed) {	if (LlapIoImpl.LOG.isTraceEnabled()) {	
removing a suppressed stream 

if (LlapIoImpl.LOG.isTraceEnabled()) {	}	iter.remove();	discardUncachedBuffers(stream.data);	continue;	}	setStreamDataToCache(newCacheDataForCol, stream);	}	}	if (LlapIoImpl.CACHE_LOGGER.isTraceEnabled()) {	
data to cache from async read 

private StripeData createSliceToCache( CacheWriter.CacheStripeData diskData, StripeData cacheData) throws IOException {	assert diskData != null;	if (cacheData == null) {	return new StripeData(diskData.knownTornStart, diskData.firstRowStart, diskData.lastRowStart, diskData.lastRowEnd, diskData.rowCount, diskData.encodings.toArray(new ColumnEncoding[diskData.encodings.size()]));	} else {	long rowCount = diskData.rowCount, encodingCount = diskData.encodings.size();	validateCacheAndDisk(cacheData, rowCount, encodingCount, diskData);	if (LlapIoImpl.LOG.isDebugEnabled()) {	
creating slice to cache in addition to an existing slice disk offsets were 

private static LlapSerDeDataBuffer[][] createArrayToCache( StripeData sliceToCache, int colIx, List<CacheWriter.CacheStreamData> streams) {	if (LlapIoImpl.LOG.isTraceEnabled()) {	
processing streams for column 

private void processColumnCacheData(LlapSerDeDataBuffer[][][] cacheBuffers, OrcEncodedColumnBatch ecb, int colIx) {	LlapSerDeDataBuffer[][] colData = cacheBuffers[colIx];	if (LlapIoImpl.CACHE_LOGGER.isTraceEnabled()) {	
processing cache data for column 

}	try {	Vectors result = null;	if (rowsPerSlice > 0 || (!maySplitTheSplit && hasUnsplittableData)) {	long fileOffset = -1;	if (!offsetReader.hasOffsets()) {	firstStartOffset = split.getStart() + 1;	lastStartOffset = split.getStart() + split.getLength();	fileOffset = lastStartOffset + 1;	if (LlapIoImpl.CACHE_LOGGER.isDebugEnabled()) {	
cache offsets based on the split first row at last row at 

private void closeOffsetReader() {	if (offsetReader == null) return;	try {	offsetReader.close();	} catch (Exception ex) {	
failed to close source reader 

ReaderWithOffsets offsetReader = null;	RecordReader sourceReader = sourceInputFormat.getRecordReader(split, jobConf, reporter);	try {	offsetReader = createOffsetReader(sourceReader);	sourceReader = null;	} finally {	if (sourceReader != null) {	try {	sourceReader.close();	} catch (Exception ex) {	
failed to close source reader 

asyncWriter.startAsync(new AsyncCacheDataCallback());	this.asyncWriters.add(asyncWriter);	}	currentFileRead = new FileReaderYieldReturn( offsetReader, split, writer, maySplitTheSplit, targetSliceRowCount);	} finally {	if (currentFileRead != null) return;	if (offsetReader == null) return;	try {	offsetReader.close();	} catch (Exception ex) {	
failed to close source reader 

CacheWriter cacheWriter = null;	try {	cacheWriter = writer.getCacheWriter();	boolean[] cacheIncludes = writer.getOriginalCacheIncludes();	Iterator<CacheWriter.CacheStripeData> iter = cacheWriter.stripes.iterator();	while (iter.hasNext()) {	processAsyncCacheData(iter.next(), cacheIncludes);	iter.remove();	}	} catch (IOException e) {	
failed to cache async data 

public abstract void setCurrentStripeOffsets(long currentKnownTornStart, long firstStartOffset, long lastStartOffset, long fileOffset);	public abstract void flushIntermediateData() throws IOException;	public abstract void writeIntermediateFooter() throws IOException;	public abstract List<VectorizedRowBatch> extractCurrentVrbs();	public void close() throws IOException {	if (orcWriter != null) {	try {	orcWriter.close();	orcWriter = null;	} catch (Exception ex) {	
failed to close orc writer 

orcWriter.close();	orcWriter = null;	} catch (Exception ex) {	}	}	if (cacheWriter != null) {	try {	cacheWriter.discardData();	cacheWriter = null;	} catch (Exception ex) {	
failed to close cache writer 

private ReaderWithOffsets createOffsetReader(RecordReader<?, ?> sourceReader) {	if (LlapIoImpl.LOG.isDebugEnabled()) {	
using to read data 

private boolean sendEcbToConsumer(OrcEncodedColumnBatch ecb, boolean hasCachedSlice, CacheWriter.CacheStripeData diskData) throws InterruptedException {	if (ecb == null) {	cleanup(true);	return false;	}	
sending a batch over to consumer 

private void cleanup(boolean isError) {	cleanUpCurrentRead();	if (!isError) return;	for (VectorDeserializeOrcWriter asyncWriter : asyncWriters) {	try {	asyncWriter.interrupt();	} catch (Exception ex) {	
failed to interrupt an async writer 

private void cleanUpCurrentRead() {	if (currentFileRead == null) return;	try {	currentFileRead.closeOffsetReader();	currentFileRead = null;	} catch (Exception ex) {	
failed to close current file reader 

private boolean processStop() {	if (!isStopped) return false;	
serde based data reader is stopping 

public void returnData(OrcEncodedColumnBatch ecb) {	for (int colIx = 0; colIx < ecb.getTotalColCount(); ++colIx) {	if (!ecb.hasData(colIx)) continue;	ColumnStreamData[] datas = ecb.getColumnData(colIx);	for (ColumnStreamData data : datas) {	if (data == null || data.decRef() != 0) continue;	if (LlapIoImpl.LOCKING_LOGGER.isTraceEnabled()) {	for (MemoryBuffer buf : data.getCacheBuffers()) {	
unlocking at the end of processing 

========================= hive sample_2201 =========================

public void testMetadata() throws IOException {	OrcStripeMetadata osm = OrcStripeMetadata.createDummy(0);	HashMap<Class<?>, ObjectEstimator> map = IncrementalObjectSizeEstimator.createEstimators(osm);	IncrementalObjectSizeEstimator.addEstimator("com.google.protobuf.LiteralByteString", map);	ObjectEstimator root = map.get(OrcStripeMetadata.class);	
estimated for a dummy osm 

HashMap<Class<?>, ObjectEstimator> map = IncrementalObjectSizeEstimator.createEstimators(osm);	IncrementalObjectSizeEstimator.addEstimator("com.google.protobuf.LiteralByteString", map);	ObjectEstimator root = map.get(OrcStripeMetadata.class);	OrcBatchKey stripeKey = null;	DummyMetadataReader mr = new DummyMetadataReader();	mr.doStreamStep = false;	mr.isEmpty = true;	StripeInformation si = Mockito.mock(StripeInformation.class);	Mockito.when(si.getNumberOfRows()).thenReturn(0L);	osm = new OrcStripeMetadata(stripeKey, mr, si, null, null, null, null);	
estimated for an empty osm 

ObjectEstimator root = map.get(OrcStripeMetadata.class);	OrcBatchKey stripeKey = null;	DummyMetadataReader mr = new DummyMetadataReader();	mr.doStreamStep = false;	mr.isEmpty = true;	StripeInformation si = Mockito.mock(StripeInformation.class);	Mockito.when(si.getNumberOfRows()).thenReturn(0L);	osm = new OrcStripeMetadata(stripeKey, mr, si, null, null, null, null);	mr.doStreamStep = true;	osm = new OrcStripeMetadata(stripeKey, mr, si, null, null, null, null);	
estimated for an empty osm after serde 

mr.doStreamStep = false;	mr.isEmpty = true;	StripeInformation si = Mockito.mock(StripeInformation.class);	Mockito.when(si.getNumberOfRows()).thenReturn(0L);	osm = new OrcStripeMetadata(stripeKey, mr, si, null, null, null, null);	mr.doStreamStep = true;	osm = new OrcStripeMetadata(stripeKey, mr, si, null, null, null, null);	mr.isEmpty = false;	stripeKey = new OrcBatchKey(0, 0, 0);	osm = new OrcStripeMetadata(stripeKey, mr, si, null, null, null, null);	
estimated for a test osm 

mr.isEmpty = true;	StripeInformation si = Mockito.mock(StripeInformation.class);	Mockito.when(si.getNumberOfRows()).thenReturn(0L);	osm = new OrcStripeMetadata(stripeKey, mr, si, null, null, null, null);	mr.doStreamStep = true;	osm = new OrcStripeMetadata(stripeKey, mr, si, null, null, null, null);	mr.isEmpty = false;	stripeKey = new OrcBatchKey(0, 0, 0);	osm = new OrcStripeMetadata(stripeKey, mr, si, null, null, null, null);	osm.resetRowIndex();	
estimated for a test osm w o row index 

Mockito.when(si.getNumberOfRows()).thenReturn(0L);	osm = new OrcStripeMetadata(stripeKey, mr, si, null, null, null, null);	mr.doStreamStep = true;	osm = new OrcStripeMetadata(stripeKey, mr, si, null, null, null, null);	mr.isEmpty = false;	stripeKey = new OrcBatchKey(0, 0, 0);	osm = new OrcStripeMetadata(stripeKey, mr, si, null, null, null, null);	osm.resetRowIndex();	mr.doStreamStep = true;	osm = new OrcStripeMetadata(stripeKey, mr, si, null, null, null, null);	
estimated for a test osm after serde 

osm = new OrcStripeMetadata(stripeKey, mr, si, null, null, null, null);	mr.doStreamStep = true;	osm = new OrcStripeMetadata(stripeKey, mr, si, null, null, null, null);	mr.isEmpty = false;	stripeKey = new OrcBatchKey(0, 0, 0);	osm = new OrcStripeMetadata(stripeKey, mr, si, null, null, null, null);	osm.resetRowIndex();	mr.doStreamStep = true;	osm = new OrcStripeMetadata(stripeKey, mr, si, null, null, null, null);	osm.resetRowIndex();	
estimated for a test osm w o row index after serde 

stripeKey = new OrcBatchKey(0, 0, 0);	osm = new OrcStripeMetadata(stripeKey, mr, si, null, null, null, null);	osm.resetRowIndex();	mr.doStreamStep = true;	osm = new OrcStripeMetadata(stripeKey, mr, si, null, null, null, null);	osm.resetRowIndex();	OrcFileMetadata ofm = OrcFileMetadata.createDummy(0);	map = IncrementalObjectSizeEstimator.createEstimators(ofm);	IncrementalObjectSizeEstimator.addEstimator("com.google.protobuf.LiteralByteString", map);	root = map.get(OrcFileMetadata.class);	
estimated for a dummy ofm 

========================= hive sample_2128 =========================

public void testLlapSessionQueuing() {	try {	random = new Random(1000);	conf.setIntVar(HiveConf.ConfVars.HIVE_SERVER2_LLAP_CONCURRENT_QUERIES, 2);	poolManager = new TestTezSessionPoolManager();	poolManager.setupPool(conf);	poolManager.startPool();	} catch (Exception e) {	
initialization error 

========================= hive sample_2578 =========================

public static Object getFileId(FileSystem fileSystem, Path path, boolean allowSynthetic, boolean checkDefaultFs) throws IOException {	if (fileSystem instanceof DistributedFileSystem) {	DistributedFileSystem dfs = (DistributedFileSystem) fileSystem;	if ((!checkDefaultFs) || isDefaultFs(dfs)) {	Object result = SHIMS.getFileId(dfs, path.toUri().getPath());	if (result != null) return result;	}	}	if (!allowSynthetic) {	
cannot get unique file id from returning null 

public static long createTestFileId( String pathStr, FileStatus fs, boolean doLog, String fsName) {	int nameHash = pathStr.hashCode();	long fileSize = fs.getLen(), modTime = fs.getModificationTime();	int fileSizeHash = (int)(fileSize ^ (fileSize >>> 32)), modTimeHash = (int)(modTime ^ (modTime >>> 32)), combinedHash = modTimeHash ^ fileSizeHash;	long id = ((nameHash & 0xffffffffL) << 32) | (combinedHash & 0xffffffffL);	if (doLog) {	
cannot get unique file id from using 

========================= hive sample_3739 =========================

private void moveFileInDfs (Path sourcePath, Path targetPath, HiveConf conf) throws HiveException, IOException {	final FileSystem srcFs, tgtFs;	try {	tgtFs = targetPath.getFileSystem(conf);	} catch (IOException e) {	
failed to get dest fs 

private void moveFileInDfs (Path sourcePath, Path targetPath, HiveConf conf) throws HiveException, IOException {	final FileSystem srcFs, tgtFs;	try {	tgtFs = targetPath.getFileSystem(conf);	} catch (IOException e) {	throw new HiveException(e.getMessage(), e);	}	try {	srcFs = sourcePath.getFileSystem(conf);	} catch (IOException e) {	
failed to get src fs 

if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_INSERT_INTO_MULTILEVEL_DIRS)) {	deletePath = createTargetPath(targetPath, tgtFs);	}	Hive.clearDestForSubDirSrc(conf, targetPath, sourcePath, false);	if (!Hive.moveFile(conf, sourcePath, targetPath, true, false)) {	try {	if (deletePath != null) {	tgtFs.delete(deletePath, true);	}	} catch (IOException e) {	
unable to delete the path created for facilitating rename 

WriteEntity output = ctx.getLoadTableOutputMap().get(ltd);	List<HiveLockObj> lockObjects = ctx.getOutputLockObjects().get(output);	if (lockObjects == null) {	return;	}	for (HiveLockObj lockObj : lockObjects) {	List<HiveLock> locks = lockMgr.getLocks(lockObj.getObj(), false, true);	for (HiveLock lock : locks) {	if (lock.getHiveLockMode() == lockObj.getMode()) {	if (ctx.getHiveLocks().remove(lock)) {	
about to release lock for output lock 

return;	}	for (HiveLockObj lockObj : lockObjects) {	List<HiveLock> locks = lockMgr.getLocks(lockObj.getObj(), false, true);	for (HiveLock lock : locks) {	if (lock.getHiveLockMode() == lockObj.getMode()) {	if (ctx.getHiveLocks().remove(lock)) {	try {	lockMgr.unlock(lock);	} catch (LockException le) {	
could not release lock 

public int execute(DriverContext driverContext) {	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
executing movework with 

try {	if (driverContext.getCtx().getExplainAnalyze() == AnalyzeState.RUNNING) {	return 0;	}	Hive db = getHive();	LoadFileDesc lfd = work.getLoadFileWork();	if (lfd != null) {	Path targetPath = lfd.getTargetDir();	Path sourcePath = lfd.getSourcePath();	if (targetPath.equals(sourcePath)) {	
movetask not moving 

if (driverContext.getCtx().getExplainAnalyze() == AnalyzeState.RUNNING) {	return 0;	}	Hive db = getHive();	LoadFileDesc lfd = work.getLoadFileWork();	if (lfd != null) {	Path targetPath = lfd.getTargetDir();	Path sourcePath = lfd.getSourcePath();	if (targetPath.equals(sourcePath)) {	} else {	
movetask moving to 

List<String> targetPrefixes = lmfd.getTargetPrefixes();	for (int i = 0; i <lmfd.getSourceDirs().size(); ++i) {	Path srcPath = lmfd.getSourceDirs().get(i);	Path destPath = lmfd.getTargetDirs().get(i);	String filePrefix = targetPrefixes == null ? null : targetPrefixes.get(i);	FileSystem destFs = destPath.getFileSystem(conf);	if (filePrefix == null) {	if (!destFs.exists(destPath.getParent())) {	destFs.mkdirs(destPath.getParent());	}	
movetask moving multi file to 

} else {	if (!destFs.exists(destPath)) {	destFs.mkdirs(destPath);	}	FileSystem srcFs = srcPath.getFileSystem(conf);	FileStatus[] children = srcFs.listStatus(srcPath);	if (children != null) {	for (FileStatus child : children) {	Path childSrc = child.getPath();	Path childDest = new Path(destPath, filePrefix + childSrc.getName());	
movetask moving multi file to 

}	FileSystem srcFs = srcPath.getFileSystem(conf);	FileStatus[] children = srcFs.listStatus(srcPath);	if (children != null) {	for (FileStatus child : children) {	Path childSrc = child.getPath();	Path childDest = new Path(destPath, filePrefix + childSrc.getName());	moveFile(childSrc, childDest, isDfsDir);	}	} else {	
movetask skipping empty directory multi file 

Utilities.FILE_OP_LOGGER.trace(mesg.toString() + " " + mesg_detail);	}	console.printInfo(mesg.toString(), mesg_detail);	Table table = db.getTable(tbd.getTable().getTableName());	checkFileFormats(db, tbd, table);	boolean isFullAcidOp = work.getLoadTableWork().getWriteType() != AcidUtils.Operation.NOT_ACID && !tbd.isMmTable();	DataContainer dc = null;	if (tbd.getPartitionSpec().size() == 0) {	dc = new DataContainer(table.getTTable());	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
loadtable called from into 

DataContainer dc = null;	if (tbd.getPartitionSpec().size() == 0) {	dc = new DataContainer(table.getTTable());	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	}	db.loadTable(tbd.getSourcePath(), tbd.getTable().getTableName(), tbd.getLoadFileType(), work.isSrcLocal(), isSkewedStoredAsDirs(tbd), isFullAcidOp, hasFollowingStatsTask(), tbd.getTxnId(), tbd.getStmtId());	if (work.getOutputs() != null) {	DDLTask.addIfAbsentByName(new WriteEntity(table, getWriteType(tbd, work.getLoadTableWork().getWriteType())), work.getOutputs());	}	} else {	
partition is 

private DataContainer handleStaticParts(Hive db, Table table, LoadTableDesc tbd, TaskInformation ti) throws HiveException, IOException, InvalidOperationException {	List<String> partVals = MetaStoreUtils.getPvals(table.getPartCols(),  tbd.getPartitionSpec());	db.validatePartitionNameCharacters(partVals);	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
loadpartition called from into 

DDLTask.addIfAbsentByName(enty, work.getOutputs());	}	if (queryPlan.getOutputs() == null) {	queryPlan.setOutputs(new LinkedHashSet<WriteEntity>());	}	queryPlan.getOutputs().add(enty);	dc = new DataContainer(table.getTTable(), partn.getTPartition());	if (work.getLoadTableWork().getWriteType() != AcidUtils.Operation.DELETE && work.getLoadTableWork().getWriteType() != AcidUtils.Operation.UPDATE) {	queryState.getLineageState().setLineage(tbd.getSourcePath(), dc, table.getCols());	}	
loading partition 

if (oldPart == null) {	flag = HiveFileFormatUtils.checkInputFormat( srcFs, conf, tbd.getTable().getInputFileFormatClass(), files);	} else {	flag = HiveFileFormatUtils.checkInputFormat( srcFs, conf, oldPart.getInputFormatClass(), files);	}	}	if (!flag) {	throw new HiveException(ErrorMsg.WRONG_FILE_FORMAT);	}	} else {	
skipping file format check as dpctx is not null 

========================= hive sample_3938 =========================

public static String hostname() {	try {	return InetAddress.getLocalHost().getHostName();	} catch (UnknownHostException e) {	
unable to resolve my host name 

========================= hive sample_1905 =========================

public void process(Object row, int tag) throws HiveException {	ObjectInspector rowInspector = inputObjInspectors[tag];	if (conditionInspector == null) {	conditionInspector = (PrimitiveObjectInspector) conditionEvaluator .initialize(rowInspector);	}	if (conf.isSortedFilter() && ioContext.useSorted()) {	if (!(conditionEvaluator instanceof ExprNodeGenericFuncEvaluator)) {	
attempted to use the fact data is sorted when the conditionevaluator is not of type exprnodegenericfuncevaluator 

========================= hive sample_3892 =========================

public void increment(String groupName, String counterName, long value) {	SparkCounter counter = getGroup(groupName).getCounter(counterName);	if (counter == null) {	
counter s s has not initialized before 

public long getValue(String groupName, String counterName) {	SparkCounter counter = getGroup(groupName).getCounter(counterName);	if (counter == null) {	
counter s s has not initialized before 

========================= hive sample_531 =========================

public static void testCommandSerialization(Command cmd) {	String serializedCmd = null;	try {	serializedCmd = ReplicationUtils.serializeCommand(cmd);	} catch (IOException e) {	
serialization error 

String serializedCmd = null;	try {	serializedCmd = ReplicationUtils.serializeCommand(cmd);	} catch (IOException e) {	assertNull(e);	}	Command cmd2 = null;	try {	cmd2 = ReplicationUtils.deserializeCommand(serializedCmd);	} catch (IOException e) {	
serialization error 

========================= hive sample_862 =========================

public static HadoopShims getHadoopShims() {	if (hadoopShims == null) {	synchronized (ShimLoader.class) {	if (hadoopShims == null) {	try {	hadoopShims = loadShims(HADOOP_SHIM_CLASSES, HadoopShims.class);	} catch (Throwable t) {	
error loading shims 

========================= hive sample_1456 =========================

public List<Task<? extends Serializable>> handle(Context context) throws SemanticException {	try {	DropPartitionMessage msg = deserializer.getDropPartitionMessage(context.dmd.getPayload());	String actualDbName = context.isDbNameEmpty() ? msg.getDB() : context.dbName;	String actualTblName = context.isTableNameEmpty() ? msg.getTable() : context.tableName;	Map<Integer, List<ExprNodeGenericFuncDesc>> partSpecs = genPartSpecs(new Table(msg.getTableObj()), msg.getPartitions());	if (partSpecs.size() > 0) {	DropTableDesc dropPtnDesc = new DropTableDesc(actualDbName + "." + actualTblName, partSpecs, null, true, context.eventOnlyReplicationSpec());	Task<DDLWork> dropPtnTask = TaskFactory.get( new DDLWork(readEntitySet, writeEntitySet, dropPtnDesc), context.hiveConf );	
added drop ptn task 

========================= hive sample_3484 =========================

public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {	try {	
creating input splits 

if (i < numSplitsWithExtraRecords) {	numRecordsInThisSplit++;	}	splits[i] = new JdbcInputSplit(numRecordsInThisSplit, offset, tablePaths[0]);	offset += numRecordsInThisSplit;	}	dbAccessor = null;	return splits;	}	catch (Exception e) {	
error while splitting input data 

========================= hive sample_1143 =========================

for (int i = 0; i < mapSize; i++) {	keyIsNull[i] = true;	if ((bytes[nullByteCur] & (1 << ((i * 2) % 8))) != 0) {	keyIsNull[i] = false;	LazyBinaryUtils.checkObjectByteInfo(((MapObjectInspector) oi) .getMapKeyObjectInspector(), bytes, lastElementByteEnd, recordInfo, vInt);	keyStart[i] = lastElementByteEnd + recordInfo.elementOffset;	keyLength[i] = recordInfo.elementSize;	lastElementByteEnd = keyStart[i] + keyLength[i];	} else if (!nullMapKey) {	nullMapKey = true;	
null map key encountered ignoring similar problems 

========================= hive sample_5229 =========================

public void closeOp(boolean abort) throws HiveException {	try {	if (mapJoinTables == null) {	
mapjointables is null 

protected void flushToFile() throws IOException, HiveException {	Path tmpURI = getExecContext().getLocalWork().getTmpPath();	
temp uri for side table 

========================= hive sample_4087 =========================

try {	Class<? extends RoutingAppender> clazz = routingAppender.getClass();	Method method = clazz.getDeclaredMethod("getControl", String.class, LogEvent.class);	method.setAccessible(true);	AppenderControl control = (AppenderControl) method.invoke(routingAppender, queryId, null);	Appender subordinateAppender = control.getAppender();	if (!subordinateAppender.isStopped()) {	subordinateAppender.stop();	}	} catch (NoSuchMethodException | SecurityException | IllegalAccessException | IllegalArgumentException | InvocationTargetException e) {	
unable to close the operation log appender for query id 

========================= hive sample_1401 =========================

JobID jobid = StatusDelegator.StringToJobID(id);	if (jobid == null) throw new BadParam("Invalid jobid: " + id);	tracker.killJob(jobid);	state = new JobState(id, Main.getAppConfigInstance());	List<JobState> children = state.getChildren();	if (children != null) {	for (JobState child : children) {	try {	tracker.killJob(StatusDelegator.StringToJobID(child.getId()));	} catch (IOException e) {	
templeton fail to kill job 

========================= hive sample_835 =========================

public void initialize(Configuration conf, Properties tbl) throws SerDeException {	serdeParams = new HBaseSerDeParameters(conf, tbl, getClass().getName());	cachedObjectInspector = HBaseLazyObjectFactory.createLazyHBaseStructInspector(serdeParams, tbl);	cachedHBaseRow = new LazyHBaseRow( (LazySimpleStructObjectInspector) cachedObjectInspector, serdeParams);	serializer = new HBaseRowSerializer(serdeParams);	if (LOG.isDebugEnabled()) {	
hbaseserde initialized with 

========================= hive sample_589 =========================

public void init(ByteArrayRef bytes, int start, int length) {	data.setFromBytes(bytes.getData(), start, length);	if (!data.isSet()) {	isNull = true;	} else {	isNull = !data.mutateEnforcePrecisionScale(precision, scale);	}	if (isNull) {	
data not in the hivedecimal data type range so converted to null given data is 

========================= hive sample_5507 =========================

private boolean checkConcurrency() {	boolean supportConcurrency = conf.getBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY);	if (!supportConcurrency) {	
concurrency mode is disabled not creating a lock manager 

public ClusterStatus getClusterStatus() throws Exception {	ClusterStatus cs;	try {	JobConf job = new JobConf(conf);	JobClient jc = new JobClient(job);	cs = jc.getClusterStatus();	} catch (Exception e) {	e.printStackTrace();	throw e;	}	
returning cluster status 

schema = new Schema(lst, null);	} else if (sem.getFetchTask() != null) {	FetchTask ft = sem.getFetchTask();	TableDesc td = ft.getTblDesc();	if (td == null && ft.getWork() != null && ft.getWork().getPartDesc() != null) {	if (ft.getWork().getPartDesc().size() > 0) {	td = ft.getWork().getPartDesc().get(0).getTableDesc();	}	}	if (td == null) {	
no returning schema 

td = ft.getWork().getPartDesc().get(0).getTableDesc();	}	}	if (td == null) {	} else {	String tableName = "result";	List<FieldSchema> lst = null;	try {	lst = HiveMetaStoreUtils.getFieldsFromDeserializer(tableName, td.getDeserializer(conf));	} catch (Exception e) {	
error getting schema 

} catch (Exception e) {	}	if (lst != null) {	schema = new Schema(lst, null);	}	}	}	if (schema == null) {	schema = new Schema();	}	
returning hive schema 

if (lst != null) {	for (FieldSchema f : lst) {	f.setType(ColumnType.typeToThriftType(f.getType()));	}	}	}	} catch (Exception e) {	e.printStackTrace();	throw e;	}	
returning thrift schema 

}	command = new VariableSubstitution(new HiveVariableSource() {	public Map<String, String> getHiveVariable() {	return SessionState.get().getHiveVariables();	}	}).substitute(conf, command);	String queryStr = command;	try {	queryStr = HookUtils.redactLogString(conf, command);	} catch (Exception e) {	
warning query command could not be redacted 

queryTxnMgr = SessionState.get().initTxnMgr(conf);	}	queryState.setTxnManager(queryTxnMgr);	ShutdownHookManager.removeShutdownHook(shutdownRunner);	final HiveTxnManager txnMgr = queryTxnMgr;	shutdownRunner = new Runnable() {	public void run() {	try {	releaseLocksAndCommitOrRollback(false, txnMgr);	} catch (LockException e) {	
exception when releasing locks in shutdownhook for driver 

tree = hook.preAnalyze(hookCtx, tree);	}	sem.analyze(tree, ctx);	hookCtx.update(sem);	for (HiveSemanticAnalyzerHook hook : saHooks) {	hook.postAnalyze(hookCtx, sem.getAllRootTasks());	}	} else {	sem.analyze(tree, ctx);	}	
semantic analysis completed 

errorMessage = authExp.getMessage();	SQLState = "42000";	throw createProcessorResponse(403);	} finally {	perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.DO_AUTHORIZATION);	}	}	if (conf.getBoolVar(ConfVars.HIVE_LOG_EXPLAIN_OUTPUT)) {	String explainOutput = getExplainOutput(sem, plan, tree);	if (explainOutput != null) {	
explain output for queryid 

}	SQLState = error.getSQLState();	downstreamError = e;	console.printError(errorMessage, "\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));	throw createProcessorResponse(error.getErrorCode());	} finally {	if (!parseError) {	try {	queryLifeTimeHookRunner.runAfterCompilationHook(command, compileError);	} catch (Exception e) {	
failed when invoking query after compilation hook 

private int handleInterruptionWithHook(String msg, HookContext hookContext, PerfLogger perfLogger) {	SQLState = "HY008";	errorMessage = "FAILED: command has been interrupted: " + msg;	console.printError(errorMessage);	if (hookContext != null) {	try {	invokeFailureHooks(perfLogger, hookContext, errorMessage, null);	} catch (Exception e) {	
caught exception attempting to invoke failure hooks 

private ImmutableMap<String, Long> dumpMetaCallTimingWithoutEx(String phase) {	try {	return Hive.get().dumpAndClearMetaCallTiming(phase);	} catch (HiveException he) {	
caught exception attempting to write metadata call information 

String ret = null;	ExplainTask task = new ExplainTask();	task.initialize(queryState, plan, null, ctx.getOpContext());	ByteArrayOutputStream baos = new ByteArrayOutputStream();	PrintStream ps = new PrintStream(baos);	try {	List<Task<?>> rootTasks = sem.getAllRootTasks();	task.getJSONPlan(ps, rootTasks, sem.getFetchTask(), false, true, true);	ret = baos.toString();	} catch (Exception e) {	
exception generating explain output 

}	ValidTxnList txns = txnMgr.getValidTxns();	if(oldList != null) {	throw new IllegalStateException("calling recordValidTxn() more than once in the same " + JavaUtils.txnIdToString(txnMgr.getCurrentTxnId()));	}	String txnStr = txns.toString();	conf.set(ValidTxnList.VALID_TXNS_KEY, txnStr);	if(plan.getFetchTask() != null) {	plan.getFetchTask().setValidTxnList(txnStr);	}	
encoding valid txns info txnid 

}	if (compileLock == null) {	throw createProcessorResponse(ErrorMsg.COMPILE_LOCK_TIMED_OUT.getErrorCode());	}	try {	compile(command, true, deferClose);	} catch (CommandProcessorResponse cpr) {	try {	releaseLocksAndCommitOrRollback(false);	} catch (LockException e) {	
exception in releasing locks 

long maxCompileLockWaitTime = HiveConf.getTimeVar( this.conf, ConfVars.HIVE_SERVER2_COMPILE_LOCK_TIMEOUT, TimeUnit.SECONDS);	final String lockAcquiredMsg = "Acquired the compile lock.";	try {	if (compileLock.tryLock(0, TimeUnit.SECONDS)) {	LOG.debug(lockAcquiredMsg);	return compileLock;	}	} catch (InterruptedException e) {	Thread.currentThread().interrupt();	if (LOG.isDebugEnabled()) {	
interrupted exception ignored 

LOG.debug(lockAcquiredMsg);	return compileLock;	}	} catch (InterruptedException e) {	Thread.currentThread().interrupt();	if (LOG.isDebugEnabled()) {	}	return null;	}	if (LOG.isDebugEnabled()) {	
waiting to acquire compile lock 

if (maxCompileLockWaitTime > 0) {	try {	if(!compileLock.tryLock(maxCompileLockWaitTime, TimeUnit.SECONDS)) {	errorMessage = ErrorMsg.COMPILE_LOCK_TIMED_OUT.getErrorCodedMsg();	LOG.error(errorMessage + ": " + command);	return null;	}	} catch (InterruptedException e) {	Thread.currentThread().interrupt();	if (LOG.isDebugEnabled()) {	
interrupted exception ignored 

private CommandProcessorResponse rollback(CommandProcessorResponse cpr) throws CommandProcessorResponse {	try {	releaseLocksAndCommitOrRollback(false);	}	catch (LockException e) {	
rollback failed 

checkInterrupted("during query execution: \n" + e.getMessage(), hookContext, perfLogger);	ctx.restoreOriginalTracker();	if (SessionState.get() != null) {	SessionState.get().getHiveHistory().setQueryProperty(queryId, Keys.QUERY_RET_CODE, String.valueOf(12));	}	errorMessage = "FAILED: Hive Internal Error: " + Utilities.getNameMessage(e);	if (hookContext != null) {	try {	invokeFailureHooks(perfLogger, hookContext, errorMessage, e);	} catch (Exception t) {	
failed to invoke failure hook 

}	}	SQLState = "08S01";	downstreamError = e;	console.printError(errorMessage + "\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));	throw createProcessorResponse(12);	} finally {	try {	queryLifeTimeHookRunner.runAfterExecutionHook(queryStr, hookContext, executionError);	} catch (Exception e) {	
failed when invoking query after execution hook 

private void releasePlan(QueryPlan plan) {	lDrvState.stateLock.lock();	try {	if (plan != null) {	plan.setDone();	if (SessionState.get() != null) {	try {	SessionState.get().getHiveHistory().logPlanProgress(plan);	} catch (Exception e) {	
could not log query plan progress 

conf.set(DagUtils.MAPREDUCE_WORKFLOW_NODE_NAME, tsk.getId());	Utilities.setWorkflowAdjacencies(conf, plan);	cxt.incCurJobNo(1);	console.printInfo("Launching Job " + cxt.getCurJobNo() + " out of " + jobs);	}	tsk.initialize(queryState, plan, cxt, ctx.getOpContext());	TaskRunner tskRun = new TaskRunner(tsk);	cxt.launching(tskRun);	if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.EXECPARALLEL) && tsk.canExecuteInParallel()) {	if (LOG.isInfoEnabled()){	
starting task in parallel 

}	tsk.initialize(queryState, plan, cxt, ctx.getOpContext());	TaskRunner tskRun = new TaskRunner(tsk);	cxt.launching(tskRun);	if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.EXECPARALLEL) && tsk.canExecuteInParallel()) {	if (LOG.isInfoEnabled()){	}	tskRun.start();	} else {	if (LOG.isInfoEnabled()){	
starting task in serial mode 

private void releaseDriverContext() {	lDrvState.stateLock.lock();	try {	if (driverCxt != null) {	driverCxt.shutdown();	driverCxt = null;	}	} catch (Exception e) {	
exception while shutting down the task runner 

try {	if (plan != null) {	fetchTask = plan.getFetchTask();	if (fetchTask != null) {	fetchTask.setDriverContext(null);	fetchTask.setQueryPlan(null);	}	}	plan = null;	} catch (Exception e) {	
exception while clearing the fetch task 

try {	if (ctx != null) {	ctx.clear();	if (ctx.getHiveLocks() != null) {	hiveLocks.addAll(ctx.getHiveLocks());	ctx.setHiveLocks(null);	}	ctx = null;	}	} catch (Exception e) {	
exception while clearing the context 

private void releaseResStream() {	try {	if (resStream != null) {	((FSDataInputStream) resStream).close();	resStream = null;	}	} catch (Exception e) {	
exception while closing the resstream 

private void releaseFetchTask() {	try {	if (fetchTask != null) {	fetchTask.clearFetch();	fetchTask = null;	}	} catch (Exception e) {	
exception while clearing the fetchtask 

releaseDriverContext();	releasePlan();	releaseFetchTask();	releaseResStream();	releaseContext();	if(destroyed) {	if (!hiveLocks.isEmpty()) {	try {	releaseLocksAndCommitOrRollback(false);	} catch (LockException e) {	
exception when releasing locking in destroy 

} else {	lDrvState.driverState = DriverState.DESTROYED;	}	} finally {	lDrvState.stateLock.unlock();	}	if (!hiveLocks.isEmpty()) {	try {	releaseLocksAndCommitOrRollback(false);	} catch (LockException e) {	
exception when releasing locking in destroy 

========================= hive sample_3579 =========================

public void minorTableWithBase() throws Exception {	
starting minortablewithbase 

if (stat[i].getPath().getName().equals(makeDeleteDeltaDirNameCompacted(21, 24))) {	sawNewDelta = true;	FileStatus[] buckets = fs.listStatus(stat[i].getPath());	Assert.assertEquals(2, buckets.length);	Assert.assertTrue(buckets[0].getPath().getName().matches("bucket_0000[01]"));	Assert.assertTrue(buckets[1].getPath().getName().matches("bucket_0000[01]"));	Assert.assertEquals(104L, buckets[0].getLen());	Assert.assertEquals(104L, buckets[1].getLen());	}	else {	
this is not the delta file you are looking for 

public void minorWithOpenInMiddle() throws Exception {	
starting minorwithopeninmiddle 

public void minorWithAborted() throws Exception {	
starting minorwithaborted 

}	if (stat[i].getPath().getName().equals(makeDeleteDeltaDirNameCompacted(21, 24))) {	sawNewDelta = true;	FileStatus[] buckets = fs.listStatus(stat[i].getPath());	Assert.assertEquals(2, buckets.length);	Assert.assertTrue(buckets[0].getPath().getName().matches("bucket_0000[01]"));	Assert.assertTrue(buckets[1].getPath().getName().matches("bucket_0000[01]"));	Assert.assertEquals(104L, buckets[0].getLen());	Assert.assertEquals(104L, buckets[1].getLen());	} else {	
this is not the delta file you are looking for 

public void minorTableNoBase() throws Exception {	
starting minortablewithbase 

}	if (stat[i].getPath().getName().equals(makeDeleteDeltaDirNameCompacted(1, 4))) {	sawNewDelta = true;	FileStatus[] buckets = fs.listStatus(stat[i].getPath());	Assert.assertEquals(2, buckets.length);	Assert.assertTrue(buckets[0].getPath().getName().matches("bucket_0000[01]"));	Assert.assertTrue(buckets[1].getPath().getName().matches("bucket_0000[01]"));	Assert.assertEquals(104L, buckets[0].getLen());	Assert.assertEquals(104L, buckets[1].getLen());	} else {	
this is not the delta file you are looking for 

public void majorTableWithBase() throws Exception {	
starting majortablewithbase 

for (int i = 0; i < stat.length; i++) {	if (stat[i].getPath().getName().equals("base_0000024")) {	sawNewBase = true;	FileStatus[] buckets = fs.listStatus(stat[i].getPath());	Assert.assertEquals(2, buckets.length);	Assert.assertTrue(buckets[0].getPath().getName().matches("bucket_0000[01]"));	Assert.assertTrue(buckets[1].getPath().getName().matches("bucket_0000[01]"));	Assert.assertEquals(624L, buckets[0].getLen());	Assert.assertEquals(624L, buckets[1].getLen());	} else {	
this is not the file you are looking for 

public void majorPartitionWithBase() throws Exception {	
starting majorpartitionwithbase 

for (int i = 0; i < stat.length; i++) {	if (stat[i].getPath().getName().equals("base_0000024")) {	sawNewBase = true;	FileStatus[] buckets = fs.listStatus(stat[i].getPath());	Assert.assertEquals(2, buckets.length);	Assert.assertTrue(buckets[0].getPath().getName().matches("bucket_0000[01]"));	Assert.assertTrue(buckets[1].getPath().getName().matches("bucket_0000[01]"));	Assert.assertEquals(624L, buckets[0].getLen());	Assert.assertEquals(624L, buckets[1].getLen());	} else {	
this is not the file you are looking for 

public void majorTableNoBase() throws Exception {	
starting majortablenobase 

for (int i = 0; i < stat.length; i++) {	if (stat[i].getPath().getName().equals("base_0000004")) {	sawNewBase = true;	FileStatus[] buckets = fs.listStatus(stat[i].getPath());	Assert.assertEquals(2, buckets.length);	Assert.assertTrue(buckets[0].getPath().getName().matches("bucket_0000[01]"));	Assert.assertTrue(buckets[1].getPath().getName().matches("bucket_0000[01]"));	Assert.assertEquals(104L, buckets[0].getLen());	Assert.assertEquals(104L, buckets[1].getLen());	} else {	
this is not the file you are looking for 

public void majorTableLegacy() throws Exception {	
starting majortablelegacy 

for (int i = 0; i < stat.length; i++) {	if (stat[i].getPath().getName().equals("base_0000024")) {	sawNewBase = true;	FileStatus[] buckets = fs.listStatus(stat[i].getPath());	Assert.assertEquals(2, buckets.length);	Assert.assertTrue(buckets[0].getPath().getName().matches("bucket_0000[01]"));	Assert.assertTrue(buckets[1].getPath().getName().matches("bucket_0000[01]"));	Assert.assertEquals(624L, buckets[0].getLen());	Assert.assertEquals(624L, buckets[1].getLen());	} else {	
this is not the file you are looking for 

public void minorTableLegacy() throws Exception {	
starting minortablelegacy 

FileStatus[] stat = fs.listStatus(new Path(t.getSd().getLocation()));	boolean sawNewDelta = false;	for (int i = 0; i < stat.length; i++) {	if (stat[i].getPath().getName().equals(makeDeltaDirNameCompacted(21, 24))) {	sawNewDelta = true;	FileStatus[] buckets = fs.listStatus(stat[i].getPath());	Assert.assertEquals(2, buckets.length);	Assert.assertTrue(buckets[0].getPath().getName().matches("bucket_0000[01]"));	Assert.assertTrue(buckets[1].getPath().getName().matches("bucket_0000[01]"));	} else {	
this is not the file you are looking for 

public void majorPartitionWithBaseMissingBuckets() throws Exception {	
starting majorpartitionwithbasemissingbuckets 

boolean sawNewBase = false;	for (int i = 0; i < stat.length; i++) {	if (stat[i].getPath().getName().equals("base_0000026")) {	sawNewBase = true;	FileStatus[] buckets = fs.listStatus(stat[i].getPath());	Assert.assertEquals(2, buckets.length);	Assert.assertTrue(buckets[0].getPath().getName().matches("bucket_0000[01]"));	Assert.assertTrue(buckets[1].getPath().getName().matches("bucket_0000[01]"));	Assert.assertTrue( ("bucket_00000".equals(buckets[0].getPath().getName()) && 104L == buckets[0].getLen() && "bucket_00001".equals(buckets[1].getPath().getName()) && 676L == buckets[1] .getLen()) || ("bucket_00000".equals(buckets[1].getPath().getName()) && 104L == buckets[1].getLen() && "bucket_00001".equals(buckets[0].getPath().getName()) && 676L == buckets[0] .getLen()) );	} else {	
this is not the file you are looking for 

public void majorWithOpenInMiddle() throws Exception {	
starting majorwithopeninmiddle 

public void majorWithAborted() throws Exception {	
starting majorwithaborted 

========================= hive sample_2411 =========================

if (LOG.isDebugEnabled()) {	LOG.debug("Serialized tail " + (tfd == null ? "not " : "") + "cached for path: " + path);	}	if (tfd == null) continue;	if (file.getLen() == tfd.fileLength && file.getModificationTime() == tfd.fileModTime) {	result[i] = ReaderImpl.extractFileTail(tfd.bb.duplicate(), tfd.fileLength, tfd.fileModTime);	continue;	}	cache.invalidate(path);	if (LOG.isDebugEnabled()) {	
meta info for changed cachedmodificationtime currentmodificationtime cachedlength currentlength 

========================= hive sample_3650 =========================

public VectorReduceSinkObjectHashOperator(CompilationOpContext ctx, OperatorDesc conf, VectorizationContext vContext, VectorDesc vectorDesc) throws HiveException {	super(ctx, conf, vContext, vectorDesc);	
vectorreducesinkobjecthashoperator constructor vectorreducesinkinfo 

public void process(Object row, int tag) throws HiveException {	try {	VectorizedRowBatch batch = (VectorizedRowBatch) row;	batchCounter++;	if (batch.size == 0) {	if (LOG.isDebugEnabled()) {	
batch empty 

========================= hive sample_4195 =========================

public static String hostname() {	try {	return InetAddress.getLocalHost().getHostName();	} catch (UnknownHostException e) {	
unable to resolve my host name 

public void run() {	do {	boolean launchedJob = false;	try {	final CompactionInfo ci = txnHandler.findNextToCompact(name);	if (ci == null && !stop.get()) {	try {	Thread.sleep(SLEEP_TIME);	continue;	} catch (InterruptedException e) {	
worker thread sleep interrupted 

Thread.sleep(SLEEP_TIME);	continue;	} catch (InterruptedException e) {	continue;	}	}	Table t1 = null;	try {	t1 = resolveTable(ci);	if (t1 == null) {	
unable to find table assuming it was dropped and moving on 

}	} catch (MetaException e) {	txnHandler.markCleaned(ci);	continue;	}	final Table t = t1;	Partition p = null;	try {	p = resolvePartition(ci);	if (p == null && ci.partName != null) {	
unable to find partition assuming it was dropped and moving on 

if (p == null && ci.partName != null) {	txnHandler.markCleaned(ci);	continue;	}	} catch (Exception e) {	txnHandler.markCleaned(ci);	continue;	}	final StorageDescriptor sd =  resolveStorageDescriptor(t, p);	if (sd.getSortCols() != null && !sd.getSortCols().isEmpty()) {	
attempt to compact sorted table which is not yet supported 

txnHandler.markCleaned(ci);	continue;	}	final StorageDescriptor sd =  resolveStorageDescriptor(t, p);	if (sd.getSortCols() != null && !sd.getSortCols().isEmpty()) {	txnHandler.markCleaned(ci);	continue;	}	final boolean isMajor = ci.isMajorCompaction();	final ValidTxnList txns = TxnUtils.createValidCompactTxnList(txnHandler.getOpenTxnsInfo());	
validcompacttxnlist 

final StringBuilder jobName = new StringBuilder(name);	jobName.append("-compactor-");	jobName.append(ci.getFullPartitionName());	String runAs;	if (ci.runAs == null) {	runAs = findUserToRunAs(sd.getLocation(), t);	txnHandler.setRunAs(ci.id, runAs);	} else {	runAs = ci.runAs;	}	
starting compaction for 

UserGroupInformation ugi = UserGroupInformation.createProxyUser(t.getOwner(), UserGroupInformation.getLoginUser());	ugi.doAs(new PrivilegedExceptionAction<Object>() {	public Object run() throws Exception {	mr.run(conf, jobName.toString(), t, sd, txns, ci, su, txnHandler);	return null;	}	});	try {	FileSystem.closeAllForUGI(ugi);	} catch (IOException exception) {	
could not clean up file system handles for ugi for 

try {	FileSystem.closeAllForUGI(ugi);	} catch (IOException exception) {	}	}	txnHandler.markCompacted(ci);	if (conf.getBoolVar(HiveConf.ConfVars.HIVE_IN_TEST)) {	mrJob = mr.getMrJob();	}	} catch (Exception e) {	
caught exception while trying to compact marking failed to avoid repeated failures 

}	}	txnHandler.markCompacted(ci);	if (conf.getBoolVar(HiveConf.ConfVars.HIVE_IN_TEST)) {	mrJob = mr.getMrJob();	}	} catch (Exception e) {	txnHandler.markFailed(ci);	}	} catch (Throwable t) {	
caught an exception in the main loop of compactor worker 

========================= hive sample_2807 =========================

String currentUrl = MetaStoreInit.getConnectionURL(activeConf);	try {	initConnectionUrlHook(originalConf, updateData);	if (updateData.urlHook != null) {	if (badUrl != null) {	updateData.urlHook.notifyBadConnectionUrl(badUrl);	}	connectUrl = updateData.urlHook.getJdoConnectionUrl(originalConf);	}	} catch (Exception e) {	
exception while getting connection url from the hook 

initConnectionUrlHook(originalConf, updateData);	if (updateData.urlHook != null) {	if (badUrl != null) {	updateData.urlHook.notifyBadConnectionUrl(badUrl);	}	connectUrl = updateData.urlHook.getJdoConnectionUrl(originalConf);	}	} catch (Exception e) {	}	if (connectUrl != null && !connectUrl.equals(currentUrl)) {	
overriding s with s 

========================= hive sample_1944 =========================

public void initialize() {	ByteBuffer payload = context.getUserPayload().getPayload();	
initializing the edge payload 

throw new RuntimeException("Invalid payload");	}	DataInputByteBuffer dibb = new DataInputByteBuffer();	dibb.reset(payload);	conf = new CustomEdgeConfiguration();	try {	conf.readFields(dibb);	} catch (IOException e) {	throw new RuntimeException(e);	}	
routing table num buckets 

========================= hive sample_3979 =========================

Map tableInfo = (Map) m.get("tableInfo");	String res = jsonRun(user, exec);	JsonBuilder jb = JsonBuilder.create(singleTable(res, table)) .remove("tableName") .put("database", db) .put("table", table) .put("retention", tableInfo.get("retention")) .put("sd", tableInfo.get("sd")) .put("parameters", tableInfo.get("parameters")) .put("parametersSize", tableInfo.get("parametersSize")) .put("tableType", tableInfo.get("tableType"));	String loc = (String) jb.getMap().get("location");	if (loc != null && loc.startsWith("hdfs: try {	FileSystem fs = FileSystem.get(appConf);	FileStatus status = fs.getFileStatus(new Path(new URI(loc)));	jb.put("group", status.getGroup());	jb.put("permission", status.getPermission().toString());	} catch (Exception e) {	
couldn t get permissions for 

========================= hive sample_823 =========================

static void executeStatementOnDriver(String cmd, IDriver driver) throws IOException, CommandNeedRetryException {	
executing 

========================= hive sample_989 =========================

} else if (mapWork.getPathToAliases() == null) {	return;	}	final Set<String> aliases = new HashSet<String>();	try {	ArrayList<String> a = HiveFileFormatUtils.getFromPathRecursively( mapWork.getPathToAliases(), new Path(splitPath), null, false, true);	if (a != null) {	aliases.addAll(a);	}	if (a == null || a.isEmpty()) {	
couldn t find aliases for 

if (!noFilters) {	try {	for (ExprNodeGenericFuncDesc filterExpr : filterExprs) {	if (tableFilterExpr == null ) {	tableFilterExpr = filterExpr;	} else {	tableFilterExpr = ExprNodeGenericFuncDesc.newInstance(new GenericUDFOPOr(), Arrays.<ExprNodeDesc>asList(tableFilterExpr, filterExpr));	}	}	} catch(UDFArgumentException ex) {	
turn off filtering due to 

private void pushFilters(final JobConf jobConf, RowSchema rowSchema, ExprNodeGenericFuncDesc filterExpr) {	Utilities.setColumnNameList(jobConf, rowSchema);	if (filterExpr == null) {	
not pushing filters because filterexpr is null 

========================= hive sample_3703 =========================

private RunningJob getJob(JobID jobid) throws IOException {	try {	return jc.getJob(jobid);	}	catch(IOException ex) {	String msg = ex.getMessage();	if(msg != null && (msg.contains("ApplicationNotFoundException") || Pattern.compile("History file.*not found").matcher(msg).find())) {	
job not found 

public void killJobs(String tag, long timestamp) {	try {	
looking for jobs to kill 

public void killJobs(String tag, long timestamp) {	try {	Set<ApplicationId> childJobs = getYarnChildJobs(tag, timestamp);	if (childJobs.isEmpty()) {	
no jobs found from 

public void killJobs(String tag, long timestamp) {	try {	Set<ApplicationId> childJobs = getYarnChildJobs(tag, timestamp);	if (childJobs.isEmpty()) {	return;	} else {	
found mr jobs count d 

public void killJobs(String tag, long timestamp) {	try {	Set<ApplicationId> childJobs = getYarnChildJobs(tag, timestamp);	if (childJobs.isEmpty()) {	return;	} else {	
killing all found jobs 

public void killJobs(String tag, long timestamp) {	try {	Set<ApplicationId> childJobs = getYarnChildJobs(tag, timestamp);	if (childJobs.isEmpty()) {	return;	} else {	YarnClient yarnClient = YarnClient.createYarnClient();	yarnClient.init(conf);	yarnClient.start();	for (ApplicationId app: childJobs) {	
killing job s 

try {	Set<ApplicationId> childJobs = getYarnChildJobs(tag, timestamp);	if (childJobs.isEmpty()) {	return;	} else {	YarnClient yarnClient = YarnClient.createYarnClient();	yarnClient.init(conf);	yarnClient.start();	for (ApplicationId app: childJobs) {	yarnClient.killApplication(app);	
job s killed 

========================= hive sample_1440 =========================

public void initialize(Configuration hconf) throws HiveException {	aggregationBuffers =  allocateAggregationBuffer();	
using global aggregation processing mode 

else {	this.percentEntriesToFlush = HiveConf.ConfVars.HIVE_VECTORIZATION_GROUPBY_FLUSH_PERCENT.defaultFloatVal;	this.checkInterval = HiveConf.ConfVars.HIVE_VECTORIZATION_GROUPBY_CHECKINTERVAL.defaultIntVal;	this.maxHtEntries = HiveConf.ConfVars.HIVE_VECTORIZATION_GROUPBY_MAXENTRIES.defaultIntVal;	this.minReductionHashAggr = HiveConf.ConfVars.HIVEMAPAGGRHASHMINREDUCTION.defaultFloatVal;	this.numRowsCompareHashAggr = HiveConf.ConfVars.HIVEGROUPBYMAPINTERVAL.defaultIntVal;	}	sumBatchSize = 0;	mapKeysAggregationBuffers = new HashMap<KeyWrapper, VectorAggregationBufferRow>();	computeMemoryLimits();	
using hash aggregation processing mode 

processAggregators(batch);	int preFlushEntriesCount = numEntriesHashTable;	while (shouldFlush(batch)) {	flush(false);	if(gcCanary.get() == null) {	gcCanaryFlushes++;	gcCanary = new SoftReference<Object>(new Object());	}	if (!(numEntriesHashTable < preFlushEntriesCount)) {	if (LOG.isDebugEnabled()) {	
flush did not progress d entries before d entries after 

JavaDataModel model = JavaDataModel.get();	fixedHashEntrySize = model.hashMapEntry() + keyWrappersBatch.getKeysFixedSize() + aggregationBatchInfo.getAggregatorsFixedSize();	MemoryMXBean memoryMXBean = ManagementFactory.getMemoryMXBean();	maxMemory = memoryMXBean.getHeapMemoryUsage().getMax();	memoryThreshold = conf.getMemoryThreshold();	if (memoryThreshold == 0.0f) {	memoryThreshold = 1.0f;	}	maxHashTblMemory = (int)(maxMemory * memoryThreshold);	if (LOG.isDebugEnabled()) {	
maxmemory dmb d f fixsize d key d agg d 

if (++entriesFlushed >= entriesToFlush) {	break;	}	}	}	if (all) {	mapKeysAggregationBuffers.clear();	numEntriesHashTable = 0;	}	if (all && LOG.isDebugEnabled()) {	
gc canary caused d flushes 

private void checkHashModeEfficiency() throws HiveException {	if (lastModeCheckRowCount > numRowsCompareHashAggr) {	lastModeCheckRowCount = 0;	if (LOG.isDebugEnabled()) {	
checkhashmodeefficiency ht d rc d min d 

public void initialize(Configuration hconf) throws HiveException {	streamAggregationBufferRowPool = new VectorUtilBatchObjectPool<VectorAggregationBufferRow>( VectorizedRowBatch.DEFAULT_SIZE, new VectorUtilBatchObjectPool.IAllocator<VectorAggregationBufferRow>() {	public VectorAggregationBufferRow alloc() throws HiveException {	return allocateAggregationBuffer();	}	public void free(VectorAggregationBufferRow t) {	}	});	
using unsorted streaming aggregation processing mode 

public void initialize(Configuration hconf) throws HiveException {	isLastGroupBatch = true;	groupKeyHelper = new VectorGroupKeyHelper(outputKeyLength);	groupKeyHelper.init(keyExpressions);	groupAggregators = allocateAggregationBuffer();	buffer = new DataOutputBuffer();	
using sorted group batch aggregation processing mode 

private void changeToStreamingMode() throws HiveException {	processingMode = this.new ProcessingModeStreaming();	processingMode.initialize(null);	
switched to streaming mode 

========================= hive sample_4212 =========================

public void load( MapJoinTableContainer[] mapJoinTables, MapJoinTableContainerSerDe[] mapJoinTableSerdes) throws HiveException {	String currentInputPath = context.getCurrentInputPath().toString();	
load from hashtable for input file 

Path baseDir = getBaseDir(localWork);	if (baseDir == null) {	return;	}	String fileName = localWork.getBucketFileName(currentInputPath);	for (int pos = 0; pos < mapJoinTables.length; pos++) {	if (pos == desc.getPosBigTable() || mapJoinTables[pos] != null) {	continue;	}	Path path = Utilities.generatePath(baseDir, desc.getDumpFilePrefix(), (byte)pos, fileName);	
tload back hashtable file from tmp file uri 

========================= hive sample_3884 =========================

public static Entry<String,String> parseMapping(String columnSpec) throws InvalidColumnMappingException {	int index = 0;	while (true) {	if (index >= columnSpec.length()) {	
cannot parse as colon separated column configuration 

public static Entry<String,String> parseMapping(String columnSpec) throws InvalidColumnMappingException {	int index = 0;	while (true) {	if (index >= columnSpec.length()) {	throw new InvalidColumnMappingException( "Columns must be provided as colon-separated family and qualifier pairs");	}	index = columnSpec.indexOf(AccumuloHiveConstants.COLON, index);	if (-1 == index) {	
cannot parse as colon separated column configuration 

========================= hive sample_198 =========================

protected void serviceStart() throws Exception {	druidNodes.stream().forEach(node -> {	try {	node.start();	} catch (IOException e) {	
failed to start node consequently will destroy the cluster 

protected void serviceStart() throws Exception {	druidNodes.stream().forEach(node -> {	try {	node.start();	} catch (IOException e) {	druidNodes.stream().filter(node1 -> node1.isAlive()).forEach(nodeToStop -> {	try {	
stopping node 

protected void serviceStart() throws Exception {	druidNodes.stream().forEach(node -> {	try {	node.start();	} catch (IOException e) {	druidNodes.stream().filter(node1 -> node1.isAlive()).forEach(nodeToStop -> {	try {	nodeToStop.close();	} catch (IOException e1) {	
error while stopping 

protected void serviceStop() throws Exception {	druidNodes.stream().forEach(node -> {	try {	node.close();	} catch (IOException e) {	
failed to stop druid node s 

========================= hive sample_462 =========================

stmt.execute("CREATE TABLE COMPACTION_QUEUE (" + " CQ_ID bigint PRIMARY KEY," + " CQ_DATABASE varchar(128) NOT NULL," + " CQ_TABLE varchar(128) NOT NULL," + " CQ_PARTITION varchar(767)," + " CQ_STATE char(1) NOT NULL," + " CQ_TYPE char(1) NOT NULL," + " CQ_TBLPROPERTIES varchar(2048)," + " CQ_WORKER_ID varchar(128)," + " CQ_START bigint," + " CQ_RUN_AS varchar(128)," + " CQ_HIGHEST_TXN_ID bigint," + " CQ_META_INFO varchar(2048) for bit data," + " CQ_HADOOP_JOB_ID varchar(32))");	stmt.execute("CREATE TABLE NEXT_COMPACTION_QUEUE_ID (NCQ_NEXT bigint NOT NULL)");	stmt.execute("INSERT INTO NEXT_COMPACTION_QUEUE_ID VALUES(1)");	stmt.execute("CREATE TABLE COMPLETED_COMPACTIONS (" + " CC_ID bigint PRIMARY KEY," + " CC_DATABASE varchar(128) NOT NULL," + " CC_TABLE varchar(128) NOT NULL," + " CC_PARTITION varchar(767)," + " CC_STATE char(1) NOT NULL," + " CC_TYPE char(1) NOT NULL," + " CC_TBLPROPERTIES varchar(2048)," + " CC_WORKER_ID varchar(128)," + " CC_START bigint," + " CC_END bigint," + " CC_RUN_AS varchar(128)," + " CC_HIGHEST_TXN_ID bigint," + " CC_META_INFO varchar(2048) for bit data," + " CC_HADOOP_JOB_ID varchar(32))");	stmt.execute("CREATE TABLE AUX_TABLE (" + " MT_KEY1 varchar(128) NOT NULL," + " MT_KEY2 bigint NOT NULL," + " MT_COMMENT varchar(255)," + " PRIMARY KEY(MT_KEY1, MT_KEY2))");	stmt.execute("CREATE TABLE WRITE_SET (" + " WS_DATABASE varchar(128) NOT NULL," + " WS_TABLE varchar(128) NOT NULL," + " WS_PARTITION varchar(767)," + " WS_TXNID bigint NOT NULL," + " WS_COMMIT_ID bigint NOT NULL," + " WS_OPERATION_TYPE char(1) NOT NULL)" );	} catch (SQLException e) {	try {	conn.rollback();	} catch (SQLException re) {	
error rolling back 

stmt.execute("INSERT INTO NEXT_COMPACTION_QUEUE_ID VALUES(1)");	stmt.execute("CREATE TABLE COMPLETED_COMPACTIONS (" + " CC_ID bigint PRIMARY KEY," + " CC_DATABASE varchar(128) NOT NULL," + " CC_TABLE varchar(128) NOT NULL," + " CC_PARTITION varchar(767)," + " CC_STATE char(1) NOT NULL," + " CC_TYPE char(1) NOT NULL," + " CC_TBLPROPERTIES varchar(2048)," + " CC_WORKER_ID varchar(128)," + " CC_START bigint," + " CC_END bigint," + " CC_RUN_AS varchar(128)," + " CC_HIGHEST_TXN_ID bigint," + " CC_META_INFO varchar(2048) for bit data," + " CC_HADOOP_JOB_ID varchar(32))");	stmt.execute("CREATE TABLE AUX_TABLE (" + " MT_KEY1 varchar(128) NOT NULL," + " MT_KEY2 bigint NOT NULL," + " MT_COMMENT varchar(255)," + " PRIMARY KEY(MT_KEY1, MT_KEY2))");	stmt.execute("CREATE TABLE WRITE_SET (" + " WS_DATABASE varchar(128) NOT NULL," + " WS_TABLE varchar(128) NOT NULL," + " WS_PARTITION varchar(767)," + " WS_TXNID bigint NOT NULL," + " WS_COMMIT_ID bigint NOT NULL," + " WS_OPERATION_TYPE char(1) NOT NULL)" );	} catch (SQLException e) {	try {	conn.rollback();	} catch (SQLException re) {	}	if (e.getMessage() != null && e.getMessage().contains("already exists")) {	
txn tables already exist returning 

stmt.execute("CREATE TABLE WRITE_SET (" + " WS_DATABASE varchar(128) NOT NULL," + " WS_TABLE varchar(128) NOT NULL," + " WS_PARTITION varchar(767)," + " WS_TXNID bigint NOT NULL," + " WS_COMMIT_ID bigint NOT NULL," + " WS_OPERATION_TYPE char(1) NOT NULL)" );	} catch (SQLException e) {	try {	conn.rollback();	} catch (SQLException re) {	}	if (e.getMessage() != null && e.getMessage().contains("already exists")) {	return;	}	if (e instanceof SQLTransactionRollbackException && deadlockCnt++ < 5) {	
caught deadlock retrying db creation 

private static boolean dropTable(Statement stmt, String name, int retryCount) throws SQLException {	for (int i = 0; i < 3; i++) {	try {	stmt.execute("DROP TABLE " + name);	
successfully dropped table 

private static boolean dropTable(Statement stmt, String name, int retryCount) throws SQLException {	for (int i = 0; i < 3; i++) {	try {	stmt.execute("DROP TABLE " + name);	return true;	} catch (SQLException e) {	if ("42Y55".equals(e.getSQLState()) && 30000 == e.getErrorCode()) {	
not dropping because it doesn t exist 

private static boolean dropTable(Statement stmt, String name, int retryCount) throws SQLException {	for (int i = 0; i < 3; i++) {	try {	stmt.execute("DROP TABLE " + name);	return true;	} catch (SQLException e) {	if ("42Y55".equals(e.getSQLState()) && 30000 == e.getErrorCode()) {	return true;	}	if ("X0Y25".equals(e.getSQLState()) && 30000 == e.getErrorCode()) {	
intermittent drop failure retrying try number 

} catch (SQLException e) {	if ("42Y55".equals(e.getSQLState()) && 30000 == e.getErrorCode()) {	return true;	}	if ("X0Y25".equals(e.getSQLState()) && 30000 == e.getErrorCode()) {	continue;	}	LOG.error("Unable to drop table " + name + ": " + e.getMessage() + " State=" + e.getSQLState() + " code=" + e.getErrorCode() + " retryCount=" + retryCount);	}	}	
failed to drop table don t know why 

static void closeResources(Connection conn, Statement stmt, ResultSet rs) {	if (rs != null) {	try {	rs.close();	} catch (SQLException e) {	
error closing resultset 

========================= hive sample_1856 =========================

private Callable<Void> createQueryCallable(final String queryStringFormat, final Map<String, String> confOverlay, final long longPollingTimeout, final int queryCount, final OperationState expectedOperationState, final boolean syncThreadStart, final CountDownLatch cdlIn, final CountDownLatch cdlOut) {	return new Callable<Void>() {	public Void call() throws Exception {	if (syncThreadStart) {	syncThreadStart(cdlIn, cdlOut);	}	SessionHandle sessionHandle = openSession(confOverlay);	OperationHandle[] hs  = new OperationHandle[queryCount];	for (int i = 0; i < hs.length; ++i) {	String queryString = String.format(queryStringFormat, i);	
submitting 

========================= hive sample_2283 =========================

public List<Task<? extends Serializable>> handle(Context context) throws SemanticException {	DropTableMessage msg = deserializer.getDropTableMessage(context.dmd.getPayload());	String actualDbName = context.isDbNameEmpty() ? msg.getDB() : context.dbName;	String actualTblName = context.isTableNameEmpty() ? msg.getTable() : context.tableName;	DropTableDesc dropTableDesc = new DropTableDesc( actualDbName + "." + actualTblName, null, true, true, context.eventOnlyReplicationSpec(), false );	Task<DDLWork> dropTableTask = TaskFactory.get( new DDLWork(readEntitySet, writeEntitySet, dropTableDesc), context.hiveConf );	
added drop tbl task 

========================= hive sample_3492 =========================

private List<Long> determineFileIdsToQuery( List<HdfsFileStatusWithId> files, OrcTail[] result, HashMap<Long, Integer> posMap) {	for (int i = 0; i < result.length; ++i) {	if (result[i] != null) continue;	HdfsFileStatusWithId file = files.get(i);	final FileStatus fs = file.getFileStatus();	Long fileId = file.getFileId();	if (fileId == null) {	if (!isInTest) {	if (!isWarnLogged || LOG.isDebugEnabled()) {	
not using metastore cache because fileid is missing 

final FileStatus fs = file.getFileStatus();	Long fileId = file.getFileId();	if (fileId == null) {	if (!isInTest) {	if (!isWarnLogged || LOG.isDebugEnabled()) {	isWarnLogged = true;	}	continue;	}	fileId = generateTestFileId(fs, files, i);	
generated file id at 

}	}	List<PredicateLeaf> leaves = sarg.getLeaves();	for (int i = 0; i < leaves.size(); ++i) {	PredicateLeaf pl = leaves.get(i);	Integer colId = nameIdMap.get(pl.getColumnName());	String newColName = RecordReaderImpl.encodeTranslatedSargColumn(rootColumn, colId);	SearchArgumentFactory.setPredicateLeafColumn(pl, newColName);	}	if (LOG.isDebugEnabled()) {	
sarg translated into 

========================= hive sample_3666 =========================

public void printNumDistinctValueEstimator() {	String t = new String();	
NumDistinctValueEstimator 

public void printNumDistinctValueEstimator() {	String t = new String();	
number of vectors 

public void printNumDistinctValueEstimator() {	String t = new String();	
vector size 

public void printNumDistinctValueEstimator() {	String t = new String();	for (int i=0; i < numBitVectors; i++) {	t = t + bitVector[i].toString();	}	
serialized vectors 

========================= hive sample_1821 =========================

s = new String(bytes.getData(), start, length, "US-ASCII");	if (s.equals("NULL")) {	isNull = true;	logExceptionMessage(bytes, start, length, serdeConstants.TIMESTAMPLOCALTZ_TYPE_NAME.toUpperCase());	} else {	t = TimestampTZUtil.parse(s, timeZone);	isNull = false;	}	} catch (UnsupportedEncodingException e) {	isNull = true;	
unsupported encoding found 

========================= hive sample_5474 =========================

public SimpleBufferManager(Allocator allocator, LlapDaemonCacheMetrics metrics) {	
simple buffer manager 

private void unlockBuffer(LlapAllocatorBuffer buffer) {	if (buffer.decRef() == 0) {	if (LlapIoImpl.CACHE_LOGGER.isTraceEnabled()) {	
deallocating that was not cached 

public long[] putFileData(Object fileKey, DiskRange[] ranges, MemoryBuffer[] chunks, long baseOffset, Priority priority, LowLevelCacheCounters qfCounters) {	for (int i = 0; i < chunks.length; ++i) {	LlapAllocatorBuffer buffer = (LlapAllocatorBuffer)chunks[i];	if (LlapIoImpl.LOCKING_LOGGER.isTraceEnabled()) {	
locking at put time no cache 

========================= hive sample_2165 =========================

public void close() {	try {	ctx.close();	} catch (NamingException e) {	
exception when closing ldap context 

allLdapNames = execute(Collections.singletonList(userBaseDn), queries.findUserDnByRdn(userRdn)).getAllLdapNames();	} else {	allLdapNames = findDnByPattern(userPatterns, user);	if (allLdapNames.isEmpty()) {	allLdapNames = execute(userBases, queries.findUserDnByName(user)).getAllLdapNames();	}	}	if (allLdapNames.size() == 1) {	return allLdapNames.get(0);	} else {	
expected exactly one user result for the user but got returning null 

allLdapNames = execute(Collections.singletonList(userBaseDn), queries.findUserDnByRdn(userRdn)).getAllLdapNames();	} else {	allLdapNames = findDnByPattern(userPatterns, user);	if (allLdapNames.isEmpty()) {	allLdapNames = execute(userBases, queries.findUserDnByName(user)).getAllLdapNames();	}	}	if (allLdapNames.size() == 1) {	return allLdapNames.get(0);	} else {	
matched users 

private SearchResultHandler execute(Collection<String> baseDns, Query query) {	List<NamingEnumeration<SearchResult>> searchResults = new ArrayList<>();	
executing a query with base dns 

private SearchResultHandler execute(Collection<String> baseDns, Query query) {	List<NamingEnumeration<SearchResult>> searchResults = new ArrayList<>();	for (String aBaseDn : baseDns) {	try {	NamingEnumeration<SearchResult> searchResult = ctx.search(aBaseDn, query.getFilter(), query.getControls());	if (searchResult != null) {	searchResults.add(searchResult);	}	} catch (NamingException ex) {	
exception happened for query with base dn 

========================= hive sample_2309 =========================

public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procContext, Object... nodeOutputs) throws SemanticException {	GenTezProcContext context = (GenTezProcContext) procContext;	assert context != null && context.currentTask != null && context.currentRootOperator != null;	Operator<?> operator = (Operator<?>) nd;	Operator<?> root = context.currentRootOperator;	
root operator 

public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procContext, Object... nodeOutputs) throws SemanticException {	GenTezProcContext context = (GenTezProcContext) procContext;	assert context != null && context.currentTask != null && context.currentRootOperator != null;	Operator<?> operator = (Operator<?>) nd;	Operator<?> root = context.currentRootOperator;	
leaf operator 

ReduceSinkOperator parentRS = (ReduceSinkOperator) parentOp;	GenMapRedUtils.setKeyAndValueDesc(reduceWork, parentRS);	if (!context.mapJoinToUnprocessedSmallTableReduceSinks.get(mj).contains(parentRS)) {	BaseWork parentWork = ReduceSinkMapJoinProc.getMapJoinParentWork(context, parentRS);	int tag = parentRS.getConf().getTag();	tag = (tag == -1 ? 0 : tag);	reduceWork.getTagToInput().put(tag, parentWork.getName());	}	}	}	
processing map join 

workItems.add(work);	context.mapJoinWorkMap.put(mj, workItems);	} else {	context.mapJoinWorkMap.get(mj).add(work);	}	if (context.linkOpWithWorkMap.containsKey(mj)) {	Map<BaseWork,TezEdgeProperty> linkWorkMap = context.linkOpWithWorkMap.get(mj);	if (linkWorkMap != null) {	if (context.linkChildOpWithDummyOp.containsKey(mj)) {	if (LOG.isDebugEnabled()) {	
adding dummy ops to work 

if (linkWorkMap != null) {	if (context.linkChildOpWithDummyOp.containsKey(mj)) {	if (LOG.isDebugEnabled()) {	}	for (Operator<?> dummy: context.linkChildOpWithDummyOp.get(mj)) {	work.addDummyOp((HashTableDummyOperator) dummy);	}	}	for (Entry<BaseWork,TezEdgeProperty> parentWorkMap : linkWorkMap.entrySet()) {	BaseWork parentWork = parentWorkMap.getKey();	
connecting with 

TezEdgeProperty edgeProp = parentWorkMap.getValue();	tezWork.connect(parentWork, work, edgeProp);	if (edgeProp.getEdgeType() == EdgeType.CUSTOM_EDGE) {	tezWork.setVertexType(work, VertexType.INITIALIZED_EDGES);	}	for (ReduceSinkOperator r: context.linkWorkWithReduceSinkMap.get(parentWork)) {	if (!context.mapJoinParentMap.get(mj).contains(r)) {	continue;	}	if (r.getConf().getOutputName() != null) {	
cloning reduce sink for multi child broadcast edge 

context.connectedReduceSinks.add(r);	}	}	}	}	}	context.currentMapJoinOperators.clear();	}	for (Operator<?> parent : new ArrayList<Operator<?>>(root.getParentOperators())) {	if (LOG.isDebugEnabled()) {	
removing as parent from 

unionWork = GenTezUtils.createUnionWork(context, root, operator, tezWork);	connectUnionWorkWithWork(unionWork, work, tezWork, context);	}	}	context.currentUnionOperators.clear();	work = unionWork;	}	if (context.leafOperatorToFollowingWork.containsKey(operator)) {	BaseWork followingWork = context.leafOperatorToFollowingWork.get(operator);	long bytesPerReducer = context.conf.getLongVar(HiveConf.ConfVars.BYTESPERREDUCER);	
second pass leaf operator has common downstream work 

edgeProp = new TezEdgeProperty(context.conf, edgeType, true, rWork.isSlowStart(), rWork.getMinReduceTasks(), rWork.getMaxReduceTasks(), bytesPerReducer);	} else {	edgeProp = new TezEdgeProperty(edgeType);	edgeProp.setSlowStart(rWork.isSlowStart());	}	tezWork.connect(work, followingWork, edgeProp);	context.connectedReduceSinks.add(rs);	}	}	} else {	
first pass leaf operator 

private void connectUnionWorkWithWork(UnionWork unionWork, BaseWork work, TezWork tezWork, GenTezProcContext context) {	
connecting union work with work 

========================= hive sample_3518 =========================

public static HushableRandomAccessFileAppender createAppender( @PluginAttribute("fileName") final String fileName, @PluginAttribute("fileName") final String fileName, @PluginAttribute("append") final String append, @PluginAttribute("append") final String append, @PluginAttribute("name") final String name, @PluginAttribute("name") final String name, @PluginAttribute("immediateFlush") final String immediateFlush, @PluginAttribute("immediateFlush") final String immediateFlush, @PluginAttribute("bufferSize") final String bufferSizeStr, @PluginAttribute("bufferSize") final String bufferSizeStr, @PluginAttribute("ignoreExceptions") final String ignore, @PluginAttribute("ignoreExceptions") final String ignore, @PluginElement("Layout") Layout<? extends Serializable> layout, @PluginElement("Layout") Layout<? extends Serializable> layout, @PluginElement("Filter") final Filter filter, @PluginElement("Filter") final Filter filter, @PluginAttribute("advertise") final String advertise, @PluginAttribute("advertise") final String advertise, @PluginAttribute("advertiseURI") final String advertiseURI, @PluginAttribute("advertiseURI") final String advertiseURI, @PluginConfiguration final Configuration config) {	final boolean isAppend = Booleans.parseBoolean(append, true);	final boolean isFlush = Booleans.parseBoolean(immediateFlush, true);	final boolean ignoreExceptions = Booleans.parseBoolean(ignore, true);	final boolean isAdvertise = Boolean.parseBoolean(advertise);	final int bufferSize = Integers.parseInt(bufferSizeStr, 256 * 1024 /* RandomAccessFileManager.DEFAULT_BUFFER_SIZE */);	if (name == null) {	
no name provided for fileappender 

public static HushableRandomAccessFileAppender createAppender( @PluginAttribute("fileName") final String fileName, @PluginAttribute("fileName") final String fileName, @PluginAttribute("append") final String append, @PluginAttribute("append") final String append, @PluginAttribute("name") final String name, @PluginAttribute("name") final String name, @PluginAttribute("immediateFlush") final String immediateFlush, @PluginAttribute("immediateFlush") final String immediateFlush, @PluginAttribute("bufferSize") final String bufferSizeStr, @PluginAttribute("bufferSize") final String bufferSizeStr, @PluginAttribute("ignoreExceptions") final String ignore, @PluginAttribute("ignoreExceptions") final String ignore, @PluginElement("Layout") Layout<? extends Serializable> layout, @PluginElement("Layout") Layout<? extends Serializable> layout, @PluginElement("Filter") final Filter filter, @PluginElement("Filter") final Filter filter, @PluginAttribute("advertise") final String advertise, @PluginAttribute("advertise") final String advertise, @PluginAttribute("advertiseURI") final String advertiseURI, @PluginAttribute("advertiseURI") final String advertiseURI, @PluginConfiguration final Configuration config) {	final boolean isAppend = Booleans.parseBoolean(append, true);	final boolean isFlush = Booleans.parseBoolean(immediateFlush, true);	final boolean ignoreExceptions = Booleans.parseBoolean(ignore, true);	final boolean isAdvertise = Boolean.parseBoolean(advertise);	final int bufferSize = Integers.parseInt(bufferSizeStr, 256 * 1024 /* RandomAccessFileManager.DEFAULT_BUFFER_SIZE */);	if (name == null) {	return null;	}	if (fileName == null) {	
no filename provided for fileappender with name 

========================= hive sample_4662 =========================

public List<IteratorSetting> getIterators(Configuration conf, ColumnMapper columnMapper) throws SerDeException {	List<IteratorSetting> itrs = Lists.newArrayList();	boolean shouldPushdown = conf.getBoolean(AccumuloSerDeParameters.ITERATOR_PUSHDOWN_KEY, AccumuloSerDeParameters.ITERATOR_PUSHDOWN_DEFAULT);	if (!shouldPushdown) {	
iterator pushdown is disabled for this table 

public DecomposedPredicate decompose(Configuration conf, ExprNodeDesc desc) {	IndexPredicateAnalyzer analyzer = newAnalyzer(conf);	List<IndexSearchCondition> sConditions = new ArrayList<IndexSearchCondition>();	ExprNodeDesc residualPredicate = analyzer.analyzePredicate(desc, sConditions);	if (sConditions.size() == 0) {	
nothing to decompose returning 

========================= hive sample_157 =========================

public Token<LlapTokenIdentifier> createToken( String appId, String user, boolean isSignatureRequired) throws IOException {	try {	Token<LlapTokenIdentifier> token = secretManager.createLlapToken( appId, user, isSignatureRequired);	if (LOG.isInfoEnabled()) {	
created a llap delegation token locally 

========================= hive sample_638 =========================

Output output = new Output();	boolean[] sortableSortOrders = new boolean[fields.size()];	Arrays.fill(sortableSortOrders, false);	byte[] columnNullMarker = new byte[fields.size()];	Arrays.fill(columnNullMarker, BinarySortableSerDe.ZERO);	byte[] columnNotNullMarker = new byte[fields.size()];	Arrays.fill(columnNotNullMarker, BinarySortableSerDe.ONE);	BinarySortableSerDe.serializeStruct(output, data, fois, sortableSortOrders, columnNullMarker, columnNotNullMarker);	hasTag = (output.getLength() != b.getLength());	if (hasTag) {	
tag found in keys and will be removed this should not happen 

public void setSerde(MapJoinObjectSerDeContext keyContext, MapJoinObjectSerDeContext valueContext) throws SerDeException {	AbstractSerDe keySerde = keyContext.getSerDe(), valSerde = valueContext.getSerDe();	if (writeHelper == null) {	
initializing container with and 

========================= hive sample_4048 =========================

public void uncaughtException(Thread thread, Throwable throwable) {	error = throwable;	
thread died 

txnBatch.write(data.getBytes());	txnBatch.commit();	}	} catch (Exception e) {	throw new RuntimeException(e);	} finally {	if (txnBatch != null) {	try {	txnBatch.close();	} catch (Exception e) {	
txnbatch close failed 

if (txnBatch != null) {	try {	txnBatch.close();	} catch (Exception e) {	conn.close();	}	}	try {	conn.close();	} catch (Exception e) {	
conn close failed 

private static boolean runDDL(IDriver driver, String sql) throws QueryFailedException {	LOG.debug(sql);	System.out.println(sql);	int retryCount = 1;	for (int attempt=0; attempt <= retryCount; ++attempt) {	try {	CommandProcessorResponse cpr = driver.run(sql);	if(cpr.getResponseCode() == 0) {	return true;	}	
statement failed 

========================= hive sample_907 =========================

public synchronized void updateTableColStatsInCache(String dbName, String tableName, List<ColumnStatisticsObj> colStatsForTable) {	for (ColumnStatisticsObj colStatObj : colStatsForTable) {	String key = CacheUtils.buildKey(dbName, tableName, colStatObj.getColName());	ColumnStatisticsObj oldStatsObj = tableColStatsCache.get(key);	if (oldStatsObj != null) {	
cachedstore updating table column stats for column of table and database 

public synchronized void updatePartitionColStatsInCache(String dbName, String tableName, List<String> partVals, List<ColumnStatisticsObj> colStatsObjs) {	for (ColumnStatisticsObj colStatObj : colStatsObjs) {	String key = CacheUtils.buildKey(dbName, tableName, partVals, colStatObj.getColName());	ColumnStatisticsObj oldStatsObj = partitionColStatsCache.get(key);	if (oldStatsObj != null) {	
cachedstore updating partition column stats for column of table and database 

public synchronized void addPartitionColStatsToCache( List<ColStatsObjWithSourceInfo> colStatsForDB) {	for (ColStatsObjWithSourceInfo colStatWithSourceInfo : colStatsForDB) {	List<String> partVals;	try {	partVals = Warehouse.getPartValuesFromPartName(colStatWithSourceInfo.getPartName());	ColumnStatisticsObj colStatObj = colStatWithSourceInfo.getColStatsObj();	String key = CacheUtils.buildKey(colStatWithSourceInfo.getDbName(), colStatWithSourceInfo.getTblName(), partVals, colStatObj.getColName());	partitionColStatsCache.put(key, colStatObj);	} catch (MetaException e) {	
unable to add partition stats for to sharedcache 

public synchronized void refreshPartitionColStats(String dbName, List<ColStatsObjWithSourceInfo> colStatsForDB) {	
cachedstore updating cached partition column stats objects for database 

public synchronized void refreshAggregateStatsCache(String dbName, String tblName, AggrStats aggrStatsAllPartitions, AggrStats aggrStatsAllButDefaultPartition) {	
cachedstore updating aggregate stats cache for database table 

public synchronized void refreshTableColStats(String dbName, String tableName, List<ColumnStatisticsObj> colStatsForTable) {	
cachedstore updating cached table column stats objects for database and table 

public synchronized void refreshDatabases(List<Database> databases) {	
cachedstore updating cached database objects 

public synchronized void refreshTables(String dbName, List<Table> tables) {	
cachedstore updating cached table objects for database 

public synchronized void refreshPartitions(String dbName, String tblName, List<Partition> partitions) {	
cachedstore updating cached partition objects for database and table 

========================= hive sample_1860 =========================

NumDistinctValueEstimator oldEst = aggregateData.getNdvEstimator();	NumDistinctValueEstimator newEst = newData.getNdvEstimator();	long ndv = -1;	if (oldEst.canMerge(newEst)) {	oldEst.mergeEstimators(newEst);	ndv = oldEst.estimateNumDistinctValues();	aggregateData.setNdvEstimator(oldEst);	} else {	ndv = Math.max(aggregateData.getNumDVs(), newData.getNumDVs());	}	
use bitvector to merge column s ndvs of and to be 

========================= hive sample_1925 =========================

private boolean tryReconnectToRunningJob(Configuration conf, Context context, LauncherDelegator.JobType jobType, String statusdir) throws IOException, InterruptedException {	if (!reconnectToRunningJobEnabledAndSupported(conf, jobType)) {	return false;	}	long startTime = getTempletonLaunchTime(conf);	UserGroupInformation ugi = UserGroupInformation.getCurrentUser();	WebHCatJTShim tracker = ShimLoader.getHadoopShims().getWebHCatShim(conf, ugi);	try {	Set<String> childJobs = tracker.getJobs(context.getJobID().toString(), startTime);	if (childJobs.size() == 0) {	
no child jobs found to reconnect with 

}	long startTime = getTempletonLaunchTime(conf);	UserGroupInformation ugi = UserGroupInformation.getCurrentUser();	WebHCatJTShim tracker = ShimLoader.getHadoopShims().getWebHCatShim(conf, ugi);	try {	Set<String> childJobs = tracker.getJobs(context.getJobID().toString(), startTime);	if (childJobs.size() == 0) {	return false;	}	if (childJobs.size() > 1) {	
found more than one child job to reconnect with s skipping reconnect 

try {	Set<String> childJobs = tracker.getJobs(context.getJobID().toString(), startTime);	if (childJobs.size() == 0) {	return false;	}	if (childJobs.size() > 1) {	return false;	}	String childJobIdString = childJobs.iterator().next();	org.apache.hadoop.mapred.JobID childJobId = org.apache.hadoop.mapred.JobID.forName(childJobIdString);	
reconnecting to an existing job s 

}	if (childJobs.size() > 1) {	return false;	}	String childJobIdString = childJobs.iterator().next();	org.apache.hadoop.mapred.JobID childJobId = org.apache.hadoop.mapred.JobID.forName(childJobIdString);	updateJobStatePercentAndChildId(conf, context.getJobID().toString(), null, childJobIdString);	do {	org.apache.hadoop.mapred.JobStatus jobStatus = tracker.getJobStatus(childJobId);	if (jobStatus.isJobComplete()) {	
child job s completed 

if (jobStatus.isJobComplete()) {	int exitCode = 0;	if (jobStatus.getRunState() != org.apache.hadoop.mapred.JobStatus.SUCCEEDED) {	exitCode = 1;	}	updateJobStateToDoneAndWriteExitValue(conf, statusdir, context.getJobID().toString(), exitCode);	break;	}	String percent = String.format("map %s%%, reduce %s%%", jobStatus.mapProgress()*100, jobStatus.reduceProgress()*100);	updateJobStatePercentAndChildId(conf, context.getJobID().toString(), percent, null);	
keepalive heart beat 

break;	}	String percent = String.format("map %s%%, reduce %s%%", jobStatus.mapProgress()*100, jobStatus.reduceProgress()*100);	updateJobStatePercentAndChildId(conf, context.getJobID().toString(), percent, null);	context.progress();	Thread.sleep(POLL_JOBPROGRESS_MSEC);	} while (true);	return true;	}	catch (IOException ex) {	
exception encountered in tryreconnecttorunningjob 

KeepAlive keepAlive = startCounterKeepAlive(pool, context);	proc.waitFor();	keepAlive.sendReport = false;	pool.shutdown();	if (!pool.awaitTermination(WATCHER_TIMEOUT_SECS, TimeUnit.SECONDS)) {	pool.shutdownNow();	}	updateJobStateToDoneAndWriteExitValue(conf, statusdir, context.getJobID().toString(), proc.exitValue());	Boolean enablelog = Boolean.parseBoolean(conf.get(ENABLE_LOG));	if (enablelog && TempletonUtils.isset(statusdir)) {	
templeton collecting logs for to logs 

private void updateJobStateToDoneAndWriteExitValue(Configuration conf, String statusdir, String jobId, int exitCode) throws IOException {	writeExitValue(conf, exitCode, statusdir);	JobState state = new JobState(jobId, conf);	state.setExitValue(exitCode);	state.setCompleteStatus("done");	state.close();	if (exitCode != 0) {	
templeton job failed with exit code 

private void updateJobStateToDoneAndWriteExitValue(Configuration conf, String statusdir, String jobId, int exitCode) throws IOException {	writeExitValue(conf, exitCode, statusdir);	JobState state = new JobState(jobId, conf);	state.setExitValue(exitCode);	state.setCompleteStatus("done");	state.close();	if (exitCode != 0) {	} else {	
templeton job completed with exit code 

state.setPercentComplete(percent);	}	if (childid != null) {	JobState childState = new JobState(childid, conf);	childState.setParent(jobId);	state.addChild(childid);	state.close();	}	}	} catch (IOException e) {	
templeton state error 

state.addChild(childid);	state.close();	}	}	} catch (IOException e) {	} finally {	if (state != null) {	try {	state.close();	} catch (IOException e) {	
caught exception while closing job state 

private void writeExitValue(Configuration conf, int exitValue, String statusdir) throws IOException {	if (TempletonUtils.isset(statusdir)) {	Path p = new Path(statusdir, EXIT_FNAME);	FileSystem fs = p.getFileSystem(conf);	OutputStream out = fs.create(p);	
templeton writing exit value to 

private void writeExitValue(Configuration conf, int exitValue, String statusdir) throws IOException {	if (TempletonUtils.isset(statusdir)) {	Path p = new Path(statusdir, EXIT_FNAME);	FileSystem fs = p.getFileSystem(conf);	OutputStream out = fs.create(p);	PrintWriter writer = new PrintWriter(out);	writer.println(exitValue);	writer.close();	
templeton exit value successfully written 

if (name.equals(STDERR_FNAME)) {	out = System.err;	} else {	out = System.out;	}	if (TempletonUtils.isset(statusdir)) {	Path p = new Path(statusdir, name);	FileSystem fs = p.getFileSystem(conf);	out = fs.create(p);	needCloseOutput = true;	
templeton writing status to 

writer.println(line);	String percent = TempletonUtils.extractPercentComplete(line);	String childid = TempletonUtils.extractChildJobId(line);	updateJobStatePercentAndChildId(conf, jobid.toString(), percent, childid);	}	writer.flush();	if(out != System.err && out != System.out) {	writer.close();	}	} catch (IOException e) {	
templeton execute error 

========================= hive sample_842 =========================

hiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");	hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");	hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");	System.setProperty(HiveConf.ConfVars.PREEXECHOOKS.varname, " ");	System.setProperty(HiveConf.ConfVars.POSTEXECHOOKS.varname, " ");	hiveConf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, warehousedir.toString());	try {	hmsc = new HiveMetaStoreClient(hiveConf);	initalizeTables();	} catch (Throwable e) {	
exception encountered while setting up testcase 

Assert.assertEquals("Comparing output of table " + tableNames[0] + " is not correct", outputs.get(0), "a,a,1,ag");	Assert.assertEquals("Comparing output of table " + tableNames[1] + " is not correct", outputs.get(1), "a,1,ag");	Assert.assertEquals("Comparing output of table " + tableNames[2] + " is not correct", outputs.get(2), "a,a,extra,1,ag");	for (int i = 0; i < tableNames.length; i++) {	Path partitionFile = new Path(warehousedir + "/" + tableNames[i] + "/ds=1/cluster=ag/part-m-00000");	FileSystem fs = partitionFile.getFileSystem(mrConf);	Assert.assertEquals("File permissions of table " + tableNames[i] + " is not correct", fs.getFileStatus(partitionFile).getPermission(), new FsPermission(tablePerms[i]));	Assert.assertEquals("File permissions of table " + tableNames[i] + " is not correct", fs.getFileStatus(partitionFile.getParent()).getPermission(), new FsPermission(tablePerms[i]));	Assert.assertEquals("File permissions of table " + tableNames[i] + " is not correct", fs.getFileStatus(partitionFile.getParent().getParent()).getPermission(), new FsPermission(tablePerms[i]));	}	
file permissions verified 

========================= hive sample_680 =========================

work = (MapWork) mrTask.getWork();	}	int lbLevel = (ctx.getLbCtx() == null) ? 0 : ctx.getLbCtx().calculateListBucketingLevel();	if (dpCtx != null &&  dpCtx.getNumDPCols() > 0) {	int numDPCols = dpCtx.getNumDPCols();	int dpLbLevel = numDPCols + lbLevel;	generateActualTasks(conf, resTsks, trgtSize, avgConditionSize, mvTask, mrTask, mrAndMvTask, dirPath, inpFs, ctx, work, dpLbLevel);	} else {	if(lbLevel == 0) {	long totalSz = getMergeSize(inpFs, dirPath, avgConditionSize);	
merge resolve simple case totalsz from 

setupMapRedWork(conf, work, trgtSize, totalSz);	resTsks.add(mrTask);	} else {	resTsks.add(mvTask);	}	} else {	generateActualTasks(conf, resTsks, trgtSize, avgConditionSize, mvTask, mrTask, mrAndMvTask, dirPath, inpFs, ctx, work, lbLevel);	}	}	} else {	
resolver returning movetask for 

private void generateActualTasks(HiveConf conf, List<Task<? extends Serializable>> resTsks, long trgtSize, long avgConditionSize, Task<? extends Serializable> mvTask, Task<? extends Serializable> mrTask, Task<? extends Serializable> mrAndMvTask, Path dirPath, FileSystem inpFs, ConditionalResolverMergeFilesCtx ctx, MapWork work, int dpLbLevel) throws IOException {	DynamicPartitionCtx dpCtx = ctx.getDPCtx();	FileStatus[] status = HiveStatsUtils.getFileStatusRecurse(dirPath, dpLbLevel, inpFs);	Map<Path, PartitionDesc> ptpi = work.getPathToPartitionInfo();	assert ptpi.size() == 1;	Path path = ptpi.keySet().iterator().next();	PartitionDesc partDesc = ptpi.get(path);	TableDesc tblDesc = partDesc.getTableDesc();	
merge resolver removing 

long totalSz = 0;	boolean doMerge = false;	List<Path> toMove = new ArrayList<Path>();	for (int i = 0; i < status.length; ++i) {	long len = getMergeSize(inpFs, status[i].getPath(), avgConditionSize);	if (len >= 0) {	doMerge = true;	totalSz += len;	PartitionDesc pDesc = (dpCtx != null) ? generateDPFullPartSpec(dpCtx, status, tblDesc, i) : partDesc;	if (pDesc == null) {	
merger ignoring invalid dp path 

List<Path> toMove = new ArrayList<Path>();	for (int i = 0; i < status.length; ++i) {	long len = getMergeSize(inpFs, status[i].getPath(), avgConditionSize);	if (len >= 0) {	doMerge = true;	totalSz += len;	PartitionDesc pDesc = (dpCtx != null) ? generateDPFullPartSpec(dpCtx, status, tblDesc, i) : partDesc;	if (pDesc == null) {	continue;	}	
merge resolver will merge 

long len = getMergeSize(inpFs, status[i].getPath(), avgConditionSize);	if (len >= 0) {	doMerge = true;	totalSz += len;	PartitionDesc pDesc = (dpCtx != null) ? generateDPFullPartSpec(dpCtx, status, tblDesc, i) : partDesc;	if (pDesc == null) {	continue;	}	work.resolveDynamicPartitionStoredAsSubDirsMerge(conf, status[i].getPath(), tblDesc, aliases, pDesc);	} else {	
merge resolver will move 

private AverageSize getAverageSize(FileSystem inpFs, Path dirPath) {	AverageSize error = new AverageSize(-1, -1);	try {	FileStatus[] fStats = inpFs.listStatus(dirPath);	long totalSz = 0;	int numFiles = 0;	for (FileStatus fStat : fStats) {	
resolver looking at 

========================= hive sample_3220 =========================

public RecordReader rowsOptions(Options options) throws IOException {	
reading orc rows from with 

========================= hive sample_3670 =========================

public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procContext, Object... nodeOutputs) throws SemanticException {	OptimizeTezProcContext context = (OptimizeTezProcContext) procContext;	AppMasterEventOperator event = (AppMasterEventOperator) nd;	AppMasterEventDesc desc = event.getConf();	if (desc.getStatistics().getDataSize() > context.conf .getLongVar(ConfVars.TEZ_DYNAMIC_PARTITION_PRUNING_MAX_DATA_SIZE) && (context.pruningOpsRemovedByPriorOpt.isEmpty() || !context.pruningOpsRemovedByPriorOpt.contains(event))) {	context.pruningOpsRemovedByPriorOpt.add(event);	GenTezUtils.removeBranch(event);	
disabling dynamic pruning for expected data size is too big 

========================= hive sample_3109 =========================

if (!LazyUtils.isNumberMaybe(bytes.getData(), start, length)) {	isNull = true;	return;	}	try {	byteData = Text.decode(bytes.getData(), start, length);	data.set(Double.parseDouble(byteData));	isNull = false;	} catch (NumberFormatException e) {	isNull = true;	
data not in the double data type range so converted to null given data is 

return;	}	try {	byteData = Text.decode(bytes.getData(), start, length);	data.set(Double.parseDouble(byteData));	isNull = false;	} catch (NumberFormatException e) {	isNull = true;	} catch (CharacterCodingException e) {	isNull = true;	
data not in the double data type range so converted to null 

========================= hive sample_5462 =========================

hashSet = (VectorMapJoinBytesHashSet) vectorMapJoinHashTable;	needHashTableSetup = false;	}	batchCounter++;	for(VectorExpression ve : bigTableFilterExpressions) {	ve.evaluate(batch);	}	final int inputLogicalSize = batch.size;	if (inputLogicalSize == 0) {	if (LOG.isDebugEnabled()) {	
batch empty 

keyVectorSerializeWrite.serializeWrite(batch, 0);	JoinUtil.JoinResult joinResult;	if (keyVectorSerializeWrite.getHasAnyNulls()) {	joinResult = JoinUtil.JoinResult.NOMATCH;	} else {	byte[] keyBytes = currentKeyOutput.getData();	int keyLength = currentKeyOutput.getLength();	joinResult = hashSet.contains(keyBytes, 0, keyLength, hashSetResults[0]);	}	if (LOG.isDebugEnabled()) {	
batch repeated joinresult 

} else {	byte[] keyBytes = currentKeyOutput.getData();	int keyLength = currentKeyOutput.getLength();	joinResult = hashSet.contains(keyBytes, 0, keyLength, hashSetResults[0]);	}	if (LOG.isDebugEnabled()) {	}	finishLeftSemiRepeated(batch, joinResult, hashSetResults[0]);	} else {	if (LOG.isDebugEnabled()) {	
batch non repeated 

}	if (haveSaveKey) {	switch (saveJoinResult) {	case MATCH: break;	case SPILL: hashSetResultCount++;	break;	case NOMATCH: break;	}	}	if (LOG.isDebugEnabled()) {	
allmatchs spills spillhashmapresultindices hashmapresults 

========================= hive sample_4121 =========================

public void handle(Context withinContext) throws Exception {	
processing drop table message 

========================= hive sample_3467 =========================

private void setupOutputFileStreams() throws IOException {	parentDir = FileUtils.createLocalDirsTempFile(spillLocalDirs, "bytes-container", "", true);	parentDir.deleteOnExit();	tmpFile = File.createTempFile("BytesContainer", ".tmp", parentDir);	
bytescontainer created temp file 

========================= hive sample_4208 =========================

int i = tsParent.getChildren().indexOf(tsChild);	if (i != -1) {	tsParent.getChildren().remove(i);	tsParent.getChildren().add(i, constNode);	}	}	break;	}	}	if (LOG.isInfoEnabled()) {	
dynamic pruning condition removed 

========================= hive sample_3037 =========================

static void checkFileAccess(FileSystem fs, FileStatus stat, FsAction action, UserGroupInformation ugi) throws IOException {	String user = ugi.getShortUserName();	String[] groups = ugi.getGroupNames();	if (groups != null) {	String superGroupName = fs.getConf().get("dfs.permissions.supergroup", "");	if (arrayContains(groups, superGroupName)) {	
user user belongs to super group supergroupname permission granted for action 

} else {	fullPath = path.getFileSystem(conf).makeQualified(path);	}	if(!"hdfs".equalsIgnoreCase(path.toUri().getScheme())) {	return false;	}	try {	HdfsAdmin hdfsAdmin = new HdfsAdmin(fsUri, conf);	return (hdfsAdmin.getEncryptionZoneForPath(fullPath) != null);	} catch (FileNotFoundException fnfe) {	
failed to get ez for non existent path 

public HadoopFileStatus(Configuration conf, FileSystem fs, Path file) throws IOException {	FileStatus fileStatus = fs.getFileStatus(file);	AclStatus aclStatus = null;	if (Objects.equal(conf.get("dfs.namenode.acls.enabled"), "true")) {	try {	aclStatus = fs.getAclStatus(file);	} catch (Exception e) {	
skipping acl inheritance file system for path does not support acls but dfs namenode acls enabled is set to true 

public HadoopFileStatus(Configuration conf, FileSystem fs, Path file) throws IOException {	FileStatus fileStatus = fs.getFileStatus(file);	AclStatus aclStatus = null;	if (Objects.equal(conf.get("dfs.namenode.acls.enabled"), "true")) {	try {	aclStatus = fs.getAclStatus(file);	} catch (Exception e) {	
the details are 

fsShell.setConf(conf);	if (group != null && !group.isEmpty()) {	run(fsShell, new String[]{"-chgrp", "-R", group, target.toString()});	}	if (aclEnabled) {	if (null != aclEntries) {	try {	String aclEntry = Joiner.on(",").join(aclEntries);	run(fsShell, new String[]{"-setfacl", "-R", "--set", aclEntry, target.toString()});	} catch (Exception e) {	
skipping acl inheritance file system for path does not support acls but dfs namenode acls enabled is set to true 

fsShell.setConf(conf);	if (group != null && !group.isEmpty()) {	run(fsShell, new String[]{"-chgrp", "-R", group, target.toString()});	}	if (aclEnabled) {	if (null != aclEntries) {	try {	String aclEntry = Joiner.on(",").join(aclEntries);	run(fsShell, new String[]{"-setfacl", "-R", "--set", aclEntry, target.toString()});	} catch (Exception e) {	
the details are 

}	if (aclEnabled) {	if (null != aclEntries) {	fs.setAcl(target, aclEntries);	}	} else {	fs.setPermission(target, sourcePerm);	}	}	} catch (Exception e) {	
unable to inherit permissions for file from file 

}	if (aclEnabled) {	if (null != aclEntries) {	fs.setAcl(target, aclEntries);	}	} else {	fs.setPermission(target, sourcePerm);	}	}	} catch (Exception e) {	
exception while inheriting permissions 

private static void run(FsShell shell, String[] command) throws Exception {	LOG.debug(ArrayUtils.toString(command));	int retval = shell.run(command);	
return value is 

========================= hive sample_1909 =========================

public long getLength() {	int len = 0;	try {	return split.getLength();	} catch (IOException e) {	
error getting length for split 

========================= hive sample_182 =========================

public static String getUserNameFromCookieToken(String tokenStr) {	Map<String, String> map = splitCookieToken(tokenStr);	if (!map.keySet().equals(COOKIE_ATTRIBUTES)) {	
invalid token with missing attributes 

private static Map<String, String> splitCookieToken(String tokenStr) {	Map<String, String> map = new HashMap<String, String>();	StringTokenizer st = new StringTokenizer(tokenStr, COOKIE_ATTR_SEPARATOR);	while (st.hasMoreTokens()) {	String part = st.nextToken();	int separator = part.indexOf(COOKIE_KEY_VALUE_SEPARATOR);	if (separator == -1) {	
invalid token string 

========================= hive sample_2299 =========================

public synchronized void shutdown() {	
shutting down query 

public synchronized void shutdown() {	shutdown = true;	for (TaskRunner runner : running) {	if (runner.isRunning()) {	Task<?> task = runner.getTask();	
shutting down task 

========================= hive sample_3172 =========================

public List<Task<? extends Serializable>> handle(Context context) throws SemanticException {	try {	FunctionDescBuilder builder = new FunctionDescBuilder(context);	CreateFunctionDesc descToLoad = builder.build();	this.functionName = builder.metadata.function.getFunctionName();	
loading function desc 

public List<Task<? extends Serializable>> handle(Context context) throws SemanticException {	try {	FunctionDescBuilder builder = new FunctionDescBuilder(context);	CreateFunctionDesc descToLoad = builder.build();	this.functionName = builder.metadata.function.getFunctionName();	Task<FunctionWork> createTask = TaskFactory.get( new FunctionWork(descToLoad), context.hiveConf );	
added create function task 

========================= hive sample_3488 =========================

public VectorReduceSinkCommonOperator(CompilationOpContext ctx, OperatorDesc conf, VectorizationContext vContext, VectorDesc vectorDesc) throws HiveException {	this(ctx);	
vectorreducesinkcommonoperator constructor 

private void doCollect(HiveKey keyWritable, BytesWritable valueWritable) throws IOException {	if (null != out) {	numRows++;	if (LOG.isInfoEnabled()) {	if (numRows == cntr) {	cntr = logEveryNRows == 0 ? cntr * 10 : numRows + logEveryNRows;	if (cntr < 0 || numRows < 0) {	cntr = 0;	numRows = 1;	}	
records written 

protected void closeOp(boolean abort) throws HiveException {	if (!abort && reducerHash != null) {	reducerHash.flush();	}	super.closeOp(abort);	out = null;	reducerHash = null;	if (LOG.isInfoEnabled()) {	
records written 

========================= hive sample_4199 =========================

private Collection<TestBatch> parse() throws IOException {	RootConfig rootConfig = getRootConfig(unitRootContext);	
rootconfig 

private Collection<TestBatch> parse() throws IOException {	RootConfig rootConfig = getRootConfig(unitRootContext);	Map<String, ModuleConfig> moduleConfigs = extractModuleConfigs();	
moduleconfigs 

String absUnitTestDir = unitTestParent.getCanonicalPath();	Preconditions.checkState(absUnitTestDir.startsWith(srcDirString), "Unit test dir: " + absUnitTestDir + " is not under provided src dir: " + srcDirString);	String modulePath = absUnitTestDir.substring(srcDirString.length());	modulePath = stripEndAndStart(modulePath, "/");	Preconditions.checkState(!modulePath.startsWith("/"), String.format("Illegal module path: [%s]", modulePath));	if (StringUtils.isEmpty(modulePath)) {	modulePath = PREFIX_TOP_LEVEL;	}	String moduleName = getModuleNameFromPathPrefix(modulePath);	logger.info("modulePath determined as {} for testdir={}, DerivedModuleName={}", modulePath, absUnitTestDir, moduleName);	
adding unittests dir 

String modulePath = absUnitTestDir.substring(srcDirString.length());	modulePath = stripEndAndStart(modulePath, "/");	Preconditions.checkState(!modulePath.startsWith("/"), String.format("Illegal module path: [%s]", modulePath));	if (StringUtils.isEmpty(modulePath)) {	modulePath = PREFIX_TOP_LEVEL;	}	String moduleName = getModuleNameFromPathPrefix(modulePath);	logger.info("modulePath determined as {} for testdir={}, DerivedModuleName={}", modulePath, absUnitTestDir, moduleName);	unitTestsDirs.add(new TestDir(unitTestParent, moduleName));	} else {	
unit test directory does not exist or is a file 

String testName = className.replaceAll("\\.class$", "");	String pathPrefix = getPathPrefix(classFile, rootConfig.subDirForPrefix);	String moduleName = getModuleNameFromPathPrefix(pathPrefix);	logger.debug("In {}, found class {} with pathPrefix={}, moduleName={}", unitTestDir.path, className, pathPrefix, moduleName);	ModuleConfig moduleConfig = moduleConfigs.get(moduleName);	if (moduleConfig == null) {	moduleConfig = FAKE_MODULE_CONFIG;	}	TestInfo testInfo = checkAndGetTestInfo(moduleName, pathPrefix, testName, rootConfig, moduleConfig);	if (testInfo != null) {	
adding test 

Preconditions.checkState(fname.startsWith(sourceDirectory.getCanonicalPath()));	fname = fname.substring(sourceDirectory.getCanonicalPath().length(), fname.length());	if (fname.contains(subDirPrefix)) {	fname = fname.substring(0, fname.indexOf(subDirPrefix));	fname = StringUtils.stripStart(fname, "/");	if (StringUtils.isEmpty(fname)) {	fname = PREFIX_TOP_LEVEL;	}	return fname;	} else {	
could not find subdirprefix in path 

}	}	if (excludedProvided.contains(testName)) {	rejectReason = "test present in provided exclude list";	return null;	}	testInfo = new TestInfo(moduleName, moduleRelDir, testName, rootConfig.skipBatching.contains(testName) || moduleConfig.skipBatching.contains(testName), rootConfig.isolate.contains(testName) || moduleConfig.isolate.contains(testName));	return testInfo;	} finally {	if (rejectReason != null) {	
excluding due to 

========================= hive sample_5622 =========================

public void init(RegistryConf conf) throws Exception {	RegistryConfTez rct = (RegistryConfTez) conf;	for (String inputSourceName : rct.baseWork.getInputSourceToRuntimeValuesInfo().keySet()) {	
runtime value source 

while (kvReader.next()) {	Object row = deserializer.deserialize((Writable) kvReader.getCurrentValue());	rowCount++;	for (int colIdx = 0; colIdx < colExprEvaluators.size(); ++colIdx) {	ExprNodeEvaluator eval = colExprEvaluators.get(colIdx);	Object val = eval.evaluate(row);	setValue(runtimeValuesInfo.getDynamicValueIDs().get(colIdx), val);	}	}	if (rowCount == 0) {	
no input rows from filling dynamic values with nulls 

========================= hive sample_3963 =========================

parseContext = pctx;	hiveConf = parseContext.getConf();	try {	hiveDb = Hive.get(hiveConf);	} catch (HiveException e) {	LOG.error(org.apache.hadoop.util.StringUtils.stringifyException(e));	throw new SemanticException(e.getMessage(), e);	}	HiveConf.setBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTINDEXFILTER, false);	if (shouldApplyOptimization()) {	
rewriting original query using optimization 

private boolean checkIfIndexBuiltOnAllTablePartitions(TableScanOperator tableScan, List<Index> indexes) throws SemanticException {	Set<Partition> queryPartitions;	try {	queryPartitions = IndexUtils.checkPartitionsCoveredByIndex(tableScan, parseContext, indexes);	if (queryPartitions == null) {	return false;	}	} catch (HiveException e) {	
fatal error problem accessing metastore 

private void rewriteOriginalQuery() throws SemanticException {	for (RewriteCanApplyCtx canApplyCtx : tsOpToProcess.values()) {	RewriteQueryUsingAggregateIndexCtx rewriteQueryCtx = RewriteQueryUsingAggregateIndexCtx.getInstance(parseContext, hiveDb, canApplyCtx);	rewriteQueryCtx.invokeRewriteQueryProc();	parseContext = rewriteQueryCtx.getParseContext();	}	
finished rewriting query 

========================= hive sample_3060 =========================

String dateString = sdf.format(now);	String exportPathString = MetastoreConf.getVar(conf, MetastoreConf.ConfVars.METADATA_EXPORT_LOCATION);	boolean moveMetadataToTrash = MetastoreConf .getBoolVar(conf, MetastoreConf.ConfVars.MOVE_EXPORTED_METADATA_TO_TRASH);	Path exportPath = null;	if (exportPathString != null && exportPathString.length() == 0) {	exportPath = fs.getHomeDirectory();	} else {	exportPath = new Path(exportPathString);	}	Path metaPath = new Path(exportPath, name + "." + dateString);	
exporting the metadata of table to path 

========================= hive sample_3575 =========================

val = fields[iColumn];	if ("\\N".equals(val)) {	continue;	}	}	byte [] valBytes = Bytes.toBytes(val);	KeyValue kv = new KeyValue( rowKeyBytes, columnFamilyNameBytes, columnNameBytes, valBytes);	try {	fileWriter.write(null, kv);	} catch (IOException e) {	
failed while writing row 

========================= hive sample_585 =========================

public synchronized void stop() {	if (isStarted && !isEmbedded) {	if(server != null) {	server.stop();	
thrift server has stopped 

public synchronized void stop() {	if (isStarted && !isEmbedded) {	if(server != null) {	server.stop();	}	if((httpServer != null) && httpServer.isStarted()) {	try {	httpServer.stop();	
http server has stopped 

public synchronized void stop() {	if (isStarted && !isEmbedded) {	if(server != null) {	server.stop();	}	if((httpServer != null) && httpServer.isStarted()) {	try {	httpServer.stop();	} catch (Exception e) {	
error stopping http server 

public TGetDelegationTokenResp GetDelegationToken(TGetDelegationTokenReq req) throws TException {	TGetDelegationTokenResp resp = new TGetDelegationTokenResp();	if (hiveAuthFactory == null || !hiveAuthFactory.isSASLKerberosUser()) {	resp.setStatus(unsecureTokenErrorStatus());	} else {	try {	String token = cliService.getDelegationToken( new SessionHandle(req.getSessionHandle()), hiveAuthFactory, req.getOwner(), req.getRenewer());	resp.setDelegationToken(token);	resp.setStatus(OK_STATUS);	} catch (HiveSQLException e) {	
error obtaining delegation token 

public TCancelDelegationTokenResp CancelDelegationToken(TCancelDelegationTokenReq req) throws TException {	TCancelDelegationTokenResp resp = new TCancelDelegationTokenResp();	if (hiveAuthFactory == null || !hiveAuthFactory.isSASLKerberosUser()) {	resp.setStatus(unsecureTokenErrorStatus());	} else {	try {	cliService.cancelDelegationToken(new SessionHandle(req.getSessionHandle()), hiveAuthFactory, req.getDelegationToken());	resp.setStatus(OK_STATUS);	} catch (HiveSQLException e) {	
error canceling delegation token 

public TRenewDelegationTokenResp RenewDelegationToken(TRenewDelegationTokenReq req) throws TException {	TRenewDelegationTokenResp resp = new TRenewDelegationTokenResp();	if (hiveAuthFactory == null || !hiveAuthFactory.isSASLKerberosUser()) {	resp.setStatus(unsecureTokenErrorStatus());	} else {	try {	cliService.renewDelegationToken(new SessionHandle(req.getSessionHandle()), hiveAuthFactory, req.getDelegationToken());	resp.setStatus(OK_STATUS);	} catch (HiveSQLException e) {	
error obtaining renewing token 

public TOpenSessionResp OpenSession(TOpenSessionReq req) throws TException {	
client protocol version 

Map<String, String> configurationMap = new HashMap<String, String>();	HiveConf sessionConf = cliService.getSessionConf(sessionHandle);	configurationMap.put( HiveConf.ConfVars.HIVE_SERVER2_THRIFT_RESULTSET_DEFAULT_FETCH_SIZE.varname, Integer.toString(sessionConf != null ? sessionConf.getIntVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_RESULTSET_DEFAULT_FETCH_SIZE) : hiveConf.getIntVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_RESULTSET_DEFAULT_FETCH_SIZE)));	resp.setConfiguration(configurationMap);	resp.setStatus(OK_STATUS);	ThriftCLIServerContext context = (ThriftCLIServerContext)currentServerContext.get();	if (context != null) {	context.setSessionHandle(sessionHandle);	}	} catch (Exception e) {	
error opening session 

sh = new SessionHandle(req.getSessionHandle());	sb = new StringBuilder("Client information for ").append(sh).append(": ");	} else {	sb.append(", ");	}	sb.append(e.getKey()).append(" = ").append(e.getValue());	if ("ApplicationName".equals(e.getKey())) {	try {	cliService.setApplicationName(sh, e.getValue());	} catch (Exception ex) {	
error setting application name 

clientIpAddress = SessionManager.getIpAddress();	}	else {	if (hiveAuthFactory != null && hiveAuthFactory.isSASLWithKerberizedHadoop()) {	clientIpAddress = hiveAuthFactory.getIpAddress();	}	else {	clientIpAddress = TSetIpAddressProcessor.getUserIpAddress();	}	}	
client s ip address 

userName = TSetIpAddressProcessor.getUserName();	}	if (cliService.getHiveConf().getVar( ConfVars.HIVE_SERVER2_TRANSPORT_MODE).equalsIgnoreCase("http")) {	userName = SessionManager.getUserName();	}	if (userName == null) {	userName = req.getUsername();	}	userName = getShortName(userName);	String effectiveClientUser = getProxyUser(userName, req.getConfiguration(), getIpAddress());	
client s username 

TCloseSessionResp resp = new TCloseSessionResp();	try {	SessionHandle sessionHandle = new SessionHandle(req.getSessionHandle());	cliService.closeSession(sessionHandle);	resp.setStatus(OK_STATUS);	ThriftCLIServerContext context = (ThriftCLIServerContext)currentServerContext.get();	if (context != null) {	context.setSessionHandle(null);	}	} catch (Exception e) {	
error closing session 

public TGetInfoResp GetInfo(TGetInfoReq req) throws TException {	TGetInfoResp resp = new TGetInfoResp();	try {	GetInfoValue getInfoValue = cliService.getInfo(new SessionHandle(req.getSessionHandle()), GetInfoType.getGetInfoType(req.getInfoType()));	resp.setInfoValue(getInfoValue.toTGetInfoValue());	resp.setStatus(OK_STATUS);	} catch (Exception e) {	
error getting info 

try {	SessionHandle sessionHandle = new SessionHandle(req.getSessionHandle());	String statement = req.getStatement();	Map<String, String> confOverlay = req.getConfOverlay();	Boolean runAsync = req.isRunAsync();	long queryTimeout = req.getQueryTimeout();	OperationHandle operationHandle = runAsync ? cliService.executeStatementAsync(sessionHandle, statement, confOverlay, queryTimeout) : cliService.executeStatement(sessionHandle, statement, confOverlay, queryTimeout);	resp.setOperationHandle(operationHandle.toTOperationHandle());	resp.setStatus(OK_STATUS);	} catch (Exception e) {	
error executing statement 

public TGetTypeInfoResp GetTypeInfo(TGetTypeInfoReq req) throws TException {	TGetTypeInfoResp resp = new TGetTypeInfoResp();	try {	OperationHandle operationHandle = cliService.getTypeInfo(new SessionHandle(req.getSessionHandle()));	resp.setOperationHandle(operationHandle.toTOperationHandle());	resp.setStatus(OK_STATUS);	} catch (Exception e) {	
error getting type info 

public TGetSchemasResp GetSchemas(TGetSchemasReq req) throws TException {	TGetSchemasResp resp = new TGetSchemasResp();	try {	OperationHandle opHandle = cliService.getSchemas( new SessionHandle(req.getSessionHandle()), req.getCatalogName(), req.getSchemaName());	resp.setOperationHandle(opHandle.toTOperationHandle());	resp.setStatus(OK_STATUS);	} catch (Exception e) {	
error getting schemas 

public TGetTablesResp GetTables(TGetTablesReq req) throws TException {	TGetTablesResp resp = new TGetTablesResp();	try {	OperationHandle opHandle = cliService .getTables(new SessionHandle(req.getSessionHandle()), req.getCatalogName(), req.getSchemaName(), req.getTableName(), req.getTableTypes());	resp.setOperationHandle(opHandle.toTOperationHandle());	resp.setStatus(OK_STATUS);	} catch (Exception e) {	
error getting tables 

public TGetTableTypesResp GetTableTypes(TGetTableTypesReq req) throws TException {	TGetTableTypesResp resp = new TGetTableTypesResp();	try {	OperationHandle opHandle = cliService.getTableTypes(new SessionHandle(req.getSessionHandle()));	resp.setOperationHandle(opHandle.toTOperationHandle());	resp.setStatus(OK_STATUS);	} catch (Exception e) {	
error getting table types 

public TGetColumnsResp GetColumns(TGetColumnsReq req) throws TException {	TGetColumnsResp resp = new TGetColumnsResp();	try {	OperationHandle opHandle = cliService.getColumns( new SessionHandle(req.getSessionHandle()), req.getCatalogName(), req.getSchemaName(), req.getTableName(), req.getColumnName());	resp.setOperationHandle(opHandle.toTOperationHandle());	resp.setStatus(OK_STATUS);	} catch (Exception e) {	
error getting columns 

public TGetFunctionsResp GetFunctions(TGetFunctionsReq req) throws TException {	TGetFunctionsResp resp = new TGetFunctionsResp();	try {	OperationHandle opHandle = cliService.getFunctions( new SessionHandle(req.getSessionHandle()), req.getCatalogName(), req.getSchemaName(), req.getFunctionName());	resp.setOperationHandle(opHandle.toTOperationHandle());	resp.setStatus(OK_STATUS);	} catch (Exception e) {	
error getting functions 

if (opException != null) {	resp.setSqlState(opException.getSQLState());	resp.setErrorCode(opException.getErrorCode());	if (opException.getErrorCode() == 29999) resp.setErrorMessage(org.apache.hadoop.util.StringUtils.stringifyException(opException));	else resp.setErrorMessage(opException.getMessage());	} else if (executionStatus == TJobExecutionStatus.NOT_AVAILABLE && OperationType.EXECUTE_STATEMENT.equals(operationHandle.getOperationType())) {	resp.getProgressUpdateResponse().setProgressedPercentage( getProgressedPercentage(operationHandle));	}	resp.setStatus(OK_STATUS);	} catch (Exception e) {	
error getting operation status 

public TCancelOperationResp CancelOperation(TCancelOperationReq req) throws TException {	TCancelOperationResp resp = new TCancelOperationResp();	try {	cliService.cancelOperation(new OperationHandle(req.getOperationHandle()));	resp.setStatus(OK_STATUS);	} catch (Exception e) {	
error cancelling operation 

public TCloseOperationResp CloseOperation(TCloseOperationReq req) throws TException {	TCloseOperationResp resp = new TCloseOperationResp();	try {	cliService.closeOperation(new OperationHandle(req.getOperationHandle()));	resp.setStatus(OK_STATUS);	} catch (Exception e) {	
error closing operation 

public TGetResultSetMetadataResp GetResultSetMetadata(TGetResultSetMetadataReq req) throws TException {	TGetResultSetMetadataResp resp = new TGetResultSetMetadataResp();	try {	TableSchema schema = cliService.getResultSetMetadata(new OperationHandle(req.getOperationHandle()));	resp.setSchema(schema.toTTableSchema());	resp.setStatus(OK_STATUS);	} catch (Exception e) {	
error getting result set metadata 

try {	int maxFetchSize = hiveConf.getIntVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_RESULTSET_MAX_FETCH_SIZE);	if (req.getMaxRows() > maxFetchSize) {	req.setMaxRows(maxFetchSize);	}	RowSet rowSet = cliService.fetchResults( new OperationHandle(req.getOperationHandle()), FetchOrientation.getFetchOrientation(req.getOrientation()), req.getMaxRows(), FetchType.getFetchType(req.getFetchType()));	resp.setResults(rowSet.toTRowSet());	resp.setHasMoreRows(false);	resp.setStatus(OK_STATUS);	} catch (Exception e) {	
error fetching results 

public TGetPrimaryKeysResp GetPrimaryKeys(TGetPrimaryKeysReq req) throws TException {	TGetPrimaryKeysResp resp = new TGetPrimaryKeysResp();	try {	OperationHandle opHandle = cliService.getPrimaryKeys( new SessionHandle(req.getSessionHandle()), req.getCatalogName(), req.getSchemaName(), req.getTableName());	resp.setOperationHandle(opHandle.toTOperationHandle());	resp.setStatus(OK_STATUS);	} catch (Exception e) {	
error getting functions 

public TGetCrossReferenceResp GetCrossReference(TGetCrossReferenceReq req) throws TException {	TGetCrossReferenceResp resp = new TGetCrossReferenceResp();	try {	OperationHandle opHandle = cliService.getCrossReference( new SessionHandle(req.getSessionHandle()), req.getParentCatalogName(), req.getParentSchemaName(), req.getParentTableName(), req.getForeignCatalogName(), req.getForeignSchemaName(), req.getForeignTableName());	resp.setOperationHandle(opHandle.toTOperationHandle());	resp.setStatus(OK_STATUS);	} catch (Exception e) {	
error getting functions 

public abstract void run();	String proxyUser = null;	if (cliService.getHiveConf().getVar( ConfVars.HIVE_SERVER2_TRANSPORT_MODE).equalsIgnoreCase("http")) {	proxyUser = SessionManager.getProxyUserName();	
proxy user from query string 

public abstract void run();	String proxyUser = null;	if (cliService.getHiveConf().getVar( ConfVars.HIVE_SERVER2_TRANSPORT_MODE).equalsIgnoreCase("http")) {	proxyUser = SessionManager.getProxyUserName();	}	if (proxyUser == null && sessionConf != null && sessionConf.containsKey(HiveAuthConstants.HS2_PROXY_USER)) {	String proxyUserFromThriftBody = sessionConf.get(HiveAuthConstants.HS2_PROXY_USER);	
proxy user from thrift body 

if (proxyUser == null) {	return realUser;	}	if (!hiveConf.getBoolVar(HiveConf.ConfVars.HIVE_SERVER2_ALLOW_USER_SUBSTITUTION)) {	throw new HiveSQLException("Proxy user substitution is not allowed");	}	if (HiveAuthConstants.AuthTypes.NONE.toString(). equalsIgnoreCase(hiveConf.getVar(ConfVars.HIVE_SERVER2_AUTHENTICATION))) {	return proxyUser;	}	HiveAuthFactory.verifyProxyAccess(realUser, proxyUser, ipAddress, hiveConf);	
verified proxy user 

========================= hive sample_2364 =========================

public AsyncMethodCallback<String> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<String>() {	public void onComplete(String o) {	getMetaConf_result result = new getMetaConf_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	setMetaConf_result result = new setMetaConf_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	create_database_result result = new create_database_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Database> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Database>() {	public void onComplete(Database o) {	get_database_result result = new get_database_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	drop_database_result result = new drop_database_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<String>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<String>>() {	public void onComplete(List<String> o) {	get_databases_result result = new get_databases_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<String>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<String>>() {	public void onComplete(List<String> o) {	get_all_databases_result result = new get_all_databases_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	alter_database_result result = new alter_database_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Type> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Type>() {	public void onComplete(Type o) {	get_type_result result = new get_type_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Boolean>() {	public void onComplete(Boolean o) {	create_type_result result = new create_type_result();	result.success = o;	result.setSuccessIsSet(true);	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Boolean>() {	public void onComplete(Boolean o) {	drop_type_result result = new drop_type_result();	result.success = o;	result.setSuccessIsSet(true);	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Map<String,Type>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Map<String,Type>>() {	public void onComplete(Map<String,Type> o) {	get_type_all_result result = new get_type_all_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<FieldSchema>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<FieldSchema>>() {	public void onComplete(List<FieldSchema> o) {	get_fields_result result = new get_fields_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<FieldSchema>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<FieldSchema>>() {	public void onComplete(List<FieldSchema> o) {	get_fields_with_environment_context_result result = new get_fields_with_environment_context_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<FieldSchema>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<FieldSchema>>() {	public void onComplete(List<FieldSchema> o) {	get_schema_result result = new get_schema_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<FieldSchema>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<FieldSchema>>() {	public void onComplete(List<FieldSchema> o) {	get_schema_with_environment_context_result result = new get_schema_with_environment_context_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	create_table_result result = new create_table_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	create_table_with_environment_context_result result = new create_table_with_environment_context_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	create_table_with_constraints_result result = new create_table_with_constraints_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	drop_constraint_result result = new drop_constraint_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	add_primary_key_result result = new add_primary_key_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	add_foreign_key_result result = new add_foreign_key_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	add_unique_constraint_result result = new add_unique_constraint_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	add_not_null_constraint_result result = new add_not_null_constraint_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	drop_table_result result = new drop_table_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	drop_table_with_environment_context_result result = new drop_table_with_environment_context_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	truncate_table_result result = new truncate_table_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<String>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<String>>() {	public void onComplete(List<String> o) {	get_tables_result result = new get_tables_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<String>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<String>>() {	public void onComplete(List<String> o) {	get_tables_by_type_result result = new get_tables_by_type_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<String>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<String>>() {	public void onComplete(List<String> o) {	get_materialized_views_for_rewriting_result result = new get_materialized_views_for_rewriting_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<TableMeta>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<TableMeta>>() {	public void onComplete(List<TableMeta> o) {	get_table_meta_result result = new get_table_meta_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<String>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<String>>() {	public void onComplete(List<String> o) {	get_all_tables_result result = new get_all_tables_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Table> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Table>() {	public void onComplete(Table o) {	get_table_result result = new get_table_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<Table>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<Table>>() {	public void onComplete(List<Table> o) {	get_table_objects_by_name_result result = new get_table_objects_by_name_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	get_table_objects_by_name_result result = new get_table_objects_by_name_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<GetTableResult> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<GetTableResult>() {	public void onComplete(GetTableResult o) {	get_table_req_result result = new get_table_req_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<GetTablesResult> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<GetTablesResult>() {	public void onComplete(GetTablesResult o) {	get_table_objects_by_name_req_result result = new get_table_objects_by_name_req_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Map<String,Materialization>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Map<String,Materialization>>() {	public void onComplete(Map<String,Materialization> o) {	get_materialization_invalidation_info_result result = new get_materialization_invalidation_info_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<String>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<String>>() {	public void onComplete(List<String> o) {	get_table_names_by_filter_result result = new get_table_names_by_filter_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	alter_table_result result = new alter_table_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	alter_table_with_environment_context_result result = new alter_table_with_environment_context_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	alter_table_with_cascade_result result = new alter_table_with_cascade_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Partition> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Partition>() {	public void onComplete(Partition o) {	add_partition_result result = new add_partition_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Partition> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Partition>() {	public void onComplete(Partition o) {	add_partition_with_environment_context_result result = new add_partition_with_environment_context_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Integer>() {	public void onComplete(Integer o) {	add_partitions_result result = new add_partitions_result();	result.success = o;	result.setSuccessIsSet(true);	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Integer>() {	public void onComplete(Integer o) {	add_partitions_pspec_result result = new add_partitions_pspec_result();	result.success = o;	result.setSuccessIsSet(true);	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Partition> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Partition>() {	public void onComplete(Partition o) {	append_partition_result result = new append_partition_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<AddPartitionsResult> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<AddPartitionsResult>() {	public void onComplete(AddPartitionsResult o) {	add_partitions_req_result result = new add_partitions_req_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Partition> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Partition>() {	public void onComplete(Partition o) {	append_partition_with_environment_context_result result = new append_partition_with_environment_context_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Partition> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Partition>() {	public void onComplete(Partition o) {	append_partition_by_name_result result = new append_partition_by_name_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Partition> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Partition>() {	public void onComplete(Partition o) {	append_partition_by_name_with_environment_context_result result = new append_partition_by_name_with_environment_context_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Boolean>() {	public void onComplete(Boolean o) {	drop_partition_result result = new drop_partition_result();	result.success = o;	result.setSuccessIsSet(true);	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Boolean>() {	public void onComplete(Boolean o) {	drop_partition_with_environment_context_result result = new drop_partition_with_environment_context_result();	result.success = o;	result.setSuccessIsSet(true);	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Boolean>() {	public void onComplete(Boolean o) {	drop_partition_by_name_result result = new drop_partition_by_name_result();	result.success = o;	result.setSuccessIsSet(true);	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Boolean>() {	public void onComplete(Boolean o) {	drop_partition_by_name_with_environment_context_result result = new drop_partition_by_name_with_environment_context_result();	result.success = o;	result.setSuccessIsSet(true);	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<DropPartitionsResult> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<DropPartitionsResult>() {	public void onComplete(DropPartitionsResult o) {	drop_partitions_req_result result = new drop_partitions_req_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Partition> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Partition>() {	public void onComplete(Partition o) {	get_partition_result result = new get_partition_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Partition> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Partition>() {	public void onComplete(Partition o) {	exchange_partition_result result = new exchange_partition_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<Partition>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<Partition>>() {	public void onComplete(List<Partition> o) {	exchange_partitions_result result = new exchange_partitions_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Partition> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Partition>() {	public void onComplete(Partition o) {	get_partition_with_auth_result result = new get_partition_with_auth_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Partition> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Partition>() {	public void onComplete(Partition o) {	get_partition_by_name_result result = new get_partition_by_name_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<Partition>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<Partition>>() {	public void onComplete(List<Partition> o) {	get_partitions_result result = new get_partitions_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<Partition>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<Partition>>() {	public void onComplete(List<Partition> o) {	get_partitions_with_auth_result result = new get_partitions_with_auth_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<PartitionSpec>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<PartitionSpec>>() {	public void onComplete(List<PartitionSpec> o) {	get_partitions_pspec_result result = new get_partitions_pspec_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<String>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<String>>() {	public void onComplete(List<String> o) {	get_partition_names_result result = new get_partition_names_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<PartitionValuesResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<PartitionValuesResponse>() {	public void onComplete(PartitionValuesResponse o) {	get_partition_values_result result = new get_partition_values_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<Partition>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<Partition>>() {	public void onComplete(List<Partition> o) {	get_partitions_ps_result result = new get_partitions_ps_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<Partition>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<Partition>>() {	public void onComplete(List<Partition> o) {	get_partitions_ps_with_auth_result result = new get_partitions_ps_with_auth_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<String>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<String>>() {	public void onComplete(List<String> o) {	get_partition_names_ps_result result = new get_partition_names_ps_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<Partition>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<Partition>>() {	public void onComplete(List<Partition> o) {	get_partitions_by_filter_result result = new get_partitions_by_filter_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<PartitionSpec>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<PartitionSpec>>() {	public void onComplete(List<PartitionSpec> o) {	get_part_specs_by_filter_result result = new get_part_specs_by_filter_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<PartitionsByExprResult> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<PartitionsByExprResult>() {	public void onComplete(PartitionsByExprResult o) {	get_partitions_by_expr_result result = new get_partitions_by_expr_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Integer>() {	public void onComplete(Integer o) {	get_num_partitions_by_filter_result result = new get_num_partitions_by_filter_result();	result.success = o;	result.setSuccessIsSet(true);	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<Partition>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<Partition>>() {	public void onComplete(List<Partition> o) {	get_partitions_by_names_result result = new get_partitions_by_names_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	alter_partition_result result = new alter_partition_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	alter_partitions_result result = new alter_partitions_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	alter_partitions_with_environment_context_result result = new alter_partitions_with_environment_context_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	alter_partition_with_environment_context_result result = new alter_partition_with_environment_context_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	rename_partition_result result = new rename_partition_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Boolean>() {	public void onComplete(Boolean o) {	partition_name_has_valid_characters_result result = new partition_name_has_valid_characters_result();	result.success = o;	result.setSuccessIsSet(true);	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<String> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<String>() {	public void onComplete(String o) {	get_config_value_result result = new get_config_value_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<String>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<String>>() {	public void onComplete(List<String> o) {	partition_name_to_vals_result result = new partition_name_to_vals_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Map<String,String>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Map<String,String>>() {	public void onComplete(Map<String,String> o) {	partition_name_to_spec_result result = new partition_name_to_spec_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	markPartitionForEvent_result result = new markPartitionForEvent_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Boolean>() {	public void onComplete(Boolean o) {	isPartitionMarkedForEvent_result result = new isPartitionMarkedForEvent_result();	result.success = o;	result.setSuccessIsSet(true);	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Index> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Index>() {	public void onComplete(Index o) {	add_index_result result = new add_index_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	alter_index_result result = new alter_index_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Boolean>() {	public void onComplete(Boolean o) {	drop_index_by_name_result result = new drop_index_by_name_result();	result.success = o;	result.setSuccessIsSet(true);	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Index> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Index>() {	public void onComplete(Index o) {	get_index_by_name_result result = new get_index_by_name_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<Index>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<Index>>() {	public void onComplete(List<Index> o) {	get_indexes_result result = new get_indexes_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<String>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<String>>() {	public void onComplete(List<String> o) {	get_index_names_result result = new get_index_names_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<PrimaryKeysResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<PrimaryKeysResponse>() {	public void onComplete(PrimaryKeysResponse o) {	get_primary_keys_result result = new get_primary_keys_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<ForeignKeysResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<ForeignKeysResponse>() {	public void onComplete(ForeignKeysResponse o) {	get_foreign_keys_result result = new get_foreign_keys_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<UniqueConstraintsResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<UniqueConstraintsResponse>() {	public void onComplete(UniqueConstraintsResponse o) {	get_unique_constraints_result result = new get_unique_constraints_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<NotNullConstraintsResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<NotNullConstraintsResponse>() {	public void onComplete(NotNullConstraintsResponse o) {	get_not_null_constraints_result result = new get_not_null_constraints_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Boolean>() {	public void onComplete(Boolean o) {	update_table_column_statistics_result result = new update_table_column_statistics_result();	result.success = o;	result.setSuccessIsSet(true);	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Boolean>() {	public void onComplete(Boolean o) {	update_partition_column_statistics_result result = new update_partition_column_statistics_result();	result.success = o;	result.setSuccessIsSet(true);	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<ColumnStatistics> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<ColumnStatistics>() {	public void onComplete(ColumnStatistics o) {	get_table_column_statistics_result result = new get_table_column_statistics_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<ColumnStatistics> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<ColumnStatistics>() {	public void onComplete(ColumnStatistics o) {	get_partition_column_statistics_result result = new get_partition_column_statistics_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<TableStatsResult> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<TableStatsResult>() {	public void onComplete(TableStatsResult o) {	get_table_statistics_req_result result = new get_table_statistics_req_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<PartitionsStatsResult> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<PartitionsStatsResult>() {	public void onComplete(PartitionsStatsResult o) {	get_partitions_statistics_req_result result = new get_partitions_statistics_req_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<AggrStats> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<AggrStats>() {	public void onComplete(AggrStats o) {	get_aggr_stats_for_result result = new get_aggr_stats_for_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Boolean>() {	public void onComplete(Boolean o) {	set_aggr_stats_for_result result = new set_aggr_stats_for_result();	result.success = o;	result.setSuccessIsSet(true);	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Boolean>() {	public void onComplete(Boolean o) {	delete_partition_column_statistics_result result = new delete_partition_column_statistics_result();	result.success = o;	result.setSuccessIsSet(true);	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Boolean>() {	public void onComplete(Boolean o) {	delete_table_column_statistics_result result = new delete_table_column_statistics_result();	result.success = o;	result.setSuccessIsSet(true);	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	create_function_result result = new create_function_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	drop_function_result result = new drop_function_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	alter_function_result result = new alter_function_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<String>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<String>>() {	public void onComplete(List<String> o) {	get_functions_result result = new get_functions_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Function> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Function>() {	public void onComplete(Function o) {	get_function_result result = new get_function_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<GetAllFunctionsResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<GetAllFunctionsResponse>() {	public void onComplete(GetAllFunctionsResponse o) {	get_all_functions_result result = new get_all_functions_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Boolean>() {	public void onComplete(Boolean o) {	create_role_result result = new create_role_result();	result.success = o;	result.setSuccessIsSet(true);	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Boolean>() {	public void onComplete(Boolean o) {	drop_role_result result = new drop_role_result();	result.success = o;	result.setSuccessIsSet(true);	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<String>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<String>>() {	public void onComplete(List<String> o) {	get_role_names_result result = new get_role_names_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Boolean>() {	public void onComplete(Boolean o) {	grant_role_result result = new grant_role_result();	result.success = o;	result.setSuccessIsSet(true);	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Boolean>() {	public void onComplete(Boolean o) {	revoke_role_result result = new revoke_role_result();	result.success = o;	result.setSuccessIsSet(true);	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<Role>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<Role>>() {	public void onComplete(List<Role> o) {	list_roles_result result = new list_roles_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<GrantRevokeRoleResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<GrantRevokeRoleResponse>() {	public void onComplete(GrantRevokeRoleResponse o) {	grant_revoke_role_result result = new grant_revoke_role_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<GetPrincipalsInRoleResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<GetPrincipalsInRoleResponse>() {	public void onComplete(GetPrincipalsInRoleResponse o) {	get_principals_in_role_result result = new get_principals_in_role_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<GetRoleGrantsForPrincipalResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<GetRoleGrantsForPrincipalResponse>() {	public void onComplete(GetRoleGrantsForPrincipalResponse o) {	get_role_grants_for_principal_result result = new get_role_grants_for_principal_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<PrincipalPrivilegeSet> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<PrincipalPrivilegeSet>() {	public void onComplete(PrincipalPrivilegeSet o) {	get_privilege_set_result result = new get_privilege_set_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<HiveObjectPrivilege>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<HiveObjectPrivilege>>() {	public void onComplete(List<HiveObjectPrivilege> o) {	list_privileges_result result = new list_privileges_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Boolean>() {	public void onComplete(Boolean o) {	grant_privileges_result result = new grant_privileges_result();	result.success = o;	result.setSuccessIsSet(true);	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Boolean>() {	public void onComplete(Boolean o) {	revoke_privileges_result result = new revoke_privileges_result();	result.success = o;	result.setSuccessIsSet(true);	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<GrantRevokePrivilegeResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<GrantRevokePrivilegeResponse>() {	public void onComplete(GrantRevokePrivilegeResponse o) {	grant_revoke_privileges_result result = new grant_revoke_privileges_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<String>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<String>>() {	public void onComplete(List<String> o) {	set_ugi_result result = new set_ugi_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<String> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<String>() {	public void onComplete(String o) {	get_delegation_token_result result = new get_delegation_token_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Long>() {	public void onComplete(Long o) {	renew_delegation_token_result result = new renew_delegation_token_result();	result.success = o;	result.setSuccessIsSet(true);	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	cancel_delegation_token_result result = new cancel_delegation_token_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Boolean>() {	public void onComplete(Boolean o) {	add_token_result result = new add_token_result();	result.success = o;	result.setSuccessIsSet(true);	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	add_token_result result = new add_token_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Boolean>() {	public void onComplete(Boolean o) {	remove_token_result result = new remove_token_result();	result.success = o;	result.setSuccessIsSet(true);	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	remove_token_result result = new remove_token_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<String> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<String>() {	public void onComplete(String o) {	get_token_result result = new get_token_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	get_token_result result = new get_token_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<String>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<String>>() {	public void onComplete(List<String> o) {	get_all_token_identifiers_result result = new get_all_token_identifiers_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	get_all_token_identifiers_result result = new get_all_token_identifiers_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Integer>() {	public void onComplete(Integer o) {	add_master_key_result result = new add_master_key_result();	result.success = o;	result.setSuccessIsSet(true);	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	update_master_key_result result = new update_master_key_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Boolean>() {	public void onComplete(Boolean o) {	remove_master_key_result result = new remove_master_key_result();	result.success = o;	result.setSuccessIsSet(true);	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	remove_master_key_result result = new remove_master_key_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<String>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<String>>() {	public void onComplete(List<String> o) {	get_master_keys_result result = new get_master_keys_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	get_master_keys_result result = new get_master_keys_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<GetOpenTxnsResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<GetOpenTxnsResponse>() {	public void onComplete(GetOpenTxnsResponse o) {	get_open_txns_result result = new get_open_txns_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	get_open_txns_result result = new get_open_txns_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<GetOpenTxnsInfoResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<GetOpenTxnsInfoResponse>() {	public void onComplete(GetOpenTxnsInfoResponse o) {	get_open_txns_info_result result = new get_open_txns_info_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	get_open_txns_info_result result = new get_open_txns_info_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<OpenTxnsResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<OpenTxnsResponse>() {	public void onComplete(OpenTxnsResponse o) {	open_txns_result result = new open_txns_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	open_txns_result result = new open_txns_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	abort_txn_result result = new abort_txn_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	abort_txns_result result = new abort_txns_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	commit_txn_result result = new commit_txn_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<LockResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<LockResponse>() {	public void onComplete(LockResponse o) {	lock_result result = new lock_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<LockResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<LockResponse>() {	public void onComplete(LockResponse o) {	check_lock_result result = new check_lock_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	unlock_result result = new unlock_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<ShowLocksResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<ShowLocksResponse>() {	public void onComplete(ShowLocksResponse o) {	show_locks_result result = new show_locks_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	show_locks_result result = new show_locks_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	heartbeat_result result = new heartbeat_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<HeartbeatTxnRangeResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<HeartbeatTxnRangeResponse>() {	public void onComplete(HeartbeatTxnRangeResponse o) {	heartbeat_txn_range_result result = new heartbeat_txn_range_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	heartbeat_txn_range_result result = new heartbeat_txn_range_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	compact_result result = new compact_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	compact_result result = new compact_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<CompactionResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<CompactionResponse>() {	public void onComplete(CompactionResponse o) {	compact2_result result = new compact2_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	compact2_result result = new compact2_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<ShowCompactResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<ShowCompactResponse>() {	public void onComplete(ShowCompactResponse o) {	show_compact_result result = new show_compact_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	show_compact_result result = new show_compact_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	add_dynamic_partitions_result result = new add_dynamic_partitions_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<List<BasicTxnInfo>> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<List<BasicTxnInfo>>() {	public void onComplete(List<BasicTxnInfo> o) {	get_last_completed_transaction_for_tables_result result = new get_last_completed_transaction_for_tables_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	get_last_completed_transaction_for_tables_result result = new get_last_completed_transaction_for_tables_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<BasicTxnInfo> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<BasicTxnInfo>() {	public void onComplete(BasicTxnInfo o) {	get_last_completed_transaction_for_table_result result = new get_last_completed_transaction_for_table_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	get_last_completed_transaction_for_table_result result = new get_last_completed_transaction_for_table_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<NotificationEventResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<NotificationEventResponse>() {	public void onComplete(NotificationEventResponse o) {	get_next_notification_result result = new get_next_notification_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	get_next_notification_result result = new get_next_notification_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<CurrentNotificationEventId> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<CurrentNotificationEventId>() {	public void onComplete(CurrentNotificationEventId o) {	get_current_notificationEventId_result result = new get_current_notificationEventId_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	get_current_notificationEventId_result result = new get_current_notificationEventId_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<NotificationEventsCountResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<NotificationEventsCountResponse>() {	public void onComplete(NotificationEventsCountResponse o) {	get_notification_events_count_result result = new get_notification_events_count_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	get_notification_events_count_result result = new get_notification_events_count_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<FireEventResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<FireEventResponse>() {	public void onComplete(FireEventResponse o) {	fire_listener_event_result result = new fire_listener_event_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	fire_listener_event_result result = new fire_listener_event_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<Void>() {	public void onComplete(Void o) {	flushCache_result result = new flushCache_result();	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	flushCache_result result = new flushCache_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<CmRecycleResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<CmRecycleResponse>() {	public void onComplete(CmRecycleResponse o) {	cm_recycle_result result = new cm_recycle_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<GetFileMetadataByExprResult> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<GetFileMetadataByExprResult>() {	public void onComplete(GetFileMetadataByExprResult o) {	get_file_metadata_by_expr_result result = new get_file_metadata_by_expr_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	get_file_metadata_by_expr_result result = new get_file_metadata_by_expr_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<GetFileMetadataResult> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<GetFileMetadataResult>() {	public void onComplete(GetFileMetadataResult o) {	get_file_metadata_result result = new get_file_metadata_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	get_file_metadata_result result = new get_file_metadata_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<PutFileMetadataResult> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<PutFileMetadataResult>() {	public void onComplete(PutFileMetadataResult o) {	put_file_metadata_result result = new put_file_metadata_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	put_file_metadata_result result = new put_file_metadata_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<ClearFileMetadataResult> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<ClearFileMetadataResult>() {	public void onComplete(ClearFileMetadataResult o) {	clear_file_metadata_result result = new clear_file_metadata_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	clear_file_metadata_result result = new clear_file_metadata_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<CacheFileMetadataResult> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<CacheFileMetadataResult>() {	public void onComplete(CacheFileMetadataResult o) {	cache_file_metadata_result result = new cache_file_metadata_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	cache_file_metadata_result result = new cache_file_metadata_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<String> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<String>() {	public void onComplete(String o) {	get_metastore_db_uuid_result result = new get_metastore_db_uuid_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<WMCreateResourcePlanResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<WMCreateResourcePlanResponse>() {	public void onComplete(WMCreateResourcePlanResponse o) {	create_resource_plan_result result = new create_resource_plan_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<WMGetResourcePlanResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<WMGetResourcePlanResponse>() {	public void onComplete(WMGetResourcePlanResponse o) {	get_resource_plan_result result = new get_resource_plan_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<WMGetActiveResourcePlanResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<WMGetActiveResourcePlanResponse>() {	public void onComplete(WMGetActiveResourcePlanResponse o) {	get_active_resource_plan_result result = new get_active_resource_plan_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<WMGetAllResourcePlanResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<WMGetAllResourcePlanResponse>() {	public void onComplete(WMGetAllResourcePlanResponse o) {	get_all_resource_plans_result result = new get_all_resource_plans_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<WMAlterResourcePlanResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<WMAlterResourcePlanResponse>() {	public void onComplete(WMAlterResourcePlanResponse o) {	alter_resource_plan_result result = new alter_resource_plan_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<WMValidateResourcePlanResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<WMValidateResourcePlanResponse>() {	public void onComplete(WMValidateResourcePlanResponse o) {	validate_resource_plan_result result = new validate_resource_plan_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<WMDropResourcePlanResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<WMDropResourcePlanResponse>() {	public void onComplete(WMDropResourcePlanResponse o) {	drop_resource_plan_result result = new drop_resource_plan_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<WMCreateTriggerResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<WMCreateTriggerResponse>() {	public void onComplete(WMCreateTriggerResponse o) {	create_wm_trigger_result result = new create_wm_trigger_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<WMAlterTriggerResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<WMAlterTriggerResponse>() {	public void onComplete(WMAlterTriggerResponse o) {	alter_wm_trigger_result result = new alter_wm_trigger_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<WMDropTriggerResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<WMDropTriggerResponse>() {	public void onComplete(WMDropTriggerResponse o) {	drop_wm_trigger_result result = new drop_wm_trigger_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<WMGetTriggersForResourePlanResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<WMGetTriggersForResourePlanResponse>() {	public void onComplete(WMGetTriggersForResourePlanResponse o) {	get_triggers_for_resourceplan_result result = new get_triggers_for_resourceplan_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<WMCreatePoolResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<WMCreatePoolResponse>() {	public void onComplete(WMCreatePoolResponse o) {	create_wm_pool_result result = new create_wm_pool_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<WMAlterPoolResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<WMAlterPoolResponse>() {	public void onComplete(WMAlterPoolResponse o) {	alter_wm_pool_result result = new alter_wm_pool_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<WMDropPoolResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<WMDropPoolResponse>() {	public void onComplete(WMDropPoolResponse o) {	drop_wm_pool_result result = new drop_wm_pool_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<WMCreateOrUpdateMappingResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<WMCreateOrUpdateMappingResponse>() {	public void onComplete(WMCreateOrUpdateMappingResponse o) {	create_or_update_wm_mapping_result result = new create_or_update_wm_mapping_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<WMDropMappingResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<WMDropMappingResponse>() {	public void onComplete(WMDropMappingResponse o) {	drop_wm_mapping_result result = new drop_wm_mapping_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<WMCreateOrDropTriggerToPoolMappingResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<WMCreateOrDropTriggerToPoolMappingResponse>() {	public void onComplete(WMCreateOrDropTriggerToPoolMappingResponse o) {	create_or_drop_wm_trigger_to_pool_mapping_result result = new create_or_drop_wm_trigger_to_pool_mapping_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

msg = result;	}	else {	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

========================= hive sample_1732 =========================

public static PrunedPartitionList prune(Table tab, ExprNodeDesc prunerExpr, HiveConf conf, String alias, Map<String, PrunedPartitionList> prunedPartitionsMap) throws SemanticException {	if (LOG.isTraceEnabled()) {	
started pruning partition 

ExprNodeDesc compactExpr = compactExpr(prunerExpr.clone());	String oldFilter = prunerExpr.getExprString();	if (compactExpr == null || isBooleanExpr(compactExpr)) {	if (isFalseExpr(compactExpr)) {	return new PrunedPartitionList(tab, key + compactExpr.getExprString(), new LinkedHashSet<Partition>(0), new ArrayList<String>(0), false);	}	return getAllPartsFromCacheOrServer(tab, key, true, prunedPartitionsMap);	}	String compactExprString = compactExpr.getExprString();	if (LOG.isDebugEnabled()) {	
filter w compacting filter w o compacting 

try {	boolean doEvalClientSide = hasUserFunctions(compactExpr);	List<Partition> partitions = new ArrayList<Partition>();	boolean hasUnknownPartitions = false;	PerfLogger perfLogger = SessionState.getPerfLogger();	if (!doEvalClientSide) {	perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.PARTITION_RETRIEVING);	try {	hasUnknownPartitions = Hive.get().getPartitionsByExpr( tab, compactExpr, conf, partitions);	} catch (IMetaStoreClient.IncompatibleMetastoreException ime) {	
metastore doesn t support getpartitionsbyexpr 

}	}	Boolean isNeeded = (Boolean)PartExprEvalUtils.evaluateExprOnPart(handle, convertedValues);	boolean isUnknown = (isNeeded == null);	if (!isUnknown && !isNeeded) {	partIter.remove();	continue;	}	if (isUnknown && values.contains(defaultPartitionName)) {	if (LOG.isDebugEnabled()) {	
skipping default bad partition 

continue;	}	if (isUnknown && values.contains(defaultPartitionName)) {	if (LOG.isDebugEnabled()) {	}	partIter.remove();	continue;	}	hasUnknownPartitions |= isUnknown;	if (LOG.isDebugEnabled()) {	
retained unknown partition 

========================= hive sample_3076 =========================

private boolean returnSessionInternal(SessionType session, boolean isAsync) {	SessionState sessionState = SessionState.get();	if (sessionState != null) {	sessionState.setTezSession(null);	}	if (!session.stopUsing()) return true;	boolean canPutBack = putSessionBack(session, true);	if (canPutBack) return true;	if (LOG.isDebugEnabled()) {	
closing an unneeded returned session 

}	if (!session.stopUsing()) return true;	boolean canPutBack = putSessionBack(session, true);	if (canPutBack) return true;	if (LOG.isDebugEnabled()) {	}	if (isAsync) return false;	try {	session.close(false);	} catch (Exception ex) {	
failed to close 

private void startInitialSession(SessionType session) throws Exception {	boolean isUsable = session.tryUse(true);	if (!isUsable) throw new IOException(session + " is not usable at pool startup");	session.getConf().set(TezConfiguration.TEZ_QUEUE_NAME, session.getQueueName());	configureAmRegistry(session);	session.open();	if (session.stopUsing()) {	if (!putSessionBack(session, false)) {	
couldn t add a session during initialization 

boolean isUsable = session.tryUse(true);	if (!isUsable) throw new IOException(session + " is not usable at pool startup");	session.getConf().set(TezConfiguration.TEZ_QUEUE_NAME, session.getQueueName());	configureAmRegistry(session);	session.open();	if (session.stopUsing()) {	if (!putSessionBack(session, false)) {	try {	session.close(false);	} catch (Exception ex) {	
failed to close an unneeded session 

public void onCreate(TezAmInstance si, int ephSeqVersion) {	String sessionId = si.getSessionId();	SessionType session = bySessionId.get(sessionId);	if (session != null) {	
am for v has registered updating with an endpoint at 

public void onCreate(TezAmInstance si, int ephSeqVersion) {	String sessionId = si.getSessionId();	SessionType session = bySessionId.get(sessionId);	if (session != null) {	session.updateFromRegistry(si, ephSeqVersion);	} else {	
am for an unknown has registered ignoring 

public void onRemove(TezAmInstance serviceInstance, int ephSeqVersion) {	String sessionId = serviceInstance.getSessionId();	SessionType session = bySessionId.get(sessionId);	if (session != null) {	
am for v has unregistered updating 

public void onRemove(TezAmInstance serviceInstance, int ephSeqVersion) {	String sessionId = serviceInstance.getSessionId();	SessionType session = bySessionId.get(sessionId);	if (session != null) {	session.updateFromRegistry(null, ephSeqVersion);	} else {	
am for an unknown has unregistered ignoring 

private ListenableFuture<?> resizeUpInternal(int delta) {	int oldVal;	do {	oldVal = deltaRemaining.get();	} while (!deltaRemaining.compareAndSet(oldVal, oldVal + delta));	int toStart = oldVal + delta;	if (toStart <= 0) return createDummyFuture();	
resizing the pool adding sessions 

========================= hive sample_3975 =========================

private static void setupAtsExecutor(HiveConf conf) {	synchronized(LOCK) {	if (executor == null) {	int queueCapacity = conf.getIntVar(HiveConf.ConfVars.ATSHOOKQUEUECAPACITY);	
creating ats executor queue with capacity 

public ATSHook() {	
created ats hook 

private void createTimelineDomain(String domainId, String readers, String writers) throws Exception {	TimelineDomain timelineDomain = new TimelineDomain();	timelineDomain.setId(domainId);	timelineDomain.setReaders(readers);	timelineDomain.setWriters(writers);	timelineClient.putDomain(timelineDomain);	
ats domain created 

domainId = DEFAULT_ATS_DOMAIN;	}	if (create) {	final String readers = domainReaders;	final String writers = domainWriters;	executor.submit(new Runnable() {	public void run() {	try {	createTimelineDomain(domainId, readers, writers);	} catch (Exception e) {	
failed to create ats domain 

ApplicationId llapId = determineLlapId(conf, plan);	fireAndForget( createPreHookEvent(queryId, query, explainPlan, queryStartTime, user, requestuser, numMrJobs, numTezJobs, opId, hookContext.getIpAddress(), hiveInstanceAddress, hiveInstanceType, hookContext.getSessionId(), logID, hookContext.getThreadId(), executionMode, tablesRead, tablesWritten, conf, llapId, domainId));	break;	case POST_EXEC_HOOK: fireAndForget(createPostHookEvent(queryId, currentTime, user, requestuser, true, opId, durations, domainId));	break;	case ON_FAILURE_HOOK: fireAndForget(createPostHookEvent(queryId, currentTime, user, requestuser , false, opId, durations, domainId));	break;	default: break;	}	} catch (Exception e) {	
failed to submit plan to ats for 

break;	case ON_FAILURE_HOOK: fireAndForget(createPostHookEvent(queryId, currentTime, user, requestuser , false, opId, durations, domainId));	break;	default: break;	}	} catch (Exception e) {	}	}	});	} catch (Exception e) {	
failed to submit to ats for 

public void run() {	try {	timelineClient.putEntities(entity);	} catch (Exception err) {	
failed to send event to ats 

private ApplicationId determineLlapId(final HiveConf conf, QueryPlan plan) throws IOException {	for (TezTask tezTask : Utilities.getTezTasks(plan.getRootTasks())) {	if (!tezTask.getWork().getLlapMode()) continue;	String hosts = HiveConf.getVar(conf, HiveConf.ConfVars.LLAP_DAEMON_SERVICE_HOSTS);	if (hosts != null && !hosts.isEmpty()) {	ApplicationId llapId = LlapRegistryService.getClient(conf).getApplicationId();	
the query will use llap instance 

private ApplicationId determineLlapId(final HiveConf conf, QueryPlan plan) throws IOException {	for (TezTask tezTask : Utilities.getTezTasks(plan.getRootTasks())) {	if (!tezTask.getWork().getLlapMode()) continue;	String hosts = HiveConf.getVar(conf, HiveConf.ConfVars.LLAP_DAEMON_SERVICE_HOSTS);	if (hosts != null && !hosts.isEmpty()) {	ApplicationId llapId = LlapRegistryService.getClient(conf).getApplicationId();	return llapId;	} else {	
cannot determine llap instance on client service hosts are not set 

========================= hive sample_3830 =========================

finder.prependPathComponent(SparkFiles.getRootDirectory());	}	File f = finder.getAbsolutePath(prog);	if (f != null) {	cmdArgs[0] = f.getAbsolutePath();	}	f = null;	}	String[] wrappedCmdArgs = addWrapper(cmdArgs);	if (LOG.isInfoEnabled()) {	
executing 

throw new HiveException(ErrorMsg.SCRIPT_INIT_ERROR.getErrorCodedMsg(), e);	}	}	if (scriptError != null) {	throw new HiveException(ErrorMsg.SCRIPT_GENERIC_ERROR.getErrorCodedMsg(), scriptError);	}	try {	Writable res = scriptInputSerializer.serialize(row, inputObjInspectors[tag]);	scriptOutWriter.write(res);	} catch (SerDeException e) {	
error in serializing the row 

if (isBrokenPipeException(e) && allowPartialConsumption()) {	try {	scriptPid.waitFor();	} catch (InterruptedException interruptedException) {	}	try {	if (outThread != null) {	outThread.join(0);	}	} catch (Exception e2) {	
exception in closing outthread 

scriptPid.waitFor();	} catch (InterruptedException interruptedException) {	}	try {	if (outThread != null) {	outThread.join(0);	}	} catch (Exception e2) {	}	setDone(true);	
got broken pipe during write ignoring exception and setting operator to done 

} catch (InterruptedException interruptedException) {	}	try {	if (outThread != null) {	outThread.join(0);	}	} catch (Exception e2) {	}	setDone(true);	} else {	
error in writing to script 

if (scriptError != null) {	throw new HiveException(ErrorMsg.SCRIPT_GENERIC_ERROR.getErrorCodedMsg(), scriptError);	}	try {	try {	if (scriptOutWriter != null) {	scriptOutWriter.close();	}	} catch (IOException e) {	if (isBrokenPipeException(e) && allowPartialConsumption()) {	
got broken pipe ignoring exception 

displayBrokenPipeInfo();	}	throw e;	}	}	int exitVal = 0;	if (scriptPid != null) {	exitVal = scriptPid.waitFor();	}	if (exitVal != 0) {	
script failed with code 

}	}	int exitVal = 0;	if (scriptPid != null) {	exitVal = scriptPid.waitFor();	}	if (exitVal != 0) {	new_abort = true;	}	} catch (IOException e) {	
got ioexception 

timer.schedule(new TimerTask() {	public void run() {	mythread.interrupt();	}	}, 1000);	int exitVal = 0;	if (scriptPid != null) {	scriptPid.waitFor();	}	timer.cancel();	
script exited with code 

public void run() {	mythread.interrupt();	}	}, 1000);	int exitVal = 0;	if (scriptPid != null) {	scriptPid.waitFor();	}	timer.cancel();	} catch (InterruptedException e) {	
script has not exited yet it will be killed 

}	timer.cancel();	} catch (InterruptedException e) {	}	}	try {	if (outThread != null) {	outThread.join(0);	}	} catch (Exception e) {	
exception in closing outthread 

if (outThread != null) {	outThread.join(0);	}	} catch (Exception e) {	}	try {	if (errThread != null) {	errThread.join(0);	}	} catch (Exception e) {	
exception in closing errthread 

if (errThread != null) {	errThread.join(0);	}	} catch (Exception e) {	}	try {	if (scriptPid != null) {	scriptPid.destroy();	}	} catch (Exception e) {	
exception in destroying scriptpid 

private void incrCounter(String line) {	String  trimmedLine = line.substring(counterPrefix.length()).trim();	String[] columns = trimmedLine.split(",");	if (columns.length == 3) {	try {	reporter.incrCounter(columns[0], columns[1], Long.parseLong(columns[2]));	} catch (NumberFormatException e) {	
cannot parse counter increment from line 

private void incrCounter(String line) {	String  trimmedLine = line.substring(counterPrefix.length()).trim();	String[] columns = trimmedLine.split(",");	if (columns.length == 3) {	try {	reporter.incrCounter(columns[0], columns[1], Long.parseLong(columns[2]));	} catch (NumberFormatException e) {	}	} else {	
cannot parse counter line 

String stringLine = line.toString();	int len = 0;	if (line instanceof Text) {	len = ((Text) line).getLength();	} else if (line instanceof BytesWritable) {	len = ((BytesWritable) line).getSize();	}	long now = System.currentTimeMillis();	if (now - lastReportTime > 60 * 1000 && reporter != null) {	if (LOG.isInfoEnabled()) {	
errorstreamprocessor calling reporter progress 

try {	Writable row = in.createRow();	while (true) {	long bytes = in.next(row);	if (bytes <= 0) {	break;	}	proc.processLine(row);	}	if (LOG.isInfoEnabled()) {	
streamthread done 

long bytes = in.next(row);	if (bytes <= 0) {	break;	}	proc.processLine(row);	}	if (LOG.isInfoEnabled()) {	}	} catch (Throwable th) {	scriptError = th;	
exception in streamthread run cause 

}	} catch (Throwable th) {	scriptError = th;	LOG.warn(StringUtils.stringifyException(th));	} finally {	try {	if (in != null) {	in.close();	}	} catch (Exception e) {	
error in closing 

in.close();	}	} catch (Exception e) {	LOG.warn(StringUtils.stringifyException(e));	}	try {	if (null != proc) {	proc.close();	}	}catch (Exception e) {	
error in closing 

========================= hive sample_4072 =========================

this.jobID = jobRef.getSparkJobStatus().getAppID();	rc = jobRef.monitorJob();	SparkJobStatus sparkJobStatus = jobRef.getSparkJobStatus();	getSparkJobInfo(sparkJobStatus, rc);	if (rc == 0) {	sparkStatistics = sparkJobStatus.getSparkStatistics();	if (LOG.isInfoEnabled() && sparkStatistics != null) {	LOG.info(String.format("=====Spark Job[%s] statistics=====", jobRef.getJobId()));	logSparkStatistic(sparkStatistics);	}	
execution completed successfully 

rc = jobRef.monitorJob();	SparkJobStatus sparkJobStatus = jobRef.getSparkJobStatus();	getSparkJobInfo(sparkJobStatus, rc);	if (rc == 0) {	sparkStatistics = sparkJobStatus.getSparkStatistics();	if (LOG.isInfoEnabled() && sparkStatistics != null) {	LOG.info(String.format("=====Spark Job[%s] statistics=====", jobRef.getJobId()));	logSparkStatistic(sparkStatistics);	}	} else if (rc == 2) {	
failed to submit spark job 

getSparkJobInfo(sparkJobStatus, rc);	if (rc == 0) {	sparkStatistics = sparkJobStatus.getSparkStatistics();	if (LOG.isInfoEnabled() && sparkStatistics != null) {	LOG.info(String.format("=====Spark Job[%s] statistics=====", jobRef.getJobId()));	logSparkStatistic(sparkStatistics);	}	} else if (rc == 2) {	killJob();	} else if (rc == 4) {	
the spark job or one stage of it has too many tasks cancelling spark job with application id 

if (startTime < submitTime) {	startTime = submitTime;	}	finishTime = perfLogger.getEndTime(PerfLogger.SPARK_RUN_JOB);	Utilities.clearWork(conf);	if (sparkSession != null && sparkSessionManager != null) {	rc = close(rc);	try {	sparkSessionManager.returnSession(sparkSession);	} catch (HiveException ex) {	
failed to return the session to sessionmanager 

if (!jobKilled) {	jobKilled = true;	needToKillJob = true;	}	}	}	if (needToKillJob) {	try {	jobRef.cancelJob();	} catch (Exception e) {	
failed to kill job 

HiveException he;	if (isOOMError(error)) {	he = new HiveException(error, ErrorMsg.SPARK_RUNTIME_OOM);	} else {	he = new HiveException(error, ErrorMsg.SPARK_JOB_RUNTIME_ERROR);	}	setException(he);	}	}	} catch (Exception e) {	
failed to get spark job information 

========================= hive sample_4588 =========================

getBaseOutputCommitter().cleanupJob(HCatMapRedUtil.createJobContext(context));	IMetaStoreClient client = null;	try {	HiveConf hiveConf = HCatUtil.getHiveConf(context.getConfiguration());	client = HCatUtil.getHiveMetastoreClient(hiveConf);	String tokenStrForm = client.getTokenStrForm();	if (tokenStrForm != null && context.getConfiguration().get(HCatConstants.HCAT_KEY_TOKEN_SIGNATURE) != null) {	client.cancelDelegationToken(tokenStrForm);	}	} catch (Exception e) {	
failed to cancel delegation token 

========================= hive sample_747 =========================

final int size = map.size();	current.fieldIndex = size;	LazyBinaryUtils.writeVInt(output, size);	int b = 0;	byte nullByte = 0;	for (Map.Entry<?, ?> entry : map.entrySet()) {	if (null != entry.getKey()) {	nullByte |= 1 << (b % 8);	} else if (warnedOnceNullMapKey != null) {	if (!warnedOnceNullMapKey.value) {	
null map key encountered ignoring similar problems 

========================= hive sample_5233 =========================

}	if (null != toReplID) {	dumpCmd = dumpCmd + " TO " + toReplID;	}	if (null != limit) {	dumpCmd = dumpCmd + " LIMIT " + limit;	}	run(dumpCmd, driver);	String dumpLocation = getResult(0, 0, driver);	String lastReplId = getResult(0, 1, true, driver);	
dumped to with id for command 

public void testBootstrapLoadOnExistingDb() throws IOException {	String testName = "bootstrapLoadOnExistingDb";	
testing 

return table;	}	}	};	InjectableBehaviourObjectStore.setGetTableBehaviour(ptnedTableNuller);	run("REPL DUMP " + dbName, driver);	ptnedTableNuller.assertInjectionsPerformed(true,true);	InjectableBehaviourObjectStore.resetGetTableBehaviour();	String replDumpLocn = getResult(0, 0, driver);	String replDumpId = getResult(0, 1, true, driver);	
bootstrap dump dumped to with id 

run("REPL LOAD " + dbName + "_dupe FROM '" + replDumpLocn + "'", driverMirror);	verifyRun("SELECT * from " + dbName + "_dupe.unptned", unptn_data, driverMirror);	verifyFail("SELECT a from " + dbName + "_dupe.ptned WHERE b=1", driverMirror);	verifyIfTableNotExist(dbName + "_dupe", "ptned", metaStoreClient);	run("DROP TABLE " + dbName + ".ptned", driver);	verifyIfTableNotExist(dbName, "ptned", metaStoreClient);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	String postDropReplDumpLocn = getResult(0,0, driver);	String postDropReplDumpId = getResult(0,1,true,driver);	
dumped to with id 

injectionPathCalled = true;	return new ArrayList<String>();	}	};	InjectableBehaviourObjectStore.setListPartitionNamesBehaviour(listPartitionNamesNuller);	run("REPL DUMP " + dbName, driver);	listPartitionNamesNuller.assertInjectionsPerformed(true, false);	InjectableBehaviourObjectStore.resetListPartitionNamesBehaviour();	String replDumpLocn = getResult(0, 0, driver);	String replDumpId = getResult(0, 1, true, driver);	
bootstrap dump dumped to with id 

run("ALTER TABLE " + dbName + ".ptned DROP PARTITION (b=1)", driver);	run("ALTER TABLE " + dbName + ".ptned DROP PARTITION (b=2)", driver);	verifyIfPartitionNotExist(dbName, "ptned", new ArrayList<>(Arrays.asList("1")), metaStoreClient);	verifyIfPartitionNotExist(dbName, "ptned", new ArrayList<>(Arrays.asList("2")), metaStoreClient);	verifySetup("SELECT a from " + dbName + ".ptned WHERE b=1", empty, driver);	verifySetup("SELECT a from " + dbName + ".ptned WHERE b=2", empty, driver);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	String postDropReplDumpLocn = getResult(0,0,driver);	String postDropReplDumpId = getResult(0,1,true,driver);	
dumped to with id 

BehaviourInjection<Table,Table> ptnedTableRenamer = new BehaviourInjection<Table,Table>(){	boolean success = false;	public Table apply(@Nullable Table table) {	if (injectionPathCalled) {	nonInjectedPathCalled = true;	} else {	injectionPathCalled = true;	Thread t = new Thread(new Runnable() {	public void run() {	try {	
entered new thread 

public void run() {	try {	IDriver driver2 = DriverFactory.newDriver(hconf);	SessionState.start(new CliSessionState(hconf));	CommandProcessorResponse ret = driver2.run("ALTER TABLE " + dbName + ".ptned PARTITION (b=1) RENAME TO PARTITION (b=10)");	success = (ret.getException() == null);	assertFalse(success);	ret = driver2.run("ALTER TABLE " + dbName + ".ptned RENAME TO " + dbName + ".ptned_renamed");	success = (ret.getException() == null);	assertFalse(success);	
exit new thread success 

try {	IDriver driver2 = DriverFactory.newDriver(hconf);	SessionState.start(new CliSessionState(hconf));	CommandProcessorResponse ret = driver2.run("ALTER TABLE " + dbName + ".ptned PARTITION (b=1) RENAME TO PARTITION (b=10)");	success = (ret.getException() == null);	assertFalse(success);	ret = driver2.run("ALTER TABLE " + dbName + ".ptned RENAME TO " + dbName + ".ptned_renamed");	success = (ret.getException() == null);	assertFalse(success);	} catch (CommandNeedRetryException e) {	
hit exception from new thread 

assertFalse(success);	ret = driver2.run("ALTER TABLE " + dbName + ".ptned RENAME TO " + dbName + ".ptned_renamed");	success = (ret.getException() == null);	assertFalse(success);	} catch (CommandNeedRetryException e) {	throw new RuntimeException(e);	}	}	});	t.start();	
created new thread 

BehaviourInjection<Table,Table> ptnedTableRenamer = new BehaviourInjection<Table,Table>(){	boolean success = false;	public Table apply(@Nullable Table table) {	if (injectionPathCalled) {	nonInjectedPathCalled = true;	} else {	injectionPathCalled = true;	Thread t = new Thread(new Runnable() {	public void run() {	try {	
entered new thread 

} else {	injectionPathCalled = true;	Thread t = new Thread(new Runnable() {	public void run() {	try {	IDriver driver2 = DriverFactory.newDriver(hconf);	SessionState.start(new CliSessionState(hconf));	CommandProcessorResponse ret = driver2.run("DROP TABLE " + dbName + ".ptned");	success = (ret.getException() == null);	assertTrue(success);	
exit new thread success 

injectionPathCalled = true;	Thread t = new Thread(new Runnable() {	public void run() {	try {	IDriver driver2 = DriverFactory.newDriver(hconf);	SessionState.start(new CliSessionState(hconf));	CommandProcessorResponse ret = driver2.run("DROP TABLE " + dbName + ".ptned");	success = (ret.getException() == null);	assertTrue(success);	} catch (CommandNeedRetryException e) {	
hit exception from new thread 

SessionState.start(new CliSessionState(hconf));	CommandProcessorResponse ret = driver2.run("DROP TABLE " + dbName + ".ptned");	success = (ret.getException() == null);	assertTrue(success);	} catch (CommandNeedRetryException e) {	throw new RuntimeException(e);	}	}	});	t.start();	
created new thread 

String name = testName.getMethodName();	String dbName = createDB(name, driver);	run("CREATE TABLE " + dbName + ".unptned(a string) STORED AS TEXTFILE", driver);	run("CREATE TABLE " + dbName + ".ptned(a string) partitioned by (b int) STORED AS TEXTFILE", driver);	run("CREATE TABLE " + dbName + ".unptned_empty(a string) STORED AS TEXTFILE", driver);	run("CREATE TABLE " + dbName + ".ptned_empty(a string) partitioned by (b int) STORED AS TEXTFILE", driver);	advanceDumpDir();	run("REPL DUMP " + dbName, driver);	String replDumpLocn = getResult(0,0,driver);	String replDumpId = getResult(0,1,true,driver);	
dumped to with id 

verifySetup("SELECT a from " + dbName + ".ptned WHERE b=2", ptn_data_2, driver);	run("CREATE TABLE " + dbName + ".ptned_late(a string) PARTITIONED BY (b int) STORED AS TEXTFILE", driver);	run("LOAD DATA LOCAL INPATH '" + ptn_locn_1 + "' OVERWRITE INTO TABLE " + dbName + ".ptned_late PARTITION(b=1)", driver);	verifySetup("SELECT a from " + dbName + ".ptned_late WHERE b=1",ptn_data_1, driver);	run("LOAD DATA LOCAL INPATH '" + ptn_locn_2 + "' OVERWRITE INTO TABLE " + dbName + ".ptned_late PARTITION(b=2)", driver);	verifySetup("SELECT a from " + dbName + ".ptned_late WHERE b=2", ptn_data_2, driver);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	String incrementalDumpLocn = getResult(0,0,driver);	String incrementalDumpId = getResult(0,1,true,driver);	
dumped to with id 

public void testIncrementalLoadWithVariableLengthEventId() throws IOException, TException {	String testName = "incrementalLoadWithVariableLengthEventId";	String dbName = createDB(testName, driver);	run("CREATE TABLE " + dbName + ".unptned(a string) STORED AS TEXTFILE", driver);	run("INSERT INTO TABLE " + dbName + ".unptned values('ten')", driver);	advanceDumpDir();	run("REPL DUMP " + dbName, driver);	String replDumpLocn = getResult(0, 0, driver);	String replDumpId = getResult(0, 1, true, driver);	
bootstrap dump dumped to with id 

for (int i = 0; i < eventIds.size(); i++) {	NotificationEvent event = eventIds.get(i);	if (event.getDbName().equalsIgnoreCase(dbName)) {	switch (event.getEventType()) {	case "CREATE_TABLE": {	nextEventId = (long) Math.pow(10.0, (double) String.valueOf(event.getEventId()).length()) * 2;	break;	}	case "INSERT": {	nextEventId = (long) Math.pow(10.0, (double) String.valueOf(nextEventId).length());	
changed eventid to 

nextEventId = (long) Math.pow(10.0, (double) String.valueOf(event.getEventId()).length()) * 2;	break;	}	case "INSERT": {	nextEventId = (long) Math.pow(10.0, (double) String.valueOf(nextEventId).length());	event.setEventId(nextEventId++);	break;	}	default: {	if (nextEventId > 0) {	
changed eventid to 

}	};	InjectableBehaviourObjectStore.setGetNextNotificationBehaviour(eventIdModifier);	String cmd = "REPL DUMP " + dbName + " FROM " + replDumpId + " TO " + String.valueOf(metaStoreClient.getCurrentNotificationEventId().getEventId()*100);	advanceDumpDir();	run(cmd, driver);	eventIdModifier.assertInjectionsPerformed(true,false);	InjectableBehaviourObjectStore.resetGetNextNotificationBehaviour();	String incrementalDumpLocn = getResult(0, 0, driver);	String incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

run("DROP TABLE " + dbName + ".ptned2", driver);	run("ALTER TABLE " + dbName + ".ptned3 DROP PARTITION (b=1)", driver);	verifySetup("SELECT a from " + dbName + ".ptned WHERE b='2'", empty, driver);	verifySetup("SELECT a from " + dbName + ".ptned", ptn_data_1, driver);	verifySetup("SELECT a from " + dbName + ".ptned3 WHERE b=1",empty, driver);	verifySetup("SELECT a from " + dbName + ".ptned3", ptn_data_2, driver);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	String postDropReplDumpLocn = getResult(0,0,driver);	String postDropReplDumpId = getResult(0,1,true,driver);	
dumped to with id 

run("ALTER TABLE " + dbName + ".ptned DROP PARTITION (b='2')", driver);	run("DROP TABLE " + dbName + ".ptned2", driver);	run("SELECT a from " + dbName + ".ptned WHERE b=2", driver);	verifyResults(empty, driver);	run("SELECT a from " + dbName + ".ptned", driver);	verifyResults(ptn_data_1, driver);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	String postDropReplDumpLocn = getResult(0,0,driver);	String postDropReplDumpId = getResult(0,1,true,driver);	
dumped to with id 

} catch (TException e) {	assertNull(e);	}	verifySetup("SELECT a from " + dbName + ".ptned2 WHERE b=2", ptn_data_2, driver);	run("ALTER TABLE " + dbName + ".ptned2 RENAME TO " + dbName + ".ptned2_rn", driver);	verifySetup("SELECT a from " + dbName + ".ptned2_rn WHERE b=2", ptn_data_2, driver);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	String postAlterReplDumpLocn = getResult(0,0,driver);	String postAlterReplDumpId = getResult(0,1,true,driver);	
dumped to with id 

String testName = "incrementalLoad";	String dbName = createDB(testName, driver);	run("CREATE TABLE " + dbName + ".unptned(a string) STORED AS TEXTFILE", driver);	run("CREATE TABLE " + dbName + ".ptned(a string) partitioned by (b int) STORED AS TEXTFILE", driver);	run("CREATE TABLE " + dbName + ".unptned_empty(a string) STORED AS TEXTFILE", driver);	run("CREATE TABLE " + dbName + ".ptned_empty(a string) partitioned by (b int) STORED AS TEXTFILE", driver);	advanceDumpDir();	run("REPL DUMP " + dbName, driver);	String replDumpLocn = getResult(0, 0,driver);	String replDumpId = getResult(0, 1, true, driver);	
bootstrap dump dumped to with id 

verifySetup("SELECT * from " + dbName + ".unptned_empty", empty, driverMirror);	run("LOAD DATA LOCAL INPATH '" + unptn_locn + "' OVERWRITE INTO TABLE " + dbName + ".unptned", driver);	verifySetup("SELECT * from " + dbName + ".unptned", unptn_data, driver);	run("CREATE TABLE " + dbName + ".unptned_late LIKE " + dbName + ".unptned", driver);	run("INSERT INTO TABLE " + dbName + ".unptned_late SELECT * FROM " + dbName + ".unptned", driver);	verifySetup("SELECT * from " + dbName + ".unptned_late", unptn_data, driver);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	String incrementalDumpLocn = getResult(0, 0, driver);	String incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

verifySetup("SELECT a from " + dbName + ".ptned WHERE b=2", ptn_data_2, driver);	run("CREATE TABLE " + dbName + ".ptned_late(a string) PARTITIONED BY (b int) STORED AS TEXTFILE", driver);	run("INSERT INTO TABLE " + dbName + ".ptned_late PARTITION(b=1) SELECT a FROM " + dbName + ".ptned WHERE b=1", driver);	verifySetup("SELECT a from " + dbName + ".ptned_late WHERE b=1", ptn_data_1, driver);	run("INSERT INTO TABLE " + dbName + ".ptned_late PARTITION(b=2) SELECT a FROM " + dbName + ".ptned WHERE b=2", driver);	verifySetup("SELECT a from " + dbName + ".ptned_late WHERE b=2", ptn_data_2, driver);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	incrementalDumpLocn = getResult(0, 0, driver);	incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

public void testIncrementalInserts() throws IOException {	String testName = "incrementalInserts";	String dbName = createDB(testName, driver);	run("CREATE TABLE " + dbName + ".unptned(a string) STORED AS TEXTFILE", driver);	advanceDumpDir();	run("REPL DUMP " + dbName, driver);	String replDumpLocn = getResult(0, 0, driver);	String replDumpId = getResult(0, 1, true, driver);	
bootstrap dump dumped to with id 

run("INSERT INTO TABLE " + dbName + ".unptned values('" + unptn_data[0] + "')", driver);	run("INSERT INTO TABLE " + dbName + ".unptned values('" + unptn_data[1] + "')", driver);	verifySetup("SELECT a from " + dbName + ".unptned ORDER BY a", unptn_data, driver);	run("CREATE TABLE " + dbName + ".unptned_late LIKE " + dbName + ".unptned", driver);	run("INSERT INTO TABLE " + dbName + ".unptned_late SELECT * FROM " + dbName + ".unptned", driver);	verifySetup("SELECT * from " + dbName + ".unptned_late ORDER BY a", unptn_data, driver);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	String incrementalDumpLocn = getResult(0, 0, driver);	String incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

String[] unptn_data_after_ins = new String[] { "eleven", "thirteen", "twelve" };	String[] data_after_ovwrite = new String[] { "hundred" };	run("INSERT INTO TABLE " + dbName + ".unptned_late values('" + unptn_data_after_ins[1] + "')", driver);	verifySetup("SELECT a from " + dbName + ".unptned_late ORDER BY a", unptn_data_after_ins, driver);	run("INSERT OVERWRITE TABLE " + dbName + ".unptned values('" + data_after_ovwrite[0] + "')", driver);	verifySetup("SELECT a from " + dbName + ".unptned", data_after_ovwrite, driver);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	incrementalDumpLocn = getResult(0, 0, driver);	incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

String[] ptn_data = new String[]{ "ten"};	run("INSERT INTO TABLE " + dbName + ".ptned partition(b=1) values('" + ptn_data[0] + "')", driver);	BehaviourInjection<NotificationEventResponse,NotificationEventResponse> eventTypeValidator = new BehaviourInjection<NotificationEventResponse,NotificationEventResponse>(){	public NotificationEventResponse apply(@Nullable NotificationEventResponse eventsList) {	if (null != eventsList) {	List<NotificationEvent> events = eventsList.getEvents();	for (int i = 0; i < events.size(); i++) {	NotificationEvent event = events.get(i);	if (event.getDbName().equalsIgnoreCase(dbName)) {	if (event.getEventType().equalsIgnoreCase("INSERT")) {	
encountered insert event when it was not expected to 

public void testIncrementalInsertToPartition() throws IOException {	String testName = "incrementalInsertToPartition";	
testing 

public void testIncrementalInsertToPartition() throws IOException {	String testName = "incrementalInsertToPartition";	String dbName = testName + "_" + tid;	run("CREATE DATABASE " + dbName, driver);	run("CREATE TABLE " + dbName + ".ptned(a string) partitioned by (b int) STORED AS TEXTFILE", driver);	advanceDumpDir();	run("REPL DUMP " + dbName, driver);	String replDumpLocn = getResult(0, 0, driver);	String replDumpId = getResult(0, 1, true, driver);	
bootstrap dump dumped to with id 

run("ALTER TABLE " + dbName + ".ptned ADD PARTITION (b=2)", driver);	run("INSERT INTO TABLE " + dbName + ".ptned partition(b=2) values('" + ptn_data_2[0] + "')", driver);	run("INSERT INTO TABLE " + dbName + ".ptned partition(b=2) values('" + ptn_data_2[1] + "')", driver);	run("INSERT INTO TABLE " + dbName + ".ptned partition(b=2) values('" + ptn_data_2[2] + "')", driver);	verifySetup("SELECT a from " + dbName + ".ptned where (b=1) ORDER BY a", ptn_data_1, driver);	verifySetup("SELECT a from " + dbName + ".ptned where (b=2) ORDER BY a", ptn_data_2, driver);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	String incrementalDumpLocn = getResult(0, 0, driver);	String incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

verifyRun("SELECT a from " + dbName + "_dupe.ptned where (b=2) ORDER BY a", ptn_data_2, driverMirror);	String[] data_after_ovwrite = new String[] { "hundred" };	run("INSERT OVERWRITE TABLE " + dbName + ".ptned partition(b=2) values('" + data_after_ovwrite[0] + "')", driver);	verifySetup("SELECT a from " + dbName + ".ptned where (b=2)", data_after_ovwrite, driver);	run("INSERT OVERWRITE TABLE " + dbName + ".ptned partition(b=3) values('" + data_after_ovwrite[0] + "')", driver);	verifySetup("SELECT a from " + dbName + ".ptned where (b=3)", data_after_ovwrite, driver);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	incrementalDumpLocn = getResult(0, 0, driver);	incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

public void testInsertToMultiKeyPartition() throws IOException {	String testName = "insertToMultiKeyPartition";	
testing 

verifySetup("SELECT name from " + dbName + ".namelist where (year=1980) ORDER BY name", ptn_year_1980, driver);	verifySetup("SELECT name from " + dbName + ".namelist where (day=1) ORDER BY name", ptn_day_1, driver);	verifySetup("SELECT name from " + dbName + ".namelist where (year=1984 and month=4 and day=1) ORDER BY name", ptn_year_1984_month_4_day_1_1, driver);	verifySetup("SELECT name from " + dbName + ".namelist ORDER BY name", ptn_data_1, driver);	verifySetup("SHOW PARTITIONS " + dbName + ".namelist", ptn_list_1, driver);	verifyRunWithPatternMatch("SHOW TABLE EXTENDED LIKE namelist PARTITION (year=1980,month=4,day=1)", "location", "namelist/year=1980/month=4/day=1", driver);	advanceDumpDir();	run("REPL DUMP " + dbName, driver);	String replDumpLocn = getResult(0, 0, driver);	String replDumpId = getResult(0, 1, true, driver);	
bootstrap dump dumped to with id 

verifySetup("SELECT name from " + dbName + ".namelist where (year=1980) ORDER BY name", ptn_year_1980, driver);	verifySetup("SELECT name from " + dbName + ".namelist where (day=1) ORDER BY name", ptn_day_1_2, driver);	verifySetup("SELECT name from " + dbName + ".namelist where (year=1984 and month=4 and day=1) ORDER BY name", ptn_year_1984_month_4_day_1_2, driver);	verifySetup("SELECT name from " + dbName + ".namelist ORDER BY name", ptn_data_2, driver);	verifyRun("SHOW PARTITIONS " + dbName + ".namelist", ptn_list_2, driver);	verifyRunWithPatternMatch("SHOW TABLE EXTENDED LIKE namelist PARTITION (year=1990,month=5,day=25)", "location", "namelist/year=1990/month=5/day=25", driver);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	String incrementalDumpLocn = getResult(0, 0, driver);	String incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

run("USE " + dbName, driverMirror);	String[] ptn_data_3 = new String[] { "abraham", "bob", "carter", "david", "fisher" };	String[] data_after_ovwrite = new String[] { "fisher" };	run("INSERT OVERWRITE TABLE " + dbName + ".namelist partition(year=1990,month=5,day=25) values('" + data_after_ovwrite[0] + "')", driver);	verifySetup("SELECT name from " + dbName + ".namelist where (year=1990 and month=5 and day=25)", data_after_ovwrite, driver);	verifySetup("SELECT name from " + dbName + ".namelist ORDER BY name", ptn_data_3, driver);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	incrementalDumpLocn = getResult(0, 0, driver);	incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

public void testIncrementalInsertDropUnpartitionedTable() throws IOException {	String testName = "incrementalInsertDropUnpartitionedTable";	String dbName = createDB(testName, driver);	run("CREATE TABLE " + dbName + ".unptned(a string) STORED AS TEXTFILE", driver);	advanceDumpDir();	run("REPL DUMP " + dbName, driver);	String replDumpLocn = getResult(0, 0, driver);	String replDumpId = getResult(0, 1, true, driver);	
bootstrap dump dumped to with id 

run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	String lastDumpIdWithoutDrop = getResult(0, 1, driver);	run("DROP TABLE " + dbName + ".unptned", driver);	run("DROP TABLE " + dbName + ".unptned_tmp", driver);	verifyFail("SELECT * FROM " + dbName + ".unptned", driver);	verifyFail("SELECT * FROM " + dbName + ".unptned_tmp", driver);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId + " TO " + lastDumpIdWithoutDrop, driver);	String incrementalDumpLocn = getResult(0, 0, driver);	String incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

String incrementalDumpLocn = getResult(0, 0, driver);	String incrementalDumpId = getResult(0, 1, true, driver);	replDumpId = incrementalDumpId;	run("REPL LOAD " + dbName + "_dupe FROM '" + incrementalDumpLocn + "'", driverMirror);	verifyRun("SELECT a from " + dbName + "_dupe.unptned ORDER BY a", unptn_data, driverMirror);	verifyRun("SELECT a from " + dbName + "_dupe.unptned_tmp ORDER BY a", unptn_data, driverMirror);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	incrementalDumpLocn = getResult(0, 0, driver);	incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

public void testIncrementalInsertDropPartitionedTable() throws IOException {	String testName = "incrementalInsertDropPartitionedTable";	String dbName = createDB(testName, driver);	run("CREATE TABLE " + dbName + ".ptned(a string) PARTITIONED BY (b int) STORED AS TEXTFILE", driver);	advanceDumpDir();	run("REPL DUMP " + dbName, driver);	String replDumpLocn = getResult(0, 0, driver);	String replDumpId = getResult(0, 1, true, driver);	
bootstrap dump dumped to with id 

run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	String lastDumpIdWithoutDrop = getResult(0, 1, driver);	run("DROP TABLE " + dbName + ".ptned_tmp", driver);	run("DROP TABLE " + dbName + ".ptned", driver);	verifyFail("SELECT * FROM " + dbName + ".ptned_tmp", driver);	verifyFail("SELECT * FROM " + dbName + ".ptned", driver);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId + " TO " + lastDumpIdWithoutDrop, driver);	String incrementalDumpLocn = getResult(0, 0, driver);	String incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

replDumpId = incrementalDumpId;	run("REPL LOAD " + dbName + "_dupe FROM '" + incrementalDumpLocn + "'", driverMirror);	verifyRun("SELECT a from " + dbName + "_dupe.ptned where (b=1) ORDER BY a", ptn_data_1, driverMirror);	verifyRun("SELECT a from " + dbName + "_dupe.ptned where (b=2) ORDER BY a", ptn_data_2, driverMirror);	verifyRun("SELECT a from " + dbName + "_dupe.ptned_tmp where (b=1) ORDER BY a", ptn_data_1, driverMirror);	verifyRun("SELECT a from " + dbName + "_dupe.ptned_tmp where (b=2) ORDER BY a", ptn_data_2, driverMirror);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	incrementalDumpLocn = getResult(0, 0, driver);	incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

public void testInsertOverwriteOnUnpartitionedTableWithCM() throws IOException {	String testName = "insertOverwriteOnUnpartitionedTableWithCM";	
testing 

public void testInsertOverwriteOnUnpartitionedTableWithCM() throws IOException {	String testName = "insertOverwriteOnUnpartitionedTableWithCM";	String dbName = testName + "_" + tid;	run("CREATE DATABASE " + dbName, driver);	run("CREATE TABLE " + dbName + ".unptned(a string) STORED AS TEXTFILE", driver);	advanceDumpDir();	run("REPL DUMP " + dbName, driver);	String replDumpLocn = getResult(0, 0, driver);	String replDumpId = getResult(0, 1, true, driver);	
bootstrap dump dumped to with id 

String[] unptn_data = new String[] { "thirteen" };	run("INSERT INTO TABLE " + dbName + ".unptned values('" + unptn_data[0] + "')", driver);	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	String insertDumpId = getResult(0, 1, false, driver);	String[] data_after_ovwrite = new String[] { "hundred" };	run("INSERT OVERWRITE TABLE " + dbName + ".unptned values('" + data_after_ovwrite[0] + "')", driver);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId + " TO " + insertDumpId, driver);	String incrementalDumpLocn = getResult(0, 0, driver);	String incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

run("REPL DUMP " + dbName + " FROM " + replDumpId + " TO " + insertDumpId, driver);	String incrementalDumpLocn = getResult(0, 0, driver);	String incrementalDumpId = getResult(0, 1, true, driver);	replDumpId = incrementalDumpId;	run("REPL LOAD " + dbName + "_dupe FROM '" + incrementalDumpLocn + "'", driverMirror);	verifyRun("SELECT a from " + dbName + "_dupe.unptned ORDER BY a", unptn_data, driverMirror);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	incrementalDumpLocn = getResult(0, 0, driver);	incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

public void testInsertOverwriteOnPartitionedTableWithCM() throws IOException {	String testName = "insertOverwriteOnPartitionedTableWithCM";	
testing 

public void testInsertOverwriteOnPartitionedTableWithCM() throws IOException {	String testName = "insertOverwriteOnPartitionedTableWithCM";	String dbName = testName + "_" + tid;	run("CREATE DATABASE " + dbName, driver);	run("CREATE TABLE " + dbName + ".ptned(a string) partitioned by (b int) STORED AS TEXTFILE", driver);	advanceDumpDir();	run("REPL DUMP " + dbName, driver);	String replDumpLocn = getResult(0, 0, driver);	String replDumpId = getResult(0, 1, true, driver);	
bootstrap dump dumped to with id 

run("INSERT INTO TABLE " + dbName + ".ptned partition(b=2) values('" + ptn_data_2[1] + "')", driver);	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	String insertDumpId = getResult(0, 1, false, driver);	String[] data_after_ovwrite = new String[] { "hundred" };	run("INSERT OVERWRITE TABLE " + dbName + ".ptned partition(b=2) values('" + data_after_ovwrite[0] + "')", driver);	verifySetup("SELECT a from " + dbName + ".ptned where (b=2)", data_after_ovwrite, driver);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId + " TO " + insertDumpId, driver);	String incrementalDumpLocn = getResult(0, 0, driver);	String incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

String incrementalDumpLocn = getResult(0, 0, driver);	String incrementalDumpId = getResult(0, 1, true, driver);	replDumpId = incrementalDumpId;	run("REPL LOAD " + dbName + "_dupe FROM '" + incrementalDumpLocn + "'", driverMirror);	verifyRun("SELECT a from " + dbName + "_dupe.ptned where (b=1) ORDER BY a", ptn_data_1, driverMirror);	verifyRun("SELECT a from " + dbName + "_dupe.ptned where (b=2) ORDER BY a", ptn_data_2, driverMirror);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	incrementalDumpLocn = getResult(0, 0, driver);	incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

public void testRenameTableWithCM() throws IOException {	String testName = "renameTableWithCM";	
testing 

public void testRenameTableWithCM() throws IOException {	String testName = "renameTableWithCM";	String dbName = testName + "_" + tid;	run("CREATE DATABASE " + dbName, driver);	run("CREATE TABLE " + dbName + ".unptned(a string) STORED AS TEXTFILE", driver);	run("CREATE TABLE " + dbName + ".ptned(a string) partitioned by (b int) STORED AS TEXTFILE", driver);	advanceDumpDir();	run("REPL DUMP " + dbName, driver);	String replDumpLocn = getResult(0, 0, driver);	String replDumpId = getResult(0, 1, true, driver);	
bootstrap dump dumped to with id 

run("INSERT INTO TABLE " + dbName + ".ptned partition(b=2) values('" + ptn_data_2[1] + "')", driver);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	String lastDumpIdWithoutRename = getResult(0, 1, driver);	run("ALTER TABLE " + dbName + ".unptned RENAME TO " + dbName + ".unptned_renamed", driver);	run("ALTER TABLE " + dbName + ".ptned RENAME TO " + dbName + ".ptned_renamed", driver);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId + " TO " + lastDumpIdWithoutRename, driver);	String incrementalDumpLocn = getResult(0, 0, driver);	String incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

String incrementalDumpId = getResult(0, 1, true, driver);	replDumpId = incrementalDumpId;	run("REPL LOAD " + dbName + "_dupe FROM '" + incrementalDumpLocn + "'", driverMirror);	verifyRun("SELECT a from " + dbName + "_dupe.unptned ORDER BY a", unptn_data, driverMirror);	verifyRun("SELECT a from " + dbName + "_dupe.ptned where (b=1) ORDER BY a", ptn_data_1, driverMirror);	verifyRun("SELECT a from " + dbName + "_dupe.ptned where (b=2) ORDER BY a", ptn_data_2, driverMirror);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	incrementalDumpLocn = getResult(0, 0, driver);	incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

public void testRenamePartitionWithCM() throws IOException {	String testName = "renamePartitionWithCM";	
testing 

public void testRenamePartitionWithCM() throws IOException {	String testName = "renamePartitionWithCM";	String dbName = testName + "_" + tid;	run("CREATE DATABASE " + dbName, driver);	run("CREATE TABLE " + dbName + ".ptned(a string) partitioned by (b int) STORED AS TEXTFILE", driver);	advanceDumpDir();	run("REPL DUMP " + dbName, driver);	String replDumpLocn = getResult(0, 0, driver);	String replDumpId = getResult(0, 1, true, driver);	
bootstrap dump dumped to with id 

run("INSERT INTO TABLE " + dbName + ".ptned partition(b=2) values('" + ptn_data_2[0] + "')", driver);	run("INSERT INTO TABLE " + dbName + ".ptned partition(b=2) values('" + ptn_data_2[1] + "')", driver);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	String lastDumpIdWithoutRename = getResult(0, 1, driver);	run("ALTER TABLE " + dbName + ".ptned PARTITION (b=2) RENAME TO PARTITION (b=10)", driver);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId + " TO " + lastDumpIdWithoutRename, driver);	String incrementalDumpLocn = getResult(0, 0, driver);	String incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

String incrementalDumpId = getResult(0, 1, true, driver);	replDumpId = incrementalDumpId;	run("REPL LOAD " + dbName + "_dupe FROM '" + incrementalDumpLocn + "'", driverMirror);	verifyRun("SELECT a from " + dbName + "_dupe.ptned where (b=1) ORDER BY a", ptn_data_1, driverMirror);	verifyRun("SELECT a from " + dbName + "_dupe.ptned where (b=2) ORDER BY a", ptn_data_2, driverMirror);	verifyRun("SELECT a from " + dbName + "_dupe.ptned where (b=10) ORDER BY a", empty, driverMirror);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	incrementalDumpLocn = getResult(0, 0, driver);	incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

public void testRenameTableAcrossDatabases() throws IOException {	String testName = "renameTableAcrossDatabases";	
testing 

public void testRenamePartitionedTableAcrossDatabases() throws IOException {	String testName = "renamePartitionedTableAcrossDatabases";	
testing 

run("LOAD DATA LOCAL INPATH '" + ptn_locn_1 + "' OVERWRITE INTO TABLE " + dbName + ".ptned PARTITION(b=1)", driver);	verifySetup("SELECT a from " + dbName + ".ptned WHERE b=1", ptn_data_1, driver);	run("LOAD DATA LOCAL INPATH '" + ptn_locn_2 + "' OVERWRITE INTO TABLE " + dbName + ".ptned PARTITION(b=2)", driver);	verifySetup("SELECT a from " + dbName + ".ptned WHERE b=2", ptn_data_2, driver);	run("CREATE MATERIALIZED VIEW " + dbName + ".mat_view AS SELECT a FROM " + dbName + ".ptned where b=1", driver);	verifySetup("SELECT a from " + dbName + ".mat_view", ptn_data_1, driver);	advanceDumpDir();	run("REPL DUMP " + dbName, driver);	String replDumpLocn = getResult(0,0,driver);	String replDumpId = getResult(0,1,true,driver);	
bootstrap dump dumped to with id 

verifyRun("SELECT a from " + dbName + "_dupe.mat_view", ptn_data_1, driverMirror);	run("CREATE VIEW " + dbName + ".virtual_view2 AS SELECT a FROM " + dbName + ".ptned where b=2", driver);	verifySetup("SELECT a from " + dbName + ".virtual_view2", ptn_data_2, driver);	run("CREATE VIEW " + dbName + ".virtual_view2 AS SELECT a FROM " + dbName + ".ptned where b=2", driver);	run("CREATE MATERIALIZED VIEW " + dbName + ".mat_view2 AS SELECT * FROM " + dbName + ".unptned", driver);	verifySetup("SELECT * from " + dbName + ".mat_view2", unptn_data, driver);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	String incrementalDumpLocn = getResult(0,0,driver);	String incrementalDumpId = getResult(0,1,true,driver);	
incremental dump dumped to with id 

verifyRun("SELECT * from " + dbName + "_dupe.virtual_view", empty, driverMirror);	verifyRun("SELECT a from " + dbName + "_dupe.mat_view", ptn_data_1, driverMirror);	verifyRun("SELECT * from " + dbName + "_dupe.virtual_view2", empty, driverMirror);	verifyRun("SELECT * from " + dbName + "_dupe.mat_view2", unptn_data, driverMirror);	run("ALTER VIEW " + dbName + ".virtual_view RENAME TO " + dbName + ".virtual_view_rename", driver);	verifySetup("SELECT * from " + dbName + ".virtual_view_rename", unptn_data, driver);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + incrementalDumpId, driver);	incrementalDumpLocn = getResult(0, 0, driver);	incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id 

incrementalDumpLocn = getResult(0, 0, driver);	incrementalDumpId = getResult(0, 1, true, driver);	run("REPL LOAD " + dbName + "_dupe FROM '" + incrementalDumpLocn + "'", driverMirror);	verifyRun("SELECT * from " + dbName + "_dupe.virtual_view_rename", empty, driverMirror);	run("ALTER VIEW " + dbName + ".virtual_view_rename AS SELECT a, concat(a, '_') as a_ FROM " + dbName + ".unptned", driver);	verifySetup("SHOW COLUMNS FROM " + dbName + ".virtual_view_rename", new String[] {"a", "a_"}, driver);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + incrementalDumpId, driver);	incrementalDumpLocn = getResult(0, 0, driver);	incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id 

incrementalDumpLocn = getResult(0, 0, driver);	incrementalDumpId = getResult(0, 1, true, driver);	run("REPL LOAD " + dbName + "_dupe FROM '" + incrementalDumpLocn + "'", driverMirror);	verifyRun("SHOW COLUMNS FROM " + dbName + "_dupe.virtual_view_rename", new String[] {"a", "a_"}, driverMirror);	run("DROP VIEW " + dbName + ".virtual_view", driver);	verifyIfTableNotExist(dbName, "virtual_view", metaStoreClient);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + incrementalDumpId, driver);	incrementalDumpLocn = getResult(0, 0, driver);	incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id 

public void testDumpLimit() throws IOException {	String name = testName.getMethodName();	String dbName = createDB(name, driver);	run("CREATE TABLE " + dbName + ".unptned(a string) STORED AS TEXTFILE", driver);	advanceDumpDir();	run("REPL DUMP " + dbName, driver);	String replDumpLocn = getResult(0, 0, driver);	String replDumpId = getResult(0, 1, true, driver);	
bootstrap dump dumped to with id 

run("INSERT INTO TABLE " + dbName + ".unptned values('" + unptn_data[1] + "')", driver);	String secondInsertLastReplId = replDumpDb(dbName, firstInsertLastReplId, null, null).lastReplId;	Integer numOfEventsIns2 = Integer.valueOf(secondInsertLastReplId) - Integer.valueOf(firstInsertLastReplId);	run("INSERT INTO TABLE " + dbName + ".unptned values('" + unptn_data[2] + "')", driver);	verifyRun("SELECT a from " + dbName + ".unptned ORDER BY a", unptn_data, driver);	run("REPL LOAD " + dbName + "_dupe FROM '" + replDumpLocn + "'", driverMirror);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId + " LIMIT " + numOfEventsIns1, driver);	String incrementalDumpLocn = getResult(0, 0, driver);	String incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

run("REPL LOAD " + dbName + "_dupe FROM '" + incrementalDumpLocn + "'", driverMirror);	verifyRun("SELECT a from " + dbName + ".unptned ORDER BY a", unptn_data, driver);	verifyRun("SELECT a from " + dbName + "_dupe.unptned ORDER BY a", unptn_data_load1, driverMirror);	advanceDumpDir();	Integer lastReplID = Integer.valueOf(replDumpId);	lastReplID += 1000;	String toReplID = String.valueOf(lastReplID);	run("REPL DUMP " + dbName + " FROM " + replDumpId + " TO " + toReplID + " LIMIT " + numOfEventsIns2, driver);	incrementalDumpLocn = getResult(0, 0, driver);	incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

run("REPL DUMP " + dbName + " FROM " + replDumpId + " TO " + toReplID + " LIMIT " + numOfEventsIns2, driver);	incrementalDumpLocn = getResult(0, 0, driver);	incrementalDumpId = getResult(0, 1, true, driver);	replDumpId = incrementalDumpId;	run("REPL LOAD " + dbName + "_dupe FROM '" + incrementalDumpLocn + "'", driverMirror);	verifyRun("SELECT a from " + dbName + "_dupe.unptned ORDER BY a", unptn_data_load2, driverMirror);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	incrementalDumpLocn = getResult(0, 0, driver);	incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

public void testExchangePartition() throws IOException {	String testName = "exchangePartition";	
testing 

run("INSERT INTO TABLE " + dbName + ".ptned_src partition(b=2, c=3) values('" + ptn_data_2[0] + "')", driver);	run("INSERT INTO TABLE " + dbName + ".ptned_src partition(b=2, c=3) values('" + ptn_data_2[1] + "')", driver);	run("INSERT INTO TABLE " + dbName + ".ptned_src partition(b=2, c=3) values('" + ptn_data_2[2] + "')", driver);	verifySetup("SELECT a from " + dbName + ".ptned_src where (b=1 and c=1) ORDER BY a", ptn_data_1, driver);	verifySetup("SELECT a from " + dbName + ".ptned_src where (b=2 and c=2) ORDER BY a", ptn_data_2, driver);	verifySetup("SELECT a from " + dbName + ".ptned_src where (b=2 and c=3) ORDER BY a", ptn_data_2, driver);	advanceDumpDir();	run("REPL DUMP " + dbName, driver);	String replDumpLocn = getResult(0, 0, driver);	String replDumpId = getResult(0, 1, true, driver);	
bootstrap dump dumped to with id 

verifySetup("SELECT a from " + dbName + ".ptned_src where (b=1 and c=1)", empty, driver);	verifySetup("SELECT a from " + dbName + ".ptned_src where (b=2 and c=2) ORDER BY a", ptn_data_2, driver);	verifySetup("SELECT a from " + dbName + ".ptned_src where (b=2 and c=3) ORDER BY a", ptn_data_2, driver);	verifySetup("SELECT a from " + dbName + ".ptned_dest where (b=1 and c=1) ORDER BY a", ptn_data_1, driver);	verifySetup("SELECT a from " + dbName + ".ptned_dest where (b=2 and c=2)", empty, driver);	verifySetup("SELECT a from " + dbName + ".ptned_dest where (b=2 and c=3)", empty, driver);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	String incrementalDumpLocn = getResult(0, 0, driver);	String incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

verifySetup("SELECT a from " + dbName + ".ptned_src where (b=1 and c=1)", empty, driver);	verifySetup("SELECT a from " + dbName + ".ptned_src where (b=2 and c=2)", empty, driver);	verifySetup("SELECT a from " + dbName + ".ptned_src where (b=2 and c=3)", empty, driver);	verifySetup("SELECT a from " + dbName + ".ptned_dest where (b=1 and c=1) ORDER BY a", ptn_data_1, driver);	verifySetup("SELECT a from " + dbName + ".ptned_dest where (b=2 and c=2) ORDER BY a", ptn_data_2, driver);	verifySetup("SELECT a from " + dbName + ".ptned_dest where (b=2 and c=3) ORDER BY a", ptn_data_2, driver);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	incrementalDumpLocn = getResult(0, 0, driver);	incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

public void testTruncateTable() throws IOException {	String testName = "truncateTable";	
testing 

public void testTruncateTable() throws IOException {	String testName = "truncateTable";	String dbName = testName + "_" + tid;	run("CREATE DATABASE " + dbName, driver);	run("CREATE TABLE " + dbName + ".unptned(a string) STORED AS TEXTFILE", driver);	advanceDumpDir();	run("REPL DUMP " + dbName, driver);	String replDumpLocn = getResult(0, 0, driver);	String replDumpId = getResult(0, 1, true, driver);	
bootstrap dump dumped to with id 

run("REPL LOAD " + dbName + "_dupe FROM '" + replDumpLocn + "'", driverMirror);	String[] unptn_data = new String[] { "eleven", "twelve" };	String[] empty = new String[] {};	run("INSERT INTO TABLE " + dbName + ".unptned values('" + unptn_data[0] + "')", driver);	run("INSERT INTO TABLE " + dbName + ".unptned values('" + unptn_data[1] + "')", driver);	verifyRun("SELECT a from " + dbName + ".unptned ORDER BY a", unptn_data, driver);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	String incrementalDumpLocn = getResult(0, 0, driver);	String incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

replDumpId = incrementalDumpId;	run("REPL LOAD " + dbName + "_dupe FROM '" + incrementalDumpLocn + "'", driverMirror);	verifyRun("SELECT a from " + dbName + ".unptned ORDER BY a", unptn_data, driver);	verifyRun("SELECT a from " + dbName + "_dupe.unptned ORDER BY a", unptn_data, driverMirror);	run("TRUNCATE TABLE " + dbName + ".unptned", driver);	verifySetup("SELECT a from " + dbName + ".unptned", empty, driver);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	incrementalDumpLocn = getResult(0, 0, driver);	incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

run("REPL LOAD " + dbName + "_dupe FROM '" + incrementalDumpLocn + "'", driverMirror);	verifyRun("SELECT a from " + dbName + ".unptned", empty, driverMirror);	verifyRun("SELECT a from " + dbName + "_dupe.unptned", empty, driverMirror);	String[] unptn_data_after_ins = new String[] { "thirteen" };	run("INSERT INTO TABLE " + dbName + ".unptned values('" + unptn_data_after_ins[0] + "')", driver);	verifySetup("SELECT a from " + dbName + ".unptned ORDER BY a", unptn_data_after_ins, driver);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	incrementalDumpLocn = getResult(0, 0, driver);	incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

public void testTruncatePartitionedTable() throws IOException {	String testName = "truncatePartitionedTable";	
testing 

run("INSERT INTO TABLE " + dbName + ".ptned_2 PARTITION(b=20) values('" + ptn_data_2[1] + "')", driver);	run("INSERT INTO TABLE " + dbName + ".ptned_2 PARTITION(b=20) values('" + ptn_data_2[2] + "')", driver);	verifyRun("SELECT a from " + dbName + ".ptned_1 where (b=1) ORDER BY a", ptn_data_1, driver);	verifyRun("SELECT a from " + dbName + ".ptned_1 where (b=2) ORDER BY a", ptn_data_2, driver);	verifyRun("SELECT a from " + dbName + ".ptned_2 where (b=10) ORDER BY a", ptn_data_1, driver);	verifyRun("SELECT a from " + dbName + ".ptned_2 where (b=20) ORDER BY a", ptn_data_2, driver);	advanceDumpDir();	run("REPL DUMP " + dbName, driver);	String replDumpLocn = getResult(0, 0, driver);	String replDumpId = getResult(0, 1, true, driver);	
bootstrap dump dumped to with id 

run("TRUNCATE TABLE " + dbName + ".ptned_1 PARTITION(b=2)", driver);	verifySetup("SELECT a from " + dbName + ".ptned_1 where (b=1) ORDER BY a", ptn_data_1, driver);	verifySetup("SELECT a from " + dbName + ".ptned_1 where (b=2)", empty, driver);	run("TRUNCATE TABLE " + dbName + ".ptned_2", driver);	verifySetup("SELECT a from " + dbName + ".ptned_2 where (b=10)", empty, driver);	verifySetup("SELECT a from " + dbName + ".ptned_2 where (b=20)", empty, driver);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	String incrementalDumpLocn = getResult(0, 0, driver);	String incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

public void testTruncateWithCM() throws IOException {	String testName = "truncateWithCM";	
testing 

public void testTruncateWithCM() throws IOException {	String testName = "truncateWithCM";	String dbName = testName + "_" + tid;	run("CREATE DATABASE " + dbName, driver);	run("CREATE TABLE " + dbName + ".unptned(a string) STORED AS TEXTFILE", driver);	advanceDumpDir();	run("REPL DUMP " + dbName, driver);	String replDumpLocn = getResult(0, 0, driver);	String replDumpId = getResult(0, 1, true, driver);	
bootstrap dump dumped to with id 

verifyRun("SELECT a from " + dbName + ".unptned ORDER BY a", empty, driver);	String thirdTruncLastReplId = replDumpDb(dbName, secondInsertLastReplId, null, null).lastReplId;	Integer numOfEventsTrunc3 = Integer.valueOf(thirdTruncLastReplId) - Integer.valueOf(secondInsertLastReplId);	run("INSERT INTO TABLE " + dbName + ".unptned values('" + unptn_data_load1[0] + "')", driver);	verifyRun("SELECT a from " + dbName + ".unptned ORDER BY a", unptn_data_load1, driver);	run("REPL LOAD " + dbName + "_dupe FROM '" + replDumpLocn + "'", driverMirror);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId + " LIMIT " + numOfEventsIns1, driver);	String incrementalDumpLocn = getResult(0, 0, driver);	String incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

run("REPL LOAD " + dbName + "_dupe FROM '" + incrementalDumpLocn + "'", driverMirror);	verifyRun("SELECT a from " + dbName + ".unptned ORDER BY a", unptn_data_load1, driver);	verifyRun("SELECT a from " + dbName + "_dupe.unptned ORDER BY a", unptn_data_load1, driverMirror);	advanceDumpDir();	Integer lastReplID = Integer.valueOf(replDumpId);	lastReplID += 1000;	String toReplID = String.valueOf(lastReplID);	run("REPL DUMP " + dbName + " FROM " + replDumpId + " TO " + toReplID + " LIMIT " + numOfEventsIns2, driver);	incrementalDumpLocn = getResult(0, 0, driver);	incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

run("REPL DUMP " + dbName + " FROM " + replDumpId + " TO " + toReplID + " LIMIT " + numOfEventsIns2, driver);	incrementalDumpLocn = getResult(0, 0, driver);	incrementalDumpId = getResult(0, 1, true, driver);	replDumpId = incrementalDumpId;	run("REPL LOAD " + dbName + "_dupe FROM '" + incrementalDumpLocn + "'", driverMirror);	verifyRun("SELECT a from " + dbName + "_dupe.unptned ORDER BY a", unptn_data_load2, driverMirror);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId + " LIMIT " + numOfEventsTrunc3, driver);	incrementalDumpLocn = getResult(0, 0, driver);	incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

run("REPL DUMP " + dbName + " FROM " + replDumpId + " LIMIT " + numOfEventsTrunc3, driver);	incrementalDumpLocn = getResult(0, 0, driver);	incrementalDumpId = getResult(0, 1, true, driver);	replDumpId = incrementalDumpId;	run("REPL LOAD " + dbName + "_dupe FROM '" + incrementalDumpLocn + "'", driverMirror);	verifyRun("SELECT a from " + dbName + "_dupe.unptned ORDER BY a", empty, driverMirror);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	incrementalDumpLocn = getResult(0, 0, driver);	incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id from 

public void testConstraints() throws IOException {	String testName = "constraints";	
testing 

String testName = "constraints";	String dbName = testName + "_" + tid;	run("CREATE DATABASE " + dbName, driver);	run("CREATE TABLE " + dbName + ".tbl1(a string, b string, primary key (a, b) disable novalidate rely)", driver);	run("CREATE TABLE " + dbName + ".tbl2(a string, b string, foreign key (a, b) references " + dbName + ".tbl1(a, b) disable novalidate)", driver);	run("CREATE TABLE " + dbName + ".tbl3(a string, b string not null disable, unique (a) disable)", driver);	advanceDumpDir();	run("REPL DUMP " + dbName, driver);	String replDumpLocn = getResult(0, 0, driver);	String replDumpId = getResult(0, 1, true, driver);	
dumped to with id 

} catch (TException te) {	assertNull(te);	}	run("CREATE TABLE " + dbName + ".tbl4(a string, b string, primary key (a, b) disable novalidate rely)", driver);	run("CREATE TABLE " + dbName + ".tbl5(a string, b string, foreign key (a, b) references " + dbName + ".tbl4(a, b) disable novalidate)", driver);	run("CREATE TABLE " + dbName + ".tbl6(a string, b string not null disable, unique (a) disable)", driver);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	String incrementalDumpLocn = getResult(0, 0, driver);	String incrementalDumpId = getResult(0, 1, true, driver);	
dumped to with id 

assertNull(te);	}	run("ALTER TABLE " + dbName + ".tbl4 DROP CONSTRAINT `" + pkName + "`", driver);	run("ALTER TABLE " + dbName + ".tbl4 DROP CONSTRAINT `" + ukName + "`", driver);	run("ALTER TABLE " + dbName + ".tbl5 DROP CONSTRAINT `" + fkName + "`", driver);	run("ALTER TABLE " + dbName + ".tbl6 DROP CONSTRAINT `" + nnName + "`", driver);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + incrementalDumpId, driver);	incrementalDumpLocn = getResult(0, 0, driver);	incrementalDumpId = getResult(0, 1, true, driver);	
dumped to with id 

verifySetup("SELECT * from " + dbName + ".unptned", unptn_data, driver);	verifySetup("SELECT a from " + dbName + ".ptned WHERE b=1", ptn_data_1, driver);	verifySetup("SELECT count(*) from " + dbName + ".unptned", new String[]{"2"}, driver);	verifySetup("SELECT count(*) from " + dbName + ".ptned", new String[]{"3"}, driver);	verifySetup("SELECT max(a) from " + dbName + ".unptned", new String[]{"2"}, driver);	verifySetup("SELECT max(a) from " + dbName + ".ptned where b=1", new String[]{"8"}, driver);	advanceDumpDir();	run("REPL DUMP " + dbName, driver);	String replDumpLocn = getResult(0,0,driver);	String replDumpId = getResult(0,1,true,driver);	
dumped to with id 

run("CREATE TABLE " + dbName + ".ptned2(a int) partitioned by (b int) STORED AS TEXTFILE", driver);	run("LOAD DATA LOCAL INPATH '" + ptn_locn_1 + "' OVERWRITE INTO TABLE " + dbName + ".ptned2 PARTITION(b=1)", driver);	run("ANALYZE TABLE " + dbName + ".unptned2 COMPUTE STATISTICS FOR COLUMNS", driver);	run("ANALYZE TABLE " + dbName + ".unptned2 COMPUTE STATISTICS", driver);	run("ANALYZE TABLE " + dbName + ".ptned2 partition(b) COMPUTE STATISTICS FOR COLUMNS", driver);	run("ANALYZE TABLE " + dbName + ".ptned2 partition(b) COMPUTE STATISTICS", driver);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	String incrementalDumpLocn = getResult(0,0,driver);	String incrementalDumpId = getResult(0,1,true,driver);	
dumped to with id 

public void testSkipTables() throws IOException {	String testName = "skipTables";	String dbName = createDB(testName, driver);	run("CREATE TABLE " + dbName + ".acid_table (key int, value int) PARTITIONED BY (load_date date) " + "CLUSTERED BY(key) INTO 2 BUCKETS STORED AS ORC TBLPROPERTIES ('transactional'='true')", driver);	verifyIfTableExist(dbName, "acid_table", metaStoreClient);	advanceDumpDir();	run("REPL DUMP " + dbName, driver);	String replDumpLocn = getResult(0, 0,driver);	String replDumpId = getResult(0, 1, true, driver);	
bootstrap dump dumped to with id 

String replDumpLocn = getResult(0, 0,driver);	String replDumpId = getResult(0, 1, true, driver);	run("REPL LOAD " + dbName + "_dupe FROM '" + replDumpLocn + "'", driverMirror);	verifyIfTableNotExist(dbName + "_dupe", "acid_table", metaStoreClientMirror);	run("ALTER TABLE " + dbName + ".acid_table RENAME TO " + dbName + ".acid_table_rename", driver);	verifyIfTableExist(dbName, "acid_table_rename", metaStoreClient);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);	String incrementalDumpLocn = getResult(0, 0, driver);	String incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id 

String incrementalDumpLocn = getResult(0, 0, driver);	String incrementalDumpId = getResult(0, 1, true, driver);	run("REPL LOAD " + dbName + "_dupe FROM '"+incrementalDumpLocn+"'", driverMirror);	verifyIfTableNotExist(dbName + "_dupe", "acid_table_rename", metaStoreClientMirror);	run("CREATE TABLE " + dbName + ".acid_table_incremental (key int, value int) PARTITIONED BY (load_date date) " + "CLUSTERED BY(key) INTO 2 BUCKETS STORED AS ORC TBLPROPERTIES ('transactional'='true')", driver);	verifyIfTableExist(dbName, "acid_table_incremental", metaStoreClient);	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + incrementalDumpId, driver);	incrementalDumpLocn = getResult(0, 0, driver);	incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id 

try {	List<SQLPrimaryKey> pks = metaStoreClient.getPrimaryKeys(new PrimaryKeysRequest(dbName, "acid_table_incremental"));	assertEquals(pks.size(), 1);	} catch (TException te) {	assertNull(te);	}	advanceDumpDir();	run("REPL DUMP " + dbName + " FROM " + incrementalDumpId, driver);	incrementalDumpLocn = getResult(0, 0, driver);	incrementalDumpId = getResult(0, 1, true, driver);	
incremental dump dumped to with id 

return;	}	PathFilter filter = new PathFilter() {	public boolean accept(Path path) {	return path.getName().startsWith(HiveConf.getVar(hconf, HiveConf.ConfVars.STAGINGDIR));	}	};	FileStatus[] statuses = fs.listStatus(path, filter);	assertEquals(0, statuses.length);	} catch (IOException e) {	
failed to list files in 

String testName = "cmConflict";	String dbName = createDB(testName, driver);	run("CREATE TABLE " + dbName + ".unptned(a string) STORED AS TEXTFILE", driver);	run("INSERT INTO TABLE " + dbName + ".unptned values('ten')", driver);	run("INSERT INTO TABLE " + dbName + ".unptned values('ten')", driver);	advanceDumpDir();	run("REPL DUMP " + dbName, driver);	String replDumpLocn = getResult(0, 0,driver);	String replDumpId = getResult(0, 1, true, driver);	run("TRUNCATE TABLE " + dbName + ".unptned", driver);	
bootstrap dump dumped to with id 

private static String createDB(String name, IDriver myDriver) {	
testing 

private void verifyResults(String[] data, IDriver myDriver) throws IOException {	List<String> results = getOutput(myDriver);	
expecting 

private void verifyResults(String[] data, IDriver myDriver) throws IOException {	List<String> results = getOutput(myDriver);	
got 

private void verifyFail(String cmd, IDriver myDriver) throws RuntimeException {	boolean success = false;	try {	success = run(cmd, false, myDriver);	} catch (AssertionError ae){	
assertionerror 

private static void run(String cmd, IDriver myDriver) throws RuntimeException {	try {	run(cmd,false, myDriver);	} catch (AssertionError ae){	
assertionerror 

private static boolean run(String cmd, boolean errorOnFail, IDriver myDriver) throws RuntimeException {	boolean success = false;	try {	CommandProcessorResponse ret = myDriver.run(cmd);	success = ((ret.getException() == null) && (ret.getErrorMessage() == null));	if (!success){	
error running 

========================= hive sample_252 =========================

public String getAppID() {	Future<String> getAppID = sparkClient.run(new GetAppIDJob());	try {	return getAppID.get(sparkClientTimeoutInSeconds, TimeUnit.SECONDS);	} catch (Exception e) {	
failed to get app id 

private SparkJobInfo getSparkJobInfo() throws HiveException {	Integer sparkJobId = jobHandle.getSparkJobIds().size() == 1 ? jobHandle.getSparkJobIds().get(0) : null;	if (sparkJobId == null) {	return null;	}	Future<SparkJobInfo> getJobInfo = sparkClient.run( new GetJobInfoJob(jobHandle.getClientJobId(), sparkJobId));	try {	return getJobInfo.get(sparkClientTimeoutInSeconds, TimeUnit.SECONDS);	} catch (Exception e) {	
failed to get job info 

private SparkStageInfo getSparkStageInfo(int stageId) {	Future<SparkStageInfo> getStageInfo = sparkClient.run(new GetStageInfoJob(stageId));	try {	return getStageInfo.get(sparkClientTimeoutInSeconds, TimeUnit.SECONDS);	} catch (Throwable t) {	
error getting stage info 

SparkJobInfo jobInfo = jc.sc().statusTracker().getJobInfo(sparkJobId);	if (jobInfo == null) {	List<JavaFutureAction<?>> list = jc.getMonitoredJobs().get(clientJobId);	if (list != null && list.size() == 1) {	JavaFutureAction<?> futureAction = list.get(0);	if (futureAction.isDone()) {	boolean futureSucceed = true;	try {	futureAction.get();	} catch (Exception e) {	
failed to run job 

========================= hive sample_4567 =========================

public static Builder create(String dbName, String tableName, String location, Map<String, String> partSpec ) throws HCatException {	
unsupported hcataddpartitiondesc requires hcattable to be specified explicitly 

========================= hive sample_897 =========================

if (foundNextKeyGroup[i] == false) {	canEmit = false;	break;	}	if (compareKeys(i, key, keyWritables[i]) != 0) {	canEmit = false;	break;	}	}	if (canEmit) {	
we are emitting rows since we hit the join emit interval of 

public void closeOp(boolean abort) throws HiveException {	super.closeOp(abort);	
cleaning up the operator state 

private void fetchOneRow(byte tag) throws HiveException {	try {	boolean hasMore = sources[tag].pushRecord();	if (fetchDone[tag] && hasMore) {	
fetchdone was set to true by a recursive call and will be reset 

========================= hive sample_3867 =========================

try {	if (hiveConf.getBoolVar(ConfVars.HIVE_SERVER2_METRICS_ENABLED)) {	MetricsFactory.init(hiveConf);	}	tezSessionPoolManager = TezSessionPoolManager.getInstance();	tezSessionPoolManager.initTriggers(hiveConf);	if (hiveConf.getBoolVar(ConfVars.HIVE_SERVER2_TEZ_INITIALIZE_DEFAULT_SESSIONS)) {	tezSessionPoolManager.setupPool(hiveConf);	}	} catch (Throwable t) {	
could not initiate the metrics system metrics may not be reported 

sessionHive = Hive.get(hiveConf);	} catch (HiveException e) {	throw new RuntimeException("Failed to get metastore connection", e);	}	initializeWorkloadManagement(hiveConf, sessionHive);	HiveMaterializedViewsRegistry.get().init();	try {	int webUIPort = hiveConf.getIntVar(ConfVars.HIVE_SERVER2_WEBUI_PORT);	boolean uiDisabledInTest = hiveConf.getBoolVar(ConfVars.HIVE_IN_TEST) && (webUIPort == Integer.valueOf(ConfVars.HIVE_SERVER2_WEBUI_PORT.getDefaultValue()));	if (uiDisabledInTest) {	
web ui is disabled in test mode since webui port was not specified 

throw new RuntimeException("Failed to get metastore connection", e);	}	initializeWorkloadManagement(hiveConf, sessionHive);	HiveMaterializedViewsRegistry.get().init();	try {	int webUIPort = hiveConf.getIntVar(ConfVars.HIVE_SERVER2_WEBUI_PORT);	boolean uiDisabledInTest = hiveConf.getBoolVar(ConfVars.HIVE_IN_TEST) && (webUIPort == Integer.valueOf(ConfVars.HIVE_SERVER2_WEBUI_PORT.getDefaultValue()));	if (uiDisabledInTest) {	} else {	if (webUIPort <= 0) {	
web ui is disabled since port is set to 

}	initializeWorkloadManagement(hiveConf, sessionHive);	HiveMaterializedViewsRegistry.get().init();	try {	int webUIPort = hiveConf.getIntVar(ConfVars.HIVE_SERVER2_WEBUI_PORT);	boolean uiDisabledInTest = hiveConf.getBoolVar(ConfVars.HIVE_IN_TEST) && (webUIPort == Integer.valueOf(ConfVars.HIVE_SERVER2_WEBUI_PORT.getDefaultValue()));	if (uiDisabledInTest) {	} else {	if (webUIPort <= 0) {	} else {	
starting web ui on port 

private void initializeWorkloadManagement(HiveConf hiveConf, Hive sessionHive) {	String wmQueue = HiveConf.getVar(hiveConf, ConfVars.HIVE_SERVER2_TEZ_INTERACTIVE_QUEUE);	boolean hasQueue = wmQueue != null && !wmQueue.isEmpty();	WMFullResourcePlan resourcePlan;	try {	resourcePlan = sessionHive.getActiveResourcePlan();	} catch (HiveException e) {	throw new RuntimeException(e);	}	if (hasQueue && resourcePlan == null && HiveConf.getBoolVar(hiveConf, ConfVars.HIVE_IN_TEST)) {	
creating a default resource plan for test 

try {	resourcePlan = sessionHive.getActiveResourcePlan();	} catch (HiveException e) {	throw new RuntimeException(e);	}	if (hasQueue && resourcePlan == null && HiveConf.getBoolVar(hiveConf, ConfVars.HIVE_IN_TEST)) {	resourcePlan = createTestResourcePlan();	}	if (resourcePlan == null) {	if (!hasQueue) {	
workload management is not enabled and there s no resource plan 

} catch (HiveException e) {	throw new RuntimeException(e);	}	if (hasQueue && resourcePlan == null && HiveConf.getBoolVar(hiveConf, ConfVars.HIVE_IN_TEST)) {	resourcePlan = createTestResourcePlan();	}	if (resourcePlan == null) {	if (!hasQueue) {	return;	}	
workload management is enabled but there s no resource plan 

throw new RuntimeException(e);	}	if (hasQueue && resourcePlan == null && HiveConf.getBoolVar(hiveConf, ConfVars.HIVE_IN_TEST)) {	resourcePlan = createTestResourcePlan();	}	if (resourcePlan == null) {	if (!hasQueue) {	return;	}	}	
initializing workload management 

return;	}	}	try {	wm = WorkloadManager.create(wmQueue, hiveConf, resourcePlan);	} catch (ExecutionException | InterruptedException e) {	throw new ServiceException("Unable to instantiate Workload Manager", e);	}	if (resourcePlan != null) {	tezSessionPoolManager.updateTriggers(resourcePlan);	
updated tez session pool manager with active resource plan 

String rootNamespace = hiveConf.getVar(HiveConf.ConfVars.HIVE_SERVER2_ZOOKEEPER_NAMESPACE);	String instanceURI = getServerInstanceURI();	setUpZooKeeperAuth(hiveConf);	int sessionTimeout = (int) hiveConf.getTimeVar(HiveConf.ConfVars.HIVE_ZOOKEEPER_SESSION_TIMEOUT, TimeUnit.MILLISECONDS);	int baseSleepTime = (int) hiveConf.getTimeVar(HiveConf.ConfVars.HIVE_ZOOKEEPER_CONNECTION_BASESLEEPTIME, TimeUnit.MILLISECONDS);	int maxRetries = hiveConf.getIntVar(HiveConf.ConfVars.HIVE_ZOOKEEPER_CONNECTION_MAX_RETRIES);	zooKeeperClient = CuratorFrameworkFactory.builder().connectString(zooKeeperEnsemble) .sessionTimeoutMs(sessionTimeout).aclProvider(zooKeeperAclProvider) .retryPolicy(new ExponentialBackoffRetry(baseSleepTime, maxRetries)).build();	zooKeeperClient.start();	try {	zooKeeperClient.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT) .forPath(ZooKeeperHiveHelper.ZOOKEEPER_PATH_SEPARATOR + rootNamespace);	
created the root name space on zookeeper for 

setUpZooKeeperAuth(hiveConf);	int sessionTimeout = (int) hiveConf.getTimeVar(HiveConf.ConfVars.HIVE_ZOOKEEPER_SESSION_TIMEOUT, TimeUnit.MILLISECONDS);	int baseSleepTime = (int) hiveConf.getTimeVar(HiveConf.ConfVars.HIVE_ZOOKEEPER_CONNECTION_BASESLEEPTIME, TimeUnit.MILLISECONDS);	int maxRetries = hiveConf.getIntVar(HiveConf.ConfVars.HIVE_ZOOKEEPER_CONNECTION_MAX_RETRIES);	zooKeeperClient = CuratorFrameworkFactory.builder().connectString(zooKeeperEnsemble) .sessionTimeoutMs(sessionTimeout).aclProvider(zooKeeperAclProvider) .retryPolicy(new ExponentialBackoffRetry(baseSleepTime, maxRetries)).build();	zooKeeperClient.start();	try {	zooKeeperClient.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT) .forPath(ZooKeeperHiveHelper.ZOOKEEPER_PATH_SEPARATOR + rootNamespace);	} catch (KeeperException e) {	if (e.code() != KeeperException.Code.NODEEXISTS) {	
unable to create namespace on zookeeper 

znode.start();	long znodeCreationTimeout = 120;	if (!znode.waitForInitialCreate(znodeCreationTimeout, TimeUnit.SECONDS)) {	throw new Exception("Max znode creation wait time: " + znodeCreationTimeout + "s exhausted");	}	setDeregisteredWithZooKeeper(false);	znodePath = znode.getActualPath();	if (zooKeeperClient.checkExists().usingWatcher(new DeRegisterWatcher()).forPath(znodePath) == null) {	throw new Exception("Unable to create znode for this HiveServer2 instance on ZooKeeper.");	}	
created a znode on zookeeper for uri 

long znodeCreationTimeout = 120;	if (!znode.waitForInitialCreate(znodeCreationTimeout, TimeUnit.SECONDS)) {	throw new Exception("Max znode creation wait time: " + znodeCreationTimeout + "s exhausted");	}	setDeregisteredWithZooKeeper(false);	znodePath = znode.getActualPath();	if (zooKeeperClient.checkExists().usingWatcher(new DeRegisterWatcher()).forPath(znodePath) == null) {	throw new Exception("Unable to create znode for this HiveServer2 instance on ZooKeeper.");	}	} catch (Exception e) {	
unable to create a znode for this server instance 

public void process(WatchedEvent event) {	if (event.getType().equals(Watcher.Event.EventType.NodeDeleted)) {	if (znode != null) {	try {	znode.close();	
this instance is now de registered from zookeeper the server will be shut down after the last client session completes 

public void process(WatchedEvent event) {	if (event.getType().equals(Watcher.Event.EventType.NodeDeleted)) {	if (znode != null) {	try {	znode.close();	} catch (IOException e) {	
failed to close the persistent ephemeral znode 

public void process(WatchedEvent event) {	if (event.getType().equals(Watcher.Event.EventType.NodeDeleted)) {	if (znode != null) {	try {	znode.close();	} catch (IOException e) {	} finally {	HiveServer2.this.setDeregisteredWithZooKeeper(true);	if (cliService.getSessionManager().getOpenSessionCount() == 0) {	
this instance of has been removed from the list of server instances available for dynamic service discovery the last client session has ended will shutdown now 

private void removeServerInstanceFromZooKeeper() throws Exception {	setDeregisteredWithZooKeeper(true);	if (znode != null) {	znode.close();	}	zooKeeperClient.close();	
server instance removed from zookeeper 

public synchronized void start() {	super.start();	HiveConf hiveConf = this.getHiveConf();	if (hiveConf.getBoolVar(ConfVars.HIVE_SERVER2_SUPPORT_DYNAMIC_SERVICE_DISCOVERY)) {	try {	addServerInstanceToZooKeeper(hiveConf);	} catch (Exception e) {	
error adding this instance to zookeeper 

if (hiveConf.getBoolVar(ConfVars.HIVE_SERVER2_SUPPORT_DYNAMIC_SERVICE_DISCOVERY)) {	try {	addServerInstanceToZooKeeper(hiveConf);	} catch (Exception e) {	throw new ServiceException(e);	}	}	if (webServer != null) {	try {	webServer.start();	
web ui has started on port 

try {	addServerInstanceToZooKeeper(hiveConf);	} catch (Exception e) {	throw new ServiceException(e);	}	}	if (webServer != null) {	try {	webServer.start();	} catch (Exception e) {	
error starting web ui 

if (webServer != null) {	try {	webServer.start();	} catch (Exception e) {	throw new ServiceException(e);	}	}	if (tezSessionPoolManager != null) {	try {	tezSessionPoolManager.startPool();	
started tez session pool manager 

try {	webServer.start();	} catch (Exception e) {	throw new ServiceException(e);	}	}	if (tezSessionPoolManager != null) {	try {	tezSessionPoolManager.startPool();	} catch (Exception e) {	
error starting tez session pool manager 

if (tezSessionPoolManager != null) {	try {	tezSessionPoolManager.startPool();	} catch (Exception e) {	throw new ServiceException(e);	}	}	if (wm != null) {	try {	wm.start();	
started workload manager 

try {	tezSessionPoolManager.startPool();	} catch (Exception e) {	throw new ServiceException(e);	}	}	if (wm != null) {	try {	wm.start();	} catch (Exception e) {	
error starting workload manager 

public synchronized void stop() {	
shutting down 

public synchronized void stop() {	HiveConf hiveConf = this.getHiveConf();	super.stop();	if (webServer != null) {	try {	webServer.stop();	
web ui has stopped 

public synchronized void stop() {	HiveConf hiveConf = this.getHiveConf();	super.stop();	if (webServer != null) {	try {	webServer.stop();	} catch (Exception e) {	
error stopping web ui 

if (webServer != null) {	try {	webServer.stop();	} catch (Exception e) {	}	}	if (MetricsFactory.getInstance() != null) {	try {	MetricsFactory.close();	} catch (Exception e) {	
error in metrics deinit 

if (MetricsFactory.getInstance() != null) {	try {	MetricsFactory.close();	} catch (Exception e) {	}	}	if (hiveConf != null && hiveConf.getBoolVar(ConfVars.HIVE_SERVER2_SUPPORT_DYNAMIC_SERVICE_DISCOVERY)) {	try {	removeServerInstanceFromZooKeeper();	} catch (Exception e) {	
error removing znode for this instance from zookeeper 

if (hiveConf != null && hiveConf.getBoolVar(ConfVars.HIVE_SERVER2_SUPPORT_DYNAMIC_SERVICE_DISCOVERY)) {	try {	removeServerInstanceFromZooKeeper();	} catch (Exception e) {	}	}	if (hiveConf != null && hiveConf.getBoolVar(ConfVars.HIVE_SERVER2_TEZ_INITIALIZE_DEFAULT_SESSIONS) && tezSessionPoolManager != null) {	try {	tezSessionPoolManager.stop();	} catch (Exception e) {	
tez session pool manager stop had an error during stop of shutting down anyway 

if (hiveConf != null && hiveConf.getBoolVar(ConfVars.HIVE_SERVER2_TEZ_INITIALIZE_DEFAULT_SESSIONS) && tezSessionPoolManager != null) {	try {	tezSessionPoolManager.stop();	} catch (Exception e) {	}	}	if (wm != null) {	try {	wm.stop();	} catch (Exception e) {	
workload manager stop had an error during stop of shutting down anyway 

if (wm != null) {	try {	wm.stop();	} catch (Exception e) {	}	}	if (hiveConf != null && hiveConf.getVar(ConfVars.HIVE_EXECUTION_ENGINE).equals("spark")) {	try {	SparkSessionManagerImpl.getInstance().shutdown();	} catch(Exception ex) {	
spark session pool manager failed to stop during shutdown 

private static void startHiveServer2() throws Throwable {	long attempts = 0, maxAttempts = 1;	while (true) {	
starting 

try {	ServerUtils.cleanUpScratchDir(hiveConf);	scheduleClearDanglingScratchDir(hiveConf, new Random().nextInt(600));	server = new HiveServer2();	server.init(hiveConf);	server.start();	try {	JvmPauseMonitor pauseMonitor = new JvmPauseMonitor(hiveConf);	pauseMonitor.start();	} catch (Throwable t) {	
could not initiate the jvmpausemonitor thread gcs and pauses may not be warned upon 

}	if (hiveConf.getVar(ConfVars.HIVE_EXECUTION_ENGINE).equals("spark")) {	SparkSessionManagerImpl.getInstance().setup(hiveConf);	}	break;	} catch (Throwable throwable) {	if (server != null) {	try {	server.stop();	} catch (Throwable t) {	
exception caught when calling stop of before retrying start 

try {	server.stop();	} catch (Throwable t) {	} finally {	server = null;	}	}	if (++attempts >= maxAttempts) {	throw new Error("Max start attempts " + maxAttempts + " exhausted", throwable);	} else {	
error starting on attempt will retry in ms 

int maxRetries = hiveConf.getIntVar(HiveConf.ConfVars.HIVE_ZOOKEEPER_CONNECTION_MAX_RETRIES);	CuratorFramework zooKeeperClient = CuratorFrameworkFactory.builder().connectString(zooKeeperEnsemble) .retryPolicy(new ExponentialBackoffRetry(baseSleepTime, maxRetries)).build();	zooKeeperClient.start();	List<String> znodePaths = zooKeeperClient.getChildren().forPath( ZooKeeperHiveHelper.ZOOKEEPER_PATH_SEPARATOR + rootNamespace);	List<String> znodePathsUpdated;	for (int i = 0; i < znodePaths.size(); i++) {	String znodePath = znodePaths.get(i);	deleteSignal = new CountDownLatch(1);	if (znodePath.contains("version=" + versionNumber + ";")) {	String fullZnodePath = ZooKeeperHiveHelper.ZOOKEEPER_PATH_SEPARATOR + rootNamespace + ZooKeeperHiveHelper.ZOOKEEPER_PATH_SEPARATOR + znodePath;	
will attempt to remove the znode from zookeeper 

HiveConf.setLoadHiveServer2Config(true);	try {	ServerOptionsProcessor oproc = new ServerOptionsProcessor("hiveserver2");	ServerOptionsProcessorResponse oprocResponse = oproc.parse(args);	String initLog4jMessage = LogUtils.initHiveLog4j();	LOG.debug(initLog4jMessage);	HiveStringUtils.startupShutdownMessage(HiveServer2.class, args, LOG);	LOG.debug(oproc.getDebugMessage().toString());	oprocResponse.getServerOptionsExecutor().execute();	} catch (LogInitializationException e) {	
error initializing log 

public void execute() {	try {	startHiveServer2();	} catch (Throwable t) {	
error starting 

public void execute() {	try {	deleteServerInstancesFromZooKeeper(versionNumber);	} catch (Exception e) {	
error deregistering instances for version from zookeeper 

========================= hive sample_2325 =========================

protected int execute(DriverContext driverContext) {	
replcopytask execute 

} else {	return 0;	}	}	for (FileStatus oneSrc : srcs) {	console.printInfo("Copying file: " + oneSrc.getPath().toString());	LOG.debug("ReplCopyTask :cp:{}=>{}", oneSrc.getPath(), toPath);	srcFiles.add(new ReplChangeManager.FileInfo(oneSrc.getPath().getFileSystem(conf), oneSrc.getPath()));	}	}	
replcopytask numfiles 

private List<ReplChangeManager.FileInfo> filesInFileListing(FileSystem fs, Path dataPath) throws IOException {	Path fileListing = new Path(dataPath, EximUtil.FILES_NAME);	
replcopytask filesinfilelisting reading 

private List<ReplChangeManager.FileInfo> filesInFileListing(FileSystem fs, Path dataPath) throws IOException {	Path fileListing = new Path(dataPath, EximUtil.FILES_NAME);	if (! fs.exists(fileListing)){	
replcopytask files does not exist 

private List<ReplChangeManager.FileInfo> filesInFileListing(FileSystem fs, Path dataPath) throws IOException {	Path fileListing = new Path(dataPath, EximUtil.FILES_NAME);	if (! fs.exists(fileListing)){	return null;	}	List<ReplChangeManager.FileInfo> filePaths = new ArrayList<>();	try (BufferedReader br = new BufferedReader(new InputStreamReader(fs.open(fileListing)))) {	String line = null;	while ((line = br.readLine()) != null) {	
replcopytask filesreadline 

}	List<ReplChangeManager.FileInfo> filePaths = new ArrayList<>();	try (BufferedReader br = new BufferedReader(new InputStreamReader(fs.open(fileListing)))) {	String line = null;	while ((line = br.readLine()) != null) {	String[] fileWithChksum = ReplChangeManager.getFileWithChksumFromURI(line);	try {	ReplChangeManager.FileInfo f = ReplChangeManager .getFileInfo(new Path(fileWithChksum[0]), fileWithChksum[1], conf);	filePaths.add(f);	} catch (MetaException e) {	
cannot find in source repo or cmroot 

public static Task<?> getLoadCopyTask(ReplicationSpec replicationSpec, Path srcPath, Path dstPath, HiveConf conf) {	Task<?> copyTask = null;	LOG.debug("ReplCopyTask:getLoadCopyTask: {}=>{}", srcPath, dstPath);	if ((replicationSpec != null) && replicationSpec.isInReplicationScope()){	ReplCopyWork rcwork = new ReplCopyWork(srcPath, dstPath, false);	
replcopytask trcwork 

public static Task<?> getLoadCopyTask(ReplicationSpec replicationSpec, Path srcPath, Path dstPath, HiveConf conf) {	Task<?> copyTask = null;	LOG.debug("ReplCopyTask:getLoadCopyTask: {}=>{}", srcPath, dstPath);	if ((replicationSpec != null) && replicationSpec.isInReplicationScope()){	ReplCopyWork rcwork = new ReplCopyWork(srcPath, dstPath, false);	if (replicationSpec.isLazy()) {	
replcopytask tlazy 

LOG.debug("ReplCopyTask:getLoadCopyTask: {}=>{}", srcPath, dstPath);	if ((replicationSpec != null) && replicationSpec.isInReplicationScope()){	ReplCopyWork rcwork = new ReplCopyWork(srcPath, dstPath, false);	if (replicationSpec.isLazy()) {	rcwork.setReadSrcAsFilesList(true);	String distCpDoAsUser = conf.getVar(HiveConf.ConfVars.HIVE_DISTCP_DOAS_USER);	rcwork.setDistCpDoAsUser(distCpDoAsUser);	}	copyTask = TaskFactory.get(rcwork, conf, true);	} else {	
replcopytask tcwork 

========================= hive sample_4079 =========================

private void processFiles(MapWork work, JobConf jobConf) throws HiveException {	ObjectInputStream in = null;	try {	Path baseDir = work.getTmpPathForPartitionPruning();	FileSystem fs = FileSystem.get(baseDir.toUri(), jobConf);	for (String name : sourceInfoMap.keySet()) {	Path sourceDir = new Path(baseDir, name);	for (FileStatus fstatus : fs.listStatus(sourceDir)) {	
start processing pruning file 

Map<String, String> spec = desc.getPartSpec();	if (spec == null) {	throw new AssertionException("No partition spec found in dynamic pruning");	}	String partValueString = spec.get(columnName);	if (partValueString == null) {	throw new AssertionException("Could not find partition value for column: " + columnName);	}	Object partValue = converter.convert(partValueString);	if (LOG.isDebugEnabled()) {	
converted partition value original 

String partValueString = spec.get(columnName);	if (partValueString == null) {	throw new AssertionException("Could not find partition value for column: " + columnName);	}	Object partValue = converter.convert(partValueString);	if (LOG.isDebugEnabled()) {	}	row[0] = partValue;	partValue = eval.evaluate(row);	if (LOG.isDebugEnabled()) {	
part key expr applied 

throw new AssertionException("Could not find partition value for column: " + columnName);	}	Object partValue = converter.convert(partValueString);	if (LOG.isDebugEnabled()) {	}	row[0] = partValue;	partValue = eval.evaluate(row);	if (LOG.isDebugEnabled()) {	}	if (!values.contains(partValue)) {	
pruning path 

========================= hive sample_4583 =========================

Set<Integer> newVcolsInCalcite = new HashSet<Integer>();	newVcolsInCalcite.addAll(inputs[0].vcolsInCalcite);	if (joinRel instanceof HiveMultiJoin || !(joinRel instanceof SemiJoin)) {	int shift = inputs[0].inputs.get(0).getSchema().getSignature().size();	for (int i = 1; i < inputs.length; i++) {	newVcolsInCalcite.addAll(HiveCalciteUtil.shiftVColsSet(inputs[i].vcolsInCalcite, shift));	shift += inputs[i].inputs.get(0).getSchema().getSignature().size();	}	}	if (LOG.isDebugEnabled()) {	
translating operator rel with row type 

order.append(orderChar);	nullOrder.append(nullOrderChar);	}	}	SelectOperator selectOp = genReduceSinkAndBacktrackSelect(input, keyCols.toArray(new ExprNodeDesc[keyCols.size()]), 0, partCols, order.toString(), nullOrder.toString(), -1, Operation.NOT_ACID, hiveConf);	PTFTranslator translator = new PTFTranslator();	PTFDesc ptfDesc = translator.translate(wSpec, semanticAnalyzer, hiveConf, rr, unparseTranslator);	RowResolver ptfOpRR = ptfDesc.getFuncDef().getOutputShape().getRr();	Operator<?> ptfOp = OperatorFactory.getAndMakeChild( ptfDesc, new RowSchema(ptfOpRR.getColumnInfos()), selectOp);	if (LOG.isDebugEnabled()) {	
generated with row schema 

if (tableAlias == null) {	throw new SemanticException( "In CBO return path, genReduceSinkAndBacktrackSelect is expecting only one tableAlias but there is none");	}	ReduceSinkOperator rsOp = genReduceSink(input, tableAlias, keys, tag, partitionCols, order, nullOrder, numReducers, acidOperation, hiveConf);	Map<String, ExprNodeDesc> descriptors = buildBacktrackFromReduceSink(keepColNames, rsOp.getConf().getOutputKeyColumnNames(), rsOp.getConf().getOutputValueColumnNames(), rsOp.getValueIndex(), input);	SelectDesc selectDesc = new SelectDesc(new ArrayList<ExprNodeDesc>(descriptors.values()), new ArrayList<String>(descriptors.keySet()));	ArrayList<ColumnInfo> cinfoLst = createColInfosSubset(input, keepColNames);	SelectOperator selectOp = (SelectOperator) OperatorFactory.getAndMakeChild( selectDesc, new RowSchema(cinfoLst), rsOp);	selectOp.setColumnExprMap(descriptors);	if (LOG.isDebugEnabled()) {	
generated with row schema 

colExprMap.put(Utilities.ReduceField.KEY + "." + keyColNames.get(i), reduceKeys.get(i));	}	List<String> valColNames = rsDesc.getOutputValueColumnNames();	for (int i = 0; i < valColNames.size(); i++) {	colExprMap.put(Utilities.ReduceField.VALUE + "." + valColNames.get(i), reduceValues.get(i));	}	rsOp.setValueIndex(index);	rsOp.setColumnExprMap(colExprMap);	rsOp.setInputAliases(input.getSchema().getTableNames() .toArray(new String[input.getSchema().getTableNames().size()]));	if (LOG.isDebugEnabled()) {	
generated with row schema 

filters.put(tag, filtersPerInput.get(pos));	}	JoinDesc desc = new JoinDesc(exprMap, outputColumnNames, noOuterJoin, joinCondns, filters, joinExpressions, null);	desc.setReversedExprs(reversedExprs);	desc.setFilterMap(filterMap);	JoinOperator joinOp = (JoinOperator) OperatorFactory.getAndMakeChild( childOps[0].getCompilationOpContext(), desc, new RowSchema(outputColumns), childOps);	joinOp.setColumnExprMap(colExprMap);	joinOp.setPosToAliasMap(posToAliasMap);	joinOp.getConf().setBaseSrc(baseSrc);	if (LOG.isDebugEnabled()) {	
generated with row schema 

========================= hive sample_2841 =========================

} else if (matches.length == 0) {	errors.add(new IOException("Input Pattern " + dir + " matches 0 files"));	} else {	for (FileStatus globStat : matches) {	FileUtils.listStatusRecursively(fs, globStat, result);	}	}	if (!errors.isEmpty()) {	throw new InvalidInputException(errors);	}	
matches for 

} else if (matches.length == 0) {	errors.add(new IOException("Input Pattern " + dir + " matches 0 files"));	} else {	for (FileStatus globStat : matches) {	FileUtils.listStatusRecursively(fs, globStat, result);	}	}	if (!errors.isEmpty()) {	throw new InvalidInputException(errors);	}	
total input paths to process from dir 

JobConf newjob = new JobConf(job);	ArrayList<InputSplit> result = new ArrayList<InputSplit>();	int numOrigSplits = 0;	for (Path dir : dirs) {	PartitionDesc part = getPartitionDescFromPath(pathToPartitionInfo, dir);	Class inputFormatClass = part.getInputFileFormatClass();	InputFormat inputFormat = getInputFormatFromCache(inputFormatClass, job);	newjob.setInputFormat(inputFormat.getClass());	FileStatus[] listStatus = listStatus(newjob, dir);	for (FileStatus status : listStatus) {	
block size 

JobConf newjob = new JobConf(job);	ArrayList<InputSplit> result = new ArrayList<InputSplit>();	int numOrigSplits = 0;	for (Path dir : dirs) {	PartitionDesc part = getPartitionDescFromPath(pathToPartitionInfo, dir);	Class inputFormatClass = part.getInputFileFormatClass();	InputFormat inputFormat = getInputFormatFromCache(inputFormatClass, job);	newjob.setInputFormat(inputFormat.getClass());	FileStatus[] listStatus = listStatus(newjob, dir);	for (FileStatus status : listStatus) {	
file length 

FileStatus[] listStatus = listStatus(newjob, dir);	for (FileStatus status : listStatus) {	FileInputFormat.setInputPaths(newjob, status.getPath());	InputSplit[] iss = inputFormat.getSplits(newjob, 0);	if (iss != null && iss.length > 0) {	numOrigSplits += iss.length;	result.add(new BucketizedHiveInputSplit(iss, inputFormatClass .getName()));	}	}	}	
bucketized splits generated from original splits 

========================= hive sample_3753 =========================

public JdbcRecordReader(JobConf conf, JdbcInputSplit split) {	
initializing jdbcrecordreader 

public boolean next(LongWritable key, MapWritable value) throws IOException {	try {	
jdbcrecordreader next called 

public boolean next(LongWritable key, MapWritable value) throws IOException {	try {	if (dbAccessor == null) {	dbAccessor = DatabaseAccessorFactory.getAccessor(conf);	iterator = dbAccessor.getRecordIterator(conf, split.getLimit(), split.getOffset());	}	if (iterator.hasNext()) {	
jdbcrecordreader has more records to read 

key.set(pos);	pos++;	Map<String, Object> record = iterator.next();	if ((record != null) && (!record.isEmpty())) {	for (Entry<String, Object> entry : record.entrySet()) {	value.put(new Text(entry.getKey()), entry.getValue() == null ? NullWritable.get() : new ObjectWritable(entry.getValue()));	}	return true;	}	else {	
jdbcrecordreader got null record 

for (Entry<String, Object> entry : record.entrySet()) {	value.put(new Text(entry.getKey()), entry.getValue() == null ? NullWritable.get() : new ObjectWritable(entry.getValue()));	}	return true;	}	else {	return false;	}	}	else {	
jdbcrecordreader has no more records to read 

}	else {	return false;	}	}	else {	return false;	}	}	catch (Exception e) {	
an error occurred while reading the next record from db 

========================= hive sample_1152 =========================

public static TxnStore getTxnStore(Configuration conf) {	String className = MetastoreConf.getVar(conf, ConfVars.TXN_STORE_IMPL);	try {	TxnStore handler = JavaUtils.getClass(className, TxnStore.class).newInstance();	handler.setConf(conf);	return handler;	} catch (Exception e) {	
unable to instantiate raw store directly in fastpath mode 

========================= hive sample_1851 =========================

public synchronized void init(HiveConf hiveConf) {	ensureCurrentState(STATE.NOTINITED);	this.hiveConf = hiveConf;	changeState(STATE.INITED);	
service is inited 

public synchronized void start() {	startTime = System.currentTimeMillis();	ensureCurrentState(STATE.INITED);	changeState(STATE.STARTED);	
service is started 

public synchronized void stop() {	if (state == STATE.STOPPED || state == STATE.INITED || state == STATE.NOTINITED) {	return;	}	ensureCurrentState(STATE.STARTED);	changeState(STATE.STOPPED);	
service is stopped 

========================= hive sample_2400 =========================

final List<BlockMetaData> splitGroup = new ArrayList<BlockMetaData>();	final long splitStart = ((FileSplit) oldSplit).getStart();	final long splitLength = ((FileSplit) oldSplit).getLength();	for (final BlockMetaData block : blocks) {	final long firstDataPage = block.getColumns().get(0).getFirstDataPageOffset();	if (firstDataPage >= splitStart && firstDataPage < splitStart + splitLength) {	splitGroup.add(block);	}	}	if (splitGroup.isEmpty()) {	
skipping split could not find row group in 

splitGroup.add(block);	}	}	if (splitGroup.isEmpty()) {	return null;	}	FilterCompat.Filter filter = setFilter(jobConf, fileMetaData.getSchema());	if (filter != null) {	filtedBlocks = RowGroupFilter.filterRowGroups(filter, splitGroup, fileMetaData.getSchema());	if (filtedBlocks.isEmpty()) {	
all row groups are dropped due to filter predicates 

return null;	}	FilterCompat.Filter filter = setFilter(jobConf, fileMetaData.getSchema());	if (filter != null) {	filtedBlocks = RowGroupFilter.filterRowGroups(filter, splitGroup, fileMetaData.getSchema());	if (filtedBlocks.isEmpty()) {	return null;	}	long droppedBlocks = splitGroup.size() - filtedBlocks.size();	if (droppedBlocks > 0) {	
dropping row groups that do not pass filter predicate 

public FilterCompat.Filter setFilter(final JobConf conf, MessageType schema) {	SearchArgument sarg = ConvertAstToSearchArg.createFromConf(conf);	if (sarg == null) {	return null;	}	FilterPredicate p = ParquetFilterPredicateConverter.toFilterPredicate(sarg, schema);	if (p != null) {	
parquet predicate push down generated 

public FilterCompat.Filter setFilter(final JobConf conf, MessageType schema) {	SearchArgument sarg = ConvertAstToSearchArg.createFromConf(conf);	if (sarg == null) {	return null;	}	FilterPredicate p = ParquetFilterPredicateConverter.toFilterPredicate(sarg, schema);	if (p != null) {	ParquetInputFormat.setFilterPredicate(conf, p);	return FilterCompat.get(p);	} else {	
no parquet predicate push down is generated 

========================= hive sample_3704 =========================

addService(operationManager);	initSessionImplClassName();	Metrics metrics = MetricsFactory.getInstance();	if(metrics != null){	registerOpenSesssionMetrics(metrics);	registerActiveSesssionMetrics(metrics);	}	userLimit = hiveConf.getIntVar(ConfVars.HIVE_SERVER2_LIMIT_CONNECTIONS_PER_USER);	ipAddressLimit = hiveConf.getIntVar(ConfVars.HIVE_SERVER2_LIMIT_CONNECTIONS_PER_IPADDRESS);	userIpAddressLimit = hiveConf.getIntVar(ConfVars.HIVE_SERVER2_LIMIT_CONNECTIONS_PER_USER_IPADDRESS);	
connections limit are user ipaddress user ipaddress 

private void createBackgroundOperationPool() {	int poolSize = hiveConf.getIntVar(ConfVars.HIVE_SERVER2_ASYNC_EXEC_THREADS);	
background operation thread pool size 

private void createBackgroundOperationPool() {	int poolSize = hiveConf.getIntVar(ConfVars.HIVE_SERVER2_ASYNC_EXEC_THREADS);	int poolQueueSize = hiveConf.getIntVar(ConfVars.HIVE_SERVER2_ASYNC_EXEC_WAIT_QUEUE_SIZE);	
background operation thread wait queue size 

private void createBackgroundOperationPool() {	int poolSize = hiveConf.getIntVar(ConfVars.HIVE_SERVER2_ASYNC_EXEC_THREADS);	int poolQueueSize = hiveConf.getIntVar(ConfVars.HIVE_SERVER2_ASYNC_EXEC_WAIT_QUEUE_SIZE);	long keepAliveTime = HiveConf.getTimeVar( hiveConf, ConfVars.HIVE_SERVER2_ASYNC_EXEC_KEEPALIVE_TIME, TimeUnit.SECONDS);	
background operation thread keepalive time seconds 

public void run() {	sleepFor(interval);	while (!shutdown) {	long current = System.currentTimeMillis();	for (HiveSession session : new ArrayList<HiveSession>(handleToSession.values())) {	if (shutdown) {	break;	}	if (sessionTimeout > 0 && session.getLastAccessTime() + sessionTimeout <= current && (!checkOperation || session.getNoOperationTime() > sessionTimeout)) {	SessionHandle handle = session.getSessionHandle();	
session is timed out last access and will be closed 

long current = System.currentTimeMillis();	for (HiveSession session : new ArrayList<HiveSession>(handleToSession.values())) {	if (shutdown) {	break;	}	if (sessionTimeout > 0 && session.getLastAccessTime() + sessionTimeout <= current && (!checkOperation || session.getNoOperationTime() > sessionTimeout)) {	SessionHandle handle = session.getSessionHandle();	try {	closeSession(handle);	} catch (HiveSQLException e) {	
exception is thrown closing session 

} catch (Exception e) {	throw new HiveSQLException("Cannot initialize session class:" + sessionImplclassName, e);	}	}	}	session.setSessionManager(this);	session.setOperationManager(operationManager);	try {	session.open(sessionConf);	} catch (Exception e) {	
failed to open session 

}	}	session.setSessionManager(this);	session.setOperationManager(operationManager);	try {	session.open(sessionConf);	} catch (Exception e) {	try {	session.close();	} catch (Throwable t) {	
error closing session 

}	session = null;	throw new HiveSQLException("Failed to open new session: " + e.getMessage(), e);	}	if (isOperationLogEnabled) {	session.setOperationLogSessionDir(operationLogRootDir);	}	try {	executeSessionHooks(session);	} catch (Exception e) {	
failed to execute session hooks 

}	if (isOperationLogEnabled) {	session.setOperationLogSessionDir(operationLogRootDir);	}	try {	executeSessionHooks(session);	} catch (Exception e) {	try {	session.close();	} catch (Throwable t) {	
error closing session 

executeSessionHooks(session);	} catch (Exception e) {	try {	session.close();	} catch (Throwable t) {	}	session = null;	throw new HiveSQLException("Failed to execute session hooks: " + e.getMessage(), e);	}	handleToSession.put(session.getSessionHandle(), session);	
session opened current sessions 

public synchronized void closeSession(SessionHandle sessionHandle) throws HiveSQLException {	HiveSession session = handleToSession.remove(sessionHandle);	if (session == null) {	throw new HiveSQLException("Session does not exist: " + sessionHandle);	}	
session closed current sessions 

HiveSession session = handleToSession.remove(sessionHandle);	if (session == null) {	throw new HiveSQLException("Session does not exist: " + sessionHandle);	}	try {	session.close();	} finally {	decrementConnections(session);	if (!(hiveServer2 == null) && (hiveConf.getBoolVar(ConfVars.HIVE_SERVER2_SUPPORT_DYNAMIC_SERVICE_DISCOVERY)) && (hiveServer2.isDeregisteredWithZooKeeper())) {	if (getOpenSessionCount() == 0) {	
this instance of has been removed from the list of server instances available for dynamic service discovery the last client session has ended will shutdown now 

public static void setProxyUserName(String userName) {	
setting proxy user name based on query param to 

========================= hive sample_2396 =========================

public void testTokenMerge() throws Exception {	final Text service = new Text("service");	Token<?> token = Mockito.mock(Token.class);	JobConf jobConf = new JobConf();	Mockito.when(token.getService()).thenReturn(service);	try {	helper.mergeTokenIntoJobConf(jobConf, token);	} catch (IOException e) {	
ignoring exception likely coming from hadoop 

ArrayList<Token<?>> tokens = new ArrayList<>();	Text service = new Text("service");	Token<?> token = Mockito.mock(Token.class);	tokens.add(token);	Mockito.when(ugi.getTokens()).thenReturn(tokens);	Mockito.when(token.getKind()).thenReturn(HiveAccumuloHelper.ACCUMULO_SERVICE);	Mockito.when(token.getService()).thenReturn(service);	try {	helper.addTokenFromUserToJobConf(ugi, jobConf);	} catch (IOException e) {	
ignoring exception likely coming from hadoop 

========================= hive sample_127 =========================

public static InputSplit createTableSnapshotRegionSplit() {	try {	assertSupportsTableSnapshots();	} catch (RuntimeException e) {	
probably don t support table snapshots returning null instance 

========================= hive sample_598 =========================

public Object doDeserialize(Writable field) throws SerDeException {	
delimitedjsonserde cannot deserialize 

========================= hive sample_5516 =========================

public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx, Object... nodeOutputs) throws SemanticException {	TableScanOperator tsop = (TableScanOperator) nd;	AnnotateStatsProcCtx aspCtx = (AnnotateStatsProcCtx) procCtx;	PrunedPartitionList partList = aspCtx.getParseContext().getPrunedPartitions(tsop);	ColumnStatsList colStatsCached = aspCtx.getParseContext().getColStatsCached(partList);	Table table = tsop.getConf().getTableMetadata();	try {	Statistics stats = StatsUtils.collectStatistics(aspCtx.getConf(), partList, colStatsCached, table, tsop);	tsop.setStatistics(stats.clone());	if (LOG.isDebugEnabled()) {	
stats 

AnnotateStatsProcCtx aspCtx = (AnnotateStatsProcCtx) procCtx;	PrunedPartitionList partList = aspCtx.getParseContext().getPrunedPartitions(tsop);	ColumnStatsList colStatsCached = aspCtx.getParseContext().getColStatsCached(partList);	Table table = tsop.getConf().getTableMetadata();	try {	Statistics stats = StatsUtils.collectStatistics(aspCtx.getConf(), partList, colStatsCached, table, tsop);	tsop.setStatistics(stats.clone());	if (LOG.isDebugEnabled()) {	}	} catch (HiveException e) {	
failed to retrieve stats 

}	if (satisfyPrecondition(parentStats)) {	List<ColStatistics> colStats = StatsUtils.getColStatisticsFromExprMap(conf, parentStats, sop.getColumnExprMap(), sop.getSchema());	stats.setColumnStats(colStats);	if (!sop.getConf().isSelectStar() && !sop.getConf().isSelStarNoCompute()) {	long dataSize = StatsUtils.getDataSizeFromColumnStats(stats.getNumRows(), colStats);	stats.setDataSize(dataSize);	}	sop.setStatistics(stats);	if (LOG.isDebugEnabled()) {	
stats 

long dataSize = StatsUtils.getDataSizeFromColumnStats(stats.getNumRows(), colStats);	stats.setDataSize(dataSize);	}	sop.setStatistics(stats);	if (LOG.isDebugEnabled()) {	}	} else {	if (parentStats != null) {	sop.setStatistics(parentStats.clone());	if (LOG.isDebugEnabled()) {	
stats 

}	if (parentStats != null) {	ExprNodeDesc pred = fop.getConf().getPredicate();	long newNumRows = evaluateExpression(parentStats, pred, aspCtx, neededCols, fop, parentStats.getNumRows());	Statistics st = parentStats.clone();	if (satisfyPrecondition(parentStats)) {	if (newNumRows <= parentStats.getNumRows()) {	updateStats(st, newNumRows, true, fop);	}	if (LOG.isDebugEnabled()) {	
stats 

if (newNumRows <= parentStats.getNumRows()) {	updateStats(st, newNumRows, true, fop);	}	if (LOG.isDebugEnabled()) {	}	} else {	if (newNumRows <= parentStats.getNumRows()) {	updateStats(st, newNumRows, false, fop);	}	if (LOG.isDebugEnabled()) {	
stats 

protected long evaluateExpression(Statistics stats, ExprNodeDesc pred, AnnotateStatsProcCtx aspCtx, List<String> neededCols, Operator<?> op, long currNumRows) throws SemanticException {	long newNumRows = 0;	Statistics andStats = null;	if (currNumRows <= 1 || stats.getDataSize() <= 0) {	if (LOG.isDebugEnabled()) {	
estimating row count for original num rows original data size new num rows 

}	} else if (pred instanceof ExprNodeConstantDesc) {	ExprNodeConstantDesc encd = (ExprNodeConstantDesc) pred;	if (Boolean.FALSE.equals(encd.getValue())) {	newNumRows = 0;	} else {	newNumRows = stats.getNumRows();	}	}	if (LOG.isDebugEnabled()) {	
estimating row count for original num rows new num rows 

private long evaluateNotNullExpr(Statistics parentStats, ExprNodeGenericFuncDesc pred, long currNumRows) {	long noOfNulls = getMaxNulls(parentStats, pred);	long parentCardinality = currNumRows;	long newPredCardinality = parentCardinality;	if (parentCardinality > noOfNulls) {	newPredCardinality = parentCardinality - noOfNulls;	} else {	
invalid column stats no of nulls cardinality 

TableScanOperator top = OperatorUtils.findSingleOperatorUpstream(gop, TableScanOperator.class);	if (top == null) {	inputSize = parentStats.getDataSize();	maxSplitSize = HiveConf.getLongVar(conf, HiveConf.ConfVars.BYTESPERREDUCER);	} else {	inputSize = top.getConf().getStatistics().getDataSize();	}	parallelism = (int) Math.ceil((double) inputSize / maxSplitSize);	}	if (LOG.isDebugEnabled()) {	
stats inputsize maxsplitsize parallelism containsgroupingset sizeofgroupingset 

} else {	inputSize = top.getConf().getStatistics().getDataSize();	}	parallelism = (int) Math.ceil((double) inputSize / maxSplitSize);	}	if (LOG.isDebugEnabled()) {	}	if (satisfyPrecondition(parentStats)) {	hashAgg = checkMapSideAggregation(gop, colStats, conf);	if (LOG.isDebugEnabled()) {	
stats hashagg 

continue;	} else {	ndvProduct = 0;	}	break;	}	}	if (ndvProduct == 0) {	ndvProduct = parentNumRows / 2;	if (LOG.isDebugEnabled()) {	
stats ndvproduct became as some column does not have stats ndvproduct changed to 

if (ndvProduct == 0) {	ndvProduct = parentNumRows / 2;	if (LOG.isDebugEnabled()) {	}	}	if (interReduction) {	if (hashAgg) {	if (containsGroupingSet) {	cardinality = Math.min( (StatsUtils.safeMult(parentNumRows, sizeOfGroupingSet)) / 2, StatsUtils.safeMult(StatsUtils.safeMult(ndvProduct, parallelism), sizeOfGroupingSet));	if (LOG.isDebugEnabled()) {	
case stats cardinality 

}	if (interReduction) {	if (hashAgg) {	if (containsGroupingSet) {	cardinality = Math.min( (StatsUtils.safeMult(parentNumRows, sizeOfGroupingSet)) / 2, StatsUtils.safeMult(StatsUtils.safeMult(ndvProduct, parallelism), sizeOfGroupingSet));	if (LOG.isDebugEnabled()) {	}	} else {	cardinality = Math.min(parentNumRows / 2, StatsUtils.safeMult(ndvProduct, parallelism));	if (LOG.isDebugEnabled()) {	
case stats cardinality 

}	} else {	cardinality = Math.min(parentNumRows / 2, StatsUtils.safeMult(ndvProduct, parallelism));	if (LOG.isDebugEnabled()) {	}	}	} else {	if (containsGroupingSet) {	cardinality = StatsUtils.safeMult(parentNumRows, sizeOfGroupingSet);	if (LOG.isDebugEnabled()) {	
case stats cardinality 

}	}	} else {	if (containsGroupingSet) {	cardinality = StatsUtils.safeMult(parentNumRows, sizeOfGroupingSet);	if (LOG.isDebugEnabled()) {	}	} else {	cardinality = parentNumRows;	if (LOG.isDebugEnabled()) {	
case stats cardinality 

}	} else {	GroupByOperator mGop = OperatorUtils.findSingleOperatorUpstream(parent, GroupByOperator.class);	if (mGop != null) {	containsGroupingSet = mGop.getConf().isGroupingSetsPresent();	}	if (containsGroupingSet) {	sizeOfGroupingSet = mGop.getConf().getListGroupingSets().size();	cardinality = Math.min(parentNumRows, StatsUtils.safeMult(ndvProduct, sizeOfGroupingSet));	if (LOG.isDebugEnabled()) {	
case stats cardinality 

containsGroupingSet = mGop.getConf().isGroupingSetsPresent();	}	if (containsGroupingSet) {	sizeOfGroupingSet = mGop.getConf().getListGroupingSets().size();	cardinality = Math.min(parentNumRows, StatsUtils.safeMult(ndvProduct, sizeOfGroupingSet));	if (LOG.isDebugEnabled()) {	}	} else {	cardinality = Math.min(parentNumRows, ndvProduct);	if (LOG.isDebugEnabled()) {	
case stats cardinality 

}	updateStats(stats, cardinality, true, gop, false);	} else {	if (parentStats != null) {	stats = parentStats.clone();	final long parentNumRows = stats.getNumRows();	if (interReduction) {	if (containsGroupingSet) {	cardinality = StatsUtils.safeMult(parentNumRows, sizeOfGroupingSet);	if (LOG.isDebugEnabled()) {	
case stats cardinality 

stats = parentStats.clone();	final long parentNumRows = stats.getNumRows();	if (interReduction) {	if (containsGroupingSet) {	cardinality = StatsUtils.safeMult(parentNumRows, sizeOfGroupingSet);	if (LOG.isDebugEnabled()) {	}	} else {	cardinality = parentNumRows;	if (LOG.isDebugEnabled()) {	
case stats cardinality 

if (LOG.isDebugEnabled()) {	}	} else {	cardinality = parentNumRows;	if (LOG.isDebugEnabled()) {	}	}	} else {	cardinality = parentNumRows / 2;	if (LOG.isDebugEnabled()) {	
case stats cardinality 

if (!stats.getColumnStatsState().equals(Statistics.State.NONE)) {	updateStats(stats, stats.getNumRows(), true, gop);	}	}	if (colExprMap.isEmpty()) {	updateStats(stats, 1, true, gop);	}	}	gop.setStatistics(stats);	if (LOG.isDebugEnabled() && stats != null) {	
stats 

} else {	pred = jop.getConf().getResidualFilterExprs().get(0);	}	newNumRows = evaluateExpression(stats, pred, aspCtx, jop.getSchema().getColumnNames(), jop, stats.getNumRows());	if (newNumRows <= joinRowCount) {	updateStats(stats, newNumRows, true, jop);	}	}	jop.setStatistics(stats);	if (LOG.isDebugEnabled()) {	
stats 

} else {	pred = jop.getConf().getResidualFilterExprs().get(0);	}	newNumRows = evaluateExpression(wcStats, pred, aspCtx, jop.getSchema().getColumnNames(), jop, wcStats.getNumRows());	if (newNumRows <= joinRowCount) {	updateStats(wcStats, newNumRows, false, jop);	}	}	jop.setStatistics(wcStats);	if (LOG.isDebugEnabled()) {	
stats 

private long inferPKFKRelationship(int numAttr, List<Operator<? extends OperatorDesc>> parents, CommonJoinOperator<? extends JoinDesc> jop) {	long newNumRows = -1;	if (numAttr != 1) {	return newNumRows;	}	Map<Integer, ColStatistics> parentsWithPK = getPrimaryKeyCandidates(parents);	if (parentsWithPK.size() != 1) {	
stats detects none multiple pk parents 

Integer pkPos = parentsWithPK.keySet().iterator().next();	ColStatistics csPK = parentsWithPK.values().iterator().next();	Map<Integer, ColStatistics> csFKs = getForeignKeyCandidates(parents, csPK);	if (csFKs.size() + 1 == parents.size()) {	newNumRows = getCardinality(parents, pkPos, csPK, csFKs, jop);	if (LOG.isDebugEnabled()) {	List<String> parentIds = Lists.newArrayList();	for (Integer i : parentsWithPK.keySet()) {	parentIds.add(parents.get(i).toString());	}	
stats pk parent id s 

newNumRows = getCardinality(parents, pkPos, csPK, csFKs, jop);	if (LOG.isDebugEnabled()) {	List<String> parentIds = Lists.newArrayList();	for (Integer i : parentsWithPK.keySet()) {	parentIds.add(parents.get(i).toString());	}	parentIds.clear();	for (Integer i : csFKs.keySet()) {	parentIds.add(parents.get(i).toString());	}	
stats fk parent id s 

private void updateColStats(HiveConf conf, Statistics stats, long interimNumRows, long newNumRows, CommonJoinOperator<? extends JoinDesc> jop, Map<Integer, Long> rowCountParents) {	if (newNumRows < 0) {	
stats overflow in number of rows rows will be set to long max value 

private void updateColStats(HiveConf conf, Statistics stats, long interimNumRows, long newNumRows, CommonJoinOperator<? extends JoinDesc> jop, Map<Integer, Long> rowCountParents) {	if (newNumRows < 0) {	}	if (newNumRows == 0) {	
stats equals in number of rows rows will be set to 

switch (joinCond.getType()) {	case JoinDesc.INNER_JOIN: break;	case JoinDesc.LEFT_OUTER_JOIN : result = Math.max(rowCountParents.get(joinCond.getLeft()), result);	break;	case JoinDesc.RIGHT_OUTER_JOIN : result = Math.max(rowCountParents.get(joinCond.getRight()), result);	break;	case JoinDesc.FULL_OUTER_JOIN : result = Math.max(StatsUtils.safeAdd(rowCountParents.get(joinCond.getRight()), rowCountParents.get(joinCond.getLeft())), result);	break;	case JoinDesc.LEFT_SEMI_JOIN : result = Math.min(rowCountParents.get(joinCond.getLeft()), result);	break;	
unhandled join type in stats estimation 

limit = lop.getConf().getLimit();	if (satisfyPrecondition(parentStats)) {	Statistics stats = parentStats.clone();	List<ColStatistics> colStats = StatsUtils.getColStatisticsUpdatingTableAlias( parentStats, lop.getSchema());	stats.setColumnStats(colStats);	if (limit <= parentStats.getNumRows()) {	updateStats(stats, limit, true, lop);	}	lop.setStatistics(stats);	if (LOG.isDebugEnabled()) {	
stats 

}	lop.setStatistics(stats);	if (LOG.isDebugEnabled()) {	}	} else {	if (parentStats != null) {	limit = StatsUtils.getMaxIfOverflow(limit);	Statistics wcStats = parentStats.scaleToRowCount(limit);	lop.setStatistics(wcStats);	if (LOG.isDebugEnabled()) {	
stats 

if (cs != null) {	cs.setColumnName(prefixedVal);	colStats.add(cs);	}	}	}	outStats.setColumnStats(colStats);	}	rop.setStatistics(outStats);	if (LOG.isDebugEnabled()) {	
stats 

Statistics parentStats = parent.getStatistics();	if (stats == null) {	stats = parentStats.clone();	} else {	stats.addBasicStats(parentStats);	}	stats.updateColumnStatsState(parentStats.getColumnStatsState());	List<ColStatistics> colStats = StatsUtils.getColStatisticsFromExprMap(hconf, parentStats, op.getColumnExprMap(), op.getSchema());	stats.addToColumnStats(colStats);	if (LOG.isDebugEnabled()) {	
stats 

static void updateStats(Statistics stats, long newNumRows, boolean useColStats, Operator<? extends OperatorDesc> op, boolean updateNDV) {	if (newNumRows < 0) {	
stats overflow in number of rows rows will be set to long max value 

static void updateStats(Statistics stats, long newNumRows, boolean useColStats, Operator<? extends OperatorDesc> op, boolean updateNDV) {	if (newNumRows < 0) {	newNumRows = StatsUtils.getMaxIfOverflow(newNumRows);	}	if (newNumRows == 0) {	
stats equals in number of rows rows will be set to 

========================= hive sample_3092 =========================

public void next(OrcStruct next) throws IOException {	if (getRecordReader().hasNext()) {	nextRecord = (OrcStruct) getRecordReader().next(next);	getKey().setValues(OrcRecordUpdater.getOriginalTransaction(nextRecord()), OrcRecordUpdater.getBucket(nextRecord()), OrcRecordUpdater.getRowId(nextRecord()), OrcRecordUpdater.getCurrentTransaction(nextRecord()), statementId);	if (getMaxKey() != null && getKey().compareRow(getMaxKey()) > 0) {	
key maxkey 

========================= hive sample_3657 =========================

public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx, Object... nodeOutputs) throws SemanticException {	FileSinkOperator fsOp = (FileSinkOperator) nd;	
sorted dynamic partitioning optimization kicked in 

public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx, Object... nodeOutputs) throws SemanticException {	FileSinkOperator fsOp = (FileSinkOperator) nd;	if (fsOp.getConf().getDynPartCtx() == null) {	
bailing out of sort dynamic partition optimization as dynamic partitioning context is null 

public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx, Object... nodeOutputs) throws SemanticException {	FileSinkOperator fsOp = (FileSinkOperator) nd;	if (fsOp.getConf().getDynPartCtx() == null) {	return null;	}	ListBucketingCtx lbCtx = fsOp.getConf().getLbCtx();	if (lbCtx != null && !lbCtx.getSkewedColNames().isEmpty() && !lbCtx.getSkewedColValues().isEmpty()) {	
bailing out of sort dynamic partition optimization as list bucketing is enabled 

FileSinkOperator fsOp = (FileSinkOperator) nd;	if (fsOp.getConf().getDynPartCtx() == null) {	return null;	}	ListBucketingCtx lbCtx = fsOp.getConf().getLbCtx();	if (lbCtx != null && !lbCtx.getSkewedColNames().isEmpty() && !lbCtx.getSkewedColValues().isEmpty()) {	return null;	}	Table destTable = fsOp.getConf().getTable();	if (destTable == null) {	
bailing out of sort dynamic partition optimization as destination table is null 

ListBucketingCtx lbCtx = fsOp.getConf().getLbCtx();	if (lbCtx != null && !lbCtx.getSkewedColNames().isEmpty() && !lbCtx.getSkewedColValues().isEmpty()) {	return null;	}	Table destTable = fsOp.getConf().getTable();	if (destTable == null) {	return null;	}	Operator<? extends OperatorDesc> fsParent = fsOp.getParentOperators().get(0);	if (allStaticPartitions(fsParent, fsOp.getConf().getDynPartCtx())) {	
bailing out of sorted dynamic partition optimizer as all dynamic partition columns got constant folded static partitioning 

}	Table destTable = fsOp.getConf().getTable();	if (destTable == null) {	return null;	}	Operator<? extends OperatorDesc> fsParent = fsOp.getParentOperators().get(0);	if (allStaticPartitions(fsParent, fsOp.getConf().getDynPartCtx())) {	return null;	}	if (!removeRSInsertedByEnforceBucketing(fsOp)) {	
bailing out of sort dynamic partition optimization as some partition columns got constant folded 

sortOrder = Lists.newArrayList();	inferSortPositions(fsParent, sortPositions, sortOrder);	}	List<ColumnInfo> colInfos = fsParent.getSchema().getSignature();	bucketColumns = getPositionsToExprNodes(bucketPositions, colInfos);	}	List<Integer> sortNullOrder = new ArrayList<Integer>();	for (int order : sortOrder) {	sortNullOrder.add(order == 1 ? 0 : 1);	}	
got sort order 

sortOrder = Lists.newArrayList();	inferSortPositions(fsParent, sortPositions, sortOrder);	}	List<ColumnInfo> colInfos = fsParent.getSchema().getSignature();	bucketColumns = getPositionsToExprNodes(bucketPositions, colInfos);	}	List<Integer> sortNullOrder = new ArrayList<Integer>();	for (int order : sortOrder) {	sortNullOrder.add(order == 1 ? 0 : 1);	}	
sort position 

sortOrder = Lists.newArrayList();	inferSortPositions(fsParent, sortPositions, sortOrder);	}	List<ColumnInfo> colInfos = fsParent.getSchema().getSignature();	bucketColumns = getPositionsToExprNodes(bucketPositions, colInfos);	}	List<Integer> sortNullOrder = new ArrayList<Integer>();	for (int order : sortOrder) {	sortNullOrder.add(order == 1 ? 0 : 1);	}	
sort order 

sortOrder = Lists.newArrayList();	inferSortPositions(fsParent, sortPositions, sortOrder);	}	List<ColumnInfo> colInfos = fsParent.getSchema().getSignature();	bucketColumns = getPositionsToExprNodes(bucketPositions, colInfos);	}	List<Integer> sortNullOrder = new ArrayList<Integer>();	for (int order : sortOrder) {	sortNullOrder.add(order == 1 ? 0 : 1);	}	
sort null order 

SelectDesc selConf = new SelectDesc(descs, colNames);	SelectOperator selOp = (SelectOperator) OperatorFactory.getAndMakeChild( selConf, selRS, rsOp);	fsOp.getParentOperators().clear();	fsOp.getParentOperators().add(selOp);	selOp.getChildOperators().add(fsOp);	fsOp.getConf().setDpSortState(FileSinkDesc.DPSortState.PARTITION_SORTED);	if (!bucketColumns.isEmpty()) {	fsOp.getConf().setDpSortState(FileSinkDesc.DPSortState.PARTITION_BUCKET_SORTED);	}	fsOp.getConf().setPartitionCols( rsOp.getConf().getPartitionCols());	
inserted and as parent of and child of 

Operator<? extends OperatorDesc> rsChild = rsToRemove.getChildOperators().get(0);	Operator<? extends OperatorDesc> rsGrandChild = rsChild.getChildOperators().get(0);	if (rsChild instanceof SelectOperator) {	if (rsParent.getSchema().getSignature().size() != rsChild.getSchema().getSignature().size()) {	return false;	}	rsParent.getChildOperators().clear();	rsParent.getChildOperators().add(rsGrandChild);	rsGrandChild.getParentOperators().clear();	rsGrandChild.getParentOperators().add(rsParent);	
removed and as it was introduced by enforce bucketing sorting 

========================= hive sample_2945 =========================

if(hiveSiteURL == null) {	hiveSiteURL = findConfigFile(classLoader, "hive-site.xml");	}	if (hiveSiteURL != null) conf.addResource(hiveSiteURL);	hiveMetastoreSiteURL = findConfigFile(classLoader, "hivemetastore-site.xml");	if (hiveMetastoreSiteURL != null) conf.addResource(hiveMetastoreSiteURL);	metastoreSiteURL = findConfigFile(classLoader, "metastore-site.xml");	if (metastoreSiteURL !=  null) conf.addResource(metastoreSiteURL);	for (ConfVars var : ConfVars.values()) {	if (System.getProperty(var.varname) != null) {	
setting conf value using value 

result = seeIfConfAtThisLocation("METASTORE_HOME", name, true);	if (result == null) {	result = seeIfConfAtThisLocation("HIVE_CONF_DIR", name, false);	if (result == null) {	result = seeIfConfAtThisLocation("HIVE_HOME", name, true);	if (result == null) {	URI jarUri = null;	try {	jarUri = MetastoreConf.class.getProtectionDomain().getCodeSource().getLocation().toURI();	} catch (Throwable e) {	
cannot get jar uri 

if (result == null) {	result = seeIfConfAtThisLocation("HIVE_HOME", name, true);	if (result == null) {	URI jarUri = null;	try {	jarUri = MetastoreConf.class.getProtectionDomain().getCodeSource().getLocation().toURI();	} catch (Throwable e) {	}	result = seeIfConfAtThisLocation(new File(jarUri).getParent(), name, true);	if (result == null) {	
unable to find config file 

} catch (Throwable e) {	}	result = seeIfConfAtThisLocation(new File(jarUri).getParent(), name, true);	if (result == null) {	}	}	}	}	}	}	
found configuration file 

private static URL checkConfigFile(File f) {	try {	return (f.exists() && f.isFile()) ? f.toURI().toURL() : null;	} catch (Throwable e) {	
error looking for config 

========================= hive sample_2046 =========================

Text cf = new Text(col);	LOG.debug("Using Column Family=" + toString());	scan.fetchColumnFamily(cf);	for (Map.Entry<Key, Value> entry : scan) {	rowIds.add(new Range(entry.getKey().getColumnQualifier()));	if (rowIds.size() > maxRowIds) {	return null;	}	}	if (rowIds.isEmpty()) {	
found index matches 

LOG.debug("Using Column Family=" + toString());	scan.fetchColumnFamily(cf);	for (Map.Entry<Key, Value> entry : scan) {	rowIds.add(new Range(entry.getKey().getColumnQualifier()));	if (rowIds.size() > maxRowIds) {	return null;	}	}	if (rowIds.isEmpty()) {	} else {	
found index matches 

rowIds.add(new Range(entry.getKey().getColumnQualifier()));	if (rowIds.size() > maxRowIds) {	return null;	}	}	if (rowIds.isEmpty()) {	} else {	}	return rowIds;	} catch (AccumuloException | AccumuloSecurityException | TableNotFoundException e) {	
failed to scan index table 

} else {	}	return rowIds;	} catch (AccumuloException | AccumuloSecurityException | TableNotFoundException e) {	} finally {	if (scan != null) {	scan.close();	}	}	}	
index lookup failed for table 

========================= hive sample_218 =========================

public LlapZookeeperRegistryImpl(String instanceName, Configuration conf) {	super(instanceName, conf, HiveConf.getVar(conf, ConfVars.LLAP_ZK_REGISTRY_NAMESPACE), NAMESPACE_PREFIX, USER_SCOPE_PATH_PREFIX, WORKER_PREFIX, LlapProxy.isDaemon() ? SASL_LOGIN_CONTEXT_NAME : null, HiveConf.getVar(conf, ConfVars.LLAP_KERBEROS_PRINCIPAL), HiveConf.getVar(conf, ConfVars.LLAP_KERBEROS_KEYTAB_FILE), ConfVars.LLAP_VALIDATE_ACLS);	
llap zookeeper registry is enabled with registryid 

}	}	String uniqueId = registerServiceRecord(srv);	long znodeCreationTimeout = 120;	try {	slotZnode = new SlotZnode( zooKeeperClient, workersPath, SLOT_PREFIX, WORKER_PREFIX, uniqueId);	if (!slotZnode.start(znodeCreationTimeout, TimeUnit.SECONDS)) {	throw new Exception( "Max znode creation wait time: " + znodeCreationTimeout + "s exhausted");	}	} catch (Exception e) {	
unable to create a znode for this server instance 

try {	slotZnode = new SlotZnode( zooKeeperClient, workersPath, SLOT_PREFIX, WORKER_PREFIX, uniqueId);	if (!slotZnode.start(znodeCreationTimeout, TimeUnit.SECONDS)) {	throw new Exception( "Max znode creation wait time: " + znodeCreationTimeout + "s exhausted");	}	} catch (Exception e) {	CloseableUtils.closeQuietly(slotZnode);	super.stop();	throw (e instanceof IOException) ? (IOException)e : new IOException(e);	}	
registered node created a znode on zookeeper for llap instance rpc shuffle webui mgmt znodepath 

public ApplicationId getApplicationId() {	for (ChildData childData : instancesCache.getCurrentData()) {	byte[] data = getWorkerData(childData, WORKER_PREFIX);	if (data == null) continue;	ServiceRecord sr = null;	try {	sr = encoder.fromBytes(childData.getPath(), data);	} catch (IOException e) {	
unable to decode data for zkpath ignoring from current instances list 

========================= hive sample_648 =========================

projectionPusher = new ProjectionPusher();	colsToInclude = ColumnProjectionUtils.getReadColumnIDs(conf);	jobConf = conf;	rbCtx = Utilities.getVectorizedRowBatchCtx(jobConf);	ParquetInputSplit inputSplit = getSplit(oldInputSplit, conf);	if (inputSplit != null) {	initialize(inputSplit, conf);	}	initPartitionValues((FileSplit) oldInputSplit, conf);	} catch (Throwable e) {	
failed to create the vectorized reader due to exception 

private ParquetMetadata readSplitFooter( JobConf configuration, final Path file, Object cacheKey, MetadataFilter filter) throws IOException {	MemoryBufferOrBuffers footerData = (cacheKey == null || metadataCache == null) ? null : metadataCache.getFileMetadata(cacheKey);	if (footerData != null) {	if (LOG.isInfoEnabled()) {	
found the footer in cache for 

final FileStatus stat = fs.getFileStatus(file);	if (cacheKey == null || metadataCache == null) {	return readFooterFromFile(file, fs, stat, filter);	}	try (SeekableInputStream stream = HadoopStreams.wrap(fs.open(file))) {	long footerLengthIndex = stat.getLen() - ParquetFooterInputFromCache.FOOTER_LENGTH_SIZE - ParquetFileWriter.MAGIC.length;	stream.seek(footerLengthIndex);	int footerLength = BytesUtils.readIntLittleEndian(stream);	stream.seek(footerLengthIndex - footerLength);	if (LOG.isInfoEnabled()) {	
caching the footer of length for 

========================= hive sample_3712 =========================

boolean unknown = false;	FileSystem fs = tblPath.getFileSystem(conf);	try {	FileStatus tmpStatus = fs.getFileStatus(tblPath);	fd.lastAccessTime = tmpStatus.getAccessTime();	fd.lastUpdateTime = tmpStatus.getModificationTime();	if (partSpecified) {	tmpStatus = fs.getFileStatus(locations.get(0));	}	} catch (IOException e) {	
cannot access file system file system status will be unknown 

========================= hive sample_5015 =========================

} else {	appName = appName + sessionIdString;	}	sparkConf.put(appNameKey, appName);	sparkConf.put("spark.serializer", SPARK_DEFAULT_SERIALIZER);	sparkConf.put("spark.kryo.referenceTracking", SPARK_DEFAULT_REFERENCE_TRACKING);	InputStream inputStream = null;	try {	inputStream = HiveSparkClientFactory.class.getClassLoader() .getResourceAsStream(SPARK_DEFAULT_CONF_FILE);	if (inputStream != null) {	
loading spark properties from 

InputStream inputStream = null;	try {	inputStream = HiveSparkClientFactory.class.getClassLoader() .getResourceAsStream(SPARK_DEFAULT_CONF_FILE);	if (inputStream != null) {	Properties properties = new Properties();	properties.load(new InputStreamReader(inputStream, CharsetNames.UTF_8));	for (String propertyName : properties.stringPropertyNames()) {	if (propertyName.startsWith("spark")) {	String value = properties.getProperty(propertyName);	sparkConf.put(propertyName, properties.getProperty(propertyName));	
load spark property from s s s 

Properties properties = new Properties();	properties.load(new InputStreamReader(inputStream, CharsetNames.UTF_8));	for (String propertyName : properties.stringPropertyNames()) {	if (propertyName.startsWith("spark")) {	String value = properties.getProperty(propertyName);	sparkConf.put(propertyName, properties.getProperty(propertyName));	}	}	}	} catch (IOException e) {	
failed to open spark configuration file 

sparkConf.put(propertyName, properties.getProperty(propertyName));	}	}	}	} catch (IOException e) {	} finally {	if (inputStream != null) {	try {	inputStream.close();	} catch (IOException e) {	
failed to close inputstream 

}	}	if (SparkClientUtilities.isYarnClusterMode(sparkMaster, deployMode)) {	sparkConf.put("spark.yarn.maxAppAttempts", "1");	}	for (Map.Entry<String, String> entry : hiveConf) {	String propertyName = entry.getKey();	if (propertyName.startsWith("spark")) {	String value = hiveConf.get(propertyName);	sparkConf.put(propertyName, value);	
load spark property from hive configuration s s 

sparkConf.put("spark.yarn.maxAppAttempts", "1");	}	for (Map.Entry<String, String> entry : hiveConf) {	String propertyName = entry.getKey();	if (propertyName.startsWith("spark")) {	String value = hiveConf.get(propertyName);	sparkConf.put(propertyName, value);	} else if (propertyName.startsWith("yarn") && SparkClientUtilities.isYarnMaster(sparkMaster)) {	String value = hiveConf.get(propertyName);	sparkConf.put("spark.hadoop." + propertyName, value);	
load yarn property from hive configuration in s mode s s 

String value = hiveConf.get(propertyName);	sparkConf.put("spark.hadoop." + propertyName, value);	} else if (propertyName.equals(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY)) {	String value = hiveConf.get(propertyName);	if (value != null && !value.isEmpty()) {	sparkConf.put("spark.hadoop." + propertyName, value);	}	} else if (propertyName.startsWith("hbase") || propertyName.startsWith("zookeeper.znode")) {	String value = hiveConf.get(propertyName);	sparkConf.put("spark.hadoop." + propertyName, value);	
load hbase configuration s s 

String value = hiveConf.get(propertyName);	if (value != null && !value.isEmpty()) {	sparkConf.put("spark.hadoop." + propertyName, value);	}	} else if (propertyName.startsWith("hbase") || propertyName.startsWith("zookeeper.znode")) {	String value = hiveConf.get(propertyName);	sparkConf.put("spark.hadoop." + propertyName, value);	} else if (propertyName.startsWith("oozie")) {	String value = hiveConf.get(propertyName);	sparkConf.put("spark." + propertyName, value);	
pass oozie configuration s s 

} else if (propertyName.startsWith("hbase") || propertyName.startsWith("zookeeper.znode")) {	String value = hiveConf.get(propertyName);	sparkConf.put("spark.hadoop." + propertyName, value);	} else if (propertyName.startsWith("oozie")) {	String value = hiveConf.get(propertyName);	sparkConf.put("spark." + propertyName, value);	}	if (RpcConfiguration.HIVE_SPARK_RSC_CONFIGS.contains(propertyName)) {	String value = RpcConfiguration.getValue(hiveConf, propertyName);	sparkConf.put(propertyName, value);	
load rpc property from hive configuration s s 

========================= hive sample_4604 =========================

if (table != null) {	segmentGranularity = table.getParameters().get(Constants.DRUID_SEGMENT_GRANULARITY);	} else if (parseCtx.getCreateViewDesc() != null) {	segmentGranularity = parseCtx.getCreateViewDesc().getTblProps() .get(Constants.DRUID_SEGMENT_GRANULARITY);	} else if (parseCtx.getCreateTable() != null) {	segmentGranularity = parseCtx.getCreateTable().getTblProps() .get(Constants.DRUID_SEGMENT_GRANULARITY);	} else {	throw new SemanticException("Druid storage handler used but not an INSERT, " + "CMVAS or CTAS statement");	}	segmentGranularity = !Strings.isNullOrEmpty(segmentGranularity) ? segmentGranularity : HiveConf.getVar(parseCtx.getConf(), HiveConf.ConfVars.HIVE_DRUID_INDEXING_GRANULARITY );	
sorted dynamic partitioning on time granularity optimization kicked in 

RowSchema selRS = new RowSchema(granularitySelOp.getSchema());	SelectDesc selConf = new SelectDesc(descs, colNames);	SelectOperator backtrackSelOp = (SelectOperator) OperatorFactory.getAndMakeChild( selConf, selRS, rsOp);	fsOp.getParentOperators().clear();	fsOp.getParentOperators().add(backtrackSelOp);	backtrackSelOp.getChildOperators().add(fsOp);	fsOp.getConf().setDpSortState(FileSinkDesc.DPSortState.PARTITION_SORTED);	fsOp.getConf().setPartitionCols(rsOp.getConf().getPartitionCols());	ColumnInfo ci = new ColumnInfo(granularitySelOp.getSchema().getSignature().get( granularitySelOp.getSchema().getSignature().size() - 1));	fsOp.getSchema().getSignature().add(ci);	
inserted and as parent of and child of 

========================= hive sample_3084 =========================

public static void startMetaStore(final int port, final HadoopThriftAuthBridge bridge, Configuration conf) throws Exception{	if (conf == null) {	conf = MetastoreConf.newMetastoreConf();	}	final Configuration finalConf = conf;	Thread thread = new Thread(new Runnable() {	public void run() {	try {	HiveMetaStore.startMetaStore(port, bridge, finalConf);	} catch (Throwable e) {	
metastore thrift server threw an exception 

socket.close();	return;	} catch (Exception e) {	if (retries++ > 60) {	exc = e;	break;	}	Thread.sleep(1000);	}	}	
unable to connect to metastore server 

socket.close();	return;	} catch (Exception e) {	if (retries++ > 60) {	exc = e;	break;	}	Thread.sleep(1000);	}	}	
printing all thread stack traces for debugging before throwing exception 

========================= hive sample_1543 =========================

public void run() {	try {	ScheduledExecutorService validatorExecutorService = Executors .newScheduledThreadPool(sessionTriggerProviders.size());	for (Map.Entry<String, SessionTriggerProvider> entry : sessionTriggerProviders.entrySet()) {	String poolName = entry.getKey();	if (!poolValidators.containsKey(poolName)) {	
creating trigger validator for pool 

ScheduledExecutorService validatorExecutorService = Executors .newScheduledThreadPool(sessionTriggerProviders.size());	for (Map.Entry<String, SessionTriggerProvider> entry : sessionTriggerProviders.entrySet()) {	String poolName = entry.getKey();	if (!poolValidators.containsKey(poolName)) {	TriggerValidatorRunnable poolValidator = new TriggerValidatorRunnable(entry.getValue(), triggerActionHandler);	validatorExecutorService.scheduleWithFixedDelay(poolValidator, triggerValidationIntervalMs, triggerValidationIntervalMs, TimeUnit.MILLISECONDS);	poolValidators.put(poolName, poolValidator);	}	}	} catch (Throwable t) {	
caught exception 

========================= hive sample_3987 =========================

private static boolean checkTextInputFormat(FileSystem fs, HiveConf conf, List<FileStatus> files) throws HiveException {	List<FileStatus> files2 = new LinkedList<>(files);	Iterator<FileStatus> iter = files2.iterator();	while (iter.hasNext()) {	FileStatus file = iter.next();	if (file == null) continue;	if (isPipe(fs, file)) {	
skipping format check for as it is a pipe 

}	int mode = 0;	Object pathToLog = file.getPath();	try {	java.nio.file.Path realPath = Paths.get(file.getPath().toUri());	pathToLog = realPath;	mode = (Integer)Files.getAttribute(realPath, "unix:mode");	} catch (FileSystemNotFoundException t) {	return false;	} catch (UnsupportedOperationException | IOException | SecurityException | IllegalArgumentException t) {	
failed to check mode for 

========================= hive sample_3600 =========================

private void registerDir(Path path, WatchedPathInfo watchedPathInfo) {	watchedPaths.put(path, watchedPathInfo);	try {	WatchKey watchKey = path.register(watchService, ENTRY_CREATE);	watchedPathInfo.setWatchKey(watchKey);	watchedPathQueue.add(watchedPathInfo);	if (watchedPathInfo.type == Type.FINAL) {	trackWatchForAttempt(watchedPathInfo, watchKey);	}	} catch (IOException e) {	
unable to setup watch for 

public void watch() {	while (!shutdown.get()) {	WatchKey watchKey;	try {	watchKey = watchService.take();	} catch (InterruptedException e) {	if (shutdown.get()) {	
shutting down watcher 

public void watch() {	while (!shutdown.get()) {	WatchKey watchKey;	try {	watchKey = watchService.take();	} catch (InterruptedException e) {	if (shutdown.get()) {	break;	} else {	
watcher interrupted before being shutdown 

resolvedPath = watchedPath.resolve(event.context());	watchedPathInfo = new WatchedPathInfo(parentWatchedPathInfo, Type.OUTPUT, null);	registerDir(resolvedPath, watchedPathInfo);	try (DirectoryStream<Path> dirStream = Files.newDirectoryStream(resolvedPath)) {	for (Path path : dirStream) {	if (path.toFile().isDirectory()) {	watchedPathInfo = new WatchedPathInfo(parentWatchedPathInfo, Type.FINAL, path.getFileName().toString());	registerDir(path, watchedPathInfo);	scanForFinalFiles(watchedPathInfo, path);	} else {	
ignoring unexpected file 

try (DirectoryStream<Path> dirStream = Files.newDirectoryStream(resolvedPath)) {	for (Path path : dirStream) {	if (path.toFile().isDirectory()) {	watchedPathInfo = new WatchedPathInfo(parentWatchedPathInfo, Type.FINAL, path.getFileName().toString());	registerDir(path, watchedPathInfo);	scanForFinalFiles(watchedPathInfo, path);	} else {	}	}	} catch (IOException e) {	
unable to list files under 

registerDir(path, watchedPathInfo);	scanForFinalFiles(watchedPathInfo, path);	} else {	}	}	} catch (IOException e) {	}	cancelledWatch = true;	watchKey.cancel();	} else {	
debug found unexpected directory while looking for output under 

watchedPathInfo = new WatchedPathInfo(parentWatchedPathInfo, Type.FINAL, event.context().getFileName().toString());	registerDir(resolvedPath, watchedPathInfo);	scanForFinalFiles(watchedPathInfo, resolvedPath);	break;	case FINAL: resolvedPath = watchedPath.resolve(event.context());	if (event.context().getFileName().toString().equals(ShuffleHandler.DATA_FILE_NAME)) {	registerFoundAttempt(parentWatchedPathInfo.pathIdentifier, null, resolvedPath);	} else if (event.context().getFileName().toString().equals(ShuffleHandler.INDEX_FILE_NAME)) {	registerFoundAttempt(parentWatchedPathInfo.pathIdentifier, resolvedPath, null);	} else {	
ignoring unexpected file 

} else if (event.context().getFileName().toString().equals(ShuffleHandler.INDEX_FILE_NAME)) {	registerFoundAttempt(parentWatchedPathInfo.pathIdentifier, resolvedPath, null);	} else {	}	break;	}	}	if (!cancelledWatch) {	boolean valid = watchKey.reset();	if (!valid) {	
debug watchkey no longer valid 

private void scanForFinalFiles(WatchedPathInfo watchedPathInfo, Path path) {	try (DirectoryStream<Path> dirStream = Files.newDirectoryStream(path) ) {	for (Path p : dirStream) {	if (p.getFileName().toString().equals(ShuffleHandler.DATA_FILE_NAME)) {	registerFoundAttempt(watchedPathInfo.pathIdentifier, null, path);	} else if (p.getFileName().toString().equals(ShuffleHandler.INDEX_FILE_NAME)) {	registerFoundAttempt(watchedPathInfo.pathIdentifier, path, null);	} else {	
ignoring unknown file 

try (DirectoryStream<Path> dirStream = Files.newDirectoryStream(path) ) {	for (Path p : dirStream) {	if (p.getFileName().toString().equals(ShuffleHandler.DATA_FILE_NAME)) {	registerFoundAttempt(watchedPathInfo.pathIdentifier, null, path);	} else if (p.getFileName().toString().equals(ShuffleHandler.INDEX_FILE_NAME)) {	registerFoundAttempt(watchedPathInfo.pathIdentifier, path, null);	} else {	}	}	} catch (IOException e) {	
unable to open dir stream for attemptdir 

public Void call() {	while (!shutdown.get()) {	WatchedPathInfo pathInfo;	try {	pathInfo = watchedPathQueue.take();	} catch (InterruptedException e) {	if (shutdown.get()) {	
shutting down watchexpirer 

public Void call() {	while (!shutdown.get()) {	WatchedPathInfo pathInfo;	try {	pathInfo = watchedPathQueue.take();	} catch (InterruptedException e) {	if (shutdown.get()) {	break;	} else {	
watchexpirer interrupted before being shutdown 

========================= hive sample_2154 =========================

public ReplicationV1CompatRule(IMetaStoreClient metaStoreClient, HiveConf hconf, List<String> testsToSkip){	this.metaStoreClient = metaStoreClient;	this.hconf = hconf;	testEventId = new ThreadLocal<Long>(){	protected Long initialValue(){	return getCurrentNotificationId();	}	};	this.testsToSkip = testsToSkip;	
backward compatibility tester initialized at 

public void doBackwardCompatibilityCheck(long testEventIdBefore, long testEventIdAfter){	long timeBefore = System.currentTimeMillis();	Map<NotificationEvent,RuntimeException> unhandledTasks = new LinkedHashMap<>();	Map<NotificationEvent,RuntimeException> incompatibleTasks = new LinkedHashMap<>();	int eventCount = 0;	
checking backward compatibility for events between 

unhandledTasks.put(ev, ((ErroredReplicationTask) rtask).getCause());	}	} catch (RuntimeException re){	incompatibleTasks.put(ev, re);	}	}	} catch (IOException e) {	assertNull("Got an exception when we shouldn't have - replv1 backward incompatibility issue:",e);	}	if (unhandledTasks.size() > 0){	
events found that would not be coverable by replication 

} catch (RuntimeException re){	incompatibleTasks.put(ev, re);	}	}	} catch (IOException e) {	assertNull("Got an exception when we shouldn't have - replv1 backward incompatibility issue:",e);	}	if (unhandledTasks.size() > 0){	for (NotificationEvent ev : unhandledTasks.keySet()){	RuntimeException re = unhandledTasks.get(ev);	
erroredreplicationtask encountered new event type does not correspond to a task 

}	} catch (IOException e) {	assertNull("Got an exception when we shouldn't have - replv1 backward incompatibility issue:",e);	}	if (unhandledTasks.size() > 0){	for (NotificationEvent ev : unhandledTasks.keySet()){	RuntimeException re = unhandledTasks.get(ev);	}	}	if (incompatibleTasks.size() > 0){	
events found that caused errors in replication 

assertNull("Got an exception when we shouldn't have - replv1 backward incompatibility issue:",e);	}	if (unhandledTasks.size() > 0){	for (NotificationEvent ev : unhandledTasks.keySet()){	RuntimeException re = unhandledTasks.get(ev);	}	}	if (incompatibleTasks.size() > 0){	for (NotificationEvent ev : incompatibleTasks.keySet()){	RuntimeException re = incompatibleTasks.get(ev);	
runtimeexception encountered new event type caused a break 

public Statement apply(Statement statement, Description description) {	return new Statement() {	public void evaluate() throws Throwable {	Long prevNotificationId = getCurrentNotificationId();	statement.evaluate();	Long currNotificationId = getCurrentNotificationId();	if(!testsToSkip.contains(description.getMethodName())){	doBackwardCompatibilityCheck(prevNotificationId,currNotificationId);	} else {	
skipping backward compatibility check as requested for test 

========================= hive sample_865 =========================

SessionState ss = new SessionState(conf);	SessionState.start(ss);	ss = SessionState.get();	File dist = null;	try {	dist = new File(reloadFolder.getAbsolutePath() + File.separator + reloadClazzFileName);	Files.copy(new File(HiveTestUtils.getFileFromClasspath(clazzDistFileName)), dist);	ss.loadReloadableAuxJars();	Assert.assertEquals("version1", getReloadedClazzVersion(ss.getConf().getClassLoader()));	} catch (Exception e) {	
reload auxiliary jar test fail with message 

ss.loadReloadableAuxJars();	Assert.assertEquals("version1", getReloadedClazzVersion(ss.getConf().getClassLoader()));	} catch (Exception e) {	Assert.fail(e.getMessage());	} finally {	FileUtils.deleteQuietly(dist);	try {	ss.close();	} catch (IOException ioException) {	Assert.fail(ioException.getMessage());	
fail to close the created session 

public void testReloadExistingAuxJars2() {	HiveConf conf = new HiveConf();	HiveConf.setVar(conf, ConfVars.HIVERELOADABLEJARS, hiveReloadPath);	SessionState ss = new SessionState(conf);	SessionState.start(ss);	File dist = null;	try {	ss = SessionState.get();	
copy jar file 

HiveConf.setVar(conf, ConfVars.HIVERELOADABLEJARS, hiveReloadPath);	SessionState ss = new SessionState(conf);	SessionState.start(ss);	File dist = null;	try {	ss = SessionState.get();	dist = new File(reloadFolder.getAbsolutePath() + File.separator + reloadClazzFileName);	Files.copy(new File(HiveTestUtils.getFileFromClasspath(clazzDistFileName)), dist);	ss.loadReloadableAuxJars();	Assert.assertEquals("version1", getReloadedClazzVersion(ss.getConf().getClassLoader()));	
copy jar file 

Files.copy(new File(HiveTestUtils.getFileFromClasspath(clazzDistFileName)), dist);	ss.loadReloadableAuxJars();	Assert.assertEquals("version1", getReloadedClazzVersion(ss.getConf().getClassLoader()));	FileUtils.deleteQuietly(dist);	Files.copy(new File(HiveTestUtils.getFileFromClasspath(clazzV2FileName)), dist);	ss.loadReloadableAuxJars();	Assert.assertEquals("version2", getReloadedClazzVersion(ss.getConf().getClassLoader()));	FileUtils.deleteQuietly(dist);	ss.loadReloadableAuxJars();	} catch (Exception e) {	
refresh existing jar file case failed with message 

FileUtils.deleteQuietly(dist);	ss.loadReloadableAuxJars();	} catch (Exception e) {	Assert.fail(e.getMessage());	} finally {	FileUtils.deleteQuietly(dist);	try {	ss.close();	} catch (IOException ioException) {	Assert.fail(ioException.getMessage());	
fail to close the created session 

========================= hive sample_2787 =========================

public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {	String opId = (String) request.getParameter("operationId");	ServletContext ctx = getServletContext();	SessionManager sessionManager = (SessionManager)ctx.getAttribute("hive.sm");	OperationManager opManager = sessionManager.getOperationManager();	QueryInfo queryInfo = opManager.getQueryInfo(opId);	HiveConf hiveConf = opManager.getHiveConf();	if (queryInfo == null) {	
no display object found for operation 

========================= hive sample_2292 =========================

public static void addDependencyJars(Configuration conf, List<Class<?>> classes) throws IOException {	FileSystem localFs = FileSystem.getLocal(conf);	Set<String> jars = new HashSet<String>();	jars.addAll(conf.getStringCollection("tmpjars"));	Map<String,String> packagedClasses = new HashMap<String,String>();	for (Class<?> clazz : classes) {	if (clazz == null) continue;	Path path = findOrCreateJar(clazz, localFs, packagedClasses);	if (path == null) {	
could not find jar for class in order to ship it to the cluster 

Set<String> jars = new HashSet<String>();	jars.addAll(conf.getStringCollection("tmpjars"));	Map<String,String> packagedClasses = new HashMap<String,String>();	for (Class<?> clazz : classes) {	if (clazz == null) continue;	Path path = findOrCreateJar(clazz, localFs, packagedClasses);	if (path == null) {	continue;	}	if (!localFs.exists(path)) {	
could not validate jar file for class 

private static Path findOrCreateJar(Class<?> my_class, FileSystem fs, Map<String,String> packagedClasses) throws IOException {	String jar = findContainingJar(my_class, packagedClasses);	if (null == jar || jar.isEmpty()) {	jar = getJar(my_class);	updateMap(jar, packagedClasses);	}	if (null == jar || jar.isEmpty()) {	return null;	}	
for class s using jar s 

private static String getJar(Class<?> my_class) {	String ret = null;	String hadoopJarFinder = "org.apache.hadoop.util.JarFinder";	Class<?> jarFinder = null;	try {	
looking for 

private static String getJar(Class<?> my_class) {	String ret = null;	String hadoopJarFinder = "org.apache.hadoop.util.JarFinder";	Class<?> jarFinder = null;	try {	jarFinder = JavaUtils.loadClass(hadoopJarFinder);	
found 

private static String getJar(Class<?> my_class) {	String ret = null;	String hadoopJarFinder = "org.apache.hadoop.util.JarFinder";	Class<?> jarFinder = null;	try {	jarFinder = JavaUtils.loadClass(hadoopJarFinder);	Method getJar = jarFinder.getMethod("getJar", Class.class);	ret = (String) getJar.invoke(null, my_class);	} catch (ClassNotFoundException e) {	
using backported jarfinder 

========================= hive sample_189 =========================

}	String actualDbName = context.isDbNameEmpty() ? nns.get(0).getTable_db() : context.dbName;	String actualTblName = context.isTableNameEmpty() ? nns.get(0).getTable_name() : context.tableName;	for (SQLNotNullConstraint nn : nns) {	nn.setTable_db(actualDbName);	nn.setTable_name(actualTblName);	}	AlterTableDesc addConstraintsDesc = new AlterTableDesc(actualDbName + "." + actualTblName, new ArrayList<SQLPrimaryKey>(), new ArrayList<SQLForeignKey>(), new ArrayList<SQLUniqueConstraint>(), nns, context.eventOnlyReplicationSpec());	Task<DDLWork> addConstraintsTask = TaskFactory.get(new DDLWork(readEntitySet, writeEntitySet, addConstraintsDesc), context.hiveConf);	tasks.add(addConstraintsTask);	
added add constrains task 

========================= hive sample_3483 =========================

public void analyzeInternal(ASTNode ast) throws SemanticException {	if (ast.getType() == HiveParser.TOK_CREATEFUNCTION) {	analyzeCreateFunction(ast);	} else if (ast.getType() == HiveParser.TOK_DROPFUNCTION) {	analyzeDropFunction(ast);	} else if (ast.getType() == HiveParser.TOK_RELOADFUNCTION) {	rootTasks.add(TaskFactory.get(new FunctionWork(new ReloadFunctionDesc()), conf));	}	
analyze done 

private void analyzeCreateFunction(ASTNode ast) throws SemanticException {	String functionName = ast.getChild(0).getText().toLowerCase();	boolean isTemporaryFunction = (ast.getFirstChildWithType(HiveParser.TOK_TEMPORARY) != null);	String className = unescapeSQLString(ast.getChild(1).getText());	if (isTemporaryFunction && FunctionUtils.isQualifiedFunctionName(functionName)) {	throw new SemanticException("Temporary function cannot be created with a qualified name.");	}	List<ResourceUri> resources = getResourceList(ast);	if (!isTemporaryFunction && resources == null) {	
permanent functions created without using clause will not be replicated 

private void addEntities(String functionName, String className, boolean isTemporaryFunction, List<ResourceUri> resources) throws SemanticException {	Database database = null;	if (!isTemporaryFunction) {	try {	String[] qualifiedNameParts = FunctionUtils.getQualifiedFunctionNameParts(functionName);	String dbName = qualifiedNameParts[0];	functionName = qualifiedNameParts[1];	database = getDatabase(dbName);	} catch (HiveException e) {	
failed to get database 

========================= hive sample_3529 =========================

public HiveSiteHS2ConnectionFileParser() {	hiveSiteURI = HiveConf.getHiveSiteLocation();	conf = new Configuration();	if(hiveSiteURI == null) {	
hive site xml not found for constructing the connection url 

public HiveSiteHS2ConnectionFileParser() {	hiveSiteURI = HiveConf.getHiveSiteLocation();	conf = new Configuration();	if(hiveSiteURI == null) {	} else {	
using hive site xml at 

========================= hive sample_1514 =========================

needHashTableSetup = false;	}	batchCounter++;	innerPerBatchSetup(batch);	for(VectorExpression ve : bigTableFilterExpressions) {	ve.evaluate(batch);	}	final int inputLogicalSize = batch.size;	if (inputLogicalSize == 0) {	if (LOG.isDebugEnabled()) {	
batch empty 

keyVectorSerializeWrite.serializeWrite(batch, 0);	JoinUtil.JoinResult joinResult;	if (keyVectorSerializeWrite.getHasAnyNulls()) {	joinResult = JoinUtil.JoinResult.NOMATCH;	} else {	byte[] keyBytes = currentKeyOutput.getData();	int keyLength = currentKeyOutput.getLength();	joinResult = hashMap.lookup(keyBytes, 0, keyLength, hashMapResults[0]);	}	if (LOG.isDebugEnabled()) {	
batch repeated joinresult 

} else {	byte[] keyBytes = currentKeyOutput.getData();	int keyLength = currentKeyOutput.getLength();	joinResult = hashMap.lookup(keyBytes, 0, keyLength, hashMapResults[0]);	}	if (LOG.isDebugEnabled()) {	}	finishInnerRepeated(batch, joinResult, hashMapResults[0]);	} else {	if (LOG.isDebugEnabled()) {	
batch non repeated 

switch (saveJoinResult) {	case MATCH: hashMapResultCount++;	equalKeySeriesCount++;	break;	case SPILL: hashMapResultCount++;	break;	case NOMATCH: break;	}	}	if (LOG.isDebugEnabled()) {	
allmatchs equalkeyserieshashmapresultindices equalkeyseriesallmatchindices equalkeyseriesissinglevalue equalkeyseriesduplicatecounts spills spillhashmapresultindices hashmapresults 

========================= hive sample_4151 =========================

} else if (key.equals("tagToInput:")) {	JSONObject tagToInput = vertexObject.getJSONObject(key);	for (String tag : JSONObject.getNames(tagToInput)) {	this.tagToInput.put(tag, (String) tagToInput.get(tag));	}	} else if (key.equals("tag:")) {	this.tag = vertexObject.getString(key);	} else if (key.equals("Local Work:")) {	extractOp(vertexObject.getJSONObject(key), null);	} else {	
skip unsupported in vertex 

========================= hive sample_1353 =========================

private <Intermediate, Final> void inner_go( final Request request, final HttpResponseHandler<Intermediate, Final> httpResponseHandler, final Duration duration, final SettableFuture<Final> future ) {	try {	final String host = request.getUrl().getHost();	final URI uri = request.getUrl().toURI();	The goal us to avoid doing KDC requests for every request.*/ Map<String, List<String>> cookieMap = cookieManager.get(uri, Collections.<String, List<String>>emptyMap());	for (Map.Entry<String, List<String>> entry : cookieMap.entrySet()) {	request.addHeaderValues(entry.getKey(), entry.getValue());	}	final boolean should_retry_on_unauthorized_response;	if (DruidKerberosUtil.needToSendCredentials(cookieManager.getCookieStore(), uri)) {	
no auth cookie found for uri existing cookies authenticating 

final String host = request.getUrl().getHost();	final URI uri = request.getUrl().toURI();	The goal us to avoid doing KDC requests for every request.*/ Map<String, List<String>> cookieMap = cookieManager.get(uri, Collections.<String, List<String>>emptyMap());	for (Map.Entry<String, List<String>> entry : cookieMap.entrySet()) {	request.addHeaderValues(entry.getKey(), entry.getValue());	}	final boolean should_retry_on_unauthorized_response;	if (DruidKerberosUtil.needToSendCredentials(cookieManager.getCookieStore(), uri)) {	UserGroupInformation currentUser = UserGroupInformation.getCurrentUser();	currentUser.checkTGTAndReloginFromKeytab();	
the user credential is 

});	request.setHeader(HttpHeaders.Names.AUTHORIZATION, "Negotiate " + challenge);	should_retry_on_unauthorized_response = false;	} else {	therefore we need to resend a valid Kerberos challenge*/ should_retry_on_unauthorized_response = true;	}	ListenableFuture<RetryResponseHolder<Final>> internalFuture = delegate.go( request, new RetryIfUnauthorizedResponseHandler<Intermediate, Final>(new ResponseCookieHandler( request.getUrl().toURI(), cookieManager, httpResponseHandler )), duration );	Futures.addCallback(internalFuture, new FutureCallback<RetryResponseHolder<Final>>() {	public void onSuccess(RetryResponseHolder<Final> result) {	if (should_retry_on_unauthorized_response && result.shouldRetry()) {	
preparing for retry 

therefore we need to resend a valid Kerberos challenge*/ should_retry_on_unauthorized_response = true;	}	ListenableFuture<RetryResponseHolder<Final>> internalFuture = delegate.go( request, new RetryIfUnauthorizedResponseHandler<Intermediate, Final>(new ResponseCookieHandler( request.getUrl().toURI(), cookieManager, httpResponseHandler )), duration );	Futures.addCallback(internalFuture, new FutureCallback<RetryResponseHolder<Final>>() {	public void onSuccess(RetryResponseHolder<Final> result) {	if (should_retry_on_unauthorized_response && result.shouldRetry()) {	DruidKerberosUtil.removeAuthCookie(cookieManager.getCookieStore(), uri);	request.setHeader("Cookie", "");	inner_go(request.copy(), httpResponseHandler, duration, future);	} else {	
not retrying and returning future response 

========================= hive sample_14 =========================

private void setupSessionIO(SessionState sessionState) {	try {	
putting temp output to file and error output to file 

private void setupSessionIO(SessionState sessionState) {	try {	sessionState.in = null;	sessionState.out = new PrintStream(new FileOutputStream(sessionState.getTmpOutputFile()), true, CharEncoding.UTF_8);	sessionState.err = new PrintStream(new FileOutputStream(sessionState.getTmpErrOutputFile()), true,CharEncoding.UTF_8);	} catch (IOException e) {	
error in creating temp output file 

sessionState.out = new PrintStream(new FileOutputStream(sessionState.getTmpOutputFile()), true, CharEncoding.UTF_8);	sessionState.err = new PrintStream(new FileOutputStream(sessionState.getTmpErrOutputFile()), true,CharEncoding.UTF_8);	} catch (IOException e) {	ServiceUtils.cleanup(LOG, parentSession.getSessionState().out, parentSession.getSessionState().err);	closeSessionStreams = false;	try {	sessionState.in = null;	sessionState.out = new PrintStream(System.out, true, CharEncoding.UTF_8);	sessionState.err = new PrintStream(System.err, true, CharEncoding.UTF_8);	} catch (UnsupportedEncodingException ee) {	
error creating printstream 

private List<String> readResults(int nLines) throws HiveSQLException {	if (resultReader == null) {	SessionState sessionState = getParentSession().getSessionState();	File tmp = sessionState.getTmpOutputFile();	try {	resultReader = new BufferedReader(new FileReader(tmp));	} catch (FileNotFoundException e) {	
file not found 

List<String> results = new ArrayList<String>();	for (int i = 0; i < nLines || nLines <= 0; ++i) {	try {	String line = resultReader.readLine();	if (line == null) {	break;	} else {	results.add(line);	}	} catch (IOException e) {	
reading temp results encountered an exception 

========================= hive sample_2343 =========================

public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx, Object... nodeOutputs) throws SemanticException {	if (pGraphContext.isReduceSinkAddedBySortedDynPartition()) {	
reduce sink is added by sorted dynamic partition optimizer bailing out of bucketing sorting reduce sink optimizer 

========================= hive sample_3047 =========================

int scale = decimalTypeInfo.getScale();	decimalIsNull = !currentHiveDecimalWritable.mutateEnforcePrecisionScale(precision, scale);	if (!decimalIsNull) {	if (field.dataTypePhysicalVariation == DataTypePhysicalVariation.DECIMAL_64) {	currentDecimal64 = currentHiveDecimalWritable.serialize64(scale);	}	return true;	}	}	if (LOG.isDebugEnabled()) {	
data not in the hivedecimal data type range so converted to null given data is 

========================= hive sample_5469 =========================

childOperatorTags.add(index);	}	index++;	}	} else {	childOperatorTags.add(child.getParentOperators().indexOf(this));	}	newChildOperatorsTag[i] = toArray(childOperatorTags);	}	if (LOG.isInfoEnabled()) {	
newchildoperatorstag 

protected void initializeChildren(Configuration hconf) throws HiveException {	state = State.INIT;	if (LOG.isInfoEnabled()) {	
operator initialized 

protected void initializeChildren(Configuration hconf) throws HiveException {	state = State.INIT;	if (LOG.isInfoEnabled()) {	
initializing children of 

protected void initializeChildren(Configuration hconf) throws HiveException {	state = State.INIT;	if (LOG.isInfoEnabled()) {	}	for (int i = 0; i < childOperatorsArray.length; i++) {	if (LOG.isInfoEnabled()) {	
initializing child 

}	} else {	continue;	}	if (reporter != null) {	childOperatorsArray[i].setReporter(reporter);	}	}	for (int i = 0; i < childOperatorsArray.length; i++) {	if (LOG.isInfoEnabled()) {	
initializing child 

========================= hive sample_3944 =========================

private boolean initialize(String type) {	ClassLoader classLoader = Utilities.getSessionSpecifiedClassLoader();	try {	StatDB statDB = StatDB.valueOf(type);	publisherImplementation = (Class<? extends Serializable>) Class.forName(statDB.getPublisher(jobConf), true, classLoader);	aggregatorImplementation = (Class<? extends Serializable>) Class.forName(statDB.getAggregator(jobConf), true, classLoader);	} catch (Exception e) {	
publisher aggregator classes cannot be loaded 

========================= hive sample_5039 =========================

public void testVersionRestriction () throws Exception {	System.setProperty(HiveConf.ConfVars.METASTORE_SCHEMA_VERIFICATION.toString(), "true");	hiveConf = new HiveConf(this.getClass());	assertTrue(hiveConf.getBoolVar(HiveConf.ConfVars.METASTORE_SCHEMA_VERIFICATION));	assertFalse(hiveConf.getBoolVar(HiveConf.ConfVars.METASTORE_AUTO_CREATE_ALL));	try {	SessionState.start(new CliSessionState(hiveConf));	Hive.get(hiveConf).getMSC();	fail("An exception is expected since schema is not created.");	} catch (Exception re) {	
exception in testversionrestriction 

========================= hive sample_263 =========================

int current = 0;	for(int i = 0; i < numPartition - 1; i++) {	current += Math.round((float)(sorted.length - current) / (numPartition - i));	while (i > 0 && current < sorted.length && C.compare(sorted[last], sorted[current]) == 0) {	current++;	}	if (current >= sorted.length) {	return Arrays.copyOfRange(partitionKeys, 0, i);	}	if (LOG.isDebugEnabled()) {	
partition key th 

========================= hive sample_4043 =========================

public StructObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {	
initializing genericudfgetsplits 

public StructObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {	if (SessionState.get() == null || SessionState.get().getConf() == null) {	throw new IllegalStateException("Cannot run get splits outside HS2");	}	
initialized conf jc and metastore connection 

public StructObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {	if (SessionState.get() == null || SessionState.get().getConf() == null) {	throw new IllegalStateException("Cannot run get splits outside HS2");	}	if (arguments.length != 2) {	throw new UDFArgumentLengthException( "The function GET_SPLITS accepts 2 arguments.");	} else if (!(arguments[0] instanceof StringObjectInspector)) {	
got instead of string 

public StructObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {	if (SessionState.get() == null || SessionState.get().getConf() == null) {	throw new IllegalStateException("Cannot run get splits outside HS2");	}	if (arguments.length != 2) {	throw new UDFArgumentLengthException( "The function GET_SPLITS accepts 2 arguments.");	} else if (!(arguments[0] instanceof StringObjectInspector)) {	throw new UDFArgumentTypeException(0, "\"" + "string\" is expected at function GET_SPLITS, " + "but \"" + arguments[0].getTypeName() + "\" is found");	} else if (!(arguments[1] instanceof IntObjectInspector)) {	
got instead of int 

} else if (!(arguments[0] instanceof StringObjectInspector)) {	throw new UDFArgumentTypeException(0, "\"" + "string\" is expected at function GET_SPLITS, " + "but \"" + arguments[0].getTypeName() + "\" is found");	} else if (!(arguments[1] instanceof IntObjectInspector)) {	throw new UDFArgumentTypeException(1, "\"" + "int\" is expected at function GET_SPLITS, " + "but \"" + arguments[1].getTypeName() + "\" is found");	}	stringOI = (StringObjectInspector) arguments[0];	intOI = (IntObjectInspector) arguments[1];	List<String> names = Arrays.asList("split");	List<ObjectInspector> fieldOIs = Arrays .<ObjectInspector> asList(PrimitiveObjectInspectorFactory.javaByteArrayObjectInspector);	StructObjectInspector outputOI = ObjectInspectorFactory .getStandardStructObjectInspector(names, fieldOIs);	
done initializing genericudfgetsplits 

public void process(Object[] arguments) throws HiveException {	String query = stringOI.getPrimitiveJavaObject(arguments[0]);	int num = intOI.get(arguments[1]);	LlapCoordinator coordinator = LlapCoordinator.getInstance();	if (coordinator == null) {	throw new HiveException("LLAP coordinator is not initialized; must be running in HS2 with " + ConfVars.LLAP_HS2_ENABLE_COORDINATOR.varname + " enabled");	}	ApplicationId applicationId = coordinator.createExtClientAppId();	
generated appid for llap splits 

QueryPlan plan = driver.getPlan();	List<Task<?>> roots = plan.getRootTasks();	Schema schema = convertSchema(plan.getResultSchema());	if (roots == null || roots.size() != 1 || !(roots.get(0) instanceof TezTask)) {	throw new HiveException("Was expecting a single TezTask.");	}	TezWork tezWork = ((TezTask)roots.get(0)).getWork();	if (tezWork.getAllWork().size() != 1) {	String tableName = "table_"+UUID.randomUUID().toString().replaceAll("[^A-Za-z0-9 ]", "");	String ctas = "create temporary table " + tableName + " as " + query;	
materializing the query for llapif ctas 

}	String llapUser = UserGroupInformation.getLoginUser().getShortUserName();	String queryUser = null;	byte[] tokenBytes = null;	LlapSigner signer = null;	if (UserGroupInformation.isSecurityEnabled()) {	signer = coordinator.getLlapSigner(job);	queryUser = SessionState.getUserFromAuthenticator();	if (queryUser == null) {	queryUser = UserGroupInformation.getCurrentUser().getUserName();	
cannot determine the session user using instead 

byte[] tokenBytes = null;	LlapSigner signer = null;	if (UserGroupInformation.isSecurityEnabled()) {	signer = coordinator.getLlapSigner(job);	queryUser = SessionState.getUserFromAuthenticator();	if (queryUser == null) {	queryUser = UserGroupInformation.getCurrentUser().getUserName();	}	LlapTokenLocalClient tokenClient = coordinator.getLocalTokenClient(job, llapUser);	Token<LlapTokenIdentifier> token = tokenClient.createToken( applicationId.toString(), queryUser, true);	
created the token for remote user 

}	LlapTokenLocalClient tokenClient = coordinator.getLocalTokenClient(job, llapUser);	Token<LlapTokenIdentifier> token = tokenClient.createToken( applicationId.toString(), queryUser, true);	bos.reset();	token.write(dos);	tokenBytes = bos.toByteArray();	} else {	queryUser = UserGroupInformation.getCurrentUser().getUserName();	}	Token<JobTokenIdentifier> umbilicalToken = JobTokenCreator.createJobToken(applicationId);	
number of splits 

public void close() throws IOException {	try {	
drivercleanup for llap splits 

public void close() throws IOException {	try {	driver.releaseLocksAndCommitOrRollback(true);	driver.close();	driver.destroy();	txnManager.closeTxnManager();	} catch (Exception err) {	
error closing driver resources 

private SplitLocationInfo[] makeLocationHints(TaskLocationHint hint) {	Set<String> hosts = hint.getHosts();	if (hosts.size() != 1) {	
bad of locations 

========================= hive sample_4940 =========================

private void dedupWithTableInfo() {	assert tableInfo != null : "TableInfo can't be null at this point.";	if (partitionSchema != null) {	if (partitionSchema.equals(tableInfo.getDataColumns())) {	partitionSchema = null;	} else {	if (LOG.isDebugEnabled()) {	
can t suppress data schema partition schema and table schema seem to differ partitionschema tableschema 

} else {	if (LOG.isDebugEnabled()) {	}	}	}	if (storageHandlerClassName != null) {	if (storageHandlerClassName.equals(tableInfo.getStorerInfo().getStorageHandlerClass())) {	storageHandlerClassName = null;	} else {	if (LOG.isDebugEnabled()) {	
partition s storagehandler differs from table s storagehandler 

} else {	if (LOG.isDebugEnabled()) {	}	}	}	if (inputFormatClassName != null) {	if (inputFormatClassName.equals(tableInfo.getStorerInfo().getIfClass())) {	inputFormatClassName = null;	} else {	if (LOG.isDebugEnabled()) {	
partition s inputformat differs from table s inputformat 

} else {	if (LOG.isDebugEnabled()) {	}	}	}	if (outputFormatClassName != null) {	if (outputFormatClassName.equals(tableInfo.getStorerInfo().getOfClass())) {	outputFormatClassName = null;	} else {	if (LOG.isDebugEnabled()) {	
partition s outputformat differs from table s outputformat 

} else {	if (LOG.isDebugEnabled()) {	}	}	}	if (serdeClassName != null) {	if (serdeClassName.equals(tableInfo.getStorerInfo().getSerdeClass())) {	serdeClassName = null;	} else {	if (LOG.isDebugEnabled()) {	
partition s serde differs from table s serde 

========================= hive sample_776 =========================

private void keyComplete(String key) {	Preconditions.checkNotNull(key, "Key must be specified");	boolean removed = knownAppenders.remove(key);	if (removed) {	if (LOGGER.isDebugEnabled()) {	
deleting appender for key 

private void keyComplete(String key) {	Preconditions.checkNotNull(key, "Key must be specified");	boolean removed = knownAppenders.remove(key);	if (removed) {	if (LOGGER.isDebugEnabled()) {	}	routingAppender.deleteAppender(key);	} else {	
ignoring call to remove unknown key 

public static PurgePolicy createPurgePolicy( @PluginAttribute("name") final String name) {	Preconditions.checkNotNull(name, "Name must be specified for " + LlapRoutingAppenderPurgePolicy.class.getName());	LlapRoutingAppenderPurgePolicy llapRoutingAppenderPurgePolicy = new LlapRoutingAppenderPurgePolicy(name);	LlapRoutingAppenderPurgePolicy old = INSTANCES.putIfAbsent(name, llapRoutingAppenderPurgePolicy);	if (old != null) {	
attempt to create multiple instances of with the name using original instance 

========================= hive sample_2251 =========================

public Void call() throws Exception {	try {	cacheMetadata(type, location);	} catch (InterruptedException ex) {	Thread.currentThread().interrupt();	} catch (Exception ex) {	
failed to cache file metadata in background for 

if (lfs.isDirectory()) continue;	files.add(lfs.getPath());	}	}	for (Path file : files) {	long fileId;	if (!(fs instanceof DistributedFileSystem)) return;	try {	fileId = HdfsUtils.getFileId(fs, Path.getPathWithoutSchemeAndAuthority(file).toString());	} catch (UnsupportedOperationException ex) {	
cannot cache file metadata for does not support fileid 

}	}	for (Path file : files) {	long fileId;	if (!(fs instanceof DistributedFileSystem)) return;	try {	fileId = HdfsUtils.getFileId(fs, Path.getPathWithoutSchemeAndAuthority(file).toString());	} catch (UnsupportedOperationException ex) {	return;	}	
caching file metadata for file id 

========================= hive sample_1884 =========================

String uniqueKey = BOOTSTRAP_DUMP_STATE_KEY_PREFIX + UUID.randomUUID().toString();	newParams.put(uniqueKey, ReplDumpState.ACTIVE.name());	Map<String, String> params = database.getParameters();	if (params != null) {	params.putAll(newParams);	database.setParameters(params);	} else {	database.setParameters(newParams);	}	hiveDb.alterDatabase(dbName, database);	
repl dump set property for database property value 

public static void resetDbBootstrapDumpState(Hive hiveDb, String dbName, String uniqueKey) throws HiveException {	Database database = hiveDb.getDatabase(dbName);	if (database != null) {	Map<String, String> params = database.getParameters();	if ((params != null) && params.containsKey(uniqueKey)) {	params.remove(uniqueKey);	database.setParameters(params);	hiveDb.alterDatabase(dbName, database);	
repl dump reset property for database property 

public static boolean shouldReplicate(NotificationEvent tableForEvent, ReplicationSpec replicationSpec, Hive db, HiveConf hiveConf) {	Table table;	try {	table = db.getTable(tableForEvent.getDbName(), tableForEvent.getTableName());	} catch (HiveException e) {	
error while getting table info for 

========================= hive sample_3440 =========================

public void run() {	while (!stop) {	try {	FileSystem fs=null;	try {	fs = new Path(storage_root).getFileSystem(appConf);	checkFiles(fs);	} catch (Exception e) {	
cleanup cycle failed 

try {	fs = new Path(storage_root).getFileSystem(appConf);	checkFiles(fs);	} catch (Exception e) {	} finally {	if(fs != null) {	try {	fs.close();	}	catch (Exception e) {	
closing file system failed 

} finally {	if(fs != null) {	try {	fs.close();	}	catch (Exception e) {	}	}	}	long sleepMillis = (long) (Math.random() * interval);	
next execution 

fs.close();	}	catch (Exception e) {	}	}	}	long sleepMillis = (long) (Math.random() * interval);	Thread.sleep(sleepMillis);	} catch (Exception e) {	isRunning = false;	
cleanup failed 

private void checkFiles(FileSystem fs) throws IOException {	long now = new Date().getTime();	for (Type type : Type.values()) {	try {	for (FileStatus status : fs.listStatus(new Path( HDFSStorage.getPath(type, storage_root)))) {	if (now - status.getModificationTime() > maxage) {	
deleting 

========================= hive sample_849 =========================

public LlapTaskCommunicator( TaskCommunicatorContext taskCommunicatorContext) {	super(taskCommunicatorContext);	Credentials credentials = taskCommunicatorContext.getAMCredentials();	if (credentials != null) {	Token<LlapTokenIdentifier> llapToken = (Token<LlapTokenIdentifier>)credentials.getToken(LlapTokenIdentifier.KIND_NAME);	this.token = llapToken;	} else {	this.token = null;	}	if (LOG.isInfoEnabled()) {	
task communicator with a token 

private boolean processSendError(Throwable t) {	Throwable cause = t;	while (cause != null) {	if (cause instanceof RetriableException) return false;	if (((cause instanceof InvalidToken && cause.getMessage() != null) || (cause instanceof RemoteException && cause.getCause() == null && cause.getMessage() != null && cause.getMessage().contains("InvalidToken"))) && cause.getMessage().contains(LLAP_TOKEN_NAME)) {	break;	}	cause = cause.getCause();	}	if (cause == null) return false;	
reporting fatal error llap token appears to be invalid 

public void registerContainerEnd(ContainerId containerId, ContainerEndReason endReason, String diagnostics) {	super.registerContainerEnd(containerId, endReason, diagnostics);	if (endReason == ContainerEndReason.INTERNAL_PREEMPTION) {	
processing containerend for container caused by internal preemption 

public <T> void startUpdateGuaranteed(TezTaskAttemptID attemptId, NodeInfo assignedNode, boolean newState, final OperationCallback<Boolean, T> callback, final T ctx) {	LlapNodeId nodeId = entityTracker.getNodeIdForTaskAttempt(attemptId);	if (nodeId == null) {	if (assignedNode != null) {	nodeId = LlapNodeId.getInstance(assignedNode.getHost(), assignedNode.getRpcPort());	}	
untracked node for nodeinfo points to 

callback.setDone(ctx, false);	return;	}	}	UpdateFragmentRequestProto request = UpdateFragmentRequestProto.newBuilder() .setIsGuaranteed(newState).setFragmentIdentifierString(attemptId.toString()) .setQueryIdentifier(constructQueryIdentifierProto( attemptId.getTaskID().getVertexID().getDAGId().getId())).build();	communicator.sendUpdateFragment(request, nodeId.getHostname(), nodeId.getPort(), new LlapProtocolClientProxy.ExecuteRequestCallback<UpdateFragmentResponseProto>() {	public void setResponse(UpdateFragmentResponseProto response) {	callback.setDone(ctx, response.getResult());	}	public void indicateError(Throwable t) {	
failed to send update fragment request for 

requestProto = constructSubmitWorkRequest(containerId, taskSpec, fragmentRuntimeInfo, currentHiveQueryId);	} catch (IOException e) {	throw new RuntimeException("Failed to construct request", e);	}	getContext().taskStartedRemotely(taskSpec.getTaskAttemptID(), containerId);	communicator.sendSubmitWork(requestProto, host, port, new LlapProtocolClientProxy.ExecuteRequestCallback<SubmitWorkResponseProto>() {	public void setResponse(SubmitWorkResponseProto response) {	if (response.hasSubmissionState()) {	LlapDaemonProtocolProtos.SubmissionStateProto ss = response.getSubmissionState();	if (ss.equals(LlapDaemonProtocolProtos.SubmissionStateProto.REJECTED)) {	
unable to run task on containerid service busy 

if (ss.equals(LlapDaemonProtocolProtos.SubmissionStateProto.REJECTED)) {	getContext().taskKilled(taskSpec.getTaskAttemptID(), TaskAttemptEndReason.EXECUTOR_BUSY, "Service Busy");	return;	}	} else {	throw new RuntimeException("SubmissionState in response is expected!");	}	if (response.hasUniqueNodeId()) {	entityTracker.registerTaskSubmittedToNode( taskSpec.getTaskAttemptID(), response.getUniqueNodeId());	}	
successfully launched task 

}	scheduler.notifyStarted(taskSpec.getTaskAttemptID());	}	public void indicateError(Throwable t) {	Throwable originalError = t;	if (t instanceof ServiceException) {	ServiceException se = (ServiceException) t;	t = se.getCause();	}	if (t instanceof RemoteException) {	
failed to run task on containerid 

Throwable originalError = t;	if (t instanceof ServiceException) {	ServiceException se = (ServiceException) t;	t = se.getCause();	}	if (t instanceof RemoteException) {	processSendError(originalError);	getContext() .taskFailed(taskSpec.getTaskAttemptID(), TaskFailureType.NON_FATAL, TaskAttemptEndReason.OTHER, t.toString());	} else {	if (t instanceof IOException) {	
unable to run task on containerid communication error 

t = se.getCause();	}	if (t instanceof RemoteException) {	processSendError(originalError);	getContext() .taskFailed(taskSpec.getTaskAttemptID(), TaskFailureType.NON_FATAL, TaskAttemptEndReason.OTHER, t.toString());	} else {	if (t instanceof IOException) {	processSendError(originalError);	getContext().taskKilled(taskSpec.getTaskAttemptID(), TaskAttemptEndReason.COMMUNICATION_ERROR, "Communication Error");	} else {	
failed to run task on containerid 

public void unregisterRunningTaskAttempt(final TezTaskAttemptID taskAttemptId, TaskAttemptEndReason endReason, String diagnostics) {	super.unregisterRunningTaskAttempt(taskAttemptId, endReason, diagnostics);	if (endReason == TaskAttemptEndReason.INTERNAL_PREEMPTION) {	
processing taskend for task caused by internal preemption 

private void sendTaskTerminated(final TezTaskAttemptID taskAttemptId, boolean invokedByContainerEnd) {	
attempting to send terminaterequest for fragment due to internal preemption invoked by containerEnd taskEnd 

private void sendTaskTerminated(final TezTaskAttemptID taskAttemptId, boolean invokedByContainerEnd) {	LlapNodeId nodeId = entityTracker.getNodeIdForTaskAttempt(taskAttemptId);	if (nodeId != null) {	TerminateFragmentRequestProto request = TerminateFragmentRequestProto.newBuilder().setQueryIdentifier( constructQueryIdentifierProto( taskAttemptId.getTaskID().getVertexID().getDAGId().getId())) .setFragmentIdentifierString(taskAttemptId.toString()).build();	communicator.sendTerminateFragment(request, nodeId.getHostname(), nodeId.getPort(), new LlapProtocolClientProxy.ExecuteRequestCallback<TerminateFragmentResponseProto>() {	public void setResponse(TerminateFragmentResponseProto response) {	}	public void indicateError(Throwable t) {	
failed to send terminate fragment request for 

if (nodeId != null) {	TerminateFragmentRequestProto request = TerminateFragmentRequestProto.newBuilder().setQueryIdentifier( constructQueryIdentifierProto( taskAttemptId.getTaskID().getVertexID().getDAGId().getId())) .setFragmentIdentifierString(taskAttemptId.toString()).build();	communicator.sendTerminateFragment(request, nodeId.getHostname(), nodeId.getPort(), new LlapProtocolClientProxy.ExecuteRequestCallback<TerminateFragmentResponseProto>() {	public void setResponse(TerminateFragmentResponseProto response) {	}	public void indicateError(Throwable t) {	processSendError(t);	}	});	} else {	
not sending terminate request for fragment since it s node is not known already unregistered 

public void dagComplete(final int dagIdentifier) {	QueryIdentifierProto queryIdentifierProto = constructQueryIdentifierProto(dagIdentifier);	QueryCompleteRequestProto request = QueryCompleteRequestProto.newBuilder() .setQueryIdentifier(queryIdentifierProto).setDeleteDelay(deleteDelayOnDagComplete).build();	for (final LlapNodeId llapNodeId : nodesForQuery) {	
sending dagcomplete message for to 

communicator.sendSourceStateUpdate(request, nodeId, new LlapProtocolClientProxy.ExecuteRequestCallback<SourceStateUpdatedResponseProto>() {	public void setResponse(SourceStateUpdatedResponseProto response) {	}	public void indicateError(Throwable t) {	LOG.error("Failed to send state update to node: {}, Killing all attempts running on " + "node. Attempted StateUpdate={}", nodeId, request, t);	processSendError(t);	BiMap<ContainerId, TezTaskAttemptID> biMap = entityTracker.getContainerAttemptMapForNode(nodeId);	if (biMap != null) {	synchronized (biMap) {	for (Map.Entry<ContainerId, TezTaskAttemptID> entry : biMap.entrySet()) {	
sending a kill for attempt due to a communication failure while sending a finishable state update 

public void registerKnownNode(LlapNodeId nodeId) {	Long old = knownNodeMap.putIfAbsent(nodeId, TimeUnit.MILLISECONDS.convert(System.nanoTime(), TimeUnit.NANOSECONDS));	if (old == null) {	if (LOG.isInfoEnabled()) {	
added new known node 

public void registerPingingNode(LlapNodeId nodeId) {	long currentTs = TimeUnit.MILLISECONDS.convert(System.nanoTime(), TimeUnit.NANOSECONDS);	PingingNodeInfo ni = new PingingNodeInfo(currentTs);	PingingNodeInfo old = pingedNodeMap.put(nodeId, ni);	if (old == null) {	if (LOG.isInfoEnabled()) {	
added new pinging node 

public void nodeHeartbeat( Text hostname, Text uniqueId, int port, TezAttemptArray aw) throws IOException {	if (LOG.isDebugEnabled()) {	
received heartbeat from 

public void registerTaskSubmittedToNode( TezTaskAttemptID taskAttemptID, String uniqueNodeId) {	synchronized (attemptToNodeMap) {	if (attemptToNodeMap.containsKey(taskAttemptID)) {	String prev = uniqueNodeMap.putIfAbsent(taskAttemptID, uniqueNodeId);	if (prev != null) {	
replaced the unique node mapping for task from to 

========================= hive sample_625 =========================

assert oldRefCount >= 0 : "oldValue is " + oldValue + " " + this;	if (oldRefCount == State.MAX_REFCOUNT) throw new AssertionError(this);	newValue = State.incRefCount(oldValue);	if (State.hasFlags(oldValue, State.FLAG_NEW_ALLOC)) {	newValue = State.switchFlag(newValue, State.FLAG_NEW_ALLOC);	}	if (state.compareAndSet(oldValue, newValue)) break;	}	int newRefCount = State.getRefCount(newValue);	if (LlapIoImpl.LOCKING_LOGGER.isTraceEnabled()) {	
locked new ref count 

do {	oldState = state.get();	int oldRefCount = State.getRefCount(oldState);	if (oldRefCount == 0) {	throw new AssertionError("Invalid decRef when refCount is 0: " + this);	}	newState = State.decRefCount(oldState);	} while (!state.compareAndSet(oldState, newState));	int newRefCount = State.getRefCount(newState);	if (LlapIoImpl.LOCKING_LOGGER.isTraceEnabled()) {	
unlocked refcount 

public int invalidate() {	while (true) {	long oldValue = state.get();	if (State.getRefCount(oldValue) != 0) return INVALIDATE_FAILED;	if (State.hasFlags(oldValue, State.FLAG_EVICTED)) return INVALIDATE_ALREADY_INVALID;	long newValue = State.setFlag(oldValue, State.FLAG_EVICTED | State.FLAG_MEM_RELEASED);	if (state.compareAndSet(oldValue, newValue)) break;	}	if (LlapIoImpl.LOCKING_LOGGER.isTraceEnabled()) {	
invalidated due to eviction 

if (State.hasFlags(oldValue, State.FLAG_EVICTED)) {	return -1;	}	newValue = State.setFlag(oldValue, State.FLAG_EVICTED);	if (!State.hasFlags(oldValue, State.FLAG_MOVING)) {	newValue = State.setFlag(newValue, State.FLAG_REMOVED | State.FLAG_MEM_RELEASED);	result = true;	}	} while (!state.compareAndSet(oldValue, newValue));	if (LlapIoImpl.LOCKING_LOGGER.isTraceEnabled()) {	
invalidated due to direct deallocation 

long oldValue, newValue;	do {	oldValue = state.get();	if (!State.hasFlags(oldValue, State.FLAG_EVICTED)) {	throw new AssertionError("Not invalidated");	}	if (State.hasFlags(oldValue, State.FLAG_MOVING | State.FLAG_REMOVED)) return -1;	newValue = State.setFlag(oldValue, State.FLAG_REMOVED);	} while (!state.compareAndSet(oldValue, newValue));	if (LlapIoImpl.LOCKING_LOGGER.isTraceEnabled()) {	
removed 

private boolean waitForState() {	synchronized (state) {	try {	state.wait(10);	return true;	} catch (InterruptedException e) {	
buffer incref is deffering an interrupt 

int flags = State.getAllFlags(oldValue);	if (flags != 0 && (isForceDiscard || flags != State.FLAG_NEW_ALLOC)) {	return false;	}	if (State.getArena(oldValue) != arenaIx || State.getHeader(oldValue) != headerIx) {	return false;	}	newValue = State.setFlag(oldValue, State.FLAG_MOVING);	} while (!state.compareAndSet(oldValue, newValue));	if (LlapIoImpl.LOCKING_LOGGER.isTraceEnabled()) {	
locked in preparation for a move 

result = null;	if (State.hasFlags(oldValue, State.FLAG_EVICTED)) {	if (State.hasFlags(oldValue, State.FLAG_REMOVED)) {	throw new AssertionError("Removed during the move " + this);	}	result = !State.hasFlags(oldValue, State.FLAG_MEM_RELEASED);	newValue = State.setFlag(newValue, State.FLAG_MEM_RELEASED | State.FLAG_REMOVED);	}	} while (!state.compareAndSet(oldValue, newValue));	if (LlapIoImpl.LOCKING_LOGGER.isTraceEnabled()) {	
move ended for 

newValue = State.setFlag(newValue, State.FLAG_EVICTED | State.FLAG_MEM_RELEASED | State.FLAG_REMOVED);	result = null;	if (State.hasFlags(oldValue, State.FLAG_EVICTED)) {	if (State.hasFlags(oldValue, State.FLAG_REMOVED)) {	throw new AssertionError("Removed during the move " + this);	}	result = !State.hasFlags(oldValue, State.FLAG_MEM_RELEASED);	}	} while (!state.compareAndSet(oldValue, newValue));	if (LlapIoImpl.LOCKING_LOGGER.isTraceEnabled()) {	
discared 

========================= hive sample_2181 =========================

public SQLStdHiveAccessController(HiveMetastoreClientFactory metastoreClientFactory, HiveConf conf, HiveAuthenticationProvider authenticator, HiveAuthzSessionContext ctx) throws HiveAuthzPluginException {	this.metastoreClientFactory = metastoreClientFactory;	this.authenticator = authenticator;	this.sessionCtx = SQLAuthorizationUtils.applyTestSettings(ctx, conf);	
created sqlstdhiveaccesscontroller for session context 

private void initUserRoles() throws HiveAuthzPluginException {	String newUserName = authenticator.getUserName();	if (Objects.equals(currentUserName, newUserName)) {	return;	}	this.currentUserName = newUserName;	this.currentRoles = getRolesFromMS();	
current user current roles 

currentRoles.clear();	currentRoles.add(role);	return;	}	}	if (HiveMetaStore.ADMIN.equalsIgnoreCase(roleName) && null != this.adminRole) {	currentRoles.clear();	currentRoles.add(adminRole);	return;	}	
current user current roles 

public void applyAuthorizationConfigPolicy(HiveConf hiveConf) throws HiveAuthzPluginException {	hiveConf.setVar(ConfVars.HIVE_AUTHORIZATION_TABLE_OWNER_GRANTS, "INSERT,SELECT,UPDATE,DELETE");	if (sessionCtx.getClientType() == CLIENT_TYPE.HIVESERVER2 && hiveConf.getBoolVar(ConfVars.HIVE_AUTHORIZATION_ENABLED)) {	String hooks = hiveConf.getVar(ConfVars.PREEXECHOOKS).trim();	if (hooks.isEmpty()) {	hooks = DisallowTransformHook.class.getName();	} else {	hooks = hooks + "," + DisallowTransformHook.class.getName();	}	
configuring hooks 

========================= hive sample_3143 =========================

public void handle(Context withinContext) throws Exception {	
processing alter partition message 

========================= hive sample_3456 =========================

protected RetryingMetaStoreClient(Configuration conf, Class<?>[] constructorArgTypes, Object[] constructorArgs, ConcurrentHashMap<String, Long> metaCallTimeMap, Class<? extends IMetaStoreClient> msClientClass) throws MetaException {	this.ugi = getUGI();	if (this.ugi == null) {	
retryingmetastoreclient unable to determine current user ugi 

}	}	}	while (true) {	try {	reloginExpiringKeytabUser();	if (allowReconnect) {	if (retriesMade > 0 || hasConnectionLifeTimeReached(method)) {	if (this.ugi != null) {	try {	
retryingmetastoreclient trying reconnect as 

} catch (UndeclaredThrowableException e) {	Throwable te = e.getCause();	if (te instanceof PrivilegedActionException) {	throw te.getCause();	} else {	throw te;	}	}	lastConnectionTime = System.currentTimeMillis();	} else {	
retryingmetastoreclient unable to reconnect no ugi information 

if (e.getMessage().matches("(?s).*(IO|TTransport)Exception.*") && !e.getMessage().contains("java.sql.SQLIntegrityConstraintViolationException")) {	caughtException = e;	} else {	throw e;	}	}	if (retriesMade >= retryLimit || base.isLocalMetaStore() || !allowRetry) {	throw caughtException;	}	retriesMade++;	
metastoreclient lost connection attempting to reconnect of after s 

private boolean hasConnectionLifeTimeReached(Method method) {	if (connectionLifeTimeInMillis <= 0 || localMetaStore) {	return false;	}	boolean shouldReconnect = (System.currentTimeMillis() - lastConnectionTime) >= connectionLifeTimeInMillis;	if (LOG.isDebugEnabled()) {	
reconnection status for method is 

========================= hive sample_1938 =========================

writer.write("LLAP IO not found");	} else {	writer.write(llapIo.getMemoryInfo());	}	} finally {	if (writer != null) {	writer.close();	}	}	} catch (Exception e) {	
caught exception while processing llap status request 

========================= hive sample_2247 =========================

private static void setRuntimeStatsDir(Operator<? extends OperatorDesc> op, ParseContext pctx) throws SemanticException {	try {	OperatorDesc conf = op.getConf();	if (conf != null) {	
setruntimestatsdir for 

private static void setRuntimeStatsDir(Operator<? extends OperatorDesc> op, ParseContext pctx) throws SemanticException {	try {	OperatorDesc conf = op.getConf();	if (conf != null) {	String path = new Path(pctx.getContext().getExplainConfig().getExplainRootPath(), op.getOperatorId()).toString();	StatsPublisher statsPublisher = new FSStatsPublisher();	StatsCollectionContext runtimeStatsContext = new StatsCollectionContext(pctx.getConf());	runtimeStatsContext.setStatsTmpDir(path);	if (!statsPublisher.init(runtimeStatsContext)) {	
statspublishing error statspublisher is not initialized 

if (conf != null) {	String path = new Path(pctx.getContext().getExplainConfig().getExplainRootPath(), op.getOperatorId()).toString();	StatsPublisher statsPublisher = new FSStatsPublisher();	StatsCollectionContext runtimeStatsContext = new StatsCollectionContext(pctx.getConf());	runtimeStatsContext.setStatsTmpDir(path);	if (!statsPublisher.init(runtimeStatsContext)) {	throw new HiveException(ErrorMsg.STATSPUBLISHER_NOT_OBTAINED.getErrorCodedMsg());	}	conf.setRuntimeStatsTmpDir(path);	} else {	
skip setruntimestatsdir for because operatordesc is null 

private static void annotateRuntimeStats(Operator<? extends OperatorDesc> op, ParseContext pctx) {	Long runTimeNumRows = pctx.getContext().getExplainConfig().getOpIdToRuntimeNumRows() .get(op.getOperatorId());	if (op.getConf() != null && op.getConf().getStatistics() != null && runTimeNumRows != null) {	
annotateruntimestats for 

private static void annotateRuntimeStats(Operator<? extends OperatorDesc> op, ParseContext pctx) {	Long runTimeNumRows = pctx.getContext().getExplainConfig().getOpIdToRuntimeNumRows() .get(op.getOperatorId());	if (op.getConf() != null && op.getConf().getStatistics() != null && runTimeNumRows != null) {	op.getConf().getStatistics().setRunTimeNumRows(runTimeNumRows);	} else {	
skip annotateruntimestats for 

========================= hive sample_2990 =========================

smallTableOutputVectorColumns = smallTableMapping.getOutputColumns();	bigTableByteColumnVectorColumns = getByteColumnVectorColumns(bigTableOuterKeyMapping);	smallTableByteColumnVectorColumns = getByteColumnVectorColumns(smallTableMapping);	outputProjection = projectionMapping.getOutputColumns();	outputTypeInfos = projectionMapping.getTypeInfos();	if (LOG.isDebugEnabled()) {	int[] orderDisplayable = new int[order.length];	for (int i = 0; i < order.length; i++) {	orderDisplayable[i] = (int) order[i];	}	
vectormapjoincommonoperator constructor order 

smallTableOutputVectorColumns = smallTableMapping.getOutputColumns();	bigTableByteColumnVectorColumns = getByteColumnVectorColumns(bigTableOuterKeyMapping);	smallTableByteColumnVectorColumns = getByteColumnVectorColumns(smallTableMapping);	outputProjection = projectionMapping.getOutputColumns();	outputTypeInfos = projectionMapping.getTypeInfos();	if (LOG.isDebugEnabled()) {	int[] orderDisplayable = new int[order.length];	for (int i = 0; i < order.length; i++) {	orderDisplayable[i] = (int) order[i];	}	
vectormapjoincommonoperator constructor posbigtable 

smallTableOutputVectorColumns = smallTableMapping.getOutputColumns();	bigTableByteColumnVectorColumns = getByteColumnVectorColumns(bigTableOuterKeyMapping);	smallTableByteColumnVectorColumns = getByteColumnVectorColumns(smallTableMapping);	outputProjection = projectionMapping.getOutputColumns();	outputTypeInfos = projectionMapping.getTypeInfos();	if (LOG.isDebugEnabled()) {	int[] orderDisplayable = new int[order.length];	for (int i = 0; i < order.length; i++) {	orderDisplayable[i] = (int) order[i];	}	
vectormapjoincommonoperator constructor possinglevectormapjoinsmalltable 

smallTableOutputVectorColumns = smallTableMapping.getOutputColumns();	bigTableByteColumnVectorColumns = getByteColumnVectorColumns(bigTableOuterKeyMapping);	smallTableByteColumnVectorColumns = getByteColumnVectorColumns(smallTableMapping);	outputProjection = projectionMapping.getOutputColumns();	outputTypeInfos = projectionMapping.getTypeInfos();	if (LOG.isDebugEnabled()) {	int[] orderDisplayable = new int[order.length];	for (int i = 0; i < order.length; i++) {	orderDisplayable[i] = (int) order[i];	}	
vectormapjoincommonoperator constructor bigtablekeycolumnmap 

smallTableOutputVectorColumns = smallTableMapping.getOutputColumns();	bigTableByteColumnVectorColumns = getByteColumnVectorColumns(bigTableOuterKeyMapping);	smallTableByteColumnVectorColumns = getByteColumnVectorColumns(smallTableMapping);	outputProjection = projectionMapping.getOutputColumns();	outputTypeInfos = projectionMapping.getTypeInfos();	if (LOG.isDebugEnabled()) {	int[] orderDisplayable = new int[order.length];	for (int i = 0; i < order.length; i++) {	orderDisplayable[i] = (int) order[i];	}	
vectormapjoincommonoperator constructor bigtablekeycolumnnames 

smallTableOutputVectorColumns = smallTableMapping.getOutputColumns();	bigTableByteColumnVectorColumns = getByteColumnVectorColumns(bigTableOuterKeyMapping);	smallTableByteColumnVectorColumns = getByteColumnVectorColumns(smallTableMapping);	outputProjection = projectionMapping.getOutputColumns();	outputTypeInfos = projectionMapping.getTypeInfos();	if (LOG.isDebugEnabled()) {	int[] orderDisplayable = new int[order.length];	for (int i = 0; i < order.length; i++) {	orderDisplayable[i] = (int) order[i];	}	
vectormapjoincommonoperator constructor bigtablekeytypeinfos 

smallTableOutputVectorColumns = smallTableMapping.getOutputColumns();	bigTableByteColumnVectorColumns = getByteColumnVectorColumns(bigTableOuterKeyMapping);	smallTableByteColumnVectorColumns = getByteColumnVectorColumns(smallTableMapping);	outputProjection = projectionMapping.getOutputColumns();	outputTypeInfos = projectionMapping.getTypeInfos();	if (LOG.isDebugEnabled()) {	int[] orderDisplayable = new int[order.length];	for (int i = 0; i < order.length; i++) {	orderDisplayable[i] = (int) order[i];	}	
vectormapjoincommonoperator constructor bigtablevaluecolumnmap 

smallTableOutputVectorColumns = smallTableMapping.getOutputColumns();	bigTableByteColumnVectorColumns = getByteColumnVectorColumns(bigTableOuterKeyMapping);	smallTableByteColumnVectorColumns = getByteColumnVectorColumns(smallTableMapping);	outputProjection = projectionMapping.getOutputColumns();	outputTypeInfos = projectionMapping.getTypeInfos();	if (LOG.isDebugEnabled()) {	int[] orderDisplayable = new int[order.length];	for (int i = 0; i < order.length; i++) {	orderDisplayable[i] = (int) order[i];	}	
vectormapjoincommonoperator constructor bigtablevaluecolumnnames 

smallTableOutputVectorColumns = smallTableMapping.getOutputColumns();	bigTableByteColumnVectorColumns = getByteColumnVectorColumns(bigTableOuterKeyMapping);	smallTableByteColumnVectorColumns = getByteColumnVectorColumns(smallTableMapping);	outputProjection = projectionMapping.getOutputColumns();	outputTypeInfos = projectionMapping.getTypeInfos();	if (LOG.isDebugEnabled()) {	int[] orderDisplayable = new int[order.length];	for (int i = 0; i < order.length; i++) {	orderDisplayable[i] = (int) order[i];	}	
vectormapjoincommonoperator constructor bigtablevaluetypenames 

smallTableOutputVectorColumns = smallTableMapping.getOutputColumns();	bigTableByteColumnVectorColumns = getByteColumnVectorColumns(bigTableOuterKeyMapping);	smallTableByteColumnVectorColumns = getByteColumnVectorColumns(smallTableMapping);	outputProjection = projectionMapping.getOutputColumns();	outputTypeInfos = projectionMapping.getTypeInfos();	if (LOG.isDebugEnabled()) {	int[] orderDisplayable = new int[order.length];	for (int i = 0; i < order.length; i++) {	orderDisplayable[i] = (int) order[i];	}	
vectormapjoincommonoperator constructor bigtableretainedmapping 

smallTableOutputVectorColumns = smallTableMapping.getOutputColumns();	bigTableByteColumnVectorColumns = getByteColumnVectorColumns(bigTableOuterKeyMapping);	smallTableByteColumnVectorColumns = getByteColumnVectorColumns(smallTableMapping);	outputProjection = projectionMapping.getOutputColumns();	outputTypeInfos = projectionMapping.getTypeInfos();	if (LOG.isDebugEnabled()) {	int[] orderDisplayable = new int[order.length];	for (int i = 0; i < order.length; i++) {	orderDisplayable[i] = (int) order[i];	}	
vectormapjoincommonoperator constructor bigtableouterkeymapping 

smallTableOutputVectorColumns = smallTableMapping.getOutputColumns();	bigTableByteColumnVectorColumns = getByteColumnVectorColumns(bigTableOuterKeyMapping);	smallTableByteColumnVectorColumns = getByteColumnVectorColumns(smallTableMapping);	outputProjection = projectionMapping.getOutputColumns();	outputTypeInfos = projectionMapping.getTypeInfos();	if (LOG.isDebugEnabled()) {	int[] orderDisplayable = new int[order.length];	for (int i = 0; i < order.length; i++) {	orderDisplayable[i] = (int) order[i];	}	
vectormapjoincommonoperator constructor smalltablemapping 

smallTableOutputVectorColumns = smallTableMapping.getOutputColumns();	bigTableByteColumnVectorColumns = getByteColumnVectorColumns(bigTableOuterKeyMapping);	smallTableByteColumnVectorColumns = getByteColumnVectorColumns(smallTableMapping);	outputProjection = projectionMapping.getOutputColumns();	outputTypeInfos = projectionMapping.getTypeInfos();	if (LOG.isDebugEnabled()) {	int[] orderDisplayable = new int[order.length];	for (int i = 0; i < order.length; i++) {	orderDisplayable[i] = (int) order[i];	}	
vectormapjoincommonoperator constructor bigtablebytecolumnvectorcolumns 

smallTableOutputVectorColumns = smallTableMapping.getOutputColumns();	bigTableByteColumnVectorColumns = getByteColumnVectorColumns(bigTableOuterKeyMapping);	smallTableByteColumnVectorColumns = getByteColumnVectorColumns(smallTableMapping);	outputProjection = projectionMapping.getOutputColumns();	outputTypeInfos = projectionMapping.getTypeInfos();	if (LOG.isDebugEnabled()) {	int[] orderDisplayable = new int[order.length];	for (int i = 0; i < order.length; i++) {	orderDisplayable[i] = (int) order[i];	}	
vectormapjoincommonoperator constructor smalltablebytecolumnvectorcolumns 

smallTableOutputVectorColumns = smallTableMapping.getOutputColumns();	bigTableByteColumnVectorColumns = getByteColumnVectorColumns(bigTableOuterKeyMapping);	smallTableByteColumnVectorColumns = getByteColumnVectorColumns(smallTableMapping);	outputProjection = projectionMapping.getOutputColumns();	outputTypeInfos = projectionMapping.getTypeInfos();	if (LOG.isDebugEnabled()) {	int[] orderDisplayable = new int[order.length];	for (int i = 0; i < order.length; i++) {	orderDisplayable[i] = (int) order[i];	}	
vectormapjoincommonoperator constructor outputprojection 

smallTableOutputVectorColumns = smallTableMapping.getOutputColumns();	bigTableByteColumnVectorColumns = getByteColumnVectorColumns(bigTableOuterKeyMapping);	smallTableByteColumnVectorColumns = getByteColumnVectorColumns(smallTableMapping);	outputProjection = projectionMapping.getOutputColumns();	outputTypeInfos = projectionMapping.getTypeInfos();	if (LOG.isDebugEnabled()) {	int[] orderDisplayable = new int[order.length];	for (int i = 0; i < order.length; i++) {	orderDisplayable[i] = (int) order[i];	}	
vectormapjoincommonoperator constructor outputtypeinfos 

protected void setupVOutContext(List<String> outputColumnNames) {	if (LOG.isDebugEnabled()) {	
vectormapjoincommonoperator constructor outputcolumnnames 

}	if (outputColumnNames.size() != outputProjection.length) {	throw new RuntimeException("Output column names " + outputColumnNames + " length and output projection " + Arrays.toString(outputProjection) + " / " + Arrays.toString(outputTypeInfos) + " length mismatch");	}	vOutContext.resetProjectionColumns();	for (int i = 0; i < outputColumnNames.size(); ++i) {	String columnName = outputColumnNames.get(i);	int outputColumn = outputProjection[i];	vOutContext.addProjectionColumn(columnName, outputColumn);	if (LOG.isDebugEnabled()) {	
vectormapjoincommonoperator constructor addprojectioncolumn columnname outputcolumn 

}	if (bigTableOuterKeyMapping.getCount() > 0) {	bigTableVectorCopyOuterKeys = new VectorCopyRow();	bigTableVectorCopyOuterKeys.init(bigTableOuterKeyMapping);	}	overflowBatch = setupOverflowBatch();	needCommonSetup = true;	needHashTableSetup = true;	if (LOG.isDebugEnabled()) {	int[] currentScratchColumns = vOutContext.currentScratchColumns();	
vectormapjoincommonoperator initializeop currentscratchcolumns 

}	overflowBatch = setupOverflowBatch();	needCommonSetup = true;	needHashTableSetup = true;	if (LOG.isDebugEnabled()) {	int[] currentScratchColumns = vOutContext.currentScratchColumns();	StructObjectInspector structOutputObjectInspector = (StructObjectInspector) outputObjInspector;	List<? extends StructField> fields = structOutputObjectInspector.getAllStructFieldRefs();	int i = 0;	for (StructField field : fields) {	
vectormapjoincommonoperator initializeop field type 

vectorMapJoinHashTable = VectorMapJoinOptimizedCreateHashTable.createHashTable(conf, mapJoinTables[posSingleVectorMapJoinSmallTable]);	}	break;	case FAST: {	VectorMapJoinTableContainer vectorMapJoinTableContainer = (VectorMapJoinTableContainer) mapJoinTables[posSingleVectorMapJoinSmallTable];	vectorMapJoinHashTable = vectorMapJoinTableContainer.vectorMapJoinHashTable();	}	break;	default: throw new RuntimeException("Unknown vector map join hash table implementation type " + hashTableImplementationType.name());	}	
using from 

private void allocateOverflowBatchColumnVector(VectorizedRowBatch overflowBatch, int outputColumn, String typeName) throws HiveException {	if (overflowBatch.cols[outputColumn] == null) {	typeName = VectorizationContext.mapTypeNameSynonyms(typeName);	TypeInfo typeInfo = TypeInfoUtils.getTypeInfoFromTypeString(typeName);	overflowBatch.cols[outputColumn] = VectorizedBatchUtil.createColumnVector(typeInfo);	if (LOG.isDebugEnabled()) {	
vectormapjoincommonoperator initializeop overflowbatch outputcolumn class 

protected void commonSetup(VectorizedRowBatch batch) throws HiveException {	if (LOG.isDebugEnabled()) {	
vectormapjoininnercommonoperator commonsetup begin 

protected void displayBatchColumns(VectorizedRowBatch batch, String batchName) {	
vectormapjoincommonoperator commonsetup column count 

========================= hive sample_4147 =========================

private static Map<ASTNode, ExprNodeDesc> convert(Map<Node, Object> outputs) {	Map<ASTNode, ExprNodeDesc> converted = new LinkedHashMap<ASTNode, ExprNodeDesc>();	for (Map.Entry<Node, Object> entry : outputs.entrySet()) {	if (entry.getKey() instanceof ASTNode && (entry.getValue() == null || entry.getValue() instanceof ExprNodeDesc)) {	converted.put((ASTNode)entry.getKey(), (ExprNodeDesc)entry.getValue());	} else {	
invalid type entry 

}else if (PrimitiveObjectInspectorUtils.doubleTypeEntry.equals(colTypeInfo.getPrimitiveTypeEntry()) && (constVal instanceof Number || constVal instanceof String)) {	children.set(constIdx, new ExprNodeConstantDesc(new Double(constVal.toString())));	} else if (PrimitiveObjectInspectorUtils.floatTypeEntry.equals(colTypeInfo.getPrimitiveTypeEntry()) && (constVal instanceof Number || constVal instanceof String)) {	children.set(constIdx, new ExprNodeConstantDesc(new Float(constVal.toString())));	} else if (PrimitiveObjectInspectorUtils.byteTypeEntry.equals(colTypeInfo.getPrimitiveTypeEntry()) && (constVal instanceof Number || constVal instanceof String)) {	children.set(constIdx, new ExprNodeConstantDesc(new Byte(constVal.toString())));	} else if (PrimitiveObjectInspectorUtils.shortTypeEntry.equals(colTypeInfo.getPrimitiveTypeEntry()) && (constVal instanceof Number || constVal instanceof String)) {	children.set(constIdx, new ExprNodeConstantDesc(new Short(constVal.toString())));	}	} catch (NumberFormatException nfe) {	
failed to narrow type of constant 

========================= hive sample_3397 =========================

public void setUp() throws Exception {	conf = MetastoreConf.newMetastoreConf();	System.setProperty(ConfVars.EVENT_LISTENERS.toString(), IpAddressListener.class.getName());	MetaStoreTestUtils.setConfForStandloneMode(conf);	int port = MetaStoreTestUtils.startMetaStoreWithRetry(HadoopThriftAuthBridge.getBridge(), conf);	
starting metastore server on port 

========================= hive sample_1579 =========================

public static ReduceWork createReduceWork( GenTezProcContext context, Operator<?> root, TezWork tezWork) {	assert !root.getParentOperators().isEmpty();	boolean isAutoReduceParallelism = context.conf.getBoolVar(HiveConf.ConfVars.TEZ_AUTO_REDUCER_PARALLELISM);	float maxPartitionFactor = context.conf.getFloatVar(HiveConf.ConfVars.TEZ_MAX_PARTITION_FACTOR);	float minPartitionFactor = context.conf.getFloatVar(HiveConf.ConfVars.TEZ_MIN_PARTITION_FACTOR);	long bytesPerReducer = context.conf.getLongVar(HiveConf.ConfVars.BYTESPERREDUCER);	ReduceWork reduceWork = new ReduceWork(Utilities.REDUCENAME + context.nextSequenceNumber());	
adding reduce work for 

private static void setupReduceSink( GenTezProcContext context, ReduceWork reduceWork, ReduceSinkOperator reduceSink) {	
setting up reduce sink with following reduce work 

public MapWork createMapWork(GenTezProcContext context, Operator<?> root, TezWork tezWork, PrunedPartitionList partitions) throws SemanticException {	assert root.getParentOperators().isEmpty();	MapWork mapWork = new MapWork(Utilities.MAPNAME + context.nextSequenceNumber());	
adding map work for 

FileSinkDesc desc = fileSink.getConf();	Path path = desc.getDirName();	List<FileSinkDesc> linked;	if (!context.linkedFileSinks.containsKey(path)) {	linked = new ArrayList<FileSinkDesc>();	context.linkedFileSinks.put(path, linked);	}	linked = context.linkedFileSinks.get(path);	linked.add(desc);	desc.setDirName(new Path(path, AbstractFileMergeOperator.UNION_SUDBIR_PREFIX + linked.size()));	
removing union new desc with parent 

} else {	parent.removeChildAndAdoptItsChildren(current);	}	}	if (current instanceof FileSinkOperator || current instanceof ReduceSinkOperator) {	current.setChildOperators(null);	} else {	operators.addAll(current.getChildOperators());	}	}	
setting dummy ops for work 

public static void processFileSink(GenTezProcContext context, FileSinkOperator fileSink) throws SemanticException {	ParseContext parseContext = context.parseContext;	boolean isInsertTable = GenMapRedUtils.isInsertInto(parseContext, fileSink);	HiveConf hconf = parseContext.getConf();	boolean chDir = GenMapRedUtils.isMergeRequired(context.moveTask, hconf, fileSink, context.currentTask, isInsertTable);	Path finalName = GenMapRedUtils.createMoveTask(context.currentTask, chDir, fileSink, parseContext, context.moveTask, hconf, context.dependencyTask);	if (chDir) {	
using combinehiveinputformat for the merge job 

public static void processFileSink(GenTezProcContext context, FileSinkOperator fileSink) throws SemanticException {	ParseContext parseContext = context.parseContext;	boolean isInsertTable = GenMapRedUtils.isInsertInto(parseContext, fileSink);	HiveConf hconf = parseContext.getConf();	boolean chDir = GenMapRedUtils.isMergeRequired(context.moveTask, hconf, fileSink, context.currentTask, isInsertTable);	Path finalName = GenMapRedUtils.createMoveTask(context.currentTask, chDir, fileSink, parseContext, context.moveTask, hconf, context.dependencyTask);	if (chDir) {	
will generate mr work for merging files from to 

StringBuilder sb = new StringBuilder();	for (BaseWork curWork : rsWorkList) {	if ( sb.length() > 0) {	sb.append(", ");	}	sb.append(curWork.getName());	}	throw new SemanticException(rs + " belongs to multiple BaseWorks: " + sb.toString());	}	TableScanOperator ts = sjInfo.getTsOp();	
resducesink to tablescan 

if ( sb.length() > 0) {	sb.append(", ");	}	sb.append(curWork.getName());	}	throw new SemanticException(rs + " belongs to multiple BaseWorks: " + sb.toString());	}	TableScanOperator ts = sjInfo.getTsOp();	BaseWork parentWork = rsWorkList.get(0);	BaseWork childWork = procCtx.rootToWorkMap.get(ts);	
connecting baswork to 

public static void removeSemiJoinOperator(ParseContext context, ReduceSinkOperator rs, TableScanOperator ts) throws SemanticException{	
removing reducesink and tablescan 

public static void removeSemiJoinOperator(ParseContext context, AppMasterEventOperator eventOp, TableScanOperator ts) throws SemanticException{	
removing appmastereventoperator and tablescan 

========================= hive sample_3544 =========================

protected void checkAndGenObject() throws HiveException {	if (closeOpCalled) {	
checkandgenobject is called after operator called closeop 

========================= hive sample_3952 =========================

if (!databaseEventProcessed) {	FSDatabaseEvent event = new FSDatabaseEvent(hiveConf, next.toString());	databaseEventProcessed = true;	return postProcessing(event);	}	if (replicationState != null) {	return eventForReplicationState();	}	String currentPath = next.toString();	if (currentPath.contains(FUNCTIONS_ROOT_DIR_NAME)) {	
functions directory 

private BootstrapEvent postProcessing(BootstrapEvent bootstrapEvent) {	previous = next;	next = null;	
processing 

========================= hive sample_3925 =========================

NumDistinctValueEstimator oldEst = aggregateData.getNdvEstimator();	NumDistinctValueEstimator newEst = newData.getNdvEstimator();	long ndv = -1;	if (oldEst.canMerge(newEst)) {	oldEst.mergeEstimators(newEst);	ndv = oldEst.estimateNumDistinctValues();	aggregateData.setNdvEstimator(oldEst);	} else {	ndv = Math.max(aggregateData.getNumDVs(), newData.getNumDVs());	}	
use bitvector to merge column s ndvs of and to be 

========================= hive sample_1918 =========================

jobProperties.put(AccumuloSerDeParameters.ITERATOR_PUSHDOWN_KEY, useIterators);	}	String storageType = props.getProperty(AccumuloSerDeParameters.DEFAULT_STORAGE_TYPE);	if (null != storageType) {	jobProperties.put(AccumuloSerDeParameters.DEFAULT_STORAGE_TYPE, storageType);	}	String authValue = props.getProperty(AccumuloSerDeParameters.AUTHORIZATIONS_KEY);	if (null != authValue) {	jobProperties.put(AccumuloSerDeParameters.AUTHORIZATIONS_KEY, authValue);	}	
computed input job properties of 

if (null != authValue) {	jobProperties.put(AccumuloSerDeParameters.AUTHORIZATIONS_KEY, authValue);	}	Configuration conf = getConf();	helper.loadDependentJars(conf);	if (connectionParams.useSasl()) {	try {	Connector conn = connectionParams.getConnector();	Token<? extends TokenIdentifier> accumuloToken = helper.setConnectorInfoForInputAndOutput(connectionParams, conn, conf);	if (conf instanceof JobConf) {	
adding hadoop token for accumulo to job s credentials 

}	Configuration conf = getConf();	helper.loadDependentJars(conf);	if (connectionParams.useSasl()) {	try {	Connector conn = connectionParams.getConnector();	Token<? extends TokenIdentifier> accumuloToken = helper.setConnectorInfoForInputAndOutput(connectionParams, conn, conf);	if (conf instanceof JobConf) {	JobConf jobConf = (JobConf) conf;	jobConf.getCredentials().addToken(accumuloToken.getService(), accumuloToken);	
all job tokens 

Configuration conf = getConf();	helper.loadDependentJars(conf);	if (connectionParams.useSasl()) {	try {	Connector conn = connectionParams.getConnector();	Token<? extends TokenIdentifier> accumuloToken = helper.setConnectorInfoForInputAndOutput(connectionParams, conn, conf);	if (conf instanceof JobConf) {	JobConf jobConf = (JobConf) conf;	jobConf.getCredentials().addToken(accumuloToken.getService(), accumuloToken);	} else {	
don t have a jobconf so we cannot persist tokens have to do it later 

public DecomposedPredicate decomposePredicate(JobConf conf, Deserializer deserializer, ExprNodeDesc desc) {	if (!(deserializer instanceof AccumuloSerDe)) {	throw new RuntimeException("Expected an AccumuloSerDe but got " + deserializer.getClass().getName());	}	AccumuloSerDe serDe = (AccumuloSerDe) deserializer;	if (serDe.getIteratorPushdown()) {	return predicateHandler.decompose(conf, desc);	} else {	
set to ignore accumulo iterator pushdown skipping predicate handler 

public void configureJobConf(TableDesc tableDesc, JobConf jobConf) {	helper.loadDependentJars(jobConf);	Properties tblProperties = tableDesc.getProperties();	AccumuloSerDeParameters serDeParams = null;	try {	serDeParams = new AccumuloSerDeParameters(jobConf, tblProperties, AccumuloSerDe.class.getName());	} catch (SerDeException e) {	
could not instantiate accumuloserdeparameters 

Properties tblProperties = tableDesc.getProperties();	AccumuloSerDeParameters serDeParams = null;	try {	serDeParams = new AccumuloSerDeParameters(jobConf, tblProperties, AccumuloSerDe.class.getName());	} catch (SerDeException e) {	return;	}	try {	serDeParams.getRowIdFactory().addDependencyJars(jobConf);	} catch (IOException e) {	
could not add necessary dependencies for 

return;	}	try {	serDeParams.getRowIdFactory().addDependencyJars(jobConf);	} catch (IOException e) {	}	if (connectionParams.useSasl()) {	try {	Connector conn = connectionParams.getConnector();	Token<? extends TokenIdentifier> accumuloToken = helper.setConnectorInfoForInputAndOutput(connectionParams, conn, jobConf);	
adding hadoop token for accumulo to job s credentials 

}	try {	serDeParams.getRowIdFactory().addDependencyJars(jobConf);	} catch (IOException e) {	}	if (connectionParams.useSasl()) {	try {	Connector conn = connectionParams.getConnector();	Token<? extends TokenIdentifier> accumuloToken = helper.setConnectorInfoForInputAndOutput(connectionParams, conn, jobConf);	helper.mergeTokenIntoJobConf(jobConf, accumuloToken);	
all job tokens 

========================= hive sample_193 =========================

public void testAutoPurge() throws Exception {	
running 

public void testAutoPurgeInvalid() throws Exception {	
running 

public void testAutoPurgeUnset() throws Exception {	
running 

public void testExternalTable() throws Exception {	
running 

public void testPartitionedTable() throws Exception {	
running 

public void testExternalPartitionedTable() throws Exception {	
running 

public void testNoAutoPurge() throws Exception {	
running 

public void testExternalNoAutoPurge() throws Exception {	
running 

public void testPartitionedNoAutoPurge() throws Exception {	
running 

public void testPartitionedExternalNoAutoPurge() throws Exception {	
running 

public void testTruncatePartitionedExternalNoAutoPurge() throws Exception {	
running 

public void testTruncateExternalNoAutoPurge() throws Exception {	
running 

public void testTruncatePartitionedNoAutoPurge() throws Exception {	
running 

public void testTruncateNoAutoPurge() throws Exception {	
running 

public void testTruncateInvalidAutoPurge() throws Exception {	
running 

public void testTruncateUnsetAutoPurge() throws Exception {	
running 

public void testTruncatePartitionedExternalAutoPurge() throws Exception {	
running 

public void testTruncateExternalAutoPurge() throws Exception {	
running 

public void testTruncatePartitionedAutoPurge() throws Exception {	
running 

========================= hive sample_255 =========================

public void close() {	isClosed = true;	workThread.interrupt();	try {	workThread.join(1000);	} catch (InterruptedException e) {	
interrupted during close 

private void runWorkThread() {	while (true) {	if (isClosed) {	deleteAllLocalResources();	return;	}	LocalizerWork lw = null;	try {	lw = workQueue.take();	} catch (InterruptedException ex) {	
localizer thread interrupted 

} catch (InterruptedException ex) {	isClosed = true;	}	if (isClosed) {	deleteAllLocalResources();	return;	}	try {	lw.run(this);	} catch (InterruptedException ex) {	
localizer thread interrupted 

}	if (isClosed) {	deleteAllLocalResources();	return;	}	try {	lw.run(this);	} catch (InterruptedException ex) {	isClosed = true;	} catch (Exception ex) {	
failed to run 

private void deleteAllLocalResources() {	try {	executorClassloader.close();	} catch (Exception ex) {	
failed to close the classloader 

try {	executorClassloader.close();	} catch (Exception ex) {	}	resourcesByFn.clear();	for (RefCountedResource rcr : localFiles.values()) {	for (FunctionResource fr : rcr.resources) {	File file = new File(fr.getResourceURI());	try {	if (!file.delete()) {	
failed to delete 

} catch (Exception ex) {	}	resourcesByFn.clear();	for (RefCountedResource rcr : localFiles.values()) {	for (FunctionResource fr : rcr.resources) {	File file = new File(fr.getResourceURI());	try {	if (!file.delete()) {	}	} catch (Exception ex) {	
failed to delete 

public void refreshClassloader() throws IOException {	if (recentlyLocalizedJars.isEmpty()) return;	String[] jars = recentlyLocalizedJars.toArray(new String[0]);	recentlyLocalizedJars.clear();	ClassLoader updatedCl = null;	try {	updatedCl = Utilities.addToClassPath(executorClassloader, jars);	if (LOG.isInfoEnabled()) {	
added jars to classpath 

private void localizeFunctionResources(String fqfn, List<ResourceUri> resources, String className, FnResources result, boolean doRefreshClassloader) throws URISyntaxException, IOException {	if (LOG.isInfoEnabled()) {	
localizing resources for 

RefCountedResource rcr = localFiles.get(srcUri);	if (rcr != null && rcr.refCount > 0) {	logFilesUsed("Reusing", fqfn, srcUri, rcr);	++rcr.refCount;	result.addResources(rcr);	return;	}	rcr = new RefCountedResource();	List<URI> localUris = resourceDownloader.downloadExternal(srcUri, fqfn, false);	if (localUris == null || localUris.isEmpty()) {	
cannot download for 

========================= hive sample_2230 =========================

public Multimap<Integer, InputSplit> group(Configuration conf, Multimap<Integer, InputSplit> bucketSplitMultimap, int availableSlots, float waves, SplitLocationProvider splitLocationProvider) throws IOException {	Map<Integer, Integer> bucketTaskMap = estimateBucketSizes(availableSlots, waves, bucketSplitMultimap.asMap());	Multimap<Integer, InputSplit> bucketGroupedSplitMultimap = ArrayListMultimap.<Integer, InputSplit> create();	for (int bucketId : bucketSplitMultimap.keySet()) {	Collection<InputSplit> inputSplitCollection = bucketSplitMultimap.get(bucketId);	InputSplit[] rawSplits = inputSplitCollection.toArray(new InputSplit[0]);	InputSplit[] groupedSplits = tezGrouper.getGroupedSplits(conf, rawSplits, bucketTaskMap.get(bucketId), HiveInputFormat.class.getName(), new ColumnarSplitSizeEstimator(), splitLocationProvider);	
original split count is grouped split count is for bucket 

Multimap<Integer, InputSplit> bucketSplitMultiMap = ArrayListMultimap.<Integer, InputSplit> create();	int i = 0;	InputSplit prevSplit = null;	for (InputSplit s : splits) {	if (schemaEvolved(s, prevSplit, groupAcrossFiles, work)) {	++i;	prevSplit = s;	}	bucketSplitMultiMap.put(i, s);	}	
src groups for split generation 

bucketSizeMap.put(bucketId, size);	}	if (earlyExit) {	return bucketTaskMap;	}	for (int bucketId : bucketSizeMap.keySet()) {	int numEstimatedTasks = 0;	if (totalSize != 0) {	numEstimatedTasks = (int) (availableSlots * waves * bucketSizeMap.get(bucketId) / totalSize);	}	
estimated number of tasks for bucket 

return !path.equals(prevPath);	}	PartitionDesc prevPD = HiveFileFormatUtils.getFromPathRecursively(work.getPathToPartitionInfo(), prevPath, cache);	previousDeserializerClass = prevPD.getDeserializerClassName();	previousInputFormatClass = prevPD.getInputFileFormatClass();	}	if ((currentInputFormatClass != previousInputFormatClass) || (!currentDeserializerClass.equals(previousDeserializerClass))) {	retval = true;	}	if (LOG.isDebugEnabled()) {	
adding split to src new group 

========================= hive sample_3972 =========================

public ASTNode parse(String command, Context ctx, String viewFullyQualifiedName) throws ParseException {	if (LOG.isDebugEnabled()) {	
parsing command 

}	parser.setTreeAdaptor(adaptor);	HiveParser.statement_return r = null;	try {	r = parser.statement();	} catch (RecognitionException e) {	e.printStackTrace();	throw new ParseException(parser.errors);	}	if (lexer.getErrors().size() == 0 && parser.errors.size() == 0) {	
parse completed 

public ASTNode parseHint(String command) throws ParseException {	
parsing hint 

HintParser parser = new HintParser(tokens);	parser.setTreeAdaptor(adaptor);	HintParser.hint_return r = null;	try {	r = parser.hint();	} catch (RecognitionException e) {	e.printStackTrace();	throw new ParseException(parser.errors);	}	if (lexer.getErrors().size() == 0 && parser.errors.size() == 0) {	
parse completed 

public ASTNode parseSelect(String command, Context ctx) throws ParseException {	if (LOG.isDebugEnabled()) {	
parsing command 

HiveParser parser = new HiveParser(tokens);	parser.setTreeAdaptor(adaptor);	HiveParser_SelectClauseParser.selectClause_return r = null;	try {	r = parser.selectClause();	} catch (RecognitionException e) {	e.printStackTrace();	throw new ParseException(parser.errors);	}	if (lexer.getErrors().size() == 0 && parser.errors.size() == 0) {	
parse completed 

public ASTNode parseExpression(String command) throws ParseException {	
parsing expression 

HiveParser parser = new HiveParser(tokens);	parser.setTreeAdaptor(adaptor);	HiveParser_IdentifiersParser.expression_return r = null;	try {	r = parser.expression();	} catch (RecognitionException e) {	e.printStackTrace();	throw new ParseException(parser.errors);	}	if (lexer.getErrors().size() == 0 && parser.errors.size() == 0) {	
parse completed 

========================= hive sample_3421 =========================

public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procContext, Object... nodeOutputs) throws SemanticException {	GenSparkProcContext context = (GenSparkProcContext) procContext;	Preconditions.checkArgument(context != null, "AssertionError: expected context to be not null");	Preconditions.checkArgument(context.currentTask != null, "AssertionError: expected context.currentTask to be not null");	Preconditions.checkArgument(context.currentRootOperator != null, "AssertionError: expected context.currentRootOperator to be not null");	Operator<? extends OperatorDesc> operator = (Operator<? extends OperatorDesc>) nd;	Operator<?> root = context.currentRootOperator;	
root operator 

public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procContext, Object... nodeOutputs) throws SemanticException {	GenSparkProcContext context = (GenSparkProcContext) procContext;	Preconditions.checkArgument(context != null, "AssertionError: expected context to be not null");	Preconditions.checkArgument(context.currentTask != null, "AssertionError: expected context.currentTask to be not null");	Preconditions.checkArgument(context.currentRootOperator != null, "AssertionError: expected context.currentRootOperator to be not null");	Operator<? extends OperatorDesc> operator = (Operator<? extends OperatorDesc>) nd;	Operator<?> root = context.currentRootOperator;	
leaf operator 

}	if (!context.childToWorkMap.containsKey(operator)) {	List<BaseWork> workItems = new LinkedList<BaseWork>();	workItems.add(work);	context.childToWorkMap.put(operator, workItems);	} else {	context.childToWorkMap.get(operator).add(work);	}	if (!context.currentMapJoinOperators.isEmpty()) {	for (MapJoinOperator mj: context.currentMapJoinOperators) {	
processing map join 

if (context.linkOpWithWorkMap.containsKey(mj)) {	Map<BaseWork, SparkEdgeProperty> linkWorkMap = context.linkOpWithWorkMap.get(mj);	if (linkWorkMap != null) {	if (context.linkChildOpWithDummyOp.containsKey(mj)) {	for (Operator<?> dummy: context.linkChildOpWithDummyOp.get(mj)) {	work.addDummyOp((HashTableDummyOperator) dummy);	}	}	for (Entry<BaseWork, SparkEdgeProperty> parentWorkMap : linkWorkMap.entrySet()) {	BaseWork parentWork = parentWorkMap.getKey();	
connecting with 

for (Operator<?> dummy: context.linkChildOpWithDummyOp.get(mj)) {	work.addDummyOp((HashTableDummyOperator) dummy);	}	}	for (Entry<BaseWork, SparkEdgeProperty> parentWorkMap : linkWorkMap.entrySet()) {	BaseWork parentWork = parentWorkMap.getKey();	SparkEdgeProperty edgeProp = parentWorkMap.getValue();	sparkWork.connect(parentWork, work, edgeProp);	for (ReduceSinkOperator r : context.linkWorkWithReduceSinkMap.get(parentWork)) {	if (r.getConf().getOutputName() != null) {	
cloning reduce sink for multi child broadcast edge 

if (root.getNumParent() > 0) {	Preconditions.checkArgument(work instanceof ReduceWork, "AssertionError: expected work to be a ReduceWork, but was " + work.getClass().getName());	ReduceWork reduceWork = (ReduceWork) work;	for (Operator<?> parent : new ArrayList<Operator<?>>(root.getParentOperators())) {	Preconditions.checkArgument(parent instanceof ReduceSinkOperator, "AssertionError: expected operator to be a ReduceSinkOperator, but was " + parent.getClass().getName());	ReduceSinkOperator rsOp = (ReduceSinkOperator) parent;	SparkEdgeProperty edgeProp = GenSparkUtils.getEdgeProperty(context.conf, rsOp, reduceWork);	rsOp.getConf().setOutputName(reduceWork.getName());	GenMapRedUtils.setKeyAndValueDesc(reduceWork, rsOp);	context.leafOpToFollowingWorkInfo.put(rsOp, ObjectPair.create(edgeProp, reduceWork));	
removing as parent from 

}	}	if (!context.currentUnionOperators.isEmpty()) {	context.currentUnionOperators.clear();	context.workWithUnionOperators.add(work);	}	if (context.leafOpToFollowingWorkInfo.containsKey(operator)) {	ObjectPair<SparkEdgeProperty, ReduceWork> childWorkInfo = context. leafOpToFollowingWorkInfo.get(operator);	SparkEdgeProperty edgeProp = childWorkInfo.getFirst();	ReduceWork childWork = childWorkInfo.getSecond();	
second pass leaf operator has common downstream work 

context.currentUnionOperators.clear();	context.workWithUnionOperators.add(work);	}	if (context.leafOpToFollowingWorkInfo.containsKey(operator)) {	ObjectPair<SparkEdgeProperty, ReduceWork> childWorkInfo = context. leafOpToFollowingWorkInfo.get(operator);	SparkEdgeProperty edgeProp = childWorkInfo.getFirst();	ReduceWork childWork = childWorkInfo.getSecond();	if (sparkWork.getEdgeProperty(work, childWork) == null) {	sparkWork.connect(work, childWork, edgeProp);	} else {	
work is already connected to before 

}	if (context.leafOpToFollowingWorkInfo.containsKey(operator)) {	ObjectPair<SparkEdgeProperty, ReduceWork> childWorkInfo = context. leafOpToFollowingWorkInfo.get(operator);	SparkEdgeProperty edgeProp = childWorkInfo.getFirst();	ReduceWork childWork = childWorkInfo.getSecond();	if (sparkWork.getEdgeProperty(work, childWork) == null) {	sparkWork.connect(work, childWork, edgeProp);	} else {	}	} else {	
first pass leaf operator 

========================= hive sample_3564 =========================

public String[] getLocations(InputSplit split) throws IOException {	if (!(split instanceof FileSplit)) {	if (LOG.isDebugEnabled()) {	
split is not a filesplit using default locations 

========================= hive sample_4022 =========================

}	needHashTableSetup = false;	}	batchCounter++;	for(VectorExpression ve : bigTableFilterExpressions) {	ve.evaluate(batch);	}	final int inputLogicalSize = batch.size;	if (inputLogicalSize == 0) {	if (LOG.isDebugEnabled()) {	
batch empty 

joinResult = JoinUtil.JoinResult.NOMATCH;	} else {	long key = vector[0];	if (useMinMax && (key < min || key > max)) {	joinResult = JoinUtil.JoinResult.NOMATCH;	} else {	joinResult = hashMultiSet.contains(key, hashMultiSetResults[0]);	}	}	if (LOG.isDebugEnabled()) {	
batch repeated joinresult 

joinResult = JoinUtil.JoinResult.NOMATCH;	} else {	joinResult = hashMultiSet.contains(key, hashMultiSetResults[0]);	}	}	if (LOG.isDebugEnabled()) {	}	finishInnerBigOnlyRepeated(batch, joinResult, hashMultiSetResults[0]);	} else {	if (LOG.isDebugEnabled()) {	
batch non repeated 

if (haveSaveKey) {	switch (saveJoinResult) {	case MATCH: equalKeySeriesCount++;	break;	case SPILL: hashMultiSetResultCount++;	break;	case NOMATCH: break;	}	}	if (LOG.isDebugEnabled()) {	
allmatchs equalkeyseriesvaluecounts equalkeyseriesallmatchindices equalkeyseriesduplicatecounts spills spillhashmapresultindices hashmapresults 

========================= hive sample_4188 =========================

} else {	test.setStatus(Status.inProgress());	test.setDequeueTime(System.currentTimeMillis());	if(mExecutionContext == null) {	mExecutionContext = createExceutionContext();	}	test.setExecutionStartTime(System.currentTimeMillis());	TestStartRequest startRequest = test.getStartRequest();	String profile = startRequest.getProfile();	File profileConfFile = new File(mExecutionContextConfiguration.getProfileDirectory(), String.format("%s.properties", profile));	
attempting to run using profile file 

if(result == Constants.EXIT_CODE_SUCCESS) {	test.setStatus(Status.ok());	} else {	test.setStatus(Status.failed("Tests failed with exit code " + result));	}	logStream.flush();	mExecutionContext.replaceBadHosts();	}	}	} catch (Exception e) {	
unxpected error 

private ExecutionContext createExceutionContext() throws ServiceNotAvailableException, InterruptedException, CreateHostsFailedException {	long start = System.currentTimeMillis();	
attempting to create a new execution context 

private ExecutionContext createExceutionContext() throws ServiceNotAvailableException, InterruptedException, CreateHostsFailedException {	long start = System.currentTimeMillis();	ExecutionContext result = mExecutionContextProvider.createExecutionContext();	long elapsedTime = System.currentTimeMillis() - start;	
context creation time seconds 

========================= hive sample_5633 =========================

if (ctx == null) {	ctx = new Context(conf);	cleanContext = true;	String queryId = HiveConf.getVar(conf, HiveConf.ConfVars.HIVEQUERYID);	WmContext wmContext = new WmContext(System.currentTimeMillis(), queryId);	ctx.setWmContext(wmContext);	}	SessionState ss = SessionState.get();	TezSessionState session = sessionRef.value = ss.getTezSession();	if (session != null && !session.isOpen()) {	
the session has not been opened 

WmContext wmContext = ctx.getWmContext();	JobConf jobConf = utils.createConfiguration(conf);	String[] allNonConfFiles = work.configureJobConfAndExtractJars(jobConf);	Path scratchDir = utils.createTezDir(ctx.getMRScratchDir(), conf);	CallerContext callerContext = CallerContext.create( "HIVE", queryPlan.getQueryId(), "HIVE_QUERY_ID", queryPlan.getQueryStr());	perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.TEZ_GET_SESSION);	session = sessionRef.value = WorkloadManagerFederation.getSession( sessionRef.value, conf, mi, getWork().getLlapMode(), wmContext);	perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.TEZ_GET_SESSION);	try {	ss.setTezSession(session);	
subscribed to counters for queryid 

}	TezJobMonitor monitor = new TezJobMonitor(work.getAllWork(), dagClient, conf, dag, ctx);	rc = monitor.monitorExecution();	if (rc != 0) {	this.setException(new HiveException(monitor.getDiagnostics()));	}	try {	Set<StatusGetOpts> statusGetOpts = EnumSet.of(StatusGetOpts.GET_COUNTERS);	counters = dagClient.getDAGStatus(statusGetOpts).getDAGCounters();	} catch (Exception err) {	
failed to get counters ignoring summary info will be incomplete 

} catch (Exception err) {	counters = null;	}	} finally {	wmContext = ctx.getWmContext();	try {	if (sessionRef.value != null) {	sessionRef.value.returnToSessionManager();	}	} catch (Exception e) {	
failed to return session to pool 

}	if (LOG.isInfoEnabled() && counters != null && (HiveConf.getBoolVar(conf, HiveConf.ConfVars.TEZ_EXEC_SUMMARY) || Utilities.isPerfOrAboveLogging(conf))) {	for (CounterGroup group: counters) {	LOG.info(group.getDisplayName() +":");	for (TezCounter counter: group) {	LOG.info("   "+counter.getDisplayName()+": "+counter.getValue());	}	}	}	} catch (Exception e) {	
failed to execute tez graph 

for (BaseWork w : work.getAllWork()) {	JobConf workCfg = workToConf.get(w);	if (workCfg != null) {	Utilities.clearWorkMapForConf(workCfg);	}	}	if (cleanContext) {	try {	ctx.clear();	} catch (Exception e) {	
failed to clean up after tez job 

private void closeDagClientOnCancellation(DAGClient dagClient) {	try {	dagClient.tryKillDAG();	
waiting for tez task to shut down 

private void closeDagClientOnCancellation(DAGClient dagClient) {	try {	dagClient.tryKillDAG();	dagClient.waitForCompletion();	} catch (Exception ex) {	
failed to shut down teztask 

private static void closeDagClientWithoutEx(DAGClient dagClient) {	try {	dagClient.close();	} catch (Exception e) {	
failed to close dagclient 

========================= hive sample_3964 =========================

public ThriftHttpServlet(TProcessor processor, TProtocolFactory protocolFactory, String authType, UserGroupInformation serviceUGI, UserGroupInformation httpUGI, HiveAuthFactory hiveAuthFactory) {	super(processor, protocolFactory);	this.authType = authType;	this.serviceUGI = serviceUGI;	this.httpUGI = httpUGI;	this.hiveAuthFactory = hiveAuthFactory;	this.isCookieAuthEnabled = hiveConf.getBoolVar( ConfVars.HIVE_SERVER2_THRIFT_HTTP_COOKIE_AUTH_ENABLED);	if (isCookieAuthEnabled) {	String secret = Long.toString(RAN.nextLong());	
using the random number as the secret for cookie generation 

protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {	String clientUserName = null;	String clientIpAddress;	boolean requireNewCookie = false;	try {	if (hiveConf.getBoolean(ConfVars.HIVE_SERVER2_XSRF_FILTER_ENABLED.varname,false)){	boolean continueProcessing = Utils.doXsrfFilter(request,response,null,null);	if (!continueProcessing){	
request did not have valid xsrf header rejecting 

if (hiveConf.getBoolean(ConfVars.HIVE_SERVER2_XSRF_FILTER_ENABLED.varname,false)){	boolean continueProcessing = Utils.doXsrfFilter(request,response,null,null);	if (!continueProcessing){	return;	}	}	if (isCookieAuthEnabled) {	clientUserName = validateCookie(request);	requireNewCookie = (clientUserName == null);	if (requireNewCookie) {	
could not validate cookie sent will try to generate a new cookie 

if ((delegationToken != null) && (!delegationToken.isEmpty())) {	clientUserName = doTokenAuth(request, response);	} else {	clientUserName = doKerberosAuth(request);	}	}	else {	clientUserName = doPasswdAuth(request, authType);	}	}	
client username 

else {	clientUserName = doPasswdAuth(request, authType);	}	}	SessionManager.setUserName(clientUserName);	String doAsQueryParam = getDoAsQueryParam(request.getQueryString());	if (doAsQueryParam != null) {	SessionManager.setProxyUserName(doAsQueryParam);	}	clientIpAddress = request.getRemoteAddr();	
client ip address 

SessionManager.setForwardedAddresses(Collections.<String>emptyList());	}	if (requireNewCookie && !authType.equalsIgnoreCase(HiveAuthConstants.AuthTypes.NOSASL.toString())) {	String cookieToken = HttpAuthUtils.createCookieToken(clientUserName);	Cookie hs2Cookie = createCookie(signer.signCookie(cookieToken));	if (isHttpOnlyCookie) {	response.setHeader("SET-COOKIE", getHttpOnlyCookieHeader(hs2Cookie));	} else {	response.addCookie(hs2Cookie);	}	
cookie added for clientusername 

Cookie hs2Cookie = createCookie(signer.signCookie(cookieToken));	if (isHttpOnlyCookie) {	response.setHeader("SET-COOKIE", getHttpOnlyCookieHeader(hs2Cookie));	} else {	response.addCookie(hs2Cookie);	}	}	super.doPost(request, response);	}	catch (HttpAuthenticationException e) {	
error 

for (Cookie currCookie : cookies) {	currName = currCookie.getName();	if (!currName.equals(AUTH_COOKIE)) {	continue;	}	currValue = currCookie.getValue();	currValue = signer.verifyAndExtract(currValue);	if (currValue != null) {	String userName = HttpAuthUtils.getUserNameFromCookieToken(currValue);	if (userName == null) {	
invalid cookie token 

continue;	}	currValue = currCookie.getValue();	currValue = signer.verifyAndExtract(currValue);	if (currValue != null) {	String userName = HttpAuthUtils.getUserNameFromCookieToken(currValue);	if (userName == null) {	continue;	}	if (LOG.isDebugEnabled()) {	
validated the cookie for user 

private String validateCookie(HttpServletRequest request) throws UnsupportedEncodingException {	Cookie[] cookies = request.getCookies();	if (cookies == null) {	if (LOG.isDebugEnabled()) {	
no valid cookies associated with the request 

private String validateCookie(HttpServletRequest request) throws UnsupportedEncodingException {	Cookie[] cookies = request.getCookies();	if (cookies == null) {	if (LOG.isDebugEnabled()) {	}	return null;	}	if (LOG.isDebugEnabled()) {	
received cookies 

private String doKerberosAuth(HttpServletRequest request) throws HttpAuthenticationException {	if (httpUGI != null) {	try {	return httpUGI.doAs(new HttpKerberosServerAction(request, httpUGI));	} catch (Exception e) {	
failed to authenticate with http host kerberos principal trying with hive host kerberos principal 

private String doKerberosAuth(HttpServletRequest request) throws HttpAuthenticationException {	if (httpUGI != null) {	try {	return httpUGI.doAs(new HttpKerberosServerAction(request, httpUGI));	} catch (Exception e) {	}	}	try {	return serviceUGI.doAs(new HttpKerberosServerAction(request, serviceUGI));	} catch (Exception e) {	
failed to authenticate with hive host kerberos principal 

private static String getDoAsQueryParam(String queryString) {	if (LOG.isDebugEnabled()) {	
url query string 

========================= hive sample_2363 =========================

private void runCycleAnalysisForPartitionPruning(OptimizeSparkProcContext procCtx) {	boolean cycleFree = false;	while (!cycleFree) {	cycleFree = true;	Set<Set<Operator<?>>> components = getComponents(procCtx);	for (Set<Operator<?>> component : components) {	if (LOG.isDebugEnabled()) {	
component 

private void runCycleAnalysisForPartitionPruning(OptimizeSparkProcContext procCtx) {	boolean cycleFree = false;	while (!cycleFree) {	cycleFree = true;	Set<Set<Operator<?>>> components = getComponents(procCtx);	for (Set<Operator<?>> component : components) {	if (LOG.isDebugEnabled()) {	for (Operator<?> co : component) {	
operator 

boolean cycleFree = false;	while (!cycleFree) {	cycleFree = true;	Set<Set<Operator<?>>> components = getComponents(procCtx);	for (Set<Operator<?>> component : components) {	if (LOG.isDebugEnabled()) {	for (Operator<?> co : component) {	}	}	if (component.size() != 1) {	
found cycle in operator plan 

if (LOG.isDebugEnabled()) {	for (Operator<?> co : component) {	}	}	if (component.size() != 1) {	cycleFree = false;	removeDPPOperator(component, procCtx);	break;	}	}	
cycle free 

if (o instanceof SparkPartitionPruningSinkOperator) {	if (toRemove == null || o.getConf().getStatistics().getDataSize() > toRemove.getConf().getStatistics() .getDataSize()) {	toRemove = (SparkPartitionPruningSinkOperator) o;	}	}	}	if (toRemove == null) {	return;	}	OperatorUtils.removeBranch(toRemove);	
disabling dynamic pruning for needed to break cyclic dependency 

private void connect(Operator<?> o, AtomicInteger index, Stack<Operator<?>> nodes, Map<Operator<?>, Integer> indexes, Map<Operator<?>, Integer> lowLinks, Set<Set<Operator<?>>> components) {	indexes.put(o, index.get());	lowLinks.put(o, index.get());	index.incrementAndGet();	nodes.push(o);	List<Operator<?>> children;	if (o instanceof SparkPartitionPruningSinkOperator) {	children = new ArrayList<>();	children.addAll(o.getChildOperators());	TableScanOperator ts = ((SparkPartitionPruningSinkDesc) o.getConf()).getTableScan();	
adding special edge 

protected void optimizeTaskPlan(List<Task<? extends Serializable>> rootTasks, ParseContext pCtx, Context ctx) throws SemanticException {	PERF_LOGGER.PerfLogBegin(CLASS_NAME, PerfLogger.SPARK_OPTIMIZE_TASK_TREE);	PhysicalContext physicalCtx = new PhysicalContext(conf, pCtx, pCtx.getContext(), rootTasks, pCtx.getFetchTask());	physicalCtx = new SplitSparkWorkResolver().resolve(physicalCtx);	if (conf.getBoolVar(HiveConf.ConfVars.HIVESKEWJOIN)) {	(new SparkSkewJoinResolver()).resolve(physicalCtx);	} else {	
skipping runtime skew join optimization 

(new SparkSkewJoinResolver()).resolve(physicalCtx);	} else {	}	physicalCtx = new SparkMapJoinResolver().resolve(physicalCtx);	if (conf.isSparkDPPAny()) {	physicalCtx = new SparkDynamicPartitionPruningResolver().resolve(physicalCtx);	}	if (conf.getBoolVar(HiveConf.ConfVars.HIVENULLSCANOPTIMIZE)) {	physicalCtx = new NullScanOptimizer().resolve(physicalCtx);	} else {	
skipping null scan query optimization 

if (conf.isSparkDPPAny()) {	physicalCtx = new SparkDynamicPartitionPruningResolver().resolve(physicalCtx);	}	if (conf.getBoolVar(HiveConf.ConfVars.HIVENULLSCANOPTIMIZE)) {	physicalCtx = new NullScanOptimizer().resolve(physicalCtx);	} else {	}	if (conf.getBoolVar(HiveConf.ConfVars.HIVEMETADATAONLYQUERIES)) {	physicalCtx = new MetadataOnlyOptimizer().resolve(physicalCtx);	} else {	
skipping metadata only query optimization 

physicalCtx = new NullScanOptimizer().resolve(physicalCtx);	} else {	}	if (conf.getBoolVar(HiveConf.ConfVars.HIVEMETADATAONLYQUERIES)) {	physicalCtx = new MetadataOnlyOptimizer().resolve(physicalCtx);	} else {	}	if (conf.getBoolVar(HiveConf.ConfVars.HIVE_CHECK_CROSS_PRODUCT)) {	physicalCtx = new SparkCrossProductCheck().resolve(physicalCtx);	} else {	
skipping cross product analysis 

physicalCtx = new MetadataOnlyOptimizer().resolve(physicalCtx);	} else {	}	if (conf.getBoolVar(HiveConf.ConfVars.HIVE_CHECK_CROSS_PRODUCT)) {	physicalCtx = new SparkCrossProductCheck().resolve(physicalCtx);	} else {	}	if (conf.getBoolVar(HiveConf.ConfVars.HIVE_VECTORIZATION_ENABLED) && ctx.getExplainAnalyze() == null) {	(new Vectorizer()).resolve(physicalCtx);	} else {	
skipping vectorization 

physicalCtx = new SparkCrossProductCheck().resolve(physicalCtx);	} else {	}	if (conf.getBoolVar(HiveConf.ConfVars.HIVE_VECTORIZATION_ENABLED) && ctx.getExplainAnalyze() == null) {	(new Vectorizer()).resolve(physicalCtx);	} else {	}	if (!"none".equalsIgnoreCase(conf.getVar(HiveConf.ConfVars.HIVESTAGEIDREARRANGE))) {	(new StageIDsRearranger()).resolve(physicalCtx);	} else {	
skipping stage id rearranger 

(new Vectorizer()).resolve(physicalCtx);	} else {	}	if (!"none".equalsIgnoreCase(conf.getVar(HiveConf.ConfVars.HIVESTAGEIDREARRANGE))) {	(new StageIDsRearranger()).resolve(physicalCtx);	} else {	}	if (conf.getBoolVar(HiveConf.ConfVars.HIVE_COMBINE_EQUIVALENT_WORK_OPTIMIZATION)) {	new CombineEquivalentWorkResolver().resolve(physicalCtx);	} else {	
skipping combine equivalent work optimization 

========================= hive sample_3572 =========================

} else if (bucketCount < den && den % bucketCount == 0) {	Path[] ret = new Path[1];	ret[0] = part.getBucketPath((num - 1) % bucketCount);	return ret;	} else {	fullScanMsg = "Tablesample denominator " + den + " is not multiple/divisor of bucket count " + bucketCount + " of table " + part.getTable().getTableName();	}	} else {	fullScanMsg = isMmTable ? "MM table" : "Tablesample not on clustered columns";	}	
using full table scan 

LOG.info("Path pattern = " + pathPattern);	FileStatus srcs[] = fs.globStatus(new Path(pathPattern));	Arrays.sort(srcs);	boolean hasFile = false, allFile = true;	for (FileStatus src : srcs) {	if (sizeLeft <= 0) {	allFile = false;	break;	}	if (src.isDir()) {	
got directory 

}	if (src.isDir()) {	AddPathReturnStatus ret = addPath(fs, src.getPath().toString() + "/*", sizeLeft, fileLimit, retPathList);	if (ret == null) {	return null;	}	sizeLeft = ret.sizeLeft;	hasFile |= ret.hasFile;	allFile &= ret.allFile;	} else {	
got file 

========================= hive sample_3088 =========================

protected void initializeOp(Configuration hconf) throws HiveException {	super.initializeOp(hconf);	TableDesc tbl = this.getConf().getTbl();	try {	Deserializer serde = tbl.getDeserializerClass().newInstance();	SerDeUtils.initializeSerDe(serde, hconf, tbl.getProperties(), null);	this.outputObjInspector = serde.getObjectInspector();	} catch (Exception e) {	
generating output obj inspector from dummy object error 

========================= hive sample_4034 =========================

DumpMetaData dmd = new DumpMetaData(dumpRoot, conf);	Path cmRoot = new Path(conf.getVar(HiveConf.ConfVars.REPLCMDIR));	Long lastReplId;	if (work.isBootStrapDump()) {	lastReplId = bootStrapDump(dumpRoot, dmd, cmRoot);	} else {	lastReplId = incrementalDump(dumpRoot, dmd, cmRoot);	}	prepareReturnValues(Arrays.asList(dumpRoot.toUri().toString(), String.valueOf(lastReplId)), dumpSchema);	} catch (Exception e) {	
failed 

private void prepareReturnValues(List<String> values, String schema) throws SemanticException {	
preparereturnvalues 

String dbName = (null != work.dbNameOrPattern && !work.dbNameOrPattern.isEmpty()) ? work.dbNameOrPattern : "?";	replLogger = new IncrementalDumpLogger(dbName, dumpRoot.toString(), evFetcher.getDbNotificationEventsCount(work.eventFrom, dbName));	replLogger.startLog();	while (evIter.hasNext()) {	NotificationEvent ev = evIter.next();	lastReplId = ev.getEventId();	Path evRoot = new Path(dumpRoot, String.valueOf(lastReplId));	dumpEvent(ev, evRoot, cmRoot);	}	replLogger.endLog(lastReplId.toString());	
done dumping events preparing to return 

private Long bootStrapDump(Path dumpRoot, DumpMetaData dmd, Path cmRoot) throws Exception {	Hive hiveDb = getHive();	Long bootDumpBeginReplId = hiveDb.getMSC().getCurrentNotificationEventId().getEventId();	for (String dbName : Utils.matchesDb(hiveDb, work.dbNameOrPattern)) {	
replicationsemanticanalyzer analyzerepldump dumping db 

private Long bootStrapDump(Path dumpRoot, DumpMetaData dmd, Path cmRoot) throws Exception {	Hive hiveDb = getHive();	Long bootDumpBeginReplId = hiveDb.getMSC().getCurrentNotificationEventId().getEventId();	for (String dbName : Utils.matchesDb(hiveDb, work.dbNameOrPattern)) {	replLogger = new BootstrapDumpLogger(dbName, dumpRoot.toString(), Utils.getAllTables(getHive(), dbName).size(), getHive().getAllFunctions().size());	replLogger.startLog();	Path dbRoot = dumpDbMetadata(dbName, dumpRoot);	dumpFunctionMetadata(dbName, dumpRoot);	String uniqueKey = Utils.setDbBootstrapDumpState(hiveDb, dbName);	for (String tblName : Utils.matchesTbl(hiveDb, dbName, work.tableNameOrPattern)) {	
analyzerepldump dumping table to db root 

dumpFunctionMetadata(dbName, dumpRoot);	String uniqueKey = Utils.setDbBootstrapDumpState(hiveDb, dbName);	for (String tblName : Utils.matchesTbl(hiveDb, dbName, work.tableNameOrPattern)) {	dumpTable(dbName, tblName, dbRoot);	dumpConstraintMetadata(dbName, tblName, dbRoot);	}	Utils.resetDbBootstrapDumpState(hiveDb, dbName, uniqueKey);	replLogger.endLog(bootDumpBeginReplId.toString());	}	Long bootDumpEndReplId = hiveDb.getMSC().getCurrentNotificationEventId().getEventId();	
bootstrap object dump phase took from to 

replLogger.endLog(bootDumpBeginReplId.toString());	}	Long bootDumpEndReplId = hiveDb.getMSC().getCurrentNotificationEventId().getEventId();	IMetaStoreClient.NotificationFilter evFilter = new DatabaseAndTableFilter(work.dbNameOrPattern, work.tableNameOrPattern);	EventUtils.MSClientNotificationFetcher evFetcher = new EventUtils.MSClientNotificationFetcher(hiveDb.getMSC());	EventUtils.NotificationEventIterator evIter = new EventUtils.NotificationEventIterator( evFetcher, bootDumpBeginReplId, Ints.checkedCast(bootDumpEndReplId - bootDumpBeginReplId) + 1, evFilter);	while (evIter.hasNext()) {	NotificationEvent ev = evIter.next();	Path eventRoot = new Path(dumpRoot, String.valueOf(ev.getEventId()));	}	
consolidation done preparing to return 

private HiveWrapper.Tuple<Function> functionTuple(String functionName, String dbName) {	try {	HiveWrapper.Tuple<Function> tuple = new HiveWrapper(getHive(), dbName).function(functionName);	if (tuple.object.getResourceUris().isEmpty()) {	
not replicating function as it seems to have been created without using clause 

private HiveWrapper.Tuple<Function> functionTuple(String functionName, String dbName) {	try {	HiveWrapper.Tuple<Function> tuple = new HiveWrapper(getHive(), dbName).function(functionName);	if (tuple.object.getResourceUris().isEmpty()) {	return null;	}	return tuple;	} catch (HiveException e) {	
function could not be found we are ignoring it as it can be a valid state 

========================= hive sample_3933 =========================

public void handle(Context withinContext) throws Exception {	
processing drop partition message 

========================= hive sample_3460 =========================

public HCatTable storageHandler(String storageHandler) throws HCatException {	this.tblProps.put( org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_STORAGE, storageHandler);	
hivestoragehandlers can t be reliably instantiated on the client side attempting to derive input outputformat settings from storagehandler on best effort 

public HCatTable storageHandler(String storageHandler) throws HCatException {	this.tblProps.put( org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_STORAGE, storageHandler);	try {	HiveStorageHandler sh = HiveUtils.getStorageHandler(getConf(), storageHandler);	this.sd.setInputFormat(sh.getInputFormatClass().getName());	this.sd.setOutputFormat(sh.getOutputFormatClass().getName());	this.sd.getSerdeInfo().setSerializationLib(sh.getSerDeClass().getName());	} catch (HiveException e) {	
could not derive input outputformat and serde settings from storagehandler these values need to be set explicitly 

========================= hive sample_869 =========================

private void addOperation(Operation operation) {	
adding operation 

private Operation removeOperation(OperationHandle opHandle) {	Operation operation = handleToOperation.remove(opHandle);	String queryId = getQueryId(operation);	queryIdOperation.remove(queryId);	
removed queryid corresponding to operation 

private Operation removeTimedOutOperation(OperationHandle operationHandle) {	Operation operation = handleToOperation.get(operationHandle);	if (operation != null && operation.isTimedOut(System.currentTimeMillis())) {	LOG.info("Operation is timed out,operation=" + operation.getHandle() + ",state=" + operation.getState().toString());	Metrics metrics = MetricsFactory.getInstance();	if (metrics != null) {	try {	metrics.decrementCounter(MetricsConstant.OPEN_OPERATIONS);	} catch (Exception e) {	
error decrementing open operations metric reported values may be incorrect 

private void removeSafeQueryInfo(OperationHandle operationHandle) {	synchronized (webuiLock) {	String opKey = operationHandle.getHandleIdentifier().toString();	QueryInfo display = liveQueryInfos.remove(opKey);	if (display == null) {	
unexpected display object value of null for operation 

public void cancelOperation(OperationHandle opHandle, String errMsg) throws HiveSQLException {	Operation operation = getOperation(opHandle);	OperationState opState = operation.getStatus().getState();	if (opState.isTerminal()) {	
operation is already aborted in state 

public void cancelOperation(OperationHandle opHandle, String errMsg) throws HiveSQLException {	Operation operation = getOperation(opHandle);	OperationState opState = operation.getStatus().getState();	if (opState.isTerminal()) {	} else {	
attempting to cancel from state 

public void closeOperation(OperationHandle opHandle) throws HiveSQLException {	
closing operation 

public void closeOperation(OperationHandle opHandle) throws HiveSQLException {	Operation operation = removeOperation(opHandle);	if (operation == null) {	throw new HiveSQLException("Operation does not exist: " + opHandle);	}	Metrics metrics = MetricsFactory.getInstance();	if (metrics != null) {	try {	metrics.decrementCounter(MetricsConstant.OPEN_OPERATIONS);	} catch (Exception e) {	
error reporting close operation to metrics system 

public List<Operation> removeExpiredOperations(OperationHandle[] handles) {	List<Operation> removed = new ArrayList<Operation>();	for (OperationHandle handle : handles) {	Operation operation = removeTimedOutOperation(handle);	if (operation != null) {	
operation is timed out and will be closed 

========================= hive sample_2334 =========================

private void setupOutput(String spillLocalDirs) throws IOException, HiveException {	FileOutputStream fos = null;	try {	if (parentDir == null) {	parentDir = FileUtils.createLocalDirsTempFile(spillLocalDirs, "key-value-container", "", true);	parentDir.deleteOnExit();	}	if (tmpFile == null || input != null) {	tmpFile = File.createTempFile("KeyValueContainer", ".tmp", parentDir);	
keyvaluecontainer created temp file 

========================= hive sample_4063 =========================

public void close() throws Exception {	if (readaheadRequest != null) {	readaheadRequest.cancel();	}	if (manageOsCache && getEndOffset() - getStartOffset() > 0) {	try {	NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(identifier, fd, getStartOffset(), getEndOffset() - getStartOffset(), NativeIO.POSIX.POSIX_FADV_DONTNEED);	} catch (Throwable t) {	
failed to manage os cache for 

========================= hive sample_2158 =========================

private JsonReporter(MetricRegistry registry, String name, MetricFilter filter, TimeUnit rateUnit, TimeUnit durationUnit, Configuration conf) {	super(registry, name, filter, rateUnit, durationUnit);	String pathString = MetastoreConf.getVar(conf, MetastoreConf.ConfVars .METRICS_JSON_FILE_LOCATION);	path = Paths.get(pathString).toAbsolutePath();	
reporting metrics to 

public void start(long period, TimeUnit unit) {	if (!metricsDir.toFile().exists()) {	
metrics directory does not exist creating one 

public void start(long period, TimeUnit unit) {	if (!metricsDir.toFile().exists()) {	try {	Files.createDirectories(metricsDir, DIR_ATTRS);	} catch (IOException e) {	
failed to initialize json reporter failed to create directory 

public void report(SortedMap<String, Gauge> sortedMap, SortedMap<String, Counter> sortedMap1, SortedMap<String, Histogram> sortedMap2, SortedMap<String, Meter> sortedMap3, SortedMap<String, Timer> sortedMap4) {	String json;	try {	json = jsonWriter.writeValueAsString(registry);	} catch (JsonProcessingException e) {	
unable to convert json to string 

String json;	try {	json = jsonWriter.writeValueAsString(registry);	} catch (JsonProcessingException e) {	return;	}	Path tmpFile = null;	try {	tmpFile = Files.createTempFile(metricsDir, "hmsmetrics", "json", FILE_ATTRS);	} catch (IOException e) {	
failed to create temp file for json metrics 

json = jsonWriter.writeValueAsString(registry);	} catch (JsonProcessingException e) {	return;	}	Path tmpFile = null;	try {	tmpFile = Files.createTempFile(metricsDir, "hmsmetrics", "json", FILE_ATTRS);	} catch (IOException e) {	return;	} catch (SecurityException e) {	
failed to create temp file for json metrics no permissions 

return;	}	Path tmpFile = null;	try {	tmpFile = Files.createTempFile(metricsDir, "hmsmetrics", "json", FILE_ATTRS);	} catch (IOException e) {	return;	} catch (SecurityException e) {	return;	} catch (UnsupportedOperationException e) {	
failed to create temp file for json metrics operartion not supported 

return;	} catch (SecurityException e) {	return;	} catch (UnsupportedOperationException e) {	return;	}	try {	try (BufferedWriter bw = new BufferedWriter(new FileWriter(tmpFile.toFile()))) {	bw.write(json);	} catch (IOException e) {	
unable to write to temp file 

}	try {	try (BufferedWriter bw = new BufferedWriter(new FileWriter(tmpFile.toFile()))) {	bw.write(json);	} catch (IOException e) {	return;	}	try {	Files.move(tmpFile, path, StandardCopyOption.ATOMIC_MOVE);	} catch (Exception e) {	
unable to rename temp file to 

}	try {	try (BufferedWriter bw = new BufferedWriter(new FileWriter(tmpFile.toFile()))) {	bw.write(json);	} catch (IOException e) {	return;	}	try {	Files.move(tmpFile, path, StandardCopyOption.ATOMIC_MOVE);	} catch (Exception e) {	
exception during rename 

}	try {	Files.move(tmpFile, path, StandardCopyOption.ATOMIC_MOVE);	} catch (Exception e) {	}	} finally {	if (tmpFile.toFile().exists()) {	try {	Files.delete(tmpFile);	} catch (Exception e) {	
failed to delete temporary metrics file 

========================= hive sample_1830 =========================

} else {	if (isEscaped && bytes[fieldByteEnd] == escapeChar && fieldByteEnd + 1 < structByteEnd) {	fieldByteEnd += 2;	} else {	fieldByteEnd++;	}	}	}	if (!extraFieldWarned && fieldByteEnd < structByteEnd) {	extraFieldWarned = true;	
extra bytes detected at the end of the row ignoring similar problems 

} else {	fieldByteEnd++;	}	}	}	if (!extraFieldWarned && fieldByteEnd < structByteEnd) {	extraFieldWarned = true;	}	if (!missingFieldWarned && fieldId < fields.length) {	missingFieldWarned = true;	
missing fields expected fields but only got ignoring similar problems 

========================= hive sample_5465 =========================

basePersistDirectory = System.getProperty("java.io.tmpdir");	}	Integer maxRowInMemory = HiveConf.getIntVar(jc, HiveConf.ConfVars.HIVE_DRUID_MAX_ROW_IN_MEMORY);	IndexSpec indexSpec;	if ("concise".equals(HiveConf.getVar(jc, HiveConf.ConfVars.HIVE_DRUID_BITMAP_FACTORY_TYPE))) {	indexSpec = new IndexSpec(new ConciseBitmapSerdeFactory(), null, null, null);	} else {	indexSpec = new IndexSpec(new RoaringBitmapSerdeFactory(true), null, null, null);	}	RealtimeTuningConfig realtimeTuningConfig = new RealtimeTuningConfig(maxRowInMemory, null, null, new File(basePersistDirectory, dataSource), new CustomVersioningPolicy(version), null, null, null, indexSpec, true, 0, 0, true, null, 0L );	
running with data schema s 

========================= hive sample_18 =========================

public ParquetRecordWriterWrapper( final OutputFormat<Void, ParquetHiveRecord> realOutputFormat, final JobConf jobConf, final String name, final Progressable progress, Properties tableProperties) throws IOException {	try {	TaskAttemptID taskAttemptID = TaskAttemptID.forName(jobConf.get("mapred.task.id"));	if (taskAttemptID == null) {	taskAttemptID = new TaskAttemptID();	}	taskContext = ContextUtil.newTaskAttemptContext(jobConf, taskAttemptID);	
initialize serde with table properties 

public ParquetRecordWriterWrapper( final OutputFormat<Void, ParquetHiveRecord> realOutputFormat, final JobConf jobConf, final String name, final Progressable progress, Properties tableProperties) throws IOException {	try {	TaskAttemptID taskAttemptID = TaskAttemptID.forName(jobConf.get("mapred.task.id"));	if (taskAttemptID == null) {	taskAttemptID = new TaskAttemptID();	}	taskContext = ContextUtil.newTaskAttemptContext(jobConf, taskAttemptID);	initializeSerProperties(taskContext, tableProperties);	
creating real writer to write at 

public ParquetRecordWriterWrapper( final OutputFormat<Void, ParquetHiveRecord> realOutputFormat, final JobConf jobConf, final String name, final Progressable progress, Properties tableProperties) throws IOException {	try {	TaskAttemptID taskAttemptID = TaskAttemptID.forName(jobConf.get("mapred.task.id"));	if (taskAttemptID == null) {	taskAttemptID = new TaskAttemptID();	}	taskContext = ContextUtil.newTaskAttemptContext(jobConf, taskAttemptID);	initializeSerProperties(taskContext, tableProperties);	realWriter = ((ParquetOutputFormat) realOutputFormat).getRecordWriter(taskContext, new Path(name));	
real writer 

private void initializeSerProperties(JobContext job, Properties tableProperties) {	String blockSize = tableProperties.getProperty(ParquetOutputFormat.BLOCK_SIZE);	Configuration conf = ContextUtil.getConfiguration(job);	if (blockSize != null && !blockSize.isEmpty()) {	
get override parquet block size property via tblproperties 

private void initializeSerProperties(JobContext job, Properties tableProperties) {	String blockSize = tableProperties.getProperty(ParquetOutputFormat.BLOCK_SIZE);	Configuration conf = ContextUtil.getConfiguration(job);	if (blockSize != null && !blockSize.isEmpty()) {	conf.setInt(ParquetOutputFormat.BLOCK_SIZE, Integer.parseInt(blockSize));	}	String enableDictionaryPage = tableProperties.getProperty(ParquetOutputFormat.ENABLE_DICTIONARY);	if (enableDictionaryPage != null && !enableDictionaryPage.isEmpty()) {	
get override parquet enable dictionary property via tblproperties 

Configuration conf = ContextUtil.getConfiguration(job);	if (blockSize != null && !blockSize.isEmpty()) {	conf.setInt(ParquetOutputFormat.BLOCK_SIZE, Integer.parseInt(blockSize));	}	String enableDictionaryPage = tableProperties.getProperty(ParquetOutputFormat.ENABLE_DICTIONARY);	if (enableDictionaryPage != null && !enableDictionaryPage.isEmpty()) {	conf.setBoolean(ParquetOutputFormat.ENABLE_DICTIONARY, Boolean.parseBoolean(enableDictionaryPage));	}	String compressionName = tableProperties.getProperty(ParquetOutputFormat.COMPRESSION);	if (compressionName != null && !compressionName.isEmpty()) {	
get override compression properties via tblproperties 

========================= hive sample_3698 =========================

private void closeReader() {	if (!(reader instanceof MRReader)) {	LOG.warn("Cannot close " + (reader == null ? null : reader.getClass()));	return;	}	if (reader instanceof KeyValueInputMerger) {	KeyValueInputMerger kvMerger = (KeyValueInputMerger) reader;	kvMerger.clean();	}	
closing mrreader on error 

return;	}	if (reader instanceof KeyValueInputMerger) {	KeyValueInputMerger kvMerger = (KeyValueInputMerger) reader;	kvMerger.clean();	}	MRReader mrReader = (MRReader)reader;	try {	mrReader.close();	} catch (IOException ex) {	
failed to close the reader ignoring 

========================= hive sample_4018 =========================

private void dropPartitionsUsingExpressions(Table table, Map<String, String> partitionSpec, boolean ifExists, boolean deleteData) throws SemanticException, TException {	
hcatclient dropping partitions using partition predicate expressions 

private void dropPartitionsIteratively(String dbName, String tableName, Map<String, String> partitionSpec, boolean ifExists, boolean deleteData) throws HCatException, TException {	
hcatclient dropping partitions iteratively 

public void dropPartitions(String dbName, String tableName, Map<String, String> partitionSpec, boolean ifExists, boolean deleteData) throws HCatException {	LOG.info("HCatClient dropPartitions(db=" + dbName + ",table=" + tableName + ", partitionSpec: ["+ partitionSpec + "]).");	try {	dbName = checkDB(dbName);	Table table = hmsClient.getTable(dbName, tableName);	if (hiveConfig.getBoolVar(HiveConf.ConfVars.METASTORE_CLIENT_DROP_PARTITIONS_WITH_EXPRESSIONS)) {	try {	dropPartitionsUsingExpressions(table, partitionSpec, ifExists, deleteData);	}	catch (SemanticException parseFailure) {	
could not push down partition specification to back end for droppartitions resorting to iteration 

========================= hive sample_900 =========================

NumDistinctValueEstimator oldEst = aggregateData.getNdvEstimator();	NumDistinctValueEstimator newEst = newData.getNdvEstimator();	long ndv = -1;	if (oldEst.canMerge(newEst)) {	oldEst.mergeEstimators(newEst);	ndv = oldEst.estimateNumDistinctValues();	aggregateData.setNdvEstimator(oldEst);	} else {	ndv = Math.max(aggregateData.getNumDVs(), newData.getNumDVs());	}	
use bitvector to merge column s ndvs of and to be 

========================= hive sample_1922 =========================

public static synchronized ThreadPool initialize(Configuration conf) {	if (self == null) {	self = new ThreadPool(conf);	
threadpool initialized 

========================= hive sample_1888 =========================

if(command.getType() == RSyncCommand.Type.TO_LOCAL) {	cmd = mLocalCommandFactory.create(collector, String.format("timeout 1h rsync -vaPe \"ssh -i %s\" --timeout 600 %s@%s:%s %s", command.getPrivateKey(), command.getUser(), command.getHost(), command.getRemoteFile(), command.getLocalFile()));	} else if (command.getType() == RSyncCommand.Type.TO_LOCAL_NON_RECURSIVE) {	cmd = mLocalCommandFactory.create(collector, String.format("timeout 1h rsync --exclude \"*/\" -vaPe \"ssh -i %s\" --timeout 600 %s@%s:%s %s", command.getPrivateKey(), command.getUser(), command.getHost(), command.getRemoteFile(), command.getLocalFile()));	} else if(command.getType() == RSyncCommand.Type.FROM_LOCAL) {	cmd = mLocalCommandFactory.create(collector, String.format("timeout 1h rsync -vaPe \"ssh -i %s\" --timeout 600 --delete --delete-during --force %s %s@%s:%s", command.getPrivateKey(), command.getLocalFile(), command.getUser(), command.getHost(), command.getRemoteFile()));	} else {	throw new UnsupportedOperationException(String.valueOf(command.getType()));	}	if(mShutdown) {	
shutting down command 

cmd = mLocalCommandFactory.create(collector, String.format("timeout 1h rsync -vaPe \"ssh -i %s\" --timeout 600 --delete --delete-during --force %s %s@%s:%s", command.getPrivateKey(), command.getLocalFile(), command.getUser(), command.getHost(), command.getRemoteFile()));	} else {	throw new UnsupportedOperationException(String.valueOf(command.getType()));	}	if(mShutdown) {	cmd.kill();	command.setExitCode(Constants.EXIT_CODE_UNKNOWN);	return;	}	if(attempts++ <= 3 && cmd.getExitCode() != 0) {	
command exited with will retry 

========================= hive sample_5598 =========================

public LowLevelFifoCachePolicy() {	
fifo cache policy 

========================= hive sample_2174 =========================

public Token<LlapTokenIdentifier> selectToken(Text service, Collection<Token<? extends TokenIdentifier>> tokens) {	if (service == null) return null;	if (LOG.isDebugEnabled()) {	
looking for a token with service 

========================= hive sample_500 =========================

public void startCleanup(Configuration config) {	try {	ZooKeeperCleanup.startInstance(config);	} catch (Exception e) {	
cleanup instance didn t start 

========================= hive sample_853 =========================

public static void startHebHcatInMem() throws Exception {	Exception webhcatException = null;	int webhcatPort = 0;	boolean webhcatStarted = false;	for (int tryCount = 0; tryCount < MetaStoreTestUtils.RETRY_COUNT; tryCount++) {	try {	if (tryCount == MetaStoreTestUtils.RETRY_COUNT - 1) {	webhcatPort = 50111;	
unable to find free port using default 

for (int tryCount = 0; tryCount < MetaStoreTestUtils.RETRY_COUNT; tryCount++) {	try {	if (tryCount == MetaStoreTestUtils.RETRY_COUNT - 1) {	webhcatPort = 50111;	}	else {	webhcatPort = MetaStoreTestUtils.findFreePort();	}	templetonBaseUrl = templetonBaseUrl.replace("50111", Integer.toString(webhcatPort));	templetonServer = new Main(new String[] { "-D" + AppConfig.UNIT_TEST_MODE + "=true", "-D" + AppConfig.PORT + "=" + webhcatPort });	
starting main webhcat using port 

try {	if (tryCount == MetaStoreTestUtils.RETRY_COUNT - 1) {	webhcatPort = 50111;	}	else {	webhcatPort = MetaStoreTestUtils.findFreePort();	}	templetonBaseUrl = templetonBaseUrl.replace("50111", Integer.toString(webhcatPort));	templetonServer = new Main(new String[] { "-D" + AppConfig.UNIT_TEST_MODE + "=true", "-D" + AppConfig.PORT + "=" + webhcatPort });	templetonServer.run();	
main started 

}	else {	webhcatPort = MetaStoreTestUtils.findFreePort();	}	templetonBaseUrl = templetonBaseUrl.replace("50111", Integer.toString(webhcatPort));	templetonServer = new Main(new String[] { "-D" + AppConfig.UNIT_TEST_MODE + "=true", "-D" + AppConfig.PORT + "=" + webhcatPort });	templetonServer.run();	webhcatStarted = true;	break;	} catch (Exception ce) {	
attempt to start webhcat using port failed 

public static void stopWebHcatInMem() {	if(templetonServer != null) {	
stopping main 

public static void stopWebHcatInMem() {	if(templetonServer != null) {	templetonServer.stop();	
main stopped 

public void getStatus() throws IOException {	
getstatus 

public void getStatus() throws IOException {	MethodCallRetVal p = doHttpCall(templetonBaseUrl + "/status", HTTP_METHOD_TYPE.GET);	Assert.assertEquals(p.getAssertMsg(), HttpStatus.OK_200, p.httpStatusCode);	Assert.assertTrue(p.getAssertMsg(), jsonStringToSortedMap("{\"status\":\"ok\",\"version\":\"v1\"}").equals( jsonStringToSortedMap(p.responseBody)));	
getstatus 

public void listDataBases() throws IOException {	
listdatabases 

public void listDataBases() throws IOException {	MethodCallRetVal p = doHttpCall(templetonBaseUrl + "/ddl/database", HTTP_METHOD_TYPE.GET);	Assert.assertEquals(p.getAssertMsg(), HttpStatus.OK_200, p.httpStatusCode);	Assert.assertEquals(p.getAssertMsg(), "{\"databases\":[\"default\"]}", p.responseBody);	
listdatabases 

col.put("name", "col1");	col.put("type", "string");	List<Map<String, Object>> colList = new ArrayList<Map<String, Object>>(1);	colList.add(col);	props.put("columns", colList);	Map<String, Object> format = new HashMap<String, Object>();	format.put("storedAs", "rcfile");	props.put("format", format);	MethodCallRetVal createTbl = doHttpCall(templetonBaseUrl + "/ddl/database/default/table/test_table", HTTP_METHOD_TYPE.PUT, props, null);	Assert.assertEquals(createTbl.getAssertMsg(), HttpStatus.OK_200, createTbl.httpStatusCode);	
createtable resp 

switch (type) {	case GET: method = new GetMethod(uri);	break;	case DELETE: method = new DeleteMethod(uri);	break;	case PUT: method = new PutMethod(uri);	if(data == null) {	break;	}	String msgBody = JsonBuilder.mapToJson(data);	
msg body 

newParams[0] = new NameValuePair("user.name", username);	method.setQueryString(newParams);	}	String actualUri = "no URI";	try {	actualUri = method.getURI().toString();	LOG.debug(type + ": " + method.getURI().getEscapedURI());	int httpStatus = client.executeMethod(method);	LOG.debug("Http Status Code=" + httpStatus);	String resp = method.getResponseBodyAsString();	
response 

String actualUri = "no URI";	try {	actualUri = method.getURI().toString();	LOG.debug(type + ": " + method.getURI().getEscapedURI());	int httpStatus = client.executeMethod(method);	LOG.debug("Http Status Code=" + httpStatus);	String resp = method.getResponseBodyAsString();	return new MethodCallRetVal(httpStatus, resp, actualUri, method.getName());	}	catch (IOException ex) {	
dohttpcall failed 

========================= hive sample_783 =========================

public Token<JobTokenIdentifier> getPluginToken() {	if (this.token != null) return token;	String tokenString = getProperties().get(TezAmRegistryImpl.AM_PLUGIN_TOKEN);	if (tokenString == null || tokenString.isEmpty()) return null;	byte[] tokenBytes = Base64.decodeBase64(tokenString);	Token<JobTokenIdentifier> token = new Token<>();	try {	token.readFields(ByteStreams.newDataInput(tokenBytes));	} catch (IOException e) {	
couldn t read the plugin token from 

========================= hive sample_629 =========================

static void misbehave() throws RuntimeException{	TestObjectStoreInitRetry.debugTrace();	if (TestObjectStoreInitRetry.getInjectConnectFailure() > 0){	TestObjectStoreInitRetry.decrementInjectConnectFailure();	RuntimeException re = new JDOCanRetryException();	
misbehave 

MetastoreConf.setVar(conf, ConfVars.CONNECTURLKEY,jdbcUrl);	MetaStoreTestUtils.setConfForStandloneMode(conf);	FakeDerby fd = new FakeDerby();	ObjectStore objStore = new ObjectStore();	Exception savE = null;	try {	setInjectConnectFailure(5);	objStore.setConf(conf);	Assert.fail();	} catch (Exception e) {	
caught exception 

========================= hive sample_1575 =========================

static String getPartitionName(Path tablePath, Path partitionPath, Set<String> partCols) {	String result = null;	Path currPath = partitionPath;	
tablepath partcols 

static String getPartitionName(Path tablePath, Path partitionPath, Set<String> partCols) {	String result = null;	Path currPath = partitionPath;	while (currPath != null && !tablePath.equals(currPath)) {	String[] parts = currPath.getName().split("=");	if (parts != null && parts.length > 0) {	if (parts.length != 2) {	
is not a valid partition name 

private void checkPartitionDirs(Path basePath, Set<Path> allDirs, final List<String> partColNames) throws IOException, HiveException {	int poolSize = conf.getInt(ConfVars.METASTORE_FS_HANDLER_THREADS_COUNT.varname, 15);	ExecutorService executor;	if (poolSize <= 1) {	
using single threaded version of msck getpaths 

private void checkPartitionDirs(Path basePath, Set<Path> allDirs, final List<String> partColNames) throws IOException, HiveException {	int poolSize = conf.getInt(ConfVars.METASTORE_FS_HANDLER_THREADS_COUNT.varname, 15);	ExecutorService executor;	if (poolSize <= 1) {	executor = MoreExecutors.newDirectExecutorService();	} else {	
using multi threaded version of msck getpaths with number of threads 

========================= hive sample_5023 =========================

public void rpcClosed(Rpc rpc) {	if (isAlive) {	
client rpc channel closed unexpectedly 

public void stop() {	if (isAlive) {	isAlive = false;	try {	protocol.endSession();	} catch (Exception e) {	
exception while waiting for end session reply 

protocol.endSession();	} catch (Exception e) {	} finally {	driverRpc.close();	}	}	long endTime = System.currentTimeMillis() + DEFAULT_SHUTDOWN_TIMEOUT;	try {	driverThread.join(DEFAULT_SHUTDOWN_TIMEOUT);	} catch (InterruptedException ie) {	
interrupted before driver thread was finished 

} finally {	driverRpc.close();	}	}	long endTime = System.currentTimeMillis() + DEFAULT_SHUTDOWN_TIMEOUT;	try {	driverThread.join(DEFAULT_SHUTDOWN_TIMEOUT);	} catch (InterruptedException ie) {	}	if (endTime - System.currentTimeMillis() <= 0) {	
timed out shutting down remote driver interrupting 

String executorJavaOpts = Joiner.on(" ").skipNulls().join( "-Dhive.spark.log.dir=" + sparkLogDir, osxTestOpts, conf.get(EXECUTOR_OPTS_KEY));	File properties = File.createTempFile("spark-submit.", ".properties");	if (!properties.setReadable(false) || !properties.setReadable(true, true)) {	throw new IOException("Cannot change permissions of job properties file.");	}	properties.deleteOnExit();	Properties allProps = new Properties();	try {	URL sparkDefaultsUrl = Thread.currentThread().getContextClassLoader().getResource("spark-defaults.conf");	if (sparkDefaultsUrl != null) {	
loading spark defaults 

} finally {	writer.close();	}	String master = conf.get("spark.master");	Preconditions.checkArgument(master != null, "spark.master is not defined.");	String deployMode = conf.get("spark.submit.deployMode");	List<String> argv = Lists.newLinkedList();	if (sparkHome != null) {	argv.add(new File(sparkHome, "bin/spark-submit").getAbsolutePath());	} else {	
no spark home provided calling sparksubmit directly 

argv.add(principal);	argv.add("--keytab");	argv.add(keyTabFile);	}	}	}	if (hiveConf.getBoolVar(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS)) {	try {	String currentUser = Utils.getUGI().getShortUserName();	if (!currentUser.equals(System.getProperty("user.name"))) {	
attempting impersonation of 

argv.add("--remote-host");	argv.add(serverAddress);	argv.add("--remote-port");	argv.add(serverPort);	for (String hiveSparkConfKey : RpcConfiguration.HIVE_SPARK_RSC_CONFIGS) {	String value = RpcConfiguration.getValue(hiveConf, hiveSparkConfKey);	argv.add("--conf");	argv.add(String.format("%s=%s", hiveSparkConfKey, value));	}	String cmd = Joiner.on(" ").join(argv);	
running client driver with argv 

int exitCode = child.waitFor();	if (exitCode != 0) {	StringBuilder errStr = new StringBuilder();	synchronized(childErrorLog) {	Iterator iter = childErrorLog.iterator();	while(iter.hasNext()){	errStr.append(iter.next());	errStr.append('\n');	}	}	
child process exited with code 

synchronized(childErrorLog) {	Iterator iter = childErrorLog.iterator();	while(iter.hasNext()){	errStr.append(iter.next());	errStr.append('\n');	}	}	rpcServer.cancelClient(clientId, "Child process (spark-submit) exited before connecting back with error log " + errStr.toString());	}	} catch (InterruptedException ie) {	
thread waiting on the child process spark submit is interrupted killing the child process 

private void handle(ChannelHandlerContext ctx, Error msg) {	
error reported from remote driver 

private void handle(ChannelHandlerContext ctx, JobMetrics msg) {	JobHandleImpl<?> handle = jobs.get(msg.jobId);	if (handle != null) {	handle.getMetrics().addMetrics(msg.sparkJobId, msg.stageId, msg.taskId, msg.metrics);	} else {	
received metrics for unknown job 

private void handle(ChannelHandlerContext ctx, JobResult msg) {	JobHandleImpl<?> handle = jobs.remove(msg.id);	if (handle != null) {	
received result for 

JobHandleImpl<?> handle = jobs.remove(msg.id);	if (handle != null) {	handle.setSparkCounters(msg.sparkCounters);	Throwable error = msg.error != null ? new SparkException(msg.error) : null;	if (error == null) {	handle.setSuccess(msg.result);	} else {	handle.setFailure(error);	}	} else {	
received result for unknown job 

private void handle(ChannelHandlerContext ctx, JobStarted msg) {	JobHandleImpl<?> handle = jobs.get(msg.id);	if (handle != null) {	handle.changeState(JobHandle.State.STARTED);	} else {	
received event for unknown job 

private void handle(ChannelHandlerContext ctx, JobSubmitted msg) {	JobHandleImpl<?> handle = jobs.get(msg.clientJobId);	if (handle != null) {	
received spark job id for 

private void handle(ChannelHandlerContext ctx, JobSubmitted msg) {	JobHandleImpl<?> handle = jobs.get(msg.clientJobId);	if (handle != null) {	handle.addSparkJobId(msg.sparkJobId);	} else {	
received spark job id for unknown job 

========================= hive sample_557 =========================

Path di_test = new Path(tmppath, testdir);	if (!fs.exists(di_test)) {	throw new RuntimeException(tmpdir + File.separator + testdir + " does not exist");	}	if (!ShimLoader.getHadoopShims().isDirectory(fs.getFileStatus(di_test))) {	throw new RuntimeException(tmpdir + File.separator + testdir + " is not a directory");	}	FSDataInputStream fi_test = fs.open((fs.listStatus(di_test))[0].getPath());	FileInputStream fi_gold = new FileInputStream(new File(testFileDir,datafile));	if (!Utilities.contentsEqual(fi_gold, fi_test, false)) {	
does not match 

private void executePlan() throws Exception {	String testName = new Exception().getStackTrace()[1].getMethodName();	MapRedTask mrtask = new MapRedTask();	DriverContext dctx = new DriverContext ();	mrtask.setWork(mr);	mrtask.initialize(queryState, null, dctx, null);	int exitVal =  mrtask.execute(dctx);	if (exitVal != 0) {	
execution failed with exit status 

private void executePlan() throws Exception {	String testName = new Exception().getStackTrace()[1].getMethodName();	MapRedTask mrtask = new MapRedTask();	DriverContext dctx = new DriverContext ();	mrtask.setWork(mr);	mrtask.initialize(queryState, null, dctx, null);	int exitVal =  mrtask.execute(dctx);	if (exitVal != 0) {	assertEquals(true, false);	}	
execution completed successfully 

public void testMapPlan1() throws Exception {	
beginning 

public void testMapPlan2() throws Exception {	
beginning 

public void testMapRedPlan1() throws Exception {	
beginning 

public void testMapRedPlan2() throws Exception {	
beginning 

public void testMapRedPlan3() throws Exception {	
beginning 

public void testMapRedPlan4() throws Exception {	
beginning 

public void testMapRedPlan5() throws Exception {	
beginning 

public void testMapRedPlan6() throws Exception {	
beginning 

========================= hive sample_2564 =========================

Assert.assertTrue("Cannot find " + dummyFileURI + " in " + cacheFilesList, cacheFilesList.contains(dummyFileURI));	Assert.assertTrue(job.waitForCompletion(true));	Path textOutPath = new Path(outDir, "out1/part-m-00000");	String[] textOutput = readFully(textOutPath).split("\n");	Path seqOutPath = new Path(outDir, "out2/part-m-00000");	SequenceFile.Reader reader = new SequenceFile.Reader(fs, seqOutPath, mrConf);	Text key = new Text();	IntWritable value = new IntWritable();	String[] words = fileContent.split(" ");	Assert.assertEquals(words.length, textOutput.length);	
verifying file contents 

========================= hive sample_690 =========================

HiveAuthFactory.loginFromKeytab(hiveConf);	this.serviceUGI = Utils.getUGI();	} catch (IOException e) {	throw new ServiceException("Unable to login to kerberos with given principal/keytab", e);	} catch (LoginException e) {	throw new ServiceException("Unable to login to kerberos with given principal/keytab", e);	}	String principal = hiveConf.getVar(ConfVars.HIVE_SERVER2_SPNEGO_PRINCIPAL);	String keyTabFile = hiveConf.getVar(ConfVars.HIVE_SERVER2_SPNEGO_KEYTAB);	if (principal.isEmpty() || keyTabFile.isEmpty()) {	
spnego httpugi not created spnegoprincipal ketabfile 

throw new ServiceException("Unable to login to kerberos with given principal/keytab", e);	} catch (LoginException e) {	throw new ServiceException("Unable to login to kerberos with given principal/keytab", e);	}	String principal = hiveConf.getVar(ConfVars.HIVE_SERVER2_SPNEGO_PRINCIPAL);	String keyTabFile = hiveConf.getVar(ConfVars.HIVE_SERVER2_SPNEGO_KEYTAB);	if (principal.isEmpty() || keyTabFile.isEmpty()) {	} else {	try {	this.httpUGI = HiveAuthFactory.loginFromSpnegoKeytabAndReturnUGI(hiveConf);	
spnego httpugi successfully created 

} catch (LoginException e) {	throw new ServiceException("Unable to login to kerberos with given principal/keytab", e);	}	String principal = hiveConf.getVar(ConfVars.HIVE_SERVER2_SPNEGO_PRINCIPAL);	String keyTabFile = hiveConf.getVar(ConfVars.HIVE_SERVER2_SPNEGO_KEYTAB);	if (principal.isEmpty() || keyTabFile.isEmpty()) {	} else {	try {	this.httpUGI = HiveAuthFactory.loginFromSpnegoKeytabAndReturnUGI(hiveConf);	} catch (IOException e) {	
spnego httpugi creation failed 

public SessionHandle openSession(TProtocolVersion protocol, String username, String password, Map<String, String> configuration) throws HiveSQLException {	SessionHandle sessionHandle = sessionManager.openSession(protocol, username, password, null, configuration, false, null);	
opensession 

public SessionHandle openSessionWithImpersonation(TProtocolVersion protocol, String username, String password, Map<String, String> configuration, String delegationToken) throws HiveSQLException {	SessionHandle sessionHandle = sessionManager.openSession(protocol, username, password, null, configuration, true, delegationToken);	
opensessionwithimpersonation 

public SessionHandle openSession(TProtocolVersion protocol, String username, String password, String ipAddress, Map<String, String> configuration) throws HiveSQLException {	SessionHandle sessionHandle = sessionManager.openSession(protocol, username, password, ipAddress, configuration, false, null);	
opensession 

public SessionHandle openSessionWithImpersonation(TProtocolVersion protocol, String username, String password, String ipAddress, Map<String, String> configuration, String delegationToken) throws HiveSQLException {	SessionHandle sessionHandle = sessionManager.openSession(protocol, username, password, ipAddress, configuration, true, delegationToken);	
opensession 

public SessionHandle openSession(String username, String password, Map<String, String> configuration) throws HiveSQLException {	SessionHandle sessionHandle = sessionManager.openSession(SERVER_VERSION, username, password, null, configuration, false, null);	
opensession 

public void createSessionWithSessionHandle(SessionHandle sessionHandle, String username, String password, Map<String, String> configuration) throws HiveSQLException {	sessionManager.createSession(sessionHandle, SERVER_VERSION, username, password, null, configuration, false, null);	
createsessionwithsessionhandle 

public SessionHandle openSessionWithImpersonation(String username, String password, Map<String, String> configuration, String delegationToken) throws HiveSQLException {	SessionHandle sessionHandle = sessionManager.openSession(SERVER_VERSION, username, password, null, configuration, true, delegationToken);	
opensession 

public void closeSession(SessionHandle sessionHandle) throws HiveSQLException {	sessionManager.closeSession(sessionHandle);	
closesession 

public GetInfoValue getInfo(SessionHandle sessionHandle, GetInfoType getInfoType) throws HiveSQLException {	GetInfoValue infoValue = sessionManager.getSession(sessionHandle) .getInfo(getInfoType);	
getinfo 

public OperationHandle executeStatement(SessionHandle sessionHandle, String statement, Map<String, String> confOverlay) throws HiveSQLException {	HiveSession session = sessionManager.getSession(sessionHandle);	session.getSessionState().updateProgressMonitor(null);	OperationHandle opHandle = session.executeStatement(statement, confOverlay);	
executestatement 

public OperationHandle executeStatement(SessionHandle sessionHandle, String statement, Map<String, String> confOverlay, long queryTimeout) throws HiveSQLException {	HiveSession session = sessionManager.getSession(sessionHandle);	session.getSessionState().updateProgressMonitor(null);	OperationHandle opHandle = session.executeStatement(statement, confOverlay, queryTimeout);	
executestatement 

public OperationHandle executeStatementAsync(SessionHandle sessionHandle, String statement, Map<String, String> confOverlay) throws HiveSQLException {	HiveSession session = sessionManager.getSession(sessionHandle);	session.getSessionState().updateProgressMonitor(null);	OperationHandle opHandle = session.executeStatementAsync(statement, confOverlay);	
executestatementasync 

public OperationHandle executeStatementAsync(SessionHandle sessionHandle, String statement, Map<String, String> confOverlay, long queryTimeout) throws HiveSQLException {	HiveSession session = sessionManager.getSession(sessionHandle);	session.getSessionState().updateProgressMonitor(null);	OperationHandle opHandle = session.executeStatementAsync(statement, confOverlay, queryTimeout);	
executestatementasync 

public OperationHandle getTypeInfo(SessionHandle sessionHandle) throws HiveSQLException {	OperationHandle opHandle = sessionManager.getSession(sessionHandle) .getTypeInfo();	
gettypeinfo 

public OperationHandle getSchemas(SessionHandle sessionHandle, String catalogName, String schemaName) throws HiveSQLException {	OperationHandle opHandle = sessionManager.getSession(sessionHandle) .getSchemas(catalogName, schemaName);	
getschemas 

public OperationHandle getTables(SessionHandle sessionHandle, String catalogName, String schemaName, String tableName, List<String> tableTypes) throws HiveSQLException {	OperationHandle opHandle = sessionManager.getSession(sessionHandle) .getTables(catalogName, schemaName, tableName, tableTypes);	
gettables 

public OperationHandle getTableTypes(SessionHandle sessionHandle) throws HiveSQLException {	OperationHandle opHandle = sessionManager.getSession(sessionHandle) .getTableTypes();	
gettabletypes 

public OperationHandle getColumns(SessionHandle sessionHandle, String catalogName, String schemaName, String tableName, String columnName) throws HiveSQLException {	OperationHandle opHandle = sessionManager.getSession(sessionHandle) .getColumns(catalogName, schemaName, tableName, columnName);	
getcolumns 

public OperationHandle getFunctions(SessionHandle sessionHandle, String catalogName, String schemaName, String functionName) throws HiveSQLException {	OperationHandle opHandle = sessionManager.getSession(sessionHandle) .getFunctions(catalogName, schemaName, functionName);	
getfunctions 

public OperationHandle getPrimaryKeys(SessionHandle sessionHandle, String catalog, String schema, String table) throws HiveSQLException {	OperationHandle opHandle = sessionManager.getSession(sessionHandle) .getPrimaryKeys(catalog, schema, table);	
getprimarykeys 

public OperationHandle getCrossReference(SessionHandle sessionHandle, String primaryCatalog, String primarySchema, String primaryTable, String foreignCatalog, String foreignSchema, String foreignTable) throws HiveSQLException {	OperationHandle opHandle = sessionManager.getSession(sessionHandle) .getCrossReference(primaryCatalog, primarySchema, primaryTable, foreignCatalog, foreignSchema, foreignTable);	
getcrossreference 

public OperationStatus getOperationStatus(OperationHandle opHandle, boolean getProgressUpdate) throws HiveSQLException {	Operation operation = sessionManager.getOperationManager().getOperation(opHandle);	HiveConf conf = operation.getParentSession().getHiveConf();	if (operation.shouldRunAsync()) {	long maxTimeout = HiveConf.getTimeVar(conf, HiveConf.ConfVars.HIVE_SERVER2_LONG_POLLING_TIMEOUT, TimeUnit.MILLISECONDS);	final long elapsed = System.currentTimeMillis() - operation.getBeginTime();	final long timeout = Math.min(maxTimeout, (elapsed / TimeUnit.SECONDS.toMillis(10) + 1) * 500);	try {	operation.getBackgroundHandle().get(timeout, TimeUnit.MILLISECONDS);	} catch (TimeoutException e) {	
long polling timed out 

Operation operation = sessionManager.getOperationManager().getOperation(opHandle);	HiveConf conf = operation.getParentSession().getHiveConf();	if (operation.shouldRunAsync()) {	long maxTimeout = HiveConf.getTimeVar(conf, HiveConf.ConfVars.HIVE_SERVER2_LONG_POLLING_TIMEOUT, TimeUnit.MILLISECONDS);	final long elapsed = System.currentTimeMillis() - operation.getBeginTime();	final long timeout = Math.min(maxTimeout, (elapsed / TimeUnit.SECONDS.toMillis(10) + 1) * 500);	try {	operation.getBackgroundHandle().get(timeout, TimeUnit.MILLISECONDS);	} catch (TimeoutException e) {	} catch (CancellationException e) {	
the background operation was cancelled 

HiveConf conf = operation.getParentSession().getHiveConf();	if (operation.shouldRunAsync()) {	long maxTimeout = HiveConf.getTimeVar(conf, HiveConf.ConfVars.HIVE_SERVER2_LONG_POLLING_TIMEOUT, TimeUnit.MILLISECONDS);	final long elapsed = System.currentTimeMillis() - operation.getBeginTime();	final long timeout = Math.min(maxTimeout, (elapsed / TimeUnit.SECONDS.toMillis(10) + 1) * 500);	try {	operation.getBackgroundHandle().get(timeout, TimeUnit.MILLISECONDS);	} catch (TimeoutException e) {	} catch (CancellationException e) {	} catch (ExecutionException e) {	
the background operation was aborted 

final long timeout = Math.min(maxTimeout, (elapsed / TimeUnit.SECONDS.toMillis(10) + 1) * 500);	try {	operation.getBackgroundHandle().get(timeout, TimeUnit.MILLISECONDS);	} catch (TimeoutException e) {	} catch (CancellationException e) {	} catch (ExecutionException e) {	} catch (InterruptedException e) {	}	}	OperationStatus opStatus = operation.getStatus();	
getoperationstatus 

public void cancelOperation(OperationHandle opHandle) throws HiveSQLException {	sessionManager.getOperationManager().getOperation(opHandle) .getParentSession().cancelOperation(opHandle);	
canceloperation 

public void closeOperation(OperationHandle opHandle) throws HiveSQLException {	sessionManager.getOperationManager().getOperation(opHandle) .getParentSession().closeOperation(opHandle);	
closeoperation 

public TableSchema getResultSetMetadata(OperationHandle opHandle) throws HiveSQLException {	TableSchema tableSchema = sessionManager.getOperationManager() .getOperation(opHandle).getParentSession().getResultSetMetadata(opHandle);	
getresultsetmetadata 

public RowSet fetchResults(OperationHandle opHandle, FetchOrientation orientation, long maxRows, FetchType fetchType) throws HiveSQLException {	RowSet rowSet = sessionManager.getOperationManager().getOperation(opHandle) .getParentSession().fetchResults(opHandle, orientation, maxRows, fetchType);	
fetchresults 

public String getDelegationToken(SessionHandle sessionHandle, HiveAuthFactory authFactory, String owner, String renewer) throws HiveSQLException {	String delegationToken = sessionManager.getSession(sessionHandle). getDelegationToken(authFactory, owner, renewer);	
getdelegationtoken owner renewer 

public void cancelDelegationToken(SessionHandle sessionHandle, HiveAuthFactory authFactory, String tokenStr) throws HiveSQLException {	sessionManager.getSession(sessionHandle). cancelDelegationToken(authFactory, tokenStr);	
canceldelegationtoken 

public void renewDelegationToken(SessionHandle sessionHandle, HiveAuthFactory authFactory, String tokenStr) throws HiveSQLException {	sessionManager.getSession(sessionHandle).renewDelegationToken(authFactory, tokenStr);	
renewdelegationtoken 

public String getQueryId(TOperationHandle opHandle) throws HiveSQLException {	Operation operation = sessionManager.getOperationManager().getOperation( new OperationHandle(opHandle));	final String queryId = operation.getParentSession().getHiveConf().getVar(ConfVars.HIVEQUERYID);	
getqueryid 

========================= hive sample_2358 =========================

public ServiceInstanceBase(ServiceRecord srv, String rpcName) throws IOException {	this.srv = srv;	if (LOG.isTraceEnabled()) {	
working with servicerecord 

========================= hive sample_628 =========================

public HiveLock lock(HiveLockObject key, HiveLockMode mode, boolean keepAlive) throws LockException {	
acquiring lock for with mode 

private List<HiveLock> lock(List<HiveLockObj> objs, int numRetriesForLock, long sleepTime) throws LockException {	sortLocks(objs);	if (LOG.isDebugEnabled()) {	for (HiveLockObj obj : objs) {	
acquiring lock for with mode 

public void releaseLocks(List<HiveLock> hiveLocks, int numRetriesForUnLock, long sleepTime) {	for (HiveLock locked : hiveLocks) {	try {	unlock(locked, numRetriesForUnLock, sleepTime);	} catch (LockException e) {	
failed to unlock 

========================= hive sample_3772 =========================

getStore().getFileMetadata(fileIds, metadatas);	for (int i = 0; i < metadatas.length;  ++i) {	eliminated[i] = false;	results[i] = null;	if (metadatas[i] == null) continue;	ByteBuffer metadata = metadatas[i].duplicate();	SplitInfos result = null;	try {	result = getFileFormatProxy().applySargToMetadata(sarg, metadata);	} catch (IOException ex) {	
failed to apply sarg to metadata 

========================= hive sample_5649 =========================

String colType = null;	String colName = null;	boolean doAllPartitionContainStats = partNames.size() == colStatsWithSourceInfo.size();	NumDistinctValueEstimator ndvEstimator = null;	for (ColStatsObjWithSourceInfo csp : colStatsWithSourceInfo) {	ColumnStatisticsObj cso = csp.getColStatsObj();	if (statsObj == null) {	colName = cso.getColName();	colType = cso.getColType();	statsObj = ColumnStatsAggregatorFactory.newColumnStaticsObj(colName, colType, cso.getStatsData().getSetField());	
doallpartitioncontainstats for column is 

aggregateData.setNumNulls(aggregateData.getNumNulls() + newData.getNumNulls());	aggregateData.setNumDVs(Math.max(aggregateData.getNumDVs(), newData.getNumDVs()));	}	}	if (ndvEstimator != null) {	aggregateData.setNumDVs(ndvEstimator.estimateNumDistinctValues());	} else {	}	columnStatisticsData.setStringStats(aggregateData);	} else {	
start extrapolation for 

if (length > 0) {	adjustedIndexMap.put(pseudoPartName.toString(), pseudoIndexSum / length);	aggregateData.setNumDVs(ndvEstimator.estimateNumDistinctValues());	ColumnStatisticsData csd = new ColumnStatisticsData();	csd.setStringStats(aggregateData);	adjustedStatsMap.put(pseudoPartName.toString(), csd);	}	}	extrapolate(columnStatisticsData, partNames.size(), colStatsWithSourceInfo.size(), adjustedIndexMap, adjustedStatsMap, -1);	}	
ndv estimatation for is of partitions requested of partitions found 

========================= hive sample_1935 =========================

public Object getField(int fieldID) {	String bytesAsString = new String(bytes.getData(), start, length);	
data 

public Object getField(int fieldID) {	String bytesAsString = new String(bytes.getData(), start, length);	char separator = (char) ((int) oi.getSeparator() + 1);	
separator 

public Object getField(int fieldID) {	String bytesAsString = new String(bytes.getData(), start, length);	char separator = (char) ((int) oi.getSeparator() + 1);	String[] fieldBytes = StringUtils.split(bytesAsString, separator);	
fields 

========================= hive sample_151 =========================

zk = ZooKeeperStorage.zkOpen(appConf);	nodes = getChildList(zk);	for (String node : nodes) {	boolean deleted = checkAndDelete(node, zk);	if (!deleted) {	break;	}	}	zk.close();	} catch (Exception e) {	
cleanup cycle failed 

if (!deleted) {	break;	}	}	zk.close();	} catch (Exception e) {	} finally {	if (zk != null) zk.close();	}	long sleepMillis = (long) (Math.random() * interval);	
next execution 

}	zk.close();	} catch (Exception e) {	} finally {	if (zk != null) zk.close();	}	long sleepMillis = (long) (Math.random() * interval);	Thread.sleep(sleepMillis);	} catch (Exception e) {	isRunning = false;	
cleanup failed 

public List<String> getChildList(CuratorFramework zk) {	try {	List<String> jobs = JobStateTracker.getTrackingJobs(appConf, zk);	Collections.sort(jobs);	return jobs;	} catch (IOException e) {	
no jobs to check 

JobState state = null;	try {	JobStateTracker tracker = new JobStateTracker(node, zk, true, appConf.get(TempletonStorage.STORAGE_ROOT + ZooKeeperStorage.TRACKINGDIR));	long now = new Date().getTime();	state = new JobState(tracker.getJobID(), appConf);	long then = 0;	if (state.getCreated() != null) {	then = state.getCreated();	}	if (now - then > maxage) {	
deleting 

if (state.getCreated() != null) {	then = state.getCreated();	}	if (now - then > maxage) {	state.delete();	tracker.delete();	return true;	}	return false;	} catch (Exception e) {	
checkanddelete failed for due to 

return true;	}	return false;	} catch (Exception e) {	return true;	} finally {	if (state != null) {	try {	state.close();	} catch (IOException e) {	
couldn t close job state 

========================= hive sample_852 =========================

private void showLocksNewFormat(String preamble) throws LockException {	ShowLocksResponse rsp = getLocks();	ByteArrayOutputStream baos = new ByteArrayOutputStream(1024*2);	DataOutputStream os = new DataOutputStream(baos);	try {	DDLTask.dumpLockInfo(os, rsp);	os.flush();	LOG.info(baos.toString());	}	catch(IOException ex) {	
dumping lock info for failed 

public void unlock(HiveLock hiveLock) throws LockException {	long lockId = ((DbHiveLock)hiveLock).lockId;	boolean removed = false;	try {	
unlocking 

long lockId = ((DbHiveLock)hiveLock).lockId;	boolean removed = false;	try {	txnManager.getMS().unlock(lockId);	removed = locks.remove(hiveLock);	Metrics metrics = MetricsFactory.getInstance();	if (metrics != null) {	try {	metrics.decrementCounter(MetricsConstant.METASTORE_HIVE_LOCKS);	} catch (Exception e) {	
error reporting hive client metastore unlock operation to metrics system 

try {	txnManager.getMS().unlock(lockId);	removed = locks.remove(hiveLock);	Metrics metrics = MetricsFactory.getInstance();	if (metrics != null) {	try {	metrics.decrementCounter(MetricsConstant.METASTORE_HIVE_LOCKS);	} catch (Exception e) {	}	}	
removed a lock 

removed = locks.remove(hiveLock);	Metrics metrics = MetricsFactory.getInstance();	if (metrics != null) {	try {	metrics.decrementCounter(MetricsConstant.METASTORE_HIVE_LOCKS);	} catch (Exception e) {	}	}	} catch (NoSuchLockException e) {	removed = locks.remove(hiveLock);	
metastore could find no record of lock 

} catch (NoSuchLockException e) {	removed = locks.remove(hiveLock);	throw new LockException(e, ErrorMsg.LOCK_NO_SUCH_LOCK, JavaUtils.lockIdToString(lockId));	} catch (TxnOpenException e) {	throw new RuntimeException("Attempt to unlock lock " + JavaUtils.lockIdToString(lockId) + "associated with an open transaction, " + e.getMessage(), e);	} catch (TException e) {	throw new LockException(ErrorMsg.METASTORE_COMMUNICATION_FAILED.getMsg(), e);	}	finally {	if(removed) {	
removed a lock 

public void releaseLocks(List<HiveLock> hiveLocks) {	
releaselocks 

========================= hive sample_3764 =========================

protected Map<Integer, DelegationKey> reloadKeys() {	String[] allKeys = tokenStore.getMasterKeys();	Map<Integer, DelegationKey> keys = new HashMap<>(allKeys.length);	for (String keyStr : allKeys) {	DelegationKey key = new DelegationKey();	try {	decodeWritable(key, keyStr);	keys.put(key.getKeyId(), key);	} catch (IOException ex) {	
failed to load master key 

public DelegationTokenIdentifier cancelToken(Token<DelegationTokenIdentifier> token, String canceller) throws IOException {	DelegationTokenIdentifier id = getTokenIdentifier(token);	
token cancellation requested for identifier 

public synchronized void stopThreads() {	if (LOGGER.isDebugEnabled()) {	
stopping expired delegation token remover thread 

public void run() {	LOGGER.info("Starting expired delegation token remover thread, " + "tokenRemoverScanInterval=" + tokenRemoverScanInterval while (running) {	try {	long now = System.currentTimeMillis();	if (lastMasterKeyUpdate + keyUpdateInterval < now) {	try {	rollMasterKeyExt();	lastMasterKeyUpdate = now;	} catch (IOException e) {	
master key updating failed 

} catch (IOException e) {	}	}	if (lastTokenCacheCleanup + tokenRemoverScanInterval < now) {	removeExpiredTokens();	lastTokenCacheCleanup = now;	}	try {	Thread.sleep(5000);	} catch (InterruptedException ie) {	
interruptedexception received for expiredtokenremover thread 

}	if (lastTokenCacheCleanup + tokenRemoverScanInterval < now) {	removeExpiredTokens();	lastTokenCacheCleanup = now;	}	try {	Thread.sleep(5000);	} catch (InterruptedException ie) {	}	} catch (Throwable t) {	
expiredtokenremover thread received unexpected exception 

lastTokenCacheCleanup = now;	}	try {	Thread.sleep(5000);	} catch (InterruptedException ie) {	}	} catch (Throwable t) {	try {	Thread.sleep(5000);	} catch (InterruptedException ie) {	
interruptedexception received for expiredtokenremover thread during wait in exception sleep 

========================= hive sample_1866 =========================

public void delete() throws IOException {	try {	zk.delete().forPath(makeTrackingJobZnode(trackingnode));	} catch (Exception e) {	
couldn t delete 

========================= hive sample_844 =========================

private int now() {	long millis = System.currentTimeMillis();	millis /= 1000;	if (millis > Integer.MAX_VALUE) {	
we ve passed max int value in seconds since the epoch all notification times will be the same 

private void process(NotificationEvent event, ListenerEvent listenerEvent) throws MetaException {	event.setMessageFormat(msgFactory.getMessageFormat());	
dbnotificationlistener processing 

public void run() {	while (true) {	rs.cleanNotificationEvents(ttl);	
cleaner thread done 

public void run() {	while (true) {	rs.cleanNotificationEvents(ttl);	try {	Thread.sleep(sleepTime);	} catch (InterruptedException e) {	
cleaner thread sleep interrupted 

========================= hive sample_1020 =========================

NumDistinctValueEstimator oldEst = aggregateData.getNdvEstimator();	NumDistinctValueEstimator newEst = newData.getNdvEstimator();	long ndv = -1;	if (oldEst.canMerge(newEst)) {	oldEst.mergeEstimators(newEst);	ndv = oldEst.estimateNumDistinctValues();	aggregateData.setNdvEstimator(oldEst);	} else {	ndv = Math.max(aggregateData.getNumDVs(), newData.getNumDVs());	}	
use bitvector to merge column s ndvs of and to be 

========================= hive sample_1923 =========================

this.origConf = origConf;	this.baseHandler = baseHandler;	if (local) {	baseHandler.setConf(origConf);	}	activeConf = baseHandler.getConf();	MetaStoreInit.updateConnectionURL(origConf, getActiveConf(), null, metaStoreInitData);	try {	invoke(baseHandler, baseHandler.getClass().getDeclaredMethod("init", (Class<?>[]) null), null);	} catch (Throwable e) {	
hmshandler fatal error 

String methodName = method.getName();	if (!methodName.startsWith("get_database") && !methodName.startsWith("get_table") && !methodName.startsWith("get_partition") && !methodName.startsWith("get_function")) {	LOG.error(ExceptionUtils.getStackTrace(e.getCause()));	}	throw e.getCause();	} else if (e.getCause() instanceof MetaException && e.getCause().getCause() != null) {	if (e.getCause().getCause() instanceof javax.jdo.JDOException || e.getCause().getCause() instanceof NucleusException) {	caughtException = e.getCause().getCause();	} else if (e.getCause().getCause() instanceof DeadlineException) {	Deadline.clear();	
error happens in method 

} else {	LOG.error(ExceptionUtils.getStackTrace(e.getCause()));	throw e.getCause();	}	} else {	LOG.error(ExceptionUtils.getStackTrace(e.getCause()));	throw e.getCause();	}	}	if (retryCount >= retryLimit) {	
hmshandler fatal error 

throw e.getCause();	}	}	if (retryCount >= retryLimit) {	MetaException me = new MetaException(caughtException.getMessage());	me.initCause(caughtException);	throw me;	}	assert (retryInterval >= 0);	retryCount++;	
retrying hmshandler after d ms attempt d of d with error 

========================= hive sample_1889 =========================

CollectPolicy collector = new CollectPolicy();	try {	String commandText = String.format("ssh -v -i %s %s -l %s %s '%s'", command.getPrivateKey(), mSshOpts, command.getUser(), command.getHost(), command.getCommand());	int attempts = 0;	boolean retry;	LocalCommand cmd;	do {	retry = false;	cmd = mLocalCommandFactory.create(collector, commandText);	if(mShutdown) {	
shutting down command 

LocalCommand cmd;	do {	retry = false;	cmd = mLocalCommandFactory.create(collector, commandText);	if(mShutdown) {	cmd.kill();	command.setExitCode(Constants.EXIT_CODE_UNKNOWN);	return;	}	if(command.isReportErrors() && attempts++ <= 3 && cmd.getExitCode() == Constants.EXIT_CODE_UNKNOWN) {	
command exited with will retry 

========================= hive sample_5599 =========================

final FileSystem fs;	try {	fs = getFs(path);	stat = fs.getFileStatus(path);	HdfsUtils.checkFileAccess(fs, stat, FsAction.WRITE);	return true;	} catch (FileNotFoundException fnfe){	return true;	} catch (Exception e) {	if (LOG.isDebugEnabled()) {	
exception when checking if path 

currPath = currPath.getParent();	} while (currPath != null && !currPath.getName().isEmpty());	for (int i = kvs.size(); i > 0; i--) {	String key = kvs.get(i - 1)[0];	if (requiredKeys != null) {	requiredKeys.remove(key);	}	partSpec.put(key, kvs.get(i - 1)[1]);	}	if (requiredKeys == null || requiredKeys.isEmpty()) return true;	
cannot create partition spec from missing keys 

========================= hive sample_2122 =========================

currentOpenSegment = retVal;	return retVal;	} else if (currentOpenSegment.getInterval().equals(interval)) {	retVal = currentOpenSegment;	int rowCount = appenderator.getRowCount(retVal);	if (rowCount < maxPartitionSize) {	return retVal;	} else {	retVal = new SegmentIdentifier( dataSchema.getDataSource(), interval, tuningConfig.getVersioningPolicy().getVersion(interval), new LinearShardSpec(currentOpenSegment.getShardSpec().getPartitionNum() + 1) );	pushSegments(Lists.newArrayList(currentOpenSegment));	
creating new partition for segment partition num 

return retVal;	} else {	retVal = new SegmentIdentifier( dataSchema.getDataSource(), interval, tuningConfig.getVersioningPolicy().getVersion(interval), new LinearShardSpec(currentOpenSegment.getShardSpec().getPartitionNum() + 1) );	pushSegments(Lists.newArrayList(currentOpenSegment));	currentOpenSegment = retVal;	return retVal;	}	} else {	retVal = new SegmentIdentifier( dataSchema.getDataSource(), interval, tuningConfig.getVersioningPolicy().getVersion(interval), new LinearShardSpec(0) );	pushSegments(Lists.newArrayList(currentOpenSegment));	
creating segment 

private void pushSegments(List<SegmentIdentifier> segmentsToPush) {	try {	SegmentsAndMetadata segmentsAndMetadata = appenderator .push(segmentsToPush, committerSupplier.get()).get();	final HashSet<String> pushedSegmentIdentifierHashSet = new HashSet<>();	for (DataSegment pushedSegment : segmentsAndMetadata.getSegments()) {	pushedSegmentIdentifierHashSet .add(SegmentIdentifier.fromDataSegment(pushedSegment).getIdentifierAsString());	final Path segmentDescriptorOutputPath = DruidStorageHandlerUtils .makeSegmentDescriptorOutputPath(pushedSegment, segmentsDescriptorDir);	DruidStorageHandlerUtils .writeSegmentDescriptor(fileSystem, pushedSegment, segmentDescriptorOutputPath);	
pushed the segment s and persisted the descriptor located at s 

}	final HashSet<String> toPushSegmentsHashSet = new HashSet( FluentIterable.from(segmentsToPush) .transform(new Function<SegmentIdentifier, String>() {	public String apply( ) {	return input.getIdentifierAsString();	}	}) .toList());	if (!pushedSegmentIdentifierHashSet.equals(toPushSegmentsHashSet)) {	throw new IllegalStateException(String.format( "was asked to publish [%s] but was able to publish only [%s]", Joiner.on(", ").join(toPushSegmentsHashSet), Joiner.on(", ").join(pushedSegmentIdentifierHashSet) ));	}	for (SegmentIdentifier dataSegmentId : segmentsToPush) {	
dropping segment 

public String apply( ) {	return input.getIdentifierAsString();	}	}) .toList());	if (!pushedSegmentIdentifierHashSet.equals(toPushSegmentsHashSet)) {	throw new IllegalStateException(String.format( "was asked to publish [%s] but was able to publish only [%s]", Joiner.on(", ").join(toPushSegmentsHashSet), Joiner.on(", ").join(pushedSegmentIdentifierHashSet) ));	}	for (SegmentIdentifier dataSegmentId : segmentsToPush) {	appenderator.drop(dataSegmentId).get();	}	
published d segments 

return input.getIdentifierAsString();	}	}) .toList());	if (!pushedSegmentIdentifierHashSet.equals(toPushSegmentsHashSet)) {	throw new IllegalStateException(String.format( "was asked to publish [%s] but was able to publish only [%s]", Joiner.on(", ").join(toPushSegmentsHashSet), Joiner.on(", ").join(pushedSegmentIdentifierHashSet) ));	}	for (SegmentIdentifier dataSegmentId : segmentsToPush) {	appenderator.drop(dataSegmentId).get();	}	} catch (InterruptedException e) {	
got interrupted failed to push d segments 

}) .toList());	if (!pushedSegmentIdentifierHashSet.equals(toPushSegmentsHashSet)) {	throw new IllegalStateException(String.format( "was asked to publish [%s] but was able to publish only [%s]", Joiner.on(", ").join(toPushSegmentsHashSet), Joiner.on(", ").join(pushedSegmentIdentifierHashSet) ));	}	for (SegmentIdentifier dataSegmentId : segmentsToPush) {	appenderator.drop(dataSegmentId).get();	}	} catch (InterruptedException e) {	Thread.currentThread().interrupt();	} catch (IOException | ExecutionException e) {	
failed to push d segments 

segmentsToPush.addAll(appenderator.getSegments());	pushSegments(segmentsToPush);	}	appenderator.clear();	} catch (InterruptedException e) {	Throwables.propagate(e);	} finally {	try {	FileUtils.deleteDirectory(tuningConfig.getBasePersistDirectory());	} catch (Exception e){	
error cleaning of base persist directory 

========================= hive sample_19 =========================

public void run() {	try {	HiveMetaStore.main(new String[]{"-v", "-p", String.valueOf(MS_PORT)});	} catch (Throwable t) {	
exiting got exception from metastore 

========================= hive sample_669 =========================

if (directoryDesc.getOutputFormat() != null){	ret.setOutputFileFormatClass(JavaUtils.loadClass(directoryDesc.getOutputFormat()));	}	if (directoryDesc.getNullFormat() != null) {	properties.setProperty(serdeConstants.SERIALIZATION_NULL_FORMAT, directoryDesc.getNullFormat());	}	if (directoryDesc.getTblProps() != null) {	properties.putAll(directoryDesc.getTblProps());	}	} catch (ClassNotFoundException e) {	
unable to find class in getdefaulttabledesc 

}	try {	HiveStorageHandler storageHandler = HiveUtils.getStorageHandler( Hive.get().getConf(), tableDesc.getProperties().getProperty( org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_STORAGE));	if (storageHandler != null) {	Map<String, String> jobProperties = new LinkedHashMap<String, String>();	Map<String, String> jobSecrets = new LinkedHashMap<String, String>();	if(input) {	try {	storageHandler.configureInputJobProperties( tableDesc, jobProperties);	} catch(AbstractMethodError e) {	
configureinputjobproperties not found using configuretablejobproperties 

Map<String, String> jobSecrets = new LinkedHashMap<String, String>();	if(input) {	try {	storageHandler.configureInputJobProperties( tableDesc, jobProperties);	} catch(AbstractMethodError e) {	storageHandler.configureTableJobProperties(tableDesc, jobProperties);	}	try{	storageHandler.configureInputJobCredentials( tableDesc, jobSecrets);	} catch(AbstractMethodError e) {	
configureinputjobsecrets not found 

}	try{	storageHandler.configureInputJobCredentials( tableDesc, jobSecrets);	} catch(AbstractMethodError e) {	}	}	else {	try {	storageHandler.configureOutputJobProperties( tableDesc, jobProperties);	} catch(AbstractMethodError e) {	
configureoutputjobproperties not found using configuretablejobproperties 

========================= hive sample_3363 =========================

private SessionExpirationTracker( long sessionLifetimeMs, long sessionLifetimeJitterMs, RestartImpl restartImpl) {	this.sessionRestartImpl = restartImpl;	this.sessionLifetimeMs = sessionLifetimeMs;	this.sessionLifetimeJitterMs = sessionLifetimeJitterMs;	if (LOG.isDebugEnabled()) {	
session expiration is enabled session lifetime is ms 

return o1.getExpirationNs().compareTo(o2.getExpirationNs());	}	});	restartQueue = new LinkedBlockingQueue<>();	expirationThread = new Thread(new Runnable() {	public void run() {	try {	SessionState.setCurrentSessionState(initSessionState);	runExpirationThread();	} catch (Exception e) {	
exception in tezsessionpool expiration thread thread will shut down 

}	});	restartQueue = new LinkedBlockingQueue<>();	expirationThread = new Thread(new Runnable() {	public void run() {	try {	SessionState.setCurrentSessionState(initSessionState);	runExpirationThread();	} catch (Exception e) {	} finally {	
tezsessionpool expiration thread exiting 

} finally {	}	}	}, "TezSessionPool-expiration");	restartThread = new Thread(new Runnable() {	public void run() {	try {	SessionState.setCurrentSessionState(initSessionState);	runRestartThread();	} catch (Exception e) {	
exception in tezsessionpool cleanup thread thread will shut down 

}	}	}, "TezSessionPool-expiration");	restartThread = new Thread(new Runnable() {	public void run() {	try {	SessionState.setCurrentSessionState(initSessionState);	runRestartThread();	} catch (Exception e) {	} finally {	
tezsessionpool cleanup thread exiting 

private void runRestartThread() {	try {	while (true) {	TezSessionPoolSession next = restartQueue.take();	
restarting the expired session 

private void runRestartThread() {	try {	while (true) {	TezSessionPoolSession next = restartQueue.take();	try {	sessionRestartImpl.closeAndReopenExpiredSession(next);	} catch (InterruptedException ie) {	throw ie;	} catch (Exception e) {	
failed to close or restart a session ignoring 

while (true) {	TezSessionPoolSession next = restartQueue.take();	try {	sessionRestartImpl.closeAndReopenExpiredSession(next);	} catch (InterruptedException ie) {	throw ie;	} catch (Exception e) {	}	}	} catch (InterruptedException e) {	
restart thread is exiting due to an interruption 

private void runExpirationThread() {	try {	while (true) {	TezSessionPoolSession nextToExpire = null;	while (true) {	nextToExpire = expirationQueue.take();	if (LOG.isDebugEnabled()) {	
seeing if we can expire 

try {	while (true) {	TezSessionPoolSession nextToExpire = null;	while (true) {	nextToExpire = expirationQueue.take();	if (LOG.isDebugEnabled()) {	}	try {	if (!nextToExpire.tryExpire(false)) break;	} catch (Exception e) {	
failed to expire session ignoring 

while (true) {	nextToExpire = expirationQueue.take();	if (LOG.isDebugEnabled()) {	}	try {	if (!nextToExpire.tryExpire(false)) break;	} catch (Exception e) {	nextToExpire = null;	break;	}	
tez session has expired 

if (LOG.isDebugEnabled()) {	}	try {	if (!nextToExpire.tryExpire(false)) break;	} catch (Exception e) {	nextToExpire = null;	break;	}	}	if (nextToExpire != null && LOG.isDebugEnabled()) {	
is not ready to expire adding it back 

}	synchronized (expirationQueue) {	if (nextToExpire != null) {	expirationQueue.add(nextToExpire);	}	nextToExpire = expirationQueue.peek();	if (nextToExpire != null) {	long timeToWaitMs = (nextToExpire.getExpirationNs() - System.nanoTime()) / 1000000L;	timeToWaitMs = Math.max(1, timeToWaitMs + 10);	if (LOG.isDebugEnabled()) {	
waiting for ms to expire 

expirationQueue.add(nextToExpire);	}	nextToExpire = expirationQueue.peek();	if (nextToExpire != null) {	long timeToWaitMs = (nextToExpire.getExpirationNs() - System.nanoTime()) / 1000000L;	timeToWaitMs = Math.max(1, timeToWaitMs + 10);	if (LOG.isDebugEnabled()) {	}	expirationQueue.wait(timeToWaitMs);	} else if (LOG.isDebugEnabled()) {	
expiration queue is empty 

long timeToWaitMs = (nextToExpire.getExpirationNs() - System.nanoTime()) / 1000000L;	timeToWaitMs = Math.max(1, timeToWaitMs + 10);	if (LOG.isDebugEnabled()) {	}	expirationQueue.wait(timeToWaitMs);	} else if (LOG.isDebugEnabled()) {	}	}	}	} catch (InterruptedException e) {	
expiration thread is exiting due to an interruption 

public void addToExpirationQueue(TezSessionPoolSession session) {	long jitterModMs = (long)(sessionLifetimeJitterMs * rdm.nextFloat());	session.setExpirationNs(System.nanoTime() + (sessionLifetimeMs + jitterModMs) * 1000000L);	if (LOG.isDebugEnabled()) {	
adding a pool session to expiration queue 

========================= hive sample_4019 =========================

protected void setUp() throws Exception {	super.setUp();	hiveConf = new HiveConf(this.getClass());	try {	client = new HiveMetaStoreClient(hiveConf);	initTable();	} catch (Throwable e) {	
unable to open the metastore 

protected void tearDown() throws Exception {	try {	super.tearDown();	client.dropTable(dbName, tblName);	client.dropDatabase(dbName);	client.close();	} catch (Throwable e) {	
unable to close metastore 

========================= hive sample_686 =========================

}	}	return row;	} catch (final Exception e) {	throw new SerDeException(e);	} finally {	if (csv != null) {	try {	csv.close();	} catch (final Exception e) {	
fail to close csv writer 

========================= hive sample_5521 =========================

HadoopMetrics2Reporter reporter = HadoopMetrics2Reporter.forRegistry(registry) .convertRatesTo(TimeUnit.SECONDS) .convertDurationsTo(TimeUnit.MILLISECONDS) .build(DefaultMetricsSystem.initialize("metastore"), "metastore", "Runtime metadata" + " catalog", "general-metadata");	reporter.start(1, TimeUnit.MINUTES);	reporters.add(reporter);	scheduledReporters.add(reporter);	hadoopMetricsStarted = true;	} else {	throw new RuntimeException("Unknown metric type " + reporterName);	}	}	} else {	
no metrics reporters configured 

========================= hive sample_1831 =========================

HiveInputFormat.pushFilters(job, ts);	AcidUtils.setAcidTableScan(job, ts.getConf().isAcidTable());	AcidUtils.setAcidOperationalProperties(job, ts.getConf().getAcidOperationalProperties());	}	sink = work.getSink();	fetch = new FetchOperator(work, job, source, getVirtualColumns(source));	source.initialize(conf, new ObjectInspector[]{fetch.getOutputObjectInspector()});	totalRows = 0;	ExecMapper.setDone(false);	} catch (Exception e) {	
initialize failed 

========================= hive sample_3949 =========================

public void expandAndRehashToTarget(int estimateNewRowCount) {	int oldRefsCount = refs.length;	int newRefsCount = oldRefsCount + estimateNewRowCount;	if (resizeThreshold <= newRefsCount) {	newRefsCount = (Long.bitCount(newRefsCount) == 1) ? estimateNewRowCount : nextHighestPowerOfTwo(newRefsCount);	expandAndRehashImpl(newRefsCount);	
expand and rehash to from 

long ref = refs[slot];	if (ref == 0 || isSameKey(keyOffset, keyLength, ref, hashCode)) {	break;	}	++metricPutConflict;	probeSlot += (++i);	slot = (int)(probeSlot & bucketMask);	}	if (largestNumberOfSteps < i) {	if (LOG.isDebugEnabled()) {	
probed slots the longest so far to find space 

dump.append("Found " + examined + " keys!\n");	}	long currentOffset = 0;	for (Map.Entry<Long, Integer> e : byteIntervals.entrySet()) {	long start = e.getKey(), len = e.getValue();	if (start - currentOffset > 4) {	dump.append("Gap! [" + currentOffset + ", " + start + ")\n");	}	currentOffset = start + len;	}	
hashtable dump 

public void debugDumpMetrics() {	
map metrics keys allocated keys assigned write conflict write max dist read conflict expanded times in ms 

========================= hive sample_4062 =========================

} else {	synchronized(info) {	while (isUnderConstruction(info)) {	try {	info.wait();	} catch (InterruptedException e) {	throw new IOException("Interrupted waiting for construction", e);	}	}	}	
indexcache hit mapid found 

if ((info = cache.putIfAbsent(mapId, newInd)) != null) {	synchronized(info) {	while (isUnderConstruction(info)) {	try {	info.wait();	} catch (InterruptedException e) {	throw new IOException("Interrupted waiting for construction", e);	}	}	}	
indexcache hit mapid found 

while (isUnderConstruction(info)) {	try {	info.wait();	} catch (InterruptedException e) {	throw new IOException("Interrupted waiting for construction", e);	}	}	}	return info;	}	
indexcache miss mapid not found 

public void removeMap(String mapId) {	IndexInformation info = cache.get(mapId);	if (info == null || ((info != null) && isUnderConstruction(info))) {	return;	}	info = cache.remove(mapId);	if (info != null) {	totalMemoryUsed.addAndGet(-info.getSize());	if (!queue.remove(mapId)) {	
map id not found in queue 

IndexInformation info = cache.get(mapId);	if (info == null || ((info != null) && isUnderConstruction(info))) {	return;	}	info = cache.remove(mapId);	if (info != null) {	totalMemoryUsed.addAndGet(-info.getSize());	if (!queue.remove(mapId)) {	}	} else {	
map id not found in cache 

========================= hive sample_2159 =========================

assertTrue(allGiven.add(o));	++given;	}	}	for (TakeRunnable t : takers) {	for (Object o : t.objects) {	assertTrue(allGiven.remove(o));	++takenOld;	}	}	
mtt test size takers givers offered attempted reused allocated took ms including thread sync 

========================= hive sample_1325 =========================

public final void initialize(Configuration hconf, ObjectInspector[] inputOIs) throws HiveException {	this.done = false;	if (state == State.INIT) {	return;	}	this.configuration = hconf;	if (!areAllParentsInitialized()) {	return;	}	if (LOG.isInfoEnabled()) {	
initializing operator 

throw new HiveException("Internal Error during operator initialization.");	}	outputObjInspector = inputObjInspectors[0];	boolean isInitOk = false;	try {	initializeOp(hconf);	if (!rootInitializeCalled || childOperatorsArray.length != childOperators.size()) {	throw new AssertionError("Internal error during operator initialization");	}	if (LOG.isDebugEnabled()) {	
initialization done 

if (LOG.isDebugEnabled()) {	}	initializeChildren(hconf);	isInitOk = true;	} finally {	if (!isInitOk) {	cancelAsyncInitOps();	}	}	if (LOG.isDebugEnabled()) {	
initialization done done is reset 

protected void initializeChildren(Configuration hconf) throws HiveException {	state = State.INIT;	if (LOG.isDebugEnabled()) {	
operator initialized 

protected void initializeChildren(Configuration hconf) throws HiveException {	state = State.INIT;	if (LOG.isDebugEnabled()) {	}	if (childOperators == null || childOperators.isEmpty()) {	return;	}	if (LOG.isDebugEnabled()) {	
initializing children of 

public void abort() {	
received abort in operator 

protected void initialize(Configuration hconf, ObjectInspector inputOI, int parentId) throws HiveException {	if (LOG.isDebugEnabled()) {	
initializing child 

public abstract void process(Object row, int tag) throws HiveException;	protected final void defaultStartGroup() throws HiveException {	if (LOG.isDebugEnabled()) {	
starting group 

public abstract void process(Object row, int tag) throws HiveException;	protected final void defaultStartGroup() throws HiveException {	if (LOG.isDebugEnabled()) {	}	if (childOperators == null) {	return;	}	if (LOG.isDebugEnabled()) {	
starting group for children 

}	if (childOperators == null) {	return;	}	if (LOG.isDebugEnabled()) {	}	for (Operator<? extends OperatorDesc> op : childOperators) {	op.startGroup();	}	if (LOG.isDebugEnabled()) {	
start group done 

protected final void defaultEndGroup() throws HiveException {	if (LOG.isDebugEnabled()) {	
ending group 

protected final void defaultEndGroup() throws HiveException {	if (LOG.isDebugEnabled()) {	}	if (childOperators == null) {	return;	}	if (LOG.isDebugEnabled()) {	
ending group for children 

}	if (childOperators == null) {	return;	}	if (LOG.isDebugEnabled()) {	}	for (Operator<? extends OperatorDesc> op : childOperators) {	op.endGroup();	}	if (LOG.isDebugEnabled()) {	
end group done 

public void close(boolean abort) throws HiveException {	if (LOG.isDebugEnabled()) {	
close called for operator 

public void close(boolean abort) throws HiveException {	if (LOG.isDebugEnabled()) {	}	if (state == State.CLOSE) {	return;	}	if (!allInitializedParentsAreClosed()) {	if (LOG.isDebugEnabled()) {	
not all parent operators are closed not closing 

if (state == State.CLOSE) {	return;	}	if (!allInitializedParentsAreClosed()) {	if (LOG.isDebugEnabled()) {	}	return;	}	state = State.CLOSE;	if (LOG.isDebugEnabled()) {	
closing operator 

if (childOperators == null) {	return;	}	for (Operator<? extends OperatorDesc> op : childOperators) {	if (LOG.isDebugEnabled()) {	LOG.debug("Closing child = " + op);	}	op.close(abort);	}	if (LOG.isDebugEnabled()) {	
close done 

}	for (Operator<? extends OperatorDesc> op : childOperators) {	if (LOG.isDebugEnabled()) {	LOG.debug("Closing child = " + op);	}	op.close(abort);	}	if (LOG.isDebugEnabled()) {	}	} catch (HiveException e) {	
caught exception while closing operator 

public void setOpTraits(OpTraits metaInfo) {	if (LOG.isInfoEnabled()) {	
setting traits on 

public void setOpTraits(OpTraits metaInfo) {	if (LOG.isInfoEnabled()) {	}	if (conf != null) {	conf.setTraits(metaInfo);	} else {	
cannot set traits when there s no descriptor 

public void setStatistics(Statistics stats) {	if (LOG.isInfoEnabled()) {	
setting stats on 

public void setStatistics(Statistics stats) {	if (LOG.isInfoEnabled()) {	}	if (conf != null) {	conf.setStatistics(stats);	} else {	
cannot set stats when there s no descriptor 

private void publishRunTimeStats() throws HiveException {	StatsPublisher statsPublisher = new FSStatsPublisher();	StatsCollectionContext sContext = new StatsCollectionContext(hconf);	sContext.setIndexForTezUnion(indexForTezUnion);	sContext.setStatsTmpDir(conf.getRuntimeStatsTmpDir());	if (!statsPublisher.connect(sContext)) {	
statspublishing error cannot connect to database 

========================= hive sample_3898 =========================

public void handle(Context withinContext) throws Exception {	
processing alter database message 

========================= hive sample_3461 =========================

if (onlyContainsPartnCols(tab, filterOps)) {	String alias = (String) topOps.keySet().toArray()[0];	PrunedPartitionList partsList = pctx.getPrunedPartitions(alias, ts);	if (!partsList.hasUnknownPartitions()) {	Integer tempOffset = tempGlobalLimitDesc.getOffset();	globalLimitCtx.enableOpt(tempGlobalLimitDesc.getLimit(), (tempOffset == null) ? 0 : tempOffset);	}	}	}	if (globalLimitCtx.isEnable()) {	
qualify the optimize that reduces input size for offset for offset 

if (onlyContainsPartnCols(tab, filterOps)) {	String alias = (String) topOps.keySet().toArray()[0];	PrunedPartitionList partsList = pctx.getPrunedPartitions(alias, ts);	if (!partsList.hasUnknownPartitions()) {	Integer tempOffset = tempGlobalLimitDesc.getOffset();	globalLimitCtx.enableOpt(tempGlobalLimitDesc.getLimit(), (tempOffset == null) ? 0 : tempOffset);	}	}	}	if (globalLimitCtx.isEnable()) {	
qualify the optimize that reduces input size for limit for limit 

========================= hive sample_3031 =========================

public static void tearDown() throws Exception {	if (!useExternalMS) {	
shutting down metastore 

public void testDropTableException() throws Exception {	HCatClient client = HCatClient.create(new Configuration(hcatConf));	String tableName = "tableToBeDropped";	boolean isExceptionCaught = false;	client.dropTable(null, tableName, true);	try {	client.dropTable(null, tableName, false);	} catch (Exception exp) {	isExceptionCaught = true;	assertTrue(exp instanceof HCatException);	
drop table exception 

client.dropDatabase(dbName, true, HCatClient.DropDBMode.CASCADE);	client.createDatabase(HCatCreateDBDesc.create(dbName).build());	List<HCatFieldSchema> oldSchema = Arrays.asList(new HCatFieldSchema("foo", Type.INT, ""), new HCatFieldSchema("bar", Type.STRING, ""));	client.createTable(HCatCreateTableDesc.create(dbName, tableName, oldSchema).build());	List<HCatFieldSchema> newSchema = Arrays.asList(new HCatFieldSchema("completely", Type.DOUBLE, ""), new HCatFieldSchema("new", Type.STRING, ""), new HCatFieldSchema("fields", Type.STRING, ""));	client.updateTableSchema(dbName, tableName, newSchema);	assertArrayEquals(newSchema.toArray(), client.getTable(dbName, tableName).getCols().toArray());	client.dropDatabase(dbName, false, HCatClient.DropDBMode.CASCADE);	}	catch (Exception exception) {	
unexpected exception 

public void testObjectNotFoundException() throws Exception {	try {	HCatClient client = HCatClient.create(new  Configuration(hcatConf));	String dbName = "testObjectNotFoundException_DBName";	String tableName = "testObjectNotFoundException_TableName";	client.dropDatabase(dbName, true, HCatClient.DropDBMode.CASCADE);	try {	client.getDatabase(dbName);	assertTrue("Expected ObjectNotFoundException.", false);	} catch(Exception exception) {	
got exception 

client.getDatabase(dbName);	assertTrue("Expected ObjectNotFoundException.", false);	} catch(Exception exception) {	assertTrue("Expected ObjectNotFoundException. Got:" + exception.getClass(), exception instanceof ObjectNotFoundException);	}	client.createDatabase(HCatCreateDBDesc.create(dbName).build());	try {	client.getTable(dbName, tableName);	assertTrue("Expected ObjectNotFoundException.", false);	} catch(Exception exception) {	
got exception 

ArrayList<HCatFieldSchema> partitionColumns = new ArrayList<HCatFieldSchema>( Arrays.asList(new HCatFieldSchema(partitionColumn, Type.STRING, "")));	HCatTable table = new HCatTable(dbName, tableName).cols(columns).partCols(partitionColumns);	client.createTable(HCatCreateTableDesc.create(table, false).build());	HCatTable createdTable = client.getTable(dbName,tableName);	Map<String, String> partitionSpec = new HashMap<String, String>();	partitionSpec.put(partitionColumn, "foobar");	try {	client.getPartition(dbName, tableName, partitionSpec);	assertTrue("Expected ObjectNotFoundException.", false);	} catch(Exception exception) {	
got exception 

} catch(Exception exception) {	assertTrue("Expected ObjectNotFoundException. Got:" + exception.getClass(), exception instanceof ObjectNotFoundException);	}	client.addPartition(HCatAddPartitionDesc.create(new HCatPartition(createdTable, partitionSpec, makePartLocation(createdTable,partitionSpec))).build());	assertEquals("Expected empty set of partitions.", 0, client.listPartitionsByFilter(dbName, tableName, partitionColumn + " < 'foobar'").size());	try {	partitionSpec.put("NonExistentKey", "foobar");	client.getPartition(dbName, tableName, partitionSpec);	assertTrue("Expected HCatException.", false);	} catch(Exception exception) {	
got exception 

try {	partitionSpec.put("NonExistentKey", "foobar");	client.getPartition(dbName, tableName, partitionSpec);	assertTrue("Expected HCatException.", false);	} catch(Exception exception) {	assertTrue("Expected HCatException. Got:" + exception.getClass(), exception instanceof HCatException);	assertFalse("Did not expect ObjectNotFoundException.", exception instanceof ObjectNotFoundException);	}	}	catch (Throwable t) {	
unexpected exception 

client.createDatabase(HCatCreateDBDesc.create(dbName).build());	String messageBusTopicName = "MY.topic.name";	Map<String, String> tableProperties = new HashMap<String, String>(1);	tableProperties.put(HCatConstants.HCAT_MSGBUS_TOPIC_NAME, messageBusTopicName);	client.createTable(HCatCreateTableDesc.create(dbName, tableName, Arrays.asList(new HCatFieldSchema("foo", Type.STRING, ""))).tblProps(tableProperties).build());	assertEquals("MessageBus topic-name doesn't match!", messageBusTopicName, client.getMessageBusTopicName(dbName, tableName));	client.dropDatabase(dbName, true, HCatClient.DropDBMode.CASCADE);	client.close();	}	catch (Exception exception) {	
unexpected exception 

client.createDatabase(HCatCreateDBDesc.create(dbName).build());	List<HCatFieldSchema> columnSchema = Arrays.asList(new HCatFieldSchema("foo", Type.INT, ""), new HCatFieldSchema("bar", Type.STRING, ""));	List<HCatFieldSchema> partitionSchema = Arrays.asList(new HCatFieldSchema("dt", Type.STRING, ""), new HCatFieldSchema("grid", Type.STRING, ""));	client.createTable(HCatCreateTableDesc.create(dbName, tableName, columnSchema).partCols(partitionSchema).build());	HCatTable table = client.getTable(dbName, tableName);	List<HCatFieldSchema> partitionColumns = table.getPartCols();	assertArrayEquals("Didn't get expected partition-schema back from the HCatTable.", partitionSchema.toArray(), partitionColumns.toArray());	client.dropDatabase(dbName, false, HCatClient.DropDBMode.CASCADE);	}	catch (Exception unexpected) {	
unexpected exception 

Map<String, String> partialPartitionSpec = new HashMap<String, String>();	partialPartitionSpec.put("dt", "2012_01_01");	List<HCatPartition> partitions = client.getPartitions(dbName, tableName, partialPartitionSpec);	assertEquals("Unexpected number of partitions.", 3, partitions.size());	assertArrayEquals("Mismatched partition.", new String[]{"2012_01_01", "AB"}, partitions.get(0).getValues().toArray());	assertArrayEquals("Mismatched partition.", new String[]{"2012_01_01", "OB"}, partitions.get(1).getValues().toArray());	assertArrayEquals("Mismatched partition.", new String[]{"2012_01_01", "XB"}, partitions.get(2).getValues().toArray());	client.dropDatabase(dbName, false, HCatClient.DropDBMode.CASCADE);	}	catch (Exception unexpected) {	
unexpected exception 

List<HCatPartition> partitions = client.getPartitions(dbName, tableName);	assertEquals("Unexpected number of partitions.", 1, partitions.size());	assertArrayEquals("Mismatched partition.", new String[]{"2011_12_31", "AB"}, partitions.get(0).getValues().toArray());	List<HCatFieldSchema> partColumns = partitions.get(0).getPartColumns();	assertEquals(2, partColumns.size());	assertEquals("dt", partColumns.get(0).getName());	assertEquals("grid", partColumns.get(1).getName());	client.dropDatabase(dbName, false, HCatClient.DropDBMode.CASCADE);	}	catch (Exception unexpected) {	
unexpected exception 

sourceMetastore.dropTable(dbName, tblName2, true);	long currId = sourceMetastore.getCurrentNotificationEventId();	assertTrue("currId[" + currId + "] must be more than 25 greater than baseId[" + baseId + "]", currId > baseId + 25);	}	List<HCatNotificationEvent> notifs = sourceMetastore.getNextNotification( 0, 0, new IMetaStoreClient.NotificationFilter() {	public boolean accept(NotificationEvent event) {	return true;	}	});	for(HCatNotificationEvent n : notifs){	
notif from dblistener t o 

public boolean accept(NotificationEvent event) {	return true;	}	});	for(HCatNotificationEvent n : notifs){	}	Iterator<ReplicationTask> taskIter = sourceMetastore.getReplicationTasks(0, -1, dbName, null);	while(taskIter.hasNext()){	ReplicationTask task = taskIter.next();	HCatNotificationEvent n = task.getEvent();	
notif from tasks t o s 

public boolean accept(NotificationEvent event) {	return true;	}	});	for(HCatNotificationEvent n : notifs){	}	Iterator<ReplicationTask> taskIter = sourceMetastore.getReplicationTasks(0, -1, dbName, null);	while(taskIter.hasNext()){	ReplicationTask task = taskIter.next();	HCatNotificationEvent n = task.getEvent();	
task 

});	for(HCatNotificationEvent n : notifs){	}	Iterator<ReplicationTask> taskIter = sourceMetastore.getReplicationTasks(0, -1, dbName, null);	while(taskIter.hasNext()){	ReplicationTask task = taskIter.next();	HCatNotificationEvent n = task.getEvent();	if (task.needsStagingDirs()){	StagingDirectoryProvider provider = new StagingDirectoryProvider() {	public String getStagingDirectory(String key) {	
getstagingdirectory called 

HCatNotificationEvent n = task.getEvent();	if (task.needsStagingDirs()){	StagingDirectoryProvider provider = new StagingDirectoryProvider() {	public String getStagingDirectory(String key) {	return "/tmp/" + key.replaceAll(" ","_");	}	};	task .withSrcStagingDirProvider(provider) .withDstStagingDirProvider(provider);	}	if (task.isActionable()){	
task was actionable 

}	locns = command.cleanupLocationsAfterEvent();	sb.append("cleanupLocationsAfterEvent entries :" + locns.size());	for (String s : locns){	sb.append("AFTER_EVENT_CLEANUP:"+s);	sb.append("\n");	}	return sb.toString();	}	};	
on src 

for (String s : locns){	sb.append("AFTER_EVENT_CLEANUP:"+s);	sb.append("\n");	}	return sb.toString();	}	};	for (String s : Iterables.transform(task.getSrcWhCommands(), commandDebugPrinter)){	LOG.info(s);	}	
on dest 

return sb.toString();	}	};	for (String s : Iterables.transform(task.getSrcWhCommands(), commandDebugPrinter)){	LOG.info(s);	}	for (String s : Iterables.transform(task.getDstWhCommands(), commandDebugPrinter)){	LOG.info(s);	}	} else {	
task was not actionable 

assertTrue("Couldn't find change in InputFormat.", diff.contains(HCatTable.TableAttribute.INPUT_FORMAT));	assertTrue("Couldn't find change in OutputFormat.", diff.contains(HCatTable.TableAttribute.OUTPUT_FORMAT));	assertTrue("Couldn't find change in SerDe.", diff.contains(HCatTable.TableAttribute.SERDE));	assertTrue("Couldn't find change in SerDe parameters.", diff.contains(HCatTable.TableAttribute.SERDE_PROPERTIES));	assertTrue("Couldn't find change in Table parameters.", diff.contains(HCatTable.TableAttribute.TABLE_PROPERTIES));	targetMetaStore().updateTableSchema(dbName, tableName, targetTable.resolve(sourceTable, diff));	targetTable = targetMetaStore().getTable(dbName, tableName);	assertEquals("After propagating schema changes, source and target tables should have been equivalent.", HCatTable.NO_DIFF, targetTable.diff(sourceTable));	}	catch (Exception unexpected) {	
unexpected exception 

for (int i=0; i<targetPartitions.size(); ++i) {	HCatPartition sourcePartition = sourcePartitions.get(i), targetPartition = targetPartitions.get(i);	assertEquals("Column schema doesn't match.", sourcePartition.getColumns(), targetPartition.getColumns());	assertEquals("InputFormat doesn't match.", sourcePartition.getInputFormat(), targetPartition.getInputFormat());	assertEquals("OutputFormat doesn't match.", sourcePartition.getOutputFormat(), targetPartition.getOutputFormat());	assertEquals("SerDe doesn't match.", sourcePartition.getSerDe(), targetPartition.getSerDe());	assertEquals("SerDe params don't match.", sourcePartition.getSerdeParams(), targetPartition.getSerdeParams());	}	}	catch (Exception unexpected) {	
unexpected exception 

HCatPartition sourcePartition = sourceIterator.next();	HCatPartition targetPartition = targetIterator.next();	assertEquals("Column schema doesn't match.", sourcePartition.getColumns(), targetPartition.getColumns());	assertEquals("InputFormat doesn't match.", sourcePartition.getInputFormat(), targetPartition.getInputFormat());	assertEquals("OutputFormat doesn't match.", sourcePartition.getOutputFormat(), targetPartition.getOutputFormat());	assertEquals("SerDe doesn't match.", sourcePartition.getSerDe(), targetPartition.getSerDe());	assertEquals("SerDe params don't match.", sourcePartition.getSerdeParams(), targetPartition.getSerdeParams());	}	}	catch (Exception unexpected) {	
unexpected exception 

========================= hive sample_868 =========================

public String[] mapToHiveType(String clientTypeName) {	Collection<String> hiveTableType = clientToHiveMap.get(clientTypeName.toUpperCase());	if (hiveTableType == null) {	
not supported client table type 

public String mapToClientType(String hiveTypeName) {	String clientTypeName = hiveToClientMap.get(hiveTypeName);	if (clientTypeName == null) {	
invalid hive table type 

========================= hive sample_2335 =========================

try {	columnMapper = getColumnMapper(jobConf);	} catch (TooManyAccumuloColumnsException e) {	throw new IOException(e);	}	JobContext context = ShimLoader.getHadoopShims().newJobContext(Job.getInstance(jobConf));	Path[] tablePaths = FileInputFormat.getInputPaths(context);	try {	Connector connector = null;	if (accumuloParams.useSasl()) {	
current user 

} catch (TooManyAccumuloColumnsException e) {	throw new IOException(e);	}	JobContext context = ShimLoader.getHadoopShims().newJobContext(Job.getInstance(jobConf));	Path[] tablePaths = FileInputFormat.getInputPaths(context);	try {	Connector connector = null;	if (accumuloParams.useSasl()) {	AuthenticationToken token = ConfiguratorBase.getAuthenticationToken( AccumuloInputFormat.class, jobConf);	if (null != token && !jobConf.getCredentials().getAllTokens().isEmpty()) {	
found authentication token in configuration 

} catch (TooManyAccumuloColumnsException e) {	throw new IOException(e);	}	JobContext context = ShimLoader.getHadoopShims().newJobContext(Job.getInstance(jobConf));	Path[] tablePaths = FileInputFormat.getInputPaths(context);	try {	Connector connector = null;	if (accumuloParams.useSasl()) {	AuthenticationToken token = ConfiguratorBase.getAuthenticationToken( AccumuloInputFormat.class, jobConf);	if (null != token && !jobConf.getCredentials().getAllTokens().isEmpty()) {	
job credential tokens 

throw new IOException(e);	}	JobContext context = ShimLoader.getHadoopShims().newJobContext(Job.getInstance(jobConf));	Path[] tablePaths = FileInputFormat.getInputPaths(context);	try {	Connector connector = null;	if (accumuloParams.useSasl()) {	AuthenticationToken token = ConfiguratorBase.getAuthenticationToken( AccumuloInputFormat.class, jobConf);	if (null != token && !jobConf.getCredentials().getAllTokens().isEmpty()) {	AuthenticationToken unwrappedToken = ConfiguratorBase.unwrapAuthenticationToken(jobConf, token);	
converted authentication token from configuration into 

}	JobContext context = ShimLoader.getHadoopShims().newJobContext(Job.getInstance(jobConf));	Path[] tablePaths = FileInputFormat.getInputPaths(context);	try {	Connector connector = null;	if (accumuloParams.useSasl()) {	AuthenticationToken token = ConfiguratorBase.getAuthenticationToken( AccumuloInputFormat.class, jobConf);	if (null != token && !jobConf.getCredentials().getAllTokens().isEmpty()) {	AuthenticationToken unwrappedToken = ConfiguratorBase.unwrapAuthenticationToken(jobConf, token);	if (unwrappedToken != token) {	
creating accumulo connector with unwrapped delegation token 

Path[] tablePaths = FileInputFormat.getInputPaths(context);	try {	Connector connector = null;	if (accumuloParams.useSasl()) {	AuthenticationToken token = ConfiguratorBase.getAuthenticationToken( AccumuloInputFormat.class, jobConf);	if (null != token && !jobConf.getCredentials().getAllTokens().isEmpty()) {	AuthenticationToken unwrappedToken = ConfiguratorBase.unwrapAuthenticationToken(jobConf, token);	if (unwrappedToken != token) {	connector = instance.getConnector(accumuloParams.getAccumuloUserName(), unwrappedToken);	} else {	
job credentials did not contain delegation token fetching new token 

if (accumuloParams.useSasl()) {	AuthenticationToken token = ConfiguratorBase.getAuthenticationToken( AccumuloInputFormat.class, jobConf);	if (null != token && !jobConf.getCredentials().getAllTokens().isEmpty()) {	AuthenticationToken unwrappedToken = ConfiguratorBase.unwrapAuthenticationToken(jobConf, token);	if (unwrappedToken != token) {	connector = instance.getConnector(accumuloParams.getAccumuloUserName(), unwrappedToken);	} else {	}	}	if (connector == null) {	
obtaining accumulo connector using kerberostoken 

if (numColumns < readColIds.size()) throw new IOException("Number of column mappings (" + numColumns + ")" + " numbers less than the hive table columns. (" + readColIds.size() + ")");	InputSplit[] splits = accumuloInputFormat.getSplits(jobConf, numSplits);	HiveAccumuloSplit[] hiveSplits = new HiveAccumuloSplit[splits.length];	for (int i = 0; i < splits.length; i++) {	RangeInputSplit ris = (RangeInputSplit) splits[i];	ris.setLogLevel(Level.DEBUG);	hiveSplits[i] = new HiveAccumuloSplit(ris, tablePaths[0]);	}	return hiveSplits;	} catch (AccumuloException e) {	
could not configure accumuloinputformat 

HiveAccumuloSplit[] hiveSplits = new HiveAccumuloSplit[splits.length];	for (int i = 0; i < splits.length; i++) {	RangeInputSplit ris = (RangeInputSplit) splits[i];	ris.setLogLevel(Level.DEBUG);	hiveSplits[i] = new HiveAccumuloSplit(ris, tablePaths[0]);	}	return hiveSplits;	} catch (AccumuloException e) {	throw new IOException(StringUtils.stringifyException(e));	} catch (AccumuloSecurityException e) {	
could not configure accumuloinputformat 

RangeInputSplit ris = (RangeInputSplit) splits[i];	ris.setLogLevel(Level.DEBUG);	hiveSplits[i] = new HiveAccumuloSplit(ris, tablePaths[0]);	}	return hiveSplits;	} catch (AccumuloException e) {	throw new IOException(StringUtils.stringifyException(e));	} catch (AccumuloSecurityException e) {	throw new IOException(StringUtils.stringifyException(e));	} catch (SerDeException e) {	
could not configure accumuloinputformat 

try {	columnMapper = getColumnMapper(jobConf);	} catch (TooManyAccumuloColumnsException e) {	throw new IOException(e);	}	try {	final AccumuloConnectionParameters accumuloParams = new AccumuloConnectionParameters( jobConf);	final List<IteratorSetting> iterators = predicateHandler.getIterators(jobConf, columnMapper);	HiveAccumuloSplit hiveSplit = (HiveAccumuloSplit) inputSplit;	RangeInputSplit rangeSplit = hiveSplit.getSplit();	
split 

columnMapper = getColumnMapper(jobConf);	} catch (TooManyAccumuloColumnsException e) {	throw new IOException(e);	}	try {	final AccumuloConnectionParameters accumuloParams = new AccumuloConnectionParameters( jobConf);	final List<IteratorSetting> iterators = predicateHandler.getIterators(jobConf, columnMapper);	HiveAccumuloSplit hiveSplit = (HiveAccumuloSplit) inputSplit;	RangeInputSplit rangeSplit = hiveSplit.getSplit();	if (null == rangeSplit.getIterators() || (rangeSplit.getIterators().isEmpty() && !iterators.isEmpty())) {	
re setting iterators on inputsplit due to accumulo bug 

getHelper().setInputFormatConnectorInfo(conf, accumuloParams.getAccumuloUserName(), new PasswordToken(accumuloParams.getAccumuloPassword()));	}	setInputTableName(conf, accumuloParams.getAccumuloTableName());	Authorizations auths = AccumuloSerDeParameters.getAuthorizationsFromConf(conf);	if (null == auths) {	auths = connector.securityOperations().getUserAuthorizations( accumuloParams.getAccumuloUserName());	}	setScanAuthorizations(conf, auths);	addIterators(conf, iterators);	if (null != ranges) {	
setting ranges 

Text cq = null;	if (null != accumuloColumnMapping.getColumnQualifier()) {	cq = new Text(accumuloColumnMapping.getColumnQualifier());	}	pairs.add(new Pair<Text,Text>(cf, cq));	} else if (columnMapping instanceof HiveAccumuloMapColumnMapping) {	HiveAccumuloMapColumnMapping mapMapping = (HiveAccumuloMapColumnMapping) columnMapping;	pairs.add(new Pair<Text,Text>(new Text(mapMapping.getColumnFamily()), null));	}	}	
computed columns to fetch from 

========================= hive sample_184 =========================

setNumberOfReducers();	if (!ctx.isLocalOnlyExecutionMode() && conf.getBoolVar(HiveConf.ConfVars.LOCALMODEAUTO)) {	if (inputSummary == null) {	inputSummary = Utilities.getInputSummary(driverContext.getCtx(), work.getMapWork(), null);	}	double samplePercentage = Utilities.getHighestSamplePercentage(work.getMapWork());	totalInputFileSize = Utilities.getTotalInputFileSize(inputSummary, work.getMapWork(), samplePercentage);	totalInputNumFiles = Utilities.getTotalInputNumFiles(inputSummary, work.getMapWork(), samplePercentage);	int numReducers = work.getReduceWork() == null ? 0 : work.getReduceWork().getNumReduceTasks();	if (LOG.isDebugEnabled()) {	
task summary 

}	cloneConf();	super.setInputAttributes(conf);	String hadoopExec = conf.getVar(HiveConf.ConfVars.HADOOPBIN);	String hiveJar = conf.getJar();	String libJars = super.getResource(conf, ResourceType.JAR);	String libJarsOption = StringUtils.isEmpty(libJars) ? " " : " -libjars " + libJars + " ";	String hiveConfArgs = generateCmdLine(conf, ctx);	Path planPath = new Path(ctx.getLocalTmpPath(), "plan.xml");	MapredWork plan = getWork();	
generating plan file 

}	for (String f: StringUtils.split(files, ',')) {	Path p = new Path(f);	String target = p.toUri().getPath();	String link = workDir + Path.SEPARATOR + p.getName();	if (FileUtil.symLink(target, link) != 0) {	throw new IOException ("Cannot link to added file: " + target + " from: " + link);	}	}	}	
executing 

executor = Runtime.getRuntime().exec(cmdLine, env, new File(workDir));	CachingPrintStream errPrintStream = new CachingPrintStream(SessionState.getConsole().getChildErrStream());	StreamPrinter outPrinter = new StreamPrinter( executor.getInputStream(), null, SessionState.getConsole().getChildOutStream());	StreamPrinter errPrinter = new StreamPrinter( executor.getErrorStream(), null, errPrintStream);	outPrinter.start();	errPrinter.start();	int exitVal = jobExecHelper.progressLocal(executor, getId());	outPrinter.join();	errPrinter.join();	if (exitVal != 0) {	
execution failed with exit status 

outPrinter.start();	errPrinter.start();	int exitVal = jobExecHelper.progressLocal(executor, getId());	outPrinter.join();	errPrinter.join();	if (exitVal != 0) {	if (SessionState.get() != null) {	SessionState.get().addLocalMapRedErrors(getId(), errPrintStream.getOutput());	}	} else {	
execution completed successfully 

outPrinter.join();	errPrinter.join();	if (exitVal != 0) {	if (SessionState.get() != null) {	SessionState.get().addLocalMapRedErrors(getId(), errPrintStream.getOutput());	}	} else {	}	return exitVal;	} catch (Exception e) {	
got exception 

}	return exitVal;	} catch (Exception e) {	return (1);	} finally {	try {	if(ctxCreated) {	ctx.clear();	}	} catch (Exception e) {	
exception 

========================= hive sample_3877 =========================

public HiveSplitGenerator(Configuration conf, MapWork work) throws IOException {	super(null);	this.conf = conf;	this.work = work;	this.jobConf = new JobConf(conf);	userPayloadProto = MRInputUserPayloadProto.newBuilder().setGroupingEnabled(true).build();	this.splitLocationProvider = Utils.getSplitLocationProvider(conf, LOG);	
splitlocationprovider 

public HiveSplitGenerator(InputInitializerContext initializerContext) throws IOException, SerDeException {	super(initializerContext);	Preconditions.checkNotNull(initializerContext);	userPayloadProto = MRInputHelpers.parseMRInputPayload(initializerContext.getInputUserPayload());	this.conf = TezUtils.createConfFromByteString(userPayloadProto.getConfigurationBytes());	this.jobConf = new JobConf(conf);	this.splitLocationProvider = Utils.getSplitLocationProvider(conf, LOG);	
splitlocationprovider 

if (getContext() != null) {	totalResource = getContext().getTotalAvailableResource().getMemory();	taskResource = getContext().getVertexTaskResource().getMemory();	availableSlots = totalResource / taskResource;	}	if (HiveConf.getLongVar(conf, HiveConf.ConfVars.MAPREDMINSPLITSIZE, 1) <= 1) {	final long blockSize = conf.getLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, DFSConfigKeys.DFS_BLOCK_SIZE_DEFAULT);	final long minGrouping = conf.getLong( TezMapReduceSplitsGrouper.TEZ_GROUPING_SPLIT_MIN_SIZE, TezMapReduceSplitsGrouper.TEZ_GROUPING_SPLIT_MIN_SIZE_DEFAULT);	final long preferredSplitSize = Math.min(blockSize / 2, minGrouping);	HiveConf.setLongVar(jobConf, HiveConf.ConfVars.MAPREDMINSPLITSIZE, preferredSplitSize);	
the preferred split size is 

}	if (HiveConf.getLongVar(conf, HiveConf.ConfVars.MAPREDMINSPLITSIZE, 1) <= 1) {	final long blockSize = conf.getLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, DFSConfigKeys.DFS_BLOCK_SIZE_DEFAULT);	final long minGrouping = conf.getLong( TezMapReduceSplitsGrouper.TEZ_GROUPING_SPLIT_MIN_SIZE, TezMapReduceSplitsGrouper.TEZ_GROUPING_SPLIT_MIN_SIZE_DEFAULT);	final long preferredSplitSize = Math.min(blockSize / 2, minGrouping);	HiveConf.setLongVar(jobConf, HiveConf.ConfVars.MAPREDMINSPLITSIZE, preferredSplitSize);	}	float waves = conf.getFloat(TezMapReduceSplitsGrouper.TEZ_GROUPING_SPLIT_WAVES, TezMapReduceSplitsGrouper.TEZ_GROUPING_SPLIT_WAVES_DEFAULT);	InputSplit[] splits = inputFormat.getSplits(jobConf, (int) (availableSlots * waves));	Arrays.sort(splits, new InputSplitComparator());	
number of input splits available slots waves input format is 

HiveConf.setLongVar(jobConf, HiveConf.ConfVars.MAPREDMINSPLITSIZE, preferredSplitSize);	}	float waves = conf.getFloat(TezMapReduceSplitsGrouper.TEZ_GROUPING_SPLIT_WAVES, TezMapReduceSplitsGrouper.TEZ_GROUPING_SPLIT_WAVES_DEFAULT);	InputSplit[] splits = inputFormat.getSplits(jobConf, (int) (availableSlots * waves));	Arrays.sort(splits, new InputSplitComparator());	if (work.getIncludedBuckets() != null) {	splits = pruneBuckets(work, splits);	}	Multimap<Integer, InputSplit> groupedSplits = splitGrouper.generateGroupedSplits(jobConf, conf, splits, waves, availableSlots, splitLocationProvider);	InputSplit[] flatSplits = groupedSplits.values().toArray(new InputSplit[0]);	
number of split groups 

private InputSplit[] pruneBuckets(MapWork work, InputSplit[] splits) {	final BitSet buckets = work.getIncludedBuckets();	final String bucketIn = buckets.toString();	List<InputSplit> filteredSplits = new ArrayList<InputSplit>(splits.length / 2);	for (InputSplit split : splits) {	final int bucket = Utilities.parseSplitBucket(split);	if (bucket < 0 || buckets.get(bucket)) {	filteredSplits.add(split);	} else {	
pruning with in removing 

========================= hive sample_3965 =========================

public void testStringAppender() throws Exception {	Logger logger = LogManager.getRootLogger();	StringAppender appender = StringAppender.createStringAppender("%m");	appender.addToLogger(logger.getName(), Level.INFO);	appender.start();	
hello 

public void testStringAppender() throws Exception {	Logger logger = LogManager.getRootLogger();	StringAppender appender = StringAppender.createStringAppender("%m");	appender.addToLogger(logger.getName(), Level.INFO);	appender.start();	
world 

public void testHiveEventCounterAppender() throws Exception {	Logger logger = LogManager.getRootLogger();	HiveEventCounter appender = HiveEventCounter.createInstance("EventCounter", true, null, null);	appender.addToLogger(logger.getName(), Level.INFO);	appender.start();	
Test 

public void testHiveEventCounterAppender() throws Exception {	Logger logger = LogManager.getRootLogger();	HiveEventCounter appender = HiveEventCounter.createInstance("EventCounter", true, null, null);	appender.addToLogger(logger.getName(), Level.INFO);	appender.start();	
Test 

public void testHiveEventCounterAppender() throws Exception {	Logger logger = LogManager.getRootLogger();	HiveEventCounter appender = HiveEventCounter.createInstance("EventCounter", true, null, null);	appender.addToLogger(logger.getName(), Level.INFO);	appender.start();	
Test 

public void testHiveEventCounterAppender() throws Exception {	Logger logger = LogManager.getRootLogger();	HiveEventCounter appender = HiveEventCounter.createInstance("EventCounter", true, null, null);	appender.addToLogger(logger.getName(), Level.INFO);	appender.start();	
Test 

public void testHiveEventCounterAppender() throws Exception {	Logger logger = LogManager.getRootLogger();	HiveEventCounter appender = HiveEventCounter.createInstance("EventCounter", true, null, null);	appender.addToLogger(logger.getName(), Level.INFO);	appender.start();	
Test 

public void testHiveEventCounterAppender() throws Exception {	Logger logger = LogManager.getRootLogger();	HiveEventCounter appender = HiveEventCounter.createInstance("EventCounter", true, null, null);	appender.addToLogger(logger.getName(), Level.INFO);	appender.start();	
Test 

public void testHiveEventCounterAppender() throws Exception {	Logger logger = LogManager.getRootLogger();	HiveEventCounter appender = HiveEventCounter.createInstance("EventCounter", true, null, null);	appender.addToLogger(logger.getName(), Level.INFO);	appender.start();	
Test 

public void testHiveEventCounterAppender() throws Exception {	Logger logger = LogManager.getRootLogger();	HiveEventCounter appender = HiveEventCounter.createInstance("EventCounter", true, null, null);	appender.addToLogger(logger.getName(), Level.INFO);	appender.start();	
Test 

public void testHiveEventCounterAppender() throws Exception {	Logger logger = LogManager.getRootLogger();	HiveEventCounter appender = HiveEventCounter.createInstance("EventCounter", true, null, null);	appender.addToLogger(logger.getName(), Level.INFO);	appender.start();	
Test 

public void testHiveEventCounterAppender() throws Exception {	Logger logger = LogManager.getRootLogger();	HiveEventCounter appender = HiveEventCounter.createInstance("EventCounter", true, null, null);	appender.addToLogger(logger.getName(), Level.INFO);	appender.start();	
Test 

========================= hive sample_2677 =========================

public void serviceStart() {	QueueLookupCallable queueDrainerCallable = new QueueLookupCallable();	queueLookupFuture = queueLookupExecutor.submit(queueDrainerCallable);	Futures.addCallback(queueLookupFuture, new FutureCallback<Void>() {	public void onSuccess(Void result) {	
amreporter queuedrainer exited 

public void serviceStart() {	QueueLookupCallable queueDrainerCallable = new QueueLookupCallable();	queueLookupFuture = queueLookupExecutor.submit(queueDrainerCallable);	Futures.addCallback(queueLookupFuture, new FutureCallback<Void>() {	public void onSuccess(Void result) {	}	public void onFailure(Throwable t) {	if (t instanceof CancellationException && isShutdown.get()) {	
amreporter queuedrainer exited as a result of a cancellation after shutdown 

public void serviceStart() {	QueueLookupCallable queueDrainerCallable = new QueueLookupCallable();	queueLookupFuture = queueLookupExecutor.submit(queueDrainerCallable);	Futures.addCallback(queueLookupFuture, new FutureCallback<Void>() {	public void onSuccess(Void result) {	}	public void onFailure(Throwable t) {	if (t instanceof CancellationException && isShutdown.get()) {	} else {	
amreporter queuedrainer exited with error 

public void onSuccess(Void result) {	}	public void onFailure(Throwable t) {	if (t instanceof CancellationException && isShutdown.get()) {	} else {	Thread.getDefaultUncaughtExceptionHandler().uncaughtException(Thread.currentThread(), t);	}	}	});	nodeId = LlapNodeId.getInstance(localAddress.get().getHostName(), localAddress.get().getPort());	
amreporter running with daemonid nodeid 

public void serviceStop() {	if (!isShutdown.getAndSet(true)) {	if (queueLookupFuture != null) {	queueLookupFuture.cancel(true);	}	queueLookupExecutor.shutdownNow();	executor.shutdownNow();	
stopped service 

public void unregisterTask(String amLocation, int port, QueryIdentifier queryIdentifier, TezTaskAttemptID ta) {	if (LOG.isTraceEnabled()) {	LOG.trace("Un-registering for heartbeat: {}, attempt={}", (amLocation + ":" + port), ta);	}	AMNodeInfo amNodeInfo;	synchronized (knownAppMasters) {	amNodeInfo = getAMNodeInfo(amLocation, port, queryIdentifier);	if (amNodeInfo == null) {	
ignoring duplicate unregisterrequest for am at 

AMNodeInfo amNodeInfo;	synchronized (knownAppMasters) {	amNodeInfo = getAMNodeInfo(amLocation, port, queryIdentifier);	if (amNodeInfo == null) {	amNodeInfo = new AMNodeInfo(amNodeId, umbilicalUser, jobToken, queryIdentifier, retryPolicy, retryTimeout, socketFactory, conf);	}	}	ListenableFuture<Void> future = executor.submit(new KillTaskCallable(taskAttemptId, amNodeInfo));	Futures.addCallback(future, new FutureCallback<Void>() {	public void onSuccess(Void result) {	
sent taskkilled for 

amNodeInfo = getAMNodeInfo(amLocation, port, queryIdentifier);	if (amNodeInfo == null) {	amNodeInfo = new AMNodeInfo(amNodeId, umbilicalUser, jobToken, queryIdentifier, retryPolicy, retryTimeout, socketFactory, conf);	}	}	ListenableFuture<Void> future = executor.submit(new KillTaskCallable(taskAttemptId, amNodeInfo));	Futures.addCallback(future, new FutureCallback<Void>() {	public void onSuccess(Void result) {	}	public void onFailure(Throwable t) {	
failed to send taskkilled for the attempt will likely time out 

public void queryComplete(QueryIdentifier queryIdentifier) {	if (queryIdentifier != null) {	synchronized (knownAppMasters) {	
query complete received for 

public void queryComplete(QueryIdentifier queryIdentifier) {	if (queryIdentifier != null) {	synchronized (knownAppMasters) {	Map<LlapNodeId, AMNodeInfo> amNodeInfoPerQuery = knownAppMasters.remove(queryIdentifier);	if (amNodeInfoPerQuery != null) {	
removed following ams due to query complete 

QueryIdentifier currentQueryIdentifier = amNodeInfo.getQueryIdentifier();	amNodeInfo.setAmFailed(true);	LOG.warn("Heartbeat failed to AM {}. Marking query as failed. query={}", amNodeInfo.amNodeId, currentQueryIdentifier, t);	queryFailedHandler.queryFailed(currentQueryIdentifier);	}	});	}	}	} catch (InterruptedException e) {	if (isShutdown.get()) {	
queuelookup thread interrupted after shutdown 

amNodeInfo.setAmFailed(true);	LOG.warn("Heartbeat failed to AM {}. Marking query as failed. query={}", amNodeInfo.amNodeId, currentQueryIdentifier, t);	queryFailedHandler.queryFailed(currentQueryIdentifier);	}	});	}	}	} catch (InterruptedException e) {	if (isShutdown.get()) {	} else {	
received unexpected interrupt while waiting on heartbeat queue 

protected Void callInternal() {	try {	amNodeInfo.getUmbilical().taskKilled(taskAttemptId);	} catch (IOException e) {	
failed to send taskkilled message for task will re run after it times out 

protected Void callInternal() {	try {	amNodeInfo.getUmbilical().taskKilled(taskAttemptId);	} catch (IOException e) {	} catch (InterruptedException e) {	if (!isShutdown.get()) {	
interrupted while trying to send taskkilled message for task 

protected Void callInternal() {	if (LOG.isTraceEnabled()) {	
attempting to heartbeat to am 

protected Void callInternal() {	if (LOG.isTraceEnabled()) {	}	List<TezTaskAttemptID> tasks = amNodeInfo.getTasksSnapshot();	if (tasks.isEmpty()) {	return null;	}	try {	if (LOG.isTraceEnabled()) {	
nodeheartbeat to 

}	try {	if (LOG.isTraceEnabled()) {	}	TezAttemptArray aw = new TezAttemptArray();	aw.set(tasks.toArray(new TezTaskAttemptID[tasks.size()]));	amNodeInfo.getUmbilical().nodeHeartbeat(new Text(nodeId.getHostname()), new Text(daemonId.getUniqueNodeIdInCluster()), nodeId.getPort(), aw);	} catch (IOException e) {	QueryIdentifier currentQueryIdentifier = amNodeInfo.getQueryIdentifier();	amNodeInfo.setAmFailed(true);	
failed to communicated with am at killing remaining fragments for query 

}	TezAttemptArray aw = new TezAttemptArray();	aw.set(tasks.toArray(new TezTaskAttemptID[tasks.size()]));	amNodeInfo.getUmbilical().nodeHeartbeat(new Text(nodeId.getHostname()), new Text(daemonId.getUniqueNodeIdInCluster()), nodeId.getPort(), aw);	} catch (IOException e) {	QueryIdentifier currentQueryIdentifier = amNodeInfo.getQueryIdentifier();	amNodeInfo.setAmFailed(true);	queryFailedHandler.queryFailed(currentQueryIdentifier);	} catch (InterruptedException e) {	if (!isShutdown.get()) {	
interrupted while trying to send heartbeat to am 

========================= hive sample_2227 =========================

public void onRootVertexInitialized(String inputName, InputDescriptor inputDescriptor, List<Event> events) {	numInputsSeenSoFar++;	
on root vertex initialized 

MRInputUserPayloadProto updatedPayload = MRInputUserPayloadProto.newBuilder(protoPayload).setGroupingEnabled(true).build();	inputDescriptor.setUserPayload(UserPayload.create(updatedPayload.toByteString().asReadOnlyByteBuffer()));	} catch (IOException e) {	e.printStackTrace();	throw new RuntimeException(e);	}	boolean dataInformationEventSeen = false;	Map<String, Set<FileSplit>> pathFileSplitsMap = new TreeMap<String, Set<FileSplit>>();	for (Event event : events) {	if (event instanceof InputConfigureVertexTasksEvent) {	
got a input configure vertex event for input 

e.printStackTrace();	throw new RuntimeException(e);	}	boolean dataInformationEventSeen = false;	Map<String, Set<FileSplit>> pathFileSplitsMap = new TreeMap<String, Set<FileSplit>>();	for (Event event : events) {	if (event instanceof InputConfigureVertexTasksEvent) {	Preconditions.checkState(dataInformationEventSeen == false);	InputConfigureVertexTasksEvent cEvent = (InputConfigureVertexTasksEvent) event;	configureVertexTaskEvent = cEvent;	
configure task for input name num tasks 

throw new RuntimeException("Failed to get file split for event: " + diEvent, e);	}	Set<FileSplit> fsList = pathFileSplitsMap.get(Utilities.getBucketFileNameFromPathSubString(fileSplit.getPath() .getName()));	if (fsList == null) {	fsList = new TreeSet<FileSplit>(new PathComparatorForSplit());	pathFileSplitsMap.put( Utilities.getBucketFileNameFromPathSubString(fileSplit.getPath().getName()), fsList);	}	fsList.add(fileSplit);	}	}	
path file splits map for input name is 

}	fsList.add(fileSplit);	}	}	Multimap<Integer, InputSplit> bucketToInitialSplitMap = getBucketSplitMapForPath(pathFileSplitsMap);	try {	int totalResource = context.getTotalAvailableResource().getMemory();	int taskResource = context.getVertexTaskResource().getMemory();	float waves = conf.getFloat(TezMapReduceSplitsGrouper.TEZ_GROUPING_SPLIT_WAVES, TezMapReduceSplitsGrouper.TEZ_GROUPING_SPLIT_WAVES_DEFAULT);	int availableSlots = totalResource / taskResource;	
grouping splits available slots waves bucket initial splits map 

bucketToGroupedSplitMap.putAll(key, groupedSplit.values());	}	processAllEvents(inputName, bucketToGroupedSplitMap, secondLevelGroupingDone);	} else {	SplitLocationProvider splitLocationProvider = Utils.getSplitLocationProvider(conf, LOG);	for (Integer key : bucketToInitialSplitMap.keySet()) {	InputSplit[] inputSplitArray = (bucketToInitialSplitMap.get(key).toArray(new InputSplit[0]));	Multimap<Integer, InputSplit> groupedSplit = grouper.generateGroupedSplits(jobConf, conf, inputSplitArray, waves, availableSlots, inputName, false, splitLocationProvider);	bucketToGroupedSplitMap.putAll(key, groupedSplit.values());	}	
this is the side work multi mr work 

private void processAllSideEventsSetParallelism(String inputName, Multimap<Integer, InputSplit> bucketToGroupedSplitMap) throws IOException {	
processing events for input 

private void processAllSideEventsSetParallelism(String inputName, Multimap<Integer, InputSplit> bucketToGroupedSplitMap) throws IOException {	if (inputNameInputSpecMap.get(mainWorkName) == null) {	
we don t have a routing table yet will need to wait for the main input initialization 

private void processAllSideEvents(String inputName, Multimap<Integer, InputSplit> bucketToGroupedSplitMap) throws IOException {	List<InputDataInformationEvent> taskEvents = new ArrayList<InputDataInformationEvent>();	
we have a routing table and we are going to set the destination tasks for the multi mr inputs 

for (ByteBuffer buf : entry.getValue()) {	count++;	InputDataInformationEvent diEvent = InputDataInformationEvent.createWithSerializedPayload(count, buf);	diEvent.setTargetIndex(task);	taskEvents.add(diEvent);	}	numSplitsForTask[task] = count;	}	}	inputNameInputSpecMap.put(inputName, InputSpecUpdate.createPerTaskInputSpecUpdate(Arrays.asList(numSplitsForTask)));	
for input name task events size is 

if ((vertexType == VertexType.MULTI_INPUT_INITIALIZED_EDGES) || (vertexType == VertexType.INITIALIZED_EDGES)) {	hiveEdgeManagerDesc = EdgeManagerPluginDescriptor.create(CustomPartitionEdge.class.getName());	UserPayload payload = getBytePayload(bucketToTaskMap);	hiveEdgeManagerDesc.setUserPayload(payload);	}	for (Entry<String, EdgeProperty> edgeEntry : context.getInputVertexEdgeProperties().entrySet()) {	if (edgeEntry.getValue().getDataMovementType() == DataMovementType.CUSTOM && edgeEntry.getValue().getEdgeManagerDescriptor().getClassName() .equals(CustomPartitionEdge.class.getName())) {	emMap.put(edgeEntry.getKey(), hiveEdgeManagerDesc);	}	}	
task count is for input name 

taskEvents.add(diEvent);	}	} else {	MRSplitProto serializedSplit = MRInputHelpers.createSplitProto(inputSplit);	InputDataInformationEvent diEvent = InputDataInformationEvent.createWithSerializedPayload(count, serializedSplit .toByteString().asReadOnlyByteBuffer());	diEvent.setTargetIndex(count);	taskEvents.add(diEvent);	}	count++;	}	
for input name task events size is 

========================= hive sample_4023 =========================

needHashTableSetup = false;	}	batchCounter++;	innerPerBatchSetup(batch);	for(VectorExpression ve : bigTableFilterExpressions) {	ve.evaluate(batch);	}	final int inputLogicalSize = batch.size;	if (inputLogicalSize == 0) {	if (LOG.isDebugEnabled()) {	
batch empty 

joinResult = JoinUtil.JoinResult.NOMATCH;	} else {	long key = vector[0];	if (useMinMax && (key < min || key > max)) {	joinResult = JoinUtil.JoinResult.NOMATCH;	} else {	joinResult = hashMap.lookup(key, hashMapResults[0]);	}	}	if (LOG.isDebugEnabled()) {	
batch repeated joinresult 

joinResult = JoinUtil.JoinResult.NOMATCH;	} else {	joinResult = hashMap.lookup(key, hashMapResults[0]);	}	}	if (LOG.isDebugEnabled()) {	}	finishInnerRepeated(batch, joinResult, hashMapResults[0]);	} else {	if (LOG.isDebugEnabled()) {	
batch non repeated 

switch (saveJoinResult) {	case MATCH: hashMapResultCount++;	equalKeySeriesCount++;	break;	case SPILL: hashMapResultCount++;	break;	case NOMATCH: break;	}	}	if (LOG.isDebugEnabled()) {	
allmatchs equalkeyserieshashmapresultindices equalkeyseriesallmatchindices equalkeyseriesissinglevalue equalkeyseriesduplicatecounts spills spillhashmapresultindices hashmapresults 

========================= hive sample_4189 =========================

private void makeAcid(Table newTable) throws MetaException {	if(newTable.getParameters() != null && newTable.getParameters().containsKey(hive_metastoreConstants.TABLE_IS_TRANSACTIONAL)) {	LOG.info("Could not make " + Warehouse.getQualifiedName(newTable) + " acid: already has " + hive_metastoreConstants.TABLE_IS_TRANSACTIONAL + "=" + newTable.getParameters().get(hive_metastoreConstants.TABLE_IS_TRANSACTIONAL));	return;	}	Configuration conf = MetastoreConf.newMetastoreConf();	boolean makeAcid = MetastoreConf.getBoolVar(conf, MetastoreConf.ConfVars.CREATE_TABLES_AS_ACID) && MetastoreConf.getBoolVar(conf, MetastoreConf.ConfVars.HIVE_SUPPORT_CONCURRENCY) && "org.apache.hadoop.hive.ql.lockmgr.DbTxnManager".equals( MetastoreConf.getVar(conf, MetastoreConf.ConfVars.HIVE_TXN_MANAGER) );	if(makeAcid) {	if(!conformToAcid(newTable)) {	
could not make acid wrong io format 

LOG.info("Could not make " + Warehouse.getQualifiedName(newTable) + " acid: already has " + hive_metastoreConstants.TABLE_IS_TRANSACTIONAL + "=" + newTable.getParameters().get(hive_metastoreConstants.TABLE_IS_TRANSACTIONAL));	return;	}	Configuration conf = MetastoreConf.newMetastoreConf();	boolean makeAcid = MetastoreConf.getBoolVar(conf, MetastoreConf.ConfVars.CREATE_TABLES_AS_ACID) && MetastoreConf.getBoolVar(conf, MetastoreConf.ConfVars.HIVE_SUPPORT_CONCURRENCY) && "org.apache.hadoop.hive.ql.lockmgr.DbTxnManager".equals( MetastoreConf.getVar(conf, MetastoreConf.ConfVars.HIVE_TXN_MANAGER) );	if(makeAcid) {	if(!conformToAcid(newTable)) {	return;	}	if(!TableType.MANAGED_TABLE.toString().equalsIgnoreCase(newTable.getTableType())) {	
could not make acid it s 

Configuration conf = MetastoreConf.newMetastoreConf();	boolean makeAcid = MetastoreConf.getBoolVar(conf, MetastoreConf.ConfVars.CREATE_TABLES_AS_ACID) && MetastoreConf.getBoolVar(conf, MetastoreConf.ConfVars.HIVE_SUPPORT_CONCURRENCY) && "org.apache.hadoop.hive.ql.lockmgr.DbTxnManager".equals( MetastoreConf.getVar(conf, MetastoreConf.ConfVars.HIVE_TXN_MANAGER) );	if(makeAcid) {	if(!conformToAcid(newTable)) {	return;	}	if(!TableType.MANAGED_TABLE.toString().equalsIgnoreCase(newTable.getTableType())) {	return;	}	if(newTable.getSd().getSortColsSize() > 0) {	
could not make acid it s sorted 

}	if(newTable.getSd().getSortColsSize() > 0) {	return;	}	Map<String, String> parameters = newTable.getParameters();	if (parameters == null || parameters.isEmpty()) {	parameters = new HashMap<>();	}	parameters.put(hive_metastoreConstants.TABLE_IS_TRANSACTIONAL, "true");	newTable.setParameters(parameters);	
automatically chose to make acid 

========================= hive sample_2118 =========================

private static void addChildren(FileSystem fsAsUser, Path path, List<FileStatus> children) throws IOException {	if (children != null) {	FileStatus[] listStatus;	try {	listStatus = fsAsUser.listStatus(path);	} catch (IOException e) {	
unable to list files under 

if (isDir) {	dirActionNeeded.and(FsAction.EXECUTE);	}	List<FileStatus> subDirsToCheck = null;	if (isDir && recurse) {	subDirsToCheck = new ArrayList<FileStatus>();	}	try {	checkFileAccessWithImpersonation(fs, fileStatus, action, userName, subDirsToCheck);	} catch (AccessControlException err) {	
action denied on for user 

public static boolean isLocalFile(HiveConf conf, String fileName) {	try {	return isLocalFile(conf, new URI(fileName));	} catch (URISyntaxException e) {	
unable to create uri from 

public static boolean isLocalFile(HiveConf conf, URI fileUri) {	try {	FileSystem fsForFile = FileSystem.get(fileUri, conf);	return LocalFileSystem.class.isInstance(fsForFile);	} catch (IOException e) {	
unable to get filesystem for 

public static boolean mkdir(FileSystem fs, Path f, Configuration conf) throws IOException {	
creating directory if it doesn t exist 

static boolean copy(FileSystem srcFS, Path src, FileSystem dstFS, Path dst, boolean deleteSource, boolean overwrite, HiveConf conf, HadoopShims shims) throws IOException {	boolean copied = false;	boolean triedDistcp = false;	if (srcFS.getUri().getScheme().equals("hdfs")) {	ContentSummary srcContentSummary = srcFS.getContentSummary(src);	if (srcContentSummary.getFileCount() > conf.getLongVar(HiveConf.ConfVars.HIVE_EXEC_COPYFILE_MAXNUMFILES) && srcContentSummary.getLength() > conf.getLongVar(HiveConf.ConfVars.HIVE_EXEC_COPYFILE_MAXSIZE)) {	
source is bytes max 

static boolean copy(FileSystem srcFS, Path src, FileSystem dstFS, Path dst, boolean deleteSource, boolean overwrite, HiveConf conf, HadoopShims shims) throws IOException {	boolean copied = false;	boolean triedDistcp = false;	if (srcFS.getUri().getScheme().equals("hdfs")) {	ContentSummary srcContentSummary = srcFS.getContentSummary(src);	if (srcContentSummary.getFileCount() > conf.getLongVar(HiveConf.ConfVars.HIVE_EXEC_COPYFILE_MAXNUMFILES) && srcContentSummary.getLength() > conf.getLongVar(HiveConf.ConfVars.HIVE_EXEC_COPYFILE_MAXSIZE)) {	
source is files max 

static boolean copy(FileSystem srcFS, Path src, FileSystem dstFS, Path dst, boolean deleteSource, boolean overwrite, HiveConf conf, HadoopShims shims) throws IOException {	boolean copied = false;	boolean triedDistcp = false;	if (srcFS.getUri().getScheme().equals("hdfs")) {	ContentSummary srcContentSummary = srcFS.getContentSummary(src);	if (srcContentSummary.getFileCount() > conf.getLongVar(HiveConf.ConfVars.HIVE_EXEC_COPYFILE_MAXNUMFILES) && srcContentSummary.getLength() > conf.getLongVar(HiveConf.ConfVars.HIVE_EXEC_COPYFILE_MAXSIZE)) {	
launch distributed copy distcp job 

public static boolean moveToTrash(FileSystem fs, Path f, Configuration conf, boolean purge) throws IOException {	
deleting 

public static boolean moveToTrash(FileSystem fs, Path f, Configuration conf, boolean purge) throws IOException {	boolean result = false;	try {	if(purge) {	
purge is set to true not moving to trash 

public static boolean moveToTrash(FileSystem fs, Path f, Configuration conf, boolean purge) throws IOException {	boolean result = false;	try {	if(purge) {	} else {	result = Trash.moveToAppropriateTrash(fs, f, conf);	if (result) {	
moved to trash 

boolean result = false;	try {	if(purge) {	} else {	result = Trash.moveToAppropriateTrash(fs, f, conf);	if (result) {	return true;	}	}	} catch (IOException ioe) {	
force to delete it 

} else {	result = Trash.moveToAppropriateTrash(fs, f, conf);	if (result) {	return true;	}	}	} catch (IOException ioe) {	}	result = fs.delete(f, true);	if (!result) {	
failed to delete 

public static boolean rename(FileSystem fs, Path sourcePath, Path destPath, Configuration conf) throws IOException {	
renaming to 

return createFileInTmp(prefix, suffix, "Cannot access or create " + targetDir, isDirectory);	}	try {	File file = File.createTempFile(prefix, suffix, targetDir);	if (isDirectory && (!file.delete() || !file.mkdirs())) {	return createFileInTmp(prefix, suffix, "Cannot recreate " + file + " as directory", isDirectory);	}	file.deleteOnExit();	return file;	} catch (IOException ex) {	
error creating a file in 

private static File createFileInTmp(String prefix, String suffix, String reason, boolean isDirectory) throws IOException {	File file = File.createTempFile(prefix, suffix);	if (isDirectory && (!file.delete() || !file.mkdirs())) {	throw new IOException("Cannot recreate " + file + " as directory");	}	file.deleteOnExit();	
created a tmp file 

Set<String> result = new HashSet<String>();	if (pathString == null || org.apache.commons.lang.StringUtils.isBlank(pathString)) {	return result;	}	String[] paths = pathString.split(",");	for(String path : paths) {	try {	Path p = new Path(getURI(path));	FileSystem fs = p.getFileSystem(conf);	if (!fs.exists(p)) {	
the jar file path doesn t exist 

}	if (fs.isDirectory(p)) {	FileStatus[] files = fs.listStatus(p, new GlobFilter("*.jar"));	for(FileStatus file : files) {	result.add(file.getPath().toUri().toString());	}	} else {	result.add(p.toUri().toString());	}	} catch(URISyntaxException | IOException e) {	
invalid file path 

========================= hive sample_1364 =========================

public org.apache.hadoop.mapred.RecordReader<NullWritable, ArrayWritable> getRecordReader( final org.apache.hadoop.mapred.InputSplit split, final org.apache.hadoop.mapred.JobConf job, final org.apache.hadoop.mapred.Reporter reporter ) throws IOException {	try {	if (Utilities.getUseVectorizedInputFileFormat(job)) {	if (LOG.isDebugEnabled()) {	
using vectorized record reader 

public org.apache.hadoop.mapred.RecordReader<NullWritable, ArrayWritable> getRecordReader( final org.apache.hadoop.mapred.InputSplit split, final org.apache.hadoop.mapred.JobConf job, final org.apache.hadoop.mapred.Reporter reporter ) throws IOException {	try {	if (Utilities.getUseVectorizedInputFileFormat(job)) {	if (LOG.isDebugEnabled()) {	}	return (RecordReader) vectorizedSelf.getRecordReader(split, job, reporter);	}	else {	if (LOG.isDebugEnabled()) {	
using row mode record reader 

========================= hive sample_3709 =========================

static List<LlapTokenIdentifier> getLlapTokens( UserGroupInformation ugi, String clusterId) {	List<LlapTokenIdentifier> tokens = null;	for (TokenIdentifier id : ugi.getTokenIdentifiers()) {	if (!LlapTokenIdentifier.KIND_NAME.equals(id.getKind())) continue;	if (LOG.isDebugEnabled()) {	
token 

========================= hive sample_2231 =========================

edgeType = EdgeType.CUSTOM_SIMPLE_EDGE;	}	}	if (edgeType == EdgeType.CUSTOM_EDGE) {	parentRS.getConf().setReducerTraits(EnumSet.of(FIXED));	}	TezEdgeProperty edgeProp = new TezEdgeProperty(null, edgeType, numBuckets);	if (mapJoinWork != null) {	for (BaseWork myWork: mapJoinWork) {	TezWork tezWork = context.currentTask.getWork();	
connecting with 

TezEdgeProperty edgeProp = new TezEdgeProperty(null, edgeType, numBuckets);	if (mapJoinWork != null) {	for (BaseWork myWork: mapJoinWork) {	TezWork tezWork = context.currentTask.getWork();	tezWork.connect(parentWork, myWork, edgeProp);	if (edgeType == EdgeType.CUSTOM_EDGE) {	tezWork.setVertexType(myWork, VertexType.INITIALIZED_EDGES);	}	ReduceSinkOperator r = null;	if (context.connectedReduceSinks.contains(parentRS)) {	
cloning reduce sink for multi child broadcast edge 

mapJoinOp.replaceParent(parentRS, dummyOp);	List<Operator<? extends OperatorDesc>> dummyChildren = new ArrayList<Operator<? extends OperatorDesc>>();	dummyChildren.add(mapJoinOp);	dummyOp.setChildOperators(dummyChildren);	dummyOperators.add(dummyOp);	List<Operator<? extends OperatorDesc>> childOperators = parentRS.getChildOperators();	int childIndex = childOperators.indexOf(mapJoinOp);	childOperators.remove(childIndex);	if (mapJoinWork != null) {	for (BaseWork myWork: mapJoinWork) {	
adding dummy op to work from mj work 

========================= hive sample_3070 =========================

public CompleteBean completeJob(@PathParam("jobid") String jobid, @QueryParam("status") String jobStatus) @QueryParam("status") String jobStatus) throws CallbackFailedException, IOException {	
received callback 

private static String getRequestingHost(String requestingUser, HttpServletRequest request) {	final String unkHost = "???";	if(request == null) {	
request is null cannot determine hostname 

private static String getRequestingHost(String requestingUser, HttpServletRequest request) {	final String unkHost = "???";	if(request == null) {	return unkHost;	}	try {	String address = request.getRemoteAddr();	if(address == null) {	
request remote address is null for user 

if(request == null) {	return unkHost;	}	try {	String address = request.getRemoteAddr();	if(address == null) {	return unkHost;	}	String hostName = InetAddress.getByName(address).getCanonicalHostName();	if(LOG.isDebugEnabled()) {	
resolved remote hostname 

try {	String address = request.getRemoteAddr();	if(address == null) {	return unkHost;	}	String hostName = InetAddress.getByName(address).getCanonicalHostName();	if(LOG.isDebugEnabled()) {	}	return hostName;	} catch (UnknownHostException ex) {	
request remote address could not be resolved 

========================= hive sample_829 =========================

public ReduceWork createReduceWork(GenSparkProcContext context, Operator<?> root, SparkWork sparkWork) throws SemanticException {	Preconditions.checkArgument(!root.getParentOperators().isEmpty(), "AssertionError: expected root.getParentOperators() to be non-empty");	ReduceWork reduceWork = new ReduceWork("Reducer " + (++sequenceNumber));	
adding reduce work for 

protected void setupReduceSink(GenSparkProcContext context, ReduceWork reduceWork, ReduceSinkOperator reduceSink) {	
setting up reduce sink with following reduce work 

public MapWork createMapWork(GenSparkProcContext context, Operator<?> root, SparkWork sparkWork, PrunedPartitionList partitions, boolean deferSetup) throws SemanticException {	Preconditions.checkArgument(root.getParentOperators().isEmpty(), "AssertionError: expected root.getParentOperators() to be empty");	MapWork mapWork = new MapWork("Map " + (++sequenceNumber));	
adding map work for 

boolean  chDir = GenMapRedUtils.isMergeRequired(context.moveTask, hconf, fileSink, context.currentTask, isInsertTable);	List<FileSinkOperator> fileSinkList = context.fileSinkMap.get(fileSink);	if (fileSinkList != null) {	for (FileSinkOperator fsOp : fileSinkList) {	fsOp.getConf().setGatherStats(fileSink.getConf().isGatherStats());	fsOp.getConf().setStatsReliable(fileSink.getConf().isStatsReliable());	}	}	Path finalName = createMoveTask(context.currentTask, chDir, fileSink, parseContext, context.moveTask, hconf, context.dependencyTask);	if (chDir) {	
using combinehiveinputformat for the merge job 

public void processPartitionPruningSink(GenSparkProcContext context, SparkPartitionPruningSinkOperator pruningSink) {	SparkPartitionPruningSinkDesc desc = pruningSink.getConf();	TableScanOperator ts = desc.getTableScan();	MapWork targetWork = (MapWork) context.rootToWorkMap.get(ts);	Preconditions.checkArgument( targetWork != null, "No targetWork found for tablescan " + ts);	String sourceId = pruningSink.getUniqueId();	Path tmpPath = targetWork.getTmpPathForPartitionPruning();	if (tmpPath == null) {	tmpPath = getDPPOutputPath(context.parseContext.getContext());	targetWork.setTmpPathForPartitionPruning(tmpPath);	
setting tmp path between source work and target work 

public static SparkEdgeProperty getEdgeProperty(HiveConf conf, ReduceSinkOperator reduceSink, ReduceWork reduceWork) throws SemanticException {	boolean useSparkGroupBy = conf.getBoolVar(HiveConf.ConfVars.SPARK_USE_GROUPBY_SHUFFLE);	SparkEdgeProperty edgeProperty = new SparkEdgeProperty(SparkEdgeProperty.SHUFFLE_NONE);	edgeProperty.setNumPartitions(reduceWork.getNumReduceTasks());	String sortOrder = Strings.nullToEmpty(reduceSink.getConf().getOrder()).trim();	if (hasGBYOperator(reduceSink)) {	edgeProperty.setShuffleGroup();	if (!useSparkGroupBy || (!sortOrder.isEmpty() && groupByNeedParLevelOrder(reduceSink))) {	if (!useSparkGroupBy) {	
hive spark use groupby shuffle is off use repartition shuffle instead 

}	if (edgeProperty.isShuffleNone() && !sortOrder.isEmpty()) {	if ((reduceSink.getConf().getPartitionCols() == null || reduceSink.getConf().getPartitionCols().isEmpty() || isSame(reduceSink.getConf().getPartitionCols(), reduceSink.getConf().getKeyCols())) && reduceSink.getConf().hasOrderBy()) {	edgeProperty.setShuffleSort();	} else {	edgeProperty.setMRShuffle();	}	}	if (edgeProperty.isShuffleNone()) {	if (!useSparkGroupBy) {	
hive spark use groupby shuffle is off use repartition shuffle instead 

========================= hive sample_3565 =========================

public Object evaluate(DeferredObject[] arguments) throws HiveException {	url = ((StringObjectInspector) argumentOI[0]) .getPrimitiveJavaObject(arguments[0].get());	user = ((StringObjectInspector) argumentOI[1]) .getPrimitiveJavaObject(arguments[1].get());	pass = ((StringObjectInspector) argumentOI[2]) .getPrimitiveJavaObject(arguments[2].get());	try {	connection = DriverManager.getConnection(url, user, pass);	} catch (SQLException ex) {	
driver loading or connection issue 

try {	PreparedStatement ps = connection .prepareStatement(((StringObjectInspector) argumentOI[3]) .getPrimitiveJavaObject(arguments[3].get()));	for (int i = 4; i < arguments.length; ++i) {	PrimitiveObjectInspector poi = ((PrimitiveObjectInspector) argumentOI[i]);	ps.setObject(i - 3, poi.getPrimitiveJavaObject(arguments[i].get()));	}	ps.execute();	ps.close();	result.set(0);	} catch (SQLException e) {	
underlying sql exception 

}	ps.execute();	ps.close();	result.set(0);	} catch (SQLException e) {	result.set(1);	} finally {	try {	connection.close();	} catch (Exception ex) {	
underlying sql exception during close 

========================= hive sample_1108 =========================

public void initialize(Configuration conf, Properties tbl) throws SerDeException {	
initializing hcatrecordserde 

public void initialize(Configuration conf, Properties tbl) throws SerDeException {	
props to serde 

if (columnNameProperty.length() == 0) {	columnNames = new ArrayList<String>();	} else {	columnNames = Arrays.asList(columnNameProperty.split(columnNameDelimiter));	}	if (columnTypeProperty.length() == 0) {	columnTypes = new ArrayList<TypeInfo>();	} else {	columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);	}	
columns 

if (columnNameProperty.length() == 0) {	columnNames = new ArrayList<String>();	} else {	columnNames = Arrays.asList(columnNameProperty.split(columnNameDelimiter));	}	if (columnTypeProperty.length() == 0) {	columnTypes = new ArrayList<TypeInfo>();	} else {	columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);	}	
types 

public void initialize(HCatSchema hsch) throws SerDeException {	
initializing hcatrecordserde through hcatschema 

========================= hive sample_741 =========================

public JsonFileMetricsReporter(MetricRegistry registry, HiveConf conf) {	this.metricRegistry = registry;	this.jsonWriter = new ObjectMapper().registerModule(new MetricsModule(TimeUnit.MILLISECONDS, TimeUnit.MILLISECONDS, false)).writerWithDefaultPrettyPrinter();	interval = conf.getTimeVar(HiveConf.ConfVars.HIVE_METRICS_JSON_FILE_INTERVAL, TimeUnit.MILLISECONDS);	String pathString = conf.getVar(HiveConf.ConfVars.HIVE_METRICS_JSON_FILE_LOCATION);	path = Paths.get(pathString).toAbsolutePath();	
reporting metrics to 

public void start() {	if (!metricsDir.toFile().exists()) {	
metrics directory does not exist creating one 

public void start() {	if (!metricsDir.toFile().exists()) {	try {	Files.createDirectories(metricsDir, DIR_ATTRS);	} catch (IOException e) {	
failed to create directory 

public void run() {	Path tmpFile = null;	try {	String json = null;	try {	json = jsonWriter.writeValueAsString(metricRegistry);	} catch (JsonProcessingException e) {	
unable to convert json to string 

try {	String json = null;	try {	json = jsonWriter.writeValueAsString(metricRegistry);	} catch (JsonProcessingException e) {	return;	}	try {	tmpFile = Files.createTempFile(metricsDir, "hmetrics", "json", FILE_ATTRS);	} catch (IOException e) {	
failed to create temp file for json metrics 

try {	json = jsonWriter.writeValueAsString(metricRegistry);	} catch (JsonProcessingException e) {	return;	}	try {	tmpFile = Files.createTempFile(metricsDir, "hmetrics", "json", FILE_ATTRS);	} catch (IOException e) {	return;	} catch (SecurityException e) {	
failed to create temp file for json metrics no permissions 

} catch (JsonProcessingException e) {	return;	}	try {	tmpFile = Files.createTempFile(metricsDir, "hmetrics", "json", FILE_ATTRS);	} catch (IOException e) {	return;	} catch (SecurityException e) {	return;	} catch (UnsupportedOperationException e) {	
failed to create temp file for json metrics operartion not supported 

} catch (IOException e) {	return;	} catch (SecurityException e) {	return;	} catch (UnsupportedOperationException e) {	return;	}	try (BufferedWriter bw = new BufferedWriter(new FileWriter(tmpFile.toFile()))) {	bw.write(json);	} catch (IOException e) {	
unable to write to temp file 

return;	}	try (BufferedWriter bw = new BufferedWriter(new FileWriter(tmpFile.toFile()))) {	bw.write(json);	} catch (IOException e) {	return;	}	try {	Files.move(tmpFile, path, StandardCopyOption.ATOMIC_MOVE);	} catch (Exception e) {	
unable to rename temp file to 

return;	}	try (BufferedWriter bw = new BufferedWriter(new FileWriter(tmpFile.toFile()))) {	bw.write(json);	} catch (IOException e) {	return;	}	try {	Files.move(tmpFile, path, StandardCopyOption.ATOMIC_MOVE);	} catch (Exception e) {	
exception during rename 

try (BufferedWriter bw = new BufferedWriter(new FileWriter(tmpFile.toFile()))) {	bw.write(json);	} catch (IOException e) {	return;	}	try {	Files.move(tmpFile, path, StandardCopyOption.ATOMIC_MOVE);	} catch (Exception e) {	}	} catch (Throwable t) {	
error executing scheduled task 

try {	Files.move(tmpFile, path, StandardCopyOption.ATOMIC_MOVE);	} catch (Exception e) {	}	} catch (Throwable t) {	} finally {	if (tmpFile != null && tmpFile.toFile().exists()) {	try {	Files.delete(tmpFile);	} catch (Exception e) {	
failed to delete temporary metrics file 

========================= hive sample_1333 =========================

public List<Task<? extends Serializable>> handle(Context context) throws SemanticException {	DropFunctionMessage msg = deserializer.getDropFunctionMessage(context.dmd.getPayload());	String actualDbName = context.isDbNameEmpty() ? msg.getDB() : context.dbName;	String qualifiedFunctionName = FunctionUtils.qualifyFunctionName(msg.getFunctionName(), actualDbName);	DropFunctionDesc desc = new DropFunctionDesc( qualifiedFunctionName, false, context.eventOnlyReplicationSpec());	Task<FunctionWork> dropFunctionTask = TaskFactory.get(new FunctionWork(desc), context.hiveConf);	
added drop function task 

========================= hive sample_3477 =========================

private static long getMaxHeapSize(MemoryMXBean bean) {	long maxHeapSize = bean.getHeapMemoryUsage().getMax();	if(maxHeapSize == -1) {	
memorymxbean getheapmemoryusage getmax returned defaulting maxheapsize to 

========================= hive sample_3861 =========================

if (appReport == null) {	return null;	}	String diagnostics = appReport.getDiagnostics();	if (diagnostics == null || diagnostics.isEmpty()) {	return null;	}	try {	return ApplicationDiagnostics.fromJson(diagnostics);	} catch (IOException e) {	
failed to parse application diagnostics from yarn diagnostics 

public static void startCluster(Configuration conf, String name, String packageName, Path packageDir, String queue) {	
starting cluster with 

public static void startCluster(Configuration conf, String name, String packageName, Path packageDir, String queue) {	SliderClient sc;	try {	sc = createSliderClient(conf);	} catch (Exception e) {	throw new RuntimeException(e);	}	try {	
executing the freeze command 

} catch (Exception e) {	throw new RuntimeException(e);	}	try {	ActionFreezeArgs freezeArgs = new ActionFreezeArgs();	freezeArgs.force = true;	freezeArgs.setWaittime(3600);	try {	sc.actionFreeze(name, freezeArgs);	} catch (UnknownApplicationInstanceException ex) {	
there was no old application instance to freeze 

throw new RuntimeException(e);	}	try {	ActionFreezeArgs freezeArgs = new ActionFreezeArgs();	freezeArgs.force = true;	freezeArgs.setWaittime(3600);	try {	sc.actionFreeze(name, freezeArgs);	} catch (UnknownApplicationInstanceException ex) {	}	
executing the destroy command 

freezeArgs.setWaittime(3600);	try {	sc.actionFreeze(name, freezeArgs);	} catch (UnknownApplicationInstanceException ex) {	}	ActionDestroyArgs destroyArg = new ActionDestroyArgs();	destroyArg.force = true;	try {	sc.actionDestroy(name, destroyArg);	} catch (UnknownApplicationInstanceException ex) {	
there was no old application instance to destroy 

try {	sc.actionFreeze(name, freezeArgs);	} catch (UnknownApplicationInstanceException ex) {	}	ActionDestroyArgs destroyArg = new ActionDestroyArgs();	destroyArg.force = true;	try {	sc.actionDestroy(name, destroyArg);	} catch (UnknownApplicationInstanceException ex) {	}	
executing the install command 

destroyArg.force = true;	try {	sc.actionDestroy(name, destroyArg);	} catch (UnknownApplicationInstanceException ex) {	}	ActionInstallPackageArgs installArgs = new ActionInstallPackageArgs();	installArgs.name = "LLAP";	installArgs.packageURI = new Path(packageDir, packageName).toString();	installArgs.replacePkg = true;	sc.actionInstallPkg(installArgs);	
executing the create command 

createArgs.setWaittime(3600);	if (queue != null) {	createArgs.queue = queue;	}	File bogusSliderFile = startSetSliderLibDir();	try {	sc.actionCreate(name, createArgs);	} finally {	endSetSliderLibDir(bogusSliderFile);	}	
started the cluster via slider api 

sc.actionCreate(name, createArgs);	} finally {	endSetSliderLibDir(bogusSliderFile);	}	} catch (YarnException | IOException e) {	throw new RuntimeException(e);	} finally {	try {	sc.close();	} catch (IOException e) {	
failed to close slider client 

public static File startSetSliderLibDir() throws IOException {	File sliderJarDir = SliderUtils.findContainingJar(SliderClient.class).getParentFile();	File gz = new File(sliderJarDir, SLIDER_GZ);	if (gz.exists()) {	String path = sliderJarDir.getAbsolutePath();	
setting slider libdir based on jar file location 

File sliderJarDir = SliderUtils.findContainingJar(SliderClient.class).getParentFile();	File gz = new File(sliderJarDir, SLIDER_GZ);	if (gz.exists()) {	String path = sliderJarDir.getAbsolutePath();	System.setProperty("slider.libdir", path);	return null;	}	String path = System.getProperty("slider.libdir");	gz = null;	if (path != null && !path.isEmpty()) {	
slider libdir was already set 

if (!gz.exists()) {	gz = null;	}	}	if (gz == null) {	path = System.getenv("SLIDER_HOME");	if (path != null && !path.isEmpty()) {	gz = new File(new File(path, "lib"), SLIDER_GZ);	if (gz.exists()) {	path = gz.getParentFile().getAbsolutePath();	
setting slider libdir based on slider home 

}	}	}	if (gz == null) {	File rootDir = SliderUtils.findContainingJar(HiveConf.class) .getParentFile().getParentFile().getParentFile();	File sliderJarDir2 = new File(new File(rootDir, "slider"), "lib");	if (sliderJarDir2.exists()) {	gz = new File(sliderJarDir2, SLIDER_GZ);	if (gz.exists()) {	path = sliderJarDir2.getAbsolutePath();	
setting slider libdir based on guesswork 

System.setProperty("slider.libdir", path);	} else {	gz = null;	}	}	}	if (gz == null) {	throw new IOException("Cannot find " + SLIDER_GZ + ". Please ensure SLIDER_HOME is set.");	}	File newGz = new File(sliderJarDir, SLIDER_GZ);	
copying to 

public static void endSetSliderLibDir(File newGz) throws IOException {	if (newGz == null || !newGz.exists()) return;	
deleting 

========================= hive sample_2211 =========================

boolean exception = false;	try {	OrcFileValueWrapper v;	OrcFileKeyWrapper k;	if (key instanceof CombineHiveKey) {	k = (OrcFileKeyWrapper) ((CombineHiveKey) key).getKey();	} else {	k = (OrcFileKeyWrapper) key;	}	if (k.isIncompatFile()) {	
incompatible orc file merge stripe statistics is missing 

if (key instanceof CombineHiveKey) {	k = (OrcFileKeyWrapper) ((CombineHiveKey) key).getKey();	} else {	k = (OrcFileKeyWrapper) key;	}	if (k.isIncompatFile()) {	addIncompatibleFile(k.getInputPath());	return;	}	filePath = k.getInputPath().toUri().getPath();	
orcfilemergeoperator processing 

addIncompatibleFile(k.getInputPath());	return;	}	filePath = k.getInputPath().toUri().getPath();	fixTmpPath(k.getInputPath().getParent());	v = (OrcFileValueWrapper) value;	if (prevPath == null) {	prevPath = k.getInputPath();	reader = OrcFile.createReader(fs, k.getInputPath());	if (LOG.isInfoEnabled()) {	
orc merge file input path 

writerVersion = k.getWriterVersion();	fileSchema = k.getFileSchema();	rowIndexStride = k.getRowIndexStride();	OrcFile.WriterOptions options = OrcFile.writerOptions(jc) .compress(compression) .version(fileVersion) .rowIndexStride(rowIndexStride) .inspector(reader.getObjectInspector());	if (compression != CompressionKind.NONE) {	options.bufferSize(compressBuffSize).enforceBufferSize();	}	Path outPath = getOutPath();	outWriter = OrcFile.createWriter(outPath, options);	if (LOG.isDebugEnabled()) {	
orc merge file output path 

return;	}	if (!k.getInputPath().equals(prevPath)) {	reader = OrcFile.createReader(fs, k.getInputPath());	}	byte[] buffer = new byte[(int) v.getStripeInformation().getLength()];	fdis = fs.open(k.getInputPath());	fdis.readFully(v.getStripeInformation().getOffset(), buffer, 0, (int) v.getStripeInformation().getLength());	outWriter.appendStripe(buffer, 0, buffer.length, v.getStripeInformation(), v.getStripeStatistics());	if (LOG.isInfoEnabled()) {	
merged stripe from file offset length row 

fdis = fs.open(k.getInputPath());	fdis.readFully(v.getStripeInformation().getOffset(), buffer, 0, (int) v.getStripeInformation().getLength());	outWriter.appendStripe(buffer, 0, buffer.length, v.getStripeInformation(), v.getStripeStatistics());	if (LOG.isInfoEnabled()) {	}	if (v.isLastStripeInFile()) {	outWriter.appendUserMetadata(v.getUserMetadata());	}	} catch (Throwable e) {	exception = true;	
closing operator exception 

private boolean checkCompatibility(OrcFileKeyWrapper k) {	if (!fileSchema.equals(k.getFileSchema())) {	
incompatible orc file merge schema mismatch for 

private boolean checkCompatibility(OrcFileKeyWrapper k) {	if (!fileSchema.equals(k.getFileSchema())) {	return false;	}	if (!k.getCompression().equals(compression)) {	
incompatible orc file merge compression codec mismatch for 

private boolean checkCompatibility(OrcFileKeyWrapper k) {	if (!fileSchema.equals(k.getFileSchema())) {	return false;	}	if (!k.getCompression().equals(compression)) {	return false;	}	if (k.getCompressBufferSize() != compressBuffSize) {	
incompatible orc file merge compression buffer size mismatch for 

if (!fileSchema.equals(k.getFileSchema())) {	return false;	}	if (!k.getCompression().equals(compression)) {	return false;	}	if (k.getCompressBufferSize() != compressBuffSize) {	return false;	}	if (!k.getFileVersion().equals(fileVersion)) {	
incompatible orc file merge file version mismatch for 

if (!k.getCompression().equals(compression)) {	return false;	}	if (k.getCompressBufferSize() != compressBuffSize) {	return false;	}	if (!k.getFileVersion().equals(fileVersion)) {	return false;	}	if (!k.getWriterVersion().equals(writerVersion)) {	
incompatible orc file merge writer version mismatch for 

if (k.getCompressBufferSize() != compressBuffSize) {	return false;	}	if (!k.getFileVersion().equals(fileVersion)) {	return false;	}	if (!k.getWriterVersion().equals(writerVersion)) {	return false;	}	if (k.getRowIndexStride() != rowIndexStride) {	
incompatible orc file merge row index stride mismatch for 

========================= hive sample_3953 =========================

public void analyzeInternal(ASTNode ast) throws SemanticException {	
replicationsemanticaanalyzer analyzeinternal 

public void analyzeInternal(ASTNode ast) throws SemanticException {	LOG.debug(ast.getName() + ":" + ast.getToken().getText() + "=" + ast.getText());	switch (ast.getToken().getType()) {	case TOK_REPL_DUMP: {	
replicationsemanticanalyzer analyzeinternal dump 

public void analyzeInternal(ASTNode ast) throws SemanticException {	LOG.debug(ast.getName() + ":" + ast.getToken().getText() + "=" + ast.getText());	switch (ast.getToken().getType()) {	case TOK_REPL_DUMP: {	initReplDump(ast);	analyzeReplDump(ast);	break;	}	case TOK_REPL_LOAD: {	
replicationsemanticanalyzer analyzeinternal load 

initReplDump(ast);	analyzeReplDump(ast);	break;	}	case TOK_REPL_LOAD: {	initReplLoad(ast);	analyzeReplLoad(ast);	break;	}	case TOK_REPL_STATUS: {	
replicationsemanticanalyzer analyzeinternal status 

private void analyzeReplDump(ASTNode ast) throws SemanticException {	
replicationsemanticanalyzer analyzerepldump from to maxeventlimit 

for (String tblName : Utils.matchesTbl(db, dbName, tblNameOrPattern)) {	inputs.add(new ReadEntity(db.getTable(dbName, tblName)));	}	} else {	inputs.add(new ReadEntity(db.getDatabase(dbName)));	}	}	}	setFetchTask(createFetchTask(dumpSchema));	} catch (Exception e) {	
error during analyzerepldump 

private void analyzeReplLoad(ASTNode ast) throws SemanticException {	
replsemanticanalyzer analyzereplload from 

private void analyzeReplLoad(ASTNode ast) throws SemanticException {	try {	Path loadPath = new Path(path);	final FileSystem fs = loadPath.getFileSystem(conf);	if (!fs.exists(loadPath)) {	throw new FileNotFoundException(loadPath.toUri().toString());	}	DumpMetaData dmd = new DumpMetaData(loadPath, conf);	boolean evDump = false;	if (dmd.isIncrementalDump()){	
contains an incremental dump 

Path loadPath = new Path(path);	final FileSystem fs = loadPath.getFileSystem(conf);	if (!fs.exists(loadPath)) {	throw new FileNotFoundException(loadPath.toUri().toString());	}	DumpMetaData dmd = new DumpMetaData(loadPath, conf);	boolean evDump = false;	if (dmd.isIncrementalDump()){	evDump = true;	} else {	
contains an bootstrap dump 

evDump = true;	} else {	}	if ((!evDump) && (tblNameOrPattern != null) && !(tblNameOrPattern.isEmpty())) {	ReplLoadWork replLoadWork = new ReplLoadWork(conf, loadPath.toString(), dbNameOrPattern, tblNameOrPattern, queryState.getLineageState(), SessionState.get().getTxnMgr().getCurrentTxnId());	rootTasks.add(TaskFactory.get(replLoadWork, conf, true));	return;	}	FileStatus[] srcs = LoadSemanticAnalyzer.matchFilesOrDir(fs, loadPath);	if (srcs == null || (srcs.length == 0)) {	
nothing to load at 

FileStatus[] srcs = LoadSemanticAnalyzer.matchFilesOrDir(fs, loadPath);	if (srcs == null || (srcs.length == 0)) {	return;	}	FileStatus[] dirsInLoadPath = fs.listStatus(loadPath, EximUtil.getDirectoryFilter(fs));	if ((dirsInLoadPath == null) || (dirsInLoadPath.length == 0)) {	throw new IllegalArgumentException("No data to load in path " + loadPath.toUri().toString());	}	if (!evDump){	if ((dbNameOrPattern != null) && (dirsInLoadPath.length > 1)) {	
found multiple dirs when we expected 

throw new IllegalArgumentException( "Multiple dirs in " + loadPath.toUri().toString() + " does not correspond to REPL LOAD expecting to load to a singular destination point.");	}	ReplLoadWork replLoadWork = new ReplLoadWork(conf, loadPath.toString(), dbNameOrPattern, queryState.getLineageState(), SessionState.get().getTxnMgr().getCurrentTxnId());	rootTasks.add(TaskFactory.get(replLoadWork, conf, true));	} else {	Arrays.sort(dirsInLoadPath, new EventDumpDirComparator());	Task<? extends Serializable> evTaskRoot = TaskFactory.get(new DependencyCollectionWork(), conf);	Task<? extends Serializable> taskChainTail = evTaskRoot;	ReplLogger replLogger = new IncrementalLoadLogger(dbNameOrPattern, loadPath.toString(), dirsInLoadPath.length);	for (FileStatus dir : dirsInLoadPath){	
loading event from to 

for (FileStatus dir : dirsInLoadPath){	String locn = dir.getPath().toUri().toString();	DumpMetaData eventDmd = new DumpMetaData(new Path(locn), conf);	MessageHandler.Context context = new MessageHandler.Context(dbNameOrPattern, tblNameOrPattern, locn, taskChainTail, eventDmd, conf, db, ctx, LOG);	List<Task<? extends Serializable>> evTasks = analyzeEventLoad(context);	if ((evTasks != null) && (!evTasks.isEmpty())){	ReplStateLogWork replStateLogWork = new ReplStateLogWork(replLogger, dir.getPath().getName(), eventDmd.getDumpType().toString());	Task<? extends Serializable> barrierTask = TaskFactory.get(replStateLogWork, conf);	for (Task<? extends Serializable> t : evTasks){	t.addDependentTask(barrierTask);	
added as a precursor of barrier task 

String locn = dir.getPath().toUri().toString();	DumpMetaData eventDmd = new DumpMetaData(new Path(locn), conf);	MessageHandler.Context context = new MessageHandler.Context(dbNameOrPattern, tblNameOrPattern, locn, taskChainTail, eventDmd, conf, db, ctx, LOG);	List<Task<? extends Serializable>> evTasks = analyzeEventLoad(context);	if ((evTasks != null) && (!evTasks.isEmpty())){	ReplStateLogWork replStateLogWork = new ReplStateLogWork(replLogger, dir.getPath().getName(), eventDmd.getDumpType().toString());	Task<? extends Serializable> barrierTask = TaskFactory.get(replStateLogWork, conf);	for (Task<? extends Serializable> t : evTasks){	t.addDependentTask(barrierTask);	}	
updated taskchaintail from to 

}	taskChainTail = barrierTask;	}	}	if (!evTaskRoot.equals(taskChainTail)) {	Map<String, String> dbProps = new HashMap<>();	dbProps.put(ReplicationSpec.KEY.CURR_STATE_ID.toString(), String.valueOf(dmd.getEventTo()));	ReplStateLogWork replStateLogWork = new ReplStateLogWork(replLogger, dbProps);	Task<? extends Serializable> barrierTask = TaskFactory.get(replStateLogWork, conf);	taskChainTail.addDependentTask(barrierTask);	
added as a precursor of barrier task 

private List<Task<? extends Serializable>> analyzeEventLoad( MessageHandler.Context context) throws SemanticException {	MessageHandler messageHandler = context.dmd.getDumpType().handler();	List<Task<? extends Serializable>> tasks = messageHandler.handle(context);	if (context.precursor != null) {	for (Task<? extends Serializable> t : tasks) {	context.precursor.addDependentTask(t);	
added as a precursor of 

private Task<? extends Serializable> tableUpdateReplStateTask( String dbName, String tableName, Map<String, String> partSpec, String replState, Task<? extends Serializable> preCursor) {	HashMap<String, String> mapProp = new HashMap<>();	mapProp.put(ReplicationSpec.KEY.CURR_STATE_ID.toString(), replState);	AlterTableDesc alterTblDesc =  new AlterTableDesc( AlterTableDesc.AlterTableTypes.ADDPROPS, new ReplicationSpec(replState, replState));	alterTblDesc.setProps(mapProp);	alterTblDesc.setOldName(StatsUtils.getFullyQualifiedTableName(dbName, tableName));	alterTblDesc.setPartSpec((HashMap<String, String>)partSpec);	Task<? extends Serializable> updateReplIdTask = TaskFactory.get( new DDLWork(inputs, outputs, alterTblDesc), conf);	if (preCursor != null) {	preCursor.addDependentTask(updateReplIdTask);	
added as a precursor of 

private Task<? extends Serializable> dbUpdateReplStateTask( String dbName, String replState, Task<? extends Serializable> preCursor) {	HashMap<String, String> mapProp = new HashMap<>();	mapProp.put(ReplicationSpec.KEY.CURR_STATE_ID.toString(), replState);	AlterDatabaseDesc alterDbDesc = new AlterDatabaseDesc( dbName, mapProp, new ReplicationSpec(replState, replState));	Task<? extends Serializable> updateReplIdTask = TaskFactory.get( new DDLWork(inputs, outputs, alterDbDesc), conf);	if (preCursor != null) {	preCursor.addDependentTask(updateReplIdTask);	
added as a precursor of 

private List<Task<? extends Serializable>> addUpdateReplStateTasks( boolean isDatabaseLoad, UpdatedMetaDataTracker updatedMetadata, List<Task<? extends Serializable>> importTasks) {	String replState = updatedMetadata.getReplicationState();	String dbName = updatedMetadata.getDatabase();	String tableName = updatedMetadata.getTable();	if (importTasks.isEmpty() || (!isDatabaseLoad && (tableName == null))) {	
no objects need update of repl state either import tasks or table level load 

private List<Task<? extends Serializable>> addUpdateReplStateTasks( boolean isDatabaseLoad, UpdatedMetaDataTracker updatedMetadata, List<Task<? extends Serializable>> importTasks) {	String replState = updatedMetadata.getReplicationState();	String dbName = updatedMetadata.getDatabase();	String tableName = updatedMetadata.getTable();	if (importTasks.isEmpty() || (!isDatabaseLoad && (tableName == null))) {	return importTasks;	}	Task<? extends Serializable> barrierTask = TaskFactory.get(new DependencyCollectionWork(), conf);	for (Task<? extends Serializable> t : importTasks){	t.addDependentTask(barrierTask);	
added as a precursor of barrier task 

private void analyzeReplStatus(ASTNode ast) throws SemanticException {	
replicationsemanticanalyzer analyzereplstatus 

private void prepareReturnValues(List<String> values, String schema) throws SemanticException {	
preparereturnvalues 

========================= hive sample_3400 =========================

public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procContext, Object... nodeOutputs) throws SemanticException {	OptimizeTezProcContext context = (OptimizeTezProcContext) procContext;	ReduceSinkOperator sink = (ReduceSinkOperator) nd;	ReduceSinkDesc desc = sink.getConf();	long bytesPerReducer = context.conf.getLongVar(HiveConf.ConfVars.BYTESPERREDUCER);	int maxReducers = context.conf.getIntVar(HiveConf.ConfVars.MAXREDUCERS);	int constantReducers = context.conf.getIntVar(HiveConf.ConfVars.HADOOPNUMREDUCERS);	if (context.visitedReduceSinks.contains(sink)) {	
already processed reduce sink 

ReduceSinkDesc desc = sink.getConf();	long bytesPerReducer = context.conf.getLongVar(HiveConf.ConfVars.BYTESPERREDUCER);	int maxReducers = context.conf.getIntVar(HiveConf.ConfVars.MAXREDUCERS);	int constantReducers = context.conf.getIntVar(HiveConf.ConfVars.HADOOPNUMREDUCERS);	if (context.visitedReduceSinks.contains(sink)) {	return true;	}	context.visitedReduceSinks.add(sink);	if (desc.getNumReducers() <= 0) {	if (constantReducers > 0) {	
parallelism for reduce sink set by user to 

context.visitedReduceSinks.add(sink);	if (desc.getNumReducers() <= 0) {	if (constantReducers > 0) {	desc.setNumReducers(constantReducers);	} else {	long numberOfBytes = 0;	for (Operator<? extends OperatorDesc> sibling: sink.getChildOperators().get(0).getParentOperators()) {	if (sibling.getStatistics() != null) {	numberOfBytes = StatsUtils.safeAdd( numberOfBytes, sibling.getStatistics().getDataSize());	} else {	
no stats available from 

desc.setNumReducers(constantReducers);	} else {	long numberOfBytes = 0;	for (Operator<? extends OperatorDesc> sibling: sink.getChildOperators().get(0).getParentOperators()) {	if (sibling.getStatistics() != null) {	numberOfBytes = StatsUtils.safeAdd( numberOfBytes, sibling.getStatistics().getDataSize());	} else {	}	}	int numReducers = Utilities.estimateReducers(numberOfBytes, bytesPerReducer, maxReducers, false);	
set parallelism for reduce sink to 

desc.setNumReducers(numReducers);	final Collection<ExprNodeDescEqualityWrapper> keyCols = ExprNodeDescEqualityWrapper.transform(desc.getKeyCols());	final Collection<ExprNodeDescEqualityWrapper> partCols = ExprNodeDescEqualityWrapper.transform(desc.getPartitionCols());	if (keyCols != null && keyCols.equals(partCols)) {	desc.setReducerTraits(EnumSet.of(UNIFORM, AUTOPARALLEL));	} else {	desc.setReducerTraits(EnumSet.of(AUTOPARALLEL));	}	}	} else {	
number of reducers determined to be 

========================= hive sample_3050 =========================

if (newNode.equals(splitLocation)) {	++newLoc;	}	lastLocations[splitIx] = splitLocation;	}	if (locs == MIN_LOC_COUNT) {	continue;	}	String msgTail = " when going to " + locs + " locations";	String movedMsg = moved + " splits moved", newMsg = newLoc + " splits went to the new node";	
and 

double sum = 0;	double[] cvs = new double[MAX_MISS_COUNT + 1];	for (int missCount = 0; missCount <= MAX_MISS_COUNT; ++missCount) {	double cv = cvs[missCount] = testHashDistribution(locs, missCount, splits, errorCount);	sum += cv;	if (missCount > 0 && cv > sum / (missCount + 1)) {	++aboveAvgCount;	}	}	if (aboveAvgCount > 2) {	
cvs for locations aren t to our liking 

int index = HostAffinitySplitLocationProvider.determineLocation(partLocs, splits[splitIx].getPath().toString(), splits[splitIx].getStart(), null);	++hitCounts[index];	}	SummaryStatistics ss = new SummaryStatistics();	for (int hitCount : hitCounts) {	ss.addValue(hitCount);	}	double avg = ss.getSum()/ss.getN(), stdev = ss.getStandardDeviation(), cv = stdev/avg;	double allowedMin = avg - 2.5 * stdev, allowedMax = avg + 2.5 * stdev;	if (allowedMin > ss.getMin() || allowedMax < ss.getMax() || cv > 0.22) {	
the distribution for locations misses isn t to our liking avg stdev cv min max 

========================= hive sample_2568 =========================

public void testRW() throws Exception {	Configuration conf = new Configuration();	for (Entry<Properties, HCatRecord> e : getData().entrySet()) {	Properties tblProps = e.getKey();	HCatRecord r = e.getValue();	HCatRecordSerDe hrsd = new HCatRecordSerDe();	SerDeUtils.initializeSerDe(hrsd, conf, tblProps, null);	
orig 

public void testRW() throws Exception {	Configuration conf = new Configuration();	for (Entry<Properties, HCatRecord> e : getData().entrySet()) {	Properties tblProps = e.getKey();	HCatRecord r = e.getValue();	HCatRecordSerDe hrsd = new HCatRecordSerDe();	SerDeUtils.initializeSerDe(hrsd, conf, tblProps, null);	Writable s = hrsd.serialize(r, hrsd.getObjectInspector());	
one 

Configuration conf = new Configuration();	for (Entry<Properties, HCatRecord> e : getData().entrySet()) {	Properties tblProps = e.getKey();	HCatRecord r = e.getValue();	HCatRecordSerDe hrsd = new HCatRecordSerDe();	SerDeUtils.initializeSerDe(hrsd, conf, tblProps, null);	Writable s = hrsd.serialize(r, hrsd.getObjectInspector());	HCatRecord r2 = (HCatRecord) hrsd.deserialize(s);	Assert.assertTrue(HCatDataCheckUtil.recordsEqual(r, r2));	Writable s2 = hrsd.serialize(s, hrsd.getObjectInspector());	
two 

SerDeUtils.initializeSerDe(hrsd, conf, tblProps, null);	Writable s = hrsd.serialize(r, hrsd.getObjectInspector());	HCatRecord r2 = (HCatRecord) hrsd.deserialize(s);	Assert.assertTrue(HCatDataCheckUtil.recordsEqual(r, r2));	Writable s2 = hrsd.serialize(s, hrsd.getObjectInspector());	Assert.assertTrue(HCatDataCheckUtil.recordsEqual(r, (HCatRecord) s));	Assert.assertTrue(HCatDataCheckUtil.recordsEqual(r, (HCatRecord) s2));	LazySimpleSerDe testSD = new LazySimpleSerDe();	SerDeUtils.initializeSerDe(testSD, conf, tblProps, null);	Writable s3 = testSD.serialize(s, hrsd.getObjectInspector());	
three 

Assert.assertTrue(HCatDataCheckUtil.recordsEqual(r, r2));	Writable s2 = hrsd.serialize(s, hrsd.getObjectInspector());	Assert.assertTrue(HCatDataCheckUtil.recordsEqual(r, (HCatRecord) s));	Assert.assertTrue(HCatDataCheckUtil.recordsEqual(r, (HCatRecord) s2));	LazySimpleSerDe testSD = new LazySimpleSerDe();	SerDeUtils.initializeSerDe(testSD, conf, tblProps, null);	Writable s3 = testSD.serialize(s, hrsd.getObjectInspector());	Object o3 = testSD.deserialize(s3);	Assert.assertFalse(r.getClass().equals(o3.getClass()));	HCatRecord s4 = (HCatRecord) hrsd.serialize(o3, testSD.getObjectInspector());	
four 

Writable s2 = hrsd.serialize(s, hrsd.getObjectInspector());	Assert.assertTrue(HCatDataCheckUtil.recordsEqual(r, (HCatRecord) s));	Assert.assertTrue(HCatDataCheckUtil.recordsEqual(r, (HCatRecord) s2));	LazySimpleSerDe testSD = new LazySimpleSerDe();	SerDeUtils.initializeSerDe(testSD, conf, tblProps, null);	Writable s3 = testSD.serialize(s, hrsd.getObjectInspector());	Object o3 = testSD.deserialize(s3);	Assert.assertFalse(r.getClass().equals(o3.getClass()));	HCatRecord s4 = (HCatRecord) hrsd.serialize(o3, testSD.getObjectInspector());	LazyHCatRecord s5 = new LazyHCatRecord(o3, testSD.getObjectInspector());	
five 

Assert.assertTrue(HCatDataCheckUtil.recordsEqual(r, (HCatRecord) s));	Assert.assertTrue(HCatDataCheckUtil.recordsEqual(r, (HCatRecord) s2));	LazySimpleSerDe testSD = new LazySimpleSerDe();	SerDeUtils.initializeSerDe(testSD, conf, tblProps, null);	Writable s3 = testSD.serialize(s, hrsd.getObjectInspector());	Object o3 = testSD.deserialize(s3);	Assert.assertFalse(r.getClass().equals(o3.getClass()));	HCatRecord s4 = (HCatRecord) hrsd.serialize(o3, testSD.getObjectInspector());	LazyHCatRecord s5 = new LazyHCatRecord(o3, testSD.getObjectInspector());	LazyHCatRecord s6 = new LazyHCatRecord(s4, hrsd.getObjectInspector());	
six 

========================= hive sample_673 =========================

private void set(Type type, ASTNode ast) {	if (this.type != Type.NONE) {	
setting when already node vs old node 

semanticException = e;	throw new RuntimeException(e);	}	perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.OPTIMIZER, "Calcite: Plan generation");	RexExecutor executorProvider = new HiveRexExecutorImpl(optCluster);	calciteGenPlan.getCluster().getPlanner().setExecutor(executorProvider);	HiveRelFieldTrimmer fieldTrimmer = new HiveRelFieldTrimmer(null, HiveRelFactories.HIVE_BUILDER.create(optCluster, null), this.columnAccessInfo, this.viewProjectToTableSchema);	fieldTrimmer.trim(calciteGenPlan);	HiveDefaultRelMetadataProvider mdProvider = new HiveDefaultRelMetadataProvider(conf);	RelMetadataQuery.THREAD_PROVIDERS.set( JaninoRelMetadataProvider.of(mdProvider.getMetadataProvider()));	
plan before removing subquery 

throw new RuntimeException(e);	}	perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.OPTIMIZER, "Calcite: Plan generation");	RexExecutor executorProvider = new HiveRexExecutorImpl(optCluster);	calciteGenPlan.getCluster().getPlanner().setExecutor(executorProvider);	HiveRelFieldTrimmer fieldTrimmer = new HiveRelFieldTrimmer(null, HiveRelFactories.HIVE_BUILDER.create(optCluster, null), this.columnAccessInfo, this.viewProjectToTableSchema);	fieldTrimmer.trim(calciteGenPlan);	HiveDefaultRelMetadataProvider mdProvider = new HiveDefaultRelMetadataProvider(conf);	RelMetadataQuery.THREAD_PROVIDERS.set( JaninoRelMetadataProvider.of(mdProvider.getMetadataProvider()));	calciteGenPlan = hepPlan(calciteGenPlan, false, mdProvider.getMetadataProvider(), null, new HiveSubQueryRemoveRule(conf));	
plan just after removing subquery 

}	perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.OPTIMIZER, "Calcite: Plan generation");	RexExecutor executorProvider = new HiveRexExecutorImpl(optCluster);	calciteGenPlan.getCluster().getPlanner().setExecutor(executorProvider);	HiveRelFieldTrimmer fieldTrimmer = new HiveRelFieldTrimmer(null, HiveRelFactories.HIVE_BUILDER.create(optCluster, null), this.columnAccessInfo, this.viewProjectToTableSchema);	fieldTrimmer.trim(calciteGenPlan);	HiveDefaultRelMetadataProvider mdProvider = new HiveDefaultRelMetadataProvider(conf);	RelMetadataQuery.THREAD_PROVIDERS.set( JaninoRelMetadataProvider.of(mdProvider.getMetadataProvider()));	calciteGenPlan = hepPlan(calciteGenPlan, false, mdProvider.getMetadataProvider(), null, new HiveSubQueryRemoveRule(conf));	calciteGenPlan = HiveRelDecorrelator.decorrelateQuery(calciteGenPlan);	
plan after decorrelation 

RelNode rootRel = calcitePreCboPlan;	hepPlanner.setRoot(rootRel);	if (!calcitePreCboPlan.getTraitSet().equals(desiredTraits)) {	rootRel = hepPlanner.changeTraits(calcitePreCboPlan, desiredTraits);	}	hepPlanner.setRoot(rootRel);	calciteOptimizedPlan = hepPlanner.findBestExp();	} catch (Exception e) {	boolean isMissingStats = noColsMissingStats.get() > 0;	if (isMissingStats) {	
missing column stats see previous messages skipping join reordering in cbo 

final DruidQuery dq = (DruidQuery) scan;	newScan = DruidQuery.create(optCluster, optCluster.traitSetOf(BindableConvention.INSTANCE), scan.getTable(), dq.getDruidTable(), ImmutableList.<RelNode>of(dq.getTableScan()));	} else {	newScan = new HiveTableScan(optCluster, optCluster.traitSetOf(HiveRelNode.CONVENTION), (RelOptHiveTable) scan.getTable(), scan.getTable().getQualifiedName().get(0), null, false, false);	}	return newScan;	}	}	);	} catch (HiveException e) {	
exception loading materialized views 

perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.OPTIMIZER);	calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, false, mdProvider.getMetadataProvider(), null, HepMatchOrder.BOTTOM_UP, HiveAggregateProjectMergeRule.INSTANCE);	perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.OPTIMIZER, "Calcite: Optimizations without stats 2");	if (conf.getBoolVar(ConfVars.AGGR_JOIN_TRANSPOSE)) {	perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.OPTIMIZER);	try {	calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, false, mdProvider.getMetadataProvider(), null, HepMatchOrder.BOTTOM_UP, HiveAggregateJoinTransposeRule.INSTANCE);	} catch (Exception e) {	boolean isMissingStats = noColsMissingStats.get() > 0;	if (isMissingStats) {	
missing column stats see previous messages skipping aggregate join transpose in cbo 

perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.OPTIMIZER);	calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, true, mdProvider.getMetadataProvider(), null, HepMatchOrder.BOTTOM_UP, HiveJoinProjectTransposeRule.BOTH_PROJECT_INCLUDE_OUTER, HiveJoinProjectTransposeRule.LEFT_PROJECT_INCLUDE_OUTER, HiveJoinProjectTransposeRule.RIGHT_PROJECT_INCLUDE_OUTER, HiveJoinToMultiJoinRule.INSTANCE, HiveProjectMergeRule.INSTANCE);	fieldTrimmer = new HiveRelFieldTrimmer(null, HiveRelFactories.HIVE_BUILDER.create(optCluster, null));	calciteOptimizedPlan = fieldTrimmer.trim(calciteOptimizedPlan);	calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, false, mdProvider.getMetadataProvider(), null, HepMatchOrder.BOTTOM_UP, ProjectRemoveRule.INSTANCE, new ProjectMergeRule(false, HiveRelFactories.HIVE_BUILDER));	calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, true, mdProvider.getMetadataProvider(), null, HiveFilterProjectTSTransposeRule.INSTANCE, HiveFilterProjectTSTransposeRule.INSTANCE_DRUID, HiveProjectFilterPullUpConstantsRule.INSTANCE);	calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, false, mdProvider.getMetadataProvider(), null, HepMatchOrder.BOTTOM_UP, HiveInsertExchange4JoinRule.EXCHANGE_BELOW_JOIN, HiveInsertExchange4JoinRule.EXCHANGE_BELOW_MULTIJOIN);	perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.OPTIMIZER, "Calcite: Translation from Calcite tree to Hive tree");	}	if (LOG.isDebugEnabled() && !conf.getBoolVar(ConfVars.HIVE_IN_TEST)) {	
cbo planning details 

perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.OPTIMIZER);	calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, true, mdProvider.getMetadataProvider(), null, HepMatchOrder.BOTTOM_UP, HiveJoinProjectTransposeRule.BOTH_PROJECT_INCLUDE_OUTER, HiveJoinProjectTransposeRule.LEFT_PROJECT_INCLUDE_OUTER, HiveJoinProjectTransposeRule.RIGHT_PROJECT_INCLUDE_OUTER, HiveJoinToMultiJoinRule.INSTANCE, HiveProjectMergeRule.INSTANCE);	fieldTrimmer = new HiveRelFieldTrimmer(null, HiveRelFactories.HIVE_BUILDER.create(optCluster, null));	calciteOptimizedPlan = fieldTrimmer.trim(calciteOptimizedPlan);	calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, false, mdProvider.getMetadataProvider(), null, HepMatchOrder.BOTTOM_UP, ProjectRemoveRule.INSTANCE, new ProjectMergeRule(false, HiveRelFactories.HIVE_BUILDER));	calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, true, mdProvider.getMetadataProvider(), null, HiveFilterProjectTSTransposeRule.INSTANCE, HiveFilterProjectTSTransposeRule.INSTANCE_DRUID, HiveProjectFilterPullUpConstantsRule.INSTANCE);	calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, false, mdProvider.getMetadataProvider(), null, HepMatchOrder.BOTTOM_UP, HiveInsertExchange4JoinRule.EXCHANGE_BELOW_JOIN, HiveInsertExchange4JoinRule.EXCHANGE_BELOW_MULTIJOIN);	perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.OPTIMIZER, "Calcite: Translation from Calcite tree to Hive tree");	}	if (LOG.isDebugEnabled() && !conf.getBoolVar(ConfVars.HIVE_IN_TEST)) {	
original plan 

perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.OPTIMIZER);	calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, true, mdProvider.getMetadataProvider(), null, HepMatchOrder.BOTTOM_UP, HiveJoinProjectTransposeRule.BOTH_PROJECT_INCLUDE_OUTER, HiveJoinProjectTransposeRule.LEFT_PROJECT_INCLUDE_OUTER, HiveJoinProjectTransposeRule.RIGHT_PROJECT_INCLUDE_OUTER, HiveJoinToMultiJoinRule.INSTANCE, HiveProjectMergeRule.INSTANCE);	fieldTrimmer = new HiveRelFieldTrimmer(null, HiveRelFactories.HIVE_BUILDER.create(optCluster, null));	calciteOptimizedPlan = fieldTrimmer.trim(calciteOptimizedPlan);	calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, false, mdProvider.getMetadataProvider(), null, HepMatchOrder.BOTTOM_UP, ProjectRemoveRule.INSTANCE, new ProjectMergeRule(false, HiveRelFactories.HIVE_BUILDER));	calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, true, mdProvider.getMetadataProvider(), null, HiveFilterProjectTSTransposeRule.INSTANCE, HiveFilterProjectTSTransposeRule.INSTANCE_DRUID, HiveProjectFilterPullUpConstantsRule.INSTANCE);	calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, false, mdProvider.getMetadataProvider(), null, HepMatchOrder.BOTTOM_UP, HiveInsertExchange4JoinRule.EXCHANGE_BELOW_JOIN, HiveInsertExchange4JoinRule.EXCHANGE_BELOW_MULTIJOIN);	perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.OPTIMIZER, "Calcite: Translation from Calcite tree to Hive tree");	}	if (LOG.isDebugEnabled() && !conf.getBoolVar(ConfVars.HIVE_IN_TEST)) {	
plan after ppd partpruning columnpruning 

perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.OPTIMIZER);	calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, true, mdProvider.getMetadataProvider(), null, HepMatchOrder.BOTTOM_UP, HiveJoinProjectTransposeRule.BOTH_PROJECT_INCLUDE_OUTER, HiveJoinProjectTransposeRule.LEFT_PROJECT_INCLUDE_OUTER, HiveJoinProjectTransposeRule.RIGHT_PROJECT_INCLUDE_OUTER, HiveJoinToMultiJoinRule.INSTANCE, HiveProjectMergeRule.INSTANCE);	fieldTrimmer = new HiveRelFieldTrimmer(null, HiveRelFactories.HIVE_BUILDER.create(optCluster, null));	calciteOptimizedPlan = fieldTrimmer.trim(calciteOptimizedPlan);	calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, false, mdProvider.getMetadataProvider(), null, HepMatchOrder.BOTTOM_UP, ProjectRemoveRule.INSTANCE, new ProjectMergeRule(false, HiveRelFactories.HIVE_BUILDER));	calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, true, mdProvider.getMetadataProvider(), null, HiveFilterProjectTSTransposeRule.INSTANCE, HiveFilterProjectTSTransposeRule.INSTANCE_DRUID, HiveProjectFilterPullUpConstantsRule.INSTANCE);	calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, false, mdProvider.getMetadataProvider(), null, HepMatchOrder.BOTTOM_UP, HiveInsertExchange4JoinRule.EXCHANGE_BELOW_JOIN, HiveInsertExchange4JoinRule.EXCHANGE_BELOW_MULTIJOIN);	perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.OPTIMIZER, "Calcite: Translation from Calcite tree to Hive tree");	}	if (LOG.isDebugEnabled() && !conf.getBoolVar(ConfVars.HIVE_IN_TEST)) {	
plan after join reordering 

final List<Integer> rightKeys = new ArrayList<Integer>();	RexNode remainingEquiCond = HiveCalciteUtil.projectNonColumnEquiConditions(HiveRelFactories.HIVE_PROJECT_FACTORY, inputRels, leftJoinKeys, rightJoinKeys, 0, leftKeys, rightKeys);	if (inputRels[0] != leftRel) {	nonEquiConds = RexUtil.shift(nonEquiConds, leftRel.getRowType().getFieldCount(), inputRels[0].getRowType().getFieldCount() - leftRel.getRowType().getFieldCount());	}	calciteJoinCond = remainingEquiCond != null ? RexUtil.composeConjunction(cluster.getRexBuilder(), ImmutableList.of(remainingEquiCond, nonEquiConds), false) : nonEquiConds;	topRel = HiveSemiJoin.getSemiJoin(cluster, cluster.traitSetOf(HiveRelNode.CONVENTION), inputRels[0], inputRels[1], calciteJoinCond, ImmutableIntList.copyOf(leftKeys), ImmutableIntList.copyOf(rightKeys));	if (inputRels[0] != leftRel) {	RowResolver newLeftRR = new RowResolver();	if (!RowResolver.add(newLeftRR, leftRR)) {	
duplicates detected when adding columns to rr see previous message 

RowResolver newLeftRR = new RowResolver();	if (!RowResolver.add(newLeftRR, leftRR)) {	}	for (int i = leftRel.getRowType().getFieldCount();	i < inputRels[0].getRowType().getFieldCount(); i++) {	ColumnInfo oColInfo = new ColumnInfo( SemanticAnalyzer.getColumnInternalName(i), TypeConverter.convert(inputRels[0].getRowType().getFieldList().get(i).getType()), null, false);	newLeftRR.put(oColInfo.getTabAlias(), oColInfo.getInternalName(), oColInfo);	}	RowResolver joinRR = new RowResolver();	if (!RowResolver.add(joinRR, newLeftRR)) {	
duplicates detected when adding columns to rr see previous message 

List<String> topFieldNames = new ArrayList<String>();	for (int i = 0; i < leftRel.getRowType().getFieldCount(); i++) {	final RelDataTypeField field = leftRel.getRowType().getFieldList().get(i);	topFields.add(leftRel.getCluster().getRexBuilder().makeInputRef(field.getType(), i));	topFieldNames.add(field.getName());	}	topRel = HiveRelFactories.HIVE_PROJECT_FACTORY.createProject(topRel, topFields, topFieldNames);	}	topRR = new RowResolver();	if (!RowResolver.add(topRR, leftRR)) {	
duplicates detected when adding columns to rr see previous message 

ImmutableBitSet.Builder builder = ImmutableBitSet.builder();	for (String nnCol : nnc.getNotNullConstraints().values()) {	int nnPos = -1;	for (int i = 0; i < rowType.getFieldNames().size(); i++) {	if (rowType.getFieldNames().get(i).equals(nnCol)) {	nnPos = i;	break;	}	}	if (nnPos == -1) {	
column for not null constraint definition not found 

}	for (String pkCol : pkc.getColNames().values()) {	int pkPos = -1;	for (int i = 0; i < rowType.getFieldNames().size(); i++) {	if (rowType.getFieldNames().get(i).equals(pkCol)) {	pkPos = i;	break;	}	}	if (pkPos == -1) {	
column for not null constraint definition not found 

genericUDAFEvaluator = SemanticAnalyzer.getGenericUDAFEvaluator(aggName, aggParameters, aggAst, isDistinct, isAllColumns);	assert (genericUDAFEvaluator != null);	GenericUDAFInfo udaf = SemanticAnalyzer.getGenericUDAFInfo(genericUDAFEvaluator, amode, aggParameters);	if (FunctionRegistry.pivotResult(aggName)) {	udafRetType = ((ListTypeInfo)udaf.returnType).getListElementTypeInfo();	} else {	udafRetType = udaf.returnType;	}	}	} catch (Exception e) {	
cbo couldn t obtain udaf evaluators for trying to translate to genericudf 

getQBParseInfo(qb);	WindowingSpec wSpec = (!qb.getAllWindowingSpecs().isEmpty()) ? qb.getAllWindowingSpecs() .values().iterator().next() : null;	if (wSpec == null) return null;	wSpec.validateAndMakeEffective();	List<WindowExpressionSpec> windowExpressions = wSpec.getWindowExpressions();	if (windowExpressions == null || windowExpressions.isEmpty()) return null;	RowResolver inputRR = this.relToHiveRR.get(srcRel);	List<RexNode> projsForWindowSelOp = new ArrayList<RexNode>( HiveCalciteUtil.getProjsFromBelowAsInputRef(srcRel));	RowResolver out_rwsch = new RowResolver();	if (!RowResolver.add(out_rwsch, inputRR)) {	
duplicates detected when adding columns to rr see previous message 

private void setQueryHints(QB qb) throws SemanticException {	QBParseInfo qbp = getQBParseInfo(qb);	String selClauseName = qbp.getClauseNames().iterator().next();	Tree selExpr0 = qbp.getSelForClause(selClauseName).getChild(0);	if (selExpr0.getType() != HiveParser.QUERY_HINT) return;	String hint = ctx.getTokenRewriteStream().toString( selExpr0.getTokenStartIndex(), selExpr0.getTokenStopIndex());	
handling query hints 

}	if (!qbp.getDestToSortBy().isEmpty()) {	throw new SemanticException(ErrorMsg.UDTF_NO_SORT_BY.getMsg());	}	if (!qbp.getDestToClusterBy().isEmpty()) {	throw new SemanticException(ErrorMsg.UDTF_NO_CLUSTER_BY.getMsg());	}	if (!qbp.getAliasToLateralViews().isEmpty()) {	throw new SemanticException(ErrorMsg.UDTF_LATERAL_VIEW.getMsg());	}	
table alias col aliases 

========================= hive sample_3396 =========================

public void execute() throws Exception {	if(mPatchFile != null) {	
reading patchfile 

public void execute() throws Exception {	if(mPatchFile != null) {	FileReader fr = null;	try {	fr = new FileReader(mPatchFile);	BufferedReader br = new BufferedReader(fr);	String line;	while ((line = br.readLine()) != null) {	if(line.startsWith("+++")) {	
searching line 

if (javaTestMatcher.find() || fileName.endsWith(".q")) {	modifiedTestFiles.add(fileName);	}	}	}	}	} finally {	fr.close();	}	} else {	
patch file is null 

========================= hive sample_5612 =========================

nullByte = bytes[lastFieldByteEnd];	lastFieldByteEnd++;	} else {	nullByte = 0;	lastFieldByteEnd++;	}	}	}	if (!extraFieldWarned && lastFieldByteEnd < structByteEnd) {	extraFieldWarned = true;	
extra bytes detected at the end of the row last field end and serialize buffer end ignoring similar problems 

nullByte = 0;	lastFieldByteEnd++;	}	}	}	if (!extraFieldWarned && lastFieldByteEnd < structByteEnd) {	extraFieldWarned = true;	}	if (!missingFieldWarned && lastFieldByteEnd > structByteEnd) {	missingFieldWarned = true;	
missing fields expected fields but only got last field end and serialize buffer end ignoring similar problems 

========================= hive sample_5242 =========================

typeInfos.add(SHORT);	typeInfos.add(INT);	typeInfos.add(LONG);	typeInfos.add(FLOAT);	typeInfos.add(DOUBLE);	typeInfos.add(BOOLEAN);	typeInfos.add(DECIMAL);	typeInfos.add(DATE);	typeInfos.add(VOID);	structTypeInfo.setAllStructFieldTypeInfos(typeInfos);	
structtypeinfo is 

========================= hive sample_5096 =========================

Hive db = getHive();	Table tbl = getTable(db);	for (IStatsProcessor task : processors) {	task.setDpPartSpecs(dpPartSpecs);	ret = task.process(db, tbl);	if (ret != 0) {	return ret;	}	}	} catch (Exception e) {	
failed to run stats task 

public static ExecutorService newThreadPool(HiveConf conf) {	int numThreads = HiveConf.getIntVar(conf, ConfVars.HIVE_STATS_GATHER_NUM_THREADS);	ExecutorService executor = Executors.newFixedThreadPool(numThreads, new ThreadFactoryBuilder().setDaemon(true).setNameFormat("StatsNoJobTask-Thread-%d").build());	
initialized threadpool for stats computation with threads 

========================= hive sample_3904 =========================

public AggrColStats get(String dbName, String tblName, String colName, List<String> partNames) {	Key key = new Key(dbName, tblName, colName);	AggrColStatsList candidateList = cacheStore.get(key);	if ((candidateList == null) || (candidateList.nodes.size() == 0)) {	
no aggregate stats cached for 

AggrColStats match = null;	boolean isLocked = false;	try {	isLocked = candidateList.readLock.tryLock(maxReaderWaitTime, TimeUnit.MILLISECONDS);	if (isLocked) {	match = findBestMatch(partNames, candidateList.nodes);	}	if (match != null) {	candidateList.updateLastAccessTime();	cacheHits.incrementAndGet();	
returning aggregate stats from the cache total hits total misses hit ratio 

match = findBestMatch(partNames, candidateList.nodes);	}	if (match != null) {	candidateList.updateLastAccessTime();	cacheHits.incrementAndGet();	}	else {	cacheMisses.incrementAndGet();	}	} catch (InterruptedException e) {	
interrupted exception ignored 

boolean isLocked = false;	try {	isLocked = nodeList.writeLock.tryLock(maxWriterWaitTime, TimeUnit.MILLISECONDS);	if (isLocked) {	nodeList.nodes.add(node);	node.updateLastAccessTime();	nodeList.updateLastAccessTime();	currentNodes.getAndIncrement();	}	} catch (InterruptedException e) {	
interrupted exception ignored 

if (isCleaning) {	return;	}	isCleaning = true;	}	Thread cleaner = new Thread("AggregateStatsCache-CleanerThread") {	public void run() {	numRemovedTTL = 0;	numRemovedLRU = 0;	long cleanerStartTime = System.currentTimeMillis();	
aggregatestatscache is full with nodes starting cleaner thread 

for (Iterator<AggrColStats> listIterator = nodes.iterator(); listIterator.hasNext();) {	node = listIterator.next();	if (isExpired(node)) {	listIterator.remove();	numRemovedTTL++;	currentNodes.getAndDecrement();	}	}	}	} catch (InterruptedException e) {	
interrupted exception ignored 

candidateList.writeLock.unlock();	}	}	Thread.yield();	}	while (getCurrentNodes() / maxCacheNodes > cleanUntil) {	evictOneNode();	}	} finally {	isCleaning = false;	
stopping cleaner thread aggregatestatscache is now full with nodes 

candidateList.writeLock.unlock();	}	}	Thread.yield();	}	while (getCurrentNodes() / maxCacheNodes > cleanUntil) {	evictOneNode();	}	} finally {	isCleaning = false;	
number of expired nodes removed 

candidateList.writeLock.unlock();	}	}	Thread.yield();	}	while (getCurrentNodes() / maxCacheNodes > cleanUntil) {	evictOneNode();	}	} finally {	isCleaning = false;	
number of lru nodes removed 

candidateList.writeLock.unlock();	}	}	Thread.yield();	}	while (getCurrentNodes() / maxCacheNodes > cleanUntil) {	evictOneNode();	}	} finally {	isCleaning = false;	
cleaner ran for ms 

lruNode = candidate;	deleteIndex = currentIndex;	}	}	}	candidateList.nodes.remove(deleteIndex);	currentNodes.getAndDecrement();	numRemovedLRU++;	}	} catch (InterruptedException e) {	
interrupted exception ignored 

========================= hive sample_1939 =========================

}	throw rte;	} catch (InterruptedException ie) {	throw new RuntimeException(ie);	} catch (IOException ioe) {	throw new RuntimeException(ioe);	} finally {	try {	FileSystem.closeAllForUGI(clientUgi);	} catch (IOException e) {	
could not clean up file system handles for ugi 

========================= hive sample_1893 =========================

return true;	}	if(Strings.nullToEmpty(node.getName()).startsWith(groupName)) {	result = "true due to name " + groupName;	return true;	}	if(node.getTags().contains(groupTag)) {	result = "true due to tag " + groupName;	return true;	}	
found node result 

========================= hive sample_5583 =========================

}	builder.setContextAttribute(REGISTRY_ATTRIBUTE, registry);	builder.setContextAttribute(PARENT_ATTRIBUTE, parent);	try {	this.http = builder.build();	this.http.addServlet("status", "/status", LlapStatusServlet.class);	this.http.addServlet("peers", "/peers", LlapPeerRegistryServlet.class);	this.http.addServlet("iomem", "/iomem", LlapIoMemoryServlet.class);	this.http.addServlet("system", "/system", SystemConfigurationServlet.class);	} catch (IOException e) {	
llap web service failed to come up 

jg.writeEndObject();	} finally {	if (jg != null) {	jg.close();	}	if (writer != null) {	writer.close();	}	}	} catch (IOException e) {	
caught an exception while processing status request 

jg.writeEndObject();	} finally {	if (jg != null) {	jg.close();	}	if (writer != null) {	writer.close();	}	}	} catch (IOException e) {	
caught an exception while processing status request 

========================= hive sample_2246 =========================

private static Map<Integer, Set<Integer>> getTransitiveVertexOutputs(DagInfo info) {	if (!(info instanceof DAG)) {	
dag info is not a dag cannot derive dependencies 

protected void checkAndSendGuaranteedStateUpdate(TaskInfo ti) {	boolean newState = false;	synchronized (ti) {	assert ti.isPendingUpdate;	if (ti.lastSetGuaranteed != null && ti.lastSetGuaranteed == ti.isGuaranteed) {	ti.requestedValue = ti.isGuaranteed;	setUpdateDoneUnderTiLock(ti);	
not sending update to 

protected void handleUpdateResult(TaskInfo ti, boolean isOk) {	Boolean newStateSameTask = null, newStateAnyTask = null;	
received response for 

}	if (requestedValue != ti.isGuaranteed) {	ti.isPendingUpdate = false;	ti.requestedValue = null;	return;	}	newStateAnyTask = requestedValue;	}	}	if (newStateSameTask != null) {	
sending update to the same task in response handling 

if (newStateAnyTask == null) return;	List<TaskInfo> toUpdate = new ArrayList<>(1);	writeLock.lock();	try {	synchronized (ti) {	ti.isPendingUpdate = false;	ti.requestedValue = null;	if (newStateAnyTask != ti.isGuaranteed) {	return;	}	
sending update to a different task in response handling 

if (pluginEndpoint != null) {	pluginEndpoint.start();	}	writeLock.lock();	try {	scheduledLoggingExecutor.schedule(new Callable<Void>() {	public Void call() throws Exception {	readLock.lock();	try {	if (dagRunning) {	
stats for current dag 

public void onCreate(LlapServiceInstance serviceInstance, int ephSeqVersion) {	
added node with identity as a result of registry callback 

public void onRemove(LlapServiceInstance serviceInstance, int ephSeqVersion) {	NodeReport nodeReport = constructNodeReport(serviceInstance, false);	
sending out nodereport for onremove 

public void onRemove(LlapServiceInstance serviceInstance, int ephSeqVersion) {	NodeReport nodeReport = constructNodeReport(serviceInstance, false);	getContext().nodesUpdated(Collections.singletonList(nodeReport));	instanceToNodeMap.remove(serviceInstance.getWorkerIdentity());	LOG.info("Removed node with identity: {} due to RegistryNotification. currentActiveInstances={}", serviceInstance.getWorkerIdentity(), activeInstances.size());	if (metrics != null) {	metrics.setClusterNodeCount(activeInstances.size());	}	if (activeInstances.size() == 0) {	
no node found signalling scheduler timeout monitor thread to start timer 

private void startTimeoutMonitor() {	timeoutLock.lock();	try {	if ((timeoutFuture == null || (timeoutFuture != null && timeoutFuture.isDone())) && activeInstances.size() == 0) {	timeoutFuture = timeoutExecutor.schedule(timeoutMonitor, timeout, TimeUnit.MILLISECONDS);	timeoutFutureRef.set(timeoutFuture);	
scheduled timeout monitor task to run after ms 

private void stopTimeoutMonitor() {	timeoutLock.lock();	try {	if (timeoutFuture != null && activeInstances.size() != 0 && timeoutFuture.cancel(false)) {	timeoutFutureRef.set(null);	
stopped timeout monitor task 

public void dagComplete() {	
dag completed scheduling stats 

public void blacklistNode(NodeId nodeId) {	
blacklistnode not supported 

public void unblacklistNode(NodeId nodeId) {	
unblacklistnode not supported 

public boolean deallocateTask( Object task, boolean taskSucceeded, TaskAttemptEndReason endReason, String diagnostics) {	if (LOG.isDebugEnabled()) {	LOG.debug("Processing deallocateTask for task={}, taskSucceeded={}, endReason={}", task, taskSucceeded, endReason);	}	boolean isEarlyExit = false;	TaskInfo toUpdate = null, taskInfo;	writeLock.lock();	try {	taskInfo = unregisterTask(task);	if (taskInfo == null) {	
could not determine containerid for task could have hit a race condition ignoring the query may hang since this container is now taking up a slot permanently 

TaskInfo toUpdate = null, taskInfo;	writeLock.lock();	try {	taskInfo = unregisterTask(task);	if (taskInfo == null) {	return false;	}	boolean isGuaranteedFreed = false;	synchronized (taskInfo) {	if (taskInfo.isGuaranteed == null) {	
task appears to have been deallocated twice there may be inconsistencies in guaranteed task counts 

} else {	if (metrics != null) {	metrics.setWmTaskFinished(taskInfo.isGuaranteed, taskInfo.isPendingUpdate);	}	isGuaranteedFreed = taskInfo.isGuaranteed;	taskInfo.isGuaranteed = null;	}	}	if (taskInfo.containerId == null) {	if (taskInfo.getState() == TaskInfo.State.ASSIGNED) {	
task assigned but could not find the corresponding containerid the query may hang since this container is now taking up a slot permanently 

if (metrics != null) {	metrics.setWmTaskFinished(taskInfo.isGuaranteed, taskInfo.isPendingUpdate);	}	isGuaranteedFreed = taskInfo.isGuaranteed;	taskInfo.isGuaranteed = null;	}	}	if (taskInfo.containerId == null) {	if (taskInfo.getState() == TaskInfo.State.ASSIGNED) {	} else {	
ignoring deallocate request for task which hasn t been assigned to a container 

} else {	nodeInfo.registerUnsuccessfulTaskEnd(false);	if (endReason != null && EnumSet .of(TaskAttemptEndReason.EXECUTOR_BUSY, TaskAttemptEndReason.COMMUNICATION_ERROR) .contains(endReason)) {	if (endReason == TaskAttemptEndReason.COMMUNICATION_ERROR) {	dagStats.registerCommFailure(taskInfo.assignedNode.getHost());	} else if (endReason == TaskAttemptEndReason.EXECUTOR_BUSY) {	dagStats.registerTaskRejected(taskInfo.assignedNode.getHost());	}	}	if (endReason != null && endReason == TaskAttemptEndReason.NODE_FAILED) {	
task ended on with a node failed message a message should come in from the registry to disable this node unless this was a temporary communication failure 

public void notifyStarted(TezTaskAttemptID attemptId) {	TaskInfo info = null;	writeLock.lock();	try {	info = tasksById.get(attemptId);	if (info == null) {	
unknown task start notification 

private TaskInfo distributeGuaranteedOnTaskCompletion() {	List<TaskInfo> toUpdate = new ArrayList<>(1);	int updatedCount = distributeGuaranteed(1, null, toUpdate);	assert updatedCount <= 1;	if (updatedCount == 0) {	int result = ++unusedGuaranteed;	if (metrics != null) {	metrics.setWmUnusedGuaranteed(result);	}	
returning the unused duck unused is now 

public Object deallocateContainer(ContainerId containerId) {	if (LOG.isDebugEnabled()) {	
ignoring deallocatecontainer for containerid 

} else {	if (nodeInfo.getEnableTime() > request.getLocalityDelayTimeout() && nodeInfo.isDisabled() && nodeInfo.hadCommFailure()) {	LOG.debug("Host={} will not become available within requested timeout", nodeInfo);	} else {	requestedHostsWillBecomeAvailable = true;	}	}	}	}	} else {	
null nodeinfo when attempting to get host with worker and host 

}	}	if (shouldDelayForLocality) {	if (requestedHostsWillBecomeAvailable) {	if (LOG.isDebugEnabled()) {	LOG.debug("Delaying local allocation for [" + request.task + "] when trying to allocate on [" + requestedHostsDebugStr + "]" + ". ScheduleAttemptTime=" + schedulerAttemptTime + ", taskDelayTimeout=" + request.getLocalityDelayTimeout());	}	return SELECT_HOST_RESULT_DELAYED_LOCALITY;	} else {	if (LOG.isDebugEnabled()) {	
skipping local allocation for when trying to allocate on since none of these hosts are part of the known list 

NodeInfo nodeInfo = allNodes.get(i);	if (nodeInfo != null) {	if (nodeInfo.getHost().equals(firstRequestedHost)){	requestedHostIdx = i;	break;	}	}	}	if (requestedHostIdx == -1) {	if (LOG.isDebugEnabled()) {	
requested node in consistent order does not exist falling back to random selection for request 

private void addNode(NodeInfo node, LlapServiceInstance serviceInstance) {	if (activeInstances.size() != 0 && timeoutFutureRef.get() != null) {	
new node added signalling scheduler timeout monitor thread to stop timer 

private void reenableDisabledNode(NodeInfo nodeInfo) {	writeLock.lock();	try {	
attempting to re enable node 

private void reenableDisabledNode(NodeInfo nodeInfo) {	writeLock.lock();	try {	if (activeInstances.getInstance(nodeInfo.getNodeIdentity()) != null) {	nodeInfo.enableNode();	if (metrics != null) {	metrics.setDisabledNodeCount(disabledNodesQueue.size());	}	} else {	if (LOG.isInfoEnabled()) {	
not re enabling node since it is not present in the registryactivenodelist 

private void queueNodeForReEnablement(final NodeInfo nodeInfo) {	if ( disabledNodesQueue.remove(nodeInfo)) {	
queueing node for re enablement 

private void disableNode(NodeInfo nodeInfo, boolean isCommFailure) {	writeLock.lock();	try {	if (nodeInfo == null || nodeInfo.isDisabled()) {	if (LOG.isDebugEnabled()) {	if (nodeInfo != null) {	
node already disabled or invalid not doing anything 

private void disableNode(NodeInfo nodeInfo, boolean isCommFailure) {	writeLock.lock();	try {	if (nodeInfo == null || nodeInfo.isDisabled()) {	if (LOG.isDebugEnabled()) {	if (nodeInfo != null) {	} else {	
ignoring disablenode invocation for null nodeinfo 

private void removePendingTask(TaskInfo taskInfo) {	writeLock.lock();	try {	Priority priority = taskInfo.priority;	List<TaskInfo> taskInfoList = pendingTasks.get(priority);	if (taskInfoList == null || taskInfoList.isEmpty() || !taskInfoList.remove(taskInfo)) {	
could not find task in pending list at priority 

taskInfo.isPendingUpdate = true;	taskInfo.requestedValue = taskInfo.isGuaranteed;	if (metrics != null) {	metrics.setWmTaskStarted(taskInfo.requestedValue);	}	setUpdateStartedUnderTiLock(taskInfo);	}	TreeMap<Integer, TreeSet<TaskInfo>> runningTasks = isGuaranteed ? guaranteedTasks : speculativeTasks;	writeLock.lock();	try {	
registering 

private TaskInfo unregisterTask(Object task) {	writeLock.lock();	try {	TaskInfo taskInfo = knownTasks.remove(task);	if (taskInfo != null) {	tasksById.remove(taskInfo.attemptId);	
unregistering 

try {	TaskInfo taskInfo = knownTasks.remove(task);	if (taskInfo != null) {	tasksById.remove(taskInfo.attemptId);	if (taskInfo.getState() == TaskInfo.State.ASSIGNED) {	if (!removeFromRunningTaskMap(speculativeTasks, task, taskInfo) && !removeFromRunningTaskMap(guaranteedTasks, task, taskInfo)) {	Preconditions.checkState(false, "runningTasks should contain an entry if the task" + " was in running state. Caused by task: {}", task);	}	}	} else {	
could not find taskinfo for task not removing it from the running set 

protected void schedulePendingTasks() throws InterruptedException {	Ref<TaskInfo> downgradedTask = new Ref<>(null);	writeLock.lock();	try {	if (LOG.isDebugEnabled()) {	
schedulerun 

}	taskInfo.triedAssigningTask();	ScheduleResult scheduleResult = scheduleTask(taskInfo, totalResource, downgradedTask);	if (LOG.isDebugEnabled()) {	LOG.debug("ScheduleResult for Task: {} = {}", taskInfo, scheduleResult);	}	if (scheduleResult == ScheduleResult.SCHEDULED) {	taskIter.remove();	} else {	if (scheduleResult == ScheduleResult.INADEQUATE_TOTAL_RESOURCES) {	
inadequate total resources before scheduling pending tasks signalling scheduler timeout monitor thread to start timer 

}	if (!scheduledAllAtPriority) {	LOG.debug( "Unable to schedule all requests at priority={}. Skipping subsequent priority levels", entry.getKey());	break;	}	}	} finally {	writeLock.unlock();	}	if (downgradedTask.value != null) {	
downgrading 

return selectHostResult.scheduleResult;	}	if (unusedGuaranteed > 0) {	boolean wasGuaranteed = false;	synchronized (taskInfo) {	assert !taskInfo.isPendingUpdate;	wasGuaranteed = taskInfo.isGuaranteed;	taskInfo.isGuaranteed = true;	}	if (wasGuaranteed) {	
the task had guaranteed flag set before scheduling 

assert !taskInfo.isPendingUpdate;	wasGuaranteed = taskInfo.isGuaranteed;	taskInfo.isGuaranteed = true;	}	if (wasGuaranteed) {	} else {	int result = --unusedGuaranteed;	if (metrics != null) {	metrics.setWmUnusedGuaranteed(result);	}	
using an unused duck for unused is now 

numTasksToPreempt -= preemptedTaskList.size();	}	if (numTasksToPreempt > 0) {	preemptedTaskList = preemptTasksFromMap(guaranteedTasks, forPriority, forVertex, numTasksToPreempt, potentialHosts, preemptHosts, preemptedTaskList);	}	} finally {	writeLock.unlock();	}	if (preemptedTaskList != null) {	for (TaskInfo taskInfo : preemptedTaskList) {	
preempting task 

TaskInfo taskInfo = taskInfoIterator.next();	if (preemptHosts != null && !preemptHosts.contains(taskInfo.assignedNode.getHost())) {	continue;	}	Map<Integer,Set<Integer>> depInfo = getDependencyInfo( taskInfo.attemptId.getTaskID().getVertexID().getDAGId());	Set<Integer> vertexDepInfo = null;	if (depInfo != null) {	vertexDepInfo = depInfo.get(forVertex);	}	if (depInfo != null && vertexDepInfo == null) {	
cannot find info for 

private int handleUpdateForSinglePriorityLevel(int remainingCount, Iterator<Entry<Integer, TreeSet<TaskInfo>>> iterator, TaskInfo failedUpdate, List<TaskInfo> toUpdate, boolean newValue) {	Entry<Integer, TreeSet<TaskInfo>> entry = iterator.next();	TreeSet<TaskInfo> atPriority = entry.getValue();	
at priority observing 

TreeMap<Integer, TreeSet<TaskInfo>> toMap = newValue ? guaranteedTasks : speculativeTasks, fromMap = newValue ? speculativeTasks : guaranteedTasks;	while (atPriorityIter.hasNext() && remainingCount > 0) {	TaskInfo taskInfo = atPriorityIter.next();	if (taskInfo == failedUpdate) continue;	atPriorityIter.remove();	synchronized (taskInfo) {	assert taskInfo.isGuaranteed != newValue;	taskInfo.isGuaranteed = newValue;	if (!taskInfo.isPendingUpdate) {	setUpdateStartedUnderTiLock(taskInfo);	
adding to update 

TaskInfo taskInfo = atPriorityIter.next();	if (taskInfo == failedUpdate) continue;	atPriorityIter.remove();	synchronized (taskInfo) {	assert taskInfo.isGuaranteed != newValue;	taskInfo.isGuaranteed = newValue;	if (!taskInfo.isPendingUpdate) {	setUpdateStartedUnderTiLock(taskInfo);	toUpdate.add(taskInfo);	} else {	
not adding to update already pending 

if (atPriority.isEmpty()) {	iterator.remove();	}	if (failedUpdate != null && entry.getKey() == failedUpdate.priority.getPriority() && remainingCount > 0) {	removeFromRunningTaskMap(fromMap, failedUpdate.task, failedUpdate);	synchronized (failedUpdate) {	assert failedUpdate.isGuaranteed != newValue;	failedUpdate.isGuaranteed = newValue;	setUpdateStartedUnderTiLock(failedUpdate);	}	
adding failed to update 

public Void call() {	while (!isShutdown.get() && !Thread.currentThread().isInterrupted()) {	try {	TaskInfo taskInfo = getNextTask();	taskInfo.setInDelayedQueue(false);	processEvictedTask(taskInfo);	} catch (InterruptedException e) {	if (isShutdown.get()) {	
delayedtaskscheduler thread interrupted after shutdown 

public Void call() {	while (!isShutdown.get() && !Thread.currentThread().isInterrupted()) {	try {	TaskInfo taskInfo = getNextTask();	taskInfo.setInDelayedQueue(false);	processEvictedTask(taskInfo);	} catch (InterruptedException e) {	if (isShutdown.get()) {	break;	} else {	
delayedtaskscheduler thread interrupted before being shutdown 

public Void call() {	while (!isShutdown.get() && !Thread.currentThread().isInterrupted()) {	try {	NodeInfo nodeInfo = disabledNodesQueue.poll(POLL_TIMEOUT, TimeUnit.MILLISECONDS);	if (nodeInfo != null) {	reenableDisabledNode(nodeInfo);	trySchedulingPendingTasks();	}	} catch (InterruptedException e) {	if (isShutdown.get()) {	
nodeenabler thread interrupted after shutdown 

try {	NodeInfo nodeInfo = disabledNodesQueue.poll(POLL_TIMEOUT, TimeUnit.MILLISECONDS);	if (nodeInfo != null) {	reenableDisabledNode(nodeInfo);	trySchedulingPendingTasks();	}	} catch (InterruptedException e) {	if (isShutdown.get()) {	break;	} else {	
nodeenabler thread interrupted without being shutdown 

public void run() {	
reporting service unavailable error as no instances are running 

public Void call() throws Exception {	while (!isShutdown.get() && !Thread.currentThread().isInterrupted()) {	scheduleLock.lock();	try {	while (!pendingScheduleInvocations.get()) {	scheduleCondition.await();	}	} catch (InterruptedException e) {	if (isShutdown.get()) {	
scheduler thread interrupted after shutdown 

while (!isShutdown.get() && !Thread.currentThread().isInterrupted()) {	scheduleLock.lock();	try {	while (!pendingScheduleInvocations.get()) {	scheduleCondition.await();	}	} catch (InterruptedException e) {	if (isShutdown.get()) {	break;	} else {	
scheduler thread interrupted without being shutdown 

} finally {	scheduleLock.unlock();	}	pendingScheduleInvocations.set(false);	try {	schedulePendingTasks();	} catch (InterruptedException ie) {	if (isShutdown.get()) {	return null;	}	
scheduler thread was interrupte without shutdown and will now exit 

}	pendingScheduleInvocations.set(false);	try {	schedulePendingTasks();	} catch (InterruptedException ie) {	if (isShutdown.get()) {	return null;	}	throw ie;	} catch (Throwable t) {	
fatal error scheduler thread has failed and will now exit 

protected void sendUpdateMessageAsync(TaskInfo ti, boolean newState) {	
sending message to 

========================= hive sample_624 =========================

public void abortTask(TaskAttemptContext context) throws IOException;	}	private HashMap<String, TaskCommitterProxy> taskCommitters = new HashMap<String, TaskCommitterProxy>();	String key = generateKey(context);	if (!taskCommitters.containsKey(key)) {	
no callback registered for taskattemptid skipping 

public void abortTask(TaskAttemptContext context) throws IOException;	}	private HashMap<String, TaskCommitterProxy> taskCommitters = new HashMap<String, TaskCommitterProxy>();	String key = generateKey(context);	if (!taskCommitters.containsKey(key)) {	return;	}	try {	
committing taskattempt 

public synchronized void abortTask(TaskAttemptContext context) throws IOException {	String key = generateKey(context);	if (!taskCommitters.containsKey(key)) {	
no callback registered for taskattemptid skipping 

public synchronized void abortTask(TaskAttemptContext context) throws IOException {	String key = generateKey(context);	if (!taskCommitters.containsKey(key)) {	return;	}	try {	
aborting taskattempt 

public synchronized void register(TaskAttemptContext context, TaskCommitterProxy committer) throws IOException {	String key = generateKey(context);	
registering committer for taskattemptid 

public synchronized void register(TaskAttemptContext context, TaskCommitterProxy committer) throws IOException {	String key = generateKey(context);	if (taskCommitters.containsKey(key)) {	
replacing previous committer 

public synchronized void discardCleanupFor(TaskAttemptContext context) throws IOException {	String key = generateKey(context);	
discarding all cleanup for taskattemptid 

public synchronized void discardCleanupFor(TaskAttemptContext context) throws IOException {	String key = generateKey(context);	if (!taskCommitters.containsKey(key)) {	
no committer registered for taskattemptid 

========================= hive sample_761 =========================

public long determineMaxMmSize(long defragHeadroom, long maxMmSize) {	if (defragHeadroom > 0) {	long maxHeadroom = (long) Math.floor(maxSize * MAX_DEFRAG_HEADROOM_FRACTION);	defragHeadroom = Math.min(maxHeadroom, defragHeadroom);	
leaving of defragmentation headroom 

public long validateAndDetermineMaxSize(long maxSizeVal) {	if ((maxSizeVal % arenaSize) > 0) {	long oldMaxSize = maxSizeVal;	maxSizeVal = (maxSizeVal / arenaSize) * arenaSize;	
rounding cache size to from to be divisible by arena size 

public int validateAndDetermineArenaSize(int arenaCount, long maxSizeVal) {	long arenaSizeVal = (arenaCount == 0) ? MAX_ARENA_SIZE : maxSizeVal / arenaCount;	arenaSizeVal = Math.max(maxAllocation, Math.min(arenaSizeVal, MAX_ARENA_SIZE));	if (LlapIoImpl.LOG.isInfoEnabled()) {	
buddy allocator with direct byte buffers memory mapped off allocation sizes arena size total size 

}	if (maxSizeVal < maxAllocation || maxAllocation < minAllocation) {	throw new RuntimeException("Inconsistent sizes; expecting " + minName + " <= " + maxName + " <= " + ConfVars.LLAP_IO_MEMORY_MAX_SIZE.varname + "; configured with min=" + minAllocation + ", max=" + maxAllocation + " and total=" + maxSizeVal);	}	if ((Integer.bitCount(minAllocation) != 1) || (Integer.bitCount(maxAllocation) != 1)) {	throw new RuntimeException("Allocation sizes must be powers of two; configured with " + minName + "=" + minAllocation + ", " + maxName + "=" + maxAllocation);	}	if ((arenaSizeVal % maxAllocation) > 0) {	long oldArenaSize = arenaSizeVal;	arenaSizeVal = (arenaSizeVal / maxAllocation) * maxAllocation;	
rounding arena size to from to be divisible by allocation size 

String msg = "Failed to allocate " + size + "; at " + destAllocIx + " out of " + dest.length + " (entire cache is fragmented and locked, or an internal issue)";	logOomErrorMessage(msg);	isFailed = true;	throw new AllocatorOutOfMemoryException(msg);	}	++attempt;	}	} finally {	memoryManager.releaseMemory(memoryForceReleased);	if (!isFailed && attempt >= LOG_DISCARD_ATTEMPTS) {	
allocation of buffers of size took attempts to free enough memory force released 

RandomAccessFile rwf = null;	File rf = null;	Preconditions.checkArgument(isDirect, "All memory mapped allocations have to be direct buffers");	try {	rf = File.createTempFile("arena-", ".cache", cacheDir.toFile());	rwf = new RandomAccessFile(rf, "rw");	rwf.setLength(arenaSize);	ByteBuffer rwbuf = rwf.getChannel().map(MapMode.READ_WRITE, 0, arenaSize);	return rwbuf;	} catch (IOException ioe) {	
failed trying to allocate memory mapped arena 

private int dumpOneLine(int ix) {	int event = getFirstInt(log[ix]);	switch (event) {	case START_MOVE: {	
started to move 

private int dumpOneLine(int ix) {	int event = getFirstInt(log[ix]);	switch (event) {	case START_MOVE: {	return ix + 3;	}	case SET_NB: {	
set to taken of size 

private int dumpOneLine(int ix) {	int event = getFirstInt(log[ix]);	switch (event) {	case START_MOVE: {	return ix + 3;	}	case SET_NB: {	return ix + 4;	}	case SET_FREE: {	
set to free of size 

case START_MOVE: {	return ix + 3;	}	case SET_NB: {	return ix + 4;	}	case SET_FREE: {	return ix + 4;	}	case UNSET: {	
unset 

case SET_NB: {	return ix + 4;	}	case SET_FREE: {	return ix + 4;	}	case UNSET: {	return ix + 4;	}	case SET_BUF: {	
set to 

case SET_BUF: {	return ix + 4;	}	case ADD_TO_LIST: {	return ix + 4;	}	case REMOVE_FROM_LIST: {	return ix + 4;	}	case ERROR: {	
failed 

========================= hive sample_2180 =========================

public CommandProcessorResponse run(String command) throws CommandNeedRetryException {	SessionState ss = SessionState.get();	try {	ss.loadReloadableAuxJars();	} catch (IOException e) {	
fail to reload auxiliary jar files 

========================= hive sample_3791 =========================

if (!processor.process(searchResult.next())) {	return;	}	}	}	} finally {	for (NamingEnumeration<SearchResult> searchResult : searchResults) {	try {	searchResult.close();	} catch (NamingException ex) {	
failed to close ldap search result 

========================= hive sample_2315 =========================

private void existingTableTasks(ImportTableDesc tblDesc, Table table, ReplicationSpec replicationSpec) {	if (!table.isPartitioned()) {	
table non partitioned 

private void newTableTasks(ImportTableDesc tblDesc) throws SemanticException {	Table table;	table = new Table(tblDesc.getDatabaseName(), tblDesc.getTableName());	Task<?> createTableTask = tblDesc.getCreateTableTask(new HashSet<>(), new HashSet<>(), context.hiveConf);	if (event.replicationSpec().isMetadataOnly()) {	tracker.addTask(createTableTask);	return;	}	if (!isPartitioned(tblDesc)) {	
adding dependent copywork movework for table 

========================= hive sample_3912 =========================

public int run(String[] args) throws IOException, InterruptedException, ClassNotFoundException, TException {	if(LOG.isDebugEnabled()) {	
preparing to submit job 

private String buildHcatDelegationToken(String user) throws IOException, InterruptedException, TException {	final HiveConf c = new HiveConf();	
creating hive metastore delegation token for user 

========================= hive sample_848 =========================

this.userPathPrefix = userScopePathPrefix + getZkPathUser(this.conf);	this.workerNodePrefix = workerPrefix;	this.workersPath =  "/" + userPathPrefix + "/" + instanceName + "/workers";	this.instancesCache = null;	this.stateChangeListeners = new HashSet<>();	this.pathToInstanceCache = new ConcurrentHashMap<>();	this.nodeToInstanceCache = new ConcurrentHashMap<>();	final boolean isSecure = UserGroupInformation.isSecurityEnabled();	ACLProvider zooKeeperAclProvider = new ACLProvider() {	public List<ACL> getDefaultAcl() {	
getdefaultacl was called 

try {	checkAndSetAcls();	} catch (Exception ex) {	throw new IOException("Error validating or setting ACLs. " + disableMessage, ex);	}	}	if (zooKeeperClient.checkExists().forPath(znodePath) == null) {	throw new Exception("Unable to create znode for this instance on ZooKeeper.");	}	} catch (Exception e) {	
unable to create a znode for this server instance 

} catch (NodeExistsException ex) {	}	if (doCheckAcls) {	try {	checkAndSetAcls();	} catch (Exception ex) {	throw new IOException("Error validating or setting ACLs. " + disableMessage, ex);	}	}	} catch (Exception e) {	
unable to create a parent znode for the registry 

private void checkAndSetAcls() throws Exception {	if (!UserGroupInformation.isSecurityEnabled()) return;	String pathToCheck = workersPath;	List<ACL> acls = zooKeeperClient.getACL().forPath(pathToCheck);	if (acls == null || acls.isEmpty()) {	
no acls on setting up acls 

if (acls == null || acls.isEmpty()) {	setUpAcls(pathToCheck);	return;	}	assert userNameFromPrincipal != null;	Id currentUser = new Id("sasl", userNameFromPrincipal);	for (ACL acl : acls) {	if ((acl.getPerms() & ~ZooDefs.Perms.READ) == 0 || currentUser.equals(acl.getId())) {	continue;	}	
the acl is unnacceptable for setting up acls 

try {	ServiceRecord srv = encoder.fromBytes(childData.getPath(), data);	InstanceType instance = createServiceInstance(srv);	addToCache(childData.getPath(), instance.getHost(), instance);	if (doInvokeListeners) {	for (ServiceInstanceStateChangeListener<InstanceType> listener : stateChangeListeners) {	listener.onCreate(instance, ephSeqVersion);	}	}	} catch (IOException e) {	
unable to decode data for zkpath ignoring from current instances list 

public void childEvent(final CuratorFramework client, final PathChildrenCacheEvent event) throws Exception {	Preconditions.checkArgument(client != null && client.getState() == CuratorFrameworkState.STARTED, "client is not started");	synchronized (this) {	ChildData childData = event.getData();	if (childData == null) return;	String nodeName = extractNodeName(childData);	if (!nodeName.startsWith(workerNodePrefix)) return;	
for zknode 

protected final Set<InstanceType> getByHostInternal(String host) {	Set<InstanceType> byHost = nodeToInstanceCache.get(host);	byHost = (byHost == null) ? Sets.<InstanceType>newHashSet() : byHost;	if (LOG.isDebugEnabled()) {	
returning hosts for locality allocation on 

private InstanceType extractServiceInstance( PathChildrenCacheEvent event, ChildData childData) {	byte[] data = childData.getData();	if (data == null) return null;	try {	ServiceRecord srv = encoder.fromBytes(event.getData().getPath(), data);	return createServiceInstance(srv);	} catch (IOException e) {	
unable to decode data for zknode dropping notification of type 

instancesCache = new PathChildrenCache(zooKeeperClient, workersPath, true);	instancesCache.getListenable().addListener(new InstanceStateChangeListener(), tp);	try {	instancesCache.start(PathChildrenCache.StartMode.BUILD_INITIAL_CACHE);	this.instancesCache = instancesCache;	return instancesCache;	} catch (InvalidACLException e) {	CloseableUtils.closeQuietly(instancesCache);	long elapsedNs = System.nanoTime() - startTimeNs;	if (deltaNs == 0 || deltaNs <= elapsedNs) {	
unable to start curator pathchildrencache 

try {	instancesCache.start(PathChildrenCache.StartMode.BUILD_INITIAL_CACHE);	this.instancesCache = instancesCache;	return instancesCache;	} catch (InvalidACLException e) {	CloseableUtils.closeQuietly(instancesCache);	long elapsedNs = System.nanoTime() - startTimeNs;	if (deltaNs == 0 || deltaNs <= elapsedNs) {	throw new IOException(e);	}	
the cluster is not started yet invalidacl will retry 

return instancesCache;	} catch (InvalidACLException e) {	CloseableUtils.closeQuietly(instancesCache);	long elapsedNs = System.nanoTime() - startTimeNs;	if (deltaNs == 0 || deltaNs <= elapsedNs) {	throw new IOException(e);	}	try {	Thread.sleep(Math.min(sleepTimeMs, (deltaNs - elapsedNs)/1000000L));	} catch (InterruptedException e1) {	
interrupted while retrying the pathchildrencache startup 

throw new IOException(e);	}	try {	Thread.sleep(Math.min(sleepTimeMs, (deltaNs - elapsedNs)/1000000L));	} catch (InterruptedException e1) {	throw new IOException(e1);	}	sleepTimeMs = sleepTimeMs << 1;	} catch (Exception e) {	CloseableUtils.closeQuietly(instancesCache);	
unable to start curator pathchildrencache 

private int extractSeqNum(String nodeName) {	String ephSeqVersionStr = nodeName.substring(workerNodePrefix.length() + 1);	try {	return Integer.parseInt(ephSeqVersionStr);	} catch (NumberFormatException e) {	
cannot parse from 

========================= hive sample_632 =========================

public static EncodingWriter create(InputFormat<?, ?> sourceIf, Deserializer serDe, Map<Path, PartitionDesc> parts, Configuration daemonConf, Configuration jobConf, Path splitPath, StructObjectInspector sourceOi, List<Integer> sourceIncludes, boolean[] cacheIncludes, int allocSize) throws IOException {	if (!HiveConf.getBoolVar(daemonConf, ConfVars.LLAP_IO_ENCODE_VECTOR_SERDE_ENABLED) || !HiveConf.getBoolVar(jobConf, ConfVars.LLAP_IO_ENCODE_VECTOR_SERDE_ENABLED) || !(sourceIf instanceof TextInputFormat) || !(serDe instanceof LazySimpleSerDe)) {	return new DeserializerOrcWriter(serDe, sourceOi, allocSize);	}	Path path = splitPath.getFileSystem(daemonConf).makeQualified(splitPath);	PartitionDesc partDesc = HiveFileFormatUtils.getFromPathRecursively(parts, path, null);	if (partDesc == null) {	
not using vertordeserializeorcwriter no partition desc for 

if (!HiveConf.getBoolVar(daemonConf, ConfVars.LLAP_IO_ENCODE_VECTOR_SERDE_ENABLED) || !HiveConf.getBoolVar(jobConf, ConfVars.LLAP_IO_ENCODE_VECTOR_SERDE_ENABLED) || !(sourceIf instanceof TextInputFormat) || !(serDe instanceof LazySimpleSerDe)) {	return new DeserializerOrcWriter(serDe, sourceOi, allocSize);	}	Path path = splitPath.getFileSystem(daemonConf).makeQualified(splitPath);	PartitionDesc partDesc = HiveFileFormatUtils.getFromPathRecursively(parts, path, null);	if (partDesc == null) {	return new DeserializerOrcWriter(serDe, sourceOi, allocSize);	}	Properties tblProps = partDesc.getTableDesc().getProperties();	if ("true".equalsIgnoreCase(tblProps.getProperty( serdeConstants.SERIALIZATION_LAST_COLUMN_TAKES_REST))) {	
not using vertordeserializeorcwriter due to 

if (partDesc == null) {	return new DeserializerOrcWriter(serDe, sourceOi, allocSize);	}	Properties tblProps = partDesc.getTableDesc().getProperties();	if ("true".equalsIgnoreCase(tblProps.getProperty( serdeConstants.SERIALIZATION_LAST_COLUMN_TAKES_REST))) {	return new DeserializerOrcWriter(serDe, sourceOi, allocSize);	}	for (StructField sf : sourceOi.getAllStructFieldRefs()) {	Category c = sf.getFieldObjectInspector().getCategory();	if (c != Category.PRIMITIVE) {	
not using vertordeserializeorcwriter is not supported 

Properties tblProps = partDesc.getTableDesc().getProperties();	if ("true".equalsIgnoreCase(tblProps.getProperty( serdeConstants.SERIALIZATION_LAST_COLUMN_TAKES_REST))) {	return new DeserializerOrcWriter(serDe, sourceOi, allocSize);	}	for (StructField sf : sourceOi.getAllStructFieldRefs()) {	Category c = sf.getFieldObjectInspector().getCategory();	if (c != Category.PRIMITIVE) {	return new DeserializerOrcWriter(serDe, sourceOi, allocSize);	}	}	
creating vertordeserializeorcwriter for 

includes[columnId] = true;	assert inclBatchIx <= columnId;	destinationBatch.cols[inclBatchIx++] = sourceBatch.cols[columnId];	StructField sourceField = sourceFields.get(columnId);	childNames.add(sourceField.getFieldName());	childOis.add(sourceField.getFieldObjectInspector());	}	destinationOi = new LazySimpleStructObjectInspector( childNames, childOis, null, (byte)0, null);	destinationBatch.setPartitionInfo(sourceIncludes.size(), 0);	if (LlapIoImpl.LOG.isDebugEnabled()) {	
includes for deserializer are 

public void run() {	while (true) {	WriteOperation op = null;	int fallbackMs = 8;	while (true) {	op = queue.poll();	if (op != null) break;	if (fallbackMs > 262144) {	
orc encoder timed out waiting for input 

while (true) {	op = queue.poll();	if (op != null) break;	if (fallbackMs > 262144) {	discardData();	return;	}	try {	Thread.sleep(fallbackMs);	} catch (InterruptedException e) {	
orc encoder interrupted waiting for input 

try {	Thread.sleep(fallbackMs);	} catch (InterruptedException e) {	discardData();	return;	}	fallbackMs <<= 1;	}	try {	if (op.apply(orcWriter, cacheWriter)) {	
orc encoder received a exit event 

return;	}	fallbackMs <<= 1;	}	try {	if (op.apply(orcWriter, cacheWriter)) {	completion.onComplete(this);	return;	}	} catch (Exception e) {	
orc encoder failed 

private void discardData() {	try {	cacheWriter.discardData();	} catch (Exception ex) {	
failed to close an async cache writer 

========================= hive sample_2196 =========================

private void setupSessionIO(SessionState sessionState) {	try {	sessionState.in = null;	sessionState.out = new PrintStream(System.out, true, CharEncoding.UTF_8);	sessionState.info = new PrintStream(System.err, true, CharEncoding.UTF_8);	sessionState.err = new PrintStream(System.err, true, CharEncoding.UTF_8);	} catch (UnsupportedEncodingException e) {	
error creating printstream 

public void prepare(QueryState queryState) throws HiveSQLException {	setState(OperationState.RUNNING);	try {	driver = DriverFactory.newDriver(queryState, getParentSession().getUserName(), queryInfo);	if (queryTimeout > 0) {	timeoutExecutor = new ScheduledThreadPoolExecutor(1);	Runnable timeoutTask = new Runnable() {	public void run() {	try {	String queryId = queryState.getQueryId();	
query timed out after seconds cancelling the execution now 

try {	driver = DriverFactory.newDriver(queryState, getParentSession().getUserName(), queryInfo);	if (queryTimeout > 0) {	timeoutExecutor = new ScheduledThreadPoolExecutor(1);	Runnable timeoutTask = new Runnable() {	public void run() {	try {	String queryId = queryState.getQueryId();	SQLOperation.this.cancel(OperationState.TIMEDOUT);	} catch (HiveSQLException e) {	
error cancelling the query after timeout seconds 

private void runQuery() throws HiveSQLException {	try {	OperationState opState = getStatus().getState();	if (opState.isTerminal()) {	
not running the query operation is already in terminal state perhaps cancelled due to query timeout or by another thread 

if (opState.isTerminal()) {	return;	}	driver.setTryCount(Integer.MAX_VALUE);	response = driver.run();	if (0 != response.getResponseCode()) {	throw toSQLException("Error while processing statement", response);	}	} catch (Throwable e) {	if ((getStatus().getState() == OperationState.CANCELED) || (getStatus().getState() == OperationState.TIMEDOUT) || (getStatus().getState() == OperationState.CLOSED) || (getStatus().getState() == OperationState.FINISHED)) {	
ignore exception in terminal state 

SessionState.setCurrentSessionState(parentSessionState);	PerfLogger.setPerfLogger(parentPerfLogger);	LogUtils.registerLoggingContext(queryState.getConf());	try {	if (asyncPrepare) {	prepare(queryState);	}	runQuery();	} catch (HiveSQLException e) {	setOperationException(e);	
error running hive query 

} finally {	LogUtils.unregisterLoggingContext();	}	return null;	}	};	try {	currentUGI.doAs(doAsAction);	} catch (Exception e) {	setOperationException(new HiveSQLException(e));	
error running hive query as user 

private synchronized void cleanup(OperationState state) throws HiveSQLException {	setState(state);	if (shouldRunAsync() && state != OperationState.CANCELED && state != OperationState.TIMEDOUT) {	Future<?> backgroundHandle = getBackgroundHandle();	if (backgroundHandle != null) {	boolean success = backgroundHandle.cancel(true);	String queryId = queryState.getQueryId();	if (success) {	
the running operation has been successfully interrupted 

private synchronized void cleanup(OperationState state) throws HiveSQLException {	setState(state);	if (shouldRunAsync() && state != OperationState.CANCELED && state != OperationState.TIMEDOUT) {	Future<?> backgroundHandle = getBackgroundHandle();	if (backgroundHandle != null) {	boolean success = backgroundHandle.cancel(true);	String queryId = queryState.getQueryId();	if (success) {	} else if (state == OperationState.CANCELED) {	
the running operation could not be cancelled typically because it has already completed normally 

}	}	}	if (driver != null) {	driver.close();	driver.destroy();	}	driver = null;	SessionState ss = SessionState.get();	if (ss == null) {	
operation seems to be in invalid state sessionstate is null 

public void cancel(OperationState stateAfterCancel) throws HiveSQLException {	String queryId = null;	if (stateAfterCancel == OperationState.CANCELED) {	queryId = queryState.getQueryId();	
cancelling the query execution 

public void cancel(OperationState stateAfterCancel) throws HiveSQLException {	String queryId = null;	if (stateAfterCancel == OperationState.CANCELED) {	queryId = queryState.getQueryId();	}	cleanup(stateAfterCancel);	cleanupOperationLog();	if (stateAfterCancel == OperationState.CANCELED) {	
successfully cancelled the query 

}	namesSb.append(fieldSchemas.get(pos).getName());	typesSb.append(fieldSchemas.get(pos).getType());	}	}	String names = namesSb.toString();	String types = typesSb.toString();	serde = new LazySimpleSerDe();	Properties props = new Properties();	if (names.length() > 0) {	
column names 

}	}	String names = namesSb.toString();	String types = typesSb.toString();	serde = new LazySimpleSerDe();	Properties props = new Properties();	if (names.length() > 0) {	props.setProperty(serdeConstants.LIST_COLUMNS, names);	}	if (types.length() > 0) {	
column types 

========================= hive sample_2342 =========================

public static void tearDown() throws Exception {	
shutting down metastore 

========================= hive sample_1581 =========================

private static long getNumRows(HiveConf conf, List<ColumnInfo> schema, Table table, long ds) {	long nr = getNumRows(table);	if (nr < 0) {	int avgRowSize = estimateRowSizeFromSchema(conf, schema);	if (avgRowSize > 0) {	if (LOG.isDebugEnabled()) {	
estimated average row size 

List<String> neededColsToRetrieve;	List<String> partitionColsToRetrieve;	List<ColStatistics> columnStats = new ArrayList<>();	if (colStatsCache != null) {	neededColsToRetrieve = new ArrayList<String>(neededColumns.size());	for (String colName : neededColumns) {	ColStatistics colStats = colStatsCache.getColStats().get(colName);	if (colStats == null) {	neededColsToRetrieve.add(colName);	if (LOG.isDebugEnabled()) {	
stats for column in table could not be retrieved from cache 

neededColsToRetrieve = new ArrayList<String>(neededColumns.size());	for (String colName : neededColumns) {	ColStatistics colStats = colStatsCache.getColStats().get(colName);	if (colStats == null) {	neededColsToRetrieve.add(colName);	if (LOG.isDebugEnabled()) {	}	} else {	columnStats.add(colStats);	if (LOG.isDebugEnabled()) {	
stats for column in table retrieved from cache 

if (LOG.isDebugEnabled()) {	}	}	}	partitionColsToRetrieve = new ArrayList<>(partitionCols.size());	for (String colName : partitionCols) {	ColStatistics colStats = colStatsCache.getColStats().get(colName);	if (colStats == null) {	partitionColsToRetrieve.add(colName);	if (LOG.isDebugEnabled()) {	
stats for column in table could not be retrieved from cache 

partitionColsToRetrieve = new ArrayList<>(partitionCols.size());	for (String colName : partitionCols) {	ColStatistics colStats = colStatsCache.getColStats().get(colName);	if (colStats == null) {	partitionColsToRetrieve.add(colName);	if (LOG.isDebugEnabled()) {	}	} else {	columnStats.add(colStats);	if (LOG.isDebugEnabled()) {	
stats for column in table retrieved from cache 

addPartitionColumnStats(conf, partitionColsToRetrieve, schema, table, partList, columnStats);	stats.addToDataSize(getDataSizeFromColumnStats(nr, columnStats));	stats.updateColumnStatsState(deriveStatType(columnStats, referencedColumns));	stats.addToColumnStats(columnStats);	} else {	if (statsRetrieved) {	columnStats.addAll(convertColStats(aggrStats.getColStats(), table.getTableName()));	}	int colStatsAvailable = neededColumns.size() + partitionCols.size() - partitionColsToRetrieve.size();	if (columnStats.size() != colStatsAvailable) {	
column stats requested for columns able to retrieve for columns 

long betterDS = getDataSizeFromColumnStats(nr, columnStats);	stats.setDataSize((betterDS < 1 || columnStats.isEmpty()) ? ds : betterDS);	inferAndSetPrimaryKey(stats.getNumRows(), columnStats);	stats.addToColumnStats(columnStats);	stats.setColumnStatsState(deriveStatType(columnStats, referencedColumns));	if (neededColumns.size() != neededColsToRetrieve.size() || partitionCols.size() != partitionColsToRetrieve.size()) {	stats.updateColumnStatsState(colStatsCache.getState());	}	if (aggrStats != null && aggrStats.getPartsFound() != partNames.size() && stats.getColumnStatsState() != State.NONE) {	stats.updateColumnStatsState(State.PARTIAL);	
column stats requested for partitions able to retrieve for partitions 

private static void setUnknownRcDsToAverage( List<Long> rowCounts, List<Long> dataSizes, int avgRowSize) {	if (LOG.isDebugEnabled()) {	
estimated average row size 

public static List<Long> getFileSizeForPartitions(final HiveConf conf, List<Partition> parts) {	
number of partitions 

public static List<Long> getFileSizeForPartitions(final HiveConf conf, List<Partition> parts) {	ArrayList<Future<Long>> futures = new ArrayList<>();	int threads = Math.max(1, conf.getIntVar(ConfVars.METASTORE_FS_HANDLER_THREADS_COUNT));	final ExecutorService pool = Executors.newFixedThreadPool(threads, new ThreadFactoryBuilder() .setDaemon(true) .setNameFormat("Get-Partitions-Size-%d") .build());	final ArrayList<Long> sizes = new ArrayList<>(parts.size());	for (final Partition part : parts) {	final Path path = part.getDataLocation();	futures.add(pool.submit(new Callable<Long>() {	public Long call() throws Exception {	try {	
partition path 

return 0L;	}	}	}));	}	try {	for(int i = 0; i < futures.size(); i++) {	sizes.add(i, futures.get(i).get());	}	} catch (InterruptedException | ExecutionException e) {	
exception in processing files 

public static List<ColStatistics> getTableColumnStats( Table table, List<ColumnInfo> schema, List<String> neededColumns, ColumnStatsList colStatsCache) {	if (table.isMaterializedTable()) {	
materialized table does not contain table statistics 

} else {	colStatsToRetrieve = neededColumns;	}	String dbName = table.getDbName();	String tabName = table.getTableName();	List<ColStatistics> stats = null;	try {	List<ColumnStatisticsObj> colStat = Hive.get().getTableColumnStatistics( dbName, tabName, colStatsToRetrieve);	stats = convertColStats(colStat, tabName);	} catch (HiveException e) {	
failed to retrieve table statistics 

stats = convertColStats(colStat, tabName);	} catch (HiveException e) {	stats = new ArrayList<ColStatistics>();	}	if (colStatsCache != null) {	for(String col:neededColumns) {	ColStatistics cs = colStatsCache.getColStats().get(col);	if (cs != null) {	stats.add(cs);	if (LOG.isDebugEnabled()) {	
stats for column in table retrieved from cache 

========================= hive sample_5038 =========================

public static Compressor getCompressor(CompressionCodec codec) {	Compressor compressor = borrow(COMPRESSOR_POOL, codec.getCompressorType());	if (compressor == null) {	compressor = codec.createCompressor();	
got brand new compressor 

public static Compressor getCompressor(CompressionCodec codec) {	Compressor compressor = borrow(COMPRESSOR_POOL, codec.getCompressorType());	if (compressor == null) {	compressor = codec.createCompressor();	} else {	
got recycled compressor 

public static Decompressor getDecompressor(CompressionCodec codec) {	Decompressor decompressor = borrow(DECOMPRESSOR_POOL, codec .getDecompressorType());	if (decompressor == null) {	decompressor = codec.createDecompressor();	
got brand new decompressor 

public static Decompressor getDecompressor(CompressionCodec codec) {	Decompressor decompressor = borrow(DECOMPRESSOR_POOL, codec .getDecompressorType());	if (decompressor == null) {	decompressor = codec.createDecompressor();	} else {	
got recycled decompressor 

========================= hive sample_3695 =========================

public void print(JSONObject inputObject, PrintStream outputStream) throws Exception {	
jsonparser is parsing 

========================= hive sample_1352 =========================

assertTrue(e instanceof NoSuchObjectException);	assertEquals("default.simptbl table not found", e.getMessage());	}	try {	dfsPath.getFileSystem(hcatConf).getFileStatus(dfsPath);	assert false;	} catch (Exception e) {	assertTrue(e instanceof FileNotFoundException);	}	} catch (Exception e) {	
testcustomperms failed 

========================= hive sample_665 =========================

public ClientResponse<RetryResponseHolder<Intermediate>> handleResponse(HttpResponse httpResponse) {	
unauthorizedresponsehandler got response status 

========================= hive sample_15 =========================

public void initialize(Configuration conf, Properties tbl) throws SerDeException {	charset = Charset.forName(tbl.getProperty(serdeConstants.SERIALIZATION_ENCODING, "UTF-8"));	if (this.charset.equals(Charsets.ISO_8859_1) || this.charset.equals(Charsets.US_ASCII)) {	
the data may not be properly converted to target charset 

========================= hive sample_5254 =========================

for (Class<? extends VectorExpression> ve : list) {	try {	if (ve.newInstance().getDescriptor().matches(descriptor)) {	return ve;	}	} catch (Exception ex) {	throw new HiveException("Could not instantiate VectorExpression class " + ve.getSimpleName(), ex);	}	}	if (LOG.isDebugEnabled()) {	
getvectorexpressionclass udf descriptor 

if (ve.newInstance().getDescriptor().matches(descriptor)) {	return ve;	}	} catch (Exception ex) {	throw new HiveException("Could not instantiate VectorExpression class " + ve.getSimpleName(), ex);	}	}	if (LOG.isDebugEnabled()) {	for (Class<? extends VectorExpression> ve : list) {	try {	
getvectorexpressionclass doesn t match 

========================= hive sample_4202 =========================

public static boolean moveToTrash(FileSystem fs, Path f, Configuration conf, boolean purge) throws IOException {	
deleting 

public static boolean moveToTrash(FileSystem fs, Path f, Configuration conf, boolean purge) throws IOException {	boolean result;	try {	if(purge) {	
purge is set to true not moving to trash 

public static boolean moveToTrash(FileSystem fs, Path f, Configuration conf, boolean purge) throws IOException {	boolean result;	try {	if(purge) {	} else {	result = Trash.moveToAppropriateTrash(fs, f, conf);	if (result) {	
moved to trash 

boolean result;	try {	if(purge) {	} else {	result = Trash.moveToAppropriateTrash(fs, f, conf);	if (result) {	return true;	}	}	} catch (IOException ioe) {	
force to delete it 

} else {	result = Trash.moveToAppropriateTrash(fs, f, conf);	if (result) {	return true;	}	}	} catch (IOException ioe) {	}	result = fs.delete(f, true);	if (!result) {	
failed to delete 

public static boolean copy(FileSystem srcFS, Path src, FileSystem dstFS, Path dst, boolean deleteSource, boolean overwrite, Configuration conf) throws IOException {	boolean copied = false;	boolean triedDistcp = false;	if (srcFS.getUri().getScheme().equals("hdfs")) {	ContentSummary srcContentSummary = srcFS.getContentSummary(src);	if (srcContentSummary.getFileCount() > MetastoreConf.getLongVar(conf, ConfVars.REPL_COPYFILE_MAXNUMFILES) && srcContentSummary.getLength() > MetastoreConf.getLongVar(conf,ConfVars.REPL_COPYFILE_MAXSIZE)) {	
source is bytes max 

public static boolean copy(FileSystem srcFS, Path src, FileSystem dstFS, Path dst, boolean deleteSource, boolean overwrite, Configuration conf) throws IOException {	boolean copied = false;	boolean triedDistcp = false;	if (srcFS.getUri().getScheme().equals("hdfs")) {	ContentSummary srcContentSummary = srcFS.getContentSummary(src);	if (srcContentSummary.getFileCount() > MetastoreConf.getLongVar(conf, ConfVars.REPL_COPYFILE_MAXNUMFILES) && srcContentSummary.getLength() > MetastoreConf.getLongVar(conf,ConfVars.REPL_COPYFILE_MAXSIZE)) {	
source is files max 

public static boolean copy(FileSystem srcFS, Path src, FileSystem dstFS, Path dst, boolean deleteSource, boolean overwrite, Configuration conf) throws IOException {	boolean copied = false;	boolean triedDistcp = false;	if (srcFS.getUri().getScheme().equals("hdfs")) {	ContentSummary srcContentSummary = srcFS.getContentSummary(src);	if (srcContentSummary.getFileCount() > MetastoreConf.getLongVar(conf, ConfVars.REPL_COPYFILE_MAXNUMFILES) && srcContentSummary.getLength() > MetastoreConf.getLongVar(conf,ConfVars.REPL_COPYFILE_MAXSIZE)) {	
launch distributed copy distcp job 

public static boolean mkdir(FileSystem fs, Path f) throws IOException {	
creating directory if it doesn t exist 

public static boolean rename(FileSystem srcFs, FileSystem destFs, Path srcPath, Path destPath) throws IOException {	
renaming to 

========================= hive sample_1903 =========================

public void process(Object row, int tag) throws HiveException {	if (hasReachedMaxSize) {	return;	}	ObjectInspector rowInspector = inputObjInspectors[0];	try {	Writable writableRow = serializer.serialize(row, rowInspector);	writableRow.write(buffer);	if (buffer.getLength() > MAX_SIZE) {	if (LOG.isInfoEnabled()) {	
disabling am events buffer size too large 

if (buffer.getLength() > MAX_SIZE) {	if (LOG.isInfoEnabled()) {	}	hasReachedMaxSize = true;	buffer = null;	}	} catch (Exception e) {	throw new HiveException(e);	}	if (LOG.isDebugEnabled()) {	
appmasterevent 

========================= hive sample_3894 =========================

String columnName = colName.get(i);	String columnType = colType.get(i);	Object values = list.get(i);	try {	ColumnStatisticsObj statObj = ColumnStatisticsObjTranslator.readHiveStruct(columnName, columnType, structField, values);	statsObjs.add(statObj);	} catch (Exception e) {	if (isStatsReliable) {	throw new HiveException("Statistics collection failed while (hive.stats.reliable)", e);	} else {	
because is infinite or nan we skip stats 

========================= hive sample_5043 =========================

public void addTokenFromUserToJobConf(UserGroupInformation user, JobConf jobConf) throws IOException {	checkNotNull(user, "Provided UGI was null");	checkNotNull(jobConf, "JobConf was null");	Token<?> accumuloToken = getAccumuloToken(user);	if (null == accumuloToken) {	
could not find accumulo token in user 

public void setInputFormatConnectorInfo(JobConf conf, String username, AuthenticationToken token) throws AccumuloSecurityException {	try {	AccumuloInputFormat.setConnectorInfo(conf, username, token);	} catch (IllegalStateException e) {	
ignoring exception setting accumulo connector instance for user 

public void setOutputFormatConnectorInfo(JobConf conf, String username, AuthenticationToken token) throws AccumuloSecurityException {	try {	AccumuloOutputFormat.setConnectorInfo(conf, username, token);	} catch (IllegalStateException e) {	
ignoring exception setting accumulo connector instance for user 

public void setInputFormatZooKeeperInstance(JobConf conf, String instanceName, String zookeepers, boolean isSasl) throws IOException {	try {	ClientConfiguration clientConf = getClientConfiguration(zookeepers, instanceName, isSasl);	AccumuloInputFormat.setZooKeeperInstance(conf, clientConf);	} catch (IllegalStateException ise) {	
ignoring exception setting zookeeper instance of at 

public void setOutputFormatZooKeeperInstance(JobConf conf, String instanceName, String zookeepers, boolean isSasl) throws IOException {	try {	ClientConfiguration clientConf = getClientConfiguration(zookeepers, instanceName, isSasl);	AccumuloOutputFormat.setZooKeeperInstance(conf, clientConf);	} catch (IllegalStateException ise) {	
ignoring exception setting zookeeper instance of at 

public void setInputFormatMockInstance(JobConf conf, String instanceName) {	try {	AccumuloInputFormat.setMockInstance(conf, instanceName);	} catch (IllegalStateException e) {	
ignoring exception setting mock instance of 

public void setOutputFormatMockInstance(JobConf conf, String instanceName) {	try {	AccumuloOutputFormat.setMockInstance(conf, instanceName);	} catch (IllegalStateException e) {	
ignoring exception setting mock instance of 

public void loadDependentJars(Configuration conf) {	List<Class<?>> classesToLoad = new ArrayList<>(Arrays.asList(Tracer.class, Fate.class, Connector.class, Main.class, ZooKeeper.class, AccumuloStorageHandler.class));	try {	classesToLoad.add(Class.forName("org.apache.htrace.Trace"));	} catch (Exception e) {	
failed to load class for htrace jar trying to continue 

public void loadDependentJars(Configuration conf) {	List<Class<?>> classesToLoad = new ArrayList<>(Arrays.asList(Tracer.class, Fate.class, Connector.class, Main.class, ZooKeeper.class, AccumuloStorageHandler.class));	try {	classesToLoad.add(Class.forName("org.apache.htrace.Trace"));	} catch (Exception e) {	}	try {	Utils.addDependencyJars(conf, classesToLoad);	} catch (IOException e) {	
could not add necessary accumulo dependencies to classpath 

public Token<? extends TokenIdentifier> setConnectorInfoForInputAndOutput(AccumuloConnectionParameters params, Connector conn, Configuration conf) throws Exception {	AuthenticationToken token = getDelegationToken(conn);	try {	InputConfigurator.setConnectorInfo(AccumuloInputFormat.class, conf, params.getAccumuloUserName(), token);	} catch (IllegalStateException e) {	
ignoring illegalargumentexception about re setting connector information 

public Token<? extends TokenIdentifier> setConnectorInfoForInputAndOutput(AccumuloConnectionParameters params, Connector conn, Configuration conf) throws Exception {	AuthenticationToken token = getDelegationToken(conn);	try {	InputConfigurator.setConnectorInfo(AccumuloInputFormat.class, conf, params.getAccumuloUserName(), token);	} catch (IllegalStateException e) {	}	try {	OutputConfigurator.setConnectorInfo(AccumuloOutputFormat.class, conf, params.getAccumuloUserName(), token);	} catch (IllegalStateException e) {	
ignoring illegalargumentexception about re setting connector information 

========================= hive sample_188 =========================

private Task<? extends Serializable> resolveDriverAlias(ConditionalResolverCommonJoinCtx ctx, HiveConf conf) {	try {	resolveUnknownSizes(ctx, conf);	return resolveMapJoinTask(ctx, conf);	} catch (Exception e) {	
failed to resolve driver alias by exception falling back to common join 

continue;	}	long aliasSize = Utilities.sumOf(aliasToKnownSize, aliases);	if (bigTableSize == null || aliasSize > bigTableSize) {	nextTask = entry;	bigTableSize = aliasSize;	smallTablesSize = sumOfOthers;	}	}	if (nextTask != null) {	
driver alias is with size total size of others threshold 

long aliasSize = Utilities.sumOf(aliasToKnownSize, aliases);	if (bigTableSize == null || aliasSize > bigTableSize) {	nextTask = entry;	bigTableSize = aliasSize;	smallTablesSize = sumOfOthers;	}	}	if (nextTask != null) {	return nextTask.getKey();	}	
failed to resolve driver alias threshold length mapping 

========================= hive sample_3178 =========================

this.schema = Lists.<TypeInfo>newArrayList(vrbCtx.getRowColumnTypeInfos());	boolean[] included = new boolean[schema.size()];	if (includedCols != null) {	for (int colIx : includedCols) {	included[colIx] = true;	}	} else {	Arrays.fill(included, true);	}	if (LOG.isDebugEnabled()) {	
including the columns 

public boolean next(NullWritable key, Object previous) throws IOException {	if (!ensureBatch()) {	return false;	}	StructType value = (StructType)previous;	for (int i = 0; i < schema.size(); ++i) {	if (!included[i]) continue;	try {	setStructCol(value, i, nextValue(batch.cols[i], rowInBatch, schema.get(i), getStructCol(value, i)));	} catch (Throwable t) {	
error at row column 

========================= hive sample_3601 =========================

for (SparkTran leafTran : leafTrans) {	JavaPairRDD<HiveKey, BytesWritable> rdd = tranToOutputRDDMap.get(leafTran);	if (finalRDD == null) {	finalRDD = rdd;	} else {	finalRDD = finalRDD.union(rdd);	}	}	perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.SPARK_BUILD_RDD_GRAPH);	if (LOG.isDebugEnabled()) {	
print generated spark rdd graph 

========================= hive sample_4594 =========================

ThreadFactory workerFactory = new ThreadFactoryBuilder() .setNameFormat("ShuffleHandler Netty Worker #%d") .build();	selector = new NioServerSocketChannelFactory( Executors.newCachedThreadPool(bossFactory), Executors.newCachedThreadPool(workerFactory), maxShuffleThreads);	sslFileBufferSize = conf.getInt(SUFFLE_SSL_FILE_BUFFER_SIZE_KEY, DEFAULT_SUFFLE_SSL_FILE_BUFFER_SIZE);	connectionKeepAliveEnabled = conf.getBoolean(SHUFFLE_CONNECTION_KEEP_ALIVE_ENABLED, DEFAULT_SHUFFLE_CONNECTION_KEEP_ALIVE_ENABLED);	connectionKeepAliveTimeOut = Math.max(1, conf.getInt(SHUFFLE_CONNECTION_KEEP_ALIVE_TIME_OUT, DEFAULT_SHUFFLE_CONNECTION_KEEP_ALIVE_TIME_OUT));	mapOutputMetaInfoCacheSize = Math.max(1, conf.getInt(SHUFFLE_MAPOUTPUT_META_INFO_CACHE_SIZE, DEFAULT_SHUFFLE_MAPOUTPUT_META_INFO_CACHE_SIZE));	userRsrc = new ConcurrentHashMap<>();	secretManager = new JobTokenSecretManager();	shuffle = new Shuffle(conf);	if (conf.getBoolean(SHUFFLE_DIR_WATCHER_ENABLED, SHUFFLE_DIR_WATCHER_ENABLED_DEFAULT)) {	
attempting to start dirwatcher 

connectionKeepAliveTimeOut = Math.max(1, conf.getInt(SHUFFLE_CONNECTION_KEEP_ALIVE_TIME_OUT, DEFAULT_SHUFFLE_CONNECTION_KEEP_ALIVE_TIME_OUT));	mapOutputMetaInfoCacheSize = Math.max(1, conf.getInt(SHUFFLE_MAPOUTPUT_META_INFO_CACHE_SIZE, DEFAULT_SHUFFLE_MAPOUTPUT_META_INFO_CACHE_SIZE));	userRsrc = new ConcurrentHashMap<>();	secretManager = new JobTokenSecretManager();	shuffle = new Shuffle(conf);	if (conf.getBoolean(SHUFFLE_DIR_WATCHER_ENABLED, SHUFFLE_DIR_WATCHER_ENABLED_DEFAULT)) {	DirWatcher localDirWatcher = null;	try {	localDirWatcher = new DirWatcher(this);	} catch (IOException e) {	
unable to start dirwatcher active scans disabled 

secretManager = new JobTokenSecretManager();	shuffle = new Shuffle(conf);	if (conf.getBoolean(SHUFFLE_DIR_WATCHER_ENABLED, SHUFFLE_DIR_WATCHER_ENABLED_DEFAULT)) {	DirWatcher localDirWatcher = null;	try {	localDirWatcher = new DirWatcher(this);	} catch (IOException e) {	}	dirWatcher = localDirWatcher;	} else {	
dirwatcher disabled by config 

if (conf.getBoolean(SHUFFLE_DIR_WATCHER_ENABLED, SHUFFLE_DIR_WATCHER_ENABLED_DEFAULT)) {	DirWatcher localDirWatcher = null;	try {	localDirWatcher = new DirWatcher(this);	} catch (IOException e) {	}	dirWatcher = localDirWatcher;	} else {	dirWatcher = null;	}	
manageoscache shouldalwaysevictoscache readaheadlength maxshuffleconnections localdirs shufflebuffersize shuffletransfertoallowed connectionkeepaliveenabled connectionkeepalivetimeout mapoutputmetainfocachesize sslfilebuffersize 

bootstrap.setOption("backlog", NetUtil.SOMAXCONN);	port = conf.getInt(SHUFFLE_PORT_CONFIG_KEY, DEFAULT_SHUFFLE_PORT);	Channel ch = bootstrap.bind(new InetSocketAddress(port));	accepted.add(ch);	port = ((InetSocketAddress)ch.getLocalAddress()).getPort();	conf.set(SHUFFLE_PORT_CONFIG_KEY, Integer.toString(port));	pipelineFact.SHUFFLE.setPort(port);	if (dirWatcher != null) {	dirWatcher.start();	}	
LlapShuffleHandler listening on port somaxconn backlog 

}	if (registeredDagIdentifier == null || !registeredDagIdentifier.equals(dagIdentifier)) {	if (dirWatcher != null) {	if (LOG.isDebugEnabled()) {	LOG.debug("Registering watches for AppDirs: appId={}, dagId={}", applicationIdString, dagIdentifier);	}	for (String appDir : appDirs) {	try {	dirWatcher.registerDagDir(appDir, applicationIdString, dagIdentifier, user, 5 * 60 * 1000);	} catch (IOException e) {	
unable to register dir with watcher 

private void addJobToken(String appIdString, String user, Token<JobTokenIdentifier> jobToken) {	String jobIdString = appIdString.replace("application", "job");	userRsrc.putIfAbsent(jobIdString, user);	secretManager.addTokenForJob(jobIdString, jobToken);	
added token for 

public AttemptPathInfo load(AttemptPathIdentifier key) throws Exception {	String base = getBaseLocation(key.jobId, key.dagId, key.user);	String attemptBase = base + key.attemptId;	Path indexFileName = lDirAlloc.getLocalPathToRead(attemptBase + "/" + INDEX_FILE_NAME, conf);	Path mapOutputFileName = lDirAlloc.getLocalPathToRead(attemptBase + "/" + DATA_FILE_NAME, conf);	
loaded via loader 

public void channelOpen(ChannelHandlerContext ctx, ChannelStateEvent evt) throws Exception {	if ((maxShuffleConnections > 0) && (accepted.size() >= maxShuffleConnections)) {	
current number of shuffle connections d is greater than or equal to the max allowed shuffle connections d 

}	if (!ShuffleHeader.DEFAULT_HTTP_HEADER_NAME.equals( request.headers().get(ShuffleHeader.HTTP_HEADER_NAME)) || !ShuffleHeader.DEFAULT_HTTP_HEADER_VERSION.equals( request.headers().get(ShuffleHeader.HTTP_HEADER_VERSION))) {	sendError(ctx, "Incompatible shuffle request version", BAD_REQUEST);	}	final Map<String,List<String>> q = new QueryStringDecoder(request.getUri()).getParameters();	final List<String> keepAliveList = q.get("keepAlive");	boolean keepAliveParam = false;	if (keepAliveList != null && keepAliveList.size() == 1) {	keepAliveParam = Boolean.parseBoolean(keepAliveList.get(0));	if (LOG.isDebugEnabled()) {	
keepaliveparam 

if (keepAliveList != null && keepAliveList.size() == 1) {	keepAliveParam = Boolean.parseBoolean(keepAliveList.get(0));	if (LOG.isDebugEnabled()) {	}	}	final List<String> mapIds = splitMaps(q.get("map"));	final List<String> reduceQ = q.get("reduce");	final List<String> jobQ = q.get("job");	final List<String> dagIdQ = q.get("dag");	if (LOG.isDebugEnabled()) {	
recv mapid reduceid jobid dagid keepalive 

}	final String reqUri = request.getUri();	if (null == reqUri) {	sendError(ctx, FORBIDDEN);	return;	}	HttpResponse response = new DefaultHttpResponse(HTTP_1_1, OK);	try {	verifyRequest(jobId, ctx, request, response, new URL("http", "", this.port, reqUri));	} catch (IOException e) {	
shuffle failure 

Map<String, MapOutputInfo> mapOutputInfoMap = new HashMap<String, MapOutputInfo>();	Channel ch = evt.getChannel();	ChannelPipeline pipeline = ch.getPipeline();	TimeoutHandler timeoutHandler = (TimeoutHandler)pipeline.get(TIMEOUT_HANDLER);	timeoutHandler.setEnabledTimeout(false);	String user = userRsrc.get(jobId);	try {	populateHeaders(mapIds, jobId, dagId, user, reduceId, response, keepAliveParam, mapOutputInfoMap);	} catch(IOException e) {	ch.write(response);	
shuffle error in populating headers 

MapOutputInfo info = mapOutputInfoMap.get(mapId);	if (info == null) {	info = getMapOutputInfo(jobId, dagId, mapId, reduceId, user);	}	lastMap = sendMapOutput(ctx, ch, user, mapId, reduceId, info);	if (null == lastMap) {	sendError(ctx, NOT_FOUND);	return;	}	} catch (IOException e) {	
shuffle error 

protected MapOutputInfo getMapOutputInfo(String jobId, int dagId, String mapId, int reduce, String user) throws IOException {	AttemptPathInfo pathInfo;	try {	AttemptPathIdentifier identifier = new AttemptPathIdentifier(jobId, dagId, user, mapId);	pathInfo = pathCache.get(identifier);	if (LOG.isDebugEnabled()) {	
retrieved pathinfo for check for corresponding loaded messages to determine whether it was loaded or cached 

protected void setResponseHeaders(HttpResponse response, boolean keepAliveParam, long contentLength) {	if (!connectionKeepAliveEnabled && !keepAliveParam) {	
setting connection close header 

protected void setResponseHeaders(HttpResponse response, boolean keepAliveParam, long contentLength) {	if (!connectionKeepAliveEnabled && !keepAliveParam) {	response.headers().add(HttpHeaders.Names.CONNECTION, CONNECTION_CLOSE);	} else {	response.headers().add(HttpHeaders.Names.CONTENT_LENGTH, String.valueOf(contentLength));	response.headers().add(HttpHeaders.Names.CONNECTION, HttpHeaders.Values.KEEP_ALIVE);	response.headers().add(HttpHeaders.Values.KEEP_ALIVE, "timeout=" + connectionKeepAliveTimeOut);	
content length in shuffle 

protected void verifyRequest(String appid, ChannelHandlerContext ctx, HttpRequest request, HttpResponse response, URL requestUri) throws IOException {	SecretKey tokenSecret = secretManager.retrieveTokenSecret(appid);	if (null == tokenSecret) {	
request for unknown token 

protected void verifyRequest(String appid, ChannelHandlerContext ctx, HttpRequest request, HttpResponse response, URL requestUri) throws IOException {	SecretKey tokenSecret = secretManager.retrieveTokenSecret(appid);	if (null == tokenSecret) {	throw new IOException("could not find jobid");	}	String enc_str = SecureShuffleUtils.buildMsgFrom(requestUri);	String urlHashStr = request.headers().get(SecureShuffleUtils.HTTP_HEADER_URL_HASH);	if (urlHashStr == null) {	
missing header hash for 

final TezIndexRecord info = mapOutputInfo.indexRecord;	final ShuffleHeader header = new ShuffleHeader(mapId, info.getPartLength(), info.getRawLength(), reduce);	final DataOutputBuffer dob = new DataOutputBuffer();	header.write(dob);	ch.write(wrappedBuffer(dob.getData(), 0, dob.getLength()));	final File spillfile = new File(mapOutputInfo.mapOutputFileName.toString());	RandomAccessFile spill;	try {	spill = SecureIOUtils.openForRandomRead(spillfile, "r", user, null);	} catch (FileNotFoundException e) {	
not found 

public void exceptionCaught(ChannelHandlerContext ctx, ExceptionEvent e) throws Exception {	Channel ch = e.getChannel();	Throwable cause = e.getCause();	if (cause instanceof TooLongFrameException) {	sendError(ctx, BAD_REQUEST);	return;	} else if (cause instanceof IOException) {	if (cause instanceof ClosedChannelException) {	
ignoring closed channel error 

Throwable cause = e.getCause();	if (cause instanceof TooLongFrameException) {	sendError(ctx, BAD_REQUEST);	return;	} else if (cause instanceof IOException) {	if (cause instanceof ClosedChannelException) {	return;	}	String message = String.valueOf(cause.getMessage());	if (IGNORABLE_ERROR_MESSAGE.matcher(message).matches()) {	
ignoring client socket close 

return;	} else if (cause instanceof IOException) {	if (cause instanceof ClosedChannelException) {	return;	}	String message = String.valueOf(cause.getMessage());	if (IGNORABLE_ERROR_MESSAGE.matcher(message).matches()) {	return;	}	}	
shuffle error 

} else if (cause instanceof IOException) {	if (cause instanceof ClosedChannelException) {	return;	}	String message = String.valueOf(cause.getMessage());	if (IGNORABLE_ERROR_MESSAGE.matcher(message).matches()) {	return;	}	}	if (ch.isConnected()) {	
shuffle error 

========================= hive sample_2155 =========================

long lastAccessTime = 0;	long lastUpdateTime = 0;	int numOfFiles = 0;	boolean unknown = false;	FileSystem fs = tblPath.getFileSystem(conf);	try {	FileStatus tmpStatus = fs.getFileStatus(tblPath);	lastAccessTime = tmpStatus.getAccessTime();	lastUpdateTime = tmpStatus.getModificationTime();	} catch (IOException e) {	
cannot access file system file system status will be unknown 

========================= hive sample_5014 =========================

for (Map.Entry<TezSessionState, Trigger> entry : queriesViolated.entrySet()) {	switch (entry.getValue().getAction().getType()) {	case KILL_QUERY: TezSessionState sessionState = entry.getKey();	String queryId = sessionState.getWmContext().getQueryId();	try {	KillQuery killQuery = sessionState.getKillQuery();	if (killQuery != null) {	sessionState.getKillQuery().killQuery(queryId, entry.getValue().getViolationMsg());	}	} catch (HiveException e) {	
unable to kill query for trigger violation 

========================= hive sample_3969 =========================

public Object getStructFieldData(Object data, StructField fieldRef) {	if (data == null) {	return null;	}	boolean isArray = data.getClass().isArray();	if (!isArray && !(data instanceof List)) {	if (!warned) {	
invalid type for struct 

public Object getStructFieldData(Object data, StructField fieldRef) {	if (data == null) {	return null;	}	boolean isArray = data.getClass().isArray();	if (!isArray && !(data instanceof List)) {	if (!warned) {	
ignoring similar errors 

if (!isArray && !(data instanceof List)) {	if (!warned) {	warned = true;	}	return data;	}	int listSize = (isArray ? ((Object[]) data).length : ((List<Object>) data) .size());	MyField f = (MyField) fieldRef;	if (fields.size() != listSize && !warned) {	warned = true;	
trying to access fields inside a list of elements 

if (!isArray && !(data instanceof List)) {	if (!warned) {	warned = true;	}	return data;	}	int listSize = (isArray ? ((Object[]) data).length : ((List<Object>) data) .size());	MyField f = (MyField) fieldRef;	if (fields.size() != listSize && !warned) {	warned = true;	
ignoring similar errors 

========================= hive sample_5444 =========================

public void setup(HiveConf hiveConf) throws HiveException {	if (!inited) {	synchronized (this) {	if (!inited) {	
setting up the session manager 

if (!existingSession.isOpen() && doOpen) {	existingSession.open(conf);	}	return existingSession;	}	SparkSession sparkSession = new SparkSessionImpl();	if (doOpen) {	sparkSession.open(conf);	}	if (LOG.isDebugEnabled()) {	
new session s is created 

public void closeSession(SparkSession sparkSession) throws HiveException {	if (sparkSession == null) {	return;	}	if (LOG.isDebugEnabled()) {	
closing session s 

public void shutdown() {	
closing the session manager 

========================= hive sample_4605 =========================

if (desc instanceof ExprNodeConstantDesc && null == ((ExprNodeConstantDesc)desc).getValue()) {	return null;	}	if (!(ti instanceof PrimitiveTypeInfo) || !(desc.getTypeInfo() instanceof PrimitiveTypeInfo)) {	return null;	}	PrimitiveTypeInfo priti = (PrimitiveTypeInfo) ti;	PrimitiveTypeInfo descti = (PrimitiveTypeInfo) desc.getTypeInfo();	if (unSupportedTypes.contains(priti.getPrimitiveCategory()) || unSupportedTypes.contains(descti.getPrimitiveCategory())) {	if (LOG.isDebugEnabled()) {	
unsupported types 

PrimitiveTypeInfo priti = (PrimitiveTypeInfo) ti;	PrimitiveTypeInfo descti = (PrimitiveTypeInfo) desc.getTypeInfo();	if (unSupportedTypes.contains(priti.getPrimitiveCategory()) || unSupportedTypes.contains(descti.getPrimitiveCategory())) {	if (LOG.isDebugEnabled()) {	}	return null;	}	boolean brokenDataTypesCombination = unsafeConversionTypes.contains( priti.getPrimitiveCategory()) && !unsafeConversionTypes.contains( descti.getPrimitiveCategory());	if (performSafeTypeCast && brokenDataTypesCombination) {	if (LOG.isDebugEnabled()) {	
unsupported cast 

}	return null;	}	boolean brokenDataTypesCombination = unsafeConversionTypes.contains( priti.getPrimitiveCategory()) && !unsafeConversionTypes.contains( descti.getPrimitiveCategory());	if (performSafeTypeCast && brokenDataTypesCombination) {	if (LOG.isDebugEnabled()) {	}	return null;	}	if (LOG.isDebugEnabled()) {	
casting to type 

if (desc instanceof ExprNodeGenericFuncDesc) {	ExprNodeGenericFuncDesc funcDesc = (ExprNodeGenericFuncDesc) desc;	GenericUDF udf = funcDesc.getGenericUDF();	boolean propagateNext = propagate && propagatableUdfs.contains(udf.getClass());	List<ExprNodeDesc> newExprs = new ArrayList<ExprNodeDesc>();	for (ExprNodeDesc childExpr : desc.getChildren()) {	newExprs.add(foldExpr(childExpr, constants, cppCtx, op, tag, propagateNext));	}	if (!isConstantFoldableUdf(udf, newExprs)) {	if (LOG.isDebugEnabled()) {	
function is undeterministic don t evaluate immediately 

}	if (!isConstantFoldableUdf(udf, newExprs)) {	if (LOG.isDebugEnabled()) {	}	((ExprNodeGenericFuncDesc) desc).setChildren(newExprs);	return desc;	}	ExprNodeDesc shortcut = shortcutFunction(udf, newExprs, op);	if (shortcut != null) {	if (LOG.isDebugEnabled()) {	
folding expression 

if (desc instanceof ExprNodeGenericFuncDesc) {	ExprNodeGenericFuncDesc funcDesc = (ExprNodeGenericFuncDesc) desc;	GenericUDF udf = funcDesc.getGenericUDF();	boolean propagateNext = propagate && propagatableUdfs.contains(udf.getClass());	List<ExprNodeDesc> newExprs = new ArrayList<ExprNodeDesc>();	for (ExprNodeDesc childExpr : desc.getChildren()) {	newExprs.add(foldExpr(childExpr, constants, cppCtx, op, tag, propagateNext));	}	if (!isConstantFoldableUdf(udf, newExprs)) {	if (LOG.isDebugEnabled()) {	
function is undeterministic don t evaluate immediately 

}	if (!isConstantFoldableUdf(udf, newExprs)) {	if (LOG.isDebugEnabled()) {	}	((ExprNodeGenericFuncDesc) desc).setChildren(newExprs);	return desc;	} else {	ExprNodeDesc constant = evaluateFunction(udf, newExprs, desc.getChildren());	if (constant != null) {	if (LOG.isDebugEnabled()) {	
folding expression 

} else {	ExprNodeDesc constant = evaluateFunction(udf, newExprs, desc.getChildren());	if (constant != null) {	if (LOG.isDebugEnabled()) {	}	return constant;	} else {	ExprNodeDesc shortcut = shortcutFunction(udf, newExprs, op);	if (shortcut != null) {	if (LOG.isDebugEnabled()) {	
folding expression 

}	return desc;	} else if (desc instanceof ExprNodeColumnDesc) {	if (op.getParentOperators() == null || op.getParentOperators().isEmpty()) {	return desc;	}	Operator<? extends Serializable> parent = op.getParentOperators().get(tag);	ExprNodeDesc col = evaluateColumn((ExprNodeColumnDesc) desc, cppCtx, parent);	if (col != null) {	if (LOG.isDebugEnabled()) {	
folding expression 

String[] files;	String[] jars;	if (udf instanceof GenericUDFBridge) {	GenericUDFBridge bridge = (GenericUDFBridge) udf;	String udfClassName = bridge.getUdfClassName();	try {	UDF udfInternal = (UDF) Class.forName(bridge.getUdfClassName(), true, Utilities.getSessionSpecifiedClassLoader()) .newInstance();	files = udfInternal.getRequiredFiles();	jars = udfInternal.getRequiredJars();	} catch (Exception e) {	
the udf implementation class is not present in the class path 

ExprNodeColumnDesc c = ExprNodeDescUtils.getColumnExpr(lOperand);	if (null == c) {	c = ExprNodeDescUtils.getColumnExpr(rOperand);	}	if (null == c) {	return;	}	ColumnInfo ci = resolveColumn(rs, c);	if (ci != null) {	if (LOG.isDebugEnabled()) {	
filter is identified as a value assignment propagate it 

v = typeCast(v, ci.getType(), true);	}	if (v != null) {	constants.put(ci, v);	}	}	} else if (udf instanceof GenericUDFOPNull) {	ExprNodeDesc operand = newExprs.get(0);	if (operand instanceof ExprNodeColumnDesc) {	if (LOG.isDebugEnabled()) {	
filter is identified as a value assignment propagate it 

private static ExprNodeDesc evaluateColumn(ExprNodeColumnDesc desc, ConstantPropagateProcCtx cppCtx, Operator<? extends Serializable> parent) {	RowSchema rs = parent.getSchema();	ColumnInfo ci = rs.getColumnInfo(desc.getColumn());	if (ci == null) {	if (LOG.isErrorEnabled()) {	
reverse look up of column error 

private static ExprNodeDesc evaluateColumn(ExprNodeColumnDesc desc, ConstantPropagateProcCtx cppCtx, Operator<? extends Serializable> parent) {	RowSchema rs = parent.getSchema();	ColumnInfo ci = rs.getColumnInfo(desc.getColumn());	if (ci == null) {	if (LOG.isErrorEnabled()) {	}	ci = rs.getColumnInfo(desc.getTabAlias(), desc.getColumn());	}	if (ci == null) {	if (LOG.isErrorEnabled()) {	
can t resolve 

o = poi.getPrimitiveJavaObject(o);	if (typeInfo.getTypeName().contains(serdeConstants.DECIMAL_TYPE_NAME) || typeInfo.getTypeName().contains(serdeConstants.VARCHAR_TYPE_NAME) || typeInfo.getTypeName().contains(serdeConstants.CHAR_TYPE_NAME) || typeInfo.getTypeName().contains(serdeConstants.TIMESTAMPLOCALTZ_TYPE_NAME)) {	return new ExprNodeConstantDesc(typeInfo, o);	}	} else if (udf instanceof GenericUDFStruct && oi instanceof StandardConstantStructObjectInspector) {	ConstantObjectInspector coi = (ConstantObjectInspector) oi;	TypeInfo structType = TypeInfoUtils.getTypeInfoFromObjectInspector(coi);	return new ExprNodeConstantDesc(structType, ObjectInspectorUtils.copyToStandardJavaObject(o, coi));	} else if (!PrimitiveObjectInspectorUtils.isPrimitiveJavaClass(clz)) {	if (LOG.isErrorEnabled()) {	
unable to evaluate return value unrecoginizable 

}	return null;	} else {	}	String constStr = null;	if (arguments.length == 1 && FunctionRegistry.isOpCast(udf)) {	constStr = arguments[0].get().toString();	}	return new ExprNodeConstantDesc(o).setFoldedFromVal(constStr);	} catch (HiveException e) {	
evaluation function failed in constant propagation optimizer 

private static void foldOperator(Operator<? extends Serializable> op, ConstantPropagateProcCtx cppCtx) throws SemanticException {	RowSchema schema = op.getSchema();	Map<ColumnInfo, ExprNodeDesc> constants = cppCtx.getOpToConstantExprs().get(op);	if (schema != null && schema.getSignature() != null) {	for (ColumnInfo col : schema.getSignature()) {	ExprNodeDesc constant = constants.get(col);	if (constant != null) {	if (LOG.isDebugEnabled()) {	
replacing column with constant in 

public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx ctx, Object... nodeOutputs) throws SemanticException {	FilterOperator op = (FilterOperator) nd;	ConstantPropagateProcCtx cppCtx = (ConstantPropagateProcCtx) ctx;	Map<ColumnInfo, ExprNodeDesc> constants = cppCtx.getPropagatedConstants(op);	cppCtx.getOpToConstantExprs().put(op, constants);	ExprNodeDesc condn = op.getConf().getPredicate();	if (LOG.isDebugEnabled()) {	
old filter fil conditions 

cppCtx.getOpToConstantExprs().put(op, constants);	ExprNodeDesc condn = op.getConf().getPredicate();	if (LOG.isDebugEnabled()) {	}	ExprNodeDesc newCondn = foldExpr(condn, constants, cppCtx, op, 0, true);	if (newCondn instanceof ExprNodeConstantDesc) {	ExprNodeConstantDesc c = (ExprNodeConstantDesc) newCondn;	if (Boolean.TRUE.equals(c.getValue())) {	cppCtx.addOpToDelete(op);	if (LOG.isDebugEnabled()) {	
filter expression holds true will delete it 

}	ExprNodeDesc newCondn = foldExpr(condn, constants, cppCtx, op, 0, true);	if (newCondn instanceof ExprNodeConstantDesc) {	ExprNodeConstantDesc c = (ExprNodeConstantDesc) newCondn;	if (Boolean.TRUE.equals(c.getValue())) {	cppCtx.addOpToDelete(op);	if (LOG.isDebugEnabled()) {	}	} else if (Boolean.FALSE.equals(c.getValue())) {	if (LOG.isWarnEnabled()) {	
filter expression holds false 

}	} else if (Boolean.FALSE.equals(c.getValue())) {	if (LOG.isWarnEnabled()) {	}	}	}	if (newCondn instanceof ExprNodeConstantDesc && ((ExprNodeConstantDesc)newCondn).getValue() == null) {	newCondn = new ExprNodeConstantDesc(Boolean.FALSE);	}	if (LOG.isDebugEnabled()) {	
new filter fil conditions 

ColumnInfo colInfo = op.getSchema().getSignature().get(i);	if (!VirtualColumn.isVirtualColumnBasedOnAlias(colInfo)) {	constants.put(colInfo, newCol);	}	}	if (columnExprMap != null) {	columnExprMap.put(columnNames.get(i), newCol);	}	}	if (LOG.isDebugEnabled()) {	
new column list 

private void pruneDP(FileSinkDesc fsdesc) {	
dp can be rewritten to sp 

public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx ctx, Object... nodeOutputs) throws SemanticException {	Operator<?> op = (Operator<?>) nd;	ConstantPropagateProcCtx cppCtx = (ConstantPropagateProcCtx) ctx;	cppCtx.getOpToConstantExprs().put(op, new HashMap<ColumnInfo, ExprNodeDesc>());	if (LOG.isDebugEnabled()) {	
stop propagate constants on op 

}	}	}	if (constants.isEmpty()) {	return null;	}	if (op.getChildOperators().size() == 1 && op.getChildOperators().get(0) instanceof JoinOperator) {	JoinOperator joinOp = (JoinOperator) op.getChildOperators().get(0);	if (skipFolding(joinOp.getConf())) {	if (LOG.isDebugEnabled()) {	
skip folding in outer join 

JoinOperator joinOp = (JoinOperator) op.getChildOperators().get(0);	if (skipFolding(joinOp.getConf())) {	if (LOG.isDebugEnabled()) {	}	cppCtx.getOpToConstantExprs().put(op, new HashMap<ColumnInfo, ExprNodeDesc>());	return null;	}	}	if (rsDesc.getDistinctColumnIndices() != null && !rsDesc.getDistinctColumnIndices().isEmpty()) {	if (LOG.isDebugEnabled()) {	
skip folding in distinct subqueries 

public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx ctx, Object... nodeOutputs) throws SemanticException {	JoinOperator op = (JoinOperator) nd;	JoinDesc conf = op.getConf();	ConstantPropagateProcCtx cppCtx = (ConstantPropagateProcCtx) ctx;	Map<ColumnInfo, ExprNodeDesc> constants = cppCtx.getPropagatedConstants(op);	cppCtx.getOpToConstantExprs().put(op, constants);	if (constants.isEmpty()) {	return null;	}	if (op.getChildOperators().size() == 1 && op.getChildOperators().get(0) instanceof ReduceSinkOperator) {	
skip join rs structure 

ConstantPropagateProcCtx cppCtx = (ConstantPropagateProcCtx) ctx;	Map<ColumnInfo, ExprNodeDesc> constants = cppCtx.getPropagatedConstants(op);	cppCtx.getOpToConstantExprs().put(op, constants);	if (constants.isEmpty()) {	return null;	}	if (op.getChildOperators().size() == 1 && op.getChildOperators().get(0) instanceof ReduceSinkOperator) {	return null;	}	if (LOG.isInfoEnabled()) {	
old exprs 

int tag = e.getKey();	List<ExprNodeDesc> exprs = e.getValue();	if (exprs == null) {	continue;	}	List<ExprNodeDesc> newExprs = new ArrayList<ExprNodeDesc>();	for (ExprNodeDesc expr : exprs) {	ExprNodeDesc newExpr = foldExpr(expr, constants, cppCtx, op, tag, false);	if (newExpr instanceof ExprNodeConstantDesc) {	if (LOG.isInfoEnabled()) {	
expr fold from is removed 

if (newExpr instanceof ExprNodeConstantDesc) {	if (LOG.isInfoEnabled()) {	}	continue;	}	newExprs.add(newExpr);	}	e.setValue(newExprs);	}	if (LOG.isInfoEnabled()) {	
new exprs 

========================= hive sample_3085 =========================

firstSmallTable = null;	generateMapMetaData();	isTestingNoHashTableLoad = HiveConf.getBoolVar(hconf, HiveConf.ConfVars.HIVE_MAPJOIN_TESTING_NO_HASH_TABLE_LOAD);	if (isTestingNoHashTableLoad) {	return;	}	final ExecMapperContext mapContext = getExecContext();	final MapredContext mrContext = MapredContext.get();	if (!conf.isBucketMapJoin() && !conf.isDynamicPartitionHashJoin()) {	if (LOG.isDebugEnabled()) {	
this is not bucket map join so cache 

protected Pair<MapJoinTableContainer[], MapJoinTableContainerSerDe[]> loadHashTable( ExecMapperContext mapContext, MapredContext mrContext) throws HiveException {	if (canSkipReload(mapContext)) {	return new ImmutablePair<MapJoinTableContainer[], MapJoinTableContainerSerDe[]>( mapJoinTables, mapJoinTableSerdes);	}	perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.LOAD_HASHTABLE);	loader.init(mapContext, mrContext, hconf, this);	try {	loader.load(mapJoinTables, mapJoinTableSerdes);	} catch (HiveException e) {	if (LOG.isInfoEnabled()) {	
exception loading hash tables clearing partially loaded hash table containers 

} catch (HiveException e) {	if (LOG.isInfoEnabled()) {	}	clearAllTableContainers();	throw e;	}	hashTblInitedOnce = true;	Pair<MapJoinTableContainer[], MapJoinTableContainerSerDe[]> pair = new ImmutablePair<MapJoinTableContainer[], MapJoinTableContainerSerDe[]> (mapJoinTables, mapJoinTableSerdes);	perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.LOAD_HASHTABLE);	if (canSkipJoinProcessing(mapContext)) {	
skipping big table join processing for 

}	assert hybridHtContainer.getTotalInMemRowCount() == 0;	}	}	for (int i = 0; i < numPartitions; i++) {	HashPartition[] hashPartitions = firstSmallTable.getHashPartitions();	if (hashPartitions[i].isHashMapOnDisk()) {	try {	continueProcess(i);	} catch (KryoException ke) {	
processing the spilled data failed due to kryo error 

}	assert hybridHtContainer.getTotalInMemRowCount() == 0;	}	}	for (int i = 0; i < numPartitions; i++) {	HashPartition[] hashPartitions = firstSmallTable.getHashPartitions();	if (hashPartitions[i].isHashMapOnDisk()) {	try {	continueProcess(i);	} catch (KryoException ke) {	
cleaning up all spilled data 

throw new HiveException(e);	}	for (byte pos = 0; pos < order.length; pos++) {	if (pos != conf.getPosBigTable()) spilledMapJoinTables[pos] = null;	}	}	}	}	}	if (LOG.isInfoEnabled()) {	
spilled abort clearing spilled partitions 

}	}	}	if (LOG.isInfoEnabled()) {	}	clearAllTableContainers();	cache.remove(cacheKey);	}	if ((this.getExecContext() != null) && (this.getExecContext().getLocalWork() != null) && (this.getExecContext().getLocalWork().getInputFileChangeSensitive()) && !(HiveConf.getVar(hconf, ConfVars.HIVE_EXECUTION_ENGINE).equals("spark") && SparkUtilities.isDedicatedCluster(hconf))) {	if (LOG.isInfoEnabled()) {	
mr clearing all map join table containers 

private void continueProcess(int partitionId) throws HiveException, IOException, SerDeException, ClassNotFoundException {	for (byte pos = 0; pos < mapJoinTables.length; pos++) {	if (pos != conf.getPosBigTable()) {	
going to reload hash partition 

protected void reloadHashTable(byte pos, int partitionId) throws IOException, HiveException, SerDeException, ClassNotFoundException {	HybridHashTableContainer container = (HybridHashTableContainer)mapJoinTables[pos];	HashPartition partition = container.getHashPartitions()[partitionId];	
going to restore sidefile 

protected void reloadHashTable(byte pos, int partitionId) throws IOException, HiveException, SerDeException, ClassNotFoundException {	HybridHashTableContainer container = (HybridHashTableContainer)mapJoinTables[pos];	HashPartition partition = container.getHashPartitions()[partitionId];	KeyValueContainer kvContainer = partition.getSidefileKVContainer();	int rowCount = kvContainer.size();	
hybrid grace hash join number of rows restored from keyvaluecontainer 

protected void reloadHashTable(byte pos, int partitionId) throws IOException, HiveException, SerDeException, ClassNotFoundException {	HybridHashTableContainer container = (HybridHashTableContainer)mapJoinTables[pos];	HashPartition partition = container.getHashPartitions()[partitionId];	KeyValueContainer kvContainer = partition.getSidefileKVContainer();	int rowCount = kvContainer.size();	if (rowCount <= 0) {	rowCount = 1024 * 1024;	}	
going to restore hashmap 

protected void reloadHashTable(byte pos, int partitionId) throws IOException, HiveException, SerDeException, ClassNotFoundException {	HybridHashTableContainer container = (HybridHashTableContainer)mapJoinTables[pos];	HashPartition partition = container.getHashPartitions()[partitionId];	KeyValueContainer kvContainer = partition.getSidefileKVContainer();	int rowCount = kvContainer.size();	if (rowCount <= 0) {	rowCount = 1024 * 1024;	}	BytesBytesMultiHashMap restoredHashMap = partition.getHashMapFromDisk(rowCount);	rowCount += restoredHashMap.getNumValues();	
hybrid grace hash join deserializing spilled hash partition 

protected void reloadHashTable(byte pos, int partitionId) throws IOException, HiveException, SerDeException, ClassNotFoundException {	HybridHashTableContainer container = (HybridHashTableContainer)mapJoinTables[pos];	HashPartition partition = container.getHashPartitions()[partitionId];	KeyValueContainer kvContainer = partition.getSidefileKVContainer();	int rowCount = kvContainer.size();	if (rowCount <= 0) {	rowCount = 1024 * 1024;	}	BytesBytesMultiHashMap restoredHashMap = partition.getHashMapFromDisk(rowCount);	rowCount += restoredHashMap.getNumValues();	
hybrid grace hash join number of rows in hashmap 

HybridHashTableContainer container = (HybridHashTableContainer)mapJoinTables[pos];	HashPartition partition = container.getHashPartitions()[partitionId];	KeyValueContainer kvContainer = partition.getSidefileKVContainer();	int rowCount = kvContainer.size();	if (rowCount <= 0) {	rowCount = 1024 * 1024;	}	BytesBytesMultiHashMap restoredHashMap = partition.getHashMapFromDisk(rowCount);	rowCount += restoredHashMap.getNumValues();	if (rowCount * container.getTableRowSize() >= container.getMemoryThreshold() / 2) {	
hybrid grace hash join hash table cannot be reloaded since it will be greater than memory limit recursive spilling is currently not supported 

protected void reProcessBigTable(int partitionId) throws HiveException {	HashPartition partition = firstSmallTable.getHashPartitions()[partitionId];	ObjectContainer bigTable = partition.getMatchfileObjContainer();	
hybrid grace hash join going to process spilled big table rows in partition number of rows 

private void cleanupGraceHashJoin() {	for (byte pos = 0; pos < mapJoinTables.length; pos++) {	if (pos != conf.getPosBigTable()) {	
cleaning up small table data at pos 

} else {	return false;	}	boolean skipJoinProcessing = false;	for (int idx = 0; idx < mapJoinTables.length; ++idx) {	if (idx == getConf().getPosBigTable()) {	continue;	}	MapJoinTableContainer mapJoinTable = mapJoinTables[idx];	if (mapJoinTable.size() == 0) {	
hash table number is empty 

========================= hive sample_4039 =========================

private void chooseSlotToTake() throws Exception {	int slotToTake = -1;	while (true) {	List<String> allChildNodes;	try {	allChildNodes = client.getChildren().forPath(basePath);	} catch (Exception e) {	
cannot list nodes to get slots failing 

while (slotIter.hasNext()) {	int nextTaken = slotIter.next();	if (slotToTake < nextTaken) break;	slotToTake = nextTaken + 1;	}	if (slotToTake != currentSlot || !shouldFallBackOnCollision(approxWorkerCount)) break;	++fallbackCount;	Thread.sleep(rdm.nextInt(200));	}	currentSlot = slotToTake;	
will attempt to take slot 

public void close() throws IOException {	State currentState = state.getAndSet(State.CLOSED);	if (currentState == State.CLOSED || currentState == State.LATENT) return;	client.getConnectionStateListenable().removeListener(connectionStateListener);	String localNodePath = nodePath.getAndSet(null);	if (localNodePath == null) return;	try {	client.delete().guaranteed().forPath(localNodePath);	} catch (KeeperException.NoNodeException ignore) {	} catch (Exception e) {	
deleting node 

private void startCreateCurrentNode() {	if (!isActive()) return;	String createPath = null;	try {	createPath = getSlotPath(currentSlot);	
attempting to create 

private void startCreateCurrentNode() {	if (!isActive()) return;	String createPath = null;	try {	createPath = getSlotPath(currentSlot);	client.create().withMode(CreateMode.EPHEMERAL).inBackground(backgroundCallback) .forPath(createPath, data);	} catch (Exception e) {	
creating node path 

private void watchNode() throws Exception {	if (!isActive()) return;	String localNodePath = nodePath.get();	if (localNodePath == null) return;	try {	client.checkExists().usingWatcher(watcher).inBackground( checkExistsCallback).forPath(localNodePath);	} catch (Exception e) {	
watching node 

private void processWatchResult(CuratorEvent event) throws Exception {	if (event.getResultCode() != KeeperException.Code.NONODE.intValue()) return;	
trying to reacquire because of the nonode event 

private void processConnectionState(ConnectionState newState) {	if (newState != ConnectionState.RECONNECTED) return;	
trying to reacquire because of the reconnected event 

private void processWatchedEvent(WatchedEvent event) {	if (event.getType() != EventType.NodeDeleted) return;	String localPath = nodePath.get();	if (localPath == null) return;	if (!localPath.equals(event.getPath())) {	
ignoring the nodedeleted event for 

private void processWatchedEvent(WatchedEvent event) {	if (event.getType() != EventType.NodeDeleted) return;	String localPath = nodePath.get();	if (localPath == null) return;	if (!localPath.equals(event.getPath())) {	return;	}	
trying to reacquire because of the nodedeleted event 

private void processCreateResult(CuratorFramework client, CuratorEvent event) throws Exception {	boolean doesExist = event.getResultCode() == KeeperException.Code.NODEEXISTS.intValue();	if (!doesExist && event.getResultCode() != KeeperException.Code.OK.intValue()) {	if (LOG.isInfoEnabled()) {	
trying to reacquire due to create error 

if (!doesExist && event.getResultCode() != KeeperException.Code.OK.intValue()) {	if (LOG.isInfoEnabled()) {	}	startCreateCurrentNode();	return;	}	State localState = state.get();	switch (localState) {	case CLOSED: case LATENT: return;	case INITIAL_SELECTION: if (doesExist) {	
slot was occupied 

private void processExistsFromCreate(CuratorFramework client, String path) throws Exception {	byte[] actual;	try {	actual = client.getData().forPath(path);	} catch (Exception ex) {	
error getting data for the node will retry creating 

try {	actual = client.getData().forPath(path);	} catch (Exception ex) {	startCreateCurrentNode();	return;	}	if (Arrays.equals(actual, data)) {	handleCreatedNode(path);	} else {	if (LOG.isInfoEnabled()) {	
data at is from a different node we are 

========================= hive sample_649 =========================

public int startVectorizedBatch(int size) throws IOException, HiveException {	if (!isEnabled) {	return FORWARD;	} else if (topN == 0) {	return EXCLUDE;	}	if (usage > threshold) {	int excluded = this.excluded;	
top n hash is flushing rows 

public int startVectorizedBatch(int size) throws IOException, HiveException {	if (!isEnabled) {	return FORWARD;	} else if (topN == 0) {	return EXCLUDE;	}	if (usage > threshold) {	int excluded = this.excluded;	flushInternal();	if (excluded == 0) {	
top n hash has been disabled 

private int insertKeyIntoHeap(HiveKey key) throws IOException, HiveException {	if (usage > threshold) {	flushInternal();	if (excluded == 0) {	
top n hash is disabled 

========================= hive sample_4613 =========================

public String getBucketingDimensionId() {	List<String> bcols = tTable.getSd().getBucketCols();	if (bcols == null || bcols.size() == 0) {	return null;	}	if (bcols.size() > 1) {	
table has more than one dimensions which aren t supported yet 

String serializationLib = getSerializationLib();	try {	if (hasMetastoreBasedSchema(SessionState.getSessionConf(), serializationLib)) {	return tTable.getSd().getCols();	} else if (forMs && !shouldStoreFieldsInMetastore( SessionState.getSessionConf(), serializationLib, tTable.getParameters())) {	return Hive.getFieldsFromDeserializerForMsStorage(this, getDeserializer());	} else {	return HiveMetaStoreUtils.getFieldsFromDeserializer(getTableName(), getDeserializer());	}	} catch (Exception e) {	
unable to get field from serde 

try {	FileSystem fs = FileSystem.get(getPath().toUri(), SessionState.getSessionConf());	String pathPattern = getPath().toString();	if (getNumBuckets() > 0) {	pathPattern = pathPattern + "/*";	}	LOG.info("Path pattern = " + pathPattern);	FileStatus srcs[] = fs.globStatus(new Path(pathPattern), FileUtils.HIDDEN_FILES_PATH_FILTER);	Arrays.sort(srcs);	for (FileStatus src : srcs) {	
got file 

public static boolean shouldStoreFieldsInMetastore( HiveConf conf, String serdeLib, Map<String, String> tableParams) {	if (hasMetastoreBasedSchema(conf, serdeLib))  return true;	if (HiveConf.getBoolVar(conf, ConfVars.HIVE_LEGACY_SCHEMA_FOR_ALL_SERDES)) return true;	AbstractSerDe deserializer = null;	try {	Class<?> clazz = conf.getClassByName(serdeLib);	if (!AbstractSerDe.class.isAssignableFrom(clazz)) return true;	deserializer = ReflectionUtil.newInstance( conf.getClassByName(serdeLib).asSubclass(AbstractSerDe.class), conf);	} catch (Exception ex) {	
cannot initialize serde ignoring 

========================= hive sample_5002 =========================

public Path getPathForPartition(List<String> newPartitionValues) throws WorkerException {	if (newPartitionValues.isEmpty()) {	
using path for unpartitioned table 

public Path getPathForPartition(List<String> newPartitionValues) throws WorkerException {	if (newPartitionValues.isEmpty()) {	return tablePath;	} else {	try {	String location = metaStoreClient .getPartition(databaseName, tableName, newPartitionValues) .getSd() .getLocation();	
found path for partition 

public void createPartitionIfNotExists(List<String> newPartitionValues) throws WorkerException {	if (newPartitionValues.isEmpty()) {	return;	}	try {	
attempting to create partition if not exists 

Table table = metaStoreClient.getTable(databaseName, tableName);	Partition partition = new Partition();	partition.setDbName(table.getDbName());	partition.setTableName(table.getTableName());	StorageDescriptor partitionSd = new StorageDescriptor(table.getSd());	partitionSd.setLocation(table.getSd().getLocation() + Path.SEPARATOR + Warehouse.makePartName(table.getPartitionKeys(), newPartitionValues));	partition.setSd(partitionSd);	partition.setValues(newPartitionValues);	metaStoreClient.add_partition(partition);	} catch (AlreadyExistsException e) {	
partition already exisits 

Partition partition = new Partition();	partition.setDbName(table.getDbName());	partition.setTableName(table.getTableName());	StorageDescriptor partitionSd = new StorageDescriptor(table.getSd());	partitionSd.setLocation(table.getSd().getLocation() + Path.SEPARATOR + Warehouse.makePartName(table.getPartitionKeys(), newPartitionValues));	partition.setSd(partitionSd);	partition.setValues(newPartitionValues);	metaStoreClient.add_partition(partition);	} catch (AlreadyExistsException e) {	} catch (NoSuchObjectException e) {	
failed to create partition 

partition.setTableName(table.getTableName());	StorageDescriptor partitionSd = new StorageDescriptor(table.getSd());	partitionSd.setLocation(table.getSd().getLocation() + Path.SEPARATOR + Warehouse.makePartName(table.getPartitionKeys(), newPartitionValues));	partition.setSd(partitionSd);	partition.setValues(newPartitionValues);	metaStoreClient.add_partition(partition);	} catch (AlreadyExistsException e) {	} catch (NoSuchObjectException e) {	throw new PartitionCreationException("Table not found '" + databaseName + "." + tableName + "'.", e);	} catch (TException e) {	
failed to create partition 

========================= hive sample_960 =========================

if (retCols[i] == null) {	retCols[i] = cols[i];	}	retCols[i].set(ret);	}	}	forward(retCols);	return;	} catch (MalformedURLException e) {	if (!seenErrors) {	
the input is not a valid url string skipping such error messages in the future 

========================= hive sample_4886 =========================

ExprNodeDesc desc = ConstantPropagateProcFactory.foldExpr((ExprNodeGenericFuncDesc)wrapper.outExpr);	if (desc != null && desc instanceof ExprNodeConstantDesc && Boolean.TRUE.equals(((ExprNodeConstantDesc)desc).getValue())) {	owc.getOpToRemove().add(new PcrOpWalkerCtx.OpToDeleteInfo(pop, fop));	} else {	fop.getConf().setPredicate(wrapper.outExpr);	}	}	else if (wrapper.state != PcrExprProcFactory.WalkState.FALSE) {	fop.getConf().setPredicate(wrapper.outExpr);	} else {	
filter passes no row 

========================= hive sample_2952 =========================

public org.apache.hadoop.hive.ql.exec.FileSinkOperator.RecordWriter getHiveRecordWriter( final JobConf jobConf, final Path finalOutPath, final Class<? extends Writable> valueClass, final boolean isCompressed, final Properties tableProperties, final Progressable progress) throws IOException {	
creating new record writer 

========================= hive sample_3705 =========================

public void testUnlockWithTxn() throws Exception {	
starting testunlockwithtxn 

public void deadlockDetected() throws Exception {	
starting deadlock test 

try {	for (int i = 0; i < 5; i++) {	Thread t1 = new Thread() {	public void run() {	try {	try {	updateTxns(conn1);	updateLocks(conn1);	Thread.sleep(1000);	conn1.commit();	
no exception no deadlock 

public void run() {	try {	try {	updateTxns(conn1);	updateLocks(conn1);	Thread.sleep(1000);	conn1.commit();	} catch (SQLException e) {	try {	tHndlr.checkRetryable(conn1, e, "thread t1");	
got an exception but not a deadlock sqlstate is class of exception is msg is 

try {	try {	updateTxns(conn1);	updateLocks(conn1);	Thread.sleep(1000);	conn1.commit();	} catch (SQLException e) {	try {	tHndlr.checkRetryable(conn1, e, "thread t1");	} catch (TxnHandler.RetryException de) {	
forced a deadlock sqlstate is class of exception is msg is 

}	};	Thread t2 = new Thread() {	public void run() {	try {	try {	updateLocks(conn2);	updateTxns(conn2);	Thread.sleep(1000);	conn2.commit();	
no exception no deadlock 

public void run() {	try {	try {	updateLocks(conn2);	updateTxns(conn2);	Thread.sleep(1000);	conn2.commit();	} catch (SQLException e) {	try {	tHndlr.checkRetryable(conn2, e, "thread t2");	
got an exception but not a deadlock sqlstate is class of exception is msg is 

try {	try {	updateLocks(conn2);	updateTxns(conn2);	Thread.sleep(1000);	conn2.commit();	} catch (SQLException e) {	try {	tHndlr.checkRetryable(conn2, e, "thread t2");	} catch (TxnHandler.RetryException de) {	
forced a deadlock sqlstate is class of exception is msg is 

}	};	t2.setDaemon(true);	ErrorHandle ueh2 = new ErrorHandle();	t2.setUncaughtExceptionHandler(ueh2);	t1.start();	try {	Thread.sleep(1000);	}	catch(InterruptedException ex) {	
sleep was interrupted 

public void uncaughtException(Thread t, Throwable e) {	
uncaught exception from 

========================= hive sample_2790 =========================

public int executeInChildVM(DriverContext driverContext) {	try {	Context ctx = driverContext.getCtx();	String hiveJar = conf.getJar();	String hadoopExec = conf.getVar(HiveConf.ConfVars.HADOOPBIN);	conf.setVar(ConfVars.HIVEADDEDJARS, Utilities.getResourceFiles(conf, SessionState.ResourceType.JAR));	Path planPath = new Path(ctx.getLocalTmpPath(), "plan.xml");	MapredLocalWork plan = getWork();	
generating plan file 

String[] env;	Map<String, String> variables = new HashMap<String, String>(System.getenv());	int hadoopMem = conf.getIntVar(HiveConf.ConfVars.HIVEHADOOPMAXMEM);	if (hadoopMem == 0) {	variables.remove(HADOOP_MEM_KEY);	} else {	console.printInfo(" set heap size\t" + hadoopMem + "MB");	variables.put(HADOOP_MEM_KEY, String.valueOf(hadoopMem));	}	String endUserName = Utils.getUGI().getShortUserName();	
setting hadoop user name t 

}	}	env = new String[variables.size()];	int pos = 0;	for (Map.Entry<String, String> entry : variables.entrySet()) {	String name = entry.getKey();	String value = entry.getValue();	env[pos++] = name + "=" + value;	LOG.debug("Setting env: " + name + "=" + LogUtils.maskIfPassword(name, value));	}	
executing 

LogRedirector.redirect( Thread.currentThread().getName() + "-LocalTask-" + getName() + "-stderr", new LogRedirector(executor.getErrorStream(), LOG, callback));	CachingPrintStream errPrintStream = new CachingPrintStream(System.err);	StreamPrinter outPrinter = new StreamPrinter(executor.getInputStream(), null, System.out);	StreamPrinter errPrinter = new StreamPrinter(executor.getErrorStream(), null, errPrintStream);	outPrinter.start();	errPrinter.start();	int exitVal = jobExecHelper.progressLocal(executor, getId());	outPrinter.join();	errPrinter.join();	if (exitVal != 0) {	
execution failed with exit status 

outPrinter.start();	errPrinter.start();	int exitVal = jobExecHelper.progressLocal(executor, getId());	outPrinter.join();	errPrinter.join();	if (exitVal != 0) {	if (SessionState.get() != null) {	SessionState.get().addLocalMapRedErrors(getId(), errPrintStream.getOutput());	}	} else {	
execution completed successfully 

outPrinter.join();	errPrinter.join();	if (exitVal != 0) {	if (SessionState.get() != null) {	SessionState.get().addLocalMapRedErrors(getId(), errPrintStream.getOutput());	}	} else {	}	return exitVal;	} catch (Exception e) {	
exception 

========================= hive sample_3882 =========================

private LoadFileDesc(final Path sourcePath, final Path targetDir, final boolean isDfsDir, final String columns, final String columnTypes, AcidUtils.Operation writeType, boolean isMmCtas) {	super(sourcePath, writeType);	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
creating lfd from to 

========================= hive sample_3181 =========================

switch (hType) {	case BOOLEAN: if (!pigHasBooleanSupport) {	throw new PigException("Incompatible type found in HCat table schema: " + hcatField, PigHCatUtil.PIG_EXCEPTION_CODE);	}	break;	case ARRAY: validateHCatSchemaFollowsPigRules(hcatField.getArrayElementSchema());	break;	case STRUCT: validateHCatSchemaFollowsPigRules(hcatField.getStructSubSchema());	break;	case MAP: if (hcatField.getMapKeyType() != Type.STRING) {	
converting non string key of map from to string 

========================= hive sample_1016 =========================

}	String actualDbName = context.isDbNameEmpty() ? uks.get(0).getTable_db() : context.dbName;	String actualTblName = context.isTableNameEmpty() ? uks.get(0).getTable_name() : context.tableName;	for (SQLUniqueConstraint uk : uks) {	uk.setTable_db(actualDbName);	uk.setTable_name(actualTblName);	}	AlterTableDesc addConstraintsDesc = new AlterTableDesc(actualDbName + "." + actualTblName, new ArrayList<SQLPrimaryKey>(), new ArrayList<SQLForeignKey>(), uks, context.eventOnlyReplicationSpec());	Task<DDLWork> addConstraintsTask = TaskFactory.get(new DDLWork(readEntitySet, writeEntitySet, addConstraintsDesc), context.hiveConf);	tasks.add(addConstraintsTask);	
added add constrains task 

========================= hive sample_3494 =========================

public void run() {	TxnStore.MutexAPI.LockHandle handle = null;	try {	handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.HouseKeeper.name());	long startTime = System.currentTimeMillis();	txnHandler.performTimeOuts();	
timeout reaper ran for seconds 

public void run() {	TxnStore.MutexAPI.LockHandle handle = null;	try {	handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.HouseKeeper.name());	long startTime = System.currentTimeMillis();	txnHandler.performTimeOuts();	} catch(Throwable t) {	
serious error in 

========================= hive sample_1850 =========================

public static byte[] serializeJobConf(JobConf jobConf) {	ByteArrayOutputStream out = new ByteArrayOutputStream();	try {	jobConf.write(new DataOutputStream(out));	} catch (IOException e) {	
error serializing job configuration 

public static byte[] serializeJobConf(JobConf jobConf) {	ByteArrayOutputStream out = new ByteArrayOutputStream();	try {	jobConf.write(new DataOutputStream(out));	} catch (IOException e) {	return null;	} finally {	try {	out.close();	} catch (IOException e) {	
error closing output stream 

========================= hive sample_4582 =========================

if (obChild.getRowType().getFieldCount() <= resultSchema.size()) {	return;	}	RelDataType rt = obChild.getRowType();	Set<Integer> collationInputRefs = new HashSet( RelCollations.ordinals(obRel.getCollation()));	ImmutableMap.Builder<Integer, RexNode> inputRefToCallMapBldr = ImmutableMap.builder();	for (int i = resultSchema.size(); i < rt.getFieldCount(); i++) {	if (collationInputRefs.contains(i)) {	RexNode obyExpr = obChild.getChildExps().get(i);	if (obyExpr instanceof RexCall) {	
old rexcall 

return;	}	RelDataType rt = obChild.getRowType();	Set<Integer> collationInputRefs = new HashSet( RelCollations.ordinals(obRel.getCollation()));	ImmutableMap.Builder<Integer, RexNode> inputRefToCallMapBldr = ImmutableMap.builder();	for (int i = resultSchema.size(); i < rt.getFieldCount(); i++) {	if (collationInputRefs.contains(i)) {	RexNode obyExpr = obChild.getChildExps().get(i);	if (obyExpr instanceof RexCall) {	obyExpr = adjustOBSchema((RexCall) obyExpr, obChild, resultSchema);	
new rexcall 

========================= hive sample_2840 =========================

public LlapStatusOptions parseOptions(String[] args) throws LlapStatusCliException {	LlapStatusOptionsProcessor optionsProcessor = new LlapStatusOptionsProcessor();	LlapStatusOptions options;	try {	options = optionsProcessor.processOptions(args);	return options;	} catch (Exception e) {	
failed to parse arguments 

} else {	appName = null;	}	}	if (StringUtils.isEmpty(appName)) {	String message = "Invalid app name. This must be setup via config or passed in as a parameter." + " This tool works with clusters deployed by Slider/YARN";	LOG.info(message);	return ExitCode.INCORRECT_USAGE.getInt();	}	if (LOG.isDebugEnabled()) {	
using appname 

try {	ret = populateAppStatusFromLlapRegistry(appStatusBuilder, watchTimeoutMs);	} catch (LlapStatusCliException e) {	logError(e);	return e.getExitCode().getInt();	}	}	return ret.getInt();	} finally {	if (LOG.isDebugEnabled()) {	
final appstate 

public void outputJson(PrintWriter writer) throws LlapStatusCliException {	ObjectMapper mapper = new ObjectMapper();	mapper.configure(SerializationConfig.Feature.FAIL_ON_EMPTY_BEANS, false);	mapper.setSerializationInclusion(JsonSerialize.Inclusion.NON_NULL);	mapper.setSerializationInclusion(JsonSerialize.Inclusion.NON_EMPTY);	try {	writer.println(mapper.writerWithDefaultPrettyPrinter().writeValueAsString(appStatusBuilder));	} catch (IOException e) {	
failed to create json 

private ExitCode processAppReport(ApplicationReport appReport, AppStatusBuilder appStatusBuilder) throws LlapStatusCliException {	if (appReport == null) {	appStatusBuilder.setState(State.APP_NOT_FOUND);	
no application found 

case ACCEPTED: appStatusBuilder.maybeCreateAndGetAmInfo().setAppId(appReport.getApplicationId().toString());	appStatusBuilder.setState(State.LAUNCHING);	return ExitCode.SUCCESS;	case RUNNING: appStatusBuilder.maybeCreateAndGetAmInfo().setAppId(appReport.getApplicationId().toString());	return ExitCode.SUCCESS;	case FINISHED: case FAILED: case KILLED: appStatusBuilder.maybeCreateAndGetAmInfo().setAppId(appReport.getApplicationId().toString());	appStatusBuilder.setAppFinishTime(appReport.getFinishTime());	appStatusBuilder.setState(State.COMPLETE);	ApplicationDiagnostics appDiagnostics = LlapSliderUtils.getApplicationDiagnosticsFromYarnDiagnostics( appReport, appStatusBuilder, LOG);	if (appDiagnostics == null) {	
appdiagnostics not available for yarn application report 

private ExitCode populateAppStatusFromSliderStatus(String appName, SliderClient sliderClient, AppStatusBuilder appStatusBuilder) throws LlapStatusCliException {	ClusterDescription clusterDescription;	try {	clusterDescription = sliderClient.getClusterDescription(appName);	} catch (SliderException e) {	throw new LlapStatusCliException(ExitCode.SLIDER_CLIENT_ERROR_OTHER, "Failed to get cluster description from slider. SliderErrorCode=" + (e).getExitCode(), e);	} catch (Exception e) {	throw new LlapStatusCliException(ExitCode.SLIDER_CLIENT_ERROR_OTHER, "Failed to get cluster description from slider", e);	}	if (clusterDescription == null) {	
slider clusterdescription not available 

ApplicationDiagnostics appDiagnostics;	try {	ActionDiagnosticArgs args = new ActionDiagnosticArgs();	args.containers = true;	args.name = appName;	appDiagnostics = sliderClient.actionDiagnosticContainers(args);	} catch (YarnException | IOException | URISyntaxException e) {	throw new LlapStatusCliException( ExitCode.SLIDER_CLIENT_ERROR_OTHER, "Failed to get container diagnostics from slider", e);	}	if (appDiagnostics == null) {	
slider container diagnostics not available 

}	}	Collection<LlapServiceInstance> serviceInstances;	try {	serviceInstances = llapRegistry.getInstances(watchTimeoutMs).getAll();	} catch (Exception e) {	throw new LlapStatusCliException(ExitCode.LLAP_REGISTRY_ERROR, "Failed to get instances from llap registry", e);	}	if (serviceInstances == null || serviceInstances.isEmpty()) {	if (LOG.isDebugEnabled()) {	
no information found in the llap registry 

validatedInstances.add(llapInstance);	} else {	llapExtraInstances.add(containerIdString);	}	}	appStatusBuilder.setLiveInstances(validatedInstances.size());	appStatusBuilder.setLaunchingInstances(llapExtraInstances.size());	if (validatedInstances.size() >= appStatusBuilder.getDesiredInstances()) {	appStatusBuilder.setState(State.RUNNING_ALL);	if (validatedInstances.size() > appStatusBuilder.getDesiredInstances()) {	
found more entries in llap registry as compared to desired entries 

if (validatedInstances.size() > appStatusBuilder.getDesiredInstances()) {	}	} else {	if (validatedInstances.size() > 0) {	appStatusBuilder.setState(State.RUNNING_PARTIAL);	} else {	appStatusBuilder.setState(State.LAUNCHING);	}	}	if (appStatusBuilder.allRunningInstances().size() > 0) {	
potential instances starting up 

} else {	if (validatedInstances.size() > 0) {	appStatusBuilder.setState(State.RUNNING_PARTIAL);	} else {	appStatusBuilder.setState(State.LAUNCHING);	}	}	if (appStatusBuilder.allRunningInstances().size() > 0) {	}	if (llapExtraInstances.size() > 0) {	
instances likely to shutdown soon 

instance .setYarnContainerExitStatus(containerInformation.getExitCode());	}	instance.setDiagnostics(containerInformation.getDiagnostics());	appStatusBuilder.addNewCompleteLlapInstance(instance);	} else {	LOG.warn("Unexpected containerstate={}, for container={}", containerInformation.getState(), containerInformation);	}	}	} else {	if (LOG.isDebugEnabled()) {	
containerinfos is null 

final boolean watchMode = options.isWatchMode();	final long watchTimeout = options.getWatchTimeoutMs();	long numAttempts = watchTimeout / refreshInterval;	numAttempts = watchMode ? numAttempts : 1;	LlapStatusHelpers.State launchingState = null;	LlapStatusHelpers.State currentState = null;	boolean desiredStateAttained = false;	final float runningNodesThreshold = options.getRunningNodesThreshold();	try (OutputStream os = options.getOutputFile() == null ? System.out : new BufferedOutputStream(new FileOutputStream(options.getOutputFile()));	PrintWriter pw = new PrintWriter(os)) {	
configured refresh interval s watch timeout s attempts remaining watch mode running nodes threshold 

break;	}	} else {	firstAttempt = false;	}	ret = statusServiceDriver.run(options, watchMode ? watchTimeout : 0);	currentState = statusServiceDriver.appStatusBuilder.getState();	try {	lastSummaryLogTime = LlapStatusServiceDriver .maybeLogSummary(clock, lastSummaryLogTime, statusServiceDriver, watchMode, watchTimeout, launchingState);	} catch (Exception e) {	
failed to log summary 

lastSummaryLogTime = LlapStatusServiceDriver .maybeLogSummary(clock, lastSummaryLogTime, statusServiceDriver, watchMode, watchTimeout, launchingState);	} catch (Exception e) {	}	if (ret == ExitCode.SUCCESS.getInt()) {	if (watchMode) {	if (launchingState == null && LAUNCHING_STATES.contains(currentState)) {	launchingState = currentState;	}	if (currentState.equals(State.COMPLETE)) {	if (launchingState != null || options.isLaunched()) {	
complete state reached while waiting for running state failing 

if (ret == ExitCode.SUCCESS.getInt()) {	if (watchMode) {	if (launchingState == null && LAUNCHING_STATES.contains(currentState)) {	launchingState = currentState;	}	if (currentState.equals(State.COMPLETE)) {	if (launchingState != null || options.isLaunched()) {	System.err.println("Final diagnostics: " + statusServiceDriver.appStatusBuilder.getDiagnostics());	break;	} else {	
found a stopped application assuming it was a previous attempt and waiting for the next one omit the l flag to avoid this 

}	numAttempts--;	continue;	}	final int liveInstances = statusServiceDriver.appStatusBuilder.getLiveInstances();	final int desiredInstances = statusServiceDriver.appStatusBuilder.getDesiredInstances();	if (desiredInstances > 0) {	final float ratio = (float) liveInstances / (float) desiredInstances;	if (ratio < runningNodesThreshold) {	if (LOG.isDebugEnabled()) {	
waiting until running nodes threshold is reached current desired instances 

} else {	desiredStateAttained = true;	statusServiceDriver.appStatusBuilder.setRunningThresholdAchieved(true);	}	} else {	numAttempts--;	continue;	}	}	} else if (ret == ExitCode.YARN_ERROR.getInt() && watchMode) {	
watch mode enabled and got yarn error retrying 

}	} else {	numAttempts--;	continue;	}	}	} else if (ret == ExitCode.YARN_ERROR.getInt() && watchMode) {	numAttempts--;	continue;	} else if (ret == ExitCode.SLIDER_CLIENT_ERROR_CREATE_FAILED.getInt() && watchMode) {	
watch mode enabled and slider client creation failed retrying 

continue;	}	}	} else if (ret == ExitCode.YARN_ERROR.getInt() && watchMode) {	numAttempts--;	continue;	} else if (ret == ExitCode.SLIDER_CLIENT_ERROR_CREATE_FAILED.getInt() && watchMode) {	numAttempts--;	continue;	} else if (ret == ExitCode.SLIDER_CLIENT_ERROR_OTHER.getInt() && watchMode) {	
watch mode enabled and got slider client error retrying 

} else if (ret == ExitCode.YARN_ERROR.getInt() && watchMode) {	numAttempts--;	continue;	} else if (ret == ExitCode.SLIDER_CLIENT_ERROR_CREATE_FAILED.getInt() && watchMode) {	numAttempts--;	continue;	} else if (ret == ExitCode.SLIDER_CLIENT_ERROR_OTHER.getInt() && watchMode) {	numAttempts--;	continue;	} else if (ret == ExitCode.LLAP_REGISTRY_ERROR.getInt() && watchMode) {	
watch mode enabled and got llap registry error retrying 

continue;	}	break;	}	LlapStatusServiceDriver .maybeLogSummary(clock, 0L, statusServiceDriver, watchMode, watchTimeout, launchingState);	CONSOLE_LOGGER.info("\n\n\n");	statusServiceDriver.outputJson(pw);	os.flush();	pw.flush();	if (numAttempts == 0 && watchMode && !desiredStateAttained) {	
watch timeout s exhausted before desired state running is attained 

}	} catch (Throwable t) {	logError(t);	if (t instanceof LlapStatusCliException) {	LlapStatusCliException ce = (LlapStatusCliException) t;	ret = ce.getExitCode().getInt();	} else {	ret = ExitCode.INTERNAL_ERROR.getInt();	}	} finally {	
llap status finished 

if (t instanceof LlapStatusCliException) {	LlapStatusCliException ce = (LlapStatusCliException) t;	ret = ce.getExitCode().getInt();	} else {	ret = ExitCode.INTERNAL_ERROR.getInt();	}	} finally {	statusServiceDriver.close();	}	if (LOG.isDebugEnabled()) {	
completed processing exiting with 

========================= hive sample_2212 =========================

public SecureCmdDoAs(HiveConf conf) throws HiveException, IOException{	String uname = UserGroupInformation.getLoginUser().getShortUserName();	FileSystem fs = FileSystem.get(conf);	Credentials cred = new Credentials();	ShimLoader.getHadoopShims().addDelegationTokens(fs, cred, uname);	for (String uri : conf.getStringCollection("mapreduce.job.hdfs-servers")) {	try {	ShimLoader.getHadoopShims().addDelegationTokens( FileSystem.get(new URI(uri), conf), cred, uname);	} catch (URISyntaxException e) {	
invalid uri in mapreduce job hdfs servers ignoring 

========================= hive sample_3950 =========================

public void run(HookContext hookContext) throws Exception {	assert (hookContext.getHookType() == HookContext.HookType.POST_EXEC_HOOK);	HiveConf conf = hookContext.getConf();	if (!"tez".equals(HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE))) {	return;	}	
executing post execution hook to print orc row groups read counter 

return;	}	SessionState ss = SessionState.get();	SessionState.LogHelper console = ss.getConsole();	QueryPlan plan = hookContext.getQueryPlan();	if (plan == null) {	return;	}	List<TezTask> rootTasks = Utilities.getTezTasks(plan.getRootTasks());	for (TezTask tezTask : rootTasks) {	
printing orc row group counter for tez task 

========================= hive sample_3815 =========================

public Long getNumCounter() {	try {	return (Long) metrics.get(numCounter);	} catch (JMException e) {	
could not find counter value for returning null instead 

public Long getTimeCounter() {	try {	return (Long) metrics.get(timeCounter);	} catch (JMException e) {	
could not find timer value for returning null instead 

public void open() {	if (!isOpen) {	isOpen = true;	startTime = System.currentTimeMillis();	} else {	
scope named is not closed cannot be opened 

if (isOpen) {	Long endTime = System.currentTimeMillis();	synchronized(metrics) {	Long num = metrics.incrementCounter(numCounter);	Long time = metrics.incrementCounter(timeCounter, endTime - startTime);	if (num != null && time != null) {	metrics.set(avgTimeCounter, Double.valueOf(time.doubleValue() / num.doubleValue()));	}	}	} else {	
scope named is not open cannot be closed 

Long value = null;	synchronized(metrics) {	if (!metrics.hasKey(name)) {	value = Long.valueOf(increment);	set(name, value);	} else {	try {	value = ((Long)get(name)) + increment;	set(name, value);	} catch (JMException e) {	
could not find counter value for increment operation skipped 

Long value = null;	synchronized(metrics) {	if (!metrics.hasKey(name)) {	value = Long.valueOf(decrement);	set(name, -value);	} else {	try {	value = ((Long)get(name)) - decrement;	set(name, value);	} catch (JMException e) {	
could not find counter value for decrement operation skipped 

========================= hive sample_1347 =========================

MapJoinDesc mapJoinDesc = mapJoinOp.getConf();	mapJoinDesc.resetOrder();	HiveConf conf = context.getParseCtx().getConf();	float hashtableMemoryUsage;	if (context.isFollowedByGroupBy()) {	hashtableMemoryUsage = conf.getFloatVar( HiveConf.ConfVars.HIVEHASHTABLEFOLLOWBYGBYMAXMEMORYUSAGE);	} else {	hashtableMemoryUsage = conf.getFloatVar( HiveConf.ConfVars.HIVEHASHTABLEMAXMEMORYUSAGE);	}	mapJoinDesc.setHashTableMemoryUsage(hashtableMemoryUsage);	
setting max memory usage to for table sink not followed by group by 

========================= hive sample_2992 =========================

public void cleanupNotifs() throws Exception {	Database db = new Database("cleanup1", "no description", "file:/tmp", emptyParameters);	msClient.createDatabase(db);	msClient.dropDatabase("cleanup1");	
pulling events immediately after createdatabase dropdatabase 

public void cleanupNotifs() throws Exception {	Database db = new Database("cleanup1", "no description", "file:/tmp", emptyParameters);	msClient.createDatabase(db);	msClient.dropDatabase("cleanup1");	NotificationEventResponse rsp = msClient.getNextNotification(firstEventId, 0, null);	assertEquals(2, rsp.getEventsSize());	Thread.sleep(EVENTS_TTL * 2 * 1000);	
pulling events again after cleanup 

public void cleanupNotifs() throws Exception {	Database db = new Database("cleanup1", "no description", "file:/tmp", emptyParameters);	msClient.createDatabase(db);	msClient.dropDatabase("cleanup1");	NotificationEventResponse rsp = msClient.getNextNotification(firstEventId, 0, null);	assertEquals(2, rsp.getEventsSize());	Thread.sleep(EVENTS_TTL * 2 * 1000);	NotificationEventResponse rsp2 = msClient.getNextNotification(firstEventId, 0, null);	
second trigger done 

========================= hive sample_220 =========================

public ExecutionContextProvider build(Context context, String workingDirectory) throws Exception {	String privateKey = Preconditions.checkNotNull(context.getString(PRIVATE_KEY), PRIVATE_KEY + " is required");	Set<Host> hosts = Sets.newHashSet();	for(String alias : Splitter.on(" ").omitEmptyStrings().split(context.getString("hosts", ""))) {	Context hostContext = new Context(context.getSubProperties( Joiner.on(".").join("host", alias, "")));	
processing host 

========================= hive sample_5580 =========================

Task<? extends Serializable> task = (Task<? extends Serializable>) nd;	if (!task.isMapRedTask() || task instanceof ConditionalTask || ((MapredWork) task.getWork()).getReduceWork() == null) {	return null;	}	ParseContext pc = physicalContext.getParseContext();	if (pc.getLoadTableWork() != null) {	for (LoadTableDesc ltd : pc.getLoadTableWork()) {	if (!ltd.isMmTable()) {	continue;	}	
not using skew join because the destination table is an insert only table 

continue;	}	return null;	}	}	if (pc.getLoadFileWork() != null) {	for (LoadFileDesc lfd : pc.getLoadFileWork()) {	if (!lfd.isMmCtas()) {	continue;	}	
not using skew join because the destination table is an insert only table 

========================= hive sample_3004 =========================

public void handle(Context withinContext) throws Exception {	
processing add foreignkey message message 

========================= hive sample_3454 =========================

public static int calculateTableSize( float keyCountAdj, int threshold, float loadFactor, long keyCount) {	if (keyCount >= 0 && keyCountAdj != 0) {	threshold = (int)Math.ceil(keyCount / (keyCountAdj * loadFactor));	}	
key count from statistics is setting map size to 

========================= hive sample_4057 =========================

Deserializer deserializer = ReflectionUtil.newInstance(conf.getClassByName(lib). asSubclass(Deserializer.class), conf);	if (skipConfError) {	SerDeUtils.initializeSerDeWithoutErrorCheck(deserializer, conf, MetaStoreUtils.getTableMetadata(table), null);	} else {	SerDeUtils.initializeSerDe(deserializer, conf, MetaStoreUtils.getTableMetadata(table), null);	}	return deserializer;	} catch (RuntimeException e) {	throw e;	} catch (Throwable e) {	
error in initserde 

static public Deserializer getDeserializer(Configuration conf, org.apache.hadoop.hive.metastore.api.Partition part, org.apache.hadoop.hive.metastore.api.Table table) throws MetaException {	String lib = part.getSd().getSerdeInfo().getSerializationLib();	try {	Deserializer deserializer = ReflectionUtil.newInstance(conf.getClassByName(lib). asSubclass(Deserializer.class), conf);	SerDeUtils.initializeSerDe(deserializer, conf, MetaStoreUtils.getTableMetadata(table), MetaStoreUtils.getPartitionMetadata(part, table));	return deserializer;	} catch (RuntimeException e) {	throw e;	} catch (Throwable e) {	
error in initserde 

========================= hive sample_5646 =========================

public void setUp() throws Exception {	System.setProperty(DebugUtils.PROPERTY_DONT_LOG_CONNECTION_ISSUES, "true");	while (server == null) {	try {	server = new TestingServer();	} catch (BindException e) {	
getting bind exception retrying to allocate server 

for (int i = 0; i < nodeCount; ++i) {	if (isFallback) {	curator.create().creatingParentsIfNeeded().forPath(PATH + "/worker-" + i);	}	final int ix = i;	startTasks[i] = new FutureTask<SlotZnode>(new Callable<SlotZnode>() {	SlotZnode node = createZnode(curator);	public SlotZnode call() throws Exception {	syncThreadStart(cdlIn, cdlOut);	int id = System.identityHashCode(node);	
starting the node from task 

if (isFallback) {	curator.create().creatingParentsIfNeeded().forPath(PATH + "/worker-" + i);	}	final int ix = i;	startTasks[i] = new FutureTask<SlotZnode>(new Callable<SlotZnode>() {	SlotZnode node = createZnode(curator);	public SlotZnode call() throws Exception {	syncThreadStart(cdlIn, cdlOut);	int id = System.identityHashCode(node);	boolean result = node.start(30, TimeUnit.SECONDS);	
Started failed to start the node from task 

for (int i = 0; i < startTasks.length; ++i) {	SlotZnode node = startTasks[i].get();	assertNotNull("failed to start the node from task #" + i, node);	totalFallbackCount += node.getFallbackCount();	int slot = node.getCurrentSlot();	assertTrue(slot < found.length);	assertFalse(found[slot]);	found[slot] = true;	}	if (isFallback) {	
total fallback count 

========================= hive sample_627 =========================

try {	resultList = client.executeCustomQuery(query);	} catch (NamingException e) {	throw new AuthenticationException("LDAP Authentication failed for user", e);	}	if (resultList != null) {	for (String matchedDn : resultList) {	String shortUserName = LdapUtils.getShortName(matchedDn);	LOG.info("<queried user=" + shortUserName + ",user=" + user + ">");	if (shortUserName.equalsIgnoreCase(user) || matchedDn.equalsIgnoreCase(user)) {	
authentication succeeded based on result set from ldap query 

}	if (resultList != null) {	for (String matchedDn : resultList) {	String shortUserName = LdapUtils.getShortName(matchedDn);	LOG.info("<queried user=" + shortUserName + ",user=" + user + ">");	if (shortUserName.equalsIgnoreCase(user) || matchedDn.equalsIgnoreCase(user)) {	return;	}	}	}	
authentication failed based on result set from custom ldap query 

========================= hive sample_2316 =========================

}	mHostLog.seek(0);	mHostLog.setLength(0);	Thread thread = new Thread() {	public void run() {	while (true) {	try {	TimeUnit.MINUTES.sleep(60);	performBackgroundWork();	} catch (Exception e) {	
unexpected error in background worker 

Set<String> hostsToTerminate = Sets.newHashSet();	Set<Host> hostsNotRemoved = Sets.newHashSet();	for(Host host : executionContext.getBadHosts()) {	hostsToTerminate.add(host.getName());	if(!executionContext.removeHost(host)) {	hostsNotRemoved.add(host);	}	}	executionContext.clearBadHosts();	if(!hostsToTerminate.isEmpty()) {	
replacing 

Set<NodeMetadata> result = Sets.newHashSet();	int attempts = 0;	int numRequired = numHosts;	try {	TimeUnit.SECONDS.sleep(mRetrySleepInterval);	} catch (InterruptedException e) {	Thread.currentThread().interrupt();	}	do {	boolean error = false;	
attempting to create nodes 

for (NodeMetadata node : result) {	newAddresses.addAll(node.getPublicAddresses());	}	synchronized (mTerminatedHosts) {	for (String newAddress : newAddresses) {	mTerminatedHosts.remove(newAddress);	}	}	} catch (RunNodesException e) {	error = true;	
error creating nodes 

}	} catch (RunNodesException e) {	error = true;	terminateInternal(e.getNodeErrors().keySet());	result.addAll(e.getSuccessfulNodes());	}	result = verifyHosts(result);	for (NodeMetadata node : result) {	mLiveHosts.put(publicIpOrHostname(node), System.currentTimeMillis());	}	
successfully created nodes 

result = verifyHosts(result);	for (NodeMetadata node : result) {	mLiveHosts.put(publicIpOrHostname(node), System.currentTimeMillis());	}	numRequired = numHosts - result.size();	if (numRequired > 0) {	long sleepTime = mRetrySleepInterval;	if (error) {	sleepTime *= ++attempts;	}	
pausing creation process for seconds 

public void close() {	
shutting down terminationexecutor 

public void close() {	mTerminationExecutor.shutdown();	
closing cloudcomputeservice 

persistHostnamesToLog(hosts);	ExecutorService executorService = Executors.newFixedThreadPool(Math.min(hosts.size(), 25));	try {	for(final NodeMetadata node : hosts) {	executorService.submit(new Runnable() {	public void run() {	String ip = publicIpOrHostname(node);	SSHCommand command = new SSHCommand(mSSHCommandExecutor, mPrivateKey, mUser, ip, 0, "pkill -f java", true);	mSSHCommandExecutor.execute(command);	if(command.getExitCode() == Constants.EXIT_CODE_UNKNOWN || command.getException() != null) {	
node is bad on startup 

if(command.getExitCode() == Constants.EXIT_CODE_UNKNOWN || command.getException() != null) {	terminateInternal(node);	} else {	result.add(node);	}	}	});	}	executorService.shutdown();	if(!executorService.awaitTermination(10, TimeUnit.MINUTES)) {	
verify command still executing on a host after minutes 

private synchronized void performBackgroundWork() {	
performing background work 

private synchronized void performBackgroundWork() {	Map<String, Long> terminatedHosts = Maps.newHashMap();	synchronized (mTerminatedHosts) {	terminatedHosts.putAll(mTerminatedHosts);	}	
currently tracked terminated hosts 

private synchronized void performBackgroundWork() {	Map<String, Long> terminatedHosts = Maps.newHashMap();	synchronized (mTerminatedHosts) {	terminatedHosts.putAll(mTerminatedHosts);	}	for (NodeMetadata node : getRunningNodes()) {	String ip = publicIpOrHostname(node);	if (terminatedHosts.containsKey(ip)) {	terminateInternal(node);	
found zombie node previously terminated at 

private synchronized void performBackgroundWork() {	Map<String, Long> terminatedHosts = Maps.newHashMap();	synchronized (mTerminatedHosts) {	terminatedHosts.putAll(mTerminatedHosts);	}	for (NodeMetadata node : getRunningNodes()) {	String ip = publicIpOrHostname(node);	if (terminatedHosts.containsKey(ip)) {	terminateInternal(node);	} else if(!mLiveHosts.containsKey(ip)) {	
found zombie node previously unknown to ptest 

private void terminateInternal(final NodeMetadata node) {	
submitting termination for 

mLiveHosts.remove(publicIpOrHostname(node));	mTerminationExecutor.submit(new Runnable() {	public void run() {	try {	TimeUnit.SECONDS.sleep(mRetrySleepInterval);	} catch (InterruptedException e) {	Thread.currentThread().interrupt();	}	try {	String ip = publicIpOrHostname(node);	
terminating 

} catch (InterruptedException e) {	Thread.currentThread().interrupt();	}	try {	String ip = publicIpOrHostname(node);	if (!mTerminatedHosts.containsKey(ip)) {	mTerminatedHosts.put(ip, System.currentTimeMillis());	}	mCloudComputeService.destroyNode(node.getId());	} catch (Exception e) {	
error attempting to terminate host 

private void terminate(Set<String> hosts, boolean warnIfHostsNotFound) {	
requesting termination of 

private void terminate(Set<String> hosts, boolean warnIfHostsNotFound) {	Set<NodeMetadata> nodesToTerminate = Sets.newHashSet();	for (NodeMetadata node : getRunningNodes()) {	String ip = publicIpOrHostname(node);	if (hosts.contains(ip)) {	nodesToTerminate.add(node);	}	}	terminateInternal(nodesToTerminate);	if (warnIfHostsNotFound && nodesToTerminate.size() != hosts.size()) {	
requested termination of but found only 

========================= hive sample_5579 =========================

tmpPath = tp;	if (isMmTable) {	taskTmpPath = null;	outPath = finalPath = new Path(tmpPath, taskId + ".merged");	} else {	taskTmpPath = ttp;	finalPath = new Path(tp, taskId);	outPath = new Path(ttp, Utilities.toTempPath(taskId));	}	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
paths for merge tmp task final out 

dpPath = inputPath;	Path newPath = new Path(".");	while (inputPath != null && depthDiff > 0) {	newPath = new Path(inputPath.getName(), newPath);	depthDiff--;	inputPath = inputPath.getParent();	}	Path newTmpPath = new Path(tmpPath, newPath);	if (!fs.exists(newTmpPath)) {	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
creating 

protected void fixTmpPath(Path path) throws IOException {	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
calling fixtmppath with 

fs.delete(outPath, true);	}	return;	}	if (fs.exists(outPath)) {	FileStatus fss = fs.getFileStatus(outPath);	if (!isMmTable) {	if (!fs.rename(outPath, finalPath)) {	throw new IOException("Unable to rename " + outPath + " to " + finalPath);	}	
renamed path to bytes 

}	if (fs.exists(outPath)) {	FileStatus fss = fs.getFileStatus(outPath);	if (!isMmTable) {	if (!fs.rename(outPath, finalPath)) {	throw new IOException("Unable to rename " + outPath + " to " + finalPath);	}	} else {	assert finalPath.equals(outPath);	Utilities.writeMmCommitManifest(Lists.newArrayList(outPath), tmpPath.getParent(), fs, taskId, conf.getTxnId(), conf.getStmtId(), null, false);	
merged into bytes 

Path destDir = finalPath.getParent();	Path destPath = destDir;	if (incompatFileSet != null && !incompatFileSet.isEmpty()) {	for (Path incompatFile : incompatFileSet) {	if (!Utilities.isHiveManagedFile(incompatFile)) {	final String taskId = Utilities.getTaskId(jc);	Path destFilePath = new Path(destDir, new Path(taskId));	for (int counter = 1; fs.exists(destFilePath); counter++) {	destFilePath = new Path(destDir, taskId + (Utilities.COPY_KEYWORD + counter));	}	
path doesn t conform to hive s expectation renaming to 

if (!Utilities.isHiveManagedFile(incompatFile)) {	final String taskId = Utilities.getTaskId(jc);	Path destFilePath = new Path(destDir, new Path(taskId));	for (int counter = 1; fs.exists(destFilePath); counter++) {	destFilePath = new Path(destDir, taskId + (Utilities.COPY_KEYWORD + counter));	}	destPath = destFilePath;	}	try {	Utilities.renameOrMoveFiles(fs, incompatFile, destPath);	
moved incompatible file to 

final String taskId = Utilities.getTaskId(jc);	Path destFilePath = new Path(destDir, new Path(taskId));	for (int counter = 1; fs.exists(destFilePath); counter++) {	destFilePath = new Path(destDir, taskId + (Utilities.COPY_KEYWORD + counter));	}	destPath = destFilePath;	}	try {	Utilities.renameOrMoveFiles(fs, incompatFile, destPath);	} catch (HiveException e) {	
unable to move to 

public void jobCloseOp(Configuration hconf, boolean success) throws HiveException {	try {	Path outputDir = conf.getOutputPath();	FileSystem fs = outputDir.getFileSystem(hconf);	Long mmWriteId = conf.getTxnId();	int stmtId = conf.getStmtId();	if (!isMmTable) {	Path backupPath = backupOutputPath(fs, outputDir);	Utilities.mvFileToFinalPath( outputDir, hconf, success, LOG, conf.getDpCtx(), null, reporter);	if (success) {	
jobcloseop moved merged files to output dir 

========================= hive sample_4104 =========================

jc = job;	MapredContext.init(false, new JobConf(jc));	MapredContext.get().setReporter(reporter);	oc = output;	rp = reporter;	LOG.info("maximum memory = " + memoryMXBean.getHeapMemoryUsage().getMax());	try {	LOG.info("conf classpath = " + Arrays.asList(((URLClassLoader) job.getClassLoader()).getURLs()));	LOG.info("thread classpath = " + Arrays.asList(((URLClassLoader) Thread.currentThread() .getContextClassLoader()).getURLs()));	} catch (Exception e) {	
cannot get classpath 

========================= hive sample_4554 =========================

private <T extends LlapBufferOrBuffers> boolean lockOldVal(Object key, T newVal, T oldVal) {	if (LlapIoImpl.CACHE_LOGGER.isTraceEnabled()) {	
trying to cache when metadata is already cached for old new 

private <T extends LlapBufferOrBuffers> boolean lockOldVal(Object key, T newVal, T oldVal) {	if (LlapIoImpl.CACHE_LOGGER.isTraceEnabled()) {	}	if (LlapIoImpl.LOCKING_LOGGER.isTraceEnabled()) {	
locking due to cache collision 

private <T extends LlapBufferOrBuffers> boolean lockOldVal(Object key, T newVal, T oldVal) {	if (LlapIoImpl.CACHE_LOGGER.isTraceEnabled()) {	}	if (LlapIoImpl.LOCKING_LOGGER.isTraceEnabled()) {	}	if (lockBuffer(oldVal, true)) {	if (LlapIoImpl.LOCKING_LOGGER.isTraceEnabled()) {	
unlocking due to cache collision with 

========================= hive sample_2204 =========================

public VerifyingObjectStore() {	super();	
is being used test run 

private void verifyObjects( Object sqlResult, Object jdoResult, Class<?> clazz) throws MetaException {	if (EqualsBuilder.reflectionEquals(sqlResult, jdoResult)) return;	StringBuilder errorStr = new StringBuilder("Objects are different: \n");	try {	dumpObject(errorStr, "SQL", sqlResult, clazz, 0);	errorStr.append("\n");	dumpObject(errorStr, "ORM", jdoResult, clazz, 0);	} catch (Throwable t) {	errorStr.append("Error getting the diff: " + t);	}	
different results 

errorStr.append("\n\n" + msg);	LOG.error(msg, t);	break;	}	if (++errors == MAX_DIFFS) {	errorStr.append("\n\nToo many diffs, giving up (lists might be sorted differently)");	break;	}	}	if (errorStr.length() > 0) {	
different results 

========================= hive sample_1582 =========================

private void deleteDeltaIfExists(Path partitionPath, long transactionId, int bucketId) throws IOException {	Path deltaPath = AcidUtils.createFilename(partitionPath, new AcidOutputFormat.Options(configuration) .bucket(bucketId) .minimumTransactionId(transactionId) .maximumTransactionId(transactionId));	FileSystem fileSystem = deltaPath.getFileSystem(configuration);	if (fileSystem.exists(deltaPath)) {	
deleting existing delta path 

========================= hive sample_959 =========================

logs = new ProfileLogs(profile);	profiles.put(profile, logs);	}	logs.dirs.add(logDir);	}	}	for(String profile : profiles.keySet()) {	ProfileLogs logs = profiles.get(profile);	if(logs.dirs.size() > mMaxDirectoriesPerProfile) {	File oldest = logs.getOldest();	
deleting from 

}	for(String profile : profiles.keySet()) {	ProfileLogs logs = profiles.get(profile);	if(logs.dirs.size() > mMaxDirectoriesPerProfile) {	File oldest = logs.getOldest();	FileUtils.deleteQuietly(oldest);	}	}	}	} catch(Throwable t) {	
unexpected error cleaning 

========================= hive sample_5607 =========================

public void testCTLPass() throws IOException, MetaException, TException, NoSuchObjectException, CommandNeedRetryException {	try {	hcatDriver.run("drop table junit_sem_analysis");	} catch (Exception e) {	
error in drop table 

========================= hive sample_666 =========================

executeStatementOnDriver("drop table if exists " + tblNameStg, driver);	executeStatementOnDriver("CREATE TABLE " + tblName + "(a INT, b STRING) " + " PARTITIONED BY(bkt INT)" + " CLUSTERED BY(a) INTO 4 BUCKETS" + " STORED AS ORC  TBLPROPERTIES ('transactional'='true')", driver);	executeStatementOnDriver("CREATE EXTERNAL TABLE " + tblNameStg + "(a INT, b STRING)" + " ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' LINES TERMINATED BY '\\n'" + " STORED AS TEXTFILE" + " LOCATION '" + stagingFolder.newFolder().toURI().getPath() + "'", driver);	executeStatementOnDriver("load data local inpath '" + BASIC_FILE_NAME + "' overwrite into table " + tblNameStg, driver);	execSelectAndDumpData("select * from " + tblNameStg, driver, "Dumping data for " + tblNameStg + " after load:");	executeStatementOnDriver("FROM " + tblNameStg + " INSERT INTO TABLE " + tblName + " PARTITION(bkt=0) " + "SELECT a, b where a < 2", driver);	executeStatementOnDriver("FROM " + tblNameStg + " INSERT INTO TABLE " + tblName + " PARTITION(bkt=1) " + "SELECT a, b where a >= 2", driver);	execSelectAndDumpData("select * from " + tblName, driver, "Dumping data for " + tblName + " after load:");	TxnStore txnHandler = TxnUtils.getTxnStore(conf);	CompactionInfo ci = new CompactionInfo("default", tblName, "bkt=0", CompactionType.MAJOR);	
list of stats columns before analyze 

executeStatementOnDriver("CREATE EXTERNAL TABLE " + tblNameStg + "(a INT, b STRING)" + " ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' LINES TERMINATED BY '\\n'" + " STORED AS TEXTFILE" + " LOCATION '" + stagingFolder.newFolder().toURI().getPath() + "'", driver);	executeStatementOnDriver("load data local inpath '" + BASIC_FILE_NAME + "' overwrite into table " + tblNameStg, driver);	execSelectAndDumpData("select * from " + tblNameStg, driver, "Dumping data for " + tblNameStg + " after load:");	executeStatementOnDriver("FROM " + tblNameStg + " INSERT INTO TABLE " + tblName + " PARTITION(bkt=0) " + "SELECT a, b where a < 2", driver);	executeStatementOnDriver("FROM " + tblNameStg + " INSERT INTO TABLE " + tblName + " PARTITION(bkt=1) " + "SELECT a, b where a >= 2", driver);	execSelectAndDumpData("select * from " + tblName, driver, "Dumping data for " + tblName + " after load:");	TxnStore txnHandler = TxnUtils.getTxnStore(conf);	CompactionInfo ci = new CompactionInfo("default", tblName, "bkt=0", CompactionType.MAJOR);	Worker.StatsUpdater su = Worker.StatsUpdater.init(ci, colNames, conf, System.getProperty("user.name"));	su.gatherStats();	
list of stats columns after analyze 

executeStatementOnDriver("load data local inpath '" + BASIC_FILE_NAME + "' overwrite into table " + tblNameStg, driver);	execSelectAndDumpData("select * from " + tblNameStg, driver, "Dumping data for " + tblNameStg + " after load:");	executeStatementOnDriver("FROM " + tblNameStg + " INSERT INTO TABLE " + tblName + " PARTITION(bkt=0) " + "SELECT a, b where a < 2", driver);	executeStatementOnDriver("FROM " + tblNameStg + " INSERT INTO TABLE " + tblName + " PARTITION(bkt=1) " + "SELECT a, b where a >= 2", driver);	execSelectAndDumpData("select * from " + tblName, driver, "Dumping data for " + tblName + " after load:");	TxnStore txnHandler = TxnUtils.getTxnStore(conf);	CompactionInfo ci = new CompactionInfo("default", tblName, "bkt=0", CompactionType.MAJOR);	Worker.StatsUpdater su = Worker.StatsUpdater.init(ci, colNames, conf, System.getProperty("user.name"));	su.gatherStats();	CompactionInfo ciPart2 = new CompactionInfo("default", tblName, "bkt=1", CompactionType.MAJOR);	
list of stats columns before analyze 

executeStatementOnDriver("FROM " + tblNameStg + " INSERT INTO TABLE " + tblName + " PARTITION(bkt=0) " + "SELECT a, b where a < 2", driver);	executeStatementOnDriver("FROM " + tblNameStg + " INSERT INTO TABLE " + tblName + " PARTITION(bkt=1) " + "SELECT a, b where a >= 2", driver);	execSelectAndDumpData("select * from " + tblName, driver, "Dumping data for " + tblName + " after load:");	TxnStore txnHandler = TxnUtils.getTxnStore(conf);	CompactionInfo ci = new CompactionInfo("default", tblName, "bkt=0", CompactionType.MAJOR);	Worker.StatsUpdater su = Worker.StatsUpdater.init(ci, colNames, conf, System.getProperty("user.name"));	su.gatherStats();	CompactionInfo ciPart2 = new CompactionInfo("default", tblName, "bkt=1", CompactionType.MAJOR);	su = Worker.StatsUpdater.init(ciPart2, colNames, conf, System.getProperty("user.name"));	su.gatherStats();	
list of stats columns after analyze 

static void executeStatementOnDriver(String cmd, IDriver driver) throws IOException, CommandNeedRetryException {	
executing 

========================= hive sample_236 =========================

}	List<String> badRecords = retVal.getBadRecords();	if (badRecords.size() > 0) {	System.err.println("Warning: Found records with bad " + fieldName +  " in " + tblName + " table.. ");	for (String badRecord:badRecords) {	System.err.println("bad location URI: " + badRecord);	}	}	int numNullRecords = retVal.getNumNullRecords();	if (numNullRecords != 0) {	
number of null location uri this can happen for view or index 

========================= hive sample_1845 =========================

clusterName = clusterName.trim();	}	if (clusterName == null || clusterName.isEmpty()) {	writer.print("{\"LLAP\": \"No llap daemons configured. ");	writer.print("Check hive.llap.daemon.service.hosts.\"}");	return;	}	if (clusterName.startsWith("@")) {	clusterName = clusterName.substring(1);	}	
retrieving info for cluster 

int ret = driver.run(new LlapStatusOptionsProcessor.LlapStatusOptions(clusterName), 0);	if (ret == LlapStatusServiceDriver.ExitCode.SUCCESS.getInt()) {	driver.outputJson(writer);	}	} finally {	if (writer != null) {	writer.close();	}	}	} catch (Exception e) {	
caught exception while processing llap status request 

========================= hive sample_2290 =========================

public boolean init(StatsCollectionContext context) {	try {	for (String tmpDir : context.getStatsTmpDirs()) {	Path statsDir = new Path(tmpDir);	
initing fsstatspublisher with 

public boolean init(StatsCollectionContext context) {	try {	for (String tmpDir : context.getStatsTmpDirs()) {	Path statsDir = new Path(tmpDir);	statsDir.getFileSystem(context.getHiveConf()).mkdirs(statsDir);	
created 

public boolean init(StatsCollectionContext context) {	try {	for (String tmpDir : context.getStatsTmpDirs()) {	Path statsDir = new Path(tmpDir);	statsDir.getFileSystem(context.getHiveConf()).mkdirs(statsDir);	}	return true;	} catch (IOException e) {	
failed to create dir 

public boolean connect(StatsCollectionContext context) {	conf = context.getHiveConf();	List<String> statsDirs = context.getStatsTmpDirs();	assert statsDirs.size() == 1 : "Found multiple stats dirs: " + statsDirs;	Path statsDir = new Path(statsDirs.get(0));	
connecting to 

public boolean connect(StatsCollectionContext context) {	conf = context.getHiveConf();	List<String> statsDirs = context.getStatsTmpDirs();	assert statsDirs.size() == 1 : "Found multiple stats dirs: " + statsDirs;	Path statsDir = new Path(statsDirs.get(0));	statsMap = new HashMap<String, Map<String,String>>();	try {	return statsDir.getFileSystem(conf).exists(statsDir);	} catch (IOException e) {	
failed to check if dir exists 

assert statsDirs.size() == 1 : "Found multiple stats dirs: " + statsDirs;	Path statsDir = new Path(statsDirs.get(0));	try {	Path statsFile = null;	if (context.getIndexForTezUnion() != -1) {	statsFile = new Path(statsDir, StatsSetupConst.STATS_FILE_PREFIX + conf.getInt("mapred.task.partition", 0) + "_" + context.getIndexForTezUnion());	} else {	statsFile = new Path(statsDir, StatsSetupConst.STATS_FILE_PREFIX + conf.getInt("mapred.task.partition", 0));	}	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
about to create stats file for this task 

try {	Path statsFile = null;	if (context.getIndexForTezUnion() != -1) {	statsFile = new Path(statsDir, StatsSetupConst.STATS_FILE_PREFIX + conf.getInt("mapred.task.partition", 0) + "_" + context.getIndexForTezUnion());	} else {	statsFile = new Path(statsDir, StatsSetupConst.STATS_FILE_PREFIX + conf.getInt("mapred.task.partition", 0));	}	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	}	Output output = new Output(statsFile.getFileSystem(conf).create(statsFile,true));	
created file 

try {	Path statsFile = null;	if (context.getIndexForTezUnion() != -1) {	statsFile = new Path(statsDir, StatsSetupConst.STATS_FILE_PREFIX + conf.getInt("mapred.task.partition", 0) + "_" + context.getIndexForTezUnion());	} else {	statsFile = new Path(statsDir, StatsSetupConst.STATS_FILE_PREFIX + conf.getInt("mapred.task.partition", 0));	}	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	}	Output output = new Output(statsFile.getFileSystem(conf).create(statsFile,true));	
writing stats in it 

Output output = new Output(statsFile.getFileSystem(conf).create(statsFile,true));	Kryo kryo = SerializationUtilities.borrowKryo();	try {	kryo.writeObject(output, statsMap);	} finally {	SerializationUtilities.releaseKryo(kryo);	}	output.close();	return true;	} catch (IOException e) {	
failed to persist stats on filesystem 

========================= hive sample_5040 =========================

runStatementOnDriver("drop table if exists tmp");	runStatementOnDriver("create table tmp (c1 integer, c2 integer, c3 integer) stored as orc tblproperties('transactional'='false')");	runStatementOnDriver("insert into tmp " + makeValuesClause(sourceVals1));	runStatementOnDriver("insert into tmp " + makeValuesClause(sourceVals2));	runStatementOnDriver("drop table if exists nobuckets");	runStatementOnDriver("create table nobuckets (c1 integer, c2 integer, c3 integer) stored " + "as orc tblproperties('transactional'='true', 'transactional_properties'='default')");	String stmt = "insert into nobuckets select * from tmp";	runStatementOnDriver(stmt);	List<String> rs = runStatementOnDriver( "select ROW__ID, c1, c2, c3, INPUT__FILE__NAME from nobuckets order by ROW__ID");	Assert.assertEquals("", 4, rs.size());	
after insert 

Assert.assertTrue(rs.get(0), rs.get(0).startsWith("{\"transactionid\":19,\"bucketid\":536870912,\"rowid\":0}\t0\t0\t0\t"));	Assert.assertTrue(rs.get(0), rs.get(0).endsWith("nobuckets/delta_0000019_0000019_0000/bucket_00000"));	Assert.assertTrue(rs.get(1), rs.get(1).startsWith("{\"transactionid\":19,\"bucketid\":536870912,\"rowid\":1}\t3\t3\t3\t"));	Assert.assertTrue(rs.get(1), rs.get(1).endsWith("nobuckets/delta_0000019_0000019_0000/bucket_00000"));	Assert.assertTrue(rs.get(2), rs.get(2).startsWith("{\"transactionid\":19,\"bucketid\":536936448,\"rowid\":0}\t1\t1\t1\t"));	Assert.assertTrue(rs.get(2), rs.get(2).endsWith("nobuckets/delta_0000019_0000019_0000/bucket_00001"));	Assert.assertTrue(rs.get(3), rs.get(3).startsWith("{\"transactionid\":19,\"bucketid\":536936448,\"rowid\":1}\t2\t2\t2\t"));	Assert.assertTrue(rs.get(3), rs.get(3).endsWith("nobuckets/delta_0000019_0000019_0000/bucket_00001"));	runStatementOnDriver("update nobuckets set c3 = 17 where c3 in(0,1)");	rs = runStatementOnDriver("select ROW__ID, c1, c2, c3, INPUT__FILE__NAME from nobuckets order by INPUT__FILE__NAME, ROW__ID");	
after update 

Assert.assertTrue(rs.get(3), rs.get(3).endsWith("nobuckets/delta_0000021_0000021_0000/bucket_00000"));	Set<String> expectedFiles = new HashSet<>();	expectedFiles.add("ts/delete_delta_0000021_0000021_0000/bucket_00000");	expectedFiles.add("nobuckets/delta_0000019_0000019_0000/bucket_00000");	expectedFiles.add("nobuckets/delta_0000019_0000019_0000/bucket_00001");	expectedFiles.add("nobuckets/delta_0000021_0000021_0000/bucket_00000");	assertExpectedFileSet(expectedFiles, getWarehouseDir() + "/nobuckets");	runStatementOnDriver("alter table nobuckets compact 'major'");	TestTxnCommands2.runWorker(hiveConf);	rs = runStatementOnDriver("select ROW__ID, c1, c2, c3, INPUT__FILE__NAME from nobuckets order by INPUT__FILE__NAME, ROW__ID");	
after major compact 

public void testInsertFromUnion() throws Exception {	int[][] values = {{1,2},{2,4},{5,6},{6,8},{9,10}};	runStatementOnDriver("insert into " + Table.NONACIDNONBUCKET + makeValuesClause(values));	runStatementOnDriver("drop table if exists T");	runStatementOnDriver("create table T (a int, b int) stored as ORC  TBLPROPERTIES ('transactional'='true')");	runStatementOnDriver("insert into T(a,b) select a, b from " + Table.NONACIDNONBUCKET + " where a between 1 and 3 group by a, b union all select a, b from " + Table.NONACIDNONBUCKET + " where a between 5 and 7 union all select a, b from " + Table.NONACIDNONBUCKET + " where a >= 9");	List<String> rs = runStatementOnDriver("select ROW__ID, a, b, INPUT__FILE__NAME from T order by a, b, INPUT__FILE__NAME");	
before converting to acid 

public void testToAcidConversion02() throws Exception {	runStatementOnDriver("insert into " + Table.NONACIDNONBUCKET + "(a,b) values(1,2),(1,3)");	runStatementOnDriver("insert into " + Table.NONACIDNONBUCKET + "(a,b) values(0,12),(0,13),(1,4),(1,5)");	runStatementOnDriver("insert into " + Table.NONACIDNONBUCKET + "(a,b) values(1,6)");	runStatementOnDriver("alter table " + Table.NONACIDNONBUCKET + " SET TBLPROPERTIES ('transactional'='true', 'transactional_properties'='default')");	List<String> rs = runStatementOnDriver("select ROW__ID, a, b, INPUT__FILE__NAME from " +  Table.NONACIDNONBUCKET + " order by ROW__ID");	
before acid ops after convert 

runStatementOnDriver("alter table " + Table.NONACIDNONBUCKET + " SET TBLPROPERTIES ('transactional'='true', 'transactional_properties'='default')");	List<String> rs = runStatementOnDriver("select ROW__ID, a, b, INPUT__FILE__NAME from " +  Table.NONACIDNONBUCKET + " order by ROW__ID");	for(String s : rs) {	LOG.warn(s);	}	runStatementOnDriver("insert into " + Table.NONACIDNONBUCKET + "(a,b) values(0,15),(1,16)");	runStatementOnDriver("update " + Table.NONACIDNONBUCKET + " set b = 120 where a = 0 and b = 12");	runStatementOnDriver("insert into " + Table.NONACIDNONBUCKET + "(a,b) values(0,17)");	runStatementOnDriver("delete from " + Table.NONACIDNONBUCKET + " where a = 1 and b = 3");	rs = runStatementOnDriver("select ROW__ID, a, b, INPUT__FILE__NAME from " +  Table.NONACIDNONBUCKET + " order by a,b");	
before compact 

{"{\"transactionid\":0,\"bucketid\":536870912,\"rowid\":3}\t0\t13",  "bucket_00000", "000000_0_copy_1"}, {"{\"transactionid\":18,\"bucketid\":536870912,\"rowid\":0}\t0\t15", "bucket_00000", "bucket_00000"}, {"{\"transactionid\":20,\"bucketid\":536870912,\"rowid\":0}\t0\t17", "bucket_00000", "bucket_00000"}, {"{\"transactionid\":19,\"bucketid\":536870912,\"rowid\":0}\t0\t120", "bucket_00000", "bucket_00000"}, {"{\"transactionid\":0,\"bucketid\":536870912,\"rowid\":0}\t1\t2",   "bucket_00000", "000000_0"}, {"{\"transactionid\":0,\"bucketid\":536870912,\"rowid\":4}\t1\t4",   "bucket_00000", "000000_0_copy_1"}, {"{\"transactionid\":0,\"bucketid\":536870912,\"rowid\":5}\t1\t5",   "bucket_00000", "000000_0_copy_1"}, {"{\"transactionid\":0,\"bucketid\":536870912,\"rowid\":6}\t1\t6",   "bucket_00000", "000000_0_copy_2"}, {"{\"transactionid\":18,\"bucketid\":536870912,\"rowid\":1}\t1\t16", "bucket_00000", "bucket_00000"}	};	Assert.assertEquals("Unexpected row count before compaction", expected.length, rs.size());	for(int i = 0; i < expected.length; i++) {	Assert.assertTrue("Actual line " + i + " bc: " + rs.get(i), rs.get(i).startsWith(expected[i][0]));	Assert.assertTrue("Actual line(file) " + i + " bc: " + rs.get(i), rs.get(i).endsWith(expected[i][2]));	}	runStatementOnDriver("alter table "+ Table.NONACIDNONBUCKET +" compact 'major'");	TestTxnCommands2.runWorker(hiveConf);	nonacidnonbucket/ ├── 000000_0 ├── 000000_0_copy_1 ├── 000000_0_copy_2 ├── base_0000021 │   └── bucket_00000 ├── delete_delta_0000019_0000019_0000 │   └── bucket_00000 ├── delete_delta_0000021_0000021_0000 │   └── bucket_00000 ├── delta_0000018_0000018_0000 │   └── bucket_00000 ├── delta_0000019_0000019_0000 │   └── bucket_00000 └── delta_0000020_0000020_0000 └── bucket_00000 6 directories, 9 files rs = runStatementOnDriver("select ROW__ID, a, b, INPUT__FILE__NAME from " + Table.NONACIDNONBUCKET + " order by a,b");	
after compact 

========================= hive sample_2785 =========================

public void run() {	if (rt != null) {	
stopping reporter timer for 

public void go() {	
running reportertask every miliseconds 

========================= hive sample_3897 =========================

for (int i = 0; i < length; i++) {	PartitionDesc part = HiveFileFormatUtils.getFromPathRecursively( pathToPartitionInfo, paths[i + start], IOPrepareCache.get().allocatePartitionDescMap());	Class<? extends InputFormat> inputFormatClass = part.getInputFileFormatClass();	InputFormat<WritableComparable, Writable> inputFormat = getInputFormatFromCache(inputFormatClass, conf);	boolean isAvoidSplitCombine = inputFormat instanceof AvoidSplitCombination && ((AvoidSplitCombination) inputFormat).shouldSkipCombine(paths[i + start], conf);	TableDesc tbl = part.getTableDesc();	boolean isMmNonMerge = false;	if (tbl != null) {	isMmNonMerge = !isMerge && AcidUtils.isInsertOnlyTable(tbl.getProperties());	} else {	
assuming not insert only no table in partition spec 

Class<? extends InputFormat> inputFormatClass = part.getInputFileFormatClass();	InputFormat<WritableComparable, Writable> inputFormat = getInputFormatFromCache(inputFormatClass, conf);	boolean isAvoidSplitCombine = inputFormat instanceof AvoidSplitCombination && ((AvoidSplitCombination) inputFormat).shouldSkipCombine(paths[i + start], conf);	TableDesc tbl = part.getTableDesc();	boolean isMmNonMerge = false;	if (tbl != null) {	isMmNonMerge = !isMerge && AcidUtils.isInsertOnlyTable(tbl.getProperties());	} else {	}	if (isAvoidSplitCombine || isMmNonMerge) {	
the path is being parked for hiveinputformat getsplits 

}	Path filterPath = path;	CombineFilter f = null;	List<Operator<? extends OperatorDesc>> opList = null;	if (!mrwork.isMapperCannotSpanPartns()) {	opList = HiveFileFormatUtils.doGetWorksFromPath( pathToAliases, aliasToWork, filterPath);	CombinePathInputFormat combinePathInputFormat = new CombinePathInputFormat(opList, inputFormatClassName, deserializerClassName);	f = poolMap.get(combinePathInputFormat);	if (f == null) {	f = new CombineFilter(filterPath);	
combinehiveinputsplit creating pool for using filter path 

List<Operator<? extends OperatorDesc>> opList = null;	if (!mrwork.isMapperCannotSpanPartns()) {	opList = HiveFileFormatUtils.doGetWorksFromPath( pathToAliases, aliasToWork, filterPath);	CombinePathInputFormat combinePathInputFormat = new CombinePathInputFormat(opList, inputFormatClassName, deserializerClassName);	f = poolMap.get(combinePathInputFormat);	if (f == null) {	f = new CombineFilter(filterPath);	combine.createPool(job, f);	poolMap.put(combinePathInputFormat, f);	} else {	
combinehiveinputsplit pool is already created for using filter path 

processPaths(job, combine, iss, inpFiles.toArray(new Path[0]));	}	}	if (mrwork.getNameToSplitSample() != null && !mrwork.getNameToSplitSample().isEmpty()) {	iss = sampleSplits(iss);	}	for (CombineFileSplit is : iss) {	CombineHiveInputSplit csplit = new CombineHiveInputSplit(job, is, pathToPartitionInfo);	result.add(csplit);	}	
number of splits 

public Set<Integer> getNonCombinablePathIndices(JobConf job, Path[] paths, int numThreads) throws ExecutionException, InterruptedException {	
total number of paths launching threads to check non combinable ones 

try {	Set<Integer> nonCombinablePathIndices = getNonCombinablePathIndices(job, paths, numThreads);	for (int i = 0; i < paths.length; i++) {	if (nonCombinablePathIndices.contains(i)) {	nonCombinablePaths.add(paths[i]);	} else {	combinablePaths.add(paths[i]);	}	}	} catch (Exception e) {	
error checking non combinable path 

combinablePaths.add(paths[i]);	}	}	} catch (Exception e) {	perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.GET_SPLITS);	throw new IOException(e);	}	}	String oldPaths = job.get(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR);	if (LOG.isDebugEnabled()) {	
the received input paths are against the property 

Map<Path, PartitionDesc> pathToPartitionInfo = this.pathToPartitionInfo != null ? this.pathToPartitionInfo : Utilities.getMapWork(job).getPathToPartitionInfo();	InputSplit[] splits = getCombineSplits(job, numSplits, pathToPartitionInfo);	for (InputSplit split : splits) {	result.add(split);	}	}	if (oldPaths != null) {	job.set(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR, oldPaths);	}	Utilities.clearWorkMapForConf(job);	
number of all splits 

}	SplitSample splitSample = nameToSamples.get(entry.getKey());	long targetSize = splitSample.getTargetSize(totalSize);	int startIndex = splitSample.getSeedNum() % splitList.size();	long size = 0;	for (int i = 0; i < splitList.size(); i++) {	CombineFileSplit split = splitList.get((startIndex + i) % splitList.size());	retLists.add(split);	long splitgLength = split.getLength();	if (size + splitgLength >= targetSize) {	
sample alias using splits 

========================= hive sample_3602 =========================

static void initializeDeserializer(Deserializer deserializer, Configuration conf, HCatTableInfo info, HCatSchema schema) throws SerDeException {	Properties props = getSerdeProperties(info, schema);	
initializing with properties 

========================= hive sample_758 =========================

Class.forName(BeeLine.BEELINE_DEFAULT_JDBC_DRIVER);	Connection con = DriverManager.getConnection(miniHS2.getBaseJdbcURL(), "", "");	assertNotNull("Connection is null", con);	assertFalse("Connection should not be closed", con.isClosed());	Statement stmt = con.createStatement();	assertNotNull("Statement is null", stmt);	stmt.execute("set hive.support.concurrency = false");	try {	stmt.execute("drop table if exists " + tableName);	} catch (Exception ex) {	
failed due to exception 

========================= hive sample_308 =========================

public static void killRunningJobs() {	try {	if (KILL_RUNNING_TEZ_JOBS != null) {	KILL_RUNNING_TEZ_JOBS.invoke(null, null);	} else {	
unable to find tez method for killing jobs 

public static void killRunningJobs() {	try {	if (KILL_RUNNING_TEZ_JOBS != null) {	KILL_RUNNING_TEZ_JOBS.invoke(null, null);	} else {	}	} catch (Exception e) {	
could not stop tez dags 

========================= hive sample_3985 =========================

final double overSubscriptionFactor = conf.getFloatVar(ConfVars.LLAP_MAPJOIN_MEMORY_OVERSUBSCRIBE_FACTOR);	final int maxSlotsPerQuery = conf.getIntVar(ConfVars.LLAP_MEMORY_OVERSUBSCRIPTION_MAX_EXECUTORS_PER_QUERY);	final long memoryCheckInterval = conf.getLongVar(ConfVars.LLAP_MAPJOIN_MEMORY_MONITOR_CHECK_INTERVAL);	final float inflationFactor = conf.getFloatVar(ConfVars.HIVE_HASH_TABLE_INFLATION_FACTOR);	final MemoryMonitorInfo memoryMonitorInfo;	if ("llap".equalsIgnoreCase(conf.getVar(ConfVars.HIVE_EXECUTION_MODE))) {	LlapClusterStateForCompile llapInfo = LlapClusterStateForCompile.getClusterInfo(conf);	llapInfo.initClusterInfo();	final int executorsPerNode;	if (!llapInfo.hasClusterInfo()) {	
llap cluster information not available falling back to getting executors from hiveconf 

final MemoryMonitorInfo memoryMonitorInfo;	if ("llap".equalsIgnoreCase(conf.getVar(ConfVars.HIVE_EXECUTION_MODE))) {	LlapClusterStateForCompile llapInfo = LlapClusterStateForCompile.getClusterInfo(conf);	llapInfo.initClusterInfo();	final int executorsPerNode;	if (!llapInfo.hasClusterInfo()) {	executorsPerNode = conf.getIntVar(ConfVars.LLAP_DAEMON_NUM_EXECUTORS);	} else {	final int numExecutorsPerNodeFromCluster = llapInfo.getNumExecutorsPerNode();	if (numExecutorsPerNodeFromCluster == -1) {	
cannot determine executor count from llap cluster information falling back to getting executors from hiveconf 

}	}	final int slotsPerQuery = Math.min(maxSlotsPerQuery, executorsPerNode);	final long llapMaxSize = (long) (maxSize + (maxSize * overSubscriptionFactor * slotsPerQuery));	final long adjustedMaxSize = Math.max(maxSize, llapMaxSize);	memoryMonitorInfo = new MemoryMonitorInfo(true, executorsPerNode, maxSlotsPerQuery, overSubscriptionFactor, maxSize, adjustedMaxSize, memoryCheckInterval, inflationFactor);	} else {	memoryMonitorInfo = new MemoryMonitorInfo(false, 1, maxSlotsPerQuery, overSubscriptionFactor, maxSize, maxSize, memoryCheckInterval, inflationFactor);	}	if (LOG.isInfoEnabled()) {	
memory monitor info set to 

private boolean convertJoinBucketMapJoin(JoinOperator joinOp, OptimizeTezProcContext context, int bigTablePosition, TezBucketJoinProcCtx tezBucketJoinProcCtx) throws SemanticException {	if (!checkConvertJoinBucketMapJoin(joinOp, context, bigTablePosition, tezBucketJoinProcCtx)) {	
check conversion to bucket map join failed 

for (ExprNodeDesc bigTableExpr : bigTablePartitionCols) {	for (String colName : listBucketCols.get(0)) {	if (colExprMap.get(colName).isSame(bigTableExpr)) {	positions.add(i++);	}	}	}	}	MapJoinOperator mapJoinOp = convertJoinMapJoin(joinOp, context, bigTablePosition, true);	if (mapJoinOp == null) {	
conversion to bucket map join failed 

continue;	}	if (((size > 0) && (set.size() > 0)) || ((size == 0) && (set.size() == 0))) {	continue;	} else {	return false;	}	}	for (Operator<? extends OperatorDesc> parentOp : joinOp.getParentOperators()) {	if (!(parentOp instanceof ReduceSinkOperator)) {	
found correlation optimizer operators cannot convert to smb at this time 

} else {	return false;	}	}	for (Operator<? extends OperatorDesc> parentOp : joinOp.getParentOperators()) {	if (!(parentOp instanceof ReduceSinkOperator)) {	return false;	}	ReduceSinkOperator rsOp = (ReduceSinkOperator) parentOp;	if (checkColEquality(rsOp.getParentOperators().get(0).getOpTraits().getSortCols(), rsOp .getOpTraits().getSortCols(), rsOp.getColumnExprMap(), tezBucketJoinProcCtx, false) == false) {	
we cannot convert to smb because the sort column names do not match 

}	for (Operator<? extends OperatorDesc> parentOp : joinOp.getParentOperators()) {	if (!(parentOp instanceof ReduceSinkOperator)) {	return false;	}	ReduceSinkOperator rsOp = (ReduceSinkOperator) parentOp;	if (checkColEquality(rsOp.getParentOperators().get(0).getOpTraits().getSortCols(), rsOp .getOpTraits().getSortCols(), rsOp.getColumnExprMap(), tezBucketJoinProcCtx, false) == false) {	return false;	}	if (checkColEquality(rsOp.getParentOperators().get(0).getOpTraits().getBucketColNames(), rsOp .getOpTraits().getBucketColNames(), rsOp.getColumnExprMap(), tezBucketJoinProcCtx, true) == false) {	
we cannot convert to smb because bucket column names do not match 

return false;	}	if (checkColEquality(rsOp.getParentOperators().get(0).getOpTraits().getBucketColNames(), rsOp .getOpTraits().getBucketColNames(), rsOp.getColumnExprMap(), tezBucketJoinProcCtx, true) == false) {	return false;	}	}	if (numBuckets < 0) {	numBuckets = bigTableRS.getConf().getNumReducers();	}	tezBucketJoinProcCtx.setNumBuckets(numBuckets);	
we can convert the join to an smb join 

private boolean checkConvertJoinBucketMapJoin(JoinOperator joinOp, OptimizeTezProcContext context, int bigTablePosition, TezBucketJoinProcCtx tezBucketJoinProcCtx) throws SemanticException {	if (!(joinOp.getParentOperators().get(0) instanceof ReduceSinkOperator)) {	
operator is cannot convert to bucket map join 

private boolean checkConvertJoinBucketMapJoin(JoinOperator joinOp, OptimizeTezProcContext context, int bigTablePosition, TezBucketJoinProcCtx tezBucketJoinProcCtx) throws SemanticException {	if (!(joinOp.getParentOperators().get(0) instanceof ReduceSinkOperator)) {	return false;	}	ReduceSinkOperator rs = (ReduceSinkOperator) joinOp.getParentOperators().get(bigTablePosition);	List<List<String>> parentColNames = rs.getOpTraits().getBucketColNames();	Operator<? extends OperatorDesc> parentOfParent = rs.getParentOperators().get(0);	List<List<String>> grandParentColNames = parentOfParent.getOpTraits().getBucketColNames();	int numBuckets = parentOfParent.getOpTraits().getNumBuckets();	if (checkColEquality(grandParentColNames, parentColNames, rs.getColumnExprMap(), tezBucketJoinProcCtx, true) == false) {	
no info available to check for bucket map join cannot convert 

int bigTablePosition = -1;	long bigInputCumulativeCardinality = -1L;	Statistics bigInputStat = null;	boolean foundInputNotFittingInMemory = false;	long totalSize = 0;	boolean convertDPHJ = false;	for (int pos = 0; pos < joinOp.getParentOperators().size(); pos++) {	Operator<? extends OperatorDesc> parentOp = joinOp.getParentOperators().get(pos);	Statistics currInputStat = parentOp.getStatistics();	if (currInputStat == null) {	
couldn t get statistics from 

if (totalSize/buckets > maxSize) {	return -1;	}	if (selectedBigTable) {	bigTablePosition = pos;	bigInputCumulativeCardinality = currentInputCumulativeCardinality;	bigInputStat = currInputStat;	}	}	if (checkMapJoinThresholds && convertDPHJ && checkShuffleSizeForLargeTable(joinOp, bigTablePosition, context)) {	
conditions to convert to mapjoin are not met 

for (Operator<? extends OperatorDesc> inputOp : op.getParentOperators()) {	Long inputCardinality = computeCumulativeCardinality(inputOp);	if (inputCardinality == null) {	return null;	}	cumulativeCardinality += inputCardinality;	}	}	Statistics currInputStat = op.getStatistics();	if (currInputStat == null) {	
couldn t get statistics from 

for (Operator<?> c : p.getChildOperators()) {	AppMasterEventOperator event = findDynamicPartitionBroadcast(c);	if (event != null) {	dynamicPartitionOperators.add(c);	opEventPairs.put(c, event);	}	}	for (Operator<?> c : dynamicPartitionOperators) {	if (context.pruningOpsRemovedByPriorOpt.isEmpty() || !context.pruningOpsRemovedByPriorOpt.contains(opEventPairs.get(c))) {	p.removeChild(c);	
disabling dynamic pruning for need to be removed together with reduce sink 

break;	}	}	if (found) break;	}	}	}	if (semiJoinMap.size() > 0) {	for (ReduceSinkOperator rs : semiJoinMap.keySet()) {	if (LOG.isDebugEnabled()) {	
found semijoin optimization from the big table side of a map join which will cause a task cycle removing semijoin 

private boolean convertJoinDynamicPartitionedHashJoin(JoinOperator joinOp, OptimizeTezProcContext context, final long maxSize) throws SemanticException {	int numReducers = estimateNumBuckets(joinOp, false);	
try dynamic partitioned hash join with estimated reducers 

private boolean convertJoinDynamicPartitionedHashJoin(JoinOperator joinOp, OptimizeTezProcContext context, final long maxSize) throws SemanticException {	int numReducers = estimateNumBuckets(joinOp, false);	int bigTablePos = getMapJoinConversionPos(joinOp, context, numReducers, false, maxSize,false);	if (bigTablePos >= 0) {	ReduceSinkOperator bigTableParentRS = (ReduceSinkOperator) (joinOp.getParentOperators().get(bigTablePos));	numReducers = bigTableParentRS.getConf().getNumReducers();	LOG.debug("Real big table reducers = " + numReducers);	MapJoinOperator mapJoinOp = convertJoinMapJoin(joinOp, context, bigTablePos, false);	if (mapJoinOp != null) {	
selected dynamic partitioned hash join 

private void fallbackToMergeJoin(JoinOperator joinOp, OptimizeTezProcContext context) throws SemanticException {	int pos = getMapJoinConversionPos(joinOp, context, estimateNumBuckets(joinOp, false), true, Long.MAX_VALUE, false);	if (pos < 0) {	
could not get a valid join position defaulting to position 

private void fallbackToMergeJoin(JoinOperator joinOp, OptimizeTezProcContext context) throws SemanticException {	int pos = getMapJoinConversionPos(joinOp, context, estimateNumBuckets(joinOp, false), true, Long.MAX_VALUE, false);	if (pos < 0) {	pos = 0;	}	
fallback to common merge join operator 

if (max < 1) {	return true;	}	ReduceSinkOperator rsOp = (ReduceSinkOperator) joinOp.getParentOperators().get(position);	List<String> keys = StatsUtils.getQualifedReducerKeyNames(rsOp.getConf().getOutputKeyColumnNames());	Statistics inputStats = rsOp.getStatistics();	List<ColStatistics> columnStats = new ArrayList<>();	for (String key : keys) {	ColStatistics cs = inputStats.getColumnStatisticsFromColName(key);	if (cs == null) {	
couldn t get statistics for 

List<ColStatistics> columnStats = new ArrayList<>();	for (String key : keys) {	ColStatistics cs = inputStats.getColumnStatisticsFromColName(key);	if (cs == null) {	return true;	}	columnStats.add(cs);	}	long numRows = inputStats.getNumRows();	long estimation = estimateNDV(numRows, columnStats);	
estimated ndv for input max ndv for mapjoin conversion 

for (String key : keys) {	ColStatistics cs = inputStats.getColumnStatisticsFromColName(key);	if (cs == null) {	return true;	}	columnStats.add(cs);	}	long numRows = inputStats.getNumRows();	long estimation = estimateNDV(numRows, columnStats);	if (estimation > max) {	
number of different entries for hashtable is greater than the max we do not convert to mapjoin 

private boolean checkShuffleSizeForLargeTable(JoinOperator joinOp, int position, OptimizeTezProcContext context) {	long max = HiveConf.getLongVar(context.parseContext.getConf(), HiveConf.ConfVars.HIVECONVERTJOINMAXSHUFFLESIZE);	if (max < 1) {	return false;	}	ReduceSinkOperator rsOp = (ReduceSinkOperator) joinOp.getParentOperators().get(position);	Statistics inputStats = rsOp.getStatistics();	long inputSize = inputStats.getDataSize();	
estimated size for input max size for dphj conversion 

private boolean checkShuffleSizeForLargeTable(JoinOperator joinOp, int position, OptimizeTezProcContext context) {	long max = HiveConf.getLongVar(context.parseContext.getConf(), HiveConf.ConfVars.HIVECONVERTJOINMAXSHUFFLESIZE);	if (max < 1) {	return false;	}	ReduceSinkOperator rsOp = (ReduceSinkOperator) joinOp.getParentOperators().get(position);	Statistics inputStats = rsOp.getStatistics();	long inputSize = inputStats.getDataSize();	if (inputSize > max) {	
size of input is greater than the max we do not convert to dphj 

========================= hive sample_3086 =========================

public void connect() throws ConnectionException {	if (connected) {	throw new ConnectionException("Already connected.");	}	for (AcidTable table : tables) {	checkTable(metaStoreClient, table);	}	
connected to end point 

public Transaction newTransaction() throws TransactionException {	if (!connected) {	throw new TransactionException("Not connected - cannot create transaction.");	}	Transaction transaction = new Transaction(metaStoreClient, lockOptions);	for (AcidTable table : tables) {	table.setTransactionId(transaction.getTransactionId());	}	
created transaction 

public void close() throws IOException {	metaStoreClient.close();	
closed client 

private void checkTable(IMetaStoreClient metaStoreClient, AcidTable acidTable) throws ConnectionException {	try {	
checking table 

private void checkTable(IMetaStoreClient metaStoreClient, AcidTable acidTable) throws ConnectionException {	try {	Table metaStoreTable = metaStoreClient.getTable(acidTable.getDatabaseName(), acidTable.getTableName());	if (acidTable.getTableType() == TableType.SINK) {	Map<String, String> parameters = metaStoreTable.getParameters();	if (!Boolean.parseBoolean(parameters.get(TRANSACTIONAL_PARAM_KEY))) {	throw new ConnectionException("Cannot stream to table that is not transactional: '" + acidTable.getQualifiedName() + "'.");	}	int totalBuckets = metaStoreTable.getSd().getNumBuckets();	
table has buckets 

if (acidTable.getTableType() == TableType.SINK) {	Map<String, String> parameters = metaStoreTable.getParameters();	if (!Boolean.parseBoolean(parameters.get(TRANSACTIONAL_PARAM_KEY))) {	throw new ConnectionException("Cannot stream to table that is not transactional: '" + acidTable.getQualifiedName() + "'.");	}	int totalBuckets = metaStoreTable.getSd().getNumBuckets();	if (totalBuckets <= 0) {	throw new ConnectionException("Cannot stream to table that has not been bucketed: '" + acidTable.getQualifiedName() + "'.");	}	String outputFormat = metaStoreTable.getSd().getOutputFormat();	
table has outputformat 

throw new ConnectionException("Cannot stream to table that has not been bucketed: '" + acidTable.getQualifiedName() + "'.");	}	String outputFormat = metaStoreTable.getSd().getOutputFormat();	acidTable.setTable(metaStoreTable);	}	} catch (NoSuchObjectException e) {	throw new ConnectionException("Invalid table '" + acidTable.getQualifiedName() + "'", e);	} catch (TException e) {	throw new ConnectionException("Error communicating with the meta store", e);	}	
table ok 

========================= hive sample_975 =========================

private PartitionDesc extractSinglePartSpec(CombineHiveInputSplit hsplit) throws IOException {	PartitionDesc part = null;	Map<Map<Path,PartitionDesc>, Map<Path,PartitionDesc>> cache = new HashMap<>();	for (Path path : hsplit.getPaths()) {	PartitionDesc otherPart = HiveFileFormatUtils.getFromPathRecursively( pathToPartInfo, path, cache);	
found spec for from 

private PartitionDesc extractSinglePartSpec(CombineHiveInputSplit hsplit) throws IOException {	PartitionDesc part = null;	Map<Map<Path,PartitionDesc>, Map<Path,PartitionDesc>> cache = new HashMap<>();	for (Path path : hsplit.getPaths()) {	PartitionDesc otherPart = HiveFileFormatUtils.getFromPathRecursively( pathToPartInfo, path, cache);	if (part == null) {	part = otherPart;	} else if (otherPart != part) {	
multiple partitions found not going to pass a part spec to llap io and 

========================= hive sample_3593 =========================

public AppConfig() {	init();	
using hadoop version 

public JobsListOrder getListJobsOrder() {	String requestedOrder = get(TEMPLETON_JOBSLIST_ORDER);	if (requestedOrder != null) {	try {	return JobsListOrder.valueOf(requestedOrder.toLowerCase());	}	catch(IllegalArgumentException ex) {	
ignoring setting configured with in correct value 

private boolean loadOneFileConfig(String dir, String fname) {	if (dir != null) {	File f = new File(dir, fname);	if (f.exists()) {	addResource(new Path(f.getAbsolutePath()));	
loaded config file 

private boolean loadOneClasspathConfig(String fname) {	URL x = getResource(fname);	if (x != null) {	addResource(x);	
loaded config from classpath 

========================= hive sample_830 =========================

protected TaskRunner2Result callInternal() throws Exception {	setMDCFromNDC();	try {	isStarted.set(true);	this.startTime = System.currentTimeMillis();	threadName = Thread.currentThread().getName();	this.threadName = Thread.currentThread().getName();	if (LOG.isDebugEnabled()) {	
canfinish 

isStarted.set(true);	this.startTime = System.currentTimeMillis();	threadName = Thread.currentThread().getName();	this.threadName = Thread.currentThread().getName();	if (LOG.isDebugEnabled()) {	}	TezTaskAttemptID ta = taskSpec.getTaskAttemptID();	this.amReporter.unregisterTask(request.getAmHost(), request.getAmPort(), fragmentInfo.getQueryInfo().getQueryIdentifier(), ta);	synchronized (this) {	if (!shouldRunTask) {	
not starting task since it was killed earlier 

final UserGroupInformation taskOwner;	if (!vertex.getIsExternalSubmission()) {	taskOwner = fragmentInfo.getQueryInfo().getUmbilicalUgi();	} else {	taskOwner = UserGroupInformation.createRemoteUser(vertex.getTokenIdentifier());	taskOwner.addToken(jobToken);	final InetSocketAddress address = NetUtils.createSocketAddrForHost(request.getAmHost(), request.getAmPort());	SecurityUtil.setTokenService(jobToken, address);	}	if (LOG.isDebugEnabled()) {	
taskowner hashcode 

taskReporter = new LlapTaskReporter( completionListener, umbilical, confParams.amHeartbeatIntervalMsMax, confParams.amCounterHeartbeatInterval, confParams.amMaxEventsPerHeartbeat, new AtomicLong(0), request.getContainerIdString(), fragmentId, initialEvent, requestId);	String attemptId = fragmentInfo.getFragmentIdentifierString();	IOContextMap.setThreadAttemptId(attemptId);	try {	synchronized (this) {	if (shouldRunTask) {	taskRunner = new TezTaskRunner2(conf, fsTaskUgi, fragmentInfo.getLocalDirs(), taskSpec, vertex.getQueryIdentifier().getAppAttemptNumber(), serviceConsumerMetadata, envMap, startedInputsMap, taskReporter, executor, objectRegistry, pid, executionContext, memoryAvailable, false, tezHadoopShim);	}	}	if (taskRunner == null) {	
not starting task since it was killed earlier 

if (shouldRunTask) {	taskRunner = new TezTaskRunner2(conf, fsTaskUgi, fragmentInfo.getLocalDirs(), taskSpec, vertex.getQueryIdentifier().getAppAttemptNumber(), serviceConsumerMetadata, envMap, startedInputsMap, taskReporter, executor, objectRegistry, pid, executionContext, memoryAvailable, false, tezHadoopShim);	}	}	if (taskRunner == null) {	return new TaskRunner2Result(EndReason.KILL_REQUESTED, null, null, false);	}	try {	TaskRunner2Result result = taskRunner.run();	if (result.isContainerShutdownRequested()) {	
unexpected container shutdown requested while running task ignoring 

TaskRunner2Result result = taskRunner.run();	if (result.isContainerShutdownRequested()) {	}	isCompleted.set(true);	return result;	} finally {	FileSystem.closeAllForUGI(fsTaskUgi);	fragmentInfo.getQueryInfo().returnUmbilicalUgi(taskOwner);	LOG.info("ExecutionTime for Container: " + request.getContainerIdString() + "=" + runtimeWatch.stop().elapsed(TimeUnit.MILLISECONDS));	if (LOG.isDebugEnabled()) {	
canfinish post completion 

public void killTask() {	if (!isCompleted.get()) {	if (!killInvoked.getAndSet(true)) {	synchronized (this) {	TezTaskAttemptID ta = taskSpec.getTaskAttemptID();	LOG.info("Kill task requested for id={}, taskRunnerSetup={}", ta, taskRunner != null);	shouldRunTask = false;	if (taskRunner != null) {	killtimerWatch.start();	
issuing kill to task 

if (!isCompleted.get()) {	if (!killInvoked.getAndSet(true)) {	synchronized (this) {	TezTaskAttemptID ta = taskSpec.getTaskAttemptID();	LOG.info("Kill task requested for id={}, taskRunnerSetup={}", ta, taskRunner != null);	shouldRunTask = false;	if (taskRunner != null) {	killtimerWatch.start();	boolean killed = taskRunner.killTask();	if (killed) {	
kill request for task completed informing am 

TezTaskAttemptID ta = taskSpec.getTaskAttemptID();	LOG.info("Kill task requested for id={}, taskRunnerSetup={}", ta, taskRunner != null);	shouldRunTask = false;	if (taskRunner != null) {	killtimerWatch.start();	boolean killed = taskRunner.killTask();	if (killed) {	completionListener.fragmentCompleting(getRequestId(), SchedulerFragmentCompletingListener.State.KILLED);	reportTaskKilled();	} else {	
kill request for task did not complete because the task is already complete 

shouldRunTask = false;	if (taskRunner != null) {	killtimerWatch.start();	boolean killed = taskRunner.killTask();	if (killed) {	completionListener.fragmentCompleting(getRequestId(), SchedulerFragmentCompletingListener.State.KILLED);	reportTaskKilled();	} else {	}	} else {	
reporting taskkilled for non started fragment 

}	} else {	reportTaskKilled();	}	if (!isStarted.get()) {	fragmentCompletionHanler.fragmentComplete(fragmentInfo);	this.amReporter .unregisterTask(request.getAmHost(), request.getAmPort(), fragmentInfo.getQueryInfo().getQueryIdentifier(), ta);	}	}	} else {	
ignoring kill request for task since a previous kill request was processed 

reportTaskKilled();	}	if (!isStarted.get()) {	fragmentCompletionHanler.fragmentComplete(fragmentInfo);	this.amReporter .unregisterTask(request.getAmHost(), request.getAmPort(), fragmentInfo.getQueryInfo().getQueryIdentifier(), ta);	}	}	} else {	}	} else {	
ignoring kill request for task since it s already complete 

public void onSuccess(TaskRunner2Result result) {	isCompleted.set(true);	switch(result.getEndReason()) {	
successfully finished 

public void onSuccess(TaskRunner2Result result) {	isCompleted.set(true);	switch(result.getEndReason()) {	if (metrics != null) {	metrics.incrExecutorTotalSuccess();	}	break;	
received container stop request am preemption for 

isCompleted.set(true);	switch(result.getEndReason()) {	if (metrics != null) {	metrics.incrExecutorTotalSuccess();	}	break;	if (metrics != null) {	metrics.incrExecutorTotalKilled();	}	break;	
killed task 

metrics.incrExecutorTotalSuccess();	}	break;	if (metrics != null) {	metrics.incrExecutorTotalKilled();	}	break;	if (killtimerWatch.isRunning()) {	killtimerWatch.stop();	long elapsed = killtimerWatch.elapsed(TimeUnit.MILLISECONDS);	
time to die for task 

long elapsed = killtimerWatch.elapsed(TimeUnit.MILLISECONDS);	if (metrics != null) {	metrics.addMetricsPreemptionTimeToKill(elapsed);	}	}	if (metrics != null) {	metrics.addMetricsPreemptionTimeLost(runtimeWatch.elapsed(TimeUnit.MILLISECONDS));	metrics.incrExecutorTotalKilled();	}	break;	
failed to run due to communication failure 

}	if (metrics != null) {	metrics.addMetricsPreemptionTimeLost(runtimeWatch.elapsed(TimeUnit.MILLISECONDS));	metrics.incrExecutorTotalKilled();	}	break;	if (metrics != null) {	metrics.incrExecutorTotalExecutionFailed();	}	break;	
failed to run due to task error 

public void onFailure(Throwable t) {	
teztaskrunner execution failed for 

========================= hive sample_2228 =========================

public synchronized Instance retrieve(SeedObject hv, Set<SeedObject> seenSchemas) throws AvroSerdeException {	
checking for hv 

public synchronized Instance retrieve(SeedObject hv, Set<SeedObject> seenSchemas) throws AvroSerdeException {	if(cache.containsKey(hv)) {	
returning cache result 

public synchronized Instance retrieve(SeedObject hv, Set<SeedObject> seenSchemas) throws AvroSerdeException {	if(cache.containsKey(hv)) {	return cache.get(hv);	}	
creating new instance and storing in cache 

========================= hive sample_5294 =========================

is = new FileInputStream(inputFile);	}	final TarArchiveInputStream debInputStream = (TarArchiveInputStream) new ArchiveStreamFactory().createArchiveInputStream("tar", is);	TarArchiveEntry entry = null;	while ((entry = (TarArchiveEntry) debInputStream.getNextEntry()) != null) {	final File outputFile = new File(outputDir, entry.getName());	if (entry.isDirectory()) {	if (flatten) {	continue;	}	
attempting to write output directory s 

}	final TarArchiveInputStream debInputStream = (TarArchiveInputStream) new ArchiveStreamFactory().createArchiveInputStream("tar", is);	TarArchiveEntry entry = null;	while ((entry = (TarArchiveEntry) debInputStream.getNextEntry()) != null) {	final File outputFile = new File(outputDir, entry.getName());	if (entry.isDirectory()) {	if (flatten) {	continue;	}	if (!outputFile.exists()) {	
attempting to create output directory s 

}	if (!outputFile.exists()) {	if (!outputFile.mkdirs()) {	throw new IllegalStateException(String.format("Couldn't create directory %s.", outputFile.getAbsolutePath()));	}	}	} else {	final OutputStream outputFileStream;	if (flatten) {	File flatOutputFile = new File(outputDir, outputFile.getName());	
creating flat output file s 

if (!outputFile.mkdirs()) {	throw new IllegalStateException(String.format("Couldn't create directory %s.", outputFile.getAbsolutePath()));	}	}	} else {	final OutputStream outputFileStream;	if (flatten) {	File flatOutputFile = new File(outputDir, outputFile.getName());	outputFileStream = new FileOutputStream(flatOutputFile);	} else if (!outputFile.getParentFile().exists()) {	
attempting to create output directory s 

}	} else {	final OutputStream outputFileStream;	if (flatten) {	File flatOutputFile = new File(outputDir, outputFile.getName());	outputFileStream = new FileOutputStream(flatOutputFile);	} else if (!outputFile.getParentFile().exists()) {	if (!outputFile.getParentFile().getAbsoluteFile().mkdirs())  {	throw new IllegalStateException(String.format("Couldn't create directory %s.", outputFile.getParentFile().getAbsolutePath()));	}	
creating output file s 

========================= hive sample_1392 =========================

public Map<ColumnInfo, ExprNodeDesc> getPropagatedConstants(Operator<? extends Serializable> op) {	Map<ColumnInfo, ExprNodeDesc> constants = new HashMap<ColumnInfo, ExprNodeDesc>();	if (op.getSchema() == null) {	return constants;	}	RowSchema rs = op.getSchema();	
getting constants of op with rs 

RowSchema rs = op.getSchema();	if (op.getParentOperators() == null) {	return constants;	}	List<Map<Integer, ExprNodeDesc>> parentsToConstant = new ArrayList<>();	boolean areAllParentsContainConstant = true;	boolean noParentsContainConstant = true;	for (Operator<?> parent : op.getParentOperators()) {	Map<ColumnInfo, ExprNodeDesc> constMap = opToConstantExprs.get(parent);	if (constMap == null) {	
constant of op is not found 

Map<ColumnInfo, ExprNodeDesc> constMap = opToConstantExprs.get(parent);	if (constMap == null) {	areAllParentsContainConstant = false;	} else {	noParentsContainConstant = false;	Map<Integer, ExprNodeDesc> map = new HashMap<>();	for (Entry<ColumnInfo, ExprNodeDesc> entry : constMap.entrySet()) {	map.put(parent.getSchema().getPosition(entry.getKey().getInternalName()), entry.getValue());	}	parentsToConstant.add(map);	
constant of op 

String parentColName = ((ExprNodeColumnDesc) expr).getColumn();	int parentPos = parent.getSchema().getPosition(parentColName);	if (parentsToConstant.get(0).containsKey(parentPos)) {	constants.put(signature.get(op.getSchema().getPosition(entry.getKey())), parentsToConstant.get(0).get(parentPos));	}	}	}	}	}	}	
offering constants to operator 

========================= hive sample_2947 =========================

public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx ctx, Object... nodeOutputs) throws SemanticException {	ReduceSinkOperator op = (ReduceSinkOperator) nd;	ColumnPrunerProcCtx cppCtx = (ColumnPrunerProcCtx) ctx;	ReduceSinkDesc conf = op.getConf();	List<FieldNode> colLists = new ArrayList<>();	ArrayList<ExprNodeDesc> keys = conf.getKeyCols();	
reduce sink operator key 

private static void pruneReduceSinkOperator(boolean[] retainFlags, ReduceSinkOperator reduce, ColumnPrunerProcCtx cppCtx) throws SemanticException {	ReduceSinkDesc reduceConf = reduce.getConf();	Map<String, ExprNodeDesc> oldMap = reduce.getColumnExprMap();	
rs oldcolexprmap 

newValueColNames.add(outputCol);	newValueExprs.add(outputColExpr);	}	}	oldRS.setSignature(signature);	reduce.getSchema().setSignature(signature);	reduceConf.setOutputValueColumnNames(newValueColNames);	reduceConf.setValueCols(newValueExprs);	TableDesc newValueTable = PlanUtils.getReduceValueTableDesc(PlanUtils .getFieldSchemasFromColumnList(reduceConf.getValueCols(), newValueColNames, 0, ""));	reduceConf.setValueSerializeInfo(newValueTable);	
rs newcolexprmap 

private static void pruneJoinOperator(NodeProcessorCtx ctx, CommonJoinOperator op, JoinDesc conf, Map<String, ExprNodeDesc> columnExprMap, Map<Byte, List<Integer>> retainMap, boolean mapJoin) throws SemanticException {	ColumnPrunerProcCtx cppCtx = (ColumnPrunerProcCtx) ctx;	List<Operator<? extends OperatorDesc>> childOperators = op .getChildOperators();	
join oldexprs 

if (child instanceof ReduceSinkOperator) {	boolean[] flags = getPruneReduceSinkOpRetainFlags(toColumnNames(neededColList), (ReduceSinkOperator) child);	pruneReduceSinkOperator(flags, (ReduceSinkOperator) child, cppCtx);	}	}	for (int i = 0; i < outputCols.size(); i++) {	String internalName = outputCols.get(i);	ColumnInfo col = joinRS.getColumnInfo(internalName);	rs.add(col);	}	
join newexprs 

========================= hive sample_3090 =========================

public void close() throws IOException {	if (outWriter == null) {	return;	}	outWriter.close();	outWriter = null;	if (!exception) {	FileStatus fss = fs.getFileStatus(outPath);	
renamed path to file size is 

========================= hive sample_3585 =========================

}	needHashTableSetup = false;	}	batchCounter++;	for(VectorExpression ve : bigTableFilterExpressions) {	ve.evaluate(batch);	}	final int inputLogicalSize = batch.size;	if (inputLogicalSize == 0) {	if (LOG.isDebugEnabled()) {	
batch empty 

joinResult = JoinUtil.JoinResult.NOMATCH;	} else {	long key = vector[0];	if (useMinMax && (key < min || key > max)) {	joinResult = JoinUtil.JoinResult.NOMATCH;	} else {	joinResult = hashSet.contains(key, hashSetResults[0]);	}	}	if (LOG.isDebugEnabled()) {	
batch repeated joinresult 

joinResult = JoinUtil.JoinResult.NOMATCH;	} else {	joinResult = hashSet.contains(key, hashSetResults[0]);	}	}	if (LOG.isDebugEnabled()) {	}	finishLeftSemiRepeated(batch, joinResult, hashSetResults[0]);	} else {	if (LOG.isDebugEnabled()) {	
batch non repeated 

}	if (haveSaveKey) {	switch (saveJoinResult) {	case MATCH: break;	case SPILL: hashSetResultCount++;	break;	case NOMATCH: break;	}	}	if (LOG.isDebugEnabled()) {	
allmatchs spills spillhashmapresultindices hashmapresults 

========================= hive sample_4169 =========================

public static TimestampTZ parseOrNull(String s, ZoneId defaultTimeZone) {	try {	return parse(s, defaultTimeZone);	} catch (DateTimeParseException e) {	if (LOG.isDebugEnabled()) {	
invalid string for timestamp with time zone 

========================= hive sample_1384 =========================

public String signCookie(String str) {	if (str == null || str.isEmpty()) {	throw new IllegalArgumentException("NULL or empty string to sign");	}	String signature = getSignature(str);	if (LOG.isDebugEnabled()) {	
signature generated for is 

public String verifyAndExtract(String signedStr) {	int index = signedStr.lastIndexOf(SIGNATURE);	if (index == -1) {	throw new IllegalArgumentException("Invalid input sign: " + signedStr);	}	String originalSignature = signedStr.substring(index + SIGNATURE.length());	String rawValue = signedStr.substring(0, index);	String currentSignature = getSignature(rawValue);	if (LOG.isDebugEnabled()) {	
signature generated for inside verify is 

========================= hive sample_2402 =========================

public void run() {	TxnStore.MutexAPI.LockHandle handle = null;	try {	handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.WriteSetCleaner.name());	long startTime = System.currentTimeMillis();	txnHandler.performWriteSetGC();	
cleaner ran for seconds 

public void run() {	TxnStore.MutexAPI.LockHandle handle = null;	try {	handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.WriteSetCleaner.name());	long startTime = System.currentTimeMillis();	txnHandler.performWriteSetGC();	} catch(Throwable t) {	
serious error in 

========================= hive sample_1853 =========================

for (SQLForeignKey fk : fks) {	if (fk.getPktable_db().equals(fk.getFktable_db())) {	fk.setPktable_db(actualDbName);	}	fk.setFktable_db(actualDbName);	fk.setFktable_name(actualTblName);	}	AlterTableDesc addConstraintsDesc = new AlterTableDesc(actualDbName + "." + actualTblName, new ArrayList<SQLPrimaryKey>(), fks, new ArrayList<SQLUniqueConstraint>(), context.eventOnlyReplicationSpec());	Task<DDLWork> addConstraintsTask = TaskFactory.get(new DDLWork(readEntitySet, writeEntitySet, addConstraintsDesc), context.hiveConf);	tasks.add(addConstraintsTask);	
added add constrains task 

========================= hive sample_3478 =========================

private boolean isDataLengthWithInThreshold(ParseContext pctx, final long threshold) throws Exception {	if (splitSample != null && splitSample.getTotalLength() != null) {	if (LOG.isDebugEnabled()) {	
threshold exceeded for pseudomr mode 

if (LOG.isDebugEnabled()) {	}	return (threshold - splitSample.getTotalLength()) > 0;	}	Status status = checkThresholdWithMetastoreStats(table, partsList, threshold);	if (status.equals(Status.PASS)) {	return true;	} else if (status.equals(Status.FAIL)) {	return false;	} else {	
cannot fetch stats from metastore for table falling back to filesystem scan 

Utilities.setColumnNameList(jobConf, scanOp, true);	Utilities.setColumnTypeList(jobConf, scanOp, true);	HiveStorageHandler handler = table.getStorageHandler();	if (handler instanceof InputEstimator) {	InputEstimator estimator = (InputEstimator) handler;	TableDesc tableDesc = Utilities.getTableDesc(table);	PlanUtils.configureInputJobPropertiesForStorageHandler(tableDesc);	Utilities.copyTableJobPropertiesToConf(tableDesc, jobConf);	long len = estimator.estimate(jobConf, scanOp, threshold).getTotalLength();	if (LOG.isDebugEnabled()) {	
threshold exceeded for pseudomr mode 

if (LOG.isDebugEnabled()) {	}	return (threshold - len) > 0;	}	if (table.isNonNative()) {	return true;	}	if (!table.isPartitioned()) {	long len = getPathLength(jobConf, table.getPath(), table.getInputFormatClass(), threshold);	if (LOG.isDebugEnabled()) {	
threshold exceeded for pseudomr mode 

private Status checkThresholdWithMetastoreStats(final Table table, final PrunedPartitionList partsList, final long threshold) {	if (table != null && !table.isPartitioned()) {	long dataSize = StatsUtils.getTotalSize(table);	if (dataSize <= 0) {	
cannot determine basic stats for table from metastore falling back 

if (table != null && !table.isPartitioned()) {	long dataSize = StatsUtils.getTotalSize(table);	if (dataSize <= 0) {	return Status.UNAVAILABLE;	}	return (threshold - dataSize) >= 0 ? Status.PASS : Status.FAIL;	} else if (table != null && table.isPartitioned() && partsList != null) {	List<Long> dataSizes = StatsUtils.getBasicStatForPartitions(table, partsList.getNotDeniedPartns(), StatsSetupConst.TOTAL_SIZE);	long totalDataSize = StatsUtils.getSumIgnoreNegatives(dataSizes);	if (totalDataSize <= 0) {	
cannot determine basic stats for partitioned table from metastore falling back 

========================= hive sample_3028 =========================

try {	configuredOptions = parser.parse(validOptions, optsArr);	} catch (ParseException e) {	HelpFormatter formatter = new HelpFormatter();	formatter.printHelp( "[-" + ON_OOR_VALUE_OPT + "]", validOptions );	throw e;	}	Properties udfProps = UDFContext.getUDFContext().getUDFProperties(this.getClass(), new String[]{sign});	udfProps.put(ON_OORA_VALUE_PROP, configuredOptions.getOptionValue(ON_OOR_VALUE_OPT, getDefaultValue().name()));	if(LOG.isDebugEnabled()) {	
setting 

========================= hive sample_1015 =========================

if (shouldBreak) {	break;	}	} catch (InterruptedException e) {	handleInterrupt(e);	break;	} finally {	lock.unlock();	}	}	
callscheduler loop exiting 

private void handleInterrupt(InterruptedException e) {	if (isShutdown.get()) return;	
runloop interrupted without being shutdown first 

public void serviceStart() {	requestManagerFuture = requestManagerExecutor.submit(requestManager);	Futures.addCallback(requestManagerFuture, new FutureCallback<Void>() {	public void onSuccess(Void result) {	
requestmanager shutdown 

public void serviceStart() {	requestManagerFuture = requestManagerExecutor.submit(requestManager);	Futures.addCallback(requestManagerFuture, new FutureCallback<Void>() {	public void onSuccess(Void result) {	}	public void onFailure(Throwable t) {	if (!(t instanceof CancellationException)) {	
requestmanager shutdown with error 

private ProtocolType createProxy(final LlapNodeId nodeId, Token<TokenType> nodeToken) {	if (nodeToken == null && token == null) {	if (LOG.isDebugEnabled()) {	
creating a client without a token for 

}	return createProtocolImpl(getConfig(), nodeId.getHostname(), nodeId.getPort(), null, retryPolicy, socketFactory);	}	final UserGroupInformation ugi = UserGroupInformation.createRemoteUser(tokenUser);	if (nodeToken == null) {	nodeToken = new Token<TokenType>(token);	}	SecurityUtil.setTokenService(nodeToken, NetUtils.createSocketAddrForHost( nodeId.getHostname(), nodeId.getPort()));	ugi.addToken(nodeToken);	if (LOG.isDebugEnabled()) {	
creating a client for the token is 

========================= hive sample_522 =========================

public void release(String key) {	if (LOG.isDebugEnabled()) {	
no longer needed 

public <T> T retrieve(String key, Callable<T> fn) throws HiveException {	try {	if (LOG.isDebugEnabled()) {	
creating 

========================= hive sample_3881 =========================

public static void unregisterFile(Path cachePath) {	if (LOG.isDebugEnabled()) {	
unregistering 

public int read(byte[] array, final int arrayOffset, final int len) throws IOException {	long readStartPos = position;	DiskRangeList drl = new DiskRangeList(readStartPos, readStartPos + len);	DataCache.BooleanRef gotAllData = new DataCache.BooleanRef();	drl = cache.getFileData(fileKey, drl, 0, new DataCache.DiskRangeListFactory() {	public DiskRangeList createCacheChunk( MemoryBuffer buffer, long startOffset, long endOffset) {	return new CacheChunk(buffer, startOffset, endOffset);	}	}, gotAllData);	if (LOG.isInfoEnabled()) {	
buffers after cache 

private void copyDiskDataToCacheBuffer(byte[] diskData, int offsetInDiskData, int sizeToCopy, ByteBuffer cacheBuffer, DiskRange[] cacheRanges, int cacheRangeIx, long cacheRangeStart) {	int bbPos = cacheBuffer.position();	long cacheRangeEnd = cacheRangeStart + sizeToCopy;	if (LOG.isTraceEnabled()) {	
caching 

========================= hive sample_2793 =========================

public MoveWork(HashSet<ReadEntity> inputs, HashSet<WriteEntity> outputs, final LoadTableDesc loadTableWork, final LoadFileDesc loadFileWork, boolean checkFileFormat, boolean srcLocal) {	this(inputs, outputs);	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
creating movework with 

========================= hive sample_3244 =========================

final List<FileStatus> abortedDirectories = new ArrayList<>();	List<HdfsFileStatusWithId> childrenWithId = null;	Boolean val = useFileIds.value;	if (val == null || val) {	try {	childrenWithId = SHIMS.listLocatedHdfsStatus(fs, directory, hiddenFileFilter);	if (val == null) {	useFileIds.value = true;	}	} catch (Throwable t) {	
failed to get files with id using regular api 

assert stat.isDir();	List<HdfsFileStatusWithId> childrenWithId = null;	Boolean val = useFileIds.value;	if (val == null || val) {	try {	childrenWithId = SHIMS.listLocatedHdfsStatus(fs, stat.getPath(), hiddenFileFilter);	if (val == null) {	useFileIds.value = true;	}	} catch (Throwable t) {	
failed to get files with id using regular api 

Path dataFile = chooseFile(baseOrDeltaDir, fs);	if (dataFile == null) {	return false;	}	try {	Reader reader = OrcFile.createReader(dataFile, OrcFile.readerOptions(fs.getConf()));	acid file would have schema like <op, otid, writerId, rowid, ctid, <f1, ... fn>> so could check it this way once/if OrcRecordUpdater.ACID_KEY_INDEX_NAME is removed TypeDescription schema = reader.getSchema();	List<String> columns = schema.getFieldNames();	return OrcInputFormat.isOriginal(reader);	} catch (FileFormatException ex) {	
israwformat called on which is not an orc file 

========================= hive sample_3614 =========================

public List<InputSplit> getSplits(JobContext jobContext) throws IOException, InterruptedException {	if (LOG.isDebugEnabled()) {	
getsplits started 

public List<InputSplit> getSplits(JobContext jobContext) throws IOException, InterruptedException {	if (LOG.isDebugEnabled()) {	}	Configuration conf = ShimLoader.getHadoopShims().getConfiguration(jobContext);	List<OrcSplit> splits = OrcInputFormat.generateSplitsInfo(conf, createContext(conf, -1));	List<InputSplit> result = new ArrayList<InputSplit>(splits.size());	for(OrcSplit split: splits) {	result.add(new OrcNewSplit(split));	}	if (LOG.isDebugEnabled()) {	
getsplits finished 

========================= hive sample_3663 =========================

qb.setAnalyzeRewrite(true);	qbp = qb.getParseInfo();	analyzeRewrite = new AnalyzeRewriteContext();	analyzeRewrite.setTableName(tbl.getFullyQualifiedName());	analyzeRewrite.setTblLvl(isTableLevel);	analyzeRewrite.setColName(colNames);	analyzeRewrite.setColType(colType);	qbp.setAnalyzeRewrite(analyzeRewrite);	initCtx(ctx);	ctx.setExplainConfig(origCtx.getExplainConfig());	
invoking analyze on rewritten query 

analyzeRewrite.setTableName(tbl.getFullyQualifiedName());	analyzeRewrite.setTblLvl(isTableLevel);	analyzeRewrite.setColName(colNames);	analyzeRewrite.setColType(colType);	qbp.setAnalyzeRewrite(analyzeRewrite);	initCtx(ctx);	ctx.setExplainConfig(origCtx.getExplainConfig());	analyzeInternal(rewrittenTree);	} else {	initCtx(origCtx);	
invoking analyze on original query 

========================= hive sample_3399 =========================

public void setUp() throws Exception {	super.setUp();	if (setUpComplete) {	return;	}	Path intStringSeq = new Path(TEST_DATA_DIR + "/data/intString.seq");	
creating data file 

public void map(NullWritable key, HCatRecord value, Context context) throws IOException, InterruptedException {	
hcatrecord 

========================= hive sample_693 =========================

public static MiniLlapCluster startAndGetMiniLlapCluster(Configuration conf, MiniZooKeeperCluster miniZkCluster, String confDir) throws IOException {	MiniLlapCluster llapCluster;	
using conf dir 

}	Configuration daemonConf = new LlapDaemonConfiguration(conf);	final String clusterName = "llap";	final long maxMemory = LlapDaemon.getTotalHeapSize();	final long memoryForCache = (long) (0.15f * maxMemory);	final long totalExecutorMemory = (long) (0.75f * maxMemory);	final int numExecutors = HiveConf.getIntVar(conf, HiveConf.ConfVars.LLAP_DAEMON_NUM_EXECUTORS);	final boolean asyncIOEnabled = true;	final boolean directMemoryEnabled = false;	final int numLocalDirs = 1;	
minillap configs maxmemory memoryforcache totalexecutormemory numexecutors asyncioenabled directmemoryenabled numlocaldirs 

========================= hive sample_369 =========================

protected void reloadHashTable(byte pos, int partitionId) throws IOException, HiveException, SerDeException, ClassNotFoundException {	this.vectorMapJoinHashTable = null;	super.reloadHashTable(pos, partitionId);	MapJoinTableContainer smallTable = spilledMapJoinTables[pos];	vectorMapJoinHashTable = VectorMapJoinOptimizedCreateHashTable.createHashTable(conf, smallTable);	needHashTableSetup = true;	
created from 

protected void reloadHashTable(byte pos, int partitionId) throws IOException, HiveException, SerDeException, ClassNotFoundException {	this.vectorMapJoinHashTable = null;	super.reloadHashTable(pos, partitionId);	MapJoinTableContainer smallTable = spilledMapJoinTables[pos];	vectorMapJoinHashTable = VectorMapJoinOptimizedCreateHashTable.createHashTable(conf, smallTable);	needHashTableSetup = true;	if (LOG.isDebugEnabled()) {	
reloadhashtable 

protected void reProcessBigTable(int partitionId) throws HiveException {	if (LOG.isDebugEnabled()) {	
reprocessbigtable enter 

batchCount++;	}	}	if (spillReplayBatch.size > 0) {	process(spillReplayBatch, posBigTable);	spillReplayBatch.reset();	batchCount++;	}	bigTable.clear();	} catch (Exception e) {	
reprocessbigtable exception 

if (spillReplayBatch.size > 0) {	process(spillReplayBatch, posBigTable);	spillReplayBatch.reset();	batchCount++;	}	bigTable.clear();	} catch (Exception e) {	throw new HiveException(e);	}	if (LOG.isDebugEnabled()) {	
reprocessbigtable exit row processed and batches processed 

public void closeOp(boolean aborted) throws HiveException {	super.closeOp(aborted);	if (!aborted && overflowBatch.size > 0) {	forwardOverflow();	}	if (LOG.isDebugEnabled()) {	
vectormapjoininnerlongoperator closeop batches processed 

========================= hive sample_4149 =========================

public static List<String> addToClassPath(Map<String, Long> newPaths, Configuration conf, File localTmpDir) throws Exception {	URLClassLoader loader = (URLClassLoader) Thread.currentThread().getContextClassLoader();	List<URL> curPath = Lists.newArrayList(loader.getURLs());	List<String> localNewPaths = new ArrayList<>();	boolean newPathAdded = false;	for (Map.Entry<String, Long> entry : newPaths.entrySet()) {	URL newUrl = urlFromPathString(entry.getKey(), entry.getValue(), conf, localTmpDir);	localNewPaths.add(newUrl.toString());	if (newUrl != null && !curPath.contains(newUrl)) {	curPath.add(newUrl);	
added jar to classpath 

if (StringUtils.indexOf(path, "file:/") == 0) {	url = new URL(path);	} else if (StringUtils.indexOf(path, "hdfs:/") == 0 || StringUtils.indexOf(path, "viewfs:/") == 0) {	Path remoteFile = new Path(path);	Path localFile = new Path(localTmpDir.getAbsolutePath() + File.separator + remoteFile.getName());	Long currentTS = downloadedFiles.get(path);	if (currentTS == null) {	currentTS = -1L;	}	if (!new File(localFile.toString()).exists() || currentTS < timeStamp) {	
copying to 

if (!new File(localFile.toString()).exists() || currentTS < timeStamp) {	FileSystem remoteFS = remoteFile.getFileSystem(conf);	remoteFS.copyToLocalFile(remoteFile, localFile);	downloadedFiles.put(path, timeStamp);	}	return urlFromPathString(localFile.toString(), timeStamp, conf, localTmpDir);	} else {	url = new File(path).toURL();	}	} catch (Exception err) {	
bad url ignoring path 

========================= hive sample_555 =========================

this.jobTimeoutConfigName = jobTimeoutConfigName;	this.requestType = requestType;	this.enableCancelTask = enableCancelTask;	int threads = !StringUtils.isEmpty(concurrentRequestsConfigName) ? appConf.getInt(concurrentRequestsConfigName, 0) : 0;	if (threads > 0) {	this.jobExecutePool = new ThreadPoolExecutor(threads, threads, threadKeepAliveTimeInHours, TimeUnit.HOURS, new SynchronousQueue<Runnable>());	this.jobExecutePool.allowCoreThreadTimeOut(true);	if (!StringUtils.isEmpty(jobTimeoutConfigName)) {	this.requestExecutionTimeoutInSec = appConf.getInt(jobTimeoutConfigName, 0);	}	
configured threads for job request type with time out s 

this.requestType = requestType;	this.enableCancelTask = enableCancelTask;	int threads = !StringUtils.isEmpty(concurrentRequestsConfigName) ? appConf.getInt(concurrentRequestsConfigName, 0) : 0;	if (threads > 0) {	this.jobExecutePool = new ThreadPoolExecutor(threads, threads, threadKeepAliveTimeInHours, TimeUnit.HOURS, new SynchronousQueue<Runnable>());	this.jobExecutePool.allowCoreThreadTimeOut(true);	if (!StringUtils.isEmpty(jobTimeoutConfigName)) {	this.requestExecutionTimeoutInSec = appConf.getInt(jobTimeoutConfigName, 0);	}	} else {	
no thread pool configured for job request type 

public T execute(JobCallable<T> jobExecuteCallable) throws InterruptedException, TimeoutException, TooManyRequestsException, ExecutionException {	assert (jobExecuteCallable != null);	assert (this.jobExecutePool != null);	String type = this.requestType.toString().toLowerCase();	String retryMessageForConcurrentRequests = "Please wait for some time before retrying " + "the operation. Please refer to the config " + concurrentRequestsConfigName + " to configure concurrent requests.";	
starting new job request with time out seconds 

if ((result = tryGetJobResultOrSetJobStateFailed(jobExecuteCallable)) == null) {	String message = this.requestType + " job request got cancelled and thread got interrupted. " + "Please wait for some time before retrying the operation.";	LOG.warn(message);	throw new InterruptedException(message);	}	} finally {	if (enableCancelTask) {	cancelExecutePoolThread(future);	}	}	
completed job request 

private void cancelExecutePoolThread(Future<T> future) {	int retryCount = 0;	while(retryCount < this.maxTaskCancelRetryCount && !future.isDone()) {	
task is still executing the job request cancelling it with retry count 

private void cancelExecutePoolThread(Future<T> future) {	int retryCount = 0;	while(retryCount < this.maxTaskCancelRetryCount && !future.isDone()) {	if (future.cancel(true)) {	
cancel job request issued successfully 

while(retryCount < this.maxTaskCancelRetryCount && !future.isDone()) {	if (future.cancel(true)) {	return;	}	retryCount++;	try {	Thread.sleep(this.maxTaskCancelRetryWaitTimeInMs);	} catch (InterruptedException e) {	}	}	
failed to cancel the job iscancelled retry count 

private T tryGetJobResultOrSetJobStateFailed(JobCallable<T> jobExecuteCallable) {	if (!jobExecuteCallable.setJobStateFailed()) {	
job is already completed returning the result 

private T tryGetJobResultOrSetJobStateFailed(JobCallable<T> jobExecuteCallable) {	if (!jobExecuteCallable.setJobStateFailed()) {	return jobExecuteCallable.returnResult;	} else {	
job status set to failed job clean up to be done by execute thread after job request is executed 

========================= hive sample_806 =========================

private int execCommand(String cmd) throws Exception {	int exitCode;	try {	String output = Shell.execCommand("bash", "-c", cmd);	
output from 

hs2Conn.close();	Assert.assertEquals("Expected exit code of 1", 1, execCommand("openssl s_client -connect " + miniHS2.getHost() + ":" + miniHS2.getBinaryPort() + " -ssl2 < /dev/null"));	Assert.assertEquals("Expected exit code of 1", 1, execCommand("openssl s_client -connect " + miniHS2.getHost() + ":" + miniHS2.getBinaryPort() + " -ssl3 < /dev/null"));	miniHS2.stop();	SSLTestUtils.setHttpConfOverlay(confOverlay);	miniHS2.start(confOverlay);	try {	hs2Conn = DriverManager.getConnection(miniHS2.getJdbcURL() + ";ssl=true;sslTrustStore=" + dataFileDir + File.separator + TRUST_STORE_NAME + ";trustStorePassword=" + KEY_STORE_TRUST_STORE_PASSWORD, System.getProperty("user.name"), "bar");	Assert.fail("Expected SQLException during connect");	} catch (SQLException e) {	
expected exception 

========================= hive sample_294 =========================

public HiveLockManager getLockManager() throws LockException {	if (lockMgr == null) {	boolean supportConcurrency = conf.getBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY);	if (supportConcurrency) {	String lockMgrName = conf.getVar(HiveConf.ConfVars.HIVE_LOCK_MANAGER);	if ((lockMgrName == null) || (lockMgrName.isEmpty())) {	throw new LockException(ErrorMsg.LOCKMGR_NOT_SPECIFIED.getMsg());	}	try {	
creating lock manager of type 

if (lockMgr != null) {	try {	lockMgr.close();	} catch (LockException e1) {	}	lockMgr = null;	}	throw new LockException(ErrorMsg.LOCKMGR_NOT_INITIALIZED.getMsg() + e.getMessage());	}	} else {	
concurrency mode is disabled not creating a lock manager 

public void acquireLocks(QueryPlan plan, Context ctx, String username, LockedDriverState lDrvState) throws LockException {	getLockManager();	if (lockMgr == null) return;	List<HiveLockObj> lockObjects = new ArrayList<HiveLockObj>();	for (ReadEntity input : plan.getInputs()) {	if (!input.needsLock()) {	continue;	}	
adding to list of lock inputs 

lockObjects.addAll(getLockObjects(plan, null, input.getTable(), null, HiveLockMode.SHARED));	} else {	lockObjects.addAll(getLockObjects(plan, null, null, input.getPartition(), HiveLockMode.SHARED));	}	}	for (WriteEntity output : plan.getOutputs()) {	HiveLockMode lockMode = getWriteEntityLockMode(output);	if (lockMode == null) {	continue;	}	
adding to list of lock outputs 

protected void destruct() {	if (lockMgr != null) {	try {	lockMgr.close();	} catch (LockException e) {	
got exception when closing lock manager 

========================= hive sample_3771 =========================

public void handle(Context withinContext) throws Exception {	
processing add notnullconstraint message message 

========================= hive sample_3458 =========================

public void stop() {	if (!(this.isStopping() || this.isStopped())) {	super.stop();	if (appenderControl.get() != null) {	appenderControl.get().stop();	realAppender.get().stop();	}	if (LOGGER.isDebugEnabled()) {	
stop invoked for 

public void stop() {	if (!(this.isStopping() || this.isStopped())) {	super.stop();	if (appenderControl.get() != null) {	appenderControl.get().stop();	realAppender.get().stop();	}	if (LOGGER.isDebugEnabled()) {	}	if (realAppender.get() == null) {	
realappender is null ignoring stop 

}	RandomAccessFileAppender raf = (RandomAccessFileAppender) realAppender.get();	Path renamedPath = null;	if (renameFileOnClose) {	try {	int counter = 0;	while(true) {	renamedPath = getRenamedPath(raf.getFileName(), counter);	if (!Files.exists(renamedPath)) {	if (LOGGER.isTraceEnabled()) {	
renaming file to 

renamedPath = getRenamedPath(raf.getFileName(), counter);	if (!Files.exists(renamedPath)) {	if (LOGGER.isTraceEnabled()) {	}	Files.move(Paths.get(raf.getFileName()), renamedPath);	break;	}	counter++;	}	} catch (IOException e) {	
failed to rename file to 

public static LlapWrappedAppender createAppender( @PluginAttribute("name") final String name, @PluginAttribute("name") final String name, @PluginAttribute("renameFileOnClose") final String renameFileOnCloseProvided, @PluginAttribute("renameFileOnClose") final String renameFileOnCloseProvided, @PluginAttribute("renamedFileSuffix") final String renamedFileSuffixProvided, @PluginAttribute("renamedFileSuffix") final String renamedFileSuffixProvided, @PluginNode final Node node, @PluginNode final Node node, @PluginConfiguration final Configuration config @PluginConfiguration final Configuration config ) {	if (config == null) {	
pluginconfiguration not expected to be null 

public static LlapWrappedAppender createAppender( @PluginAttribute("name") final String name, @PluginAttribute("name") final String name, @PluginAttribute("renameFileOnClose") final String renameFileOnCloseProvided, @PluginAttribute("renameFileOnClose") final String renameFileOnCloseProvided, @PluginAttribute("renamedFileSuffix") final String renamedFileSuffixProvided, @PluginAttribute("renamedFileSuffix") final String renamedFileSuffixProvided, @PluginNode final Node node, @PluginNode final Node node, @PluginConfiguration final Configuration config @PluginConfiguration final Configuration config ) {	if (config == null) {	return null;	}	if (node == null) {	
node must be specified as an appender specification 

========================= hive sample_2249 =========================

String colType = null;	String colName = null;	boolean doAllPartitionContainStats = partNames.size() == colStatsWithSourceInfo.size();	NumDistinctValueEstimator ndvEstimator = null;	for (ColStatsObjWithSourceInfo csp : colStatsWithSourceInfo) {	ColumnStatisticsObj cso = csp.getColStatsObj();	if (statsObj == null) {	colName = cso.getColName();	colType = cso.getColType();	statsObj = ColumnStatsAggregatorFactory.newColumnStaticsObj(colName, colType, cso.getStatsData().getSetField());	
doallpartitioncontainstats for column is 

} else if (estimation > higherBound) {	estimation = higherBound;	}	} else {	estimation = (long) (lowerBound + (higherBound - lowerBound) * ndvTuner);	}	aggregateData.setNumDVs(estimation);	}	columnStatisticsData.setDoubleStats(aggregateData);	} else {	
start extrapolation for 

ColumnStatisticsData csd = new ColumnStatisticsData();	csd.setDoubleStats(aggregateData);	adjustedStatsMap.put(pseudoPartName.toString(), csd);	if (useDensityFunctionForNDVEstimation) {	densityAvgSum += (aggregateData.getHighValue() - aggregateData.getLowValue()) / aggregateData.getNumDVs();	}	}	}	extrapolate(columnStatisticsData, partNames.size(), colStatsWithSourceInfo.size(), adjustedIndexMap, adjustedStatsMap, densityAvgSum / adjustedStatsMap.size());	}	
ndv estimatation for is of partitions requested of partitions found 

========================= hive sample_1936 =========================

}	if (uri.equalsIgnoreCase(URL_PREFIX)) {	connParams.setEmbeddedMode(true);	return connParams;	}	String dummyAuthorityString = "dummyhost:00000";	String suppliedAuthorities = getAuthorities(uri, connParams);	if ((suppliedAuthorities == null) || (suppliedAuthorities.isEmpty())) {	connParams.setEmbeddedMode(true);	} else {	
supplied authorities 

newUsage = usageUrlBase + JdbcConnectionParams.TRANSPORT_MODE + "=<transport_mode_value>";	handleParamDeprecation(connParams.getHiveConfs(), connParams.getSessionVars(), JdbcConnectionParams.TRANSPORT_MODE_DEPRECATED, JdbcConnectionParams.TRANSPORT_MODE, newUsage);	newUsage = usageUrlBase + JdbcConnectionParams.HTTP_PATH + "=<http_path_value>";	handleParamDeprecation(connParams.getHiveConfs(), connParams.getSessionVars(), JdbcConnectionParams.HTTP_PATH_DEPRECATED, JdbcConnectionParams.HTTP_PATH, newUsage);	if (connParams.isEmbeddedMode()) {	connParams.setHost(jdbcURI.getHost());	connParams.setPort(jdbcURI.getPort());	} else {	configureConnParams(connParams);	String authorityStr = connParams.getHost() + ":" + connParams.getPort();	
resolved authority 

private static void handleParamDeprecation(Map<String, String> fromMap, Map<String, String> toMap, String deprecatedName, String newName, String newUsage) {	if (fromMap.containsKey(deprecatedName)) {	
jdbc param deprecation 

private static void handleParamDeprecation(Map<String, String> fromMap, Map<String, String> toMap, String deprecatedName, String newName, String newUsage) {	if (fromMap.containsKey(deprecatedName)) {	
the use of is deprecated 

private static void handleParamDeprecation(Map<String, String> fromMap, Map<String, String> toMap, String deprecatedName, String newName, String newUsage) {	if (fromMap.containsKey(deprecatedName)) {	
please use like so 

static boolean updateConnParamsFromZooKeeper(JdbcConnectionParams connParams) {	connParams.getRejectedHostZnodePaths().add(connParams.getCurrentHostZnodePath());	String oldServerHost = connParams.getHost();	int oldServerPort = connParams.getPort();	try {	ZooKeeperHiveClientHelper.configureConnParams(connParams);	connParams.setJdbcUriString(connParams.getJdbcUriString().replace( oldServerHost + ":" + oldServerPort, connParams.getHost() + ":" + connParams.getPort()));	
selected instance with uri 

public static String getCanonicalHostName(String hostName) {	try {	return InetAddress.getByName(hostName).getCanonicalHostName();	}	catch(UnknownHostException exception) {	
could not retrieve canonical hostname for 

========================= hive sample_1170 =========================

private int waitForAllThreadsToStart(JobRunnable jobRunnable, int poolThreadCount) {	int currentId = jobRunnable.threadStartCount.incrementAndGet();	
waiting for other threads with thread id 

private int waitForAllThreadsToStart(JobRunnable jobRunnable, int poolThreadCount) {	int currentId = jobRunnable.threadStartCount.incrementAndGet();	synchronized(lock) {	if (currentId > poolThreadCount) {	
waking up all threads 

public JobRunnable ConcurrentJobsStatus(final int threadCount, AppConfig appConfig, final boolean killThreads, boolean interruptThreads, final Answer<QueueStatusBean> answer) throws IOException, InterruptedException, QueueException, NotAuthorizedException, BadParam, BusyException {	StatusDelegator delegator = new StatusDelegator(appConfig);	final StatusDelegator mockDelegator = Mockito.spy(delegator);	Mockito.doAnswer(answer).when(mockDelegator).getJobStatus(Mockito.any(String.class), Mockito.any(String.class));	JobRunnable statusJobRunnable = new JobRunnable() {	public void run() {	try {	int threadId = waitForAllThreadsToStart(this, threadCount);	
started executing job status operation threadid 

public JobRunnable ConcurrentListJobs(final int threadCount, AppConfig config, final boolean killThreads, boolean interruptThreads, final Answer<List<JobItemBean>> answer) throws IOException, InterruptedException, QueueException, NotAuthorizedException, BadParam, BusyException {	ListDelegator delegator = new ListDelegator(config);	final ListDelegator mockDelegator = Mockito.spy(delegator);	Mockito.doAnswer(answer).when(mockDelegator).listJobs(Mockito.any(String.class), Mockito.any(boolean.class), Mockito.any(String.class), Mockito.any(int.class), Mockito.any(boolean.class));	JobRunnable listJobRunnable = new JobRunnable() {	public void run() {	try {	int threadId = waitForAllThreadsToStart(this, threadCount);	
started executing job list operation threadid 

TempletonControllerJob mockCtrl = Mockito.mock(TempletonControllerJob.class);	Mockito.doReturn(jobIdResponse).when(mockCtrl).getSubmittedId();	Mockito.doReturn(mockCtrl).when(mockDelegator).getTempletonController();	Mockito.doAnswer(responseAnswer).when(mockDelegator).runTempletonControllerJob( Mockito.any(TempletonControllerJob.class), Mockito.any(List.class));	Mockito.doAnswer(timeoutResponseAnswer).when(mockDelegator).killJob( Mockito.any(String.class), Mockito.any(String.class));	Mockito.doNothing().when(mockDelegator).registerJob(Mockito.any(String.class), Mockito.any(String.class), Mockito.any(String.class), Mockito.any(Map.class));	JobRunnable submitJobRunnable = new JobRunnable() {	public void run() {	try {	int threadId = waitForAllThreadsToStart(this, threadCount);	
started executing job submit operation threadid 

public void executeJobOperations(JobRunnable jobRunnable, int threadCount, boolean killThreads, boolean interruptThreads) throws IOException, InterruptedException, QueueException, NotAuthorizedException {	started = false;	ExecutorService executorService = new ThreadPoolExecutor(threadCount, threadCount, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue<Runnable>());;	ArrayList<Future<?>> futures = new ArrayList<Future<?>>();	for (int i = 0; i < threadCount; i++) {	futures.add(executorService.submit(jobRunnable));	}	waitForAllThreadsToStart(jobRunnable, threadCount);	
started all threads 

ArrayList<Future<?>> futures = new ArrayList<Future<?>>();	for (int i = 0; i < threadCount; i++) {	futures.add(executorService.submit(jobRunnable));	}	waitForAllThreadsToStart(jobRunnable, threadCount);	if (killThreads) {	executorService.shutdownNow();	} else {	if (interruptThreads){	for (Future<?> future : futures) {	
cancelling the thread 

executorService.shutdownNow();	} else {	if (interruptThreads){	for (Future<?> future : futures) {	future.cancel(true);	}	}	executorService.shutdown();	}	if (!executorService.awaitTermination(60, TimeUnit.SECONDS)) {	
force shutting down the pool 

========================= hive sample_778 =========================

public void stop() {	
encoded reader is being stopped 

protected Void performDataRead() throws IOException, InterruptedException {	long startTime = counters.startTimeCounter();	
processing data for file 

if (processStop()) {	recordReaderTime(startTime);	return null;	}	int stripeIx = stripeIxFrom + stripeIxMod;	boolean[] rgs = null;	OrcStripeMetadata stripeMetadata = null;	StripeInformation si;	try {	si = fileMetadata.getStripes().get(stripeIx);	
reading stripe 

}	int stripeIx = stripeIxFrom + stripeIxMod;	boolean[] rgs = null;	OrcStripeMetadata stripeMetadata = null;	StripeInformation si;	try {	si = fileMetadata.getStripes().get(stripeIx);	trace.logReadingStripe(stripeIx, si.getOffset(), si.getLength());	rgs = stripeRgs[stripeIxMod];	if (LlapIoImpl.ORC_LOGGER.isTraceEnabled()) {	
stripergs 

}	try {	stripeReader.readEncodedColumns(stripeIx, si, stripeMetadata.getRowIndexes(), stripeMetadata.getEncodings(), stripeMetadata.getStreams(), fileIncludes, rgs, consumer);	} catch (Throwable t) {	handleReaderError(startTime, t);	return null;	}	}	recordReaderTime(startTime);	consumer.setDone();	
done processing 

private void validateFileMetadata() throws IOException {	if (fileMetadata.getCompressionKind() == CompressionKind.NONE) return;	int bufferSize = fileMetadata.getCompressionBufferSize();	long minAllocSize = HiveConf.getSizeVar(daemonConf, ConfVars.LLAP_ALLOCATOR_MIN_ALLOC);	if (bufferSize < minAllocSize) {	
orc compression buffer size is smaller than llap low level cache minimum allocation size decrease the value for to avoid wasting memory 

private boolean processStop() {	if (!isStopped) return false;	
encoded data reader is stopping 

private static Object determineFileId(FileSystem fs, FileSplit split, boolean allowSynthetic, boolean checkDefaultFs) throws IOException {	if (split instanceof OrcSplit) {	Object fileKey = ((OrcSplit)split).getFileKey();	if (fileKey != null) {	return fileKey;	}	}	
split for does not have file id 

private void ensureOrcReader() throws IOException {	if (orcReader != null) return;	path = split.getPath();	if (fileKey instanceof Long && HiveConf.getBoolVar( daemonConf, ConfVars.LLAP_IO_USE_FILEID_PATH)) {	path = HdfsUtils.getFileIdPath(fs, path, (long)fileKey);	}	
creating reader for 

} finally {	metadataCache.decRefBuffer(footerBuffers);	}	}	counters.incrCounter(LlapIOCounters.METADATA_CACHE_MISS);	}	long offset = si.getOffset() + si.getIndexLength() + si.getDataLength();	long startTime = counters.startTimeCounter();	ensureRawDataReader(true);	if (LOG.isTraceEnabled()) {	
reading based on 

public void returnData(OrcEncodedColumnBatch ecb) {	for (int colIx = 0; colIx < ecb.getTotalColCount(); ++colIx) {	if (!ecb.hasData(colIx)) continue;	ColumnStreamData[] datas = ecb.getColumnData(colIx);	for (ColumnStreamData data : datas) {	if (data == null || data.decRef() != 0) continue;	if (LlapIoImpl.LOCKING_LOGGER.isTraceEnabled()) {	for (MemoryBuffer buf : data.getCacheBuffers()) {	
unlocking at the end of processing 

int rgCount = getRgCount(stripe, rowIndexStride);	boolean[] rgsToRead = null;	if (sargApp != null) {	OrcStripeMetadata stripeMetadata = metadata.get(stripeIxMod);	rgsToRead = sargApp.pickRowGroups(stripe, stripeMetadata.getRowIndexes(), stripeMetadata.getBloomFilterKinds(), stripeMetadata.getEncodings(), stripeMetadata.getBloomFilterIndexes(), true);	}	boolean isNone = rgsToRead == RecordReaderImpl.SargApplier.READ_NO_RGS, isAll = rgsToRead == RecordReaderImpl.SargApplier.READ_ALL_RGS;	hasAnyData = hasAnyData || !isNone;	if (LlapIoImpl.ORC_LOGGER.isTraceEnabled()) {	if (isNone) {	
sarg eliminated all rgs for stripe 

if (sargApp != null) {	OrcStripeMetadata stripeMetadata = metadata.get(stripeIxMod);	rgsToRead = sargApp.pickRowGroups(stripe, stripeMetadata.getRowIndexes(), stripeMetadata.getBloomFilterKinds(), stripeMetadata.getEncodings(), stripeMetadata.getBloomFilterIndexes(), true);	}	boolean isNone = rgsToRead == RecordReaderImpl.SargApplier.READ_NO_RGS, isAll = rgsToRead == RecordReaderImpl.SargApplier.READ_ALL_RGS;	hasAnyData = hasAnyData || !isNone;	if (LlapIoImpl.ORC_LOGGER.isTraceEnabled()) {	if (isNone) {	trace.logSargResult(stripeIx, 0);	} else if (!isAll) {	
sarg picked rgs for stripe 

rgsToRead = sargApp.pickRowGroups(stripe, stripeMetadata.getRowIndexes(), stripeMetadata.getBloomFilterKinds(), stripeMetadata.getEncodings(), stripeMetadata.getBloomFilterIndexes(), true);	}	boolean isNone = rgsToRead == RecordReaderImpl.SargApplier.READ_NO_RGS, isAll = rgsToRead == RecordReaderImpl.SargApplier.READ_ALL_RGS;	hasAnyData = hasAnyData || !isNone;	if (LlapIoImpl.ORC_LOGGER.isTraceEnabled()) {	if (isNone) {	trace.logSargResult(stripeIx, 0);	} else if (!isAll) {	trace.logSargResult(stripeIx, rgsToRead);	} else {	
will read all rgs for stripe 

stripeIxFrom = stripeIx;	}	if (stripeStart >= maxOffset) {	stripeIxTo = stripeIx;	LlapIoImpl.ORC_LOGGER.trace("Including stripes until {} ({} >= {}); {} stripes", stripeIxTo, stripeStart, maxOffset, (stripeIxTo - stripeIxFrom));	break;	}	++stripeIx;	}	if (stripeIxFrom == -1) {	
not including any stripes empty split 

stripeIxTo = stripeIx;	LlapIoImpl.ORC_LOGGER.trace("Including stripes until {} ({} >= {}); {} stripes", stripeIxTo, stripeStart, maxOffset, (stripeIxTo - stripeIxFrom));	break;	}	++stripeIx;	}	if (stripeIxFrom == -1) {	}	if (stripeIxTo == -1 && stripeIxFrom != -1) {	stripeIxTo = stripeIx;	
including stripes until end of file stripes 

public DiskRangeList getFileData(Object fileKey, DiskRangeList range, long baseOffset, DiskRangeListFactory factory, BooleanRef gotAllData) {	DiskRangeList result = lowLevelCache.getFileData( fileKey, range, baseOffset, factory, counters, gotAllData);	if (LlapIoImpl.ORC_LOGGER.isTraceEnabled()) {	
disk ranges after data cache file base offset 

public DiskRangeList readFileData(DiskRangeList range, long baseOffset, boolean doForceDirect) throws IOException {	long startTime = counters.startTimeCounter();	DiskRangeList result = orcDataReader.readFileData(range, baseOffset, doForceDirect);	counters.recordHdfsTime(startTime);	if (LlapIoImpl.ORC_LOGGER.isTraceEnabled()) {	
disk ranges after disk read file base offset 

========================= hive sample_2199 =========================

public SimpleAllocator(Configuration conf) {	isDirect = HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOCATOR_DIRECT);	if (LlapIoImpl.LOG.isInfoEnabled()) {	
simple allocator with direct byte buffers 

public void deallocate(MemoryBuffer buffer) {	LlapAllocatorBuffer buf = (LlapAllocatorBuffer)buffer;	ByteBuffer bb = buf.byteBuffer;	buf.byteBuffer = null;	if (!bb.isDirect()) return;	Field field = cleanerField;	if (field == null) return;	try {	((Cleaner)field.get(bb)).clean();	} catch (Throwable t) {	
error using directbytebuffer cleaner stopping its use 

========================= hive sample_2176 =========================

sleepTime = conf.getTimeVar( HiveConf.ConfVars.HIVE_LOCK_SLEEP_BETWEEN_RETRIES, TimeUnit.MILLISECONDS);	numRetriesForLock = conf.getIntVar(HiveConf.ConfVars.HIVE_LOCK_NUMRETRIES);	numRetriesForUnLock = conf.getIntVar(HiveConf.ConfVars.HIVE_UNLOCK_NUMRETRIES);	try {	curatorFramework = CuratorFrameworkSingleton.getInstance(conf);	parent = conf.getVar(HiveConf.ConfVars.HIVE_ZOOKEEPER_NAMESPACE);	try{	curatorFramework.create().withMode(CreateMode.PERSISTENT).forPath("/" +  parent, new byte[0]);	} catch (Exception e) {	if (!(e instanceof KeeperException) || ((KeeperException)e).code() != KeeperException.Code.NODEEXISTS) {	
unexpected zk exception when creating parent node 

try {	curatorFramework = CuratorFrameworkSingleton.getInstance(conf);	parent = conf.getVar(HiveConf.ConfVars.HIVE_ZOOKEEPER_NAMESPACE);	try{	curatorFramework.create().withMode(CreateMode.PERSISTENT).forPath("/" +  parent, new byte[0]);	} catch (Exception e) {	if (!(e instanceof KeeperException) || ((KeeperException)e).code() != KeeperException.Code.NODEEXISTS) {	}	}	} catch (Exception e) {	
failed to create curatorframework object 

if (lDrvState.isAborted()) {	isInterrupted = true;	}	lDrvState.stateLock.unlock();	}	if (!isInterrupted) {	try {	lock = lock(lockObject.getObj(), lockObject.getMode(), keepAlive, true);	} catch (LockException e) {	console.printError("Error in acquireLocks..." );	
error in acquirelocks 

public void releaseLocks(List<HiveLock> hiveLocks) {	if (hiveLocks != null) {	int len = hiveLocks.size();	for (int pos = len-1; pos >= 0; pos--) {	HiveLock hiveLock = hiveLocks.get(pos);	try {	
about to release lock for 

public void releaseLocks(List<HiveLock> hiveLocks) {	if (hiveLocks != null) {	int len = hiveLocks.size();	for (int pos = len-1; pos >= 0; pos--) {	HiveLock hiveLock = hiveLocks.get(pos);	try {	unlock(hiveLock);	} catch (LockException e) {	
error when releasing lock 

private ZooKeeperHiveLock lock (HiveLockObject key, HiveLockMode mode, boolean keepAlive, boolean parentCreated) throws LockException {	
acquiring lock for with mode 

}	ret = lockPrimitive(key, mode, keepAlive, parentCreated, conflictingLocks);	if (ret != null) {	break;	}	} catch (Exception e1) {	lastException = e1;	if (e1 instanceof KeeperException) {	KeeperException e = (KeeperException) e1;	switch (e.code()) {	
possibly transient zookeeper exception 

ret = lockPrimitive(key, mode, keepAlive, parentCreated, conflictingLocks);	if (ret != null) {	break;	}	} catch (Exception e1) {	lastException = e1;	if (e1 instanceof KeeperException) {	KeeperException e = (KeeperException) e1;	switch (e.code()) {	break;	
serious zookeeper exception 

}	} catch (Exception e1) {	lastException = e1;	if (e1 instanceof KeeperException) {	KeeperException e = (KeeperException) e1;	switch (e.code()) {	break;	break;	}	} else {	
other unexpected exception 

break;	}	} else {	}	}	} while (tryNum < numRetriesForLock);	if (ret == null) {	console.printError("Unable to acquire " + key.getData().getLockMode() + ", " + mode + " lock " + key.getDisplayName() + " after " + tryNum + " attempts.");	printConflictingLocks(key,mode,conflictingLocks);	if (lastException != null) {	
exceeds maximum retries with errors 

private void printConflictingLocks(HiveLockObject key, HiveLockMode mode, Set<String> conflictingLocks) {	if (!conflictingLocks.isEmpty()) {	HiveLockObjectData requestedLock = new HiveLockObjectData(key.getData().toString());	
requested lock mode query 

private void printConflictingLocks(HiveLockObject key, HiveLockMode mode, Set<String> conflictingLocks) {	if (!conflictingLocks.isEmpty()) {	HiveLockObjectData requestedLock = new HiveLockObjectData(key.getData().toString());	for (String conflictingLock : conflictingLocks) {	HiveLockObjectData conflictingLockData = new HiveLockObjectData(conflictingLock);	
conflicting lock to mode query queryid clientip 

try {	switch(mode) {	case EXCLUSIVE: metrics.incrementCounter(MetricsConstant.ZOOKEEPER_HIVE_EXCLUSIVELOCKS);	break;	case SEMI_SHARED: metrics.incrementCounter(MetricsConstant.ZOOKEEPER_HIVE_SEMISHAREDLOCKS);	break;	default: metrics.incrementCounter(MetricsConstant.ZOOKEEPER_HIVE_SHAREDLOCKS);	break;	}	} catch (Exception e) {	
error reporting hive client zookeeper lock operation to metrics system 

try {	tryNum++;	if (tryNum > 1) {	Thread.sleep(sleepTime);	}	unlockPrimitive(hiveLock, parent, curatorFramework);	break;	} catch (Exception e) {	if (tryNum >= numRetriesForUnLock) {	String name = ((ZooKeeperHiveLock)hiveLock).getPath();	
node can not be deleted after attempts 

try {	switch(lMode) {	case EXCLUSIVE: metrics.decrementCounter(MetricsConstant.ZOOKEEPER_HIVE_EXCLUSIVELOCKS);	break;	case SEMI_SHARED: metrics.decrementCounter(MetricsConstant.ZOOKEEPER_HIVE_SEMISHAREDLOCKS);	break;	default: metrics.decrementCounter(MetricsConstant.ZOOKEEPER_HIVE_SHAREDLOCKS);	break;	}	} catch (Exception e) {	
error reporting hive client zookeeper unlock operation to metrics system 

break;	case SEMI_SHARED: metrics.decrementCounter(MetricsConstant.ZOOKEEPER_HIVE_SEMISHAREDLOCKS);	break;	default: metrics.decrementCounter(MetricsConstant.ZOOKEEPER_HIVE_SHAREDLOCKS);	break;	}	} catch (Exception e) {	}	}	} catch (KeeperException.NoNodeException nne) {	
node or its parent has already been deleted 

case SEMI_SHARED: metrics.decrementCounter(MetricsConstant.ZOOKEEPER_HIVE_SEMISHAREDLOCKS);	break;	default: metrics.decrementCounter(MetricsConstant.ZOOKEEPER_HIVE_SHAREDLOCKS);	break;	}	} catch (Exception e) {	}	}	} catch (KeeperException.NoNodeException nne) {	} catch (KeeperException.NotEmptyException nee) {	
node to be deleted is not empty 

break;	default: metrics.decrementCounter(MetricsConstant.ZOOKEEPER_HIVE_SHAREDLOCKS);	break;	}	} catch (Exception e) {	}	}	} catch (KeeperException.NoNodeException nne) {	} catch (KeeperException.NotEmptyException nee) {	} catch (Exception e) {	
failed to release zookeeper lock 

unlockPrimitive(lock, parent, curatorFramework);	} catch (Exception e) {	lastExceptionGot = e;	}	}	}	if(lastExceptionGot != null) {	throw lastExceptionGot;	}	} catch (Exception e) {	
failed to release all locks 

HiveLockObject obj = getLockObject(conf, curChild, mode, data, parent, verifyTablePartition);	if (obj == null) {	continue;	}	if ((key == null) || (obj.getName().equals(key.getName()))) {	if (fetchData) {	try {	data = new HiveLockObjectData(new String(curatorFramework.getData().watched().forPath(curChild)));	data.setClientIp(clientIp);	} catch (Exception e) {	
error in getting data for 

private void removeAllRedundantNodes() {	try {	checkRedundantNode("/" + parent);	} catch (Exception e) {	
exception while removing all redundant nodes 

}	List<String> children = curatorFramework.getChildren().forPath(node);	for (String child : children) {	checkRedundantNode(node + "/" + child);	}	children = curatorFramework.getChildren().forPath(node);	if ((children == null) || (children.isEmpty())) {	curatorFramework.delete().forPath(node);	}	} catch (Exception e) {	
error in checkredundantnode for node 

public void close() throws LockException {	try {	if (HiveConf.getBoolVar(ctx.getConf(), HiveConf.ConfVars.HIVE_ZOOKEEPER_CLEAN_EXTRA_NODES)) {	removeAllRedundantNodes();	}	} catch (Exception e) {	
failed to close zookeeper client 

try {	partn = db.getPartition(tab, partSpec, false);	} catch (HiveException e) {	partn = null;	}	if (partn == null) {	return new HiveLockObject(new DummyPartition(tab, path, partSpec), data);	}	return new HiveLockObject(partn, data);	} catch (Exception e) {	
failed to create zookeeper object 

========================= hive sample_3767 =========================

private FunctionInfo registerToSessionRegistry(String qualifiedName, FunctionInfo function) {	FunctionInfo ret = null;	ClassLoader prev = Utilities.getSessionSpecifiedClassLoader();	try {	FunctionResource[] resources = function.getResources();	try {	FunctionTask.addFunctionResources(resources);	} catch (Exception e) {	
unable to load resources for 

FunctionResource[] resources = function.getResources();	try {	FunctionTask.addFunctionResources(resources);	} catch (Exception e) {	return null;	}	ClassLoader loader = Utilities.getSessionSpecifiedClassLoader();	Class<?> udfClass = Class.forName(function.getClassName(), true, loader);	ret = SessionState.getRegistryForWrite().registerFunction( qualifiedName, FunctionType.PERSISTENT, udfClass, resources);	if (ret == null) {	
is not a valid udf class and was not registered 

}	ClassLoader loader = Utilities.getSessionSpecifiedClassLoader();	Class<?> udfClass = Class.forName(function.getClassName(), true, loader);	ret = SessionState.getRegistryForWrite().registerFunction( qualifiedName, FunctionType.PERSISTENT, udfClass, resources);	if (ret == null) {	}	if (SessionState.get().isHiveServerQuery()) {	SessionState.getRegistryForWrite().addToUDFLoaders(loader);	}	} catch (ClassNotFoundException e) {	
unable to load udf class 

public void closeCUDFLoaders() {	lock.lock();	try {	try {	for(ClassLoader loader: mSessionUDFLoaders) {	JavaUtils.closeClassLoader(loader);	}	} catch (IOException ie) {	
error in close loader 

private FunctionInfo getFunctionInfoFromMetastoreNoLock(String functionName, HiveConf conf) {	try {	String[] parts = FunctionUtils.getQualifiedFunctionNameParts(functionName);	Function func = Hive.get(conf).getFunction(parts[0].toLowerCase(), parts[1]);	if (func == null) {	return null;	}	FunctionInfo fi = registerPermanentFunction(functionName, func.getClassName(), true, FunctionTask.toFunctionResource(func.getResourceUris()));	if (fi == null) {	
is not a valid udf class and was not registered 

Function func = Hive.get(conf).getFunction(parts[0].toLowerCase(), parts[1]);	if (func == null) {	return null;	}	FunctionInfo fi = registerPermanentFunction(functionName, func.getClassName(), true, FunctionTask.toFunctionResource(func.getResourceUris()));	if (fi == null) {	return null;	}	return fi;	} catch (Throwable e) {	
unable to look up in metastore 

========================= hive sample_4108 =========================

public LoadTableDesc(final Path sourcePath, final TableDesc table, final DynamicPartitionCtx dpCtx, final AcidUtils.Operation writeType, boolean isReplace, Long txnId) {	super(sourcePath, writeType);	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
creating ltd from to 

========================= hive sample_3337 =========================

NumDistinctValueEstimator oldEst = aggregateData.getNdvEstimator();	NumDistinctValueEstimator newEst = newData.getNdvEstimator();	long ndv = -1;	if (oldEst.canMerge(newEst)) {	oldEst.mergeEstimators(newEst);	ndv = oldEst.estimateNumDistinctValues();	aggregateData.setNdvEstimator(oldEst);	} else {	ndv = Math.max(aggregateData.getNumDVs(), newData.getNumDVs());	}	
use bitvector to merge column s ndvs of and to be 

========================= hive sample_1920 =========================

public void testBadConnection() throws Exception {	Configuration conf = MetastoreConf.newMetastoreConf();	MetastoreConf.setVar(conf, MetastoreConf.ConfVars.CONNECTURLKEY, "blah");	RuntimeException e = null;	try {	TxnUtils.getTxnStore(conf);	}	catch(RuntimeException ex) {	
expected error 

========================= hive sample_1536 =========================

public void basicTest() throws IOException {	ByteArrayOutputStream out = new ByteArrayOutputStream();	PrintStream logFile = new PrintStream(out);	TestLogger logger = new TestLogger(logFile, TestLogger.LEVEL.INFO);	
trace 

public void basicTest() throws IOException {	ByteArrayOutputStream out = new ByteArrayOutputStream();	PrintStream logFile = new PrintStream(out);	TestLogger logger = new TestLogger(logFile, TestLogger.LEVEL.INFO);	
debug 

public void basicTest() throws IOException {	ByteArrayOutputStream out = new ByteArrayOutputStream();	PrintStream logFile = new PrintStream(out);	TestLogger logger = new TestLogger(logFile, TestLogger.LEVEL.INFO);	
info 

public void basicTest() throws IOException {	ByteArrayOutputStream out = new ByteArrayOutputStream();	PrintStream logFile = new PrintStream(out);	TestLogger logger = new TestLogger(logFile, TestLogger.LEVEL.INFO);	
warn 

public void basicTest() throws IOException {	ByteArrayOutputStream out = new ByteArrayOutputStream();	PrintStream logFile = new PrintStream(out);	TestLogger logger = new TestLogger(logFile, TestLogger.LEVEL.INFO);	
error 

========================= hive sample_5575 =========================

public static StructTreeReader createRootTreeReader(TypeDescription schema, List<OrcProto.ColumnEncoding> encodings, OrcEncodedColumnBatch batch, CompressionCodec codec, TreeReaderFactory.Context context, int[] columnMapping) throws IOException {	if (schema.getCategory() != Category.STRUCT) {	throw new AssertionError("Schema is not a struct: " + schema);	}	List<TypeDescription> children = schema.getChildren();	int childCount = children.size(), includedCount = 0;	for (int childIx = 0; childIx < childCount; ++childIx) {	int batchColIx = children.get(childIx).getId();	if (!batch.hasData(batchColIx) && !batch.hasVectors(batchColIx)) {	if (LOG.isDebugEnabled()) {	
column at has no data 

private static TreeReader createEncodedTreeReader(TypeDescription schema, List<OrcProto.ColumnEncoding> encodings, OrcEncodedColumnBatch batch, CompressionCodec codec, TreeReaderFactory.Context context) throws IOException {	int columnIndex = schema.getId();	ColumnStreamData[] streamBuffers = null;	List<ColumnVector> vectors = null;	if (batch.hasData(columnIndex)) {	streamBuffers = batch.getColumnData(columnIndex);	} else if (batch.hasVectors(columnIndex)) {	vectors = batch.getColumnVectors(columnIndex);	} else {	if (LOG.isDebugEnabled()) {	
batch has no data for 

========================= hive sample_3675 =========================

if ( reducer instanceof JoinOperator || reducer instanceof CommonMergeJoinOperator ) {	boolean noOuterJoin = ((JoinDesc)reducer.getConf()).isNoOuterJoin();	Map<Integer, ExtractReduceSinkInfo.Info> rsInfo = new TreeMap<Integer, ExtractReduceSinkInfo.Info>();	for(Map.Entry<Integer, String> e : rWork.getTagToInput().entrySet()) {	rsInfo.putAll(getReducerInfo(tezWork, rWork.getName(), e.getValue()));	}	if (checkForCrossProduct(rWork.getName(), reducer, rsInfo) && cartesianProductEdgeEnabled && noOuterJoin) {	List<BaseWork> parents = tezWork.getParents(null == origWrk ? wrk : origWrk);	for (BaseWork p: parents) {	TezEdgeProperty prop = tezWork.getEdgeProperty(p, null == origWrk ? wrk : origWrk);	
edge type 

========================= hive sample_2988 =========================

public static void main(String[] args) {	
version 

========================= hive sample_1424 =========================

} else if (key.equals("--secret")) {	conf.set(SparkClientFactory.CONF_KEY_SECRET, getArg(args, idx));	} else if (key.equals("--conf")) {	String[] val = getArg(args, idx).split("[=]", 2);	conf.set(val[0], val[1]);	} else {	throw new IllegalArgumentException("Invalid command line: " + Joiner.on(" ").join(args));	}	}	executor = Executors.newCachedThreadPool();	
connecting to 

Preconditions.checkArgument(clientId != null, "No client ID provided.");	String secret = mapConf.get(SparkClientFactory.CONF_KEY_SECRET);	Preconditions.checkArgument(secret != null, "No secret provided.");	int threadCount = new RpcConfiguration(mapConf).getRpcThreadCount();	this.egroup = new NioEventLoopGroup( threadCount, new ThreadFactoryBuilder() .setNameFormat("Driver-RPC-Handler-%d") .setDaemon(true) .build());	this.protocol = new DriverProtocol();	this.clientRpc = Rpc.createClient(mapConf, egroup, serverAddress, serverPort, clientId, secret, protocol).get();	this.running = true;	this.clientRpc.addListener(new Rpc.Listener() {	public void rpcClosed(Rpc rpc) {	
shutting down driver because rpc channel was closed 

}	});	try {	JavaSparkContext sc = new JavaSparkContext(conf);	sc.sc().addSparkListener(new ClientListener());	synchronized (jcLock) {	jc = new JobContextImpl(sc, localTmpDir);	jcLock.notifyAll();	}	} catch (Exception e) {	
failed to start sparkcontext 

private void run() throws InterruptedException {	synchronized (shutdownLock) {	while (running) {	shutdownLock.wait();	}	}	executor.shutdownNow();	try {	FileUtils.deleteDirectory(localTmpDir);	} catch (IOException e) {	
failed to delete local tmp dir 

private void submit(JobWrapper<?> job) {	synchronized (jcLock) {	if (jc != null) {	job.submit();	} else {	
sparkcontext not yet up queueing job request 

private synchronized void shutdown(Throwable error) {	if (running) {	if (error == null) {	
shutting down remote driver 

private synchronized void shutdown(Throwable error) {	if (running) {	if (error == null) {	} else {	
shutting down remote driver due to error 

private void handle(ChannelHandlerContext ctx, CancelJob msg) {	JobWrapper<?> job = activeJobs.get(msg.id);	if (job == null || !cancelJob(job)) {	
requested to cancel an already finished job 

private void handle(ChannelHandlerContext ctx, EndSession msg) {	
shutting down due to endsession request 

private void handle(ChannelHandlerContext ctx, JobRequest msg) {	
received job request 

try {	jc.setMonitorCb(new MonitorCallback() {	public void call(JavaFutureAction<?> future, SparkCounters sparkCounters, Set<Integer> cachedRDDIds) {	monitorJob(future, sparkCounters, cachedRDDIds);	}	});	T result = req.job.call(jc);	for (JavaFutureAction<?> future : jobs) {	future.get();	completed++;	
client job of spark jobs finished 

}	}	}	}	SparkCounters counters = null;	if (sparkCounters != null) {	counters = sparkCounters.snapshot();	}	protocol.jobFinished(req.id, result, null, counters);	} catch (Throwable t) {	
failed to run job 

========================= hive sample_543 =========================

public void testShowRoleGrant() throws SQLException {	Statement stmt = con.createStatement();	try {	stmt.execute("drop role role1");	} catch (Exception ex) {	
ignoring error during drop role 

public void run() {	while (statement.hasMoreLogs()) {	try {	incrementalLogs.addAll(statement.getQueryLog());	Thread.sleep(500);	} catch (SQLException e) {	
failed getquerylog error message 

public void run() {	while (statement.hasMoreLogs()) {	try {	incrementalLogs.addAll(statement.getQueryLog());	Thread.sleep(500);	} catch (SQLException e) {	fail("error in getting log thread");	} catch (InterruptedException e) {	
getting log thread is interrupted error message 

========================= hive sample_304 =========================

public String mapToClientType(String hiveTypeName) {	try {	TableType hiveType = TableType.valueOf(hiveTypeName.toUpperCase());	return hiveType.name();	} catch (IllegalArgumentException e) {	
invalid hive table type 

========================= hive sample_2336 =========================

public void configure(JobConf job) {	rowObjectInspector = new ObjectInspector[Byte.MAX_VALUE];	ObjectInspector[] valueObjectInspector = new ObjectInspector[Byte.MAX_VALUE];	ObjectInspector keyObjectInspector;	if (LOG.isInfoEnabled()) {	try {	LOG.info("conf classpath = " + Arrays.asList(((URLClassLoader) job.getClassLoader()).getURLs()));	LOG.info("thread classpath = " + Arrays.asList(((URLClassLoader) Thread.currentThread() .getContextClassLoader()).getURLs()));	} catch (Exception e) {	
cannot get classpath 

if (isTagged) {	int size = keyWritable.getSize() - 1;	tag = keyWritable.get()[size];	keyWritable.setSize(size);	}	if (!keyWritable.equals(groupKey)) {	if (groupKey == null) {	groupKey = new BytesWritable();	} else {	if (LOG.isTraceEnabled()) {	
end group 

}	reducer.endGroup();	}	try {	keyObject = inputKeyDeserializer.deserialize(keyWritable);	} catch (Exception e) {	throw new HiveException( "Hive Runtime Error: Unable to deserialize reduce input key from " + Utilities.formatBinaryString(keyWritable.get(), 0, keyWritable.getSize()) + " with properties " + keyTableDesc.getProperties(), e);	}	groupKey.set(keyWritable.get(), 0, keyWritable.getSize());	if (LOG.isTraceEnabled()) {	
start group 

public void close() {	if (oc == null && LOG.isTraceEnabled()) {	
close called without any rows processed 

public void close() {	if (oc == null && LOG.isTraceEnabled()) {	}	try {	if (groupKey != null) {	if (LOG.isTraceEnabled()) {	
end group 

if (groupKey != null) {	if (LOG.isTraceEnabled()) {	}	reducer.endGroup();	}	reducer.close(abort);	ReportStats rps = new ReportStats(rp, jc);	reducer.preorderMap(rps);	} catch (Exception e) {	if (!abort) {	
hit error while closing operators failing tree 

========================= hive sample_3874 =========================

public Set<CompactionInfo> findPotentialCompactions(int maxAborted) throws MetaException {	Connection dbConn = null;	Set<CompactionInfo> response = new HashSet<>();	Statement stmt = null;	ResultSet rs = null;	try {	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	String s = "select distinct ctc_database, ctc_table, " + "ctc_partition from COMPLETED_TXN_COMPONENTS";	
going to execute query 

rs = stmt.executeQuery(s);	while (rs.next()) {	CompactionInfo info = new CompactionInfo();	info.dbname = rs.getString(1);	info.tableName = rs.getString(2);	info.partName = rs.getString(3);	response.add(info);	}	rs.close();	s = "select tc_database, tc_table, tc_partition " + "from TXNS, TXN_COMPONENTS " + "where txn_id = tc_txnid and txn_state = '" + TXN_ABORTED + "' " + "group by tc_database, tc_table, tc_partition " + "having count(*) > " + maxAborted;	
going to execute query 

s = "select tc_database, tc_table, tc_partition " + "from TXNS, TXN_COMPONENTS " + "where txn_id = tc_txnid and txn_state = '" + TXN_ABORTED + "' " + "group by tc_database, tc_table, tc_partition " + "having count(*) > " + maxAborted;	rs = stmt.executeQuery(s);	while (rs.next()) {	CompactionInfo info = new CompactionInfo();	info.dbname = rs.getString(1);	info.tableName = rs.getString(2);	info.partName = rs.getString(3);	info.tooManyAborts = true;	response.add(info);	}	
going to rollback 

while (rs.next()) {	CompactionInfo info = new CompactionInfo();	info.dbname = rs.getString(1);	info.tableName = rs.getString(2);	info.partName = rs.getString(3);	info.tooManyAborts = true;	response.add(info);	}	dbConn.rollback();	} catch (SQLException e) {	
unable to connect to transaction database 

public void setRunAs(long cq_id, String user) throws MetaException {	try {	Connection dbConn = null;	Statement stmt = null;	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	String s = "update COMPACTION_QUEUE set cq_run_as = '" + user + "' where cq_id = " + cq_id;	
going to execute update 

try {	Connection dbConn = null;	Statement stmt = null;	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	String s = "update COMPACTION_QUEUE set cq_run_as = '" + user + "' where cq_id = " + cq_id;	int updCnt = stmt.executeUpdate(s);	if (updCnt != 1) {	LOG.error("Unable to set cq_run_as=" + user + " for compaction record with cq_id=" + cq_id + ".  updCnt=" + updCnt);	
going to rollback 

Statement stmt = null;	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	String s = "update COMPACTION_QUEUE set cq_run_as = '" + user + "' where cq_id = " + cq_id;	int updCnt = stmt.executeUpdate(s);	if (updCnt != 1) {	LOG.error("Unable to set cq_run_as=" + user + " for compaction record with cq_id=" + cq_id + ".  updCnt=" + updCnt);	dbConn.rollback();	}	
going to commit 

dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	String s = "update COMPACTION_QUEUE set cq_run_as = '" + user + "' where cq_id = " + cq_id;	int updCnt = stmt.executeUpdate(s);	if (updCnt != 1) {	LOG.error("Unable to set cq_run_as=" + user + " for compaction record with cq_id=" + cq_id + ".  updCnt=" + updCnt);	dbConn.rollback();	}	dbConn.commit();	} catch (SQLException e) {	
unable to update compaction queue 

dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	String s = "update COMPACTION_QUEUE set cq_run_as = '" + user + "' where cq_id = " + cq_id;	int updCnt = stmt.executeUpdate(s);	if (updCnt != 1) {	LOG.error("Unable to set cq_run_as=" + user + " for compaction record with cq_id=" + cq_id + ".  updCnt=" + updCnt);	dbConn.rollback();	}	dbConn.commit();	} catch (SQLException e) {	
going to rollback 

public CompactionInfo findNextToCompact(String workerId) throws MetaException {	try {	Connection dbConn = null;	Statement stmt = null;	Statement updStmt = null;	ResultSet rs = null;	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	String s = "select cq_id, cq_database, cq_table, cq_partition, " + "cq_type, cq_tblproperties from COMPACTION_QUEUE where cq_state = '" + INITIATED_STATE + "'";	
going to execute query 

Connection dbConn = null;	Statement stmt = null;	Statement updStmt = null;	ResultSet rs = null;	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	String s = "select cq_id, cq_database, cq_table, cq_partition, " + "cq_type, cq_tblproperties from COMPACTION_QUEUE where cq_state = '" + INITIATED_STATE + "'";	rs = stmt.executeQuery(s);	if (!rs.next()) {	
no compactions found ready to compact 

do {	CompactionInfo info = new CompactionInfo();	info.id = rs.getLong(1);	info.dbname = rs.getString(2);	info.tableName = rs.getString(3);	info.partName = rs.getString(4);	info.type = dbCompactionType2ThriftType(rs.getString(5).charAt(0));	info.properties = rs.getString(6);	long now = getDbTime(dbConn);	s = "update COMPACTION_QUEUE set cq_worker_id = '" + workerId + "', " + "cq_start = " + now + ", cq_state = '" + WORKING_STATE + "' where cq_id = " + info.id + " AND cq_state='" + INITIATED_STATE + "'";	
going to execute update 

info.type = dbCompactionType2ThriftType(rs.getString(5).charAt(0));	info.properties = rs.getString(6);	long now = getDbTime(dbConn);	s = "update COMPACTION_QUEUE set cq_worker_id = '" + workerId + "', " + "cq_start = " + now + ", cq_state = '" + WORKING_STATE + "' where cq_id = " + info.id + " AND cq_state='" + INITIATED_STATE + "'";	int updCount = updStmt.executeUpdate(s);	if(updCount == 1) {	dbConn.commit();	return info;	}	if(updCount == 0) {	
another worker picked up 

if(updCount == 0) {	continue;	}	LOG.error("Unable to set to cq_state=" + WORKING_STATE + " for compaction record: " + info + ". updCnt=" + updCount + ".");	dbConn.rollback();	return null;	} while( rs.next());	dbConn.rollback();	return null;	} catch (SQLException e) {	
unable to select next element for compaction 

if(updCount == 0) {	continue;	}	LOG.error("Unable to set to cq_state=" + WORKING_STATE + " for compaction record: " + info + ". updCnt=" + updCount + ".");	dbConn.rollback();	return null;	} while( rs.next());	dbConn.rollback();	return null;	} catch (SQLException e) {	
going to rollback 

public void markCompacted(CompactionInfo info) throws MetaException {	try {	Connection dbConn = null;	Statement stmt = null;	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	String s = "update COMPACTION_QUEUE set cq_state = '" + READY_FOR_CLEANING + "', " + "cq_worker_id = null where cq_id = " + info.id;	
going to execute update 

try {	Connection dbConn = null;	Statement stmt = null;	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	String s = "update COMPACTION_QUEUE set cq_state = '" + READY_FOR_CLEANING + "', " + "cq_worker_id = null where cq_id = " + info.id;	int updCnt = stmt.executeUpdate(s);	if (updCnt != 1) {	LOG.error("Unable to set cq_state=" + READY_FOR_CLEANING + " for compaction record: " + info + ". updCnt=" + updCnt);	
going to rollback 

Statement stmt = null;	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	String s = "update COMPACTION_QUEUE set cq_state = '" + READY_FOR_CLEANING + "', " + "cq_worker_id = null where cq_id = " + info.id;	int updCnt = stmt.executeUpdate(s);	if (updCnt != 1) {	LOG.error("Unable to set cq_state=" + READY_FOR_CLEANING + " for compaction record: " + info + ". updCnt=" + updCnt);	dbConn.rollback();	}	
going to commit 

dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	String s = "update COMPACTION_QUEUE set cq_state = '" + READY_FOR_CLEANING + "', " + "cq_worker_id = null where cq_id = " + info.id;	int updCnt = stmt.executeUpdate(s);	if (updCnt != 1) {	LOG.error("Unable to set cq_state=" + READY_FOR_CLEANING + " for compaction record: " + info + ". updCnt=" + updCnt);	dbConn.rollback();	}	dbConn.commit();	} catch (SQLException e) {	
unable to update compaction queue 

dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	String s = "update COMPACTION_QUEUE set cq_state = '" + READY_FOR_CLEANING + "', " + "cq_worker_id = null where cq_id = " + info.id;	int updCnt = stmt.executeUpdate(s);	if (updCnt != 1) {	LOG.error("Unable to set cq_state=" + READY_FOR_CLEANING + " for compaction record: " + info + ". updCnt=" + updCnt);	dbConn.rollback();	}	dbConn.commit();	} catch (SQLException e) {	
going to rollback 

public List<CompactionInfo> findReadyToClean() throws MetaException {	Connection dbConn = null;	List<CompactionInfo> rc = new ArrayList<>();	Statement stmt = null;	ResultSet rs = null;	try {	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	String s = "select cq_id, cq_database, cq_table, cq_partition, " + "cq_type, cq_run_as, cq_highest_txn_id from COMPACTION_QUEUE where cq_state = '" + READY_FOR_CLEANING + "'";	
going to execute query 

info.partName = rs.getString(4);	switch (rs.getString(5).charAt(0)) {	case MAJOR_TYPE: info.type = CompactionType.MAJOR; break;	case MINOR_TYPE: info.type = CompactionType.MINOR; break;	default: throw new MetaException("Unexpected compaction type " + rs.getString(5));	}	info.runAs = rs.getString(6);	info.highestTxnId = rs.getLong(7);	rc.add(info);	}	
going to rollback 

case MINOR_TYPE: info.type = CompactionType.MINOR; break;	default: throw new MetaException("Unexpected compaction type " + rs.getString(5));	}	info.runAs = rs.getString(6);	info.highestTxnId = rs.getLong(7);	rc.add(info);	}	dbConn.rollback();	return rc;	} catch (SQLException e) {	
unable to select next element for cleaning 

case MINOR_TYPE: info.type = CompactionType.MINOR; break;	default: throw new MetaException("Unexpected compaction type " + rs.getString(5));	}	info.runAs = rs.getString(6);	info.highestTxnId = rs.getLong(7);	rc.add(info);	}	dbConn.rollback();	return rc;	} catch (SQLException e) {	
going to rollback 

if(rs.next()) {	info = CompactionInfo.loadFullFromCompactionQueue(rs);	}	else {	throw new IllegalStateException("No record with CQ_ID=" + info.id + " found in COMPACTION_QUEUE");	}	close(rs);	String s = "delete from COMPACTION_QUEUE where cq_id = ?";	pStmt = dbConn.prepareStatement(s);	pStmt.setLong(1, info.id);	
going to execute update 

else {	throw new IllegalStateException("No record with CQ_ID=" + info.id + " found in COMPACTION_QUEUE");	}	close(rs);	String s = "delete from COMPACTION_QUEUE where cq_id = ?";	pStmt = dbConn.prepareStatement(s);	pStmt.setLong(1, info.id);	int updCount = pStmt.executeUpdate();	if (updCount != 1) {	LOG.error("Unable to delete compaction record: " + info +  ".  Update count=" + updCount);	
going to rollback 

pStmt = dbConn.prepareStatement(s);	int paramCount = 1;	pStmt.setString(paramCount++, info.dbname);	pStmt.setString(paramCount++, info.tableName);	if (info.partName != null) {	pStmt.setString(paramCount++, info.partName);	}	if(info.highestTxnId != 0) {	pStmt.setLong(paramCount++, info.highestTxnId);	}	
going to execute update 

int paramCount = 1;	pStmt.setString(paramCount++, info.dbname);	pStmt.setString(paramCount++, info.tableName);	if (info.partName != null) {	pStmt.setString(paramCount++, info.partName);	}	if(info.highestTxnId != 0) {	pStmt.setLong(paramCount++, info.highestTxnId);	}	if (pStmt.executeUpdate() < 1) {	
expected to remove at least one row from completed txn components when marking compaction entry as clean 

pStmt = dbConn.prepareStatement(s);	paramCount = 1;	pStmt.setString(paramCount++, info.dbname);	pStmt.setString(paramCount++, info.tableName);	if(info.highestTxnId != 0) {	pStmt.setLong(paramCount++, info.highestTxnId);	}	if (info.partName != null) {	pStmt.setString(paramCount++, info.partName);	}	
going to execute update 

suffix.append(" and tc_database = ?");	suffix.append(" and tc_table = ?");	if (info.partName != null) {	suffix.append(" and tc_partition = ?");	}	List<Integer> counts = TxnUtils .buildQueryWithINClauseStrings(conf, queries, prefix, suffix, questions, "tc_txnid", true, false);	int totalCount = 0;	for (int i = 0; i < queries.size(); i++) {	String query = queries.get(i);	int insertCount = counts.get(i);	
going to execute update 

pStmt.setLong(j + 1, txnids.get(totalCount + j));	}	totalCount += insertCount;	paramCount = insertCount + 1;	pStmt.setString(paramCount++, info.dbname);	pStmt.setString(paramCount++, info.tableName);	if (info.partName != null) {	pStmt.setString(paramCount++, info.partName);	}	int rc = pStmt.executeUpdate();	
removed records from txn components 

totalCount += insertCount;	paramCount = insertCount + 1;	pStmt.setString(paramCount++, info.dbname);	pStmt.setString(paramCount++, info.tableName);	if (info.partName != null) {	pStmt.setString(paramCount++, info.partName);	}	int rc = pStmt.executeUpdate();	}	}	
going to commit 

pStmt.setString(paramCount++, info.dbname);	pStmt.setString(paramCount++, info.tableName);	if (info.partName != null) {	pStmt.setString(paramCount++, info.partName);	}	int rc = pStmt.executeUpdate();	}	}	dbConn.commit();	} catch (SQLException e) {	
unable to delete from compaction queue 

pStmt.setString(paramCount++, info.dbname);	pStmt.setString(paramCount++, info.tableName);	if (info.partName != null) {	pStmt.setString(paramCount++, info.partName);	}	int rc = pStmt.executeUpdate();	}	}	dbConn.commit();	} catch (SQLException e) {	
going to rollback 

public void cleanEmptyAbortedTxns() throws MetaException {	try {	Connection dbConn = null;	Statement stmt = null;	ResultSet rs = null;	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	String s = "select txn_id from TXNS where " + "txn_id not in (select tc_txnid from TXN_COMPONENTS) and " + "txn_state = '" + TXN_ABORTED + "'";	
going to execute query 

return;	}	Collections.sort(txnids);	List<String> queries = new ArrayList<>();	StringBuilder prefix = new StringBuilder();	StringBuilder suffix = new StringBuilder();	prefix.append("delete from TXNS where ");	suffix.append("");	TxnUtils.buildQueryWithINClause(conf, queries, prefix, suffix, txnids, "txn_id", false, false);	for (String query : queries) {	
going to execute update 

}	Collections.sort(txnids);	List<String> queries = new ArrayList<>();	StringBuilder prefix = new StringBuilder();	StringBuilder suffix = new StringBuilder();	prefix.append("delete from TXNS where ");	suffix.append("");	TxnUtils.buildQueryWithINClause(conf, queries, prefix, suffix, txnids, "txn_id", false, false);	for (String query : queries) {	int rc = stmt.executeUpdate(query);	
removed empty aborted transactions from txns 

Collections.sort(txnids);	List<String> queries = new ArrayList<>();	StringBuilder prefix = new StringBuilder();	StringBuilder suffix = new StringBuilder();	prefix.append("delete from TXNS where ");	suffix.append("");	TxnUtils.buildQueryWithINClause(conf, queries, prefix, suffix, txnids, "txn_id", false, false);	for (String query : queries) {	int rc = stmt.executeUpdate(query);	}	
aborted transactions removed from txns 

Collections.sort(txnids);	List<String> queries = new ArrayList<>();	StringBuilder prefix = new StringBuilder();	StringBuilder suffix = new StringBuilder();	prefix.append("delete from TXNS where ");	suffix.append("");	TxnUtils.buildQueryWithINClause(conf, queries, prefix, suffix, txnids, "txn_id", false, false);	for (String query : queries) {	int rc = stmt.executeUpdate(query);	}	
going to commit 

StringBuilder prefix = new StringBuilder();	StringBuilder suffix = new StringBuilder();	prefix.append("delete from TXNS where ");	suffix.append("");	TxnUtils.buildQueryWithINClause(conf, queries, prefix, suffix, txnids, "txn_id", false, false);	for (String query : queries) {	int rc = stmt.executeUpdate(query);	}	dbConn.commit();	} catch (SQLException e) {	
unable to delete from txns table 

StringBuilder prefix = new StringBuilder();	StringBuilder suffix = new StringBuilder();	prefix.append("delete from TXNS where ");	suffix.append("");	TxnUtils.buildQueryWithINClause(conf, queries, prefix, suffix, txnids, "txn_id", false, false);	for (String query : queries) {	int rc = stmt.executeUpdate(query);	}	dbConn.commit();	} catch (SQLException e) {	
going to rollback 

public void revokeFromLocalWorkers(String hostname) throws MetaException {	try {	Connection dbConn = null;	Statement stmt = null;	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	String s = "update COMPACTION_QUEUE set cq_worker_id = null, cq_start = null, cq_state = '" + INITIATED_STATE+ "' where cq_state = '" + WORKING_STATE + "' and cq_worker_id like '" +  hostname + "%'";	
going to execute update 

public void revokeFromLocalWorkers(String hostname) throws MetaException {	try {	Connection dbConn = null;	Statement stmt = null;	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	String s = "update COMPACTION_QUEUE set cq_worker_id = null, cq_start = null, cq_state = '" + INITIATED_STATE+ "' where cq_state = '" + WORKING_STATE + "' and cq_worker_id like '" +  hostname + "%'";	stmt.executeUpdate(s);	
going to commit 

try {	Connection dbConn = null;	Statement stmt = null;	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	String s = "update COMPACTION_QUEUE set cq_worker_id = null, cq_start = null, cq_state = '" + INITIATED_STATE+ "' where cq_state = '" + WORKING_STATE + "' and cq_worker_id like '" +  hostname + "%'";	stmt.executeUpdate(s);	dbConn.commit();	} catch (SQLException e) {	
unable to change dead worker s records back to initiated state 

try {	Connection dbConn = null;	Statement stmt = null;	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	String s = "update COMPACTION_QUEUE set cq_worker_id = null, cq_start = null, cq_state = '" + INITIATED_STATE+ "' where cq_state = '" + WORKING_STATE + "' and cq_worker_id like '" +  hostname + "%'";	stmt.executeUpdate(s);	dbConn.commit();	} catch (SQLException e) {	
going to rollback 

public void revokeTimedoutWorkers(long timeout) throws MetaException {	try {	Connection dbConn = null;	Statement stmt = null;	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	long latestValidStart = getDbTime(dbConn) - timeout;	stmt = dbConn.createStatement();	String s = "update COMPACTION_QUEUE set cq_worker_id = null, cq_start = null, cq_state = '" + INITIATED_STATE+ "' where cq_state = '" + WORKING_STATE + "' and cq_start < " +  latestValidStart;	
going to execute update 

public void revokeTimedoutWorkers(long timeout) throws MetaException {	try {	Connection dbConn = null;	Statement stmt = null;	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	long latestValidStart = getDbTime(dbConn) - timeout;	stmt = dbConn.createStatement();	String s = "update COMPACTION_QUEUE set cq_worker_id = null, cq_start = null, cq_state = '" + INITIATED_STATE+ "' where cq_state = '" + WORKING_STATE + "' and cq_start < " +  latestValidStart;	stmt.executeUpdate(s);	
going to commit 

Connection dbConn = null;	Statement stmt = null;	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	long latestValidStart = getDbTime(dbConn) - timeout;	stmt = dbConn.createStatement();	String s = "update COMPACTION_QUEUE set cq_worker_id = null, cq_start = null, cq_state = '" + INITIATED_STATE+ "' where cq_state = '" + WORKING_STATE + "' and cq_start < " +  latestValidStart;	stmt.executeUpdate(s);	dbConn.commit();	} catch (SQLException e) {	
unable to change dead worker s records back to initiated state 

Connection dbConn = null;	Statement stmt = null;	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	long latestValidStart = getDbTime(dbConn) - timeout;	stmt = dbConn.createStatement();	String s = "update COMPACTION_QUEUE set cq_worker_id = null, cq_start = null, cq_state = '" + INITIATED_STATE+ "' where cq_state = '" + WORKING_STATE + "' and cq_start < " +  latestValidStart;	stmt.executeUpdate(s);	dbConn.commit();	} catch (SQLException e) {	
going to rollback 

suffix.append("");	List<String> questions = new ArrayList<>(deleteSet.size());	for (int  i = 0; i < deleteSet.size(); i++) {	questions.add("?");	}	List<Integer> counts = TxnUtils.buildQueryWithINClauseStrings(conf, queries, prefix, suffix, questions, "cc_id", false, false);	int totalCount = 0;	for (int i = 0; i < queries.size(); i++) {	String query = queries.get(i);	long insertCount = counts.get(i);	
going to execute update 

int totalCount = 0;	for (int i = 0; i < queries.size(); i++) {	String query = queries.get(i);	long insertCount = counts.get(i);	pStmt = dbConn.prepareStatement(query);	for (int j = 0; j < insertCount; j++) {	pStmt.setLong(j + 1, deleteSet.get(totalCount + j));	}	totalCount += insertCount;	int count = pStmt.executeUpdate();	
removed records from completed compactions 

if(rs.getString(1).charAt(0) == FAILED_STATE) {	numFailed++;	}	else {	numFailed--;	}	}	return numFailed == failedThreshold;	}	catch (SQLException e) {	
unable to check for failed compactions 

if(rs.getString(1).charAt(0) == FAILED_STATE) {	numFailed++;	}	else {	numFailed--;	}	}	return numFailed == failedThreshold;	}	catch (SQLException e) {	
going to rollback 

}	else {	numFailed--;	}	}	return numFailed == failedThreshold;	}	catch (SQLException e) {	rollbackDBConn(dbConn);	checkRetryable(dbConn, e, "checkFailedCompactions(" + ci + ")");	
unable to connect to transaction database 

dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	pStmt = dbConn.prepareStatement("select CQ_ID, CQ_DATABASE, CQ_TABLE, CQ_PARTITION, CQ_STATE, CQ_TYPE, CQ_TBLPROPERTIES, CQ_WORKER_ID, CQ_START, CQ_RUN_AS, CQ_HIGHEST_TXN_ID, CQ_META_INFO, CQ_HADOOP_JOB_ID from COMPACTION_QUEUE WHERE CQ_ID = ?");	pStmt.setLong(1, ci.id);	rs = pStmt.executeQuery();	if(rs.next()) {	ci = CompactionInfo.loadFullFromCompactionQueue(rs);	String s = "delete from COMPACTION_QUEUE where cq_id = ?";	pStmt = dbConn.prepareStatement(s);	pStmt.setLong(1, ci.id);	
going to execute update 

ci.start = getDbTime(dbConn);	}	else {	ci.state = FAILED_STATE;	}	close(rs, stmt, null);	closeStmt(pStmt);	pStmt = dbConn.prepareStatement("insert into COMPLETED_COMPACTIONS(CC_ID, CC_DATABASE, CC_TABLE, CC_PARTITION, CC_STATE, CC_TYPE, CC_TBLPROPERTIES, CC_WORKER_ID, CC_START, CC_END, CC_RUN_AS, CC_HIGHEST_TXN_ID, CC_META_INFO, CC_HADOOP_JOB_ID) VALUES(?,?,?,?,?, ?,?,?,?,?, ?,?,?,?)");	CompactionInfo.insertIntoCompletedCompactions(pStmt, ci, getDbTime(dbConn));	int updCount = pStmt.executeUpdate();	
going to commit 

ci.state = FAILED_STATE;	}	close(rs, stmt, null);	closeStmt(pStmt);	pStmt = dbConn.prepareStatement("insert into COMPLETED_COMPACTIONS(CC_ID, CC_DATABASE, CC_TABLE, CC_PARTITION, CC_STATE, CC_TYPE, CC_TBLPROPERTIES, CC_WORKER_ID, CC_START, CC_END, CC_RUN_AS, CC_HIGHEST_TXN_ID, CC_META_INFO, CC_HADOOP_JOB_ID) VALUES(?,?,?,?,?, ?,?,?,?,?, ?,?,?,?)");	CompactionInfo.insertIntoCompletedCompactions(pStmt, ci, getDbTime(dbConn));	int updCount = pStmt.executeUpdate();	closeStmt(pStmt);	dbConn.commit();	} catch (SQLException e) {	
markfailed 

ci.state = FAILED_STATE;	}	close(rs, stmt, null);	closeStmt(pStmt);	pStmt = dbConn.prepareStatement("insert into COMPLETED_COMPACTIONS(CC_ID, CC_DATABASE, CC_TABLE, CC_PARTITION, CC_STATE, CC_TYPE, CC_TBLPROPERTIES, CC_WORKER_ID, CC_START, CC_END, CC_RUN_AS, CC_HIGHEST_TXN_ID, CC_META_INFO, CC_HADOOP_JOB_ID) VALUES(?,?,?,?,?, ?,?,?,?,?, ?,?,?,?)");	CompactionInfo.insertIntoCompletedCompactions(pStmt, ci, getDbTime(dbConn));	int updCount = pStmt.executeUpdate();	closeStmt(pStmt);	dbConn.commit();	} catch (SQLException e) {	
going to rollback 

CompactionInfo.insertIntoCompletedCompactions(pStmt, ci, getDbTime(dbConn));	int updCount = pStmt.executeUpdate();	closeStmt(pStmt);	dbConn.commit();	} catch (SQLException e) {	rollbackDBConn(dbConn);	try {	checkRetryable(dbConn, e, "markFailed(" + ci + ")");	}	catch(MetaException ex) {	
unable to connect to transaction database 

int updCount = pStmt.executeUpdate();	closeStmt(pStmt);	dbConn.commit();	} catch (SQLException e) {	rollbackDBConn(dbConn);	try {	checkRetryable(dbConn, e, "markFailed(" + ci + ")");	}	catch(MetaException ex) {	}	
markfailed failed 

public void setHadoopJobId(String hadoopJobId, long id) {	try {	Connection dbConn = null;	Statement stmt = null;	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	String s = "update COMPACTION_QUEUE set CQ_HADOOP_JOB_ID = " + quoteString(hadoopJobId) + " WHERE CQ_ID = " + id;	
going to execute 

public void setHadoopJobId(String hadoopJobId, long id) {	try {	Connection dbConn = null;	Statement stmt = null;	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	String s = "update COMPACTION_QUEUE set CQ_HADOOP_JOB_ID = " + quoteString(hadoopJobId) + " WHERE CQ_ID = " + id;	int updateCount = stmt.executeUpdate(s);	
going to commit 

Connection dbConn = null;	Statement stmt = null;	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	String s = "update COMPACTION_QUEUE set CQ_HADOOP_JOB_ID = " + quoteString(hadoopJobId) + " WHERE CQ_ID = " + id;	int updateCount = stmt.executeUpdate(s);	closeStmt(stmt);	dbConn.commit();	} catch (SQLException e) {	
sethadoopjobid 

Connection dbConn = null;	Statement stmt = null;	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	String s = "update COMPACTION_QUEUE set CQ_HADOOP_JOB_ID = " + quoteString(hadoopJobId) + " WHERE CQ_ID = " + id;	int updateCount = stmt.executeUpdate(s);	closeStmt(stmt);	dbConn.commit();	} catch (SQLException e) {	
going to rollback 

String s = "update COMPACTION_QUEUE set CQ_HADOOP_JOB_ID = " + quoteString(hadoopJobId) + " WHERE CQ_ID = " + id;	int updateCount = stmt.executeUpdate(s);	closeStmt(stmt);	dbConn.commit();	} catch (SQLException e) {	rollbackDBConn(dbConn);	try {	checkRetryable(dbConn, e, "setHadoopJobId(" + hadoopJobId + "," + id + ")");	}	catch(MetaException ex) {	
unable to connect to transaction database 

int updateCount = stmt.executeUpdate(s);	closeStmt(stmt);	dbConn.commit();	} catch (SQLException e) {	rollbackDBConn(dbConn);	try {	checkRetryable(dbConn, e, "setHadoopJobId(" + hadoopJobId + "," + id + ")");	}	catch(MetaException ex) {	}	
sethadoopjobid failed 

========================= hive sample_1849 =========================

public static String getDaemonLocalDirString(Configuration conf, String workDirsEnvString) {	String localDirList = HiveConf.getVar(conf, ConfVars.LLAP_DAEMON_WORK_DIRS);	if (localDirList != null && !localDirList.isEmpty()) {	
local dirs from configuration 

public static String getDaemonLocalDirString(Configuration conf, String workDirsEnvString) {	String localDirList = HiveConf.getVar(conf, ConfVars.LLAP_DAEMON_WORK_DIRS);	if (localDirList != null && !localDirList.isEmpty()) {	if (!localDirList.equalsIgnoreCase("useYarnEnvDirs") && !StringUtils.isBlank(localDirList)) {	
using local dirs from configuration 

public static String getDaemonLocalDirString(Configuration conf, String workDirsEnvString) {	String localDirList = HiveConf.getVar(conf, ConfVars.LLAP_DAEMON_WORK_DIRS);	if (localDirList != null && !localDirList.isEmpty()) {	if (!localDirList.equalsIgnoreCase("useYarnEnvDirs") && !StringUtils.isBlank(localDirList)) {	return localDirList;	}	}	if (StringUtils.isNotBlank(workDirsEnvString)) {	
using local dirs from environment 

public static RPC.Server startProtocolServer(int srvPort, int numHandlers, AtomicReference<InetSocketAddress> bindAddress, Configuration conf, BlockingService impl, Class<?> protocolClass, SecretManager<?> secretManager, PolicyProvider provider, ConfVars... aclVars) {	InetSocketAddress addr = new InetSocketAddress(srvPort);	RPC.Server server;	try {	server = createRpcServer(protocolClass, addr, conf, numHandlers, impl, secretManager, provider, aclVars);	server.start();	} catch (IOException e) {	
failed to run rpc server on port 

server = createRpcServer(protocolClass, addr, conf, numHandlers, impl, secretManager, provider, aclVars);	server.start();	} catch (IOException e) {	throw new RuntimeException(e);	}	InetSocketAddress serverBindAddress = NetUtils.getConnectAddress(server);	InetSocketAddress bindAddressVal = NetUtils.createSocketAddrForHost( serverBindAddress.getAddress().getCanonicalHostName(), serverBindAddress.getPort());	if (bindAddress != null) {	bindAddress.set(bindAddressVal);	}	
instantiated at 

========================= hive sample_492 =========================

isTez = HiveConf.getVar(hconf, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals("tez");	isLlap = LlapDaemonInfo.INSTANCE.isLlap();	numExecutors = isLlap ? LlapDaemonInfo.INSTANCE.getNumExecutors() : 1;	firstRow = true;	if (hashAggr) {	computeMaxEntriesHashAggr();	}	memoryMXBean = ManagementFactory.getMemoryMXBean();	maxMemory = isTez ? getConf().getMaxMemoryAvailable() : memoryMXBean.getHeapMemoryUsage().getMax();	memoryThreshold = this.getConf().getMemoryThreshold();	
istez isllap numexecutors maxmemory 

private void computeMaxEntriesHashAggr() throws HiveException {	float memoryPercentage = this.getConf().getGroupByMemoryUsage();	if (isTez) {	maxHashTblMemory = (long) (memoryPercentage * getConf().getMaxMemoryAvailable());	} else {	maxHashTblMemory = (long) (memoryPercentage * Runtime.getRuntime().maxMemory());	}	
max hash table memory bytes 

countAfterReport = 0;	if (complete) {	Iterator<Map.Entry<KeyWrapper, AggregationBuffer[]>> iter = hashAggregations .entrySet().iterator();	while (iter.hasNext()) {	Map.Entry<KeyWrapper, AggregationBuffer[]> m = iter.next();	forward(m.getKey().getKeyArray(), m.getValue());	}	hashAggregations.clear();	hashAggregations = null;	if (LOG.isInfoEnabled()) {	
hash table completed flushed 

========================= hive sample_4036 =========================

private void initialize() {	try {	fillTokenizer();	} catch (Exception e) {	
unable to initialize tokenizer 

========================= hive sample_5277 =========================

public static boolean isAuthorizationEnabled(Configuration conf) {	if (!HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_AUTHORIZATION_ENABLED)) {	return false;	}	if (SessionState.get().getAuthorizer() == null || DefaultHiveAuthorizationProvider.class.getName().equals(HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER))) {	
metastore authorizer is skipped for authorizer or defaulthiveauthorizationprovider 

========================= hive sample_703 =========================

TThreadPoolServer.Args sargs = new TThreadPoolServer.Args(serverSocket) .processorFactory(processorFactory).transportFactory(transportFactory) .protocolFactory(new TBinaryProtocol.Factory()) .inputProtocolFactory(new TBinaryProtocol.Factory(true, true, maxMessageSize, maxMessageSize)) .requestTimeout(requestTimeout).requestTimeoutUnit(TimeUnit.SECONDS) .beBackoffSlotLength(beBackoffSlotLength).beBackoffSlotLengthUnit(TimeUnit.MILLISECONDS) .executorService(executorService);	server = new TThreadPoolServer(sargs);	server.setServerEventHandler(new TServerEventHandler() {	public ServerContext createContext( TProtocol input, TProtocol output) {	Metrics metrics = MetricsFactory.getInstance();	if (metrics != null) {	try {	metrics.incrementCounter(MetricsConstant.OPEN_CONNECTIONS);	metrics.incrementCounter(MetricsConstant.CUMULATIVE_CONNECTION_COUNT);	} catch (Exception e) {	
error reporting jdo operation to metrics system 

}	}	return new ThriftCLIServerContext();	}	public void deleteContext(ServerContext serverContext, TProtocol input, TProtocol output) {	Metrics metrics = MetricsFactory.getInstance();	if (metrics != null) {	try {	metrics.decrementCounter(MetricsConstant.OPEN_CONNECTIONS);	} catch (Exception e) {	
error reporting jdo operation to metrics system 

Metrics metrics = MetricsFactory.getInstance();	if (metrics != null) {	try {	metrics.decrementCounter(MetricsConstant.OPEN_CONNECTIONS);	} catch (Exception e) {	}	}	ThriftCLIServerContext context = (ThriftCLIServerContext) serverContext;	SessionHandle sessionHandle = context.getSessionHandle();	if (sessionHandle != null) {	
session disconnected without closing properly 

try {	metrics.decrementCounter(MetricsConstant.OPEN_CONNECTIONS);	} catch (Exception e) {	}	}	ThriftCLIServerContext context = (ThriftCLIServerContext) serverContext;	SessionHandle sessionHandle = context.getSessionHandle();	if (sessionHandle != null) {	try {	boolean close = cliService.getSessionManager().getSession(sessionHandle).getHiveConf() .getBoolVar(ConfVars.HIVE_SERVER2_CLOSE_SESSION_ON_DISCONNECT);	
not closing the session 

}	ThriftCLIServerContext context = (ThriftCLIServerContext) serverContext;	SessionHandle sessionHandle = context.getSessionHandle();	if (sessionHandle != null) {	try {	boolean close = cliService.getSessionManager().getSession(sessionHandle).getHiveConf() .getBoolVar(ConfVars.HIVE_SERVER2_CLOSE_SESSION_ON_DISCONNECT);	if (close) {	cliService.closeSession(sessionHandle);	}	} catch (HiveSQLException e) {	
failed to close session 

public void preServe() {	}	public void processContext(ServerContext serverContext, TTransport input, TTransport output) {	currentServerContext.set(serverContext);	}	});	String msg = "Starting " + ThriftBinaryCLIService.class.getSimpleName() + " on port " + portNum + " with " + minWorkerThreads + "..." + maxWorkerThreads + " worker threads";	LOG.info(msg);	server.serve();	} catch (Throwable t) {	
error starting could not start 

========================= hive sample_2361 =========================

public LlapProtocolServerImpl(SecretManager secretManager, int numHandlers, ContainerRunner containerRunner, AtomicReference<InetSocketAddress> srvAddress, AtomicReference<InetSocketAddress> mngAddress, int srvPort, int mngPort, DaemonId daemonId) {	super("LlapDaemonProtocolServerImpl");	this.numHandlers = numHandlers;	this.containerRunner = containerRunner;	this.secretManager = secretManager;	this.srvAddress = srvAddress;	this.srvPort = srvPort;	this.mngAddress = mngAddress;	this.mngPort = mngPort;	this.daemonId = daemonId;	
creating with port configured to 

if (!UserGroupInformation.isSecurityEnabled()) {	startProtocolServers(conf, daemonImpl, managementImpl);	return;	}	try {	this.clusterUser = UserGroupInformation.getCurrentUser().getShortUserName();	} catch (IOException e) {	throw new RuntimeException(e);	}	if (isPermissiveManagementAcl(conf)) {	
management protocol has a acl 

========================= hive sample_2224 =========================

FilterOperator filter = (FilterOperator) nd;	FilterDesc desc = filter.getConf();	if (!parseContext.getConf().getBoolVar(ConfVars.TEZ_DYNAMIC_PARTITION_PRUNING) && !parseContext.getConf().isSparkDPPAny()) {	return null;	}	TableScanOperator ts = null;	if (filter.getParentOperators().size() == 1 && filter.getParentOperators().get(0) instanceof TableScanOperator) {	ts = (TableScanOperator) filter.getParentOperators().get(0);	}	if (LOG.isDebugEnabled()) {	
parent 

FilterOperator filter = (FilterOperator) nd;	FilterDesc desc = filter.getConf();	if (!parseContext.getConf().getBoolVar(ConfVars.TEZ_DYNAMIC_PARTITION_PRUNING) && !parseContext.getConf().isSparkDPPAny()) {	return null;	}	TableScanOperator ts = null;	if (filter.getParentOperators().size() == 1 && filter.getParentOperators().get(0) instanceof TableScanOperator) {	ts = (TableScanOperator) filter.getParentOperators().get(0);	}	if (LOG.isDebugEnabled()) {	
filter 

FilterOperator filter = (FilterOperator) nd;	FilterDesc desc = filter.getConf();	if (!parseContext.getConf().getBoolVar(ConfVars.TEZ_DYNAMIC_PARTITION_PRUNING) && !parseContext.getConf().isSparkDPPAny()) {	return null;	}	TableScanOperator ts = null;	if (filter.getParentOperators().size() == 1 && filter.getParentOperators().get(0) instanceof TableScanOperator) {	ts = (TableScanOperator) filter.getParentOperators().get(0);	}	if (LOG.isDebugEnabled()) {	
tablescan 

String column = ExprNodeDescUtils.extractColName(ctx.parent);	boolean semiJoinAttempted = false;	if (column != null) {	String keyBaseAlias = "";	Table table = ts.getConf().getTableMetadata();	if (table != null && table.isPartitionKey(column)) {	String columnType = table.getPartColByName(column).getType();	String alias = ts.getConf().getAlias();	PrunedPartitionList plist = parseContext.getPrunedPartitions(alias, ts);	if (LOG.isDebugEnabled()) {	
alias 

String column = ExprNodeDescUtils.extractColName(ctx.parent);	boolean semiJoinAttempted = false;	if (column != null) {	String keyBaseAlias = "";	Table table = ts.getConf().getTableMetadata();	if (table != null && table.isPartitionKey(column)) {	String columnType = table.getPartColByName(column).getType();	String alias = ts.getConf().getAlias();	PrunedPartitionList plist = parseContext.getPrunedPartitions(alias, ts);	if (LOG.isDebugEnabled()) {	
pruned partition list 

String alias = ts.getConf().getAlias();	PrunedPartitionList plist = parseContext.getPrunedPartitions(alias, ts);	if (LOG.isDebugEnabled()) {	if (plist != null) {	for (Partition p : plist.getPartitions()) {	LOG.debug(p.getCompleteName());	}	}	}	if (plist == null || plist.getPartitions().size() != 0) {	
dynamic partitioning 

if (LOG.isDebugEnabled()) {	if (plist != null) {	for (Partition p : plist.getPartitions()) {	LOG.debug(p.getCompleteName());	}	}	}	if (plist == null || plist.getPartitions().size() != 0) {	generateEventOperatorPlan(ctx, parseContext, ts, column, columnType);	} else {	
no partition pruning necessary 

for (Partition p : plist.getPartitions()) {	LOG.debug(p.getCompleteName());	}	}	}	if (plist == null || plist.getPartitions().size() != 0) {	generateEventOperatorPlan(ctx, parseContext, ts, column, columnType);	} else {	}	} else {	
column is not a partition column 

LOG.debug(p.getCompleteName());	}	}	}	if (plist == null || plist.getPartitions().size() != 0) {	generateEventOperatorPlan(ctx, parseContext, ts, column, columnType);	} else {	}	} else {	if (semiJoin && ts.getConf().getFilterExpr() != null) {	
initiate semijoin reduction for 

private void generateEventOperatorPlan(DynamicListContext ctx, ParseContext parseContext, TableScanOperator ts, String column, String columnType) {	Operator<? extends OperatorDesc> parentOfRS = ctx.generator.getParentOperators().get(0);	ExprNodeDesc key = ctx.generator.getConf().getKeyCols().get(ctx.desc.getKeyIndex());	ExprNodeDesc partKey = ctx.parent.getChildren().get(0);	if (LOG.isDebugEnabled()) {	
key expr 

private void generateEventOperatorPlan(DynamicListContext ctx, ParseContext parseContext, TableScanOperator ts, String column, String columnType) {	Operator<? extends OperatorDesc> parentOfRS = ctx.generator.getParentOperators().get(0);	ExprNodeDesc key = ctx.generator.getConf().getKeyCols().get(ctx.desc.getKeyIndex());	ExprNodeDesc partKey = ctx.parent.getChildren().get(0);	if (LOG.isDebugEnabled()) {	
partition key expr 

ArrayList<ExprNodeDesc> params = new ArrayList<ExprNodeDesc>();	params.add( new ExprNodeColumnDesc(key.getTypeInfo(), outputNames.get(0), "", false));	ArrayList<AggregationDesc> aggs = new ArrayList<AggregationDesc>();	try {	AggregationDesc min = new AggregationDesc("min", FunctionRegistry.getGenericUDAFEvaluator("min", aggFnOIs, false, false), params, false, Mode.PARTIAL1);	AggregationDesc max = new AggregationDesc("max", FunctionRegistry.getGenericUDAFEvaluator("max", aggFnOIs, false, false), params, false, Mode.PARTIAL1);	AggregationDesc bloomFilter = new AggregationDesc("bloom_filter", FunctionRegistry.getGenericUDAFEvaluator("bloom_filter", aggFnOIs, false, false), params, false, Mode.PARTIAL1);	GenericUDAFBloomFilterEvaluator bloomFilterEval = (GenericUDAFBloomFilterEvaluator) bloomFilter.getGenericUDAFEvaluator();	bloomFilterEval.setSourceOperator(selectOp);	if (sjHint != null && sjHint.getNumEntries() > 0) {	
setting size for to based on the hint 

bloomFilterEval.setHintEntries(sjHint.getNumEntries());	}	bloomFilterEval.setMaxEntries(parseContext.getConf().getLongVar(ConfVars.TEZ_MAX_BLOOM_FILTER_ENTRIES));	bloomFilterEval.setMinEntries(parseContext.getConf().getLongVar(ConfVars.TEZ_MIN_BLOOM_FILTER_ENTRIES));	bloomFilterEval.setFactor(parseContext.getConf().getFloatVar(ConfVars.TEZ_BLOOM_FILTER_FACTOR));	bloomFilter.setGenericUDAFWritableEvaluator(bloomFilterEval);	aggs.add(min);	aggs.add(max);	aggs.add(bloomFilter);	} catch (SemanticException e) {	
error creating min max aggregations on key 

bloomFilterEval.setHintEntries(sjHint.getNumEntries());	}	bloomFilterEval.setMaxEntries(parseContext.getConf().getLongVar(ConfVars.TEZ_MAX_BLOOM_FILTER_ENTRIES));	bloomFilterEval.setMinEntries(parseContext.getConf().getLongVar(ConfVars.TEZ_MIN_BLOOM_FILTER_ENTRIES));	bloomFilterEval.setFactor(parseContext.getConf().getFloatVar(ConfVars.TEZ_BLOOM_FILTER_FACTOR));	bloomFilter.setGenericUDAFWritableEvaluator(bloomFilterEval);	aggsFinal.add(min);	aggsFinal.add(max);	aggsFinal.add(bloomFilter);	} catch (SemanticException e) {	
error creating min max aggregations on key 

for (int i = 0; i < gbOutputNames.size() - 1; i++) {	ExprNodeColumnDesc expr = new ExprNodeColumnDesc(key.getTypeInfo(), gbOutputNames.get(colPos++), "", false);	rsValueCols.add(expr);	}	ExprNodeColumnDesc colBFExpr = new ExprNodeColumnDesc(TypeInfoFactory.binaryTypeInfo, gbOutputNames.get(colPos++), "", false);	rsValueCols.add(colBFExpr);	ReduceSinkDesc rsDescFinal = PlanUtils.getReduceSinkDesc( new ArrayList<ExprNodeDesc>(), rsValueCols, gbOutputNames, false, -1, 0, 1, Operation.NOT_ACID);	ReduceSinkOperator rsOpFinal = (ReduceSinkOperator)OperatorFactory.getAndMakeChild( rsDescFinal, new RowSchema(gb.getSchema()), gb);	Map<String, ExprNodeDesc> columnExprMap = new HashMap<>();	rsOpFinal.setColumnExprMap(columnExprMap);	
dynamicsemijoinpushdown saving rs to ts mapping 

========================= hive sample_3065 =========================

private String coerce(String originalValue) {	if (originalValue == null || originalValue.isEmpty()) return originalValue;	try {	Path originalPath = new Path(originalValue);	Path absolutePath = FileUtils.makeAbsolute(LOCAL_FILE_SYSTEM, originalPath);	return absolutePath.toString();	} catch (IOException exception) {	
unable to resolve java io tmpdir for absolute path s 

========================= hive sample_1408 =========================

static void executeStatementOnDriver(String cmd, IDriver driver) throws IOException, CommandNeedRetryException {	
executing 

private void associateEncryptionZoneWithPath(String path) throws SQLException, CommandNeedRetryException {	
associateencryptionzonewithpath 

private void removeEncryptionZone() throws SQLException, CommandNeedRetryException {	
removeencryptionzone 

public void map(WritableComparable key, HCatRecord value, Context context) throws IOException, InterruptedException {	try {	readRecords.add(value);	} catch (Exception e) {	
error when read record 

========================= hive sample_996 =========================

}	return;	}	if (dataSourceName != null) {	throw new MetaException( String.format("Datasource name cannot be specified using [%s] for managed tables " + "using Druid", Constants.DRUID_DATA_SOURCE));	}	dataSourceName = Warehouse.getQualifiedName(table);	try {	getConnector().createSegmentTable();	} catch (Exception e) {	
exception while trying to create druid segments table 

if (dataSourceName != null) {	throw new MetaException( String.format("Datasource name cannot be specified using [%s] for managed tables " + "using Druid", Constants.DRUID_DATA_SOURCE));	}	dataSourceName = Warehouse.getQualifiedName(table);	try {	getConnector().createSegmentTable();	} catch (Exception e) {	throw new MetaException(e.getMessage());	}	Collection<String> existingDataSources = DruidStorageHandlerUtils .getAllDataSourceNames(getConnector(), getDruidMetadataStorageTablesConfig());	
pre create data source with name 

if (MetaStoreUtils.isExternalTable(table)) {	return;	}	final Path segmentDescriptorDir = getSegmentDescriptorDir();	try {	List<DataSegment> dataSegmentList = DruidStorageHandlerUtils .getCreatedSegments(segmentDescriptorDir, getConf());	for (DataSegment dataSegment : dataSegmentList) {	try {	deleteSegment(dataSegment);	} catch (SegmentLoadingException e) {	
error while trying to clean the segment s 

final Path segmentDescriptorDir = getSegmentDescriptorDir();	try {	List<DataSegment> dataSegmentList = DruidStorageHandlerUtils .getCreatedSegments(segmentDescriptorDir, getConf());	for (DataSegment dataSegment : dataSegmentList) {	try {	deleteSegment(dataSegment);	} catch (SegmentLoadingException e) {	}	}	} catch (IOException e) {	
exception while rollback 

protected void loadDruidSegments(Table table, boolean overwrite) throws MetaException {	final String dataSourceName = table.getParameters().get(Constants.DRUID_DATA_SOURCE);	final List<DataSegment> segmentList = Lists.newArrayList();	final Path tableDir = getSegmentDescriptorDir();	try {	segmentList.addAll(DruidStorageHandlerUtils.getCreatedSegments(tableDir, getConf()));	} catch (IOException e) {	
failed to load segments descriptor from directory 

final Path tableDir = getSegmentDescriptorDir();	try {	segmentList.addAll(DruidStorageHandlerUtils.getCreatedSegments(tableDir, getConf()));	} catch (IOException e) {	Throwables.propagate(e);	cleanWorkingDir();	}	final HdfsDataSegmentPusherConfig hdfsSegmentPusherConfig = new HdfsDataSegmentPusherConfig();	List<DataSegment> publishedDataSegmentList = Lists.newArrayList();	final String segmentDirectory = table.getParameters().get(Constants.DRUID_SEGMENT_DIRECTORY) != null ? table.getParameters().get(Constants.DRUID_SEGMENT_DIRECTORY) : HiveConf.getVar(getConf(), HiveConf.ConfVars.DRUID_SEGMENT_DIRECTORY);	
moving s druid segments from staging directory s to deep storage s 

cleanWorkingDir();	}	final HdfsDataSegmentPusherConfig hdfsSegmentPusherConfig = new HdfsDataSegmentPusherConfig();	List<DataSegment> publishedDataSegmentList = Lists.newArrayList();	final String segmentDirectory = table.getParameters().get(Constants.DRUID_SEGMENT_DIRECTORY) != null ? table.getParameters().get(Constants.DRUID_SEGMENT_DIRECTORY) : HiveConf.getVar(getConf(), HiveConf.ConfVars.DRUID_SEGMENT_DIRECTORY);	hdfsSegmentPusherConfig.setStorageDirectory(segmentDirectory);	try {	DataSegmentPusher dataSegmentPusher = new HdfsDataSegmentPusher(hdfsSegmentPusherConfig, getConf(), DruidStorageHandlerUtils.JSON_MAPPER );	publishedDataSegmentList = DruidStorageHandlerUtils.publishSegmentsAndCommit( getConnector(), getDruidMetadataStorageTablesConfig(), dataSourceName, segmentList, overwrite, getConf(), dataSegmentPusher );	} catch (CallbackFailedException | IOException e) {	
failed to move segments from staging directory 

private int checkLoadStatus(List<DataSegment> segments){	final String coordinatorAddress = HiveConf .getVar(getConf(), HiveConf.ConfVars.HIVE_DRUID_COORDINATOR_DEFAULT_ADDRESS);	int maxTries = HiveConf.getIntVar(getConf(), HiveConf.ConfVars.HIVE_DRUID_MAX_TRIES);	if (maxTries == 0) {	return segments.size();	}	
checking load status from coordinator 

} catch (MalformedURLException e) {	Throwables.propagate(e);	}	return null;	}).collect(Collectors.toSet());	int numRetries = 0;	while (numRetries++ < maxTries && !UrlsOfUnloadedSegments.isEmpty()) {	UrlsOfUnloadedSegments = ImmutableSet.copyOf(Sets.filter(UrlsOfUnloadedSegments, input -> {	try {	String result = DruidStorageHandlerUtils.getURL(getHttpClient(), input);	
checking segment response is 

}	return null;	}).collect(Collectors.toSet());	int numRetries = 0;	while (numRetries++ < maxTries && !UrlsOfUnloadedSegments.isEmpty()) {	UrlsOfUnloadedSegments = ImmutableSet.copyOf(Sets.filter(UrlsOfUnloadedSegments, input -> {	try {	String result = DruidStorageHandlerUtils.getURL(getHttpClient(), input);	return Strings.isNullOrEmpty(result);	} catch (IOException e) {	
error while checking url s 

protected void deleteSegment(DataSegment segment) throws SegmentLoadingException {	final Path path = DruidStorageHandlerUtils.getPath(segment);	
removing segment located at path 

protected void deleteSegment(DataSegment segment) throws SegmentLoadingException {	final Path path = DruidStorageHandlerUtils.getPath(segment);	try {	if (path.getName().endsWith(".zip")) {	final FileSystem fs = path.getFileSystem(getConf());	if (!fs.exists(path)) {	
segment path does not exist it appears to have been deleted already 

public void commitDropTable(Table table, boolean deleteData) throws MetaException {	if (MetaStoreUtils.isExternalTable(table)) {	return;	}	String dataSourceName = Preconditions .checkNotNull(table.getParameters().get(Constants.DRUID_DATA_SOURCE), "DataSource name is null !" );	if (deleteData == true) {	
dropping with purge all the data for data source 

public void commitDropTable(Table table, boolean deleteData) throws MetaException {	if (MetaStoreUtils.isExternalTable(table)) {	return;	}	String dataSourceName = Preconditions .checkNotNull(table.getParameters().get(Constants.DRUID_DATA_SOURCE), "DataSource name is null !" );	if (deleteData == true) {	List<DataSegment> dataSegmentList = DruidStorageHandlerUtils .getDataSegmentList(getConnector(), getDruidMetadataStorageTablesConfig(), dataSourceName);	if (dataSegmentList.isEmpty()) {	
nothing to delete for data source 

String dataSourceName = Preconditions .checkNotNull(table.getParameters().get(Constants.DRUID_DATA_SOURCE), "DataSource name is null !" );	if (deleteData == true) {	List<DataSegment> dataSegmentList = DruidStorageHandlerUtils .getDataSegmentList(getConnector(), getDruidMetadataStorageTablesConfig(), dataSourceName);	if (dataSegmentList.isEmpty()) {	return;	}	for (DataSegment dataSegment : dataSegmentList) {	try {	deleteSegment(dataSegment);	} catch (SegmentLoadingException e) {	
error while deleting segment s 

return;	}	for (DataSegment dataSegment : dataSegmentList) {	try {	deleteSegment(dataSegment);	} catch (SegmentLoadingException e) {	}	}	}	if (DruidStorageHandlerUtils .disableDataSource(getConnector(), getDruidMetadataStorageTablesConfig(), dataSourceName)) {	
successfully dropped druid data source 

public void commitInsertTable(Table table, boolean overwrite) throws MetaException {	
commit insert into table overwrite 

public void configureJobConf(TableDesc tableDesc, JobConf jobConf) {	if (UserGroupInformation.isSecurityEnabled()) {	
setting to to enable split generation on 

private void cleanWorkingDir() {	final FileSystem fileSystem;	try {	fileSystem = getStagingWorkingDir().getFileSystem(getConf());	fileSystem.delete(getStagingWorkingDir(), true);	} catch (IOException e) {	
got exception while cleaning working directory 

private static HttpClient makeHttpClient(Lifecycle lifecycle) {	final int numConnection = HiveConf .getIntVar(SessionState.getSessionConf(), HiveConf.ConfVars.HIVE_DRUID_NUM_HTTP_CONNECTION );	final Period readTimeout = new Period( HiveConf.getVar(SessionState.getSessionConf(), HiveConf.ConfVars.HIVE_DRUID_HTTP_READ_TIMEOUT ));	
creating druid http client with max parallel connections and ms read timeout 

private static HttpClient makeHttpClient(Lifecycle lifecycle) {	final int numConnection = HiveConf .getIntVar(SessionState.getSessionConf(), HiveConf.ConfVars.HIVE_DRUID_NUM_HTTP_CONNECTION );	final Period readTimeout = new Period( HiveConf.getVar(SessionState.getSessionConf(), HiveConf.ConfVars.HIVE_DRUID_HTTP_READ_TIMEOUT ));	final HttpClient httpClient = HttpClientInit.createClient( HttpClientConfig.builder().withNumConnections(numConnection) .withReadTimeout(new Period(readTimeout).toStandardDuration()).build(), lifecycle );	if (UserGroupInformation.isSecurityEnabled()) {	
building kerberos http client 

========================= hive sample_20 =========================

registry = new LlapRegistryService(false);	registry.init(conf);	registry.start();	yarnRegistries.put(key, registry);	}	} else {	registry = new LlapRegistryService(false);	registry.init(conf);	registry.start();	}	
using llap registry client type 

public void serviceInit(Configuration conf) {	String hosts = HiveConf.getTrimmedVar(conf, ConfVars.LLAP_DAEMON_SERVICE_HOSTS);	if (hosts.startsWith("@")) {	registry = new LlapZookeeperRegistryImpl(hosts.substring(1), conf);	this.isDynamic=true;	} else {	registry = new LlapFixedRegistryImpl(hosts, conf);	this.isDynamic=false;	}	
using llap registry type 

public void serviceStop() throws Exception {	if (isDaemon) {	unregisterWorker();	}	if (this.registry != null) {	this.registry.stop();	} else {	
stopping non existent registry service 

========================= hive sample_650 =========================

private TezSessionPoolSession createAndInitSession( String queue, boolean isDefault, HiveConf conf) {	TezSessionPoolSession sessionState = createSession(TezSessionState.makeSessionId(), conf);	if (queue != null) {	sessionState.setQueueName(queue);	}	if (isDefault) {	sessionState.setDefault();	}	
created new tez session for queue with session id 

private TezSessionState getSession(HiveConf conf, boolean doOpen) throws Exception {	String queueName = conf.get(TezConfiguration.TEZ_QUEUE_NAME);	boolean hasQueue = (queueName != null) && !queueName.isEmpty();	if (hasQueue) {	switch (customQueueAllowed) {	case FALSE: throw new HiveException("Specifying " + TezConfiguration.TEZ_QUEUE_NAME + " is not allowed");	case IGNORE: {	
user has specified queue ignoring the setting 

default: }	}	if (restrictedConfig != null) {	restrictedConfig.validate(conf);	}	if (initConf != null) {	conf.set(ConfVars.LLAP_CACHE_DEFAULT_FS_FILE_ID.varname, HiveConf.getVarWithoutType(initConf, ConfVars.LLAP_CACHE_DEFAULT_FS_FILE_ID));	}	boolean nonDefaultUser = conf.getBoolVar(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS);	if (nonDefaultUser || !hasInitialSessions || hasQueue) {	
queuename nondefaultuser defaultqueuepool hasinitialsessions 

if (restrictedConfig != null) {	restrictedConfig.validate(conf);	}	if (initConf != null) {	conf.set(ConfVars.LLAP_CACHE_DEFAULT_FS_FILE_ID.varname, HiveConf.getVarWithoutType(initConf, ConfVars.LLAP_CACHE_DEFAULT_FS_FILE_ID));	}	boolean nonDefaultUser = conf.getBoolVar(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS);	if (nonDefaultUser || !hasInitialSessions || hasQueue) {	return getNewSessionState(conf, queueName, doOpen);	}	
choosing a session from the defaultqueuepool 

private TezSessionState getNewSessionState(HiveConf conf, String queueName, boolean doOpen) throws Exception {	TezSessionPoolSession retTezSessionState = createAndInitSession(queueName, false, conf);	if (queueName != null) {	conf.set(TezConfiguration.TEZ_QUEUE_NAME, queueName);	}	if (doOpen) {	retTezSessionState.open();	
started a new session for queue session id 

public static void closeIfNotDefault( TezSessionState tezSessionState, boolean keepTmpDir) throws Exception {	
closing tez session if not default 

public void destroy(TezSessionState tezSessionState) throws Exception {	
we are closing a default non default session because of retry failure 

private static boolean canWorkWithSameSession(TezSessionState session, HiveConf conf) throws HiveException {	if (session == null || conf == null || !session.isOpen()) {	return false;	}	try {	UserGroupInformation ugi = Utils.getUGI();	String userName = ugi.getShortUserName();	
the current user session user 

private static boolean canWorkWithSameSession(TezSessionState session, HiveConf conf) throws HiveException {	if (session == null || conf == null || !session.isOpen()) {	return false;	}	try {	UserGroupInformation ugi = Utils.getUGI();	String userName = ugi.getShortUserName();	if (userName.equals(session.getUser()) == false) {	
different users incoming existing 

} catch (Exception e) {	throw new HiveException(e);	}	boolean doAsEnabled = conf.getBoolVar(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS);	if (doAsEnabled != session.getConf().getBoolVar(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS)) {	return false;	}	if (!session.isDefault()) {	String queueName = session.getQueueName();	String confQueueName = conf.get(TezConfiguration.TEZ_QUEUE_NAME);	
current queue name is incoming queue name is 

public void closeAndReopenExpiredSession(TezSessionPoolSession oldSession) throws Exception {	String queueName = oldSession.getQueueName();	if (queueName == null) {	
pool session has a null queue 

public void unregisterOpenSession(TezSessionPoolSession session) {	if (LOG.isDebugEnabled()) {	
closed a pool session 

========================= hive sample_4020 =========================

public void run(HookContext hookContext) throws Exception {	assert (hookContext.getHookType() == HookContext.HookType.POST_EXEC_HOOK);	HiveConf conf = hookContext.getConf();	
executing post execution hook to print orc file dump 

FetchTask fetchTask = plan.getFetchTask();	if (fetchTask != null) {	SessionState ss = SessionState.get();	SessionState.LogHelper console = ss.getConsole();	PrintStream old = System.out;	System.setOut(console.getErrStream());	FetchWork fetchWork = fetchTask.getWork();	boolean partitionedTable = fetchWork.isPartitioned();	List<Path> directories;	if (partitionedTable) {	
printing orc file dump for files from partitioned directory 

SessionState ss = SessionState.get();	SessionState.LogHelper console = ss.getConsole();	PrintStream old = System.out;	System.setOut(console.getErrStream());	FetchWork fetchWork = fetchTask.getWork();	boolean partitionedTable = fetchWork.isPartitioned();	List<Path> directories;	if (partitionedTable) {	directories = fetchWork.getPartDir();	} else {	
printing orc file dump for files from table directory 

if (partitionedTable) {	directories = fetchWork.getPartDir();	} else {	directories = Lists.newArrayList();	directories.add(fetchWork.getTblDir());	}	for (Path dir : directories) {	FileSystem fs = dir.getFileSystem(conf);	List<FileStatus> fileList = HdfsUtils.listLocatedStatus(fs, dir, hiddenFileFilter);	for (FileStatus fileStatus : fileList) {	
printing orc file dump for 

FileSystem fs = dir.getFileSystem(conf);	List<FileStatus> fileList = HdfsUtils.listLocatedStatus(fs, dir, hiddenFileFilter);	for (FileStatus fileStatus : fileList) {	if (fileStatus.getLen() > 0) {	try {	OrcFile.createReader(fs, fileStatus.getPath());	console.printError("-- BEGIN ORC FILE DUMP --");	FileDump.main(new String[]{fileStatus.getPath().toString(), "--rowindex=*"});	console.printError("-- END ORC FILE DUMP --");	} catch (FileFormatException e) {	
file is not orc skip printing orc file dump 

List<FileStatus> fileList = HdfsUtils.listLocatedStatus(fs, dir, hiddenFileFilter);	for (FileStatus fileStatus : fileList) {	if (fileStatus.getLen() > 0) {	try {	OrcFile.createReader(fs, fileStatus.getPath());	console.printError("-- BEGIN ORC FILE DUMP --");	FileDump.main(new String[]{fileStatus.getPath().toString(), "--rowindex=*"});	console.printError("-- END ORC FILE DUMP --");	} catch (FileFormatException e) {	} catch (IOException e) {	
skip printing orc file dump exception 

if (fileStatus.getLen() > 0) {	try {	OrcFile.createReader(fs, fileStatus.getPath());	console.printError("-- BEGIN ORC FILE DUMP --");	FileDump.main(new String[]{fileStatus.getPath().toString(), "--rowindex=*"});	console.printError("-- END ORC FILE DUMP --");	} catch (FileFormatException e) {	} catch (IOException e) {	}	} else {	
zero length file encountered skip printing orc file dump 

========================= hive sample_3835 =========================

public void rememberCompletedDag(QueryIdentifier queryIdentifier) {	if (completedDagMap.add(queryIdentifier)) {	executorService.schedule(new DagMapCleanerCallable(queryIdentifier), 1, TimeUnit.HOURS);	} else {	
couldn t add to completed dag set 

public void serviceStart() {	
started 

public void serviceStop() {	executorService.shutdownNow();	
stopped 

private void cleanupDir(String dir, long deleteDelay) {	
scheduling deletion of after seconds 

protected Void callInternal() {	Path pathToDelete = new Path(dirToDelete);	if (LOG.isDebugEnabled()) {	
deleting path 

protected Void callInternal() {	Path pathToDelete = new Path(dirToDelete);	if (LOG.isDebugEnabled()) {	}	try {	localFs.delete(new Path(dirToDelete), true);	} catch (IOException e) {	
ignoring exception while cleaning up path 

protected Void callInternal() {	
external cleanup callable for 

protected Void callInternal() {	ReentrantReadWriteLock dagLock = getDagLockNoCreate(queryIdentifier);	if (dagLock == null) {	if (LOG.isTraceEnabled()) {	
null daglock no cleanup required at the moment for 

protected Void callInternal() {	ReentrantReadWriteLock dagLock = getDagLockNoCreate(queryIdentifier);	if (dagLock == null) {	if (LOG.isTraceEnabled()) {	}	return null;	}	boolean locked = dagLock.writeLock().tryLock();	if (!locked) {	if (LOG.isTraceEnabled()) {	
lock not obtained skipping cleanup for 

boolean locked = dagLock.writeLock().tryLock();	if (!locked) {	if (LOG.isTraceEnabled()) {	}	return null;	}	try {	QueryInfo queryInfo = queryInfoMap.get(queryIdentifier);	if (queryInfo != null) {	if (LOG.isTraceEnabled()) {	
queryinfo found for expecting future cleanup 

}	return null;	}	try {	QueryInfo queryInfo = queryInfoMap.get(queryIdentifier);	if (queryInfo != null) {	if (LOG.isTraceEnabled()) {	}	return null;	}	
processing cleanup for 

private void handleFragmentCompleteExternalQuery(QueryInfo queryInfo) {	if (queryInfo.isExternalQuery()) {	ReentrantReadWriteLock dagLock = getDagLock(queryInfo.getQueryIdentifier());	if (dagLock == null) {	
ignoring fragment completion for unknown query 

}	try {	if (queryInfo.getRegisteredFragments().size() == 0) {	queryComplete(queryInfo.getQueryIdentifier(), -1, true);	} else {	if (LOG.isTraceEnabled()) {	LOG.trace( "Not invoking queryComplete on fragmentComplete for {}, since there are known fragments. count={}", queryInfo.getHiveQueryIdString(), queryInfo.getRegisteredFragments().size());	}	}	} catch (IOException e) {	
failed to process query complete for external submission 

========================= hive sample_2232 =========================

public static Exception stopQuietly(Service service) {	try {	stop(service);	} catch (Exception e) {	
when stopping the service 

========================= hive sample_2398 =========================

if (key == tableKey) {	isNewKey = false;	break;	}	++metricPutConflict;	probeSlot += (++i);	slot = (int)(probeSlot & logicalHashBucketMask);	}	if (largestNumberOfSteps < i) {	if (LOG.isDebugEnabled()) {	
probed slots the longest so far to find space 

long newValueRef = newSlotPairs[newPairIndex];	if (newValueRef == 0) {	break;	}	++newMetricPutConflict;	newProbeSlot += (++i);	newSlot = (int)(newProbeSlot & newLogicalHashBucketMask);	}	if (newLargestNumberOfSteps < i) {	if (LOG.isDebugEnabled()) {	
probed slots the longest so far to find space 

========================= hive sample_4132 =========================

public static void clearThreadAttempt(String attemptId) {	assert attemptId != null;	String attemptIdCheck = threadAttemptId.get();	if (!attemptId.equals(attemptIdCheck)) {	
thread is clearing context for but expected 

========================= hive sample_3618 =========================

int unionByteEnd = start + length;	byte[] byteArr = this.bytes.getData();	final int tagEnd = start + 1;	tag = byteArr[start];	field = LazyBinaryFactory.createLazyBinaryObject(uoi.getObjectInspectors().get(tag));	LazyBinaryUtils.checkObjectByteInfo(uoi.getObjectInspectors().get(tag), byteArr, tagEnd, recordInfo, vInt);	fieldStart = tagEnd + recordInfo.elementOffset;	fieldLength = recordInfo.elementSize;	if (!extraFieldWarned &&  (fieldStart + fieldLength) < unionByteEnd) {	extraFieldWarned = true;	
extra bytes detected at the end of the row ignoring similar problems 

tag = byteArr[start];	field = LazyBinaryFactory.createLazyBinaryObject(uoi.getObjectInspectors().get(tag));	LazyBinaryUtils.checkObjectByteInfo(uoi.getObjectInspectors().get(tag), byteArr, tagEnd, recordInfo, vInt);	fieldStart = tagEnd + recordInfo.elementOffset;	fieldLength = recordInfo.elementSize;	if (!extraFieldWarned &&  (fieldStart + fieldLength) < unionByteEnd) {	extraFieldWarned = true;	}	if (!missingFieldWarned && (fieldStart + fieldLength) > unionByteEnd) {	missingFieldWarned = true;	
missing fields expected fields but only got ignoring similar problems 

========================= hive sample_5236 =========================

public void closeOp(boolean abort) throws HiveException {	try {	MapJoinPersistableTableContainer[] mapJoinTables = htsOperator.mapJoinTables;	byte tag = conf.getTag();	if (mapJoinTables == null || mapJoinTables.length < tag || mapJoinTables[tag] == null) {	
mapjointable is null 

public void closeOp(boolean abort) throws HiveException {	try {	MapJoinPersistableTableContainer[] mapJoinTables = htsOperator.mapJoinTables;	byte tag = conf.getTag();	if (mapJoinTables == null || mapJoinTables.length < tag || mapJoinTables[tag] == null) {	} else if (abort) {	if (LOG.isDebugEnabled()) {	
aborting skip dumping side table for tag 

protected void flushToFile(MapJoinPersistableTableContainer tableContainer, byte tag) throws Exception {	MapredLocalWork localWork = getExecContext().getLocalWork();	BucketMapJoinContext mapJoinCtx = localWork.getBucketMapjoinContext();	Path inputPath = getExecContext().getCurrentInputPath();	String bigInputPath = null;	if (inputPath != null && mapJoinCtx != null) {	Set<String> aliases = ((SparkBucketMapJoinContext)mapJoinCtx).getPosToAliasMap().get((int)tag);	bigInputPath = mapJoinCtx.getMappingBigFile( aliases.iterator().next(), inputPath.toString());	}	Path tmpURI = localWork.getTmpHDFSPath();	
temp uri for side table 

} else if (os != null) {	os.close();	}	}	FileStatus status = fs.getFileStatus(path);	htsOperator.console.printInfo(Utilities.now() + "\tUploaded 1 File to: " + path + " (" + status.getLen() + " bytes)");	} catch (Exception e) {	try {	fs.delete(path, false);	} catch (Exception ex) {	
got exception in deleting partial side table dump for tag file 

========================= hive sample_3862 =========================

protected Session initialValue() {	try {	return createSession();	} catch (Exception e) {	
couldn t create jms session 

public void remove() {	if (get() != null) {	try {	get().close();	} catch (Exception e) {	
unable to close bad jms session ignored error 

Destination topic = createTopic(topicName);	Message msg = session.get().createTextMessage(hCatEventMessage.toString());	msg.setStringProperty(HCatConstants.HCAT_EVENT, hCatEventMessage.getEventType().toString());	msg.setStringProperty(HCatConstants.HCAT_MESSAGE_VERSION, messageFactory.getVersion());	msg.setStringProperty(HCatConstants.HCAT_MESSAGE_FORMAT, messageFactory.getMessageFormat());	MessageProducer producer = createProducer(topic);	producer.send(msg);	session.get().commit();	} catch (Exception e) {	if (retries >= 0) {	
seems like connection is lost will retry retries left error was 

msg.setStringProperty(HCatConstants.HCAT_MESSAGE_VERSION, messageFactory.getVersion());	msg.setStringProperty(HCatConstants.HCAT_MESSAGE_FORMAT, messageFactory.getMessageFormat());	MessageProducer producer = createProducer(topic);	producer.send(msg);	session.get().commit();	} catch (Exception e) {	if (retries >= 0) {	testAndCreateConnection();	send(hCatEventMessage, topicName, retries - 1);	} else {	
failed to send message on topic event after retries 

protected synchronized void testAndCreateConnection() {	if (conn != null) {	session.remove();	if (!isConnectionHealthy()) {	try {	conn.close();	} catch (Exception e) {	
unable to close bad jms connection ignored error 

} catch (Exception e) {	}	conn = createConnection();	}	} else {	conn = createConnection();	}	try {	session.set(createSession());	} catch (JMSException e) {	
couldn t create jms session ignored the error 

protected Connection createConnection() {	
will create new jms connection 

protected Connection createConnection() {	Context jndiCntxt;	Connection jmsConnection = null;	try {	jndiCntxt = new InitialContext();	ConnectionFactory connFac = (ConnectionFactory) jndiCntxt.lookup("ConnectionFactory");	jmsConnection = connFac.createConnection();	jmsConnection.start();	jmsConnection.setExceptionListener(new ExceptionListener() {	public void onException(JMSException jmse) {	
jms exception listener received exception ignored the error 

try {	jndiCntxt = new InitialContext();	ConnectionFactory connFac = (ConnectionFactory) jndiCntxt.lookup("ConnectionFactory");	jmsConnection = connFac.createConnection();	jmsConnection.start();	jmsConnection.setExceptionListener(new ExceptionListener() {	public void onException(JMSException jmse) {	}	});	} catch (NamingException e) {	
jndi error while setting up message bus connection please make sure file named jndi properties is in classpath and contains appropriate key value pairs 

jndiCntxt = new InitialContext();	ConnectionFactory connFac = (ConnectionFactory) jndiCntxt.lookup("ConnectionFactory");	jmsConnection = connFac.createConnection();	jmsConnection.start();	jmsConnection.setExceptionListener(new ExceptionListener() {	public void onException(JMSException jmse) {	}	});	} catch (NamingException e) {	} catch (JMSException e) {	
failed to initialize connection to message bus 

ConnectionFactory connFac = (ConnectionFactory) jndiCntxt.lookup("ConnectionFactory");	jmsConnection = connFac.createConnection();	jmsConnection.start();	jmsConnection.setExceptionListener(new ExceptionListener() {	public void onException(JMSException jmse) {	}	});	} catch (NamingException e) {	} catch (JMSException e) {	} catch (Throwable t) {	
unable to connect to jms provider 

protected void finalize() throws Throwable {	if (conn != null) {	try {	conn.close();	} catch (Exception e) {	
couldn t close jms connection ignored the error 

========================= hive sample_1019 =========================

if(pctx.hasAcidWrite()) {	StringBuilder tblNames = new StringBuilder();	for(FileSinkDesc fsd : pctx.getAcidSinks()) {	if(fsd.getTable() != null) {	tblNames.append(fsd.getTable().getDbName()).append('.').append(fsd.getTable().getTableName()).append(',');	}	}	if(tblNames.length() > 0) {	tblNames.setLength(tblNames.length() - 1);	}	
overriding to due to a write to transactional table s 

========================= hive sample_3024 =========================

fsShell.setConf(conf);	if (group != null && !group.isEmpty()) {	run(fsShell, new String[]{"-chgrp", "-R", group, target.toString()});	}	if (aclEnabled) {	if (null != aclEntries) {	try {	String aclEntry = Joiner.on(",").join(aclEntries);	run(fsShell, new String[]{"-setfacl", "-R", "--set", aclEntry, target.toString()});	} catch (Exception e) {	
skipping acl inheritance file system for path does not support acls but dfs namenode acls enabled is set to true 

fsShell.setConf(conf);	if (group != null && !group.isEmpty()) {	run(fsShell, new String[]{"-chgrp", "-R", group, target.toString()});	}	if (aclEnabled) {	if (null != aclEntries) {	try {	String aclEntry = Joiner.on(",").join(aclEntries);	run(fsShell, new String[]{"-setfacl", "-R", "--set", aclEntry, target.toString()});	} catch (Exception e) {	
the details are 

}	if (aclEnabled) {	if (null != aclEntries) {	fs.setAcl(target, aclEntries);	}	} else {	fs.setPermission(target, sourcePerm);	}	}	} catch (Exception e) {	
unable to inherit permissions for file from file 

}	if (aclEnabled) {	if (null != aclEntries) {	fs.setAcl(target, aclEntries);	}	} else {	fs.setPermission(target, sourcePerm);	}	}	} catch (Exception e) {	
exception while inheriting permissions 

private static void run(FsShell shell, String[] command) throws Exception {	LOG.debug(ArrayUtils.toString(command));	int retval = shell.run(command);	
return value is 

public HadoopFileStatus(Configuration conf, FileSystem fs, Path file) throws IOException {	FileStatus fileStatus = fs.getFileStatus(file);	AclStatus aclStatus = null;	if (Objects.equal(conf.get("dfs.namenode.acls.enabled"), "true")) {	try {	aclStatus = fs.getAclStatus(file);	} catch (Exception e) {	
skipping acl inheritance file system for path does not support acls but dfs namenode acls enabled is set to true 

public HadoopFileStatus(Configuration conf, FileSystem fs, Path file) throws IOException {	FileStatus fileStatus = fs.getFileStatus(file);	AclStatus aclStatus = null;	if (Objects.equal(conf.get("dfs.namenode.acls.enabled"), "true")) {	try {	aclStatus = fs.getAclStatus(file);	} catch (Exception e) {	
the details are 

========================= hive sample_1449 =========================

private List<HivePrivilegeObject> getFilteredObjects(List<HivePrivilegeObject> listObjs) throws MetaException {	SessionState ss = SessionState.get();	HiveAuthzContext.Builder authzContextBuilder = new HiveAuthzContext.Builder();	authzContextBuilder.setUserIpAddress(ss.getUserIpAddress());	authzContextBuilder.setForwardedAddresses(ss.getForwardedAddresses());	try {	return ss.getAuthorizerV2().filterListCmdObjects(listObjs, authzContextBuilder.build());	} catch (HiveAuthzPluginException e) {	
authorization error 

private List<HivePrivilegeObject> getFilteredObjects(List<HivePrivilegeObject> listObjs) throws MetaException {	SessionState ss = SessionState.get();	HiveAuthzContext.Builder authzContextBuilder = new HiveAuthzContext.Builder();	authzContextBuilder.setUserIpAddress(ss.getUserIpAddress());	authzContextBuilder.setForwardedAddresses(ss.getForwardedAddresses());	try {	return ss.getAuthorizerV2().filterListCmdObjects(listObjs, authzContextBuilder.build());	} catch (HiveAuthzPluginException e) {	throw new MetaException(e.getMessage());	} catch (HiveAccessControlException e) {	
AccessControlException 

========================= hive sample_3160 =========================

ctxBuilder.setCommandString(cmdString);	ctxBuilder.setUserIpAddress(ss.getUserIpAddress());	ctxBuilder.setForwardedAddresses(ss.getForwardedAddresses());	queryContext = ctxBuilder.build();	if (authorizer != null && needTransform() && !skipTableMasking) {	enable = true;	translator = new UnparseTranslator(conf);	translator.enable();	}	} catch (Exception e) {	
failed to initialize masking policy 

sb.append(" " + maskAndFilterInfo.additionalTabInfo);	String filter = privObject.getRowFilterExpression();	if (filter != null) {	sb.append(" WHERE " + filter);	doRowFiltering = true;	}	sb.append(")" + HiveUtils.unparseIdentifier(maskAndFilterInfo.alias, conf));	if (!doColumnMasking && !doRowFiltering) {	return null;	} else {	
tablemask creates 

========================= hive sample_3530 =========================

public int execute(DriverContext driverContext) {	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
executing importcommit for 

========================= hive sample_3863 =========================

protected void setIpAddress(final TProtocol in) {	TTransport transport = in.getTransport();	TSocket tSocket = getUnderlyingSocketFromTransport(transport);	if (tSocket == null) {	
unknown transport cannot determine ipaddress 

========================= hive sample_2321 =========================

}	ImmutableList.Builder<ReduceSinkOperator> l = ImmutableList.builder();	for (Operator<?> parent: pRSChild.getParentOperators()) {	ReduceSinkOperator rsOp = (ReduceSinkOperator) parent;	l.add(rsOp);	if (rsOp.getConf().getNumReducers() > maxNumReducers) {	maxNumReducers = rsOp.getConf().getNumReducers();	}	}	if (ReduceSinkDeDuplicationUtils.strictMerge(cRS, l.build())) {	
set to forward data 

if (!(parent instanceof ReduceSinkOperator)) {	return false;	}	ReduceSinkOperator rsOp = (ReduceSinkOperator) parent;	l.add(rsOp);	if (rsOp.getConf().getNumReducers() > maxNumReducers) {	maxNumReducers = rsOp.getConf().getNumReducers();	}	}	if (ReduceSinkDeDuplicationUtils.strictMerge(cRS, l.build())) {	
set to forward data 

if (ReduceSinkDeDuplicationUtils.strictMerge(cRS, l.build())) {	cRS.getConf().setForwarding(true);	propagateMaxNumReducers(dedupCtx, cRS, maxNumReducers);	return true;	}	} else {	if (pRS.getConf().getNumReducers() > maxNumReducers) {	maxNumReducers = pRS.getConf().getNumReducers();	}	if (ReduceSinkDeDuplicationUtils.strictMerge(cRS, pRS)) {	
set to forward data 

private static void propagateMaxNumReducers(ReduceSinkJoinDeDuplicateProcCtx dedupCtx, ReduceSinkOperator rsOp, int maxNumReducers) throws SemanticException {	if (rsOp == null) {	return;	}	if (rsOp.getChildOperators().get(0) instanceof MapJoinOperator || rsOp.getChildOperators().get(0) instanceof CommonMergeJoinOperator) {	for (Operator<?> p : rsOp.getChildOperators().get(0).getParentOperators()) {	ReduceSinkOperator pRSOp = (ReduceSinkOperator) p;	pRSOp.getConf().setReducerTraits(EnumSet.of(ReducerTraits.FIXED));	pRSOp.getConf().setNumReducers(maxNumReducers);	
set to fixed parallelism 

pRSOp.getConf().setReducerTraits(EnumSet.of(ReducerTraits.FIXED));	pRSOp.getConf().setNumReducers(maxNumReducers);	if (pRSOp.getConf().isForwarding()) {	ReduceSinkOperator newRSOp = CorrelationUtilities.findFirstPossibleParent( pRSOp, ReduceSinkOperator.class, dedupCtx.trustScript());	propagateMaxNumReducers(dedupCtx, newRSOp, maxNumReducers);	}	}	} else {	rsOp.getConf().setReducerTraits(EnumSet.of(ReducerTraits.FIXED));	rsOp.getConf().setNumReducers(maxNumReducers);	
set to fixed parallelism 

========================= hive sample_3025 =========================

public Class<? extends InputFormat> getInputFormatClass() {	if (HiveConf.getVar(jobConf, HiveConf.ConfVars.HIVE_HBASE_SNAPSHOT_NAME) != null) {	
using tablesnapshotinputformat 

public Class<? extends InputFormat> getInputFormatClass() {	if (HiveConf.getVar(jobConf, HiveConf.ConfVars.HIVE_HBASE_SNAPSHOT_NAME) != null) {	return HiveHBaseTableSnapshotInputFormat.class;	}	
using hivehbasetableinputformat 

tableName = tableProperties.getProperty(hive_metastoreConstants.META_TABLE_NAME);	tableName = tableName.toLowerCase();	if (tableName.startsWith(DEFAULT_PREFIX)) {	tableName = tableName.substring(DEFAULT_PREFIX.length());	}	}	jobProperties.put(HBaseSerDe.HBASE_TABLE_NAME, tableName);	Configuration jobConf = getJobConf();	addHBaseResources(jobConf, jobProperties);	if (this.configureInputJobProps) {	
configuring input job properties 

for (String k : jobProperties.keySet()) {	jobConf.set(k, jobProperties.get(k));	}	try {	addHBaseDelegationToken(jobConf);	} catch (IOException | MetaException e) {	throw new IllegalStateException("Error while configuring input job properties", e);	}	}	else {	
configuring output job properties 

========================= hive sample_606 =========================

public static HCatRecordObjectInspector getHCatRecordObjectInspector( StructTypeInfo typeInfo) throws SerDeException {	HCatRecordObjectInspector oi = cachedHCatRecordObjectInspectors.get(typeInfo);	if (oi == null) {	
got asked for oi for 

public static ObjectInspector getStandardObjectInspectorFromTypeInfo(TypeInfo typeInfo) {	ObjectInspector oi = cachedObjectInspectors.get(typeInfo);	if (oi == null) {	
got asked for oi for 

========================= hive sample_743 =========================

}	Map<List<ExprNodeDesc>, List<List<String>>> skewedValues = getSkewedValues(joinOp, tableScanOpsForJoin);	if (skewedValues == null || skewedValues.size() == 0) {	return null;	}	Operator<? extends OperatorDesc> currOpClone;	try {	currOpClone = currOp.clone();	insertRowResolvers(currOp, currOpClone, ctx);	} catch (CloneNotSupportedException e) {	
operator tree could not be cloned 

JoinOperator joinOpClone;	if (processSelect) {	joinOpClone = (JoinOperator)(currOpClone.getParentOperators().get(0));	} else {	joinOpClone = (JoinOperator)currOpClone;	}	joinOpClone.getConf().cloneQBJoinTreeProps(joinOp.getConf());	parseContext.getJoinOps().add(joinOpClone);	List<TableScanOperator> tableScanCloneOpsForJoin = new ArrayList<TableScanOperator>();	if (!getTableScanOpsForJoin(joinOpClone, tableScanCloneOpsForJoin)) {	
operator tree not properly cloned 

========================= hive sample_3072 =========================

public synchronized void registerTask(RuntimeTask task, ErrorReporter errorReporter) {	TezCounters tezCounters = task.addAndGetTezCounter(fragmentId);	FragmentCountersMap.registerCountersForFragment(fragmentId, tezCounters);	
registered counters for fragment vertexname 

public synchronized void unregisterTask(TezTaskAttemptID taskAttemptID) {	
unregistered counters for fragment 

public Boolean call() throws Exception {	while (!task.isTaskDone() && !task.wasErrorReported()) {	ResponseWrapper response = heartbeat(null);	if (response.shouldDie) {	
asked to die via task heartbeat 

TezHeartbeatRequest request = new TezHeartbeatRequest(requestId, events, fromPreRoutedEventId, containerIdStr, task.getTaskAttemptID(), fromEventId, maxEvents);	if (LOG.isDebugEnabled()) {	LOG.debug("Sending heartbeat to AM, request=" + request);	}	maybeLogCounters();	TezHeartbeatResponse response = umbilical.heartbeat(request);	if (LOG.isDebugEnabled()) {	LOG.debug("Received heartbeat response from AM, response=" + response);	}	if (response.shouldDie()) {	
received should die response from am 

private boolean taskSucceeded(TezTaskAttemptID taskAttemptID) throws IOException, TezException {	if (!finalEventQueued.getAndSet(true)) {	TezEvent statusUpdateEvent = new TezEvent(getStatusUpdateEvent(true), updateEventMetadata);	TezEvent taskCompletedEvent = new TezEvent(new TaskAttemptCompletedEvent(), updateEventMetadata);	if (LOG.isDebugEnabled()) {	LOG.debug("Invoking OOB heartbeat for successful attempt: {}, isTaskDone={}", taskAttemptID, task.isTaskDone());	}	completionListener.fragmentCompleting(fragmentRequestId, SchedulerFragmentCompletingListener.State.SUCCESS);	return !heartbeat(Lists.newArrayList(statusUpdateEvent, taskCompletedEvent)).shouldDie;	} else {	
a final task state event has already been sent not sending again 

diagnostics = diagnostics + ":" + ExceptionUtils.getStackTrace(t);	}	if (isKilled) {	tezEvents.add(new TezEvent(new TaskAttemptKilledEvent(diagnostics), srcMeta == null ? updateEventMetadata : srcMeta));	} else {	tezEvents.add(new TezEvent(new TaskAttemptFailedEvent(diagnostics, taskFailureType), srcMeta == null ? updateEventMetadata : srcMeta));	}	try {	tezEvents.add(new TezEvent(getStatusUpdateEvent(true), updateEventMetadata));	} catch (Exception e) {	
error when get constructing taskstatusupdateevent not sending it out 

try {	tezEvents.add(new TezEvent(getStatusUpdateEvent(true), updateEventMetadata));	} catch (Exception e) {	}	if (LOG.isDebugEnabled()) {	LOG.debug( "Invoking OOB heartbeat for failed/killed attempt: {}, isTaskDone={}, isKilled={}", taskAttemptID, task.isTaskDone(), isKilled);	}	completionListener.fragmentCompleting(fragmentRequestId, isKilled ? SchedulerFragmentCompletingListener.State.KILLED : SchedulerFragmentCompletingListener.State.FAILED);	return !heartbeat(tezEvents).shouldDie;	} else {	
a final task state event has already been sent not sending again 

========================= hive sample_2233 =========================

public void run() {	while (true) {	int executorCount = getExecutorCount(true);	if (executorCount != lastExecutorCount && lastExecutorCount >= 0) {	clusterChangedCallback.run();	}	lastExecutorCount = executorCount;	try {	Thread.sleep(CLUSTER_INFO_UPDATE_INTERVAL_MS / 2);	} catch (InterruptedException e) {	
cluster state update thread was interrupted 

protected int getExecutorCount(boolean allowUpdate) {	if (allowUpdate && !clusterState.initClusterInfo()) {	
failed to get llap cluster information for we may rely on outdated cluster status 

protected int getExecutorCount(boolean allowUpdate) {	if (allowUpdate && !clusterState.initClusterInfo()) {	}	if (!clusterState.hasClusterInfo()) {	
no cluster information available to allocate no guaranteed tasks will be used 

protected int getExecutorCount(boolean allowUpdate) {	if (allowUpdate && !clusterState.initClusterInfo()) {	}	if (!clusterState.hasClusterInfo()) {	return 0;	}	int unknownNodes = clusterState.getNodeCountWithUnknownExecutors();	if (unknownNodes > 0) {	
there are nodes with unknown executor count only guaranteed tasks will be allocated 

private void updateSessionAsync(final WmTezSession session, final int intAlloc) {	boolean needsUpdate = session.setSendingGuaranteed(intAlloc);	if (!needsUpdate) return;	UpdateQueryRequestProto request = UpdateQueryRequestProto .newBuilder().setGuaranteedTaskCount(intAlloc).build();	
updating with guaranteed tasks 

public void setResponse(UpdateQueryResponseProto response) {	int nextUpdate = session.setSentGuaranteed();	if (nextUpdate >= 0) {	
sending a new update to in the response 

public void indicateError(Throwable t) {	
failed to update guaranteed tasks count for the session 

public void indicateError(Throwable t) {	boolean isOkToFail = session.setFailedToSendGuaranteed();	if (isOkToFail) return;	try {	session.handleUpdateError(endpointVersion);	} catch (Exception e) {	
failed to kill the session 

========================= hive sample_3971 =========================

public Object evaluate(DeferredObject[] arg0) throws HiveException {	if (arg0.length != 1) {	
udfhellotest expects exactly argument 

public Object evaluate(DeferredObject[] arg0) throws HiveException {	if (arg0.length != 1) {	throw new HiveException("UDFHelloTest expects exactly 1 argument");	}	if (arg0[0].get() == null) {	
empty input 

========================= hive sample_2663 =========================

public synchronized void start() {	int i = 0;	try {	for (int n = serviceList.size(); i < n; i++) {	Service service = serviceList.get(i);	service.start();	}	super.start();	} catch (Throwable e) {	
error starting services 

private synchronized void stop(int numOfServicesStarted) {	for (int i = numOfServicesStarted; i >= 0; i--) {	Service service = serviceList.get(i);	try {	service.stop();	} catch (Throwable t) {	
error stopping 

public void run() {	try {	compositeService.stop();	} catch (Throwable t) {	
error stopping 

========================= hive sample_2399 =========================

public static boolean disableDataSource(SQLMetadataConnector connector, final MetadataStorageTablesConfig metadataStorageTablesConfig, final String dataSource ) {	try {	if (!getAllDataSourceNames(connector, metadataStorageTablesConfig).contains(dataSource)) {	
cannot delete data source does not exist 

try {	if (!getAllDataSourceNames(connector, metadataStorageTablesConfig).contains(dataSource)) {	return false;	}	connector.getDBI().withHandle( (HandleCallback<Void>) handle -> {	disableDataSourceWithHandle(handle, metadataStorageTablesConfig, dataSource);	return null;	}	);	} catch (Exception e) {	
error removing datasource s 

return connector.getDBI().inTransaction( (handle, transactionStatus) -> {	VersionedIntervalTimeline<String, DataSegment> timeline;	if (overwrite) {	disableDataSourceWithHandle(handle, metadataStorageTablesConfig, dataSource);	timeline = new VersionedIntervalTimeline<>(Ordering.natural());	} else {	if (segments.isEmpty()) {	return Collections.EMPTY_LIST;	}	Interval indexedInterval = JodaUtils .umbrellaInterval(Iterables.transform(segments, input -> input.getInterval() ));	
building timeline for umbrella interval 

newShardSpec = getNextPartitionShardSpec(max.getShardSpec());	newVersion = max.getVersion();	}	DataSegment publishedSegment = publishSegmentWithShardSpec( segment, newShardSpec, newVersion, getPath(segment).getFileSystem(conf), dataSegmentPusher );	finalSegmentsToPublish.add(publishedSegment);	timeline.add( publishedSegment.getInterval(), publishedSegment.getVersion(), publishedSegment.getShardSpec().createChunk(publishedSegment) );	}	final PreparedBatch batch = handle.prepareBatch( String.format( "INSERT INTO %1$s (id, dataSource, created_date, start, \"end\", partitioned, version, used, payload) " + "VALUES (:id, :dataSource, :created_date, :start, :end, :partitioned, :version, :used, :payload)", metadataStorageTablesConfig.getSegmentsTable() ) );	for (final DataSegment segment : finalSegmentsToPublish) {	batch.add( new ImmutableMap.Builder<String, Object>() .put("id", segment.getIdentifier()) .put("dataSource", segment.getDataSource()) .put("created_date", new DateTime().toString()) .put("start", segment.getInterval().getStart().toString()) .put("end", segment.getInterval().getEnd().toString()) .put("partitioned", (segment.getShardSpec() instanceof NoneShardSpec) ? false : true ) .put("version", segment.getVersion()) .put("used", true) .put("payload", JSON_MAPPER.writeValueAsBytes(segment)) .build() );	
published 

========================= hive sample_21 =========================

public FixedServiceInstance(String host) {	if (resolveHosts) {	try {	InetAddress inetAddress = InetAddress.getByName(host);	if (NetUtils.isLocalAddress(inetAddress)) {	InetSocketAddress socketAddress = new InetSocketAddress(0);	socketAddress = NetUtils.getConnectAddress(socketAddress);	
adding host identified as local as 

public FixedServiceInstance(String host) {	if (resolveHosts) {	try {	InetAddress inetAddress = InetAddress.getByName(host);	if (NetUtils.isLocalAddress(inetAddress)) {	InetSocketAddress socketAddress = new InetSocketAddress(0);	socketAddress = NetUtils.getConnectAddress(socketAddress);	host = socketAddress.getHostName();	}	} catch (UnknownHostException e) {	
ignoring resolution issues for host 

public void registerStateChangeListener( final ServiceInstanceStateChangeListener<LlapServiceInstance> listener) {	
callbacks for instance state changes are not supported in fixed registry 

========================= hive sample_651 =========================

Map<Integer, Long> parentKeyCounts = desc.getParentKeyCounts();	boolean isCrossProduct = false;	List<ExprNodeDesc> joinExprs = desc.getKeys().values().iterator().next();	if (joinExprs.size() == 0) {	isCrossProduct = true;	}	boolean useOptimizedTables = HiveConf.getBoolVar( hconf, HiveConf.ConfVars.HIVEMAPJOINUSEOPTIMIZEDTABLE);	boolean useHybridGraceHashJoin = desc.isHybridHashJoin();	boolean isFirstKey = true;	long totalMapJoinMemory = desc.getMemoryNeeded();	
memory manager allocates bytes for the loading hashtable 

boolean useOptimizedTables = HiveConf.getBoolVar( hconf, HiveConf.ConfVars.HIVEMAPJOINUSEOPTIMIZEDTABLE);	boolean useHybridGraceHashJoin = desc.isHybridHashJoin();	boolean isFirstKey = true;	long totalMapJoinMemory = desc.getMemoryNeeded();	if (totalMapJoinMemory <= 0) {	totalMapJoinMemory = HiveConf.getLongVar( hconf, HiveConf.ConfVars.HIVECONVERTJOINNOCONDITIONALTASKTHRESHOLD);	}	long processMaxMemory = ManagementFactory.getMemoryMXBean().getHeapMemoryUsage().getMax();	if (totalMapJoinMemory > processMaxMemory) {	float hashtableMemoryUsage = HiveConf.getFloatVar( hconf, HiveConf.ConfVars.HIVEHASHTABLEFOLLOWBYGBYMAXMEMORYUSAGE);	
totalmapjoinmemory value of is greater than the max memory size of 

if (totalMapJoinMemory > processMaxMemory) {	float hashtableMemoryUsage = HiveConf.getFloatVar( hconf, HiveConf.ConfVars.HIVEHASHTABLEFOLLOWBYGBYMAXMEMORYUSAGE);	totalMapJoinMemory = (long) (processMaxMemory * hashtableMemoryUsage);	}	HybridHashTableConf nwayConf = null;	long totalSize = 0;	int biggest = 0;	Map<Integer, Long> tableMemorySizes = null;	if (useHybridGraceHashJoin && mapJoinTables.length > 2) {	nwayConf = new HybridHashTableConf();	
n way join small tables 

boolean doMemCheck = false;	long effectiveThreshold = 0;	if (memoryMonitorInfo != null) {	effectiveThreshold = memoryMonitorInfo.getEffectiveThreshold(desc.getMaxMemoryAvailable());	if (!LlapDaemonInfo.INSTANCE.isLlap()) {	memoryMonitorInfo.setLlap(false);	}	if (memoryMonitorInfo.doMemoryMonitoring()) {	doMemCheck = true;	if (LOG.isInfoEnabled()) {	
memory monitoring for hash table loader enabled 

memoryMonitorInfo.setLlap(false);	}	if (memoryMonitorInfo.doMemoryMonitoring()) {	doMemCheck = true;	if (LOG.isInfoEnabled()) {	}	}	}	if (!doMemCheck) {	if (LOG.isInfoEnabled()) {	
not doing hash table memory monitoring 

throw new HiveException(e);	}	try {	KeyValueReader kvReader = (KeyValueReader) input.getReader();	MapJoinObjectSerDeContext keyCtx = mapJoinTableSerdes[pos].getKeyContext(), valCtx = mapJoinTableSerdes[pos].getValueContext();	if (useOptimizedTables) {	ObjectInspector keyOi = keyCtx.getSerDe().getObjectInspector();	if (!MapJoinBytesTableContainer.isSupportedKey(keyOi)) {	if (isFirstKey) {	useOptimizedTables = false;	
not using optimized hash table only a subset of mapjoin keys is supported unsupported key 

MapJoinTableContainer tableContainer;	if (useOptimizedTables) {	if (!useHybridGraceHashJoin || isCrossProduct) {	tableContainer = new MapJoinBytesTableContainer(hconf, valCtx, keyCount, 0);	} else {	tableContainer = new HybridHashTableContainer(hconf, keyCount, memory, desc.getParentDataSizes().get(pos), nwayConf);	}	} else {	tableContainer = new HashMapWrapper(hconf, keyCount);	}	
loading hash table for input cachekey tablecontainer smalltablepos 

tableContainer.putRow((Writable) kvReader.getCurrentKey(), (Writable) kvReader.getCurrentValue());	numEntries++;	if (doMemCheck && (numEntries % memoryMonitorInfo.getMemoryCheckInterval() == 0)) {	final long estMemUsage = tableContainer.getEstimatedMemorySize();	if (estMemUsage > effectiveThreshold) {	String msg = "Hash table loading exceeded memory limits for input: " + inputName + " numEntries: " + numEntries + " estimatedMemoryUsage: " + estMemUsage + " effectiveThreshold: " + effectiveThreshold + " memoryMonitorInfo: " + memoryMonitorInfo;	LOG.error(msg);	throw new MapJoinMemoryExhaustionError(msg);	} else {	if (LOG.isInfoEnabled()) {	
checking hash table loader memory usage for input numentries estimatedmemoryusage effectivethreshold 

throw new MapJoinMemoryExhaustionError(msg);	} else {	if (LOG.isInfoEnabled()) {	}	}	}	}	tableContainer.seal();	mapJoinTables[pos] = tableContainer;	if (doMemCheck) {	
finished loading hash table for input cachekey numentries estimatedmemoryusage 

} else {	if (LOG.isInfoEnabled()) {	}	}	}	}	tableContainer.seal();	mapJoinTables[pos] = tableContainer;	if (doMemCheck) {	} else {	
finished loading hash table for input cachekey numentries 

========================= hive sample_4021 =========================

public static boolean closeClassLoadersTo(ClassLoader current, ClassLoader stop) {	if (!isValidHierarchy(current, stop)) {	return false;	}	for (; current != null && current != stop; current = current.getParent()) {	try {	closeClassLoader(current);	} catch (IOException e) {	
failed to close class loader 

public static Long extractTxnId(Path file) {	String fileName = file.getName();	String[] parts = fileName.split("_", 4);	if (parts.length < 2 || !(DELTA_PREFIX.equals(parts[0]) || BASE_PREFIX.equals(parts[0]))) {	
cannot extract transaction id for a mm table 

public static Long extractTxnId(Path file) {	String fileName = file.getName();	String[] parts = fileName.split("_", 4);	if (parts.length < 2 || !(DELTA_PREFIX.equals(parts[0]) || BASE_PREFIX.equals(parts[0]))) {	return null;	}	long writeId = -1;	try {	writeId = Long.parseLong(parts[1]);	} catch (NumberFormatException ex) {	
cannot extract transaction id for a mm table parsing got 

========================= hive sample_1389 =========================

public void begin() throws TransactionException {	try {	lock.acquire(transactionId);	} catch (LockException e) {	throw new TransactionException("Unable to acquire lock for transaction: " + transactionId, e);	}	state = TxnState.OPEN;	
begin transaction id 

try {	metaStoreClient.commitTxn(transactionId);	state = TxnState.COMMITTED;	} catch (NoSuchTxnException e) {	throw new TransactionException("Invalid transaction id: " + transactionId, e);	} catch (TxnAbortedException e) {	throw new TransactionException("Aborted transaction cannot be committed: " + transactionId, e);	} catch (TException e) {	throw new TransactionException("Unable to commit transaction: " + transactionId, e);	}	
committed transaction id 

throw new TransactionException("Unable to release lock: " + lock + " for transaction: " + transactionId, e);	}	try {	metaStoreClient.rollbackTxn(transactionId);	state = TxnState.ABORTED;	} catch (NoSuchTxnException e) {	throw new TransactionException("Unable to abort invalid transaction id : " + transactionId, e);	} catch (TException e) {	throw new TransactionException("Unable to abort transaction id : " + transactionId, e);	}	
aborted transaction id 

private long open(String user) throws TransactionException {	long transactionId = -1;	try {	transactionId = metaStoreClient.openTxn(user);	state = TxnState.INACTIVE;	} catch (TException e) {	throw new TransactionException("Unable to open transaction for user: " + user, e);	}	
opened transaction with id 

========================= hive sample_978 =========================

analyzer.setAcceptsFields(true);	analyzer.setFieldValidator(getFieldValidator());	DecomposedPredicate decomposed = new DecomposedPredicate();	List<IndexSearchCondition> conditions = new ArrayList<IndexSearchCondition>();	decomposed.residualPredicate = (ExprNodeGenericFuncDesc) analyzer.analyzePredicate(predicate, conditions);	if (!conditions.isEmpty()) {	decomposed.pushedPredicate = analyzer.translateSearchConditions(conditions);	try {	decomposed.pushedPredicateObject = getScanRange(conditions);	} catch (Exception e) {	
failed to decompose predicates 

========================= hive sample_581 =========================

private void replaceTableScanProcess(TableScanOperator scanOperator) throws SemanticException {	RewriteQueryUsingAggregateIndexCtx rewriteQueryCtx = this;	String alias = rewriteQueryCtx.getAlias();	HashMap<String, TableScanOperator> topOps = rewriteQueryCtx.getParseContext() .getTopOps();	topOps.remove(alias);	String indexTableName = rewriteQueryCtx.getIndexName();	Table indexTableHandle = null;	try {	indexTableHandle = rewriteQueryCtx.getHiveDb().getTable(indexTableName);	} catch (HiveException e) {	
error while getting the table handle for index table 

indexTableScanDesc.setGatherStats(false);	String k = org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.encodeTableName(indexTableName) + Path.SEPARATOR;	indexTableScanDesc.setStatsAggPrefix(k);	scanOperator.setConf(indexTableScanDesc);	ArrayList<ColumnInfo> sigRS = new ArrayList<ColumnInfo>();	try {	StructObjectInspector rowObjectInspector = (StructObjectInspector) indexTableHandle .getDeserializer().getObjectInspector();	StructField field = rowObjectInspector.getStructFieldRef(rewriteQueryCtx.getIndexKey());	sigRS.add(new ColumnInfo(field.getFieldName(), TypeInfoUtils.getTypeInfoFromObjectInspector( field.getFieldObjectInspector()), indexTableName, false));	} catch (SerDeException e) {	
error while creating the rowresolver for new tablescanoperator 

========================= hive sample_3064 =========================

private static URL checkConfigFile(File f) {	try {	return (f.exists() && f.isFile()) ? f.toURI().toURL() : null;	} catch (Throwable e) {	
error looking for config 

setLoadMetastoreConfig(true);	}	if (isLoadMetastoreConfig() && hivemetastoreSiteUrl != null) {	addResource(hivemetastoreSiteUrl);	}	if (isLoadHiveServer2Config() && hiveServer2SiteUrl != null) {	addResource(hiveServer2SiteUrl);	}	applySystemProperties();	if ((this.get("hive.metastore.ds.retry.attempts") != null) || this.get("hive.metastore.ds.retry.interval") != null) {	
deprecated hive metastore ds retry no longer has any effect use hive hmshandler retry instead 

continue;	}	ConfVars var = HiveConf.getConfVars(key);	if (var == null) {	var = HiveConf.getConfVars(key.trim());	if (var != null) {	trimmed.add(key);	}	}	if (var == null) {	
hiveconf of name does not exist 

}	ConfVars var = HiveConf.getConfVars(key);	if (var == null) {	var = HiveConf.getConfVars(key.trim());	if (var != null) {	trimmed.add(key);	}	}	if (var == null) {	} else if (!var.isType(entry.getValue())) {	
hiveconf expects type value 

========================= hive sample_1409 =========================

public synchronized void removeTokenForJob(String tokenIdentifier) {	int[] refCount = tokenRefMap.get(tokenIdentifier);	if (refCount == null) {	
no refcount found for tokenidentifier 

========================= hive sample_647 =========================

ss.err = new PrintStream(System.err, true, "UTF-8");	} catch (UnsupportedEncodingException e) {	System.exit(1);	}	HiveConf conf = ss.getConf();	HiveConf.setVar(conf, ConfVars.SEMANTIC_ANALYZER_HOOK, HCatSemanticAnalyzer.class.getName());	String engine = HiveConf.getVar(conf, ConfVars.HIVE_EXECUTION_ENGINE);	final String MR_ENGINE = "mr";	if(!MR_ENGINE.equalsIgnoreCase(engine)) {	HiveConf.setVar(conf, ConfVars.HIVE_EXECUTION_ENGINE, MR_ENGINE);	
forcing to 

========================= hive sample_702 =========================

public OperationStatus getStatus() {	String taskStatus = null;	try {	taskStatus = getTaskStatus();	} catch (HiveSQLException sqlException) {	
error getting task status for 

========================= hive sample_2337 =========================

public void handle(Context withinContext) throws Exception {	
processing drop constraint message message 

========================= hive sample_3463 =========================

if (hashCode == slotTriples[tripleIndex + 1] && keyStore.unsafeEqualKey(slotTriples[tripleIndex], keyBytes, keyStart, keyLength)) {	isNewKey = false;	break;	}	++metricPutConflict;	probeSlot += (++i);	slot = (int) (probeSlot & logicalHashBucketMask);	}	if (largestNumberOfSteps < i) {	if (LOG.isDebugEnabled()) {	
probed slots the longest so far to find space 

long newKeyRef = newSlotTriples[newTripleIndex];	if (newKeyRef == 0) {	break;	}	++newMetricPutConflict;	newProbeSlot += (++i);	newSlot = (int)(newProbeSlot & newLogicalHashBucketMask);	}	if (newLargestNumberOfSteps < i) {	if (LOG.isDebugEnabled()) {	
probed slots the longest so far to find space 

========================= hive sample_4125 =========================

protected void schedulePendingTasks() throws InterruptedException {	
attempted schedulpendingtasks 

try {	while (!shouldRun) {	triggerRunCondition.await();	}	shouldRun = false;	} finally {	lock.unlock();	}	TaskInfo taskInfo = delayedTaskQueue.peek();	if (taskInfo == null) {	
triggered gettask but the queue is empty 

} finally {	lock.unlock();	}	TaskInfo taskInfo = delayedTaskQueue.peek();	if (taskInfo == null) {	lastState = STATE_NULL_FOUND;	signalRunComplete();	continue;	}	if (taskInfo.shouldDelayForLocality( LlapTaskSchedulerServiceForTestControlled.this.clock.getTime())) {	
triggered gettask but the first element is not ready to execute 

========================= hive sample_610 =========================

public void addRow(List<Object> t) throws HiveException {	
add is called with objects 

public void addRow(Object[] value) throws HiveException {	
add is called with objects 

========================= hive sample_4060 =========================

Operator<? extends OperatorDesc> curParent = ancestorList.remove(0);	if ((curParent instanceof PTFOperator)) {	return null;	}	if ((curParent instanceof FilterOperator) && curParent.getParentOperators() != null) {	ancestorList.addAll(curParent.getParentOperators());	}	}	if(sel.isIdentitySelect()) {	parent.removeChildAndAdoptItsChildren(sel);	
identity project remover optimization removed 

========================= hive sample_3048 =========================

public int process(Hive db, Table tbl) throws Exception {	
executing stats task 

if (p.isAcid()) {	StatsSetupConst.setBasicStatsState(parameters, StatsSetupConst.FALSE);	}	if (work.isTargetRewritten()) {	StatsSetupConst.setBasicStatsState(parameters, StatsSetupConst.TRUE);	}	if (!work.isExplicitAnalyze() && !followedColStats1) {	StatsSetupConst.clearColumnStatsState(parameters);	}	if(partfileStatus == null){	
partition partfiles is null for 

basicStatsProcessor.collectFileStatus(wh);	Table res = (Table) basicStatsProcessor.process(statsAggregator);	if (res == null) {	return 0;	}	res.getTTable().unsetCreationMetadata();	db.alterTable(tableFullName, res, environmentContext);	if (conf.getBoolVar(ConfVars.TEZ_EXEC_SUMMARY)) {	console.printInfo("Table " + tableFullName + " stats: [" + toString(p.getPartParameters()) + ']');	}	
table stats 

bsp.collectFileStatus(wh);	return null;	}	}));	}	pool.shutdown();	for (Future<Void> future : futures) {	future.get();	}	} catch (InterruptedException e) {	
cancelling file stats lookup tasks 

for (Future future : futures) {	future.cancel(true);	}	if (work.isStatsReliable()) {	ret = 1;	}	} finally {	if (pool != null) {	pool.shutdownNow();	}	
finished getting file stats of all partitions 

ret = 1;	}	} finally {	if (pool != null) {	pool.shutdownNow();	}	}	for (BasicStatsProcessor basicStatsProcessor : processors) {	Object res = basicStatsProcessor.process(statsAggregator);	if (res == null) {	
partition stats 

}	for (BasicStatsProcessor basicStatsProcessor : processors) {	Object res = basicStatsProcessor.process(statsAggregator);	if (res == null) {	continue;	}	updates.add((Partition) res);	if (conf.getBoolVar(ConfVars.TEZ_EXEC_SUMMARY)) {	console.printInfo("Partition " + basicStatsProcessor.partish.getPartition().getSpec() + " stats: [" + toString(basicStatsProcessor.partish.getPartParameters()) + ']');	}	
partition stats 

private ExecutorService buildBasicStatsExecutor() {	int poolSize = conf.getInt(ConfVars.HIVE_MOVE_FILES_THREAD_COUNT.varname, 1);	poolSize = Math.max(poolSize, 1);	final ExecutorService pool = Executors.newFixedThreadPool(poolSize, new ThreadFactoryBuilder().setDaemon(true).setNameFormat("stats-updater-thread-%d").build());	
getting file stats of all partitions threadpool size 

========================= hive sample_5036 =========================

public static void tearDown() throws Exception {	
shutting down metastore 

locationToDateMap.get("isLocatedOutsideTablePath").clear();	iterator = partitionSpecProxy.getPartitionIterator();	while (iterator.hasNext()) {	Partition partition = iterator.next();	locationToDateMap.get(partition.getValues().get(1)).add(partition.getValues().get(0));	}	Assert.assertEquals("Unexpected date-values.", 0, locationToDateMap.get("isLocatedInTablePath").size());	Assert.assertArrayEquals("Unexpected date-values.", expectedDates.toArray(), locationToDateMap.get("isLocatedOutsideTablePath").toArray());	}	catch (Throwable t) {	
unexpected exception 

PartitionSpecProxy clonedPartitions = hmsc.listPartitionSpecs(dbName, targetTableName, -1);	Assert.assertEquals("Unexpected number of partitions returned. ", partitionsForAddition.size(), clonedPartitions.size());	PartitionSpecProxy.PartitionIterator sourceIterator = partitionsForAddition.getPartitionIterator(), targetIterator = clonedPartitions.getPartitionIterator();	while (targetIterator.hasNext()) {	Partition sourcePartition = sourceIterator.next(), targetPartition = targetIterator.next();	Assert.assertEquals("Mismatched values.", sourcePartition.getValues(), targetPartition.getValues());	Assert.assertEquals("Mismatched locations.", sourcePartition.getSd().getLocation(), targetPartition.getSd().getLocation());	}	}	catch (Throwable t) {	
unexpected exception 

Assert.assertEquals("Unexpected number of columns.", 3, partition.getSd().getCols().size());	Assert.assertEquals("Unexpected first column.", "foo", partition.getSd().getCols().get(0).getName());	Assert.assertEquals("Unexpected second column.", "bar", partition.getSd().getCols().get(1).getName());	Assert.assertEquals("Unexpected third column.", "goo", partition.getSd().getCols().get(2).getName());	String partitionLocation = partition.getSd().getLocation();	String tableLocation = table.getSd().getLocation();	Assert.assertTrue("Unexpected partition location: " + partitionLocation + ". " + "Partition should have been within table location: " + tableLocation, partitionLocation.startsWith(tableLocation));	}	}	catch (Throwable t) {	
unexpected exception 

========================= hive sample_1560 =========================

public static Path[] prune(ParseContext ctx, Partition part, ExprNodeDesc pruner) {	Path[] finalPaths = null;	try {	finalPaths = execute(ctx, part, pruner);	} catch (SemanticException e) {	
using full partition scan 

private static Path[] execute(ParseContext ctx, Partition part, ExprNodeDesc pruner) throws SemanticException {	Path[] finalPaths;	List<Path> selectedPaths = new ArrayList<Path>();	if (ListBucketingPrunerUtils.isUnknownState(pruner)) {	
list bucketing pruner is either null or in unknown state so that it uses full partition scan 

private static Path[] generateFinalPath(Partition part, List<Path> selectedPaths) {	Path[] finalPaths;	if (selectedPaths.size() == 0) {	
using full partition scan 

========================= hive sample_2966 =========================

public Process run(List<String> cmd, List<String> removeEnv, Map<String, String> environmentVariables) throws IOException {	
run cmd removeenv environmentvariables 

public Process run(List<String> cmd, List<String> removeEnv, Map<String, String> environmentVariables) throws IOException {	
starting cmd 

========================= hive sample_846 =========================

return 0;	}	}	if (!FileUtils.mkdir(dstFs, toPath, conf)) {	console.printError("Cannot make target directory: " + toPath.toString());	return 2;	}	for (FileStatus oneSrc : srcs) {	String oneSrcPathStr = oneSrc.getPath().toString();	console.printInfo("Copying file: " + oneSrcPathStr);	
copying file 

private static FileStatus[] matchFilesOrDir( FileSystem fs, Path path, boolean isSourceMm) throws IOException {	if (!fs.exists(path)) return null;	if (!isSourceMm) return matchFilesOneDir(fs, path, null);	FileStatus[] mmDirs = fs.listStatus(path, new JavaUtils.AnyIdDirFilter());	if (mmDirs == null || mmDirs.length == 0) return null;	List<FileStatus> allFiles = new ArrayList<FileStatus>();	for (FileStatus mmDir : mmDirs) {	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
found source mm directory 

========================= hive sample_3859 =========================

private void initIOContext(long startPos, boolean isBlockPointer, Path inputPath) {	ioCxtRef = this.getIOContext();	ioCxtRef.setCurrentBlockStart(startPos);	ioCxtRef.setBlockPointer(isBlockPointer);	ioCxtRef.setInputPath(inputPath);	
processing file 

if (this.ioCxtRef.getCurrentBlockStart() == 0) {	footerBuffer = null;	Path filePath = this.ioCxtRef.getInputPath();	PartitionDesc part = null;	try {	if (pathToPartitionInfo == null) {	pathToPartitionInfo = Utilities .getMapWork(jobConf).getPathToPartitionInfo();	}	part = HiveFileFormatUtils .getFromPathRecursively(pathToPartitionInfo, filePath, IOPrepareCache.get().getPartitionDescMap());	} catch (AssertionError ae) {	
cannot get partition description from because 

Path filePath = this.ioCxtRef.getInputPath();	PartitionDesc part = null;	try {	if (pathToPartitionInfo == null) {	pathToPartitionInfo = Utilities .getMapWork(jobConf).getPathToPartitionInfo();	}	part = HiveFileFormatUtils .getFromPathRecursively(pathToPartitionInfo, filePath, IOPrepareCache.get().getPartitionDescMap());	} catch (AssertionError ae) {	part = null;	} catch (Exception e) {	
cannot get partition description from because 

if (this.getIOContext().isBinarySearching()) {	beginLinearSearch();	}	} else if (genericUDFClassName.equals(GenericUDFOPEqualOrLessThan.class.getName())) {	stopComparisons.add(Comparison.GREATER);	if (this.getIOContext().isBinarySearching()) {	beginLinearSearch();	}	} else if (genericUDFClassName.equals(GenericUDFOPGreaterThan.class.getName()) || genericUDFClassName.equals(GenericUDFOPEqualOrGreaterThan.class.getName())) {	} else {	
is not the name of a supported class continuing linearly 

========================= hive sample_3623 =========================

public void run() {	try {	RawStore ms = HiveMetaStore.HMSHandler.getMSForConf(conf);	long deleteCnt = ms.cleanupEvents();	if (deleteCnt > 0L){	
number of events deleted from event table 

public void run() {	try {	RawStore ms = HiveMetaStore.HMSHandler.getMSForConf(conf);	long deleteCnt = ms.cleanupEvents();	if (deleteCnt > 0L){	}	} catch (Exception e) {	
exception while trying to delete events 

========================= hive sample_2032 =========================

public void handle(Context withinContext) throws Exception {	CreateTableMessage ctm = deserializer.getCreateTableMessage(event.getMessage());	
processing create table message 

public void handle(Context withinContext) throws Exception {	CreateTableMessage ctm = deserializer.getCreateTableMessage(event.getMessage());	org.apache.hadoop.hive.metastore.api.Table tobj = ctm.getTableObj();	if (tobj == null) {	
event was a create table event with no table listed 

========================= hive sample_3459 =========================

TableScanOperator operator = (TableScanOperator) nd;	List<Node> opChildren = operator.getChildren();	TableScanDesc operatorDesc = operator.getConf();	if (operatorDesc == null || !tsToIndices.containsKey(operator)) {	return null;	}	List<Index> indexes = tsToIndices.get(operator);	ExprNodeDesc predicate = operatorDesc.getFilterExpr();	IndexWhereProcCtx context = (IndexWhereProcCtx) procCtx;	ParseContext pctx = context.getParseContext();	
processing predicate for index optimization 

List<Node> opChildren = operator.getChildren();	TableScanDesc operatorDesc = operator.getConf();	if (operatorDesc == null || !tsToIndices.containsKey(operator)) {	return null;	}	List<Index> indexes = tsToIndices.get(operator);	ExprNodeDesc predicate = operatorDesc.getFilterExpr();	IndexWhereProcCtx context = (IndexWhereProcCtx) procCtx;	ParseContext pctx = context.getParseContext();	if (predicate == null) {	
null predicate pushed down 

return null;	}	LOG.info(predicate.getExprString());	Set<Partition> queryPartitions;	try {	queryPartitions = IndexUtils.checkPartitionsCoveredByIndex(operator, pctx, indexes);	if (queryPartitions == null) {	return null;	}	} catch (HiveException e) {	
fatal error problem accessing metastore 

private void rewriteForIndexes(ExprNodeDesc predicate, List<Index> indexes, ParseContext pctx, Task<MapredWork> task, HiveIndexQueryContext queryContext) throws SemanticException {	HiveIndexHandler indexHandler;	Index index = indexes.get(0);	try {	indexHandler = HiveUtils.getIndexHandler(pctx.getConf(), index.getIndexHandlerClass());	} catch (HiveException e) {	
exception while loading indexhandler 

========================= hive sample_3009 =========================

jg.useDefaultPrettyPrinter();	jg.writeStartObject();	jg.writeObjectField("pid", LlapDaemonInfo.INSTANCE.getPID());	jg.writeObjectField("os.name", System.getProperty("os.name"));	if (Shell.WINDOWS) {	jg.writeObjectField("net.core.somaxconn", NetUtil.SOMAXCONN);	} else {	String sysctlCmd = "sysctl -a";	try {	if (sysctlOutRef.get() == null || refresh) {	
reading kernel configs via sysctl 

if (Shell.WINDOWS) {	jg.writeObjectField("net.core.somaxconn", NetUtil.SOMAXCONN);	} else {	String sysctlCmd = "sysctl -a";	try {	if (sysctlOutRef.get() == null || refresh) {	String sysctlOutput = Shell.execCommand(sysctlCmd.split("\\s+"));	sysctlOutRef.set(sysctlOutput);	}	} catch (IOException e) {	
unable to execute command 

String value = sepIdx == -1 ? null : line.substring(sepIdx + 1).trim().replaceAll("\t", "  ");	if (!key.isEmpty()) {	jg.writeObjectField(key, value);	}	}	}	if (!Shell.MAC) {	String thpFileName = "/sys/kernel/mm/transparent_hugepage/enabled";	String thpFileStr = PrivilegedFileReader.read(thpFileName);	if (thpFileStr == null) {	
unable to read contents of 

String thpFileStr = PrivilegedFileReader.read(thpFileName);	if (thpFileStr == null) {	thpFileName = "/sys/kernel/mm/redhat_transparent_hugepage/enabled";	thpFileStr = PrivilegedFileReader.read(thpFileName);	}	if (thpFileStr != null) {	int strIdx = thpFileStr.indexOf('[');	int endIdx = thpFileStr.indexOf(']');	jg.writeObjectField(thpFileName, thpFileStr.substring(strIdx + 1, endIdx));	} else {	
unable to read contents of 

}	if (thpFileStr != null) {	int strIdx = thpFileStr.indexOf('[');	int endIdx = thpFileStr.indexOf(']');	jg.writeObjectField(thpFileName, thpFileStr.substring(strIdx + 1, endIdx));	} else {	}	String thpDefragFileName = "/sys/kernel/mm/transparent_hugepage/defrag";	String thpDefragFileStr = PrivilegedFileReader.read(thpDefragFileName);	if (thpDefragFileStr == null) {	
unable to read contents of 

String thpDefragFileStr = PrivilegedFileReader.read(thpDefragFileName);	if (thpDefragFileStr == null) {	thpDefragFileName = "/sys/kernel/mm/redhat_transparent_hugepage/defrag";	thpDefragFileStr = PrivilegedFileReader.read(thpDefragFileName);	}	if (thpDefragFileStr != null) {	int strIdx = thpDefragFileStr.indexOf('[');	int endIdx = thpDefragFileStr.indexOf(']');	jg.writeObjectField(thpDefragFileName, thpDefragFileStr.substring(strIdx + 1, endIdx));	} else {	
unable to read contents of 

int strIdx = thpDefragFileStr.indexOf('[');	int endIdx = thpDefragFileStr.indexOf(']');	jg.writeObjectField(thpDefragFileName, thpDefragFileStr.substring(strIdx + 1, endIdx));	} else {	}	}	}	jg.writeEndObject();	response.setStatus(HttpServletResponse.SC_OK);	} catch (Exception e) {	
caught exception while processing llap system web service request 

public static String read(String filename) {	String result = AccessController.doPrivileged(new PrivilegedAction<String>() {	public String run() {	String fileString = null;	try {	fileString = new String(Files.readAllBytes(Paths.get(filename)), StandardCharsets.UTF_8);	} catch (Exception e) {	
unable to read file 

========================= hive sample_2248 =========================

writeTextFile(dir2_file2, "dir2_file2_line1\n" + "dir2_file2_line2\n");	symbolLinkedFileSize += fs.getFileStatus(dir2_file2).getLen();	writeSymlinkFile( new Path(symlinkDir, "symlink_file"), new Path(dataDir1, "file1"), new Path(dataDir2, "file2"));	SymlinkTextInputFormat inputFormat = new SymlinkTextInputFormat();	ContentSummary cs = inputFormat.getContentSummary(symlinkDir, job);	assertEquals(symbolLinkedFileSize, cs.getLength());	assertEquals(2, cs.getFileCount());	assertEquals(0, cs.getDirectoryCount());	FileInputFormat.setInputPaths(job, symlinkDir);	InputSplit[] splits = inputFormat.getSplits(job, 2);	
number of splits 

public void testAccuracy2() throws IOException {	fileSystem.mkdirs(symlinkDir);	FileInputFormat.setInputPaths(job, symlinkDir);	SymlinkTextInputFormat inputFormat = new SymlinkTextInputFormat();	ContentSummary cs = inputFormat.getContentSummary(symlinkDir, job);	assertEquals(0, cs.getLength());	assertEquals(0, cs.getFileCount());	assertEquals(0, cs.getDirectoryCount());	InputSplit[] splits = inputFormat.getSplits(job, 2);	
number of splits 

========================= hive sample_2474 =========================

public void testShowRoleGrant() throws SQLException {	Statement stmt = con.createStatement();	try {	stmt.execute("drop role role1");	} catch (Exception ex) {	
ignoring error during drop role 

public void run() {	while (statement.hasMoreLogs()) {	try {	incrementalLogs.addAll(statement.getQueryLog());	Thread.sleep(500);	} catch (SQLException e) {	
failed getquerylog error message 

public void run() {	while (statement.hasMoreLogs()) {	try {	incrementalLogs.addAll(statement.getQueryLog());	Thread.sleep(500);	} catch (SQLException e) {	fail("error in getting log thread");	} catch (InterruptedException e) {	
getting log thread is interrupted error message 

========================= hive sample_295 =========================

private Schema getSchema(JobConf job, FileSplit split) throws AvroSerdeException, IOException {	if(AvroSerdeUtils.insideMRJob(job)) {	MapWork mapWork = Utilities.getMapWork(job);	for (Map.Entry<Path,PartitionDesc> pathsAndParts: mapWork.getPathToPartitionInfo().entrySet()){	Path partitionPath = pathsAndParts.getKey();	if(pathIsInPartition(split.getPath(), partitionPath)) {	if(LOG.isInfoEnabled()) {	
matching partition with input split 

Properties props = pathsAndParts.getValue().getProperties();	if(props.containsKey(AvroTableProperties.SCHEMA_LITERAL.getPropName()) || props.containsKey(AvroTableProperties.SCHEMA_URL.getPropName())) {	return AvroSerdeUtils.determineSchemaOrThrowException(job, props);	}	else {	return null;	}	}	}	if(LOG.isInfoEnabled()) {	
unable to match filesplit with a partition 

else {	return null;	}	}	}	if(LOG.isInfoEnabled()) {	}	}	String s = job.get(AvroTableProperties.AVRO_SERDE_SCHEMA.getPropName());	if(s != null) {	
found the avro schema in the job 

========================= hive sample_3635 =========================

iRefSet.addAll(ImmutableBitSet.range(fieldCount).asList());	}	}	if (tableAccessRel instanceof HiveTableScan) {	iRefSet.removeAll(((HiveTableScan)tableAccessRel).getVirtualCols());	}	if (!iRefSet.isEmpty()) {	final RelOptTable table = tableAccessRel.getTable();	if (table instanceof RelOptHiveTable) {	((RelOptHiveTable) table).getColStat(iRefSet, true);	
got col stats for in 

========================= hive sample_2862 =========================

public LowLevelLrfuCachePolicy(int minBufferSize, long maxSize, Configuration conf) {	lambda = HiveConf.getFloatVar(conf, HiveConf.ConfVars.LLAP_LRFU_LAMBDA);	int maxBuffers = (int)Math.ceil((maxSize * 1.0) / minBufferSize);	int maxHeapSize = -1;	if (lambda == 0) {	maxHeapSize = maxBuffers;	} else {	int lrfuThreshold = (int)((Math.log(1 - Math.pow(0.5, lambda)) / Math.log(0.5)) / lambda);	maxHeapSize = Math.min(lrfuThreshold, maxBuffers);	}	
lrfu cache policy with min buffer size and lambda heap size 

public void notifyUnlock(LlapCacheableBuffer buffer) {	long time = timer.incrementAndGet();	if (LlapIoImpl.CACHE_LOGGER.isTraceEnabled()) {	
touching at 

private LlapCacheableBuffer evictHeapElementUnderLock(long time, int ix) {	LlapCacheableBuffer result = heap[ix];	if (LlapIoImpl.CACHE_LOGGER.isTraceEnabled()) {	
evicting at 

private void debugDumpListOnError(LlapCacheableBuffer... buffers) {	StringBuilder listDump = new StringBuilder("Invalid list removal. List: ");	try {	dumpList(listDump, listHead, listTail);	int i = 0;	for (LlapCacheableBuffer buffer : buffers) {	listDump.append("; list from the buffer #").append(i).append(" being removed: ");	dumpList(listDump, buffer, null);	}	} catch (Throwable t) {	
error dumping the lists on error 

========================= hive sample_2161 =========================

public void closeBatch() throws StreamingIOFailure {	boolean haveError = false;	for (RecordUpdater updater : updaters) {	if (updater != null) {	try {	updater.close(false);	} catch (Exception ex) {	haveError = true;	
unable to close due to 

========================= hive sample_942 =========================

try {	TableScanOperator tsOp = (TableScanOperator) stack.get(0);	if (tsOp.getNumParent() > 0) {	return null;	}	if (tsOp.getConf().getRowLimit() != -1) {	return null;	}	Table tbl = tsOp.getConf().getTableMetadata();	if (MetaStoreUtils.isExternalTable(tbl.getTTable())) {	
table is external skip statsoptimizer 

return null;	}	if (tsOp.getConf().getRowLimit() != -1) {	return null;	}	Table tbl = tsOp.getConf().getTableMetadata();	if (MetaStoreUtils.isExternalTable(tbl.getTTable())) {	return null;	}	if (AcidUtils.isTransactionalTable(tbl)) {	
table is acid table skip statsoptimizer 

if (rowCnt == null) {	return null;	}	}	} else {	ExprNodeColumnDesc desc = (ExprNodeColumnDesc) exprMap.get(((ExprNodeColumnDesc) aggr .getParameters().get(0)).getColumn());	String colName = desc.getColumn();	StatType type = getType(desc.getTypeString());	if (!tbl.isPartitioned()) {	if (!StatsSetupConst.areBasicStatsUptoDate(tbl.getParameters())) {	
stats for table are not up to date 

} else {	ExprNodeColumnDesc desc = (ExprNodeColumnDesc) exprMap.get(((ExprNodeColumnDesc) aggr .getParameters().get(0)).getColumn());	String colName = desc.getColumn();	StatType type = getType(desc.getTypeString());	if (!tbl.isPartitioned()) {	if (!StatsSetupConst.areBasicStatsUptoDate(tbl.getParameters())) {	return null;	}	rowCnt = Long.parseLong(tbl.getProperty(StatsSetupConst.ROW_COUNT));	if (rowCnt == null) {	
table doesn t have up to date stats 

StatType type = getType(desc.getTypeString());	if (!tbl.isPartitioned()) {	if (!StatsSetupConst.areBasicStatsUptoDate(tbl.getParameters())) {	return null;	}	rowCnt = Long.parseLong(tbl.getProperty(StatsSetupConst.ROW_COUNT));	if (rowCnt == null) {	return null;	}	if (!StatsSetupConst.areColumnStatsUptoDate(tbl.getParameters(), colName)) {	
stats for table column are not up to date 

}	rowCnt = Long.parseLong(tbl.getProperty(StatsSetupConst.ROW_COUNT));	if (rowCnt == null) {	return null;	}	if (!StatsSetupConst.areColumnStatsUptoDate(tbl.getParameters(), colName)) {	return null;	}	List<ColumnStatisticsObj> stats = hive.getMSC().getTableColumnStatistics( tbl.getDbName(), tbl.getTableName(), Lists.newArrayList(colName));	if (stats.isEmpty()) {	
no stats for column 

}	if (!StatsSetupConst.areColumnStatsUptoDate(tbl.getParameters(), colName)) {	return null;	}	List<ColumnStatisticsObj> stats = hive.getMSC().getTableColumnStatistics( tbl.getDbName(), tbl.getTableName(), Lists.newArrayList(colName));	if (stats.isEmpty()) {	return null;	}	Long nullCnt = getNullcountFor(type, stats.get(0).getStatsData());	if (null == nullCnt) {	
unsupported type encountered in metadata optimizer for column 

Long nullCnt = getNullcountFor(type, stats.get(0).getStatsData());	if (null == nullCnt) {	return null;	} else {	rowCnt -= nullCnt;	}	} else {	Set<Partition> parts = pctx.getPrunedPartitions(tsOp.getConf().getAlias(), tsOp) .getPartitions();	for (Partition part : parts) {	if (!StatsSetupConst.areBasicStatsUptoDate(part.getParameters())) {	
stats for part are not up to date 

rowCnt -= nullCnt;	}	} else {	Set<Partition> parts = pctx.getPrunedPartitions(tsOp.getConf().getAlias(), tsOp) .getPartitions();	for (Partition part : parts) {	if (!StatsSetupConst.areBasicStatsUptoDate(part.getParameters())) {	return null;	}	Long partRowCnt = Long.parseLong(part.getParameters().get( StatsSetupConst.ROW_COUNT));	if (partRowCnt == null) {	
partition doesn t have up to date stats 

}	Collection<List<ColumnStatisticsObj>> result = verifyAndGetPartColumnStats(hive, tbl, colName, parts);	if (result == null) {	return null;	}	for (List<ColumnStatisticsObj> statObj : result) {	ColumnStatisticsData statData = validateSingleColStat(statObj);	if (statData == null) return null;	Long nullCnt = getNullcountFor(type, statData);	if (nullCnt == null) {	
unsupported type encountered in metadata optimizer for column 

}	}	}	oneRow.add(rowCnt);	} else if (udaf instanceof GenericUDAFMax) {	ExprNodeColumnDesc colDesc = (ExprNodeColumnDesc)exprMap.get(((ExprNodeColumnDesc)aggr.getParameters().get(0)).getColumn());	String colName = colDesc.getColumn();	StatType type = getType(colDesc.getTypeString());	if(!tbl.isPartitioned()) {	if (!StatsSetupConst.areColumnStatsUptoDate(tbl.getParameters(), colName)) {	
stats for table column are not up to date 

} else if (udaf instanceof GenericUDAFMax) {	ExprNodeColumnDesc colDesc = (ExprNodeColumnDesc)exprMap.get(((ExprNodeColumnDesc)aggr.getParameters().get(0)).getColumn());	String colName = colDesc.getColumn();	StatType type = getType(colDesc.getTypeString());	if(!tbl.isPartitioned()) {	if (!StatsSetupConst.areColumnStatsUptoDate(tbl.getParameters(), colName)) {	return null;	}	List<ColumnStatisticsObj> stats = hive.getMSC().getTableColumnStatistics( tbl.getDbName(),tbl.getTableName(), Lists.newArrayList(colName));	if (stats.isEmpty()) {	
no stats for column 

case Double: {	DoubleSubType subType = DoubleSubType.valueOf(name);	DoubleColumnStatsData dstats = statData.getDoubleStats();	if (dstats.isSetHighValue()) {	oneRow.add(subType.cast(dstats.getHighValue()));	} else {	oneRow.add(null);	}	break;	}	
unsupported type encountered in metadata optimizer for column 

double curVal = statData.getDoubleStats().getHighValue();	maxVal = maxVal == null ? curVal : Math.max(maxVal, curVal);	}	if (maxVal != null) {	oneRow.add(subType.cast(maxVal));	} else {	oneRow.add(null);	}	break;	}	
unsupported type encountered in metadata optimizer for column 

}	return null;	}	}	}  else if (udaf instanceof GenericUDAFMin) {	ExprNodeColumnDesc colDesc = (ExprNodeColumnDesc)exprMap.get(((ExprNodeColumnDesc)aggr.getParameters().get(0)).getColumn());	String colName = colDesc.getColumn();	StatType type = getType(colDesc.getTypeString());	if (!tbl.isPartitioned()) {	if (!StatsSetupConst.areColumnStatsUptoDate(tbl.getParameters(), colName)) {	
stats for table column are not up to date 

case Double: {	DoubleSubType subType = DoubleSubType.valueOf(name);	DoubleColumnStatsData dstats = statData.getDoubleStats();	if (dstats.isSetLowValue()) {	oneRow.add(subType.cast(dstats.getLowValue()));	} else {	oneRow.add(null);	}	break;	}	
unsupported type encountered in metadata optimizer for column 

double curVal = statData.getDoubleStats().getLowValue();	minVal = minVal == null ? curVal : Math.min(minVal, curVal);	}	if (minVal != null) {	oneRow.add(subType.cast(minVal));	} else {	oneRow.add(minVal);	}	break;	}	
unsupported type encountered in metadata optimizer for column 

oneRow.add(subType.cast(minVal));	} else {	oneRow.add(minVal);	}	break;	}	return null;	}	}	} else {	
unsupported aggregation for metadata optimizer 

} else {	StandardStructObjectInspector sOI = ObjectInspectorFactory. getStandardStructObjectInspector(colNames, ois);	fWork = new FetchWork(allRows, sOI);	fTask = (FetchTask)TaskFactory.get(fWork, pctx.getConf());	pctx.setFetchTask(fTask);	}	fWork.setLimit(fWork.getRowsComputedUsingStats().size());	isOptimized = true;	return null;	} catch (Exception e) {	
failed to optimize using metadata optimizer 

private ColumnStatisticsData validateSingleColStat(List<ColumnStatisticsObj> statObj) {	if (statObj.size() > 1) {	
more than one stat for a single column 

private ColumnStatisticsData validateSingleColStat(List<ColumnStatisticsObj> statObj) {	if (statObj.size() > 1) {	return null;	} else if (statObj.isEmpty()) {	
no stats for some partition and column 

private Collection<List<ColumnStatisticsObj>> verifyAndGetPartColumnStats( Hive hive, Table tbl, String colName, Set<Partition> parts) throws TException {	List<String> partNames = new ArrayList<String>(parts.size());	for (Partition part : parts) {	if (!StatsSetupConst.areColumnStatsUptoDate(part.getParameters(), colName)) {	
stats for part column are not up to date 

private Collection<List<ColumnStatisticsObj>> verifyAndGetPartColumnStats( Hive hive, Table tbl, String colName, Set<Partition> parts) throws TException {	List<String> partNames = new ArrayList<String>(parts.size());	for (Partition part : parts) {	if (!StatsSetupConst.areColumnStatsUptoDate(part.getParameters(), colName)) {	return null;	}	partNames.add(part.getName());	}	Map<String, List<ColumnStatisticsObj>> result = hive.getMSC().getPartitionColumnStatistics( tbl.getDbName(), tbl.getTableName(), partNames, Lists.newArrayList(colName));	if (result.size() != parts.size()) {	
received stats for partitions 

private Long getRowCnt( ParseContext pCtx, TableScanOperator tsOp, Table tbl) throws HiveException {	Long rowCnt = 0L;	if (tbl.isPartitioned()) {	for (Partition part : pctx.getPrunedPartitions( tsOp.getConf().getAlias(), tsOp).getPartitions()) {	if (!StatsSetupConst.areBasicStatsUptoDate(part.getParameters())) {	return null;	}	Long partRowCnt = Long.parseLong(part.getParameters().get(StatsSetupConst.ROW_COUNT));	if (partRowCnt == null) {	
partition doesn t have up to date stats 

return null;	}	rowCnt += partRowCnt;	}	} else {	if (!StatsSetupConst.areBasicStatsUptoDate(tbl.getParameters())) {	return null;	}	rowCnt = Long.parseLong(tbl.getProperty(StatsSetupConst.ROW_COUNT));	if (rowCnt == null) {	
table doesn t have up to date stats 

========================= hive sample_2970 =========================

if (globalLimitCtx != null && globalLimitCtx.isEnable()) {	long sizePerRow = HiveConf.getLongVar(conf, HiveConf.ConfVars.HIVELIMITMAXROWSIZE);	estimatedInput = (globalLimitCtx.getGlobalOffset() + globalLimitCtx.getGlobalLimit()) * sizePerRow;	long minSplitSize = HiveConf.getLongVar(conf, HiveConf.ConfVars.MAPREDMINSPLITSIZE);	long estimatedNumMap = inputSummary.getLength() / minSplitSize + 1;	estimatedInput = estimatedInput * (estimatedNumMap + 1);	} else {	estimatedInput = inputSummary.getLength();	}	if (LOG.isDebugEnabled()) {	
task summary estimated input 

========================= hive sample_3398 =========================

public static void setUpTestDataDir() throws Exception {	
using warehouse directory 

========================= hive sample_692 =========================

needHashTableSetup = false;	}	batchCounter++;	innerPerBatchSetup(batch);	for(VectorExpression ve : bigTableFilterExpressions) {	ve.evaluate(batch);	}	final int inputLogicalSize = batch.size;	if (inputLogicalSize == 0) {	if (LOG.isDebugEnabled()) {	
batch empty 

JoinUtil.JoinResult joinResult;	if (!joinColVector.noNulls && joinColVector.isNull[0]) {	joinResult = JoinUtil.JoinResult.NOMATCH;	} else {	byte[] keyBytes = vector[0];	int keyStart = start[0];	int keyLength = length[0];	joinResult = hashMap.lookup(keyBytes, keyStart, keyLength, hashMapResults[0]);	}	if (LOG.isDebugEnabled()) {	
batch repeated joinresult 

byte[] keyBytes = vector[0];	int keyStart = start[0];	int keyLength = length[0];	joinResult = hashMap.lookup(keyBytes, keyStart, keyLength, hashMapResults[0]);	}	if (LOG.isDebugEnabled()) {	}	finishInnerRepeated(batch, joinResult, hashMapResults[0]);	} else {	if (LOG.isDebugEnabled()) {	
batch non repeated 

switch (saveJoinResult) {	case MATCH: hashMapResultCount++;	equalKeySeriesCount++;	break;	case SPILL: hashMapResultCount++;	break;	case NOMATCH: break;	}	}	if (LOG.isDebugEnabled()) {	
allmatchs equalkeyserieshashmapresultindices equalkeyseriesallmatchindices equalkeyseriesissinglevalue equalkeyseriesduplicatecounts spills spillhashmapresultindices hashmapresults 

========================= hive sample_4148 =========================

public List<ErrorAndSolution> getErrors() {	for (String urlString : taskLogUrls) {	URL taskAttemptLogUrl;	try {	taskAttemptLogUrl = new URL(urlString);	} catch (MalformedURLException e) {	
bad task log url 

try {	in = new BufferedReader( new InputStreamReader(taskAttemptLogUrl.openStream()));	String inputLine;	while ((inputLine = in.readLine()) != null) {	for(ErrorHeuristic e : heuristics.keySet()) {	e.processLogLine(inputLine);	}	}	in.close();	} catch (IOException e) {	
error while reading from task log url 

========================= hive sample_4095 =========================

if (isVectorized && !isSupported && HiveConf.getBoolVar(conf, ConfVars.LLAP_IO_ENCODE_ENABLED)) {	isSupported = isSerdeBased = checkInputFormatForLlapEncode(conf, ifName);	}	if ((!isSupported || !isVectorized) && !isCacheOnly) {	if (LOG.isInfoEnabled()) {	LOG.info("Not using llap for " + ifName + ": supported = " + isSupported + ", vectorized = " + isVectorized + ", cache only = " + isCacheOnly);	}	return inputFormat;	}	if (LOG.isDebugEnabled()) {	
processing 

if (LOG.isInfoEnabled()) {	LOG.info("Not using llap for " + ifName + ": supported = " + isSupported + ", vectorized = " + isVectorized + ", cache only = " + isCacheOnly);	}	return inputFormat;	}	if (LOG.isDebugEnabled()) {	}	LlapIo<VectorizedRowBatch> llapIo = LlapProxy.getIo();	if (llapIo == null) {	if (LOG.isInfoEnabled()) {	
not using llap io because it is not initialized 

LlapIo<VectorizedRowBatch> llapIo = LlapProxy.getIo();	if (llapIo == null) {	if (LOG.isInfoEnabled()) {	}	return inputFormat;	}	Deserializer serde = null;	if (isSerdeBased) {	if (part == null) {	if (isCacheOnly) {	
using cache only because there s no partition spec for serde based if 

if (LOG.isInfoEnabled()) {	}	return inputFormat;	}	Deserializer serde = null;	if (isSerdeBased) {	if (part == null) {	if (isCacheOnly) {	injectLlapCaches(inputFormat, llapIo);	} else {	
not using llap io because there s no partition spec for serde based if 

private static boolean checkInputFormatForLlapEncode(Configuration conf, String ifName) {	String formatList = HiveConf.getVar(conf, ConfVars.LLAP_IO_ENCODE_FORMATS);	if (LOG.isDebugEnabled()) {	
checking against 

private static boolean checkInputFormatForLlapEncode(Configuration conf, String ifName) {	String formatList = HiveConf.getVar(conf, ConfVars.LLAP_IO_ENCODE_FORMATS);	if (LOG.isDebugEnabled()) {	}	String[] formats = StringUtils.getStrings(formatList);	if (formats != null) {	for (String format : formats) {	if (ifName.equals(format)) {	if (LOG.isInfoEnabled()) {	
using serde based llap reader for 

private static Deserializer findSerDeForLlapSerDeIf( Configuration conf, PartitionDesc part) throws HiveException {	VectorPartitionDesc vpart =  part.getVectorPartitionDesc();	if (vpart != null) {	VectorMapOperatorReadType old = vpart.getVectorMapOperatorReadType();	if (old != VectorMapOperatorReadType.VECTORIZED_INPUT_FILE_FORMAT) {	if (LOG.isInfoEnabled()) {	
resetting vectormapoperatorreadtype from for partition 

public static void injectLlapCaches(InputFormat<WritableComparable, Writable> inputFormat, LlapIo<VectorizedRowBatch> llapIo) {	
injecting llap caches into 

inputFormatClass = job.getClassByName(inputFormatClassName);	} catch (Exception e) {	throw new IOException("cannot find class " + inputFormatClassName, e);	}	if (this.mrwork == null || pathToPartitionInfo == null) {	init(job);	}	boolean nonNative = false;	PartitionDesc part = HiveFileFormatUtils.getFromPathRecursively( pathToPartitionInfo, hsplit.getPath(), null);	if (LOG.isDebugEnabled()) {	
found spec for from 

public static Path[] processPathsForMmRead(List<Path> dirs, JobConf conf, ValidTxnList validTxnList) throws IOException {	if (validTxnList == null) {	return dirs.toArray(new Path[dirs.size()]);	} else {	List<Path> finalPaths = new ArrayList<>(dirs.size());	for (Path dir : dirs) {	processForWriteIds(dir, conf, validTxnList, finalPaths);	}	if (finalPaths.isEmpty()) {	
no valid inputs found in 

private static void processForWriteIds(Path dir, JobConf conf, ValidTxnList validTxnList, List<Path> finalPaths) throws IOException {	FileSystem fs = dir.getFileSystem(conf);	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
checking root for inputs 

}	LinkedList<Path> subdirs = new LinkedList<>();	subdirs.add(dir);	while (!subdirs.isEmpty()) {	Path currDir = subdirs.poll();	FileStatus[] files = fs.listStatus(currDir);	boolean hadAcidState = false;	for (FileStatus file : files) {	Path path = file.getPath();	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
checking for inputs 

subdirs.add(dir);	while (!subdirs.isEmpty()) {	Path currDir = subdirs.poll();	FileStatus[] files = fs.listStatus(currDir);	boolean hadAcidState = false;	for (FileStatus file : files) {	Path path = file.getPath();	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	}	if (!file.isDirectory()) {	
ignoring a file not in mm directory 

} else if (JavaUtils.extractTxnId(path) == null) {	subdirs.add(path);	} else if (!hadAcidState) {	AcidUtils.Directory dirInfo = AcidUtils.getAcidState(currDir, conf, validTxnList, Ref.from(false), true, null);	hadAcidState = true;	Path base = dirInfo.getBaseDirectory();	if (base != null) {	finalPaths.add(base);	}	for (AcidUtils.ParsedDelta delta : dirInfo.getCurrentDirectories()) {	
adding input 

if ((op != null) && (op instanceof TableScanOperator)) {	tableScan = (TableScanOperator) op;	readColumnsBuffer.setLength(0);	readColumnNamesBuffer.setLength(0);	ColumnProjectionUtils.appendReadColumns(readColumnsBuffer, readColumnNamesBuffer, tableScan.getNeededColumnIDs(), tableScan.getNeededColumns());	pushDownProjection = true;	pushFilters(newjob, tableScan);	}	} else {	if (LOG.isDebugEnabled()) {	
aliases pathtoaliases dir 

} else {	if (LOG.isDebugEnabled()) {	}	}	if (!currentDirs.isEmpty() && inputFormatClass.equals(currentInputFormatClass) && table.equals(currentTable) && tableScan == currentTableScan) {	currentDirs.add(dir);	continue;	}	if (!currentDirs.isEmpty()) {	if (LOG.isInfoEnabled()) {	
generating splits as currentdirs is not empty currentdirs 

currentDirs.add(dir);	currentTableScan = tableScan;	currentTable = table;	currentInputFormatClass = inputFormatClass;	}	if (pushDownProjection) {	pushProjection(newjob, readColumnsBuffer, readColumnNamesBuffer);	}	if (dirs.length != 0) {	if (LOG.isInfoEnabled()) {	
generating splits for dirs 

if (pushDownProjection) {	pushProjection(newjob, readColumnsBuffer, readColumnNamesBuffer);	}	if (dirs.length != 0) {	if (LOG.isInfoEnabled()) {	}	addSplitsForGroup(currentDirs, currentTableScan, newjob, getInputFormatFromCache(currentInputFormatClass, job), currentInputFormatClass, currentDirs.size()*(numSplits / dirs.length), currentTable, result);	}	Utilities.clearWorkMapForConf(job);	if (LOG.isInfoEnabled()) {	
number of splits 

========================= hive sample_3750 =========================

public static SqlOperator getCalciteOperator(String funcTextName, GenericUDF hiveUDF, ImmutableList<RelDataType> calciteArgTypes, RelDataType retType) throws SemanticException {	if (hiveUDF instanceof GenericUDFOPNegative) {	return SqlStdOperatorTable.UNARY_MINUS;	} else if (hiveUDF instanceof GenericUDFOPPositive) {	return SqlStdOperatorTable.UNARY_PLUS;	}	String name = null;	if (StringUtils.isEmpty(funcTextName)) {	name = getName(hiveUDF);	
the function text was empty name from annotation is 

if ("+".equals(name)) {	name = FunctionRegistry.UNARY_PLUS_FUNC_NAME;	} else if ("-".equals(name)) {	name = FunctionRegistry.UNARY_MINUS_FUNC_NAME;	}	}	FunctionInfo hFn;	try {	hFn = name != null ? FunctionRegistry.getFunctionInfo(name) : null;	} catch (SemanticException e) {	
failed to load udf 

FunctionInfo hFn;	try {	hFn = name != null ? FunctionRegistry.getFunctionInfo(name) : null;	} catch (SemanticException e) {	hFn = null;	}	if (hFn == null) {	try {	hFn = handleExplicitCast(op, dt);	} catch (SemanticException e) {	
failed to load udf 

private void registerFunction(String name, SqlOperator calciteFn, HiveToken hiveToken) {	reverseOperatorMap.put(calciteFn, name);	FunctionInfo hFn;	try {	hFn = FunctionRegistry.getFunctionInfo(name);	} catch (SemanticException e) {	
failed to load udf 

========================= hive sample_2842 =========================

Preconditions.checkArgument(numExecutorsPerService > 0);	Preconditions.checkArgument(execMemoryPerService > 0);	Preconditions.checkArgument(numLocalDirs > 0);	this.numInstances = numInstances;	this.clusterNameTrimmed = clusterName.replace("$", "") + "_" + MiniLlapCluster.class.getSimpleName();	this.llapDaemons = new LlapDaemon[numInstances];	File targetWorkDir = new File("target", clusterNameTrimmed);	try {	FileContext.getLocalFSFileContext().delete( new Path(targetWorkDir.getAbsolutePath()), true);	} catch (Exception e) {	
could not cleanup test workdir 

this.numExecutorsPerService = numExecutorsPerService;	this.execBytesPerService = execMemoryPerService;	this.ioIsDirect = ioIsDirect;	this.llapIoEnabled = llapIoEnabled;	this.ioBytesPerService = ioBytesPerService;	LlapDaemonInfo.initialize("mini-llap-cluster", numExecutorsPerService, execMemoryPerService, ioBytesPerService, ioIsDirect, llapIoEnabled, "-1");	localDirs = new String[numLocalDirs];	for (int i = 0 ; i < numLocalDirs ; i++) {	File f = new File(testWorkDir, "localDir");	f.mkdirs();	
created localdir 

public void serviceInit(Configuration conf) throws IOException, InterruptedException {	int rpcPort = 0;	int mngPort = 0;	int shufflePort = 0;	int webPort = 0;	int outputFormatServicePort = 0;	boolean usePortsFromConf = conf.getBoolean("minillap.usePortsFromConf", false);	
minillap configured to use ports from conf 

public void serviceStop() throws IOException {	for (int i = 0 ; i < numInstances ; i++) {	if (llapDaemons[i] != null) {	llapDaemons[i].stop();	llapDaemons[i] = null;	}	}	if (ownZkCluster) {	if (miniZooKeeperCluster != null) {	
stopping minizookeeper cluster 

for (int i = 0 ; i < numInstances ; i++) {	if (llapDaemons[i] != null) {	llapDaemons[i].stop();	llapDaemons[i] = null;	}	}	if (ownZkCluster) {	if (miniZooKeeperCluster != null) {	miniZooKeeperCluster.shutdown();	miniZooKeeperCluster = null;	
stopped minizookeeper cluster 

llapDaemons[i].stop();	llapDaemons[i] = null;	}	}	if (ownZkCluster) {	if (miniZooKeeperCluster != null) {	miniZooKeeperCluster.shutdown();	miniZooKeeperCluster = null;	}	} else {	
not stopping minizk cluster since it is now owned by us 

========================= hive sample_2141 =========================

try {	long startTime = System.currentTimeMillis();	isAliveCounter++;	txnHandler.countOpenTxns();	if (System.currentTimeMillis() - lastLogTime > 60 * 1000) {	LOG.info("AcidOpenTxnsCounterService ran for " + ((System.currentTimeMillis() - startTime) / 1000) + " seconds.  isAliveCounter = " + isAliveCounter);	lastLogTime = System.currentTimeMillis();	}	}	catch(Throwable t) {	
serious error in 

========================= hive sample_1852 =========================

sortOrder = tbl.getProperty(serdeConstants.SERIALIZATION_SORT_ORDER);	if (sortOrder == null) {	sortOrder = "";	}	for (int i = 0; i < sortOrder.length(); i++) {	char c = sortOrder.charAt(i);	if (c != '+' && c != '-') {	throw new TException(serdeConstants.SERIALIZATION_SORT_ORDER + " should be a string consists of only '+' and '-'!");	}	}	
sort order is sortorder 

========================= hive sample_5283 =========================

hashMultiSet = (VectorMapJoinBytesHashMultiSet) vectorMapJoinHashTable;	needHashTableSetup = false;	}	batchCounter++;	for(VectorExpression ve : bigTableFilterExpressions) {	ve.evaluate(batch);	}	final int inputLogicalSize = batch.size;	if (inputLogicalSize == 0) {	if (LOG.isDebugEnabled()) {	
batch empty 

JoinUtil.JoinResult joinResult;	if (!joinColVector.noNulls && joinColVector.isNull[0]) {	joinResult = JoinUtil.JoinResult.NOMATCH;	} else {	byte[] keyBytes = vector[0];	int keyStart = start[0];	int keyLength = length[0];	joinResult = hashMultiSet.contains(keyBytes, keyStart, keyLength, hashMultiSetResults[0]);	}	if (LOG.isDebugEnabled()) {	
batch repeated joinresult 

byte[] keyBytes = vector[0];	int keyStart = start[0];	int keyLength = length[0];	joinResult = hashMultiSet.contains(keyBytes, keyStart, keyLength, hashMultiSetResults[0]);	}	if (LOG.isDebugEnabled()) {	}	finishInnerBigOnlyRepeated(batch, joinResult, hashMultiSetResults[0]);	} else {	if (LOG.isDebugEnabled()) {	
batch non repeated 

if (haveSaveKey) {	switch (saveJoinResult) {	case MATCH: equalKeySeriesCount++;	break;	case SPILL: hashMultiSetResultCount++;	break;	case NOMATCH: break;	}	}	if (LOG.isDebugEnabled()) {	
allmatchs equalkeyseriesvaluecounts equalkeyseriesallmatchindices equalkeyseriesduplicatecounts spills spillhashmapresultindices hashmapresults 

========================= hive sample_4187 =========================

public void abort() {	RecordProcessor rProcLocal;	synchronized (this) {	
received abort 

public void abort() {	RecordProcessor rProcLocal;	synchronized (this) {	aborted.set(true);	rProcLocal = rproc;	}	if (rProcLocal != null) {	
forwarding abort to recordprocessor 

public void abort() {	RecordProcessor rProcLocal;	synchronized (this) {	aborted.set(true);	rProcLocal = rproc;	}	if (rProcLocal != null) {	rProcLocal.abort();	} else {	
recordprocessor not yet setup abort will be ignored 

========================= hive sample_3992 =========================

public static CompatLevel getCompatLevel(String compatStr) {	if (compatStr.equalsIgnoreCase("latest")) {	compatStr = LATEST_COMPAT_LEVEL;	}	for (CompatLevel cl : CompatLevel.values()) {	if (cl.value.equals(compatStr)) {	return cl;	}	}	
could not find compatlevel for using default of 

========================= hive sample_1413 =========================

writable = new DoubleWritable();	}	((DoubleWritable) writable).set(deserializeRead.currentDouble);	}	break;	case BINARY: {	if (writable == null) {	writable = new BytesWritable();	}	if (deserializeRead.currentBytes == null) {	
null binary entry batchindex 

========================= hive sample_4191 =========================

private static boolean containsPartition(Hive hive, Partition part, List<Index> indexes) throws HiveException {	HashMap<String, String> partSpec = part.getSpec();	if (partSpec.isEmpty()) {	return isIndexTableFresh(hive, indexes, part.getTable());	}	for (Index index : indexes) {	String[] qualified = Utilities.getDbTableName(index.getDbName(), index.getIndexTableName());	Table indexTable = hive.getTable(qualified[0], qualified[1]);	Partition matchingPartition = hive.getPartition(indexTable, partSpec, false);	if (matchingPartition == null) {	
index table did not contain built partition that matched 

private static boolean isIndexPartitionFresh(Hive hive, Index index, Partition part) throws HiveException {	
checking index staleness 

private static boolean isIndexPartitionFresh(Hive hive, Index index, Partition part) throws HiveException {	try {	String indexTs = index.getParameters().get(part.getSpec().toString());	if (indexTs == null) {	return false;	}	FileSystem partFs = part.getDataLocation().getFileSystem(hive.getConf());	FileStatus[] parts = partFs.listStatus(part.getDataLocation(), FileUtils.HIDDEN_FILES_PATH_FILTER);	for (FileStatus status : parts) {	if (status.getModificationTime() > Long.parseLong(indexTs)) {	
index is stale on partition modified time for is higher than index creation time 

private static boolean isIndexTableFresh(Hive hive, List<Index> indexes, Table src) throws HiveException {	if (indexes == null || indexes.size() == 0) {	return false;	}	for (Index index : indexes) {	
checking index staleness 

for (Index index : indexes) {	try {	String indexTs = index.getParameters().get("base_timestamp");	if (indexTs == null) {	return false;	}	FileSystem srcFs = src.getPath().getFileSystem(hive.getConf());	FileStatus[] srcs = srcFs.listStatus(src.getPath(), FileUtils.HIDDEN_FILES_PATH_FILTER);	for (FileStatus status : srcs) {	if (status.getModificationTime() > Long.parseLong(indexTs)) {	
index is stale on table modified time for is higher than index creation time 

========================= hive sample_3087 =========================

public void open(HiveConf conf) throws HiveException {	
trying to open spark session 

hiveSparkClient = HiveSparkClientFactory.createHiveSparkClient(conf, sessionId);	} catch (Throwable e) {	HiveException he;	if (isOpen) {	he = getHiveException(e);	} else {	he = new HiveException(e, ErrorMsg.SPARK_CREATE_CLIENT_CLOSED_SESSION, sessionId);	}	throw he;	}	
spark session is successfully opened 

String masterURL = sparkConf.get("spark.master");	if (masterURL.startsWith("spark") || masterURL.startsWith("local")) {	totalCores = sparkConf.contains("spark.default.parallelism") ? sparkConf.getInt("spark.default.parallelism", 1) : hiveSparkClient.getDefaultParallelism();	totalCores = Math.max(totalCores, numExecutors);	} else {	int coresPerExecutor = sparkConf.getInt("spark.executor.cores", 1);	totalCores = numExecutors * coresPerExecutor;	}	totalCores = totalCores / sparkConf.getInt("spark.task.cpus", 1);	long memoryPerTaskInBytes = totalMemory / totalCores;	
spark cluster current has executors total cores memory per executor m memoryfraction 

public void close() {	
trying to close spark session 

public void close() {	isOpen = false;	if (hiveSparkClient != null) {	try {	hiveSparkClient.close();	
spark session is successfully closed 

public void close() {	isOpen = false;	if (hiveSparkClient != null) {	try {	hiveSparkClient.close();	cleanScratchDir();	} catch (IOException e) {	
failed to close spark session 

private boolean matches(String input, String regex) {	if (!errorPatterns.containsKey(regex)) {	
no error pattern found for regex 

========================= hive sample_4607 =========================

public void transferSuccessful() {	if (manageOsCache && getCount() > 0) {	try {	if (canEvictAfterTransfer) {	
shufflebuffersize path 

public void transferSuccessful() {	if (manageOsCache && getCount() > 0) {	try {	if (canEvictAfterTransfer) {	NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(identifier, fd, getPosition(), getCount(), NativeIO.POSIX.POSIX_FADV_DONTNEED);	}	} catch (Throwable t) {	
failed to manage os cache for 

========================= hive sample_2157 =========================

protected void perform(RelOptRuleCall call, ImmutableBitSet topRefs, RelNode topOperator, Join join, RelNode left, Aggregate aggregate) {	
matched hivesemijoinrule 

if(join.getJoinType() == JoinRelType.LEFT) {	call.transformTo(topOperator.copy(topOperator.getTraitSet(), ImmutableList.of(left)));	return;	}	if (join.getJoinType() != JoinRelType.INNER) {	return;	}	if (!joinInfo.isEqui()) {	return;	}	
all conditions matched for hivesemijoinrule going to apply transformation 

========================= hive sample_2854 =========================

public void resolveConcatenateMerge(HiveConf conf) {	isListBucketingAlterTableConcatenate = ((listBucketingCtx == null) ? false : listBucketingCtx .isSkewedStoredAsDir());	
islistbucketingaltertableconcatenate 

========================= hive sample_3603 =========================

public long getLength() {	try {	return baseMapRedSplit.getLength();	} catch (IOException e) {	
exception in hcatsplit 

public String[] getLocations() {	try {	return baseMapRedSplit.getLocations();	} catch (IOException e) {	
exception in hcatsplit 

========================= hive sample_759 =========================

private static void resolveMetadata(Properties props) throws HiveException, IOException {	DatabaseType dbType = DatabaseType.valueOf( props.getProperty(JdbcStorageConfig.DATABASE_TYPE.getPropertyName()));	
resolving db type 

========================= hive sample_1147 =========================

public List<Task<? extends Serializable>> handle(Context context) throws SemanticException {	AlterTableMessage msg = deserializer.getAlterTableMessage(context.dmd.getPayload());	String actualDbName = context.isDbNameEmpty() ? msg.getDB() : context.dbName;	String actualTblName = context.isTableNameEmpty() ? msg.getTable() : context.tableName;	TruncateTableDesc truncateTableDesc = new TruncateTableDesc( actualDbName + "." + actualTblName, null, context.eventOnlyReplicationSpec());	Task<DDLWork> truncateTableTask = TaskFactory.get( new DDLWork(readEntitySet, writeEntitySet, truncateTableDesc), context.hiveConf);	
added truncate tbl task 

========================= hive sample_3480 =========================

public static void main(String[] args) {	
version 

========================= hive sample_1901 =========================

public boolean connect(StatsCollectionContext scc) {	List<String> statsDirs = scc.getStatsTmpDirs();	assert statsDirs.size() == 1 : "Found multiple stats dirs: " + statsDirs;	Path statsDir = new Path(statsDirs.get(0));	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
about to read stats from 

try {	statsMap = kryo.readObject(in, statsMap.getClass());	} finally {	SerializationUtilities.releaseKryo(kryo);	}	statsList.add(statsMap);	in.close();	}	return true;	} catch (IOException e) {	
failed to read stats from filesystem 

public String aggregateStats(String partID, String statType) {	long counter = 0;	
part id t 

Map<String,String> partStat = statsMap.get(partID);	if (null == partStat) {	continue;	}	String statVal = partStat.get(statType);	if (null == statVal) {	continue;	}	counter += Long.parseLong(statVal);	}	
read stats for t t 

public boolean closeConnection(StatsCollectionContext scc) {	List<String> statsDirs = scc.getStatsTmpDirs();	assert statsDirs.size() == 1 : "Found multiple stats dirs: " + statsDirs;	Path statsDir = new Path(statsDirs.get(0));	
about to delete stats tmp dir 

public boolean closeConnection(StatsCollectionContext scc) {	List<String> statsDirs = scc.getStatsTmpDirs();	assert statsDirs.size() == 1 : "Found multiple stats dirs: " + statsDirs;	Path statsDir = new Path(statsDirs.get(0));	try {	fs.delete(statsDir,true);	return true;	} catch (IOException e) {	
failed to delete stats dir 

========================= hive sample_5041 =========================

}	try {	FileSystem fs = mapPath.getFileSystem(conf);	if (fs.exists(mapPath)) {	fs.delete(mapPath, true);	}	if (fs.exists(reducePath)) {	fs.delete(reducePath, true);	}	} catch (Exception e) {	
failed to clean up tmp directories 

public static void cacheBaseWork(Configuration conf, String name, BaseWork work, Path hiveScratchDir) {	try {	setPlanPath(conf, hiveScratchDir);	setBaseWork(conf, name, work);	} catch (IOException e) {	
failed to cache plan 

return null;	}	BaseWork gWork = gWorkMap.get(conf).get(path);	if (gWork == null) {	Path localPath = path;	LOG.debug("local path = {}", localPath);	final long serializedSize;	final String planMode;	if (HiveConf.getBoolVar(conf, ConfVars.HIVE_RPC_QUERY_PLAN)) {	String planStringPath = path.toUri().getPath();	
loading plan from string 

BaseWork gWork = gWorkMap.get(conf).get(path);	if (gWork == null) {	Path localPath = path;	LOG.debug("local path = {}", localPath);	final long serializedSize;	final String planMode;	if (HiveConf.getBoolVar(conf, ConfVars.HIVE_RPC_QUERY_PLAN)) {	String planStringPath = path.toUri().getPath();	String planString = conf.getRaw(planStringPath);	if (planString == null) {	
could not find plan string in conf 

String planString = conf.getRaw(planStringPath);	if (planString == null) {	return null;	}	serializedSize = planString.length();	planMode = "RPC";	byte[] planBytes = Base64.decodeBase64(planString);	in = new ByteArrayInputStream(planBytes);	in = new InflaterInputStream(in);	} else {	
open file to read in plan 

}	} else if (name.contains(MERGE_PLAN_NAME)) {	if (name.startsWith(MAPNAME)) {	gWork = SerializationUtilities.deserializePlan(kryo, in, MapWork.class);	} else if (name.startsWith(REDUCENAME)) {	gWork = SerializationUtilities.deserializePlan(kryo, in, ReduceWork.class);	} else {	throw new RuntimeException("Unknown work type: " + name);	}	}	
deserialized plan via name size 

if (name.startsWith(MAPNAME)) {	gWork = SerializationUtilities.deserializePlan(kryo, in, MapWork.class);	} else if (name.startsWith(REDUCENAME)) {	gWork = SerializationUtilities.deserializePlan(kryo, in, ReduceWork.class);	} else {	throw new RuntimeException("Unknown work type: " + name);	}	}	gWorkMap.get(conf).put(path, gWork);	} else {	
found plan in cache for name 

gWork = SerializationUtilities.deserializePlan(kryo, in, ReduceWork.class);	} else {	throw new RuntimeException("Unknown work type: " + name);	}	}	gWorkMap.get(conf).put(path, gWork);	} else {	}	return gWork;	} catch (FileNotFoundException fnf) {	
no plan file found 

if (useCache && !ShimLoader.getHadoopShims().isLocalMode(conf)) {	if (!DistributedCache.getSymlink(conf)) {	DistributedCache.createSymlink(conf);	}	String uriWithLink = planPath.toUri().toString() + "#" + name;	DistributedCache.addCacheFile(new URI(uriWithLink), conf);	short replication = (short) conf.getInt("mapred.submit.replication", 10);	fs.setReplication(planPath, replication);	}	}	
serialized plan via name size 

private static String getIdFromFilename(String filename, Pattern pattern) {	String taskId = filename;	int dirEnd = filename.lastIndexOf(Path.SEPARATOR);	if (dirEnd != -1) {	taskId = filename.substring(dirEnd + 1);	}	Matcher m = pattern.matcher(taskId);	if (!m.matches()) {	
unable to get task id from file name using last component as task id 

public static String replaceTaskId(String taskId, int bucketNum) {	String bucketNumStr = String.valueOf(bucketNum);	Matcher m = PREFIXED_TASK_ID_REGEX.matcher(taskId);	if (!m.matches()) {	
unable to determine bucket number from task id using task id as bucket number 

private static String replaceTaskId(String taskId, String strBucketNum) {	Matcher m = PREFIXED_TASK_ID_REGEX.matcher(strBucketNum);	if (!m.matches()) {	
unable to determine bucket number from file id using file id as bucket number 

Set<Path> filesKept = new HashSet<Path>();	perfLogger.PerfLogBegin("FileSinkOperator", "RemoveTempOrDuplicateFiles");	List<Path> emptyBuckets = Utilities.removeTempOrDuplicateFiles( fs, statuses, dpCtx, conf, hconf, filesKept, false);	perfLogger.PerfLogEnd("FileSinkOperator", "RemoveTempOrDuplicateFiles");	if (!emptyBuckets.isEmpty()) {	perfLogger.PerfLogBegin("FileSinkOperator", "CreateEmptyBuckets");	createEmptyBuckets( hconf, emptyBuckets, conf.getCompressed(), conf.getTableInfo(), reporter);	filesKept.addAll(emptyBuckets);	perfLogger.PerfLogEnd("FileSinkOperator", "CreateEmptyBuckets");	}	
moving tmp dir to 

}	perfLogger.PerfLogBegin("FileSinkOperator", "RenameOrMoveFiles");	if (isBlobStorage) {	Utilities.moveSpecifiedFiles(fs, tmpPath, specPath, filesKept);	} else {	Utilities.renameOrMoveFiles(fs, tmpPath, specPath);	}	perfLogger.PerfLogEnd("FileSinkOperator", "RenameOrMoveFiles");	}	} else {	
deleting tmppath 

if (isBlobStorage) {	Utilities.moveSpecifiedFiles(fs, tmpPath, specPath, filesKept);	} else {	Utilities.renameOrMoveFiles(fs, tmpPath, specPath);	}	perfLogger.PerfLogEnd("FileSinkOperator", "RenameOrMoveFiles");	}	} else {	fs.delete(tmpPath, true);	}	
deleting tasktmppath 

outputClass = serializer.getSerializedClass();	hiveOutputFormat = HiveFileFormatUtils.getHiveOutputFormat(hconf, tableInfo);	} catch (SerDeException e) {	throw new HiveException(e);	} catch (InstantiationException e) {	throw new HiveException(e);	} catch (IllegalAccessException e) {	throw new HiveException(e);	}	for (Path path : paths) {	
creating empty bucket for 

} catch (SerDeException e) {	throw new HiveException(e);	} catch (InstantiationException e) {	throw new HiveException(e);	} catch (IllegalAccessException e) {	throw new HiveException(e);	}	for (Path path : paths) {	RecordWriter writer = HiveFileFormatUtils.getRecordWriter( jc, hiveOutputFormat, outputClass, isCompressed, tableInfo.getProperties(), path, reporter);	writer.close(false);	
created empty bucket for enforcing bucketing at 

private static boolean removeEmptyDpDirectory(FileSystem fs, Path path) throws IOException {	FileStatus[] items = fs.listStatus(path);	if (items.length != 0) {	return false;	}	if (!fs.delete(path, true)) {	
cannot delete empty directory 

Path path = parts[i].getPath();	if (removeEmptyDpDirectory(fs, path)) {	parts[i] = null;	continue;	}	if (isMmTable) {	Path mmDir = parts[i].getPath();	if (!mmDir.getName().equals(AcidUtils.baseOrDeltaSubdir(isBaseDir, txnId, txnId, stmtId))) {	throw new IOException("Unexpected non-MM directory name " + mmDir);	}	
removetemporduplicatefiles processing files in mm directory 

private static Path extractNonDpMmDir(Long txnId, int stmtId, FileStatus[] items, boolean isBaseDir) throws IOException {	if (items.length > 1) {	throw new IOException("Unexpected directories for non-DP MM: " + Arrays.toString(items));	}	Path mmDir = items[0].getPath();	if (!mmDir.getName().equals(AcidUtils.baseOrDeltaSubdir(isBaseDir, txnId, txnId, stmtId))) {	throw new IOException("Unexpected non-MM directory " + mmDir);	}	
removetemporduplicatefiles processing files in mm directory 

private static void addBucketFileIfMissing(List<Path> result, HashMap<String, FileStatus> taskIDToFile, String taskID1, Path bucketPath, int j) {	String taskID2 = replaceTaskId(taskID1, j);	if (!taskIDToFile.containsKey(taskID2)) {	URI bucketUri = bucketPath.toUri();	String path2 = replaceTaskIdFromFilename(bucketUri.getPath().toString(), j);	
creating an empty bucket file 

private static HashMap<String, FileStatus> removeTempOrDuplicateFilesNonMm( FileStatus[] files, FileSystem fs) throws IOException {	if (files == null || fs == null) {	return null;	}	HashMap<String, FileStatus> taskIdToFile = new HashMap<String, FileStatus>();	for (FileStatus one : files) {	if (isTempPath(one)) {	Path onePath = one.getPath();	
removetemporduplicatefiles deleting 

private static void ponderRemovingTempOrDuplicateFile(FileSystem fs, FileStatus file, HashMap<String, FileStatus> taskIdToFile) throws IOException {	Path filePath = file.getPath();	String taskId = getPrefixedTaskIdFromFilename(filePath.getName());	
removetemporduplicatefiles looking at taskid 

private static FileStatus compareTempOrDuplicateFiles(FileSystem fs, FileStatus file, FileStatus existingFile) throws IOException {	FileStatus toDelete = null, toRetain = null;	Path filePath = file.getPath();	if (isCopyFile(filePath.getName())) {	
file identified as duplicate this file is not deleted as it has copysuffix 

if (existingFile.getLen() >= file.getLen()) {	toDelete = file;	toRetain = existingFile;	} else {	toDelete = existingFile;	toRetain = file;	}	if (!fs.delete(toDelete.getPath(), true)) {	throw new IOException( "Unable to delete duplicate file: " + toDelete.getPath() + ". Existing file: " + toRetain.getPath());	} else {	
duplicate taskid file removed with length existing file with length 

public static boolean isCopyFile(String filename) {	String taskId = filename;	String copyFileSuffix = null;	int dirEnd = filename.lastIndexOf(Path.SEPARATOR);	if (dirEnd != -1) {	taskId = filename.substring(dirEnd + 1);	}	Matcher m = COPY_FILE_NAME_TO_TASK_ID_REGEX.matcher(taskId);	if (!m.matches()) {	
unable to verify if file name has copy suffix 

int dirEnd = filename.lastIndexOf(Path.SEPARATOR);	if (dirEnd != -1) {	taskId = filename.substring(dirEnd + 1);	}	Matcher m = COPY_FILE_NAME_TO_TASK_ID_REGEX.matcher(taskId);	if (!m.matches()) {	} else {	taskId = m.group(1);	copyFileSuffix = m.group(4);	}	
filename taskid copysuffix 

private static String validateFiles(Configuration conf, Set<String> files){	if (files != null) {	List<String> realFiles = new ArrayList<String>(files.size());	for (String one : files) {	try {	String onefile = realFile(one, conf);	if (onefile != null) {	realFiles.add(realFile(one, conf));	} else {	
the file does not exist 

public static ClassLoader getSessionSpecifiedClassLoader() {	SessionState state = SessionState.get();	if (state == null || state.getConf() == null) {	
hive conf not found or session not initiated use thread based class loader instead 

public static ClassLoader getSessionSpecifiedClassLoader() {	SessionState state = SessionState.get();	if (state == null || state.getConf() == null) {	return JavaUtils.getClassLoader();	}	ClassLoader sessionCL = state.getConf().getClassLoader();	if (sessionCL != null) {	
use session specified class loader 

public static ClassLoader getSessionSpecifiedClassLoader() {	SessionState state = SessionState.get();	if (state == null || state.getConf() == null) {	return JavaUtils.getClassLoader();	}	ClassLoader sessionCL = state.getConf().getClassLoader();	if (sessionCL != null) {	return sessionCL;	}	
session specified class loader not found use thread based class loader 

private static URL urlFromPathString(String onestr) {	URL oneurl = null;	try {	if (StringUtils.indexOf(onestr, "file:/") == 0) {	oneurl = new URL(onestr);	} else {	oneurl = new File(onestr).toURL();	}	} catch (Exception err) {	
bad url ignoring path 

static int getMaxExecutorsForInputListing(final Configuration conf, int inputLocationListSize) {	if (inputLocationListSize < 1) {	return 0;	}	int maxExecutors = 1;	if (inputLocationListSize > 1) {	int listingMaxThreads = HiveConf.getIntVar(conf, ConfVars.HIVE_EXEC_INPUT_LISTING_MAX_THREADS);	if (listingMaxThreads <= 0) {	listingMaxThreads = conf.getInt(DEPRECATED_MAPRED_DFSCLIENT_PARALLELISM_MAX, 0);	if (listingMaxThreads > 0) {	
deprecated configuration is used please use 

} else {	summary[0] += cs.getLength();	summary[1] += cs.getFileCount();	summary[2] += cs.getDirectoryCount();	}	}	final Map<String, ContentSummary> resultMap = new ConcurrentHashMap<String, ContentSummary>();	final ExecutorService executor;	int numExecutors = getMaxExecutorsForInputListing(ctx.getConf(), pathNeedProcess.size());	if (numExecutors > 1) {	
using threads for getcontentsummary 

static ContentSummary getInputSummaryWithPool(final Context ctx, Set<Path> pathNeedProcess, MapWork work, long[] summary, ExecutorService executor) throws IOException {	List<Future<?>> results = new ArrayList<Future<?>>();	final Map<String, ContentSummary> resultMap = new ConcurrentHashMap<String, ContentSummary>();	HiveInterruptCallback interrup = HiveInterruptUtils.add(new HiveInterruptCallback() {	public void interrupt() {	for (Path path : pathNeedProcess) {	try {	path.getFileSystem(ctx.getConf()).close();	} catch (IOException ignore) {	
failed to close filesystem 

PlanUtils.configureInputJobPropertiesForStorageHandler(tableDesc);	Utilities.copyTableJobPropertiesToConf(tableDesc, jobConf);	total += estimator.estimate(jobConf, scanOp, -1).getTotalLength();	}	resultMap.put(pathStr, new ContentSummary(total, -1, -1));	} else {	FileSystem fs = p.getFileSystem(myConf);	resultMap.put(pathStr, fs.getContentSummary(p));	}	} catch (Exception e) {	
cannot get size of safely ignored 

}	}	if (executor != null) {	for (Future<?> result : results) {	boolean executorDone = false;	do {	try {	result.get();	executorDone = true;	} catch (InterruptedException e) {	
interrupted when waiting threads 

executor.shutdown();	}	HiveInterruptUtils.checkInterrupted();	for (Map.Entry<String, ContentSummary> entry : resultMap.entrySet()) {	ContentSummary cs = entry.getValue();	summary[0] += cs.getLength();	summary[1] += cs.getFileCount();	summary[2] += cs.getDirectoryCount();	ctx.addCS(entry.getKey(), cs);	if (LOG.isInfoEnabled()) {	
cache content summary for length file count directory count 

public static boolean isEmptyPath(JobConf job, Path dirPath, Context ctx) throws Exception {	if (ctx != null) {	ContentSummary cs = ctx.getCS(dirPath);	if (cs != null) {	if (LOG.isInfoEnabled()) {	
content summary length num files num directories 

public static boolean isEmptyPath(JobConf job, Path dirPath, Context ctx) throws Exception {	if (ctx != null) {	ContentSummary cs = ctx.getCS(dirPath);	if (cs != null) {	if (LOG.isInfoEnabled()) {	}	return (cs.getLength() == 0 && cs.getFileCount() == 0 && cs.getDirectoryCount() <= 1);	} else {	
content summary not cached for 

public static List<LinkedHashMap<String, String>> getFullDPSpecs(Configuration conf, DynamicPartitionCtx dpCtx) throws HiveException {	try {	Path loadPath = dpCtx.getRootPath();	FileSystem fs = loadPath.getFileSystem(conf);	int numDPCols = dpCtx.getNumDPCols();	FileStatus[] status = HiveStatsUtils.getFileStatusRecurse(loadPath, numDPCols, fs);	if (status.length == 0) {	
no partition is generated by dynamic partitioning 

if (status.length == 0) {	return null;	}	Map<String, String> partSpec = dpCtx.getPartSpec();	List<LinkedHashMap<String, String>> fullPartSpecs = new ArrayList<LinkedHashMap<String, String>>();	for (int i = 0; i < status.length; ++i) {	Path partPath = status[i].getPath();	assert fs.getFileStatus(partPath).isDir() : "partitions " + partPath + " is not a directory !";	LinkedHashMap<String, String> fullPartSpec = new LinkedHashMap<String, String>(partSpec);	if (!Warehouse.makeSpecFromName(fullPartSpec, partPath, new HashSet<String>(partSpec.keySet()))) {	
ignoring invalid dp directory 

}	Map<String, String> partSpec = dpCtx.getPartSpec();	List<LinkedHashMap<String, String>> fullPartSpecs = new ArrayList<LinkedHashMap<String, String>>();	for (int i = 0; i < status.length; ++i) {	Path partPath = status[i].getPath();	assert fs.getFileStatus(partPath).isDir() : "partitions " + partPath + " is not a directory !";	LinkedHashMap<String, String> fullPartSpec = new LinkedHashMap<String, String>(partSpec);	if (!Warehouse.makeSpecFromName(fullPartSpec, partPath, new HashSet<String>(partSpec.keySet()))) {	continue;	}	
adding partition spec from 

public static <T> T executeWithRetry(SQLCommand<T> cmd, PreparedStatement stmt, long baseWindow, int maxRetries)  throws SQLException {	T result = null;	for (int failures = 0; ; failures++) {	try {	result = cmd.run(stmt);	return result;	} catch (SQLTransientException e) {	
failure and retry 

public static Connection connectWithRetry(String connectionString, long waitWindow, int maxRetries) throws SQLException {	for (int failures = 0; ; failures++) {	try {	Connection conn = DriverManager.getConnection(connectionString);	return conn;	} catch (SQLTransientException e) {	if (failures >= maxRetries) {	
error during jdbc connection 

public static PreparedStatement prepareWithRetry(Connection conn, String stmt, long waitWindow, int maxRetries) throws SQLException {	for (int failures = 0; ; failures++) {	try {	return conn.prepareStatement(stmt);	} catch (SQLTransientException e) {	if (failures >= maxRetries) {	
error preparing jdbc statement 

public static void setQueryTimeout(java.sql.Statement stmt, int timeout) throws SQLException {	if (timeout < 0) {	
invalid query timeout 

public static void setQueryTimeout(java.sql.Statement stmt, int timeout) throws SQLException {	if (timeout < 0) {	return;	}	try {	stmt.setQueryTimeout(timeout);	} catch (SQLException e) {	String message = e.getMessage() == null ? null : e.getMessage().toLowerCase();	if (e instanceof SQLFeatureNotSupportedException || (message != null && (message.contains("implemented") || message.contains("supported")))) {	
setquerytimeout is not supported 

public static List<Path> getInputPaths(JobConf job, MapWork work, Path hiveScratchDir, Context ctx, boolean skipDummy) throws Exception {	Set<Path> pathsProcessed = new HashSet<Path>();	List<Path> pathsToAdd = new LinkedList<Path>();	LockedDriverState lDrvStat = LockedDriverState.getLockedDriverState();	Collection<String> aliasToWork = work.getAliasToWork().keySet();	if (!skipDummy) {	aliasToWork = new ArrayList<>(aliasToWork);	}	for (String alias : aliasToWork) {	
processing alias 

for (Map.Entry<Path, ArrayList<String>> e : pathToAliases) {	if (lDrvStat != null && lDrvStat.isAborted()) {	throw new IOException("Operation is Canceled.");	}	Path file = e.getKey();	List<String> aliases = e.getValue();	if (aliases.contains(alias)) {	if (file != null) {	isEmptyTable = false;	} else {	
found a null path for alias 

if (file != null) {	isEmptyTable = false;	} else {	continue;	}	if (pathsProcessed.contains(file)) {	continue;	}	StringInternUtils.internUriStringsInPath(file);	pathsProcessed.add(file);	
adding input file 

} else {	continue;	}	if (pathsProcessed.contains(file)) {	continue;	}	StringInternUtils.internUriStringsInPath(file);	pathsProcessed.add(file);	if (!hasLogged) {	hasLogged = true;	
adding inputs the first input is 

private static Path createDummyFileForEmptyPartition(Path path, JobConf job, PartitionDesc partDesc, Path hiveScratchDir) throws Exception {	String strPath = path.toString();	if (partDesc.getTableDesc().isNonNative()) {	return path;	}	Properties props = SerDeUtils.createOverlayedProperties( partDesc.getTableDesc().getProperties(), partDesc.getProperties());	HiveOutputFormat outFileFormat = HiveFileFormatUtils.getHiveOutputFormat(job, partDesc);	boolean oneRow = partDesc.getInputFileFormatClass() == OneNullRowInputFormat.class;	Path newPath = createEmptyFile(hiveScratchDir, outFileFormat, job, props, oneRow);	
changed input file to empty file 

private static Path createDummyFileForEmptyTable(JobConf job, MapWork work, Path hiveScratchDir, String alias) throws Exception {	TableDesc tableDesc = work.getAliasToPartnInfo().get(alias).getTableDesc();	if (tableDesc.isNonNative()) {	return null;	}	Properties props = tableDesc.getProperties();	HiveOutputFormat outFileFormat = HiveFileFormatUtils.getHiveOutputFormat(job, tableDesc);	Path newPath = createEmptyFile(hiveScratchDir, outFileFormat, job, props, false);	
changed input file for alias to newpath 

public static boolean createDirsWithPermission(Configuration conf, Path mkdirPath, FsPermission fsPermission, boolean recursive) throws IOException {	String origUmask = null;	
create dirs with permission recursive 

return new ArrayList<>(0);	}	Collection<Class<?>> classList = new ArrayList<Class<?>>(classNames.length);	for (String className : classNames) {	if (StringUtils.isEmpty(className)) {	continue;	}	try {	classList.add(Class.forName(className));	} catch (Exception ex) {	
cannot create class for checks 

public static void addTableSchemaToConf(Configuration conf, TableScanOperator tableScanOp) {	String schemaEvolutionColumns = tableScanOp.getSchemaEvolutionColumns();	if (schemaEvolutionColumns != null) {	conf.set(IOConstants.SCHEMA_EVOLUTION_COLUMNS, tableScanOp.getSchemaEvolutionColumns());	conf.set(IOConstants.SCHEMA_EVOLUTION_COLUMNS_TYPES, tableScanOp.getSchemaEvolutionColumnsTypes());	} else {	
schema evolution columns and schema evolution columns types not available 

private static void tryDelete(FileSystem fs, Path path) {	try {	fs.delete(path, true);	} catch (IOException ex) {	
failed to delete 

RemoteIterator<LocatedFileStatus> allFiles = fs.listFiles(path, true);	while (allFiles.hasNext()) {	LocatedFileStatus lfs = allFiles.next();	Path lfsPath = lfs.getPath();	Path dirPath = Path.getPathWithoutSchemeAndAuthority(lfsPath);	String dir = dirPath.toString();	if (!dir.startsWith(relRoot)) {	throw new IOException("Path " + lfsPath + " is not under " + relRoot + " (when shortened to " + dir + ")");	}	String subDir = dir.substring(relRoot.length());	
looking at from 

throw new IOException("Path " + lfsPath + " is not under " + relRoot + " (when shortened to " + dir + ")");	}	String subDir = dir.substring(relRoot.length());	if (lastRelDir != null && subDir.startsWith(lastRelDir)) {	continue;	}	int startIx = skipLevels > 0 ? -1 : 0;	for (int i = 0; i < skipLevels; ++i) {	startIx = subDir.indexOf(Path.SEPARATOR_CHAR, startIx + 1);	if (startIx == -1) {	
expected level of nesting is not present in from 

startIx = subDir.indexOf(Path.SEPARATOR_CHAR, startIx + 1);	if (startIx == -1) {	break;	}	}	if (startIx == -1) {	continue;	}	int endIx = subDir.indexOf(Path.SEPARATOR_CHAR, startIx + 1);	if (endIx == -1) {	
expected level of nesting is not present in from 

private static void tryDeleteAllMmFiles(FileSystem fs, Path specPath, Path manifestDir, int dpLevels, int lbLevels, JavaUtils.IdPathFilter filter, long txnId, int stmtId, Configuration conf, boolean isBaseDir) throws IOException {	Path[] files = getMmDirectoryCandidates( fs, specPath, dpLevels, lbLevels, filter, txnId, stmtId, conf, isBaseDir);	if (files != null) {	for (Path path : files) {	
deleting on failure 

private static void tryDeleteAllMmFiles(FileSystem fs, Path specPath, Path manifestDir, int dpLevels, int lbLevels, JavaUtils.IdPathFilter filter, long txnId, int stmtId, Configuration conf, boolean isBaseDir) throws IOException {	Path[] files = getMmDirectoryCandidates( fs, specPath, dpLevels, lbLevels, filter, txnId, stmtId, conf, isBaseDir);	if (files != null) {	for (Path path : files) {	tryDelete(fs, path);	}	}	
deleting on failure 

public static void writeMmCommitManifest(List<Path> commitPaths, Path specPath, FileSystem fs, String taskId, Long txnId, int stmtId, String unionSuffix, boolean isInsertOverwrite) throws HiveException {	if (commitPaths.isEmpty()) {	return;	}	Path manifestPath = getManifestDir(specPath, txnId, stmtId, unionSuffix, isInsertOverwrite);	manifestPath = new Path(manifestPath, taskId + MANIFEST_EXTENSION);	
writing manifest to with 

public static void handleMmTableFinalPath(Path specPath, String unionSuffix, Configuration hconf, boolean success, int dpLevels, int lbLevels, MissingBucketsContext mbc, long txnId, int stmtId, Reporter reporter, boolean isMmTable, boolean isMmCtas, boolean isInsertOverwrite) throws IOException, HiveException {	FileSystem fs = specPath.getFileSystem(hconf);	Path manifestDir = getManifestDir(specPath, txnId, stmtId, unionSuffix, isInsertOverwrite);	if (!success) {	JavaUtils.IdPathFilter filter = new JavaUtils.IdPathFilter(txnId, stmtId, true);	tryDeleteAllMmFiles(fs, specPath, manifestDir, dpLevels, lbLevels, filter, txnId, stmtId, hconf, isInsertOverwrite);	return;	}	
looking for manifests in 

tryDeleteAllMmFiles(fs, specPath, manifestDir, dpLevels, lbLevels, filter, txnId, stmtId, hconf, isInsertOverwrite);	return;	}	List<Path> manifests = new ArrayList<>();	if (fs.exists(manifestDir)) {	FileStatus[] manifestFiles = fs.listStatus(manifestDir);	if (manifestFiles != null) {	for (FileStatus status : manifestFiles) {	Path path = status.getPath();	if (path.getName().endsWith(MANIFEST_EXTENSION)) {	
reading manifest 

FileStatus[] manifestFiles = fs.listStatus(manifestDir);	if (manifestFiles != null) {	for (FileStatus status : manifestFiles) {	Path path = status.getPath();	if (path.getName().endsWith(MANIFEST_EXTENSION)) {	manifests.add(path);	}	}	}	} else {	
no manifests found query produced no output 

for (FileStatus status : manifestFiles) {	Path path = status.getPath();	if (path.getName().endsWith(MANIFEST_EXTENSION)) {	manifests.add(path);	}	}	}	} else {	manifestDir = null;	}	
looking for files in 

if (path.getName().endsWith(MANIFEST_EXTENSION)) {	manifests.add(path);	}	}	}	} else {	manifestDir = null;	}	JavaUtils.IdPathFilter filter = new JavaUtils.IdPathFilter(txnId, stmtId, true, false, isInsertOverwrite);	if (isMmCtas && !fs.exists(specPath)) {	
creating table directory for ctas with no output at 

manifestDir = null;	}	JavaUtils.IdPathFilter filter = new JavaUtils.IdPathFilter(txnId, stmtId, true, false, isInsertOverwrite);	if (isMmCtas && !fs.exists(specPath)) {	FileUtils.mkdir(fs, specPath, hconf);	}	Path[] files = getMmDirectoryCandidates( fs, specPath, dpLevels, lbLevels, filter, txnId, stmtId, hconf, isInsertOverwrite);	ArrayList<Path> mmDirectories = new ArrayList<>();	if (files != null) {	for (Path path : files) {	
looking at path 

int fileCount = mdis.readInt();	for (int i = 0; i < fileCount; ++i) {	String nextFile = mdis.readUTF();	if (!committed.add(nextFile)) {	throw new HiveException(nextFile + " was specified in multiple manifests");	}	}	}	}	if (manifestDir != null) {	
deleting manifest directory 

}	}	}	}	if (manifestDir != null) {	tryDelete(fs, manifestDir);	if (unionSuffix != null) {	manifestDir = manifestDir.getParent();	FileStatus[] remainingFiles = fs.listStatus(manifestDir);	if (remainingFiles == null || remainingFiles.length == 0) {	
deleting manifest directory 

}	deleteUncommitedFile(childPath, fs);	} else if (!child.isDirectory()) {	if (committed.contains(childPath.toString())) {	throw new HiveException("Union FSOP has commited " + childPath + " outside of union directory" + unionSuffix);	}	deleteUncommitedFile(childPath, fs);	} else if (childPath.getName().equals(unionSuffix)) {	cleanMmDirectory(childPath, fs, null, committed);	} else {	
fsop for is ignoring the other side of the union 

private static void deleteUncommitedFile(Path childPath, FileSystem fs) throws IOException, HiveException {	
deleting that was not committed 

public static List<Path> getValidMmDirectoriesFromTableOrPart(Path path, Configuration conf, ValidTxnList validTxnList, int lbLevels) throws IOException {	
looking for valid mm paths under 

public static List<Path> getValidMmDirectoriesFromTableOrPart(Path path, Configuration conf, ValidTxnList validTxnList, int lbLevels) throws IOException {	List<Path> result = null;	FileSystem fs = path.getFileSystem(conf);	FileStatus[] children = (lbLevels == 0) ? fs.listStatus(path) : fs.globStatus(new Path(path, StringUtils.repeat("*" + Path.SEPARATOR, lbLevels) + "*"));	for (int i = 0; i < children.length; ++i) {	FileStatus file = children[i];	Path childPath = file.getPath();	Long txnId = JavaUtils.extractTxnId(childPath);	if (!file.isDirectory() || txnId == null || !validTxnList.isTxnValid(txnId)) {	
skipping path 

========================= hive sample_3896 =========================

Command testCmd = new DropDatabaseCommand(dbName, evid);	assertEquals(evid,testCmd.getEventId());	assertEquals(1, testCmd.get().size());	assertEquals(true,testCmd.isRetriable());	assertEquals(false,testCmd.isUndoable());	CommandTestUtils.testCommandSerialization(testCmd);	client.dropDatabase(dbName, true, HCatClient.DropDBMode.CASCADE);	client.createDatabase(HCatCreateDBDesc.create(dbName).ifNotExists(false).build());	HCatDatabase db = client.getDatabase(dbName);	assertNotNull(db);	
about to run 

assertEquals(false,testNormalDropCmd.isUndoable());	CommandTestUtils.testCommandSerialization(testNormalDropCmd);	client.dropDatabase(dbName, true, HCatClient.DropDBMode.CASCADE);	client.createDatabase(HCatCreateDBDesc.create(dbName).ifNotExists(false).build());	Map<String, String> tprops = new HashMap<String,String>();	tprops.put(ReplicationUtils.REPL_STATE_ID,String.valueOf(evid + 5));	HCatTable tableToCreate = (new HCatTable(dbName, tableName)).tblProps(tprops).cols(cols);	client.createTable(HCatCreateTableDesc.create(tableToCreate).build());	HCatTable t1 = client.getTable(dbName, tableName);	assertNotNull(t1);	
about to run 

client.createDatabase(HCatCreateDBDesc.create(dbName).ifNotExists(false).build());	Map<String, String> tprops = new HashMap<String,String>();	tprops.put(ReplicationUtils.REPL_STATE_ID,String.valueOf(evid + 5));	HCatTable tableToCreate = (new HCatTable(dbName, tableName)).tblProps(tprops).cols(cols);	client.createTable(HCatCreateTableDesc.create(tableToCreate).build());	HCatTable t1 = client.getTable(dbName, tableName);	assertNotNull(t1);	driver.run(testReplicatedDropCmd.get().get(0));	HCatTable t2 = client.getTable(dbName,tableName);	assertNotNull(t2);	
about to run 

onfe = e;	}	assertNotNull(onfe);	assertTrue(onfe instanceof ObjectNotFoundException);	Map<String, String> tprops2 = new HashMap<String,String>();	tprops2.put(ReplicationUtils.REPL_STATE_ID,String.valueOf(evid - 5));	HCatTable tableToCreate2 = (new HCatTable(dbName, tableName)).tblProps(tprops2).cols(cols);	client.createTable(HCatCreateTableDesc.create(tableToCreate2).build());	HCatTable t3 = client.getTable(dbName, tableName);	assertNotNull(t3);	
about to run 

Map<String, String> props = new HashMap<String,String>();	props.put(ReplicationUtils.REPL_STATE_ID,String.valueOf(evid + 5));	HCatTable table = (new HCatTable(dbName, tableName)).tblProps(props).cols(cols).partCols(pcols);	client.createTable(HCatCreateTableDesc.create(table).build());	HCatTable tableCreated = client.getTable(dbName, tableName);	assertNotNull(tableCreated);	HCatPartition ptnToAdd = (new HCatPartition(tableCreated, ptnDesc, TestHCatClient.makePartLocation(tableCreated,ptnDesc))).parameters(props);	client.addPartition(HCatAddPartitionDesc.create(ptnToAdd).build());	HCatPartition p1 = client.getPartition(dbName,tableName,ptnDesc);	assertNotNull(p1);	
about to run 

client.createTable(HCatCreateTableDesc.create(table).build());	HCatTable tableCreated = client.getTable(dbName, tableName);	assertNotNull(tableCreated);	HCatPartition ptnToAdd = (new HCatPartition(tableCreated, ptnDesc, TestHCatClient.makePartLocation(tableCreated,ptnDesc))).parameters(props);	client.addPartition(HCatAddPartitionDesc.create(ptnToAdd).build());	HCatPartition p1 = client.getPartition(dbName,tableName,ptnDesc);	assertNotNull(p1);	driver.run(testReplicatedDropPtnCmd.get().get(0));	HCatPartition p2 = client.getPartition(dbName,tableName,ptnDesc);	assertNotNull(p2);	
about to run 

onfe = e;	}	assertNotNull(onfe);	assertTrue(onfe instanceof ObjectNotFoundException);	Map<String, String> props2 = new HashMap<String,String>();	props2.put(ReplicationUtils.REPL_STATE_ID,String.valueOf(evid - 5));	HCatPartition ptnToAdd2 = (new HCatPartition(tableCreated, ptnDesc, TestHCatClient.makePartLocation(tableCreated,ptnDesc))).parameters(props2);	client.addPartition(HCatAddPartitionDesc.create(ptnToAdd2).build());	HCatPartition p3 = client.getPartition(dbName,tableName,ptnDesc);	assertNotNull(p3);	
about to run 

Map<String, String> ptnDesc2 = new HashMap<String,String>();	ptnDesc2.put("b","test-newer");	Map<String, String> props2 = new HashMap<String,String>();	props2.put(ReplicationUtils.REPL_STATE_ID, String.valueOf(evid + 5));	HCatPartition ptnToAdd2 = (new HCatPartition(tableCreated, ptnDesc2, TestHCatClient.makePartLocation(tableCreated,ptnDesc2))).parameters(props2);	client.addPartition(HCatAddPartitionDesc.create(ptnToAdd2).build());	HCatPartition p1 = client.getPartition(dbName,tableName,ptnDesc1);	assertNotNull(p1);	HCatPartition p2 = client.getPartition(dbName,tableName,ptnDesc2);	assertNotNull(p2);	
about to run 

CommandProcessorResponse ret = driver.run( "LOAD DATA LOCAL INPATH '"+tempLocation+"' OVERWRITE INTO TABLE "+ dbName+ "." + tableName );	assertEquals(ret.getResponseCode() + ":" + ret.getErrorMessage(), null, ret.getException());	CommandProcessorResponse selectRet = driver.run("SELECT * from " + dbName + "." + tableName);	assertEquals(selectRet.getResponseCode() + ":" + selectRet.getErrorMessage(), null, selectRet.getException());	List<String> values = new ArrayList<String>();	driver.getResults(values);	assertEquals(2, values.size());	assertEquals(data[0],values.get(0));	assertEquals(data[1],values.get(1));	ExportCommand exportCmd = new ExportCommand(dbName,tableName,null, exportLocation, false, evid);	
about to run 

driver.getResults(values);	assertEquals(2, values.size());	assertEquals(data[0],values.get(0));	assertEquals(data[1],values.get(1));	ExportCommand exportCmd = new ExportCommand(dbName,tableName,null, exportLocation, false, evid);	CommandProcessorResponse ret2 = driver.run(exportCmd.get().get(0));	assertEquals(ret2.getResponseCode() + ":" + ret2.getErrorMessage(), null, ret2.getException());	List<String> exportPaths = exportCmd.cleanupLocationsAfterEvent();	assertEquals(1,exportPaths.size());	String metadata = getMetadataContents(exportPaths.get(0));	
export returned the following metadata contents 

ExportCommand exportCmd = new ExportCommand(dbName,tableName,null, exportLocation, false, evid);	CommandProcessorResponse ret2 = driver.run(exportCmd.get().get(0));	assertEquals(ret2.getResponseCode() + ":" + ret2.getErrorMessage(), null, ret2.getException());	List<String> exportPaths = exportCmd.cleanupLocationsAfterEvent();	assertEquals(1,exportPaths.size());	String metadata = getMetadataContents(exportPaths.get(0));	LOG.info(metadata);	assertTrue(metadata + "did not match \"repl.scope\"=\"all\"", metadata.matches(".*\"repl.scope\":\"all\".*"));	assertTrue(metadata + "has \"repl.last.id\"",metadata.matches(".*\"repl.last.id\":.*"));	ImportCommand importCmd = new ImportCommand(dbName, importedTableName, null, exportLocation, false, evid);	
about to run 

CommandProcessorResponse ret = driver.run( "LOAD DATA LOCAL INPATH '"+tempLocation+"' OVERWRITE INTO TABLE "+ dbName+ "." + tableName );	assertEquals(ret.getResponseCode() + ":" + ret.getErrorMessage(), null, ret.getException());	CommandProcessorResponse selectRet = driver.run("SELECT * from " + dbName + "." + tableName);	assertEquals(selectRet.getResponseCode() + ":" + selectRet.getErrorMessage(), null, selectRet.getException());	List<String> values = new ArrayList<String>();	driver.getResults(values);	assertEquals(2, values.size());	assertEquals(data[0],values.get(0));	assertEquals(data[1],values.get(1));	ExportCommand exportMdCmd = new ExportCommand(dbName,tableName,null, exportLocation, true, evid);	
about to run 

driver.getResults(values);	assertEquals(2, values.size());	assertEquals(data[0],values.get(0));	assertEquals(data[1],values.get(1));	ExportCommand exportMdCmd = new ExportCommand(dbName,tableName,null, exportLocation, true, evid);	CommandProcessorResponse ret2 = driver.run(exportMdCmd.get().get(0));	assertEquals(ret2.getResponseCode() + ":" + ret2.getErrorMessage(), null, ret2.getException());	List<String> exportPaths = exportMdCmd.cleanupLocationsAfterEvent();	assertEquals(1,exportPaths.size());	String metadata = getMetadataContents(exportPaths.get(0));	
export returned the following metadata contents 

ExportCommand exportMdCmd = new ExportCommand(dbName,tableName,null, exportLocation, true, evid);	CommandProcessorResponse ret2 = driver.run(exportMdCmd.get().get(0));	assertEquals(ret2.getResponseCode() + ":" + ret2.getErrorMessage(), null, ret2.getException());	List<String> exportPaths = exportMdCmd.cleanupLocationsAfterEvent();	assertEquals(1,exportPaths.size());	String metadata = getMetadataContents(exportPaths.get(0));	LOG.info(metadata);	assertTrue(metadata + "did not match \"repl.scope\"=\"metadata\"",metadata.matches(".*\"repl.scope\":\"metadata\".*"));	assertTrue(metadata + "has \"repl.last.id\"",metadata.matches(".*\"repl.last.id\":.*"));	ImportCommand importMdCmd = new ImportCommand(dbName, importedTableName, null, exportLocation, true, evid);	
about to run 

public void testNoopReplEximCommands() throws CommandNeedRetryException, IOException {	int evid = 333;	String exportLocation = TEST_PATH + File.separator + "testNoopReplExim";	String dbName = "doesNotExist" + System.currentTimeMillis();	String tableName = "nope" + System.currentTimeMillis();	ExportCommand noopExportCmd = new ExportCommand(dbName,tableName,null, exportLocation, false, evid);	
about to run 

int evid = 333;	String exportLocation = TEST_PATH + File.separator + "testNoopReplExim";	String dbName = "doesNotExist" + System.currentTimeMillis();	String tableName = "nope" + System.currentTimeMillis();	ExportCommand noopExportCmd = new ExportCommand(dbName,tableName,null, exportLocation, false, evid);	CommandProcessorResponse ret = driver.run(noopExportCmd.get().get(0));	assertEquals(ret.getResponseCode() + ":" + ret.getErrorMessage(), null, ret.getException());	List<String> exportPaths = noopExportCmd.cleanupLocationsAfterEvent();	assertEquals(1,exportPaths.size());	String metadata = getMetadataContents(exportPaths.get(0));	
export returned the following metadata contents 

String tableName = "nope" + System.currentTimeMillis();	ExportCommand noopExportCmd = new ExportCommand(dbName,tableName,null, exportLocation, false, evid);	CommandProcessorResponse ret = driver.run(noopExportCmd.get().get(0));	assertEquals(ret.getResponseCode() + ":" + ret.getErrorMessage(), null, ret.getException());	List<String> exportPaths = noopExportCmd.cleanupLocationsAfterEvent();	assertEquals(1,exportPaths.size());	String metadata = getMetadataContents(exportPaths.get(0));	LOG.info(metadata);	assertTrue(metadata + "did not match \"repl.noop\"=\"true\"",metadata.matches(".*\"repl.noop\":\"true\".*"));	ImportCommand noopImportCmd = new ImportCommand(dbName, tableName, null, exportLocation, false, evid);	
about to run 

========================= hive sample_866 =========================

public Path getMRScratchDir() {	if(isLocalOnlyExecutionMode()) {	return getLocalScratchDir(!isExplainSkipExecution());	}	try {	Path dir = FileUtils.makeQualified(nonLocalScratchPath, conf);	URI uri = dir.toUri();	Path newScratchDir = getScratchDir(uri.getScheme(), uri.getAuthority(), !isExplainSkipExecution(), uri.getPath());	
new scratch dir is 

public void removeScratchDir() {	for (Map.Entry<String, Path> entry : fsScratchDirs.entrySet()) {	try {	Path p = entry.getValue();	FileSystem fs = p.getFileSystem(conf);	
deleting scratch dir 

public void removeScratchDir() {	for (Map.Entry<String, Path> entry : fsScratchDirs.entrySet()) {	try {	Path p = entry.getValue();	FileSystem fs = p.getFileSystem(conf);	fs.delete(p, true);	fs.cancelDeleteOnExit(p);	} catch (Exception e) {	
error removing scratch 

public void removeMaterializedCTEs() {	for (Table materializedTable : cteTables.values()) {	Path location = materializedTable.getDataLocation();	try {	FileSystem fs = location.getFileSystem(conf);	boolean status = fs.delete(location, true);	LOG.info("Removed " + location + " for materialized " + materializedTable.getTableName() + ", status=" + status);	} catch (IOException e) {	
error removing for materialized 

public void clear() throws IOException {	for (Context subContext : rewrittenStatementContexts) {	subContext.clear();	}	if (resDir != null) {	try {	FileSystem fs = resDir.getFileSystem(conf);	
deleting result dir 

public void clear() throws IOException {	for (Context subContext : rewrittenStatementContexts) {	subContext.clear();	}	if (resDir != null) {	try {	FileSystem fs = resDir.getFileSystem(conf);	fs.delete(resDir, true);	} catch (IOException e) {	
context clear error 

if (resDir != null) {	try {	FileSystem fs = resDir.getFileSystem(conf);	fs.delete(resDir, true);	} catch (IOException e) {	}	}	if (resFile != null) {	try {	FileSystem fs = resFile.getFileSystem(conf);	
deleting result file 

FileSystem fs = resDir.getFileSystem(conf);	fs.delete(resDir, true);	} catch (IOException e) {	}	}	if (resFile != null) {	try {	FileSystem fs = resFile.getFileSystem(conf);	fs.delete(resFile, false);	} catch (IOException e) {	
context clear error 

}	}	if (pos == 0) {	return null;	}	return resFs.open(resDirPaths[resDirFilesNum++]);	} else {	return getNextStream();	}	} catch (FileNotFoundException e) {	
getstream error 

if (pos == 0) {	return null;	}	return resFs.open(resDirPaths[resDirFilesNum++]);	} else {	return getNextStream();	}	} catch (FileNotFoundException e) {	return null;	} catch (IOException e) {	
getstream error 

private DataInput getNextStream() {	try {	if (resDir != null && resDirFilesNum < resDirPaths.length && (resDirPaths[resDirFilesNum] != null)) {	return resFs.open(resDirPaths[resDirFilesNum++]);	}	} catch (FileNotFoundException e) {	
getnextstream error 

private DataInput getNextStream() {	try {	if (resDir != null && resDirFilesNum < resDirPaths.length && (resDirPaths[resDirFilesNum] != null)) {	return resFs.open(resDirPaths[resDirFilesNum++]);	}	} catch (FileNotFoundException e) {	return null;	} catch (IOException e) {	
getnextstream error 

========================= hive sample_3382 =========================

}	} finally {	writer.close();	}	execLocally("cp -f " + mPatchFile.getPath() + " $workingDir/scratch/build.patch");	}	long start;	long elapsedTime;	start = System.currentTimeMillis();	File sourcePrepScript = new File(mScratchDir, "source-prep.sh");	
writing from template 

writer.close();	}	execLocally("cp -f " + mPatchFile.getPath() + " $workingDir/scratch/build.patch");	}	long start;	long elapsedTime;	start = System.currentTimeMillis();	File sourcePrepScript = new File(mScratchDir, "source-prep.sh");	Templates.writeTemplateResult("source-prep.vm", sourcePrepScript, getTemplateDefaults());	execLocally("bash " + sourcePrepScript.getPath());	
deleting 

}	long start;	long elapsedTime;	start = System.currentTimeMillis();	File sourcePrepScript = new File(mScratchDir, "source-prep.sh");	Templates.writeTemplateResult("source-prep.vm", sourcePrepScript, getTemplateDefaults());	execLocally("bash " + sourcePrepScript.getPath());	execHostsIgnoreErrors("pkill -f java");	execHostsIgnoreErrors("pkill -9 -f java");	elapsedTime = TimeUnit.MINUTES.convert((System.currentTimeMillis() - start), TimeUnit.MILLISECONDS);	
perf source prep took minutes 

========================= hive sample_5606 =========================

private HiveDruidSplit[] getInputSplits(Configuration conf) throws IOException {	String address = HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_DRUID_BROKER_DEFAULT_ADDRESS );	if (StringUtils.isEmpty(address)) {	throw new IOException("Druid broker address not specified in configuration");	}	String druidQuery = StringEscapeUtils.unescapeJava(conf.get(Constants.DRUID_QUERY_JSON));	String druidQueryType;	if (StringUtils.isEmpty(druidQuery)) {	if (LOG.isWarnEnabled()) {	
druid query is empty creating select query 

private static HiveDruidSplit[] distributeSelectQuery(Configuration conf, String address, SelectQuery query, Path dummyPath) throws IOException {	final boolean isFetch = query.getContextBoolean(Constants.DRUID_QUERY_FETCH, false);	if (isFetch) {	return new HiveDruidSplit[] { new HiveDruidSplit( DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(query), dummyPath, new String[]{address} ) };	}	final String intervals = StringUtils.join(query.getIntervals(), ",");	final String request = String.format( "http: address, query.getDataSource().getNames().get(0), URLEncoder.encode(intervals, "UTF-8"));	
sending request to query for segments 

========================= hive sample_17 =========================

hashSet = (VectorMapJoinBytesHashSet) vectorMapJoinHashTable;	needHashTableSetup = false;	}	batchCounter++;	for(VectorExpression ve : bigTableFilterExpressions) {	ve.evaluate(batch);	}	final int inputLogicalSize = batch.size;	if (inputLogicalSize == 0) {	if (LOG.isDebugEnabled()) {	
batch empty 

JoinUtil.JoinResult joinResult;	if (!joinColVector.noNulls && joinColVector.isNull[0]) {	joinResult = JoinUtil.JoinResult.NOMATCH;	} else {	byte[] keyBytes = vector[0];	int keyStart = start[0];	int keyLength = length[0];	joinResult = hashSet.contains(keyBytes, keyStart, keyLength, hashSetResults[0]);	}	if (LOG.isDebugEnabled()) {	
batch repeated joinresult 

byte[] keyBytes = vector[0];	int keyStart = start[0];	int keyLength = length[0];	joinResult = hashSet.contains(keyBytes, keyStart, keyLength, hashSetResults[0]);	}	if (LOG.isDebugEnabled()) {	}	finishLeftSemiRepeated(batch, joinResult, hashSetResults[0]);	} else {	if (LOG.isDebugEnabled()) {	
batch non repeated 

}	if (haveSaveKey) {	switch (saveJoinResult) {	case MATCH: break;	case SPILL: hashSetResultCount++;	break;	case NOMATCH: break;	}	}	if (LOG.isDebugEnabled()) {	
allmatchs spills spillhashmapresultindices hashmapresults 

========================= hive sample_4152 =========================

fetch.setSource(pCtx.getFetchSource());	fetch.setSink(pCtx.getFetchSink());	if (isHiveServerQuery && null != resultTab && resultTab.getSerdeClassName().equalsIgnoreCase(ThriftJDBCBinarySerDe.class.getName()) && HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_SERVER2_THRIFT_RESULTSET_SERIALIZE_IN_TASKS)) {	fetch.setIsUsingThriftJDBCBinarySerDe(true);	} else {	fetch.setIsUsingThriftJDBCBinarySerDe(false);	}	pCtx.setFetchTask((FetchTask) TaskFactory.get(fetch, conf));	int fetchLimit = HiveConf.getIntVar(conf, HiveConf.ConfVars.HIVELIMITOPTMAXFETCH);	if (globalLimitCtx.isEnable() && globalLimitCtx.getGlobalLimit() > fetchLimit) {	
for fetchtask limit doesn t qualify limit optimization 

fetch.setIsUsingThriftJDBCBinarySerDe(true);	} else {	fetch.setIsUsingThriftJDBCBinarySerDe(false);	}	pCtx.setFetchTask((FetchTask) TaskFactory.get(fetch, conf));	int fetchLimit = HiveConf.getIntVar(conf, HiveConf.ConfVars.HIVELIMITOPTMAXFETCH);	if (globalLimitCtx.isEnable() && globalLimitCtx.getGlobalLimit() > fetchLimit) {	globalLimitCtx.disableOpt();	}	if (outerQueryLimit == 0) {	
limit no query execution needed 

CreateTableDesc crtTblDesc = pCtx.getCreateTable();	crtTblDesc.validate(conf);	Task<? extends Serializable> crtTblTask = TaskFactory.get(new DDLWork( inputs, outputs, crtTblDesc), conf);	patchUpAfterCTASorMaterializedView(rootTasks, outputs, crtTblTask);	} else if (pCtx.getQueryProperties().isMaterializedView()) {	CreateViewDesc viewDesc = pCtx.getCreateViewDesc();	Task<? extends Serializable> crtViewTask = TaskFactory.get(new DDLWork( inputs, outputs, viewDesc), conf);	patchUpAfterCTASorMaterializedView(rootTasks, outputs, crtViewTask);	}	if (globalLimitCtx.isEnable() && pCtx.getFetchTask() != null) {	
set least row check for fetchtask 

patchUpAfterCTASorMaterializedView(rootTasks, outputs, crtTblTask);	} else if (pCtx.getQueryProperties().isMaterializedView()) {	CreateViewDesc viewDesc = pCtx.getCreateViewDesc();	Task<? extends Serializable> crtViewTask = TaskFactory.get(new DDLWork( inputs, outputs, viewDesc), conf);	patchUpAfterCTASorMaterializedView(rootTasks, outputs, crtViewTask);	}	if (globalLimitCtx.isEnable() && pCtx.getFetchTask() != null) {	pCtx.getFetchTask().getWork().setLeastNumRows(globalLimitCtx.getGlobalLimit());	}	if (globalLimitCtx.isEnable() && globalLimitCtx.getLastReduceLimitDesc() != null) {	
set least row check for limitdesc 

private Task<?> genTableStats(ParseContext parseContext, TableScanOperator tableScan, Task currentTask, final HashSet<WriteEntity> outputs) throws HiveException {	Class<? extends InputFormat> inputFormat = tableScan.getConf().getTableMetadata() .getInputFormatClass();	Table table = tableScan.getConf().getTableMetadata();	List<Partition> partitions = new ArrayList<>();	if (table.isPartitioned()) {	partitions.addAll(parseContext.getPrunedPartitions(tableScan).getPartitions());	for (Partition partn : partitions) {	
adding part 

loc = ctd.getLocation();	} else {	loc = pCtx.getCreateViewDesc().getLocation();	}	Path location = (loc == null) ? getDefaultCtasLocation(pCtx) : new Path(loc);	if (txnIdForCtas != null) {	dataSinkForCtas.setDirName(location);	location = new Path(location, AcidUtils.deltaSubdir(txnIdForCtas, txnIdForCtas, stmtId));	lfd.setSourcePath(location);	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
setting mm ctas to 

}	Path location = (loc == null) ? getDefaultCtasLocation(pCtx) : new Path(loc);	if (txnIdForCtas != null) {	dataSinkForCtas.setDirName(location);	location = new Path(location, AcidUtils.deltaSubdir(txnIdForCtas, txnIdForCtas, stmtId));	lfd.setSourcePath(location);	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	}	}	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
location for lfd is being set to moving from 

========================= hive sample_3551 =========================

private RecordReader<NullWritable, VectorizedRowBatch> wrapLlapReader( List<Integer> includedCols, LlapRecordReader rr, InputSplit split) throws IOException {	if (sourceInputFormat instanceof BatchToRowInputFormat) {	
using batch to row converter for split 

private RecordReader<NullWritable, VectorizedRowBatch> wrapLlapReader( List<Integer> includedCols, LlapRecordReader rr, InputSplit split) throws IOException {	if (sourceInputFormat instanceof BatchToRowInputFormat) {	return bogusCast(((BatchToRowInputFormat) sourceInputFormat).getWrapper( rr, rr.getVectorizedRowBatchCtx(), includedCols));	}	
not using llap io for an unsupported split 

public RecordReader<NullWritable, VectorizedRowBatch> checkLlapSplit( InputSplit split, JobConf job, Reporter reporter) throws IOException {	boolean useLlapIo = true;	if (split instanceof LlapAwareSplit) {	useLlapIo = ((LlapAwareSplit) split).canUseLlapIo(job);	}	if (useLlapIo) return null;	
not using llap io for an unsupported split 

========================= hive sample_2194 =========================

private TezAmRegistryImpl(String instanceName, Configuration conf, boolean useSecureZk) {	super(instanceName, conf, null, NAMESPACE_PREFIX, USER_SCOPE_PATH_PREFIX, WORKER_PREFIX, useSecureZk ? SASL_LOGIN_CONTEXT_NAME : null, HiveConf.getVar(conf, ConfVars.LLAP_TASK_SCHEDULER_AM_REGISTRY_PRINCIPAL), HiveConf.getVar(conf, ConfVars.LLAP_TASK_SCHEDULER_AM_REGISTRY_KEYTAB_FILE), null);	this.registryName = instanceName;	
am zookeeper registry is enabled with registryid 

Endpoint pluginEndpoint = null;	if (pluginPort >= 0) {	pluginEndpoint = RegistryTypeUtils.ipcEndpoint( IPC_PLUGIN, new InetSocketAddress(hostname, pluginPort));	srv.addInternalEndpoint(pluginEndpoint);	}	srv.set(AM_SESSION_ID, sessionId);	boolean hasToken = serializedToken != null;	srv.set(AM_PLUGIN_TOKEN, hasToken ? serializedToken : "");	srv.set(AM_PLUGIN_JOBID, jobIdForToken != null ? jobIdForToken : "");	String uniqueId = registerServiceRecord(srv);	
registered this am rpc plugin sessionid token znodepath 

========================= hive sample_630 =========================

for (int i = 0; i < record.length; i++) {	BytesRefWritable cu = new BytesRefWritable(record[i], 0, record[i].length);	bytes.set(i, cu);	}	writer.append(bytes);	bytes.clear();	}	writer.close();	RandomAccessFile raf = new RandomAccessFile(file.toUri().getPath(), "rw");	long corruptOffset = raf.length() / 2;	
corrupting at offset 

public void fullyReadTest(FileSystem fs, int count, Path file) throws IOException, SerDeException {	
reading records 

assertEquals("Field " + i, standardWritableData, expectedFieldsData[i]);	}	assertEquals( "Class of the serialized object should be BytesRefArrayWritable", BytesRefArrayWritable.class, serDe.getSerializedClass());	BytesRefArrayWritable serializedText = (BytesRefArrayWritable) serDe .serialize(row, oi);	assertEquals("Serialized data", s, serializedText);	actualRead++;	}	reader.close();	assertEquals("Expect " + count + " rows, actual read " + actualRead, actualRead, count);	long cost = System.currentTimeMillis() - start;	
reading fully costs milliseconds 

private void partialReadTest(FileSystem fs, int count, Path file) throws IOException, SerDeException {	
reading records 

Object fieldData = oi.getStructFieldData(row, fieldRefs.get(i));	Object standardWritableData = ObjectInspectorUtils .copyToStandardObject(fieldData, fieldRefs.get(i) .getFieldObjectInspector(), ObjectInspectorCopyOption.WRITABLE);	assertEquals("Field " + i, standardWritableData, expectedPartitalFieldsData[i]);	}	assertEquals( "Class of the serialized object should be BytesRefArrayWritable", BytesRefArrayWritable.class, serDe.getSerializedClass());	BytesRefArrayWritable serializedBytes = (BytesRefArrayWritable) serDe .serialize(row, oi);	assertEquals("Serialized data", patialS, serializedBytes);	}	reader.close();	long cost = System.currentTimeMillis() - start;	
reading fully costs milliseconds 

========================= hive sample_2481 =========================

public void cacheFileMetadata(long fileId, FileSystem fs, Path path) throws IOException, InterruptedException {	ByteBuffer[] cols = fileFormatProxy.getAddedColumnsToCache();	ByteBuffer[] vals = (cols == null) ? null : new ByteBuffer[cols.length];	ByteBuffer metadata = fileFormatProxy.getMetadataToCache(fs, path, vals);	
caching file metadata for size 

========================= hive sample_1887 =========================

public Void call() throws Exception {	Stopwatch stopwatch = Stopwatch.createStarted();	
starting submittests on host 

private void executeTests(final BlockingQueue<TestBatch> parallelWorkQueue, final BlockingQueue<TestBatch> isolatedWorkQueue, final Set<TestBatch> failedTestResults) throws Exception {	if(mShutdown) {	
shutting down host 

private void executeTests(final BlockingQueue<TestBatch> parallelWorkQueue, final BlockingQueue<TestBatch> isolatedWorkQueue, final Set<TestBatch> failedTestResults) throws Exception {	if(mShutdown) {	return;	}	
starting parallel execution on 

List<ListenableFuture<Void>> droneResults = Lists.newArrayList();	for(final Drone drone : ImmutableList.copyOf(mDrones)) {	droneResults.add(mExecutor.submit(new Callable<Void>() {	public Void call() throws Exception {	TestBatch batch = null;	Stopwatch sw = Stopwatch.createUnstarted();	try {	do {	batch = parallelWorkQueue.poll(mNumPollSeconds, TimeUnit.SECONDS);	if(mShutdown) {	
shutting down host 

failedTestResults.add(batch);	}	} finally {	sw.stop();	mLogger.info("Finished processing parallel batch [{}] on host {}. ElapsedTime(ms)={}", new Object[]{batch.getName(), getHost().toShortString(), sw.elapsed(TimeUnit.MILLISECONDS)});	}	}	} while(!mShutdown && !parallelWorkQueue.isEmpty());	} catch(AbortDroneException ex) {	mDrones.remove(drone);	
aborting drone during parallel execution 

mDrones.remove(drone);	if(batch != null) {	Preconditions.checkState(parallelWorkQueue.add(batch), "Could not add batch to parallel queue " + batch);	}	}	return null;	}	}));	}	if(mShutdown) {	
shutting down host 

}	}	return null;	}	}));	}	if(mShutdown) {	return;	}	Futures.allAsList(droneResults).get();	
starting isolated execution on 

failedTestResults.add(batch);	}	} finally {	sw.stop();	mLogger.info("Finished processing isolated batch [{}] on host {}. ElapsedTime(ms)={}", new Object[]{batch.getName(), getHost().toShortString(), sw.elapsed(TimeUnit.MILLISECONDS)});	}	}	} while(!mShutdown && !isolatedWorkQueue.isEmpty());	} catch(AbortDroneException ex) {	mDrones.remove(drone);	
aborting drone during isolated execution 

templateVariables.put("testArguments", batch.getTestArguments());	templateVariables.put("localDir", drone.getLocalDirectory());	templateVariables.put("logDir", drone.getLocalLogDirectory());	Preconditions.checkArgument(StringUtils.isNotBlank(batch.getTestModuleRelativeDir()));	templateVariables.put("testModule", batch.getTestModuleRelativeDir());	String command = Templates.getTemplateResult("bash $localDir/$instanceName/scratch/" + script.getName(), templateVariables);	Templates.writeTemplateResult("batch-exec.vm", script, templateVariables);	copyToDroneFromLocal(drone, script.getAbsolutePath(), "$localDir/$instanceName/scratch/" + scriptName);	script.delete();	Stopwatch sw = Stopwatch.createStarted();	
executing with 

script.delete();	Stopwatch sw = Stopwatch.createStarted();	RemoteCommandResult sshResult = new SSHCommand(mSSHCommandExecutor, drone.getPrivateKey(), drone.getUser(), drone.getHost(), drone.getInstance(), command, true). call();	sw.stop();	mLogger.info("Completed executing tests for batch [{}] on host {}. ElapsedTime(ms)={}", new Object[]{batch.getName(), getHost().toShortString(), sw.elapsed(TimeUnit.MILLISECONDS)});	File batchLogDir = null;	if(sshResult.getExitCode() == Constants.EXIT_CODE_UNKNOWN) {	throw new AbortDroneException("Drone " + drone.toString() + " exited with " + Constants.EXIT_CODE_UNKNOWN + ": " + sshResult);	}	if(mShutdown) {	
shutting down host 

for(final Drone drone : ImmutableList.copyOf(mDrones)) {	Preconditions.checkState(remoteStagingLocation == null, "Remote staging location must be null at the start of the loop");	final Map<String, String> templateVariables = Maps.newHashMap(mTemplateDefaults);	templateVariables.put("instanceName", drone.getInstanceName());	templateVariables.put("localDir", drone.getLocalDirectory());	String resolvedRemoteLocation = Files.simplifyPath(Templates.getTemplateResult(remoteFile, templateVariables));	RSyncResult result = new RSyncCommand(mRSyncCommandExecutor, drone.getPrivateKey(), drone.getUser(), drone.getHost(), drone.getInstance(), resolvedLocalLocation, resolvedRemoteLocation, RSyncCommand.Type.FROM_LOCAL).call();	if(result.getExitCode() == Constants.EXIT_CODE_SUCCESS) {	remoteStagingLocation = resolvedRemoteLocation;	drones.remove(drone);	
successfully staged on 

templateVariables.put("instanceName", drone.getInstanceName());	templateVariables.put("localDir", drone.getLocalDirectory());	String resolvedRemoteLocation = Files.simplifyPath(Templates.getTemplateResult(remoteFile, templateVariables));	RSyncResult result = new RSyncCommand(mRSyncCommandExecutor, drone.getPrivateKey(), drone.getUser(), drone.getHost(), drone.getInstance(), resolvedLocalLocation, resolvedRemoteLocation, RSyncCommand.Type.FROM_LOCAL).call();	if(result.getExitCode() == Constants.EXIT_CODE_SUCCESS) {	remoteStagingLocation = resolvedRemoteLocation;	drones.remove(drone);	break;	} else {	mDrones.remove(drone);	
aborting drone during rsync drone exited with 

if(result.getExitCode() == Constants.EXIT_CODE_SUCCESS) {	remoteStagingLocation = resolvedRemoteLocation;	drones.remove(drone);	break;	} else {	mDrones.remove(drone);	}	}	if(remoteStagingLocation == null) {	Preconditions.checkState(mDrones.isEmpty(), "If remote staging location is not set all drones should be bad");	
unable to stage directory on remote host all drones must be bad 

return mExecutor.submit(new Callable<SSHResult>() {	public SSHResult call() throws Exception {	for(final Drone drone : ImmutableList.copyOf(mDrones)) {	Map<String, String> templateVariables = Maps.newHashMap(mTemplateDefaults);	templateVariables.put("instanceName", drone.getInstanceName());	templateVariables.put("localDir", drone.getLocalDirectory());	String command = Templates.getTemplateResult(cmd, templateVariables);	SSHResult result = new SSHCommand(mSSHCommandExecutor, drone.getPrivateKey(), drone.getUser(), drone.getHost(), drone.getInstance(), command, reportErrors).call();	if(reportErrors && result.getExitCode() == Constants.EXIT_CODE_UNKNOWN) {	mDrones.remove(drone);	
aborting drone during exec drone exited with 

for(final Drone drone : ImmutableList.copyOf(drones)) {	result.add(mExecutor.submit(new Callable<RemoteCommandResult>() {	public RemoteCommandResult call() throws Exception {	Map<String, String> templateVariables = Maps.newHashMap(mTemplateDefaults);	templateVariables.put("instanceName", drone.getInstanceName());	templateVariables.put("localDir", drone.getLocalDirectory());	String command = Templates.getTemplateResult(cmd, templateVariables);	SSHResult result = new SSHCommand(mSSHCommandExecutor, drone.getPrivateKey(), drone.getUser(), drone.getHost(), drone.getInstance(), command, true).call();	if(result.getExitCode() != Constants.EXIT_CODE_SUCCESS) {	mDrones.remove(drone);	
aborting drone during exec drone exited with 

========================= hive sample_5578 =========================

opType = OpType.LIST;	}	else if (commandLine.hasOption("delete")) {	opType = OpType.DELETE;	}	else {	throw new IllegalArgumentException("Operation must be delete, list or get!");	}	isDryRun = (commandLine.hasOption("dryRun"));	if (commandLine.hasOption("expired")) {	
working on expired delegation tokens 

String olderThanLimitString = commandLine.getOptionValue("olderThan");	switch (olderThanLimitString.charAt(olderThanLimitString.length()-1)) {	case 'd': case 'D': timeLimitMillis = System.currentTimeMillis() - 24*60*60*1000*Integer.parseInt(olderThanLimitString.substring(0, olderThanLimitString.length()-1));	break;	case 'h': case 'H': timeLimitMillis = System.currentTimeMillis() - 60*60*1000*Integer.parseInt(olderThanLimitString.substring(0, olderThanLimitString.length()-1));	break;	case 'm': case 'M': timeLimitMillis = System.currentTimeMillis() - 60*1000*Integer.parseInt(olderThanLimitString.substring(0, olderThanLimitString.length()-1));	break;	default: throw new IllegalArgumentException("Unsupported time-limit: " + olderThanLimitString);	}	
working on delegation tokens older than current time 

}	else {	if (opType == OpType.DELETE) {	throw new IllegalArgumentException("Attempting to delete tokens. " + "Specify deletion criteria (either expired or time-range).");	}	}	if (commandLine.hasOption("batchSize")) {	String batchSizeString = commandLine.getOptionValue("batchSize");	batchSize = Integer.parseInt(batchSizeString);	if (batchSize < 1) {	
invalid batch size resetting to defaults 

batchSize = Integer.parseInt(batchSizeString);	if (batchSize < 1) {	batchSize = BATCH_SIZE_DEFAULT;	}	LOG.info("Batch-size for drop == " + batchSize);	}	if (commandLine.hasOption("sleepTime")) {	String sleepTimeString = commandLine.getOptionValue("sleepTime");	sleepTimeMillis = 1000 * Integer.parseInt(sleepTimeString);	if (sleepTimeMillis <= 0) {	
invalid sleep time resetting to defaults 

batchSize = BATCH_SIZE_DEFAULT;	}	LOG.info("Batch-size for drop == " + batchSize);	}	if (commandLine.hasOption("sleepTime")) {	String sleepTimeString = commandLine.getOptionValue("sleepTime");	sleepTimeMillis = 1000 * Integer.parseInt(sleepTimeString);	if (sleepTimeMillis <= 0) {	sleepTimeMillis = SLEEP_TIME_MILLIS_DEFAULT;	}	
sleep between drop batches milliseconds 

private void doDelete() throws Exception {	int nDeletedTokens = 0;	List<DelegationTokenIdentifier> allDelegationTokenIDs = getAllDelegationTokenIDs();	for (DelegationTokenIdentifier tokenId : Iterables.filter(allDelegationTokenIDs, selectForDeletion)) {	if ((++nDeletedTokens % batchSize) == 0) {	
deleted sleeping for ms 

private void doDelete() throws Exception {	int nDeletedTokens = 0;	List<DelegationTokenIdentifier> allDelegationTokenIDs = getAllDelegationTokenIDs();	for (DelegationTokenIdentifier tokenId : Iterables.filter(allDelegationTokenIDs, selectForDeletion)) {	if ((++nDeletedTokens % batchSize) == 0) {	try {Thread.sleep(sleepTimeMillis); } catch (InterruptedException ignore) {}	}	
deleting token 

try {	readArgs(args);	init();	switch (opType) {	case LIST: doList(); break;	case DELETE: doDelete(); break;	}	return 0;	}	catch (Exception exception) {	
unexpected exception 

========================= hive sample_1868 =========================

public void run() {	
trying to delete old dump dirs 

public void run() {	try {	FileSystem fs = FileSystem.get(dumpRoot.toUri(), conf);	FileStatus[] statuses = fs.listStatus(dumpRoot);	for (FileStatus status : statuses) {	if (status.getModificationTime() < System.currentTimeMillis() - ttl) {	fs.delete(status.getPath(), true);	
deleted old dump dir 

public void run() {	try {	FileSystem fs = FileSystem.get(dumpRoot.toUri(), conf);	FileStatus[] statuses = fs.listStatus(dumpRoot);	for (FileStatus status : statuses) {	if (status.getModificationTime() < System.currentTimeMillis() - ttl) {	fs.delete(status.getPath(), true);	}	}	} catch (IOException e) {	
error while trying to delete dump dir 

========================= hive sample_5647 =========================

public SerDeLowLevelCacheImpl( LlapDaemonCacheMetrics metrics, LowLevelCachePolicy cachePolicy, Allocator allocator) {	this.cachePolicy = cachePolicy;	this.allocator = allocator;	this.cleanupInterval = DEFAULT_CLEANUP_INTERVAL;	this.metrics = metrics;	
serde low level level cache cleanup interval sec 

public FileData getFileData(Object fileKey, long start, long end, boolean[] includes, DiskRangeListFactory factory, LowLevelCacheCounters qfCounters, BooleanRef gotAllData) throws IOException {	FileCache<FileData> subCache = cache.get(fileKey);	if (subCache == null || !subCache.incRef()) {	if (LlapIoImpl.CACHE_LOGGER.isTraceEnabled()) {	
cannot find cache for in 

if (subCache == null || !subCache.incRef()) {	if (LlapIoImpl.CACHE_LOGGER.isTraceEnabled()) {	}	markAllAsMissed(start, end, qfCounters, gotAllData);	return null;	}	try {	FileData cached = subCache.getCache();	cached.rwLock.readLock().lock();	if (LlapIoImpl.CACHE_LOGGER.isTraceEnabled()) {	
cache for is 

markAllAsMissed(start, end, qfCounters, gotAllData);	return null;	}	try {	FileData cached = subCache.getCache();	cached.rwLock.readLock().lock();	if (LlapIoImpl.CACHE_LOGGER.isTraceEnabled()) {	}	try {	if (cached.stripes == null) {	
cannot find any stripes for 

}	if (includes.length > cached.colCount) {	throw new IOException("Includes " + DebugUtils.toString(includes) + " for " + cached.colCount + " columns");	}	FileData result = new FileData(cached.fileKey, cached.colCount);	if (gotAllData != null) {	gotAllData.value = true;	}	long origStart = start, origEnd = end;	int startIx = Integer.MIN_VALUE, endIx = Integer.MIN_VALUE;	
looking for data between and 

}	FileData result = new FileData(cached.fileKey, cached.colCount);	if (gotAllData != null) {	gotAllData.value = true;	}	long origStart = start, origEnd = end;	int startIx = Integer.MIN_VALUE, endIx = Integer.MIN_VALUE;	for (int i = 0; i < cached.stripes.size() && endIx == Integer.MIN_VALUE; ++i) {	StripeData si = cached.stripes.get(i);	if (LlapIoImpl.CACHE_LOGGER.isTraceEnabled()) {	
looking at 

end = si.lastEnd;	} else if (si.lastStart <= end) {	endIx = i + 1;	end = si.lastEnd;	} else {	endIx = i;	end = (endIx > 0) ? cached.stripes.get(endIx - 1).lastEnd : start;	}	}	}	
determined stripe indexes 

private void getCacheDataForOneSlice(int stripeIx, FileData cached, FileData result, BooleanRef gotAllData, boolean[] includes, LowLevelCacheCounters qfCounters) {	StripeData cStripe = cached.stripes.get(stripeIx);	if (LlapIoImpl.CACHE_LOGGER.isTraceEnabled()) {	
got stripe in cache 

}	stripe.encodings[colIx] = cStripe.encodings[colIx];	LlapSerDeDataBuffer[][] cColData = cStripe.data[colIx];	assert cColData != null;	for (int streamIx = 0;	cColData != null && streamIx < cColData.length; ++streamIx) {	LlapSerDeDataBuffer[] streamData = cColData[streamIx];	if (streamData == null) continue;	for (int i = 0; i < streamData.length; ++i) {	if (!lockBuffer(streamData[i], true)) {	
couldn t lock data for stripe at colix stream type 

public void putFileData(final FileData data, Priority priority, LowLevelCacheCounters qfCounters) {	if (data.stripes == null || data.stripes.isEmpty()) {	
trying to cache filedata with no data for 

try {	if (data != cached) {	cached.rwLock.writeLock().lock();	}	try {	for (StripeData si : data.stripes) {	lockAllBuffersForPut(si, priority);	}	if (data == cached) {	if (LlapIoImpl.CACHE_LOGGER.isTraceEnabled()) {	
cached new data 

try {	for (StripeData si : data.stripes) {	lockAllBuffersForPut(si, priority);	}	if (data == cached) {	if (LlapIoImpl.CACHE_LOGGER.isTraceEnabled()) {	}	return;	}	if (LlapIoImpl.CACHE_LOGGER.isTraceEnabled()) {	
merging old and new 

--lastIx;	continue;	}	boolean isCurOriginal = cached.stripes.contains(cur);	handleRemovedStripeInfo(combined.remove(isCurOriginal ? ix : ix + 1));	--ix;	--lastIx;	}	cached.stripes = combined;	if (LlapIoImpl.CACHE_LOGGER.isTraceEnabled()) {	
new cache data is 

private void mergeStripeInfos(StripeData to, StripeData from) {	if (LlapIoImpl.CACHE_LOGGER.isTraceEnabled()) {	
merging slices data old and new 

private void unlockBuffer(LlapSerDeDataBuffer buffer, boolean handleLastDecRef) {	boolean isLastDecref = (buffer.decRef() == 0);	if (handleLastDecRef && isLastDecref) {	if (buffer.isCached) {	cachePolicy.notifyUnlock(buffer);	} else {	if (LlapIoImpl.CACHE_LOGGER.isTraceEnabled()) {	
deallocating that was not cached 

========================= hive sample_2182 =========================

public MetaStoreDirectSql(PersistenceManager pm, Configuration conf, String schema) {	this.pm = pm;	this.schema = schema;	DatabaseProduct dbType = null;	try {	dbType = DatabaseProduct.determineDatabaseProduct(getProductName(pm));	} catch (SQLException e) {	
cannot determine database product assuming other 

try {	f.set(this, getFullyQualifiedName(schema, f.getName()));	} catch (IllegalArgumentException | IllegalAccessException e) {	throw new RuntimeException("Internal error, cannot set " + f.getName());	}	}	convertMapNullsToEmptyStrings = MetastoreConf.getBoolVar(conf, ConfVars.ORM_RETRIEVE_MAPNULLS_AS_EMPTY_STRINGS);	defaultPartName = MetastoreConf.getVar(conf, ConfVars.DEFAULTPARTITIONNAME);	String jdoIdFactory = MetastoreConf.getVar(conf, ConfVars.IDENTIFIER_FACTORY);	if (! ("datanucleus1".equalsIgnoreCase(jdoIdFactory))){	
underlying metastore does not use for its orm naming scheme disabling directsql as it uses hand hardcoded sql with that assumption 

}	}	convertMapNullsToEmptyStrings = MetastoreConf.getBoolVar(conf, ConfVars.ORM_RETRIEVE_MAPNULLS_AS_EMPTY_STRINGS);	defaultPartName = MetastoreConf.getVar(conf, ConfVars.DEFAULTPARTITIONNAME);	String jdoIdFactory = MetastoreConf.getVar(conf, ConfVars.IDENTIFIER_FACTORY);	if (! ("datanucleus1".equalsIgnoreCase(jdoIdFactory))){	isCompatibleDatastore = false;	} else {	isCompatibleDatastore = ensureDbInit() && runTestQuery();	if (isCompatibleDatastore) {	
using direct sql underlying db is 

static String getProductName(PersistenceManager pm) {	JDOConnection jdoConn = pm.getDataStoreConnection();	try {	return ((Connection)jdoConn.getNativeConnection()).getMetaData().getDatabaseProductName();	} catch (Throwable t) {	
error retrieving product name 

initQueries.add(pm.newQuery(MNotificationNextId.class, "nextEventId < -1"));	initQueries.add(pm.newQuery(MWMResourcePlan.class, "name == ''"));	Query q;	while ((q = initQueries.peekFirst()) != null) {	q.execute();	initQueries.pollFirst();	}	return true;	} catch (Exception ex) {	doCommit = false;	
database initialization failed direct sql is disabled 

}	Query query = null;	String selfTestQuery = "select \"DB_ID\" from " + DBS + "";	try {	prepareTxn();	query = pm.newQuery("javax.jdo.query.SQL", selfTestQuery);	query.execute();	return true;	} catch (Throwable t) {	doCommit = false;	
self test query failed direct sql is disabled 

public Database getDatabase(String dbName) throws MetaException{	Query queryDbSelector = null;	Query queryDbParams = null;	try {	dbName = dbName.toLowerCase();	String queryTextDbSelector= "select " + "\"DB_ID\", \"NAME\", \"DB_LOCATION_URI\", \"DESC\", " + "\"OWNER_NAME\", \"OWNER_TYPE\" " + "FROM "+ DBS +" where \"NAME\" = ? ";	Object[] params = new Object[] { dbName };	queryDbSelector = pm.newQuery("javax.jdo.query.SQL", queryTextDbSelector);	if (LOG.isTraceEnabled()) {	
getdatabase query instantiated with param 

assert(sqlResult.size() == 1);	if (sqlResult.get(0) == null) {	return null;	}	Object[] dbline = sqlResult.get(0);	Long dbid = extractSqlLong(dbline[0]);	String queryTextDbParams = "select \"PARAM_KEY\", \"PARAM_VALUE\" " + " from " + DATABASE_PARAMS + " " + " WHERE \"DB_ID\" = ? " + " AND \"PARAM_KEY\" IS NOT NULL";	params[0] = dbid;	queryDbParams = pm.newQuery("javax.jdo.query.SQL", queryTextDbParams);	if (LOG.isTraceEnabled()) {	
getdatabase instantiated with param 

}	Database db = new Database();	db.setName(extractSqlString(dbline[1]));	db.setLocationUri(extractSqlString(dbline[2]));	db.setDescription(extractSqlString(dbline[3]));	db.setOwnerName(extractSqlString(dbline[4]));	String type = extractSqlString(dbline[5]);	db.setOwnerType( (null == type || type.trim().isEmpty()) ? null : PrincipalType.valueOf(type));	db.setParameters(MetaStoreUtils.trimMapNulls(dbParams,convertMapNullsToEmptyStrings));	if (LOG.isDebugEnabled()){	
getdatabase directsql returning db locn desc owner ownertype 

private void timingTrace(boolean doTrace, String queryText, long start, long queryTime) {	if (!doTrace) return;	
direct sql query in ms ms the query is 

try {	return ((Blob) value).getBytes(1, (int) ((Blob) value).length());	} catch (SQLException e) {	throw new MetaException("Encounter error while processing blob.");	}	}	else if (value instanceof byte[]) {	return (byte[]) value;	}	else {	
expected blob type but got 

assert table != null;	if (tree == null) {	return null;	}	if (tree.getRoot() == null) {	return "";	}	PartitionFilterGenerator visitor = new PartitionFilterGenerator( table, params, joins, dbHasJoinCastBug, defaultPartName, dbType, schema);	tree.accept(visitor);	if (visitor.filterBuffer.hasError()) {	
unable to push down sql filter 

public AggrStats aggrColStatsForPartitions(String dbName, String tableName, List<String> partNames, List<String> colNames, boolean useDensityFunctionForNDVEstimation, double ndvTuner, boolean enableBitVector) throws MetaException {	if (colNames.isEmpty() || partNames.isEmpty()) {	
columns is empty or partnames is empty short circuiting stats eval 

public void closeAllQueries() {	for (Query q : queries) {	try {	q.closeAll();	} catch (Throwable t) {	
failed to close a query 

========================= hive sample_1891 =========================

private void executeCMD(String[] args, String input, int retCode) {	InputStream inputStream = null;	int ret = 0;	try {	if (input != null) {	inputStream = IOUtils.toInputStream(input);	}	ret = cli.runWithArgs(args, inputStream);	} catch (Throwable e) {	
failed to execute command due to the error 

InputStream inputStream = null;	int ret = 0;	try {	if (input != null) {	inputStream = IOUtils.toInputStream(input);	}	ret = cli.runWithArgs(args, inputStream);	} catch (Throwable e) {	} finally {	if (retCode != ret) {	
failed due to the error 

private File generateTmpFile(String context) {	File file = null;	BufferedWriter bw = null;	try {	file = File.createTempFile("test", ".sql");	bw = new BufferedWriter(new FileWriter(file));	bw.write(context);	} catch (IOException e) {	
failed to write tmp file due to the exception 

========================= hive sample_1468 =========================

public void handle(Context withinContext) throws Exception {	
dummy processing message 

========================= hive sample_3455 =========================

public static Set<String> getSQLCompleters(BeeLine beeLine, boolean skipmeta) throws IOException, SQLException {	Set<String> completions = new TreeSet<String>();	String keywords = new BufferedReader(new InputStreamReader( SQLCompleter.class.getResourceAsStream( "/sql-keywords.properties"))).readLine();	try {	keywords += "," + beeLine.getDatabaseConnection().getDatabaseMetaData().getSQLKeywords();	} catch (Exception e) {	
fail to get sql key words from database metadata due to the exception 

public static Set<String> getSQLCompleters(BeeLine beeLine, boolean skipmeta) throws IOException, SQLException {	Set<String> completions = new TreeSet<String>();	String keywords = new BufferedReader(new InputStreamReader( SQLCompleter.class.getResourceAsStream( "/sql-keywords.properties"))).readLine();	try {	keywords += "," + beeLine.getDatabaseConnection().getDatabaseMetaData().getSQLKeywords();	} catch (Exception e) {	}	try {	keywords += "," + beeLine.getDatabaseConnection().getDatabaseMetaData().getStringFunctions();	} catch (Exception e) {	
fail to get string function names from database metadata due to the exception 

keywords += "," + beeLine.getDatabaseConnection().getDatabaseMetaData().getSQLKeywords();	} catch (Exception e) {	}	try {	keywords += "," + beeLine.getDatabaseConnection().getDatabaseMetaData().getStringFunctions();	} catch (Exception e) {	}	try {	keywords += "," + beeLine.getDatabaseConnection().getDatabaseMetaData().getNumericFunctions();	} catch (Exception e) {	
fail to get numeric function names from database metadata due to the exception 

keywords += "," + beeLine.getDatabaseConnection().getDatabaseMetaData().getStringFunctions();	} catch (Exception e) {	}	try {	keywords += "," + beeLine.getDatabaseConnection().getDatabaseMetaData().getNumericFunctions();	} catch (Exception e) {	}	try {	keywords += "," + beeLine.getDatabaseConnection().getDatabaseMetaData().getSystemFunctions();	} catch (Exception e) {	
fail to get system function names from database metadata due to the exception 

keywords += "," + beeLine.getDatabaseConnection().getDatabaseMetaData().getNumericFunctions();	} catch (Exception e) {	}	try {	keywords += "," + beeLine.getDatabaseConnection().getDatabaseMetaData().getSystemFunctions();	} catch (Exception e) {	}	try {	keywords += "," + beeLine.getDatabaseConnection().getDatabaseMetaData().getTimeDateFunctions();	} catch (Exception e) {	
fail to get time date function names from database metadata due to the exception 

========================= hive sample_1487 =========================

templateVars.put("patchFile", mPatchFile.getAbsolutePath());	templateVars.put("jiraUrl", conf.getJiraUrl());	templateVars.put("jiraUser", conf.getJiraUser());	templateVars.put("jiraPass", conf.getJiraPassword());	templateVars.put("outputDir", mOutputDir.getAbsolutePath());	templateVars.put("buildUrl", buildUrl);	templateVars.put("buildUrlLog", YETUS_LOG_FILE);	templateVars.put("buildUrlOutputDir", YETUS_OUTPUT_FOLDER);	templateVars.put("logFile", mLogFile.getAbsolutePath());	try {	
writing from template 

templateVars.put("outputDir", mOutputDir.getAbsolutePath());	templateVars.put("buildUrl", buildUrl);	templateVars.put("buildUrlLog", YETUS_LOG_FILE);	templateVars.put("buildUrlOutputDir", YETUS_OUTPUT_FOLDER);	templateVars.put("logFile", mLogFile.getAbsolutePath());	try {	Templates.writeTemplateResult(YETUS_EXEC_VM, yetusExecScript, templateVars);	Process proc = new ProcessBuilder().command("bash", yetusExecScript.getPath()).start();	int exitCode = proc.waitFor();	if (exitCode == 0) {	
finished processing yetus check successfully 

templateVars.put("buildUrlLog", YETUS_LOG_FILE);	templateVars.put("buildUrlOutputDir", YETUS_OUTPUT_FOLDER);	templateVars.put("logFile", mLogFile.getAbsolutePath());	try {	Templates.writeTemplateResult(YETUS_EXEC_VM, yetusExecScript, templateVars);	Process proc = new ProcessBuilder().command("bash", yetusExecScript.getPath()).start();	int exitCode = proc.waitFor();	if (exitCode == 0) {	}	} catch (Exception e) {	
error processing yetus check 

templateVars.put("buildUrlOutputDir", YETUS_OUTPUT_FOLDER);	templateVars.put("logFile", mLogFile.getAbsolutePath());	try {	Templates.writeTemplateResult(YETUS_EXEC_VM, yetusExecScript, templateVars);	Process proc = new ProcessBuilder().command("bash", yetusExecScript.getPath()).start();	int exitCode = proc.waitFor();	if (exitCode == 0) {	}	} catch (Exception e) {	} finally {	
deleting 

templateVars.put("buildUrlOutputDir", YETUS_OUTPUT_FOLDER);	templateVars.put("logFile", mLogFile.getAbsolutePath());	try {	Templates.writeTemplateResult(YETUS_EXEC_VM, yetusExecScript, templateVars);	Process proc = new ProcessBuilder().command("bash", yetusExecScript.getPath()).start();	int exitCode = proc.waitFor();	if (exitCode == 0) {	}	} catch (Exception e) {	} finally {	
deleting 

Process proc = new ProcessBuilder().command("bash", yetusExecScript.getPath()).start();	int exitCode = proc.waitFor();	if (exitCode == 0) {	}	} catch (Exception e) {	} finally {	}	}	});	t.start();	
started yetus check 

private boolean checkDependencies(){	if (mPatchFile == null || !mPatchFile.canRead()) {	
cannot run yetus check patch file is null or not readable 

private boolean checkDependencies(){	if (mPatchFile == null || !mPatchFile.canRead()) {	return false;	}	if (!((mWorkingDir.isDirectory() && mWorkingDir.canWrite()) && (mOutputDir.isDirectory() && mOutputDir.canWrite()))) {	
cannot run yetus check output directories not present and writable workingdir s outputdir s 

private boolean checkDependencies(){	if (mPatchFile == null || !mPatchFile.canRead()) {	return false;	}	if (!((mWorkingDir.isDirectory() && mWorkingDir.canWrite()) && (mOutputDir.isDirectory() && mOutputDir.canWrite()))) {	return false;	}	if (Strings.isNullOrEmpty(conf.getJiraUrl()) || Strings.isNullOrEmpty(conf.getJiraName()) || Strings.isNullOrEmpty(conf.getJiraPassword()) || Strings.isNullOrEmpty(conf.getJiraUser())) {	
cannot run yetus check credentials for jira not provided 

========================= hive sample_5610 =========================

public void release(String key) {	
releasing key 

public <T> T retrieve(String key) throws HiveException {	T value = null;	try {	value = (T) registry.get(key);	if ( value != null) {	
found in cache with value 

public <T> T retrieve(String key, Callable<T> fn) throws HiveException {	T value;	try {	value = (T) registry.get(key);	if (value == null) {	value = fn.call();	
caching key 

public <T> T retrieve(String key, Callable<T> fn) throws HiveException {	T value;	try {	value = (T) registry.get(key);	if (value == null) {	value = fn.call();	registry.cacheForVertex(key, value);	} else {	
found in cache with value 

public void remove(String key) {	
removing key 

========================= hive sample_4001 =========================

public static URI getValidatedURI(HiveConf conf, String dcPath) throws SemanticException {	try {	boolean testMode = conf.getBoolVar(HiveConf.ConfVars.HIVETESTMODE) || conf.getBoolVar(HiveConf.ConfVars.HIVEEXIMTESTMODE);	URI uri = new Path(dcPath).toUri();	FileSystem fs = FileSystem.get(uri, conf);	String scheme = fs.getScheme();	String authority = uri.getAuthority();	String path = uri.getPath();	
path before norm 

if (testMode) {	path = (new Path(System.getProperty("test.tmp.dir"), path)).toUri().getPath();	} else {	path = (new Path(new Path("/user/" + System.getProperty("user.name")), path)).toUri() .getPath();	}	}	if (StringUtils.isEmpty(authority)) {	URI defaultURI = FileSystem.get(conf).getUri();	authority = defaultURI.getAuthority();	}	
scheme authority path 

========================= hive sample_3394 =========================

public PhysicalContext resolve(PhysicalContext pctx) throws SemanticException {	HiveConf conf = pctx.getConf();	LlapMode mode = LlapMode.valueOf(HiveConf.getVar(conf, HiveConf.ConfVars.LLAP_EXECUTION_MODE));	if (mode == none) {	
llap disabled 

========================= hive sample_2993 =========================

private void setUpMetastore() throws Exception {	if (hiveConf == null) hiveConf = new HiveConf(this.getClass());	hiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");	hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");	hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");	hiveConf.set(HiveConf.ConfVars.METASTORECONNECTURLKEY.varname, "jdbc:derby:" + new File(workDir + "/metastore_db") + ";create=true");	hiveConf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.toString(), new File(workDir, "warehouse").toString());	File derbyLogFile = new File(workDir + "/derby.log");	derbyLogFile.createNewFile();	
derby stream error file 

========================= hive sample_221 =========================

public void run(HookContext hookContext) throws Exception {	assert (hookContext.getHookType() == HookContext.HookType.POST_EXEC_HOOK);	HiveConf conf = hookContext.getConf();	if (!"tez".equals(HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE))) {	return;	}	
executing post execution hook to print tez summary 

return;	}	SessionState ss = SessionState.get();	SessionState.LogHelper console = ss.getConsole();	QueryPlan plan = hookContext.getQueryPlan();	if (plan == null) {	return;	}	List<TezTask> rootTasks = Utilities.getTezTasks(plan.getRootTasks());	for (TezTask tezTask : rootTasks) {	
printing summary for tez task 

========================= hive sample_3814 =========================

private void evaluateWork(BaseWork w) throws SemanticException {	if (w instanceof MapWork) {	evaluateMapWork((MapWork) w);	} else if (w instanceof ReduceWork) {	evaluateReduceWork((ReduceWork) w);	} else if (w instanceof MergeJoinWork) {	evaluateMergeWork((MergeJoinWork) w);	} else {	
we are not going to evaluate this work type 

});	disp = new DefaultRuleDispatcher(null, rules, null);	GraphWalker ogw = new DefaultGraphWalker(disp);	ArrayList<Node> topNodes = new ArrayList<Node>();	topNodes.addAll(w.getAllRootOperators());	LinkedHashMap<Node, Object> nodeOutput = new LinkedHashMap<Node, Object>();	ogw.startWalking(topNodes, nodeOutput);	for (TableScanOperator ts: tableScans) {	if (ts.getConf() != null && ts.getConf().getFilterExpr() != null) {	if (LOG.isDebugEnabled()) {	
serializing 

LinkedHashMap<Node, Object> nodeOutput = new LinkedHashMap<Node, Object>();	ogw.startWalking(topNodes, nodeOutput);	for (TableScanOperator ts: tableScans) {	if (ts.getConf() != null && ts.getConf().getFilterExpr() != null) {	if (LOG.isDebugEnabled()) {	}	ts.getConf().setSerializedFilterExpr( SerializationUtilities.serializeExpression(ts.getConf().getFilterExpr()));	}	if (ts.getConf() != null && ts.getConf().getFilterObject() != null) {	if (LOG.isDebugEnabled()) {	
serializing 

========================= hive sample_2998 =========================

private FakeSecretManager rollKey(FakeSecretManager fsm, int idToPreserve) throws IOException {	DelegationKey dk = fsm.getDelegationKey(idToPreserve), curDk = fsm.getCurrentKey();	if (curDk.getKeyId() != idToPreserve) {	
the current key is not the one we expect key rolled in background signed with but got 

========================= hive sample_2131 =========================

return false;	}	if (keys.size() != oldKeySize) {	return false;	}	if (joinKeyOrder == null) {	joinKeyOrder = new Integer[keys.size()];	}	Table tbl = tso.getConf().getTableMetadata();	if (AcidUtils.isInsertOnlyTable(tbl.getParameters())) {	
no bucketed join on mm table 

========================= hive sample_3059 =========================

listBeans(jg, new ObjectName(qry), null, response);	} finally {	if (jg != null) {	jg.close();	}	if (writer != null) {	writer.close();	}	}	} catch (IOException e) {	
caught an exception while processing jmx request 

if (jg != null) {	jg.close();	}	if (writer != null) {	writer.close();	}	}	} catch (IOException e) {	response.setStatus(HttpServletResponse.SC_INTERNAL_SERVER_ERROR);	} catch (MalformedObjectNameException e) {	
caught an exception while processing jmx request 

private void listBeans(JsonGenerator jg, ObjectName qry, String attribute, HttpServletResponse response) throws IOException {	
listing beans for 

try {	if ("org.apache.commons.modeler.BaseModelMBean".equals(code)) {	prs = "modelerType";	code = (String) mBeanServer.getAttribute(oname, prs);	}	if (attribute!=null) {	prs = attribute;	attributeinfo = mBeanServer.getAttribute(oname, prs);	}	} catch (AttributeNotFoundException e) {	
getting attribute of threw an exception 

if ("org.apache.commons.modeler.BaseModelMBean".equals(code)) {	prs = "modelerType";	code = (String) mBeanServer.getAttribute(oname, prs);	}	if (attribute!=null) {	prs = attribute;	attributeinfo = mBeanServer.getAttribute(oname, prs);	}	} catch (AttributeNotFoundException e) {	} catch (MBeanException e) {	
getting attribute of threw an exception 

prs = "modelerType";	code = (String) mBeanServer.getAttribute(oname, prs);	}	if (attribute!=null) {	prs = attribute;	attributeinfo = mBeanServer.getAttribute(oname, prs);	}	} catch (AttributeNotFoundException e) {	} catch (MBeanException e) {	} catch (RuntimeException e) {	
getting attribute of threw an exception 

code = (String) mBeanServer.getAttribute(oname, prs);	}	if (attribute!=null) {	prs = attribute;	attributeinfo = mBeanServer.getAttribute(oname, prs);	}	} catch (AttributeNotFoundException e) {	} catch (MBeanException e) {	} catch (RuntimeException e) {	} catch ( ReflectionException e ) {	
getting attribute of threw an exception 

attributeinfo = mBeanServer.getAttribute(oname, prs);	}	} catch (AttributeNotFoundException e) {	} catch (MBeanException e) {	} catch (RuntimeException e) {	} catch ( ReflectionException e ) {	}	} catch (InstanceNotFoundException e) {	continue;	} catch ( IntrospectionException e ) {	
problem while trying to process jmx query with mbean 

} catch (AttributeNotFoundException e) {	} catch (MBeanException e) {	} catch (RuntimeException e) {	} catch ( ReflectionException e ) {	}	} catch (InstanceNotFoundException e) {	continue;	} catch ( IntrospectionException e ) {	continue;	} catch ( ReflectionException e ) {	
problem while trying to process jmx query with mbean 

return;	}	if (attName.indexOf("=") >= 0 || attName.indexOf(":") >= 0 || attName.indexOf(" ") >= 0) {	return;	}	Object value = null;	try {	value = mBeanServer.getAttribute(oname, attName);	} catch (RuntimeMBeanException e) {	if (e.getCause() instanceof UnsupportedOperationException) {	
getting attribute of threw an exception 

}	if (attName.indexOf("=") >= 0 || attName.indexOf(":") >= 0 || attName.indexOf(" ") >= 0) {	return;	}	Object value = null;	try {	value = mBeanServer.getAttribute(oname, attName);	} catch (RuntimeMBeanException e) {	if (e.getCause() instanceof UnsupportedOperationException) {	} else {	
getting attribute of threw an exception 

}	Object value = null;	try {	value = mBeanServer.getAttribute(oname, attName);	} catch (RuntimeMBeanException e) {	if (e.getCause() instanceof UnsupportedOperationException) {	} else {	}	return;	} catch (RuntimeErrorException e) {	
getting attribute of threw an exception 

} catch (RuntimeMBeanException e) {	if (e.getCause() instanceof UnsupportedOperationException) {	} else {	}	return;	} catch (RuntimeErrorException e) {	return;	} catch (AttributeNotFoundException e) {	return;	} catch (MBeanException e) {	
getting attribute of threw an exception 

} else {	}	return;	} catch (RuntimeErrorException e) {	return;	} catch (AttributeNotFoundException e) {	return;	} catch (MBeanException e) {	return;	} catch (RuntimeException e) {	
getting attribute of threw an exception 

return;	} catch (RuntimeErrorException e) {	return;	} catch (AttributeNotFoundException e) {	return;	} catch (MBeanException e) {	return;	} catch (RuntimeException e) {	return;	} catch (ReflectionException e) {	
getting attribute of threw an exception 

========================= hive sample_1434 =========================

public void init(ByteArrayRef bytes, int start, int length) {	String s = null;	if (!LazyUtils.isDateMaybe(bytes.getData(), start, length)) {	isNull = true;	return;	}	try {	s = new String(bytes.getData(), start, length, "US-ASCII");	} catch (UnsupportedEncodingException e) {	
unsupported encoding found 

========================= hive sample_5477 =========================

public RpcServer(Map<String, String> mapConf) throws IOException, InterruptedException {	this.config = new RpcConfiguration(mapConf);	this.group = new NioEventLoopGroup( this.config.getRpcThreadCount(), new ThreadFactoryBuilder() .setNameFormat("RPC-Handler-%d") .setDaemon(true) .build());	ServerBootstrap serverBootstrap = new ServerBootstrap() .group(group) .channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializer<SocketChannel>() {	public void initChannel(SocketChannel ch) throws Exception {	SaslServerHandler saslHandler = new SaslServerHandler(config);	final Rpc newRpc = Rpc.createServer(saslHandler, config, ch, group);	saslHandler.rpc = newRpc;	Runnable cancelTask = new Runnable() {	public void run() {	
timed out waiting for hello from client 

========================= hive sample_553 =========================

public Object deserialize(Writable blob) throws SerDeException {	if (inputPattern == null) {	throw new SerDeException( "This table does not have serde property \"input.regex\"!");	}	Text rowText = (Text) blob;	Matcher m = inputPattern.matcher(rowText.toString());	if (!m.matches()) {	unmatchedRows++;	if (unmatchedRows >= nextUnmatchedRows) {	nextUnmatchedRows = getNextNumberToDisplay(nextUnmatchedRows);	
unmatched rows are found 

}	return null;	}	for (int c = 0; c < numColumns; c++) {	try {	row.set(c, m.group(c + 1));	} catch (RuntimeException e) {	partialMatchedRows++;	if (partialMatchedRows >= nextPartialMatchedRows) {	nextPartialMatchedRows = getNextNumberToDisplay(nextPartialMatchedRows);	
partially unmatched rows are found cannot find group 

========================= hive sample_1121 =========================

public GenericColumnVectorProducer(SerDeLowLevelCacheImpl serdeCache, BufferUsageManager bufferManager, Configuration conf, LlapDaemonCacheMetrics cacheMetrics, LlapDaemonIOMetrics ioMetrics, FixedSizedObjectPool<IoTrace> tracePool) {	
initializing orc column vector producer 

SerDeFileMetadata fm;	try {	fm = new SerDeFileMetadata(sourceSerDe);	} catch (SerDeException e) {	throw new IOException(e);	}	edc.setFileMetadata(fm);	SerDeEncodedDataReader reader = new SerDeEncodedDataReader(cache, bufferManager, conf, split, columnIds, edc, job, reporter, sourceInputFormat, sourceSerDe, counters, fm.getSchema(), parts);	edc.init(reader, reader, new IoTrace(0, false));	if (LlapIoImpl.LOG.isDebugEnabled()) {	
ignoring schema 

========================= hive sample_2189 =========================

public float getProgress() {	try {	return baseRecordReader.getProgress();	} catch (IOException e) {	
exception in hcatrecord reader 

========================= hive sample_768 =========================

String colType = null;	String colName = null;	boolean doAllPartitionContainStats = partNames.size() == colStatsWithSourceInfo.size();	NumDistinctValueEstimator ndvEstimator = null;	for (ColStatsObjWithSourceInfo csp : colStatsWithSourceInfo) {	ColumnStatisticsObj cso = csp.getColStatsObj();	if (statsObj == null) {	colName = cso.getColName();	colType = cso.getColType();	statsObj = ColumnStatsAggregatorFactory.newColumnStaticsObj(colName, colType, cso.getStatsData().getSetField());	
doallpartitioncontainstats for column is 

} else if (estimation > higherBound) {	estimation = higherBound;	}	} else {	estimation = (long) (lowerBound + (higherBound - lowerBound) * ndvTuner);	}	aggregateData.setNumDVs(estimation);	}	columnStatisticsData.setLongStats(aggregateData);	} else {	
start extrapolation for 

ColumnStatisticsData csd = new ColumnStatisticsData();	csd.setLongStats(aggregateData);	adjustedStatsMap.put(pseudoPartName.toString(), csd);	if (useDensityFunctionForNDVEstimation) {	densityAvgSum += (aggregateData.getHighValue() - aggregateData.getLowValue()) / aggregateData.getNumDVs();	}	}	}	extrapolate(columnStatisticsData, partNames.size(), colStatsWithSourceInfo.size(), adjustedIndexMap, adjustedStatsMap, densityAvgSum / adjustedStatsMap.size());	}	
ndv estimatation for is of partitions requested of partitions found 

========================= hive sample_1930 =========================

public void delete() throws IOException {	try {	storage.delete(type, id);	} catch (Exception e) {	
couldn t delete 

public static TempletonStorage getStorageInstance(Configuration conf) {	TempletonStorage storage = null;	try {	storage = (TempletonStorage) JavaUtils.loadClass(conf.get(TempletonStorage.STORAGE_CLASS)) .newInstance();	} catch (Exception e) {	
no storage method found 

public static TempletonStorage getStorageInstance(Configuration conf) {	TempletonStorage storage = null;	try {	storage = (TempletonStorage) JavaUtils.loadClass(conf.get(TempletonStorage.STORAGE_CLASS)) .newInstance();	} catch (Exception e) {	try {	storage = new HDFSStorage();	} catch (Exception ex) {	
couldn t create storage 

public Long getLongField(String name) throws IOException {	String s = storage.getField(type, id, name);	if (s == null) return null;	else {	try {	return new Long(s);	} catch (NumberFormatException e) {	
templeton bug 

========================= hive sample_857 =========================

double v = PrimitiveObjectInspectorUtils.getDouble(p, inputOI);	myagg.count++;	myagg.sum += v;	if(myagg.count > 1) {	myagg.variance = calculateIntermediate( myagg.count, myagg.sum, v, myagg.variance);	}	} catch (NumberFormatException e) {	if (!warned) {	warned = true;	LOG.warn(getClass().getSimpleName() + " " + StringUtils.stringifyException(e));	
ignoring similar exceptions 

========================= hive sample_4948 =========================

}	if (this.getPartCols() != null) {	Iterator<FieldSchema> partColsIter = this.getPartCols().iterator();	while (partColsIter.hasNext()) {	FieldSchema fs = partColsIter.next();	String partCol = fs.getName();	TypeInfo pti = null;	try {	pti = TypeInfoFactory.getPrimitiveTypeInfo(fs.getType());	} catch (Exception err) {	
failed to get type info 

tbl.setNumBuckets(getNumBuckets());	}	if (getStorageHandler() != null) {	tbl.setProperty( org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_STORAGE, getStorageHandler());	}	HiveStorageHandler storageHandler = tbl.getStorageHandler();	String serDeClassName;	if (getSerName() == null) {	if (storageHandler == null) {	serDeClassName = PlanUtils.getDefaultSerDe().getName();	
default to for table 

if (getStorageHandler() != null) {	tbl.setProperty( org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_STORAGE, getStorageHandler());	}	HiveStorageHandler storageHandler = tbl.getStorageHandler();	String serDeClassName;	if (getSerName() == null) {	if (storageHandler == null) {	serDeClassName = PlanUtils.getDefaultSerDe().getName();	} else {	serDeClassName = storageHandler.getSerDeClass().getName();	
use storagehandler supplied for table 

========================= hive sample_3274 =========================

public void run() {	if (heartbeatExecutorService != null && !heartbeatExecutorService.isShutdown() && !heartbeatExecutorService.isTerminated()) {	
shutting down heartbeater thread pool 

public void commitTxn() throws LockException {	if (!isTxnOpen()) {	throw new RuntimeException("Attempt to commit before opening a transaction");	}	try {	lockMgr.clearLocalLockRecords();	stopHeartbeat();	
committing txn 

public void commitTxn() throws LockException {	if (!isTxnOpen()) {	throw new RuntimeException("Attempt to commit before opening a transaction");	}	try {	lockMgr.clearLocalLockRecords();	stopHeartbeat();	getMS().commitTxn(txnId);	} catch (NoSuchTxnException e) {	
metastore could not find 

public void rollbackTxn() throws LockException {	if (!isTxnOpen()) {	throw new RuntimeException("Attempt to rollback before opening a transaction");	}	try {	lockMgr.clearLocalLockRecords();	stopHeartbeat();	
rolling back 

public void rollbackTxn() throws LockException {	if (!isTxnOpen()) {	throw new RuntimeException("Attempt to rollback before opening a transaction");	}	try {	lockMgr.clearLocalLockRecords();	stopHeartbeat();	getMS().rollbackTxn(txnId);	} catch (NoSuchTxnException e) {	
metastore could not find 

}	if(LOG.isInfoEnabled()) {	StringBuilder sb = new StringBuilder("Sending heartbeat for ") .append(JavaUtils.txnIdToString(txnId)).append(" and");	for(HiveLock lock : locks) {	sb.append(" ").append(lock.toString());	}	LOG.info(sb.toString());	}	if(!isTxnOpen() && locks.isEmpty()) {	if (LOG.isDebugEnabled()) {	
no need to send heartbeat as there is no transaction and no locks 

if(!isTxnOpen() && locks.isEmpty()) {	if (LOG.isDebugEnabled()) {	}	return;	}	for (HiveLock lock : locks) {	long lockId = ((DbLockManager.DbHiveLock)lock).lockId;	try {	getMS().heartbeat(txnId, lockId);	} catch (NoSuchLockException e) {	
unable to find lock 

}	return;	}	for (HiveLock lock : locks) {	long lockId = ((DbLockManager.DbHiveLock)lock).lockId;	try {	getMS().heartbeat(txnId, lockId);	} catch (NoSuchLockException e) {	throw new LockException(e, ErrorMsg.LOCK_NO_SUCH_LOCK, JavaUtils.lockIdToString(lockId));	} catch (NoSuchTxnException e) {	
unable to find transaction 

private void stopHeartbeat() throws LockException {	if (heartbeatTask != null) {	heartbeatTask.cancel(true);	long startTime = System.currentTimeMillis();	long sleepInterval = 100;	while (!heartbeatTask.isCancelled() && !heartbeatTask.isDone()) {	long now = System.currentTimeMillis();	if (now - startTime > 30000) {	
heartbeat task cannot be cancelled for unknown reason queryid 

if (now - startTime > 30000) {	break;	}	try {	Thread.sleep(sleepInterval);	} catch (InterruptedException e) {	}	sleepInterval *= 2;	}	if (heartbeatTask.isCancelled() || heartbeatTask.isDone()) {	
stopped heartbeat for query 

protected void destruct() {	try {	stopHeartbeat();	if (shutdownRunner != null) {	ShutdownHookManager.removeShutdownHook(shutdownRunner);	}	if (isTxnOpen()) rollbackTxn();	if (lockMgr != null) lockMgr.close();	} catch (Exception e) {	
caught exception with message swallowing as there is nothing we can do with it 

public void run() {	try {	if(conf.getBoolVar(HiveConf.ConfVars.HIVE_IN_TEST) && conf.getBoolVar(HiveConf.ConfVars.HIVETESTMODEFAILHEARTBEATER)) {	throw new LockException(HiveConf.ConfVars.HIVETESTMODEFAILHEARTBEATER.name() + "=true");	}	
heartbeating for currentuser 

========================= hive sample_3761 =========================

public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx, Object... nodeOutputs) throws SemanticException {	
processing for 

public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx, Object... nodeOutputs) throws SemanticException {	
processing for 

public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx, Object... nodeOutputs) throws SemanticException {	
processing for 

public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx, Object... nodeOutputs) throws SemanticException {	
processing for 

public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx, Object... nodeOutputs) throws SemanticException {	
processing for 

public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx, Object... nodeOutputs) throws SemanticException {	
processing for 

Utilities.setColumnTypeList(jobConf, tableScanOp);	try {	Utilities.copyTableJobPropertiesToConf( Utilities.getTableDesc(tbl), jobConf);	} catch (Exception e) {	throw new SemanticException(e);	}	Deserializer deserializer = tbl.getDeserializer();	HiveStoragePredicateHandler.DecomposedPredicate decomposed = predicateHandler.decomposePredicate( jobConf, deserializer, originalPredicate);	if (decomposed == null) {	if (LOG.isDebugEnabled()) {	
no pushdown possible for predicate 

throw new SemanticException(e);	}	Deserializer deserializer = tbl.getDeserializer();	HiveStoragePredicateHandler.DecomposedPredicate decomposed = predicateHandler.decomposePredicate( jobConf, deserializer, originalPredicate);	if (decomposed == null) {	if (LOG.isDebugEnabled()) {	}	return originalPredicate;	}	if (LOG.isDebugEnabled()) {	
original predicate 

}	Deserializer deserializer = tbl.getDeserializer();	HiveStoragePredicateHandler.DecomposedPredicate decomposed = predicateHandler.decomposePredicate( jobConf, deserializer, originalPredicate);	if (decomposed == null) {	if (LOG.isDebugEnabled()) {	}	return originalPredicate;	}	if (LOG.isDebugEnabled()) {	if (decomposed.pushedPredicate != null) {	
pushed predicate 

HiveStoragePredicateHandler.DecomposedPredicate decomposed = predicateHandler.decomposePredicate( jobConf, deserializer, originalPredicate);	if (decomposed == null) {	if (LOG.isDebugEnabled()) {	}	return originalPredicate;	}	if (LOG.isDebugEnabled()) {	if (decomposed.pushedPredicate != null) {	}	if (decomposed.residualPredicate != null) {	
residual predicate 

========================= hive sample_3389 =========================

private void allocateAndUseBuffer(BuddyAllocator a, MemoryBuffer[][] allocs, long[][] testValues, int allocCount, int index, int sizeLog2) throws Exception {	allocs[index] = new MemoryBuffer[allocCount];	testValues[index] = new long[allocCount];	int size = (1 << sizeLog2) - 1;	try {	a.allocateMultiple(allocs[index], size);	} catch (AllocatorOutOfMemoryException ex) {	
failed to allocate of 

========================= hive sample_2127 =========================

public VectorPTFOperator(CompilationOpContext ctx, OperatorDesc conf, VectorizationContext vContext, VectorDesc vectorDesc) throws HiveException {	this(ctx);	
vectorptf constructor 

========================= hive sample_4509 =========================

static void processProxyuserConfig(AppConfig conf) {	for(Map.Entry<String, String> confEnt : conf) {	if(confEnt.getKey().startsWith(CONF_PROXYUSER_PREFIX) && confEnt.getKey().endsWith(CONF_GROUPS_SUFFIX)) {	String proxyUser = confEnt.getKey().substring(CONF_PROXYUSER_PREFIX.length(), confEnt.getKey().lastIndexOf(CONF_GROUPS_SUFFIX));	Set<String> groups;	if("*".equals(confEnt.getValue())) {	groups = WILD_CARD;	if(LOG.isDebugEnabled()) {	
user is authorized to do doas any user 

String proxyUser = confEnt.getKey().substring(CONF_PROXYUSER_PREFIX.length(), confEnt.getKey().lastIndexOf(CONF_GROUPS_SUFFIX));	Set<String> groups;	if("*".equals(confEnt.getValue())) {	groups = WILD_CARD;	if(LOG.isDebugEnabled()) {	}	}	else if(confEnt.getValue() != null && confEnt.getValue().trim().length() > 0) {	groups = new HashSet<String>(Arrays.asList(confEnt.getValue().trim().split(",")));	if(LOG.isDebugEnabled()) {	
user is authorized to do doas for users in the following groups 

}	}	else if(confEnt.getValue() != null && confEnt.getValue().trim().length() > 0) {	groups = new HashSet<String>(Arrays.asList(confEnt.getValue().trim().split(",")));	if(LOG.isDebugEnabled()) {	}	}	else {	groups = Collections.emptySet();	if(LOG.isDebugEnabled()) {	
user is authorized to do doas for users in the following groups 

}	}	proxyUserGroups.put(proxyUser, groups);	}	else if(confEnt.getKey().startsWith(CONF_PROXYUSER_PREFIX) && confEnt.getKey().endsWith(CONF_HOSTS_SUFFIX)) {	String proxyUser = confEnt.getKey().substring(CONF_PROXYUSER_PREFIX.length(), confEnt.getKey().lastIndexOf(CONF_HOSTS_SUFFIX));	Set<String> hosts;	if("*".equals(confEnt.getValue())) {	hosts = WILD_CARD;	if(LOG.isDebugEnabled()) {	
user is authorized to do doas from any host 

else if(confEnt.getValue() != null && confEnt.getValue().trim().length() > 0) {	String[] hostValues = confEnt.getValue().trim().split(",");	hosts = new HashSet<String>();	for(String hostname : hostValues) {	String nhn = normalizeHostname(hostname);	if(nhn != null) {	hosts.add(nhn);	}	}	if(LOG.isDebugEnabled()) {	
user is authorized to do doas from the following hosts 

if(nhn != null) {	hosts.add(nhn);	}	}	if(LOG.isDebugEnabled()) {	}	}	else {	hosts = Collections.emptySet();	if(LOG.isDebugEnabled()) {	
user is authorized to do doas from the following hosts 

static void validate(String proxyUser, String proxyHost, String doAsUser) throws NotAuthorizedException {	assertNotEmpty(proxyUser, "proxyUser", "If you're attempting to use user-impersonation via a proxy user, please make sure that " + CONF_PROXYUSER_PREFIX + "#USER#" + CONF_HOSTS_SUFFIX + " and " + CONF_PROXYUSER_PREFIX + "#USER#" + CONF_GROUPS_SUFFIX + " are configured correctly");	assertNotEmpty(proxyHost, "proxyHost", "If you're attempting to use user-impersonation via a proxy user, please make sure that " + CONF_PROXYUSER_PREFIX + proxyUser + CONF_HOSTS_SUFFIX + " and " + CONF_PROXYUSER_PREFIX + proxyUser + CONF_GROUPS_SUFFIX + " are configured correctly");	assertNotEmpty(doAsUser, Server.DO_AS_PARAM);	
authorization check proxyuser host doas 

Groups groupsInfo = new Groups(Main.getAppConfigInstance());	try {	List<String> userGroups = groupsInfo.getGroups(doAsUser);	for (String g : validGroups) {	if (userGroups.contains(g)) {	return;	}	}	}	catch (IOException ex) {	
unable to get list of groups for doasuser 

private static String normalizeHostname(String name) {	try {	InetAddress address = InetAddress.getByName( "localhost".equalsIgnoreCase(name) ? null : name);	return address.getCanonicalHostName();	}	catch (UnknownHostException ex) {	
unable to normalize hostname 

========================= hive sample_816 =========================

public boolean hasNext() {	try {	return rs.next();	}	catch (Exception se) {	
hasnext threw exception 

int numColumns = metadata.getColumnCount();	Map<String, Object> record = new HashMap<String, Object>(numColumns);	for (int i = 0; i < numColumns; i++) {	String key = metadata.getColumnName(i + 1);	Object value = rs.getObject(i + 1);	record.put(key, value);	}	return record;	}	catch (Exception e) {	
next threw exception 

public void close() {	try {	rs.close();	ps.close();	conn.close();	}	catch (Exception e) {	
caught exception while trying to close database objects 

========================= hive sample_1137 =========================

if (fsType == FsType.local) {	fs = FileSystem.getLocal(conf);	} else if (fsType == FsType.hdfs || fsType == FsType.encrypted_hdfs) {	int numDataNodes = 4;	if (fsType == FsType.encrypted_hdfs) {	conf.set(SECURITY_KEY_PROVIDER_URI_NAME, getKeyProviderURI());	conf.setInt("fs.trash.interval", 50);	dfs = shims.getMiniDfs(conf, numDataNodes, true, null);	fs = dfs.getFileSystem();	hes = shims.createHdfsEncryptionShim(fs, conf);	
key provider is initialized 

}	if (druidCluster != null) {	druidCluster.stop();	druidCluster = null;	}	setup.tearDown();	if (sparkSession != null) {	try {	SparkSessionManagerImpl.getInstance().closeSession(sparkSession);	} catch (Exception ex) {	
error closing spark session 

public void clearKeysCreatedInTests() {	if (hes == null) {	return;	}	try {	for (String keyAlias : hes.getKeys()) {	hes.deleteKey(keyAlias);	}	} catch (IOException e) {	
fail to clean the keys created in test due to the error 

conf.set("hive.metastore.filter.hook", "org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl");	db = Hive.get(conf);	for (String dbName : db.getAllDatabases()) {	SessionState.get().setCurrentDatabase(dbName);	for (String tblName : db.getAllTables()) {	if (!DEFAULT_DATABASE_NAME.equals(dbName) || !srcTables.contains(tblName)) {	Table tblObj = null;	try {	tblObj = db.getTable(tblName);	} catch (InvalidTableException e) {	
trying to drop table but it does not exist 

}	if (System.getenv(QTEST_LEAVE_FILES) != null) {	return;	}	clearTablesCreatedDuringTests();	clearUDFsCreatedDuringTests();	clearKeysCreatedInTests();	File cleanupFile = new File(cleanupScript);	if (cleanupFile.isFile()) {	String cleanupCommands = readEntireFileIntoString(cleanupFile);	
cleanup 

if(cliDriver == null) {	cliDriver = new CliDriver();	}	SessionState.get().getConf().setBoolean("hive.test.shutdown.phase", true);	int result = cliDriver.processLine(cleanupCommands);	if (result != 0) {	LOG.error("Failed during cleanup processLine with code={}. Ignoring", result);	}	SessionState.get().getConf().setBoolean("hive.test.shutdown.phase", false);	} else {	
no cleanup script detected skipping 

boolean canReuseSession = (tname == null) || !qNoSessionReuseQuerySet.contains(tname);	if(!isSessionStateStarted) {	startSessionState(canReuseSession);	}	if(cliDriver == null) {	cliDriver = new CliDriver();	}	cliDriver.processLine("set test.data.dir=" + testFiles + ";");	File scriptFile = new File(this.initScript);	if (!scriptFile.isFile()) {	
no init script detected skipping 

if(cliDriver == null) {	cliDriver = new CliDriver();	}	cliDriver.processLine("set test.data.dir=" + testFiles + ";");	File scriptFile = new File(this.initScript);	if (!scriptFile.isFile()) {	return;	}	conf.setBoolean("hive.test.init.phase", true);	String initCommands = readEntireFileIntoString(scriptFile);	
initial setup 

public int executeAdhocCommand(String q) {	if (!q.contains(";")) {	return -1;	}	String q1 = q.split(";")[0] + ";";	
executing 

public int execute(String tname) {	try {	return drv.run(qMap.get(tname)).getResponseCode();	} catch (CommandNeedRetryException e) {	
driver failed to run the command due to the exception 

Connection conn = null;	ArrayList<Statement> statements = new ArrayList<Statement>();	try {	Properties props = new Properties();	props.put("user", conf.get("javax.jdo.option.ConnectionUserName"));	props.put("password", conf.get("javax.jdo.option.ConnectionPassword"));	conn = DriverManager.getConnection(conf.get("javax.jdo.option.ConnectionURL"), props);	ResultSet rs = null;	Statement s = conn.createStatement();	if (LOG.isDebugEnabled()) {	
connected to metastore database 

String mdbPath =   AbstractCliConfig.HIVE_ROOT + "/data/files/tpcds-perf/metastore_export/";	BufferedReader br = new BufferedReader( new FileReader( new File(AbstractCliConfig.HIVE_ROOT + "/metastore/scripts/upgrade/derby/022-HIVE-11107.derby.sql")));	String command;	s.execute("DROP TABLE APP.TABLE_PARAMS");	s.execute("DROP TABLE APP.TAB_COL_STATS");	while ((command = br.readLine()) != null) {	if (!command.endsWith(";")) {	continue;	}	if (LOG.isDebugEnabled()) {	
going to run command 

if (!command.endsWith(";")) {	continue;	}	if (LOG.isDebugEnabled()) {	}	try {	PreparedStatement psCommand = conn.prepareStatement(command.substring(0, command.length()-1));	statements.add(psCommand);	psCommand.execute();	if (LOG.isDebugEnabled()) {	
successfully completed 

}	if (LOG.isDebugEnabled()) {	}	try {	PreparedStatement psCommand = conn.prepareStatement(command.substring(0, command.length()-1));	statements.add(psCommand);	psCommand.execute();	if (LOG.isDebugEnabled()) {	}	} catch (SQLException e) {	
got sql exception 

return str1.compareTo(str2);	}	}	final SortedMap<String, Integer> tableNameToID = new TreeMap<String, Integer>(new MyComp());	rs = s.executeQuery("SELECT * FROM APP.TBLS");	while(rs.next()) {	String tblName = rs.getString("TBL_NAME");	Integer tblId = rs.getInt("TBL_ID");	tableNameToID.put(tblName, tblId);	if (LOG.isDebugEnabled()) {	
resultset 

});	Files.write(tmpFileLoc2, (Iterable<String>)replacedStream::iterator);	Files.write(tmpFileLoc2, (Iterable<String>)colStats.entrySet().stream() .map(map->map.getKey()+"@COLUMN_STATS_ACCURATE@"+map.getValue())::iterator, StandardOpenOption.APPEND);	replacedStream.close();	reader2.close();	String importStatement1 =  "CALL SYSCS_UTIL.SYSCS_IMPORT_TABLE(null, '" + "TAB_COL_STATS" + "', '" + tmpFileLoc1.toAbsolutePath().toString() + "', '@', null, 'UTF-8', 1)";	String importStatement2 =  "CALL SYSCS_UTIL.SYSCS_IMPORT_TABLE(null, '" + "TABLE_PARAMS" + "', '" + tmpFileLoc2.toAbsolutePath().toString() + "', '@', null, 'UTF-8', 1)";	try {	PreparedStatement psImport1 = conn.prepareStatement(importStatement1);	if (LOG.isDebugEnabled()) {	
going to execute 

reader2.close();	String importStatement1 =  "CALL SYSCS_UTIL.SYSCS_IMPORT_TABLE(null, '" + "TAB_COL_STATS" + "', '" + tmpFileLoc1.toAbsolutePath().toString() + "', '@', null, 'UTF-8', 1)";	String importStatement2 =  "CALL SYSCS_UTIL.SYSCS_IMPORT_TABLE(null, '" + "TABLE_PARAMS" + "', '" + tmpFileLoc2.toAbsolutePath().toString() + "', '@', null, 'UTF-8', 1)";	try {	PreparedStatement psImport1 = conn.prepareStatement(importStatement1);	if (LOG.isDebugEnabled()) {	}	statements.add(psImport1);	psImport1.execute();	if (LOG.isDebugEnabled()) {	
successfully completed 

try {	PreparedStatement psImport1 = conn.prepareStatement(importStatement1);	if (LOG.isDebugEnabled()) {	}	statements.add(psImport1);	psImport1.execute();	if (LOG.isDebugEnabled()) {	}	PreparedStatement psImport2 = conn.prepareStatement(importStatement2);	if (LOG.isDebugEnabled()) {	
going to execute 

statements.add(psImport1);	psImport1.execute();	if (LOG.isDebugEnabled()) {	}	PreparedStatement psImport2 = conn.prepareStatement(importStatement2);	if (LOG.isDebugEnabled()) {	}	statements.add(psImport2);	psImport2.execute();	if (LOG.isDebugEnabled()) {	
successfully completed 

if (LOG.isDebugEnabled()) {	}	PreparedStatement psImport2 = conn.prepareStatement(importStatement2);	if (LOG.isDebugEnabled()) {	}	statements.add(psImport2);	psImport2.execute();	if (LOG.isDebugEnabled()) {	}	} catch (SQLException e) {	
got sql exception 

PreparedStatement psImport2 = conn.prepareStatement(importStatement2);	if (LOG.isDebugEnabled()) {	}	statements.add(psImport2);	psImport2.execute();	if (LOG.isDebugEnabled()) {	}	} catch (SQLException e) {	}	} catch (FileNotFoundException e1) {	
got file not found exception 

if (LOG.isDebugEnabled()) {	}	statements.add(psImport2);	psImport2.execute();	if (LOG.isDebugEnabled()) {	}	} catch (SQLException e) {	}	} catch (FileNotFoundException e1) {	} catch (IOException e1) {	
got ioexception 

}	statements.add(psImport2);	psImport2.execute();	if (LOG.isDebugEnabled()) {	}	} catch (SQLException e) {	}	} catch (FileNotFoundException e1) {	} catch (IOException e1) {	} catch (SQLException e1) {	
got sqlexception 

========================= hive sample_400 =========================

public DataSource create(Configuration hdpConfig) throws SQLException {	
creating bonecp connection pool for the metastore 

public boolean supports(Configuration configuration) {	String poolingType = MetastoreConf.getVar(configuration, MetastoreConf.ConfVars.CONNECTION_POOLING_TYPE).toLowerCase();	if (BONECP.equals(poolingType)) {	int boneCpPropsNr = DataSourceProvider.getPrefixedProperties(configuration, BONECP).size();	
found nr of bonecp specific configurations 

public boolean supports(Configuration configuration) {	String poolingType = MetastoreConf.getVar(configuration, MetastoreConf.ConfVars.CONNECTION_POOLING_TYPE).toLowerCase();	if (BONECP.equals(poolingType)) {	int boneCpPropsNr = DataSourceProvider.getPrefixedProperties(configuration, BONECP).size();	return boneCpPropsNr > 0;	}	
configuration requested pooling bonecpdsprovider exiting 

========================= hive sample_1834 =========================

public void run(HookContext hookContext) throws Exception {	assert (hookContext.getHookType() == HookContext.HookType.POST_EXEC_HOOK || hookContext.getHookType() == HookContext.HookType.ON_FAILURE_HOOK);	HiveConf conf = hookContext.getConf();	if (!"tez".equals(HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE))) {	return;	}	
executing post execution hook to print workload manager events summary 

========================= hive sample_3809 =========================

public void launchContainer(ContainerLaunchRequest containerLaunchRequest) {	if (LOG.isDebugEnabled()) {	
no op launch for container succeeded on host 

========================= hive sample_617 =========================

private static UserGroupInformation getUserGroupInfo(String user) throws ImpersonationFailed {	try {	return UserGroupInformation.createProxyUser( user, UserGroupInformation.getLoginUser());	} catch (IOException e) {	
unable to get usergroupinfo for user 

private void checkEndPoint(HiveEndPoint endPoint, IMetaStoreClient msClient) throws InvalidTable, ConnectionError {	Table t;	try {	t = msClient.getTable(endPoint.database, endPoint.table);	} catch (Exception e) {	
unable to check the endpoint 

private void checkEndPoint(HiveEndPoint endPoint, IMetaStoreClient msClient) throws InvalidTable, ConnectionError {	Table t;	try {	t = msClient.getTable(endPoint.database, endPoint.table);	} catch (Exception e) {	throw new InvalidTable(endPoint.database, endPoint.table, e);	}	if (!AcidUtils.isAcidTable(t)) {	
hiveendpoint must use an acid table 

ugi.doAs ( new PrivilegedExceptionAction<Void>() {	public Void run() throws Exception {	msClient.close();	heartbeaterMSClient.close();	return null;	}	} );	try {	FileSystem.closeAllForUGI(ugi);	} catch (IOException exception) {	
could not clean up file system handles for ugi 

msClient.close();	heartbeaterMSClient.close();	return null;	}	} );	try {	FileSystem.closeAllForUGI(ugi);	} catch (IOException exception) {	}	} catch (IOException e) {	
error closing connection to 

heartbeaterMSClient.close();	return null;	}	} );	try {	FileSystem.closeAllForUGI(ugi);	} catch (IOException exception) {	}	} catch (IOException e) {	} catch (InterruptedException e) {	
interrupted when closing connection to 

if (ep.partitionVals.isEmpty()) {	return;	}	SessionState localSession = null;	if(SessionState.get() == null) {	localSession = SessionState.start(new CliSessionState(conf));	}	IDriver driver = DriverFactory.newDriver(conf);	try {	if (LOG.isDebugEnabled()) {	
attempting to create partition if not existent 

}	IDriver driver = DriverFactory.newDriver(conf);	try {	if (LOG.isDebugEnabled()) {	}	List<FieldSchema> partKeys = msClient.getTable(ep.database, ep.table) .getPartitionKeys();	runDDL(driver, "use " + ep.database);	String query = "alter table " + ep.table + " add if not exists partition " + partSpecStr(partKeys, ep.partitionVals);	runDDL(driver, query);	} catch (MetaException e) {	
failed to create partition 

try {	if (LOG.isDebugEnabled()) {	}	List<FieldSchema> partKeys = msClient.getTable(ep.database, ep.table) .getPartitionKeys();	runDDL(driver, "use " + ep.database);	String query = "alter table " + ep.table + " add if not exists partition " + partSpecStr(partKeys, ep.partitionVals);	runDDL(driver, query);	} catch (MetaException e) {	throw new PartitionCreationFailed(ep, e);	} catch (NoSuchObjectException e) {	
failed to create partition 

}	List<FieldSchema> partKeys = msClient.getTable(ep.database, ep.table) .getPartitionKeys();	runDDL(driver, "use " + ep.database);	String query = "alter table " + ep.table + " add if not exists partition " + partSpecStr(partKeys, ep.partitionVals);	runDDL(driver, query);	} catch (MetaException e) {	throw new PartitionCreationFailed(ep, e);	} catch (NoSuchObjectException e) {	throw new InvalidTable(ep.database, ep.table);	} catch (TException e) {	
failed to create partition 

runDDL(driver, "use " + ep.database);	String query = "alter table " + ep.table + " add if not exists partition " + partSpecStr(partKeys, ep.partitionVals);	runDDL(driver, query);	} catch (MetaException e) {	throw new PartitionCreationFailed(ep, e);	} catch (NoSuchObjectException e) {	throw new InvalidTable(ep.database, ep.table);	} catch (TException e) {	throw new PartitionCreationFailed(ep, e);	} catch (QueryFailedException e) {	
failed to create partition 

throw new PartitionCreationFailed(ep, e);	} catch (QueryFailedException e) {	throw new PartitionCreationFailed(ep, e);	} finally {	driver.close();	try {	if(localSession != null) {	localSession.close();	}	} catch (IOException e) {	
error closing sessionstate used to run hive ddl 

private static boolean runDDL(IDriver driver, String sql) throws QueryFailedException {	int retryCount = 1;	for (int attempt=0; attempt<=retryCount; ++attempt) {	try {	if (LOG.isDebugEnabled()) {	
running hive query 

private void markDead(boolean success) {	if(success) {	return;	}	isClosed = true;	try {	abort(true);	}	catch(Exception ex) {	
fatal error on cause 

isClosed = true;	try {	abort(true);	}	catch(Exception ex) {	}	try {	closeImpl();	}	catch (Exception ex) {	
fatal error on cause 

ugi.doAs ( new PrivilegedExceptionAction<Void>() {	public Void run() throws StreamingException {	recordWriter.closeBatch();	return null;	}	}	);	try {	FileSystem.closeAllForUGI(ugi);	} catch (IOException exception) {	
could not clean up file system handles for ugi 

========================= hive sample_928 =========================

}	setDefaultAccumuloTableName(job, tableName);	final String indexTableName = job.get(AccumuloIndexParameters.INDEXTABLE_NAME);	final String indexedColumns = job.get(AccumuloIndexParameters.INDEXED_COLUMNS);	final String columnTypes = job.get(serdeConstants.LIST_COLUMN_TYPES);	final boolean binaryEncoding = ColumnEncoding.BINARY.getName() .equalsIgnoreCase(job.get(AccumuloSerDeParameters.DEFAULT_STORAGE_TYPE));	setAccumuloIndexTableName(job, indexTableName);	setAccumuloIndexColumns(job, indexedColumns);	setAccumuloStringEncoding(job, !binaryEncoding);	} catch (AccumuloSecurityException e) {	
could not connect to accumulo with provided credentials 

========================= hive sample_181 =========================

public static void killRunningJobs() {	synchronized (runningJobs) {	for (RunningJob rj : runningJobs) {	try {	System.err.println("killing job with: " + rj.getID());	rj.killJob();	} catch (Exception e) {	
failed to kill job 

continue;	}	errMsg.setLength(0);	updateCounters(ctrs, rj);	if (clientStatPublishers.size() > 0 && ctrs != null) {	Map<String, Double> exctractedCounters = extractAllCounterValues(ctrs);	for (ClientStatsPublisher clientStatPublisher : clientStatPublishers) {	try {	clientStatPublisher.run(exctractedCounters, rj.getID().toString());	} catch (RuntimeException runtimeException) {	
exception thrown when running clientstatspublishers the stack trace is 

String confString = HiveConf.getVar(job, HiveConf.ConfVars.CLIENTSTATSPUBLISHERS);	confString = confString.trim();	if (confString.equals("")) {	return clientStatsPublishers;	}	String[] clientStatsPublisherClasses = confString.split(",");	for (String clientStatsPublisherClass : clientStatsPublisherClasses) {	try {	clientStatsPublishers.add((ClientStatsPublisher) Class.forName( clientStatsPublisherClass.trim(), true, Utilities.getSessionSpecifiedClassLoader()).newInstance());	} catch (Exception e) {	
occured when trying to create class implementing clientstatspublisher interface 

String confString = HiveConf.getVar(job, HiveConf.ConfVars.CLIENTSTATSPUBLISHERS);	confString = confString.trim();	if (confString.equals("")) {	return clientStatsPublishers;	}	String[] clientStatsPublisherClasses = confString.split(",");	for (String clientStatsPublisherClass : clientStatsPublisherClasses) {	try {	clientStatsPublishers.add((ClientStatsPublisher) Class.forName( clientStatsPublisherClass.trim(), true, Utilities.getSessionSpecifiedClassLoader()).newInstance());	} catch (Exception e) {	
the exception message is 

String confString = HiveConf.getVar(job, HiveConf.ConfVars.CLIENTSTATSPUBLISHERS);	confString = confString.trim();	if (confString.equals("")) {	return clientStatsPublishers;	}	String[] clientStatsPublisherClasses = confString.split(",");	for (String clientStatsPublisherClass : clientStatsPublisherClasses) {	try {	clientStatsPublishers.add((ClientStatsPublisher) Class.forName( clientStatsPublisherClass.trim(), true, Utilities.getSessionSpecifiedClassLoader()).newInstance());	} catch (Exception e) {	
program will continue but without this clientstatspublisher working 

========================= hive sample_3872 =========================

public void handle(Context withinContext) throws Exception {	
processing add primarykey message message 

========================= hive sample_3464 =========================

throw new RuntimeException("Import failed", execJob.getException());	}	server.registerQuery("X = load '" + tablename + "' using org.apache.hive.hcatalog.pig.HCatLoader();");	server.dumpSchema("X");	Iterator<Tuple> it = server.openIterator("X");	int i = 0;	while (it.hasNext()) {	Tuple input = result.get(i++);	Tuple output = it.next();	compareTuples(input, output);	
tuple 

========================= hive sample_1009 =========================

int abortedThreshold = HiveConf.getIntVar(conf, HiveConf.ConfVars.HIVE_COMPACTOR_ABORTEDTXN_THRESHOLD);	do {	long startedAt = -1;	TxnStore.MutexAPI.LockHandle handle = null;	try {	handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Initiator.name());	startedAt = System.currentTimeMillis();	ShowCompactResponse currentCompactions = txnHandler.showCompact(new ShowCompactRequest());	ValidTxnList txns = TxnUtils.createValidCompactTxnList(txnHandler.getOpenTxnsInfo());	Set<CompactionInfo> potentials = txnHandler.findPotentialCompactions(abortedThreshold);	
found potential compactions checking to see if we should compact any of them 

do {	long startedAt = -1;	TxnStore.MutexAPI.LockHandle handle = null;	try {	handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Initiator.name());	startedAt = System.currentTimeMillis();	ShowCompactResponse currentCompactions = txnHandler.showCompact(new ShowCompactRequest());	ValidTxnList txns = TxnUtils.createValidCompactTxnList(txnHandler.getOpenTxnsInfo());	Set<CompactionInfo> potentials = txnHandler.findPotentialCompactions(abortedThreshold);	for (CompactionInfo ci : potentials) {	
checking to see if we should compact 

try {	handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Initiator.name());	startedAt = System.currentTimeMillis();	ShowCompactResponse currentCompactions = txnHandler.showCompact(new ShowCompactRequest());	ValidTxnList txns = TxnUtils.createValidCompactTxnList(txnHandler.getOpenTxnsInfo());	Set<CompactionInfo> potentials = txnHandler.findPotentialCompactions(abortedThreshold);	for (CompactionInfo ci : potentials) {	try {	Table t = resolveTable(ci);	if (t == null) {	
can t find table assuming it s a temp table or has been dropped and moving on 

try {	Table t = resolveTable(ci);	if (t == null) {	continue;	}	if (noAutoCompactSet(t)) {	LOG.info("Table " + tableName(t) + " marked " + hive_metastoreConstants.TABLE_NO_AUTO_COMPACT + "=true so we will not compact it.");	continue;	}	if (t.getPartitionKeys() != null && t.getPartitionKeys().size() > 0 && ci.partName  == null) {	
skipping entry for as it is from dynamic partitioning 

continue;	}	if (noAutoCompactSet(t)) {	LOG.info("Table " + tableName(t) + " marked " + hive_metastoreConstants.TABLE_NO_AUTO_COMPACT + "=true so we will not compact it.");	continue;	}	if (t.getPartitionKeys() != null && t.getPartitionKeys().size() > 0 && ci.partName  == null) {	continue;	}	if (lookForCurrentCompactions(currentCompactions, ci)) {	
found currently initiated or working compaction for so we will not initiate another compaction 

LOG.info("Table " + tableName(t) + " marked " + hive_metastoreConstants.TABLE_NO_AUTO_COMPACT + "=true so we will not compact it.");	continue;	}	if (t.getPartitionKeys() != null && t.getPartitionKeys().size() > 0 && ci.partName  == null) {	continue;	}	if (lookForCurrentCompactions(currentCompactions, ci)) {	continue;	}	if(txnHandler.checkFailedCompactions(ci)) {	
will not initiate compaction for since last attempts to compact it failed 

}	if (lookForCurrentCompactions(currentCompactions, ci)) {	continue;	}	if(txnHandler.checkFailedCompactions(ci)) {	txnHandler.markFailed(ci);	continue;	}	Partition p = resolvePartition(ci);	if (p == null && ci.partName != null) {	
can t find partition assuming it has been dropped and moving on 

}	Partition p = resolvePartition(ci);	if (p == null && ci.partName != null) {	continue;	}	StorageDescriptor sd = resolveStorageDescriptor(t, p);	String runAs = findUserToRunAs(sd.getLocation(), t);	CompactionType compactionNeeded = checkForCompaction(ci, txns, sd, t.getParameters(), runAs);	if (compactionNeeded != null) requestCompaction(ci, runAs, compactionNeeded);	} catch (Throwable t) {	
caught exception while trying to determine if we should compact marking failed to avoid repeated failures 

String runAs = findUserToRunAs(sd.getLocation(), t);	CompactionType compactionNeeded = checkForCompaction(ci, txns, sd, t.getParameters(), runAs);	if (compactionNeeded != null) requestCompaction(ci, runAs, compactionNeeded);	} catch (Throwable t) {	txnHandler.markFailed(ci);	}	}	recoverFailedCompactions(true);	txnHandler.cleanEmptyAbortedTxns();	} catch (Throwable t) {	
initiator loop caught unexpected exception this time through the loop 

finally {	if(handle != null) {	handle.releaseLocks();	}	}	long elapsedTime = System.currentTimeMillis() - startedAt;	if (elapsedTime >= checkInterval || stop.get())  continue;	else Thread.sleep(checkInterval - elapsedTime);	} while (!stop.get());	} catch (Throwable t) {	
caught an exception in the main loop of compactor initiator exiting 

private CompactionType checkForCompaction(final CompactionInfo ci, final ValidTxnList txns, final StorageDescriptor sd, final Map<String, String> tblproperties, final String runAs) throws IOException, InterruptedException {	if (ci.tooManyAborts) {	
found too many aborted transactions for initiating major compaction 

private CompactionType checkForCompaction(final CompactionInfo ci, final ValidTxnList txns, final StorageDescriptor sd, final Map<String, String> tblproperties, final String runAs) throws IOException, InterruptedException {	if (ci.tooManyAborts) {	return CompactionType.MAJOR;	}	if (runJobAsSelf(runAs)) {	return determineCompactionType(ci, txns, sd, tblproperties);	} else {	
going to initiate as user 

} else {	UserGroupInformation ugi = UserGroupInformation.createProxyUser(runAs, UserGroupInformation.getLoginUser());	CompactionType compactionType = ugi.doAs(new PrivilegedExceptionAction<CompactionType>() {	public CompactionType run() throws Exception {	return determineCompactionType(ci, txns, sd, tblproperties);	}	});	try {	FileSystem.closeAllForUGI(ugi);	} catch (IOException exception) {	
could not clean up file system handles for ugi for 

boolean noBase = false;	Path location = new Path(sd.getLocation());	FileSystem fs = location.getFileSystem(conf);	AcidUtils.Directory dir = AcidUtils.getAcidState(location, conf, txns, false, false);	Path base = dir.getBaseDirectory();	long baseSize = 0;	FileStatus stat = null;	if (base != null) {	stat = fs.getFileStatus(base);	if (!stat.isDir()) {	
was assuming base is directory but it s a file 

}	List<HdfsFileStatusWithId> originals = dir.getOriginalFiles();	for (HdfsFileStatusWithId origStat : originals) {	baseSize += origStat.getFileStatus().getLen();	}	long deltaSize = 0;	List<AcidUtils.ParsedDelta> deltas = dir.getCurrentDirectories();	for (AcidUtils.ParsedDelta delta : deltas) {	stat = fs.getFileStatus(delta.getPath());	if (!stat.isDir()) {	
was assuming delta is a directory but it s a file 

msg.append(" will major compact: ");	msg.append(bigEnough);	LOG.debug(msg.toString());	}	if (bigEnough) return CompactionType.MAJOR;	}	String deltaNumProp = tblproperties.get(COMPACTORTHRESHOLD_PREFIX + HiveConf.ConfVars.HIVE_COMPACTOR_DELTA_NUM_THRESHOLD);	int deltaNumThreshold = deltaNumProp == null ? HiveConf.getIntVar(conf, HiveConf.ConfVars.HIVE_COMPACTOR_DELTA_NUM_THRESHOLD) : Integer.parseInt(deltaNumProp);	boolean enough = deltas.size() > deltaNumThreshold;	if (enough) {	
found delta files threshold is not and no base requesting major minor compaction 

private void requestCompaction(CompactionInfo ci, String runAs, CompactionType type) throws MetaException {	CompactionRequest rqst = new CompactionRequest(ci.dbname, ci.tableName, type);	if (ci.partName != null) rqst.setPartitionname(ci.partName);	rqst.setRunas(runAs);	
requesting compaction 

========================= hive sample_2808 =========================

public void initialize(InputSplit split, Configuration conf, ObjectMapper mapper, ObjectMapper smileMapper, HttpClient httpClient ) throws IOException {	HiveDruidSplit hiveDruidSplit = (HiveDruidSplit) split;	Preconditions.checkNotNull(hiveDruidSplit, "input split is null ???");	this.mapper = Preconditions.checkNotNull(mapper, "object Mapper can not be null");	this.smileMapper = Preconditions.checkNotNull(smileMapper, "Smile Mapper can not be null");	this.query = this.mapper.readValue(Preconditions.checkNotNull(hiveDruidSplit.getDruidQuery()), Query.class);	Preconditions.checkNotNull(query);	this.resultsType = getResultTypeDef();	this.httpClient = Preconditions.checkNotNull(httpClient, "need Http Client");	
retrieving data from druid using query 

========================= hive sample_30 =========================

Preconditions.checkArgument(value != null);	boolean hasNext = reader.next(key,  data);	if (hasNext) {	Object rowObj;	try {	StructObjectInspector rowOI = (StructObjectInspector) serde.getObjectInspector();	rowObj = serde.deserialize(data);	setRowFromStruct(value, rowObj, rowOI);	} catch (SerDeException err) {	if (LOG.isDebugEnabled()) {	
error deserializing row from data 

========================= hive sample_640 =========================

regex = regexConst;	} else {	regex = getStringValue(arguments, 1, converters);	}	if (regex == null) {	return null;	}	if (regex.length() == 0) {	if (!warned) {	warned = true;	
regex is empty additional warnings for an empty regex will be suppressed 

========================= hive sample_4749 =========================

public static Operator<? extends OperatorDesc> generateOperatorTree(QueryState queryState, String command) throws SemanticException {	Operator<? extends OperatorDesc> operatorTree;	try {	Context ctx = new Context(queryState.getConf());	ASTNode tree = ParseUtils.parse(command, ctx);	BaseSemanticAnalyzer sem = SemanticAnalyzerFactory.get(queryState, tree);	assert(sem instanceof SemanticAnalyzer);	operatorTree = doSemanticAnalysis((SemanticAnalyzer) sem, tree, ctx);	
sub query semantic analysis completed 

public static Operator<? extends OperatorDesc> generateOperatorTree(QueryState queryState, String command) throws SemanticException {	Operator<? extends OperatorDesc> operatorTree;	try {	Context ctx = new Context(queryState.getConf());	ASTNode tree = ParseUtils.parse(command, ctx);	BaseSemanticAnalyzer sem = SemanticAnalyzerFactory.get(queryState, tree);	assert(sem instanceof SemanticAnalyzer);	operatorTree = doSemanticAnalysis((SemanticAnalyzer) sem, tree, ctx);	} catch (IOException e) {	
ioexception in generating the operator tree for input command 

try {	Context ctx = new Context(queryState.getConf());	ASTNode tree = ParseUtils.parse(command, ctx);	BaseSemanticAnalyzer sem = SemanticAnalyzerFactory.get(queryState, tree);	assert(sem instanceof SemanticAnalyzer);	operatorTree = doSemanticAnalysis((SemanticAnalyzer) sem, tree, ctx);	} catch (IOException e) {	LOG.error(org.apache.hadoop.util.StringUtils.stringifyException(e));	throw new SemanticException(e.getMessage(), e);	} catch (ParseException e) {	
parseexception in generating the operator tree for input command 

BaseSemanticAnalyzer sem = SemanticAnalyzerFactory.get(queryState, tree);	assert(sem instanceof SemanticAnalyzer);	operatorTree = doSemanticAnalysis((SemanticAnalyzer) sem, tree, ctx);	} catch (IOException e) {	LOG.error(org.apache.hadoop.util.StringUtils.stringifyException(e));	throw new SemanticException(e.getMessage(), e);	} catch (ParseException e) {	LOG.error(org.apache.hadoop.util.StringUtils.stringifyException(e));	throw new SemanticException(e.getMessage(), e);	} catch (SemanticException e) {	
semanticexception in generating the operator tree for input command 

private static Operator<?> doSemanticAnalysis(SemanticAnalyzer sem, ASTNode ast, Context ctx) throws SemanticException {	QB qb = new QB(null, null, false);	ASTNode child = ast;	ParseContext subPCtx = sem.getParseContext();	subPCtx.setContext(ctx);	sem.initParseCtx(subPCtx);	
starting sub query semantic analysis 

private static Operator<?> doSemanticAnalysis(SemanticAnalyzer sem, ASTNode ast, Context ctx) throws SemanticException {	QB qb = new QB(null, null, false);	ASTNode child = ast;	ParseContext subPCtx = sem.getParseContext();	subPCtx.setContext(ctx);	sem.initParseCtx(subPCtx);	sem.doPhase1(child, qb, sem.initPhase1Ctx(), null);	
completed phase of sub query semantic analysis 

private static Operator<?> doSemanticAnalysis(SemanticAnalyzer sem, ASTNode ast, Context ctx) throws SemanticException {	QB qb = new QB(null, null, false);	ASTNode child = ast;	ParseContext subPCtx = sem.getParseContext();	subPCtx.setContext(ctx);	sem.initParseCtx(subPCtx);	sem.doPhase1(child, qb, sem.initPhase1Ctx(), null);	sem.getMetaData(qb);	
completed getting metadata in sub query semantic analysis 

private static Operator<?> doSemanticAnalysis(SemanticAnalyzer sem, ASTNode ast, Context ctx) throws SemanticException {	QB qb = new QB(null, null, false);	ASTNode child = ast;	ParseContext subPCtx = sem.getParseContext();	subPCtx.setContext(ctx);	sem.initParseCtx(subPCtx);	sem.doPhase1(child, qb, sem.initPhase1Ctx(), null);	sem.getMetaData(qb);	
sub query abstract syntax tree 

private static Operator<?> doSemanticAnalysis(SemanticAnalyzer sem, ASTNode ast, Context ctx) throws SemanticException {	QB qb = new QB(null, null, false);	ASTNode child = ast;	ParseContext subPCtx = sem.getParseContext();	subPCtx.setContext(ctx);	sem.initParseCtx(subPCtx);	sem.doPhase1(child, qb, sem.initPhase1Ctx(), null);	sem.getMetaData(qb);	Operator<?> operator = sem.genPlan(qb);	
sub query completed plan generation 

========================= hive sample_3063 =========================

case LESS_THAN: builder.lessThan(columnName, type, findLiteral(conf, expression, type));	break;	case LESS_THAN_EQUALS: builder.lessThanEquals(columnName, type, findLiteral(conf, expression, type));	break;	case IN: builder.in(columnName, type, getLiteralList(expression, type, variable + 1));	break;	case BETWEEN: builder.between(columnName, type, getLiteral(conf, expression, type, variable + 1), getLiteral(conf, expression, type, variable + 2));	break;	}	} catch (Exception e) {	
exception thrown during sarg creation returning yes no null exception 

========================= hive sample_3599 =========================

private BufferedWriter writer() throws IOException {	Path exportToFile = new Path(exportRootDataDir, EximUtil.FILES_NAME);	if (exportFileSystem.exists(exportToFile)) {	throw new IllegalArgumentException( exportToFile.toString() + " already exists and cant export data from path(dir) " + dataFileListPath);	}	
exporting data files in dir to 

========================= hive sample_3433 =========================

this.sessionHandle = sessionHandle != null ? sessionHandle : new SessionHandle(protocol);	this.sessionConf = new HiveConf(serverConf);	this.ipAddress = ipAddress;	this.forwardedAddresses = forwardedAddresses;	this.operationLock = serverConf.getBoolVar( ConfVars.HIVE_SERVER2_PARALLEL_OPS_IN_SESSION) ? null : new Semaphore(1);	try {	if (! sessionConf.getBoolVar(ConfVars.HIVE_SERVER2_ENABLE_DOAS) && sessionConf.getBoolVar(ConfVars.HIVE_SERVER2_MAP_FAIR_SCHEDULER_QUEUE)) {	ShimLoader.getHadoopShims().refreshDefaultQueue(sessionConf, username);	}	} catch (IOException e) {	
error setting scheduler queue 

protected int processCmd(String cmd) {	int rc = 0;	String cmd_trimed = cmd.trim();	OperationHandle opHandle = null;	try {	opHandle = executeStatementInternal(cmd_trimed, null, false, 0);	} catch (HiveSQLException e) {	
failed to execute command in global hiverc file 

OperationHandle opHandle = null;	try {	opHandle = executeStatementInternal(cmd_trimed, null, false, 0);	} catch (HiveSQLException e) {	return -1;	}	if (opHandle != null) {	try {	closeOperation(opHandle);	} catch (HiveSQLException e) {	
failed to close operation for command in hiverc file 

private void processGlobalInitFile() {	IHiveFileProcessor processor = new GlobalHivercFileProcessor();	try {	String hiverc = sessionConf.getVar(ConfVars.HIVE_SERVER2_GLOBAL_INIT_FILE_LOCATION);	if (hiverc != null) {	File hivercFile = new File(hiverc);	if (hivercFile.isDirectory()) {	hivercFile = new File(hivercFile, SessionManager.HIVERCFILE);	}	if (hivercFile.isFile()) {	
running global init file 

try {	String hiverc = sessionConf.getVar(ConfVars.HIVE_SERVER2_GLOBAL_INIT_FILE_LOCATION);	if (hiverc != null) {	File hivercFile = new File(hiverc);	if (hivercFile.isDirectory()) {	hivercFile = new File(hivercFile, SessionManager.HIVERCFILE);	}	if (hivercFile.isFile()) {	int rc = processor.processFile(hivercFile.getAbsolutePath());	if (rc != 0) {	
failed on initializing global hiverc file 

if (hiverc != null) {	File hivercFile = new File(hiverc);	if (hivercFile.isDirectory()) {	hivercFile = new File(hivercFile, SessionManager.HIVERCFILE);	}	if (hivercFile.isFile()) {	int rc = processor.processFile(hivercFile.getAbsolutePath());	if (rc != 0) {	}	} else {	
global init file does not exist 

hivercFile = new File(hivercFile, SessionManager.HIVERCFILE);	}	if (hivercFile.isFile()) {	int rc = processor.processFile(hivercFile.getAbsolutePath());	if (rc != 0) {	}	} else {	}	}	} catch (IOException e) {	
failed on initializing global hiverc file 

sessionState = null;	}	} catch (IOException ioe) {	throw new HiveSQLException("Failure to close", ioe);	} finally {	if (sessionState != null) {	try {	sessionState.resetThreadName();	sessionState.close();	} catch (Throwable t) {	
error closing session 

sessionState.resetThreadName();	sessionState.close();	} catch (Throwable t) {	}	sessionState = null;	}	if (sessionHive != null) {	try {	Hive.closeCurrent();	} catch (Throwable t) {	
error closing sessionhive 

private void closeTimedOutOperations(List<Operation> operations) {	acquire(false, false);	try {	for (Operation operation : operations) {	removeOpHandle(operation.getHandle());	try {	operation.close();	} catch (Exception e) {	
exception is thrown closing timed out operation reported open operations metrics may be incorrect 

public void setApplicationName(String value) {	String oldName = sessionState.getHiveVariables().put("wmapp", value);	if (oldName != null && !oldName.equals(value)) {	
applicationname changed from to 

========================= hive sample_2388 =========================

for (String key: configProps.stringPropertyNames()) {	job.set(key, configProps.getProperty(key));	}	InputSplit[] splits = format.getSplits(job, Integer.parseInt(numSplits));	if (splits.length == 0) {	System.out.println("No splits returned - empty scan");	System.out.println("Results: ");	} else {	boolean first = true;	for (InputSplit s: splits) {	
processing input split s from 

========================= hive sample_656 =========================

TServerSocket thriftServerSocket = TSSLTransportFactory.getServerSocket(portNum, 0, serverAddress.getAddress(), params);	if (thriftServerSocket.getServerSocket() instanceof SSLServerSocket) {	List<String> sslVersionBlacklistLocal = new ArrayList<String>();	for (String sslVersion : sslVersionBlacklist) {	sslVersionBlacklistLocal.add(sslVersion.trim().toLowerCase());	}	SSLServerSocket sslServerSocket = (SSLServerSocket) thriftServerSocket.getServerSocket();	List<String> enabledProtocols = new ArrayList<String>();	for (String protocol : sslServerSocket.getEnabledProtocols()) {	if (sslVersionBlacklistLocal.contains(protocol.toLowerCase())) {	
disabling ssl protocol 

}	SSLServerSocket sslServerSocket = (SSLServerSocket) thriftServerSocket.getServerSocket();	List<String> enabledProtocols = new ArrayList<String>();	for (String protocol : sslServerSocket.getEnabledProtocols()) {	if (sslVersionBlacklistLocal.contains(protocol.toLowerCase())) {	} else {	enabledProtocols.add(protocol);	}	}	sslServerSocket.setEnabledProtocols(enabledProtocols.toArray(new String[0]));	
ssl server socket enabled protocols 

========================= hive sample_1361 =========================

public void run() {	try {	metaStoreClient.heartbeat(transactionId == null ? 0 : transactionId, lockId);	LOG.debug("Sent heartbeat for lock={}, transactionId={}", lockId, transactionId);	} catch (NoSuchLockException | NoSuchTxnException | TxnAbortedException e) {	failLock(e);	} catch (TException e) {	
failed to send heartbeat to meta store 

private void failLock(Exception e) {	
lock failed cancelling heartbeat and notifying listener 

========================= hive sample_969 =========================

public <T> T retrieve(String key) throws HiveException {	T value = null;	lock.lock();	try {	value = (T) registry.getIfPresent(key);	if (value != null && LOG.isDebugEnabled()) {	
found in cache 

public <T> T retrieve(String key, Callable<T> fn) throws HiveException {	T value = null;	ReentrantLock objectLock = null;	lock.lock();	try {	value = (T) registry.getIfPresent(key);	if (value != null) {	if (LOG.isDebugEnabled()) {	
found in cache 

} finally {	lock.unlock();	}	objectLock.lock();	try{	lock.lock();	try {	value = (T) registry.getIfPresent(key);	if (value != null) {	if (LOG.isDebugEnabled()) {	
found in cache 

lock.unlock();	}	try {	value = fn.call();	} catch (Exception e) {	throw new HiveException(e);	}	lock.lock();	try {	if (LOG.isDebugEnabled()) {	
caching new object for key 

public void remove(String key) {	if (LOG.isDebugEnabled()) {	
removing key 

========================= hive sample_3999 =========================

public StaticPermanentFunctionChecker(Configuration conf) {	URL logger = conf.getResource(PERMANENT_FUNCTIONS_LIST);	if (logger == null) {	
could not find udf whitelist in configuration 

if (logger == null) {	return;	}	try (BufferedReader r = new BufferedReader(new InputStreamReader(logger.openStream()))) {	String klassName = r.readLine();	while (klassName != null) {	try {	Class<?> clazz = Class.forName(klassName.trim(), false, this.getClass().getClassLoader());	allowedUdfClasses.put(clazz, true);	} catch (ClassNotFoundException ie) {	
could not load class declared in udf whitelist 

String klassName = r.readLine();	while (klassName != null) {	try {	Class<?> clazz = Class.forName(klassName.trim(), false, this.getClass().getClassLoader());	allowedUdfClasses.put(clazz, true);	} catch (ClassNotFoundException ie) {	}	klassName = r.readLine();	}	} catch (IOException ioe) {	
could not read udf whitelist 

========================= hive sample_2234 =========================

moveFutures.put(wmTezSession, moveFuture);	break;	default: throw new RuntimeException("Unsupported action: " + entry.getValue());	}	}	for (Map.Entry<WmTezSession, Future<Boolean>> entry : moveFutures.entrySet()) {	WmTezSession wmTezSession = entry.getKey();	Future<Boolean> future = entry.getValue();	try {	if (future.get()) {	
moved session to pool 

default: throw new RuntimeException("Unsupported action: " + entry.getValue());	}	}	for (Map.Entry<WmTezSession, Future<Boolean>> entry : moveFutures.entrySet()) {	WmTezSession wmTezSession = entry.getKey();	Future<Boolean> future = entry.getValue();	try {	if (future.get()) {	}	} catch (InterruptedException | ExecutionException e) {	
exception while moving session 

if (future.get()) {	}	} catch (InterruptedException | ExecutionException e) {	}	}	for (Map.Entry<WmTezSession, Future<Boolean>> entry : killFutures.entrySet()) {	WmTezSession wmTezSession = entry.getKey();	Future<Boolean> future = entry.getValue();	try {	if (future.get()) {	
killed session 

} catch (InterruptedException | ExecutionException e) {	}	}	for (Map.Entry<WmTezSession, Future<Boolean>> entry : killFutures.entrySet()) {	WmTezSession wmTezSession = entry.getKey();	Future<Boolean> future = entry.getValue();	try {	if (future.get()) {	}	} catch (InterruptedException | ExecutionException e) {	
exception while killing session 

========================= hive sample_3976 =========================

List<String> partColumnNames = new ArrayList<>();	List<PrimitiveTypeInfo> partColumnTypeInfos = new ArrayList<>();	for (FieldSchema fs : partColumns) {	partColumnNames.add(fs.getName());	partColumnTypeInfos.add(TypeInfoFactory.getPrimitiveTypeInfo(fs.getType()));	}	ExprNodeGenericFuncDesc expr = deserializeExpr(exprBytes);	try {	ExprNodeDescUtils.replaceEqualDefaultPartition(expr, defaultPartitionName);	} catch (SemanticException ex) {	
failed to replace default partition 

ExprNodeGenericFuncDesc expr = deserializeExpr(exprBytes);	try {	ExprNodeDescUtils.replaceEqualDefaultPartition(expr, defaultPartitionName);	} catch (SemanticException ex) {	throw new MetaException(ex.getMessage());	}	try {	long startTime = System.nanoTime(), len = partitionNames.size();	boolean result = PartitionPruner.prunePartitionNames( partColumnNames, partColumnTypeInfos, expr, defaultPartitionName, partitionNames);	double timeMs = (System.nanoTime() - startTime) / 1000000.0;	
pruning partition names took ms 

ExprNodeDescUtils.replaceEqualDefaultPartition(expr, defaultPartitionName);	} catch (SemanticException ex) {	throw new MetaException(ex.getMessage());	}	try {	long startTime = System.nanoTime(), len = partitionNames.size();	boolean result = PartitionPruner.prunePartitionNames( partColumnNames, partColumnTypeInfos, expr, defaultPartitionName, partitionNames);	double timeMs = (System.nanoTime() - startTime) / 1000000.0;	return result;	} catch (HiveException ex) {	
failed to apply the expression 

private ExprNodeGenericFuncDesc deserializeExpr(byte[] exprBytes) throws MetaException {	ExprNodeGenericFuncDesc expr = null;	try {	expr = SerializationUtilities.deserializeExpressionFromKryo(exprBytes);	} catch (Exception ex) {	
failed to deserialize the expression 

public FileMetadataExprType getMetadataType(String inputFormat) {	try {	Class<?> ifClass = Class.forName(inputFormat);	if (OrcInputFormat.class.isAssignableFrom(ifClass)) {	return FileMetadataExprType.ORC_SARG;	}	return null;	} catch (Throwable t) {	
can t create the class for input format 

========================= hive sample_3075 =========================

public void initialize(Configuration conf, Properties tbl) throws SerDeException {	List<TypeInfo> columnTypes;	StructTypeInfo rowTypeInfo;	
initializing jsonserde 

if (columnNameProperty.isEmpty()) {	columnNames = Collections.emptyList();	} else {	columnNames = Arrays.asList(columnNameProperty.split(columnNameDelimiter));	}	if (columnTypeProperty.isEmpty()) {	columnTypes = Collections.emptyList();	} else {	columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);	}	
columns 

if (columnNameProperty.isEmpty()) {	columnNames = Collections.emptyList();	} else {	columnNames = Arrays.asList(columnNameProperty.split(columnNameDelimiter));	}	if (columnTypeProperty.isEmpty()) {	columnTypes = Collections.emptyList();	} else {	columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);	}	
types 

if (columnTypeProperty.isEmpty()) {	columnTypes = Collections.emptyList();	} else {	columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);	}	assert (columnNames.size() == columnTypes.size());	rowTypeInfo = (StructTypeInfo) TypeInfoFactory.getStructTypeInfo(columnNames, columnTypes);	cachedObjectInspector = HCatRecordObjectInspectorFactory.getHCatRecordObjectInspector(rowTypeInfo);	try {	schema = HCatSchemaUtils.getHCatSchema(rowTypeInfo).get(0).getStructSubSchema();	
schema 

if (columnTypeProperty.isEmpty()) {	columnTypes = Collections.emptyList();	} else {	columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);	}	assert (columnNames.size() == columnTypes.size());	rowTypeInfo = (StructTypeInfo) TypeInfoFactory.getStructTypeInfo(columnNames, columnTypes);	cachedObjectInspector = HCatRecordObjectInspectorFactory.getHCatRecordObjectInspector(rowTypeInfo);	try {	schema = HCatSchemaUtils.getHCatSchema(rowTypeInfo).get(0).getStructSubSchema();	
fields 

try {	p = jsonFactory.createJsonParser(new ByteArrayInputStream((t.getBytes())));	if (p.nextToken() != JsonToken.START_OBJECT) {	throw new IOException("Start token not found where expected");	}	JsonToken token;	while (((token = p.nextToken()) != JsonToken.END_OBJECT) && (token != null)) {	populateRecord(r, token, p, schema);	}	} catch (JsonParseException e) {	
error parsing json text 

if (p.nextToken() != JsonToken.START_OBJECT) {	throw new IOException("Start token not found where expected");	}	JsonToken token;	while (((token = p.nextToken()) != JsonToken.END_OBJECT) && (token != null)) {	populateRecord(r, token, p, schema);	}	} catch (JsonParseException e) {	throw new SerDeException(e);	} catch (IOException e) {	
error parsing json text 

private void populateRecord(List<Object> r, JsonToken token, JsonParser p, HCatSchema s) throws IOException {	if (token != JsonToken.FIELD_NAME) {	throw new IOException("Field name expected");	}	String fieldName = p.getText();	Integer fpos = s.getPosition(fieldName);	if (fpos == null) {	fpos = getPositionFromHiveInternalColumnName(fieldName);	
npe finding position for field in schema attempting to check if it is an internal column name like 

}	String fieldName = p.getText();	Integer fpos = s.getPosition(fieldName);	if (fpos == null) {	fpos = getPositionFromHiveInternalColumnName(fieldName);	if (fpos == -1) {	skipValue(p);	return;	}	if (!fieldName.equalsIgnoreCase(getHiveInternalColumnName(fpos))) {	
hive internal column name and position encoding for the column name are at odds 

throw new IOException("Start of Object expected");	}	HCatSchema subSchema = hcatFieldSchema.getStructSubSchema();	int sz = subSchema.getFieldNames().size();	List<Object> struct = new ArrayList<Object>(Collections.nCopies(sz, null));	while ((valueToken = p.nextToken()) != JsonToken.END_OBJECT) {	populateRecord(struct, valueToken, p, subSchema);	}	val = struct;	break;	
unknown type found 

if (i > 0) {	sb.append(SerDeUtils.COMMA);	}	appendWithQuotes(sb, columnNames.get(i));	sb.append(SerDeUtils.COLON);	buildJSONString(sb, soi.getStructFieldData(obj, structFields.get(i)), structFields.get(i).getFieldObjectInspector());	}	sb.append(SerDeUtils.RBRACE);	}	} catch (IOException e) {	
error generating json text from object 

========================= hive sample_744 =========================

StructObjectInspector soi = (StructObjectInspector) inputObjInspectors[tag];	StructField sf = soi.getStructFieldRef(Utilities.ReduceField.KEY .toString());	List keyObject = (List) soi.getStructFieldData(row, sf);	if (alias == numAliases - 1 && !(handleSkewJoin && skewJoinKeyContext.currBigKeyTag >= 0) && !hasLeftSemiJoin) {	if (sz == joinEmitInterval && !hasFilter(condn[alias-1].getLeft()) && !hasFilter(condn[alias-1].getRight())) {	checkAndGenObject();	storage[alias].clearRows();	}	} else {	if (LOG.isInfoEnabled() && (sz == nextSz)) {	
table has rows for join key 

private void  mvFileToFinalPath(Path specPath, Configuration hconf, boolean success, Logger log) throws IOException, HiveException {	FileSystem fs = specPath.getFileSystem(hconf);	Path tmpPath = Utilities.toTempPath(specPath);	Path intermediatePath = new Path(tmpPath.getParent(), tmpPath.getName() + ".intermediate");	if (success) {	if (fs.exists(tmpPath)) {	
moving tmp dir to spec 

private void  mvFileToFinalPath(Path specPath, Configuration hconf, boolean success, Logger log) throws IOException, HiveException {	FileSystem fs = specPath.getFileSystem(hconf);	Path tmpPath = Utilities.toTempPath(specPath);	Path intermediatePath = new Path(tmpPath.getParent(), tmpPath.getName() + ".intermediate");	if (success) {	if (fs.exists(tmpPath)) {	Utilities.rename(fs, tmpPath, intermediatePath);	Utilities.removeTempOrDuplicateFiles(fs, intermediatePath, false);	
moving tmp dir to 

========================= hive sample_3864 =========================

public static void initSharedCacheAsync(Configuration conf) {	String clazzName = null;	boolean isEnabled = false;	try {	clazzName = MetastoreConf.getVar(conf, MetastoreConf.ConfVars.RAW_STORE_IMPL);	isEnabled = JavaUtils.getClass(clazzName, RawStore.class).isAssignableFrom(CachedStore.class);	} catch (MetaException e) {	
cannot instantiate metastore class 

public static void initSharedCacheAsync(Configuration conf) {	String clazzName = null;	boolean isEnabled = false;	try {	clazzName = MetastoreConf.getVar(conf, MetastoreConf.ConfVars.RAW_STORE_IMPL);	isEnabled = JavaUtils.getClass(clazzName, RawStore.class).isAssignableFrom(CachedStore.class);	} catch (MetaException e) {	}	if (!isEnabled) {	
cachedstore is not enabled using 

try {	rawStore = (JavaUtils.getClass(rawStoreClassName, RawStore.class)).newInstance();	} catch (Exception e) {	throw new RuntimeException("Cannot instantiate " + rawStoreClassName, e);	}	}	rawStore.setConf(conf);	Configuration oldConf = this.conf;	this.conf = conf;	if (expressionProxy != null && conf != oldConf) {	
unexpected setconf when we were already configured 

static void prewarm(RawStore rawStore) throws Exception {	Deadline.registerIfNot(1000000);	List<String> dbNames = rawStore.getAllDatabases();	
number of databases to prewarm 

List<String> dbNames = rawStore.getAllDatabases();	SharedCache sharedCache = sharedCacheWrapper.getUnsafe();	for (int i = 0; i < dbNames.size(); i++) {	String dbName = StringUtils.normalizeIdentifier(dbNames.get(i));	Deadline.startTimer("getColStatsForDatabase");	List<ColStatsObjWithSourceInfo> colStatsForDB = rawStore.getPartitionColStatsForDatabase(dbName);	Deadline.stopTimer();	if (colStatsForDB != null) {	sharedCache.addPartitionColStatsToCache(colStatsForDB);	}	
caching database cached databases so far 

String dbName = StringUtils.normalizeIdentifier(dbNames.get(i));	Deadline.startTimer("getColStatsForDatabase");	List<ColStatsObjWithSourceInfo> colStatsForDB = rawStore.getPartitionColStatsForDatabase(dbName);	Deadline.stopTimer();	if (colStatsForDB != null) {	sharedCache.addPartitionColStatsToCache(colStatsForDB);	}	Database db = rawStore.getDatabase(dbName);	sharedCache.addDatabaseToCache(dbName, db);	List<String> tblNames = rawStore.getAllTables(dbName);	
tables in database 

Deadline.stopTimer();	if (colStatsForDB != null) {	sharedCache.addPartitionColStatsToCache(colStatsForDB);	}	Database db = rawStore.getDatabase(dbName);	sharedCache.addDatabaseToCache(dbName, db);	List<String> tblNames = rawStore.getAllTables(dbName);	for (int j = 0; j < tblNames.size(); j++) {	String tblName = StringUtils.normalizeIdentifier(tblNames.get(j));	if (!shouldCacheTable(dbName, tblName)) {	
not caching database s table 

sharedCache.addPartitionColStatsToCache(colStatsForDB);	}	Database db = rawStore.getDatabase(dbName);	sharedCache.addDatabaseToCache(dbName, db);	List<String> tblNames = rawStore.getAllTables(dbName);	for (int j = 0; j < tblNames.size(); j++) {	String tblName = StringUtils.normalizeIdentifier(tblNames.get(j));	if (!shouldCacheTable(dbName, tblName)) {	continue;	}	
caching database s table cached tables so far 

public void run() {	if (isFirstRun) {	while (isFirstRun) {	try {	long startTime = System.nanoTime();	
prewarming cachedstore 

public void run() {	if (isFirstRun) {	while (isFirstRun) {	try {	long startTime = System.nanoTime();	prewarm(rawStore);	
cachedstore initialized 

public void run() {	if (isFirstRun) {	while (isFirstRun) {	try {	long startTime = System.nanoTime();	prewarm(rawStore);	long endTime = System.nanoTime();	LOG.info("Time taken in prewarming = " + (endTime - startTime) / 1000000 + "ms");	} catch (Exception e) {	
prewarm failure 

public void update() {	Deadline.registerIfNot(1000000);	
cachedstore updating cached objects 

if (!shouldCacheTable(dbName, tblName)) {	continue;	}	updateTablePartitions(rawStore, dbName, tblName);	updateTableColStats(rawStore, dbName, tblName);	updateAggregateStatsCache(rawStore, dbName, tblName);	}	}	}	} catch (Exception e) {	
updating cachedstore error happen when refresh ignoring 

private void updateDatabasePartitionColStats(RawStore rawStore, String dbName) {	try {	Deadline.startTimer("getColStatsForDatabasePartitions");	List<ColStatsObjWithSourceInfo> colStatsForDB = rawStore.getPartitionColStatsForDatabase(dbName);	Deadline.stopTimer();	if (colStatsForDB != null) {	if (partitionColStatsCacheLock.writeLock().tryLock()) {	if (isPartitionColStatsCacheDirty.compareAndSet(true, false)) {	
skipping partition column stats cache update the partition column stats list we have is dirty 

Deadline.stopTimer();	if (colStatsForDB != null) {	if (partitionColStatsCacheLock.writeLock().tryLock()) {	if (isPartitionColStatsCacheDirty.compareAndSet(true, false)) {	return;	}	sharedCacheWrapper.getUnsafe() .refreshPartitionColStats(StringUtils.normalizeIdentifier(dbName), colStatsForDB);	}	}	} catch (MetaException | NoSuchObjectException e) {	
updating cachedstore unable to read partitions column stats of database 

partVals.add(defaultPartitionValue);	}	String defaultPartitionName = FileUtils.makePartName(partCols, partVals);	partNames.remove(defaultPartitionName);	Deadline.startTimer("getAggregareStatsForAllPartitionsExceptDefault");	AggrStats aggrStatsAllButDefaultPartition = rawStore.get_aggr_stats_for(dbName, tblName, partNames, colNames);	Deadline.stopTimer();	if ((aggrStatsAllPartitions != null) && (aggrStatsAllButDefaultPartition != null)) {	if (partitionAggrColStatsCacheLock.writeLock().tryLock()) {	if (isPartitionAggrColStatsCacheDirty.compareAndSet(true, false)) {	
skipping aggregate column stats cache update the aggregate column stats we have is dirty 

if ((aggrStatsAllPartitions != null) && (aggrStatsAllButDefaultPartition != null)) {	if (partitionAggrColStatsCacheLock.writeLock().tryLock()) {	if (isPartitionAggrColStatsCacheDirty.compareAndSet(true, false)) {	return;	}	sharedCacheWrapper.getUnsafe().refreshAggregateStatsCache( StringUtils.normalizeIdentifier(dbName), StringUtils.normalizeIdentifier(tblName), aggrStatsAllPartitions, aggrStatsAllButDefaultPartition);	}	}	}	} catch (MetaException | NoSuchObjectException e) {	
updating cachedstore unable to read aggregate column stats of table 

private void updateDatabases(RawStore rawStore, List<String> dbNames) {	List<Database> databases = new ArrayList<>();	for (String dbName : dbNames) {	Database db;	try {	db = rawStore.getDatabase(dbName);	databases.add(db);	} catch (NoSuchObjectException e) {	
updating cachedstore database does not exist 

Database db;	try {	db = rawStore.getDatabase(dbName);	databases.add(db);	} catch (NoSuchObjectException e) {	}	}	try {	if (databaseCacheLock.writeLock().tryLock()) {	if (isDatabaseCacheDirty.compareAndSet(true, false)) {	
skipping database cache update the database list we have is dirty 

List<String> tblNames = rawStore.getAllTables(dbName);	for (String tblName : tblNames) {	if (!shouldCacheTable(dbName, tblName)) {	continue;	}	Table table = rawStore.getTable(StringUtils.normalizeIdentifier(dbName), StringUtils.normalizeIdentifier(tblName));	tables.add(table);	}	if (tableCacheLock.writeLock().tryLock()) {	if (isTableCacheDirty.compareAndSet(true, false)) {	
skipping table cache update the table list we have is dirty 

Table table = rawStore.getTable(StringUtils.normalizeIdentifier(dbName), StringUtils.normalizeIdentifier(tblName));	tables.add(table);	}	if (tableCacheLock.writeLock().tryLock()) {	if (isTableCacheDirty.compareAndSet(true, false)) {	return;	}	sharedCacheWrapper.getUnsafe().refreshTables(dbName, tables);	}	} catch (MetaException e) {	
updating cachedstore unable to read tables for database 

private void updateTablePartitions(RawStore rawStore, String dbName, String tblName) {	try {	Deadline.startTimer("getPartitions");	List<Partition> partitions = rawStore.getPartitions(dbName, tblName, Integer.MAX_VALUE);	Deadline.stopTimer();	if (partitionCacheLock.writeLock().tryLock()) {	if (isPartitionCacheDirty.compareAndSet(true, false)) {	
skipping partition cache update the partition list we have is dirty 

Deadline.startTimer("getPartitions");	List<Partition> partitions = rawStore.getPartitions(dbName, tblName, Integer.MAX_VALUE);	Deadline.stopTimer();	if (partitionCacheLock.writeLock().tryLock()) {	if (isPartitionCacheDirty.compareAndSet(true, false)) {	return;	}	sharedCacheWrapper.getUnsafe().refreshPartitions( StringUtils.normalizeIdentifier(dbName), StringUtils.normalizeIdentifier(tblName), partitions);	}	} catch (MetaException | NoSuchObjectException e) {	
updating cachedstore unable to read partitions of table 

private void updateTableColStats(RawStore rawStore, String dbName, String tblName) {	try {	Table table = rawStore.getTable(dbName, tblName);	List<String> colNames = MetaStoreUtils.getColumnNamesForTable(table);	Deadline.startTimer("getTableColumnStatistics");	ColumnStatistics tableColStats = rawStore.getTableColumnStatistics(dbName, tblName, colNames);	Deadline.stopTimer();	if (tableColStats != null) {	if (tableColStatsCacheLock.writeLock().tryLock()) {	if (isTableColStatsCacheDirty.compareAndSet(true, false)) {	
skipping table column stats cache update the table column stats list we have is dirty 

Deadline.stopTimer();	if (tableColStats != null) {	if (tableColStatsCacheLock.writeLock().tryLock()) {	if (isTableColStatsCacheDirty.compareAndSet(true, false)) {	return;	}	sharedCacheWrapper.getUnsafe().refreshTableColStats( StringUtils.normalizeIdentifier(dbName), StringUtils.normalizeIdentifier(tblName), tableColStats.getStatsObj());	}	}	} catch (MetaException | NoSuchObjectException e) {	
updating cachedstore unable to read table column stats of table 

static boolean isNotInBlackList(String dbName, String tblName) {	String str = dbName + "." + tblName;	for (Pattern pattern : blacklistPatterns) {	
trying to match against blacklist pattern 

static boolean isNotInBlackList(String dbName, String tblName) {	String str = dbName + "." + tblName;	for (Pattern pattern : blacklistPatterns) {	Matcher matcher = pattern.matcher(str);	if (matcher.matches()) {	
found matcher group at start index and end index 

static boolean isInWhitelist(String dbName, String tblName) {	String str = dbName + "." + tblName;	for (Pattern pattern : whitelistPatterns) {	
trying to match against whitelist pattern 

static boolean isInWhitelist(String dbName, String tblName) {	String str = dbName + "." + tblName;	for (Pattern pattern : whitelistPatterns) {	Matcher matcher = pattern.matcher(str);	if (matcher.matches()) {	
found matcher group at start index and end index 

static boolean shouldCacheTable(String dbName, String tblName) {	if (!isNotInBlackList(dbName, tblName)) {	
is in blacklist skipping 

static boolean shouldCacheTable(String dbName, String tblName) {	if (!isNotInBlackList(dbName, tblName)) {	return false;	}	if (!isInWhitelist(dbName, tblName)) {	
is not in whitelist skipping 

========================= hive sample_1859 =========================

public void killQuery(String queryId, String errMsg) throws HiveException {	try {	Operation operation = operationManager.getOperationByQueryId(queryId);	if (operation == null) {	
query not found 

========================= hive sample_2326 =========================

tbl.setDataLocation(new Path(getLocation()));	}	if (getStorageHandler() != null) {	tbl.setProperty( org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_STORAGE, getStorageHandler());	}	HiveStorageHandler storageHandler = tbl.getStorageHandler();	String serDeClassName;	if (getSerde() == null) {	if (storageHandler == null) {	serDeClassName = PlanUtils.getDefaultSerDe().getName();	
default to for materialized view 

if (getStorageHandler() != null) {	tbl.setProperty( org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_STORAGE, getStorageHandler());	}	HiveStorageHandler storageHandler = tbl.getStorageHandler();	String serDeClassName;	if (getSerde() == null) {	if (storageHandler == null) {	serDeClassName = PlanUtils.getDefaultSerDe().getName();	} else {	serDeClassName = storageHandler.getSerDeClass().getName();	
use storagehandler supplied for materialized view 

========================= hive sample_3188 =========================

LazyAccumuloMap map = (LazyAccumuloMap) field;	map.init(row, mapColumnMapping);	} else {	byte[] value;	if (columnMapping instanceof HiveAccumuloRowIdColumnMapping) {	value = row.getRowId().getBytes();	} else if (columnMapping instanceof HiveAccumuloColumnMapping) {	HiveAccumuloColumnMapping accumuloColumnMapping = (HiveAccumuloColumnMapping) columnMapping;	value = row.getValue( new Text(accumuloColumnMapping.getColumnFamilyBytes()), new Text(accumuloColumnMapping.getColumnQualifierBytes()));	} else {	
could not process columnmapping of type at offset in column mapping 

========================= hive sample_178 =========================

public void testRpcDispatcher() throws Exception {	Rpc serverRpc = autoClose(Rpc.createEmbedded(new TestDispatcher()));	Rpc clientRpc = autoClose(Rpc.createEmbedded(new TestDispatcher()));	TestMessage outbound = new TestMessage("Hello World!");	Future<TestMessage> call = clientRpc.call(outbound, TestMessage.class);	
transferring messages 

EmbeddedChannel client = (EmbeddedChannel) clientRpc.getChannel();	EmbeddedChannel server = (EmbeddedChannel) serverRpc.getChannel();	server.runPendingTasks();	client.runPendingTasks();	int count = 0;	while (!client.outboundMessages().isEmpty()) {	server.writeInbound(client.readOutbound());	count++;	}	server.flush();	
transferred outbound client messages 

server.writeInbound(client.readOutbound());	count++;	}	server.flush();	count = 0;	while (!server.outboundMessages().isEmpty()) {	client.writeInbound(server.readOutbound());	count++;	}	client.flush();	
transferred outbound server messages 

========================= hive sample_528 =========================

public static void main(String[] args) throws Exception {	LOG.info("LLAP service driver invoked with arguments={}", args);	int ret = 0;	try {	ret = new LlapServiceDriver().run(args);	} catch (Throwable t) {	System.err.println("Failed: " + t.getMessage());	t.printStackTrace();	ret = 3;	} finally {	
llap service driver finished 

int ret = 0;	try {	ret = new LlapServiceDriver().run(args);	} catch (Throwable t) {	System.err.println("Failed: " + t.getMessage());	t.printStackTrace();	ret = 3;	} finally {	}	if (LOG.isDebugEnabled()) {	
completed processing exiting with 

static void populateConfWithLlapProperties(Configuration conf, Properties properties) {	for(Entry<Object, Object> props : properties.entrySet()) {	String key = (String) props.getKey();	if (HiveConf.getLlapDaemonConfVars().contains(key)) {	conf.set(key, (String) props.getValue());	} else {	if (key.startsWith(HiveConf.PREFIX_LLAP) || key.startsWith(HiveConf.PREFIX_HIVE_LLAP)) {	
adding key even though it is not in the set of known llap server keys 

static void populateConfWithLlapProperties(Configuration conf, Properties properties) {	for(Entry<Object, Object> props : properties.entrySet()) {	String key = (String) props.getKey();	if (HiveConf.getLlapDaemonConfVars().contains(key)) {	conf.set(key, (String) props.getValue());	} else {	if (key.startsWith(HiveConf.PREFIX_LLAP) || key.startsWith(HiveConf.PREFIX_HIVE_LLAP)) {	conf.set(key, (String) props.getValue());	} else {	
ignoring unknown llap server parameter 

size = Math.min((long)(heapSize * 1.2), heapSize + 1024L*1024*1024);	if (isDirect) {	size += cache;	}	}	long containerSize = size / (1024 * 1024);	final long minAlloc = conf.getInt(YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_MB, -1);	Preconditions.checkArgument(containerSize >= minAlloc, "Container size (" + LlapUtil.humanReadableByteCount(options.getSize()) + ") should be greater" + " than minimum allocation(" + LlapUtil.humanReadableByteCount(minAlloc * 1024L * 1024L) + ")");	conf.setLong(ConfVars.LLAP_DAEMON_YARN_CONTAINER_MB.varname, containerSize);	propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_YARN_CONTAINER_MB.varname, String.valueOf(containerSize));	
memory settings container memory executor memory cache memory 

final URL logger = conf.getResource(LlapConstants.LOG4j2_PROPERTIES_FILE);	if (null == logger) {	throw new Exception("Unable to find required config file: llap-daemon-log4j2.properties");	}	Path home = new Path(System.getenv("HIVE_HOME"));	Path scriptParent = new Path(new Path(home, "scripts"), "llap");	Path scripts = new Path(scriptParent, "bin");	if (!lfs.exists(home)) {	throw new Exception("Unable to find HIVE_HOME:" + home);	} else if (!lfs.exists(scripts)) {	
unable to find llap scripts 

Path scripts = new Path(scriptParent, "bin");	if (!lfs.exists(home)) {	throw new Exception("Unable to find HIVE_HOME:" + home);	} else if (!lfs.exists(scripts)) {	}	final Path libDir = new Path(tmpDir, "lib");	final Path tezDir = new Path(libDir, "tez");	final Path udfDir = new Path(libDir, "udfs");	final Path confPath = new Path(tmpDir, "conf");	if (!lfs.mkdirs(confPath)) {	
mkdirs for returned false 

throw new Exception("Unable to find HIVE_HOME:" + home);	} else if (!lfs.exists(scripts)) {	}	final Path libDir = new Path(tmpDir, "lib");	final Path tezDir = new Path(libDir, "tez");	final Path udfDir = new Path(libDir, "udfs");	final Path confPath = new Path(tmpDir, "conf");	if (!lfs.mkdirs(confPath)) {	}	if (!lfs.mkdirs(tezDir)) {	
mkdirs for returned false 

}	final Path libDir = new Path(tmpDir, "lib");	final Path tezDir = new Path(libDir, "tez");	final Path udfDir = new Path(libDir, "udfs");	final Path confPath = new Path(tmpDir, "conf");	if (!lfs.mkdirs(confPath)) {	}	if (!lfs.mkdirs(tezDir)) {	}	if (!lfs.mkdirs(udfDir)) {	
mkdirs for returned false 

}	if (!lfs.mkdirs(tezDir)) {	}	if (!lfs.mkdirs(udfDir)) {	}	NamedCallable<Void> downloadTez = new NamedCallable<Void>("downloadTez") {	public Void call() throws Exception {	synchronized (fs) {	String tezLibs = conf.get(TezConfiguration.TEZ_LIB_URIS);	if (tezLibs == null) {	
missing tez lib uris in tez site xml 

}	if (!lfs.mkdirs(udfDir)) {	}	NamedCallable<Void> downloadTez = new NamedCallable<Void>("downloadTez") {	public Void call() throws Exception {	synchronized (fs) {	String tezLibs = conf.get(TezConfiguration.TEZ_LIB_URIS);	if (tezLibs == null) {	}	if (LOG.isDebugEnabled()) {	
copying tez libs from 

return null;	}	};	NamedCallable<Void> copyLocalJars = new NamedCallable<Void>("copyLocalJars") {	public Void call() throws Exception {	Class<?>[] dependencies = new Class<?>[] { LlapDaemonProtocolProtos.class, LlapTezUtils.class, LlapInputFormat.class, HiveInputFormat.class, SslContextFactory.class, Rule.class, RegistryUtils.ServiceRecordMarshal.class, com.lmax.disruptor.RingBuffer.class, org.apache.logging.log4j.Logger.class, org.apache.logging.log4j.core.Appender.class, org.apache.logging.slf4j.Log4jLogger.class, org.apache.log4j.NDC.class, io.netty.util.NetUtil.class };	for (Class<?> c : dependencies) {	Path jarPath = new Path(Utilities.jarFinderGetJar(c));	lfs.copyFromLocalFile(jarPath, libDir);	if (LOG.isDebugEnabled()) {	
copying to 

String err = "Failed to add HBase jars. Use --auxhbase=false to avoid localizing them";	LOG.error(err);	System.err.println(err);	throw new RuntimeException(t);	}	}	HashSet<String> auxJars = new HashSet<>();	if (options.getIsHiveAux()) {	addAuxJarsToSet(auxJars, conf.getAuxJars(), ",");	addAuxJarsToSet(auxJars, System.getenv("HIVE_AUX_JARS_PATH"), ":");	
adding the following aux jars from the environment and configs 

java_home = jre_home;	} else if (!java_home.equals(jre_home)) {	LOG.warn("Java versions might not match : JAVA_HOME=[{}],process jre=[{}]", java_home, jre_home);	}	} else {	java_home = options.getJavaPath();	}	if (java_home == null || java_home.isEmpty()) {	throw new RuntimeException( "Could not determine JAVA_HOME from command line parameters, environment or system properties");	}	
using for java home 

}	NamedCallable<Void> copyConfigs = new NamedCallable<Void>("copyConfigs") {	public Void call() throws Exception {	for (String f : NEEDED_CONFIGS) {	copyConfig(lfs, confPath, f);	}	for (String f : OPTIONAL_CONFIGS) {	try {	copyConfig(lfs, confPath, f);	} catch (Throwable t) {	
error getting an optional config ignoring 

};	final NamedCallable<Void>[] asyncWork = new NamedCallable[] {	downloadTez, copyUdfJars, copyLocalJars, copyAuxJars, copyConfigs };	final Future<Void>[] asyncResults = new Future[asyncWork.length];	for (int i = 0; i < asyncWork.length; i++) {	asyncResults[i] = asyncRunner.submit(asyncWork[i]);	}	JSONObject configs = createConfigJson(containerSize, cache, xmx, java_home);	writeConfigJson(tmpDir, lfs, configs);	if (LOG.isDebugEnabled()) {	
config generation took ns 

}	JSONObject configs = createConfigJson(containerSize, cache, xmx, java_home);	writeConfigJson(tmpDir, lfs, configs);	if (LOG.isDebugEnabled()) {	}	for (int i = 0; i < asyncWork.length; i++) {	final long t1 = System.nanoTime();	asyncResults[i].get();	final long t2 = System.nanoTime();	if (LOG.isDebugEnabled()) {	
waited for ns 

} else {	rc = 0;	}	} finally {	executor.shutdown();	lfs.close();	fs.close();	}	if (rc == 0) {	if (LOG.isDebugEnabled()) {	
exiting successfully 

scriptArgs.add("python");	scriptArgs.add(scriptPath.toString());	scriptArgs.add("--input");	scriptArgs.add(tmpDir.toString());	scriptArgs.add("--output");	scriptArgs.add(outputDir);	scriptArgs.add("--javaChild");	for (String arg : args) {	scriptArgs.add(arg);	}	
calling package py via 

HiveConf hiveConf = new HiveConf();	hiveConf.setBoolVar(HiveConf.ConfVars.METASTORE_INIT_METADATA_COUNT_ENABLED, false);	hiveConf.setBoolVar(HiveConf.ConfVars.METASTORE_METRICS, false);	Hive hive = Hive.getWithFastCheck(hiveConf, false);	ResourceDownloader resourceDownloader = new ResourceDownloader(conf, udfDir.toUri().normalize().getPath());	List<Function> fns = hive.getAllFunctions();	Set<URI> srcUris = new HashSet<>();	for (Function fn : fns) {	String fqfn = fn.getDbName() + "." + fn.getFunctionName();	if (udfs.containsKey(fn.getClassName())) {	
duplicate function names found for with and 

ResourceDownloader resourceDownloader = new ResourceDownloader(conf, udfDir.toUri().normalize().getPath());	List<Function> fns = hive.getAllFunctions();	Set<URI> srcUris = new HashSet<>();	for (Function fn : fns) {	String fqfn = fn.getDbName() + "." + fn.getFunctionName();	if (udfs.containsKey(fn.getClassName())) {	}	udfs.put(fn.getClassName(), fqfn);	List<ResourceUri> resources = fn.getResourceUris();	if (resources == null || resources.isEmpty()) {	
missing resources for 

if (resources == null || resources.isEmpty()) {	continue;	}	for (ResourceUri resource : resources) {	srcUris.add(ResourceDownloader.createURI(resource.getUri()));	}	}	for (URI srcUri : srcUris) {	List<URI> localUris = resourceDownloader.downloadExternal(srcUri, null, false);	for(URI dst : localUris) {	
downloaded from 

========================= hive sample_2214 =========================

} catch (Exception ex) {	throw new IOException("Failed to load queue allocations", ex);	}	if (allocConf.get() == null) {	allocConf.set(new AllocationConfiguration(conf));	}	QueuePlacementPolicy queuePolicy = allocConf.get().getPlacementPolicy();	if (queuePolicy != null) {	requestedQueue = queuePolicy.assignAppToQueue(requestedQueue, userName);	if (StringUtils.isNotBlank(requestedQueue)) {	
setting queue name to for user 

========================= hive sample_1438 =========================

protected static RowResolver buildRowResolverForNoop(String tabAlias, StructObjectInspector rowObjectInspector, RowResolver inputRowResolver) throws SemanticException {	
querytranslationinfo getrowresolver invoked on objectinspector 

========================= hive sample_3405 =========================

public static void stopMetaStores() throws Exception {	for(AbstractMetaStoreService metaStoreService : metaStoreServices) {	try {	metaStoreService.stop();	} catch(Exception e) {	
error stopping metastoreservice 

========================= hive sample_1592 =========================

public ParseContext transform(ParseContext pctx) throws SemanticException {	if (hooks != null && hooks.contains(ATLAS_HOOK_CLASSNAME)) {	if (!pctx.getQueryProperties().isCTAS() && !pctx.getQueryProperties().isMaterializedView() && pctx.getQueryProperties().isQuery() && pctx.getCreateTable() == null && pctx.getCreateViewDesc() == null && (pctx.getLoadTableWork() == null || pctx.getLoadTableWork().isEmpty())) {	
not evaluating lineage 

========================= hive sample_3055 =========================

public void prune() throws SerDeException, IOException, InterruptedException, HiveException {	synchronized(sourcesWaitingForEvents) {	if (sourcesWaitingForEvents.isEmpty()) {	return;	}	Set<VertexState> states = Collections.singleton(VertexState.SUCCEEDED);	for (String source : sourcesWaitingForEvents) {	context.registerForVertexStateUpdates(source, states);	}	}	
waiting for events sources 

if (sourcesWaitingForEvents.isEmpty()) {	return;	}	Set<VertexState> states = Collections.singleton(VertexState.SUCCEEDED);	for (String source : sourcesWaitingForEvents) {	context.registerForVertexStateUpdates(source, states);	}	}	this.processEvents();	this.prunePartitions();	
ok to proceed 

private void prunePartitions() throws HiveException {	int expectedEvents = 0;	for (Map.Entry<String, List<SourceInfo>> entry : this.sourceInfoMap.entrySet()) {	String source = entry.getKey();	for (SourceInfo si : entry.getValue()) {	int taskNum = context.getVertexNumTasks(source);	
expecting events for vertex for column 

int expectedEvents = 0;	for (Map.Entry<String, List<SourceInfo>> entry : this.sourceInfoMap.entrySet()) {	String source = entry.getKey();	for (SourceInfo si : entry.getValue()) {	int taskNum = context.getVertexNumTasks(source);	expectedEvents += taskNum;	prunePartitionSingleSource(source, si);	}	}	if (expectedEvents != totalEventCount) {	
expecting received 

protected void prunePartitionSingleSource(String source, SourceInfo si) throws HiveException {	if (si.skipPruning.get()) {	
skip pruning on column 

Map<String, String> spec = desc.getPartSpec();	if (spec == null) {	throw new IllegalStateException("No partition spec found in dynamic pruning");	}	String partValueString = spec.get(columnName);	if (partValueString == null) {	throw new IllegalStateException("Could not find partition value for column: " + columnName);	}	Object partValue = converter.convert(partValueString);	if (LOG.isDebugEnabled()) {	
converted partition value original 

String partValueString = spec.get(columnName);	if (partValueString == null) {	throw new IllegalStateException("Could not find partition value for column: " + columnName);	}	Object partValue = converter.convert(partValueString);	if (LOG.isDebugEnabled()) {	}	row[0] = partValue;	partValue = eval.evaluate(row);	if (LOG.isDebugEnabled()) {	
part key expr applied 

throw new IllegalStateException("Could not find partition value for column: " + columnName);	}	Object partValue = converter.convert(partValueString);	if (LOG.isDebugEnabled()) {	}	row[0] = partValue;	partValue = eval.evaluate(row);	if (LOG.isDebugEnabled()) {	}	if (!values.contains(partValue)) {	
pruning path 

public SourceInfo(TableDesc table, ExprNodeDesc partKey, String columnName, String columnType, JobConf jobConf) throws SerDeException {	this.skipPruning.set(false);	this.partKey = partKey;	this.columnName = columnName;	this.columnType = columnType;	deserializer = ReflectionUtils.newInstance(table.getDeserializerClass(), null);	deserializer.initialize(jobConf, table.getProperties());	ObjectInspector inspector = deserializer.getObjectInspector();	
type of obj insp 

this.skipPruning.set(false);	this.partKey = partKey;	this.columnName = columnName;	this.columnType = columnType;	deserializer = ReflectionUtils.newInstance(table.getDeserializerClass(), null);	deserializer.initialize(jobConf, table.getProperties());	ObjectInspector inspector = deserializer.getObjectInspector();	soi = (StructObjectInspector) inspector;	List<? extends StructField> fields = soi.getAllStructFieldRefs();	if (fields.size() > 1) {	
expecting single field in input 

private void processEvents() throws SerDeException, IOException, InterruptedException {	int eventCount = 0;	while (true) {	Object element = queue.take();	if (element == endOfEvents) {	break;	}	InputInitializerEvent event = (InputInitializerEvent) element;	
input event 

int eventCount = 0;	while (true) {	Object element = queue.take();	if (element == endOfEvents) {	break;	}	InputInitializerEvent event = (InputInitializerEvent) element;	processPayload(event.getUserPayload(), event.getSourceVertexName());	eventCount += 1;	}	
received events 

protected String processPayload(ByteBuffer payload, String sourceName) throws SerDeException, IOException {	DataInputStream in = new DataInputStream(new ByteBufferBackedInputStream(payload));	try {	String columnName = in.readUTF();	
source of event 

boolean skip = in.readBoolean();	if (skip) {	info.skipPruning.set(true);	} else {	while (payload.hasRemaining()) {	writable.readFields(in);	Object row = info.deserializer.deserialize(writable);	Object value = info.soi.getStructFieldData(row, info.field);	value = ObjectInspectorUtils.copyToStandardObject(value, info.fieldInspector);	if (LOG.isDebugEnabled()) {	
adding to list of required partitions 

public void processVertex(String name) {	
vertex succeeded 

return;	} else {	int processedEvents = numEventsSeenPerSource.get(name).getValue();	if (processedEvents == expectedEvents) {	sourcesWaitingForEvents.remove(name);	if (sourcesWaitingForEvents.isEmpty()) {	if(!queue.offer(endOfEvents)) {	throw new IllegalStateException("Queue full");	}	} else {	
waiting for sources 

========================= hive sample_4006 =========================

public static void validatePartColumnType(Table tbl, Map<String, String> partSpec, ASTNode astNode, HiveConf conf) throws SemanticException {	if (!HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_TYPE_CHECK_ON_INSERT)) {	return;	}	Map<ASTNode, ExprNodeDesc> astExprNodeMap = new HashMap<ASTNode, ExprNodeDesc>();	if (!getPartExprNodeDesc(astNode, conf, astExprNodeMap)) {	
dynamic partitioning is used only validating columns 

TypeInfo expectedType = TypeInfoUtils.getTypeInfoFromTypeString(colType);	ObjectInspector outputOI = TypeInfoUtils.getStandardJavaObjectInspectorFromTypeInfo(expectedType);	Object value = ((ExprNodeConstantDesc)astExprNodePair.getValue()).getValue();	Object convertedValue = value;	if (!inputOI.getTypeName().equals(outputOI.getTypeName())) {	convertedValue = ObjectInspectorConverters.getConverter(inputOI, outputOI).convert(value);	if (convertedValue == null) {	throw new SemanticException(ErrorMsg.PARTITION_SPEC_TYPE_MISMATCH, astKeyName, inputOI.getTypeName(), outputOI.getTypeName());	}	if (!convertedValue.toString().equals(value.toString())) {	
partition expects type but input value is in type convert to 

static void normalizeColSpec(Map<String, String> partSpec, String colName, String colType, String originalColSpec, Object colValue) throws SemanticException {	if (colValue == null) {	return;	}	String normalizedColSpec = originalColSpec;	if (colType.equals(serdeConstants.DATE_TYPE_NAME)) {	normalizedColSpec = normalizeDateCol(colValue, originalColSpec);	}	if (!normalizedColSpec.equals(originalColSpec)) {	
normalizing partition spec from to 

========================= hive sample_3393 =========================

if (setUpComplete) {	return;	}	ByteArrayOutputStream out = new ByteArrayOutputStream();	TIOStreamTransport transport = new TIOStreamTransport(out);	TBinaryProtocol protocol = new TBinaryProtocol(transport);	IntString intString = new IntString(1, "one", 1);	intString.write(protocol);	BytesWritable bytesWritable = new BytesWritable(out.toByteArray());	intStringSeq = new Path(TEST_DATA_DIR + "/data/intString.seq");	
creating data file 

========================= hive sample_226 =========================

public void testSimpleOperation() throws Exception {	String typeString = "struct<name:string,studentid:int," + "contact:struct<phNo:string,email:string>," + "currently_registered_courses:array<string>," + "current_grades:map<string,string>," + "phNos:array<struct<phNo:string,type:string>>,blah:array<int>>";	TypeInfo ti = TypeInfoUtils.getTypeInfoFromTypeString(typeString);	HCatSchema hsch = HCatSchemaUtils.getHCatSchemaFromTypeString(typeString);	
type name 

public void testSimpleOperation() throws Exception {	String typeString = "struct<name:string,studentid:int," + "contact:struct<phNo:string,email:string>," + "currently_registered_courses:array<string>," + "current_grades:map<string,string>," + "phNos:array<struct<phNo:string,type:string>>,blah:array<int>>";	TypeInfo ti = TypeInfoUtils.getTypeInfoFromTypeString(typeString);	HCatSchema hsch = HCatSchemaUtils.getHCatSchemaFromTypeString(typeString);	
hcatschema 

========================= hive sample_676 =========================

public static LlapClusterStateForCompile getClusterInfo(final Configuration conf) {	final String nodes = HiveConf.getTrimmedVar(conf, HiveConf.ConfVars.LLAP_DAEMON_SERVICE_HOSTS);	final String userName = HiveConf.getVar( conf, ConfVars.LLAP_ZK_REGISTRY_USER, LlapRegistryService.currentUser());	Callable<LlapClusterStateForCompile> generator = new Callable<LlapClusterStateForCompile>() {	public LlapClusterStateForCompile call() throws Exception {	
creating cluster info for 

public boolean initClusterInfo() {	if (!isUpdateNeeded()) return true;	synchronized (updateInfoLock) {	if (!isUpdateNeeded()) return true;	if (svc == null) {	try {	svc = LlapRegistryService.getClient(conf);	} catch (Throwable t) {	
cannot create the client ignoring 

try {	svc = LlapRegistryService.getClient(conf);	} catch (Throwable t) {	return false;	}	}	LlapServiceInstanceSet instances;	try {	instances = svc.getInstances(10);	} catch (IOException e) {	
cannot update cluster information ignoring 

========================= hive sample_2994 =========================

private void handleWork(TezWork tezWork, BaseWork work) throws SemanticException {	boolean workCanBeDoneInLlap = evaluateWork(tezWork, work);	
work can cannot be done in llap 

private void handleWork(TezWork tezWork, BaseWork work) throws SemanticException {	boolean workCanBeDoneInLlap = evaluateWork(tezWork, work);	if (workCanBeDoneInLlap) {	for (MapJoinOperator graceMapJoinOp : mapJoinOpList) {	
disabling hybrid grace hash join in case of llap and non dynamic partition hash join 

private void adjustAutoParallelism(BaseWork work) {	if (minReducersPerExec <= 0 || !(work instanceof ReduceWork)) return;	ReduceWork reduceWork = (ReduceWork)work;	if (reduceWork.isAutoReduceParallelism() == false && reduceWork.isUniformDistribution() == false) {	return;	}	clusterState.initClusterInfo();	int targetCount = 0;	if (!clusterState.hasClusterInfo()) {	
cannot determine llap cluster information 

private void convertWork(TezWork tezWork, BaseWork work) throws SemanticException {	if (shouldUber) {	if (tezWork.getChildren(work).isEmpty() && work instanceof ReduceWork && ((ReduceWork) work).getNumReduceTasks() == 1) {	
converting work to uber 

private boolean evaluateWork(TezWork tezWork, BaseWork work) throws SemanticException {	
evaluating work item 

private boolean checkExpression(ExprNodeDesc expr) {	Deque<ExprNodeDesc> exprs = new LinkedList<ExprNodeDesc>();	exprs.add(expr);	while (!exprs.isEmpty()) {	if (LOG.isDebugEnabled()) {	
checking s 

ExprNodeDesc cur = exprs.removeFirst();	if (cur == null) continue;	if (cur.getChildren() != null) {	exprs.addAll(cur.getChildren());	}	if (!doSkipUdfCheck && cur instanceof ExprNodeGenericFuncDesc) {	ExprNodeGenericFuncDesc funcDesc = (ExprNodeGenericFuncDesc)cur;	boolean isBuiltIn = FunctionRegistry.isBuiltInFuncExpr(funcDesc);	if (!isBuiltIn) {	if (!arePermanentFnsAllowed) {	
not a built in function permanent functions are disabled 

exprs.addAll(cur.getChildren());	}	if (!doSkipUdfCheck && cur instanceof ExprNodeGenericFuncDesc) {	ExprNodeGenericFuncDesc funcDesc = (ExprNodeGenericFuncDesc)cur;	boolean isBuiltIn = FunctionRegistry.isBuiltInFuncExpr(funcDesc);	if (!isBuiltIn) {	if (!arePermanentFnsAllowed) {	return false;	}	if (!FunctionRegistry.isPermanentFunction(funcDesc)) {	
not a built in or permanent function 

private boolean checkAggregator(AggregationDesc agg) throws SemanticException {	if (LOG.isDebugEnabled()) {	
checking s 

private boolean checkAggregator(AggregationDesc agg) throws SemanticException {	if (LOG.isDebugEnabled()) {	}	boolean result = checkExpressions(agg.getParameters());	FunctionInfo fi = FunctionRegistry.getFunctionInfo(agg.getGenericUDAFName());	result = result && (fi != null) && fi.isNative();	if (!result) {	
aggregator is not native 

private boolean checkAggregators(Collection<AggregationDesc> aggs) {	try {	for (AggregationDesc agg: aggs) {	if (!checkAggregator(agg)) return false;	}	} catch (SemanticException e) {	
exception testing aggregators 

private Map<Rule, NodeProcessor> getRules() {	Map<Rule, NodeProcessor> opRules = new LinkedHashMap<Rule, NodeProcessor>();	opRules.put(new RuleRegExp("No scripts", ScriptOperator.getOperatorName() + "%"), new NodeProcessor() {	public Object process(Node n, Stack<Node> s, NodeProcessorCtx c, Object... os) {	
cannot run operator in llap mode 

opRules.put(new RuleRegExp("No scripts", ScriptOperator.getOperatorName() + "%"), new NodeProcessor() {	public Object process(Node n, Stack<Node> s, NodeProcessorCtx c, Object... os) {	return new Boolean(false);	}	});	opRules.put(new RuleRegExp("No user code in fil", FilterOperator.getOperatorName() + "%"), new NodeProcessor() {	public Object process(Node n, Stack<Node> s, NodeProcessorCtx c, Object... os) {	ExprNodeDesc expr = ((FilterOperator)n).getConf().getPredicate();	Boolean retval = new Boolean(checkExpression(expr));	if (!retval) {	
cannot run filter operator in llap mode 

if (!retval) {	}	return new Boolean(retval);	}	});	opRules.put(new RuleRegExp("No user code in gby", GroupByOperator.getOperatorName() + "%"), new NodeProcessor() {	public Object process(Node n, Stack<Node> s, NodeProcessorCtx c, Object... os) {	List<AggregationDesc> aggs = ((Operator<GroupByDesc>) n).getConf().getAggregators();	Boolean retval = new Boolean(checkAggregators(aggs));	if (!retval) {	
cannot run group by operator in llap mode 

if (!retval) {	}	return new Boolean(retval);	}	});	opRules.put(new RuleRegExp("No user code in select", SelectOperator.getOperatorName() + "%"), new NodeProcessor() {	public Object process(Node n, Stack<Node> s, NodeProcessorCtx c, Object... os) {	List<ExprNodeDesc> exprs = ((Operator<SelectDesc>) n).getConf().getColList();	Boolean retval = new Boolean(checkExpressions(exprs));	if (!retval) {	
cannot run select operator in llap mode 

private boolean checkParentsInLlap(TezWork tezWork, BaseWork base) {	for (BaseWork w: tezWork.getParents(base)) {	if (!w.getLlapMode()) {	
not all parents are run in llap 

private boolean checkInputsVectorized(MapWork mapWork) {	boolean mayWrap = HiveConf.getBoolVar(conf, ConfVars.LLAP_IO_NONVECTOR_WRAPPER_ENABLED);	Collection<Class<?>> excludedInputFormats = Utilities.getClassNamesFromConfig(conf, ConfVars.HIVE_VECTORIZATION_VECTORIZED_INPUT_FILE_FORMAT_EXCLUDES);	for (PartitionDesc pd : mapWork.getPathToPartitionInfo().values()) {	if ((Utilities.isInputFileFormatVectorized(pd) && !excludedInputFormats .contains(pd.getInputFileFormatClass())) || (mayWrap && HiveInputFormat .canWrapForLlap(pd.getInputFileFormatClass(), true))) {	continue;	}	
input format doesn t provide vectorized input 

public PhysicalContext resolve(PhysicalContext pctx) throws SemanticException {	this.conf = pctx.getConf();	this.mode = LlapMode.valueOf(HiveConf.getVar(conf, HiveConf.ConfVars.LLAP_EXECUTION_MODE));	Preconditions.checkState(this.mode != null, "Unrecognized LLAP mode configuration: " + HiveConf.getVar(conf, HiveConf.ConfVars.LLAP_EXECUTION_MODE));	
llap mode 

public PhysicalContext resolve(PhysicalContext pctx) throws SemanticException {	this.conf = pctx.getConf();	this.mode = LlapMode.valueOf(HiveConf.getVar(conf, HiveConf.ConfVars.LLAP_EXECUTION_MODE));	Preconditions.checkState(this.mode != null, "Unrecognized LLAP mode configuration: " + HiveConf.getVar(conf, HiveConf.ConfVars.LLAP_EXECUTION_MODE));	if (mode == none) {	
llap disabled 

========================= hive sample_3002 =========================

public void handle(Context withinContext) throws Exception {	
processing drop table message 

========================= hive sample_3452 =========================

public ChunkedInputStream(InputStream in, String id) {	din = new DataInputStream(in);	this.id = id;	
creating chunked input for 

public void close() throws IOException {	
closing chunked input 

int bytesRead = 0;	if (len < 0) {	throw new IllegalArgumentException(id + ": Negative read length");	} else if (len == 0) {	return 0;	}	if (unreadBytes == 0) {	try {	unreadBytes = din.readInt();	if (LOG.isDebugEnabled()) {	
chunk size 

throw new IllegalArgumentException(id + ": Negative read length");	} else if (len == 0) {	return 0;	}	if (unreadBytes == 0) {	try {	unreadBytes = din.readInt();	if (LOG.isDebugEnabled()) {	}	if (unreadBytes == 0) {	
hit end of data 

========================= hive sample_508 =========================

public String getCanonicalHostName(String hostName) {	try {	return InetAddress.getByName(hostName).getCanonicalHostName();	}	catch(UnknownHostException exception) {	
could not retrieve canonical hostname for 

} else if (callback instanceof PasswordCallback) {	pc = (PasswordCallback) callback;	} else if (callback instanceof RealmCallback) {	rc = (RealmCallback) callback;	} else {	throw new UnsupportedCallbackException(callback, "Unrecognized SASL client callback");	}	}	if (nc != null) {	if (LOG.isDebugEnabled()) {	
sasl client callback setting username 

throw new UnsupportedCallbackException(callback, "Unrecognized SASL client callback");	}	}	if (nc != null) {	if (LOG.isDebugEnabled()) {	}	nc.setName(userName);	}	if (pc != null) {	if (LOG.isDebugEnabled()) {	
sasl client callback setting userpassword 

}	nc.setName(userName);	}	if (pc != null) {	if (LOG.isDebugEnabled()) {	}	pc.setPassword(userPassword);	}	if (rc != null) {	if (LOG.isDebugEnabled()) {	
sasl client callback setting realm 

protected Server(String keytabFile, String principalConf, String clientConf) throws TTransportException {	if (keytabFile == null || keytabFile.isEmpty()) {	throw new TTransportException("No keytab specified");	}	if (principalConf == null || principalConf.isEmpty()) {	throw new TTransportException("No principal specified");	}	if (clientConf == null || clientConf.isEmpty()) {	
client facing principal not set using server side setting 

throw new TTransportException("No keytab specified");	}	if (principalConf == null || principalConf.isEmpty()) {	throw new TTransportException("No principal specified");	}	if (clientConf == null || clientConf.isEmpty()) {	clientConf = principalConf;	}	String kerberosName;	try {	
logging in via client based principal 

}	if (clientConf == null || clientConf.isEmpty()) {	clientConf = principalConf;	}	String kerberosName;	try {	kerberosName = SecurityUtil.getServerPrincipal(clientConf, "0.0.0.0");	UserGroupInformation.loginUserFromKeytab( kerberosName, keytabFile);	clientValidationUGI = UserGroupInformation.getLoginUser();	assert clientValidationUGI.isFromKeytab();	
logging in via server based principal 

} else if (callback instanceof RealmCallback) {	continue;	} else {	throw new UnsupportedCallbackException(callback, "Unrecognized SASL DIGEST-MD5 Callback");	}	}	if (pc != null) {	DelegationTokenIdentifier tokenIdentifier = SaslRpcServer. getIdentifier(nc.getDefaultName(), secretManager);	char[] password = getPassword(tokenIdentifier);	if (LOG.isDebugEnabled()) {	
sasl server digest callback setting password for client 

String authid = ac.getAuthenticationID();	String authzid = ac.getAuthorizationID();	if (authid.equals(authzid)) {	ac.setAuthorized(true);	} else {	ac.setAuthorized(false);	}	if (ac.isAuthorized()) {	if (LOG.isDebugEnabled()) {	String username = SaslRpcServer.getIdentifier(authzid, secretManager).getUser().getUserName();	
sasl server digest callback setting canonicalized client id 

authenticationMethod.set(AuthenticationMethod.TOKEN);	} catch (InvalidToken e) {	throw new TException(e.getMessage());	}	}	UserGroupInformation clientUgi = null;	try {	if (useProxy) {	clientUgi = UserGroupInformation.createProxyUser( endUser, UserGroupInformation.getLoginUser());	remoteUser.set(clientUgi.getShortUserName());	
set remoteuser 

try {	return wrapped.process(inProt, outProt);	} catch (TException te) {	throw new RuntimeException(te);	}	}	});	} else {	UserGroupInformation endUserUgi = UserGroupInformation.createRemoteUser(endUser);	remoteUser.set(endUserUgi.getShortUserName());	
set remoteuser from enduser 

throw rte;	} catch (InterruptedException ie) {	throw new RuntimeException(ie);	} catch (IOException ioe) {	throw new RuntimeException(ioe);	}	finally {	if (clientUgi != null) {	try { FileSystem.closeAllForUGI(clientUgi); }	catch(IOException exception) {	
could not clean up file system handles for ugi 

========================= hive sample_1879 =========================

public void apply(DirSearch ldap, String user) throws AuthenticationException {	
authenticating user using user filter 

public void apply(DirSearch ldap, String user) throws AuthenticationException {	String userName = LdapUtils.extractUserName(user).toLowerCase();	if (!userFilter.contains(userName)) {	
authentication failed based on user membership 

========================= hive sample_2306 =========================

public void copyToDFSDirecory(FileSystem destFs, Path destPath) throws IOException, HiveException {	if (addCursor > 0) {	this.spillBlock(this.currentWriteBlock, addCursor);	}	if (tempOutPath == null || tempOutPath.toString().trim().equals("")) {	return;	}	this.closeWriter();	
rowcontainer copied temp file to dfs directory 

return;	}	if (file.isDirectory() && recursive) {	File[] files = file.listFiles();	for (File file2 : files) {	deleteLocalFile(file2, true);	}	}	boolean deleteSuccess = file.delete();	if (!deleteSuccess) {	
error deleting tmp file 

File[] files = file.listFiles();	for (File file2 : files) {	deleteLocalFile(file2, true);	}	}	boolean deleteSuccess = file.delete();	if (!deleteSuccess) {	}	}	} catch (Exception e) {	
error deleting tmp file 

protected void setupWriter() throws HiveException {	try {	if ( tmpFile != null ) {	return;	}	String suffix = ".tmp";	if (this.keyObject != null) {	String keyObjectStr = this.keyObject.toString();	String md5Str = DigestUtils.md5Hex(keyObjectStr.toString());	
using for keyobject 

return;	}	String suffix = ".tmp";	if (this.keyObject != null) {	String keyObjectStr = this.keyObject.toString();	String md5Str = DigestUtils.md5Hex(keyObjectStr.toString());	suffix = "." + md5Str + suffix;	}	parentDir = FileUtils.createLocalDirsTempFile(spillFileDirs, "hive-rowcontainer", "", true);	tmpFile = File.createTempFile("RowContainer", suffix, parentDir);	
rowcontainer created temp file 

========================= hive sample_4051 =========================

public void serviceStart() {	final Configuration conf = getConfig();	final BlockingService daemonImpl = LlapPluginProtocolProtos.LlapPluginProtocol.newReflectiveBlockingService(this);	server = LlapUtil.startProtocolServer(0, numHandlers, bindAddress , conf, daemonImpl, LlapPluginProtocolPB.class, secretManager, new LlapPluginPolicyProvider(), ConfVars.LLAP_PLUGIN_ACL, ConfVars.LLAP_PLUGIN_ACL_DENY);	
starting the plugin endpoint on port 

========================= hive sample_621 =========================

for (Entry<String, String> entry : modifiedConf.entrySet()) {	String key = entry.getKey();	String currVal = entry.getValue();	String oldVal = conf.get(key);	if (!Objects.equals(oldVal, currVal)) {	notifyMetaListeners(key, oldVal, currVal);	}	}	logInfo("Meta listeners shutdown notification completed.");	} catch (MetaException e) {	
failed to notify meta listeners on shutdown 

wh = new Warehouse(conf);	synchronized (HMSHandler.class) {	if (currentUrl == null || !currentUrl.equals(MetaStoreInit.getConnectionURL(conf))) {	createDefaultDB();	createDefaultRoles();	addAdminUsers();	currentUrl = MetaStoreInit.getConnectionURL(conf);	}	}	if (MetastoreConf.getBoolVar(conf, ConfVars.METRICS_ENABLED)) {	
begin calculating metadata count metrics 

private static RawStore newRawStoreForConf(Configuration conf) throws MetaException {	Configuration newConf = new Configuration(conf);	String rawStoreClassName = MetastoreConf.getVar(newConf, ConfVars.RAW_STORE_IMPL);	
opening raw store with implementation class 

private void createDefaultDB() throws MetaException {	try {	createDefaultDB_core(getMS());	} catch (JDOException e) {	
retrying creating default database after error 

private void createDefaultRoles() throws MetaException {	try {	createDefaultRoles_core();	} catch (JDOException e) {	
retrying creating default roles after error 

private void createDefaultRoles_core() throws MetaException {	RawStore ms = getMS();	try {	ms.addRole(ADMIN, ADMIN);	} catch (InvalidObjectException e) {	
role already exists 

private void createDefaultRoles_core() throws MetaException {	RawStore ms = getMS();	try {	ms.addRole(ADMIN, ADMIN);	} catch (InvalidObjectException e) {	} catch (NoSuchObjectException e) {	
unexpected exception while adding roles 

private void createDefaultRoles_core() throws MetaException {	RawStore ms = getMS();	try {	ms.addRole(ADMIN, ADMIN);	} catch (InvalidObjectException e) {	} catch (NoSuchObjectException e) {	}	
added role in metastore 

private void createDefaultRoles_core() throws MetaException {	RawStore ms = getMS();	try {	ms.addRole(ADMIN, ADMIN);	} catch (InvalidObjectException e) {	} catch (NoSuchObjectException e) {	}	try {	ms.addRole(PUBLIC, PUBLIC);	} catch (InvalidObjectException e) {	
role already exists 

RawStore ms = getMS();	try {	ms.addRole(ADMIN, ADMIN);	} catch (InvalidObjectException e) {	} catch (NoSuchObjectException e) {	}	try {	ms.addRole(PUBLIC, PUBLIC);	} catch (InvalidObjectException e) {	} catch (NoSuchObjectException e) {	
unexpected exception while adding roles 

try {	ms.addRole(ADMIN, ADMIN);	} catch (InvalidObjectException e) {	} catch (NoSuchObjectException e) {	}	try {	ms.addRole(PUBLIC, PUBLIC);	} catch (InvalidObjectException e) {	} catch (NoSuchObjectException e) {	}	
added role in metastore 

try {	ms.addRole(PUBLIC, PUBLIC);	} catch (InvalidObjectException e) {	} catch (NoSuchObjectException e) {	}	PrivilegeBag privs = new PrivilegeBag();	privs.addToPrivileges(new HiveObjectPrivilege( new HiveObjectRef(HiveObjectType.GLOBAL, null, null, null, null), ADMIN, PrincipalType.ROLE, new PrivilegeGrantInfo("All", 0, ADMIN, PrincipalType.ROLE, true)));	try {	ms.grantPrivileges(privs);	} catch (InvalidObjectException e) {	
failed while granting global privs to admin 

ms.addRole(PUBLIC, PUBLIC);	} catch (InvalidObjectException e) {	} catch (NoSuchObjectException e) {	}	PrivilegeBag privs = new PrivilegeBag();	privs.addToPrivileges(new HiveObjectPrivilege( new HiveObjectRef(HiveObjectType.GLOBAL, null, null, null, null), ADMIN, PrincipalType.ROLE, new PrivilegeGrantInfo("All", 0, ADMIN, PrincipalType.ROLE, true)));	try {	ms.grantPrivileges(privs);	} catch (InvalidObjectException e) {	} catch (NoSuchObjectException e) {	
failed while granting global privs to admin 

private void addAdminUsers() throws MetaException {	try {	addAdminUsers_core();	} catch (JDOException e) {	
retrying adding admin users after error 

private void addAdminUsers_core() throws MetaException {	String userStr = MetastoreConf.getVar(conf,ConfVars.USERS_IN_ADMIN_ROLE,"").trim();	if (userStr.isEmpty()) {	
no user is added in admin role since config is empty 

private void addAdminUsers_core() throws MetaException {	String userStr = MetastoreConf.getVar(conf,ConfVars.USERS_IN_ADMIN_ROLE,"").trim();	if (userStr.isEmpty()) {	return;	}	Iterator<String> users = Splitter.on(",").trimResults().omitEmptyStrings().split(userStr).iterator();	if (!users.hasNext()) {	
no user is added in admin role since config value is in incorrect format we accept comma separated list of users 

}	Iterator<String> users = Splitter.on(",").trimResults().omitEmptyStrings().split(userStr).iterator();	if (!users.hasNext()) {	return;	}	Role adminRole;	RawStore ms = getMS();	try {	adminRole = ms.getRole(ADMIN);	} catch (NoSuchObjectException e) {	
failed to retrieve just added admin role 

RawStore ms = getMS();	try {	adminRole = ms.getRole(ADMIN);	} catch (NoSuchObjectException e) {	return;	}	while (users.hasNext()) {	String userName = users.next();	try {	ms.grantRole(adminRole, userName, PrincipalType.USER, ADMIN, PrincipalType.ROLE, true);	
added to admin role 

try {	adminRole = ms.getRole(ADMIN);	} catch (NoSuchObjectException e) {	return;	}	while (users.hasNext()) {	String userName = users.next();	try {	ms.grantRole(adminRole, userName, PrincipalType.USER, ADMIN, PrincipalType.ROLE, true);	} catch (NoSuchObjectException e) {	
failed to add in admin role 

adminRole = ms.getRole(ADMIN);	} catch (NoSuchObjectException e) {	return;	}	while (users.hasNext()) {	String userName = users.next();	try {	ms.grantRole(adminRole, userName, PrincipalType.USER, ADMIN, PrincipalType.ROLE, true);	} catch (NoSuchObjectException e) {	} catch (InvalidObjectException e) {	
already in admin role 

if (!success) {	ms.rollbackTransaction();	} else if (deleteData) {	deletePartitionData(partitionPaths);	for (Path tablePath : tablePaths) {	deleteTableData(tablePath);	}	try {	wh.deleteDir(new Path(db.getLocationUri()), true);	} catch (Exception e) {	
failed to delete database directory 

throw new NoSuchObjectException("The database " + tbl.getDbName() + " does not exist");	}	if (is_table_exists(ms, tbl.getDbName(), tbl.getTableName())) {	throw new AlreadyExistsException("Table " + tbl.getTableName() + " already exists");	}	if (!TableType.VIRTUAL_VIEW.toString().equals(tbl.getTableType())) {	if (tbl.getSd().getLocation() == null || tbl.getSd().getLocation().isEmpty()) {	tblPath = wh.getDefaultTablePath( ms.getDatabase(tbl.getDbName()), tbl.getTableName());	} else {	if (!isExternal(tbl) && !MetaStoreUtils.isNonNativeTable(tbl)) {	
location specified for non external table 

private void deleteTableData(Path tablePath, boolean ifPurge) {	if (tablePath != null) {	try {	wh.deleteDir(tablePath, true, ifPurge);	} catch (Exception e) {	
failed to delete table directory 

private void deletePartitionData(List<Path> partPaths, boolean ifPurge) {	if (partPaths != null && !partPaths.isEmpty()) {	for (Path partPath : partPaths) {	try {	wh.deleteDir(partPath, true, ifPurge);	} catch (Exception e) {	
failed to delete partition directory 

}	List<Future<Partition>> partFutures = Lists.newArrayList();	final Table table = tbl;	for (final Partition part : parts) {	if (!part.getTableName().equals(tblName) || !part.getDbName().equals(dbName)) {	throw new MetaException("Partition does not belong to target table " + dbName + "." + tblName + ": " + part);	}	boolean shouldAdd = startAddPartition(ms, part, ifNotExists);	if (!shouldAdd) {	existingParts.add(part);	
not adding partition as it already exists 

firePreEvent(new PreAddPartitionEvent(tbl, partitionSpecProxy, this));	List<Future<Partition>> partFutures = Lists.newArrayList();	final Table table = tbl;	while(partitionIterator.hasNext()) {	final Partition part = partitionIterator.getCurrent();	if (!part.getTableName().equals(tblName) || !part.getDbName().equals(dbName)) {	throw new MetaException("Partition does not belong to target table " + dbName + "." + tblName + ": " + part);	}	boolean shouldAdd = startAddPartition(ms, part, ifNotExists);	if (!shouldAdd) {	
not adding partition as it already exists 

transactionalListenerResponses = MetaStoreListenerNotifier.notifyEvent(transactionalListeners, EventType.DROP_PARTITION, new DropPartitionEvent(tbl, part, true, deleteData, this), envContext);	}	success = ms.commitTransaction();	}	} finally {	if (!success) {	ms.rollbackTransaction();	} else if (deleteData && ((partPath != null) || (archiveParentDir != null))) {	if (!isExternalTbl) {	if (mustPurge) {	
droppartition will purge directly skipping trash 

success = ms.commitTransaction();	}	} finally {	if (!success) {	ms.rollbackTransaction();	} else if (deleteData && ((partPath != null) || (archiveParentDir != null))) {	if (!isExternalTbl) {	if (mustPurge) {	}	else {	
droppartition will move to trash directory 

success = ms.commitTransaction();	DropPartitionsResult result = new DropPartitionsResult();	if (needResult) {	result.setPartitions(parts);	}	return result;	} finally {	if (!success) {	ms.rollbackTransaction();	} else if (deleteData && !isExternal(tbl)) {	
droppartition will purge partition directories directly skipping trash droppartition will move partition directories to trash directory 

ms.rollbackTransaction();	} else if (deleteData && !isExternal(tbl)) {	for (Path path : archToDelete) {	wh.deleteDir(path, true, mustPurge);	}	for (PathAndPartValSize p : dirsToDelete) {	wh.deleteDir(p.path, true, mustPurge);	try {	deleteParentRecursive(p.path.getParent(), p.partValSize - 1, mustPurge);	} catch (IOException ex) {	
error from deleteparentrecursive 

private void verifyIsWritablePath(Path dir) throws MetaException {	try {	if (!wh.isWritable(dir.getParent())) {	throw new MetaException("Table partition not deleted since " + dir.getParent() + " is not writable by " + SecurityUtils.getUser());	}	} catch (IOException ex) {	
error from iswritable 

public boolean drop_partition_with_environment_context(final String db_name, final String tbl_name, final List<String> part_vals, final boolean deleteData, final EnvironmentContext envContext) throws TException {	startPartitionFunction("drop_partition", db_name, tbl_name, part_vals);	
partition values 

private void rename_partition(final String db_name, final String tbl_name, final List<String> part_vals, final Partition new_part, final EnvironmentContext envContext) throws TException {	startTableFunction("alter_partition", db_name, tbl_name);	if (LOG.isInfoEnabled()) {	
new partition values 

private void rename_partition(final String db_name, final String tbl_name, final List<String> part_vals, final Partition new_part, final EnvironmentContext envContext) throws TException {	startTableFunction("alter_partition", db_name, tbl_name);	if (LOG.isInfoEnabled()) {	if (part_vals != null && part_vals.size() > 0) {	
old partition values 

public void alter_partitions_with_environment_context(final String db_name, final String tbl_name, final List<Partition> new_parts, EnvironmentContext environmentContext) throws TException {	startTableFunction("alter_partitions", db_name, tbl_name);	if (LOG.isInfoEnabled()) {	for (Partition tmpPart : new_parts) {	
new partition values 

private StorageSchemaReader getStorageSchemaReader() throws MetaException {	if (storageSchemaReader == null) {	String className = MetastoreConf.getVar(conf, MetastoreConf.ConfVars.STORAGE_SCHEMA_READER_IMPL);	Class<? extends StorageSchemaReader> readerClass = JavaUtils.getClass(className, StorageSchemaReader.class);	try {	storageSchemaReader = readerClass.newInstance();	} catch (InstantiationException|IllegalAccessException e) {	
unable to instantiate class 

return defaultValue;	}	if (!Pattern.matches("(hive|hdfs|mapred|metastore).*", name)) {	throw new ConfigValSecurityException("For security reasons, the " + "config key " + name + " cannot be accessed");	}	String toReturn = defaultValue;	try {	toReturn = MetastoreConf.get(conf, name);	if (toReturn == null) toReturn = defaultValue;	} catch (RuntimeException e) {	
runtimeexception thrown in get config value msg cause 

for (MetaStoreEventListener transactionalListener : transactionalListeners) {	transactionalListener.onLoadPartitionDone(lpde);	}	}	success = ms.commitTransaction();	for (MetaStoreEventListener listener : listeners) {	listener.onLoadPartitionDone(new LoadPartitionDoneEvent(true, tbl, partName, this));	}	} catch (Exception original) {	ex = original;	
exception caught in mark partition event 

public boolean isPartitionMarkedForEvent(final String db_name, final String tbl_name, final Map<String, String> partName, final PartitionEventType evtType) throws TException {	startPartitionFunction("isPartitionMarkedForEvent", db_name, tbl_name, partName);	Boolean ret = null;	Exception ex = null;	try {	ret = getMS().isPartitionMarkedForEvent(db_name, tbl_name, partName, evtType);	} catch (Exception original) {	
exception caught for ispartitionmarkedforevent 

if (request.isSetNeedMerge() && request.isNeedMerge()) {	ColumnStatistics csOld = getMS().getTableColumnStatistics(dbName, tableName, colNames);	Table t = getTable(dbName, tableName);	MetaStoreUtils.getMergableCols(firstColStats, t.getParameters());	if (csOld != null && csOld.getStatsObjSize() != 0 && !firstColStats.getStatsObj().isEmpty()) {	MetaStoreUtils.mergeColStats(firstColStats, csOld);	}	if (!firstColStats.getStatsObj().isEmpty()) {	return update_table_column_statistics(firstColStats);	} else {	
all the column stats are not accurate to merge 

}	newStatsMap.put(partName, csNew);	}	Map<String, ColumnStatistics> oldStatsMap = new HashMap<>();	Map<String, Partition> mapToPart = new HashMap<>();	if (request.isSetNeedMerge() && request.isNeedMerge()) {	List<String> partitionNames = new ArrayList<>();	partitionNames.addAll(newStatsMap.keySet());	List<ColumnStatistics> csOlds = getMS().getPartitionColumnStatistics(dbName, tableName, partitionNames, colNames);	if (newStatsMap.values().size() != csOlds.size()) {	
some of the partitions miss stats 

ColumnStatistics csNew = entry.getValue();	ColumnStatistics csOld = oldStatsMap.get(entry.getKey());	if (request.isSetNeedMerge() && request.isNeedMerge()) {	MetaStoreUtils.getMergableCols(csNew, mapToPart.get(entry.getKey()).getParameters());	if (csOld != null && csOld.getStatsObjSize() != 0 && !csNew.getStatsObj().isEmpty()) {	MetaStoreUtils.mergeColStats(csNew, csOld);	}	if (!csNew.getStatsObj().isEmpty()) {	ret = ret && updatePartitonColStats(t, csNew);	} else {	
all the column stats are not accurate to merge 

public NotificationEventResponse get_next_notification(NotificationEventRequest rqst) throws TException {	try {	authorizeProxyPrivilege();	} catch (Exception ex) {	
not authorized to make the get next notification call you can try to disable 

public CurrentNotificationEventId get_current_notificationEventId() throws TException {	try {	authorizeProxyPrivilege();	} catch (Exception ex) {	
not authorized to make the get current notificationeventid call you can try to disable 

public NotificationEventsCountResponse get_notification_events_count(NotificationEventsCountRequest rqst) throws TException {	try {	authorizeProxyPrivilege();	} catch (Exception ex) {	
not authorized to make the get notification events count call you can try to disable 

private void authorizeProxyPrivilege() throws Exception {	if (!isMetaStoreRemote() || !MetastoreConf.getBoolVar(conf, ConfVars.EVENT_DB_NOTIFICATION_API_AUTH)) {	return;	}	String user = null;	try {	user = SecurityUtils.getUGI().getShortUserName();	} catch (Exception ex) {	
cannot obtain username 

public String get_metastore_db_uuid() throws TException {	try {	return getMS().getMetastoreDbUuid();	} catch (MetaException e) {	
exception thrown while querying metastore db uuid 

public WMCreateResourcePlanResponse create_resource_plan(WMCreateResourcePlanRequest request) throws AlreadyExistsException, InvalidObjectException, MetaException, TException {	int defaultPoolSize = MetastoreConf.getIntVar( conf, MetastoreConf.ConfVars.WM_DEFAULT_POOL_SIZE);	WMResourcePlan plan = request.getResourcePlan();	if (defaultPoolSize > 0 && plan.isSetQueryParallelism()) {	defaultPoolSize = plan.getQueryParallelism();	}	try {	getMS().createResourcePlan(plan, request.getCopyFrom(), defaultPoolSize);	return new WMCreateResourcePlanResponse();	} catch (MetaException e) {	
exception while trying to persist resource plan 

public WMGetResourcePlanResponse get_resource_plan(WMGetResourcePlanRequest request) throws NoSuchObjectException, MetaException, TException {	try {	WMFullResourcePlan rp = getMS().getResourcePlan(request.getResourcePlanName());	WMGetResourcePlanResponse resp = new WMGetResourcePlanResponse();	resp.setResourcePlan(rp);	return resp;	} catch (MetaException e) {	
exception while trying to retrieve resource plan 

public WMGetAllResourcePlanResponse get_all_resource_plans(WMGetAllResourcePlanRequest request) throws MetaException, TException {	try {	WMGetAllResourcePlanResponse resp = new WMGetAllResourcePlanResponse();	resp.setResourcePlans(getMS().getAllResourcePlans());	return resp;	} catch (MetaException e) {	
exception while trying to retrieve resource plans 

if (((request.isIsEnableAndActivate() ? 1 : 0) + (request.isIsReplace() ? 1 : 0) + (request.isIsForceDeactivate() ? 1 : 0)) > 1) {	throw new MetaException("Invalid request; multiple flags are set");	}	WMAlterResourcePlanResponse response = new WMAlterResourcePlanResponse();	WMFullResourcePlan fullPlanAfterAlter = getMS().alterResourcePlan( request.getResourcePlanName(), request.getResourcePlan(), request.isIsEnableAndActivate(), request.isIsForceDeactivate(), request.isIsReplace());	if (fullPlanAfterAlter != null) {	response.setFullResourcePlan(fullPlanAfterAlter);	}	return response;	} catch (MetaException e) {	
exception while trying to alter resource plan 

public WMGetActiveResourcePlanResponse get_active_resource_plan( WMGetActiveResourcePlanRequest request) throws MetaException, TException {	try {	WMGetActiveResourcePlanResponse response = new WMGetActiveResourcePlanResponse();	response.setResourcePlan(getMS().getActiveResourcePlan());	return response;	} catch (MetaException e) {	
exception while trying to get active resource plan 

public WMValidateResourcePlanResponse validate_resource_plan(WMValidateResourcePlanRequest request) throws NoSuchObjectException, MetaException, TException {	try {	return getMS().validateResourcePlan(request.getResourcePlanName());	} catch (MetaException e) {	
exception while trying to validate resource plan 

public WMDropResourcePlanResponse drop_resource_plan(WMDropResourcePlanRequest request) throws NoSuchObjectException, InvalidOperationException, MetaException, TException {	try {	getMS().dropResourcePlan(request.getResourcePlanName());	return new WMDropResourcePlanResponse();	} catch (MetaException e) {	
exception while trying to drop resource plan 

public WMCreateTriggerResponse create_wm_trigger(WMCreateTriggerRequest request) throws AlreadyExistsException, InvalidObjectException, MetaException, TException {	try {	getMS().createWMTrigger(request.getTrigger());	return new WMCreateTriggerResponse();	} catch (MetaException e) {	
exception while trying to create trigger 

public WMAlterTriggerResponse alter_wm_trigger(WMAlterTriggerRequest request) throws NoSuchObjectException, InvalidObjectException, MetaException, TException {	try {	getMS().alterWMTrigger(request.getTrigger());	return new WMAlterTriggerResponse();	} catch (MetaException e) {	
exception while trying to alter trigger 

public WMDropTriggerResponse drop_wm_trigger(WMDropTriggerRequest request) throws NoSuchObjectException, InvalidOperationException, MetaException, TException {	try {	getMS().dropWMTrigger(request.getResourcePlanName(), request.getTriggerName());	return new WMDropTriggerResponse();	} catch (MetaException e) {	
exception while trying to drop trigger 

public WMGetTriggersForResourePlanResponse get_triggers_for_resourceplan( WMGetTriggersForResourePlanRequest request) throws NoSuchObjectException, MetaException, TException {	try {	List<WMTrigger> triggers = getMS().getTriggersForResourcePlan(request.getResourcePlanName());	WMGetTriggersForResourePlanResponse response = new WMGetTriggersForResourePlanResponse();	response.setTriggers(triggers);	return response;	} catch (MetaException e) {	
exception while trying to retrieve triggers plans 

public WMCreatePoolResponse create_wm_pool(WMCreatePoolRequest request) throws AlreadyExistsException, NoSuchObjectException, InvalidObjectException, MetaException, TException {	try {	getMS().createPool(request.getPool());	return new WMCreatePoolResponse();	} catch (MetaException e) {	
exception while trying to create wmpool 

public WMAlterPoolResponse alter_wm_pool(WMAlterPoolRequest request) throws AlreadyExistsException, NoSuchObjectException, InvalidObjectException, MetaException, TException {	try {	getMS().alterPool(request.getPool(), request.getPoolPath());	return new WMAlterPoolResponse();	} catch (MetaException e) {	
exception while trying to alter wmpool 

public WMDropPoolResponse drop_wm_pool(WMDropPoolRequest request) throws NoSuchObjectException, InvalidOperationException, MetaException, TException {	try {	getMS().dropWMPool(request.getResourcePlanName(), request.getPoolPath());	return new WMDropPoolResponse();	} catch (MetaException e) {	
exception while trying to drop wmpool 

public WMCreateOrUpdateMappingResponse create_or_update_wm_mapping( WMCreateOrUpdateMappingRequest request) throws AlreadyExistsException, NoSuchObjectException, InvalidObjectException, MetaException, TException {	try {	getMS().createOrUpdateWMMapping(request.getMapping(), request.isUpdate());	return new WMCreateOrUpdateMappingResponse();	} catch (MetaException e) {	
exception while trying to create or update wmmapping 

public WMDropMappingResponse drop_wm_mapping(WMDropMappingRequest request) throws NoSuchObjectException, InvalidOperationException, MetaException, TException {	try {	getMS().dropWMMapping(request.getMapping());	return new WMDropMappingResponse();	} catch (MetaException e) {	
exception while trying to drop wmmapping 

public WMCreateOrDropTriggerToPoolMappingResponse create_or_drop_wm_trigger_to_pool_mapping( WMCreateOrDropTriggerToPoolMappingRequest request) throws AlreadyExistsException, NoSuchObjectException, InvalidObjectException, MetaException, TException {	try {	if (request.isDrop()) {	getMS().dropWMTriggerToPoolMapping( request.getResourcePlanName(), request.getTriggerName(), request.getPoolPath());	} else {	getMS().createWMTriggerToPoolMapping( request.getResourcePlanName(), request.getTriggerName(), request.getPoolPath());	}	return new WMCreateOrDropTriggerToPoolMappingResponse();	} catch (MetaException e) {	
exception while trying to create or drop pool mappings 

shutdownHookMgr.addShutdownHook(() -> {	String shutdownMsg = "Shutting down hive metastore.";	HMSHandler.LOG.info(shutdownMsg);	if (isCliVerbose) {	System.err.println(shutdownMsg);	}	if (MetastoreConf.getBoolVar(conf, ConfVars.METRICS_ENABLED)) {	try {	Metrics.shutdown();	} catch (Exception e) {	
error in metrics deinit 

} catch (Exception e) {	}	}	ThreadPool.shutdown();	}, 10);	CachedStore.initSharedCacheAsync(conf);	if (MetastoreConf.getBoolVar(conf, ConfVars.METRICS_ENABLED)) {	try {	Metrics.initialize(conf);	} catch (Exception e) {	
error in metrics init 

Metrics.initialize(conf);	} catch (Exception e) {	}	}	Lock startLock = new ReentrantLock();	Condition startCondition = startLock.newCondition();	AtomicBoolean startedServing = new AtomicBoolean();	startMetaStoreThreads(conf, startLock, startCondition, startedServing);	startMetaStore(cli.getPort(), HadoopThriftAuthBridge.getBridge(), conf, startLock, startCondition, startedServing);	} catch (Throwable t) {	
metastore thrift server threw an exception 

if (useSasl) {	if (useFramedTransport) {	throw new HiveMetaException("Framed transport is not supported with SASL enabled.");	}	saslServer = bridge.createServer( MetastoreConf.getVar(conf, ConfVars.KERBEROS_KEYTAB_FILE), MetastoreConf.getVar(conf, ConfVars.KERBEROS_PRINCIPAL), MetastoreConf.getVar(conf, ConfVars.CLIENT_KERBEROS_PRINCIPAL));	delegationTokenManager = new MetastoreDelegationTokenManager();	delegationTokenManager.startDelegationTokenSecretManager(conf, baseHandler, HadoopThriftAuthBridge.Server.ServerMode.METASTORE);	saslServer.setSecretManager(delegationTokenManager.getSecretManager());	transFactory = saslServer.createTransportFactory( MetaStoreUtils.getMetaStoreSaslProperties(conf, useSSL));	processor = saslServer.wrapProcessor( new ThriftHiveMetastore.Processor<>(handler));	
starting db backed metastore server in secure mode 

saslServer = bridge.createServer( MetastoreConf.getVar(conf, ConfVars.KERBEROS_KEYTAB_FILE), MetastoreConf.getVar(conf, ConfVars.KERBEROS_PRINCIPAL), MetastoreConf.getVar(conf, ConfVars.CLIENT_KERBEROS_PRINCIPAL));	delegationTokenManager = new MetastoreDelegationTokenManager();	delegationTokenManager.startDelegationTokenSecretManager(conf, baseHandler, HadoopThriftAuthBridge.Server.ServerMode.METASTORE);	saslServer.setSecretManager(delegationTokenManager.getSecretManager());	transFactory = saslServer.createTransportFactory( MetaStoreUtils.getMetaStoreSaslProperties(conf, useSSL));	processor = saslServer.wrapProcessor( new ThriftHiveMetastore.Processor<>(handler));	} else {	if (MetastoreConf.getBoolVar(conf, ConfVars.EXECUTE_SET_UGI)) {	transFactory = useFramedTransport ? new ChainedTTransportFactory(new TFramedTransport.Factory(), new TUGIContainingTransport.Factory()) : new TUGIContainingTransport.Factory();	processor = new TUGIBasedProcessor<>(handler);	
starting db backed metastore server with setugi enabled 

saslServer.setSecretManager(delegationTokenManager.getSecretManager());	transFactory = saslServer.createTransportFactory( MetaStoreUtils.getMetaStoreSaslProperties(conf, useSSL));	processor = saslServer.wrapProcessor( new ThriftHiveMetastore.Processor<>(handler));	} else {	if (MetastoreConf.getBoolVar(conf, ConfVars.EXECUTE_SET_UGI)) {	transFactory = useFramedTransport ? new ChainedTTransportFactory(new TFramedTransport.Factory(), new TUGIContainingTransport.Factory()) : new TUGIContainingTransport.Factory();	processor = new TUGIBasedProcessor<>(handler);	} else {	transFactory = useFramedTransport ? new TFramedTransport.Factory() : new TTransportFactory();	processor = new TSetIpAddressProcessor<>(handler);	
starting db backed metastore server 

return null;	}	public void deleteContext(ServerContext serverContext, TProtocol tProtocol, TProtocol tProtocol1) {	openConnections.decrementAndGet();	cleanupRawStore();	}	public void processContext(ServerContext serverContext, TTransport tTransport, TTransport tTransport1) {	}	};	tServer.setServerEventHandler(tServerEventHandler);	
started the new metaserver on port 

private static void signalOtherThreadsToStart(final TServer server, final Lock startLock, final Condition startCondition, final AtomicBoolean startedServing) {	Thread t = new Thread() {	public void run() {	do {	try {	Thread.sleep(1000);	} catch (InterruptedException e) {	
signalling thread was interrupted 

private static void startMetaStoreThreads(final Configuration conf, final Lock startLock, final Condition startCondition, final AtomicBoolean startedServing) {	Thread t = new Thread() {	public void run() {	startLock.lock();	try {	JvmPauseMonitor pauseMonitor = new JvmPauseMonitor(conf);	pauseMonitor.start();	} catch (Throwable t) {	
could not initiate the jvmpausemonitor thread gcs and pauses may not be warned upon 

pauseMonitor.start();	} catch (Throwable t) {	}	try {	while (!startedServing.get()) startCondition.await();	startCompactorInitiator(conf);	startCompactorWorkers(conf);	startCompactorCleaner(conf);	startRemoteOnlyTasks(conf);	} catch (Throwable e) {	
failure when starting the compactor compactions may not happen 

private static void initializeAndStartThread(MetaStoreThread thread, Configuration conf) throws MetaException {	
starting metastore thread of type 

private static void startupShutdownMessage(Class<?> clazz, String[] args, final org.slf4j.Logger LOG) {	final String hostname = getHostname();	final String classname = clazz.getSimpleName();	
startup msg 

private static void startupShutdownMessage(Class<?> clazz, String[] args, final org.slf4j.Logger LOG) {	final String hostname = getHostname();	final String classname = clazz.getSimpleName();	"Starting " + classname, "  host = " + hostname, "  args = " + Arrays.asList(args), "  version = " + MetastoreVersionInfo.getVersion(), "  classpath = " + System.getProperty("java.class.path"), "  build = " + MetastoreVersionInfo.getUrl() + " -r " + MetastoreVersionInfo.getRevision() + "; compiled by '" + MetastoreVersionInfo.getUser() + "' on " + MetastoreVersionInfo.getDate()}	) );	
shutdown msg 

========================= hive sample_1880 =========================

private static void addCollectionEstimator(HashMap<Class<?>, ObjectEstimator> byType, Deque<Object> stack, Field field, Class<?> fieldClass, Object fieldObj) {	Collection<?> fieldCol = null;	if (fieldObj != null) {	fieldCol = (Collection<?>)fieldObj;	if (fieldCol.size() == 0) {	fieldCol = null;	
empty collection 

private static void addMapEstimator(HashMap<Class<?>, ObjectEstimator> byType, Deque<Object> stack, Field field, Class<?> fieldClass, Object fieldObj) {	Map<?, ?> fieldCol = null;	if (fieldObj != null) {	fieldCol = (Map<?, ?>)fieldObj;	if (fieldCol.size() == 0) {	fieldCol = null;	
empty map 

private static Class<?>[] getMapArgs(Field field) {	Type genericType = field.getGenericType();	if (genericType instanceof ParameterizedType) {	Type[] types = ((ParameterizedType)genericType).getActualTypeArguments();	if (types.length == 2 && types[0] instanceof Class<?> && types[1] instanceof Class<?>) {	return new Class<?>[] { (Class<?>)types[0], (Class<?>)types[1] };	} else {	
cannot determine map type 

private static Class<?>[] getMapArgs(Field field) {	Type genericType = field.getGenericType();	if (genericType instanceof ParameterizedType) {	Type[] types = ((ParameterizedType)genericType).getActualTypeArguments();	if (types.length == 2 && types[0] instanceof Class<?> && types[1] instanceof Class<?>) {	return new Class<?>[] { (Class<?>)types[0], (Class<?>)types[1] };	} else {	}	} else {	
non parametrized map type 

private static Class<?> getCollectionArg(Field field) {	Type genericType = field.getGenericType();	if (genericType instanceof ParameterizedType) {	Type type = ((ParameterizedType)genericType).getActualTypeArguments()[0];	if (type instanceof Class<?>) {	return (Class<?>)type;	} else {	
cannot determine collection type 

private static Class<?> getCollectionArg(Field field) {	Type genericType = field.getGenericType();	if (genericType instanceof ParameterizedType) {	Type type = ((ParameterizedType)genericType).getActualTypeArguments()[0];	if (type instanceof Class<?>) {	return (Class<?>)type;	} else {	}	} else {	
non parametrized collection type 

private static void addArrayEstimator( HashMap<Class<?>, ObjectEstimator> byType, Deque<Object> stack, Field field, Object fieldObj) {	if (fieldObj == null) return;	int arrayLen = Array.getLength(fieldObj);	
empty array 

} catch (IllegalAccessException ex) {	throw new AssertionError("IAE: " + ex.getMessage());	}	if (fieldObj == null) continue;	if (null != uniqueObjects.put(fieldObj, Boolean.TRUE)) continue;	switch (e.type) {	case COLLECTION: {	Collection<?> c = (Collection<?>)fieldObj;	ObjectEstimator collEstimator = parent.get(fieldObj.getClass());	if (collEstimator == null) {	
approximate estimation for collection from 

ObjectEstimator collEstimator = parent.get(fieldObj.getClass());	if (collEstimator == null) {	referencedSize += memoryModel.object();	referencedSize += estimateCollectionElements(parent, c, e.field, uniqueObjects);	referencedSize += memoryModel.array() + c.size() * memoryModel.ref();	} else if (collEstimator instanceof CollectionEstimator) {	referencedSize += memoryModel.object();	referencedSize += estimateCollectionElements(parent, c, e.field, uniqueObjects);	referencedSize += ((CollectionEstimator)collEstimator).estimateOverhead(c.size());	} else {	
verbose estimation for collection from 

referencedSize += ((CollectionEstimator)collEstimator).estimateOverhead(c.size());	} else {	referencedSize += collEstimator.estimate(c, parent, uniqueObjects);	}	break;	}	case MAP: {	Map<?, ?> m = (Map<?, ?>)fieldObj;	ObjectEstimator collEstimator = parent.get(fieldObj.getClass());	if (collEstimator == null) {	
approximate estimation for map from 

Map<?, ?> m = (Map<?, ?>)fieldObj;	ObjectEstimator collEstimator = parent.get(fieldObj.getClass());	if (collEstimator == null) {	referencedSize += memoryModel.object();	referencedSize += estimateMapElements(parent, m, e.field, uniqueObjects);	referencedSize += memoryModel.array() + m.size() } else if (collEstimator instanceof CollectionEstimator) {	referencedSize += memoryModel.object();	referencedSize += estimateMapElements(parent, m, e.field, uniqueObjects);	referencedSize += ((CollectionEstimator)collEstimator).estimateOverhead(m.size());	} else {	
verbose estimation for map from 

public static void addEstimator(String className, HashMap<Class<?>, ObjectEstimator> sizeEstimators) {	Class<?> clazz = null;	try {	clazz = Class.forName(className);	} catch (ClassNotFoundException e) {	
cannot find 

========================= hive sample_2255 =========================

switch (next.eventType()) {	case Database: DatabaseEvent dbEvent = (DatabaseEvent) next;	dbTracker = new LoadDatabase(context, dbEvent, work.dbNameToLoadIn, loadTaskTracker) .tasks();	loadTaskTracker.update(dbTracker);	if (work.hasDbState()) {	loadTaskTracker.update(updateDatabaseLastReplID(maxTasks, context, scope));	}	work.updateDbEventState(dbEvent.toState());	scope.database = true;	scope.rootTasks.addAll(dbTracker.tasks());	
database 

LoadTable loadTable = new LoadTable(tableEvent, context, iterator.replLogger(), tableContext, loadTaskTracker);	tableTracker = loadTable.tasks();	if (!scope.database) {	scope.rootTasks.addAll(tableTracker.tasks());	scope.table = true;	}	setUpDependencies(dbTracker, tableTracker);	for table replication if we reach the max number of tasks then for the next run we will try to reload the same table again, this is mainly for ease of understanding the code as then we can avoid handling == > loading partitions for the table given that the creation of table lead to reaching max tasks vs,  loading next table since current one does not have partitions. LoadPartitions loadPartitions = new LoadPartitions(context, iterator.replLogger(), loadTaskTracker, tableEvent, work.dbNameToLoadIn, tableContext);	TaskTracker partitionsTracker = loadPartitions.tasks();	partitionsPostProcessing(iterator, scope, loadTaskTracker, tableTracker, partitionsTracker);	
table 

LoadTable loadTable = new LoadTable(tableEvent, context, iterator.replLogger(), tableContext, loadTaskTracker);	tableTracker = loadTable.tasks();	if (!scope.database) {	scope.rootTasks.addAll(tableTracker.tasks());	scope.table = true;	}	setUpDependencies(dbTracker, tableTracker);	for table replication if we reach the max number of tasks then for the next run we will try to reload the same table again, this is mainly for ease of understanding the code as then we can avoid handling == > loading partitions for the table given that the creation of table lead to reaching max tasks vs,  loading next table since current one does not have partitions. LoadPartitions loadPartitions = new LoadPartitions(context, iterator.replLogger(), loadTaskTracker, tableEvent, work.dbNameToLoadIn, tableContext);	TaskTracker partitionsTracker = loadPartitions.tasks();	partitionsPostProcessing(iterator, scope, loadTaskTracker, tableTracker, partitionsTracker);	
partitions for table 

partitionsPostProcessing(iterator, scope, loadTaskTracker, tableTracker, partitionsTracker);	break;	}	case Partition: {	This will happen only when loading tables and we reach the limit of number of tasks we can create;	hence we know here that the table should exist and there should be a lastPartitionName PartitionEvent event = (PartitionEvent) next;	TableContext tableContext = new TableContext(dbTracker, work.dbNameToLoadIn, work.tableNameToLoadIn);	LoadPartitions loadPartitions = new LoadPartitions(context, iterator.replLogger(), tableContext, loadTaskTracker, event.asTableEvent(), work.dbNameToLoadIn, event.lastPartitionReplicated());	the tableTracker here should be a new instance and not an existing one as this can only happen when we break in between loading partitions. TaskTracker partitionsTracker = loadPartitions.tasks();	partitionsPostProcessing(iterator, scope, loadTaskTracker, tableTracker, partitionsTracker);	
partitions 

}	case Function: {	LoadFunction loadFunction = new LoadFunction(context, iterator.replLogger(), (FunctionEvent) next, work.dbNameToLoadIn, dbTracker);	TaskTracker functionsTracker = loadFunction.tasks();	if (!scope.database) {	scope.rootTasks.addAll(functionsTracker.tasks());	} else {	setUpDependencies(dbTracker, functionsTracker);	}	loadTaskTracker.update(functionsTracker);	
functions 

setUpDependencies(dbTracker, functionsTracker);	}	loadTaskTracker.update(functionsTracker);	break;	}	case Constraint: {	LoadConstraint loadConstraint = new LoadConstraint(context, (ConstraintEvent) next, work.dbNameToLoadIn, dbTracker);	TaskTracker constraintTracker = loadConstraint.tasks();	scope.rootTasks.addAll(constraintTracker.tasks());	loadTaskTracker.update(constraintTracker);	
constraints 

createEndReplLogTask(context, scope, iterator.replLogger());	}	}	boolean addAnotherLoadTask = iterator.hasNext() || loadTaskTracker.hasReplicationState() || constraintIterator.hasNext();	createBuilderTask(scope.rootTasks, addAnotherLoadTask);	if (!iterator.hasNext() && !constraintIterator.hasNext()) {	loadTaskTracker.update(updateDatabaseLastReplID(maxTasks, context, scope));	work.updateDbEventState(null);	}	this.childTasks = scope.rootTasks;	
root tasks total tasks 

}	boolean addAnotherLoadTask = iterator.hasNext() || loadTaskTracker.hasReplicationState() || constraintIterator.hasNext();	createBuilderTask(scope.rootTasks, addAnotherLoadTask);	if (!iterator.hasNext() && !constraintIterator.hasNext()) {	loadTaskTracker.update(updateDatabaseLastReplID(maxTasks, context, scope));	work.updateDbEventState(null);	}	this.childTasks = scope.rootTasks;	driverContext.getCtx().getFsScratchDirs().putAll(context.pathInfo.getFsScratchDirs());	} catch (Exception e) {	
failed replication 

if (!iterator.hasNext() && !constraintIterator.hasNext()) {	loadTaskTracker.update(updateDatabaseLastReplID(maxTasks, context, scope));	work.updateDbEventState(null);	}	this.childTasks = scope.rootTasks;	driverContext.getCtx().getFsScratchDirs().putAll(context.pathInfo.getFsScratchDirs());	} catch (Exception e) {	setException(e);	return 1;	}	
completed load task run 

========================= hive sample_3917 =========================

private void convertMapWork(MapWork mapWork, boolean isTezOrSpark) throws SemanticException {	mapWork.setVectorizationExamined(true);	currentBaseWork = mapWork;	VectorTaskColumnInfo vectorTaskColumnInfo = new VectorTaskColumnInfo();	vectorTaskColumnInfo.assume();	mapWork.setVectorizedVertexNum(++vectorizedVertexNum);	if (!validateAndVectorizeMapWork(mapWork, vectorTaskColumnInfo, isTezOrSpark)) {	if (currentBaseWork.getVectorizationEnabled()) {	VectorizerReason notVectorizedReason  = currentBaseWork.getNotVectorizedReason();	if (notVectorizedReason == null) {	
cannot vectorize unknown 

mapWork.setVectorizationExamined(true);	currentBaseWork = mapWork;	VectorTaskColumnInfo vectorTaskColumnInfo = new VectorTaskColumnInfo();	vectorTaskColumnInfo.assume();	mapWork.setVectorizedVertexNum(++vectorizedVertexNum);	if (!validateAndVectorizeMapWork(mapWork, vectorTaskColumnInfo, isTezOrSpark)) {	if (currentBaseWork.getVectorizationEnabled()) {	VectorizerReason notVectorizedReason  = currentBaseWork.getNotVectorizedReason();	if (notVectorizedReason == null) {	} else {	
cannot vectorize 

private boolean isInputFormatExcluded(String inputFileFormatClassName, Collection<Class<?>> excludes) {	Class<?> ifClass = null;	try {	ifClass = Class.forName(inputFileFormatClassName);	} catch (ClassNotFoundException e) {	
cannot verify class for 

}	PartitionDesc partDesc = pathToPartitionInfo.get(path);	if (partDesc.getVectorPartitionDesc() != null) {	continue;	}	Set<Support> newSupportSet = new TreeSet<Support>();	if (!verifyAndSetVectorPartDesc( partDesc, isAcidTable, inputFileFormatClassNameSet, enabledConditionsMetSet, enabledConditionsNotMetList, newSupportSet)) {	mapWork.setVectorizationInputFileFormatClassNameSet(inputFileFormatClassNameSet);	mapWork.setVectorizationEnabledConditionsMet(new ArrayList(enabledConditionsMetSet));	mapWork.setVectorizationEnabledConditionsNotMet(enabledConditionsNotMetList);	
cannot enable vectorization because input file format s do not met conditions 

private boolean validateAndVectorizeMapWork(MapWork mapWork, VectorTaskColumnInfo vectorTaskColumnInfo, boolean isTezOrSpark) throws SemanticException {	
examining input format to see if vectorization is enabled 

dataTypePhysicalVariations.add(dataTypePhysicalVariation);	}	for (int i = 0; i < vectorTaskColumnInfo.partitionColumnCount; i++) {	dataTypePhysicalVariations.add(DataTypePhysicalVariation.NONE);	}	vectorTaskColumnInfo.setAlldataTypePhysicalVariations(dataTypePhysicalVariations);	availableVectorizedVirtualColumnSet = new HashSet<VirtualColumn>();	availableVectorizedVirtualColumnSet.addAll(vectorTaskColumnInfo.availableVirtualColumnList);	neededVirtualColumnSet = new HashSet<VirtualColumn>();	mapWork.setVectorizationEnabled(true);	
vectorization is enabled for input format s 

private boolean validateAndVectorizeMapOperators(MapWork mapWork, TableScanOperator tableScanOperator, boolean isTezOrSpark, VectorTaskColumnInfo vectorTaskColumnInfo) throws SemanticException {	
validating and vectorizing mapwork 

currentBaseWork = reduceWork;	currentBaseWork.setVectorizationEnabled(true);	VectorTaskColumnInfo vectorTaskColumnInfo = new VectorTaskColumnInfo();	vectorTaskColumnInfo.assume();	reduceWork.setVectorizedVertexNum(++vectorizedVertexNum);	reduceWork.setVectorizedTestingReducerBatchSize(vectorizedTestingReducerBatchSize);	if (!validateAndVectorizeReduceWork(reduceWork, vectorTaskColumnInfo)) {	if (currentBaseWork.getVectorizationEnabled()) {	VectorizerReason notVectorizedReason  = currentBaseWork.getNotVectorizedReason();	if (notVectorizedReason == null) {	
cannot vectorize unknown 

currentBaseWork.setVectorizationEnabled(true);	VectorTaskColumnInfo vectorTaskColumnInfo = new VectorTaskColumnInfo();	vectorTaskColumnInfo.assume();	reduceWork.setVectorizedVertexNum(++vectorizedVertexNum);	reduceWork.setVectorizedTestingReducerBatchSize(vectorizedTestingReducerBatchSize);	if (!validateAndVectorizeReduceWork(reduceWork, vectorTaskColumnInfo)) {	if (currentBaseWork.getVectorizationEnabled()) {	VectorizerReason notVectorizedReason  = currentBaseWork.getNotVectorizedReason();	if (notVectorizedReason == null) {	} else {	
cannot vectorize 

private boolean validateAndVectorizeReduceOperators(ReduceWork reduceWork, VectorTaskColumnInfo vectorTaskColumnInfo) throws SemanticException {	
validating and vectorizing reducework 

ArrayList<TypeInfo> reduceTypeInfos = new ArrayList<TypeInfo>();	if (reduceWork.getNeedsTagging()) {	setNodeIssue("Tagging not supported");	return false;	}	String columnSortOrder;	String columnNullOrder;	try {	TableDesc keyTableDesc = reduceWork.getKeyDesc();	if (LOG.isDebugEnabled()) {	
using reduce tag 

switch (vectorizationEnabledOverride) {	case NONE: weCanAttemptVectorization = isVectorizationEnabled;	break;	case DISABLE: weCanAttemptVectorization = false;	break;	case ENABLE: weCanAttemptVectorization = true;	break;	default: throw new RuntimeException("Unexpected vectorization enabled override " + vectorizationEnabledOverride);	}	if (!weCanAttemptVectorization) {	
vectorization is disabled 

if (!validateExprNodeDesc(valueExprs, "Value")) {	return false;	}	Byte[] order = desc.getTagOrder();	Byte posSingleVectorMapJoinSmallTable = (order[0] == posBigTable ? order[1] : order[0]);	List<ExprNodeDesc> smallTableExprs = desc.getExprs().get(posSingleVectorMapJoinSmallTable);	if (!validateExprNodeDesc(smallTableExprs, "Small Table")) {	return false;	}	if (desc.getResidualFilterExprs() != null && !desc.getResidualFilterExprs().isEmpty()) {	
cannot vectorize join with complex on clause 

if (desc.isGroupingSetsPresent() && (processingMode != ProcessingMode.HASH && processingMode != ProcessingMode.STREAMING)) {	setOperatorIssue("Vectorized GROUPING SETS only expected for HASH and STREAMING processing modes");	return false;	}	if (!validateAggregationDescs(desc.getAggregators(), desc.getMode(), hasKeys)) {	return false;	}	vectorGroupByDesc.setProcessingMode(processingMode);	vectorGroupByDesc.setIsVectorizationComplexTypesEnabled(isVectorizationComplexTypesEnabled);	vectorGroupByDesc.setIsVectorizationGroupByComplexTypesEnabled(isVectorizationGroupByComplexTypesEnabled);	
vector group by operator will use processing mode 

private boolean isBigTableOnlyResults(MapJoinDesc desc) {	Byte[] order = desc.getTagOrder();	byte posBigTable = (byte) desc.getPosBigTable();	Byte posSingleVectorMapJoinSmallTable = (order[0] == posBigTable ? order[1] : order[0]);	int[] smallTableIndices;	int smallTableIndicesSize;	if (desc.getValueIndices() != null && desc.getValueIndices().get(posSingleVectorMapJoinSmallTable) != null) {	smallTableIndices = desc.getValueIndices().get(posSingleVectorMapJoinSmallTable);	
vectorizer isbigtableonlyresults smalltableindices 

Byte[] order = desc.getTagOrder();	byte posBigTable = (byte) desc.getPosBigTable();	Byte posSingleVectorMapJoinSmallTable = (order[0] == posBigTable ? order[1] : order[0]);	int[] smallTableIndices;	int smallTableIndicesSize;	if (desc.getValueIndices() != null && desc.getValueIndices().get(posSingleVectorMapJoinSmallTable) != null) {	smallTableIndices = desc.getValueIndices().get(posSingleVectorMapJoinSmallTable);	smallTableIndicesSize = smallTableIndices.length;	} else {	smallTableIndices = null;	
vectorizer isbigtableonlyresults smalltableindices empty 

int[] smallTableIndices;	int smallTableIndicesSize;	if (desc.getValueIndices() != null && desc.getValueIndices().get(posSingleVectorMapJoinSmallTable) != null) {	smallTableIndices = desc.getValueIndices().get(posSingleVectorMapJoinSmallTable);	smallTableIndicesSize = smallTableIndices.length;	} else {	smallTableIndices = null;	smallTableIndicesSize = 0;	}	List<Integer> smallTableRetainList = desc.getRetainList().get(posSingleVectorMapJoinSmallTable);	
vectorizer isbigtableonlyresults smalltableretainlist 

for (int i = 0; i < smallTableIndicesSize; i++) {	if (smallTableIndices[i] < 0) {	setOperatorIssue("Vectorizer isBigTableOnlyResults smallTableIndices[i] < 0 returning false");	return false;	}	}	} else if (smallTableRetainSize > 0) {	setOperatorIssue("Vectorizer isBigTableOnlyResults smallTableRetainSize > 0 returning false");	return false;	}	
vectorizer isbigtableonlyresults returning true 

private Operator<? extends OperatorDesc> specializeReduceSinkOperator( Operator<? extends OperatorDesc> op, VectorizationContext vContext, ReduceSinkDesc desc, VectorReduceSinkDesc vectorDesc) throws HiveException {	VectorReduceSinkInfo vectorReduceSinkInfo = vectorDesc.getVectorReduceSinkInfo();	Type[] reduceSinkKeyColumnVectorTypes = vectorReduceSinkInfo.getReduceSinkKeyColumnVectorTypes();	VectorReduceSinkDesc.ReduceSinkKeyType reduceSinkKeyType = VectorReduceSinkDesc.ReduceSinkKeyType.MULTI_KEY;	if (reduceSinkKeyColumnVectorTypes != null && reduceSinkKeyColumnVectorTypes.length == 1) {	
vectorizer vectorizeoperator groupby typename 

}	} else {	if (vectorDesc.getIsEmptyKey() && vectorDesc.getIsEmptyBuckets() && vectorDesc.getIsEmptyPartitions()) {	opClass = VectorReduceSinkEmptyKeyOperator.class;	} else {	opClass = VectorReduceSinkObjectHashOperator.class;	}	}	vectorDesc.setReduceSinkKeyType(reduceSinkKeyType);	vectorDesc.setVectorReduceSinkInfo(vectorReduceSinkInfo);	
vectorizer vectorizeoperator reduce sink class 

} else {	opClass = VectorReduceSinkObjectHashOperator.class;	}	}	vectorDesc.setReduceSinkKeyType(reduceSinkKeyType);	vectorDesc.setVectorReduceSinkInfo(vectorReduceSinkInfo);	Operator<? extends OperatorDesc> vectorOp = null;	try {	vectorOp = OperatorFactory.getVectorOperator( opClass, op.getCompilationOpContext(), op.getConf(), vContext, vectorDesc);	} catch (Exception e) {	
vectorizer vectorizeoperator reduce sink class exception exception 

throw new VectorizerCannotVectorizeException();	}	} catch (HiveException e) {	setOperatorIssue(e.getMessage());	throw new VectorizerCannotVectorizeException();	}	Preconditions.checkState(vectorOp != null);	if (vectorTaskColumnInfo != null && !isNative) {	vectorTaskColumnInfo.setAllNative(false);	}	
vectorizeoperator 

throw new VectorizerCannotVectorizeException();	}	} catch (HiveException e) {	setOperatorIssue(e.getMessage());	throw new VectorizerCannotVectorizeException();	}	Preconditions.checkState(vectorOp != null);	if (vectorTaskColumnInfo != null && !isNative) {	vectorTaskColumnInfo.setAllNative(false);	}	
vectorizeoperator 

public void debugDisplayVertexInfo(BaseWork work) {	VectorizedRowBatchCtx vectorizedRowBatchCtx = work.getVectorizedRowBatchCtx();	String[] allColumnNames = vectorizedRowBatchCtx.getRowColumnNames();	TypeInfo[] columnTypeInfos = vectorizedRowBatchCtx.getRowColumnTypeInfos();	DataTypePhysicalVariation[] dataTypePhysicalVariations = vectorizedRowBatchCtx.getRowdataTypePhysicalVariations();	int partitionColumnCount = vectorizedRowBatchCtx.getPartitionColumnCount();	int virtualColumnCount = vectorizedRowBatchCtx.getVirtualColumnCount();	String[] scratchColumnTypeNames =vectorizedRowBatchCtx.getScratchColumnTypeNames();	DataTypePhysicalVariation[] scratchdataTypePhysicalVariations = vectorizedRowBatchCtx.getScratchDataTypePhysicalVariations();	
debugdisplayvertexinfo rowcolumnnames 

public void debugDisplayVertexInfo(BaseWork work) {	VectorizedRowBatchCtx vectorizedRowBatchCtx = work.getVectorizedRowBatchCtx();	String[] allColumnNames = vectorizedRowBatchCtx.getRowColumnNames();	TypeInfo[] columnTypeInfos = vectorizedRowBatchCtx.getRowColumnTypeInfos();	DataTypePhysicalVariation[] dataTypePhysicalVariations = vectorizedRowBatchCtx.getRowdataTypePhysicalVariations();	int partitionColumnCount = vectorizedRowBatchCtx.getPartitionColumnCount();	int virtualColumnCount = vectorizedRowBatchCtx.getVirtualColumnCount();	String[] scratchColumnTypeNames =vectorizedRowBatchCtx.getScratchColumnTypeNames();	DataTypePhysicalVariation[] scratchdataTypePhysicalVariations = vectorizedRowBatchCtx.getScratchDataTypePhysicalVariations();	
debugdisplayvertexinfo rowcolumntypeinfos 

public void debugDisplayVertexInfo(BaseWork work) {	VectorizedRowBatchCtx vectorizedRowBatchCtx = work.getVectorizedRowBatchCtx();	String[] allColumnNames = vectorizedRowBatchCtx.getRowColumnNames();	TypeInfo[] columnTypeInfos = vectorizedRowBatchCtx.getRowColumnTypeInfos();	DataTypePhysicalVariation[] dataTypePhysicalVariations = vectorizedRowBatchCtx.getRowdataTypePhysicalVariations();	int partitionColumnCount = vectorizedRowBatchCtx.getPartitionColumnCount();	int virtualColumnCount = vectorizedRowBatchCtx.getVirtualColumnCount();	String[] scratchColumnTypeNames =vectorizedRowBatchCtx.getScratchColumnTypeNames();	DataTypePhysicalVariation[] scratchdataTypePhysicalVariations = vectorizedRowBatchCtx.getScratchDataTypePhysicalVariations();	LOG.debug("debugDisplayVertexInfo rowDataTypePhysicalVariations " + (dataTypePhysicalVariations == null ? "NULL" : Arrays.toString(dataTypePhysicalVariations)));	
debugdisplayvertexinfo partitioncolumncount 

public void debugDisplayVertexInfo(BaseWork work) {	VectorizedRowBatchCtx vectorizedRowBatchCtx = work.getVectorizedRowBatchCtx();	String[] allColumnNames = vectorizedRowBatchCtx.getRowColumnNames();	TypeInfo[] columnTypeInfos = vectorizedRowBatchCtx.getRowColumnTypeInfos();	DataTypePhysicalVariation[] dataTypePhysicalVariations = vectorizedRowBatchCtx.getRowdataTypePhysicalVariations();	int partitionColumnCount = vectorizedRowBatchCtx.getPartitionColumnCount();	int virtualColumnCount = vectorizedRowBatchCtx.getVirtualColumnCount();	String[] scratchColumnTypeNames =vectorizedRowBatchCtx.getScratchColumnTypeNames();	DataTypePhysicalVariation[] scratchdataTypePhysicalVariations = vectorizedRowBatchCtx.getScratchDataTypePhysicalVariations();	LOG.debug("debugDisplayVertexInfo rowDataTypePhysicalVariations " + (dataTypePhysicalVariations == null ? "NULL" : Arrays.toString(dataTypePhysicalVariations)));	
debugdisplayvertexinfo virtualcolumncount 

public void debugDisplayVertexInfo(BaseWork work) {	VectorizedRowBatchCtx vectorizedRowBatchCtx = work.getVectorizedRowBatchCtx();	String[] allColumnNames = vectorizedRowBatchCtx.getRowColumnNames();	TypeInfo[] columnTypeInfos = vectorizedRowBatchCtx.getRowColumnTypeInfos();	DataTypePhysicalVariation[] dataTypePhysicalVariations = vectorizedRowBatchCtx.getRowdataTypePhysicalVariations();	int partitionColumnCount = vectorizedRowBatchCtx.getPartitionColumnCount();	int virtualColumnCount = vectorizedRowBatchCtx.getVirtualColumnCount();	String[] scratchColumnTypeNames =vectorizedRowBatchCtx.getScratchColumnTypeNames();	DataTypePhysicalVariation[] scratchdataTypePhysicalVariations = vectorizedRowBatchCtx.getScratchDataTypePhysicalVariations();	LOG.debug("debugDisplayVertexInfo rowDataTypePhysicalVariations " + (dataTypePhysicalVariations == null ? "NULL" : Arrays.toString(dataTypePhysicalVariations)));	
debugdisplayvertexinfo scratchcolumntypenames 

========================= hive sample_3014 =========================

public void apply(DirSearch ldap, String user) throws AuthenticationException {	
authenticating user using 

public void apply(DirSearch ldap, String user) throws AuthenticationException {	List<String> memberOf = null;	try {	String userDn = ldap.findUserDn(user);	memberOf = ldap.findGroupsForUser(userDn);	
user member of 

List<String> memberOf = null;	try {	String userDn = ldap.findUserDn(user);	memberOf = ldap.findGroupsForUser(userDn);	} catch (NamingException e) {	throw new AuthenticationException("LDAP Authentication failed for user", e);	}	for (String groupDn : memberOf) {	String shortName = LdapUtils.getShortName(groupDn);	if (groupFilter.contains(shortName)) {	
groupmembershipkeyfilter passes user is a member of group 

List<String> memberOf = null;	try {	String userDn = ldap.findUserDn(user);	memberOf = ldap.findGroupsForUser(userDn);	} catch (NamingException e) {	throw new AuthenticationException("LDAP Authentication failed for user", e);	}	for (String groupDn : memberOf) {	String shortName = LdapUtils.getShortName(groupDn);	if (groupFilter.contains(shortName)) {	
authentication succeeded based on group membership 

memberOf = ldap.findGroupsForUser(userDn);	} catch (NamingException e) {	throw new AuthenticationException("LDAP Authentication failed for user", e);	}	for (String groupDn : memberOf) {	String shortName = LdapUtils.getShortName(groupDn);	if (groupFilter.contains(shortName)) {	return;	}	}	
authentication failed based on user membership 

public void apply(DirSearch ldap, String user) throws AuthenticationException {	
authenticating user using 

public void apply(DirSearch ldap, String user) throws AuthenticationException {	List<String> groupDns = new ArrayList<>();	for (String groupId : groupFilter) {	try {	String groupDn = ldap.findGroupDn(groupId);	groupDns.add(groupDn);	} catch (NamingException e) {	
cannot find dn for group 

public void apply(DirSearch ldap, String user) throws AuthenticationException {	List<String> groupDns = new ArrayList<>();	for (String groupId : groupFilter) {	try {	String groupDn = ldap.findGroupDn(groupId);	groupDns.add(groupDn);	} catch (NamingException e) {	
cannot find dn for group 

}	}	if (groupDns.isEmpty()) {	String msg = String.format("No DN(s) has been found for any of group(s): %s", Joiner.on(',').join(groupFilter));	LOG.debug(msg);	throw new AuthenticationException("No DN(s) has been found for any of specified group(s)");	}	for (String groupDn : groupDns) {	try {	if (ldap.isUserMemberOfGroup(user, groupDn)) {	
usermembershipkeyfilter passes user is a member of group 

}	}	if (groupDns.isEmpty()) {	String msg = String.format("No DN(s) has been found for any of group(s): %s", Joiner.on(',').join(groupFilter));	LOG.debug(msg);	throw new AuthenticationException("No DN(s) has been found for any of specified group(s)");	}	for (String groupDn : groupDns) {	try {	if (ldap.isUserMemberOfGroup(user, groupDn)) {	
authentication succeeded based on user membership 

String msg = String.format("No DN(s) has been found for any of group(s): %s", Joiner.on(',').join(groupFilter));	LOG.debug(msg);	throw new AuthenticationException("No DN(s) has been found for any of specified group(s)");	}	for (String groupDn : groupDns) {	try {	if (ldap.isUserMemberOfGroup(user, groupDn)) {	return;	}	} catch (NamingException e) {	
cannot match user and group 

========================= hive sample_2310 =========================

skewTableKeyInspectors.add(keyFields.get(k).getFieldObjectInspector());	}	TableDesc joinKeyDesc = desc.getKeyTableDesc();	List<String> keyColNames = Utilities.getColumnNames(joinKeyDesc .getProperties());	StructObjectInspector structTblKeyInpector = ObjectInspectorFactory .getStandardStructObjectInspector(keyColNames, skewTableKeyInspectors);	try {	AbstractSerDe serializer = (AbstractSerDe) ReflectionUtils.newInstance(tblDesc.get( alias).getDeserializerClass(), null);	SerDeUtils.initializeSerDe(serializer, null, tblDesc.get(alias).getProperties(), null);	tblSerializers.put((byte) i, serializer);	} catch (SerDeException e) {	
skewjoin will be disabled due to 

private void delete(Path operatorOutputPath, FileSystem fs) {	try {	fs.delete(operatorOutputPath, true);	} catch (IOException e) {	
failed to delete path 

========================= hive sample_3852 =========================

private static ObjectCache getLlapObjectCache(String queryId) {	if (queryId == null) throw new RuntimeException("Query ID cannot be null");	ObjectCache result = llapQueryCaches.get(queryId);	if (result != null) return result;	result = new LlapObjectCache();	ObjectCache old = llapQueryCaches.putIfAbsent(queryId, result);	if (old == null && LOG.isInfoEnabled()) {	
created object cache for 

public static void removeLlapQueryCache(String queryId) {	if (LOG.isInfoEnabled()) {	
removing object cache for 

========================= hive sample_4047 =========================

private LlapIoImpl(Configuration conf) throws IOException {	this.daemonConf = conf;	String ioMode = HiveConf.getVar(conf, HiveConf.ConfVars.LLAP_IO_MEMORY_MODE);	boolean useLowLevelCache = LlapIoImpl.MODE_CACHE.equalsIgnoreCase(ioMode);	
initializing llap io in mode none 

String sessionId = conf.get("llap.daemon.metrics.sessionid");	this.cacheMetrics = LlapDaemonCacheMetrics.create(displayName, sessionId);	displayName = "LlapDaemonIOMetrics-" + MetricsUtils.getHostName();	String[] strIntervals = HiveConf.getTrimmedStringsVar(conf, HiveConf.ConfVars.LLAP_IO_DECODING_METRICS_PERCENTILE_INTERVALS);	List<Integer> intervalList = new ArrayList<>();	if (strIntervals != null) {	for (String strInterval : strIntervals) {	try {	intervalList.add(Integer.valueOf(strInterval));	} catch (NumberFormatException e) {	
ignoring io decoding metrics interval from as it is invalid 

List<Integer> intervalList = new ArrayList<>();	if (strIntervals != null) {	for (String strInterval : strIntervals) {	try {	intervalList.add(Integer.valueOf(strInterval));	} catch (NumberFormatException e) {	}	}	}	this.ioMetrics = LlapDaemonIOMetrics.create(displayName, sessionId, Ints.toArray(intervalList));	
started llap daemon metrics with displayname sessionid 

fileMetadataCache = null;	SimpleBufferManager sbm = new SimpleBufferManager(allocator, cacheMetrics);	bufferManager = bufferManagerOrc = bufferManagerGeneric = sbm;	dataCache = sbm;	}	int numThreads = HiveConf.getIntVar(conf, HiveConf.ConfVars.LLAP_IO_THREADPOOL_SIZE);	executor = new StatsRecordingThreadPool(numThreads, numThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue<Runnable>(), new ThreadFactoryBuilder().setNameFormat("IO-Elevator-Thread-%d").setDaemon(true).build());	FixedSizedObjectPool<IoTrace> tracePool = IoTrace.createTracePool(conf);	this.orcCvp = new OrcColumnVectorProducer( metadataCache, dataCache, bufferManagerOrc, conf, cacheMetrics, ioMetrics, tracePool);	this.genericCvp = isEncodeEnabled ? new GenericColumnVectorProducer( serdeCache, bufferManagerGeneric, conf, cacheMetrics, ioMetrics, tracePool) : null;	
llap io initialized 

public InputFormat<NullWritable, VectorizedRowBatch> getInputFormat( InputFormat<?, ?> sourceInputFormat, Deserializer sourceSerDe) {	ColumnVectorProducer cvp = genericCvp;	if (sourceInputFormat instanceof OrcInputFormat) {	cvp = orcCvp;	} else if (cvp == null) {	
llap encode is disabled cannot use for 

public void close() {	
closing llapioimpl 

========================= hive sample_2193 =========================

public static void beforeTest() throws Exception   {	hiveConf = new HiveConf();	hiveConf.setBoolVar(ConfVars.HIVE_SERVER2_SUPPORT_DYNAMIC_SERVICE_DISCOVERY, true);	hiveConf.setIntVar(ConfVars.HIVE_ZOOKEEPER_CONNECTION_MAX_RETRIES, 0);	hiveConf.setTimeVar(ConfVars.HIVE_ZOOKEEPER_CONNECTION_BASESLEEPTIME, 0, TimeUnit.MILLISECONDS);	miniHS2 = new MiniHS2(hiveConf);	Map<String, String> confOverlay = new HashMap<String, String>();	try {	miniHS2.start(confOverlay);	} catch (Exception ex) {	
zookeeper is not set up intentionally so the error is expected unless it s not related to zk 

========================= hive sample_322 =========================

public void close() {	try {	localClientCache.invalidateAll();	signers.invalidateAll();	localClientCache.cleanUp();	signers.cleanUp();	} catch (Exception ex) {	
error closing the coordinator ignoring 

========================= hive sample_637 =========================

private void setUpFetchContexts(String alias, MergeQueue mergeQueue) throws HiveException {	mergeQueue.clearFetchContext();	Path currentInputPath = getExecContext().getCurrentInputPath();	BucketMapJoinContext bucketMatcherCxt = localWork.getBucketMapjoinContext();	Class<? extends BucketMatcher> bucketMatcherCls = bucketMatcherCxt.getBucketMatcherClass();	BucketMatcher bucketMatcher = ReflectionUtil.newInstance(bucketMatcherCls, null);	getExecContext().setFileId(bucketMatcherCxt.createFileId(currentInputPath.toString()));	if (LOG.isInfoEnabled()) {	
set task id 

public final InspectableObject getNextRow() throws IOException {	if (currentMinSegment != null) {	adjustPriorityQueue(currentMinSegment);	}	Integer current = top();	if (current == null) {	if (LOG.isInfoEnabled()) {	
mergequeue forwarded rows 

========================= hive sample_4102 =========================

try {	rv =  EximUtil.readMetaData(fs, new Path(fromPath, EximUtil.METADATA_NAME));	} catch (IOException e) {	throw new SemanticException(ErrorMsg.INVALID_PATH.getMsg(), e);	}	if (rv.getTable() == null) {	return false;	}	ReplicationSpec replicationSpec = rv.getReplicationSpec();	if (replicationSpec.isNoop()){	
current update with id is noop 

x.getConf().set("import.destination.table", tblDesc.getTableName());	for (AddPartitionDesc addPartitionDesc : partitionDescs) {	addPartitionDesc.setTableName(tblDesc.getTableName());	}	}	Warehouse wh = new Warehouse(x.getConf());	Table table = tableIfExists(tblDesc, x.getHive());	boolean tableExists = false;	if (table != null){	checkTable(table, tblDesc,replicationSpec, x.getConf());	
table exists metadata checked 

private static Task<?> addSinglePartition(URI fromURI, FileSystem fs, ImportTableDesc tblDesc, Table table, Warehouse wh, AddPartitionDesc addPartitionDesc, ReplicationSpec replicationSpec, EximUtil.SemanticAnalyzerWrapperContext x, Long txnId, int stmtId, boolean isSourceMm, Task<?> commitTask) throws MetaException, IOException, HiveException {	AddPartitionDesc.OnePartitionDesc partSpec = addPartitionDesc.getPartition(0);	if (tblDesc.isExternal() && tblDesc.getLocation() == null) {	
importing in place adding addpart for partition 

private static Task<?> addSinglePartition(URI fromURI, FileSystem fs, ImportTableDesc tblDesc, Table table, Warehouse wh, AddPartitionDesc addPartitionDesc, ReplicationSpec replicationSpec, EximUtil.SemanticAnalyzerWrapperContext x, Long txnId, int stmtId, boolean isSourceMm, Task<?> commitTask) throws MetaException, IOException, HiveException {	AddPartitionDesc.OnePartitionDesc partSpec = addPartitionDesc.getPartition(0);	if (tblDesc.isExternal() && tblDesc.getLocation() == null) {	Task<?> addPartTask = TaskFactory.get(new DDLWork(x.getInputs(), x.getOutputs(), addPartitionDesc), x.getConf());	return addPartTask;	} else {	String srcLocation = partSpec.getLocation();	fixLocationInPartSpec(fs, tblDesc, table, wh, replicationSpec, partSpec, x);	
adding dependent copywork addpart movework for partition with source location 

if (tblDesc.isExternal() && tblDesc.getLocation() == null) {	Task<?> addPartTask = TaskFactory.get(new DDLWork(x.getInputs(), x.getOutputs(), addPartitionDesc), x.getConf());	return addPartTask;	} else {	String srcLocation = partSpec.getLocation();	fixLocationInPartSpec(fs, tblDesc, table, wh, replicationSpec, partSpec, x);	Path tgtLocation = new Path(partSpec.getLocation());	Path destPath = !AcidUtils.isInsertOnlyTable(table.getParameters()) ? x.getCtx().getExternalTmpPath(tgtLocation) : new Path(tgtLocation, AcidUtils.deltaSubdir(txnId, txnId, stmtId));	Path moveTaskSrc =  !AcidUtils.isInsertOnlyTable(table.getParameters()) ? destPath : tgtLocation;	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
adding import work for partition with source location target copy dest mm src for 

public static void checkTargetLocationEmpty(FileSystem fs, Path targetPath, ReplicationSpec replicationSpec, Logger logger) throws IOException, SemanticException {	if (replicationSpec.isInReplicationScope()){	return;	}	
checking emptiness of 

public static void checkTargetLocationEmpty(FileSystem fs, Path targetPath, ReplicationSpec replicationSpec, Logger logger) throws IOException, SemanticException {	if (replicationSpec.isInReplicationScope()){	return;	}	if (fs.exists(targetPath)) {	FileStatus[] status = fs.listStatus(targetPath, FileUtils.HIDDEN_FILES_PATH_FILTER);	if (status.length > 0) {	
files inc found in path 

private static void createRegularImportTasks( ImportTableDesc tblDesc, List<AddPartitionDesc> partitionDescs, boolean isPartSpecSet, ReplicationSpec replicationSpec, Table table, URI fromURI, FileSystem fs, Warehouse wh, EximUtil.SemanticAnalyzerWrapperContext x, Long txnId, int stmtId, boolean isSourceMm) throws HiveException, URISyntaxException, IOException, MetaException {	if (table != null) {	if (table.isPartitioned()) {	
table partitioned 

for (AddPartitionDesc addPartitionDesc : partitionDescs) {	Map<String, String> partSpec = addPartitionDesc.getPartition(0).getPartSpec();	org.apache.hadoop.hive.ql.metadata.Partition ptn = null;	if ((ptn = x.getHive().getPartition(table, partSpec, false)) == null) {	x.getTasks().add(addSinglePartition( fromURI, fs, tblDesc, table, wh, addPartitionDesc, replicationSpec, x, txnId, stmtId, isSourceMm, ict));	} else {	throw new SemanticException( ErrorMsg.PARTITION_EXISTS.getMsg(partSpecToString(partSpec)));	}	}	} else {	
table non partitioned 

}	}	} else {	Path tgtPath = new Path(table.getDataLocation().toString());	FileSystem tgtFs = FileSystem.get(tgtPath.toUri(), x.getConf());	checkTargetLocationEmpty(tgtFs, tgtPath, replicationSpec, x.getLOG());	loadTable(fromURI, table, false, tgtPath, replicationSpec, x, txnId, stmtId, isSourceMm);	}	x.getOutputs().add(new WriteEntity(table, WriteEntity.WriteType.DDL_NO_LOCK));	} else {	
table does not exist 

Task<?> t = createTableTask(tblDesc, x);	table = new Table(tblDesc.getDatabaseName(), tblDesc.getTableName());	Database parentDb = x.getHive().getDatabase(tblDesc.getDatabaseName());	x.getOutputs().add(new WriteEntity(parentDb, WriteEntity.WriteType.DDL_SHARED));	if (isPartitioned(tblDesc)) {	Task<?> ict = createImportCommitTask( tblDesc.getDatabaseName(), tblDesc.getTableName(), txnId, stmtId, x.getConf(), AcidUtils.isInsertOnlyTable(tblDesc.getTblProps()));	for (AddPartitionDesc addPartitionDesc : partitionDescs) {	t.addDependentTask(addSinglePartition(fromURI, fs, tblDesc, table, wh, addPartitionDesc, replicationSpec, x, txnId, stmtId, isSourceMm, ict));	}	} else {	
adding dependent copywork movework for table 

table = new Table(tblDesc.getDatabaseName(), tblDesc.getTableName());	Database parentDb = x.getHive().getDatabase(tblDesc.getDatabaseName());	x.getOutputs().add(new WriteEntity(parentDb, WriteEntity.WriteType.DDL_SHARED));	if (isPartitioned(tblDesc)) {	Task<?> ict = createImportCommitTask( tblDesc.getDatabaseName(), tblDesc.getTableName(), txnId, stmtId, x.getConf(), AcidUtils.isInsertOnlyTable(tblDesc.getTblProps()));	for (AddPartitionDesc addPartitionDesc : partitionDescs) {	t.addDependentTask(addSinglePartition(fromURI, fs, tblDesc, table, wh, addPartitionDesc, replicationSpec, x, txnId, stmtId, isSourceMm, ict));	}	} else {	if (tblDesc.isExternal() && (tblDesc.getLocation() == null)) {	
importing in place no emptiness check no copying loading 

Task<?> dr = null;	WriteEntity.WriteType lockType = WriteEntity.WriteType.DDL_NO_LOCK;	Database parentDb = x.getHive().getDatabase(tblDesc.getDatabaseName());	if (parentDb == null){	if (!waitOnPrecursor){	throw new SemanticException(ErrorMsg.DATABASE_NOT_EXISTS.getMsg(tblDesc.getDatabaseName()));	}	}	if (table != null) {	if (!replicationSpec.allowReplacementInto(table.getParameters())) {	
table is not replaced as it is newer than the update 

if (!waitOnPrecursor){	throw new SemanticException(ErrorMsg.DATABASE_NOT_EXISTS.getMsg(tblDesc.getDatabaseName()));	}	}	if (table != null) {	if (!replicationSpec.allowReplacementInto(table.getParameters())) {	return;	}	} else {	if ((parentDb != null) && (!replicationSpec.allowReplacementInto(parentDb.getParameters()))) {	
table is not created as the database is newer than the update 

if (isPartitioned(tblDesc)) {	Task<?> ict = createImportCommitTask( tblDesc.getDatabaseName(), tblDesc.getTableName(), txnId, stmtId, x.getConf(), AcidUtils.isInsertOnlyTable(tblDesc.getTblProps()));	for (AddPartitionDesc addPartitionDesc : partitionDescs) {	addPartitionDesc.setReplicationSpec(replicationSpec);	t.addDependentTask( addSinglePartition(fromURI, fs, tblDesc, table, wh, addPartitionDesc, replicationSpec, x, txnId, stmtId, isSourceMm, ict));	if (updatedMetadata != null) {	updatedMetadata.addPartition(addPartitionDesc.getPartition(0).getPartSpec());	}	}	} else {	
adding dependent copywork movework for table 

updatedMetadata.addPartition(addPartitionDesc.getPartition(0).getPartSpec());	}	}	} else {	t.addDependentTask(loadTable(fromURI, table, true, new Path(tblDesc.getLocation()), replicationSpec, x, txnId, stmtId, isSourceMm));	}	}	x.getTasks().add(t);	} else {	if (table.isPartitioned()) {	
table partitioned 

}	}	}	if (replicationSpec.isMetadataOnly() && partitionDescs.isEmpty()){	x.getTasks().add(alterTableTask(tblDesc, x,replicationSpec));	if (lockType == WriteEntity.WriteType.DDL_NO_LOCK){	lockType = WriteEntity.WriteType.DDL_SHARED;	}	}	} else {	
table non partitioned 

========================= hive sample_3413 =========================

protected void encode(ChannelHandlerContext ctx, Object msg, ByteBuf buf) throws Exception {	ByteArrayOutputStream bytes = new ByteArrayOutputStream();	Output kryoOut = new Output(bytes);	kryos.get().writeClassAndObject(kryoOut, msg);	kryoOut.flush();	byte[] msgData = maybeEncrypt(bytes.toByteArray());	
encoded message of type bytes 

========================= hive sample_549 =========================

opRules.put(new RuleRegExp("R7", UDTFOperator.getOperatorName() + "%"), OpProcFactory.getUDTFProc());	opRules.put(new RuleRegExp("R8", LateralViewForwardOperator.getOperatorName() + "%"), OpProcFactory.getLVFProc());	opRules.put(new RuleRegExp("R9", LateralViewJoinOperator.getOperatorName() + "%"), OpProcFactory.getLVJProc());	opRules.put(new RuleRegExp("R10", ReduceSinkOperator.getOperatorName() + "%"), OpProcFactory.getRSProc());	Dispatcher disp = new DefaultRuleDispatcher(OpProcFactory.getDefaultProc(), opRules, opWalkerInfo);	GraphWalker ogw = new DefaultGraphWalker(disp);	ArrayList<Node> topNodes = new ArrayList<Node>();	topNodes.addAll(pGraphContext.getTopOps().values());	ogw.startWalking(topNodes, null);	if (LOG.isDebugEnabled()) {	
after ppd 

========================= hive sample_3385 =========================

private void evaluateWork(BaseWork w) throws SemanticException {	if (w instanceof MapWork) {	evaluateMapWork((MapWork) w);	} else if (w instanceof ReduceWork) {	evaluateReduceWork((ReduceWork) w);	} else if (w instanceof MergeJoinWork) {	evaluateMergeWork((MergeJoinWork) w);	} else {	
we are not going to evaluate this work type 

};	SortedSet<MapJoinOperator> sortedMapJoins = new TreeSet<MapJoinOperator>(comp);	sortedMapJoins.addAll(mapJoins);	long remainingSize = totalAvailableMemory / 2;	Iterator<MapJoinOperator> it = sortedMapJoins.iterator();	long totalLargeJoins = 0;	while (it.hasNext()) {	MapJoinOperator mj = it.next();	long size = sizes.get(mj);	if (LOG.isDebugEnabled()) {	
mapjoin size remaining 

long remainingSize = totalAvailableMemory / 2;	Iterator<MapJoinOperator> it = sortedMapJoins.iterator();	long totalLargeJoins = 0;	while (it.hasNext()) {	MapJoinOperator mj = it.next();	long size = sizes.get(mj);	if (LOG.isDebugEnabled()) {	}	if (size < remainingSize) {	if (LOG.isInfoEnabled()) {	
setting bytes needed for in mem 

totalLargeJoins = total;	if (totalLargeJoins > totalAvailableMemory) {	throw new HiveException();	}	remainingSize = totalAvailableMemory / 2;	}	double weight = (remainingSize + totalAvailableMemory / 2) / (double) totalLargeJoins;	for (MapJoinOperator mj : sortedMapJoins) {	long size = (long)(weight * sizes.get(mj));	if (LOG.isInfoEnabled()) {	
setting bytes needed for spills 

double weight = (remainingSize + totalAvailableMemory / 2) / (double) totalLargeJoins;	for (MapJoinOperator mj : sortedMapJoins) {	long size = (long)(weight * sizes.get(mj));	if (LOG.isInfoEnabled()) {	}	mj.getConf().setMemoryNeeded(size);	}	} catch (HiveException e) {	long size = totalAvailableMemory / mapJoins.size();	if (LOG.isInfoEnabled()) {	
scaling mapjoin memory w o stats 

if (LOG.isInfoEnabled()) {	}	mj.getConf().setMemoryNeeded(size);	}	} catch (HiveException e) {	long size = totalAvailableMemory / mapJoins.size();	if (LOG.isInfoEnabled()) {	}	for (MapJoinOperator mj : mapJoins) {	if (LOG.isInfoEnabled()) {	
setting bytes needed for fallback 

========================= hive sample_2982 =========================

public void run(HookContext hookContext) {	try {	if (hookContext.getHookType().equals(HookType.POST_EXEC_HOOK)) {	ipAddress = hookContext.getIpAddress();	userName = hookContext.getUserName();	operation = hookContext.getOperationName();	}	} catch (Throwable t) {	
error in postexechook 

public void run(HookContext hookContext) {	try {	if (hookContext.getHookType().equals(HookType.PRE_EXEC_HOOK)) {	ipAddress = hookContext.getIpAddress();	userName = hookContext.getUserName();	operation = hookContext.getOperationName();	}	} catch (Throwable t) {	
error in preexechook 

public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context, ASTNode ast) throws SemanticException {	try {	userName = context.getUserName();	ipAddress = context.getIpAddress();	command = context.getCommand();	commandType = context.getHiveOperation();	} catch (Throwable t) {	
error in semantic analysis hook preanalyze 

public void postAnalyze(HiveSemanticAnalyzerHookContext context, List<Task<? extends Serializable>> rootTasks) throws SemanticException {	try {	userName = context.getUserName();	ipAddress = context.getIpAddress();	command = context.getCommand();	commandType = context.getHiveOperation();	} catch (Throwable t) {	
error in semantic analysis hook postanalyze 

========================= hive sample_230 =========================

}	String actualDbName = context.isDbNameEmpty() ? pks.get(0).getTable_db() : context.dbName;	String actualTblName = context.isTableNameEmpty() ? pks.get(0).getTable_name() : context.tableName;	for (SQLPrimaryKey pk : pks) {	pk.setTable_db(actualDbName);	pk.setTable_name(actualTblName);	}	AlterTableDesc addConstraintsDesc = new AlterTableDesc(actualDbName + "." + actualTblName, pks, new ArrayList<SQLForeignKey>(), new ArrayList<SQLUniqueConstraint>(), context.eventOnlyReplicationSpec());	Task<DDLWork> addConstraintsTask = TaskFactory.get(new DDLWork(readEntitySet, writeEntitySet, addConstraintsDesc), context.hiveConf);	tasks.add(addConstraintsTask);	
added add constrains task 

========================= hive sample_3487 =========================

public void handle(Context withinContext) throws Exception {	
processing alter table message 

========================= hive sample_3468 =========================

public static Filter getXSRFFilter() {	String filterClass = "org.apache.hadoop.security.http.RestCsrfPreventionFilter";	try {	Class<? extends Filter> klass = (Class<? extends Filter>) Class.forName(filterClass);	Filter f = klass.newInstance();	
filter found using as is 

public static Filter getXSRFFilter() {	String filterClass = "org.apache.hadoop.security.http.RestCsrfPreventionFilter";	try {	Class<? extends Filter> klass = (Class<? extends Filter>) Class.forName(filterClass);	Filter f = klass.newInstance();	return f;	} catch (Exception e) {	
unable to use got exception using internal shims impl of filter 

========================= hive sample_1455 =========================

public List<HiveInputSplit> filter(HiveInputSplit[] splits) throws IOException {	long sumSplitLengths = 0;	List<HiveInputSplit> newSplits = new ArrayList<>();	Arrays.sort(splits, new HiveInputSplitComparator());	for (HiveInputSplit split : splits) {	
split start 

public List<HiveInputSplit> filter(HiveInputSplit[] splits) throws IOException {	long sumSplitLengths = 0;	List<HiveInputSplit> newSplits = new ArrayList<>();	Arrays.sort(splits, new HiveInputSplitComparator());	for (HiveInputSplit split : splits) {	
split end 

if (sumSplitLengths > maxInputSize) {	String messageTemplate = "Size of data to read during a compact-index-based query " + "exceeded the maximum of %d set in %s";	throw new IOException(String.format(messageTemplate, maxInputSize, HiveConf.ConfVars.HIVE_INDEX_COMPACT_QUERY_MAX_SIZE.varname));	}	newSplits.add(newSplit);	}	} catch (HiveException e) {	throw new RuntimeException("Unable to get metadata for input table split " + split.getPath(), e);	}	}	
number of input splits new input splits sum of split lengths 

========================= hive sample_4657 =========================

public void testNonAcidToAcidConversion01() throws Exception {	runStatementOnDriver("insert into " + Table.NONACIDORCTBL + "(a,b) values(1,2)");	runStatementOnDriver("insert into " + Table.NONACIDORCTBL + "(a,b) values(0,12),(1,5)");	runStatementOnDriver("alter table " + Table.NONACIDORCTBL + " SET TBLPROPERTIES ('transactional'='true')");	runStatementOnDriver("insert into " + Table.NONACIDORCTBL + "(a,b) values(1,17)");	List<String> rs = runStatementOnDriver("select ROW__ID, a, b, INPUT__FILE__NAME from " +  Table.NONACIDORCTBL + " order by ROW__ID");	
before compact 

Assert.assertTrue(rs.get(0), rs.get(0).endsWith("nonacidorctbl/000000_0_copy_1"));	Assert.assertTrue(rs.get(1), rs.get(1).startsWith("{\"transactionid\":0,\"bucketid\":536936448,\"rowid\":0}\t1\t2"));	Assert.assertTrue(rs.get(1), rs.get(1).endsWith("nonacidorctbl/000001_0"));	Assert.assertTrue(rs.get(2), rs.get(2).startsWith("{\"transactionid\":0,\"bucketid\":536936448,\"rowid\":1}\t1\t5"));	Assert.assertTrue(rs.get(2), rs.get(2).endsWith("nonacidorctbl/000001_0_copy_1"));	Assert.assertTrue(rs.get(3), rs.get(3).startsWith("{\"transactionid\":16,\"bucketid\":536936448,\"rowid\":0}\t1\t17"));	Assert.assertTrue(rs.get(3), rs.get(3).endsWith("nonacidorctbl/delta_0000016_0000016_0000/bucket_00001"));	runStatementOnDriver("alter table "+ TestTxnCommands2.Table.NONACIDORCTBL +" compact 'major'");	TestTxnCommands2.runWorker(hiveConf);	rs = runStatementOnDriver("select ROW__ID, a, b, INPUT__FILE__NAME from " + Table.NONACIDORCTBL + " order by ROW__ID");	
after compact 

========================= hive sample_2783 =========================

} else {	currSerDe = needConversion(currDesc) ? currDesc.getDeserializer(job) : tableSerDe;	ObjectInspector inputOI = currSerDe.getObjectInspector();	ObjectConverter = ObjectInspectorConverters.getConverter(inputOI, convertedOI);	}	if (isPartitioned) {	row[1] = createPartValue(currDesc, partKeyOI);	}	iterSplits = Arrays.asList(splits).iterator();	if (LOG.isDebugEnabled()) {	
creating fetchtask with deserializer typeinfo 

} else {	currSerDe = needConversion(currDesc) ? currDesc.getDeserializer(job) : tableSerDe;	ObjectInspector inputOI = currSerDe.getObjectInspector();	ObjectConverter = ObjectInspectorConverters.getConverter(inputOI, convertedOI);	}	if (isPartitioned) {	row[1] = createPartValue(currDesc, partKeyOI);	}	iterSplits = Arrays.asList(splits).iterator();	if (LOG.isDebugEnabled()) {	
deserializer properties table properties partition properties 

protected FetchInputFormatSplit[] getNextSplits() throws Exception {	while (getNextPath()) {	job.set("mapred.input.dir", StringUtils.escapeString(currPath.toString()));	HiveConf.setBoolVar(job, HiveConf.ConfVars.HIVE_VECTORIZATION_ENABLED, false);	Class<? extends InputFormat> formatter = currDesc.getInputFileFormatClass();	Utilities.copyTableJobPropertiesToConf(currDesc.getTableDesc(), job);	InputFormat inputFormat = getInputFormatFromCache(formatter, job);	String inputs = processCurrPathForMmWriteIds(inputFormat);	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
setting fetch inputs to 

if (inputFormat instanceof HiveInputFormat) {	return StringUtils.escapeString(currPath.toString());	}	ValidTxnList validTxnList;	if (AcidUtils.isInsertOnlyTable(currDesc.getTableDesc().getProperties())) {	validTxnList = extractValidTxnList();	} else {	validTxnList = null;	}	if (validTxnList != null) {	
processing for mm paths 

========================= hive sample_4084 =========================

assert (parameters.length == 1);	try {	if (isEligibleValue((SumHiveDecimalWritableAgg) agg, parameters[0])) {	((SumHiveDecimalWritableAgg)agg).empty = false;	((SumHiveDecimalWritableAgg)agg).sum.mutateAdd( PrimitiveObjectInspectorUtils.getHiveDecimal(parameters[0], inputOI));	}	} catch (NumberFormatException e) {	if (!warned) {	warned = true;	LOG.warn(getClass().getSimpleName() + " " + StringUtils.stringifyException(e));	
ignoring similar exceptions 

public Object terminate(AggregationBuffer agg) throws HiveException {	SumHiveDecimalWritableAgg myagg = (SumHiveDecimalWritableAgg) agg;	if (myagg.empty || myagg.sum == null || !myagg.sum.isSet()) {	return null;	}	DecimalTypeInfo decimalTypeInfo = (DecimalTypeInfo)outputOI.getTypeInfo();	myagg.sum.mutateEnforcePrecisionScale(decimalTypeInfo.getPrecision(), decimalTypeInfo.getScale());	if (!myagg.sum.isSet()) {	
the sum of a column with data type hivedecimal is out of range 

assert (parameters.length == 1);	try {	if (isEligibleValue((SumDoubleAgg) agg, parameters[0])) {	((SumDoubleAgg)agg).empty = false;	((SumDoubleAgg)agg).sum += PrimitiveObjectInspectorUtils.getDouble(parameters[0], inputOI);	}	} catch (NumberFormatException e) {	if (!warned) {	warned = true;	LOG.warn(getClass().getSimpleName() + " " + StringUtils.stringifyException(e));	
ignoring similar exceptions 

========================= hive sample_4968 =========================

}	}	if (rj != null) {	if (work.getAliasToWork() != null) {	for (Operator<? extends OperatorDesc> op : work.getAliasToWork() .values()) {	op.jobClose(job, success);	}	}	}	} catch (Exception e) {	
job close failed 

========================= hive sample_3604 =========================

HiveMaterializedViewsRegistry.get().dropMaterializedView(names[0], names[1]);	}	}	}	} catch (HiveException e) {	if (HiveConf.getBoolVar(hiveConf, ConfVars.HIVE_MATERIALIZED_VIEW_ENABLE_AUTO_REWRITING)) {	String message = "Error updating materialized view cache; consider disabling: " + ConfVars.HIVE_MATERIALIZED_VIEW_ENABLE_AUTO_REWRITING.varname;	LOG.error(message, e);	throw new RuntimeException(message, e);	} else {	
exception during materialized view cache update 

========================= hive sample_3829 =========================

public void load(MapJoinTableContainer[] mapJoinTables, MapJoinTableContainerSerDe[] mapJoinTableSerdes) throws HiveException {	String currentInputPath = context.getCurrentInputPath() == null ? null : context.getCurrentInputPath().toString();	
load from hashtable for input file 

boolean useOptimizedContainer = !useFastContainer && HiveConf.getBoolVar( hconf, HiveConf.ConfVars.HIVEMAPJOINUSEOPTIMIZEDTABLE);	for (int pos = 0; pos < mapJoinTables.length; pos++) {	if (pos == desc.getPosBigTable() || mapJoinTables[pos] != null) {	continue;	}	if (useOptimizedContainer) {	MapJoinObjectSerDeContext keyCtx = mapJoinTableSerdes[pos].getKeyContext();	ObjectInspector keyOI = keyCtx.getSerDe().getObjectInspector();	if (!MapJoinBytesTableContainer.isSupportedKey(keyOI)) {	if (firstContainer) {	
not using optimized table container only a subset of mapjoin keys is supported 

private MapJoinTableContainer load(FileSystem fs, Path path, MapJoinTableContainerSerDe mapJoinTableSerde) throws HiveException {	
tload back all hashtable files from tmp folder uri 

========================= hive sample_4600 =========================

public static void updateJobCredentialProviders(Configuration jobConf) {	if(jobConf == null) {	return;	}	String jobKeyStoreLocation = jobConf.get(HiveConf.ConfVars.HIVE_SERVER2_JOB_CREDENTIAL_PROVIDER_PATH.varname);	String oldKeyStoreLocation = jobConf.get(Constants.HADOOP_CREDENTIAL_PROVIDER_PATH_CONFIG);	if (StringUtils.isNotBlank(jobKeyStoreLocation)) {	jobConf.set(Constants.HADOOP_CREDENTIAL_PROVIDER_PATH_CONFIG, jobKeyStoreLocation);	
setting job conf credstore location to previous location was 

========================= hive sample_1402 =========================

try {	bean = jobRequest.execute(jobExecuteCallable);	} catch (TimeoutException ex) {	throw new QueueException(ex.getMessage());	} catch (InterruptedException ex) {	throw new QueueException(ex.getMessage());	} catch (ExecutionException ex) {	throw new QueueException(ex.getMessage());	}	} else {	
no thread pool configured for submit job request executing the job request in current thread 

private JobCallable<EnqueueBean> getJobSubmitTask(final String user, final Map<String, Object> userArgs, final String callback, final List<String> args, final TempletonControllerJob controllerJob) {	return new JobCallable<EnqueueBean>() {	public EnqueueBean execute() throws NotAuthorizedException, BusyException, IOException, QueueException {	Thread.currentThread().setName(String.format("%s-%s-%s", JOB_SUBMIT_EXECUTE_THREAD_PREFIX, submitThreadId, Thread.currentThread().getId()));	return enqueueJob(user, userArgs, callback, args, controllerJob);	}	public void cleanup() {	
job kill not done by main thread trying to kill now 

public EnqueueBean enqueueJob(String user, Map<String, Object> userArgs, String callback, List<String> args, TempletonControllerJob controllerJob) throws NotAuthorizedException, BusyException, IOException, QueueException {	UserGroupInformation ugi = null;	try {	ugi = UgiFactory.getUgi(user);	final long startTime = System.nanoTime();	String id = queueAsUser(ugi, args, controllerJob);	long elapsed = ((System.nanoTime() - startTime) / ((int) 1e6));	
queued job in ms 

private String queueAsUser(UserGroupInformation ugi, final List<String> args, final TempletonControllerJob controllerJob) throws IOException, InterruptedException {	if(LOG.isDebugEnabled()) {	
launching job 

private boolean killTempletonJobWithRetry(String user, String jobId) {	if (StringUtils.startsWith(jobId, "job_")) {	
started killing the job 

private boolean killTempletonJobWithRetry(String user, String jobId) {	if (StringUtils.startsWith(jobId, "job_")) {	boolean success = false;	int count = 0;	do {	try {	count++;	killJob(user, jobId);	success = true;	
kill job attempt succeeded 

private boolean killTempletonJobWithRetry(String user, String jobId) {	if (StringUtils.startsWith(jobId, "job_")) {	boolean success = false;	int count = 0;	do {	try {	count++;	killJob(user, jobId);	success = true;	} catch (Exception e) {	
failed to kill the job due to exception 

private boolean killTempletonJobWithRetry(String user, String jobId) {	if (StringUtils.startsWith(jobId, "job_")) {	boolean success = false;	int count = 0;	do {	try {	count++;	killJob(user, jobId);	success = true;	} catch (Exception e) {	
waiting for s before retrying the operation iteration 

int count = 0;	do {	try {	count++;	killJob(user, jobId);	success = true;	} catch (Exception e) {	try {	Thread.sleep(jobTimeoutTaskRetryIntervalInSec * 1000);	} catch (InterruptedException ex) {	
got interrupted while waiting for next retry 

success = true;	} catch (Exception e) {	try {	Thread.sleep(jobTimeoutTaskRetryIntervalInSec * 1000);	} catch (InterruptedException ex) {	}	}	} while (!success && count < jobTimeoutTaskRetryCount);	return success;	} else {	
couldn t find a valid job id after job request is timed out 

========================= hive sample_836 =========================

public void initialize(Configuration configuration, Properties properties) throws SerDeException {	if (schema != null) {	
resetting already initialized avroserde 

final String columnNameDelimiter = properties.containsKey(serdeConstants.COLUMN_NAME_DELIMITER) ? properties .getProperty(serdeConstants.COLUMN_NAME_DELIMITER) : String.valueOf(SerDeUtils.COMMA);	if (hasExternalSchema(properties) || columnNameProperty == null || columnNameProperty.isEmpty() || columnTypeProperty == null || columnTypeProperty.isEmpty()) {	schema = determineSchemaOrReturnErrorSchema(configuration, properties);	} else {	columnNames = StringInternUtils.internStringsInList( Arrays.asList(columnNameProperty.split(columnNameDelimiter)));	columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);	schema = getSchemaFromCols(properties, columnNames, columnTypes, columnCommentProperty);	}	properties.setProperty(AvroSerdeUtils.AvroTableProperties.SCHEMA_LITERAL.getPropName(), schema.toString());	if (LOG.isDebugEnabled()) {	
avro schema is 

schema = determineSchemaOrReturnErrorSchema(configuration, properties);	} else {	columnNames = StringInternUtils.internStringsInList( Arrays.asList(columnNameProperty.split(columnNameDelimiter)));	columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);	schema = getSchemaFromCols(properties, columnNames, columnTypes, columnCommentProperty);	}	properties.setProperty(AvroSerdeUtils.AvroTableProperties.SCHEMA_LITERAL.getPropName(), schema.toString());	if (LOG.isDebugEnabled()) {	}	if (configuration == null) {	
configuration null not inserting schema 

public static Schema getSchemaFromCols(Properties properties, List<String> columnNames, List<TypeInfo> columnTypes, String columnCommentProperty) {	List<String> columnComments;	if (columnCommentProperty == null || columnCommentProperty.isEmpty()) {	columnComments = new ArrayList<String>();	} else {	columnComments = Arrays.asList(columnCommentProperty.split("\0"));	if (LOG.isDebugEnabled()) {	
columncomments is 

public Schema determineSchemaOrReturnErrorSchema(Configuration conf, Properties props) {	try {	configErrors = "";	return AvroSerdeUtils.determineSchemaOrThrowException(conf, props);	} catch(AvroSerdeException he) {	
encountered avroserdeexception determining schema returning signal schema to indicate problem 

public Schema determineSchemaOrReturnErrorSchema(Configuration conf, Properties props) {	try {	configErrors = "";	return AvroSerdeUtils.determineSchemaOrThrowException(conf, props);	} catch(AvroSerdeException he) {	configErrors = new String("Encountered AvroSerdeException determining schema. Returning " + "signal schema to indicate problem: " + he.getMessage());	return schema = SchemaResolutionProblem.SIGNAL_BAD_SCHEMA;	} catch (Exception e) {	
encountered exception determining schema returning signal schema to indicate problem 

========================= hive sample_5292 =========================

private JobConf createBaseJobConf(HiveConf conf, String jobName, Table t, StorageDescriptor sd, ValidTxnList txns, CompactionInfo ci) {	JobConf job = new JobConf(conf);	job.setJobName(jobName);	job.setOutputKeyClass(NullWritable.class);	job.setOutputValueClass(NullWritable.class);	job.setJarByClass(CompactorMR.class);	
user jar set to 

private void launchCompactionJob(JobConf job, Path baseDir, CompactionType compactionType, StringableList dirsToSearch, List<AcidUtils.ParsedDelta> parsedDeltas, int curDirNumber, int obsoleteDirNumber, HiveConf hiveConf, TxnStore txnHandler, long id, String jobName) throws IOException {	job.setBoolean(IS_MAJOR, compactionType == CompactionType.MAJOR);	if(dirsToSearch == null) {	dirsToSearch = new StringableList();	}	StringableList deltaDirs = new StringableList();	long minTxn = Long.MAX_VALUE;	long maxTxn = Long.MIN_VALUE;	for (AcidUtils.ParsedDelta delta : parsedDeltas) {	
adding delta to directories to search 

private void removeFiles(HiveConf conf, String location, ValidTxnList txnList, Table t) throws IOException {	AcidUtils.Directory dir = AcidUtils.getAcidState(new Path(location), conf, txnList, Ref.from(false), false, t.getParameters());	List<FileStatus> abortedDirs = dir.getAbortedDirectories();	List<Path> filesToDelete = new ArrayList<>(abortedDirs.size());	for (FileStatus stat : abortedDirs) {	filesToDelete.add(stat.getPath());	}	if (filesToDelete.size() < 1) {	
hmm nothing to delete in the worker for directory that hardly seems right 

private void removeFiles(HiveConf conf, String location, ValidTxnList txnList, Table t) throws IOException {	AcidUtils.Directory dir = AcidUtils.getAcidState(new Path(location), conf, txnList, Ref.from(false), false, t.getParameters());	List<FileStatus> abortedDirs = dir.getAbortedDirectories();	List<Path> filesToDelete = new ArrayList<>(abortedDirs.size());	for (FileStatus stat : abortedDirs) {	filesToDelete.add(stat.getPath());	}	if (filesToDelete.size() < 1) {	return;	}	
about to remove aborted directories from 

List<FileStatus> abortedDirs = dir.getAbortedDirectories();	List<Path> filesToDelete = new ArrayList<>(abortedDirs.size());	for (FileStatus stat : abortedDirs) {	filesToDelete.add(stat.getPath());	}	if (filesToDelete.size() < 1) {	return;	}	FileSystem fs = filesToDelete.get(0).getFileSystem(conf);	for (Path dead : filesToDelete) {	
going to delete path 

public void readFields(DataInput dataInput) throws IOException {	int len;	byte[] buf;	locations = new ArrayList<String>();	length = dataInput.readLong();	
read length of 

public void readFields(DataInput dataInput) throws IOException {	int len;	byte[] buf;	locations = new ArrayList<String>();	length = dataInput.readLong();	int numElements = dataInput.readInt();	
read numelements of 

public void readFields(DataInput dataInput) throws IOException {	int len;	byte[] buf;	locations = new ArrayList<String>();	length = dataInput.readLong();	int numElements = dataInput.readInt();	for (int i = 0; i < numElements; i++) {	len = dataInput.readInt();	
read file length of 

locations = new ArrayList<String>();	length = dataInput.readLong();	int numElements = dataInput.readInt();	for (int i = 0; i < numElements; i++) {	len = dataInput.readInt();	buf = new byte[len];	dataInput.readFully(buf);	locations.add(new String(buf));	}	bucketNum = dataInput.readInt();	
read bucket number of 

length = dataInput.readLong();	int numElements = dataInput.readInt();	for (int i = 0; i < numElements; i++) {	len = dataInput.readInt();	buf = new byte[len];	dataInput.readFully(buf);	locations.add(new String(buf));	}	bucketNum = dataInput.readInt();	len = dataInput.readInt();	
read base path length of 

} else {	Matcher matcher = AcidUtils.LEGACY_BUCKET_DIGIT_PATTERN.matcher(dir.getName());	addFileToMap(matcher, dir, true, splitToBucketMap);	}	}	List<InputSplit> splits = new ArrayList<InputSplit>(splitToBucketMap.size());	for (Map.Entry<Integer, BucketTracker> e : splitToBucketMap.entrySet()) {	BucketTracker bt = e.getValue();	splits.add(new CompactorInputSplit(entries, e.getKey(), bt.buckets, bt.sawBase ? baseDir : null, deltaDirs));	}	
returning splits 

String msg = "Found a non-bucket file that we thought matched the bucket pattern! " + file.toString() + " Matcher=" + matcher.toString();	LOG.error(msg);	throw new IllegalArgumentException(msg);	}	int bucketNum = Integer.parseInt(matcher.group());	BucketTracker bt = splitToBucketMap.get(bucketNum);	if (bt == null) {	bt = new BucketTracker();	splitToBucketMap.put(bucketNum, bt);	}	
adding to list of files for splits 

Class c = JavaUtils.loadClass(classname);	Object o = c.newInstance();	if (classType.isAssignableFrom(o.getClass())) {	t = (T)o;	} else {	String s = classname + " is not an instance of " + classType.getName();	LOG.error(s);	throw new IOException(s);	}	} catch (ClassNotFoundException e) {	
unable to instantiate class 

if (classType.isAssignableFrom(o.getClass())) {	t = (T)o;	} else {	String s = classname + " is not an instance of " + classType.getName();	LOG.error(s);	throw new IOException(s);	}	} catch (ClassNotFoundException e) {	throw new IOException(e);	} catch (InstantiationException e) {	
unable to instantiate class 

} else {	String s = classname + " is not an instance of " + classType.getName();	LOG.error(s);	throw new IOException(s);	}	} catch (ClassNotFoundException e) {	throw new IOException(e);	} catch (InstantiationException e) {	throw new IOException(e);	} catch (IllegalAccessException e) {	
unable to instantiate class 

public void commitJob(JobContext context) throws IOException {	JobConf conf = ShimLoader.getHadoopShims().getJobConf(context);	Path tmpLocation = new Path(conf.get(TMP_LOCATION));	Path finalLocation = new Path(conf.get(FINAL_LOCATION));	FileSystem fs = tmpLocation.getFileSystem(conf);	
moving contents of to 

public void commitJob(JobContext context) throws IOException {	JobConf conf = ShimLoader.getHadoopShims().getJobConf(context);	Path tmpLocation = new Path(conf.get(TMP_LOCATION));	Path finalLocation = new Path(conf.get(FINAL_LOCATION));	FileSystem fs = tmpLocation.getFileSystem(conf);	if(!fs.exists(tmpLocation)) {	AcidOutputFormat.Options options = new AcidOutputFormat.Options(conf) .writingBase(conf.getBoolean(IS_MAJOR, false)) .isCompressed(conf.getBoolean(IS_COMPRESSED, false)) .minimumTransactionId(conf.getLong(MIN_TXN, Long.MAX_VALUE)) .maximumTransactionId(conf.getLong(MAX_TXN, Long.MIN_VALUE)) .bucket(0) .statementId(-1);	Path newDeltaDir = AcidUtils.createFilename(finalLocation, options).getParent();	
not found assuming splits creating 

public void abortJob(JobContext context, int status) throws IOException {	JobConf conf = ShimLoader.getHadoopShims().getJobConf(context);	Path tmpLocation = new Path(conf.get(TMP_LOCATION));	FileSystem fs = tmpLocation.getFileSystem(conf);	
removing 

========================= hive sample_2804 =========================

static void setSearchArgument(Reader.Options options, List<OrcProto.Type> types, Configuration conf, boolean isOriginal) {	String neededColumnNames = getNeededColumnNamesString(conf);	if (neededColumnNames == null) {	
no orc pushdown predicate no column names 

static void setSearchArgument(Reader.Options options, List<OrcProto.Type> types, Configuration conf, boolean isOriginal) {	String neededColumnNames = getNeededColumnNamesString(conf);	if (neededColumnNames == null) {	options.searchArgument(null, null);	return;	}	SearchArgument sarg = ConvertAstToSearchArg.createFromConf(conf);	if (sarg == null) {	
no orc pushdown predicate 

if (neededColumnNames == null) {	options.searchArgument(null, null);	return;	}	SearchArgument sarg = ConvertAstToSearchArg.createFromConf(conf);	if (sarg == null) {	options.searchArgument(null, null);	return;	}	if (LOG.isInfoEnabled()) {	
orc pushdown predicate 

static boolean canCreateSargFromConf(Configuration conf) {	if (getNeededColumnNamesString(conf) == null) {	
no orc pushdown predicate no column names 

static boolean canCreateSargFromConf(Configuration conf) {	if (getNeededColumnNamesString(conf) == null) {	return false;	}	if (!ConvertAstToSearchArg.canCreateFromConf(conf)) {	
no orc pushdown predicate 

try {	List<HdfsFileStatusWithId> insertDeltaFiles = SHIMS.listLocatedHdfsStatus(fs, parsedDelta.getPath(), bucketFilter);	for (HdfsFileStatusWithId fileId : insertDeltaFiles) {	baseFiles.add(new AcidBaseFileInfo(fileId, deltaType));	}	if (val == null) {	useFileIds.value = true;	}	continue;	} catch (Throwable t) {	
failed to get files with id using regular api 

private List<HdfsFileStatusWithId> findBaseFiles( Path base, Ref<Boolean> useFileIds) throws IOException {	Boolean val = useFileIds.value;	if (val == null || val) {	try {	List<HdfsFileStatusWithId> result = SHIMS.listLocatedHdfsStatus( fs, base, AcidUtils.hiddenFileFilter);	if (val == null) {	useFileIds.value = true;	}	return result;	} catch (Throwable t) {	
failed to get files with id using regular api 

assert ppdResult.hasArray();	CodedInputStream cis = CodedInputStream.newInstance( ppdResult.array(), ppdResult.arrayOffset(), ppdResult.remaining());	cis.setSizeLimit(InStream.PROTOBUF_MESSAGE_MAX_LIMIT);	return generateSplitsFromPpd(SplitInfos.parseFrom(cis));	} else {	populateAndCacheStripeDetails();	boolean[] includeStripe = null;	if ((deltas == null || deltas.isEmpty()) && context.sarg != null) {	String[] colNames = extractNeededColNames((readerTypes == null ? fileTypes : readerTypes), context.conf, readerIncluded, isOriginal);	if (colNames == null) {	
skipping split elimination for as column names is null 

splits.add(createSplit(current.offset, current.length, orcTail));	current.offset = -1;	}	lastIdx = index;	String debugStr = null;	if (LOG.isDebugEnabled()) {	debugStr = current.toString();	}	current = generateOrUpdateSplit(splits, current, si.getOffset(), si.getLength(), null);	if (LOG.isDebugEnabled()) {	
updated split from and to 

static List<OrcSplit> generateSplitsInfo(Configuration conf, Context context) throws IOException {	if (LOG.isInfoEnabled()) {	
orc pushdown predicate 

combinedCtx.combined = null;	continue;	}	--resultsLeft;	if(adi.isEmpty()) {	continue;	}	List<SplitStrategy<?>> splitStrategies = determineSplitStrategies(combinedCtx, context, adi.fs, adi.splitPath, adi.baseFiles, adi.deleteEvents, readerTypes, ugi, allowSyntheticFileIds);	for (SplitStrategy<?> splitStrategy : splitStrategies) {	if (LOG.isDebugEnabled()) {	
split strategy 

for (Future<List<OrcSplit>> splitFuture : splitFutures) {	splits.addAll(splitFuture.get());	}	} catch (Exception e) {	cancelFutures(pathFutures);	cancelFutures(strategyFutures);	cancelFutures(splitFutures);	throw new RuntimeException("ORC split generation failed with exception: " + e.getMessage(), e);	}	if (context.cacheStripeDetails) {	
footercachehitratio 

} catch (Exception e) {	cancelFutures(pathFutures);	cancelFutures(strategyFutures);	cancelFutures(splitFutures);	throw new RuntimeException("ORC split generation failed with exception: " + e.getMessage(), e);	}	if (context.cacheStripeDetails) {	}	if (LOG.isDebugEnabled()) {	for (OrcSplit split : splits) {	
projected columns uncompressed size 

public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {	if (LOG.isDebugEnabled()) {	
getsplits started 

public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {	if (LOG.isDebugEnabled()) {	}	Configuration conf = job;	if (HiveConf.getBoolVar(job, HiveConf.ConfVars.HIVE_ORC_MS_FOOTER_CACHE_ENABLED)) {	conf = new HiveConf(conf, OrcInputFormat.class);	}	List<OrcSplit> result = generateSplitsInfo(conf, new Context(conf, numSplits, createExternalCaches()));	if (LOG.isDebugEnabled()) {	
getsplits finished 

final Path[] deltas = VectorizedOrcAcidRowBatchReader.getDeleteDeltaDirsFromSplit(split);	final Configuration conf = options.getConfiguration();	final Reader reader = OrcInputFormat.createOrcReaderForSplit(conf, split);	OrcRawRecordMerger.Options mergerOptions = new OrcRawRecordMerger.Options().isCompacting(false);	mergerOptions.rootPath(split.getRootDir());	mergerOptions.bucketPath(split.getPath());	final int bucket;	if (split.hasBase()) {	AcidOutputFormat.Options acidIOOptions = AcidUtils.parseBaseOrDeltaBucketFilename(split.getPath(), conf);	if(acidIOOptions.getBucketId() < 0) {	
can t determine bucket id for ignoring 

public static boolean[] pickStripesViaTranslatedSarg(SearchArgument sarg, OrcFile.WriterVersion writerVersion, List<OrcProto.Type> types, List<StripeStatistics> stripeStats, int stripeCount) {	
translated orc pushdown predicate 

private static boolean[] pickStripesInternal(SearchArgument sarg, int[] filterColumns, List<StripeStatistics> stripeStats, int stripeCount, Path filePath, final SchemaEvolution evolution) {	boolean[] includeStripe = new boolean[stripeCount];	for (int i = 0; i < includeStripe.length; ++i) {	includeStripe[i] = (i >= stripeStats.size()) || isStripeSatisfyPredicate(stripeStats.get(i), sarg, filterColumns, evolution);	if (LOG.isDebugEnabled() && !includeStripe[i]) {	
eliminating orc stripe of file as it did not satisfy predicate condition 

for (int pred = 0; pred < truthValues.length; pred++) {	if (filterColumns[pred] != -1) {	if (evolution != null && !evolution.isPPDSafeConversion(filterColumns[pred])) {	truthValues[pred] = TruthValue.YES_NO_NULL;	} else {	ColumnStatistics stats = stripeStatistics.getColumnStatistics()[filterColumns[pred]];	PredicateLeaf leaf = predLeaves.get(pred);	try {	truthValues[pred] = RecordReaderImpl.evaluatePredicate(stats, leaf, null);	} catch (NoDynamicValuesException dve) {	
dynamic values are not available here 

if (schemaEvolutionTypeDescrs.size() != Math.min(dataColumns, schemaEvolutionColumnNames.size())) {	haveSchemaEvolutionProperties = false;	}	}	} else if (isAcidRead) {	throw new IllegalArgumentException(ErrorMsg.SCHEMA_REQUIRED_TO_READ_ACID_TABLES.getErrorCodedMsg());	}	}	if (haveSchemaEvolutionProperties) {	if (LOG.isInfoEnabled()) {	
using schema evolution configuration variables schema evolution columns schema evolution columns types isacidread 

virtualColumnClipNum = columnNum;	break;	}	columnNum++;	}	if (virtualColumnClipNum != -1 && virtualColumnClipNum < dataColumns) {	schemaEvolutionColumnNames = Lists.newArrayList(schemaEvolutionColumnNames.subList(0, virtualColumnClipNum));	schemaEvolutionTypeDescrs = Lists.newArrayList(schemaEvolutionTypeDescrs.subList(0, virtualColumnClipNum));	}	if (LOG.isInfoEnabled()) {	
using column configuration variables columns columns types isacidread 

========================= hive sample_3653 =========================

private void incrementMetricsCounter(String name, long count) {	Metrics metrics = MetricsFactory.getInstance();	if (metrics != null) {	try {	metrics.incrementCounter(name, count);	} catch (Exception e) {	
error reporting jvmpausemonitor to metrics system 

========================= hive sample_1394 =========================

public static HiveMetaStoreClient getHiveClient(HiveConf hiveConf) throws MetaException, IOException {	
hcatutil gethiveclient is unsafe and can be a resource leak depending on hmsc implementation and caching mechanism use hcatutil gethivemetastoreclient instead 

public static void closeHiveClientQuietly(IMetaStoreClient client) {	try {	if (client != null) client.close();	} catch (Exception e) {	
error closing metastore client ignored the error 

public static HiveConf getHiveConf(Configuration conf) throws IOException {	HiveConf hiveConf = new HiveConf(conf, HCatUtil.class);	if (StringUtils.isBlank(conf.get(HCatConstants.HCAT_KEY_HIVE_CONF))) {	
not set generating configuration differences 

public static HiveConf getHiveConf(Configuration conf) throws IOException {	HiveConf hiveConf = new HiveConf(conf, HCatUtil.class);	if (StringUtils.isBlank(conf.get(HCatConstants.HCAT_KEY_HIVE_CONF))) {	Properties differences = getHiveSiteOverrides(conf);	conf.set(HCatConstants.HCAT_KEY_HIVE_CONF, HCatUtil.serialize(differences));	} else {	
is set applying configuration differences 

public static Map<String,String> getHCatKeyHiveConf(JobConf conf) {	try {	Properties properties = null;	if (! StringUtils.isBlank(conf.get(HCatConstants.HCAT_KEY_HIVE_CONF))) {	properties = (Properties) HCatUtil.deserialize( conf.get(HCatConstants.HCAT_KEY_HIVE_CONF));	LOG.info(HCatConstants.HCAT_KEY_HIVE_CONF + " is set. Using differences=" + properties);	} else {	
not set generating configuration differences 

========================= hive sample_709 =========================

LOG.warn("Transaction ID <= 0. The recipient is probably expecting a transaction ID.");	}	data.writeLong(table.getTransactionId());	data.writeByte(table.getTableType().getId());	Table metaTable = table.getTable();	if (metaTable != null) {	byte[] thrift = new TSerializer(new TCompactProtocol.Factory()).serialize(metaTable);	data.writeInt(thrift.length);	data.write(thrift);	} else {	
meta store table is null the recipient is probably expecting an instance 

========================= hive sample_965 =========================

}	List<BaseWork> mapJoinWork;	mapJoinWork = context.mapJoinWorkMap.get(mapJoinOp);	int workMapSize = context.childToWorkMap.get(parentRS).size();	Preconditions.checkArgument(workMapSize == 1, "AssertionError: expected context.childToWorkMap.get(parentRS).size() to be 1, but was " + workMapSize);	BaseWork parentWork = context.childToWorkMap.get(parentRS).get(0);	int pos = context.mapJoinParentMap.get(mapJoinOp).indexOf(parentRS);	if (pos == -1) {	throw new SemanticException("Cannot find position of parent in mapjoin");	}	
mapjoin pos 

BaseWork parentWork = context.childToWorkMap.get(parentRS).get(0);	int pos = context.mapJoinParentMap.get(mapJoinOp).indexOf(parentRS);	if (pos == -1) {	throw new SemanticException("Cannot find position of parent in mapjoin");	}	mapJoinOp.getConf().getParentToInput().put(pos, parentWork.getName());	SparkEdgeProperty edgeProp = new SparkEdgeProperty(SparkEdgeProperty.SHUFFLE_NONE);	if (mapJoinWork != null) {	for (BaseWork myWork: mapJoinWork) {	SparkWork sparkWork = context.currentTask.getWork();	
connecting with 

========================= hive sample_3096 =========================

job.setNumMapTasks(work.getNumMapTasks());	}	job.setNumReduceTasks(0);	if (work.getMinSplitSize() != null) {	HiveConf.setLongVar(job, HiveConf.ConfVars.MAPREDMINSPLITSIZE, work .getMinSplitSize().longValue());	}	if (work.getInputformat() != null) {	HiveConf.setVar(job, HiveConf.ConfVars.HIVEINPUTFORMAT, work .getInputformat());	}	String inpFormat = HiveConf.getVar(job, HiveConf.ConfVars.HIVEINPUTFORMAT);	
using 

if (ctxCreated) {	ctx.clear();	}	if (rj != null) {	if (returnVal != 0) {	rj.killJob();	}	}	ColumnTruncateMapper.jobClose(outputPath, success, job, console, work.getDynPartCtx(), null);	} catch (Exception e) {	
failed while cleaning up 

========================= hive sample_3583 =========================

public static void startupShutdownMessage(Class<?> clazz, String[] args, final org.slf4j.Logger LOG) {	final String hostname = getHostname();	final String classname = clazz.getSimpleName();	
startup msg 

public static void startupShutdownMessage(Class<?> clazz, String[] args, final org.slf4j.Logger LOG) {	final String hostname = getHostname();	final String classname = clazz.getSimpleName();	"Starting " + classname, "  host = " + hostname, "  args = " + Arrays.asList(args), "  version = " + HiveVersionInfo.getVersion(), "  classpath = " + System.getProperty("java.class.path"), "  build = " + HiveVersionInfo.getUrl() + " -r " + HiveVersionInfo.getRevision() + "; compiled by '" + HiveVersionInfo.getUser() + "' on " + HiveVersionInfo.getDate()}	) );	ShutdownHookManager.addShutdownHook( new Runnable() {	public void run() {	
shutdown msg 

========================= hive sample_1414 =========================

public List<Task<? extends Serializable>> getDependentTasks() {	count++;	System.err.println("YAH:getDepTasks got called!");	(new Exception()).printStackTrace(System.err);	
yah getdeptasks 

========================= hive sample_2668 =========================

if (!LazyUtils.isNumberMaybe(bytes.getData(), start, length)) {	isNull = true;	return;	}	try {	byteData = Text.decode(bytes.getData(), start, length);	data.set(Float.parseFloat(byteData));	isNull = false;	} catch (NumberFormatException e) {	isNull = true;	
data not in the float data type range so converted to null given data is 

return;	}	try {	byteData = Text.decode(bytes.getData(), start, length);	data.set(Float.parseFloat(byteData));	isNull = false;	} catch (NumberFormatException e) {	isNull = true;	} catch (CharacterCodingException e) {	isNull = true;	
data not in the float data type range so converted to null 

========================= hive sample_5457 =========================

Preconditions.checkArgument(numExecutors > 0);	Preconditions.checkArgument(srvPort == 0 || (srvPort > 1024 && srvPort < 65536), "Server RPC Port must be between 1025 and 65535, or 0 automatic selection");	Preconditions.checkArgument(mngPort == 0 || (mngPort > 1024 && mngPort < 65536), "Management RPC Port must be between 1025 and 65535, or 0 automatic selection");	Preconditions.checkArgument(localDirs != null && localDirs.length > 0, "Work dirs must be specified");	Preconditions.checkArgument(shufflePort == 0 || (shufflePort > 1024 && shufflePort < 65536), "Shuffle Port must be between 1024 and 65535, or 0 for automatic selection");	int outputFormatServicePort = HiveConf.getIntVar(daemonConf, HiveConf.ConfVars.LLAP_DAEMON_OUTPUT_SERVICE_PORT);	Preconditions.checkArgument(outputFormatServicePort == 0 || (outputFormatServicePort > 1024 && outputFormatServicePort < 65536), "OutputFormatService Port must be between 1024 and 65535, or 0 for automatic selection");	String hosts = HiveConf.getTrimmedVar(daemonConf, ConfVars.LLAP_DAEMON_SERVICE_HOSTS);	if (hosts.startsWith("@")) {	String zkHosts = HiveConf.getTrimmedVar(daemonConf, ConfVars.HIVE_ZOOKEEPER_QUORUM);	
zookeeper quorum 

Preconditions.checkArgument(zkHosts != null && !zkHosts.trim().isEmpty(), "LLAP service hosts startswith '@' but hive.zookeeper.quorum is not set." + " hive.zookeeper.quorum must be set.");	}	String hostName = MetricsUtils.getHostName();	try {	if (UserGroupInformation.isSecurityEnabled()) {	final String daemonPrincipal = HiveConf.getVar(daemonConf, ConfVars.LLAP_KERBEROS_PRINCIPAL);	final String daemonKeytab = HiveConf.getVar(daemonConf, ConfVars.LLAP_KERBEROS_KEYTAB_FILE);	LlapUtil.loginWithKerberosAndUpdateCurrentUser(daemonPrincipal, daemonKeytab);	}	String currentUser = UserGroupInformation.getCurrentUser().getShortUserName();	
starting daemon as user 

LlapDaemonJvmMetrics.create(displayNameJvm, sessionId, daemonConf);	String displayName = "LlapDaemonExecutorMetrics-" + hostName;	daemonConf.set("llap.daemon.metrics.sessionid", sessionId);	String[] strIntervals = HiveConf.getTrimmedStringsVar(daemonConf, HiveConf.ConfVars.LLAP_DAEMON_TASK_PREEMPTION_METRICS_INTERVALS);	List<Integer> intervalList = new ArrayList<>();	if (strIntervals != null) {	for (String strInterval : strIntervals) {	try {	intervalList.add(Integer.valueOf(strInterval));	} catch (NumberFormatException e) {	
ignoring task pre emption metrics interval from as it is invalid 

}	}	}	this.metrics = LlapDaemonExecutorMetrics.create(displayName, sessionId, numExecutors, Ints.toArray(intervalList));	this.metrics.setMemoryPerInstance(executorMemoryPerInstance);	this.metrics.setCacheMemoryPerInstance(ioMemoryBytes);	this.metrics.setJvmMaxMemory(maxJvmMemory);	this.metrics.setWaitQueueSize(waitQueueSize);	this.metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);	this.llapDaemonInfoBean = MBeans.register("LlapDaemon", "LlapDaemonInfo", this);	
started llapmetricssystem with displayname sessionid 

public void serviceStart() throws Exception {	ShuffleHandler.initializeAndStart(shuffleHandlerConf);	
setting shuffle port to 

public void serviceStop() throws Exception {	if (registry != null) {	this.registry.stop();	}	super.serviceStop();	ShuffleHandler.shutdown();	shutdown();	LlapOutputFormatService.get().stop();	
llapdaemon shutdown complete 

public void shutdown() {	
llapdaemon shutdown invoked 

public void shutdown() {	if (llapDaemonInfoBean != null) {	try {	MBeans.unregister(llapDaemonInfoBean);	} catch (Throwable ex) {	
error unregistering the bean ignoring 

LlapDaemon llapDaemon = null;	try {	LlapDaemonConfiguration daemonConf = new LlapDaemonConfiguration();	String containerIdStr = System.getenv(ApplicationConstants.Environment.CONTAINER_ID.name());	String appName = null;	if (containerIdStr != null && !containerIdStr.isEmpty()) {	daemonConf.set(ConfVars.LLAP_DAEMON_CONTAINER_ID.varname, containerIdStr);	appName = ConverterUtils.toContainerId(containerIdStr) .getApplicationAttemptId().getApplicationId().toString();	} else {	daemonConf.unset(ConfVars.LLAP_DAEMON_CONTAINER_ID.varname);	
cannot find llap tokens may grant access to subsequent instances of the cluster with the same name 

int shufflePort = daemonConf .getInt(ShuffleHandler.SHUFFLE_PORT_CONFIG_KEY, ShuffleHandler.DEFAULT_SHUFFLE_PORT);	int webPort = HiveConf.getIntVar(daemonConf, ConfVars.LLAP_DAEMON_WEB_PORT);	LlapDaemonInfo.initialize(appName, daemonConf);	int numExecutors = LlapDaemonInfo.INSTANCE.getNumExecutors();	long executorMemoryBytes = LlapDaemonInfo.INSTANCE.getExecutorMemory();	long ioMemoryBytes = LlapDaemonInfo.INSTANCE.getCacheSize();	boolean isDirectCache = LlapDaemonInfo.INSTANCE.isDirectCache();	boolean isLlapIo = LlapDaemonInfo.INSTANCE.isLlapIo();	LlapDaemon.initializeLogging(daemonConf);	llapDaemon = new LlapDaemon(daemonConf, numExecutors, executorMemoryBytes, isLlapIo, isDirectCache, ioMemoryBytes, localDirs, rpcPort, mngPort, shufflePort, webPort, appName);	
adding shutdown hook for llapdaemon 

int numExecutors = LlapDaemonInfo.INSTANCE.getNumExecutors();	long executorMemoryBytes = LlapDaemonInfo.INSTANCE.getExecutorMemory();	long ioMemoryBytes = LlapDaemonInfo.INSTANCE.getCacheSize();	boolean isDirectCache = LlapDaemonInfo.INSTANCE.isDirectCache();	boolean isLlapIo = LlapDaemonInfo.INSTANCE.isLlapIo();	LlapDaemon.initializeLogging(daemonConf);	llapDaemon = new LlapDaemon(daemonConf, numExecutors, executorMemoryBytes, isLlapIo, isDirectCache, ioMemoryBytes, localDirs, rpcPort, mngPort, shufflePort, webPort, appName);	ShutdownHookManager.addShutdownHook(new CompositeServiceShutdownHook(llapDaemon), 1);	llapDaemon.init(daemonConf);	llapDaemon.start();	
started llapdaemon with pid 

long executorMemoryBytes = LlapDaemonInfo.INSTANCE.getExecutorMemory();	long ioMemoryBytes = LlapDaemonInfo.INSTANCE.getCacheSize();	boolean isDirectCache = LlapDaemonInfo.INSTANCE.isDirectCache();	boolean isLlapIo = LlapDaemonInfo.INSTANCE.isLlapIo();	LlapDaemon.initializeLogging(daemonConf);	llapDaemon = new LlapDaemon(daemonConf, numExecutors, executorMemoryBytes, isLlapIo, isDirectCache, ioMemoryBytes, localDirs, rpcPort, mngPort, shufflePort, webPort, appName);	ShutdownHookManager.addShutdownHook(new CompositeServiceShutdownHook(llapDaemon), 1);	llapDaemon.init(daemonConf);	llapDaemon.start();	} catch (Throwable t) {	
failed to start llap daemon with exception 

public void uncaughtException(Thread t, Throwable e) {	
uncaughtexceptionhandler invoked 

public void uncaughtException(Thread t, Throwable e) {	if(ShutdownHookManager.isShutdownInProgress()) {	
thread threw a throwable but we are shutting down so ignoring this 

public void uncaughtException(Thread t, Throwable e) {	if(ShutdownHookManager.isShutdownInProgress()) {	} else if(e instanceof Error) {	try {	
thread threw an error shutting down now 

try {	System.err.println("Halting due to Out Of Memory Error...");	e.printStackTrace();	} catch (Throwable err) {	}	ExitUtil.halt(-1);	} else {	ExitUtil.terminate(-1);	}	} else {	
thread threw an exception shutting down now 

========================= hive sample_2238 =========================

public static List<HiveMetastoreAuthorizationProvider> getMetaStoreAuthorizeProviderManagers( Configuration conf, HiveConf.ConfVars authorizationProviderConfKey, HiveAuthenticationProvider authenticator) throws HiveException {	String clsStrs = HiveConf.getVar(conf, authorizationProviderConfKey);	if(clsStrs == null){	return null;	}	List<HiveMetastoreAuthorizationProvider> authProviders = new ArrayList<HiveMetastoreAuthorizationProvider>();	for (String clsStr : clsStrs.trim().split(",")) {	
adding metastore authorization provider 

========================= hive sample_5007 =========================

public void initialize(Configuration conf, Properties tbl) throws SerDeException {	MAX_BUFFERED_ROWS = HiveConf.getIntVar(conf, HiveConf.ConfVars.HIVE_SERVER2_THRIFT_RESULTSET_DEFAULT_FETCH_SIZE);	
thriftjdbcbinaryserde max number of buffered columns 

========================= hive sample_5284 =========================

}	} catch (Exception e) {	if (!(e instanceof SemanticException)) {	throw new SemanticException("Error reading message members", e);	} else {	throw (SemanticException) e;	}	}	TruncateTableDesc truncateTableDesc = new TruncateTableDesc( actualDbName + "." + actualTblName, partSpec, context.eventOnlyReplicationSpec());	Task<DDLWork> truncatePtnTask = TaskFactory.get( new DDLWork(readEntitySet, writeEntitySet, truncateTableDesc), context.hiveConf);	
added truncate ptn task 

========================= hive sample_3491 =========================

public GetOpenTxnsInfoResponse getOpenTxnsInfo() throws MetaException {	try {	Connection dbConn = null;	Statement stmt = null;	ResultSet rs = null;	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	String s = "select ntxn_next - 1 from NEXT_TXN_ID";	
going to execute query 

if (!rs.next()) {	throw new MetaException("Transaction tables not properly " + "initialized, no record found in next_txn_id");	}	long hwm = rs.getLong(1);	if (rs.wasNull()) {	throw new MetaException("Transaction tables not properly " + "initialized, null record found in next_txn_id");	}	close(rs);	List<TxnInfo> txnInfos = new ArrayList<>();	s = "select txn_id, txn_state, txn_user, txn_host, txn_started, txn_last_heartbeat from " + "TXNS where txn_id <= " + hwm;	
going to execute query 

break;	case TXN_OPEN: state = TxnState.OPEN;	break;	default: throw new MetaException("Unexpected transaction state " + c + " found in txns table");	}	TxnInfo txnInfo = new TxnInfo(rs.getLong(1), state, rs.getString(3), rs.getString(4));	txnInfo.setStartedTime(rs.getLong(5));	txnInfo.setLastHeartbeatTime(rs.getLong(6));	txnInfos.add(txnInfo);	}	
going to rollback 

default: throw new MetaException("Unexpected transaction state " + c + " found in txns table");	}	TxnInfo txnInfo = new TxnInfo(rs.getLong(1), state, rs.getString(3), rs.getString(4));	txnInfo.setStartedTime(rs.getLong(5));	txnInfo.setLastHeartbeatTime(rs.getLong(6));	txnInfos.add(txnInfo);	}	dbConn.rollback();	return new GetOpenTxnsInfoResponse(hwm, txnInfos);	} catch (SQLException e) {	
going to rollback 

public GetOpenTxnsResponse getOpenTxns() throws MetaException {	try {	Connection dbConn = null;	Statement stmt = null;	ResultSet rs = null;	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	String s = "select ntxn_next - 1 from NEXT_TXN_ID";	
going to execute query 

if (!rs.next()) {	throw new MetaException("Transaction tables not properly " + "initialized, no record found in next_txn_id");	}	long hwm = rs.getLong(1);	if (rs.wasNull()) {	throw new MetaException("Transaction tables not properly " + "initialized, null record found in next_txn_id");	}	close(rs);	List<Long> openList = new ArrayList<>();	s = "select txn_id, txn_state from TXNS where txn_id <= " + hwm + " order by txn_id";	
going to execute query 

while (rs.next()) {	long txnId = rs.getLong(1);	openList.add(txnId);	char c = rs.getString(2).charAt(0);	if(c == TXN_OPEN) {	minOpenTxn = Math.min(minOpenTxn, txnId);	} else if (c == TXN_ABORTED) {	abortedBits.set(openList.size() - 1);	}	}	
going to rollback 

}	}	dbConn.rollback();	ByteBuffer byteBuffer = ByteBuffer.wrap(abortedBits.toByteArray());	GetOpenTxnsResponse otr = new GetOpenTxnsResponse(hwm, openList, byteBuffer);	if(minOpenTxn < Long.MAX_VALUE) {	otr.setMin_open_txn(minOpenTxn);	}	return otr;	} catch (SQLException e) {	
going to rollback 

public OpenTxnsResponse openTxns(OpenTxnRequest rqst) throws MetaException {	if (!tooManyOpenTxns && numOpenTxns.get() >= maxOpenTxns) {	tooManyOpenTxns = true;	}	if (tooManyOpenTxns) {	if (numOpenTxns.get() < maxOpenTxns * 0.9) {	tooManyOpenTxns = false;	} else {	
maximum allowed number of open transactions has been reached current number of open transactions 

Connection dbConn = null;	Statement stmt = null;	ResultSet rs = null;	try {	lockInternal();	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	int maxTxns = MetastoreConf.getIntVar(conf, ConfVars.TXN_MAX_OPEN_BATCH);	if (numTxns > maxTxns) numTxns = maxTxns;	stmt = dbConn.createStatement();	String s = sqlGenerator.addForUpdateClause("select ntxn_next from NEXT_TXN_ID");	
going to execute query 

int maxTxns = MetastoreConf.getIntVar(conf, ConfVars.TXN_MAX_OPEN_BATCH);	if (numTxns > maxTxns) numTxns = maxTxns;	stmt = dbConn.createStatement();	String s = sqlGenerator.addForUpdateClause("select ntxn_next from NEXT_TXN_ID");	rs = stmt.executeQuery(s);	if (!rs.next()) {	throw new MetaException("Transaction database not properly " + "configured, can't find next transaction id.");	}	long first = rs.getLong(1);	s = "update NEXT_TXN_ID set ntxn_next = " + (first + numTxns);	
going to execute update 

stmt.executeUpdate(s);	long now = getDbTime(dbConn);	List<Long> txnIds = new ArrayList<>(numTxns);	List<String> rows = new ArrayList<>();	for (long i = first; i < first + numTxns; i++) {	txnIds.add(i);	rows.add(i + "," + quoteChar(TXN_OPEN) + "," + now + "," + now + "," + quoteString(rqst.getUser()) + "," + quoteString(rqst.getHostname()));	}	List<String> queries = sqlGenerator.createInsertValuesStmt( "TXNS (txn_id, txn_state, txn_started, txn_last_heartbeat, txn_user, txn_host)", rows);	for (String q : queries) {	
going to execute update 

List<Long> txnIds = new ArrayList<>(numTxns);	List<String> rows = new ArrayList<>();	for (long i = first; i < first + numTxns; i++) {	txnIds.add(i);	rows.add(i + "," + quoteChar(TXN_OPEN) + "," + now + "," + now + "," + quoteString(rqst.getUser()) + "," + quoteString(rqst.getHostname()));	}	List<String> queries = sqlGenerator.createInsertValuesStmt( "TXNS (txn_id, txn_state, txn_started, txn_last_heartbeat, txn_user, txn_host)", rows);	for (String q : queries) {	stmt.execute(q);	}	
going to commit 

txnIds.add(i);	rows.add(i + "," + quoteChar(TXN_OPEN) + "," + now + "," + now + "," + quoteString(rqst.getUser()) + "," + quoteString(rqst.getHostname()));	}	List<String> queries = sqlGenerator.createInsertValuesStmt( "TXNS (txn_id, txn_state, txn_started, txn_last_heartbeat, txn_user, txn_host)", rows);	for (String q : queries) {	stmt.execute(q);	}	dbConn.commit();	return new OpenTxnsResponse(txnIds);	} catch (SQLException e) {	
going to rollback 

try {	Connection dbConn = null;	Statement stmt = null;	try {	lockInternal();	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	if (abortTxns(dbConn, Collections.singletonList(txnid), true) != 1) {	stmt = dbConn.createStatement();	TxnStatus status = findTxnState(txnid,stmt);	if(status == TxnStatus.ABORTED) {	
aborttxn requested by it is already 

lockInternal();	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	if (abortTxns(dbConn, Collections.singletonList(txnid), true) != 1) {	stmt = dbConn.createStatement();	TxnStatus status = findTxnState(txnid,stmt);	if(status == TxnStatus.ABORTED) {	return;	}	raiseTxnUnexpectedState(status, txnid);	}	
going to commit 

if (abortTxns(dbConn, Collections.singletonList(txnid), true) != 1) {	stmt = dbConn.createStatement();	TxnStatus status = findTxnState(txnid,stmt);	if(status == TxnStatus.ABORTED) {	return;	}	raiseTxnUnexpectedState(status, txnid);	}	dbConn.commit();	} catch (SQLException e) {	
going to rollback 

public void abortTxns(AbortTxnsRequest rqst) throws NoSuchTxnException, MetaException {	List<Long> txnids = rqst.getTxn_ids();	try {	Connection dbConn = null;	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	int numAborted = abortTxns(dbConn, txnids, false);	if (numAborted != txnids.size()) {	
abort transactions command only aborted out of transactions it s possible that the other transactions have been aborted or committed or the transaction ids are invalid 

public void abortTxns(AbortTxnsRequest rqst) throws NoSuchTxnException, MetaException {	List<Long> txnids = rqst.getTxn_ids();	try {	Connection dbConn = null;	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	int numAborted = abortTxns(dbConn, txnids, false);	if (numAborted != txnids.size()) {	}	
going to commit 

List<Long> txnids = rqst.getTxn_ids();	try {	Connection dbConn = null;	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	int numAborted = abortTxns(dbConn, txnids, false);	if (numAborted != txnids.size()) {	}	dbConn.commit();	} catch (SQLException e) {	
going to rollback 

ResultSet lockHandle = null;	ResultSet commitIdRs = null, rs;	try {	lockInternal();	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	lockHandle = lockTransactionRecord(stmt, txnid, TXN_OPEN);	if (lockHandle == null) {	TxnStatus actualTxnStatus = findTxnState(txnid, stmt);	if(actualTxnStatus == TxnStatus.COMMITTED) {	
nth committxn msg 

}	dbConn.commit();	close(null, stmt, dbConn);	throw new TxnAbortedException(msg);	} else {	}	}	else {	}	String s = "insert into COMPLETED_TXN_COMPONENTS (ctc_txnid, ctc_database, " + "ctc_table, ctc_partition) select tc_txnid, tc_database, tc_table, " + "tc_partition from TXN_COMPONENTS where tc_txnid = " + txnid;	
going to execute insert 

close(null, stmt, dbConn);	throw new TxnAbortedException(msg);	} else {	}	}	else {	}	String s = "insert into COMPLETED_TXN_COMPONENTS (ctc_txnid, ctc_database, " + "ctc_table, ctc_partition) select tc_txnid, tc_database, tc_table, " + "tc_partition from TXN_COMPONENTS where tc_txnid = " + txnid;	int modCount = 0;	if ((modCount = stmt.executeUpdate(s)) < 1) {	
expected to move at least one record from txn components to completed txn components when committing txn 

} else {	}	}	else {	}	String s = "insert into COMPLETED_TXN_COMPONENTS (ctc_txnid, ctc_database, " + "ctc_table, ctc_partition) select tc_txnid, tc_database, tc_table, " + "tc_partition from TXN_COMPONENTS where tc_txnid = " + txnid;	int modCount = 0;	if ((modCount = stmt.executeUpdate(s)) < 1) {	}	s = "delete from TXN_COMPONENTS where tc_txnid = " + txnid;	
going to execute update 

}	else {	}	String s = "insert into COMPLETED_TXN_COMPONENTS (ctc_txnid, ctc_database, " + "ctc_table, ctc_partition) select tc_txnid, tc_database, tc_table, " + "tc_partition from TXN_COMPONENTS where tc_txnid = " + txnid;	int modCount = 0;	if ((modCount = stmt.executeUpdate(s)) < 1) {	}	s = "delete from TXN_COMPONENTS where tc_txnid = " + txnid;	modCount = stmt.executeUpdate(s);	s = "delete from HIVE_LOCKS where hl_txnid = " + txnid;	
going to execute update 

}	String s = "insert into COMPLETED_TXN_COMPONENTS (ctc_txnid, ctc_database, " + "ctc_table, ctc_partition) select tc_txnid, tc_database, tc_table, " + "tc_partition from TXN_COMPONENTS where tc_txnid = " + txnid;	int modCount = 0;	if ((modCount = stmt.executeUpdate(s)) < 1) {	}	s = "delete from TXN_COMPONENTS where tc_txnid = " + txnid;	modCount = stmt.executeUpdate(s);	s = "delete from HIVE_LOCKS where hl_txnid = " + txnid;	modCount = stmt.executeUpdate(s);	s = "delete from TXNS where txn_id = " + txnid;	
going to execute update 

String s = "insert into COMPLETED_TXN_COMPONENTS (ctc_txnid, ctc_database, " + "ctc_table, ctc_partition) select tc_txnid, tc_database, tc_table, " + "tc_partition from TXN_COMPONENTS where tc_txnid = " + txnid;	int modCount = 0;	if ((modCount = stmt.executeUpdate(s)) < 1) {	}	s = "delete from TXN_COMPONENTS where tc_txnid = " + txnid;	modCount = stmt.executeUpdate(s);	s = "delete from HIVE_LOCKS where hl_txnid = " + txnid;	modCount = stmt.executeUpdate(s);	s = "delete from TXNS where txn_id = " + txnid;	modCount = stmt.executeUpdate(s);	
going to commit 

s = "delete from TXN_COMPONENTS where tc_txnid = " + txnid;	modCount = stmt.executeUpdate(s);	s = "delete from HIVE_LOCKS where hl_txnid = " + txnid;	modCount = stmt.executeUpdate(s);	s = "delete from TXNS where txn_id = " + txnid;	modCount = stmt.executeUpdate(s);	dbConn.commit();	s = "select ctc_database, ctc_table, ctc_id, ctc_timestamp from COMPLETED_TXN_COMPONENTS where ctc_txnid = " + txnid;	rs = stmt.executeQuery(s);	if (rs.next()) {	
going to register table modification in invalidation cache 

modCount = stmt.executeUpdate(s);	dbConn.commit();	s = "select ctc_database, ctc_table, ctc_id, ctc_timestamp from COMPLETED_TXN_COMPONENTS where ctc_txnid = " + txnid;	rs = stmt.executeQuery(s);	if (rs.next()) {	MaterializationsInvalidationCache.get().notifyTableModification( rs.getString(1), rs.getString(2), rs.getLong(3), rs.getTimestamp(4, Calendar.getInstance(TimeZone.getTimeZone("UTC"))).getTime());	}	close(rs);	dbConn.commit();	} catch (SQLException e) {	
going to rollback 

}	long commitHighWaterMark;	long lowestOpenTxnId = rs.getLong(1);	if(rs.wasNull()) {	commitHighWaterMark = highestAllocatedTxnId + 1;	}	else {	commitHighWaterMark = lowestOpenTxnId;	}	int delCnt = stmt.executeUpdate("delete from WRITE_SET where ws_commit_id < " + commitHighWaterMark);	
deleted obsolete rows from wrtie set 

long lowestOpenTxnId = rs.getLong(1);	if(rs.wasNull()) {	commitHighWaterMark = highestAllocatedTxnId + 1;	}	else {	commitHighWaterMark = lowestOpenTxnId;	}	int delCnt = stmt.executeUpdate("delete from WRITE_SET where ws_commit_id < " + commitHighWaterMark);	dbConn.commit();	} catch (SQLException ex) {	
writeset gc failed due to 

public BasicTxnInfo getLastCompletedTransactionForTable( String inputDbName, String inputTableName, TxnsSnapshot txnsSnapshot) throws MetaException {	Connection dbConn = null;	Statement stmt = null;	ResultSet rs = null;	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	stmt.setMaxRows(1);	String s = "select ctc_id, ctc_timestamp, ctc_txnid, ctc_database, ctc_table " + "from COMPLETED_TXN_COMPONENTS " + "where ctc_database=" + quoteString(inputDbName) + " and ctc_table=" + quoteString(inputTableName) + " and ctc_txnid <= " + txnsSnapshot.getTxn_high_water_mark() + (txnsSnapshot.getOpen_txns().isEmpty() ? " " : " and ctc_txnid NOT IN(" + StringUtils.join(",", txnsSnapshot.getOpen_txns()) + ") ") + "order by ctc_id desc";	if (LOG.isDebugEnabled()) {	
going to execute query 

return new BasicTxnInfo(true);	}	final BasicTxnInfo txnInfo = new BasicTxnInfo(false);	txnInfo.setId(rs.getLong(1));	txnInfo.setTime(rs.getTimestamp(2, Calendar.getInstance(TimeZone.getTimeZone("UTC"))).getTime());	txnInfo.setTxnid(rs.getLong(3));	txnInfo.setDbname(rs.getString(4));	txnInfo.setTablename(rs.getString(5));	return txnInfo;	} catch (SQLException ex) {	
getlastcompletedtransactionfortable failed due to 

public BasicTxnInfo getFirstCompletedTransactionForTableAfterCommit( String inputDbName, String inputTableName, long incrementalIdentifier) throws MetaException {	Connection dbConn = null;	Statement stmt = null;	ResultSet rs = null;	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	stmt.setMaxRows(1);	String s = "select ctc_id, ctc_timestamp, ctc_txnid, ctc_database, ctc_table " + "from COMPLETED_TXN_COMPONENTS " + "where ctc_database=" + quoteString(inputDbName) + " and ctc_table=" + quoteString(inputTableName) + " and ctc_id > " + incrementalIdentifier + " order by ctc_id asc";	if (LOG.isDebugEnabled()) {	
going to execute query 

return new BasicTxnInfo(true);	}	final BasicTxnInfo txnInfo = new BasicTxnInfo(false);	txnInfo.setId(rs.getLong(1));	txnInfo.setTime(rs.getTimestamp(2, Calendar.getInstance(TimeZone.getTimeZone("UTC"))).getTime());	txnInfo.setTxnid(rs.getLong(3));	txnInfo.setDbname(rs.getString(4));	txnInfo.setTablename(rs.getString(5));	return txnInfo;	} catch (SQLException ex) {	
getlastcompletedtransactionfortable failed due to 

long txnid = rqst.getTxnid();	stmt = dbConn.createStatement();	if (isValidTxn(txnid)) {	lockHandle = lockTransactionRecord(stmt, txnid, TXN_OPEN);	if(lockHandle == null) {	ensureValidTxn(dbConn, txnid, stmt);	shouldNeverHappen(txnid);	}	}	String s = sqlGenerator.addForUpdateClause("select nl_next from NEXT_LOCK_ID");	
going to execute query 

if (isValidTxn(txnid)) {	lockHandle = lockTransactionRecord(stmt, txnid, TXN_OPEN);	if(lockHandle == null) {	ensureValidTxn(dbConn, txnid, stmt);	shouldNeverHappen(txnid);	}	}	String s = sqlGenerator.addForUpdateClause("select nl_next from NEXT_LOCK_ID");	rs = stmt.executeQuery(s);	if (!rs.next()) {	
going to rollback 

}	}	String s = sqlGenerator.addForUpdateClause("select nl_next from NEXT_LOCK_ID");	rs = stmt.executeQuery(s);	if (!rs.next()) {	dbConn.rollback();	throw new MetaException("Transaction tables not properly " + "initialized, no record found in next_lock_id");	}	long extLockId = rs.getLong(1);	s = "update NEXT_LOCK_ID set nl_next = " + (extLockId + 1);	
going to execute update 

if(!updateTxnComponents) {	continue;	}	String dbName = lc.getDbname();	String tblName = lc.getTablename();	String partName = lc.getPartitionname();	rows.add(txnid + ", '" + dbName + "', " + (tblName == null ? "null" : "'" + tblName + "'") + ", " + (partName == null ? "null" : "'" + partName + "'")+ "," + quoteString(OpertaionType.fromDataOperationType(lc.getOperationType()).toString()));	}	List<String> queries = sqlGenerator.createInsertValuesStmt( "TXN_COMPONENTS (tc_txnid, tc_database, tc_table, tc_partition, tc_operation_type)", rows);	for(String query : queries) {	
going to execute update 

case SHARED_READ: lockChar = LOCK_SHARED;	break;	case SHARED_WRITE: lockChar = LOCK_SEMI_SHARED;	break;	}	long now = getDbTime(dbConn);	rows.add(extLockId + ", " + intLockId + "," + txnid + ", " + quoteString(dbName) + ", " + valueOrNullLiteral(tblName) + ", " + valueOrNullLiteral(partName) + ", " + quoteChar(LOCK_WAITING) + ", " + quoteChar(lockChar) + ", " + (isValidTxn(txnid) ? 0 : now) + ", " + valueOrNullLiteral(rqst.getUser()) + ", " + valueOrNullLiteral(rqst.getHostname()) + ", " + valueOrNullLiteral(rqst.getAgentInfo()));	}	List<String> queries = sqlGenerator.createInsertValuesStmt( "HIVE_LOCKS (hl_lock_ext_id, hl_lock_int_id, hl_txnid, hl_db, " + "hl_table, hl_partition,hl_lock_state, hl_lock_type, " + "hl_last_heartbeat, hl_user, hl_host, hl_agent_info)", rows);	for(String query : queries) {	
going to execute update 

rows.add(extLockId + ", " + intLockId + "," + txnid + ", " + quoteString(dbName) + ", " + valueOrNullLiteral(tblName) + ", " + valueOrNullLiteral(partName) + ", " + quoteChar(LOCK_WAITING) + ", " + quoteChar(lockChar) + ", " + (isValidTxn(txnid) ? 0 : now) + ", " + valueOrNullLiteral(rqst.getUser()) + ", " + valueOrNullLiteral(rqst.getHostname()) + ", " + valueOrNullLiteral(rqst.getAgentInfo()));	}	List<String> queries = sqlGenerator.createInsertValuesStmt( "HIVE_LOCKS (hl_lock_ext_id, hl_lock_int_id, hl_txnid, hl_db, " + "hl_table, hl_partition,hl_lock_state, hl_lock_type, " + "hl_last_heartbeat, hl_user, hl_host, hl_agent_info)", rows);	for(String query : queries) {	int modCount = stmt.executeUpdate(query);	}	dbConn.commit();	success = true;	return new ConnectionLockIdPair(dbConn, extLockId);	} catch (SQLException e) {	
going to rollback 

private LockResponse checkLockWithRetry(Connection dbConn, long extLockId, long txnId) throws NoSuchLockException, NoSuchTxnException, TxnAbortedException, MetaException {	try {	try {	lockInternal();	if(dbConn.isClosed()) {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	}	return checkLock(dbConn, extLockId);	} catch (SQLException e) {	
going to rollback 

throw new NoSuchLockException("No such lock " + JavaUtils.lockIdToString(extLockId));	}	if (info.txnId > 0) {	heartbeatTxn(dbConn, info.txnId);	}	else {	heartbeatLock(dbConn, extLockId);	}	return checkLock(dbConn, extLockId);	} catch (SQLException e) {	
going to rollback 

public void unlock(UnlockRequest rqst) throws NoSuchLockException, TxnOpenException, MetaException {	try {	Connection dbConn = null;	Statement stmt = null;	long extLockId = rqst.getLockid();	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	String s = "delete from HIVE_LOCKS where hl_lock_ext_id = " + extLockId + " AND (hl_txnid = 0 OR" + " (hl_txnid <> 0 AND hl_lock_state = '" + LOCK_WAITING + "'))";	
going to execute update 

try {	Connection dbConn = null;	Statement stmt = null;	long extLockId = rqst.getLockid();	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	String s = "delete from HIVE_LOCKS where hl_lock_ext_id = " + extLockId + " AND (hl_txnid = 0 OR" + " (hl_txnid <> 0 AND hl_lock_state = '" + LOCK_WAITING + "'))";	int rc = stmt.executeUpdate(s);	if (rc < 1) {	
going to rollback 

long extLockId = rqst.getLockid();	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	String s = "delete from HIVE_LOCKS where hl_lock_ext_id = " + extLockId + " AND (hl_txnid = 0 OR" + " (hl_txnid <> 0 AND hl_lock_state = '" + LOCK_WAITING + "'))";	int rc = stmt.executeUpdate(s);	if (rc < 1) {	dbConn.rollback();	LockInfo info = getTxnIdFromLockId(dbConn, extLockId);	if(info == null) {	
no lock in mode found for unlock 

String msg = "Unlocking locks associated with transaction not permitted.  " + info;	LOG.error(msg);	throw new TxnOpenException(msg);	}	if(info.txnId == 0) {	String msg = "Found lock in unexpected state " + info;	LOG.error(msg);	throw new MetaException(msg);	}	}	
going to commit 

throw new TxnOpenException(msg);	}	if(info.txnId == 0) {	String msg = "Found lock in unexpected state " + info;	LOG.error(msg);	throw new MetaException(msg);	}	}	dbConn.commit();	} catch (SQLException e) {	
going to rollback 

if (partName != null && !partName.isEmpty()) {	if (filter.length() > 0) {	filter.append(" and ");	}	filter.append("hl_partition=").append(quoteString(partName));	}	String whereClause = filter.toString();	if (!whereClause.isEmpty()) {	s = s + " where " + whereClause;	}	
doing to execute query 

if(!rs.wasNull()) {	e.setBlockedByExtId(id);	}	id = rs.getLong(14);	if(!rs.wasNull()) {	e.setBlockedByIntId(id);	}	e.setAgentInfo(rs.getString(15));	sortedList.add(new LockInfoExt(e));	}	
going to rollback 

public void heartbeat(HeartbeatRequest ids) throws NoSuchTxnException,  NoSuchLockException, TxnAbortedException, MetaException {	try {	Connection dbConn = null;	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	heartbeatLock(dbConn, ids.getLockid());	heartbeatTxn(dbConn, ids.getTxnid());	} catch (SQLException e) {	
going to rollback 

stmt = dbConn.createStatement();	List<String> queries = new ArrayList<>();	int numTxnsToHeartbeat = (int) (rqst.getMax() - rqst.getMin() + 1);	List<Long> txnIds = new ArrayList<>(numTxnsToHeartbeat);	for (long txn = rqst.getMin(); txn <= rqst.getMax(); txn++) {	txnIds.add(txn);	}	TxnUtils.buildQueryWithINClause(conf, queries, new StringBuilder("update TXNS set txn_last_heartbeat = " + getDbTime(dbConn) + " where txn_state = " + quoteChar(TXN_OPEN) + " and "), new StringBuilder(""), txnIds, "txn_id", true, false);	int updateCnt = 0;	for (String query : queries) {	
going to execute update 

try {	heartbeatTxn(dbConn, txn);	} catch (NoSuchTxnException e) {	nosuch.add(txn);	} catch (TxnAbortedException e) {	aborted.add(txn);	}	}	return rsp;	} catch (SQLException e) {	
going to rollback 

dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	long id = generateCompactionQueueId(stmt);	StringBuilder sb = new StringBuilder("select cq_id, cq_state from COMPACTION_QUEUE where"). append(" cq_state IN(").append(quoteChar(INITIATED_STATE)). append(",").append(quoteChar(WORKING_STATE)). append(") AND cq_database=").append(quoteString(rqst.getDbname())). append(" AND cq_table=").append(quoteString(rqst.getTablename())).append(" AND ");	if(rqst.getPartitionname() == null) {	sb.append("cq_partition is null");	}	else {	sb.append("cq_partition=").append(quoteString(rqst.getPartitionname()));	}	
going to execute query 

buf.append(partName);	buf.append("', '");	}	buf.append(INITIATED_STATE);	buf.append("', '");	switch (rqst.getType()) {	case MAJOR: buf.append(MAJOR_TYPE);	break;	case MINOR: buf.append(MINOR_TYPE);	break;	
going to rollback 

if (rqst.getProperties() != null) {	buf.append("', '");	buf.append(new StringableMap(rqst.getProperties()).toString());	}	if (rqst.getRunas() != null) {	buf.append("', '");	buf.append(rqst.getRunas());	}	buf.append("')");	String s = buf.toString();	
going to execute update 

buf.append("', '");	buf.append(new StringableMap(rqst.getProperties()).toString());	}	if (rqst.getRunas() != null) {	buf.append("', '");	buf.append(rqst.getRunas());	}	buf.append("')");	String s = buf.toString();	stmt.executeUpdate(s);	
going to commit 

if (rqst.getRunas() != null) {	buf.append("', '");	buf.append(rqst.getRunas());	}	buf.append("')");	String s = buf.toString();	stmt.executeUpdate(s);	dbConn.commit();	return new CompactionResponse(id, INITIATED_RESPONSE, true);	} catch (SQLException e) {	
going to rollback 

public ShowCompactResponse showCompact(ShowCompactRequest rqst) throws MetaException {	ShowCompactResponse response = new ShowCompactResponse(new ArrayList<>());	Connection dbConn = null;	Statement stmt = null;	try {	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	String s = "select cq_database, cq_table, cq_partition, cq_state, cq_type, cq_worker_id, " + "cq_start, -1 cc_end, cq_run_as, cq_hadoop_job_id, cq_id from COMPACTION_QUEUE union all " + "select cc_database, cc_table, cc_partition, cc_state, cc_type, cc_worker_id, " + "cc_start, cc_end, cc_run_as, cc_hadoop_job_id, cc_id from COMPLETED_COMPACTIONS";	
going to execute query 

}	long endTime = rs.getLong(8);	if(endTime != -1) {	e.setEndTime(endTime);	}	e.setRunAs(rs.getString(9));	e.setHadoopJobId(rs.getString(10));	e.setId(rs.getLong(11));	response.addToCompacts(e);	}	
going to rollback 

if(endTime != -1) {	e.setEndTime(endTime);	}	e.setRunAs(rs.getString(9));	e.setHadoopJobId(rs.getString(10));	e.setId(rs.getLong(11));	response.addToCompacts(e);	}	dbConn.rollback();	} catch (SQLException e) {	
going to rollback 

if(rqst.isSetOperationType()) {	ot = OpertaionType.fromDataOperationType(rqst.getOperationType());	}	List<String> rows = new ArrayList<>();	for (String partName : rqst.getPartitionnames()) {	rows.add(rqst.getTxnid() + "," + quoteString(rqst.getDbname()) + "," + quoteString(rqst.getTablename()) + "," + quoteString(partName) + "," + quoteChar(ot.sqlConst));	}	int modCount = 0;	List<String> queries = sqlGenerator.createInsertValuesStmt( "TXN_COMPONENTS (tc_txnid, tc_database, tc_table, tc_partition, tc_operation_type)", rows);	for(String query : queries) {	
going to execute update 

}	List<String> rows = new ArrayList<>();	for (String partName : rqst.getPartitionnames()) {	rows.add(rqst.getTxnid() + "," + quoteString(rqst.getDbname()) + "," + quoteString(rqst.getTablename()) + "," + quoteString(partName) + "," + quoteChar(ot.sqlConst));	}	int modCount = 0;	List<String> queries = sqlGenerator.createInsertValuesStmt( "TXN_COMPONENTS (tc_txnid, tc_database, tc_table, tc_partition, tc_operation_type)", rows);	for(String query : queries) {	modCount = stmt.executeUpdate(query);	}	
going to commit 

for (String partName : rqst.getPartitionnames()) {	rows.add(rqst.getTxnid() + "," + quoteString(rqst.getDbname()) + "," + quoteString(rqst.getTablename()) + "," + quoteString(partName) + "," + quoteChar(ot.sqlConst));	}	int modCount = 0;	List<String> queries = sqlGenerator.createInsertValuesStmt( "TXN_COMPONENTS (tc_txnid, tc_database, tc_table, tc_partition, tc_operation_type)", rows);	for(String query : queries) {	modCount = stmt.executeUpdate(query);	}	dbConn.commit();	} catch (SQLException e) {	
going to rollback 

buff.append(tblName);	buff.append("' and cc_partition='");	buff.append(partName);	buff.append("'");	queries.add(buff.toString());	}	break;	default: throw new MetaException("Invalid object type for cleanup: " + type);	}	for (String query : queries) {	
going to execute update 

buff.append(partName);	buff.append("'");	queries.add(buff.toString());	}	break;	default: throw new MetaException("Invalid object type for cleanup: " + type);	}	for (String query : queries) {	stmt.executeUpdate(query);	}	
going to commit 

queries.add(buff.toString());	}	break;	default: throw new MetaException("Invalid object type for cleanup: " + type);	}	for (String query : queries) {	stmt.executeUpdate(query);	}	dbConn.commit();	} catch (SQLException e) {	
going to rollback 

default: throw new MetaException("Invalid object type for cleanup: " + type);	}	for (String query : queries) {	stmt.executeUpdate(query);	}	dbConn.commit();	} catch (SQLException e) {	rollbackDBConn(dbConn);	checkRetryable(dbConn, e, "cleanupRecords");	if (e.getMessage().contains("does not exist")) {	
cannot perform cleanup since metastore table does not exist 

public int numLocksInLockTable() throws SQLException, MetaException {	Connection dbConn = null;	Statement stmt = null;	ResultSet rs = null;	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	String s = "select count(*) from HIVE_LOCKS";	
going to execute query 

static void rollbackDBConn(Connection dbConn) {	try {	if (dbConn != null && !dbConn.isClosed()) dbConn.rollback();	} catch (SQLException e) {	
failed to rollback db connection 

protected static void closeDbConn(Connection dbConn) {	try {	if (dbConn != null && !dbConn.isClosed()) {	dbConn.close();	}	} catch (SQLException e) {	
failed to close db connection 

protected static void closeStmt(Statement stmt) {	try {	if (stmt != null && !stmt.isClosed()) stmt.close();	} catch (SQLException e) {	
failed to close statement 

static void close(ResultSet rs) {	try {	if (rs != null && !rs.isClosed()) {	rs.close();	}	}	catch(SQLException ex) {	
failed to close statement 

protected void checkRetryable(Connection conn, SQLException e, String caller) throws RetryException, MetaException {	boolean sendRetrySignal = false;	try {	if(dbProduct == null) {	throw new IllegalStateException("DB Type not determined yet.");	}	if (DatabaseProduct.isDeadlock(dbProduct, e)) {	if (deadlockCnt++ < ALLOWED_REPEATED_DEADLOCKS) {	long waitInterval = deadlockRetryInterval * deadlockCnt;	
deadlock detected in will wait ms try again up to times 

}	if (DatabaseProduct.isDeadlock(dbProduct, e)) {	if (deadlockCnt++ < ALLOWED_REPEATED_DEADLOCKS) {	long waitInterval = deadlockRetryInterval * deadlockCnt;	try {	Thread.sleep(waitInterval);	} catch (InterruptedException ie) {	}	sendRetrySignal = true;	} else {	
too many repeated deadlocks in giving up 

long waitInterval = deadlockRetryInterval * deadlockCnt;	try {	Thread.sleep(waitInterval);	} catch (InterruptedException ie) {	}	sendRetrySignal = true;	} else {	}	} else if (isRetryable(conf, e)) {	if (retryNum++ < retryLimit) {	
retryable error detected in will wait ms and retry up to times error 

} else {	}	} else if (isRetryable(conf, e)) {	if (retryNum++ < retryLimit) {	try {	Thread.sleep(retryInterval);	} catch (InterruptedException ex) {	}	sendRetrySignal = true;	} else {	
fatal error in retry limit reached last error 

if (retryNum++ < retryLimit) {	try {	Thread.sleep(retryInterval);	} catch (InterruptedException ex) {	}	sendRetrySignal = true;	} else {	}	}	else {	
non retryable error in 

case DERBY: s = "values current_timestamp";	break;	case MYSQL: case POSTGRES: case SQLSERVER: s = "select current_timestamp";	break;	case ORACLE: s = "select current_timestamp from dual";	break;	default: String msg = "Unknown database product: " + dbProduct.toString();	LOG.error(msg);	throw new MetaException(msg);	}	
going to execute query 

private void checkQFileTestHack() {	boolean hackOn = MetastoreConf.getBoolVar(conf, ConfVars.HIVE_IN_TEST) || MetastoreConf.getBoolVar(conf, ConfVars.HIVE_IN_TEZ_TEST);	if (hackOn) {	
hacking in canned values for transaction manager 

StringBuilder prefix = new StringBuilder();	StringBuilder suffix = new StringBuilder();	prefix.append("update TXNS set txn_state = " + quoteChar(TXN_ABORTED) + " where txn_state = " + quoteChar(TXN_OPEN) + " and ");	if(max_heartbeat > 0) {	suffix.append(" and txn_last_heartbeat < ").append(max_heartbeat);	} else {	suffix.append("");	}	TxnUtils.buildQueryWithINClause(conf, queries, prefix, suffix, txnids, "txn_id", true, false);	for (String query : queries) {	
going to execute update 

if (updateCnt < txnids.size() && isStrict) {	return updateCnt;	}	queries.clear();	prefix.setLength(0);	suffix.setLength(0);	prefix.append("delete from HIVE_LOCKS where ");	suffix.append("");	TxnUtils.buildQueryWithINClause(conf, queries, prefix, suffix, txnids, "hl_txnid", false, false);	for (String query : queries) {	
going to execute update 

return updateCnt;	}	queries.clear();	prefix.setLength(0);	suffix.setLength(0);	prefix.append("delete from HIVE_LOCKS where ");	suffix.append("");	TxnUtils.buildQueryWithINClause(conf, queries, prefix, suffix, txnids, "hl_txnid", false, false);	for (String query : queries) {	int rc = stmt.executeUpdate(query);	
removed records from hive locks 

if (first) first = false;	else query.append(", ");	query.append('\'');	query.append(s);	query.append('\'');	}	query.append("))");	}	}	query.append(" and hl_lock_ext_id <= ").append(extLockId);	
going to execute query 

}	query.append(" and hl_lock_ext_id <= ").append(extLockId);	stmt = dbConn.createStatement();	rs = stmt.executeQuery(query.toString());	SortedSet<LockInfo> lockSet = new TreeSet<LockInfo>(new LockInfoComparator());	while (rs.next()) {	lockSet.add(new LockInfo(rs));	}	LockInfo[] locks = lockSet.toArray(new LockInfo[lockSet.size()]);	if(LOG.isTraceEnabled()) {	
locks to check full 

}	for (LockInfo info : locksBeingChecked) {	int index = -1;	for (int i = 0; i < locks.length; i++) {	if (locks[i].equals(info)) {	index = i;	break;	}	}	if (index == -1) {	
going to rollback 

if (!locks[index].db.equals(locks[i].db)) {	continue;	}	if (locks[index].table != null && locks[i].table != null && !locks[index].table.equals(locks[i].table)) {	continue;	}	if (locks[index].partition != null && locks[i].partition != null && !locks[index].partition.equals(locks[i].partition)) {	continue;	}	LockAction lockAction = jumpTable.get(locks[index].type).get(locks[i].type).get(locks[i].state);	
desired lock checked lock action 

continue;	}	if (locks[index].partition != null && locks[i].partition != null && !locks[index].partition.equals(locks[i].partition)) {	continue;	}	LockAction lockAction = jumpTable.get(locks[index].type).get(locks[i].type).get(locks[i].state);	switch (lockAction) {	case WAIT: if(!ignoreConflict(info, locks[i])) {	wait(dbConn, save);	String sqlText = "update HIVE_LOCKS" + " set HL_BLOCKEDBY_EXT_ID=" + locks[i].extLockId + ", HL_BLOCKEDBY_INT_ID=" + locks[i].intLockId + " where HL_LOCK_EXT_ID=" + info.extLockId + " and HL_LOCK_INT_ID=" + info.intLockId;	
executing sql 

}	LockAction lockAction = jumpTable.get(locks[index].type).get(locks[i].type).get(locks[i].state);	switch (lockAction) {	case WAIT: if(!ignoreConflict(info, locks[i])) {	wait(dbConn, save);	String sqlText = "update HIVE_LOCKS" + " set HL_BLOCKEDBY_EXT_ID=" + locks[i].extLockId + ", HL_BLOCKEDBY_INT_ID=" + locks[i].intLockId + " where HL_LOCK_EXT_ID=" + info.extLockId + " and HL_LOCK_INT_ID=" + info.intLockId;	int updCnt = stmt.executeUpdate(sqlText);	if(updCnt != 1) {	shouldNeverHappen(info.txnId, info.extLockId, info.intLockId);	}	
going to commit 

switch (lockAction) {	case WAIT: if(!ignoreConflict(info, locks[i])) {	wait(dbConn, save);	String sqlText = "update HIVE_LOCKS" + " set HL_BLOCKEDBY_EXT_ID=" + locks[i].extLockId + ", HL_BLOCKEDBY_INT_ID=" + locks[i].intLockId + " where HL_LOCK_EXT_ID=" + info.extLockId + " and HL_LOCK_INT_ID=" + info.intLockId;	int updCnt = stmt.executeUpdate(sqlText);	if(updCnt != 1) {	shouldNeverHappen(info.txnId, info.extLockId, info.intLockId);	}	dbConn.commit();	response.setState(LockState.WAITING);	
lock waiting for lock 

response.setState(LockState.WAITING);	return response;	}	case ACQUIRE: break;	case KEEP_LOOKING: continue;	}	break;	}	}	acquire(dbConn, stmt, locksBeingChecked);	
going to commit 

private void acquire(Connection dbConn, Statement stmt, List<LockInfo> locksBeingChecked) throws SQLException, NoSuchLockException, MetaException {	if(locksBeingChecked == null || locksBeingChecked.isEmpty()) {	return;	}	long txnId = locksBeingChecked.get(0).txnId;	long extLockId = locksBeingChecked.get(0).extLockId;	long now = getDbTime(dbConn);	String s = "update HIVE_LOCKS set hl_lock_state = '" + LOCK_ACQUIRED + "', " + "hl_last_heartbeat = " + (isValidTxn(txnId) ? 0 : now) + ", hl_acquired_at = " + now + ",HL_BLOCKEDBY_EXT_ID=NULL,HL_BLOCKEDBY_INT_ID=null" + " where hl_lock_ext_id = " +  extLockId;	
going to execute update 

private void acquire(Connection dbConn, Statement stmt, List<LockInfo> locksBeingChecked) throws SQLException, NoSuchLockException, MetaException {	if(locksBeingChecked == null || locksBeingChecked.isEmpty()) {	return;	}	long txnId = locksBeingChecked.get(0).txnId;	long extLockId = locksBeingChecked.get(0).extLockId;	long now = getDbTime(dbConn);	String s = "update HIVE_LOCKS set hl_lock_state = '" + LOCK_ACQUIRED + "', " + "hl_last_heartbeat = " + (isValidTxn(txnId) ? 0 : now) + ", hl_acquired_at = " + now + ",HL_BLOCKEDBY_EXT_ID=NULL,HL_BLOCKEDBY_INT_ID=null" + " where hl_lock_ext_id = " +  extLockId;	int rc = stmt.executeUpdate(s);	if (rc < locksBeingChecked.size()) {	
going to rollback acquire connection dbconn statement stmt list lockinfo locksbeingchecked 

private void wait(Connection dbConn, Savepoint save) throws SQLException {	
going to rollback to savepoint 

private void heartbeatLock(Connection dbConn, long extLockId) throws NoSuchLockException, SQLException, MetaException {	if (extLockId == 0) return;	Statement stmt = null;	try {	stmt = dbConn.createStatement();	long now = getDbTime(dbConn);	String s = "update HIVE_LOCKS set hl_last_heartbeat = " + now + " where hl_lock_ext_id = " + extLockId;	
going to execute update 

private void heartbeatLock(Connection dbConn, long extLockId) throws NoSuchLockException, SQLException, MetaException {	if (extLockId == 0) return;	Statement stmt = null;	try {	stmt = dbConn.createStatement();	long now = getDbTime(dbConn);	String s = "update HIVE_LOCKS set hl_last_heartbeat = " + now + " where hl_lock_ext_id = " + extLockId;	int rc = stmt.executeUpdate(s);	if (rc < 1) {	
going to rollback 

Statement stmt = null;	try {	stmt = dbConn.createStatement();	long now = getDbTime(dbConn);	String s = "update HIVE_LOCKS set hl_last_heartbeat = " + now + " where hl_lock_ext_id = " + extLockId;	int rc = stmt.executeUpdate(s);	if (rc < 1) {	dbConn.rollback();	throw new NoSuchLockException("No such lock: " + JavaUtils.lockIdToString(extLockId));	}	
going to commit 

private void heartbeatTxn(Connection dbConn, long txnid) throws NoSuchTxnException, TxnAbortedException, SQLException, MetaException {	if (txnid == 0) return;	Statement stmt = null;	try {	stmt = dbConn.createStatement();	long now = getDbTime(dbConn);	String s = "update TXNS set txn_last_heartbeat = " + now + " where txn_id = " + txnid + " and txn_state = '" + TXN_OPEN + "'";	
going to execute update 

private void heartbeatTxn(Connection dbConn, long txnid) throws NoSuchTxnException, TxnAbortedException, SQLException, MetaException {	if (txnid == 0) return;	Statement stmt = null;	try {	stmt = dbConn.createStatement();	long now = getDbTime(dbConn);	String s = "update TXNS set txn_last_heartbeat = " + now + " where txn_id = " + txnid + " and txn_state = '" + TXN_OPEN + "'";	int rc = stmt.executeUpdate(s);	if (rc < 1) {	ensureValidTxn(dbConn, txnid, stmt);	
can neither heartbeat txn nor confirm it as invalid 

try {	stmt = dbConn.createStatement();	long now = getDbTime(dbConn);	String s = "update TXNS set txn_last_heartbeat = " + now + " where txn_id = " + txnid + " and txn_state = '" + TXN_OPEN + "'";	int rc = stmt.executeUpdate(s);	if (rc < 1) {	ensureValidTxn(dbConn, txnid, stmt);	dbConn.rollback();	throw new NoSuchTxnException("No such txn: " + txnid);	}	
going to commit 

private TxnStatus findTxnState(long txnid, Statement stmt) throws SQLException, MetaException {	String s = "select txn_state from TXNS where txn_id = " + txnid;	
going to execute query 

private TxnStatus findTxnState(long txnid, Statement stmt) throws SQLException, MetaException {	String s = "select txn_state from TXNS where txn_id = " + txnid;	try (ResultSet rs = stmt.executeQuery(s)) {	if (!rs.next()) {	s = sqlGenerator.addLimitClause(1, "1 from COMPLETED_TXN_COMPONENTS where CTC_TXNID = " + txnid);	
going to execute query 

private static void ensureValidTxn(Connection dbConn, long txnid, Statement stmt) throws SQLException, NoSuchTxnException, TxnAbortedException {	String s = "select txn_state from TXNS where txn_id = " + txnid;	
going to execute query 

private static void ensureValidTxn(Connection dbConn, long txnid, Statement stmt) throws SQLException, NoSuchTxnException, TxnAbortedException {	String s = "select txn_state from TXNS where txn_id = " + txnid;	try (ResultSet rs = stmt.executeQuery(s)) {	if (!rs.next()) {	s = "select count(*) from COMPLETED_TXN_COMPONENTS where CTC_TXNID = " + txnid;	try (ResultSet rs2 = stmt.executeQuery(s)) {	boolean alreadyCommitted = rs2.next() && rs2.getInt(1) > 0;	
going to rollback 

try (ResultSet rs2 = stmt.executeQuery(s)) {	boolean alreadyCommitted = rs2.next() && rs2.getInt(1) > 0;	rollbackDBConn(dbConn);	if (alreadyCommitted) {	throw new NoSuchTxnException("Transaction " + JavaUtils.txnIdToString(txnid) + " is already committed.");	}	throw new NoSuchTxnException("No such transaction " + JavaUtils.txnIdToString(txnid));	}	}	if (rs.getString(1).charAt(0) == TXN_ABORTED) {	
going to rollback 

private LockInfo getTxnIdFromLockId(Connection dbConn, long extLockId) throws NoSuchLockException, MetaException, SQLException {	Statement stmt = null;	ResultSet rs = null;	try {	stmt = dbConn.createStatement();	String s = "select hl_lock_ext_id, hl_lock_int_id, hl_db, hl_table, " + "hl_partition, hl_lock_state, hl_lock_type, hl_txnid from HIVE_LOCKS where " + "hl_lock_ext_id = " + extLockId;	
going to execute query 

Statement stmt = null;	ResultSet rs = null;	try {	stmt = dbConn.createStatement();	String s = "select hl_lock_ext_id, hl_lock_int_id, hl_db, hl_table, " + "hl_partition, hl_lock_state, hl_lock_type, hl_txnid from HIVE_LOCKS where " + "hl_lock_ext_id = " + extLockId;	rs = stmt.executeQuery(s);	if (!rs.next()) {	return null;	}	LockInfo info = new LockInfo(rs);	
gettxnidfromlockid return 

private List<LockInfo> getLockInfoFromLockId(Connection dbConn, long extLockId) throws NoSuchLockException, MetaException, SQLException {	Statement stmt = null;	try {	stmt = dbConn.createStatement();	String s = "select hl_lock_ext_id, hl_lock_int_id, hl_db, hl_table, " + "hl_partition, hl_lock_state, hl_lock_type, hl_txnid from HIVE_LOCKS where " + "hl_lock_ext_id = " + extLockId;	
going to execute query 

List<String> queries = new ArrayList<>();	StringBuilder prefix = new StringBuilder();	StringBuilder suffix = new StringBuilder();	prefix.append("delete from HIVE_LOCKS where hl_last_heartbeat < ");	prefix.append(maxHeartbeatTime);	prefix.append(" and hl_txnid = 0 and ");	suffix.append("");	TxnUtils.buildQueryWithINClause(conf, queries, prefix, suffix, extLockIDs, "hl_lock_ext_id", true, false);	int deletedLocks = 0;	for (String query : queries) {	
removing expired locks via 

suffix.append("");	TxnUtils.buildQueryWithINClause(conf, queries, prefix, suffix, extLockIDs, "hl_lock_ext_id", true, false);	int deletedLocks = 0;	for (String query : queries) {	deletedLocks += stmt.executeUpdate(query);	}	if(deletedLocks > 0) {	Collections.sort(extLockIDs);	LOG.info("Deleted " + deletedLocks + " int locks from HIVE_LOCKS due to timeout (" + "HL_LOCK_EXT_ID list:  " + extLockIDs + ") maxHeartbeatTime=" + maxHeartbeatTime);	}	
going to commit 

for (String query : queries) {	deletedLocks += stmt.executeUpdate(query);	}	if(deletedLocks > 0) {	Collections.sort(extLockIDs);	LOG.info("Deleted " + deletedLocks + " int locks from HIVE_LOCKS due to timeout (" + "HL_LOCK_EXT_ID list:  " + extLockIDs + ") maxHeartbeatTime=" + maxHeartbeatTime);	}	dbConn.commit();	}	catch(SQLException ex) {	
failed to purge timedout locks due to 

}	if(deletedLocks > 0) {	Collections.sort(extLockIDs);	LOG.info("Deleted " + deletedLocks + " int locks from HIVE_LOCKS due to timeout (" + "HL_LOCK_EXT_ID list:  " + extLockIDs + ") maxHeartbeatTime=" + maxHeartbeatTime);	}	dbConn.commit();	}	catch(SQLException ex) {	}	catch(Exception ex) {	
failed to purge timedout locks due to 

Statement stmt = null;	ResultSet rs = null;	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	long now = getDbTime(dbConn);	timeOutLocks(dbConn, now);	while(true) {	stmt = dbConn.createStatement();	String s = " txn_id from TXNS where txn_state = '" + TXN_OPEN + "' and txn_last_heartbeat <  " + (now - timeout);	s = sqlGenerator.addLimitClause(10 * TIMED_OUT_TXN_ABORT_BATCH_SIZE, s);	
going to execute query 

currentBatch.add(rs.getLong(1));	} while(rs.next());	dbConn.commit();	close(rs, stmt, null);	int numTxnsAborted = 0;	for(List<Long> batchToAbort : timedOutTxns) {	if(abortTxns(dbConn, batchToAbort, now - timeout, true) == batchToAbort.size()) {	dbConn.commit();	numTxnsAborted += batchToAbort.size();	Collections.sort(batchToAbort);	
aborted the following transactions due to timeout 

for(List<Long> batchToAbort : timedOutTxns) {	if(abortTxns(dbConn, batchToAbort, now - timeout, true) == batchToAbort.size()) {	dbConn.commit();	numTxnsAborted += batchToAbort.size();	Collections.sort(batchToAbort);	}	else {	dbConn.rollback();	}	}	
aborted transactions due to timeout 

dbConn.commit();	numTxnsAborted += batchToAbort.size();	Collections.sort(batchToAbort);	}	else {	dbConn.rollback();	}	}	}	} catch (SQLException ex) {	
aborting timedout transactions failed due to 

Collections.sort(batchToAbort);	}	else {	dbConn.rollback();	}	}	}	} catch (SQLException ex) {	}	catch(MetaException e) {	
aborting timedout transactions failed due to 

public void countOpenTxns() throws MetaException {	Connection dbConn = null;	Statement stmt = null;	ResultSet rs = null;	try {	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	String s = "select count(*) from TXNS where txn_state = '" + TXN_OPEN + "'";	
going to execute query 

Connection dbConn = null;	Statement stmt = null;	ResultSet rs = null;	try {	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	String s = "select count(*) from TXNS where txn_state = '" + TXN_OPEN + "'";	rs = stmt.executeQuery(s);	if (!rs.next()) {	
transaction database not properly configured can t find txn state from txns 

try {	try {	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);	stmt = dbConn.createStatement();	String s = "select count(*) from TXNS where txn_state = '" + TXN_OPEN + "'";	rs = stmt.executeQuery(s);	if (!rs.next()) {	} else {	Long numOpen = rs.getLong(1);	if (numOpen > Integer.MAX_VALUE) {	
open transaction count above can t count that high 

rs = stmt.executeQuery(s);	if (!rs.next()) {	} else {	Long numOpen = rs.getLong(1);	if (numOpen > Integer.MAX_VALUE) {	} else {	numOpenTxns.set(numOpen.intValue());	}	}	} catch (SQLException e) {	
going to rollback 

if (!rs.next()) {	} else {	Long numOpen = rs.getLong(1);	if (numOpen > Integer.MAX_VALUE) {	} else {	numOpenTxns.set(numOpen.intValue());	}	}	} catch (SQLException e) {	rollbackDBConn(dbConn);	
failed to update number of open transactions 

} else if ("dbcp".equals(connectionPooler)) {	GenericObjectPool objectPool = new GenericObjectPool();	objectPool.setMaxActive(maxPoolSize);	objectPool.setMaxWait(getConnectionTimeoutMs);	ConnectionFactory connFactory = new DriverManagerConnectionFactory(driverUrl, user, passwd);	PoolableConnectionFactory poolConnFactory = new PoolableConnectionFactory(connFactory, objectPool, null, null, false, true);	return new PoolingDataSource(objectPool);	} else if ("hikaricp".equals(connectionPooler)) {	return new HikariCPDataSourceProvider().create(conf);	} else if ("none".equals(connectionPooler)) {	
choosing not to pool jdbc connections 

static CompactionType dbCompactionType2ThriftType(char dbValue) {	switch (dbValue) {	case MAJOR_TYPE: return CompactionType.MAJOR;	case MINOR_TYPE: return CompactionType.MINOR;	
unexpected compaction type 

static Character thriftCompactionType2DbType(CompactionType ct) {	switch (ct) {	case MAJOR: return MAJOR_TYPE;	case MINOR: return MINOR_TYPE;	
unexpected compaction type 

Connection dbConn = null;	Statement stmt = null;	ResultSet rs = null;	try {	try {	String sqlStmt = sqlGenerator.addForUpdateClause("select MT_COMMENT from AUX_TABLE where MT_KEY1=" + quoteString(key) + " and MT_KEY2=0");	lockInternal();	dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED, connPoolMutex);	stmt = dbConn.createStatement();	if(LOG.isDebugEnabled()) {	
about to execute sql 

if (!rs.next()) {	throw new IllegalStateException("Unable to lock " + quoteString(key) + ".  Expected row in AUX_TABLE is missing.");	}	}	Semaphore derbySemaphore = null;	if(dbProduct == DatabaseProduct.DERBY) {	derbyKey2Lock.putIfAbsent(key, new Semaphore(1));	derbySemaphore =  derbyKey2Lock.get(key);	derbySemaphore.acquire();	}	
locked by 

public void releaseLocks() {	rollbackDBConn(dbConn);	close(rs, stmt, dbConn);	if(derbySemaphore != null) {	derbySemaphore.release();	}	for(String key : keys) {	
unlocked by 

public Connection getConnection(String username, String password) throws SQLException {	if (driver == null) {	String driverName = MetastoreConf.getVar(conf, ConfVars.CONNECTION_DRIVER);	if (driverName == null || driverName.equals("")) {	String msg = "JDBC driver for transaction db not set in configuration " + "file, need to set " + ConfVars.CONNECTION_DRIVER.getVarname();	LOG.error(msg);	throw new RuntimeException(msg);	}	try {	
going to load jdbc driver 

} catch (InstantiationException e) {	throw new RuntimeException("Unable to instantiate driver " + driverName + ", " + e.getMessage(), e);	} catch (IllegalAccessException e) {	throw new RuntimeException( "Unable to access driver " + driverName + ", " + e.getMessage(), e);	} catch (ClassNotFoundException e) {	throw new RuntimeException("Unable to find driver " + driverName + ", " + e.getMessage(), e);	}	connString = MetastoreConf.getVar(conf, ConfVars.CONNECTURLKEY);	}	try {	
connecting to transaction db with connection string 

========================= hive sample_1855 =========================

public static void setChildrenCollector(List<Operator<? extends OperatorDesc>> childOperators, Map<String, OutputCollector> outMap) {	if (childOperators == null) {	return;	}	for (Operator<? extends OperatorDesc> op : childOperators) {	if (op.getIsReduceSink()) {	String outputName = op.getReduceOutputName();	if (outMap.containsKey(outputName)) {	
setting output collector 

========================= hive sample_3887 =========================

private static boolean isCustomUDF(String udfName) {	if (udfName == null) {	return false;	}	FunctionInfo funcInfo;	try {	funcInfo = FunctionRegistry.getFunctionInfo(udfName);	} catch (SemanticException e) {	
failed to load 

} else if (child instanceof ExprNodeDynamicValueDesc) {	builder.setInputExpressionType(i, InputExpressionType.DYNAMICVALUE);	} else {	throw new HiveException("Cannot handle expression type: " + child.getClass().getSimpleName());	}	}	VectorExpressionDescriptor.Descriptor descriptor = builder.build();	Class<?> vclass = this.vMap.getVectorExpressionClass(udfClass, descriptor);	if (vclass == null) {	if (LOG.isDebugEnabled()) {	
no vector udf found for descriptor 

case INT: rawDecimal = HiveDecimal.create((Integer) scalar);	break;	case LONG: rawDecimal = HiveDecimal.create((Long) scalar);	break;	case DECIMAL: rawDecimal = (HiveDecimal) scalar;	break;	default: throw new HiveException("Unsupported type " + typename + " for cast to HiveDecimal");	}	if (rawDecimal == null) {	if (LOG.isDebugEnabled()) {	
casting constant scalar to hivedecimal resulted in null 

========================= hive sample_4211 =========================

private HCatFieldSchema getHCatFSFromPigFS(FieldSchema fSchema, HCatFieldSchema hcatFieldSchema, Schema pigSchema, HCatSchema tableSchema) throws FrontendException, HCatException {	if(hcatFieldSchema == null) {	if(LOG.isDebugEnabled()) {	
hcatfieldschema is null for fschema 

========================= hive sample_1013 =========================

private LlapDaemonIOMetrics(String displayName, String sessionId, int[] intervals) {	this.name = displayName;	this.sessionId = sessionId;	this.registry = new MetricsRegistry("LlapDaemonIORegistry");	this.registry.tag(ProcessName, MetricsUtils.METRICS_PROCESS_NAME).tag(SessionId, sessionId);	final int len = intervals == null ? 0 : intervals.length;	this.decodingTimes = new MutableQuantiles[len];	for (int i=0; i<len; i++) {	int interval = intervals[i];	
created interval s 

========================= hive sample_2146 =========================

boolean doMemCheck = false;	long effectiveThreshold = 0;	if (memoryMonitorInfo != null) {	effectiveThreshold = memoryMonitorInfo.getEffectiveThreshold(desc.getMaxMemoryAvailable());	if (!LlapDaemonInfo.INSTANCE.isLlap()) {	memoryMonitorInfo.setLlap(false);	}	if (memoryMonitorInfo.doMemoryMonitoring()) {	doMemCheck = true;	if (LOG.isInfoEnabled()) {	
memory monitoring for hash table loader enabled 

memoryMonitorInfo.setLlap(false);	}	if (memoryMonitorInfo.doMemoryMonitoring()) {	doMemCheck = true;	if (LOG.isInfoEnabled()) {	}	}	}	if (!doMemCheck) {	if (LOG.isInfoEnabled()) {	
not doing hash table memory monitoring 

input.start();	tezContext.getTezProcessorContext().waitForAnyInputReady( Collections.<Input> singletonList(input));	} catch (Exception e) {	throw new HiveException(e);	}	try {	KeyValueReader kvReader = (KeyValueReader) input.getReader();	Long keyCountObj = parentKeyCounts.get(pos);	long keyCount = (keyCountObj == null) ? -1 : keyCountObj.longValue();	VectorMapJoinFastTableContainer vectorMapJoinFastTableContainer = new VectorMapJoinFastTableContainer(desc, hconf, keyCount);	
loading hash table for input cachekey tablecontainer smalltablepos 

vectorMapJoinFastTableContainer.putRow((BytesWritable)kvReader.getCurrentKey(), (BytesWritable)kvReader.getCurrentValue());	numEntries++;	if (doMemCheck && (numEntries % memoryMonitorInfo.getMemoryCheckInterval() == 0)) {	final long estMemUsage = vectorMapJoinFastTableContainer.getEstimatedMemorySize();	if (estMemUsage > effectiveThreshold) {	String msg = "Hash table loading exceeded memory limits for input: " + inputName + " numEntries: " + numEntries + " estimatedMemoryUsage: " + estMemUsage + " effectiveThreshold: " + effectiveThreshold + " memoryMonitorInfo: " + memoryMonitorInfo;	LOG.error(msg);	throw new MapJoinMemoryExhaustionError(msg);	} else {	if (LOG.isInfoEnabled()) {	
checking hash table loader memory usage for input numentries estimatedmemoryusage effectivethreshold 

throw new MapJoinMemoryExhaustionError(msg);	} else {	if (LOG.isInfoEnabled()) {	}	}	}	}	vectorMapJoinFastTableContainer.seal();	mapJoinTables[pos] = vectorMapJoinFastTableContainer;	if (doMemCheck) {	
finished loading hash table for input cachekey numentries estimatedmemoryusage 

} else {	if (LOG.isInfoEnabled()) {	}	}	}	}	vectorMapJoinFastTableContainer.seal();	mapJoinTables[pos] = vectorMapJoinFastTableContainer;	if (doMemCheck) {	} else {	
finished loading hash table for input cachekey numentries 

========================= hive sample_4138 =========================

if (call.isA(SqlKind.CAST) && (call.operands.size() == 1) && SqlTypeUtil.equalSansNullability(dTFactory, call.getType(), call.operands.get(0).getType())) {	return args.get(0);	} else {	GenericUDF hiveUdf = SqlFunctionConverter.getHiveUDF(call.getOperator(), call.getType(), args.size());	if (hiveUdf == null) {	throw new RuntimeException("Cannot find UDF for " + call.getType() + " " + call.getOperator() + "[" + call.getOperator().getKind() + "]/" + args.size());	}	try {	gfDesc = ExprNodeGenericFuncDesc.newInstance(hiveUdf, args);	} catch (UDFArgumentException e) {	
failed to instantiate udf 

========================= hive sample_2845 =========================

execContext.setLocalWork(localWork);	MapredContext.init(true, new JobConf(jc));	MapredContext.get().setReporter(reporter);	mo.passExecContext(execContext);	mo.initializeLocalWork(jc);	mo.initializeMapOperator(jc);	mo.setReporter(rp);	if (localWork == null) {	return;	}	
initializing dummy operator 

public void close() {	if (!anyRow) {	
close called no row processed by map 

========================= hive sample_4587 =========================

if (bufferedRecords > 0) {	flushRecords();	}	clearColumnBuffers();	if (out != null) {	out.flush();	out.close();	out = null;	}	for (int i = 0; i < columnNumber; i++) {	
column plain total column value length compr total column value length 

init();	}	succeed = true;	} finally {	if (!succeed) {	if (in != null) {	try {	in.close();	} catch(IOException e) {	if (LOG != null && LOG.isDebugEnabled()) {	
exception in closing 

private void handleChecksumException(ChecksumException e) throws IOException {	if (conf.getBoolean("io.skip.checksum.errors", false)) {	
bad checksum at skipping entries 

private int nextKeyValueTolerateCorruptions() throws IOException {	long currentOffset = in.getPos();	int ret;	try {	ret = nextKeyBuffer();	this.currentValueBuffer();	} catch (IOException ioe) {	String msg = ioe.getMessage();	if (msg != null && msg.startsWith(BLOCK_MISSING_MESSAGE)) {	
re throwing block missing exception 

long currentOffset = in.getPos();	int ret;	try {	ret = nextKeyBuffer();	this.currentValueBuffer();	} catch (IOException ioe) {	String msg = ioe.getMessage();	if (msg != null && msg.startsWith(BLOCK_MISSING_MESSAGE)) {	throw ioe;	}	
ignoring ioexception in file after offset 

try {	ret = nextKeyBuffer();	this.currentValueBuffer();	} catch (IOException ioe) {	String msg = ioe.getMessage();	if (msg != null && msg.startsWith(BLOCK_MISSING_MESSAGE)) {	throw ioe;	}	ret = -1;	} catch (Throwable t) {	
ignoring unknown error in after offset 

========================= hive sample_3612 =========================

public JobExecutionStatus getState() {	SparkJobInfo sparkJobInfo = getJobInfo();	if (sparkJobInfo == null && future.isDone()) {	try {	future.get();	} catch (Exception e) {	
failed to run job 

========================= hive sample_4569 =========================

public static RelNode convertOpTree(RelNode rel, List<FieldSchema> resultSchema, boolean alignColumns) throws CalciteSemanticException {	RelNode newTopNode = rel;	if (LOG.isDebugEnabled()) {	
original plan for planmodifier 

public static RelNode convertOpTree(RelNode rel, List<FieldSchema> resultSchema, boolean alignColumns) throws CalciteSemanticException {	RelNode newTopNode = rel;	if (LOG.isDebugEnabled()) {	}	if (!(newTopNode instanceof Project) && !(newTopNode instanceof Sort)) {	newTopNode = introduceDerivedTable(newTopNode);	if (LOG.isDebugEnabled()) {	
plan after top level introducederivedtable 

RelNode newTopNode = rel;	if (LOG.isDebugEnabled()) {	}	if (!(newTopNode instanceof Project) && !(newTopNode instanceof Sort)) {	newTopNode = introduceDerivedTable(newTopNode);	if (LOG.isDebugEnabled()) {	}	}	convertOpTree(newTopNode, (RelNode) null);	if (LOG.isDebugEnabled()) {	
plan after nested convertoptree 

if (LOG.isDebugEnabled()) {	}	}	convertOpTree(newTopNode, (RelNode) null);	if (LOG.isDebugEnabled()) {	}	if (alignColumns) {	HiveRelColumnsAlignment propagator = new HiveRelColumnsAlignment( HiveRelFactories.HIVE_BUILDER.create(newTopNode.getCluster(), null));	newTopNode = propagator.align(newTopNode);	if (LOG.isDebugEnabled()) {	
plan after propagating order 

}	if (alignColumns) {	HiveRelColumnsAlignment propagator = new HiveRelColumnsAlignment( HiveRelFactories.HIVE_BUILDER.create(newTopNode.getCluster(), null));	newTopNode = propagator.align(newTopNode);	if (LOG.isDebugEnabled()) {	}	}	Pair<RelNode, RelNode> topSelparentPair = HiveCalciteUtil.getTopLevelSelect(newTopNode);	PlanModifierUtil.fixTopOBSchema(newTopNode, topSelparentPair, resultSchema, true);	if (LOG.isDebugEnabled()) {	
plan after fixtopobschema 

if (LOG.isDebugEnabled()) {	}	}	Pair<RelNode, RelNode> topSelparentPair = HiveCalciteUtil.getTopLevelSelect(newTopNode);	PlanModifierUtil.fixTopOBSchema(newTopNode, topSelparentPair, resultSchema, true);	if (LOG.isDebugEnabled()) {	}	topSelparentPair = HiveCalciteUtil.getTopLevelSelect(newTopNode);	newTopNode = renameTopLevelSelectInResultSchema(newTopNode, topSelparentPair, resultSchema);	if (LOG.isDebugEnabled()) {	
final plan after modifier 

========================= hive sample_2844 =========================

if (isTagged) {	int size = keyWritable.getSize() - 1;	tag = keyWritable.get()[size];	keyWritable = new BytesWritable(keyWritable.getBytes(), size);	keyWritable.setSize(size);	}	if (!keyWritable.equals(groupKey)) {	if (groupKey == null) {	groupKey = new BytesWritable();	} else {	
end group 

groupKey = new BytesWritable();	} else {	reducer.endGroup();	}	try {	keyObject = inputKeyDeserializer.deserialize(keyWritable);	} catch (Exception e) {	throw new HiveException( "Hive Runtime Error: Unable to deserialize reduce input key from " + Utilities.formatBinaryString(keyWritable.get(), 0, keyWritable.getSize()) + " with properties " + keyTableDesc.getProperties(), e);	}	groupKey.set(keyWritable.get(), 0, keyWritable.getSize());	
start group 

public void close() {	if (!anyRow) {	
close called without any rows processed 

try {	if (vectorized) {	if (batch.size > 0) {	if (handleGroupKey) {	reducer.setNextVectorBatchGroupStatus(/* isLastGroupBatch */ true);	}	forwardBatch(/* resetValueColumnsOnly */ false);	}	} else {	if (groupKey != null) {	
end group 

reducer.close(abort);	if (localWork != null) {	for (Operator<? extends OperatorDesc> dummyOp : localWork.getDummyParentOp()) {	dummyOp.close(abort);	}	}	ReportStats rps = new ReportStats(rp, jc);	reducer.preorderMap(rps);	} catch (Exception e) {	if (!abort) {	
hit error while closing operators failing tree 

========================= hive sample_4586 =========================

public HCatTable deserializeTable(String hcatTableStringRep) throws HCatException {	try {	Table table = new Table();	new TDeserializer(new TJSONProtocol.Factory()).deserialize(table, hcatTableStringRep, "UTF-8");	return new HCatTable(table);	}	catch(TException exception) {	
could not de serialize from 

public HCatPartition deserializePartition(String hcatPartitionStringRep) throws HCatException {	try {	Partition partition = new Partition();	new TDeserializer(new TJSONProtocol.Factory()).deserialize(partition, hcatPartitionStringRep, "UTF-8");	return new HCatPartition(null, partition);	}	catch(TException exception) {	
could not de serialize partition from 

========================= hive sample_899 =========================

if (!(new File(path)).exists()) {	throw new Exception(path + " does not exist. Potentially incorrect version in the metastore VERSION table");	}	try ( BufferedReader reader = new BufferedReader(new FileReader(path));	){	String line = null;	while ((line = reader.readLine()) != null) {	if (sp.isNestedScript(line)) {	String subScript = null;	subScript = sp.getScriptName(line);	
schema subscript found 

line    = line.replaceAll("\\(", " ");	line    = line.replaceAll("IF NOT EXISTS ", "");	line    = line.replaceAll("`","");	line    = line.replaceAll("'","");	line    = line.replaceAll("\"","");	matcher = regexp.matcher(line);	if (matcher.find()) {	String table = matcher.group(groupNo);	if (dbType.equals("derby")) table  = table.replaceAll("APP\\.","");	tableList.add(table.toLowerCase());	
found table in the schema 

public void runBeeLine(String sqlScriptFile) throws IOException {	CommandBuilder builder = new CommandBuilder(hiveConf, url, driver, userName, passWord, sqlScriptFile);	try (BeeLine beeLine = new BeeLine()) {	if (!verbose) {	beeLine.setOutputStream(new PrintStream(new NullOutputStream()));	beeLine.getOpts().setSilent(true);	}	beeLine.getOpts().setAllowMultiLineCommand(false);	beeLine.getOpts().setIsolation("TRANSACTION_READ_COMMITTED");	beeLine.getOpts().setEntireLineAsCommand(true);	
going to run command 

========================= hive sample_1507 =========================

Iterator<String> afterIterator = msg.getPtnObjAfter().getValuesIterator();	for (FieldSchema fs : tblObj.getPartitionKeys()) {	oldPartSpec.put(fs.getName(), beforeIterator.next());	newPartSpec.put(fs.getName(), afterIterator.next());	}	} catch (Exception e) {	throw (e instanceof SemanticException) ? (SemanticException) e : new SemanticException("Error reading message members", e);	}	RenamePartitionDesc renamePtnDesc = new RenamePartitionDesc( tableName, oldPartSpec, newPartSpec, context.eventOnlyReplicationSpec());	Task<DDLWork> renamePtnTask = TaskFactory.get( new DDLWork(readEntitySet, writeEntitySet, renamePtnDesc), context.hiveConf);	
added rename ptn task 

========================= hive sample_3490 =========================

private void publishStats() throws HiveException {	boolean isStatsReliable = conf.isStatsReliable();	StatsPublisher statsPublisher = Utilities.getStatsPublisher(jc);	StatsCollectionContext sc = new StatsCollectionContext(jc);	sc.setStatsTmpDir(conf.getTmpStatsDir());	if (!statsPublisher.connect(sc)) {	if (LOG.isInfoEnabled()) {	
statspublishing error cannot connect to database 

String key = prefix.endsWith(Path.SEPARATOR) ? prefix : prefix + Path.SEPARATOR;	for(String statType : stats.get(pspecs).getStoredStats()) {	statsToPublish.put(statType, Long.toString(stats.get(pspecs).getStat(statType)));	}	if (!statsPublisher.publishStat(key, statsToPublish)) {	if (isStatsReliable) {	throw new HiveException(ErrorMsg.STATSPUBLISHER_PUBLISHING_ERROR.getErrorCodedMsg());	}	}	if (LOG.isInfoEnabled()) {	
publishing 

========================= hive sample_3869 =========================

stmtId = conf.getStatementId();	}	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	Utilities.FILE_OP_LOGGER.trace("new FSPaths for " + numFiles + " files, dynParts = " + bDynParts + ": tmpPath " + tmpPath + ", task path " + taskOutputTempPath + " (spec path " + specPath + ")"/*, new Exception()*/);	}	outPaths = new Path[numFiles];	finalPaths = new Path[numFiles];	outWriters = new RecordWriter[numFiles];	updaters = new RecordUpdater[numFiles];	if (LOG.isDebugEnabled()) {	
created slots for 

private void commitOneOutPath(int idx, FileSystem fs, List<Path> commitPaths) throws IOException, HiveException {	if ((bDynParts || isSkewedStoredAsSubDirectories) && !fs.exists(finalPaths[idx].getParent())) {	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
commit making path for dyn skew 

private void commitOneOutPath(int idx, FileSystem fs, List<Path> commitPaths) throws IOException, HiveException {	if ((bDynParts || isSkewedStoredAsSubDirectories) && !fs.exists(finalPaths[idx].getParent())) {	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	}	FileUtils.mkdir(fs, finalPaths[idx].getParent(), hconf);	}	Operation acidOp = conf.getWriteType();	boolean needToRename = outPaths[idx] != null && ((acidOp != Operation.UPDATE && acidOp != Operation.DELETE) || fs.exists(outPaths[idx]));	if (needToRename && outPaths[idx] != null) {	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
committing to 

boolean needToRename = outPaths[idx] != null && ((acidOp != Operation.UPDATE && acidOp != Operation.DELETE) || fs.exists(outPaths[idx]));	if (needToRename && outPaths[idx] != null) {	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	}	if (isMmTable) {	assert outPaths[idx].equals(finalPaths[idx]);	commitPaths.add(outPaths[idx]);	} else if (!fs.rename(outPaths[idx], finalPaths[idx])) {	FileStatus fileStatus = FileUtils.getFileStatusOrNull(fs, finalPaths[idx]);	if (fileStatus != null) {	
target path with a size exists trying to delete it 

try {	FileSystem fpfs = finalPath.getFileSystem(hconf);	if (fpfs.exists(finalPath)) throw new RuntimeException(finalPath + " already exists");	} catch (IOException e) {	throw new RuntimeException(e);	}	finalPaths[filesIdx] = finalPath;	outPaths[filesIdx] = finalPath;	}	if (LOG.isInfoEnabled()) {	
final path fs 

FileSystem fpfs = finalPath.getFileSystem(hconf);	if (fpfs.exists(finalPath)) throw new RuntimeException(finalPath + " already exists");	} catch (IOException e) {	throw new RuntimeException(e);	}	finalPaths[filesIdx] = finalPath;	outPaths[filesIdx] = finalPath;	}	if (LOG.isInfoEnabled()) {	if (LOG.isInfoEnabled() && !isMmTable) {	
writing to temp file fs 

isUnionDp = (dpCtx != null);	if (conf.isMmTable() || isUnionDp) {	specPath = conf.getParentDir();	unionPath = conf.getDirName().getName();	} else {	specPath = conf.getDirName();	unionPath = null;	}	}	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
setting up fsop with and 

isCompressed = conf.getCompressed();	parent = Utilities.toTempPath(conf.getDirName());	statsFromRecordWriter = new boolean[numFiles];	serializer = (Serializer) conf.getTableInfo().getDeserializerClass().newInstance();	serializer.initialize(unsetNestedColumnPaths(hconf), conf.getTableInfo().getProperties());	outputClass = serializer.getSerializedClass();	destTablePath = conf.getDestPath();	isInsertOverwrite = conf.getInsertOverwrite();	counterGroup = HiveConf.getVar(hconf, HiveConf.ConfVars.HIVECOUNTERGROUP);	if (LOG.isInfoEnabled()) {	
using serializer and formatter with compression 

}	if (dpCtx != null) {	dpSetup();	}	if (lbCtx != null) {	lbSetup();	}	if (!bDynParts) {	fsp = new FSPaths(specPath, conf.isMmTable());	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
creating new paths from ctor childspec tmppath task path 

protected void createBucketFiles(FSPaths fsp) throws HiveException {	try {	int filesIdx = 0;	Set<Integer> seenBuckets = new HashSet<Integer>();	for (int idx = 0; idx < totalFiles; idx++) {	if (this.getExecContext() != null && this.getExecContext().getFileId() != null) {	if (LOG.isInfoEnabled()) {	
replace taskid from execcontext 

protected void createBucketFiles(FSPaths fsp) throws HiveException {	try {	int filesIdx = 0;	Set<Integer> seenBuckets = new HashSet<Integer>();	for (int idx = 0; idx < totalFiles; idx++) {	if (this.getExecContext() != null && this.getExecContext().getFileId() != null) {	if (LOG.isInfoEnabled()) {	}	taskId = Utilities.replaceTaskIdFromFilename(taskId, this.getExecContext().getFileId());	if (LOG.isInfoEnabled()) {	
new taskid fs 

protected void createBucketForFileIdx(FSPaths fsp, int filesIdx) throws HiveException {	try {	fsp.initializeBucketPaths(filesIdx, taskId, isNativeTable(), isSkewedStoredAsSubDirectories);	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
createbucketforfileidx final path out path spec path tmp path task 

protected void createBucketForFileIdx(FSPaths fsp, int filesIdx) throws HiveException {	try {	fsp.initializeBucketPaths(filesIdx, taskId, isNativeTable(), isSkewedStoredAsSubDirectories);	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	}	if (LOG.isInfoEnabled()) {	
new final path fs 

if (LOG.isInfoEnabled()) {	}	if (isNativeTable() && !conf.isMmTable()) {	autoDelete = fs.deleteOnExit(fsp.outPaths[filesIdx]);	}	updateDPCounters(fsp, filesIdx);	Utilities.copyTableJobPropertiesToConf(conf.getTableInfo(), jc);	if (conf.getWriteType() == AcidUtils.Operation.NOT_ACID || conf.isMmTable()) {	Path outPath = fsp.outPaths[filesIdx];	if (conf.isMmTable() && !FileUtils.mkdir(fs, outPath.getParent(), hconf)) {	
unable to create directory with inheritperms 

if (isUnionDp) {	dpStagingDir = dpStagingDir.getParent();	}	if (isInsertOverwrite) {	createDpDir(dpStagingDir);	} else {	createDpDirCheckSrc(dpStagingDir, destPartPath);	}	}	} catch (IOException e) {	
skipping to increment created dynamic partitions counter exception 

fpaths.addToStat(StatsSetupConst.RAW_DATA_SIZE, stats.getRawDataSize());	}	fpaths.addToStat(StatsSetupConst.ROW_COUNT, 1);	}	if ((++numRows == cntr) && LOG.isInfoEnabled()) {	cntr = logEveryNRows == 0 ? cntr * 10 : numRows + logEveryNRows;	if (cntr < 0 || numRows < 0) {	cntr = 0;	numRows = 1;	}	
records written 

}	if (fpaths.updaters[writerOffset] == null) {	int writerId = -1;	if(!isBucketed) {	assert !multiFileSpray;	assert writerOffset == 0;	writerId = Integer.parseInt(Utilities.getTaskIdFromFilename(taskId));	}	fpaths.updaters[writerOffset] = HiveFileFormatUtils.getAcidRecordUpdater( jc, conf.getTableInfo(), writerId >= 0 ? writerId : bucketNum, conf, fpaths.outPaths[writerOffset], rowInspector, reporter, 0);	if (LOG.isDebugEnabled()) {	
created updater for bucket number using file 

private FSPaths createNewPaths(String dirName) throws HiveException {	FSPaths fsp2 = new FSPaths(specPath, conf.isMmTable());	fsp2.configureDynPartPath(dirName, !conf.isMmTable() && isUnionDp ? unionPath : null);	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
creating new paths for childspec tmppath task path 

public void closeOp(boolean abort) throws HiveException {	row_count.set(numRows);	
records written 

if ((conf != null) && isNativeTable()) {	Path specPath = conf.getDirName();	String unionSuffix = null;	DynamicPartitionCtx dpCtx = conf.getDynPartCtx();	ListBucketingCtx lbCtx = conf.getLbCtx();	if (conf.isLinkedFileSink() && (dpCtx != null || conf.isMmTable())) {	specPath = conf.getParentDir();	unionSuffix = conf.getDirName().getName();	}	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
jobcloseop using specpath 

createHiveOutputFormat(job);	} catch (HiveException ex) {	logOutputFormatError(job, ex);	throw new IOException(ex);	}	}	if (conf.getTableInfo().isNonNative()) {	try {	hiveOutputFormat.checkOutputSpecs(ignored, job);	} catch (NoSuchMethodError e) {	
hiveoutputformat should implement checkoutputspecs method 

private void publishStats() throws HiveException {	boolean isStatsReliable = conf.isStatsReliable();	StatsPublisher statsPublisher = Utilities.getStatsPublisher(jc);	if (statsPublisher == null) {	
statspublishing error statspublisher is not initialized 

StatsPublisher statsPublisher = Utilities.getStatsPublisher(jc);	if (statsPublisher == null) {	if (isStatsReliable) {	throw new HiveException(ErrorMsg.STATSPUBLISHER_NOT_OBTAINED.getErrorCodedMsg());	}	return;	}	StatsCollectionContext sContext = new StatsCollectionContext(hconf);	sContext.setStatsTmpDir(conf.getTmpStatsDir());	if (!statsPublisher.connect(sContext)) {	
statspublishing error cannot connect to database 

if (conf.getDpSortState().equals(DPSortState.PARTITION_BUCKET_SORTED)) {	String taskID = Utilities.getTaskIdFromFilename(fspKey);	fspKey = fspKey.split(taskID)[0];	}	String[] split = splitKey(fspKey);	String dpSpec = split[0];	String prefix = conf.getTableInfo().getTableName().toLowerCase();	prefix = Utilities.join(prefix, spSpec, dpSpec);	prefix = prefix.endsWith(Path.SEPARATOR) ? prefix : prefix + Path.SEPARATOR;	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
prefix for stats from 

String prefix = conf.getTableInfo().getTableName().toLowerCase();	prefix = Utilities.join(prefix, spSpec, dpSpec);	prefix = prefix.endsWith(Path.SEPARATOR) ? prefix : prefix + Path.SEPARATOR;	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	}	Map<String, String> statsToPublish = new HashMap<String, String>();	for (String statType : fspValue.getStoredStats()) {	statsToPublish.put(statType, Long.toString(fspValue.stat.getStat(statType)));	}	if (!statsPublisher.publishStat(prefix, statsToPublish)) {	
failed to publish stats 

statsToPublish.put(statType, Long.toString(fspValue.stat.getStat(statType)));	}	if (!statsPublisher.publishStat(prefix, statsToPublish)) {	if (isStatsReliable) {	throw new HiveException(ErrorMsg.STATSPUBLISHER_PUBLISHING_ERROR.getErrorCodedMsg());	}	}	}	sContext.setIndexForTezUnion(this.getIndexForTezUnion());	if (!statsPublisher.closeConnection(sContext)) {	
failed to close stats 

========================= hive sample_3886 =========================

public void start() throws IOException {	
starting llapoutputformatservice 

int portFromConf = HiveConf.getIntVar(conf, HiveConf.ConfVars.LLAP_DAEMON_OUTPUT_SERVICE_PORT);	int sendBufferSize = HiveConf.getIntVar(conf, HiveConf.ConfVars.LLAP_DAEMON_OUTPUT_SERVICE_SEND_BUFFER_SIZE);	eventLoopGroup = new NioEventLoopGroup();	serverBootstrap = new ServerBootstrap();	serverBootstrap.group(eventLoopGroup);	serverBootstrap.channel(NioServerSocketChannel.class);	serverBootstrap.childHandler(new LlapOutputFormatServiceChannelHandler(sendBufferSize));	try {	listeningChannelFuture = serverBootstrap.bind(portFromConf).sync();	this.port = ((InetSocketAddress) listeningChannelFuture.channel().localAddress()).getPort();	
llapoutputformatservice binding to port with send buffer size 

public void stop() throws IOException, InterruptedException {	
stopping llapoutputformatservice 

public void stop() throws IOException, InterruptedException {	if (listeningChannelFuture != null) {	listeningChannelFuture.channel().close().sync();	listeningChannelFuture = null;	} else {	
llapoutputformatservice does not appear to have a listening port to close 

RecordWriter<?, ?> writer = null;	synchronized (lock) {	long startTime = System.nanoTime();	boolean isFirst = true;	while ((writer = writers.get(id)) == null) {	String error = errors.remove(id);	if (error != null) {	throw new IOException(error);	}	if (isFirst) {	
waiting for writer for 

}	if (isFirst) {	isFirst = false;	}	if (((System.nanoTime() - startTime) / 1000000) > writerTimeoutMs) {	throw new IOException("The writer for " + id + " has timed out after " + writerTimeoutMs + "ms");	}	lock.wait(writerTimeoutMs);	}	}	
returning writer for 

private void registerReader(ChannelHandlerContext ctx, String id, byte[] tokenBytes) {	if (sm != null) {	try {	sm.verifyToken(tokenBytes);	} catch (SecurityException | IOException ex) {	failChannel(ctx, id, ex.getMessage());	return;	}	}	
registering socket for 

public void operationComplete(ChannelFuture future) throws Exception {	RecordWriter<?, ?> writer = null;	synchronized (INSTANCE) {	writer = writers.remove(id);	}	if (writer == null) {	
did not find a writer for id 

========================= hive sample_2794 =========================

public boolean arePathsOnSameEncryptionZone(Path path1, Path path2, HadoopShims.HdfsEncryptionShim encryptionShim2) throws IOException {	if (!(encryptionShim2 instanceof Hadoop23Shims.HdfsEncryptionShim)) {	
encryptionshim for is of unexpected type assuming is on the same encryptionzone as 

========================= hive sample_1442 =========================

try {	Writable writableRow;	if (batch.selectedInUse) {	int selected[] = batch.selected;	for (int logical = 0 ; logical < batch.size; logical++) {	int batchIndex = selected[logical];	vectorExtractRow.extractRow(batch, batchIndex, singleRow);	writableRow = serializer.serialize(singleRow, rowInspector);	writableRow.write(buffer);	if (buffer.getLength() > MAX_SIZE) {	
disabling am events buffer size too large 

buffer = null;	break;	}	}	} else {	for (int batchIndex = 0 ; batchIndex < batch.size; batchIndex++) {	vectorExtractRow.extractRow(batch, batchIndex, singleRow);	writableRow = serializer.serialize(singleRow, rowInspector);	writableRow.write(buffer);	if (buffer.getLength() > MAX_SIZE) {	
disabling am events buffer size too large 

========================= hive sample_4210 =========================

public static void setUp() throws Exception {	
setting up output service 

public static void setUp() throws Exception {	Configuration conf = new Configuration();	HiveConf.setIntVar(conf, HiveConf.ConfVars.LLAP_DAEMON_OUTPUT_SERVICE_PORT, 0);	LlapOutputFormatService.initializeAndStart(conf, null);	service = LlapOutputFormatService.get();	LlapProxy.setDaemon(true);	
output service up 

public static void tearDown() throws IOException, InterruptedException {	
tearing down service 

public static void tearDown() throws IOException, InterruptedException {	service.stop();	
tearing down complete 

public void testValues() throws Exception {	JobConf job = new JobConf();	for (int k = 0; k < 5; ++k) {	String id = "foobar" + k;	job.set(LlapOutputFormat.LLAP_OF_ID_KEY, id);	LlapOutputFormat format = new LlapOutputFormat();	HiveConf conf = new HiveConf();	Socket socket = new Socket("localhost", service.getPort());	
socket connected 

for (int k = 0; k < 5; ++k) {	String id = "foobar" + k;	job.set(LlapOutputFormat.LLAP_OF_ID_KEY, id);	LlapOutputFormat format = new LlapOutputFormat();	HiveConf conf = new HiveConf();	Socket socket = new Socket("localhost", service.getPort());	OutputStream socketStream = socket.getOutputStream();	LlapOutputSocketInitMessage.newBuilder() .setFragmentId(id).build().writeDelimitedTo(socketStream);	socketStream.flush();	Thread.sleep(3000);	
data written 

job.set(LlapOutputFormat.LLAP_OF_ID_KEY, id);	LlapOutputFormat format = new LlapOutputFormat();	HiveConf conf = new HiveConf();	Socket socket = new Socket("localhost", service.getPort());	OutputStream socketStream = socket.getOutputStream();	LlapOutputSocketInitMessage.newBuilder() .setFragmentId(id).build().writeDelimitedTo(socketStream);	socketStream.flush();	Thread.sleep(3000);	RecordWriter<NullWritable, Text> writer = format.getRecordWriter(null, job, null, null);	Text text = new Text();	
have record writer 

Thread.sleep(3000);	RecordWriter<NullWritable, Text> writer = format.getRecordWriter(null, job, null, null);	Text text = new Text();	for (int i = 0; i < 10; ++i) {	text.set(""+i);	writer.write(NullWritable.get(),text);	}	writer.close(null);	InputStream in = socket.getInputStream();	LlapBaseRecordReader reader = new LlapBaseRecordReader( in, null, Text.class, job, null, null);	
have record reader 

public void testBadClientMessage() throws Exception {	JobConf job = new JobConf();	String id = "foobar";	job.set(LlapOutputFormat.LLAP_OF_ID_KEY, id);	LlapOutputFormat format = new LlapOutputFormat();	Socket socket = new Socket("localhost", service.getPort());	
socket connected 

JobConf job = new JobConf();	String id = "foobar";	job.set(LlapOutputFormat.LLAP_OF_ID_KEY, id);	LlapOutputFormat format = new LlapOutputFormat();	Socket socket = new Socket("localhost", service.getPort());	OutputStream socketStream = socket.getOutputStream();	LlapOutputSocketInitMessage.newBuilder() .setFragmentId(id).build().writeDelimitedTo(socketStream);	LlapOutputSocketInitMessage.newBuilder() .setFragmentId(id).build().writeDelimitedTo(socketStream);	socketStream.flush();	Thread.sleep(3000);	
data written 

========================= hive sample_2405 =========================

public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx, Object... nodeOutputs) throws SemanticException {	GroupByOperator mGby = (GroupByOperator) stack.get(stack.size() - 3);	ReduceSinkOperator rs = (ReduceSinkOperator) stack.get(stack.size() - 2);	GroupByOperator rGby = (GroupByOperator) stack.get(stack.size() - 1);	int applicableDistPos = checkCountDistinct(mGby, rs, rGby);	if (applicableDistPos != -1) {	
trigger count distinct rewrite 

========================= hive sample_2956 =========================

String desiredCounter = currentTrigger.getExpression().getCounterLimit().getName();	if (currentCounters.containsKey(desiredCounter)) {	long currentCounterValue = currentCounters.get(desiredCounter);	if (currentTrigger.apply(currentCounterValue)) {	String queryId = sessionState.getWmContext().getQueryId();	if (violatedSessions.containsKey(sessionState)) {	Trigger existingTrigger = violatedSessions.get(sessionState);	if (existingTrigger.getAction().getType().equals(Action.Type.MOVE_TO_POOL) && currentTrigger.getAction().getType().equals(Action.Type.KILL_QUERY)) {	currentTrigger.setViolationMsg("Trigger " + currentTrigger + " violated. Current value: " + currentCounterValue);	violatedSessions.put(sessionState, currentTrigger);	
kill trigger replacing move for query 

if (currentCounters.containsKey(desiredCounter)) {	long currentCounterValue = currentCounters.get(desiredCounter);	if (currentTrigger.apply(currentCounterValue)) {	String queryId = sessionState.getWmContext().getQueryId();	if (violatedSessions.containsKey(sessionState)) {	Trigger existingTrigger = violatedSessions.get(sessionState);	if (existingTrigger.getAction().getType().equals(Action.Type.MOVE_TO_POOL) && currentTrigger.getAction().getType().equals(Action.Type.KILL_QUERY)) {	currentTrigger.setViolationMsg("Trigger " + currentTrigger + " violated. Current value: " + currentCounterValue);	violatedSessions.put(sessionState, currentTrigger);	} else {	
conflicting move triggers and choosing the first move trigger 

}	} else {	currentTrigger.setViolationMsg("Trigger " + currentTrigger + " violated. Current value: " + currentCounterValue);	violatedSessions.put(sessionState, currentTrigger);	}	}	}	}	Trigger chosenTrigger = violatedSessions.get(sessionState);	if (chosenTrigger != null) {	
query applying action 

}	Trigger chosenTrigger = violatedSessions.get(sessionState);	if (chosenTrigger != null) {	}	}	}	if (!violatedSessions.isEmpty()) {	triggerActionHandler.applyAction(violatedSessions);	}	} catch (Throwable t) {	
caught exception 

========================= hive sample_3994 =========================

private void writeMetaData(PartitionIterable partitions) throws SemanticException {	try {	EximUtil.createExportDump( paths.exportFileSystem, paths.metaDataExportFile(), tableSpec == null ? null : tableSpec.tableHandle, partitions, replicationSpec, conf);	
metadata file written into 

========================= hive sample_3428 =========================

public void run() {	try {	for (String dbName : store.getAllDatabases()) {	for (Table mv : store.getTableObjectsByName(dbName, store.getTables(dbName, null, TableType.MATERIALIZED_VIEW))) {	addMaterializedView(mv, ImmutableSet.copyOf(mv.getCreationMetadata().keySet()), OpType.LOAD);	}	}	
initialized materializations invalidation cache 

public void run() {	try {	for (String dbName : store.getAllDatabases()) {	for (Table mv : store.getTableObjectsByName(dbName, store.getTables(dbName, null, TableType.MATERIALIZED_VIEW))) {	addMaterializedView(mv, ImmutableSet.copyOf(mv.getCreationMetadata().keySet()), OpType.LOAD);	}	}	} catch (Exception e) {	
problem connecting to the metastore when initializing the view registry 

modificationsTree.add(lastModificationBeforeCreation);	if (opType == OpType.LOAD) {	try {	String[] names =  qNameTableUsed.split("\\.");	BasicTxnInfo e2 = txnStore.getFirstCompletedTransactionForTableAfterCommit( names[0], names[1], lastModificationBeforeCreation.id);	if (!e2.isIsnull()) {	modificationsTree.add(new TableModificationKey(e2.getId(), e2.getTime()));	continue;	}	} catch (MetaException ex) {	
materialized view ignored error loading view into invalidation cache 

return;	}	}	}	if (opType == OpType.CREATE || opType == OpType.ALTER) {	cq.put(materializedViewTable.getTableName(), new MaterializationInvalidationInfo(materializedViewTable, tablesUsed));	} else {	cq.putIfAbsent(materializedViewTable.getTableName(), new MaterializationInvalidationInfo(materializedViewTable, tablesUsed));	}	if (LOG.isDebugEnabled()) {	
cached materialized view for rewriting in invalidation cache 

public void notifyTableModification(String dbName, String tableName, long eventId, long newModificationTime) {	if (LOG.isDebugEnabled()) {	
notification for table in database received id time 

public Map<String, Materialization> getMaterializationInvalidationInfo( String dbName, List<String> materializationNames) {	if (materializations.get(dbName) != null) {	ImmutableMap.Builder<String, Materialization> m = ImmutableMap.builder();	for (String materializationName : materializationNames) {	MaterializationInvalidationInfo materialization = materializations.get(dbName).get(materializationName);	if (materialization == null) {	
materialization skipped as there is no information in the invalidation cache about it 

if (invalidationTime < currentInvalidationTime) {	modified = materialization.compareAndSetInvalidationTime(currentInvalidationTime, invalidationTime);	} else {	modified = true;	}	}	m.put(materializationName, materialization);	}	Map<String, Materialization> result = m.build();	if (LOG.isDebugEnabled()) {	
retrieved the following materializations from the invalidation cache 

========================= hive sample_1946 =========================

rejectedException = e;	}	}	if (rejectedException != null) {	if (lastKillTimeMs != null && (clock.getTime() - lastKillTimeMs) < PREEMPTION_KILL_GRACE_MS) {	synchronized (lock) {	lock.wait(PREEMPTION_KILL_GRACE_SLEEP_MS);	}	} else {	if (LOG.isDebugEnabled() && lastKillTimeMs != null) {	
grace period ended for the previous kill preemtping more tasks 

if (LOG.isDebugEnabled() && lastKillTimeMs != null) {	}	if (handleScheduleAttemptedRejection(task)) {	lastKillTimeMs = clock.getTime();	}	}	}	}	} catch (InterruptedException e) {	if (isShutdown.get()) {	
thread has been interrupted after shutdown 

}	if (handleScheduleAttemptedRejection(task)) {	lastKillTimeMs = clock.getTime();	}	}	}	}	} catch (InterruptedException e) {	if (isShutdown.get()) {	} else {	
interrupted without shutdown 

public void onSuccess(Object result) {	if (isShutdown.get()) {	
wait queue scheduler worker exited with success 

public void onSuccess(Object result) {	if (isShutdown.get()) {	} else {	
wait queue scheduler worker exited with success 

public void onFailure(Throwable t) {	
wait queue scheduler worker exited with failure 

evictedTask = waitQueue.offer(taskWrapper, maxParallelExecutors - runningFragmentCount.get());	if (evictedTask == null || !evictedTask.equals(taskWrapper)) {	knownTasks.put(taskWrapper.getRequestId(), taskWrapper);	taskWrapper.setIsInWaitQueue(true);	task.setWmCountersQueued();	if (LOG.isDebugEnabled()) {	LOG.debug("{} added to wait queue. Current wait queue size={}", task.getRequestId(), waitQueue.size());	}	result = evictedTask == null ? SubmissionState.ACCEPTED : SubmissionState.EVICTED_OTHER;	if (LOG.isDebugEnabled() && evictedTask != null) {	
eviction 

result = evictedTask == null ? SubmissionState.ACCEPTED : SubmissionState.EVICTED_OTHER;	if (LOG.isDebugEnabled() && evictedTask != null) {	}	} else {	if (LOG.isInfoEnabled()) {	LOG.info( "wait queue full, size={}. numSlotsAvailable={}, runningFragmentCount={}. {} not added", waitQueue.size(), numSlotsAvailable.get(), runningFragmentCount.get(), task.getRequestId());	}	evictedTask.getTaskRunnerCallable().killTask();	result = SubmissionState.REJECTED;	if (LOG.isDebugEnabled()) {	
is as wait queue is full 

if (LOG.isDebugEnabled()) {	}	if (metrics != null) {	metrics.incrTotalRejectedRequests();	}	return result;	}	boolean stateChanged = !taskWrapper.maybeRegisterForFinishedStateNotifications(canFinish);	if (stateChanged) {	if (LOG.isDebugEnabled()) {	
finishable state of updated to during registration for state updates 

return result;	}	boolean stateChanged = !taskWrapper.maybeRegisterForFinishedStateNotifications(canFinish);	if (stateChanged) {	if (LOG.isDebugEnabled()) {	}	finishableStateUpdated(taskWrapper, !canFinish);	}	}	if (LOG.isDebugEnabled()) {	
wait queue 

if (stateChanged) {	if (LOG.isDebugEnabled()) {	}	finishableStateUpdated(taskWrapper, !canFinish);	}	}	if (LOG.isDebugEnabled()) {	}	if (evictedTask != null) {	if (LOG.isInfoEnabled()) {	
evicted from wait queue in favor of because of lower priority 

public boolean updateFragment(String fragmentId, boolean isGuaranteed) {	synchronized (lock) {	TaskWrapper taskWrapper = knownTasks.get(fragmentId);	if (taskWrapper == null) {	
fragment not found 

public boolean updateFragment(String fragmentId, boolean isGuaranteed) {	synchronized (lock) {	TaskWrapper taskWrapper = knownTasks.get(fragmentId);	if (taskWrapper == null) {	return false;	}	if (taskWrapper.isGuaranteed() == isGuaranteed) return true;	
fragment guaranteed state changed to finishable in wait queue in preemption queue 

private void forceReinsertIntoQueue(TaskWrapper taskWrapper, boolean isRemoved) {	if (!isRemoved) {	
failed to remove from waitqueue 

public void killFragment(String fragmentId) {	synchronized (lock) {	TaskWrapper taskWrapper = knownTasks.remove(fragmentId);	if (taskWrapper != null) {	if (taskWrapper.isInWaitQueue()) {	if (LOG.isDebugEnabled()) {	
removing from waitqueue 

taskWrapper.setIsInWaitQueue(false);	taskWrapper.getTaskRunnerCallable().setWmCountersDone();	if (waitQueue.remove(taskWrapper)) {	if (metrics != null) {	metrics.setExecutorNumQueuedRequests(waitQueue.size());	}	}	}	if (taskWrapper.isInPreemptionQueue()) {	if (LOG.isDebugEnabled()) {	
removing from preemptionqueue 

}	}	if (taskWrapper.isInPreemptionQueue()) {	if (LOG.isDebugEnabled()) {	}	removeFromPreemptionQueue(taskWrapper);	}	taskWrapper.getTaskRunnerCallable().setWmCountersDone();	taskWrapper.getTaskRunnerCallable().killTask();	} else {	
ignoring killfragment request for since it isn t known 

public void fragmentCompleting(String fragmentId, State state) {	int val = 0;	do {	val = runningFragmentCount.get();	if (val == 0) {	
runningfragmentcount is already multiple calls for the same completion 

private boolean handleScheduleAttemptedRejection(TaskWrapper rejected) {	if (!enablePreemption || preemptionQueue.isEmpty()) {	return false;	}	
preemption queue 

private boolean handleScheduleAttemptedRejection(TaskWrapper rejected) {	if (!enablePreemption || preemptionQueue.isEmpty()) {	return false;	}	TaskWrapper victim = getSuitableVictimFromPreemptionQueue(rejected);	if (victim == null) {	return false;	}	if (LOG.isInfoEnabled()) {	
invoking kill task for due to pre emption to run 

private void finishableStateUpdated(TaskWrapper taskWrapper, boolean newFinishableState) {	synchronized (lock) {	
fragment guaranteed state changed to finishable in wait queue in preemption queue 

private void insertIntoPreemptionQueueOrFailUnlocked(TaskWrapper taskWrapper) {	boolean added = preemptionQueue.offer(taskWrapper);	if (!added) {	
failed to add element to preemption queue terminating 

public void onSuccess(TaskRunner2Result result) {	if (LOG.isDebugEnabled()) {	
received successful completion for 

public void onFailure(Throwable t) {	if (LOG.isDebugEnabled()) {	
received failed completion for 

public void onFailure(Throwable t) {	if (LOG.isDebugEnabled()) {	}	updateFallOffStats(taskWrapper.getRequestId());	knownTasks.remove(taskWrapper.getRequestId());	taskWrapper.setIsInPreemptableQueue(false);	taskWrapper.maybeUnregisterForFinishedStateNotifications();	taskWrapper.getTaskRunnerCallable().setWmCountersDone();	updatePreemptionListAndNotify(null);	taskWrapper.getTaskRunnerCallable().getCallback().onFailure(t);	
failed notification received stacktrace 

private void updatePreemptionListAndNotify(EndReason reason) {	if (enablePreemption) {	String state = reason == null ? "FAILED" : reason.name();	boolean removed = removeFromPreemptionQueueUnlocked(taskWrapper);	if (removed && LOG.isInfoEnabled()) {	TaskRunnerCallable trc = taskWrapper.getTaskRunnerCallable();	
request removed from preemption list 

private void updateFallOffStats( String requestId) {	long now = clock.getTime();	FragmentCompletion fragmentCompletion = completingFragmentMap.remove(requestId);	if (fragmentCompletion == null) {	
received onsuccess onfailure for a fragment for which a completing message was not received 

public void shutDown(boolean awaitTermination) {	if (!isShutdown.getAndSet(true)) {	if (awaitTermination) {	if (LOG.isDebugEnabled()) {	
awaittermination shutting down task executor service gracefully 

public void shutDown(boolean awaitTermination) {	if (!isShutdown.getAndSet(true)) {	if (awaitTermination) {	if (LOG.isDebugEnabled()) {	}	shutdownExecutor(waitQueueExecutorService);	shutdownExecutor(executorService);	shutdownExecutor(executionCompletionExecutorService);	} else {	if (LOG.isDebugEnabled()) {	
awaittermination shutting down task executor service immediately 

========================= hive sample_2239 =========================

continue;	}	case REG_FUNCS_DONE: return;	default: throw new AssertionError(val);	}	}	try {	reloadFunctions();	didRegisterAllFuncs.compareAndSet(REG_FUNCS_PENDING, REG_FUNCS_DONE);	} catch (Exception e) {	
failed to register all functions 

public void reloadFunctions() throws HiveException {	HashSet<String> registryFunctions = new HashSet<String>( FunctionRegistry.getFunctionNames(".+\\..+"));	for (Function function : getAllFunctions()) {	String functionName = function.getFunctionName();	try {	
registering function 

public void reloadFunctions() throws HiveException {	HashSet<String> registryFunctions = new HashSet<String>( FunctionRegistry.getFunctionNames(".+\\..+"));	for (Function function : getAllFunctions()) {	String functionName = function.getFunctionName();	try {	String qualFunc = FunctionUtils.qualifyFunctionName(functionName, function.getDbName());	FunctionRegistry.registerPermanentFunction(qualFunc, function.getClassName(), false, FunctionTask.toFunctionResource(function.getResourceUris()));	registryFunctions.remove(qualFunc);	} catch (Exception e) {	
failed to register persistent function ignore and continue 

String qualFunc = FunctionUtils.qualifyFunctionName(functionName, function.getDbName());	FunctionRegistry.registerPermanentFunction(qualFunc, function.getClassName(), false, FunctionTask.toFunctionResource(function.getResourceUris()));	registryFunctions.remove(qualFunc);	} catch (Exception e) {	}	}	for (String functionName : registryFunctions) {	try {	FunctionRegistry.unregisterPermanentFunction(functionName);	} catch (Exception e) {	
failed to unregister persistent function on reload ignore and continue 

private void close() {	
closing current thread s connection to hive metastore 

public Table getTable(final String dbName, final String tableName, boolean throwException) throws HiveException {	if (tableName == null || tableName.equals("")) {	throw new HiveException("empty table creation??");	}	org.apache.hadoop.hive.metastore.api.Table tTable = null;	try {	tTable = getMSC().getTable(dbName, tableName);	} catch (NoSuchObjectException e) {	if (throwException) {	
table not found 

for (String dbName : getMSC().getAllDatabases()) {	List<String> materializedViewNames = getMaterializedViewsForRewriting(dbName);	if (materializedViewNames.isEmpty()) {	continue;	}	List<Table> materializedViewTables = getTableObjects(dbName, materializedViewNames);	Map<String, Materialization> databaseInvalidationInfo = getMSC().getMaterializationsInvalidationInfo(dbName, materializedViewNames);	for (Table materializedViewTable : materializedViewTables) {	Materialization materializationInvalidationInfo = databaseInvalidationInfo.get(materializedViewTable.getTableName());	if (materializationInvalidationInfo == null) {	
materialized view ignored for rewriting as there was no information loaded in the invalidation cache 

List<Table> materializedViewTables = getTableObjects(dbName, materializedViewNames);	Map<String, Materialization> databaseInvalidationInfo = getMSC().getMaterializationsInvalidationInfo(dbName, materializedViewNames);	for (Table materializedViewTable : materializedViewTables) {	Materialization materializationInvalidationInfo = databaseInvalidationInfo.get(materializedViewTable.getTableName());	if (materializationInvalidationInfo == null) {	continue;	}	long invalidationTime = materializationInvalidationInfo.getInvalidationTime();	if (diff == 0L) {	if (invalidationTime != 0L) {	
materialized view ignored for rewriting as its contents are outdated 

if (materializationInvalidationInfo == null) {	continue;	}	long invalidationTime = materializationInvalidationInfo.getInvalidationTime();	if (diff == 0L) {	if (invalidationTime != 0L) {	continue;	}	} else {	if (invalidationTime != 0 && minTime > invalidationTime) {	
materialized view ignored for rewriting as its contents are outdated 

RelOptMaterialization materialization = HiveMaterializedViewsRegistry.get().getRewritingMaterializedView( dbName, materializedViewTable.getTableName());	if (materialization != null) {	RelOptHiveTable cachedMaterializedViewTable = (RelOptHiveTable) materialization.tableRel.getTable();	if (cachedMaterializedViewTable.getHiveTableMD().getCreateTime() == materializedViewTable.getCreateTime()) {	result.add(materialization);	continue;	}	}	if (HiveMaterializedViewsRegistry.get().isInitialized()) {	if (LOG.isDebugEnabled()) {	
materialized view was not in the cache 

}	if (HiveMaterializedViewsRegistry.get().isInitialized()) {	if (LOG.isDebugEnabled()) {	}	materialization = HiveMaterializedViewsRegistry.get().createMaterializedView( conf, materializedViewTable);	if (materialization != null) {	result.add(materialization);	}	} else {	if (LOG.isWarnEnabled()) {	
materialized view was skipped because cache has not been loaded yet 

newPartPath = oldPartPath;	}	List<Path> newFiles = null;	PerfLogger perfLogger = SessionState.getPerfLogger();	perfLogger.PerfLogBegin("MoveTask", "FileMoves");	if (conf.getBoolVar(ConfVars.FIRE_EVENTS_FOR_DML) && !tbl.isTemporary() && (null != oldPart)) {	newFiles = Collections.synchronizedList(new ArrayList<Path>());	}	if (isMmTableWrite && loadPath.equals(newPartPath)) {	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
not moving to mm 

newFiles = Collections.synchronizedList(new ArrayList<Path>());	}	if (isMmTableWrite && loadPath.equals(newPartPath)) {	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	}	assert !isAcidIUDoperation;	if (areEventsForDmlNeeded(tbl, oldPart)) {	newFiles = listFilesCreatedByQuery(loadPath, txnId, stmtId);	}	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
maybe deleting stuff from new for replace 

PathFilter filter = FileUtils.HIDDEN_FILES_PATH_FILTER;	Path destPath = newPartPath;	if (isMmTableWrite) {	destPath = new Path(destPath, AcidUtils.deltaSubdir(txnId, txnId, stmtId));	filter = (loadFileType == LoadFileType.REPLACE_ALL) ? new JavaUtils.IdPathFilter(txnId, stmtId, false, true) : filter;	}	else if(!isAcidIUDoperation && isFullAcidTable) {	destPath = fixFullAcidPathForLoadData(loadFileType, destPath, txnId, stmtId, tbl);	}	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
moving to 

copyFiles(conf, loadPath, destPath, fs, isSrcLocal, isAcidIUDoperation, (loadFileType == LoadFileType.OVERWRITE_EXISTING), newFiles);	}	}	perfLogger.PerfLogEnd("MoveTask", "FileMoves");	Partition newTPart = oldPart != null ? oldPart : new Partition(tbl, partSpec, newPartPath);	alterPartitionSpecInMemory(tbl, partSpec, newTPart.getTPartition(), inheritTableSpecs, newPartPath.toString());	validatePartition(newTPart);	if ((null != oldPart) && (null != newFiles)) {	fireInsertEvent(tbl, partSpec, (loadFileType == LoadFileType.REPLACE_ALL), newFiles);	} else {	
no new files were created and is not a replace or we re inserting into a partition that does not exist yet skipping generating insert event 

if (!this.getConf().getBoolVar(HiveConf.ConfVars.HIVESTATSAUTOGATHER)) {	StatsSetupConst.setBasicStatsState(newTPart.getParameters(), StatsSetupConst.FALSE);	}	if (oldPart == null) {	newTPart.getTPartition().setParameters(new HashMap<String,String>());	if (this.getConf().getBoolVar(HiveConf.ConfVars.HIVESTATSAUTOGATHER)) {	StatsSetupConst.setStatsStateForCreateTable(newTPart.getParameters(), MetaStoreUtils.getColumnNames(tbl.getCols()), StatsSetupConst.TRUE);	}	MetaStoreUtils.populateQuickStats(HiveStatsUtils.getFileStatusRecurse(newPartPath, -1, newPartPath.getFileSystem(conf)), newTPart.getParameters());	try {	
adding new partition 

}	if (oldPart == null) {	newTPart.getTPartition().setParameters(new HashMap<String,String>());	if (this.getConf().getBoolVar(HiveConf.ConfVars.HIVESTATSAUTOGATHER)) {	StatsSetupConst.setStatsStateForCreateTable(newTPart.getParameters(), MetaStoreUtils.getColumnNames(tbl.getCols()), StatsSetupConst.TRUE);	}	MetaStoreUtils.populateQuickStats(HiveStatsUtils.getFileStatusRecurse(newPartPath, -1, newPartPath.getFileSystem(conf)), newTPart.getParameters());	try {	getSynchronizedMSC().add_partition(newTPart.getTPartition());	} catch (AlreadyExistsException aee) {	
caught alreadyexistsexception trying to alter partition instead 

getSynchronizedMSC().add_partition(newTPart.getTPartition());	} catch (AlreadyExistsException aee) {	setStatsPropAndAlterPartition(hasFollowingStatsTask, tbl, newTPart);	} catch (Exception e) {	try {	final FileSystem newPathFileSystem = newPartPath.getFileSystem(this.getConf());	boolean isAutoPurge = "true".equalsIgnoreCase(tbl.getProperty("auto.purge"));	final FileStatus status = newPathFileSystem.getFileStatus(newPartPath);	Hive.trashFiles(newPathFileSystem, new FileStatus[] {status}, this.getConf(), isAutoPurge);	} catch (IOException io) {	
could not delete partition directory contents after failed partition creation 

switch (loadFileType) {	case REPLACE_ALL: destPath = new Path(destPath, AcidUtils.baseDir(txnId));	break;	case KEEP_EXISTING: destPath = new Path(destPath, AcidUtils.deltaSubdir(txnId, txnId, stmtId));	break;	case OVERWRITE_EXISTING: default: throw new IllegalArgumentException("Unexpected " + LoadFileType.class.getName() + " " + loadFileType);	}	try {	FileSystem fs = tbl.getDataLocation().getFileSystem(SessionState.getSessionConf());	if(!FileUtils.mkdir(fs, destPath, conf)) {	
already exists 

private List<Path> listFilesCreatedByQuery(Path loadPath, long txnId, int stmtId) throws HiveException {	List<Path> newFiles = new ArrayList<Path>();	final String filePrefix = AcidUtils.deltaSubdir(txnId, txnId, stmtId);	FileStatus[] srcs;	FileSystem srcFs;	try {	srcFs = loadPath.getFileSystem(conf);	srcs = srcFs.listStatus(loadPath);	} catch (IOException e) {	
error listing files 

final String filePrefix = AcidUtils.deltaSubdir(txnId, txnId, stmtId);	FileStatus[] srcs;	FileSystem srcFs;	try {	srcFs = loadPath.getFileSystem(conf);	srcs = srcFs.listStatus(loadPath);	} catch (IOException e) {	throw new HiveException(e);	}	if (srcs == null) {	
no sources specified 

private void setStatsPropAndAlterPartition(boolean hasFollowingStatsTask, Table tbl, Partition newTPart) throws MetaException, TException {	EnvironmentContext environmentContext = null;	if (hasFollowingStatsTask) {	environmentContext = new EnvironmentContext();	environmentContext.putToProperties(StatsSetupConst.DO_NOT_UPDATE_STATS, StatsSetupConst.TRUE);	}	
altering existing partition 

private void walkDirTree(FileStatus fSta, FileSystem fSys, Map<List<String>, String> skewedColValueLocationMaps, Path newPartPath, SkewedInfo skewedInfo) throws IOException {	if (!fSta.isDir()) {	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
processing lb leaf 

private void walkDirTree(FileStatus fSta, FileSystem fSys, Map<List<String>, String> skewedColValueLocationMaps, Path newPartPath, SkewedInfo skewedInfo) throws IOException {	if (!fSta.isDir()) {	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	}	constructOneLBLocationMap(fSta, skewedColValueLocationMaps, newPartPath, skewedInfo);	return;	}	FileStatus[] children = fSys.listStatus(fSta.getPath(), FileUtils.HIDDEN_FILES_PATH_FILTER);	if (children != null) {	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
processing lb dir 

while (dirsToTake < dirNames.length && keysFound < colCount) {	String dirName = dirNames[dirsToTake++];	if (dirName.equalsIgnoreCase(ListBucketingPrunerUtils.HIVE_LIST_BUCKETING_DEFAULT_DIR_NAME)) {	++keysFound;	} else {	String[] kv = dirName.split("=");	if (kv.length == 2) {	skewedValue.add(kv[1]);	++keysFound;	} else {	
skipping unknown directory when expecting lb keys or default directory from 

skewedValue.add(kv[1]);	++keysFound;	} else {	}	}	}	for (int i = 0; i < (dirNames.length - dirsToTake); ++i) {	lbdPath = lbdPath.getParent();	}	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
saving lb location based on keys and 

validPartitions.add(dpPath);	}	} else {	Path[] leafStatus = Utilities.getMmDirectoryCandidates( fs, loadPath, numDP, numLB, null, txnId, -1, conf, false);	for (Path p : leafStatus) {	Path dpPath = p.getParent();	for (int i = 0; i < numLB; ++i) {	dpPath = dpPath.getParent();	}	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
found dp 

if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	}	validPartitions.add(dpPath);	}	}	} catch (IOException e) {	throw new HiveException(e);	}	int partsToLoad = validPartitions.size();	if (partsToLoad == 0) {	
no partition is generated by dynamic partitioning 

final AtomicInteger partitionsLoaded = new AtomicInteger(0);	final boolean inPlaceEligible = conf.getLong("fs.trash.interval", 0) <= 0 && InPlaceUpdate.canRenderInPlace(conf) && !SessionState.getConsole().getIsSilent();	final PrintStream ps = (inPlaceEligible) ? SessionState.getConsole().getInfoStream() : null;	final SessionState parentSession = SessionState.get();	final List<Future<Void>> futures = Lists.newLinkedList();	try {	final Map<Long, RawStore> rawStoreMap = Collections.synchronizedMap(new HashMap<Long, RawStore>());	for(final Path partPath : validPartitions) {	final LinkedHashMap<String, String> fullPartSpec = Maps.newLinkedHashMap(partSpec);	if (!Warehouse.makeSpecFromName( fullPartSpec, partPath, new HashSet<String>(partSpec.keySet()))) {	
ignoring invalid dp directory 

}	return null;	} catch (Exception t) {	LOG.error("Exception when loading partition with parameters " + " partPath=" + partPath + ", " + " table=" + tbl.getTableName() + ", " + " partSpec=" + fullPartSpec + ", " + " loadFileType=" + loadFileType.toString() + ", " + " listBucketingLevel=" + numLB + ", " + " isAcid=" + isAcid + ", " + " hasFollowingStatsTask=" + hasFollowingStatsTask, t);	throw t;	}	}	}));	}	pool.shutdown();	
number of partitions to be added is 

}));	}	pool.shutdown();	for (Future future : futures) {	future.get();	}	for (RawStore rs : rawStoreMap.values()) {	rs.shutdown();	}	} catch (InterruptedException | ExecutionException e) {	
cancelling dynamic loading tasks 

throw new HiveException("Exception when loading " + partsToLoad + " in table " + tbl.getTableName() + " with loadPath=" + loadPath, e);	}	try {	if (isAcid) {	List<String> partNames = new ArrayList<>(partitionsMap.size());	for (Partition p : partitionsMap.values()) {	partNames.add(p.getName());	}	getMSC().addDynamicPartitions(txnId, tbl.getDbName(), tbl.getTableName(), partNames, AcidUtils.toDataOperationType(operation));	}	
loaded partitions 

List<Path> newFiles = null;	Table tbl = getTable(tableName);	assert tbl.getPath() != null : "null==getPath() for " + tbl.getTableName();	boolean isMmTable = AcidUtils.isInsertOnlyTable(tbl);	boolean isFullAcidTable = AcidUtils.isAcidTable(tbl);	HiveConf sessionConf = SessionState.getSessionConf();	if (conf.getBoolVar(ConfVars.FIRE_EVENTS_FOR_DML) && !tbl.isTemporary()) {	newFiles = Collections.synchronizedList(new ArrayList<Path>());	}	if (isMmTable && loadPath.equals(tbl.getPath())) {	
not moving to 

tpart = getSynchronizedMSC().getPartitionWithAuthInfo(tbl.getDbName(), tbl.getTableName(), pvals, getUserName(), getGroupNames());	} catch (NoSuchObjectException nsoe) {	tpart = null;	} catch (Exception e) {	LOG.error(StringUtils.stringifyException(e));	throw new HiveException(e);	}	try {	if (forceCreate) {	if (tpart == null) {	
creating partition for table with partition spec 

} catch (Exception e) {	LOG.error(StringUtils.stringifyException(e));	throw new HiveException(e);	}	try {	if (forceCreate) {	if (tpart == null) {	try {	tpart = getSynchronizedMSC().appendPartition(tbl.getDbName(), tbl.getTableName(), pvals);	} catch (AlreadyExistsException aee) {	
caught already exists exception trying to alter partition instead 

try {	if (forceCreate) {	if (tpart == null) {	try {	tpart = getSynchronizedMSC().appendPartition(tbl.getDbName(), tbl.getTableName(), pvals);	} catch (AlreadyExistsException aee) {	tpart = getSynchronizedMSC().getPartitionWithAuthInfo(tbl.getDbName(), tbl.getTableName(), pvals, getUserName(), getGroupNames());	alterPartitionSpec(tbl, partSpec, tpart, inheritTableSpecs, partPath);	} catch (Exception e) {	if (CheckJDOException.isJDODataStoreException(e)) {	
caught jdo exception trying to alter partition instead 

private void alterPartitionSpecInMemory(Table tbl, Map<String, String> partSpec, org.apache.hadoop.hive.metastore.api.Partition tpart, boolean inheritTableSpecs, String partPath) throws HiveException, InvalidOperationException {	
altering partition for table with partition spec 

private void fireInsertEvent(Table tbl, Map<String, String> partitionSpec, boolean replace, List<Path> newFiles) throws HiveException {	if (conf.getBoolVar(ConfVars.FIRE_EVENTS_FOR_DML)) {	
firing dml insert event 

private void fireInsertEvent(Table tbl, Map<String, String> partitionSpec, boolean replace, List<Path> newFiles) throws HiveException {	if (conf.getBoolVar(ConfVars.FIRE_EVENTS_FOR_DML)) {	if (tbl.isTemporary()) {	
not firing dml insert event as is temporary 

}	}));	}	}	}	if (null != pool) {	pool.shutdown();	for (Future<ObjectPair<Path, Path>> future : futures) {	try {	ObjectPair<Path, Path> pair = future.get();	
moved src to dest 

private static boolean isSubDir(Path srcf, Path destf, FileSystem srcFs, FileSystem destFs, boolean isSrcLocal) {	if (srcf == null) {	
the source path is null for issubdir method 

private static boolean isSubDir(Path srcf, Path destf, FileSystem srcFs, FileSystem destFs, boolean isSrcLocal) {	if (srcf == null) {	return false;	}	String fullF1 = getQualifiedPathWithoutSchemeAndAuthority(srcf, srcFs).toString() + Path.SEPARATOR;	String fullF2 = getQualifiedPathWithoutSchemeAndAuthority(destf, destFs).toString() + Path.SEPARATOR;	boolean isInTest = HiveConf.getBoolVar(srcFs.getConf(), ConfVars.HIVE_IN_TEST);	
the source path is and the destination path is 

}	String fullF1 = getQualifiedPathWithoutSchemeAndAuthority(srcf, srcFs).toString() + Path.SEPARATOR;	String fullF2 = getQualifiedPathWithoutSchemeAndAuthority(destf, destFs).toString() + Path.SEPARATOR;	boolean isInTest = HiveConf.getBoolVar(srcFs.getConf(), ConfVars.HIVE_IN_TEST);	if (isInTest) {	return fullF1.startsWith(fullF2);	}	String schemaSrcf = srcf.toUri().getScheme();	String schemaDestf = destf.toUri().getScheme();	if (schemaDestf == null && isSrcLocal) {	
the source file is in the local while the dest not 

boolean isInTest = HiveConf.getBoolVar(srcFs.getConf(), ConfVars.HIVE_IN_TEST);	if (isInTest) {	return fullF1.startsWith(fullF2);	}	String schemaSrcf = srcf.toUri().getScheme();	String schemaDestf = destf.toUri().getScheme();	if (schemaDestf == null && isSrcLocal) {	return false;	}	if (schemaSrcf != null && schemaDestf != null && !schemaSrcf.equals(schemaDestf)) {	
the source path s schema is and the destination path s schema is 

return fullF1.startsWith(fullF2);	}	String schemaSrcf = srcf.toUri().getScheme();	String schemaDestf = destf.toUri().getScheme();	if (schemaDestf == null && isSrcLocal) {	return false;	}	if (schemaSrcf != null && schemaDestf != null && !schemaSrcf.equals(schemaDestf)) {	return false;	}	
the source path is and the destination path is 

public static void listNewFilesRecursively(final FileSystem destFs, Path dest, List<Path> newFiles) throws HiveException {	try {	for (FileStatus fileStatus : destFs.listStatus(dest, FileUtils.HIDDEN_FILES_PATH_FILTER)) {	if (fileStatus.isDirectory()) {	listNewFilesRecursively(destFs, fileStatus.getPath(), newFiles);	} else {	newFiles.add(fileStatus.getPath());	}	}	} catch (IOException e) {	
failed to get source file statuses 

public static boolean moveFile(final HiveConf conf, Path srcf, final Path destf, boolean replace, boolean isSrcLocal) throws HiveException {	final FileSystem srcFs, destFs;	try {	destFs = destf.getFileSystem(conf);	} catch (IOException e) {	
failed to get dest fs 

public static boolean moveFile(final HiveConf conf, Path srcf, final Path destf, boolean replace, boolean isSrcLocal) throws HiveException {	final FileSystem srcFs, destFs;	try {	destFs = destf.getFileSystem(conf);	} catch (IOException e) {	throw new HiveException(e.getMessage(), e);	}	try {	srcFs = srcf.getFileSystem(conf);	} catch (IOException e) {	
failed to get src fs 

}	HdfsUtils.HadoopFileStatus destStatus = null;	boolean srcIsSubDirOfDest = isSubDir(srcf, destf, srcFs, destFs, isSrcLocal), destIsSubDirOfSrc = isSubDir(destf, srcf, destFs, srcFs, false);	final String msg = "Unable to move source " + srcf + " to destination " + destf;	try {	if (replace) {	try{	destStatus = new HdfsUtils.HadoopFileStatus(conf, destFs, destf);	if (replace && !srcIsSubDirOfDest) {	destFs.delete(destf, true);	
the path is deleted 

} catch (FileNotFoundException ignore) {	}	}	final HdfsUtils.HadoopFileStatus desiredStatus = destStatus;	final SessionState parentSession = SessionState.get();	if (isSrcLocal) {	destFs.copyFromLocalFile(srcf, destf);	return true;	} else {	if (needToCopy(srcf, destf, srcFs, destFs)) {	
copying source to because hdfs encryption zones are different 

} else {	if (needToCopy(srcf, destf, srcFs, destFs)) {	return FileUtils.copy(srcf.getFileSystem(conf), srcf, destf.getFileSystem(conf), destf, true, replace, conf);	} else {	if (srcIsSubDirOfDest || destIsSubDirOfSrc) {	FileStatus[] srcs = destFs.listStatus(srcf, FileUtils.HIDDEN_FILES_PATH_FILTER);	List<Future<Void>> futures = new LinkedList<>();	final ExecutorService pool = conf.getInt(ConfVars.HIVE_MOVE_FILES_THREAD_COUNT.varname, 25) > 0 ? Executors.newFixedThreadPool(conf.getInt(ConfVars.HIVE_MOVE_FILES_THREAD_COUNT.varname, 25), new ThreadFactoryBuilder().setDaemon(true).setNameFormat("Move-Thread-%d").build()) : null;	if (destIsSubDirOfSrc && !destFs.exists(destf)) {	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
creating 

static private HiveException handlePoolException(ExecutorService pool, Exception e) {	HiveException he = null;	if (e instanceof HiveException) {	he = (HiveException) e;	if (he.getCanonicalErrorMsg() != ErrorMsg.GENERIC_ERROR) {	if (he.getCanonicalErrorMsg() == ErrorMsg.UNRESOLVED_RT_EXCEPTION) {	
failed to move 

static private HiveException handlePoolException(ExecutorService pool, Exception e) {	HiveException he = null;	if (e instanceof HiveException) {	he = (HiveException) e;	if (he.getCanonicalErrorMsg() != ErrorMsg.GENERIC_ERROR) {	if (he.getCanonicalErrorMsg() == ErrorMsg.UNRESOLVED_RT_EXCEPTION) {	} else {	
failed to move 

static private HiveException handlePoolException(ExecutorService pool, Exception e) {	HiveException he = null;	if (e instanceof HiveException) {	he = (HiveException) e;	if (he.getCanonicalErrorMsg() != ErrorMsg.GENERIC_ERROR) {	if (he.getCanonicalErrorMsg() == ErrorMsg.UNRESOLVED_RT_EXCEPTION) {	} else {	}	}	} else {	
failed to move 

FileStatus[] srcs;	FileSystem srcFs;	try {	srcFs = srcf.getFileSystem(conf);	srcs = srcFs.globStatus(srcf);	} catch (IOException e) {	LOG.error(StringUtils.stringifyException(e));	throw new HiveException("addFiles: filesystem error in check phase. " + e.getMessage(), e);	}	if (srcs == null) {	
no sources specified to move 

public static void moveAcidFiles(FileSystem fs, FileStatus[] stats, Path dst, List<Path> newFiles) throws HiveException {	Set<Path> createdDeltaDirs = new HashSet<Path>();	for (FileStatus stat : stats) {	Path srcPath = stat.getPath();	
acid move looking for original buckets in 

for(FileStatus unionSubdir : unionSubdirs) {	Collections.addAll(buckets, fs.listStatus(unionSubdir.getPath(), AcidUtils.originalBucketFilter));	}	origBucketStats = buckets.toArray(new FileStatus[buckets.size()]);	}	} catch (IOException e) {	String msg = "Unable to look for bucket files in src path " + srcPath.toUri().toString();	LOG.error(msg);	throw new HiveException(msg, e);	}	
acid move found original buckets 

private static void moveAcidFiles(String deltaFileType, PathFilter pathFilter, FileSystem fs, Path dst, Path origBucketPath, Set<Path> createdDeltaDirs, List<Path> newFiles) throws HiveException {	
acid move looking for files in bucket 

private static void moveAcidFiles(String deltaFileType, PathFilter pathFilter, FileSystem fs, Path dst, Path origBucketPath, Set<Path> createdDeltaDirs, List<Path> newFiles) throws HiveException {	FileStatus[] deltaStats = null;	try {	deltaStats = fs.listStatus(origBucketPath, pathFilter);	} catch (IOException e) {	throw new HiveException("Unable to look for " + deltaFileType + " files in original bucket " + origBucketPath.toUri().toString(), e);	}	
acid move found files 

}	for (FileStatus deltaStat : deltaStats) {	Path deltaPath = deltaStat.getPath();	Path deltaDest = new Path(dst, deltaPath.getName());	try {	if (!createdDeltaDirs.contains(deltaDest)) {	try {	fs.mkdirs(deltaDest);	createdDeltaDirs.add(deltaDest);	} catch (IOException swallowIt) {	
unable to create directory assuming it already exists 

Path deltaDest = new Path(dst, deltaPath.getName());	try {	if (!createdDeltaDirs.contains(deltaDest)) {	try {	fs.mkdirs(deltaDest);	createdDeltaDirs.add(deltaDest);	} catch (IOException swallowIt) {	}	}	FileStatus[] bucketStats = fs.listStatus(deltaPath, AcidUtils.bucketFileFilter);	
acid move found bucket files 

fs.mkdirs(deltaDest);	createdDeltaDirs.add(deltaDest);	} catch (IOException swallowIt) {	}	}	FileStatus[] bucketStats = fs.listStatus(deltaPath, AcidUtils.bucketFileFilter);	for (FileStatus bucketStat : bucketStats) {	Path bucketSrc = bucketStat.getPath();	Path bucketDest = new Path(deltaDest, bucketSrc.getName());	final String msg = "Unable to move source " + bucketSrc + " to destination " + bucketDest;	
moving bucket to 

FileSystem destFs = destf.getFileSystem(conf);	FileStatus[] srcs;	FileSystem srcFs;	try {	srcFs = srcf.getFileSystem(conf);	srcs = srcFs.globStatus(srcf);	} catch (IOException e) {	throw new HiveException("Getting globStatus " + srcf.toString(), e);	}	if (srcs == null) {	
no sources specified to move 

private void deleteOldPathForReplace(Path destPath, Path oldPath, HiveConf conf, boolean purge, PathFilter pathFilter, boolean isMmTableOverwrite, int lbLevels) throws HiveException {	
deleting old paths for replace in and old path 

isOldPathUnderDestf = isSubDir(oldPath, destPath, oldFs, destFs, false);	if (isOldPathUnderDestf || isMmTableOverwrite) {	if (lbLevels == 0 || !isMmTableOverwrite) {	cleanUpOneDirectoryForReplace(oldPath, oldFs, pathFilter, conf, purge);	}	}	} catch (IOException e) {	if (isOldPathUnderDestf || isMmTableOverwrite) {	throw new HiveException("Directory " + oldPath.toString() + " could not be cleaned up.", e);	} else {	
directory cannot be cleaned 

}	}));	}	}	if (null != pool) {	pool.shutdown();	for (Future<Boolean> future : futures) {	try {	result &= future.get();	} catch (InterruptedException | ExecutionException e) {	
failed to delete 

String msg = "Error getting current user: " + e.getMessage();	LOG.error(msg, e);	throw new MetaException(msg + "\n" + StringUtils.stringifyException(e));	}	try {	metaStoreClient = createMetaStoreClient(allowEmbedded);	} catch (RuntimeException ex) {	Throwable t = ex.getCause();	while (t != null) {	if (t instanceof JDODataStoreException && t.getMessage() != null && t.getMessage().contains("autoCreate")) {	
cannot initialize metastore due to autocreate error 

public ImmutableMap<String, Long> dumpAndClearMetaCallTiming(String phase) {	boolean phaseInfoLogged = false;	if (LOG.isDebugEnabled()) {	phaseInfoLogged = logDumpPhase(phase);	
total time spent in each metastore function ms 

boolean phaseInfoLogged = false;	if (LOG.isDebugEnabled()) {	phaseInfoLogged = logDumpPhase(phase);	}	if (LOG.isInfoEnabled()) {	for (Entry<String, Long> callTime : metaCallTimeMap.entrySet()) {	if (callTime.getValue() > 1000) {	if (!phaseInfoLogged) {	phaseInfoLogged = logDumpPhase(phase);	}	
total time spent in this metastore function was greater than 

========================= hive sample_5006 =========================

} finally {	if (compactionsCleaned.size() > 0) {	for (Long compactId : compactionsCleaned) {	compactId2LockMap.remove(compactId);	compactId2CompactInfoMap.remove(compactId);	}	}	}	}	} catch (Throwable t) {	
caught an exception in the main loop of compactor cleaner 

private void clean(CompactionInfo ci) throws MetaException {	
starting cleaning for 

private void clean(CompactionInfo ci) throws MetaException {	try {	Table t = resolveTable(ci);	if (t == null) {	
unable to find table assuming it was dropped 

try {	Table t = resolveTable(ci);	if (t == null) {	txnHandler.markCleaned(ci);	return;	}	Partition p = null;	if (ci.partName != null) {	p = resolvePartition(ci);	if (p == null) {	
unable to find partition assuming it was dropped 

txnHandler.markCleaned(ci);	return;	}	}	StorageDescriptor sd = resolveStorageDescriptor(t, p);	final String location = sd.getLocation();	final ValidTxnList txnList = ci.highestTxnId > 0 ? new ValidReadTxnList(new long[0], new BitSet(), ci.highestTxnId) : new ValidReadTxnList();	if (runJobAsSelf(ci.runAs)) {	removeFiles(location, txnList);	} else {	
cleaning as user for 

UserGroupInformation ugi = UserGroupInformation.createProxyUser(ci.runAs, UserGroupInformation.getLoginUser());	ugi.doAs(new PrivilegedExceptionAction<Object>() {	public Object run() throws Exception {	removeFiles(location, txnList);	return null;	}	});	try {	FileSystem.closeAllForUGI(ugi);	} catch (IOException exception) {	
could not clean up file system handles for ugi for 

return null;	}	});	try {	FileSystem.closeAllForUGI(ugi);	} catch (IOException exception) {	}	}	txnHandler.markCleaned(ci);	} catch (Exception e) {	
caught exception when cleaning unable to complete cleaning of 

private void removeFiles(String location, ValidTxnList txnList) throws IOException {	AcidUtils.Directory dir = AcidUtils.getAcidState(new Path(location), conf, txnList);	List<FileStatus> obsoleteDirs = dir.getObsolete();	List<Path> filesToDelete = new ArrayList<Path>(obsoleteDirs.size());	for (FileStatus stat : obsoleteDirs) {	filesToDelete.add(stat.getPath());	}	if (filesToDelete.size() < 1) {	
hmm nothing to delete in the cleaner for directory that hardly seems right 

private void removeFiles(String location, ValidTxnList txnList) throws IOException {	AcidUtils.Directory dir = AcidUtils.getAcidState(new Path(location), conf, txnList);	List<FileStatus> obsoleteDirs = dir.getObsolete();	List<Path> filesToDelete = new ArrayList<Path>(obsoleteDirs.size());	for (FileStatus stat : obsoleteDirs) {	filesToDelete.add(stat.getPath());	}	if (filesToDelete.size() < 1) {	return;	}	
about to remove obsolete directories from 

List<FileStatus> obsoleteDirs = dir.getObsolete();	List<Path> filesToDelete = new ArrayList<Path>(obsoleteDirs.size());	for (FileStatus stat : obsoleteDirs) {	filesToDelete.add(stat.getPath());	}	if (filesToDelete.size() < 1) {	return;	}	FileSystem fs = filesToDelete.get(0).getFileSystem(conf);	for (Path dead : filesToDelete) {	
going to delete path 

========================= hive sample_2805 =========================

int stripeCount = footer.getStripesCount();	boolean[] result = OrcInputFormat.pickStripesViaTranslatedSarg( sarg, orcTail.getWriterVersion(), footer.getTypesList(), orcTail.getStripeStatistics(), stripeCount);	SplitInfos.Builder sb = SplitInfos.newBuilder();	List<StripeInformation> stripes = orcTail.getStripes();	boolean isEliminated = true;	for (int i = 0; i < result.length; ++i) {	if (result != null && !result[i]) continue;	isEliminated = false;	StripeInformation si = stripes.get(i);	if (LOG.isDebugEnabled()) {	
ppd is adding a split 

========================= hive sample_3652 =========================

Assert.assertEquals(1, status.length);	Assert.assertTrue(status[0].getPath().getName().matches("delta_.*"));	}	int[][] newValsOdd = {{5,5},{11,11}};	int[][] newValsEven = {{2,2}};	runStatementOnDriver("insert into " + TableExtended.NONACIDPART + " PARTITION(p='odd') " + makeValuesClause(newValsOdd));	runStatementOnDriver("insert into " + TableExtended.NONACIDPART + " PARTITION(p='even') " + makeValuesClause(newValsEven));	List<String> rs = null;	String s = "insert overwrite table " + TableExtended.MMTBLPART + " PARTITION(p='odd') " + " select a,b from " + TableExtended.NONACIDPART + " where " + TableExtended.NONACIDPART + ".p='odd'";	rs = runStatementOnDriver("explain formatted " + s);	
explain formatted 

========================= hive sample_2413 =========================

public void runTest(String tname, String fname, String fpath) throws Exception {	Stopwatch sw = Stopwatch.createStarted();	boolean skipped = false;	boolean failed = false;	try {	
begin query 

public void runTest(String tname, String fname, String fpath) throws Exception {	Stopwatch sw = Stopwatch.createStarted();	boolean skipped = false;	boolean failed = false;	try {	System.err.println("Begin query: " + fname);	qt.addFile(fpath);	if (qt.shouldBeSkipped(fname)) {	
test skipped 

========================= hive sample_358 =========================

TableScanDesc tableScanDesc = tsOp.getConf();	Table table = tsOp.getConf().getTableMetadata().getTTable();	Map<String, String> tableParameters = table.getParameters();	Properties tableProperties = new Properties();	tableProperties.putAll(tableParameters);	Deserializer deserializer = tableScanDesc.getTableMetadata().getDeserializer();	String deserializerClassName = deserializer.getClass().getName();	try {	if (context.serdeClassesUnderConsideration.contains(deserializerClassName)) {	deserializer.initialize(context.conf, tableProperties);	
serde init succeeded for class 

deserializer.initialize(context.conf, tableProperties);	for (Map.Entry property : tableProperties.entrySet()) {	if (!property.getValue().equals(tableParameters.get(property.getKey()))) {	LOG.debug("Resolving changed parameters! key=" + property.getKey() + ", value=" + property.getValue());	tableParameters.put((String) property.getKey(), (String) property.getValue());	}	}	}	else {	if (LOG.isDebugEnabled()) {	
skipping prefetch for 

public ParseContext transform(ParseContext pctx) throws SemanticException {	
tablepropertyenrichmentoptimizer transform 

public ParseContext transform(ParseContext pctx) throws SemanticException {	Map<Rule, NodeProcessor> opRules = Maps.newLinkedHashMap();	opRules.put(new RuleRegExp("R1", TableScanOperator.getOperatorName() + "%"), new Processor());	WalkerCtx context = new WalkerCtx(pctx.getConf());	Dispatcher disp = new DefaultRuleDispatcher(null, opRules, context);	List<Node> topNodes = Lists.newArrayList();	topNodes.addAll(pctx.getTopOps().values());	GraphWalker walker = new PreOrderWalker(disp);	walker.startWalking(topNodes, null);	
tablepropertyenrichmentoptimizer transform complete 

========================= hive sample_3081 =========================

public final void addRowAndMaybeCheckAbort() throws InterruptedException {	if (nRows++ < checkInterruptionAfterRows) return;	long time = System.nanoTime();	checkAbortCondition();	long elapsedNs = (time - lastInterruptCheckNs);	if (elapsedNs >= 0) {	double diff = elapsedNs == 0 ? 10 : TARGET_INTERRUPT_CHECK_TIME_NS / elapsedNs;	int newRows = Math.min(CHECK_INTERRUPTION_AFTER_ROWS_MAX, Math.max(CHECK_INTERRUPTION_AFTER_ROWS_MIN, (int) (diff * checkInterruptionAfterRows)));	if (checkInterruptionAfterRows != newRows && LOG.isDebugEnabled()) {	
adjusting abort check rows to from 

========================= hive sample_3982 =========================

public NullRowsRecordReader(Configuration conf, InputSplit split) throws IOException {	boolean isVectorMode = Utilities.getUseVectorizedInputFileFormat(conf);	if (LOG.isDebugEnabled()) {	
in non vector mode 

public void configure(JobConf job) {	
using null rows input format 

========================= hive sample_3740 =========================

private void createRemoteClient() throws Exception {	remoteClient = SparkClientFactory.createClient(conf, hiveConf, sessionId);	if (HiveConf.getBoolVar(hiveConf, ConfVars.HIVE_PREWARM_ENABLED) && (SparkClientUtilities.isYarnMaster(hiveConf.get("spark.master")) || SparkClientUtilities.isLocalMaster(hiveConf.get("spark.master")))) {	int minExecutors = getExecutorsToWarm();	if (minExecutors <= 0) {	return;	}	
prewarm spark executors the minimum number of executors to warm is 

if (minExecutors <= 0) {	return;	}	int curExecutors = 0;	long maxPrewarmTime = HiveConf.getTimeVar(hiveConf, ConfVars.HIVE_PREWARM_SPARK_TIMEOUT, TimeUnit.MILLISECONDS);	long ts = System.currentTimeMillis();	do {	try {	curExecutors = getExecutorCount(maxPrewarmTime, TimeUnit.MILLISECONDS);	} catch (TimeoutException e) {	
timed out getting executor count 

}	int curExecutors = 0;	long maxPrewarmTime = HiveConf.getTimeVar(hiveConf, ConfVars.HIVE_PREWARM_SPARK_TIMEOUT, TimeUnit.MILLISECONDS);	long ts = System.currentTimeMillis();	do {	try {	curExecutors = getExecutorCount(maxPrewarmTime, TimeUnit.MILLISECONDS);	} catch (TimeoutException e) {	}	if (curExecutors >= minExecutors) {	
finished prewarming spark executors the current number of executors is 

do {	try {	curExecutors = getExecutorCount(maxPrewarmTime, TimeUnit.MILLISECONDS);	} catch (TimeoutException e) {	}	if (curExecutors >= minExecutors) {	return;	}	Thread.sleep(500);	} while (System.currentTimeMillis() - ts < maxPrewarmTime);	
timeout s occurred while prewarming executors the current number of executors is 

try {	URI fileUri = FileUtils.getURI(addedFile);	if (fileUri != null && !localFiles.contains(fileUri)) {	localFiles.add(fileUri);	if (SparkUtilities.needUploadToHDFS(fileUri, sparkConf)) {	fileUri = SparkUtilities.uploadToHDFS(fileUri, hiveConf);	}	remoteClient.addFile(fileUri);	}	} catch (URISyntaxException e) {	
failed to add file 

try {	URI jarUri = FileUtils.getURI(addedJar);	if (jarUri != null && !localJars.contains(jarUri)) {	localJars.add(jarUri);	if (SparkUtilities.needUploadToHDFS(jarUri, sparkConf)) {	jarUri = SparkUtilities.uploadToHDFS(jarUri, hiveConf);	}	remoteClient.addJar(jarUri);	}	} catch (URISyntaxException e) {	
failed to add jar 

========================= hive sample_4590 =========================

throw new InvalidOperationException("table new location " + destPath + " is on a different file system than the old location " + srcPath + ". This operation is not supported");	}	try {	if (destFs.exists(destPath)) {	throw new InvalidOperationException("New location for this table " + newDbName + "." + newTblName + " already exists : " + destPath);	}	if (srcFs.exists(srcPath) && wh.renameDir(srcPath, destPath, true)) {	dataWasMoved = true;	}	} catch (IOException | MetaException e) {	
alter table operation for failed 

assert(colStats == null);	if (cascade) {	msdb.alterPartition(dbname, name, part.getValues(), part);	} else {	oldPart.setParameters(part.getParameters());	msdb.alterPartition(dbname, name, part.getValues(), oldPart);	}	}	msdb.alterTable(dbname, name, newt);	} else {	
alter table not cascaded to partitions 

MetaStoreListenerNotifier.notifyEvent(transactionalListeners, EventMessage.EventType.DROP_TABLE, new DropTableEvent(oldt, true, false, handler), environmentContext);	MetaStoreListenerNotifier.notifyEvent(transactionalListeners, EventMessage.EventType.CREATE_TABLE, new CreateTableEvent(newt, true, handler), environmentContext);	if (isPartitionedTable) {	parts = msdb.getPartitions(newt.getDbName(), newt.getTableName(), -1);	MetaStoreListenerNotifier.notifyEvent(transactionalListeners, EventMessage.EventType.ADD_PARTITION, new AddPartitionEvent(newt, parts, true, handler), environmentContext);	}	}	}	success = msdb.commitTransaction();	} catch (InvalidObjectException e) {	
failed to get object from metastore 

if (isPartitionedTable) {	parts = msdb.getPartitions(newt.getDbName(), newt.getTableName(), -1);	MetaStoreListenerNotifier.notifyEvent(transactionalListeners, EventMessage.EventType.ADD_PARTITION, new AddPartitionEvent(newt, parts, true, handler), environmentContext);	}	}	}	success = msdb.commitTransaction();	} catch (InvalidObjectException e) {	throw new InvalidOperationException( "Unable to change partition or table." + " Check metastore logs for detailed stack." + e.getMessage());	} catch (InvalidInputException e) {	
accessing metastore failed due to invalid input 

MetaStoreListenerNotifier.notifyEvent(transactionalListeners, EventMessage.EventType.ADD_PARTITION, new AddPartitionEvent(newt, parts, true, handler), environmentContext);	}	}	}	success = msdb.commitTransaction();	} catch (InvalidObjectException e) {	throw new InvalidOperationException( "Unable to change partition or table." + " Check metastore logs for detailed stack." + e.getMessage());	} catch (InvalidInputException e) {	throw new InvalidOperationException( "Unable to change partition or table." + " Check metastore logs for detailed stack." + e.getMessage());	} catch (NoSuchObjectException e) {	
object not found in metastore 

}	success = msdb.commitTransaction();	} catch (InvalidObjectException e) {	throw new InvalidOperationException( "Unable to change partition or table." + " Check metastore logs for detailed stack." + e.getMessage());	} catch (InvalidInputException e) {	throw new InvalidOperationException( "Unable to change partition or table." + " Check metastore logs for detailed stack." + e.getMessage());	} catch (NoSuchObjectException e) {	throw new InvalidOperationException( "Unable to change partition or table. Database " + dbname + " does not exist" + " Check metastore logs for detailed stack." + e.getMessage());	} finally {	if (!success) {	
failed to alter table 

throw new InvalidOperationException( "Unable to change partition or table." + " Check metastore logs for detailed stack." + e.getMessage());	} catch (NoSuchObjectException e) {	throw new InvalidOperationException( "Unable to change partition or table. Database " + dbname + " does not exist" + " Check metastore logs for detailed stack." + e.getMessage());	} finally {	if (!success) {	msdb.rollbackTransaction();	if (dataWasMoved) {	try {	if (destFs.exists(destPath)) {	if (!destFs.rename(destPath, srcPath)) {	
failed to restore data from to in alter table failure manual restore is needed 

} finally {	if (!success) {	msdb.rollbackTransaction();	if (dataWasMoved) {	try {	if (destFs.exists(destPath)) {	if (!destFs.rename(destPath, srcPath)) {	}	}	} catch (IOException e) {	
failed to restore data from to in alter table failure manual restore is needed 

check_part = null;	}	if (check_part != null) {	throw new AlreadyExistsException("Partition already exists:" + dbname + "." + name + "." + new_part.getValues());	}	if (!tbl.getTableType().equals(TableType.EXTERNAL_TABLE.toString())) {	try {	destPath = wh.getPartitionPath(msdb.getDatabase(dbname), tbl, new_part.getValues());	destPath = constructRenamedPath(destPath, new Path(new_part.getSd().getLocation()));	} catch (NoSuchObjectException e) {	
didn t find object in metastore 

if (!tbl.getTableType().equals(TableType.EXTERNAL_TABLE.toString())) {	try {	destPath = wh.getPartitionPath(msdb.getDatabase(dbname), tbl, new_part.getValues());	destPath = constructRenamedPath(destPath, new Path(new_part.getSd().getLocation()));	} catch (NoSuchObjectException e) {	throw new InvalidOperationException( "Unable to change partition or table. Database " + dbname + " does not exist" + " Check metastore logs for detailed stack." + e.getMessage());	}	if (destPath != null) {	newPartLoc = destPath.toString();	oldPartLoc = oldPart.getSd().getLocation();	
srcpath 

if (!tbl.getTableType().equals(TableType.EXTERNAL_TABLE.toString())) {	try {	destPath = wh.getPartitionPath(msdb.getDatabase(dbname), tbl, new_part.getValues());	destPath = constructRenamedPath(destPath, new Path(new_part.getSd().getLocation()));	} catch (NoSuchObjectException e) {	throw new InvalidOperationException( "Unable to change partition or table. Database " + dbname + " does not exist" + " Check metastore logs for detailed stack." + e.getMessage());	}	if (destPath != null) {	newPartLoc = destPath.toString();	oldPartLoc = oldPart.getSd().getLocation();	
descpath 

try {	if (srcFs.exists(srcPath)) {	if (newPartLoc.compareTo(oldPartLoc) != 0 && destFs.exists(destPath)) {	throw new InvalidOperationException("New location for this table " + tbl.getDbName() + "." + tbl.getTableName() + " already exists : " + destPath);	}	Path destParentPath = destPath.getParent();	if (!wh.mkdirs(destParentPath)) {	throw new MetaException("Unable to create path " + destParentPath);	}	wh.renameDir(srcPath, destPath, true);	
partition directory rename from to done 

throw new InvalidOperationException("New location for this table " + tbl.getDbName() + "." + tbl.getTableName() + " already exists : " + destPath);	}	Path destParentPath = destPath.getParent();	if (!wh.mkdirs(destParentPath)) {	throw new MetaException("Unable to create path " + destParentPath);	}	wh.renameDir(srcPath, destPath, true);	dataWasMoved = true;	}	} catch (IOException e) {	
cannot rename partition directory from to 

Path destParentPath = destPath.getParent();	if (!wh.mkdirs(destParentPath)) {	throw new MetaException("Unable to create path " + destParentPath);	}	wh.renameDir(srcPath, destPath, true);	dataWasMoved = true;	}	} catch (IOException e) {	throw new InvalidOperationException("Unable to access src or dest location for partition " + tbl.getDbName() + "." + tbl.getTableName() + " " + new_part.getValues());	} catch (MetaException me) {	
cannot rename partition directory from to 

throw new InvalidOperationException("Unable to update partition stats in table rename." + iie);	} catch (NoSuchObjectException nsoe) {	}	}	if (transactionalListeners != null && !transactionalListeners.isEmpty()) {	MetaStoreListenerNotifier.notifyEvent(transactionalListeners, EventMessage.EventType.ALTER_PARTITION, new AlterPartitionEvent(oldPart, new_part, tbl, false, true, handler), environmentContext);	}	success = msdb.commitTransaction();	} finally {	if (!success) {	
failed to rename a partition rollback transaction 

}	}	if (transactionalListeners != null && !transactionalListeners.isEmpty()) {	MetaStoreListenerNotifier.notifyEvent(transactionalListeners, EventMessage.EventType.ALTER_PARTITION, new AlterPartitionEvent(oldPart, new_part, tbl, false, true, handler), environmentContext);	}	success = msdb.commitTransaction();	} finally {	if (!success) {	msdb.rollbackTransaction();	if (dataWasMoved) {	
revert the data move in renaming a partition 

success = msdb.commitTransaction();	} finally {	if (!success) {	msdb.rollbackTransaction();	if (dataWasMoved) {	try {	if (destFs.exists(destPath)) {	wh.renameDir(destPath, srcPath, false);	}	} catch (MetaException me) {	
failed to restore partition data from to in alter partition failure manual restore is needed 

} finally {	if (!success) {	msdb.rollbackTransaction();	if (dataWasMoved) {	try {	if (destFs.exists(destPath)) {	wh.renameDir(destPath, srcPath, false);	}	} catch (MetaException me) {	} catch (IOException ioe) {	
failed to restore partition data from to in alter partition failure manual restore is needed 

========================= hive sample_1842 =========================

public List<Task<? extends Serializable>> handle(Context context) throws SemanticException {	DropConstraintMessage msg = deserializer.getDropConstraintMessage(context.dmd.getPayload());	String actualDbName = context.isDbNameEmpty() ? msg.getDB() : context.dbName;	String actualTblName = context.isTableNameEmpty() ? msg.getTable() : context.tableName;	String constraintName = msg.getConstraint();	AlterTableDesc dropConstraintsDesc = new AlterTableDesc(actualDbName + "." + actualTblName, constraintName, context.eventOnlyReplicationSpec());	Task<DDLWork> dropConstraintsTask = TaskFactory.get(new DDLWork(readEntitySet, writeEntitySet, dropConstraintsDesc), context.hiveConf);	List<Task<? extends Serializable>> tasks = new ArrayList<Task<? extends Serializable>>();	tasks.add(dropConstraintsTask);	
added drop constrain task 

========================= hive sample_3486 =========================

public void initialize(Configuration conf, Properties tbl) throws SerDeException {	try {	
initializing the serde 

for (int i = 0; i < numColumns; i++) {	PrimitiveTypeInfo ti = TypeInfoFactory.getPrimitiveTypeInfo(hiveColumnTypeArray[i]);	ObjectInspector oi = PrimitiveObjectInspectorFactory.getPrimitiveJavaObjectInspector(ti);	fieldInspectors.add(oi);	}	objectInspector = ObjectInspectorFactory.getStandardStructObjectInspector(hiveColumnNames, fieldInspectors);	row = new ArrayList<Object>(numColumns);	}	}	catch (Exception e) {	
caught exception while initializing the sqlserde 

public Object deserialize(Writable blob) throws SerDeException {	
deserializing from serde 

========================= hive sample_1141 =========================

public Properties getConnectionProperties() throws BeelineHS2ConnectionFileParseException {	Properties props = new Properties();	String fileLocation = getFileLocation();	if (fileLocation == null) {	
user connection configuration file not found 

public Properties getConnectionProperties() throws BeelineHS2ConnectionFileParseException {	Properties props = new Properties();	String fileLocation = getFileLocation();	if (fileLocation == null) {	return props;	}	
using connection configuration file at 

return props;	}	props.setProperty(HS2ConnectionFileParser.URL_PREFIX_PROPERTY_KEY, "jdbc:hive2: Configuration conf = new Configuration(false);	conf.addResource(new Path(new File(fileLocation).toURI()));	try {	for (Entry<String, String> kv : conf) {	String key = kv.getKey();	if (key.startsWith(BEELINE_CONNECTION_PROPERTY_PREFIX)) {	props.setProperty(key.substring(BEELINE_CONNECTION_PROPERTY_PREFIX.length()), kv.getValue());	} else {	
ignoring since it does not start with 

========================= hive sample_1511 =========================

public void handle(Context withinContext) throws Exception {	
processing add uniqueconstraint message message 

========================= hive sample_3469 =========================

public void setDummyOps(List<HashTableDummyOperator> dummyOps) {	if (this.dummyOps != null && !this.dummyOps.isEmpty() && (dummyOps == null || dummyOps.isEmpty())) {	
removing dummy operators from 

========================= hive sample_3193 =========================

public void startThreads() throws IOException {	String principalUser = LlapUtil.getUserNameFromPrincipal( conf.get(SecretManager.ZK_DTSM_ZK_KERBEROS_PRINCIPAL));	
starting zk threads as user kerberos principal is configured for user short user name 

private void checkForZKDTSMBug() {	long expectedRenewTimeSec = conf.getLong(DelegationTokenManager.RENEW_INTERVAL, -1);	
checking for tokenrenewinterval bug 

private void checkForZKDTSMBug() {	long expectedRenewTimeSec = conf.getLong(DelegationTokenManager.RENEW_INTERVAL, -1);	if (expectedRenewTimeSec == -1) return;	java.lang.reflect.Field f = null;	try {	Class<?> c = org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.class;	f = c.getDeclaredField("tokenRenewInterval");	f.setAccessible(true);	} catch (Throwable t) {	
failed to check for tokenrenewinterval bug hoping for the best 

try {	Class<?> c = org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.class;	f = c.getDeclaredField("tokenRenewInterval");	f.setAccessible(true);	} catch (Throwable t) {	return;	}	try {	long realValue = f.getLong(this);	long expectedValue = expectedRenewTimeSec * 1000;	
tokenrenewinterval is expected 

long tokenLifetime = HiveConf.getTimeVar( conf, ConfVars.LLAP_DELEGATION_TOKEN_LIFETIME, TimeUnit.SECONDS);	zkConf.setLong(DelegationTokenManager.MAX_LIFETIME, tokenLifetime);	zkConf.setLong(DelegationTokenManager.RENEW_INTERVAL, tokenLifetime);	try {	zkConf.set(ZK_DTSM_ZK_KERBEROS_PRINCIPAL, SecurityUtil.getServerPrincipal(principal, "0.0.0.0"));	} catch (IOException e) {	throw new RuntimeException(e);	}	zkConf.set(ZK_DTSM_ZK_KERBEROS_KEYTAB, keyTab);	String zkPath = "zkdtsm_" + clusterId;	
using as zk secret manager path 

if (ugi.getRealUser() != null) {	realUser = new Text(ugi.getRealUser().getUserName());	}	renewer = new Text(ugi.getShortUserName());	} else {	renewer = new Text(user);	}	LlapTokenIdentifier llapId = new LlapTokenIdentifier( new Text(user), renewer, realUser, clusterId, appId, isSignatureRequired);	Token<LlapTokenIdentifier> token = new Token<LlapTokenIdentifier>(llapId, this);	if (LOG.isInfoEnabled()) {	
created llap token 

========================= hive sample_499 =========================

public SessionState(HiveConf conf, String userName) {	this.sessionConf = conf;	this.userName = userName;	this.registry = new Registry(false);	if (LOG.isDebugEnabled()) {	
sessionstate user 

public void updateThreadName() {	final String sessionId = getSessionId();	final String logPrefix = getConf().getLogIdVar(sessionId);	final String currThreadName = Thread.currentThread().getName();	if (!currThreadName.contains(logPrefix)) {	final String newThreadName = logPrefix + " " + currThreadName;	
updating thread name to 

public void resetThreadName() {	final String sessionId = getSessionId();	final String logPrefix = getConf().getLogIdVar(sessionId);	final String currThreadName = Thread.currentThread().getName();	if (currThreadName.contains(logPrefix)) {	final String[] names = currThreadName.split(logPrefix);	
resetting thread name to 

public HadoopShims.HdfsEncryptionShim getHdfsEncryptionShim(FileSystem fs) throws HiveException {	if (!hdfsEncryptionShims.containsKey(fs.getUri())) {	try {	if ("hdfs".equals(fs.getUri().getScheme())) {	hdfsEncryptionShims.put(fs.getUri(), ShimLoader.getHadoopShims().createHdfsEncryptionShim(fs, sessionConf));	} else {	
could not get hdfsencryptionshim it is only applicable to hdfs filesystem 

private Path createRootHDFSDir(HiveConf conf) throws IOException {	Path rootHDFSDirPath = new Path(HiveConf.getVar(conf, HiveConf.ConfVars.SCRATCHDIR));	FsPermission writableHDFSDirPermission = new FsPermission((short)00733);	FileSystem fs = rootHDFSDirPath.getFileSystem(conf);	if (!fs.exists(rootHDFSDirPath)) {	Utilities.createDirsWithPermission(conf, rootHDFSDirPath, writableHDFSDirPermission, true);	}	FsPermission currentHDFSDirPermission = fs.getFileStatus(rootHDFSDirPath).getPermission();	if (rootHDFSDirPath != null && rootHDFSDirPath.toUri() != null) {	String schema = rootHDFSDirPath.toUri().getScheme();	
hdfs root scratch dir with schema permission 

Path rootHDFSDirPath = new Path(HiveConf.getVar(conf, HiveConf.ConfVars.SCRATCHDIR));	FsPermission writableHDFSDirPermission = new FsPermission((short)00733);	FileSystem fs = rootHDFSDirPath.getFileSystem(conf);	if (!fs.exists(rootHDFSDirPath)) {	Utilities.createDirsWithPermission(conf, rootHDFSDirPath, writableHDFSDirPermission, true);	}	FsPermission currentHDFSDirPermission = fs.getFileStatus(rootHDFSDirPath).getPermission();	if (rootHDFSDirPath != null && rootHDFSDirPath.toUri() != null) {	String schema = rootHDFSDirPath.toUri().getScheme();	} else {	
hdfs root scratch dir permission 

FsPermission fsPermission = new FsPermission(permission);	FileSystem fs;	if (isLocal) {	fs = FileSystem.getLocal(conf);	} else {	fs = path.getFileSystem(conf);	}	if (!fs.exists(path)) {	fs.mkdirs(path, fsPermission);	String dirType = isLocal ? "local" : "HDFS";	
created directory 

private void dropSessionPaths(Configuration conf) throws IOException {	if (hdfsSessionPath != null) {	if (hdfsSessionPathLockFile != null) {	try {	hdfsSessionPathLockFile.close();	} catch (IOException e) {	
failed while closing remotefssessionlockfile 

private void dropPathAndUnregisterDeleteOnExit(Path path, Configuration conf, boolean localFs) {	FileSystem fs = null;	try {	if (localFs) {	fs = FileSystem.getLocal(conf);	} else {	fs = path.getFileSystem(conf);	}	fs.cancelDeleteOnExit(path);	fs.delete(path, true);	
deleted directory on fs with scheme 

if (authorizer == null) {	HiveAuthorizerFactory authorizerFactory = HiveUtils.getAuthorizerFactory(sessionConf, HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER);	HiveAuthzSessionContext.Builder authzContextBuilder = new HiveAuthzSessionContext.Builder();	authzContextBuilder.setClientType(isHiveServerQuery() ? CLIENT_TYPE.HIVESERVER2 : CLIENT_TYPE.HIVECLI);	authzContextBuilder.setSessionString(getSessionId());	authorizerV2 = authorizerFactory.createHiveAuthorizer(new HiveMetastoreClientFactoryImpl(), sessionConf, authenticator, authzContextBuilder.build());	setAuthorizerV2Config();	}	createTableGrants = CreateTableAutomaticGrant.create(sessionConf);	} catch (HiveException e) {	
error setting up authorization 

authzContextBuilder.setSessionString(getSessionId());	authorizerV2 = authorizerFactory.createHiveAuthorizer(new HiveMetastoreClientFactoryImpl(), sessionConf, authenticator, authzContextBuilder.build());	setAuthorizerV2Config();	}	createTableGrants = CreateTableAutomaticGrant.create(sessionConf);	} catch (HiveException e) {	throw new RuntimeException(e);	}	if(LOG.isDebugEnabled()){	Object authorizationClass = getActiveAuthorizer();	
session is using authorization class 

private void setAuthorizerV2Config() throws HiveException {	if (sessionConf.get(CONFIG_AUTHZ_SETTINGS_APPLIED_MARKER, "").equals(Boolean.TRUE.toString())) {	return;	}	String metastoreHook = sessionConf.get(ConfVars.METASTORE_FILTER_HOOK.name());	if (!ConfVars.METASTORE_FILTER_HOOK.getDefaultValue().equals(metastoreHook) && !AuthorizationMetaStoreFilterHook.class.getName().equals(metastoreHook)) {	
will be ignored since hive security authorization manager is set to instance of hiveauthorizerfactory 

public void close() throws IOException {	for (Closeable cleanupItem : cleanupItems) {	try {	cleanupItem.close();	} catch (Exception err) {	
error processing sessionstate cleanup item 

for (Closeable cleanupItem : cleanupItems) {	try {	cleanupItem.close();	} catch (Exception err) {	}	}	registry.clear();	if (txnMgr != null) txnMgr.closeTxnManager();	JavaUtils.closeClassLoadersTo(sessionConf.getClassLoader(), parentLoader);	File resourceDir = new File(getConf().getVar(HiveConf.ConfVars.DOWNLOADED_RESOURCES_DIR));	
removing resource dir 

}	registry.clear();	if (txnMgr != null) txnMgr.closeTxnManager();	JavaUtils.closeClassLoadersTo(sessionConf.getClassLoader(), parentLoader);	File resourceDir = new File(getConf().getVar(HiveConf.ConfVars.DOWNLOADED_RESOURCES_DIR));	try {	if (resourceDir.exists()) {	FileUtils.deleteDirectory(resourceDir);	}	} catch (IOException e) {	
error removing session resource dir 

}	} catch (IOException e) {	} finally {	detachSession();	}	try {	if (tezSessionState != null) {	TezSessionPoolManager.closeIfNotDefault(tezSessionState, false);	}	} catch (Exception e) {	
error closing tez session 

private void unCacheDataNucleusClassLoaders() {	try {	boolean isLocalMetastore = HiveConfUtil.isEmbeddedMetaStore(sessionConf.getVar(HiveConf.ConfVars.METASTOREURIS));	if (isLocalMetastore) {	if (sessionConf.getVar(ConfVars.METASTORE_RAW_STORE_IMPL) .equals(ObjectStore.class.getName()) || sessionConf.getVar(ConfVars.METASTORE_RAW_STORE_IMPL) .equals(CachedStore.class.getName()) && sessionConf .getVar(ConfVars.METASTORE_CACHED_RAW_STORE_IMPL) .equals(ObjectStore.class.getName())) {	ObjectStore.unCacheDataNucleusClassLoaders();	}	}	} catch (Exception e) {	
failed to remove classloaders from datanucleus 

public void closeSparkSession() {	if (sparkSession != null) {	try {	SparkSessionManagerImpl.getInstance().closeSession(sparkSession);	} catch (Exception ex) {	
error closing spark session 

========================= hive sample_5047 =========================

public void run() {	runner = Thread.currentThread();	try {	SessionState.start(ss);	runSequential();	} finally {	try {	Hive.closeCurrent();	} catch (Exception e) {	
exception closing metastore connection 

public void runSequential() {	int exitVal = -101;	try {	exitVal = tsk.executeTask(ss == null ? null : ss.getHiveHistory());	} catch (Throwable t) {	if (tsk.getException() == null) {	tsk.setException(t);	}	
error in executetask 

========================= hive sample_3890 =========================

public ClientResponse<Intermediate> handleResponse(HttpResponse httpResponse) {	try {	final HttpHeaders headers = httpResponse.headers();	manager.put(uri, Maps.asMap(headers.names(), input -> headers.getAll(input)));	}	catch (IOException e) {	
error while processing cookies from header 

========================= hive sample_11 =========================

}	}	FileSinkOperator fileSinkOp = (FileSinkOperator)stack.get(pos);	Path parentDirName = fileSinkOp.getConf().getDirName();	List<FileSinkDesc> fileDescLists = new ArrayList<FileSinkDesc>();	for (Operator<? extends OperatorDesc> parent : parents) {	FileSinkDesc fileSinkDesc = (FileSinkDesc) fileSinkOp.getConf().clone();	fileSinkDesc.setDirName(new Path(parentDirName, AbstractFileMergeOperator.UNION_SUDBIR_PREFIX + parent.getIdentifier()));	fileSinkDesc.setLinkedFileSink(true);	if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {	
created linkedfilesink for union parent 

========================= hive sample_3042 =========================

OptimizeSparkProcContext context = (OptimizeSparkProcContext) procContext;	ReduceSinkOperator sink = (ReduceSinkOperator) nd;	ReduceSinkDesc desc = sink.getConf();	Set<ReduceSinkOperator> parentSinks = null;	int maxReducers = context.getConf().getIntVar(HiveConf.ConfVars.MAXREDUCERS);	int constantReducers = context.getConf().getIntVar(HiveConf.ConfVars.HADOOPNUMREDUCERS);	if (!useOpStats) {	parentSinks = OperatorUtils.findOperatorsUpstream(sink, ReduceSinkOperator.class);	parentSinks.remove(sink);	if (!context.getVisitedReduceSinks().containsAll(parentSinks)) {	
skipping sink for now as we haven t seen all its parents 

int maxReducers = context.getConf().getIntVar(HiveConf.ConfVars.MAXREDUCERS);	int constantReducers = context.getConf().getIntVar(HiveConf.ConfVars.HADOOPNUMREDUCERS);	if (!useOpStats) {	parentSinks = OperatorUtils.findOperatorsUpstream(sink, ReduceSinkOperator.class);	parentSinks.remove(sink);	if (!context.getVisitedReduceSinks().containsAll(parentSinks)) {	return false;	}	}	if (context.getVisitedReduceSinks().contains(sink)) {	
already processed reduce sink 

if (!context.getVisitedReduceSinks().containsAll(parentSinks)) {	return false;	}	}	if (context.getVisitedReduceSinks().contains(sink)) {	return true;	}	context.getVisitedReduceSinks().add(sink);	if (needSetParallelism(sink, context.getConf())) {	if (constantReducers > 0) {	
parallelism for reduce sink set by user to 

context.getVisitedReduceSinks().add(sink);	if (needSetParallelism(sink, context.getConf())) {	if (constantReducers > 0) {	desc.setNumReducers(constantReducers);	} else {	FileSinkOperator fso = GenSparkUtils.getChildOperator(sink, FileSinkOperator.class);	if (fso != null) {	String bucketCount = fso.getConf().getTableInfo().getProperties().getProperty( hive_metastoreConstants.BUCKET_COUNT);	int numBuckets = bucketCount == null ? 0 : Integer.parseInt(bucketCount);	if (numBuckets > 0) {	
set parallelism for reduce sink to buckets 

return false;	}	}	if (useOpStats || parentSinks.isEmpty()) {	long numberOfBytes = 0;	if (useOpStats) {	for (Operator<? extends OperatorDesc> sibling : sink.getChildOperators().get(0).getParentOperators()) {	if (sibling.getStatistics() != null) {	numberOfBytes = StatsUtils.safeAdd(numberOfBytes, sibling.getStatistics().getDataSize());	if (LOG.isDebugEnabled()) {	
sibling has stats 

}	if (useOpStats || parentSinks.isEmpty()) {	long numberOfBytes = 0;	if (useOpStats) {	for (Operator<? extends OperatorDesc> sibling : sink.getChildOperators().get(0).getParentOperators()) {	if (sibling.getStatistics() != null) {	numberOfBytes = StatsUtils.safeAdd(numberOfBytes, sibling.getStatistics().getDataSize());	if (LOG.isDebugEnabled()) {	}	} else {	
no stats available from 

} else {	}	}	} else {	for (Operator<? extends OperatorDesc> sibling : sink.getChildOperators().get(0).getParentOperators()) {	Set<TableScanOperator> sources = OperatorUtils.findOperatorsUpstream(sibling, TableScanOperator.class);	for (TableScanOperator source : sources) {	if (source.getStatistics() != null) {	numberOfBytes = StatsUtils.safeAdd(numberOfBytes, source.getStatistics().getDataSize());	if (LOG.isDebugEnabled()) {	
table source has stats 

}	} else {	for (Operator<? extends OperatorDesc> sibling : sink.getChildOperators().get(0).getParentOperators()) {	Set<TableScanOperator> sources = OperatorUtils.findOperatorsUpstream(sibling, TableScanOperator.class);	for (TableScanOperator source : sources) {	if (source.getStatistics() != null) {	numberOfBytes = StatsUtils.safeAdd(numberOfBytes, source.getStatistics().getDataSize());	if (LOG.isDebugEnabled()) {	}	} else {	
no stats available from table source 

Set<TableScanOperator> sources = OperatorUtils.findOperatorsUpstream(sibling, TableScanOperator.class);	for (TableScanOperator source : sources) {	if (source.getStatistics() != null) {	numberOfBytes = StatsUtils.safeAdd(numberOfBytes, source.getStatistics().getDataSize());	if (LOG.isDebugEnabled()) {	}	} else {	}	}	}	
gathered stats for sink total size is bytes 

} else {	}	}	}	}	long bytesPerReducer = context.getConf().getLongVar(HiveConf.ConfVars.BYTESPERREDUCER) / 2;	int numReducers = Utilities.estimateReducers(numberOfBytes, bytesPerReducer, maxReducers, false);	getSparkMemoryAndCores(context);	if (sparkMemoryAndCores != null && sparkMemoryAndCores.getFirst() > 0 && sparkMemoryAndCores.getSecond() > 0) {	if ((double) sparkMemoryAndCores.getFirst() / bytesPerReducer < 0.5) {	
average load of a reducer is much larger than its available memory consider decreasing hive exec reducers bytes per reducer 

}	long bytesPerReducer = context.getConf().getLongVar(HiveConf.ConfVars.BYTESPERREDUCER) / 2;	int numReducers = Utilities.estimateReducers(numberOfBytes, bytesPerReducer, maxReducers, false);	getSparkMemoryAndCores(context);	if (sparkMemoryAndCores != null && sparkMemoryAndCores.getFirst() > 0 && sparkMemoryAndCores.getSecond() > 0) {	if ((double) sparkMemoryAndCores.getFirst() / bytesPerReducer < 0.5) {	}	numReducers = Math.max(numReducers, sparkMemoryAndCores.getSecond());	}	numReducers = Math.min(numReducers, maxReducers);	
set parallelism for reduce sink to calculated 

numReducers = Math.max(numReducers, sparkMemoryAndCores.getSecond());	}	numReducers = Math.min(numReducers, maxReducers);	desc.setNumReducers(numReducers);	} else {	int numberOfReducers = 0;	for (ReduceSinkOperator parent : parentSinks) {	numberOfReducers = Math.max(numberOfReducers, parent.getConf().getNumReducers());	}	desc.setNumReducers(numberOfReducers);	
set parallelism for sink to based on its parents 

}	desc.setNumReducers(numberOfReducers);	}	final Collection<ExprNodeDesc.ExprNodeDescEqualityWrapper> keyCols = ExprNodeDesc.ExprNodeDescEqualityWrapper.transform(desc.getKeyCols());	final Collection<ExprNodeDesc.ExprNodeDescEqualityWrapper> partCols = ExprNodeDesc.ExprNodeDescEqualityWrapper.transform(desc.getPartitionCols());	if (keyCols != null && keyCols.equals(partCols)) {	desc.setReducerTraits(EnumSet.of(UNIFORM));	}	}	} else {	
number of reducers for sink was already determined to be 

}	SparkSessionManager sparkSessionManager = null;	SparkSession sparkSession = null;	try {	sparkSessionManager = SparkSessionManagerImpl.getInstance();	sparkSession = SparkUtilities.getSparkSession( context.getConf(), sparkSessionManager);	sparkMemoryAndCores = sparkSession.getMemoryAndCores();	} catch (HiveException e) {	throw new SemanticException("Failed to get a spark session: " + e);	} catch (Exception e) {	
failed to get spark memory core info 

sparkSession = SparkUtilities.getSparkSession( context.getConf(), sparkSessionManager);	sparkMemoryAndCores = sparkSession.getMemoryAndCores();	} catch (HiveException e) {	throw new SemanticException("Failed to get a spark session: " + e);	} catch (Exception e) {	} finally {	if (sparkSession != null && sparkSessionManager != null) {	try {	sparkSessionManager.returnSession(sparkSession);	} catch (HiveException ex) {	
failed to return the session to sessionmanager 

========================= hive sample_3107 =========================

this.columnNames = ColumnProjectionUtils.getReadColumnNames(job);	final String fragmentId = LlapTezUtils.getFragmentId(job);	final String dagId = LlapTezUtils.getDagId(job);	final String queryId = HiveConf.getVar(job, HiveConf.ConfVars.HIVEQUERYID);	MDC.put("dagId", dagId);	MDC.put("queryId", queryId);	TezCounters taskCounters = null;	if (fragmentId != null) {	MDC.put("fragmentId", fragmentId);	taskCounters = FragmentCountersMap.getCountersForFragment(fragmentId);	
received fragment id 

final String fragmentId = LlapTezUtils.getFragmentId(job);	final String dagId = LlapTezUtils.getDagId(job);	final String queryId = HiveConf.getVar(job, HiveConf.ConfVars.HIVEQUERYID);	MDC.put("dagId", dagId);	MDC.put("queryId", queryId);	TezCounters taskCounters = null;	if (fragmentId != null) {	MDC.put("fragmentId", fragmentId);	taskCounters = FragmentCountersMap.getCountersForFragment(fragmentId);	} else {	
not using tez counters as fragment id string is null 

columnIds.add(i + ACID_FIELDS);	}	this.columnCount = columnIds.size();	} else {	this.columnIds = includedCols;	this.columnCount = columnIds.size();	}	int queueLimitBase = getQueueVar(ConfVars.LLAP_IO_VRB_QUEUE_LIMIT_BASE, job, daemonConf);	int queueLimitMin =  getQueueVar(ConfVars.LLAP_IO_VRB_QUEUE_LIMIT_MIN, job, daemonConf);	int limit = determineQueueLimit(queueLimitBase, queueLimitMin, rbCtx.getRowColumnTypeInfos());	
queue limit for llaprecordreader is 

private static MapWork findMapWork(JobConf job) throws HiveException {	String inputName = job.get(Utilities.INPUT_NAME, null);	if (LOG.isDebugEnabled()) {	
initializing for input 

private boolean checkOrcSchemaEvolution() {	SchemaEvolution evolution = rp.getSchemaEvolution();	for (int i = 0; i < columnCount; ++i) {	int projectedColId = columnIds == null ? i : columnIds.get(i);	int fileColId =  OrcInputFormat.getRootColumn(!isAcidScan) + projectedColId + 1;	if (!evolution.isPPDSafeConversion(fileColId)) {	
unsupported schema evolution disabling llap io for 

public void uncaughtException(final Thread t, final Throwable e) {	
unhandled error from reader thread threadname threadid message 

public void uncaughtException(final Thread t, final Throwable e) {	try {	setError(e);	} catch (InterruptedException e1) {	
iouncaughtexceptionhandler interrupted ignoring 

public void close() throws IOException {	if (LlapIoImpl.LOG.isTraceEnabled()) {	
close called closed interrupted err pending 

public void close() throws IOException {	if (LlapIoImpl.LOG.isTraceEnabled()) {	}	
maximum queue length observed 

public void close() throws IOException {	if (LlapIoImpl.LOG.isTraceEnabled()) {	}	
llap counters 

public void setDone() throws InterruptedException {	if (LlapIoImpl.LOG.isDebugEnabled()) {	
setdone called closed interrupted err pending 

public void consumeData(ColumnVectorBatch data) throws InterruptedException {	if (LlapIoImpl.LOG.isTraceEnabled()) {	
consume called closed interrupted err pending 

public void setError(Throwable t) throws InterruptedException {	counters.incrCounter(LlapIOCounters.NUM_ERRORS);	
seterror called closed interrupted err pending 

public void setError(Throwable t) throws InterruptedException {	counters.incrCounter(LlapIOCounters.NUM_ERRORS);	
seterror called with an error 

========================= hive sample_2192 =========================

}	ExprNodeConstantDesc c = (ExprNodeConstantDesc) condition;	if (!Boolean.FALSE.equals(c.getValue())) {	return null;	}	WalkerCtx ctx = (WalkerCtx) procCtx;	for (Node op : stack) {	if (op instanceof TableScanOperator) {	if (isNullOpPresentInAllBranches((TableScanOperator)op, filter)) {	ctx.setMayBeMetadataOnly((TableScanOperator)op);	
found where false tablescan 

LimitOperator limitOp = (LimitOperator)nd;	if(!(limitOp.getConf().getLimit() == 0)) {	return null;	}	HashSet<TableScanOperator> tsOps = ((WalkerCtx)procCtx).getMayBeMetadataOnlyTableScans();	if (tsOps != null) {	for (Iterator<TableScanOperator> tsOp = tsOps.iterator(); tsOp.hasNext();) {	if (!isNullOpPresentInAllBranches(tsOp.next(),limitOp)) tsOp.remove();	}	}	
found limit tablescan 

========================= hive sample_3015 =========================

public KerberosInfo getKerberosInfo(Class<?> protocol, Configuration conf) {	if (LOG.isDebugEnabled()) {	
trying to get kerberosinfo for 

public TokenInfo getTokenInfo(Class<?> protocol, Configuration conf) {	if (LOG.isDebugEnabled()) {	
trying to get tokeninfo for 

========================= hive sample_2184 =========================

hashMultiSet = (VectorMapJoinBytesHashMultiSet) vectorMapJoinHashTable;	needHashTableSetup = false;	}	batchCounter++;	for(VectorExpression ve : bigTableFilterExpressions) {	ve.evaluate(batch);	}	final int inputLogicalSize = batch.size;	if (inputLogicalSize == 0) {	if (LOG.isDebugEnabled()) {	
batch empty 

keyVectorSerializeWrite.serializeWrite(batch, 0);	JoinUtil.JoinResult joinResult;	if (keyVectorSerializeWrite.getHasAnyNulls()) {	joinResult = JoinUtil.JoinResult.NOMATCH;	} else {	byte[] keyBytes = currentKeyOutput.getData();	int keyLength = currentKeyOutput.getLength();	joinResult = hashMultiSet.contains(keyBytes, 0, keyLength, hashMultiSetResults[0]);	}	if (LOG.isDebugEnabled()) {	
batch repeated joinresult 

} else {	byte[] keyBytes = currentKeyOutput.getData();	int keyLength = currentKeyOutput.getLength();	joinResult = hashMultiSet.contains(keyBytes, 0, keyLength, hashMultiSetResults[0]);	}	if (LOG.isDebugEnabled()) {	}	finishInnerBigOnlyRepeated(batch, joinResult, hashMultiSetResults[0]);	} else {	if (LOG.isDebugEnabled()) {	
batch non repeated 

if (haveSaveKey) {	switch (saveJoinResult) {	case MATCH: equalKeySeriesCount++;	break;	case SPILL: hashMultiSetResultCount++;	break;	case NOMATCH: break;	}	}	if (LOG.isDebugEnabled()) {	
allmatchs equalkeyseriesvaluecounts equalkeyseriesallmatchindices equalkeyseriesduplicatecounts spills spillhashmapresultindices hashmapresults 

========================= hive sample_4115 =========================

public KerberosInfo getKerberosInfo(Class<?> protocol, Configuration conf) {	if (LOG.isDebugEnabled()) {	
trying to get kerberosinfo for 

public TokenInfo getTokenInfo(Class<?> protocol, Configuration conf) {	if (LOG.isDebugEnabled()) {	
trying to get tokeninfo for 

========================= hive sample_620 =========================

public Object dispatch(Node nd, Stack<Node> stack, Object... nodeOutputs) throws SemanticException {	Task<? extends Serializable> task = (Task<? extends Serializable>) nd;	if (task instanceof SparkTask) {	for (BaseWork baseWork : ((SparkTask) task).getWork().getAllWork()) {	Set<Operator<?>> pruningSinkOps = OperatorUtils.getOp(baseWork, SparkPartitionPruningSinkOperator.class);	for (Operator<?> op : pruningSinkOps) {	SparkPartitionPruningSinkOperator pruningSinkOp = (SparkPartitionPruningSinkOperator) op;	MapWork targetMapWork = pruningSinkOp.getConf().getTargetMapWork();	if (!taskContainsDependentMapWork(task, targetMapWork)) {	
disabling dpp for source work for target work as no dependency exists between the source and target work 

========================= hive sample_3003 =========================

for (Range childRange : childRanges) {	Range intersectedRange = andRange.clip(childRange, true);	if (null != intersectedRange) {	newRanges.add(intersectedRange);	}	}	}	andRanges = newRanges;	}	} else {	
expected range from but got 

protected Object processOrOpNode(Node nd, Object[] nodeOutputs) {	List<Range> orRanges = new ArrayList<Range>(nodeOutputs.length);	for (Object nodeOutput : nodeOutputs) {	if (nodeOutput instanceof Range) {	orRanges.add((Range) nodeOutput);	} else if (nodeOutput instanceof List) {	List<Range> childRanges = (List<Range>) nodeOutput;	orRanges.addAll(childRanges);	} else {	
expected range from but got 

========================= hive sample_159 =========================

public ChunkedOutputStream(OutputStream out, int bufSize, String id) {	
creating chunked input stream 

public void close() throws IOException {	flush();	writeChunk();	
closing underlying output stream 

private void writeChunk() throws IOException {	if (LOG.isDebugEnabled()) {	
writing chunk of size 

========================= hive sample_509 =========================

private void setupJAASConfig(Configuration conf) throws IOException {	if (!UserGroupInformation.getLoginUser().isFromKeytab()) {	
login is not from keytab 

public void ensurePath(String path, List<ACL> acl) throws TokenStoreException {	try {	CuratorFramework zk = getSession();	String node = zk.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT) .withACL(acl).forPath(path);	
created path 

case 'r': perm |= ZooDefs.Perms.READ;	break;	case 'w': perm |= ZooDefs.Perms.WRITE;	break;	case 'c': perm |= ZooDefs.Perms.CREATE;	break;	case 'd': perm |= ZooDefs.Perms.DELETE;	break;	case 'a': perm |= ZooDefs.Perms.ADMIN;	break;	
unknown perm type 

String[] aclComps = StringUtils.splitByWholeSeparator(aclString, ",");	List<ACL> acl = new ArrayList<ACL>(aclComps.length);	for (String a : aclComps) {	if (StringUtils.isBlank(a)) {	continue;	}	a = a.trim();	int firstColon = a.indexOf(':');	int lastColon = a.lastIndexOf(':');	if (firstColon == -1 || lastColon == -1 || firstColon == lastColon) {	
does not have the form scheme id perm 

public int addMasterKey(String s) {	String keysPath = rootNode + NODE_KEYS + "/";	CuratorFramework zk = getSession();	String newNode;	try {	newNode = zk.create().withMode(CreateMode.PERSISTENT_SEQUENTIAL).withACL(newNodeAcl) .forPath(keysPath, s.getBytes());	} catch (Exception e) {	throw new TokenStoreException("Error creating new node with path " + keysPath, e);	}	
added key 

public boolean addToken(DelegationTokenIdentifier tokenIdentifier, DelegationTokenInformation token) {	byte[] tokenBytes = MetastoreDelegationTokenSupport.encodeDelegationTokenInformation(token);	String tokenPath = getTokenPath(tokenIdentifier);	CuratorFramework zk = getSession();	String newNode;	try {	newNode = zk.create().withMode(CreateMode.PERSISTENT).withACL(newNodeAcl) .forPath(tokenPath, tokenBytes);	} catch (Exception e) {	throw new TokenStoreException("Error creating new node with path " + tokenPath, e);	}	
added token 

public List<DelegationTokenIdentifier> getAllDelegationTokenIdentifiers() {	String containerNode = rootNode + NODE_TOKENS;	final List<String> nodes = zkGetChildren(containerNode);	List<DelegationTokenIdentifier> result = new java.util.ArrayList<DelegationTokenIdentifier>( nodes.size());	for (String node : nodes) {	DelegationTokenIdentifier id = new DelegationTokenIdentifier();	try {	TokenStoreDelegationTokenSecretManager.decodeWritable(id, node);	result.add(id);	} catch (Exception e) {	
failed to decode token 

========================= hive sample_1878 =========================

private static void processSetColsNode(ASTNode setCols, ASTSearcher searcher) {	searcher.reset();	CommonTree rootNode = setCols;	while (rootNode != null && rootNode.getType() != HiveParser.TOK_INSERT) {	rootNode = rootNode.parent;	}	if (rootNode == null || rootNode.parent == null) {	
replacing setcolref with allcolref because we couldn t find the root insert 

rootNode = rootNode.parent;	Tree fromNode = null;	for (int j = 0; j < rootNode.getChildCount(); ++j) {	Tree child = rootNode.getChild(j);	if (child.getType() == HiveParser.TOK_FROM) {	fromNode = child;	break;	}	}	if (!(fromNode instanceof ASTNode)) {	
replacing setcolref with allcolref because we couldn t find the from 

Tree fromWhat = fromNode.getChild(0);	if (fromWhat.getType() == HiveParser.TOK_SUBQUERY && fromWhat.getChildCount() > 1) {	Tree child = fromWhat.getChild(fromWhat.getChildCount() - 1);	if (child.getType() == HiveParser.Identifier) {	alias = child.getText();	}	}	}	ASTNode select = searcher.simpleBreadthFirstSearchAny((ASTNode)fromNode, HiveParser.TOK_SELECT, HiveParser.TOK_SELECTDI);	if (select == null) {	
replacing setcolref with allcolref because we couldn t find the select 

if (select == null) {	setCols.token.setType(HiveParser.TOK_ALLCOLREF);	return;	}	while (true) {	CommonTree queryOfSelect = select.parent;	while (queryOfSelect != null && queryOfSelect.getType() != HiveParser.TOK_QUERY) {	queryOfSelect = queryOfSelect.parent;	}	if (queryOfSelect == null || queryOfSelect.parent == null) {	
replacing setcolref with allcolref because we couldn t find the query 

for (int i = 0; i < select.getChildCount(); ++i) {	Tree selExpr = select.getChild(i);	if (selExpr.getType() == HiveParser.QUERY_HINT) continue;	assert selExpr.getType() == HiveParser.TOK_SELEXPR;	assert selExpr.getChildCount() > 0;	Tree child = selExpr.getChild(selExpr.getChildCount() - 1);	switch (child.getType()) {	case HiveParser.TOK_SETCOLREF: processSetColsNode((ASTNode)child, searcher);	processSetColsNode(setCols, searcher);	return;	
replacing setcolref with allcolref because of nested allcolref 

break;	case HiveParser.DOT: {	Tree colChild = child.getChild(child.getChildCount() - 1);	assert colChild.getType() == HiveParser.Identifier : colChild;	if (!createChildColumnRef(colChild, alias, newChildren, aliases)) {	setCols.token.setType(HiveParser.TOK_ALLCOLREF);	return;	}	break;	}	
replacing setcolref with allcolref because of the nested node 

private static boolean createChildColumnRef(Tree child, String alias, List<ASTNode> newChildren, HashSet<String> aliases) {	String colAlias = child.getText();	if (!aliases.add(colAlias)) {	
replacing setcolref with allcolref because of duplicate alias 

========================= hive sample_3392 =========================

private PartitionDesc changePartitionToMetadataOnly(PartitionDesc desc, Path path) {	if (desc == null) return null;	boolean isEmpty = false;	try {	isEmpty = Utilities.isEmptyPath(physicalContext.getConf(), path);	} catch (IOException e) {	
cannot determine if the table is empty 

Task<? extends Serializable> task = (Task<? extends Serializable>) nd;	ParseContext parseContext = physicalContext.getParseContext();	WalkerCtx walkerCtx = new WalkerCtx();	List<MapWork> mapWorks = new ArrayList<MapWork>(task.getMapWork());	Collections.sort(mapWorks, new Comparator<MapWork>() {	public int compare(MapWork o1, MapWork o2) {	return o1.getName().compareTo(o2.getName());	}	});	for (MapWork mapWork : mapWorks) {	
looking at 

WalkerCtx walkerCtx = new WalkerCtx();	List<MapWork> mapWorks = new ArrayList<MapWork>(task.getMapWork());	Collections.sort(mapWorks, new Comparator<MapWork>() {	public int compare(MapWork o1, MapWork o2) {	return o1.getName().compareTo(o2.getName());	}	});	for (MapWork mapWork : mapWorks) {	Collection<Operator<? extends OperatorDesc>> topOperators = mapWork.getAliasToWork().values();	if (topOperators.size() == 0) {	
no top operators 

Collections.sort(mapWorks, new Comparator<MapWork>() {	public int compare(MapWork o1, MapWork o2) {	return o1.getName().compareTo(o2.getName());	}	});	for (MapWork mapWork : mapWorks) {	Collection<Operator<? extends OperatorDesc>> topOperators = mapWork.getAliasToWork().values();	if (topOperators.size() == 0) {	return null;	}	
looking for table scans where optimization is applicable 

for (Operator<? extends OperatorDesc> workOperator : topOperators) {	if (parseContext.getTopOps().values().contains(workOperator)) {	topNodes.add(workOperator);	}	}	Operator<? extends OperatorDesc> reducer = task.getReducer(mapWork);	if (reducer != null) {	topNodes.add(reducer);	}	ogw.startWalking(topNodes, null);	
found d null table scans 

========================= hive sample_2995 =========================

public void print(String msg) {	LOG.info(msg + "alias=" + qbp.getAlias());	for (String alias : getSubqAliases()) {	QBExpr qbexpr = getSubqForAlias(alias);	
start subquery 

public void print(String msg) {	LOG.info(msg + "alias=" + qbp.getAlias());	for (String alias : getSubqAliases()) {	QBExpr qbexpr = getSubqForAlias(alias);	qbexpr.print(msg + " ");	
end subquery 

========================= hive sample_3404 =========================

public CliDriver() {	SessionState ss = SessionState.get();	conf = (ss != null) ? ss.getConf() : new Configuration();	Logger LOG = LoggerFactory.getLogger("CliDriver");	if (LOG.isDebugEnabled()) {	
clidriver inited with classpath java class path 

========================= hive sample_1086 =========================

protected void initializeChildren(Configuration hconf) throws HiveException {	state = State.INIT;	if (LOG.isInfoEnabled()) {	
operator initialized 

protected void initializeChildren(Configuration hconf) throws HiveException {	state = State.INIT;	if (LOG.isInfoEnabled()) {	}	if (childOperators == null || childOperators.isEmpty()) {	return;	}	if (LOG.isInfoEnabled()) {	
initializing children of 

========================= hive sample_3865 =========================

public void operationComplete(ChannelFuture future) {	writeResources.release();	if (future.isCancelled()) {	
write cancelled on id 

public void operationComplete(ChannelFuture future) {	writeResources.release();	if (future.isCancelled()) {	} else if (!future.isSuccess()) {	
write error on id 

public void operationComplete(ChannelFuture future) {	if (future.isCancelled()) {	
close cancelled on id 

public void operationComplete(ChannelFuture future) {	if (future.isCancelled()) {	} else if (!future.isSuccess()) {	
close failed on id 

public void close() throws IOException {	if (closed) {	throw new IOException("Already closed: " + id);	}	try {	flush();	} catch (IOException err) {	
error flushing stream before close on 

========================= hive sample_2798 =========================

public void close() {	
closing merge operator 

========================= hive sample_4565 =========================

String colType = cols.get(iKey).getTypeName();	boolean isKeyComparable = isKeyBinary || colType.equalsIgnoreCase("string");	String tsColName = null;	if (iTimestamp >= 0) {	tsColName = jobConf.get(serdeConstants.LIST_COLUMNS).split(",")[iTimestamp];	}	IndexPredicateAnalyzer analyzer = newIndexPredicateAnalyzer(keyColName, isKeyComparable, tsColName);	List<IndexSearchCondition> conditions = new ArrayList<IndexSearchCondition>();	ExprNodeDesc residualPredicate = analyzer.analyzePredicate(filterExpr, conditions);	if (residualPredicate != null) {	
ignoring residual predicate 

========================= hive sample_600 =========================

tab_alias = tab_alias.toLowerCase();	}	boolean colPresent = invRslvMap.containsKey(colInfo.getInternalName());	LinkedHashMap<String, ColumnInfo> f_map = rslvMap.get(tab_alias);	if (f_map == null) {	f_map = new LinkedHashMap<String, ColumnInfo>();	rslvMap.put(tab_alias, f_map);	}	ColumnInfo oldColInfo = f_map.put(col_alias, colInfo);	if (oldColInfo != null) {	
duplicate column info for was overwritten in rowresolver map by 

public boolean putWithCheck(String tabAlias, String colAlias, String internalName, ColumnInfo newCI) throws SemanticException {	ColumnInfo existing = get(tabAlias, colAlias);	if (existing == null) {	put(tabAlias, colAlias, newCI);	return true;	} else if (existing.isSameColumnForRR(newCI)) {	return true;	}	
found duplicate column alias in rr adding 

return true;	}	if (internalName != null) {	existing = get(tabAlias, internalName);	if (existing == null) {	put(tabAlias, internalName, newCI);	return true;	} else if (existing.isSameColumnForRR(newCI)) {	return true;	}	
failed to use internal name after finding a duplicate 

public static RowResolver getCombinedRR(RowResolver leftRR, RowResolver rightRR) throws SemanticException {	RowResolver combinedRR = new RowResolver();	IntRef outputColPos = new IntRef();	if (!add(combinedRR, leftRR, outputColPos)) {	
duplicates detected when adding columns to rr see previous message 

public static RowResolver getCombinedRR(RowResolver leftRR, RowResolver rightRR) throws SemanticException {	RowResolver combinedRR = new RowResolver();	IntRef outputColPos = new IntRef();	if (!add(combinedRR, leftRR, outputColPos)) {	}	if (!add(combinedRR, rightRR, outputColPos)) {	
duplicates detected when adding columns to rr see previous message 

========================= hive sample_3561 =========================

field.setAccessible(true);	Stack ndcStack = (Stack) field.get(callableWithNdc);	final Stack clonedStack = (Stack) ndcStack.clone();	final String fragmentId = (String) clonedStack.pop();	final String queryId = (String) clonedStack.pop();	final String dagId = (String) clonedStack.pop();	MDC.put("dagId", dagId);	MDC.put("queryId", queryId);	MDC.put("fragmentId", fragmentId);	if (LOG.isDebugEnabled()) {	
received dagid queryid instancetype 

final Stack clonedStack = (Stack) ndcStack.clone();	final String fragmentId = (String) clonedStack.pop();	final String queryId = (String) clonedStack.pop();	final String dagId = (String) clonedStack.pop();	MDC.put("dagId", dagId);	MDC.put("queryId", queryId);	MDC.put("fragmentId", fragmentId);	if (LOG.isDebugEnabled()) {	}	} catch (Exception e) {	
not setting up mdc as ndc stack cannot be accessed reflectively for instance type exception type 

final String queryId = (String) clonedStack.pop();	final String dagId = (String) clonedStack.pop();	MDC.put("dagId", dagId);	MDC.put("queryId", queryId);	MDC.put("fragmentId", fragmentId);	if (LOG.isDebugEnabled()) {	}	} catch (Exception e) {	}	} else {	
not setting up mdc as unknown callable instance type received 

readOpsDelta = threadFSStats.getReadOps();	largeReadOpsDelta = threadFSStats.getLargeReadOps();	writeOpsDelta = threadFSStats.getWriteOps();	}	tezCounters.findCounter(scheme, FileSystemCounter.BYTES_READ) .increment(bytesReadDelta);	tezCounters.findCounter(scheme, FileSystemCounter.BYTES_WRITTEN) .increment(bytesWrittenDelta);	tezCounters.findCounter(scheme, FileSystemCounter.READ_OPS).increment(readOpsDelta);	tezCounters.findCounter(scheme, FileSystemCounter.LARGE_READ_OPS) .increment(largeReadOpsDelta);	tezCounters.findCounter(scheme, FileSystemCounter.WRITE_OPS) .increment(writeOpsDelta);	if (LOG.isDebugEnabled()) {	
updated stats instance thread name thread id scheme bytesread byteswritten readops largereadops writeops 

}	tezCounters.findCounter(scheme, FileSystemCounter.BYTES_READ) .increment(bytesReadDelta);	tezCounters.findCounter(scheme, FileSystemCounter.BYTES_WRITTEN) .increment(bytesWrittenDelta);	tezCounters.findCounter(scheme, FileSystemCounter.READ_OPS).increment(readOpsDelta);	tezCounters.findCounter(scheme, FileSystemCounter.LARGE_READ_OPS) .increment(largeReadOpsDelta);	tezCounters.findCounter(scheme, FileSystemCounter.WRITE_OPS) .increment(writeOpsDelta);	if (LOG.isDebugEnabled()) {	}	}	} else {	
file system statistics snapshot before execution of thread is null thread name id allstats 

tezCounters.findCounter(scheme, FileSystemCounter.BYTES_WRITTEN) .increment(bytesWrittenDelta);	tezCounters.findCounter(scheme, FileSystemCounter.READ_OPS).increment(readOpsDelta);	tezCounters.findCounter(scheme, FileSystemCounter.LARGE_READ_OPS) .increment(largeReadOpsDelta);	tezCounters.findCounter(scheme, FileSystemCounter.WRITE_OPS) .increment(writeOpsDelta);	if (LOG.isDebugEnabled()) {	}	}	} else {	}	} else {	
tezcounters is null for callable type 

========================= hive sample_2235 =========================

public void endEvent(final WmTezSession sessionState) {	this.wmTezSessionInfo = new WmTezSessionInfo(sessionState);	this.eventEndTimestamp = System.currentTimeMillis();	this.elapsedTime = eventEndTimestamp - eventStartTimestamp;	WmContext wmContext = sessionState.getWmContext();	if (wmContext != null) {	wmContext.addWMEvent(this);	
added wmevent 

========================= hive sample_3977 =========================

public T run() throws Exception {	int attempt = 0;	while (true) {	int size = getNextBatchSize();	if (size == 0) {	throw new RetryException("Batch size reduced to zero");	}	try {	return execute(size);	} catch (Exception ex) {	
exception thrown while processing using a batch size d 

========================= hive sample_1419 =========================

switch (tabRef.getType()) {	case HiveParser.TOK_TABREF: tableName = getQualifiedTableName((ASTNode) tabRef.getChild(0));	break;	case HiveParser.TOK_TABNAME: tableName = getQualifiedTableName(tabRef);	break;	default: throw raiseWrongType("TOK_TABREF|TOK_TABNAME", tabRef);	}	try {	mTable = db.getTable(tableName[0], tableName[1]);	} catch (InvalidTableException e) {	
failed to find table got exception 

break;	case HiveParser.TOK_TABNAME: tableName = getQualifiedTableName(tabRef);	break;	default: throw raiseWrongType("TOK_TABREF|TOK_TABNAME", tabRef);	}	try {	mTable = db.getTable(tableName[0], tableName[1]);	} catch (InvalidTableException e) {	throw new SemanticException(ErrorMsg.INVALID_TABLE.getMsg(getDotName(tableName)), e);	} catch (HiveException e) {	
failed to find table got exception 

rewrittenCtx = new Context(conf);	ctx.addRewrittenStatementContext(rewrittenCtx);	} catch (IOException e) {	throw new SemanticException(ErrorMsg.UPDATEDELETE_IO_ERROR.getMsg());	}	rewrittenCtx.setExplainConfig(ctx.getExplainConfig());	rewrittenCtx.setIsUpdateDeleteMerge(true);	rewrittenCtx.setCmd(rewrittenQueryStr.toString());	ASTNode rewrittenTree;	try {	
going to reparse as 

private void validateTargetTable(Table mTable) throws SemanticException {	if (mTable.getTableType() == TableType.VIRTUAL_VIEW || mTable.getTableType() == TableType.MATERIALIZED_VIEW) {	
table is a view or materialized view 

private boolean handleCardinalityViolation(StringBuilder rewrittenQueryStr, ASTNode target, String onClauseAsString, Table targetTable, boolean onlyHaveWhenNotMatchedClause) throws SemanticException {	if(!conf.getBoolVar(HiveConf.ConfVars.MERGE_CARDINALITY_VIOLATION_CHECK)) {	
merge statement cardinality violation check is disabled 

========================= hive sample_3424 =========================

public AccumuloSerDeParameters(Configuration conf, Properties tableProperties, String serdeName) throws SerDeException {	super(conf);	this.tableProperties = tableProperties;	this.serdeName = serdeName;	lazySerDeParameters = new LazySerDeParameters(conf, tableProperties, serdeName);	indexParams = new AccumuloIndexParameters(conf);	String defaultStorage = tableProperties.getProperty(DEFAULT_STORAGE_TYPE);	columnMapper = new ColumnMapper(getColumnMappingValue(), defaultStorage, lazySerDeParameters.getColumnNames(), lazySerDeParameters.getColumnTypes());	
constructed column mapping 

protected AccumuloRowIdFactory createRowIdFactory(Configuration job, Properties tbl) throws Exception {	String factoryClassName = tbl.getProperty(COMPOSITE_ROWID_FACTORY);	if (factoryClassName != null) {	
loading compositerowidfactory class 

protected AccumuloRowIdFactory createRowIdFactory(Configuration job, Properties tbl) throws Exception {	String factoryClassName = tbl.getProperty(COMPOSITE_ROWID_FACTORY);	if (factoryClassName != null) {	Class<?> factoryClazz = JavaUtils.loadClass(factoryClassName);	return (AccumuloRowIdFactory) ReflectionUtils.newInstance(factoryClazz, job);	}	String keyClassName = tbl.getProperty(COMPOSITE_ROWID_CLASS);	if (keyClassName != null) {	
loading compositerowid class 

========================= hive sample_207 =========================

public void initialize(Configuration conf, Properties tbl) throws SerDeException {	inputRegex = tbl.getProperty(INPUT_REGEX);	String columnNameProperty = tbl.getProperty(serdeConstants.LIST_COLUMNS);	String columnTypeProperty = tbl.getProperty(serdeConstants.LIST_COLUMN_TYPES);	boolean inputRegexIgnoreCase = "true".equalsIgnoreCase(tbl .getProperty(INPUT_REGEX_CASE_SENSITIVE));	if (null != tbl.getProperty("output.format.string")) {	
output format string has been deprecated 

public Object deserialize(Writable blob) throws SerDeException {	Text rowText = (Text) blob;	Matcher m = inputPattern.matcher(rowText.toString());	if (m.groupCount() != numColumns) {	throw new SerDeException("Number of matching groups doesn't match the number of columns");	}	if (!m.matches()) {	unmatchedRowsCount++;	if (!alreadyLoggedNoMatch) {	
unmatched rows are found 

row.set(c, hc);	break;	case VARCHAR: HiveVarchar hv = new HiveVarchar(t, ((VarcharTypeInfo)typeInfo).getLength());	row.set(c, hv);	break;	default: throw new SerDeException("Unsupported type " + typeInfo);	}	} catch (RuntimeException e) {	partialMatchedRowsCount++;	if (!alreadyLoggedPartialMatch) {	
partially unmatched rows are found cannot find group 

========================= hive sample_5289 =========================

public int execute(DriverContext driverContext) {	try {	Hive db = getHive();	return persistColumnStats(db);	} catch (Exception e) {	
failed to persist stats in metastore 

private Date readDateValue(String dateStr) {	try {	DateWritable writableVal = new DateWritable(java.sql.Date.valueOf(dateStr));	return new Date(writableVal.getDays());	} catch (IllegalArgumentException err) {	
reading date value as days since epoch 

========================= hive sample_4027 =========================

data.set(textData.toString(), maxLength);	isNull = false;	} else {	String byteData = null;	try {	byteData = Text.decode(bytes.getData(), start, length);	data.set(byteData, maxLength);	isNull = false;	} catch (CharacterCodingException e) {	isNull = true;	
data not in the hivechar data type range so converted to null 

========================= hive sample_5509 =========================

protected TTransport connectWithRetry(int retries) throws HiveSQLException {	TTransportException exception = null;	for (int i = 0 ; i < retries; i++) {	try {	return connect(conf);	} catch (TTransportException e) {	exception = e;	
connection attempt 

TTransportException exception = null;	for (int i = 0 ; i < retries; i++) {	try {	return connect(conf);	} catch (TTransportException e) {	exception = e;	}	try {	Thread.sleep(retryDelaySeconds * 1000);	} catch (InterruptedException e) {	
Interrupted 

protected synchronized TTransport connect(HiveConf conf) throws HiveSQLException, TTransportException {	if (transport != null && transport.isOpen()) {	transport.close();	}	String host = conf.getVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_BIND_HOST);	int port = conf.getIntVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_PORT);	
connecting to 

if (transport != null && transport.isOpen()) {	transport.close();	}	String host = conf.getVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_BIND_HOST);	int port = conf.getIntVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_PORT);	transport = new TSocket(host, port);	((TSocket) transport).setTimeout((int) conf.getTimeVar(HiveConf.ConfVars.SERVER_READ_SOCKET_TIMEOUT, TimeUnit.SECONDS) * 1000);	try {	((TSocket) transport).getSocket().setKeepAlive(conf.getBoolVar(HiveConf.ConfVars.SERVER_TCP_KEEP_ALIVE));	} catch (SocketException e) {	
error setting keep alive to 

((TSocket) transport).setTimeout((int) conf.getTimeVar(HiveConf.ConfVars.SERVER_READ_SOCKET_TIMEOUT, TimeUnit.SECONDS) * 1000);	try {	((TSocket) transport).getSocket().setKeepAlive(conf.getBoolVar(HiveConf.ConfVars.SERVER_TCP_KEEP_ALIVE));	} catch (SocketException e) {	}	String userName = conf.getVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_CLIENT_USER);	String passwd = conf.getVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_CLIENT_PASSWORD);	try {	transport = PlainSaslHelper.getPlainTransport(userName, passwd, transport);	} catch (SaslException e) {	
error creating plain sasl transport 

}	String userName = conf.getVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_CLIENT_USER);	String passwd = conf.getVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_CLIENT_PASSWORD);	try {	transport = PlainSaslHelper.getPlainTransport(userName, passwd, transport);	} catch (SaslException e) {	}	TProtocol protocol = new TBinaryProtocol(transport);	transport.open();	base = new ThriftCLIServiceClient(new TCLIService.Client(protocol), conf);	
connected 

public Object invoke(Object o, Method method, Object[] args) throws Throwable {	int attempts = 0;	while (true) {	attempts++;	InvocationResult invokeResult = invokeInternal(method, args);	if (invokeResult.success) {	return invokeResult.result;	}	connectWithRetry(conf.getIntVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_CLIENT_CONNECTION_RETRY_LIMIT));	if (attempts >=  retryLimit) {	
failed after retries 

========================= hive sample_2366 =========================

List<IOSpecProto> inputSpecList = vertexSpec.getInputSpecsList();	boolean canFinish = true;	if (inputSpecList != null && !inputSpecList.isEmpty()) {	for (IOSpecProto inputSpec : inputSpecList) {	if (LlapTezUtils.isSourceOfInterest(inputSpec.getIoDescriptor().getClassName())) {	LlapDaemonProtocolProtos.SourceStateProto state = queryInfo.getSourceStateMap() .get(inputSpec.getConnectedVertexName());	if (state != null && state == LlapDaemonProtocolProtos.SourceStateProto.S_SUCCEEDED) {	continue;	} else {	if (LOG.isDebugEnabled()) {	
cannot finish due to source 

========================= hive sample_2223 =========================

public static PrimitiveTypeInfo convertDruidToHiveType(String typeName) {	typeName = typeName.toUpperCase();	switch (typeName) {	case FLOAT_TYPE: return TypeInfoFactory.floatTypeInfo;	case DOUBLE_TYPE: return TypeInfoFactory.doubleTypeInfo;	case LONG_TYPE: return TypeInfoFactory.longTypeInfo;	case STRING_TYPE: return TypeInfoFactory.stringTypeInfo;	
transformation to string for unknown type 

========================= hive sample_31 =========================

public synchronized void close() throws IOException {	if (!closed) {	closed = true;	Exception caughtException = null;	try {	din.close();	} catch (Exception err) {	
error closing input stream 

Exception caughtException = null;	try {	din.close();	} catch (Exception err) {	caughtException = err;	}	if (client != null) {	try {	client.close();	} catch (Exception err) {	
error closing client 

default: throw new IOException("Got reader event type " + event.getEventType() + ", expected error event", io);	}	}	} else {	throw io;	}	} finally {	try {	close();	} catch (Exception err) {	
closing recordreader due to error and hit another error during close 

public void handleEvent(ReaderEvent event) {	switch (event.getEventType()) {	case DONE: readerEvents.add(event);	break;	case ERROR: readerEvents.add(event);	if (readerThread == null) {	throw new RuntimeException("Reader thread is unexpectedly null, during ReaderEvent error " + event.getMessage());	}	if (LOG.isDebugEnabled()) {	
interrupting reader thread due to reader event with error 

case ERROR: readerEvents.add(event);	if (readerThread == null) {	throw new RuntimeException("Reader thread is unexpectedly null, during ReaderEvent error " + event.getMessage());	}	if (LOG.isDebugEnabled()) {	}	readerThread.interrupt();	try {	socket.close();	} catch (IOException e) {	
cannot close the socket on error 

========================= hive sample_641 =========================

private void runCycleAnalysisForPartitionPruning(OptimizeTezProcContext procCtx, Set<ReadEntity> inputs, Set<WriteEntity> outputs) throws SemanticException {	if (!procCtx.conf.getBoolVar(ConfVars.TEZ_DYNAMIC_PARTITION_PRUNING)) {	return;	}	boolean cycleFree = false;	while (!cycleFree) {	cycleFree = true;	Set<Set<Operator<?>>> components = getComponents(procCtx);	for (Set<Operator<?>> component : components) {	if (LOG.isDebugEnabled()) {	
component 

if (!procCtx.conf.getBoolVar(ConfVars.TEZ_DYNAMIC_PARTITION_PRUNING)) {	return;	}	boolean cycleFree = false;	while (!cycleFree) {	cycleFree = true;	Set<Set<Operator<?>>> components = getComponents(procCtx);	for (Set<Operator<?>> component : components) {	if (LOG.isDebugEnabled()) {	for (Operator<?> co : component) {	
operator 

boolean cycleFree = false;	while (!cycleFree) {	cycleFree = true;	Set<Set<Operator<?>>> components = getComponents(procCtx);	for (Set<Operator<?>> component : components) {	if (LOG.isDebugEnabled()) {	for (Operator<?> co : component) {	}	}	if (component.size() != 1) {	
found cycle in operator plan 

if (LOG.isDebugEnabled()) {	for (Operator<?> co : component) {	}	}	if (component.size() != 1) {	cycleFree = false;	removeCycleOperator(component, procCtx);	break;	}	}	
cycle free 

}	if (hasHint && !removed) {	throw new SemanticException("The user hint is causing an operator cycle. Please fix it and retry");	}	if (victim == null || (!context.pruningOpsRemovedByPriorOpt.isEmpty() && context.pruningOpsRemovedByPriorOpt.contains(victim))) {	return;	}	GenTezUtils.removeBranch(victim);	if (victim == victimRS) {	if (LOG.isDebugEnabled()) {	
cycle found removing semijoin 

}	if (victim == null || (!context.pruningOpsRemovedByPriorOpt.isEmpty() && context.pruningOpsRemovedByPriorOpt.contains(victim))) {	return;	}	GenTezUtils.removeBranch(victim);	if (victim == victimRS) {	if (LOG.isDebugEnabled()) {	}	GenTezUtils.removeSemiJoinOperator(context.parseContext, victimRS, victimTS);	} else {	
disabling dynamic pruning for needed to break cyclic dependency 

private void connect(Operator<?> o, AtomicInteger index, Stack<Operator<?>> nodes, Map<Operator<?>, Integer> indexes, Map<Operator<?>, Integer> lowLinks, Set<Set<Operator<?>>> components, ParseContext parseContext) {	indexes.put(o, index.get());	lowLinks.put(o, index.get());	index.incrementAndGet();	nodes.push(o);	List<Operator<?>> children;	if (o instanceof AppMasterEventOperator) {	children = new ArrayList<Operator<?>>();	children.addAll(o.getChildOperators());	TableScanOperator ts = ((DynamicPruningEventDesc) o.getConf()).getTableScan();	
adding special edge 

children = new ArrayList<Operator<?>>();	children.addAll(o.getChildOperators());	TableScanOperator ts = ((DynamicPruningEventDesc) o.getConf()).getTableScan();	children.add(ts);	} else if (o instanceof ReduceSinkOperator){	children = new ArrayList<Operator<?>>();	children.addAll(o.getChildOperators());	SemiJoinBranchInfo sjInfo = parseContext.getRsToSemiJoinBranchInfo().get(o);	if (sjInfo != null ) {	TableScanOperator ts = sjInfo.getTsOp();	
adding special edge 

GenTezUtils.removeUnionOperators(procCtx, w, indexForTezUnion++);	}	for (FileSinkOperator fileSink: procCtx.fileSinkSet) {	GenTezUtils.processFileSink(procCtx, fileSink);	}	if (pCtx.getRsToRuntimeValuesInfoMap().size() > 0) {	for (ReduceSinkOperator rs : pCtx.getRsToRuntimeValuesInfoMap().keySet()) {	GenTezUtils.processDynamicSemiJoinPushDownOperator( procCtx, pCtx.getRsToRuntimeValuesInfoMap().get(rs), rs);	}	}	
there are app master events 

}	for (FileSinkOperator fileSink: procCtx.fileSinkSet) {	GenTezUtils.processFileSink(procCtx, fileSink);	}	if (pCtx.getRsToRuntimeValuesInfoMap().size() > 0) {	for (ReduceSinkOperator rs : pCtx.getRsToRuntimeValuesInfoMap().keySet()) {	GenTezUtils.processDynamicSemiJoinPushDownOperator( procCtx, pCtx.getRsToRuntimeValuesInfoMap().get(rs), rs);	}	}	for (AppMasterEventOperator event : procCtx.eventOperatorSet) {	
handling appmastereventoperator 

protected void optimizeTaskPlan(List<Task<? extends Serializable>> rootTasks, ParseContext pCtx, Context ctx) throws SemanticException {	PerfLogger perfLogger = SessionState.getPerfLogger();	perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);	PhysicalContext physicalCtx = new PhysicalContext(conf, pCtx, pCtx.getContext(), rootTasks, pCtx.getFetchTask());	if (conf.getBoolVar(HiveConf.ConfVars.HIVENULLSCANOPTIMIZE)) {	physicalCtx = new NullScanOptimizer().resolve(physicalCtx);	} else {	
skipping null scan query optimization 

PerfLogger perfLogger = SessionState.getPerfLogger();	perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);	PhysicalContext physicalCtx = new PhysicalContext(conf, pCtx, pCtx.getContext(), rootTasks, pCtx.getFetchTask());	if (conf.getBoolVar(HiveConf.ConfVars.HIVENULLSCANOPTIMIZE)) {	physicalCtx = new NullScanOptimizer().resolve(physicalCtx);	} else {	}	if (conf.getBoolVar(HiveConf.ConfVars.HIVEMETADATAONLYQUERIES)) {	physicalCtx = new MetadataOnlyOptimizer().resolve(physicalCtx);	} else {	
skipping metadata only query optimization 

physicalCtx = new NullScanOptimizer().resolve(physicalCtx);	} else {	}	if (conf.getBoolVar(HiveConf.ConfVars.HIVEMETADATAONLYQUERIES)) {	physicalCtx = new MetadataOnlyOptimizer().resolve(physicalCtx);	} else {	}	if (conf.getBoolVar(HiveConf.ConfVars.HIVE_CHECK_CROSS_PRODUCT)) {	physicalCtx = new CrossProductHandler().resolve(physicalCtx);	} else {	
skipping cross product analysis 

physicalCtx = new MetadataOnlyOptimizer().resolve(physicalCtx);	} else {	}	if (conf.getBoolVar(HiveConf.ConfVars.HIVE_CHECK_CROSS_PRODUCT)) {	physicalCtx = new CrossProductHandler().resolve(physicalCtx);	} else {	}	if ("llap".equalsIgnoreCase(conf.getVar(HiveConf.ConfVars.HIVE_EXECUTION_MODE))) {	physicalCtx = new LlapPreVectorizationPass().resolve(physicalCtx);	} else {	
skipping llap pre vectorization pass 

physicalCtx = new CrossProductHandler().resolve(physicalCtx);	} else {	}	if ("llap".equalsIgnoreCase(conf.getVar(HiveConf.ConfVars.HIVE_EXECUTION_MODE))) {	physicalCtx = new LlapPreVectorizationPass().resolve(physicalCtx);	} else {	}	if (conf.getBoolVar(HiveConf.ConfVars.HIVE_VECTORIZATION_ENABLED) && ctx.getExplainAnalyze() == null) {	physicalCtx = new Vectorizer().resolve(physicalCtx);	} else {	
skipping vectorization 

physicalCtx = new LlapPreVectorizationPass().resolve(physicalCtx);	} else {	}	if (conf.getBoolVar(HiveConf.ConfVars.HIVE_VECTORIZATION_ENABLED) && ctx.getExplainAnalyze() == null) {	physicalCtx = new Vectorizer().resolve(physicalCtx);	} else {	}	if (!"none".equalsIgnoreCase(conf.getVar(HiveConf.ConfVars.HIVESTAGEIDREARRANGE))) {	physicalCtx = new StageIDsRearranger().resolve(physicalCtx);	} else {	
skipping stage id rearranger 

physicalCtx = new StageIDsRearranger().resolve(physicalCtx);	} else {	}	if ((conf.getBoolVar(HiveConf.ConfVars.HIVE_TEZ_ENABLE_MEMORY_MANAGER)) && (conf.getBoolVar(HiveConf.ConfVars.HIVEUSEHYBRIDGRACEHASHJOIN))) {	physicalCtx = new MemoryDecider().resolve(physicalCtx);	}	if ("llap".equalsIgnoreCase(conf.getVar(HiveConf.ConfVars.HIVE_EXECUTION_MODE))) {	LlapClusterStateForCompile llapInfo = LlapClusterStateForCompile.getClusterInfo(conf);	physicalCtx = new LlapDecider(llapInfo).resolve(physicalCtx);	} else {	
skipping llap decider 

ParseContext pctx = procCtx.parseContext;	Set<ReduceSinkOperator> rsSet = new HashSet<>(pctx.getRsToSemiJoinBranchInfo().keySet());	for (TableScanOperator ts : tsOps) {	for (ReduceSinkOperator rs : rsSet) {	SemiJoinBranchInfo sjInfo = pctx.getRsToSemiJoinBranchInfo().get(rs);	if (sjInfo != null && ts == sjInfo.getTsOp()) {	if (sjInfo.getIsHint()) {	throw new SemanticException("Removing hinted semijoin as it is with SMB join " + rs + " : " + ts);	}	if (LOG.isDebugEnabled()) {	
semijoin optimization found going to smb join removing semijoin 

}	assert parent instanceof ReduceSinkOperator;	while (parent.getParentOperators().size() > 0) {	parent = parent.getParentOperators().get(0);	}	if (parent == ts) {	if (sjInfo.getIsHint()) {	throw new SemanticException("Removing hinted semijoin as it is creating cycles with mapside joins " + rs + " : " + ts);	}	if (LOG.isDebugEnabled()) {	
semijoin cycle due to mapjoin removing semijoin 

}	return null;	}	}	TableScanOperator ts = sjInfo.getTsOp();	if (ts.getStatistics() != null) {	long numRows = ts.getStatistics().getNumRows();	if (numRows < pCtx.getConf().getLongVar(ConfVars.TEZ_BIGTABLE_MIN_SIZE_SEMIJOIN_REDUCTION)) {	if (sjInfo.getShouldRemove()) {	if (LOG.isDebugEnabled()) {	
insufficient rows to justify semijoin optimization removing semijoin 

if (!findParallelSemiJoinBranch(op, (TableScanOperator) parent, procCtx.parseContext, semijoins)) {	break;	}	}	deque.addAll(op.getChildOperators());	}	}	if (semijoins.size() > 0) {	for (ReduceSinkOperator rs : semijoins.keySet()) {	if (LOG.isDebugEnabled()) {	
semijoin optimization with parallel edge to map join removing semijoin 

private static double getBloomFilterBenefit( SelectOperator sel, ExprNodeDesc selExpr, FilterOperator fil, ExprNodeDesc tsExpr) {	double benefit = -1;	Statistics selStats = sel.getStatistics();	Statistics filStats = fil.getStatistics();	if (selStats == null || filStats == null) {	
no stats available to compute bloomfilter benefit 

}	}	if (sel == null) {	throw new SemanticException("Unexpected error - could not find SEL ancestor from semijoin branch of " + rs);	}	TableScanOperator ts = sjInfo.getTsOp();	RuntimeValuesInfo rti = procCtx.parseContext.getRsToRuntimeValuesInfoMap().get(rs);	ExprNodeDesc tsExpr = rti.getTsColExpr();	ExprNodeDesc selExpr = sel.getConf().getColList().get(0);	if (LOG.isDebugEnabled()) {	
computing bloomfilter cost benefit for 

if (LOG.isDebugEnabled()) {	}	double reductionFactor = computeBloomFilterNetBenefit(sel, selExpr, (FilterOperator)ts.getChildOperators().get(0), tsExpr);	if (reductionFactor < semijoinReductionThreshold) {	semijoinRsToRemove.add(rs);	}	}	for (ReduceSinkOperator rs : semijoinRsToRemove) {	TableScanOperator ts = map.get(rs).getTsOp();	if (LOG.isDebugEnabled()) {	
reduction factor not satisfied for removing semijoin optimization 

}	Deque<Operator<?>> deque = new LinkedList<>();	deque.add(ts);	while (!deque.isEmpty()) {	Operator<?> op = deque.pollLast();	if (op instanceof AppMasterEventOperator && ((AppMasterEventOperator) op).getConf() instanceof DynamicPruningEventDesc) {	SelectOperator selOp = null;	try {	selOp = (SelectOperator) (rs.getParentOperators().get(0) .getParentOperators().get(0) .getParentOperators().get(0) .getParentOperators().get(0));	} catch (NullPointerException e) {	
marksemijoinfordpp null pointer exception caught while accessing semijoin operators 

========================= hive sample_3520 =========================

protected AccumuloRecordWriter(JobConf job) throws AccumuloException, AccumuloSecurityException, IOException {	this.isStringEncoded = AccumuloIndexedOutputFormat.getStringEncoding(job).booleanValue();	this.simulate = AccumuloIndexedOutputFormat.getSimulationMode(job).booleanValue();	this.createTables = AccumuloIndexedOutputFormat.canCreateTables(job).booleanValue();	if (this.simulate) {	
simulating output only no writes to tables will occur 

throw new IOException("No table or default table specified. Try simulation mode next time");	} else {	++this.mutCount;	this.valCount += (long)mutation.size();	this.printMutation(table, mutation);	if(!this.simulate) {	if(!this.bws.containsKey(table)) {	try {	this.addTable(table);	} catch (Exception var5) {	
could not add table 

try {	this.addTable(table);	} catch (Exception var5) {	throw new IOException(var5);	}	}	if(indexTableName != null && !this.bws.containsKey(indexTableName)) {	try {	this.addTable(indexTableName);	} catch (Exception var6) {	
could not add index table 

public void addTable(Text tableName) throws AccumuloException, AccumuloSecurityException {	if(this.simulate) {	
simulating adding table 

public void addTable(Text tableName) throws AccumuloException, AccumuloSecurityException {	if(this.simulate) {	} else {	
adding table 

public void addTable(Text tableName) throws AccumuloException, AccumuloSecurityException {	if(this.simulate) {	} else {	BatchWriter bw = null;	String table = tableName.toString();	if(this.createTables && !this.conn.tableOperations().exists(table)) {	try {	this.conn.tableOperations().create(table);	} catch (AccumuloSecurityException var8) {	
accumulo security violation creating 

if(this.simulate) {	} else {	BatchWriter bw = null;	String table = tableName.toString();	if(this.createTables && !this.conn.tableOperations().exists(table)) {	try {	this.conn.tableOperations().create(table);	} catch (AccumuloSecurityException var8) {	throw var8;	} catch (TableExistsException var9) {	
table exists 

try {	this.conn.tableOperations().create(table);	} catch (AccumuloSecurityException var8) {	throw var8;	} catch (TableExistsException var9) {	}	}	try {	bw = this.mtbw.getBatchWriter(table);	} catch (TableNotFoundException var5) {	
accumulo table doesn t exist and cannot be created 

private int printMutation(Text table, Mutation m) {	if(LOG.isTraceEnabled()) {	
table row key 

private int printMutation(Text table, Mutation m) {	if(LOG.isTraceEnabled()) {	Iterator itr = m.getUpdates().iterator();	while(itr.hasNext()) {	ColumnUpdate cu = (ColumnUpdate)itr.next();	
table column 

private int printMutation(Text table, Mutation m) {	if(LOG.isTraceEnabled()) {	Iterator itr = m.getUpdates().iterator();	while(itr.hasNext()) {	ColumnUpdate cu = (ColumnUpdate)itr.next();	
table security 

private int printMutation(Text table, Mutation m) {	if(LOG.isTraceEnabled()) {	Iterator itr = m.getUpdates().iterator();	while(itr.hasNext()) {	ColumnUpdate cu = (ColumnUpdate)itr.next();	
table value 

private List<Mutation> getIndexMutations(Mutation baseMut) {	List indexMuts = new ArrayList<Mutation>();	if (null != indexDef) {	byte[] rowId = baseMut.getRow();	for (ColumnUpdate cu : baseMut.getUpdates()) {	String cf = new String(cu.getColumnFamily());	String cq = new String(cu.getColumnQualifier());	String colType = indexDef.getColType(cf, cq);	if (colType != null) {	
building index for column 

public void close(Reporter reporter) throws IOException {	
mutations written values written 

Object secCodes;	for(Iterator itr = var7.getAuthorizationFailuresMap().entrySet().iterator();	itr.hasNext(); ((Set)secCodes).addAll((Collection)ke.getValue())) {	ke = (Map.Entry)itr.next();	secCodes = (Set)tables.get(((KeyExtent)ke.getKey()).getTableId().toString());	if(secCodes == null) {	secCodes = new HashSet();	tables.put(((KeyExtent)ke.getKey()).getTableId().toString(), secCodes);	}	}	
not authorized to write to tables 

itr.hasNext(); ((Set)secCodes).addAll((Collection)ke.getValue())) {	ke = (Map.Entry)itr.next();	secCodes = (Set)tables.get(((KeyExtent)ke.getKey()).getTableId().toString());	if(secCodes == null) {	secCodes = new HashSet();	tables.put(((KeyExtent)ke.getKey()).getTableId().toString(), secCodes);	}	}	}	if(var7.getConstraintViolationSummaries().size() > 0) {	
constraint violations 

========================= hive sample_180 =========================

public int execute(DriverContext driverContext) {	IOPrepareCache ioPrepareCache = IOPrepareCache.get();	ioPrepareCache.clear();	boolean success = true;	Context ctx = driverContext.getCtx();	boolean ctxCreated = false;	Path emptyScratchDir;	JobClient jc = null;	if (driverContext.isShutdown()) {	
task was cancelled 

propagateSplitSettings(job, mWork);	job.setNumReduceTasks(rWork != null ? rWork.getNumReduceTasks().intValue() : 0);	job.setReducerClass(ExecReducer.class);	setInputAttributes(job);	boolean useSpeculativeExecReducers = HiveConf.getBoolVar(job, HiveConf.ConfVars.HIVESPECULATIVEEXECREDUCERS);	job.setBoolean(MRJobConfig.REDUCE_SPECULATIVE, useSpeculativeExecReducers);	String inpFormat = HiveConf.getVar(job, HiveConf.ConfVars.HIVEINPUTFORMAT);	if (mWork.isUseBucketizedHiveInputFormat()) {	inpFormat = BucketizedHiveInputFormat.class.getName();	}	
using 

int fileNumber = hashtableFiles.length;	String[] fileNames = new String[fileNumber];	for ( int i = 0; i < fileNumber; i++){	fileNames[i] = hashtableFiles[i].getPath().getName();	}	String stageId = this.getId();	String archiveFileName = Utilities.generateTarFileName(stageId);	localwork.setStageID(stageId);	CompressionUtils.tar(localPath.toUri().getPath(), fileNames,archiveFileName);	Path archivePath = Utilities.generateTarPath(localPath, stageId);	
archive hash table files to 

}	String stageId = this.getId();	String archiveFileName = Utilities.generateTarFileName(stageId);	localwork.setStageID(stageId);	CompressionUtils.tar(localPath.toUri().getPath(), fileNames,archiveFileName);	Path archivePath = Utilities.generateTarPath(localPath, stageId);	Path hdfsFilePath =Utilities.generateTarPath(hdfsPath, stageId);	short replication = (short) job.getInt("mapred.submit.replication", 10);	hdfs.copyFromLocalFile(archivePath, hdfsFilePath);	hdfs.setReplication(hdfsFilePath, replication);	
upload archive file from to 

String archiveFileName = Utilities.generateTarFileName(stageId);	localwork.setStageID(stageId);	CompressionUtils.tar(localPath.toUri().getPath(), fileNames,archiveFileName);	Path archivePath = Utilities.generateTarPath(localPath, stageId);	Path hdfsFilePath =Utilities.generateTarPath(hdfsPath, stageId);	short replication = (short) job.getInt("mapred.submit.replication", 10);	hdfs.copyFromLocalFile(archivePath, hdfsFilePath);	hdfs.setReplication(hdfsFilePath, replication);	DistributedCache.createSymlink(job);	DistributedCache.addCacheArchive(hdfsFilePath.toUri(), job);	
add archive file to distributed cache archive file 

Utilities.setMapRedWork(job, work, ctx.getMRTmpPath());	if (mWork.getSamplingType() > 0 && rWork != null && job.getNumReduceTasks() > 1) {	try {	handleSampling(ctx, mWork, job);	job.setPartitionerClass(HiveTotalOrderPartitioner.class);	} catch (IllegalStateException e) {	console.printInfo("Not enough sampling data.. Rolling back to single reducer task");	rWork.setNumReduceTasks(1);	job.setNumReduceTasks(1);	} catch (Exception e) {	
sampling error 

}	}	Utilities.createTmpDirs(job, mWork);	Utilities.createTmpDirs(job, rWork);	SessionState ss = SessionState.get();	if (ss != null && HiveConf.getVar(job, ConfVars.HIVE_EXECUTION_ENGINE).equals("tez")) {	TezSessionPoolManager.closeIfNotDefault(ss.getTezSession(), true);	}	HiveConfUtil.updateJobCredentialProviders(job);	if (driverContext.isShutdown()) {	
task was cancelled 

SessionState ss = SessionState.get();	if (ss != null && HiveConf.getVar(job, ConfVars.HIVE_EXECUTION_ENGINE).equals("tez")) {	TezSessionPoolManager.closeIfNotDefault(ss.getTezSession(), true);	}	HiveConfUtil.updateJobCredentialProviders(job);	if (driverContext.isShutdown()) {	return 5;	}	rj = jc.submitJob(job);	if (driverContext.isShutdown()) {	
task was cancelled 

if (rj != null) {	if (returnVal != 0) {	killJob();	}	jobID = rj.getID().toString();	}	if (jc!=null) {	jc.close();	}	} catch (Exception e) {	
failed while cleaning up 

synchronized(this) {	if (rj != null && !jobKilled) {	jobKilled = true;	needToKillJob = true;	}	}	if (needToKillJob) {	try {	rj.killJob();	} catch (Exception e) {	
failed to kill job 

========================= hive sample_3873 =========================

public LlapProtocolBlockingPB createProxy() throws IOException {	RPC.setProtocolEngine(conf, LlapProtocolBlockingPB.class, ProtobufRpcEngine.class);	
creating protocol proxy as 

========================= hive sample_495 =========================

public void handle(Context withinContext) throws Exception {	CreateFunctionMessage createFunctionMessage = deserializer.getCreateFunctionMessage(event.getMessage());	
processing create message message 

========================= hive sample_3465 =========================

public ParseContext transform(ParseContext pctx) throws SemanticException {	final Map<String, TableScanOperator> topOps = pctx.getTopOps();	if (topOps.size() < 2) {	return pctx;	}	if (LOG.isDebugEnabled()) {	
before sharedworkoptimizer 

final Map<String, TableScanOperator> topOps = pctx.getTopOps();	if (topOps.size() < 2) {	return pctx;	}	if (LOG.isDebugEnabled()) {	}	SharedWorkOptimizerCache optimizerCache = new SharedWorkOptimizerCache();	gatherDPPTableScanOps(pctx, optimizerCache);	Multimap<String, TableScanOperator> tableNameToOps = splitTableScanOpsByTable(pctx);	List<Entry<String, Long>> sortedTables = rankTablesByAccumulatedSize(pctx);	
sorted tables by size 

SharedWorkOptimizerCache optimizerCache = new SharedWorkOptimizerCache();	gatherDPPTableScanOps(pctx, optimizerCache);	Multimap<String, TableScanOperator> tableNameToOps = splitTableScanOpsByTable(pctx);	List<Entry<String, Long>> sortedTables = rankTablesByAccumulatedSize(pctx);	Multimap<String, TableScanOperator> existingOps = ArrayListMultimap.create();	Set<Operator<?>> removedOps = new HashSet<>();	for (Entry<String, Long> tablePair : sortedTables) {	String tableName = tablePair.getKey();	for (TableScanOperator discardableTsOp : tableNameToOps.get(tableName)) {	if (removedOps.contains(discardableTsOp)) {	
skip as it has already been removed 

Set<Operator<?>> removedOps = new HashSet<>();	for (Entry<String, Long> tablePair : sortedTables) {	String tableName = tablePair.getKey();	for (TableScanOperator discardableTsOp : tableNameToOps.get(tableName)) {	if (removedOps.contains(discardableTsOp)) {	continue;	}	Collection<TableScanOperator> prevTsOps = existingOps.get(tableName);	for (TableScanOperator retainableTsOp : prevTsOps) {	if (removedOps.contains(retainableTsOp)) {	
skip as it has already been removed 

if (removedOps.contains(discardableTsOp)) {	continue;	}	Collection<TableScanOperator> prevTsOps = existingOps.get(tableName);	for (TableScanOperator retainableTsOp : prevTsOps) {	if (removedOps.contains(retainableTsOp)) {	continue;	}	boolean mergeable = areMergeable(pctx, optimizerCache, retainableTsOp, discardableTsOp);	if (!mergeable) {	
and cannot be merged 

for (TableScanOperator retainableTsOp : prevTsOps) {	if (removedOps.contains(retainableTsOp)) {	continue;	}	boolean mergeable = areMergeable(pctx, optimizerCache, retainableTsOp, discardableTsOp);	if (!mergeable) {	continue;	}	SharedResult sr = extractSharedOptimizationInfoForRoot( pctx, optimizerCache, retainableTsOp, discardableTsOp);	if (!validPreConditions(pctx, optimizerCache, sr)) {	
and do not meet preconditions 

Operator<?> lastRetainableOp = sr.retainableOps.get(sr.retainableOps.size() - 1);	Operator<?> lastDiscardableOp = sr.discardableOps.get(sr.discardableOps.size() - 1);	if (lastDiscardableOp.getNumChild() != 0) {	List<Operator<? extends OperatorDesc>> allChildren = Lists.newArrayList(lastDiscardableOp.getChildOperators());	for (Operator<? extends OperatorDesc> op : allChildren) {	lastDiscardableOp.getChildOperators().remove(op);	op.replaceParent(lastDiscardableOp, lastRetainableOp);	lastRetainableOp.getChildOperators().add(op);	}	}	
merging subtree starting at into subtree starting at 

}	}	}	retainableTsOp.getConf().setFilterExpr(exprNode);	List<Operator<? extends OperatorDesc>> allChildren = Lists.newArrayList(discardableTsOp.getChildOperators());	for (Operator<? extends OperatorDesc> op : allChildren) {	discardableTsOp.getChildOperators().remove(op);	op.replaceParent(discardableTsOp, retainableTsOp);	retainableTsOp.getChildOperators().add(op);	}	
merging into 

GenTezUtils.removeSemiJoinOperator( pctx, (ReduceSinkOperator) op, sjbi.getTsOp());	optimizerCache.tableScanToDPPSource.remove(sjbi.getTsOp(), op);	}	} else if (op instanceof AppMasterEventOperator) {	DynamicPruningEventDesc dped = (DynamicPruningEventDesc) op.getConf();	if (!sr.discardableOps.contains(dped.getTableScan()) && !sr.discardableInputOps.contains(dped.getTableScan())) {	GenTezUtils.removeSemiJoinOperator( pctx, (AppMasterEventOperator) op, dped.getTableScan());	optimizerCache.tableScanToDPPSource.remove(dped.getTableScan(), op);	}	}	
input operator removed 

for (Operator<?> dppSource : c) {	if (dppSource instanceof ReduceSinkOperator) {	GenTezUtils.removeSemiJoinOperator(pctx, (ReduceSinkOperator) dppSource, (TableScanOperator) sr.retainableOps.get(0));	optimizerCache.tableScanToDPPSource.remove(sr.retainableOps.get(0), op);	} else if (dppSource instanceof AppMasterEventOperator) {	GenTezUtils.removeSemiJoinOperator(pctx, (AppMasterEventOperator) dppSource, (TableScanOperator) sr.retainableOps.get(0));	optimizerCache.tableScanToDPPSource.remove(sr.retainableOps.get(0), op);	}	}	}	
operator removed 

}	}	Iterator<Entry<String, TableScanOperator>> it = topOps.entrySet().iterator();	while (it.hasNext()) {	Entry<String, TableScanOperator> e = it.next();	if (e.getValue().getNumChild() == 0) {	it.remove();	}	}	if (LOG.isDebugEnabled()) {	
after sharedworkoptimizer 

if (LOG.isDebugEnabled()) {	}	if(pctx.getConf().getBoolVar(ConfVars.HIVE_SHARED_WORK_EXTENDED_OPTIMIZATION)) {	Multimap<Operator<?>, ReduceSinkOperator> parentToRsOps = ArrayListMultimap.create();	Set<Operator<?>> visited = new HashSet<>();	for (Entry<String, TableScanOperator> e : topOps.entrySet()) {	gatherReduceSinkOpsByInput(parentToRsOps,  visited, findWorkOperators(optimizerCache, e.getValue()));	}	while (!parentToRsOps.isEmpty()) {	List<Entry<Operator<?>, Long>> sortedRSGroups = rankOpsByAccumulatedSize(parentToRsOps.keySet());	
sorted operators by size 

for (Entry<String, TableScanOperator> e : topOps.entrySet()) {	gatherReduceSinkOpsByInput(parentToRsOps,  visited, findWorkOperators(optimizerCache, e.getValue()));	}	while (!parentToRsOps.isEmpty()) {	List<Entry<Operator<?>, Long>> sortedRSGroups = rankOpsByAccumulatedSize(parentToRsOps.keySet());	Multimap<Operator<?>, ReduceSinkOperator> existingRsOps = ArrayListMultimap.create();	for (Entry<Operator<?>, Long> rsGroupInfo : sortedRSGroups) {	Operator<?> rsParent = rsGroupInfo.getKey();	for (ReduceSinkOperator discardableRsOp : parentToRsOps.get(rsParent)) {	if (removedOps.contains(discardableRsOp)) {	
skip as it has already been removed 

Multimap<Operator<?>, ReduceSinkOperator> existingRsOps = ArrayListMultimap.create();	for (Entry<Operator<?>, Long> rsGroupInfo : sortedRSGroups) {	Operator<?> rsParent = rsGroupInfo.getKey();	for (ReduceSinkOperator discardableRsOp : parentToRsOps.get(rsParent)) {	if (removedOps.contains(discardableRsOp)) {	continue;	}	Collection<ReduceSinkOperator> otherRsOps = existingRsOps.get(rsParent);	for (ReduceSinkOperator retainableRsOp : otherRsOps) {	if (removedOps.contains(retainableRsOp)) {	
skip as it has already been removed 

if (removedOps.contains(discardableRsOp)) {	continue;	}	Collection<ReduceSinkOperator> otherRsOps = existingRsOps.get(rsParent);	for (ReduceSinkOperator retainableRsOp : otherRsOps) {	if (removedOps.contains(retainableRsOp)) {	continue;	}	boolean mergeable = compareOperator(pctx, retainableRsOp, discardableRsOp) && compareOperator(pctx, retainableRsOp.getChildOperators().get(0), discardableRsOp.getChildOperators().get(0));	if (!mergeable) {	
and cannot be merged 

}	Collection<ReduceSinkOperator> otherRsOps = existingRsOps.get(rsParent);	for (ReduceSinkOperator retainableRsOp : otherRsOps) {	if (removedOps.contains(retainableRsOp)) {	continue;	}	boolean mergeable = compareOperator(pctx, retainableRsOp, discardableRsOp) && compareOperator(pctx, retainableRsOp.getChildOperators().get(0), discardableRsOp.getChildOperators().get(0));	if (!mergeable) {	continue;	}	
checking additional conditions for merging subtree starting at into subtree starting at 

continue;	}	boolean mergeable = compareOperator(pctx, retainableRsOp, discardableRsOp) && compareOperator(pctx, retainableRsOp.getChildOperators().get(0), discardableRsOp.getChildOperators().get(0));	if (!mergeable) {	continue;	}	Operator<?> retainableRsOpChild = retainableRsOp.getChildOperators().get(0);	Operator<?> discardableRsOpChild = discardableRsOp.getChildOperators().get(0);	SharedResult sr = extractSharedOptimizationInfo( pctx, optimizerCache, retainableRsOp, discardableRsOp, retainableRsOpChild, discardableRsOpChild);	if (sr.retainableOps.isEmpty() || !validPreConditions(pctx, optimizerCache, sr)) {	
and do not meet preconditions 

Operator<?> lastRetainableOp = sr.retainableOps.get(sr.retainableOps.size() - 1);	Operator<?> lastDiscardableOp = sr.discardableOps.get(sr.discardableOps.size() - 1);	if (lastDiscardableOp.getNumChild() != 0) {	List<Operator<? extends OperatorDesc>> allChildren = Lists.newArrayList(lastDiscardableOp.getChildOperators());	for (Operator<? extends OperatorDesc> op : allChildren) {	lastDiscardableOp.getChildOperators().remove(op);	op.replaceParent(lastDiscardableOp, lastRetainableOp);	lastRetainableOp.getChildOperators().add(op);	}	}	
merging subtree starting at into subtree starting at 

GenTezUtils.removeSemiJoinOperator( pctx, (ReduceSinkOperator) op, sjbi.getTsOp());	optimizerCache.tableScanToDPPSource.remove(sjbi.getTsOp(), op);	}	} else if (op instanceof AppMasterEventOperator) {	DynamicPruningEventDesc dped = (DynamicPruningEventDesc) op.getConf();	if (!sr.discardableOps.contains(dped.getTableScan()) && !sr.discardableInputOps.contains(dped.getTableScan())) {	GenTezUtils.removeSemiJoinOperator( pctx, (AppMasterEventOperator) op, dped.getTableScan());	optimizerCache.tableScanToDPPSource.remove(dped.getTableScan(), op);	}	}	
input operator removed 

DynamicPruningEventDesc dped = (DynamicPruningEventDesc) op.getConf();	if (!sr.discardableOps.contains(dped.getTableScan()) && !sr.discardableInputOps.contains(dped.getTableScan())) {	GenTezUtils.removeSemiJoinOperator( pctx, (AppMasterEventOperator) op, dped.getTableScan());	optimizerCache.tableScanToDPPSource.remove(dped.getTableScan(), op);	}	}	}	OperatorUtils.removeOperator(discardableRsOp);	optimizerCache.removeOp(discardableRsOp);	removedOps.add(discardableRsOp);	
operator removed 

}	}	OperatorUtils.removeOperator(discardableRsOp);	optimizerCache.removeOp(discardableRsOp);	removedOps.add(discardableRsOp);	optimizerCache.removeOpAndCombineWork(discardableRsOpChild, retainableRsOpChild);	for (Operator<?> op : sr.discardableOps) {	OperatorUtils.removeOperator(op);	optimizerCache.removeOp(op);	removedOps.add(op);	
operator removed 

}	}	it = topOps.entrySet().iterator();	while (it.hasNext()) {	Entry<String, TableScanOperator> e = it.next();	if (e.getValue().getNumChild() == 0) {	it.remove();	}	}	if (LOG.isDebugEnabled()) {	
after sharedworkextendedoptimizer 

Set<AppMasterEventOperator> s = OperatorUtils.findOperators(tableScanOps, AppMasterEventOperator.class);	for (AppMasterEventOperator a : s) {	if (a.getConf() instanceof DynamicPruningEventDesc) {	DynamicPruningEventDesc dped = (DynamicPruningEventDesc) a.getConf();	optimizerCache.tableScanToDPPSource.put(dped.getTableScan(), a);	}	}	for (Entry<ReduceSinkOperator, SemiJoinBranchInfo> e : pctx.getRsToSemiJoinBranchInfo().entrySet()) {	optimizerCache.tableScanToDPPSource.put(e.getValue().getTsOp(), e.getKey());	}	
dpp information stored in the cache 

private static boolean compareAndGatherOps(ParseContext pctx, Operator<?> op1, Operator<?> op2, List<Operator<?>> result, boolean gather) throws SemanticException {	if (!compareOperator(pctx, op1, op2)) {	
operators not equal and 

private static boolean validPreConditions(ParseContext pctx, SharedWorkOptimizerCache optimizerCache, SharedResult sr) {	if (sr.dataSize > sr.maxDataSize) {	
accumulated data size max size 

========================= hive sample_3035 =========================

retCols[i].set(MAPPER.writeValueAsString(extractObject));	} else if (extractObject != null) {	retCols[i].set(extractObject.toString());	} else {	retCols[i] = null;	}	}	forward(retCols);	return;	} catch (Throwable e) {	
json parsing evaluation exception 

private void reportInvalidJson(String jsonStr) {	if (!seenErrors) {	
the input is not a valid json string skipping such error messages in the future 

========================= hive sample_4820 =========================

public AsyncMethodCallback<TOpenSessionResp> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<TOpenSessionResp>() {	public void onComplete(TOpenSessionResp o) {	OpenSession_result result = new OpenSession_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	OpenSession_result result = new OpenSession_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<TCloseSessionResp> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<TCloseSessionResp>() {	public void onComplete(TCloseSessionResp o) {	CloseSession_result result = new CloseSession_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	CloseSession_result result = new CloseSession_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<TGetInfoResp> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<TGetInfoResp>() {	public void onComplete(TGetInfoResp o) {	GetInfo_result result = new GetInfo_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	GetInfo_result result = new GetInfo_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<TExecuteStatementResp> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<TExecuteStatementResp>() {	public void onComplete(TExecuteStatementResp o) {	ExecuteStatement_result result = new ExecuteStatement_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	ExecuteStatement_result result = new ExecuteStatement_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<TGetTypeInfoResp> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<TGetTypeInfoResp>() {	public void onComplete(TGetTypeInfoResp o) {	GetTypeInfo_result result = new GetTypeInfo_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	GetTypeInfo_result result = new GetTypeInfo_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	GetCatalogs_result result = new GetCatalogs_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<TGetSchemasResp> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<TGetSchemasResp>() {	public void onComplete(TGetSchemasResp o) {	GetSchemas_result result = new GetSchemas_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	GetSchemas_result result = new GetSchemas_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<TGetTablesResp> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<TGetTablesResp>() {	public void onComplete(TGetTablesResp o) {	GetTables_result result = new GetTables_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	GetTables_result result = new GetTables_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<TGetTableTypesResp> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<TGetTableTypesResp>() {	public void onComplete(TGetTableTypesResp o) {	GetTableTypes_result result = new GetTableTypes_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	GetTableTypes_result result = new GetTableTypes_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<TGetColumnsResp> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<TGetColumnsResp>() {	public void onComplete(TGetColumnsResp o) {	GetColumns_result result = new GetColumns_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	GetColumns_result result = new GetColumns_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<TGetFunctionsResp> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<TGetFunctionsResp>() {	public void onComplete(TGetFunctionsResp o) {	GetFunctions_result result = new GetFunctions_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	GetFunctions_result result = new GetFunctions_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<TGetPrimaryKeysResp> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<TGetPrimaryKeysResp>() {	public void onComplete(TGetPrimaryKeysResp o) {	GetPrimaryKeys_result result = new GetPrimaryKeys_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	GetPrimaryKeys_result result = new GetPrimaryKeys_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<TGetCrossReferenceResp> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<TGetCrossReferenceResp>() {	public void onComplete(TGetCrossReferenceResp o) {	GetCrossReference_result result = new GetCrossReference_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	GetCrossReference_result result = new GetCrossReference_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<TGetOperationStatusResp> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<TGetOperationStatusResp>() {	public void onComplete(TGetOperationStatusResp o) {	GetOperationStatus_result result = new GetOperationStatus_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	GetOperationStatus_result result = new GetOperationStatus_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<TCancelOperationResp> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<TCancelOperationResp>() {	public void onComplete(TCancelOperationResp o) {	CancelOperation_result result = new CancelOperation_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	CancelOperation_result result = new CancelOperation_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<TCloseOperationResp> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<TCloseOperationResp>() {	public void onComplete(TCloseOperationResp o) {	CloseOperation_result result = new CloseOperation_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	CloseOperation_result result = new CloseOperation_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<TGetResultSetMetadataResp> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<TGetResultSetMetadataResp>() {	public void onComplete(TGetResultSetMetadataResp o) {	GetResultSetMetadata_result result = new GetResultSetMetadata_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	GetResultSetMetadata_result result = new GetResultSetMetadata_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<TFetchResultsResp> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<TFetchResultsResp>() {	public void onComplete(TFetchResultsResp o) {	FetchResults_result result = new FetchResults_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	FetchResults_result result = new FetchResults_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<TGetDelegationTokenResp> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<TGetDelegationTokenResp>() {	public void onComplete(TGetDelegationTokenResp o) {	GetDelegationToken_result result = new GetDelegationToken_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	GetDelegationToken_result result = new GetDelegationToken_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<TCancelDelegationTokenResp> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<TCancelDelegationTokenResp>() {	public void onComplete(TCancelDelegationTokenResp o) {	CancelDelegationToken_result result = new CancelDelegationToken_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	CancelDelegationToken_result result = new CancelDelegationToken_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<TRenewDelegationTokenResp> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<TRenewDelegationTokenResp>() {	public void onComplete(TRenewDelegationTokenResp o) {	RenewDelegationToken_result result = new RenewDelegationToken_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	RenewDelegationToken_result result = new RenewDelegationToken_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<TGetQueryIdResp> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<TGetQueryIdResp>() {	public void onComplete(TGetQueryIdResp o) {	GetQueryId_result result = new GetQueryId_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	GetQueryId_result result = new GetQueryId_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

public AsyncMethodCallback<TSetClientInfoResp> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {	final org.apache.thrift.AsyncProcessFunction fcall = this;	return new AsyncMethodCallback<TSetClientInfoResp>() {	public void onComplete(TSetClientInfoResp o) {	SetClientInfo_result result = new SetClientInfo_result();	result.success = o;	try {	fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);	return;	} catch (Exception e) {	
exception writing to internal frame buffer 

org.apache.thrift.TBase msg;	SetClientInfo_result result = new SetClientInfo_result();	{	msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;	msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());	}	try {	fcall.sendResponse(fb,msg,msgType,seqid);	return;	} catch (Exception ex) {	
exception writing to internal frame buffer 

========================= hive sample_114 =========================

final Process p1 = Runtime.getRuntime().exec(cmdArr, null, dir);	new Thread(new Runnable() {	public void run() {	BufferedReader input = new BufferedReader(new InputStreamReader(p1.getErrorStream()));	String line;	try {	while ((line = input.readLine()) != null) {	System.out.println(line);	}	} catch (IOException e) {	
failed to execute the command due the exception 

========================= hive sample_1423 =========================

Partition part_get = client.getPartition(dbName, tblName, part.getValues());	if(isThriftClient) {	adjust(client, part, dbName, tblName);	adjust(client, part2, dbName, tblName);	adjust(client, part3, dbName, tblName);	}	assertTrue("Partitions are not same", part.equals(part_get));	List<String> vals6 = makeVals("2016-02-22 00:00:00", "16");	Partition part6 = makePartitionObject(dbName, tblName, vals6, tbl, "/part5");	part6.getSd().setCols(null);	
creating partition will null field schema 

if(isThriftClient) {	adjust(client, part, dbName, tblName);	adjust(client, part2, dbName, tblName);	adjust(client, part3, dbName, tblName);	}	assertTrue("Partitions are not same", part.equals(part_get));	List<String> vals6 = makeVals("2016-02-22 00:00:00", "16");	Partition part6 = makePartitionObject(dbName, tblName, vals6, tbl, "/part5");	part6.getSd().setCols(null);	client.add_partition(part6);	
listing all partitions for table 

assertTrue("Partitions are not same", part.equals(part_get));	List<String> vals6 = makeVals("2016-02-22 00:00:00", "16");	Partition part6 = makePartitionObject(dbName, tblName, vals6, tbl, "/part5");	part6.getSd().setCols(null);	client.add_partition(part6);	final List<Partition> partitions = client.listPartitions(dbName, tblName, (short) -1);	boolean foundPart = false;	for (Partition p : partitions) {	if (p.getValues().equals(vals6)) {	assertNull(p.getSd().getCols());	
found partition having null field schema 

private void checkFilter(HiveMetaStoreClient client, String dbName, String tblName, String filter, int expectedCount) throws TException {	
testing filter 

========================= hive sample_1566 =========================

public DataSource create(Configuration hdpConfig) throws SQLException {	
creating hikari connection pool for the metastore 

public boolean supports(Configuration configuration) {	String poolingType = MetastoreConf.getVar(configuration, MetastoreConf.ConfVars.CONNECTION_POOLING_TYPE).toLowerCase();	if (HIKARI.equals(poolingType)) {	int hikariPropsNr = DataSourceProvider.getPrefixedProperties(configuration, HIKARI).size();	
found nr of hikari specific configurations 

public boolean supports(Configuration configuration) {	String poolingType = MetastoreConf.getVar(configuration, MetastoreConf.ConfVars.CONNECTION_POOLING_TYPE).toLowerCase();	if (HIKARI.equals(poolingType)) {	int hikariPropsNr = DataSourceProvider.getPrefixedProperties(configuration, HIKARI).size();	return hikariPropsNr > 0;	}	
configuration requested pooling hikaricpdsprovider exiting 

========================= hive sample_1835 =========================

if (fileSinkDescs != null) {	Task<? extends Serializable> childTask = fileSinkDescs.get(fsOp.getConf());	processLinkedFileDesc(ctx, childTask);	return true;	}	if ((ctx.getSeenFileSinkOps() == null) || (!ctx.getSeenFileSinkOps().contains(nd))) {	chDir = GenMapRedUtils.isMergeRequired(ctx.getMvTask(), hconf, fsOp, currTask, isInsertTable);	}	Path finalName = processFS(fsOp, stack, opProcCtx, chDir);	if (chDir) {	
using combinehiveinputformat for the merge job 

========================= hive sample_2960 =========================

if (b) {	return null;	}	}	}	for (int targetPos: targets[srcPos]) {	if (srcPos == targetPos) {	continue;	}	if (LOG.isDebugEnabled()) {	
synthetic predicate 

========================= hive sample_3388 =========================

final Double rightRCount = mq.getRowCount(join.getRight());	if (leftRCount == null || rightRCount == null) {	return null;	}	final double rCount = leftRCount + rightRCount;	ImmutableList<Double> cardinalities = new ImmutableList.Builder<Double>(). add(leftRCount). add(rightRCount). build();	double cpuCost;	try {	cpuCost = algoUtils.computeSortMergeCPUCost(cardinalities, join.getSortedInputs());	} catch (CalciteSemanticException e) {	
failed to compute sort merge cpu cost 

public ImmutableList<RelCollation> getCollation(HiveJoin join) {	final MapJoinStreamingRelation streamingSide = join.getStreamingSide();	if (streamingSide != MapJoinStreamingRelation.LEFT_RELATION && streamingSide != MapJoinStreamingRelation.RIGHT_RELATION) {	
streaming side for map join not chosen 

public RelDistribution getDistribution(HiveJoin join) {	final MapJoinStreamingRelation streamingSide = join.getStreamingSide();	if (streamingSide != MapJoinStreamingRelation.LEFT_RELATION && streamingSide != MapJoinStreamingRelation.RIGHT_RELATION) {	
streaming side for map join not chosen 

public ImmutableList<RelCollation> getCollation(HiveJoin join) {	final MapJoinStreamingRelation streamingSide = join.getStreamingSide();	if (streamingSide != MapJoinStreamingRelation.LEFT_RELATION && streamingSide != MapJoinStreamingRelation.RIGHT_RELATION) {	
streaming side for map join not chosen 

List<ImmutableIntList> joinKeysInChildren = new ArrayList<ImmutableIntList>();	joinKeysInChildren.add( ImmutableIntList.copyOf( joinPredInfo.getProjsFromLeftPartOfJoinKeysInChildSchema()));	joinKeysInChildren.add( ImmutableIntList.copyOf( joinPredInfo.getProjsFromRightPartOfJoinKeysInChildSchema()));	final RelMetadataQuery mq = join.getCluster().getMetadataQuery();	for (int i=0; i<join.getInputs().size(); i++) {	RelNode input = join.getInputs().get(i);	boolean orderFound;	try {	orderFound = join.getSortedInputs().get(i);	} catch (CalciteSemanticException e) {	
not possible to do smb join 

========================= hive sample_2825 =========================

String colType = null;	String colName = null;	boolean doAllPartitionContainStats = partNames.size() == colStatsWithSourceInfo.size();	NumDistinctValueEstimator ndvEstimator = null;	for (ColStatsObjWithSourceInfo csp : colStatsWithSourceInfo) {	ColumnStatisticsObj cso = csp.getColStatsObj();	if (statsObj == null) {	colName = cso.getColName();	colType = cso.getColType();	statsObj = ColumnStatsAggregatorFactory.newColumnStaticsObj(colName, colType, cso.getStatsData().getSetField());	
doallpartitioncontainstats for column is 

} else if (estimation > higherBound) {	estimation = higherBound;	}	} else {	estimation = (long) (lowerBound + (higherBound - lowerBound) * ndvTuner);	}	aggregateData.setNumDVs(estimation);	}	columnStatisticsData.setDateStats(aggregateData);	} else {	
start extrapolation for 

aggregateData.setNumDVs(ndvEstimator.estimateNumDistinctValues());	ColumnStatisticsData csd = new ColumnStatisticsData();	csd.setDateStats(aggregateData);	adjustedStatsMap.put(pseudoPartName.toString(), csd);	if (useDensityFunctionForNDVEstimation) {	densityAvgSum += diff(aggregateData.getHighValue(), aggregateData.getLowValue()) }	}	}	extrapolate(columnStatisticsData, partNames.size(), colStatsWithSourceInfo.size(), adjustedIndexMap, adjustedStatsMap, densityAvgSum / adjustedStatsMap.size());	}	
ndv estimatation for is of partitions requested of partitions found 

========================= hive sample_1927 =========================

public Double getRowCount(Join join, RelMetadataQuery mq) {	PKFKRelationInfo pkfk = analyzeJoinForPKFK(join, mq);	if (pkfk != null) {	double selectivity = (pkfk.pkInfo.selectivity * pkfk.ndvScalingFactor);	selectivity = Math.min(1.0, selectivity);	if (LOG.isDebugEnabled()) {	
identified primary foreign key relation 

public Double getRowCount(SemiJoin rel, RelMetadataQuery mq) {	PKFKRelationInfo pkfk = analyzeJoinForPKFK(rel, mq);	if (pkfk != null) {	double selectivity = (pkfk.pkInfo.selectivity * pkfk.ndvScalingFactor);	selectivity = Math.min(1.0, selectivity);	if (LOG.isDebugEnabled()) {	
identified primary foreign key relation 

========================= hive sample_2937 =========================

public void run() {	while (true) {	try {	doOneCleanupRound();	} catch (InterruptedException ex) {	
cleanup thread has been interrupted 

public void run() {	while (true) {	try {	doOneCleanupRound();	} catch (InterruptedException ex) {	Thread.currentThread().interrupt();	break;	} catch (Throwable t) {	
cleanup has failed the thread will now exit 

========================= hive sample_2171 =========================

public void close() {	try {	secretManager.close();	} catch (Exception ex) {	
error closing the signer 

========================= hive sample_505 =========================

public void init(AccumuloSerDeParameters accumuloSerDeParams, Properties properties) throws SerDeException {	super.init(accumuloSerDeParams, properties);	String delimiter = properties.getProperty(ACCUMULO_COMPOSITE_DELIMITER);	if (null == delimiter || delimiter.isEmpty()) {	throw new SerDeException("Did not find expected delimiter in configuration: " + ACCUMULO_COMPOSITE_DELIMITER);	}	if (delimiter.length() != 1) {	
configured delimiter is longer than one character only using first character 

public void init(AccumuloSerDeParameters accumuloSerDeParams, Properties properties) throws SerDeException {	super.init(accumuloSerDeParams, properties);	String delimiter = properties.getProperty(ACCUMULO_COMPOSITE_DELIMITER);	if (null == delimiter || delimiter.isEmpty()) {	throw new SerDeException("Did not find expected delimiter in configuration: " + ACCUMULO_COMPOSITE_DELIMITER);	}	if (delimiter.length() != 1) {	}	separator = (byte) delimiter.charAt(0);	
initialized delimitedaccumulorowidfactory with separator of 

public LazyObjectBase createRowId(ObjectInspector inspector) throws SerDeException {	LazyObjectBase lazyObj = LazyFactory.createLazyObject(inspector, ColumnEncoding.BINARY == rowIdMapping.getEncoding());	
created for rowid with inspector 

========================= hive sample_155 =========================

public void startCleanup(Configuration config) {	try {	HDFSCleanup.startInstance(config);	} catch (Exception e) {	
cleanup instance didn t start 

public String getField(Type type, String id, String key) {	BufferedReader in = null;	Path p = new Path(getPath(type) + "/" + id + "/" + key);	try {	if(!fs.exists(p)) {	
does not exist 

String line = null;	String val = "";	while ((line = in.readLine()) != null) {	if (!val.equals("")) {	val += "\n";	}	val += line;	}	return val;	} catch (Exception e) {	
couldn t find 

public List<String> getAllForType(Type type) {	ArrayList<String> allNodes = new ArrayList<String>();	try {	for (FileStatus status : fs.listStatus(new Path(getPath(type)))) {	allNodes.add(status.getPath().getName());	}	return null;	} catch (Exception e) {	
couldn t find children for type 

private void close(Closeable is) {	if(is == null) {	return;	}	try {	is.close();	}	catch (IOException ex) {	
failed to close inputstream 

========================= hive sample_856 =========================

public synchronized void progressBarCompleted() {	
progress bar is complete 

========================= hive sample_1177 =========================

public void consumeData(BatchType data) throws InterruptedException {	if (isStopped) {	returnSourceData(data);	return;	}	long start = System.currentTimeMillis();	try {	decodeBatch(data, downstreamConsumer);	} catch (Throwable ex) {	
decodebatch threw 

========================= hive sample_2188 =========================

needCommonSetup = false;	}	if (needHashTableSetup) {	hashMap = (VectorMapJoinBytesHashMap) vectorMapJoinHashTable;	needHashTableSetup = false;	}	batchCounter++;	final int inputLogicalSize = batch.size;	if (inputLogicalSize == 0) {	if (LOG.isDebugEnabled()) {	
batch empty 

}	boolean someRowsFilteredOut =  false;	if (bigTableFilterExpressions.length > 0) {	for (VectorExpression ve : bigTableFilterExpressions) {	ve.evaluate(batch);	}	someRowsFilteredOut = (batch.size != inputLogicalSize);	if (LOG.isDebugEnabled()) {	if (batch.selectedInUse) {	if (inputSelectedInUse) {	
inputselected filtered batch selected 

boolean someRowsFilteredOut =  false;	if (bigTableFilterExpressions.length > 0) {	for (VectorExpression ve : bigTableFilterExpressions) {	ve.evaluate(batch);	}	someRowsFilteredOut = (batch.size != inputLogicalSize);	if (LOG.isDebugEnabled()) {	if (batch.selectedInUse) {	if (inputSelectedInUse) {	} else {	
inputlogicalsize filtered batch selected 

} else if (someKeyInputColumnIsNull) {	joinResult = JoinUtil.JoinResult.NOMATCH;	} else {	keyVectorSerializeWrite.setOutput(currentKeyOutput);	keyVectorSerializeWrite.serializeWrite(batch, 0);	byte[] keyBytes = currentKeyOutput.getData();	int keyLength = currentKeyOutput.getLength();	joinResult = hashMap.lookup(keyBytes, 0, keyLength, hashMapResults[0]);	}	if (LOG.isDebugEnabled()) {	
batch repeated joinresult 

keyVectorSerializeWrite.serializeWrite(batch, 0);	byte[] keyBytes = currentKeyOutput.getData();	int keyLength = currentKeyOutput.getLength();	joinResult = hashMap.lookup(keyBytes, 0, keyLength, hashMapResults[0]);	}	if (LOG.isDebugEnabled()) {	}	finishOuterRepeated(batch, joinResult, hashMapResults[0], someRowsFilteredOut, inputSelectedInUse, inputLogicalSize);	} else {	if (LOG.isDebugEnabled()) {	
batch non repeated 

switch (saveJoinResult) {	case MATCH: hashMapResultCount++;	equalKeySeriesCount++;	break;	case SPILL: hashMapResultCount++;	break;	case NOMATCH: break;	}	}	if (LOG.isDebugEnabled()) {	
batch allmatchs equalkeyserieshashmapresultindices equalkeyseriesallmatchindices equalkeyseriesissinglevalue equalkeyseriesduplicatecounts atleastonenonmatch inputselectedinuse inputlogicalsize spills spillhashmapresultindices hashmapresults 

========================= hive sample_4119 =========================

private static void serializePlan(Kryo kryo, Object plan, OutputStream out, boolean cloningPlan) {	PerfLogger perfLogger = SessionState.getPerfLogger();	perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.SERIALIZE_PLAN);	
serializing using kryo 

private static <T> T deserializePlan(Kryo kryo, InputStream in, Class<T> planClass, boolean cloningPlan) {	PerfLogger perfLogger = SessionState.getPerfLogger();	perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.DESERIALIZE_PLAN);	T plan;	
deserializing using kryo 

========================= hive sample_4549 =========================

public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procContext, Object... nodeOutputs) throws SemanticException {	OptimizeSparkProcContext context = (OptimizeSparkProcContext) procContext;	boolean remove = false;	SparkPartitionPruningSinkOperator op = (SparkPartitionPruningSinkOperator) nd;	SparkPartitionPruningSinkDesc desc = op.getConf();	if (context.getConf().isSparkDPPOnlyMapjoin() && !op.isWithMapjoin()) {	
disabling dynamic partition pruning based on this is not part of a map join 

public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procContext, Object... nodeOutputs) throws SemanticException {	OptimizeSparkProcContext context = (OptimizeSparkProcContext) procContext;	boolean remove = false;	SparkPartitionPruningSinkOperator op = (SparkPartitionPruningSinkOperator) nd;	SparkPartitionPruningSinkDesc desc = op.getConf();	if (context.getConf().isSparkDPPOnlyMapjoin() && !op.isWithMapjoin()) {	remove = true;	}	else if (desc.getStatistics().getDataSize() > context.getConf() .getLongVar(ConfVars.SPARK_DYNAMIC_PARTITION_PRUNING_MAX_DATA_SIZE)) {	
disabling dynamic partition pruning based on expected data size is too big 

========================= hive sample_3058 =========================

if (tokenPath != null) {	new File(tokenPath.toUri()).delete();	String checksumStr = tokenPath.getParent() + File.separator + "." + tokenPath.getName() + ".crc";	File checksumFile = null;	try {	checksumFile = new File(new URI(checksumStr));	if (checksumFile.exists()) {	checksumFile.delete();	}	} catch (URISyntaxException e) {	
failed to delete token crc file 

private Token<?>[] getFSDelegationToken(String user, final Configuration conf) throws IOException, InterruptedException {	
user loginuser 

private Token<?>[] getFSDelegationToken(String user, final Configuration conf) throws IOException, InterruptedException {	final UserGroupInformation ugi = UgiFactory.getUgi(user);	final TokenWrapper twrapper = new TokenWrapper();	ugi.doAs(new PrivilegedExceptionAction<Object>() {	public Object run() throws IOException, URISyntaxException {	Credentials creds = new Credentials();	collectTokens(FileSystem.get(conf), twrapper, creds, ugi.getShortUserName());	Collection<String> URIs = conf.getStringCollection("mapreduce.job.hdfs-servers");	for(String uri : URIs) {	
getting tokens for 

private void writeProxyDelegationTokens(final Token<?> fsTokens[], final Token<?> msToken, final Configuration conf, String user, final Path tokenPath) throws IOException, InterruptedException {	
user loginuser 

private String buildHcatDelegationToken(String user) throws IOException, InterruptedException, TException {	final HiveConf c = new HiveConf();	final IMetaStoreClient client = HCatUtil.getHiveMetastoreClient(c);	
user loginuser 

========================= hive sample_801 =========================

public void start() throws Exception {	webServer.start();	
started httpserver on port 

========================= hive sample_1435 =========================

data.set(textData.toString(), maxLength);	isNull = false;	} else {	try {	String byteData = null;	byteData = Text.decode(bytes.getData(), start, length);	data.set(byteData, maxLength);	isNull = false;	} catch (CharacterCodingException e) {	isNull = true;	
data not in the hivevarchar data type range so converted to null 

========================= hive sample_5476 =========================

protected final void channelRead0(ChannelHandlerContext ctx, Rpc.SaslMessage msg) throws Exception {	
handling sasl challenge message 

protected final void channelRead0(ChannelHandlerContext ctx, Rpc.SaslMessage msg) throws Exception {	Rpc.SaslMessage response = update(msg);	if (response != null) {	
sending sasl challenge response 

Rpc.SaslMessage response = update(msg);	if (response != null) {	hasAuthResponse = true;	ctx.channel().writeAndFlush(response).sync();	}	if (!isComplete()) {	return;	}	ctx.channel().pipeline().remove(this);	String qop = getNegotiatedProperty(Sasl.QOP);	
sasl negotiation finished with qop 

if (response != null) {	hasAuthResponse = true;	ctx.channel().writeAndFlush(response).sync();	}	if (!isComplete()) {	return;	}	ctx.channel().pipeline().remove(this);	String qop = getNegotiatedProperty(Sasl.QOP);	if (Rpc.SASL_AUTH_CONF.equals(qop)) {	
sasl confidentiality enabled 

public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) {	if (!isComplete()) {	
exception in sasl negotiation 

========================= hive sample_552 =========================

public void write(DataOutput out) throws IOException {	ByteArrayOutputStream bos = new ByteArrayOutputStream();	DataOutputStream dos = new DataOutputStream(bos);	super.write(dos);	int required = bos.size();	writeAdditionalPayload(dos);	int additional = bos.size() - required;	out.write(bos.toByteArray());	if (LOG.isTraceEnabled()) {	
writing additional bytes to orcsplit as payload required bytes 

========================= hive sample_3664 =========================

public void testRegression_HIVE_12178() throws Exception {	
testing wrong list status after eviction 

public void testLfuExtreme() {	int heapSize = 4;	
testing lambda lfu 

public void testLruExtreme() {	int heapSize = 4;	
testing lambda lru 

public void testDeadlockResolution() {	int heapSize = 4;	
testing deadlock resolution 

private void testHeapSize(int heapSize) {	
testing heap size 

} else {	if (i >= heapSize) {	assertSame(evicted[i - heapSize], evictedBuf);	assertTrue(evictedBuf.isInvalid());	} else {	assertNull(evictedBuf);	}	inserted.add(buffer);	}	}	
inserted 

if (i >= heapSize) {	assertSame(evicted[i - heapSize], evictedBuf);	assertTrue(evictedBuf.isInvalid());	} else {	assertNull(evictedBuf);	}	inserted.add(buffer);	}	}	Collections.shuffle(inserted, rdm);	
touch order 

========================= hive sample_2130 =========================


spis.get(from.id()).sendMessage(to, msg);	ConcurrentLinkedDeque8<GridTestMessage> queue = msgs.get(to.id());	if (queue == null) {	ConcurrentLinkedDeque8<GridTestMessage> old = msgs.putIfAbsent(to.id(), queue = new ConcurrentLinkedDeque8<>());	if (old != null) queue = old;	}	queue.offer(msg);	}	}	catch (IgniteException e) {	
unable to send message 

int id = threadId.getAndIncrement();	for (int i = 0; i < iterationCnt; i++) {	if (id == 0 && (i % 50) == 0) info(">>> Running iteration " + i);	try {	for (ClusterNode node : nodes) {	Message msg = new GridTestMessage(from.id(), msgId.getAndIncrement(), 0);	spis.get(from.id()).sendMessage(node, msg);	}	}	catch (IgniteException e) {	
oops unable to send message safe to ignore 

========================= ignite sample_2631 =========================

clientStartLatch.await();	Thread.sleep(10);	client = true;	Ignite cl = startGrid("client0");	IgniteCache<Object, Object> atomicCache = cl.cache(CACHE_NAME_PREFIX + '0');	IgniteCache<Object, Object> txCache = cl.cache(CACHE_NAME_PREFIX + '1');	assertEquals(100, atomicCache.size());	assertEquals(100, txCache.size());	}	catch (Exception e) {	
error occurred 

========================= ignite sample_1508 =========================

tx.commit();	} catch (Exception e) {	ex.compareAndSet(null, e);	}	}	}, 2, "tx");	fut.get(timeout << 1);	Exception e = ex.get();	assertNotNull(e);	boolean detected = X.hasCause(e, TransactionDeadlockException.class);	
failed to detect a deadlock 

Exception e = ex.get();	assertNotNull(e);	boolean detected = X.hasCause(e, TransactionDeadlockException.class);	else log.info(X.cause(e, TransactionDeadlockException.class).getMessage());	assertTrue(detected);	try {	assertEquals(TransactionTimeoutException.class, e.getCause().getClass());	assertEquals(TransactionDeadlockException.class, e.getCause().getCause().getClass());	}	catch (AssertionError err) {	
unexpected exception structure 

========================= ignite sample_1621 =========================

public void testVariousConnectionNumber() throws Exception {	startGridsMultiThreaded(3);	client = true;	startGridsMultiThreaded(3, 3);	CacheConfiguration ccfg = new CacheConfiguration(DEFAULT_CACHE_NAME);	ccfg.setCacheMode(REPLICATED);	ccfg.setWriteSynchronizationMode(FULL_SYNC);	ignite(0).createCache(ccfg);	for (int i = 0; i < 10; i++) {	
iteration 

========================= ignite sample_746 =========================

else {	IgniteBiTuple<Object, GridCacheVersion> t = (IgniteBiTuple<Object, GridCacheVersion>)loadFromStore(tx, key, false);	if (t != null) verVis.apply(key, t.get1(), t.get2());	}	return;	}	Collection<Object> keys0 = F.viewReadOnly(keys, new C1<KeyCacheObject, Object>() {	return cctx.unwrapBinaryIfNeeded(key, !convertBinary());	}	});	
loading values from store for keys 

========================= ignite sample_4248 =========================

long start = spi.getJoinTimeout() > 0 ? U.currentTimeMillis() : 0;	while (true) {	try {	spi.ipFinder.initializeLocalAddresses( U.resolveAddresses(spi.getAddressResolver(), locNode.socketAddresses()));	break;	}	catch (IllegalStateException e) {	throw new IgniteSpiException("Failed to register local node address with IP finder: " + locNode.socketAddresses(), e);	}	catch (IgniteSpiException e) {	
failed to register local node address in ip finder on start retrying every ms 

========================= ignite sample_5635 =========================

public void testAsyncFailover() throws Exception {	IgniteCache<TestKey, TestValue> cache = ignite(0).cache(DEFAULT_CACHE_NAME);	int ops = cache.getConfiguration(CacheConfiguration.class).getMaxConcurrentAsyncOperations();	
max concurrent async operations 

public void testAsyncFailover() throws Exception {	IgniteCache<TestKey, TestValue> cache = ignite(0).cache(DEFAULT_CACHE_NAME);	int ops = cache.getConfiguration(CacheConfiguration.class).getMaxConcurrentAsyncOperations();	assertTrue(ops > 0);	for (int i = 0; i < 2; i++) {	
iteration 

public void testAsyncFailover() throws Exception {	IgniteCache<TestKey, TestValue> cache = ignite(0).cache(DEFAULT_CACHE_NAME);	int ops = cache.getConfiguration(CacheConfiguration.class).getMaxConcurrentAsyncOperations();	assertTrue(ops > 0);	for (int i = 0; i < 2; i++) {	startGrid(NODE_CNT);	List<IgniteFuture<?>> futs = startAsyncOperations(ops, cache);	stopGrid(NODE_CNT);	for (IgniteFuture<?> fut : futs) fut.get();	
iteration done 

========================= ignite sample_1226 =========================

assertTrue(!ig1C.active());	assertTrue(!ig2C.active());	assertTrue(!ig3C.active());	stopPrimary(0);	boolean exc = false;	try {	ig3C.active(true);	}	catch (IgniteException e) {	exc = true;	
stack trace from remote node 

assertTrue(!ig3B.active());	assertTrue(!ig1CB.active());	assertTrue(!ig2CB.active());	assertTrue(!ig3CB.active());	stopPrimary(0);	try {	ig3CB.active(true);	fail("Activation should fail");	}	catch (IgniteException e) {	
stack trace from remote node 

========================= ignite sample_1699 =========================

for (CacheGroupHolder hld : grpHolders.values()) {	if (!hld.client() && cctx.cache().cacheGroup(hld.groupId()) == null) {	int grpId = hld.groupId();	CacheGroupHolder grpHolder = grpHolders.remove(grpId);	assert grpHolder != null && !grpHolder.client() : grpHolder;	try {	grpHolder = CacheGroupHolder2.create(cctx, caches.group(grpId), topVer, grpHolder.affinity());	grpHolders.put(grpId, grpHolder);	}	catch (IgniteCheckedException e) {	
failed to initialize cache 

public void dumpDebugInfo() {	if (!pendingAssignmentFetchFuts.isEmpty()) {	
pending assignment fetch futures 

========================= ignite sample_4161 =========================

public void testContainsKeyFailover() throws Exception {	int cnt = 3;	for (int i = 0; i < cnt; i++) {	try {	
iteration 

========================= ignite sample_1363 =========================

}	fut1.get();	fut2.get();	((IgniteKernal)grid).cache(DEFAULT_CACHE_NAME).dataStructures().removeQueue(QUEUE_NAME1);	((IgniteKernal)grid).cache(DEFAULT_CACHE_NAME).dataStructures().removeQueue(QUEUE_NAME2);	assertTrue(GridTestUtils.waitForCondition(new PAX() {	for (int i = 0; i < gridCount(); i++) {	if (getTestIgniteInstanceName(i).equals(killGridName)) continue;	Iterator<GridCacheEntryEx<Object, Object>> entries = ((GridKernal)grid(i)).context().cache().internalCache(DEFAULT_CACHE_NAME).map().allEntries0().iterator();	if (entries.hasNext()) {	
found cache entries will wait 

========================= ignite sample_992 =========================

client = true;	Ignite client1 = startGrid(4);	final Ignite client2 = startGrid(5);	final Integer key = primaryKey(srv0.cache(DEFAULT_CACHE_NAME));	final IgniteCache<Integer, Integer> cache1 = client1.createNearCache(DEFAULT_CACHE_NAME, new NearCacheConfiguration<Integer, Integer>());	final IgniteCache<Integer, Integer> cache2 = client2.createNearCache(DEFAULT_CACHE_NAME, new NearCacheConfiguration<Integer, Integer>());	cache1.put(key, 1);	final Integer newVal = 2;	testSpi(client2).blockMessages(GridNearTxFinishRequest.class, srv0.name());	IgniteInternalFuture<?> fut = GridTestUtils.runAsync(new Callable<Void>() {	
start put concurrency 

Ignite srv0 = grid(0);	IgniteCache<Integer, Integer> srv0Cache = srv0.createCache(cacheConfiguration(1, true, writeThrough));	awaitPartitionMapExchange();	final Integer key = primaryKey(srv0Cache);	srv0Cache.put(key, 1);	client = true;	Ignite client = startGrid(4);	testSpi(srv0).blockMessages(GridNearTxPrepareResponse.class, client.name());	final IgniteCache<Integer, Integer> clientCache = client.cache(DEFAULT_CACHE_NAME);	IgniteInternalFuture<?> fut = GridTestUtils.runAsync(new Callable<Void>() {	
start put 

========================= ignite sample_1119 =========================

public final void prepareRemoteTx() throws IgniteCheckedException {	if (!state(PREPARING)) {	if (state() != PREPARING || !optimistic()) {	
invalid transaction state for prepare 

assert entry != null : "Missing cached entry for transaction entry: " + txEntry;	try {	GridCacheVersion ver = txEntry.explicitVersion() != null ? txEntry.explicitVersion() : xidVer;	if (!entry.lockedBy(ver)) {	if (log.isDebugEnabled()) log.debug("Transaction does not own lock for entry (will wait) [entry=" + entry + ", tx=" + this + ']');	return;	}	break;	}	catch (GridCacheEntryRemovedException ignore) {	
got removed entry while committing will retry 

}	else if (op == RELOAD) {	CacheObject reloaded = cached.innerReload();	if (nearCached != null) {	nearCached.innerReload();	nearCached.updateOrEvict(cached.version(), reloaded, cached.expireTime(), cached.ttl(), nodeId, topVer);	}	}	else if (op == READ) {	assert near();	
ignoring read entry when committing 

if (nearCached != null) {	CacheObject val0 = cached.valueBytes();	nearCached.updateOrEvict(xidVer, val0, cached.expireTime(), cached.ttl(), nodeId, topVer);	}	}	}	assert txEntry.op() == READ || onePhaseCommit() || !cached.hasLockCandidateUnsafe(xidVer) || cached.lockedByUnsafe(xidVer) : "Transaction does not own lock for commit [entry=" + cached + ", tx=" + this + ']';	break;	}	catch (GridCacheEntryRemovedException ignored) {	
attempting to commit a removed entry will retry 

txEntry.cached(cacheCtx.cache().entryEx(txEntry.key(), topologyVersion()));	}	}	}	catch (Throwable ex) {	boolean nodeStopping = X.hasCause(ex, NodeStoppingException.class);	err = new IgniteTxHeuristicCheckedException("Commit produced a runtime exception " + "(all transaction entries will be invalidated): " + CU.txString(this), ex);	if (nodeStopping) {	U.warn(log, "Failed to commit transaction, node is stopping [tx=" + this + ", err=" + ex + ']');	}	
commit failed 

========================= ignite sample_3827 =========================

CacheConfiguration ccfg3 = new CacheConfiguration();	ccfg3.setName("cache3");	ccfg3.setAtomicityMode(CacheAtomicityMode.ATOMIC);	ccfg3.setCacheMode(CacheMode.LOCAL);	ignite.createCache(ccfg1);	ignite.createCache(ccfg2);	ignite.createCache(ccfg3).put(2, 3);	int iterations = 20;	long stopTime = U.currentTimeMillis() + 20_000;	for (int k = 0; k < iterations && U.currentTimeMillis() < stopTime; k++) {	
iteration 

========================= ignite sample_1660 =========================

final String subfolderName = genNewStyleSubfolderName(0, (UUID)consistentId);	putDummyRecords(ignite0, cacheObjectsToWrite);	stopGrid("node0");	final String workDir = U.defaultWorkDirectory();	final File db = U.resolveWorkDirectory(workDir, DFLT_STORE_DIR, false);	final File wal = new File(db, "wal");	final File walArchive = new File(wal, "archive");	final MockWalIteratorFactory mockItFactory = new MockWalIteratorFactory(log, PAGE_SIZE, consistentId, subfolderName, WAL_SEGMENTS);	final WALIterator it = mockItFactory.iterator(wal, walArchive);	final int cntUsingMockIter = iterateAndCount(it, false);	
total records loaded 

final File walArchive = new File(wal, "archive");	final MockWalIteratorFactory mockItFactory = new MockWalIteratorFactory(log, PAGE_SIZE, consistentId, subfolderName, WAL_SEGMENTS);	final WALIterator it = mockItFactory.iterator(wal, walArchive);	final int cntUsingMockIter = iterateAndCount(it, false);	assertTrue(cntUsingMockIter > 0);	assertTrue(cntUsingMockIter > cacheObjectsToWrite);	final File walArchiveDirWithConsistentId = new File(walArchive, subfolderName);	final File walWorkDirWithConsistentId = new File(wal, subfolderName);	final IgniteWalIteratorFactory factory = createWalIteratorFactory(workDir, subfolderName);	final int cntArchiveDir = iterateAndCount(factory.iteratorArchiveDirectory(walArchiveDirWithConsistentId));	
total records loaded using directory 

final MockWalIteratorFactory mockItFactory = new MockWalIteratorFactory(log, PAGE_SIZE, consistentId, subfolderName, WAL_SEGMENTS);	final WALIterator it = mockItFactory.iterator(wal, walArchive);	final int cntUsingMockIter = iterateAndCount(it, false);	assertTrue(cntUsingMockIter > 0);	assertTrue(cntUsingMockIter > cacheObjectsToWrite);	final File walArchiveDirWithConsistentId = new File(walArchive, subfolderName);	final File walWorkDirWithConsistentId = new File(wal, subfolderName);	final IgniteWalIteratorFactory factory = createWalIteratorFactory(workDir, subfolderName);	final int cntArchiveDir = iterateAndCount(factory.iteratorArchiveDirectory(walArchiveDirWithConsistentId));	final int cntArchiveFileByFile = iterateAndCount( factory.iteratorArchiveFiles( walArchiveDirWithConsistentId.listFiles(FileWriteAheadLogManager.WAL_SEGMENT_FILE_FILTER)));	
total records loaded using archive directory file by file 

final File walWorkDirWithConsistentId = new File(wal, subfolderName);	final IgniteWalIteratorFactory factory = createWalIteratorFactory(workDir, subfolderName);	final int cntArchiveDir = iterateAndCount(factory.iteratorArchiveDirectory(walArchiveDirWithConsistentId));	final int cntArchiveFileByFile = iterateAndCount( factory.iteratorArchiveFiles( walArchiveDirWithConsistentId.listFiles(FileWriteAheadLogManager.WAL_SEGMENT_FILE_FILTER)));	assertTrue(cntArchiveFileByFile > cacheObjectsToWrite);	assertTrue(cntArchiveDir > cacheObjectsToWrite);	assertTrue(cntArchiveDir == cntArchiveFileByFile);	assertTrue("Mock based reader loaded " + cntUsingMockIter + " records " + "but standalone has loaded only " + cntArchiveDir, cntUsingMockIter >= cntArchiveDir);	final File[] workFiles = walWorkDirWithConsistentId.listFiles(FileWriteAheadLogManager.WAL_SEGMENT_FILE_FILTER);	final int cntWork = iterateAndCount(factory.iteratorWorkFiles(workFiles));	
total records loaded from work 

int cnt = 0;	try (WALIterator it = walIter) {	while (it.hasNextX()) {	final IgniteBiTuple<WALPointer, WALRecord> next = it.nextX();	final WALRecord walRecord = next.get2();	if (touchEntries && walRecord.type() == WALRecord.RecordType.DATA_RECORD) {	final DataRecord record = (DataRecord)walRecord;	for (DataEntry entry : record.writeEntries()) {	final KeyCacheObject key = entry.key();	final CacheObject val = entry.value();	
op key value 

while (it.hasNextX()) {	final IgniteBiTuple<WALPointer, WALRecord> next = it.nextX();	final WALRecord walRecord = next.get2();	if (touchEntries && walRecord.type() == WALRecord.RecordType.DATA_RECORD) {	final DataRecord record = (DataRecord)walRecord;	for (DataEntry entry : record.writeEntries()) {	final KeyCacheObject key = entry.key();	final CacheObject val = entry.value();	}	}	
record 

public void testArchiveCompletedEventFired() throws Exception {	final AtomicBoolean evtRecorded = new AtomicBoolean();	final Ignite ignite = startGrid("node0");	ignite.active(true);	final IgniteEvents evts = ignite.events();	if (!evts.isEnabled(EVT_WAL_SEGMENT_ARCHIVED)) assertTrue("nothing to test", false);	evts.localListen(new IgnitePredicate<Event>() {	WalSegmentArchivedEvent archComplEvt = (WalSegmentArchivedEvent)e;	long idx = archComplEvt.getAbsWalSegmentIdx();	
finished archive for segment 

public void testArchiveIncompleteSegmentAfterInactivity() throws Exception {	final AtomicBoolean waitingForEvt = new AtomicBoolean();	final CountDownLatch archiveSegmentForInactivity = new CountDownLatch(1);	archiveIncompleteSegmentAfterInactivityMs = 1000;	final Ignite ignite = startGrid("node0");	ignite.active(true);	final IgniteEvents evts = ignite.events();	evts.localListen(new IgnitePredicate<Event>() {	WalSegmentArchivedEvent archComplEvt = (WalSegmentArchivedEvent)e;	long idx = archComplEvt.getAbsWalSegmentIdx();	
finished archive for segment 

final Map<Object, Object> ctrlMap = new HashMap<>();    for (Cache.Entry<Object, Object> next : entries) ctrlMap.put(next.getKey(), next.getValue());	final String subfolderName = genDbSubfolderName(ignite0, 0);	stopGrid("node0");	final String workDir = U.defaultWorkDirectory();	final File binaryMeta = U.resolveWorkDirectory(workDir, "binary_meta", false);	final File binaryMetaWithConsId = new File(binaryMeta, subfolderName);	final File marshallerMapping = U.resolveWorkDirectory(workDir, "marshaller", false);	final IgniteWalIteratorFactory factory = new IgniteWalIteratorFactory(log, PAGE_SIZE, binaryMetaWithConsId, marshallerMapping);	final IgniteBiInClosure<Object, Object> objConsumer = new IgniteBiInClosure<Object, Object>() {	boolean rmv = remove(ctrlMap, key, val);	
unable to remove key and value from control map k v 

final File wal = new File(db, "wal");	final File walArchive = new File(wal, "archive");	final File walArchiveDirWithConsistentId = new File(walArchive, subfolderName);	final File[] files = walArchiveDirWithConsistentId.listFiles(FileWriteAheadLogManager.WAL_SEGMENT_FILE_FILTER);	A.notNull(files, "Can't iterate over files [" + walArchiveDirWithConsistentId + "] Directory is N/A");	final WALIterator iter = factory.iteratorArchiveFiles(files);	final Map<GridCacheVersion, Integer> cntArch = iterateAndCountDataRecord(iter, objConsumer, dataRecordHnd);	int txCntObservedArch = cntArch.size();	if (cntArch.containsKey(null)) txCntObservedArch -= 1;	final int entriesArch = valuesSum(cntArch.values());	
total tx found loaded using archive directory file by file 

int txCntObservedArch = cntArch.size();	if (cntArch.containsKey(null)) txCntObservedArch -= 1;	final int entriesArch = valuesSum(cntArch.values());	final File walWorkDirWithNodeSubDir = new File(wal, subfolderName);	final File[] workFiles = walWorkDirWithNodeSubDir.listFiles(FileWriteAheadLogManager.WAL_SEGMENT_FILE_FILTER);	final WALIterator tuples = factory.iteratorWorkFiles(workFiles);	final Map<GridCacheVersion, Integer> cntWork = iterateAndCountDataRecord(tuples, objConsumer, dataRecordHnd);	int txCntObservedWork = cntWork.size();	if (cntWork.containsKey(null)) txCntObservedWork -= 1;	final int entriesWork = valuesSum(cntWork.values());	
archive directory tx found entries 

public void testFillWalForExactSegmentsCount() throws Exception {	customWalMode = WALMode.DEFAULT;	final CountDownLatch reqSegments = new CountDownLatch(15);	final Ignite ignite = startGrid("node0");	ignite.active(true);	final IgniteEvents evts = ignite.events();	if (!evts.isEnabled(EVT_WAL_SEGMENT_ARCHIVED)) assertTrue("nothing to test", false);	evts.localListen(new IgnitePredicate<Event>() {	WalSegmentArchivedEvent archComplEvt = (WalSegmentArchivedEvent)e;	long idx = archComplEvt.getAbsWalSegmentIdx();	
finished archive for segment 

private void createCache2(Ignite ig, CacheAtomicityMode mode) {	
populating the cache 

else if (entry instanceof LazyDataEntry) {	unwrappedKeyObj = null;	unwrappedValObj = null;	}	else {	final CacheObject val = entry.value();	unwrappedValObj = val instanceof BinaryObject ? val : val.value(null, false);	final CacheObject key = entry.key();	unwrappedKeyObj = key instanceof BinaryObject ? key : key.value(null, false);	}	
globaltxid unwrappedkeyobj unwrappedvalobj 

unwrappedKeyObj = key instanceof BinaryObject ? key : key.value(null, false);	}	if (cacheObjHnd != null && (unwrappedKeyObj != null || unwrappedValObj != null)) cacheObjHnd.apply(unwrappedKeyObj, unwrappedValObj);	final Integer entriesUnderTx = entriesUnderTxFound.get(globalTxId);	entriesUnderTxFound.put(globalTxId, entriesUnderTx == null ? 1 : entriesUnderTx + 1);	}	}	else if (walRecord.type() == WALRecord.RecordType.TX_RECORD && walRecord instanceof TxRecord) {	final TxRecord txRecord = (TxRecord)walRecord;	final GridCacheVersion globalTxId = txRecord.nearXidVersion();	
neartxversion globaltxid 

========================= ignite sample_1725 =========================

private void doTest(final GridClientConfiguration cfg) throws Exception {	GridTestUtils.runAsync(new IgniteCallable<Object>() {	try {	startGrid();	}	catch (Exception e) {	
grid start failed 

========================= ignite sample_245 =========================

private void testAddFailover(boolean collocated) throws Exception {	CollectionConfiguration colCfg = config(collocated);	IgniteQueue<Integer> queue = grid(0).queue(QUEUE_NAME, 0, colCfg);	assertNotNull(queue);	assertEquals(0, queue.size());	int primaryNode = primaryQueueNode(queue);	int testNodeIdx = -1;	for (int i = 0; i < gridCount(); i++) {	if (i != primaryNode) testNodeIdx = i;	}	
test node 

private void testAddFailover(boolean collocated) throws Exception {	CollectionConfiguration colCfg = config(collocated);	IgniteQueue<Integer> queue = grid(0).queue(QUEUE_NAME, 0, colCfg);	assertNotNull(queue);	assertEquals(0, queue.size());	int primaryNode = primaryQueueNode(queue);	int testNodeIdx = -1;	for (int i = 0; i < gridCount(); i++) {	if (i != primaryNode) testNodeIdx = i;	}	
header primary node 

try {	for (int i = 0; i < ITEMS; i++) {	assertTrue(queue.add(i));	if ((i + 1) % 500 == 0) log.info("Added " + (i + 1) + " items.");	}	}	finally {	stop.set(true);	}	fut.get();	
added all items 

private void testPollFailover(boolean collocated) throws Exception {	CollectionConfiguration colCfg = config(collocated);	IgniteQueue<Integer> queue = grid(0).queue(QUEUE_NAME, 0, colCfg);	assertNotNull(queue);	assertEquals(0, queue.size());	int primaryNode = primaryQueueNode(queue);	int testNodeIdx = -1;	for (int i = 0; i < gridCount(); i++) {	if (i != primaryNode) testNodeIdx = i;	}	
test node 

private void testPollFailover(boolean collocated) throws Exception {	CollectionConfiguration colCfg = config(collocated);	IgniteQueue<Integer> queue = grid(0).queue(QUEUE_NAME, 0, colCfg);	assertNotNull(queue);	assertEquals(0, queue.size());	int primaryNode = primaryQueueNode(queue);	int testNodeIdx = -1;	for (int i = 0; i < gridCount(); i++) {	if (i != primaryNode) testNodeIdx = i;	}	
primary node 

private void testPollFailover(IgniteQueue<Integer> queue, final List<Integer> killIdxs) throws Exception {	assert !killIdxs.isEmpty();	final int ITEMS = collectionCacheAtomicityMode() == ATOMIC && !queue.collocated() ? 10_000 : 3000;	for (int i = 0; i < ITEMS; i++) {	assertTrue(queue.add(i));	if ((i + 1) % 500 == 0) log.info("Added " + (i + 1) + " items.");	}	
added all items 

private IgniteInternalFuture<?> startNodeKiller(final AtomicBoolean stop, final AtomicInteger killCnt, final List<Integer> killIdxs) {	return GridTestUtils.runAsync(new Callable<Void>() {	ThreadLocalRandom rnd = ThreadLocalRandom.current();	while (!stop.get()) {	int idx = killIdxs.get(rnd.nextInt(0, killIdxs.size()));	U.sleep(rnd.nextLong(500, 1000));	
killing node 

========================= ignite sample_951 =========================

updateCache(grid, cache);	final AtomicBoolean stop = new AtomicBoolean();	ArrayList<IgniteInternalFuture> updaterFuts = new ArrayList<>();	for (int i = 0; i < updatersNumber(); i++) {	final int threadIdx = i;	IgniteInternalFuture<?> updateFut = GridTestUtils.runAsync(new Callable<Void>() {	Thread.currentThread().setName("update-thread-" + threadIdx);	assertTrue(cacheCheckedLatch.await(30_000, TimeUnit.MILLISECONDS));	int iter = 0;	while (!stop.get()) {	
start update 

assertTrue(cacheCheckedLatch.await(30_000, TimeUnit.MILLISECONDS));	int iter = 0;	while (!stop.get()) {	rwl.readLock().lock();	try {	updateCache(grid, cache);	}	finally {	rwl.readLock().unlock();	}	
end update 

int iter = 0;	while (!stop.get()) {	rwl.readLock().lock();	try {	updateCache(grid, cache);	}	finally {	rwl.readLock().unlock();	}	}	
update iterations 

========================= ignite sample_1359 =========================

private void unwrapData() throws IgniteCheckedException, SSLException {	
unwrapping received data 

========================= ignite sample_3364 =========================

try {	ipFinder.registerAddresses(Arrays.asList(new InetSocketAddress("1.1.1.1", 1024), new InetSocketAddress("1.1.1.2", 1024)));	Ignite g1 = startGrid(1);	long failureDetectTimeout = g1.configuration().getFailureDetectionTimeout();	long timeout = (long)(discoMap.get(g1.name()).getIpFinderCleanFrequency() * 1.5) + failureDetectTimeout;	GridTestUtils.waitForCondition(new GridAbsPredicate() {	return ipFinder.getRegisteredAddresses().size() == 1;	}	}, timeout);	if (ipFinder.getRegisteredAddresses().size() != 1) {	
failed to wait for ip cleanup will dump threads 

nodeSpi.set(spi0);	final Ignite ignite0 = startGrid(0);	nodeSpi.set(new TestCustomEventRaceSpi());	final Ignite ignite1 = startGrid(1);	CountDownLatch latch1 = new CountDownLatch(1);	CountDownLatch latch2 = new CountDownLatch(1);	spi0.nodeAdded1 = latch1;	spi0.nodeAdded2 = latch2;	spi0.debug = true;	IgniteInternalFuture<?> fut1 = GridTestUtils.runAsync(new Callable<Void>() {	
start 

public void testNodeShutdownOnRingMessageWorkerStartNotFinished() throws Exception {	try {	Ignite ignite0 = startGrid(0);	TestMessageWorkerFailureSpi2 spi0 = new TestMessageWorkerFailureSpi2();	nodeSpi.set(spi0);	try {	startGrid(1);	fail();	}	catch (Exception e) {	
expected error 

Ignite ignite0 = startGrid(0);	nodeSpi.set(new TestCustomEventCoordinatorFailureSpi());	Ignite ignite1 = startGrid(1);	nodeSpi.set(new TestCustomEventCoordinatorFailureSpi());	Ignite ignite2 = twoNodes ? null : startGrid(2);	final Ignite createCacheNode = ignite2 != null ? ignite2 : ignite1;	CountDownLatch latch = new CountDownLatch(1);	spi0.latch = latch;	final String CACHE_NAME = "test-cache";	IgniteInternalFuture<?> fut = GridTestUtils.runAsync(new Callable<Void>() {	
create test cache 

final int FAIL_ORDER = 3;	nodeSpi.set(new TestFailedNodesSpi(FAIL_ORDER));	final Ignite ignite0 = startGrid(0);	nodeSpi.set(new TestFailedNodesSpi(FAIL_ORDER));	Ignite ignite1 = startGrid(1);	TestFailedNodesSpi spi = new TestFailedNodesSpi(FAIL_ORDER);	spi.stopBeforeSndFail = true;	nodeSpi.set(spi);	Ignite ignite2 = startGrid(2);	waitNodeStop(ignite2.name());	
try start new node 

public void testDiscoveryEventsDiscard() throws Exception {	try {	TestEventDiscardSpi spi = new TestEventDiscardSpi();	nodeSpi.set(spi);	Ignite ignite0 = startGrid(0);	startGrid(1);	ignite0.createCache(new CacheConfiguration<>(DEFAULT_CACHE_NAME));	ignite0.destroyCache(DEFAULT_CACHE_NAME);	stopGrid(1);	
start new node 

private void checkDiscoData(Map<UUID, Map<Integer, byte[]>> discoData, TcpDiscoveryAbstractMessage msg) {	if (discoData != null && discoData.size() > 1) {	int cnt = 0;	for (Map<Integer, byte[]> map : discoData.values()) {	if (map.containsKey(GridComponent.DiscoveryDataExchangeType.CACHE_PROC.ordinal())) cnt++;	}	if (cnt > 1) {	fail = true;	
expect cache data only from one node but actually 

========================= ignite sample_2575 =========================

private void executeForNodeAndCache(CacheConfiguration ccfg, Ignite ignite, TestClosure clo, TransactionConcurrency concurrency, TransactionIsolation isolation) throws Exception {	String cacheName = ccfg.getName();	IgniteCache cache;	if (ignite.configuration().isClientMode() && ccfg.getCacheMode() == CacheMode.PARTITIONED && ccfg.getNearConfiguration() != null) cache = ignite.getOrCreateNearCache(ccfg.getName(), ccfg.getNearConfiguration());	else cache = ignite.cache(ccfg.getName());	cache.removeAll();	assertEquals(0, cache.size());	clo.configure(ignite, cache, concurrency, isolation);	
running test with node cache 

========================= ignite sample_7632 =========================

igfsName = null;	hostPort = hostPort(connStr, connStr);	}	else if (tokens.length == 2) {	String authStr = tokens[0];	if (authStr.isEmpty()) igfsName = null;	else {	String[] authTokens = authStr.split(":", -1);	igfsName = F.isEmpty(authTokens[0]) ? null : authTokens[0];	if (authTokens.length == 2) {	
grid name in igfs connection string is deprecated and will be ignored 

========================= ignite sample_7262 =========================

threads.add(runCacheOperations(grid1.cachex(CACHE_NAME), keys));	TimeUnit.SECONDS.sleep(3L);	client = testClient;	IgniteEx grid2 = startGrid(2);	assertEquals((Object)testClient, grid2.configuration().isClientMode());	client = false;	threads.add(runCacheOperations(grid2.cachex(CACHE_NAME), keys));	TimeUnit.SECONDS.sleep(3L);	IgniteEx grid3 = startGrid(3);	assertFalse(grid3.configuration().isClientMode());	
started client node 

private void stopThreads(Iterable<Thread> threads) {	try {	run = false;	for (Thread thread : threads) thread.join();	}	catch (Exception e) {	
couldn t stop threads 

Thread t = new Thread() {	while (run) {	TreeMap<Integer, String> vals = generateValues(keys);	try {	cache.lock(vals.firstKey(), 0);	try {	if (ThreadLocalRandom.current().nextDouble(1) < 0.65) cache.putAll(vals);	else cache.removeAll(vals.keySet());	}	catch (Exception e) {	
failed cache operation 

else cache.removeAll(vals.keySet());	}	catch (Exception e) {	}	finally {	cache.unlock(vals.firstKey());	}	U.sleep(100);	}	catch (Exception e){	
failed unlock 

========================= ignite sample_1162 =========================

protected abstract void startNodes() throws Exception;	Ignite ignite0 = ignite(0);	ignite0.createCache(ccfg);	int key = 0;	for (Ignite node : G.allGrids()) {	if (node.configuration().isClientMode() && ccfg.getNearConfiguration() != null) node.createNearCache(ccfg.getName(), ccfg.getNearConfiguration());	}	for (Ignite node : G.allGrids()) {	
test for node 

========================= ignite sample_1861 =========================

String catalog = conn.getCatalog();	if (catalog == null) {	String[] parts = jdbcUrl.split("[/:=]");	catalog = parts.length > 0 ? parts[parts.length - 1] : "NONE";	}	Collection<String> schemas = dbMetaReader.schemas(conn);	if (log.isDebugEnabled()) log.debug("Finished collection of schemas [jdbcUrl=" + jdbcUrl + ", catalog=" + catalog + ", count=" + schemas.size() + "]");	return new DbSchema(catalog, schemas);	}	catch (Throwable e) {	
failed to collect schemas 

protected Collection<DbTable> metadata(String jdbcDriverJarPath, String jdbcDriverCls, String jdbcUrl, Properties jdbcInfo, List<String> schemas, boolean tblsOnly) throws SQLException {	if (log.isDebugEnabled()) log.debug("Start collecting database metadata [drvJar=" + jdbcDriverJarPath + ", drvCls=" + jdbcDriverCls + ", jdbcUrl=" + jdbcUrl + "]");	try (Connection conn = connect(jdbcDriverJarPath, jdbcDriverCls, jdbcUrl, jdbcInfo)) {	Collection<DbTable> metadata = dbMetaReader.metadata(conn, schemas, tblsOnly);	if (log.isDebugEnabled()) log.debug("Finished collection of metadata [jdbcUrl=" + jdbcUrl + ", count=" + metadata.size() + "]");	return metadata;	}	catch (Throwable e) {	
failed to collect metadata 

========================= ignite sample_7370 =========================

OutputStreamWriter osw = null;	BufferedWriter bw = null;	try {	fos = new FileOutputStream(file, true);	osw = new OutputStreamWriter(fos);	bw = new BufferedWriter(osw);	if (addHdr) bw.write(HDR + U.nl());	for (Entry entry : entries0) bw.write(entry + U.nl());	}	catch (IOException e) {	
failed to flush logged entries to a disk due to an io exception 

========================= ignite sample_5137 =========================

serviceCache.context().continuousQueries().executeInternalQuery( new ServiceEntriesListener(), null, true, true, false );	}	else {	assert !ctx.isDaemon();	ctx.closure().runLocalSafe(new Runnable() {	try {	Iterable<CacheEntryEvent<?, ?>> entries = serviceCache.context().continuousQueries().existingEntries(false, null);	onSystemCacheUpdated(entries);	}	catch (IgniteCheckedException e) {	
failed to load service entries 

public IgniteInternalFuture<?> cancel(String name) {	while (true) {	try {	return removeServiceFromCache(name).fut;	}	catch (IgniteException | IgniteCheckedException e) {	if (X.hasCause(e, ClusterTopologyCheckedException.class)) {	
topology changed while cancelling service will retry 

public IgniteInternalFuture<?> cancel(String name) {	while (true) {	try {	return removeServiceFromCache(name).fut;	}	catch (IgniteException | IgniteCheckedException e) {	if (X.hasCause(e, ClusterTopologyCheckedException.class)) {	}	else {	
failed to undeploy service 

for (String name : svcNames) {	if (res == null) res = new GridCompoundFuture<>();	try {	CancelResult cr = removeServiceFromCache(name);	if (cr.rollback) toRollback.add(name);	res.add(cr.fut);	}	catch (IgniteException | IgniteCheckedException e) {	if (X.hasCause(e, ClusterTopologyCheckedException.class)) throw e;	else {	
failed to undeploy service 

res.add(new GridFinishedFuture<>(e));	}	}	}	tx.commit();	break;	}	catch (IgniteException | IgniteCheckedException e) {	for (String name : toRollback) undepFuts.remove(name).onDone(e);	if (X.hasCause(e, ClusterTopologyCheckedException.class)) {	
topology changed while cancelling service will retry 

GridServiceDeployment dep = (GridServiceDeployment)e.getValue();	ServiceDescriptorImpl desc = new ServiceDescriptorImpl(dep);	try {	GridServiceAssignments assigns = (GridServiceAssignments)serviceCache().getForcePrimary( new GridServiceAssignmentsKey(dep.configuration().getName()));	if (assigns != null) {	desc.topologySnapshot(assigns.assigns());	descs.add(desc);	}	}	catch (IgniteCheckedException ex) {	
failed to get assignments from replicated cache for service 

}	}	for (final ServiceContextImpl svcCtx : toInit) {	final Service svc;	try {	svc = copyAndInject(assigns.configuration());	svc.init(svcCtx);	svcCtx.service(svc);	}	catch (Throwable e) {	
failed to initialize service service will not be deployed 

}	else {	Service svc = cfg.getService();	try {	byte[] bytes = U.marshal(m, svc);	Service cp = U.unmarshal(m, bytes, U.resolveClassLoader(svc.getClass().getClassLoader(), ctx.config()));	ctx.resource().inject(cp);	return cp;	}	catch (IgniteCheckedException e) {	
failed to copy service will reuse same instance 

}	catch (Throwable e) {	log.error("Failed to cancel service (ignoring) [name=" + svcCtx.name() + ", execId=" + svcCtx.executionId() + ']', e);	if (e instanceof Error) throw e;	}	finally {	try {	ctx.resource().cleanup(svc);	}	catch (IgniteCheckedException e) {	
failed to clean up service will ignore 

if (fut != null) fut.onDone();	fut = undepFuts.remove(name);	if (fut != null) fut.onDone();	GridServiceAssignmentsKey key = new GridServiceAssignmentsKey(name);	IgniteInternalCache<Object, Object> cache = serviceCache();	if (cache.cache().affinity().isPrimary(ctx.discovery().localNode(), key)) {	try {	cache.getAndRemove(key);	}	catch (IgniteCheckedException ex) {	
failed to remove assignments for undeployed service 

========================= ignite sample_4819 =========================

for (int i = 1; i < GRID_CNT; i++) {	int cnt = ITEMS_CNT / (GRID_CNT - 1);	TakeJob job = new TakeJob(queueName, cnt, 10);	jobs.add(job);	comp = compute(grid(i).cluster().forLocal());	futs.add(comp.callAsync(job));	itemsLeft -= cnt;	}	assertEquals("Not all items will be polled", 0, itemsLeft);	for (TakeJob job : jobs) job.awaitItems();	
start one more grid 

========================= ignite sample_947 =========================

public void testGet() throws Exception {	final IgniteCache<Integer, Integer> cache = jcache(0);	final int KEYS = 1000;	Map<Integer, Integer> map = new HashMap<>();	for (int i = 0; i < KEYS; i++) map.put(i, i);	for (int i = 0; i < 10; i++) {	
iteration 

public void testScanQuery() throws Exception {	final IgniteCache<Integer, Integer> cache = jcache(0);	for (int i = 0; i < 5; i++) {	
iteration 

========================= ignite sample_1089 =========================

public void testQueryFromNewClient() throws Exception {	Ignite srv = startGrid("server");	for (int iter = 0; iter < 2; iter++) {	
iteration 

========================= ignite sample_7448 =========================

zkCluster = new TestingCluster(instances);	zkCluster.start();	zkCurator.blockUntilConnected();	assertEquals(4, zkCurator.getChildren().forPath(SERVICES_IGNITE_ZK_PATH).size());	stopAllGrids();	assertTrue(GridTestUtils.waitForCondition(new GridAbsPredicate() {	try {	return 0 == zkCurator.getChildren().forPath(SERVICES_IGNITE_ZK_PATH).size();	}	catch (Exception e) {	
failed to wait for zk condition 

========================= ignite sample_6259 =========================

}	assert !req.clientStartOnly() : req;	DynamicCacheDescriptor desc = registeredCaches.get(req.cacheName());	boolean needExchange = false;	boolean clientCacheStart = false;	AffinityTopologyVersion waitTopVer = null;	if (req.start()) {	if (desc == null) {	String conflictErr = checkCacheConflict(req.startCacheConfiguration());	if (conflictErr != null) {	
ignore cache start request 

DynamicCacheDescriptor old = registeredTemplates.put(cfg.getName(), desc);	assert old == null : old;	}	}	for (CacheJoinNodeDiscoveryData.CacheInfo cacheInfo : joinData.caches().values()) {	CacheConfiguration<?, ?> cfg = cacheInfo.cacheData().config();	if (!registeredCaches.containsKey(cfg.getName())) {	String conflictErr = checkCacheConflict(cfg);	if (conflictErr != null) {	if (locJoin) return conflictErr;	
ignore cache received from joining node 

========================= ignite sample_3885 =========================

private void checkCache(String cacheName, int expNodes) {	
test cache 

========================= ignite sample_526 =========================

Collection<ClusterNode> dhtRemoteNodes = F.view(dhtNodes, F.remoteNodes(nodeId()));	map(entry, dhtRemoteNodes, dhtMap);	Collection<ClusterNode> nearRemoteNodes = null;	if (nearMap != null) {	Collection<UUID> readers = entry.readers();	Collection<ClusterNode> nearNodes = null;	if (!F.isEmpty(readers)) {	nearNodes = discovery().nodes(readers, F0.notEqualTo(nearNodeId));	if (log.isDebugEnabled()) log.debug("Mapping entry to near nodes [nodes=" + U.nodeIds(nearNodes) + ", entry=" + entry + ']');	}	
entry has no near readers 

public boolean cancelRemove(@Nullable IgniteBiTuple<Boolean, ?> interceptorRes) {	if (interceptorRes != null) {	if (interceptorRes.get1() == null) {	
cacheinterceptor must not return null as cancellation flag value from onbeforeremove method 

public boolean cancelRemove(@Nullable IgniteBiTuple<Boolean, ?> interceptorRes) {	if (interceptorRes != null) {	if (interceptorRes.get1() == null) {	return false;	}	else return interceptorRes.get1();	}	else {	
cacheinterceptor must not return null from onbeforeremove method 

========================= ignite sample_3702 =========================

private void testQueryCancel(int keyCnt, int valSize, String sql, int timeoutUnits, TimeUnit timeUnit, boolean timeout) throws Exception {	try (Ignite client = startGrid("client")) {	IgniteCache<Object, Object> cache = client.cache(DEFAULT_CACHE_NAME);	assertEquals(0, cache.localSize());	int p = 1;	for (int i = 1; i <= keyCnt; i++) {	char[] tmp = new char[valSize];	Arrays.fill(tmp, ' ');	cache.put(i, new String(tmp));	if (i/(float)keyCnt >= p/10f) {	
loaded of 

cursor = cache.query(qry);	client.scheduler().runLocal(new Runnable() {	cursor.close();	}	}, timeoutUnits, timeUnit);	}	try(QueryCursor<List<?>> ignored = cursor) {	cursor.iterator();	}	catch (CacheException ex) {	
got expected exception 

========================= ignite sample_7468 =========================

private void createCacheMultinode(final TestOp op) throws Exception {	final int THREADS = NODES * 3;	for (int i = 0; i < 10; i++) {	
iteration 

========================= ignite sample_1553 =========================

for (Integer key : keys) {	info("Put " + key);	cache.put(key, 2);	}	info("Commit.");	tx.commit();	}	fail("Transaction should fail.");	}	catch (TransactionHeuristicException e) {	
expected exception 

private void checkUnlocked(final Integer key) throws Exception {	TestIndexingSpi.forceFail(false);	awaitPartitionMapExchange();	info("Check key: " + key);	for (int i = 0; i < gridCount(); i++) {	final int idx = i;	GridTestUtils.waitForCondition(new PA() {	IgniteKernal grid = (IgniteKernal)grid(idx);	GridCacheAdapter cache = grid.internalCache(DEFAULT_CACHE_NAME);	GridCacheEntryEx entry = cache.peekEx(key);	
entry 

if (locked) {	info("Unexpected entry for grid [i=" + idx + ", entry=" + entry + ']');	return false;	}	}	catch (GridCacheEntryRemovedException ignore) {	}	}	if (cache.isNear()) {	entry = ((GridNearCacheAdapter)cache).dht().peekEx(key);	
dht entry 

========================= ignite sample_1800 =========================

public void testReservedOnExchange() throws Exception {	System.setProperty(IGNITE_PDS_WAL_REBALANCE_THRESHOLD, "0");	final int entryCnt = 10_000;	final int initGridCnt = 4;	final IgniteEx ig0 = (IgniteEx)startGrids(initGridCnt + 1);	ig0.active(true);	stopGrid(initGridCnt);	Assert.assertEquals(5, ig0.context().state().clusterState().baselineTopology().consistentIds().size());	long start = U.currentTimeMillis();	
start loading 

ig0.active(true);	stopGrid(initGridCnt);	Assert.assertEquals(5, ig0.context().state().clusterState().baselineTopology().consistentIds().size());	long start = U.currentTimeMillis();	try (IgniteDataStreamer<Object, Object> st = ig0.dataStreamer("cache1")){	for (int k = 0; k < entryCnt; k++){	st.addData(k, k);	printProgress(k);	}	}	
finish loading time 

Assert.assertEquals(5, ig0.context().state().clusterState().baselineTopology().consistentIds().size());	long start = U.currentTimeMillis();	try (IgniteDataStreamer<Object, Object> st = ig0.dataStreamer("cache1")){	for (int k = 0; k < entryCnt; k++){	st.addData(k, k);	printProgress(k);	}	}	forceCheckpoint();	start = U.currentTimeMillis();	
start loading 

}	forceCheckpoint();	start = U.currentTimeMillis();	try (IgniteDataStreamer<Object, Object> st = ig0.dataStreamer("cache1")) {	st.allowOverwrite(true);	for (int k = 0; k < entryCnt; k++) {	st.addData(k, k * 2);	printProgress(k);	}	}	
finish loading time 

start = U.currentTimeMillis();	try (IgniteDataStreamer<Object, Object> st = ig0.dataStreamer("cache1")) {	st.allowOverwrite(true);	for (int k = 0; k < entryCnt; k++) {	st.addData(k, k * 2);	printProgress(k);	}	}	forceCheckpoint();	start = U.currentTimeMillis();	
start loading 

}	forceCheckpoint();	start = U.currentTimeMillis();	try (IgniteDataStreamer<Object, Object> st = ig0.dataStreamer("cache1")){	st.allowOverwrite(true);	for (int k = 0; k < entryCnt; k++){	st.addData(k, k);	printProgress(k);	}	}	
finish loading time 

========================= ignite sample_1729 =========================

public void testRemoteNodeCallback() throws Exception {	IgniteCache<Integer, Integer> cache1 = grid(0).cache(DEFAULT_CACHE_NAME);	IgniteCache<Integer, Integer> cache2 = grid(1).cache(DEFAULT_CACHE_NAME);	ContinuousQuery<Integer, Integer> qry = new ContinuousQuery<>();	final AtomicReference<Integer> val = new AtomicReference<>();	final CountDownLatch latch = new CountDownLatch(1);	qry.setLocalListener(new CacheEntryUpdatedListener<Integer, Integer>() {	Iterator<CacheEntryEvent<? extends Integer, ? extends Integer>> it = evts.iterator();	CacheEntryEvent<? extends Integer, ? extends Integer> e = it.next();	assert !it.hasNext();	
event 

public void testCrossCallback() throws Exception {	IgniteCache<Integer, Integer> cache1 = grid(0).cache(DEFAULT_CACHE_NAME);	IgniteCache<Integer, Integer> cache2 = grid(1).cache(DEFAULT_CACHE_NAME);	final int key1 = primaryKey(cache1);	final int key2 = primaryKey(cache2);	final CountDownLatch latch1 = new CountDownLatch(2);	final CountDownLatch latch2 = new CountDownLatch(2);	ContinuousQuery<Integer, Integer> qry1 = new ContinuousQuery<>();	qry1.setLocalListener(new CacheEntryUpdatedListener<Integer, Integer>() {	for (CacheEntryEvent<? extends Integer, ? extends Integer> evt : evts) {	
update in cache 

========================= ignite sample_1953 =========================

conn.commit();	}	catch (SQLException e) {	try (ResultSet tablesAgain = dbm.getTables(null, null, addrTableName, null)) {	if (!tablesAgain.next()) throw e;	}	}	}	}	committed = true;	
db schema has been initialized 

========================= ignite sample_5678 =========================

ses.update(e);	ses.save(new Entity(2, "name-2"));	ses.delete(ses.load(Entity2.class, 0));	Entity2 e2 = (Entity2)ses.load(Entity2.class, 1);	e2.setName("name-2");	ses.update(e2);	tx.commit();	fail("Commit must fail.");	}	catch (ConstraintViolationException e) {	
expected exception 

if (accessType == AccessType.READ_ONLY) return;	e0.setVersion(ver - 1);	ses = sesFactory1.openSession();	Transaction tx = ses.beginTransaction();	try {	ses.update(e0);	tx.commit();	fail("Commit must fail.");	}	catch (StaleStateException e) {	
expected exception 

assertNaturalIdCache(sesFactory1, nameToId, "name-1-changed1");	ses = sesFactory1.openSession();	Transaction tx = ses.beginTransaction();	try {	Entity e1 = (Entity)ses.load(Entity.class, 1);	e1.setName("name-0");	tx.commit();	fail("Commit must fail.");	}	catch (ConstraintViolationException e) {	
expected exception 

assertEntityCache(ENTITY_NAME, sesFactory1, idToName, 100);	ses = sesFactory1.openSession();	Transaction tx = ses.beginTransaction();	try {	ses.save(new Entity(3, "name-3"));	ses.save(new Entity(0, "name-0"));	tx.commit();	fail("Commit must fail.");	}	catch (ConstraintViolationException e) {	
expected exception 

Entity e0 = (Entity)ses.load(Entity.class, 0);	Entity e1 = (Entity)ses.load(Entity.class, 1);	e0.setName("name-10");	e1.setName("name-2");	ses.update(e0);	ses.update(e1);	tx.commit();	fail("Commit must fail.");	}	catch (ConstraintViolationException e) {	
expected exception 

tx = ses.beginTransaction();	try {	ses.save(new Entity(3, "name-3"));	Entity e1 = (Entity)ses.load(Entity.class, 1);	e1.setName("name-10");	ses.delete(ses.load(Entity.class, 0));	tx.commit();	fail("Commit must fail.");	}	catch (ConstraintViolationException e) {	
expected exception 

ses = sesFactory1.openSession();	tx = ses.beginTransaction();	try {	ses.delete(ses.load(Entity.class, 1));	idToName.remove(1);	ses.delete(ses.load(Entity.class, 0));	tx.commit();	fail("Commit must fail.");	}	catch (ConstraintViolationException e) {	
expected exception 

========================= ignite sample_6111 =========================

protected final void teraSort(boolean gzip) throws Exception {	System.out.println("TeraSort ===============================================================");	getFileSystem().delete(new Path(sortOutDir), true);	final JobConf jobConf = new JobConf();	jobConf.setUser(getUser());	jobConf.set("fs.defaultFS", getFsBase());	
desired number of reduces 

protected final void teraSort(boolean gzip) throws Exception {	System.out.println("TeraSort ===============================================================");	getFileSystem().delete(new Path(sortOutDir), true);	final JobConf jobConf = new JobConf();	jobConf.setUser(getUser());	jobConf.set("fs.defaultFS", getFsBase());	jobConf.set("mapreduce.job.reduces", String.valueOf(numReduces()));	
desired number of maps 

protected final void teraSort(boolean gzip) throws Exception {	System.out.println("TeraSort ===============================================================");	getFileSystem().delete(new Path(sortOutDir), true);	final JobConf jobConf = new JobConf();	jobConf.setUser(getUser());	jobConf.set("fs.defaultFS", getFsBase());	jobConf.set("mapreduce.job.reduces", String.valueOf(numReduces()));	final long splitSize = dataSizeBytes() / numMaps();	
desired split size 

========================= ignite sample_7079 =========================

public void testDataRebalancing() throws Exception {	Ignite ignite = startGrid(0);	final int KEYS = 10_000;	IgniteCache<Object, Object> cache = ignite.cache(DEFAULT_CACHE_NAME);	for (int i = 0; i < KEYS; i++) cache.put(i, i);	for (int i = 0; i < 3; i++) {	
iteration 

========================= ignite sample_1007 =========================

protected IgniteFuture<?> check(CacheException e) {	
expected exception 

protected IgniteFuture<?> check(CacheException e) {	
unexpected cause 

protected void checkAndWait(IgniteClientDisconnectedException e) {	
expected exception 

========================= ignite sample_2294 =========================

private BinaryMetadata readMetadata(int typeId) {	File file = new File(workDir, Integer.toString(typeId) + ".bin");	if (!file.exists()) return null;	try (FileInputStream in = new FileInputStream(file)) {	return U.unmarshal(ctx.config().getMarshaller(), in, U.resolveClassLoader(ctx.config()));	}	catch (Exception e) {	
failed to restore metadata from file exception was thrown 

========================= ignite sample_4157 =========================

public void undeployTask(String taskName, boolean locUndeploy, Collection<ClusterNode> rmtNodes) {	assert taskName != null;	assert !rmtNodes.contains(ctx.discovery().localNode());	if (locDep == null) {	if (locUndeploy) locStore.explicitUndeploy(null, taskName);	try {	comm.sendUndeployRequest(taskName, rmtNodes);	}	catch (IgniteCheckedException e) {	
failed to send undeployment request for task 

========================= ignite sample_3554 =========================

private void loadCache(IgniteCache<Integer, String> cache) {	int p = 1;	for (int i = 1; i <= CACHE_SIZE; i++) {	char[] tmp = new char[256];	Arrays.fill(tmp, ' ');	cache.put(i, new String(tmp));	if (i / (float)CACHE_SIZE >= p / 10f) {	
loaded of 

========================= ignite sample_7517 =========================

if (f != null) {	if (txFut == null) txFut = new GridCompoundFuture<>(CU.boolReducer());	txFut.add(f);	}	break;	}	catch (IgniteCheckedException err) {	return new GridFinishedFuture<>(err);	}	catch (GridCacheEntryRemovedException ignore) {	
got removed entry when getting a dht value 

========================= ignite sample_3807 =========================

break;	}	default: { assert false; }	}	}	tx.commit();	if (isTestDebug()) debug("Committed transaction [i=" + i + ", tx=" + tx + ']');	}	catch (TransactionOptimisticException e) {	if (!(concurrency == OPTIMISTIC && isolation == SERIALIZABLE)) {	
unexpected error 

}	tx.commit();	if (isTestDebug()) debug("Committed transaction [i=" + i + ", tx=" + tx + ']');	}	catch (TransactionOptimisticException e) {	if (!(concurrency == OPTIMISTIC && isolation == SERIALIZABLE)) {	throw e;	}	}	catch (Throwable e) {	
unexpected error 

========================= ignite sample_1990 =========================

private void doTestIgniteOperationOnDisconnect(Ignite client, final List<T2<Callable, C1<Object, Boolean>>> ops) throws Exception {	assertNotNull(client.cache(DEFAULT_CACHE_NAME));	final TestTcpDiscoverySpi clientSpi = spi(client);	Ignite srv = clientRouter(client);	TestTcpDiscoverySpi srvSpi = spi(srv);	final CountDownLatch disconnectLatch = new CountDownLatch(1);	final CountDownLatch reconnectLatch = new CountDownLatch(1);	
block reconnect 

========================= ignite sample_635 =========================

private UpdateResult doDelete(GridCacheContext cctx, Iterable<List<?>> cursor, int pageSize) throws IgniteCheckedException {	DmlBatchSender sender = new DmlBatchSender(cctx, pageSize, 1);	for (List<?> row : cursor) {	if (row.size() != 2) {	
invalid row size on delete expected got 

========================= ignite sample_7686 =========================

private void onForceKeysError(UUID nodeId, GridDhtLockRequest req, IgniteCheckedException e) {	GridDhtLockResponse res = new GridDhtLockResponse(ctx.cacheId(), req.version(), req.futureId(), req.miniId(), e, ctx.deploymentEnabled());	try {	ctx.io().send(nodeId, res, ctx.ioPolicy());	}	catch (ClusterTopologyCheckedException ignored) {	
failed to send lock reply to remote node because it left grid 

private void onForceKeysError(UUID nodeId, GridDhtLockRequest req, IgniteCheckedException e) {	GridDhtLockResponse res = new GridDhtLockResponse(ctx.cacheId(), req.version(), req.futureId(), req.miniId(), e, ctx.deploymentEnabled());	try {	ctx.io().send(nodeId, res, ctx.ioPolicy());	}	catch (ClusterTopologyCheckedException ignored) {	}	catch (IgniteCheckedException ignored) {	
failed to send lock reply to node 

for (KeyCacheObject key : keys) {	try {	while (true) {	GridDhtCacheEntry entry = entryExx(key, tx.topologyVersion());	try {	fut.addEntry(entry);	if (fut.isDone()) return fut;	break;	}	catch (GridCacheEntryRemovedException ignore) {	
got removed entry when adding lock will retry 

fut.addEntry(key == null ? null : entry);	if (fut.isDone()) {	timedout = true;	break;	}	}	entries.add(entry);	break;	}	catch (GridCacheEntryRemovedException ignore) {	
got removed entry when adding lock will retry 

timedout = true;	break;	}	}	entries.add(entry);	break;	}	catch (GridCacheEntryRemovedException ignore) {	}	catch (GridDistributedLockCancelledException e) {	
got lock request for cancelled lock will ignore 

}	if (ret && val == null) val = e.valueBytes(null);	res.addValueBytes( ret ? val : null, filterPassed, ver, mappedVer);	}	else {	res.addValueBytes(null, false, e.version(), mappedVer);	}	break;	}	catch (GridCacheEntryRemovedException ignore) {	
got removed entry when sending reply to dht lock request will retry 

Collection<GridCacheVersion> lessPending = new GridLeanSet<>(5);	for (GridCacheEntryEx entry : entries) {	while (true) {	try {	for (GridCacheMvccCandidate cand : entry.localCandidates()) {	if (cand.version().isLess(baseVer)) lessPending.add(cand.version());	}	break;	}	catch (GridCacheEntryRemovedException ignored) {	
got removed entry is localdhtpendingversions will retry 

if (keys != null) {	for (KeyCacheObject key : keys) {	while (true) {	GridDistributedCacheEntry entry = peekExx(key);	if (entry == null) break;	try {	entry.doneRemote( req.version(), req.version(), null, null, null, if (entry.removeLock(req.version())) {	if (log.isDebugEnabled()) log.debug("Removed lock [lockId=" + req.version() + ", key=" + key + ']');	}	else {	
received unlock request for unknown candidate added to cancelled locks set 

if (log.isDebugEnabled()) log.debug("Primary node mismatch for unlock [entry=" + cached + ", expected=" + ctx.nodeId() + ", actual=" + U.toShortString(primary) + ']');	return;	}	if (log.isDebugEnabled()) log.debug("Mapping entry to DHT nodes [nodes=" + U.toShortString(dhtNodes) + ", entry=" + cached + ']');	Collection<ClusterNode> nearNodes = null;	if (!F.isEmpty(readers)) {	nearNodes = ctx.discovery().nodes(readers, F0.not(F.idForNodeId(nodeId)));	if (log.isDebugEnabled()) log.debug("Mapping entry to near nodes [nodes=" + U.toShortString(nearNodes) + ", entry=" + cached + ']');	}	else {	
entry has no near readers 

if (entry.removeLock(dhtVer)) {	map(nodeId, topVer, entry, readers, dhtMap, nearMap);	if (log.isDebugEnabled()) log.debug("Removed lock [lockId=" + ver + ", key=" + key + ']');	}	else if (log.isDebugEnabled()) log.debug("Received unlock request for unknown candidate " + "(added to cancelled locks set) [ver=" + ver + ", entry=" + entry + ']');	if (created && entry.markObsolete(dhtVer)) removeEntry(entry);	ctx.evicts().touch(entry, topVer);	break;	}	catch (GridCacheEntryRemovedException ignored) {	
received remove lock request for removed entry will retry 

if (log.isDebugEnabled()) log.debug("Removed lock [lockId=" + ver + ", key=" + key + ']');	}	else if (log.isDebugEnabled()) log.debug("Received unlock request for unknown candidate " + "(added to cancelled locks set) [ver=" + ver + ", entry=" + entry + ']');	if (created && entry.markObsolete(dhtVer)) removeEntry(entry);	ctx.evicts().touch(entry, topVer);	break;	}	catch (GridCacheEntryRemovedException ignored) {	}	catch (IgniteCheckedException e) {	
failed to remove locks for keys 

GridDhtUnlockRequest req = new GridDhtUnlockRequest(ctx.cacheId(), keyBytes.size(), ctx.deploymentEnabled());	req.version(dhtVer);	try {	for (KeyCacheObject key : keyBytes) req.addKey(key, ctx);	keyBytes = nearMap.get(n);	if (keyBytes != null) for (KeyCacheObject key : keyBytes) req.addNearKey(key);	req.completedVersions(committed, rolledback);	ctx.io().send(n, req, ctx.ioPolicy());	}	catch (ClusterTopologyCheckedException ignore) {	
node left while sending unlock request 

try {	for (KeyCacheObject key : keyBytes) req.addKey(key, ctx);	keyBytes = nearMap.get(n);	if (keyBytes != null) for (KeyCacheObject key : keyBytes) req.addNearKey(key);	req.completedVersions(committed, rolledback);	ctx.io().send(n, req, ctx.ioPolicy());	}	catch (ClusterTopologyCheckedException ignore) {	}	catch (IgniteCheckedException e) {	
failed to send unlock request to node will make best effort to complete 

if (!dhtMap.containsKey(n)) {	List<KeyCacheObject> keyBytes = entry.getValue();	GridDhtUnlockRequest req = new GridDhtUnlockRequest(ctx.cacheId(), keyBytes.size(), ctx.deploymentEnabled());	req.version(dhtVer);	try {	for (KeyCacheObject key : keyBytes) req.addNearKey(key);	req.completedVersions(committed, rolledback);	ctx.io().send(n, req, ctx.ioPolicy());	}	catch (ClusterTopologyCheckedException ignore) {	
node left while sending unlock request 

GridDhtUnlockRequest req = new GridDhtUnlockRequest(ctx.cacheId(), keyBytes.size(), ctx.deploymentEnabled());	req.version(dhtVer);	try {	for (KeyCacheObject key : keyBytes) req.addNearKey(key);	req.completedVersions(committed, rolledback);	ctx.io().send(n, req, ctx.ioPolicy());	}	catch (ClusterTopologyCheckedException ignore) {	}	catch (IgniteCheckedException e) {	
failed to send unlock request to node will make best effort to complete 

========================= ignite sample_3780 =========================

cache1.put(key1, 0);	barrier.await();	int key2 = primaryKey(cache2);	log.info(">>> Performs put [node=" + ((IgniteKernal)ignite).localNode() + ", tx=" + tx + ", key=" + key2 + ", cache=" + cache2.getName() + ']');	cache2.put(key2, 1);	tx.commit();	commitCnt.incrementAndGet();	}	catch (Throwable e) {	if (hasCause(e, TransactionTimeoutException.class) && hasCause(e, TransactionDeadlockException.class) ) {	
at least one stack trace should contain 

========================= ignite sample_1617 =========================

private IgniteInternalFuture<?> runQueryAsync(final Query<?> qry) throws Exception {	return multithreadedAsync(new Runnable() {	try {	
query started 

private IgniteInternalFuture<?> runQueryAsync(final Query<?> qry) throws Exception {	return multithreadedAsync(new Runnable() {	try {	grid(0).cache(DEFAULT_CACHE_NAME).query(qry).getAll();	
query finished 

final GridQueryProcessor qryProc = grid(0).context().query();	Collection<GridRunningQueryInfo> queries = qryProc.runningQueries(0);	assertEquals(1, queries.size());	final Collection<GridRunningQueryInfo> finalQueries = queries;	for (GridRunningQueryInfo query : finalQueries) qryProc.cancelQueries(Collections.singleton(query.id()));	int n = 100;	while (n > 0) {	Thread.sleep(100);	queries = qryProc.runningQueries(0);	if (queries.isEmpty()) break;	
wait for cancel 

========================= ignite sample_7540 =========================

dep = ctx.deploy().deploy(cls, ldr);	if (dep == null) throw new IgniteDeploymentCheckedException("Failed to auto-deploy task " + "(was task (re|un)deployed?): " + cls);	taskName = taskName(dep, taskCls, map);	}	catch (IgniteCheckedException e) {	taskName = task.getClass().getName();	deployEx = e;	}	}	assert taskName != null;	
task deployment 

public void setAttributes(GridTaskSessionImpl ses, Map<?, ?> attrs) throws IgniteCheckedException {	long timeout = ses.getEndTime() - U.currentTimeMillis();	if (timeout <= 0) {	
task execution timed out remote session attributes won t be set 

public void setAttributes(GridTaskSessionImpl ses, Map<?, ?> attrs) throws IgniteCheckedException {	long timeout = ses.getEndTime() - U.currentTimeMillis();	if (timeout <= 0) {	return;	}	
setting session attribute s from task or future 

private void sendSessionAttributes(Map<?, ?> attrs, GridTaskSessionImpl ses) throws IgniteCheckedException {	assert attrs != null;	assert ses != null;	Collection<ComputeJobSibling> siblings = ses.getJobSiblings();	GridIoManager commMgr = ctx.io();	long timeout = ses.getEndTime() - U.currentTimeMillis();	if (timeout <= 0) {	
session attributes won t be set due to task timeout 

Collection<ComputeJobSibling> siblings = ses.getJobSiblings();	GridIoManager commMgr = ctx.io();	long timeout = ses.getEndTime() - U.currentTimeMillis();	if (timeout <= 0) {	return;	}	Set<UUID> rcvrs = new HashSet<>();	UUID locNodeId = ctx.localNodeId();	synchronized (ses) {	if (ses.isClosed()) {	
setting session attributes on closed session will ignore 

try {	commMgr.sendOrderedMessage( node, sib.jobTopic(), req, SYSTEM_POOL, timeout, false);	}	catch (IgniteCheckedException e) {	node = e instanceof  ClusterTopologyCheckedException ? null : ctx.discovery().node(nodeId);	if (node != null) {	try {	Thread.sleep(DISCO_TIMEOUT);	}	catch (InterruptedException ignore) {	
got interrupted while sending session attributes 

public void processJobExecuteResponse(UUID nodeId, GridJobExecuteResponse msg) {	assert nodeId != null;	assert msg != null;	lock.readLock();	try {	if (stopping && !waiting) {	
received job execution response while stopping grid will ignore 

public void processJobExecuteResponse(UUID nodeId, GridJobExecuteResponse msg) {	assert nodeId != null;	assert msg != null;	lock.readLock();	try {	if (stopping && !waiting) {	return;	}	GridTaskWorker<?, ?> task = tasks.get(msg.getSessionId());	if (task == null) {	
received job execution response for unknown task was task already reduced 

private void processTaskSessionRequest(UUID nodeId, GridTaskSessionRequest msg) {	assert nodeId != null;	assert msg != null;	lock.readLock();	try {	if (stopping && !waiting) {	
received task session request while stopping grid will ignore 

private void processTaskSessionRequest(UUID nodeId, GridTaskSessionRequest msg) {	assert nodeId != null;	assert msg != null;	lock.readLock();	try {	if (stopping && !waiting) {	return;	}	GridTaskWorker<?, ?> task = tasks.get(msg.getSessionId());	if (task == null) {	
received task session request for unknown task was task already reduced 

GridTaskWorker<?, ?> task = tasks.get(msg.getSessionId());	if (task == null) {	return;	}	boolean loc = ctx.localNodeId().equals(nodeId) && !ctx.config().isMarshalLocalJobs();	Map<?, ?> attrs = loc ? msg.getAttributes() : U.<Map<?, ?>>unmarshal(marsh, msg.getAttributesBytes(), U.resolveClassLoader(task.getTask().getClass().getClassLoader(), ctx.config()));	GridTaskSessionImpl ses = task.getSession();	sendSessionAttributes(attrs, ses);	}	catch (IgniteCheckedException e) {	
failed to deserialize session request 

public void onCancelled(IgniteUuid sesId) {	assert sesId != null;	lock.readLock();	try {	if (stopping && !waiting) {	
attempt to cancel task while stopping grid will ignore 

public void onCancelled(IgniteUuid sesId) {	assert sesId != null;	lock.readLock();	try {	if (stopping && !waiting) {	return;	}	GridTaskWorker<?, ?> task = tasks.get(sesId);	if (task == null) {	
attempt to cancel unknown task was task already reduced 

========================= ignite sample_4696 =========================

final Set<String> backupNames = new HashSet<>();	for (Ignite node : backups) backupNames.add(node.name());	log.info("Check recovery [key=" + key + ", pessimistic=" + pessimistic + ", primary=" + primary.name() + ", backups=" + backupNames + ", node=" + ignite.name() + ']');	final IgniteCache<Integer, Integer> cache = ignite.cache(DEFAULT_CACHE_NAME);	cache.put(key, 0);	commitStartedLatch = new CountDownLatch(backupNames.size());	commitFinishLatch = new CountDownLatch(1);	commit = false;	TestEntryProcessor.skipFirst = useStore ? ignite.name() : null;	IgniteInternalFuture<?> fut = GridTestUtils.runAsync(new Callable<Void>() {	
start update 

cache.put(key, 0);	commitStartedLatch = new CountDownLatch(backupNames.size());	commitFinishLatch = new CountDownLatch(1);	commit = false;	TestEntryProcessor.skipFirst = useStore ? ignite.name() : null;	IgniteInternalFuture<?> fut = GridTestUtils.runAsync(new Callable<Void>() {	if (pessimistic) {	try (Transaction tx = ignite.transactions().txStart(PESSIMISTIC, REPEATABLE_READ)) {	cache.invoke(key, new TestEntryProcessor(backupNames));	commit = true;	
start commit 

cache.invoke(key, new TestEntryProcessor(backupNames));	commit = true;	assertEquals(backupNames.size(), commitStartedLatch.getCount());	tx.commit();	}	}	else {	commit = true;	cache.invoke(key, new TestEntryProcessor(backupNames));	}	
end update execute get 

commit = true;	assertEquals(backupNames.size(), commitStartedLatch.getCount());	tx.commit();	}	}	else {	commit = true;	cache.invoke(key, new TestEntryProcessor(backupNames));	}	Integer val = cache.get(key);	
get value 

========================= ignite sample_1178 =========================

failed = true;	break;	}	}	}	}	}	}	}	}	
awaitpartitionmapexchange finished 

sb.append("local part=");	if (part != null) sb.append(p).append(" state=").append(part.state());	else sb.append(p).append(" is null");	sb.append(" isAffNode=") .append(affNodes.contains(g0.localNode().id())) .append("\n");	for (UUID nodeId : F.nodeIds(g0.context().discovery().allNodes())) {	if (!nodeId.equals(g0.localNode().id())) sb.append(" nodeId=") .append(nodeId) .append(" part=") .append(p) .append(" state=") .append(top.partitionState(nodeId, p)) .append(" isAffNode=") .append(affNodes.contains(nodeId)) .append("\n");	}	}	sb.append("\n");	}	
dump partitions state for 

public void dumpCacheDebugInfo(Ignite ignite) {	GridKernalContext ctx = ((IgniteKernal)ignite).context();	log.error("Cache information update [node=" + ignite.name() + ", client=" + ignite.configuration().isClientMode() + ']');	GridCacheSharedContext cctx = ctx.cache().context();	
pending transactions 

public void dumpCacheDebugInfo(Ignite ignite) {	GridKernalContext ctx = ((IgniteKernal)ignite).context();	log.error("Cache information update [node=" + ignite.name() + ", client=" + ignite.configuration().isClientMode() + ']');	GridCacheSharedContext cctx = ctx.cache().context();	for (IgniteInternalTx tx : cctx.tm().activeTransactions()) log.error(">>> " + tx);	
pending explicit locks 

public void dumpCacheDebugInfo(Ignite ignite) {	GridKernalContext ctx = ((IgniteKernal)ignite).context();	log.error("Cache information update [node=" + ignite.name() + ", client=" + ignite.configuration().isClientMode() + ']');	GridCacheSharedContext cctx = ctx.cache().context();	for (IgniteInternalTx tx : cctx.tm().activeTransactions()) log.error(">>> " + tx);	for (GridCacheExplicitLockSpan lockSpan : cctx.mvcc().activeExplicitLocks()) log.error(">>> " + lockSpan);	
pending cache futures 

public void dumpCacheDebugInfo(Ignite ignite) {	GridKernalContext ctx = ((IgniteKernal)ignite).context();	log.error("Cache information update [node=" + ignite.name() + ", client=" + ignite.configuration().isClientMode() + ']');	GridCacheSharedContext cctx = ctx.cache().context();	for (IgniteInternalTx tx : cctx.tm().activeTransactions()) log.error(">>> " + tx);	for (GridCacheExplicitLockSpan lockSpan : cctx.mvcc().activeExplicitLocks()) log.error(">>> " + lockSpan);	for (GridCacheFuture<?> fut : cctx.mvcc().activeFutures()) log.error(">>> " + fut);	
pending atomic cache futures 

========================= ignite sample_2754 =========================

Integer pKey5 = keyForNode(aff, pKey, node0);	insert(personCache, pKey5, new Person("p5"));	insert(accCache, keyForNode(aff, accKey, node0), new Account(pKey5, "a-p5-1"));	insert(accCache, keyForNode(aff, accKey, node0), new Account(pKey5, "a-p5-1"));	Integer pKey6 = keyForNode(aff, pKey, node0);	insert(personCache, pKey6, new Person("p6"));	insert(accCache, keyForNode(aff, accKey, node1), new Account(pKey6, "a-p5-1"));	insert(accCache, keyForNode(aff, accKey, node1), new Account(pKey6, "a-p5-1"));	Integer[] keys = {pKey1, pKey2, pKey3, pKey4, pKey5, pKey6};	for (int i = 0; i < keys.length; i++) {	
test key 

private void checkQuery(String sql, IgniteCache<Object, Object> cache, boolean enforceJoinOrder, Object... args) throws Exception {	String plan = (String)cache.query(new SqlFieldsQuery("explain " + sql) .setArgs(args) .setDistributedJoins(true) .setEnforceJoinOrder(enforceJoinOrder)) .getAll().get(0).get(0);	
plan 

========================= ignite sample_7501 =========================

private void record0(Event evt, Object... params) {	assert evt != null;	if (!enterBusy()) return;	try {	int type = evt.type();	if (!isRecordable(type)) {	
trying to record event without checking if it is recordable 

if (!enterBusy()) return;	try {	int type = evt.type();	if (!isRecordable(type)) {	}	if ((isDaemon || isUserRecordable(type)) && !isHiddenEvent(type)) {	try {	getSpi().record(evt);	}	catch (IgniteSpiException e) {	
failed to record event 

public synchronized void disableEvents(int[] types) {	assert types != null;	ctx.security().authorize(null, SecurityPermission.EVENTS_DISABLE, null);	boolean[] userRecordableEvts0 = userRecordableEvts;	boolean[] recordableEvts0 = recordableEvts;	int[] inclEvtTypes0 = inclEvtTypes;	int[] userTypes = new int[types.length];	int userTypesLen = 0;	for (int type : types) {	if (binarySearch(cfgInclEvtTypes, type)) {	
can t disable event since it was enabled in configuration 

private void registerListener(ListenerWrapper lsnr, Integer type) {	Listeners lsnrs0 = lsnrs.get(type);	if (lsnrs0 == null) {	lsnrs0 = new Listeners();	Listeners prev = lsnrs.putIfAbsent(type, lsnrs0);	if (prev != null) lsnrs0 = prev;	}	lsnrs0.addListener(lsnr);	
added listener for disabled event type 

private void notifyListeners(@Nullable Collection<ListenerWrapper> lsnrs, Event evt, Object[] params) {	if (lsnrs == null || lsnrs.isEmpty()) return;	for (EventListener lsnr : lsnrs) {	try {	((ListenerWrapper)lsnr).onEvent(evt, params);	}	catch (Throwable e) {	
unexpected exception in listener notification for event 

private <T extends Event> List<T> query(IgnitePredicate<T> p, Collection<? extends ClusterNode> nodes, long timeout) throws IgniteCheckedException {	assert p != null;	assert nodes != null;	if (nodes.isEmpty()) {	
failed to query events for empty nodes collection 

========================= ignite sample_3542 =========================

public IgniteInternalFuture<GridNearTxPrepareResponse> prepareColocatedTx( final GridNearTxLocal locTx, final GridNearTxPrepareRequest req ) {	req.txState(locTx.txState());	IgniteInternalFuture<GridNearTxPrepareResponse> fut = locTx.prepareAsyncLocal(req);	if (locTx.isRollbackOnly()) locTx.rollbackNearTxLocalAsync();	return fut.chain(new C1<IgniteInternalFuture<GridNearTxPrepareResponse>, GridNearTxPrepareResponse>() {	try {	return f.get();	}	catch (Exception e) {	locTx.setRollbackOnly();	
failed to prepare dht transaction 

private IgniteInternalFuture<IgniteInternalTx> finishDhtLocal(UUID nodeId, @Nullable GridNearTxLocal locTx, @Nullable GridNearTxLocal locTx, GridNearTxFinishRequest req) {	GridCacheVersion dhtVer = ctx.tm().mappedVersion(req.version());	GridDhtTxLocal tx = null;	if (dhtVer == null) {	
received transaction finish request for unknown near version was lock explicit 

}	try {	assert tx != null : "Transaction is null for near finish request [nodeId=" + nodeId + ", req=" + req + "]";	assert req.syncMode() != null : req;	tx.syncMode(req.syncMode());	tx.nearFinishFutureId(req.futureId());	tx.nearFinishMiniId(req.miniId());	tx.storeEnabled(req.storeEnabled());	if (req.commit()) {	if (!tx.markFinalizing(USER_FINISH)) {	
will not finish transaction it is handled by another thread 

public IgniteInternalFuture<IgniteInternalTx> finishColocatedLocal(boolean commit, GridNearTxLocal tx) {	try {	if (commit) {	if (!tx.markFinalizing(USER_FINISH)) {	
will not finish transaction it is handled by another thread 

dhtTx.needReturnValue(req.needReturnValue());	finish(dhtTx, req);	}	if (nearTx != null) {	nearTx.onePhaseCommit(true);	finish(nearTx, req);	}	}	}	catch (IgniteCheckedException e) {	
transaction was rolled back before prepare completed 

finish(dhtTx, req);	}	if (nearTx != null) {	nearTx.onePhaseCommit(true);	finish(nearTx, req);	}	}	}	catch (IgniteCheckedException e) {	else if (e instanceof IgniteTxOptimisticCheckedException) {	
optimistic failure for remote transaction will rollback 

if (nearTx != null) {	nearTx.onePhaseCommit(true);	finish(nearTx, req);	}	}	}	catch (IgniteCheckedException e) {	else if (e instanceof IgniteTxOptimisticCheckedException) {	}	else if (e instanceof IgniteTxHeuristicCheckedException) {	
failed to commit transaction all transaction entries were invalidated 

nearTx.onePhaseCommit(true);	finish(nearTx, req);	}	}	}	catch (IgniteCheckedException e) {	else if (e instanceof IgniteTxOptimisticCheckedException) {	}	else if (e instanceof IgniteTxHeuristicCheckedException) {	}	
failed to process prepare request 

}	}	catch (Throwable e) {	U.error(log, "Failed completing transaction [commit=" + req.commit() + ", tx=" + tx + ']', e);	tx.invalidate(true);	tx.systemInvalidate(true);	try {	tx.commitRemoteTx();	}	catch (IgniteCheckedException ex) {	
failed to invalidate transaction 

}	catch (Throwable e) {	U.error(log, "Failed committing transaction [tx=" + tx + ']', e);	tx.invalidate(true);	tx.systemInvalidate(true);	try {	tx.rollbackRemoteTx();	}	catch (Throwable e1) {	e.addSuppressed(e1);	
failed to automatically rollback transaction 

========================= ignite sample_3954 =========================

try {	locHost = U.getLocalHost();	}	catch (IOException e) {	throw new IgniteCheckedException("Failed to initialize local address.", e);	}	try {	shmemSrv = resetShmemServer();	}	catch (IgniteCheckedException e) {	
failed to start shared memory communication server 

ch.configureBlocking(true);	ch.socket().setTcpNoDelay(tcpNoDelay);	ch.socket().setKeepAlive(true);	if (sockRcvBuf > 0) ch.socket().setReceiveBufferSize(sockRcvBuf);	if (sockSndBuf > 0) ch.socket().setSendBufferSize(sockSndBuf);	ch.socket().connect(new InetSocketAddress(addr, port), (int)connTimeout);	HandshakeFinish fin = new HandshakeFinish();	GridNioFuture<GridNioSession> sesFut = nioSrvr.createSession(ch, F.<Integer, Object>asMap(HANDSHAKE_FINISH_META, fin), false, null);	GridNioSession ses = sesFut.get();	client = new HadoopTcpNioCommunicationClient(ses);	
waiting for handshake finish for client 

continue;	}	break;	}	}	if (client == null) {	assert errs != null;	if (X.hasCause(errs, ConnectException.class)) LT.warn(log, "Failed to connect to a remote Hadoop process (is process still running?). " + "Make sure operating system firewall is disabled on local and remote host) " + "[addrs=" + addr + ", port=" + port + ']');	throw errs;	}	
created client 

========================= ignite sample_7279 =========================

clientIpFinder.setAddresses( Collections.singleton("localhost:" + srv0Node.discoveryPort()));	startClientNodes(1);	Ignite client = grid("client-0");	TcpDiscoveryNode clientNode = (TcpDiscoveryNode)client.cluster().localNode();	TestTcpDiscoverySpi clientSpi = (TestTcpDiscoverySpi)client.configuration().getDiscoverySpi();	UUID clientNodeId = clientNode.id();	checkNodes(2, 1);	clientIpFinder.setAddresses(Collections.singleton("localhost:" + srv1Node.discoveryPort()));	srvFailedLatch = new CountDownLatch(1);	attachListeners(2, 1);	
pausing router 

TestTcpDiscoverySpi clientSpi = ((TestTcpDiscoverySpi)client.configuration().getDiscoverySpi());	final CountDownLatch disconnectLatch = new CountDownLatch(1);	final CountDownLatch segmentedLatch = new CountDownLatch(1);	final AtomicBoolean err = new AtomicBoolean(false);	if (!failSrv) {	srvFailedLatch = new CountDownLatch(1);	attachListeners(1, 0);	}	client.events().localListen(new IgnitePredicate<Event>() {	if (evt.type() == EVT_CLIENT_NODE_DISCONNECTED) {	
disconnected event 

attachListeners(1, 0);	}	client.events().localListen(new IgnitePredicate<Event>() {	if (evt.type() == EVT_CLIENT_NODE_DISCONNECTED) {	assertEquals(1, segmentedLatch.getCount());	assertEquals(1, disconnectLatch.getCount());	assertFalse(err.get());	disconnectLatch.countDown();	}	else if (evt.type() == EVT_NODE_SEGMENTED) {	
segmented event 

assertFalse(err.get());	disconnectLatch.countDown();	}	else if (evt.type() == EVT_NODE_SEGMENTED) {	assertEquals(1, segmentedLatch.getCount());	assertEquals(0, disconnectLatch.getCount());	assertFalse(err.get());	segmentedLatch.countDown();	}	else {	
unexpected event 

joinTimeout = 60_000;	final CountDownLatch disconnectLatch = new CountDownLatch(1);	final CountDownLatch reconnectLatch = new CountDownLatch(1);	final AtomicBoolean err = new AtomicBoolean(false);	startServerNodes(1);	startClientNodes(1);	Ignite srv = G.ignite("server-0");	Ignite client = G.ignite("client-0");	client.events().localListen(new IgnitePredicate<Event>() {	if (evt.type() == EVT_CLIENT_NODE_DISCONNECTED) {	
disconnected event 

startClientNodes(1);	Ignite srv = G.ignite("server-0");	Ignite client = G.ignite("client-0");	client.events().localListen(new IgnitePredicate<Event>() {	if (evt.type() == EVT_CLIENT_NODE_DISCONNECTED) {	assertEquals(1, reconnectLatch.getCount());	assertEquals(1, disconnectLatch.getCount());	disconnectLatch.countDown();	}	else if (evt.type() == EVT_CLIENT_NODE_RECONNECTED) {	
reconnected event 

assertEquals(1, disconnectLatch.getCount());	disconnectLatch.countDown();	}	else if (evt.type() == EVT_CLIENT_NODE_RECONNECTED) {	assertEquals(1, reconnectLatch.getCount());	assertEquals(0, disconnectLatch.getCount());	assertFalse(err.get());	reconnectLatch.countDown();	}	else {	
unexpected event 

startClientNodes(1);	final Ignite srv = G.ignite("server-0");	Ignite client = G.ignite("client-0");	TestTcpDiscoverySpi srvSpi = ((TestTcpDiscoverySpi)srv.configuration().getDiscoverySpi());	TestTcpDiscoverySpi clientSpi = ((TestTcpDiscoverySpi)client.configuration().getDiscoverySpi());	final CountDownLatch disconnectLatch = new CountDownLatch(1);	final CountDownLatch reconnectLatch = new CountDownLatch(1);	final AtomicBoolean err = new AtomicBoolean(false);	client.events().localListen(new IgnitePredicate<Event>() {	if (evt.type() == EVT_CLIENT_NODE_DISCONNECTED) {	
disconnected event 

final CountDownLatch reconnectLatch = new CountDownLatch(1);	final AtomicBoolean err = new AtomicBoolean(false);	client.events().localListen(new IgnitePredicate<Event>() {	if (evt.type() == EVT_CLIENT_NODE_DISCONNECTED) {	assertEquals(1, reconnectLatch.getCount());	assertEquals(1, disconnectLatch.getCount());	assertFalse(err.get());	disconnectLatch.countDown();	}	else if (evt.type() == EVT_CLIENT_NODE_RECONNECTED) {	
reconnected event 

assertFalse(err.get());	disconnectLatch.countDown();	}	else if (evt.type() == EVT_CLIENT_NODE_RECONNECTED) {	assertEquals(1, reconnectLatch.getCount());	assertEquals(0, disconnectLatch.getCount());	assertFalse(err.get());	reconnectLatch.countDown();	}	else {	
unexpected event 

private boolean onMessage(Socket sock, TcpDiscoveryAbstractMessage msg) throws IOException {	boolean fail = false;	if (skipNodeAdded && (msg instanceof TcpDiscoveryNodeAddedMessage || msg instanceof TcpDiscoveryNodeAddFinishedMessage)) {	
skip message 

========================= ignite sample_2581 =========================

CacheMetricsImpl metrics = cache.metrics0();	CacheMetricsImpl.EntriesStatMetrics entriesStatMetrics = metrics.getEntriesStat();	long offHeapEntriesCnt = cache.offHeapEntriesCount();	long offHeapPrimaryEntriesCnt = cctx.offheap().cacheEntriesCount(cctx.cacheId(), true, false, cctx.affinity().affinityTopologyVersion());	long offHeapBackupEntriesCnt = cctx.offheap().cacheEntriesCount(cctx.cacheId(), false, true, cctx.affinity().affinityTopologyVersion());	long heapEntriesCnt = cache.localSizeLong(ONHEAP_PEEK_MODES);	int size = cache.size();	int keySize = size;	boolean isEmpty = cache.isEmpty();	String cacheInfo = "igniteIdx=" + igniteIdx + ", cacheIdx=" + cacheIdx + " ";	
checking cache 

========================= ignite sample_1529 =========================

assertTrue(ignite(clientGrid).configuration().isClientMode());	final IgniteTransactions txs = ignite(clientGrid).transactions();	final IgniteCache<Integer, Integer> cache = jcache(clientGrid);	updateCache(cache, txs);	final AtomicBoolean stop = new AtomicBoolean();	IgniteInternalFuture<?> updateFut = GridTestUtils.runAsync(new Callable<Void>() {	Thread.currentThread().setName("update-thread");	assertTrue(latch.await(30_000, TimeUnit.MILLISECONDS));	int iter = 0;	while (!stop.get()) {	
start update 

updateCache(cache, txs);	final AtomicBoolean stop = new AtomicBoolean();	IgniteInternalFuture<?> updateFut = GridTestUtils.runAsync(new Callable<Void>() {	Thread.currentThread().setName("update-thread");	assertTrue(latch.await(30_000, TimeUnit.MILLISECONDS));	int iter = 0;	while (!stop.get()) {	synchronized (mux) {	updateCache(cache, txs);	}	
end update 

final AtomicBoolean stop = new AtomicBoolean();	IgniteInternalFuture<?> updateFut = GridTestUtils.runAsync(new Callable<Void>() {	Thread.currentThread().setName("update-thread");	assertTrue(latch.await(30_000, TimeUnit.MILLISECONDS));	int iter = 0;	while (!stop.get()) {	synchronized (mux) {	updateCache(cache, txs);	}	}	
update iterations 

private void updateCache(IgniteCache<Integer, Integer> cache, IgniteTransactions txs) {	int val = expVal + 1;	try {	try (Transaction tx = txs.txStart(OPTIMISTIC, REPEATABLE_READ)) {	for (int i = 0; i < ENTRY_CNT; i++) cache.put(i, val);	tx.commit();	expVal = val;	
updated cache new value 

private void updateCache(IgniteCache<Integer, Integer> cache, IgniteTransactions txs) {	int val = expVal + 1;	try {	try (Transaction tx = txs.txStart(OPTIMISTIC, REPEATABLE_READ)) {	for (int i = 0; i < ENTRY_CNT; i++) cache.put(i, val);	tx.commit();	expVal = val;	}	}	catch (IgniteException e) {	
update failed 

========================= ignite sample_1196 =========================

public void testAnonymousBroadcast() throws Exception {	Ignite g = grid(0);	assert g.cluster().nodes().size() == NODES_CNT;	execCntr.set(0);	g.compute().broadcast(new CARemote() {	
broadcasting 

public void testAnonymousUnicast() throws Exception {	Ignite g = grid(0);	assert g.cluster().nodes().size() == NODES_CNT;	execCntr.set(0);	ClusterNode rmt = F.first(g.cluster().forRemotes().nodes());	compute(g.cluster().forNode(rmt)).run(new CARemote() {	
unicasting 

========================= ignite sample_788 =========================

long randomKey = locRandom.nextLong(MAX_KEY_COUNT);	TestEntity entity = TestEntity.newTestEntity(locRandom);	try (Transaction tx = g.transactions().txStart(PESSIMISTIC, REPEATABLE_READ)) {	cache.put(randomKey, entity);	tx.commit();	}	operationCnt.increment();	}	}	catch (Throwable e) {	
unexpected exception 

periods++;	if (periods > WARM_UP_PERIOD) {	totalOperations += sum;	max = Math.max(max, sum);	min = Math.min(min, sum);	log.info("Operation count: " + sum + " min=" + min + " max=" + max + " avg=" + totalOperations / (periods - WARM_UP_PERIOD));	}	}	interrupt.set(true);	threadPool.shutdown();	
test complete 

========================= ignite sample_1745 =========================

public void testEventListenerReconnect() throws Exception {	Ignite client = grid(serverCount());	assertTrue(client.cluster().localNode().isClient());	Ignite srv = clientRouter(client);	TestTcpDiscoverySpi srvSpi = spi(srv);	EventListener lsnr = new EventListener();	UUID opId = client.events().remoteListen(lsnr, null, EventType.EVT_JOB_STARTED);	lsnr.latch = new CountDownLatch(1);	
created remote listener 

public void testCacheContinuousQueryReconnect() throws Exception {	Ignite client = grid(serverCount());	assertTrue(client.cluster().localNode().isClient());	IgniteCache<Object, Object> clientCache = client.getOrCreateCache(new CacheConfiguration<>(DEFAULT_CACHE_NAME));	CacheEventListener lsnr = new CacheEventListener();	ContinuousQuery<Object, Object> qry = new ContinuousQuery<>();	qry.setAutoUnsubscribe(true);	qry.setLocalListener(lsnr);	QueryCursor<?> cur = clientCache.query(qry);	for (int i = 0; i < 5; i++) {	
iteration 

assertTrue(client.cluster().localNode().isClient());	IgniteCache<Object, Object> clientCache = client.getOrCreateCache(new CacheConfiguration<>(DEFAULT_CACHE_NAME));	CacheEventListener lsnr = new CacheEventListener();	ContinuousQuery<Object, Object> qry = new ContinuousQuery<>();	qry.setAutoUnsubscribe(true);	qry.setLocalListener(lsnr);	QueryCursor<?> cur = clientCache.query(qry);	for (int i = 0; i < 5; i++) {	continuousQueryReconnect(client, clientCache, lsnr);	}	
close cursor should not get cache events anymore 

========================= ignite sample_2313 =========================

return task.reduce(results);	}	});	}	finally {	synchronized (mux) {	assert state == State.REDUCING : "Invalid task state: " + state;	state = State.REDUCED;	}	}	
reduced job responses reduceRes ses 

private void sendFailoverRequest(GridJobResultImpl jobRes) {	evtLsnr.onJobFailover(this, jobRes.getSibling(), jobRes.getNode().id());	long timeout = ses.getEndTime() - U.currentTimeMillis();	if (timeout > 0) {	recordJobEvent(EVT_JOB_FAILED_OVER, jobRes.getJobContext().getJobId(), jobRes.getNode(), "Job failed over.");	sendRequest(jobRes);	}	
failed to fail over job due to task timeout 

}	for (GridJobResultImpl res : doomed) {	UUID nodeId = res.getNode().id();	if (nodeId.equals(ctx.localNodeId())) ctx.job().cancelJob(ses.getId(), res.getJobContext().getJobId(), /*courtesy*/true);	else {	try {	ClusterNode node = ctx.discovery().node(nodeId);	if (node != null) ctx.io().sendToGridTopic(node, TOPIC_JOB_CANCEL, new GridJobCancelRequest(ses.getId(), res.getJobContext().getJobId(), /*courtesy*/true), PUBLIC_POOL);	}	catch (ClusterTopologyCheckedException e) {	
failed to send cancel request node failed 

else {	Byte ctxPlc = getThreadContext(TC_IO_POLICY);	if (ctxPlc != null) plc = ctxPlc;	else plc = PUBLIC_POOL;	}	ctx.io().sendToGridTopic(node, TOPIC_JOB, req, plc);	if (log.isDebugEnabled()) log.debug("Sent job request [req=" + req + ", node=" + node + ']');	}	if (!loc) ctx.resource().invokeAnnotated(dep, res.getJob(), ComputeJobAfterSend.class);	}	
job timed out prior to sending job execution request 

}	}	catch (IgniteCheckedException e) {	IgniteException fakeErr = null;	try {	boolean deadNode = e instanceof ClusterTopologyCheckedException || isDeadNode(res.getNode().id());	if (deadNode) {	U.warn(log, "Failed to send job request because remote node left grid (if failover is enabled, " + "will attempt fail-over to another node) [node=" + node + ", taskName=" + ses.getTaskName() + ", taskSesId=" + ses.getId() + ", jobSesId=" + res.getJobContext().getJobId() + ']');	fakeErr = new ClusterTopologyException("Failed to send job due to node failure: " + node, e);	}	
failed to send job request 

========================= ignite sample_4697 =========================

private void checkLoadCount(CacheConfiguration<Object, Object> ccfg) throws Exception {	storeMap.clear();	Ignite ignite0 = ignite(0);	ignite0.createCache(ccfg);	try {	int key = 0;	for (Ignite node : G.allGrids()) {	
test for node 

========================= ignite sample_1495 =========================

ContinuousQuery<Object, Object> qry = new ContinuousQuery<>();	qry.setLocalListener(lsnr);	qry.setRemoteFilterFactory(new FilterFactory());	Object keys[] = new Object[GRID_COUNT];	for (int i = 0; i < GRID_COUNT; ++i) {	keys[i] = primaryKey(grid(i).cache(DEFAULT_CACHE_NAME));	grid(0).cache(DEFAULT_CACHE_NAME).put(keys[i], -1);	}	try (QueryCursor<?> cur = grid(0).cache(DEFAULT_CACHE_NAME).query(qry)) {	for (int i = 0; i < KEYS_COUNT; i++) {	
put key 

========================= ignite sample_1929 =========================

protected final int partition(CacheObjectContext ctx, @Nullable GridCacheContext cctx, Object obj) {	try {	return cctx != null ? cctx.affinity().partition(obj, false) : ctx.kernalContext().affinity().partition0(ctx.cacheName(), obj, null);	}	catch (IgniteCheckedException e) {	
failed to get partition 

========================= ignite sample_4551 =========================

try (Transaction tx = ignite.transactions().txStart(PESSIMISTIC, REPEATABLE_READ, 1000, 0)) {	int key1 = threadNum == 0 ? 0 : 1;	log.info(">>> Performs put [node=" + ((IgniteKernal)ignite).localNode() + ", tx=" + tx + ", key=" + key1 + ", cache=" + cache1.getName() + ']');	cache1.put(key1, 0);	barrier.await();	int key2 = threadNum == 0 ? 1 : 0;	log.info(">>> Performs put [node=" + ((IgniteKernal)ignite).localNode() + ", tx=" + tx + ", key=" + key2 + ", cache=" + cache2.getName() + ']');	latch.countDown();	cache2.put(key2, 1);	tx.commit();	
commit done 

cache1.put(key1, 0);	barrier.await();	int key2 = threadNum == 0 ? 1 : 0;	log.info(">>> Performs put [node=" + ((IgniteKernal)ignite).localNode() + ", tx=" + tx + ", key=" + key2 + ", cache=" + cache2.getName() + ']');	latch.countDown();	cache2.put(key2, 1);	tx.commit();	}	catch (Throwable e) {	if (hasCause(e, TransactionTimeoutException.class) && hasCause(e, TransactionDeadlockException.class) ) {	
at least one stack trace should contain 

}	}	}	}, 2, "tx-thread");	latch.await();	Ignite client = grid(1);	try (Transaction tx = client.transactions().txStart(PESSIMISTIC, READ_COMMITTED, 500, 0)) {	clientCache0.put(0, 3);	clientCache0.put(1, 3);	tx.commit();	
commit done 

Ignite client = grid(1);	try (Transaction tx = client.transactions().txStart(PESSIMISTIC, READ_COMMITTED, 500, 0)) {	clientCache0.put(0, 3);	clientCache0.put(1, 3);	tx.commit();	}	catch (CacheException e) {	assertTrue(X.hasCause(e, TransactionTimeoutException.class));	}	catch (Throwable e) {	
unexpected exception occurred 

========================= ignite sample_1616 =========================

public void testConcurrentJoinAndActivate() throws Exception {	for (int iter = 0; iter < 3; iter++) {	
iteration 

========================= ignite sample_819 =========================

public void testStartStopWithUndefinedIgniteHome() throws Exception {	IgniteUtils.nullifyHomeDirectory();	String igniteHome = IgniteSystemProperties.getString(IGNITE_HOME);	assert igniteHome != null;	U.setIgniteHome(null);	String igniteHome0 = U.getIgniteHome();	assert igniteHome0 == null;	IgniteLogger log = new JavaLogger();	
test started 

public void testStartStopWithUndefinedIgniteHome() throws Exception {	IgniteUtils.nullifyHomeDirectory();	String igniteHome = IgniteSystemProperties.getString(IGNITE_HOME);	assert igniteHome != null;	U.setIgniteHome(null);	String igniteHome0 = U.getIgniteHome();	assert igniteHome0 == null;	IgniteLogger log = new JavaLogger();	
grid start stop test count 

========================= ignite sample_663 =========================

private static void clearContext( final SupplyContext sc, final IgniteLogger log) {	if (sc != null) {	final Iterator it = sc.entryIt;	if (it != null && it instanceof GridCloseableIterator && !((GridCloseableIterator)it).isClosed()) {	try {	((GridCloseableIterator)it).close();	}	catch (IgniteCheckedException e) {	
iterator close failed 

}	CacheDataRow row = iter.next();	GridCacheEntryInfo info = new GridCacheEntryInfo();	info.key(row.key());	info.expireTime(row.expireTime());	info.version(row.version());	info.value(row.value());	info.cacheId(row.cacheId());	if (preloadPred == null || preloadPred.apply(info)) s.addEntry0(part, info, grp.shared(), grp.cacheObjectContext());	else {	
rebalance predicate evaluated to false will not send cache entry 

sctx = null;	}	finally {	if (loc != null) loc.release();	}	}	reply(node, d, s, scId);	if (log.isDebugEnabled()) log.debug("Finished supplying rebalancing [cache=" + grp.cacheOrGroupName() + ", fromNode=" + node.id() + ", topology=" + d.topologyVersion() + ", updateSeq=" + d.updateSequence() + ", idx=" + idx + "]");	}	catch (IgniteCheckedException e) {	
failed to send partition supply message to node 

private boolean reply(ClusterNode n, GridDhtPartitionDemandMessage d, GridDhtPartitionSupplyMessage s, T3<UUID, Integer, AffinityTopologyVersion> scId) throws IgniteCheckedException {	try {	if (log.isDebugEnabled()) log.debug("Replying to partition demand [node=" + n.id() + ", demand=" + d + ", supply=" + s + ']');	grp.shared().io().sendOrderedMessage(n, d.topic(), s, grp.ioPolicy(), d.timeout());	if (grp.config().getRebalanceThrottle() > 0) U.sleep(grp.config().getRebalanceThrottle());	return true;	}	catch (ClusterTopologyCheckedException ignore) {	
failed to send partition supply message because node left grid 

public void dumpDebugInfo() {	synchronized (scMap) {	if (!scMap.isEmpty()) {	
rebalancing supplier reserved following partitions 

========================= ignite sample_3739 =========================

public void addEvent( int part, KeyCacheObject key, UUID evtNodeId, @Nullable IgniteUuid xid, @Nullable IgniteUuid xid, @Nullable Object lockId, @Nullable Object lockId, int type, @Nullable CacheObject newVal, @Nullable CacheObject newVal, boolean hasNewVal, @Nullable CacheObject oldVal, @Nullable CacheObject oldVal, boolean hasOldVal, UUID subjId, @Nullable String cloClsName, @Nullable String cloClsName, @Nullable String taskName, @Nullable String taskName, boolean keepBinary ) {	assert key != null || type == EVT_CACHE_STARTED || type == EVT_CACHE_STOPPED;	
added event without checking if event is recordable 

Object key0;	Object val0;	Object oldVal0;	try {	key0 = cctx.cacheObjectContext().unwrapBinaryIfNeeded(key, keepBinary, false);	val0 = cctx.cacheObjectContext().unwrapBinaryIfNeeded(newVal, keepBinary, false);	oldVal0 = cctx.cacheObjectContext().unwrapBinaryIfNeeded(oldVal, keepBinary, false);	}	catch (Exception e) {	if (!cctx.cacheObjectContext().kernalContext().cacheObjects().isBinaryEnabled(cctx.config())) throw e;	
failed to unmarshall cache object value for the event notification 

Object key0;	Object val0;	Object oldVal0;	try {	key0 = cctx.cacheObjectContext().unwrapBinaryIfNeeded(key, keepBinary, false);	val0 = cctx.cacheObjectContext().unwrapBinaryIfNeeded(newVal, keepBinary, false);	oldVal0 = cctx.cacheObjectContext().unwrapBinaryIfNeeded(oldVal, keepBinary, false);	}	catch (Exception e) {	if (!cctx.cacheObjectContext().kernalContext().cacheObjects().isBinaryEnabled(cctx.config())) throw e;	
failed to unmarshall cache object value for the event notification all further notifications will keep binary object format 

========================= ignite sample_3892 =========================

assertTrue(queue.isEmpty());	grid(0).compute().broadcast(new AddAllJob(queueName, RETRIES));	assertEquals(GRID_CNT * RETRIES, queue.size());	Collection<ClusterNode> nodes = grid(0).cluster().nodes();	for (ClusterNode node : nodes) {	Collection<Integer> queueElements = compute(grid(0).cluster().forNode(node)).call( new IgniteCallable<Collection<Integer>>() {	private Ignite grid;	Collection<Integer> values = new ArrayList<>();	grid.log().info("Running job [node=" + grid.cluster().localNode().id() + "]");	IgniteQueue<Integer> locQueue = grid.queue(queueName, 0, null);	
queue size 

grid(0).compute().broadcast(new AddAllJob(queueName, RETRIES));	assertEquals(GRID_CNT * RETRIES, queue.size());	Collection<ClusterNode> nodes = grid(0).cluster().nodes();	for (ClusterNode node : nodes) {	Collection<Integer> queueElements = compute(grid(0).cluster().forNode(node)).call( new IgniteCallable<Collection<Integer>>() {	private Ignite grid;	Collection<Integer> values = new ArrayList<>();	grid.log().info("Running job [node=" + grid.cluster().localNode().id() + "]");	IgniteQueue<Integer> locQueue = grid.queue(queueName, 0, null);	for (Integer element : locQueue) values.add(element);	
returning 

========================= ignite sample_927 =========================

throw new AssertionError(e);	}	}	}, 5000);	stopGrid(1);	assert waitCacheEmpty(cache2, 10000);	for (int i = 0; i < 3; i++) {	long swapSize = size(cacheName, grid2);	if (swapSize > 0) {	if (i < 2) {	
swap size check failed will retry in ms 

========================= ignite sample_1513 =========================

public void start(Map<Class<?>, Object> beans) {	log = (IgniteLogger)beans.get(IgniteLogger.class);	if (log == null) {	
failed to find logger definition in application context stopping the router 

return;	}	GridTcpRouterConfiguration tcpCfg = (GridTcpRouterConfiguration)beans.get(GridTcpRouterConfiguration.class);	if (tcpCfg == null) U.warn(log, "TCP router startup skipped (configuration not found).");	else {	tcpRouter = new GridTcpRouterImpl(tcpCfg);	try {	tcpRouter.start();	}	catch (Exception e) {	
failed to start tcp router on port 

public void stop() {	if (tcpRouter != null) {	try {	tcpRouter.stop();	}	catch (Exception e) {	
error while stopping the router 

========================= ignite sample_5229 =========================

private void deleteFromResourceBase(String fileName) {	File file = new File(rsrcBase + '/' + fileName);	
could not delete file 

========================= ignite sample_7886 =========================

private void checkCompletion() {	for (Iterator<ComputeTaskFuture<Integer>> iter = futures.iterator(); iter.hasNext();) {	ComputeTaskFuture<Integer> fut = iter.next();	if (fut.isDone()) {	try {	Integer res = fut.get();	assert res == params.getJobsCount() : "Task returned wrong result [taskIs=" + fut.getTaskSession().getId() + ", result=" + res + "]";	
task completed successfully task id 

private void checkCompletion() {	for (Iterator<ComputeTaskFuture<Integer>> iter = futures.iterator(); iter.hasNext();) {	ComputeTaskFuture<Integer> fut = iter.next();	if (fut.isDone()) {	try {	Integer res = fut.get();	assert res == params.getJobsCount() : "Task returned wrong result [taskIs=" + fut.getTaskSession().getId() + ", result=" + res + "]";	}	catch (IgniteFutureCancelledException ignored) {	
task cancelled 

for (Iterator<ComputeTaskFuture<Integer>> iter = futures.iterator(); iter.hasNext();) {	ComputeTaskFuture<Integer> fut = iter.next();	if (fut.isDone()) {	try {	Integer res = fut.get();	assert res == params.getJobsCount() : "Task returned wrong result [taskIs=" + fut.getTaskSession().getId() + ", result=" + res + "]";	}	catch (IgniteFutureCancelledException ignored) {	}	catch (IgniteException e) {	
get operation for completed task failed 

private void performCancel() {	iteration++;	if (iteration % cancelRate == 0) {	ComputeTaskFuture<Integer> futToCancel = futures.get( new Random().nextInt(futures.size()) );	try {	futToCancel.cancel();	
task canceled 

private void performCancel() {	iteration++;	if (iteration % cancelRate == 0) {	ComputeTaskFuture<Integer> futToCancel = futures.get( new Random().nextInt(futures.size()) );	try {	futToCancel.cancel();	}	catch (IgniteException e) {	
future cancellation failed 

========================= ignite sample_2387 =========================

private void testNonDeadLockInListener(CacheConfiguration ccfg, final boolean asyncFltr, boolean asyncLsnr, boolean jcacheApi) throws Exception {	ignite(0).createCache(ccfg);	ThreadLocalRandom rnd = ThreadLocalRandom.current();	try {	for (int i = 0; i < ITERATION_CNT; i++) {	
start iteration 

else if (!val.equals(val0)) return;	Transaction tx = null;	try {	if (cache0.getConfiguration(CacheConfiguration.class).getAtomicityMode() == TRANSACTIONAL) tx = ignite.transactions().txStart(PESSIMISTIC, REPEATABLE_READ);	assertEquals(val, val0);	cache0.put(key, newVal);	if (tx != null) tx.commit();	latch.countDown();	}	catch (Exception exp) {	
failed 

});	}	assertTrue("Failed to waiting event.", U.await(latch, 3, SECONDS));	assertEquals(cache.get(key), new QueryTestValue(2));	assertTrue("Failed to waiting event from listener.", U.await(latch, 3, SECONDS));	}	finally {	if (qry != null) qry.close();	if (lsnrCfg != null) cache.deregisterCacheEntryListener(lsnrCfg);	}	
iteration finished 

private void testNonDeadLockInFilter(CacheConfiguration ccfg, final boolean asyncFilter, final boolean asyncLsnr, boolean jcacheApi) throws Exception {	ignite(0).createCache(ccfg);	ThreadLocalRandom rnd = ThreadLocalRandom.current();	try {	for (int i = 0; i < ITERATION_CNT; i++) {	
start iteration 

else if (!val.equals(val0)) return;	Transaction tx = null;	try {	if (cache0.getConfiguration(CacheConfiguration.class) .getAtomicityMode() == TRANSACTIONAL) tx = ignite.transactions().txStart(PESSIMISTIC, REPEATABLE_READ);	assertEquals(val, val0);	cache0.put(key, newVal);	if (tx != null) tx.commit();	latch.countDown();	}	catch (Exception exp) {	
failed 

}	});	assert U.await(latch, 3, SECONDS) : "Failed to waiting event.";	assertEquals(cache.get(key), new QueryTestValue(2));	assertTrue("Failed to waiting event from filter.", U.await(latch, 3, SECONDS));	}	finally {	if (qry != null) qry.close();	if (lsnrCfg != null) cache.deregisterCacheEntryListener(lsnrCfg);	}	
iteration finished 

========================= ignite sample_1952 =========================

private void checkException(IgniteException e, Class<? extends Exception> exCls) {	for (Throwable t = e; t.getCause() != null; t = t.getCause()) {	if (t.getCause().getClass().isAssignableFrom(exCls)) {	
expected exception 

========================= ignite sample_7490 =========================

public void setCollisionExternalListener(@Nullable CollisionExternalListener lsnr) {	if (enabled()) {	if (lsnr != null && !extLsnr.compareAndSet(null, lsnr)) assert false : "Collision external listener has already been set " + "(perhaps need to add support for multiple listeners)";	
successfully set external collision listener 

========================= ignite sample_3579 =========================

client = true;	Ignite ignite = startGrid(GRID_CNT);	assertTrue(ignite.configuration().isClientMode());	client = false;	final AtomicBoolean finished = new AtomicBoolean();	IgniteInternalFuture<?> fut = restartThread(finished);	long stop = System.currentTimeMillis() + 60_000;	try {	int iter = 0;	while (System.currentTimeMillis() < stop) {	
iteration 

private IgniteInternalFuture<?> restartThread(final AtomicBoolean finished) {	return GridTestUtils.runAsync(new Callable<Object>() {	while (!finished.get()) {	for (int i = 0; i < GRID_CNT; i++) {	
stop node 

private IgniteInternalFuture<?> restartThread(final AtomicBoolean finished) {	return GridTestUtils.runAsync(new Callable<Object>() {	while (!finished.get()) {	for (int i = 0; i < GRID_CNT; i++) {	stopGrid(i);	U.sleep(500);	
start node 

========================= ignite sample_1114 =========================

public void restoreStructuresState(GridKernalContext ctx) {	onKernalStart0();	try {	for (GridCacheRemovable v : dsMap.values()) {	if (v instanceof IgniteChangeGlobalStateSupport) ((IgniteChangeGlobalStateSupport)v).onActivate(ctx);	}	}	catch (IgniteCheckedException e) {	
failed restore data structures state 

========================= ignite sample_4316 =========================

ignite(0).createCache(ccfg);	ThreadLocalRandom rnd = ThreadLocalRandom.current();	QueryCursor<?> cur = null;	final Class<Factory<CacheEntryEventFilter>> evtFilterFactory = (Class<Factory<CacheEntryEventFilter>>)getExternalClassLoader(). loadClass("org.apache.ignite.tests.p2p.CacheDeploymentEntryEventFilterFactory");	final CountDownLatch latch = new CountDownLatch(10);	ContinuousQuery<Integer, Integer> qry = new ContinuousQuery<>();	TestLocalListener localLsnr = new TestLocalListener() {	throws CacheEntryListenerException {	for (CacheEntryEvent<? extends Integer, ? extends Integer> evt : evts) {	latch.countDown();	
received event 

========================= ignite sample_1944 =========================

public void testThrottle() throws Exception {	LT.throttleTimeout(1000);	
test exception Test 

public void testThrottle() throws Exception {	LT.throttleTimeout(1000);	
test exception Test 

public void testThrottle() throws Exception {	LT.throttleTimeout(1000);	
test exception Test1 

public void testThrottle() throws Exception {	LT.throttleTimeout(1000);	
test exception Test 

public void testThrottle() throws Exception {	LT.throttleTimeout(1000);	
test without throwable 

public void testThrottle() throws Exception {	LT.throttleTimeout(1000);	
test without throwable 

public void testThrottle() throws Exception {	LT.throttleTimeout(1000);	
test without throwable 

public void testThrottle() throws Exception {	LT.throttleTimeout(1000);	
test without 

public void testThrottle() throws Exception {	LT.throttleTimeout(1000);	
test without 

public void testThrottle() throws Exception {	LT.throttleTimeout(1000);	Thread.sleep(LT.throttleTimeout());	info("Slept for throttle timeout: " + LT.throttleTimeout());	
test exception Test 

public void testThrottle() throws Exception {	LT.throttleTimeout(1000);	Thread.sleep(LT.throttleTimeout());	info("Slept for throttle timeout: " + LT.throttleTimeout());	
test exception Test 

public void testThrottle() throws Exception {	LT.throttleTimeout(1000);	Thread.sleep(LT.throttleTimeout());	info("Slept for throttle timeout: " + LT.throttleTimeout());	
test exception Test1 

public void testThrottle() throws Exception {	LT.throttleTimeout(1000);	Thread.sleep(LT.throttleTimeout());	info("Slept for throttle timeout: " + LT.throttleTimeout());	
test exception Test 

public void testThrottle() throws Exception {	LT.throttleTimeout(1000);	Thread.sleep(LT.throttleTimeout());	info("Slept for throttle timeout: " + LT.throttleTimeout());	
test without throwable 

public void testThrottle() throws Exception {	LT.throttleTimeout(1000);	Thread.sleep(LT.throttleTimeout());	info("Slept for throttle timeout: " + LT.throttleTimeout());	
test without throwable 

public void testThrottle() throws Exception {	LT.throttleTimeout(1000);	Thread.sleep(LT.throttleTimeout());	info("Slept for throttle timeout: " + LT.throttleTimeout());	Thread.sleep(LT.throttleTimeout());	info("Slept for throttle timeout: " + LT.throttleTimeout());	
test info message 

public void testThrottle() throws Exception {	LT.throttleTimeout(1000);	Thread.sleep(LT.throttleTimeout());	info("Slept for throttle timeout: " + LT.throttleTimeout());	Thread.sleep(LT.throttleTimeout());	info("Slept for throttle timeout: " + LT.throttleTimeout());	
test info message 

public void testThrottle() throws Exception {	LT.throttleTimeout(1000);	Thread.sleep(LT.throttleTimeout());	info("Slept for throttle timeout: " + LT.throttleTimeout());	Thread.sleep(LT.throttleTimeout());	info("Slept for throttle timeout: " + LT.throttleTimeout());	
test info message 

public void testThrottle() throws Exception {	LT.throttleTimeout(1000);	Thread.sleep(LT.throttleTimeout());	info("Slept for throttle timeout: " + LT.throttleTimeout());	Thread.sleep(LT.throttleTimeout());	info("Slept for throttle timeout: " + LT.throttleTimeout());	
test info message 

public void testThrottle() throws Exception {	LT.throttleTimeout(1000);	Thread.sleep(LT.throttleTimeout());	info("Slept for throttle timeout: " + LT.throttleTimeout());	Thread.sleep(LT.throttleTimeout());	info("Slept for throttle timeout: " + LT.throttleTimeout());	
test info message 

public void testThrottle() throws Exception {	LT.throttleTimeout(1000);	Thread.sleep(LT.throttleTimeout());	info("Slept for throttle timeout: " + LT.throttleTimeout());	Thread.sleep(LT.throttleTimeout());	info("Slept for throttle timeout: " + LT.throttleTimeout());	
test info message 

========================= ignite sample_570 =========================

public GridTaskCommandHandler(final GridKernalContext ctx) {	super(ctx);	ctx.io().addMessageListener(TOPIC_REST, new GridMessageListener() {	if (!(msg instanceof GridTaskResultRequest)) {	
received unexpected message instead of task result request 

else {	res.result(desc.result());	res.resultBytes(U.marshal(ctx, desc.result()));	}	}	else res.found(false);	Object topic = U.unmarshal(ctx, req.topicBytes(), U.resolveClassLoader(ctx.config()));	ctx.io().sendToCustomTopic(nodeId, topic, res, SYSTEM_POOL);	}	catch (IgniteCheckedException e) {	
failed to send job task result response 

private IgniteInternalFuture<GridRestResponse> handleAsyncUnsafe(final GridRestRequest req) throws IgniteCheckedException {	assert req instanceof GridRestTaskRequest : "Invalid command for topology handler: " + req;	assert SUPPORTED_COMMANDS.contains(req.command());	
handling task rest request 

GridMessageListener msgLsnr = new GridMessageListener() {	String err = null;	GridTaskResultResponse res = null;	if (!(msg instanceof GridTaskResultResponse)) err = "Received unexpected message: " + msg;	else if (!nodeId.equals(resHolderId)) err = "Received task result response from unexpected node [resHolderId=" + resHolderId + ", nodeId=" + nodeId + ']';	else res = (GridTaskResultResponse)msg;	try {	res.result(U.unmarshal(ctx, res.resultBytes(), U.resolveClassLoader(ctx.config())));	}	catch (IgniteCheckedException e) {	
failed to unmarshal task result 

========================= ignite sample_5004 =========================

private void testRebalance(final Operation operation) throws Exception {	long stopTime = System.currentTimeMillis() + 2 * 60_000;	for (int iter = 0; iter < TEST_ITERATIONS && System.currentTimeMillis() < stopTime; iter++) {	
iteration 

========================= ignite sample_1801 =========================

protected void runFieldsQuery(GridCacheQueryInfo qryInfo) {	assert qryInfo != null;	if (!enterBusy()) {	if (cctx.localNodeId().equals(qryInfo.senderId())) throw new IllegalStateException("Failed to process query request (grid is stopping).");	return;	}	try {	
running query 

protected void runQuery(GridCacheQueryInfo qryInfo) {	assert qryInfo != null;	assert qryInfo.query().type() != SCAN || !qryInfo.local() : qryInfo;	if (!enterBusy()) {	if (cctx.localNodeId().equals(qryInfo.senderId())) throw new IllegalStateException("Failed to process query request (grid is stopping).");	return;	}	try {	boolean loc = qryInfo.local();	QueryResult<K, V> res = null;	
running query 

if (!cctx.isReplicated() && qry.type() == SCAN && qry.partition() == null && cctx.config().getCacheMode() != LOCAL && !incBackups && !cctx.affinity().primaryByKey(cctx.localNode(), key, topVer)) {	if (log.isDebugEnabled()) log.debug("Ignoring backup element [row=" + row + ", cacheMode=" + cctx.config().getCacheMode() + ", incBackups=" + incBackups + ", primary=" + cctx.affinity().primaryByKey(cctx.localNode(), key, topVer) + ']');	continue;	}	V val = row.getValue();	if (log.isDebugEnabled()) {	ClusterNode primaryNode = cctx.affinity().primaryByKey(key, cctx.affinity().affinityTopologyVersion());	log.debug(S.toString("Record", "key", key, true, "val", val, true, "incBackups", incBackups, false, "priNode", primaryNode != null ? U.id8(primaryNode.id()) : null, false, "node", U.id8(cctx.localNode().id()), false));	}	if (val == null) {	
unsuitable record value val 

}	}	}	if (!pageSent) {	if (rdc == null) onPageReady(loc, qryInfo, data, true, null);	else onPageReady(loc, qryInfo, Collections.singletonList(rdc.reduce()), true, null);	}	}	catch (Throwable e) {	if (X.hasCause(e, ClassNotFoundException.class) && !qry.keepBinary() && cctx.binaryMarshaller() && !cctx.localNode().isClient() && !log.isQuiet()) {	
suggestion for the cause of classnotfoundexception 

}	}	if (!pageSent) {	if (rdc == null) onPageReady(loc, qryInfo, data, true, null);	else onPageReady(loc, qryInfo, Collections.singletonList(rdc.reduce()), true, null);	}	}	catch (Throwable e) {	if (X.hasCause(e, ClassNotFoundException.class) && !qry.keepBinary() && cctx.binaryMarshaller() && !cctx.localNode().isClient() && !log.isQuiet()) {	LT.warn(log, "To disable, set -D" + IGNITE_QUIET + "=true");	
ignite configured to use binarymarshaller but keepbinary is false for request 

}	}	if (!pageSent) {	if (rdc == null) onPageReady(loc, qryInfo, data, true, null);	else onPageReady(loc, qryInfo, Collections.singletonList(rdc.reduce()), true, null);	}	}	catch (Throwable e) {	if (X.hasCause(e, ClassNotFoundException.class) && !qry.keepBinary() && cctx.binaryMarshaller() && !cctx.localNode().isClient() && !log.isQuiet()) {	LT.warn(log, "To disable, set -D" + IGNITE_QUIET + "=true");	
server node need to load definition of data classes it can be reason of classnotfoundexception consider ignitecache withkeepbinary to fix 

}	}	if (!pageSent) {	if (rdc == null) onPageReady(loc, qryInfo, data, true, null);	else onPageReady(loc, qryInfo, Collections.singletonList(rdc.reduce()), true, null);	}	}	catch (Throwable e) {	if (X.hasCause(e, ClassNotFoundException.class) && !qry.keepBinary() && cctx.binaryMarshaller() && !cctx.localNode().isClient() && !log.isQuiet()) {	LT.warn(log, "To disable, set -D" + IGNITE_QUIET + "=true");	
refer this page for detailed information https 

protected GridCloseableIterator scanQueryLocal(final GridCacheQueryAdapter qry, boolean updateStatistics) throws IgniteCheckedException {	if (!enterBusy()) throw new IllegalStateException("Failed to process query request (grid is stopping).");	final boolean statsEnabled = cctx.statisticsEnabled();	updateStatistics &= statsEnabled;	long startTime = U.currentTimeMillis();	final String namex = cctx.name();	final IgniteBiPredicate<K, V> scanFilter = qry.scanFilter();	try {	assert qry.type() == SCAN;	
running local scan query 

if (futs != null) {	IgniteInternalFuture<QueryResult<K, V>> fut;	synchronized (futs) {	fut = futs.remove(reqId);	}	if (fut != null) {	try {	fut.get().closeIfNotShared(recipient(sndId, reqId));	}	catch (IgniteCheckedException e) {	
failed to close iterator 

Map<Long, GridFutureAdapter<FieldsResult>> iters = fieldsQryRes.get(sndId);	if (iters == null) {	iters = new LinkedHashMap<Long, GridFutureAdapter<FieldsResult>>(16, 0.75f, true) {	GridFutureAdapter<FieldsResult>> e) {	boolean rmv = size() > maxIterCnt;	if (rmv) {	try {	e.getValue().get().closeIfNotShared(recipient(sndId, e.getKey()));	}	catch (IgniteCheckedException ex) {	
failed to close fields query iterator 

IgniteInternalFuture<FieldsResult> fut;	synchronized (futs) {	fut = futs.remove(reqId);	}	if (fut != null) {	assert fut.isDone();	try {	fut.get().closeIfNotShared(recipient(sndId, reqId));	}	catch (IgniteCheckedException e) {	
failed to close iterator 

val = entry.peek(true, true, topVer, expiryPlc);	cctx.evicts().touch(entry, topVer);	break;	}	catch (GridCacheEntryRemovedException ignore) {	tmp = null;	}	}	}	catch (IgniteCheckedException e) {	
failed to peek value 

========================= ignite sample_4183 =========================

public void onMessage(UUID nodeId, Object msg) {	try {	assert msg != null;	ClusterNode node = ctx.discovery().node(nodeId);	if (node == null) return;	boolean processed = true;	if (msg instanceof GridQueryNextPageResponse) onNextPage(node, (GridQueryNextPageResponse)msg);	else if (msg instanceof GridQueryFailResponse) onFail(node, (GridQueryFailResponse)msg);	else if (msg instanceof GridH2DmlResponse) onDmlResponse(node, (GridH2DmlResponse)msg);	else processed = false;	
processed response 

assert msg != null;	ClusterNode node = ctx.discovery().node(nodeId);	if (node == null) return;	boolean processed = true;	if (msg instanceof GridQueryNextPageResponse) onNextPage(node, (GridQueryNextPageResponse)msg);	else if (msg instanceof GridQueryFailResponse) onFail(node, (GridQueryFailResponse)msg);	else if (msg instanceof GridH2DmlResponse) onDmlResponse(node, (GridH2DmlResponse)msg);	else processed = false;	}	catch(Throwable th) {	
failed to process message 

public void releaseRemoteResources(Collection<ClusterNode> nodes, ReduceQueryRun r, long qryReqId, boolean distributedJoins) {	if (distributedJoins) send(nodes, new GridQueryCancelRequest(qryReqId), null, false);	else {	for (GridMergeIndex idx : r.indexes()) {	if (!idx.fetchedAll()) {	send(nodes, new GridQueryCancelRequest(qryReqId), null, false);	break;	}	}	}	
query run was already removed 

========================= ignite sample_7740 =========================

private void notifyCallback0(UUID nodeId, final GridKernalContext ctx, Collection<CacheContinuousQueryEntry> entries) {	final GridCacheContext cctx = cacheContext(ctx);	if (cctx == null) {	IgniteLogger log = ctx.log(CU.CONTINUOUS_QRY_LOG_CATEGORY);	
failed to notify callback cache is not found 

depMgr.p2pContext( nodeId, depInfo.classLoaderId(), depInfo.userVersion(), depInfo.deployMode(), depInfo.participants() );	}	}	try {	e.unmarshal(cctx, ldr);	Collection<CacheEntryEvent<? extends K, ? extends V>> evts = handleEvent(ctx, e);	if (evts != null && !evts.isEmpty()) entries0.addAll(evts);	}	catch (IgniteCheckedException ex) {	if (ignoreClsNotFound) assert internal;	
failed to unmarshal entry 

public boolean filter(CacheContinuousQueryEvent evt) {	CacheContinuousQueryEntry entry = evt.entry();	boolean notify = !entry.isFiltered();	try {	if (notify && getEventFilter() != null) notify = getEventFilter().evaluate(evt);	}	catch (Exception e) {	
cacheentryeventfilter failed 

if (!internal && !skipPrimaryCheck) sendBackupAcknowledge(ackBuf.onAcknowledged(entry), routineId, ctx);	}	else {	if (!entry.isFiltered()) locLsnr.onUpdated(F.<CacheEntryEvent<? extends K, ? extends V>>asList(evt));	}	}	else {	if (!entry.isFiltered()) prepareEntry(cctx, nodeId, entry);	Object entryOrList = handleEntry(cctx, entry);	if (entryOrList != null) {	
send the following event to listener 

Object entryOrList = handleEntry(cctx, entry);	if (entryOrList != null) {	ctx.continuous().addNotification(nodeId, routineId, entryOrList, topic, sync, true);	}	}	}	catch (ClusterTopologyCheckedException ex) {	if (log.isDebugEnabled()) log.debug("Failed to send event notification to node, node left cluster " + "[node=" + nodeId + ", err=" + ex + ']');	}	catch (IgniteCheckedException ex) {	
failed to send event notification to node 

========================= ignite sample_4212 =========================

public void testBuffer2() throws Exception {	for (int i = 0; i < 10; i++) {	
iteration 

private void testBuffer(int threads) throws Exception {	long seed = System.nanoTime();	Random rnd = new Random(seed);	
start test seed 

========================= ignite sample_1913 =========================

IgniteCache<Integer, Integer> cache = ignite(0).cache(CACHE_NAME);	Random rnd = new Random();	int iter = 0;	while (!stop.get()) {	Map<Integer, Integer> map = new HashMap<>();	for (int i = 0; i < 10; i++) map.put(rnd.nextInt(1000), i);	try {	cache.putAll(map);	}	catch (CacheException e) {	
update failed 

Thread.currentThread().setName("put-thread");	Random rnd = new Random();	long endTime = System.currentTimeMillis() + 60_000;	try {	int iter = 0;	while (System.currentTimeMillis() < endTime) {	Map<Integer, Integer> map = new HashMap<>();	for (int i = 0; i < 10; i++) map.put(rnd.nextInt(1000), i);	cache.putAll(map);	iter++;	
iteration 

int iter = 0;	while (System.currentTimeMillis() < endTime) {	Map<Integer, Integer> map = new HashMap<>();	for (int i = 0; i < 10; i++) map.put(rnd.nextInt(1000), i);	cache.putAll(map);	iter++;	}	fail("Should fail.");	}	catch (CacheException | IllegalStateException e) {	
expected error 

========================= ignite sample_1505 =========================

public void testDeployServiceWithSpring() throws Exception {	generateConfigXmls(NODES);	for (int i = 0; i < TEST_ITERATIONS; ++i) {	
iteration 

========================= ignite sample_7982 =========================

private boolean enterBusy() {	if (busyLock.readLock().tryLock()) return true;	
failed to enter busy state exchanger is stopping 

catch (IgniteInterruptedCheckedException e) {	onDone(e);	throw e;	}	catch (IgniteNeedReconnectException e) {	onDone(e);	}	catch (Throwable e) {	if (reconnectOnError(e)) onDone(new IgniteNeedReconnectException(cctx.localNode(), e));	else {	
failed to reinitialize local partitions preloading will be stopped 

try {	long start = U.currentTimeMillis();	IgniteInternalFuture fut = cctx.snapshot().tryStartLocalSnapshotOperation(firstDiscoEvt);	if (fut != null) {	fut.get();	long end = U.currentTimeMillis();	if (log.isInfoEnabled()) log.info("Snapshot initialization completed [topVer=" + exchangeId().topologyVersion() + ", time=" + (end - start) + "ms]");	}	}	catch (IgniteCheckedException e) {	
error while starting snapshot operation 

private void waitPartitionRelease() throws IgniteCheckedException {	IgniteInternalFuture<?> partReleaseFut = cctx.partitionReleaseFuture(initialVersion());	this.partReleaseFut = partReleaseFut;	if (exchId.isLeft()) cctx.mvcc().removeExplicitNodeLocks(exchId.nodeId(), exchId.topologyVersion());	
before waiting for partition release future 

IgniteInternalFuture<?> locksFut = cctx.mvcc().finishLocks(exchId.topologyVersion());	nextDumpTime = 0;	dumpCnt = 0;	while (true) {	try {	locksFut.get(futTimeout, TimeUnit.MILLISECONDS);	break;	}	catch (IgniteFutureTimeoutCheckedException ignored) {	if (nextDumpTime <= U.currentTimeMillis()) {	
failed to wait for locks release future dumping pending objects that might be the cause 

IgniteInternalFuture<?> locksFut = cctx.mvcc().finishLocks(exchId.topologyVersion());	nextDumpTime = 0;	dumpCnt = 0;	while (true) {	try {	locksFut.get(futTimeout, TimeUnit.MILLISECONDS);	break;	}	catch (IgniteFutureTimeoutCheckedException ignored) {	if (nextDumpTime <= U.currentTimeMillis()) {	
locked keys 

IgniteInternalFuture<?> locksFut = cctx.mvcc().finishLocks(exchId.topologyVersion());	nextDumpTime = 0;	dumpCnt = 0;	while (true) {	try {	locksFut.get(futTimeout, TimeUnit.MILLISECONDS);	break;	}	catch (IgniteFutureTimeoutCheckedException ignored) {	if (nextDumpTime <= U.currentTimeMillis()) {	
locked key 

IgniteInternalFuture<?> locksFut = cctx.mvcc().finishLocks(exchId.topologyVersion());	nextDumpTime = 0;	dumpCnt = 0;	while (true) {	try {	locksFut.get(futTimeout, TimeUnit.MILLISECONDS);	break;	}	catch (IgniteFutureTimeoutCheckedException ignored) {	if (nextDumpTime <= U.currentTimeMillis()) {	
locked near key 

private void dumpPendingObjects(IgniteInternalFuture<?> partReleaseFut) {	U.warn(cctx.kernalContext().cluster().diagnosticLog(), "Failed to wait for partition release future [topVer=" + initialVersion() + ", node=" + cctx.localNodeId() + "]");	
partition release future 

private void dumpPendingObjects(IgniteInternalFuture<?> partReleaseFut) {	U.warn(cctx.kernalContext().cluster().diagnosticLog(), "Failed to wait for partition release future [topVer=" + initialVersion() + ", node=" + cctx.localNodeId() + "]");	
dumping pending objects that might be the cause 

private void dumpPendingObjects(IgniteInternalFuture<?> partReleaseFut) {	U.warn(cctx.kernalContext().cluster().diagnosticLog(), "Failed to wait for partition release future [topVer=" + initialVersion() + ", node=" + cctx.localNodeId() + "]");	try {	cctx.exchange().dumpDebugInfo(this);	}	catch (Exception e) {	
failed to dump debug information 

GridDhtPartitionsSingleMessage mergedMsg = mergedJoinExchMsgs.get(node.id());	if (mergedMsg != null) sndExchId = mergedMsg.exchangeId();	}	if (sndExchId != null && !sndExchId.equals(exchangeId())) {	sndMsg = sndMsg.copy();	sndMsg.exchangeId(sndExchId);	}	cctx.io().send(node, sndMsg, SYSTEM_POOL);	}	catch (ClusterTopologyCheckedException e) {	
failed to send partitions node failed 

}	if (mergedWith0 != null) {	mergedWith0.processMergedMessage(node, msg);	return;	}	initFut.listen(new CI1<IgniteInternalFuture<Boolean>>() {	try {	if (!f.get()) return;	}	catch (IgniteCheckedException e) {	
failed to initialize exchange future 

private boolean awaitSingleMapUpdates() {	try {	synchronized (mux) {	while (pendingSingleUpdates > 0) U.wait(mux);	}	return true;	}	catch (IgniteInterruptedCheckedException e) {	
failed to wait for partition map updates thread was interrupted 

private void onAffinityInitialized(IgniteInternalFuture<Map<Integer, Map<Integer, List<UUID>>>> fut) {	try {	assert fut.isDone();	Map<Integer, Map<Integer, List<UUID>>> assignmentChange = fut.get();	GridDhtPartitionsFullMessage m = createPartitionsMessage(false, false);	CacheAffinityChangeMessage msg = new CacheAffinityChangeMessage(exchId, m, assignmentChange);	
centralized affinity exchange send affinity change message 

fullMsg.joinedNodeAffinity(aff);	}	if (!fullMsg.exchangeId().equals(msg.exchangeId())) {	fullMsg = fullMsg.copy();	fullMsg.exchangeId(msg.exchangeId());	}	try {	cctx.io().send(node, fullMsg, SYSTEM_POOL);	}	catch (ClusterTopologyCheckedException e) {	
failed to send partitions node failed 

}	try {	cctx.io().send(node, fullMsg, SYSTEM_POOL);	}	catch (ClusterTopologyCheckedException e) {	}	catch (IgniteCheckedException e) {	U.error(log, "Failed to send partitions [node=" + node + ']', e);	}	}	
failed to send partitions node failed 

public void onReceiveFullMessage(final ClusterNode node, final GridDhtPartitionsFullMessage msg) {	assert msg != null;	assert msg.exchangeId() != null : msg;	assert !node.isDaemon() : node;	initFut.listen(new CI1<IgniteInternalFuture<Boolean>>() {	try {	if (!f.get()) return;	}	catch (IgniteCheckedException e) {	
failed to initialize exchange future 

FinishState finishState0 = null;	synchronized (mux) {	if (crd == null) {	if (log.isInfoEnabled()) log.info("Ignore partitions request, no coordinator [node=" + node.id() + ']');	return;	}	switch (state) {	case DONE: {	assert finishState != null;	if (node.id().equals(finishState.crdId)) {	
ignore partitions request finished exchange with this coordinator 

switch (state) {	case DONE: {	assert finishState != null;	if (node.id().equals(finishState.crdId)) {	return;	}	finishState0 = finishState;	break;	}	case CRD: case BECOME_CRD: {	
ignore partitions request node is coordinator 

}	catch (IgniteCheckedException e) {	U.error(log, "Failed to send partitions message [node=" + node + ", msg=" + msg + ']', e);	}	return;	}	try {	sendLocalPartitions(node);	}	catch (IgniteCheckedException e) {	
failed to send message to coordinator 

private void processFullMessage(boolean checkCrd, ClusterNode node, GridDhtPartitionsFullMessage msg) {	try {	assert exchId.equals(msg.exchangeId()) : msg;	assert msg.lastVersion() != null : msg;	if (checkCrd) {	assert node != null;	synchronized (mux) {	if (crd == null) {	
ignore full message all server nodes left 

assert exchId.equals(msg.exchangeId()) : msg;	assert msg.lastVersion() != null : msg;	if (checkCrd) {	assert node != null;	synchronized (mux) {	if (crd == null) {	return;	}	switch (state) {	case CRD: case BECOME_CRD: {	
ignore full message node is coordinator 

assert node != null;	synchronized (mux) {	if (crd == null) {	return;	}	switch (state) {	case CRD: case BECOME_CRD: {	return;	}	case DONE: {	
ignore full message future is done 

}	if (log.isInfoEnabled()) {	log.info("Coordinator changed, send partitions to new coordinator [" + "ver=" + initialVersion() + ", crd=" + node.id() + ", newCrd=" + crd0.id() + ']');	}	sendPartitions(crd0);	}	}	}	catch (IgniteCheckedException e) {	if (reconnectOnError(e)) onDone(new IgniteNeedReconnectException(cctx.localNode(), e));	
failed to process node left event 

if (log.isInfoEnabled()) {	log.info("New coordinator sends request [ver=" + initialVersion() + ", node=" + nodeId + ']');	}	cctx.io().send(nodeId, req, SYSTEM_POOL);	}	}	catch (ClusterTopologyCheckedException ignored) {	if (log.isDebugEnabled()) log.debug("Node left during partition exchange [nodeId=" + nodeId + ", exchId=" + exchId + ']');	}	catch (IgniteCheckedException e) {	
failed to request partitions from node 

========================= ignite sample_3754 =========================

checkFailoverSafe(fair);	IgniteLock lock1 = grid(0).reentrantLock("lock", true, fair, true);	assertFalse(lock1.isLocked());	lock1.lock();	IgniteFuture<Object> fut = grid(0).compute().callAsync(new IgniteCallable<Object>() {	private Ignite ignite;	private IgniteLogger log;	IgniteInternalFuture<?> fut = GridTestUtils.runMultiThreadedAsync( new Callable<Object>() {	IgniteLock lock = ignite.reentrantLock("lock", true, fair, true);	assert lock != null;	
thread is going to wait on reentrant lock 

IgniteLock lock1 = grid(0).reentrantLock("lock", true, fair, true);	assertFalse(lock1.isLocked());	lock1.lock();	IgniteFuture<Object> fut = grid(0).compute().callAsync(new IgniteCallable<Object>() {	private Ignite ignite;	private IgniteLogger log;	IgniteInternalFuture<?> fut = GridTestUtils.runMultiThreadedAsync( new Callable<Object>() {	IgniteLock lock = ignite.reentrantLock("lock", true, fair, true);	assert lock != null;	assert lock.tryLock(1, MINUTES);	
thread is again runnable 

========================= ignite sample_946 =========================

private void requestFromNextNode() {	boolean complete;	IgniteLogger log0 = log;	synchronized (this) {	while (!availableNodes.isEmpty()) {	ClusterNode node = availableNodes.poll();	try {	if (log0.isDebugEnabled()) log0.debug("Sending affinity fetch request to remote node [locNodeId=" + ctx.localNodeId() + ", node=" + node + ']');	ctx.io().send(node, new GridDhtAffinityAssignmentRequest(id, grpId, topVer, needPartState), AFFINITY_POOL);	if (ctx.discovery().node(node.id()) == null) {	
failed to request affinity assignment from remote node node left grid will continue to another node 

try {	if (log0.isDebugEnabled()) log0.debug("Sending affinity fetch request to remote node [locNodeId=" + ctx.localNodeId() + ", node=" + node + ']');	ctx.io().send(node, new GridDhtAffinityAssignmentRequest(id, grpId, topVer, needPartState), AFFINITY_POOL);	if (ctx.discovery().node(node.id()) == null) {	continue;	}	pendingNode = node;	break;	}	catch (ClusterTopologyCheckedException ignored) {	
failed to request affinity assignment from remote node node left grid will continue to another node 

pendingNode = node;	break;	}	catch (ClusterTopologyCheckedException ignored) {	}	catch (IgniteCheckedException e) {	if (ctx.discovery().reconnectSupported() && X.hasCause(e, IOException.class)) {	onDone(new IgniteNeedReconnectException(ctx.localNode(), e));	return;	}	
failed to request affinity assignment from remote node will continue to another node 

========================= ignite sample_3774 =========================

private void checkExpire(Ignite ignite, boolean eagerTtl) throws Exception {	CacheConfiguration ccfg = new CacheConfiguration(DEFAULT_CACHE_NAME);	ccfg.setEagerTtl(eagerTtl);	ignite.createCache(ccfg);	try {	IgniteCache<Object, Object> cache = ignite.cache(DEFAULT_CACHE_NAME).withExpiryPolicy(new TouchedExpiryPolicy(new Duration(0, 500)));	ThreadLocalRandom rnd = ThreadLocalRandom.current();	for (int i = 0; i < 10; i++) {	
iteration 

========================= ignite sample_1876 =========================

final List<TestKey> keys = new ArrayList<>();	for (int i = 0; i < KEY_RANGE; i++) keys.add(new TestKey(i));	CacheConfiguration ccfg = cache1.getConfiguration(CacheConfiguration.class);	boolean fullSync = ccfg.getWriteSynchronizationMode() == FULL_SYNC;	boolean optimistic = concurrency == OPTIMISTIC;	boolean checkData = fullSync && !optimistic;	long stopTime = System.currentTimeMillis() + 10_000;	for (int i = 0; i < 10_000; i++) {	if (i % 100 == 0) {	if (System.currentTimeMillis() > stopTime) {	
stop on timeout iteration 

CacheConfiguration ccfg = cache1.getConfiguration(CacheConfiguration.class);	boolean fullSync = ccfg.getWriteSynchronizationMode() == FULL_SYNC;	boolean optimistic = concurrency == OPTIMISTIC;	boolean checkData = fullSync && !optimistic;	long stopTime = System.currentTimeMillis() + 10_000;	for (int i = 0; i < 10_000; i++) {	if (i % 100 == 0) {	if (System.currentTimeMillis() > stopTime) {	break;	}	
iteration 

========================= ignite sample_1899 =========================

public void testDynamicCacheCreation() throws Exception {	for (int iter = 0; iter < 5; iter++) {	
iteration 

========================= ignite sample_1476 =========================

public Serializable execute() {	assert taskSes != null;	if (log.isInfoEnabled()) log.info("Computing job [job=" + this + ", arg=" + argument(0) + ']');	startSignal.countDown();	try {	if (startSignal.await(WAIT_TIME, TimeUnit.MILLISECONDS) == false) fail();	GridTaskSessionAttributeTestListener lsnr = new GridTaskSessionAttributeTestListener(log);	taskSes.addAttributeListener(lsnr, false);	
set attribute testname 

========================= ignite sample_2827 =========================

if (src.hasMoreRows()) {	if (msg.bounds() != null) qctx.putSource(node.id(), msg.segment(), msg.batchLookupId(), src);	}	else if (msg.bounds() == null) {	qctx.putSource(node.id(), msg.segment(), msg.batchLookupId(), null);	}	res.ranges(ranges);	res.status(STATUS_OK);	}	catch (Throwable th) {	
failed to process request 

========================= ignite sample_7776 =========================

public void testLoader() throws Exception {	JMXConnector jmx = null;	try {	while (true) {	try {	jmx = getJMXConnector("localhost", Integer.valueOf(GridTestProperties.getProperty("tomcat.jmx.rmi.connector.port")));	if (jmx != null) break;	}	catch (IOException e) {	
failed to connect to server will try again 

========================= ignite sample_2718 =========================

private void processResourceRequest(UUID nodeId, GridDeploymentRequest req) {	if (log.isDebugEnabled()) log.debug("Received peer class/resource loading request [node=" + nodeId + ", req=" + req + ']');	if (req.responseTopic() == null) {	try {	req.responseTopic(U.unmarshal(marsh, req.responseTopicBytes(), U.resolveClassLoader(ctx.config())));	}	catch (IgniteCheckedException e) {	
failed to process deployment request will ignore 

private void sendResponse(UUID nodeId, Object topic, Message res) {	ClusterNode node = ctx.discovery().node(nodeId);	if (node != null) {	try {	ctx.io().sendToCustomTopic(node, topic, res, GridIoPolicy.P2P_POOL);	if (log.isDebugEnabled()) log.debug("Sent peer class loading response [node=" + node.id() + ", res=" + res + ']');	}	catch (ClusterTopologyCheckedException e) {	
failed to send peer class loading response to node node does not exist 

private void sendResponse(UUID nodeId, Object topic, Message res) {	ClusterNode node = ctx.discovery().node(nodeId);	if (node != null) {	try {	ctx.io().sendToCustomTopic(node, topic, res, GridIoPolicy.P2P_POOL);	if (log.isDebugEnabled()) log.debug("Sent peer class loading response [node=" + node.id() + ", res=" + res + ']');	}	catch (ClusterTopologyCheckedException e) {	}	catch (IgniteCheckedException e) {	
failed to send peer class loading response to node 

private void sendResponse(UUID nodeId, Object topic, Message res) {	ClusterNode node = ctx.discovery().node(nodeId);	if (node != null) {	try {	ctx.io().sendToCustomTopic(node, topic, res, GridIoPolicy.P2P_POOL);	if (log.isDebugEnabled()) log.debug("Sent peer class loading response [node=" + node.id() + ", res=" + res + ']');	}	catch (ClusterTopologyCheckedException e) {	}	catch (IgniteCheckedException e) {	
failed to send peer class loading response to node node does not exist 

if (node != null) {	try {	ctx.io().sendToCustomTopic(node, topic, res, GridIoPolicy.P2P_POOL);	if (log.isDebugEnabled()) log.debug("Sent peer class loading response [node=" + node.id() + ", res=" + res + ']');	}	catch (ClusterTopologyCheckedException e) {	}	catch (IgniteCheckedException e) {	}	}	
failed to send peer class loading response to node node does not exist 

========================= ignite sample_3559 =========================

public void stop(boolean cancel) {	if (srvEndpoint == null) return;	U.cancel(acceptWorker);	U.join(acceptWorker, log);	try {	hnd.stop();	}	catch (IgniteCheckedException e) {	
failed to stop igfs server handler will close client connections anyway 

}	catch (IgniteCheckedException e) {	}	for (ClientWorker worker : clientWorkers) U.cancel(worker);	U.join(clientWorkers, log);	if (srvEndpoint.getPort() >= 0) igfsCtx.kernalContext().ports().deregisterPort(srvEndpoint.getPort(), TCP, srvEndpoint.getClass());	try {	igfsCtx.kernalContext().resource().cleanupGeneric(srvEndpoint);	}	catch (IgniteCheckedException e) {	
failed to cleanup server endpoint 

========================= ignite sample_4766 =========================

if (!cacheWorkDir.exists()) {	boolean res = cacheWorkDir.mkdirs();	if (!res) throw new IgniteCheckedException("Failed to initialize cache working directory " + "(failed to create, make sure the work folder has correct permissions): " + cacheWorkDir.getAbsolutePath());	}	else {	if (cacheWorkDir.isFile()) throw new IgniteCheckedException("Failed to initialize cache working directory " + "(a file with the same name already exists): " + cacheWorkDir.getAbsolutePath());	File lockF = new File(cacheWorkDir, IgniteCacheSnapshotManager.SNAPSHOT_RESTORE_STARTED_LOCK_FILENAME);	Path cacheWorkDirPath = cacheWorkDir.toPath();	Path tmp = cacheWorkDirPath.getParent().resolve(cacheWorkDir.getName() + ".tmp");	if (Files.exists(tmp) && Files.isDirectory(tmp) && Files.exists(tmp.resolve(IgniteCacheSnapshotManager.TEMP_FILES_COMPLETENESS_MARKER))) {	
ignite node crashed during the snapshot restore process there is a snapshot restore lock file left for cache but old version of cache was saved trying to restore it cache 

========================= ignite sample_4064 =========================

private void onHandshake(GridNioSession ses, byte[] msg) {	BinaryInputStream stream = new BinaryHeapInputStream(msg);	BinaryReaderExImpl reader = new BinaryReaderExImpl(null, stream, null, true);	byte cmd = reader.readByte();	if (cmd != ClientListenerRequest.HANDSHAKE) {	
unexpected client request will close session 

byte clientType = reader.readByte();	ClientListenerConnectionContext connCtx = null;	try {	connCtx = prepareContext(clientType);	ensureClientPermissions(clientType);	if (connCtx.isVersionSupported(ver)) {	connCtx.initializeFromHandshake(ver, reader);	ses.addMeta(CONN_CTX_META_KEY, connCtx);	}	else {	
unsupported version 

if (connCtx.isVersionSupported(ver)) {	connCtx.initializeFromHandshake(ver, reader);	ses.addMeta(CONN_CTX_META_KEY, connCtx);	}	else {	throw new IgniteCheckedException("Unsupported version.");	}	connCtx.handler().writeHandshake(writer);	}	catch (IgniteCheckedException e) {	
error on handshake 

========================= ignite sample_4571 =========================

try {	switch (scope) {	case GLOBAL_SCOPE: {	byte[] data = state == null ? null : U.marshal(marsh, state);	saved = getSpi(ses.getCheckpointSpi()).saveCheckpoint(key, data, timeout, override);	if (saved) record(EVT_CHECKPOINT_SAVED, key);	break;	}	case SESSION_SCOPE: {	if (closedSess.contains(ses.getId())) {	
checkpoint will not be saved due to session invalidation key val ses checkpoint will not be saved due to session invalidation 

byte[] data = state == null ? null : U.marshal(marsh, state);	saved = getSpi(ses.getCheckpointSpi()).saveCheckpoint(key, data, timeout, override);	if (saved) record(EVT_CHECKPOINT_SAVED, key);	break;	}	case SESSION_SCOPE: {	if (closedSess.contains(ses.getId())) {	break;	}	if (now > ses.getEndTime()) {	
checkpoint will not be saved due to session timeout key val ses checkpoint will not be saved due to session timeout 

if (now > ses.getEndTime()) {	break;	}	if (now + timeout > ses.getEndTime() || now + timeout < 0) timeout = ses.getEndTime() - now;	byte[] data = state == null ? null : U.marshal(marsh, state);	Set<String> keys = keyMap.get(ses.getId());	if (keys == null) {	Set<String> old = keyMap.putIfAbsent(ses.getId(), (CheckpointSet)(keys = new CheckpointSet(ses.session())));	if (old != null) keys = old;	if (closedSess.contains(ses.getId())) {	
checkpoint will not be saved due to session invalidation key val ses checkpoint will not be saved due to session invalidation 

byte[] data = state == null ? null : U.marshal(marsh, state);	Set<String> keys = keyMap.get(ses.getId());	if (keys == null) {	Set<String> old = keyMap.putIfAbsent(ses.getId(), (CheckpointSet)(keys = new CheckpointSet(ses.session())));	if (old != null) keys = old;	if (closedSess.contains(ses.getId())) {	keyMap.remove(ses.getId(), keys);	break;	}	}	
resolved keys for session keys ses keyMap 

public boolean removeCheckpoint(GridTaskSessionInternal ses, String key) {	if (!enabled()) return false;	assert ses != null;	assert key != null;	Set<String> keys = keyMap.get(ses.getId());	boolean rmv = false;	if (keys != null) {	keys.remove(key);	rmv = getSpi(ses.getCheckpointSpi()).removeCheckpoint(key);	}	
checkpoint will not be removed key map not found key ses 

========================= ignite sample_3522 =========================

}	catch (IgniteClientDisconnectedException e) {	checkAndWait(e);	}	if (++iter % 100 == 0) log.info("Iteration: " + iter);	if (barrier != null) barrier.await();	}	return null;	}	catch (Throwable e) {	
unexpected error in operation thread 

========================= ignite sample_643 =========================

private boolean compare(Map<Integer, Integer> expVals) throws Exception {	cmpLatch = new CountDownLatch(putThreads());	cmp = true;	killLock.lock();	try {	
comparing cache content 

private boolean compare(Map<Integer, Integer> expVals) throws Exception {	cmpLatch = new CountDownLatch(putThreads());	cmp = true;	killLock.lock();	try {	if (!cmpLatch.await(60_000, TimeUnit.MILLISECONDS)) throw new IgniteCheckedException("Failed to suspend threads executing put.");	if (compareCaches(expVals)) {	
cache comparison succeeded 

private boolean compare(Map<Integer, Integer> expVals) throws Exception {	cmpLatch = new CountDownLatch(putThreads());	cmp = true;	killLock.lock();	try {	if (!cmpLatch.await(60_000, TimeUnit.MILLISECONDS)) throw new IgniteCheckedException("Failed to suspend threads executing put.");	if (compareCaches(expVals)) {	return true;	}	else {	
cache comparison failed 

if (dhtCaches != null) dhtCacheKeys.addAll(dhtCaches.get(i).keySet());	}	boolean failed = false;	if (!F.eq(expVals.keySet(), cacheKeys)) {	Collection<Integer> expOnly = new HashSet<>();	Collection<Integer> cacheOnly = new HashSet<>();	expOnly.addAll(expVals.keySet());	expOnly.removeAll(cacheKeys);	cacheOnly.addAll(cacheKeys);	cacheOnly.removeAll(expVals.keySet());	
cache does not contain expected keys 

if (dhtCaches != null) dhtCacheKeys.addAll(dhtCaches.get(i).keySet());	}	boolean failed = false;	if (!F.eq(expVals.keySet(), cacheKeys)) {	Collection<Integer> expOnly = new HashSet<>();	Collection<Integer> cacheOnly = new HashSet<>();	expOnly.addAll(expVals.keySet());	expOnly.removeAll(cacheKeys);	cacheOnly.addAll(cacheKeys);	cacheOnly.removeAll(expVals.keySet());	
cache does contain unexpected keys 

cacheOnly.removeAll(expVals.keySet());	failed = true;	}	if (dhtCaches != null && !F.eq(expVals.keySet(), dhtCacheKeys)) {	Collection<Integer> expOnly = new HashSet<>();	Collection<Integer> cacheOnly = new HashSet<>();	expOnly.addAll(expVals.keySet());	expOnly.removeAll(dhtCacheKeys);	cacheOnly.addAll(dhtCacheKeys);	cacheOnly.removeAll(expVals.keySet());	
dht cache does not contain expected keys 

cacheOnly.removeAll(expVals.keySet());	failed = true;	}	if (dhtCaches != null && !F.eq(expVals.keySet(), dhtCacheKeys)) {	Collection<Integer> expOnly = new HashSet<>();	Collection<Integer> cacheOnly = new HashSet<>();	expOnly.addAll(expVals.keySet());	expOnly.removeAll(dhtCacheKeys);	cacheOnly.addAll(dhtCacheKeys);	cacheOnly.removeAll(expVals.keySet());	
dht cache does contain unexpected keys 

cacheOnly.removeAll(expVals.keySet());	failed = true;	}	Collection<Integer> failedKeys = new HashSet<>();	for (Map.Entry<Integer, Integer> entry : expVals.entrySet()) {	for (int i = 0; i < dataNodes(); i++) {	if (!F.eq(caches.get(i).get(entry.getKey()), entry.getValue())) failedKeys.add(entry.getKey());	}	}	if (!failedKeys.isEmpty()) {	
cache content is incorrect for keys 

========================= ignite sample_1374 =========================

protected void uncommit(boolean nodeStopping) {	try {	if (!nodeStopping) {	for (IgniteTxEntry e : writeMap().values()) {	try {	GridCacheEntryEx entry = e.cached();	if (e.op() != NOOP) entry.invalidate(xidVer);	}	catch (Throwable t) {	
failed to invalidate transaction entries while reverting a commit 

}	catch (Throwable t) {	if (t instanceof Error) throw (Error)t;	break;	}	}	cctx.tm().uncommitTx(this);	}	}	catch (Exception ex) {	
failed to do uncommit 

if (cctx.wal() != null && cctx.tm().logTxRecords()) {	if (state == PREPARED || state == COMMITTED || state == ROLLED_BACK) {	assert txNodes != null || state == ROLLED_BACK : "txNodes=" + txNodes + " state=" + state;	BaselineTopology baselineTop = cctx.kernalContext().state().clusterState().baselineTopology();	Map<Short, Collection<Short>> participatingNodes = consistentIdMapper .mapToCompactIds(topVer, txNodes, baselineTop);	TxRecord txRecord = new TxRecord( state, nearXidVersion(), writeVersion(), participatingNodes );	try {	ptr = cctx.wal().log(txRecord);	}	catch (IgniteCheckedException e) {	
failed to log txrecord 

if (intercept) {	IgniteBiTuple<Boolean, Object> t = cacheCtx.config().getInterceptor().onBeforeRemove( new CacheLazyEntry(cacheCtx, key, e.cached().rawGet(), e.keepBinary()));	if (cacheCtx.cancelRemove(t)) continue;	}	if (writeStore == null) writeStore = cacheCtx.store();	if (writeStore.isWriteThrough()) {	if (rmvCol == null) rmvCol = new ArrayList<>();	rmvCol.add(key);	}	}	
ignoring noop entry for batch store commit 

========================= ignite sample_3963 =========================

}	assertTrue(cnt > 0);	}	catch (IgniteException ignore) {	}	int val = rnd.nextInt(ITEMS);	assertTrue("Not contains: " + val, set.contains(val));	val = ITEMS + rnd.nextInt(ITEMS);	assertFalse("Contains: " + val, set.contains(val));	}	
remove set 

assertTrue(cnt > 0);	}	catch (IgniteException ignore) {	}	int val = rnd.nextInt(ITEMS);	assertTrue("Not contains: " + val, set.contains(val));	val = ITEMS + rnd.nextInt(ITEMS);	assertFalse("Contains: " + val, set.contains(val));	}	set.close();	
create new set 

private IgniteInternalFuture<?> startNodeKiller(final AtomicBoolean stop) {	return GridTestUtils.runAsync(new Callable<Void>() {	ThreadLocalRandom rnd = ThreadLocalRandom.current();	while (!stop.get()) {	int idx = rnd.nextInt(1, gridCount());	U.sleep(rnd.nextLong(2000, 3000));	
killing node 

========================= ignite sample_993 =========================

public void execute(Tuple tuple) {	if (stopped) return;	if (!(tuple.getValueByField(igniteTupleField) instanceof Map)) throw new IgniteException("Map as a streamer input is expected!");	final Map<K, V> gridVals = (Map<K, V>)tuple.getValueByField(igniteTupleField);	try {	
tuple id from storm 

public void execute(Tuple tuple) {	if (stopped) return;	if (!(tuple.getValueByField(igniteTupleField) instanceof Map)) throw new IgniteException("Map as a streamer input is expected!");	final Map<K, V> gridVals = (Map<K, V>)tuple.getValueByField(igniteTupleField);	try {	getStreamer().addData(gridVals);	collector.ack(tuple);	}	catch (Exception e) {	
error while processing tuple of 

========================= ignite sample_7920 =========================

private List<List<?>> checkQuery(String sql, IgniteCache<Object, Object> cache, int expSize, Object... args) {	SqlFieldsQuery qry = new SqlFieldsQuery(sql);	qry.setDistributedJoins(true);	qry.setArgs(args);	
plan 

========================= ignite sample_7536 =========================

private void init() {	if (initLatch.getCount() > 0) {	if (initGuard.compareAndSet(false, true)) {	
initializing cache store 

========================= ignite sample_2958 =========================

IgniteFramework igniteFramework = new IgniteFramework();	ClusterProperties clusterProps = ClusterProperties.from(args.length >= 1 ? args[0] : null);	String baseUrl = String.format("http: JettyServer httpSrv = new JettyServer();	httpSrv.start( new ResourceHandler(clusterProps.userLibs(), clusterProps.igniteCfg(), clusterProps.igniteWorkDir()), clusterProps );	ResourceProvider provider = new ResourceProvider();	IgniteProvider igniteProvider = new IgniteProvider(clusterProps.igniteWorkDir());	provider.init(clusterProps, igniteProvider, baseUrl);	Scheduler scheduler = new IgniteScheduler(clusterProps, provider);	MesosSchedulerDriver driver;	if (System.getenv(MESOS_AUTHENTICATE) != null) {	
enabling authentication for the framework 

public Protos.FrameworkInfo getFrameworkInfo() throws Exception {	final int frameworkFailoverTimeout = 0;	Protos.FrameworkInfo.Builder frameworkBuilder = Protos.FrameworkInfo.newBuilder() .setName(IGNITE_FRAMEWORK_NAME) .setUser(getUser()) .setRole(getRole()) .setFailoverTimeout(frameworkFailoverTimeout);	if (System.getenv(MESOS_CHECKPOINT) != null) {	
enabling checkpoint for the framework 

========================= ignite sample_6448 =========================

threads.add(t);	}	for (int i = 0; i < restartThreads; i++) {	final int gridIdx = i + putThreads;	Thread t = new Thread(new Runnable() {	try {	barrier.await();	info("Starting restart thread: " + gridIdx);	int cnt = 0;	while (System.currentTimeMillis() < endTime && err.get() == null) {	
stopping grid 

}	for (int i = 0; i < restartThreads; i++) {	final int gridIdx = i + putThreads;	Thread t = new Thread(new Runnable() {	try {	barrier.await();	info("Starting restart thread: " + gridIdx);	int cnt = 0;	while (System.currentTimeMillis() < endTime && err.get() == null) {	stopGrid(gridIdx);	
starting grid 

========================= ignite sample_1362 =========================

activated++;	synchronized (waitCtx.getJobContext()) {	waitCtx.activate();	}	}	else if (stealReqs.get() > 0) {	if (waitCtx.getJob().getClass().isAnnotationPresent(JobStealingDisabled.class)) continue;	Integer stealingCnt = waitCtx.getJobContext().getAttribute(STEALING_ATTEMPT_COUNT_ATTR);	if (stealingCnt != null) {	if (stealingCnt >= maxStealingAttempts) {	
waiting job exceeded stealing attempts and won t be rejected will try other jobs on waiting list 

private void checkIdle(Collection<CollisionJobContext> waitJobs, Collection<CollisionJobContext> activeJobs) {	int max = waitJobsThreshold + activeJobsThreshold;	if (max < 0) max = Integer.MAX_VALUE;	int jobsToSteal = max - (waitJobs.size() + activeJobs.size());	
total number of jobs to be stolen 

if (max < 0) max = Integer.MAX_VALUE;	int jobsToSteal = max - (waitJobs.size() + activeJobs.size());	if (jobsToSteal > 0) {	int jobsLeft = jobsToSteal;	ClusterNode next;	int nodeCnt = getSpiContext().remoteNodes().size();	int idx = 0;	while (jobsLeft > 0 && idx++ < nodeCnt && (next = nodeQueue.poll()) != null) {	if (getSpiContext().node(next.id()) == null) continue;	if (!F.isEmpty(stealAttrs) && (next.attributes() == null || !U.containsAll(next.attributes(), stealAttrs))) {	
skip node as it does not have all attributes 

int idx = 0;	while (jobsLeft > 0 && idx++ < nodeCnt && (next = nodeQueue.poll()) != null) {	if (getSpiContext().node(next.id()) == null) continue;	if (!F.isEmpty(stealAttrs) && (next.attributes() == null || !U.containsAll(next.attributes(), stealAttrs))) {	continue;	}	int delta = 0;	try {	MessageInfo msgInfo = sndMsgMap.get(next.id());	if (msgInfo == null) {	
failed to find message info for node 

continue;	}	int delta = 0;	try {	MessageInfo msgInfo = sndMsgMap.get(next.id());	if (msgInfo == null) {	continue;	}	Integer waitThreshold = next.attribute(createSpiAttributeName(WAIT_JOBS_THRESHOLD_NODE_ATTR));	if (waitThreshold == null) {	
remote node is not configured with gridjobstealingcollisionspi and jobs will not be stolen from it you must stop it and update its configuration to use gridjobstealingcollisionspi 

jobsLeft -= msgInfo.jobsToSteal();	continue;	}	if (jobsLeft < delta) delta = jobsLeft;	jobsLeft -= delta;	msgInfo.reset(delta);	}	getSpiContext().send(next, new JobStealingRequest(delta), JOB_STEALING_COMM_TOPIC);	}	catch (IgniteSpiException e) {	
failed to send job stealing message to node 

========================= ignite sample_5771 =========================

stopGrid(NODES_CNT);	}	catch (Exception ignored) {	}	}	}	}, 1, "restart-thread");	long stopTime = System.currentTimeMillis() + 2 * 60_000L;	for (int i = 0; System.currentTimeMillis() < stopTime; i++) {	boolean detectionEnabled = grid(0).context().cache().context().tm().deadlockDetectionEnabled();	
iteration detection is enabled disabled 

for (int i = 0; i < 50; i++) {	int key = rnd.nextInt(50);	if (log.isDebugEnabled()) {	log.info(">>> Performs put [node=" + ((IgniteKernal)ignite).localNode() + ", tx=" + tx + ", key=" + key + ']');	}	cache.put(key, 0);	}	tx.commit();	}	catch (Exception e) {	
ignore error 

========================= ignite sample_1620 =========================

public void testBlockListener() throws Exception {	for (int i = 0; i < ITERS; i++) {	
creating spis 

final ClusterNode node1 = nodes.get(1);	final AtomicInteger msgId = new AtomicInteger();	spi0.sendMessage(node1, new GridTestMessage(node0.id(), msgId.incrementAndGet(), 0));	GridTestUtils.waitForCondition(new GridAbsPredicate() {	return lsnr1.rcvCnt.get() >= 1;	}	}, 1000);	final AtomicInteger sentCnt = new AtomicInteger(1);	int errCnt = 0;	for (int i = 0; i < ITERS; i++) {	
iteration 

final AtomicInteger expCnt0 = new AtomicInteger();	final AtomicInteger expCnt1 = new AtomicInteger();	spi0.sendMessage(node1, new GridTestMessage(node0.id(), msgId.incrementAndGet(), 0));	GridTestUtils.waitForCondition(new GridAbsPredicate() {	return lsnr1.rcvCnt.get() >= 1;	}	}, 1000);	expCnt1.incrementAndGet();	int errCnt = 0;	for (int i = 0; i < ITERS; i++) {	
iteration 

final ClusterNode node1 = nodes.get(1);	final AtomicInteger msgId = new AtomicInteger();	spi0.sendMessage(node1, new GridTestMessage(node0.id(), msgId.incrementAndGet(), 0));	GridTestUtils.waitForCondition(new GridAbsPredicate() {	return lsnr1.rcvCnt.get() >= 1;	}	}, 1000);	final AtomicInteger sentCnt = new AtomicInteger(1);	int errCnt = 0;	for (int i = 0; i < ITERS; i++) {	
iteration 

========================= ignite sample_2630 =========================

private boolean checkRemaining() {	if (remainingTxs.isEmpty()) {	
finishing multi tx future 

========================= ignite sample_3888 =========================

private void concurrentUpdateAndNearCacheClose(CacheAtomicityMode atomicityMode, final int nearClient) throws Exception {	final String cacheName = ignite(0).createCache(cacheConfiguration(atomicityMode, false)).getName();	for (int iter = 0; iter < 5; iter++) {	
iteration 

========================= ignite sample_1781 =========================

private Map<Integer, Serializable> unmarshalData( Map<Integer, byte[]> src, Marshaller marsh, ClassLoader clsLdr, boolean clientNode, IgniteLogger log ) {	Map<Integer, Serializable> res = U.newHashMap(src.size());	for (Map.Entry<Integer, byte[]> binEntry : src.entrySet()) {	try {	Serializable compData = marsh.unmarshal(binEntry.getValue(), clsLdr);	res.put(binEntry.getKey(), compData);	}	catch (IgniteCheckedException e) {	if (CONTINUOUS_PROC.ordinal() == binEntry.getKey() && X.hasCause(e, ClassNotFoundException.class) && clientNode) U.warn(log, "Failed to unmarshal continuous query remote filter on client node. Can be ignored.");	
failed to unmarshal discovery data for component 

========================= ignite sample_5668 =========================

public void invoke(IN in) {	try {	if (!(in instanceof Map)) throw new IgniteException("Map as a streamer input is expected!");	SinkContext.getStreamer().addData((Map)in);	}	catch (Exception e) {	
error while processing in of 

========================= ignite sample_6101 =========================

public void testNodeRestart() throws Exception {	for (int i = 0; i < 10; i++) {	
iteration 

public void testNodeRestart() throws Exception {	for (int i = 0; i < 10; i++) {	client = false;	startGridsMultiThreaded(SRVS);	client = true;	startGrid(SRVS);	final AtomicBoolean stop = new AtomicBoolean();	try {	IgniteInternalFuture<?> restartFut = GridTestUtils.runAsync(new Callable<Void>() {	while (!stop.get()) {	
start node 

for (int i = 0; i < 10; i++) {	client = false;	startGridsMultiThreaded(SRVS);	client = true;	startGrid(SRVS);	final AtomicBoolean stop = new AtomicBoolean();	try {	IgniteInternalFuture<?> restartFut = GridTestUtils.runAsync(new Callable<Void>() {	while (!stop.get()) {	startGrid(SRVS + CLIENTS);	
stop node 

stopGrid(SRVS + CLIENTS);	}	return null;	}	}, "restart-thread");	final AtomicInteger idx = new AtomicInteger();	IgniteInternalFuture<?> fut = GridTestUtils.runMultiThreadedAsync(new Callable<Object>() {	int threadIdx = idx.getAndIncrement();	int node = threadIdx % (SRVS + CLIENTS);	Ignite ignite = ignite(node);	
started thread 

IgniteCache<Object, Object> cache2 = ignite.cache(TX_CACHE);	ThreadLocalRandom rnd = ThreadLocalRandom.current();	while (!stop.get()) {	try {	cache1.put(new TestClass1(true), create(rnd.nextInt(20) + 1));	cache1.invoke(new TestClass1(true), new TestEntryProcessor(rnd.nextInt(20) + 1));	cache2.put(new TestClass1(true), create(rnd.nextInt(20) + 1));	cache2.invoke(new TestClass1(true), new TestEntryProcessor(rnd.nextInt(20) + 1));	}	catch (CacheException | IgniteException e) {	
error 

========================= ignite sample_1017 =========================

private void moreInfo(StringBuilder sb, GridKernalContext ctx) {	for (DiagnosticBaseClosure c : cls.values()) {	try {	c.apply(sb, ctx);	}	catch (Exception e) {	
failed to populate diagnostic with additional information 

========================= ignite sample_3587 =========================

public void testRebalance() throws Exception {	for (int iter = 0; iter < 5; iter++) {	
iteration 

Affinity<Object> aff = qryClient.affinity(DEFAULT_CACHE_NAME);	ContinuousQuery<Object, Object> qry = new ContinuousQuery<>();	final CacheEventListener3 lsnr = asyncCallback() ? new CacheEventAsyncListener3() : new CacheEventListener3();	qry.setLocalListener(lsnr);	qry.setRemoteFilter(lsnr);	int PARTS = 10;	QueryCursor<?> cur = qryClientCache.query(qry);	Map<Object, T2<Object, Object>> updates = new HashMap<>();	final List<T3<Object, Object, Object>> expEvts = new ArrayList<>();	for (int i = 0; i < (atomicityMode() == CacheAtomicityMode.ATOMIC ? SRV_NODES - 1 : SRV_NODES - 2); i++) {	
stop iteration 

filtered = !filtered;	}	stopGrid(i);	boolean check = GridTestUtils.waitForCondition(new PAX() {	return expEvts.size() == lsnr.keys.size();	}	}, 5000L);	if (!check) {	Set<Integer> keys0 = new HashSet<>(keys);	keys0.removeAll(lsnr.keys);	
missed events for keys 

IgniteCache<Object, Object> qryClientCache = qryClient.cache(DEFAULT_CACHE_NAME);	Affinity<Object> aff = qryClient.affinity(DEFAULT_CACHE_NAME);	CacheEventListener1 lsnr = asyncCallback() ? new CacheEventAsyncListener1(false) : new CacheEventListener1(false);	ContinuousQuery<Object, Object> qry = new ContinuousQuery<>();	qry.setLocalListener(lsnr);	QueryCursor<?> cur = qryClientCache.query(qry);	int PARTS = 10;	Map<Object, T2<Object, Object>> updates = new HashMap<>();	List<T3<Object, Object, Object>> expEvts = new ArrayList<>();	for (int i = 0; i < (atomicityMode() == CacheAtomicityMode.ATOMIC ? SRV_NODES - 1 : SRV_NODES - 2); i++) {	
stop iteration 

}	if (first) {	spi.skipMsg = true;	first = false;	}	}	stopGrid(i);	if (!latch.await(5, SECONDS)) {	Set<Integer> keys0 = new HashSet<>(keys);	keys0.removeAll(lsnr.keys);	
missed events for keys 

}	stopGrid(i);	if (!latch.await(5, SECONDS)) {	Set<Integer> keys0 = new HashSet<>(keys);	keys0.removeAll(lsnr.keys);	fail("Failed to wait for notifications [exp=" + keys.size() + ", left=" + lsnr.latch.getCount() + ']');	}	checkEvents(expEvts, lsnr);	}	for (int i = 0; i < (atomicityMode() == CacheAtomicityMode.ATOMIC ? SRV_NODES - 1 : SRV_NODES - 2); i++) {	
start iteration 

else {	updates.put(key, new T2<>((Object)key, (Object)key));	expEvts.add(new T3<>((Object)key, (Object)key, (Object)key));	}	if (updateFromClient) qryClientCache.put(key, key);	else cache.put(key, key);	}	if (!latch.await(10, SECONDS)) {	Set<Integer> keys0 = new HashSet<>(keys);	keys0.removeAll(lsnr.keys);	
missed events for keys 

CacheEventListener1 lsnr = new CacheEventListener1(false);	ContinuousQuery<Object, Object> qry = new ContinuousQuery<>();	qry.setLocalListener(lsnr);	QueryCursor<?> cur = qryClient.cache(DEFAULT_CACHE_NAME).query(qry);	assertEquals(0, backupQueue(ignite(1)).size());	IgniteCache<Object, Object> cache0 = ignite(0).cache(DEFAULT_CACHE_NAME);	List<Integer> keys = primaryKeys(cache0, BACKUP_ACK_THRESHOLD);	CountDownLatch latch = new CountDownLatch(keys.size());	lsnr.latch = latch;	for (Integer key : keys) {	
put 

ContinuousQuery<Object, Object> qry = new ContinuousQuery<>();	qry.setLocalListener(lsnr);	QueryCursor<?> cur = qryClient.cache(DEFAULT_CACHE_NAME).query(qry);	assertEquals(0, backupQueue(ignite(0)).size());	long ttl = 100;	final ExpiryPolicy expiry = new TouchedExpiryPolicy(new Duration(MILLISECONDS, ttl));	final IgniteCache<Object, Object> cache0 = ignite(2).cache(DEFAULT_CACHE_NAME).withExpiryPolicy(expiry);	final List<Integer> keys = primaryKeys(ignite(1).cache(DEFAULT_CACHE_NAME), BACKUP_ACK_THRESHOLD);	lsnr.latch = new CountDownLatch(keys.size());	for (Integer key : keys) {	
put 

CacheEventListener1 lsnr = new CacheEventListener1(false);	ContinuousQuery<Object, Object> qry = new ContinuousQuery<>();	qry.setLocalListener(lsnr);	IgniteCache<Object, Object> cache = qryClient.cache(DEFAULT_CACHE_NAME);	QueryCursor<?> cur = cache.query(qry);	assertEquals(0, backupQueue(ignite(1)).size());	List<Integer> keys = primaryKeys(cache, BACKUP_ACK_THRESHOLD);	CountDownLatch latch = new CountDownLatch(keys.size());	lsnr.latch = latch;	for (Integer key : keys) {	
put 

client = false;	IgniteCache<Object, Object> qryClnCache = qryClient.cache(DEFAULT_CACHE_NAME);	Affinity<Object> aff = qryClient.affinity(DEFAULT_CACHE_NAME);	final CacheEventListener2 lsnr = new CacheEventListener2();	ContinuousQuery<Object, Object> qry = new ContinuousQuery<>();	qry.setLocalListener(lsnr);	qry.setRemoteFilter(new CacheEventFilter());	QueryCursor<?> cur = qryClnCache.query(qry);	for (int i = 0; i < 10; i++) {	final int idx = i % (SRV_NODES - 1);	
stop node 

final int idx = i % (SRV_NODES - 1);	stopGrid(idx);	awaitPartitionMapExchange();	List<T3<Object, Object, Object>> afterRestEvts = new ArrayList<>();	for (int j = 0; j < aff.partitions(); j++) {	Integer oldVal = (Integer)qryClnCache.get(j);	qryClnCache.put(j, i);	afterRestEvts.add(new T3<>((Object)j, (Object)i, (Object)oldVal));	}	checkEvents(new ArrayList<>(afterRestEvts), lsnr, false);	
start node 

qry.setLocalListener(lsnr);	qry.setRemoteFilter(asyncCallback() ? new CacheEventAsyncFilter() : new CacheEventFilter());	QueryCursor<?> cur = qryClnCache.query(qry);	CacheEventListener2 dinLsnr = null;	QueryCursor<?> dinQry = null;	final AtomicBoolean stop = new AtomicBoolean();	final AtomicReference<CountDownLatch> checkLatch = new AtomicReference<>();	IgniteInternalFuture<?> restartFut = GridTestUtils.runAsync(new Callable<Void>() {	while (!stop.get() && !err) {	final int idx = ThreadLocalRandom.current().nextInt(SRV_NODES - 1);	
stop node 

final AtomicBoolean stop = new AtomicBoolean();	final AtomicReference<CountDownLatch> checkLatch = new AtomicReference<>();	IgniteInternalFuture<?> restartFut = GridTestUtils.runAsync(new Callable<Void>() {	while (!stop.get() && !err) {	final int idx = ThreadLocalRandom.current().nextInt(SRV_NODES - 1);	awaitPartitionMapExchange();	Thread.sleep(400);	stopGrid(idx);	awaitPartitionMapExchange();	Thread.sleep(400);	
start node 

awaitPartitionMapExchange();	Thread.sleep(400);	stopGrid(idx);	awaitPartitionMapExchange();	Thread.sleep(400);	startGrid(idx);	Thread.sleep(200);	CountDownLatch latch = new CountDownLatch(1);	assertTrue(checkLatch.compareAndSet(null, latch));	if (!stop.get()) {	
wait for event check 

final int THREAD = 4;	final int PARTS = THREAD;	final List<List<T3<Object, Object, Object>>> expEvts = new ArrayList<>(THREAD + 5);	for (int i = 0; i < THREAD; i++) expEvts.add(i, new ArrayList<T3<Object, Object, Object>>());	final AtomicReference<CyclicBarrier> checkBarrier = new AtomicReference<>();	final ThreadLocalRandom rnd = ThreadLocalRandom.current();	IgniteInternalFuture<?> restartFut = GridTestUtils.runAsync(new Callable<Void>() {	try {	while (!stop.get() && !err) {	final int idx = rnd.nextInt(SRV_NODES);	
stop node 

try {	while (!stop.get() && !err) {	final int idx = rnd.nextInt(SRV_NODES);	stopGrid(idx);	Thread.sleep(300);	GridTestUtils.waitForCondition(new PA() {	return qryCln.cluster().nodes().size() == SRV_NODES;	}	}, 5000L);	try {	
start node 

}, 5000L);	try {	startGrid(idx);	Thread.sleep(300);	GridTestUtils.waitForCondition(new PA() {	return qryCln.cluster().nodes().size() == SRV_NODES + 1;	}	}, 5000L);	}	catch (Exception e) {	
failed to stop nodes 

GridTestUtils.waitForCondition(new PA() {	return lsnr.size() >= size;	}	}, 10_000L);	List<T3<Object, Object, Object>> expEvts0 = new ArrayList<>();	for (List<T3<Object, Object, Object>> evt : expEvts) expEvts0.addAll(evt);	checkEvents(expEvts0, lsnr, false, false);	for (List<T3<Object, Object, Object>> evt : expEvts) evt.clear();	}	catch (Exception e) {	
failed 

final IgniteCache<Object, Object> cache = qryClient.cache(DEFAULT_CACHE_NAME);	CacheEventListener1 lsnr = new CacheEventListener1(true);	ContinuousQuery<Object, Object> qry = new ContinuousQuery<>();	qry.setLocalListener(lsnr);	QueryCursor<?> cur = cache.query(qry);	client = false;	final int SRV_IDX = SRV_NODES - 1;	List<Integer> keys = primaryKeys(ignite(SRV_IDX).cache(DEFAULT_CACHE_NAME), 10);	final int THREADS = 10;	for (int i = 0; i < keys.size(); i++) {	
iteration 

stopGrid(SRV_IDX);	if (!latch.await(5, SECONDS)) fail("Failed to wait for notifications [exp=" + THREADS + ", left=" + lsnr.latch.getCount() + ']');	assertEquals(THREADS, lsnr.allEvts.size());	Set<Integer> vals = new HashSet<>();	boolean err = false;	for (CacheEntryEvent<?, ?> evt : lsnr.allEvts) {	assertEquals(key, evt.getKey());	assertNotNull(evt.getValue());	if (!vals.add((Integer)evt.getValue())) {	err = true;	
extra event 

for (CacheEntryEvent<?, ?> evt : lsnr.allEvts) {	assertEquals(key, evt.getKey());	assertNotNull(evt.getValue());	if (!vals.add((Integer)evt.getValue())) {	err = true;	}	}	for (int v = 0; v < THREADS; v++) {	if (!vals.contains(v)) {	err = true;	
event for value not received 

private Thread nodeRestartThread(final int restartCycles, final long initDelay, final long restartDelay) {	Thread t = new Thread(new Runnable() {	public void run() {	sleep(initDelay);	try {	for (int i = 1; i <= restartCycles && !Thread.interrupted(); i++) {	IgniteConfiguration cfg = optimize(getConfiguration("restartNode")). setGridLogger(new NullLogger());	
node restart cycle started 

Thread t = new Thread(new Runnable() {	public void run() {	sleep(initDelay);	try {	for (int i = 1; i <= restartCycles && !Thread.interrupted(); i++) {	IgniteConfiguration cfg = optimize(getConfiguration("restartNode")). setGridLogger(new NullLogger());	try (Ignite ignored = Ignition.start(cfg)) {	awaitPartitionMapExchange();	sleep(restartDelay);	}	
node restart cycle finished 

IgniteConfiguration cfg = optimize(getConfiguration("restartNode")). setGridLogger(new NullLogger());	try (Ignite ignored = Ignition.start(cfg)) {	awaitPartitionMapExchange();	sleep(restartDelay);	}	awaitPartitionMapExchange();	sleep(restartDelay);	}	}	catch (Exception e) {	
unexpected error 

========================= ignite sample_1943 =========================

public void runInIgfsThreadPool(Runnable r) {	try {	igfsSvc.execute(r);	}	catch (RejectedExecutionException ignored) {	try {	r.run();	}	catch (Exception e) {	
failed to execute igfs runnable 

========================= ignite sample_4757 =========================

blockP = null;	blockCls.clear();	blockP = null;	if (sndMsgs) {	for (T2<ClusterNode, GridIoMessage> msg : blockedMsgs) {	try {	ignite.log().info("Send blocked message [node=" + msg.get1().id() + ", msg=" + msg.get2().message() + ']');	super.sendMessage(msg.get1(), msg.get2());	}	catch (Throwable e) {	
failed to send blocked message 

========================= ignite sample_648 =========================

private void createKeyspace(KeyValuePersistenceSettings settings) {	int attempt = 0;	Throwable error = null;	String errorMsg = "Failed to create Cassandra keyspace '" + settings.getKeyspace() + "'";	while (attempt < CQL_EXECUTION_ATTEMPTS_COUNT) {	try {	log.info("-----------------------------------------------------------------------");	
creating cassandra keyspace 

private void createKeyspace(KeyValuePersistenceSettings settings) {	int attempt = 0;	Throwable error = null;	String errorMsg = "Failed to create Cassandra keyspace '" + settings.getKeyspace() + "'";	while (attempt < CQL_EXECUTION_ATTEMPTS_COUNT) {	try {	log.info("-----------------------------------------------------------------------");	log.info("-----------------------------------------------------------------------\n\n" + settings.getKeyspaceDDLStatement() + "\n");	log.info("-----------------------------------------------------------------------");	session().execute(settings.getKeyspaceDDLStatement());	
cassandra keyspace was successfully created 

String errorMsg = "Failed to create Cassandra keyspace '" + settings.getKeyspace() + "'";	while (attempt < CQL_EXECUTION_ATTEMPTS_COUNT) {	try {	log.info("-----------------------------------------------------------------------");	log.info("-----------------------------------------------------------------------\n\n" + settings.getKeyspaceDDLStatement() + "\n");	log.info("-----------------------------------------------------------------------");	session().execute(settings.getKeyspaceDDLStatement());	return;	}	catch (AlreadyExistsException ignored) {	
cassandra keyspace already exist 

private void createTable(String table, KeyValuePersistenceSettings settings) {	int attempt = 0;	Throwable error = null;	String tableFullName = settings.getKeyspace() + "." + table;	String errorMsg = "Failed to create Cassandra table '" + tableFullName + "'";	while (attempt < CQL_EXECUTION_ATTEMPTS_COUNT) {	try {	log.info("-----------------------------------------------------------------------");	
creating cassandra table 

int attempt = 0;	Throwable error = null;	String tableFullName = settings.getKeyspace() + "." + table;	String errorMsg = "Failed to create Cassandra table '" + tableFullName + "'";	while (attempt < CQL_EXECUTION_ATTEMPTS_COUNT) {	try {	log.info("-----------------------------------------------------------------------");	log.info("-----------------------------------------------------------------------\n\n" + settings.getTableDDLStatement(table) + "\n");	log.info("-----------------------------------------------------------------------");	session().execute(settings.getTableDDLStatement(table));	
cassandra table was successfully created 

String errorMsg = "Failed to create Cassandra table '" + tableFullName + "'";	while (attempt < CQL_EXECUTION_ATTEMPTS_COUNT) {	try {	log.info("-----------------------------------------------------------------------");	log.info("-----------------------------------------------------------------------\n\n" + settings.getTableDDLStatement(table) + "\n");	log.info("-----------------------------------------------------------------------");	session().execute(settings.getTableDDLStatement(table));	return;	}	catch (AlreadyExistsException ignored) {	
cassandra table already exist 

log.info("-----------------------------------------------------------------------");	session().execute(settings.getTableDDLStatement(table));	return;	}	catch (AlreadyExistsException ignored) {	return;	}	catch (Throwable e) {	if (!CassandraHelper.isHostsAvailabilityError(e) && !CassandraHelper.isKeyspaceAbsenceError(e)) throw new IgniteException(errorMsg, e);	if (CassandraHelper.isKeyspaceAbsenceError(e)) {	
failed to create cassandra table cause appropriate keyspace doesn t exist 

private void createTableIndexes(String table, KeyValuePersistenceSettings settings) {	List<String> indexDDLStatements = settings.getIndexDDLStatements(table);	if (indexDDLStatements == null || indexDDLStatements.isEmpty()) return;	int attempt = 0;	Throwable error = null;	String tableFullName = settings.getKeyspace() + "." + table;	String errorMsg = "Failed to create indexes for Cassandra table " + tableFullName;	while (attempt < CQL_EXECUTION_ATTEMPTS_COUNT) {	try {	log.info("-----------------------------------------------------------------------");	
creating indexes for cassandra table 

log.info(statement);	log.info("-----------------------------------------------------------------------");	session().execute(statement);	}	catch (AlreadyExistsException ignored) {	}	catch (Throwable e) {	if (!(e instanceof InvalidQueryException) || !"Index already exists".equals(e.getMessage())) throw new IgniteException(errorMsg, e);	}	}	
indexes for cassandra table were successfully created 

private void handleTableAbsenceError(String table, KeyValuePersistenceSettings settings) {	int hndNum = tblAbsenceHandlersCnt.incrementAndGet();	String tableFullName = settings.getKeyspace() + "." + table;	try {	synchronized (tblAbsenceHandlersCnt) {	if (hndNum != 0) {	
table absence problem detected another thread already fixed it 

private void handleTableAbsenceError(String table, KeyValuePersistenceSettings settings) {	int hndNum = tblAbsenceHandlersCnt.incrementAndGet();	String tableFullName = settings.getKeyspace() + "." + table;	try {	synchronized (tblAbsenceHandlersCnt) {	if (hndNum != 0) {	return;	}	
table absence problem detected trying to create table 

private void handlePreparedStatementClusterError(Throwable e) {	int hndNum = prepStatementHandlersCnt.incrementAndGet();	try {	synchronized (prepStatementHandlersCnt) {	if (hndNum != 0) {	
prepared statement cluster error detected another thread already fixed the problem 

private void handlePreparedStatementClusterError(Throwable e) {	int hndNum = prepStatementHandlersCnt.incrementAndGet();	try {	synchronized (prepStatementHandlersCnt) {	if (hndNum != 0) {	return;	}	
prepared statement cluster error detected refreshing cassandra session 

private void handlePreparedStatementClusterError(Throwable e) {	int hndNum = prepStatementHandlersCnt.incrementAndGet();	try {	synchronized (prepStatementHandlersCnt) {	if (hndNum != 0) {	return;	}	refresh();	
cassandra session refreshed 

private boolean handleHostsAvailabilityError(Throwable e, int attempt, String msg) {	if (attempt >= CQL_EXECUTION_ATTEMPTS_COUNT) {	
host availability problem detected number of cql execution attempts reached maximum exception will be thrown to upper execution layer 

private boolean handleHostsAvailabilityError(Throwable e, int attempt, String msg) {	if (attempt >= CQL_EXECUTION_ATTEMPTS_COUNT) {	throw msg == null ? new IgniteException(e) : new IgniteException(msg, e);	}	if (attempt == CQL_EXECUTION_ATTEMPTS_COUNT / 4  || attempt == CQL_EXECUTION_ATTEMPTS_COUNT / 2  || attempt == CQL_EXECUTION_ATTEMPTS_COUNT / 2 + CQL_EXECUTION_ATTEMPTS_COUNT / 4  || attempt == CQL_EXECUTION_ATTEMPTS_COUNT - 1) {	
host availability problem detected cql execution attempt refreshing cassandra session 

private boolean handleHostsAvailabilityError(Throwable e, int attempt, String msg) {	if (attempt >= CQL_EXECUTION_ATTEMPTS_COUNT) {	throw msg == null ? new IgniteException(e) : new IgniteException(msg, e);	}	if (attempt == CQL_EXECUTION_ATTEMPTS_COUNT / 4  || attempt == CQL_EXECUTION_ATTEMPTS_COUNT / 2  || attempt == CQL_EXECUTION_ATTEMPTS_COUNT / 2 + CQL_EXECUTION_ATTEMPTS_COUNT / 4  || attempt == CQL_EXECUTION_ATTEMPTS_COUNT - 1) {	refresh();	
cassandra session refreshed 

private boolean handleHostsAvailabilityError(Throwable e, int attempt, String msg) {	if (attempt >= CQL_EXECUTION_ATTEMPTS_COUNT) {	throw msg == null ? new IgniteException(e) : new IgniteException(msg, e);	}	if (attempt == CQL_EXECUTION_ATTEMPTS_COUNT / 4  || attempt == CQL_EXECUTION_ATTEMPTS_COUNT / 2  || attempt == CQL_EXECUTION_ATTEMPTS_COUNT / 2 + CQL_EXECUTION_ATTEMPTS_COUNT / 4  || attempt == CQL_EXECUTION_ATTEMPTS_COUNT - 1) {	refresh();	return true;	}	
host availability problem detected cql execution attempt sleeping extra milliseconds 

}	if (attempt == CQL_EXECUTION_ATTEMPTS_COUNT / 4  || attempt == CQL_EXECUTION_ATTEMPTS_COUNT / 2  || attempt == CQL_EXECUTION_ATTEMPTS_COUNT / 2 + CQL_EXECUTION_ATTEMPTS_COUNT / 4  || attempt == CQL_EXECUTION_ATTEMPTS_COUNT - 1) {	refresh();	return true;	}	try {	Thread.sleep(CQL_EXECUTION_ATTEMPT_MAX_TIMEOUT);	}	catch (InterruptedException ignored) {	}	
sleep completed 

========================= ignite sample_7028 =========================

IgniteCache<Integer, Integer> getCache = ignite.cache(cacheName);	final int NEW_NODE = SRVS + CLIENTS;	Ignite srv = startGrid(NEW_NODE);	awaitPartitionMapExchange();	try {	Integer key = primaryKey(srv.cache(cacheName));	Integer txKey = nearKey(srv.cache(cacheName));	srv.cache(cacheName).put(key, 1);	IgniteInternalFuture<?> stopFut = GridTestUtils.runAsync(new Callable<Void>() {	U.sleep(500);	
stop node 

final int NEW_NODE = SRVS + CLIENTS;	Ignite srv = startGrid(NEW_NODE);	awaitPartitionMapExchange();	try {	Integer key = primaryKey(srv.cache(cacheName));	Integer txKey = nearKey(srv.cache(cacheName));	srv.cache(cacheName).put(key, 1);	IgniteInternalFuture<?> stopFut = GridTestUtils.runAsync(new Callable<Void>() {	U.sleep(500);	stopGrid(NEW_NODE);	
node stopped 

IgniteCache<Integer, Integer> lockCache = ignite.cache(TX_CACHE1);	IgniteCache<Integer, Integer> getCache = ignite.cache(cacheName);	final int NEW_NODE = SRVS + CLIENTS;	Ignite srv = startGrid(NEW_NODE);	awaitPartitionMapExchange();	try {	Integer key = primaryKey(srv.cache(cacheName));	getCache.put(key, 1);	IgniteInternalFuture<?> stopFut = GridTestUtils.runAsync(new Callable<Void>() {	U.sleep(500);	
stop node 

IgniteCache<Integer, Integer> getCache = ignite.cache(cacheName);	final int NEW_NODE = SRVS + CLIENTS;	Ignite srv = startGrid(NEW_NODE);	awaitPartitionMapExchange();	try {	Integer key = primaryKey(srv.cache(cacheName));	getCache.put(key, 1);	IgniteInternalFuture<?> stopFut = GridTestUtils.runAsync(new Callable<Void>() {	U.sleep(500);	stopGrid(NEW_NODE);	
node stopped 

IgniteCache<Integer, Integer> txCache = ignite.cache(TX_CACHE1);	IgniteCache<Integer, Integer> getCache = ignite.cache(cacheName);	final int NEW_NODE = SRVS + CLIENTS;	Ignite srv = startGrid(NEW_NODE);	awaitPartitionMapExchange();	try {	Integer key = primaryKey(srv.cache(cacheName));	getCache.put(key, 1);	IgniteInternalFuture<?> stopFut = GridTestUtils.runAsync(new Callable<Void>() {	U.sleep(500);	
stop node 

IgniteCache<Integer, Integer> getCache = ignite.cache(cacheName);	final int NEW_NODE = SRVS + CLIENTS;	Ignite srv = startGrid(NEW_NODE);	awaitPartitionMapExchange();	try {	Integer key = primaryKey(srv.cache(cacheName));	getCache.put(key, 1);	IgniteInternalFuture<?> stopFut = GridTestUtils.runAsync(new Callable<Void>() {	U.sleep(500);	stopGrid(NEW_NODE);	
node stopped 

public void testMultithreaded() throws Exception {	fail("https: final AtomicBoolean finished = new AtomicBoolean();	final int NEW_NODE = SRVS + CLIENTS;	final AtomicInteger stopIdx = new AtomicInteger();	IgniteInternalFuture<?> restartFut = GridTestUtils.runMultiThreadedAsync(new Callable<Object>() {	int idx = stopIdx.getAndIncrement();	int node = NEW_NODE + idx;	while (!finished.get()) {	
start node 

public void testMultithreaded() throws Exception {	fail("https: final AtomicBoolean finished = new AtomicBoolean();	final int NEW_NODE = SRVS + CLIENTS;	final AtomicInteger stopIdx = new AtomicInteger();	IgniteInternalFuture<?> restartFut = GridTestUtils.runMultiThreadedAsync(new Callable<Object>() {	int idx = stopIdx.getAndIncrement();	int node = NEW_NODE + idx;	while (!finished.get()) {	startGrid(node);	U.sleep(300);	
stop node 

========================= ignite sample_998 =========================

private void bind(final IgfsIpcEndpointConfiguration endpointCfg, final boolean mgmt) throws IgniteCheckedException {	if (srvrs == null) srvrs = new ConcurrentLinkedQueue<>();	IgfsServer ipcSrv = new IgfsServer(igfsCtx, endpointCfg, mgmt);	try {	ipcSrv.start();	srvrs.add(ipcSrv);	}	catch (IpcEndpointBindException ignored) {	int port = ipcSrv.getIpcServerEndpoint().getPort();	String portMsg = port != -1 ? " Failed to bind to port (is port already in use?): " + port : "";	
failed to start igfs management endpoint will retry every s 

========================= ignite sample_4741 =========================

public static void main(String[] args) {	try {	
cassandra load tests execution started 

public static void main(String[] args) {	try {	LoadTestDriver driver = new CassandraDirectPersistenceLoadTest();	driver.runTest("WRITE", WriteWorker.class, WriteWorker.LOGGER_NAME);	driver.runTest("BULK_WRITE", BulkWriteWorker.class, BulkWriteWorker.LOGGER_NAME);	driver.runTest("READ", ReadWorker.class, ReadWorker.LOGGER_NAME);	driver.runTest("BULK_READ", BulkReadWorker.class, BulkReadWorker.LOGGER_NAME);	
cassandra load tests execution completed 

public static void main(String[] args) {	try {	LoadTestDriver driver = new CassandraDirectPersistenceLoadTest();	driver.runTest("WRITE", WriteWorker.class, WriteWorker.LOGGER_NAME);	driver.runTest("BULK_WRITE", BulkWriteWorker.class, BulkWriteWorker.LOGGER_NAME);	driver.runTest("READ", ReadWorker.class, ReadWorker.LOGGER_NAME);	driver.runTest("BULK_READ", BulkReadWorker.class, BulkReadWorker.LOGGER_NAME);	}	catch (Throwable e) {	
cassandra load tests execution failed 

========================= ignite sample_6951 =========================

private void checkPutGet(IgniteCache<Object, Object> cache, @Nullable IgniteTransactions txs, @Nullable IgniteTransactions txs, TransactionConcurrency concurrency, TransactionIsolation isolation) {	
check cache 

========================= ignite sample_1001 =========================

public void testCircularReference() throws Exception {	IgniteCache c = keepBinaryCache();	TestReferenceObject obj1 = new TestReferenceObject();	obj1.obj = new TestReferenceObject(obj1);	c.put(1, obj1);	BinaryObject po = (BinaryObject)c.get(1);	String str = po.toString();	
tostring 

private void checkTransform(Integer key) throws Exception {	
transform 

========================= ignite sample_1847 =========================

public IgniteInternalFuture<Object> delete(IgfsEntryInfo fileInfo) {	if (!fileInfo.isFile()) {	
cannot delete content of not data file 

private void processAckMessage(UUID nodeId, IgfsAckMessage ackMsg) {	try {	ackMsg.finishUnmarshal(igfsCtx.kernalContext().config().getMarshaller(), null);	}	catch (IgniteCheckedException e) {	
failed to unmarshal message will ignore 

private void markWaitingLastAck() {	awaitingLast = true;	
marked write completion future as awaiting last ack 

========================= ignite sample_4716 =========================

if (task instanceof HadoopRunnableTask) {	final HadoopTaskInfo i = ((HadoopRunnableTask)task).taskInfo();	workerName = "Hadoop-task-" + i.jobId() + "-" + i.type() + "-" + i.taskNumber() + "-" + i.attempt();	}	else workerName = task.toString();	GridWorker w = new GridWorker(igniteInstanceName, workerName, log, lsnr) {	try {	task.call();	}	catch (Exception e) {	
failed to execute task 

========================= ignite sample_7268 =========================

public void testLockUnlock() throws Exception {	for (int i = 0; i < GRID_CNT; i++) {	Ignite ignite = ignite(i);	
check node 

========================= ignite sample_1996 =========================

public void start( final IgniteConfiguration cfg, ExecutorService utilityCachePool, final ExecutorService execSvc, final ExecutorService svcExecSvc, final ExecutorService sysExecSvc, final StripedExecutor stripedExecSvc, ExecutorService p2pExecSvc, ExecutorService mgmtExecSvc, ExecutorService igfsExecSvc, StripedExecutor dataStreamExecSvc, ExecutorService restExecSvc, ExecutorService affExecSvc, @Nullable ExecutorService idxExecSvc, @Nullable ExecutorService idxExecSvc, IgniteStripedThreadPoolExecutor callbackExecSvc, ExecutorService qryExecSvc, ExecutorService schemaExecSvc, @Nullable final Map<String, ? extends ExecutorService> customExecSvcs, @Nullable final Map<String, ? extends ExecutorService> customExecSvcs, GridAbsClosure errHnd ) throws IgniteCheckedException {	gw.compareAndSet(null, new GridKernalGatewayImpl(cfg.getIgniteInstanceName()));	GridKernalGateway gw = this.gw.get();	gw.writeLock();	try {	switch (gw.getState()) {	case STARTED: {	
grid has already been started ignored 

public void start( final IgniteConfiguration cfg, ExecutorService utilityCachePool, final ExecutorService execSvc, final ExecutorService svcExecSvc, final ExecutorService sysExecSvc, final StripedExecutor stripedExecSvc, ExecutorService p2pExecSvc, ExecutorService mgmtExecSvc, ExecutorService igfsExecSvc, StripedExecutor dataStreamExecSvc, ExecutorService restExecSvc, ExecutorService affExecSvc, @Nullable ExecutorService idxExecSvc, @Nullable ExecutorService idxExecSvc, IgniteStripedThreadPoolExecutor callbackExecSvc, ExecutorService qryExecSvc, ExecutorService schemaExecSvc, @Nullable final Map<String, ? extends ExecutorService> customExecSvcs, @Nullable final Map<String, ? extends ExecutorService> customExecSvcs, GridAbsClosure errHnd ) throws IgniteCheckedException {	gw.compareAndSet(null, new GridKernalGatewayImpl(cfg.getIgniteInstanceName()));	GridKernalGateway gw = this.gw.get();	gw.writeLock();	try {	switch (gw.getState()) {	case STARTED: {	return;	}	case STARTING: {	
grid is already in process of being started ignored 

startProcessor(new GridMarshallerMappingProcessor(ctx));	for (PluginProvider provider : ctx.plugins().allProviders()) {	ctx.add(new GridPluginComponent(provider));	provider.start(ctx.plugins().pluginContextForProvider(provider));	}	if (ctx.config().getPlatformConfiguration() != null) startProcessor(new PlatformPluginProcessor(ctx));	ctx.cluster().initDiagnosticListeners();	fillNodeAttributes(clusterProc.updateNotifierEnabled());	}	catch (Throwable e) {	
exception during start processors node will be stopped and close connections 

}	checkPhysicalRam();	suggestOptimizations(cfg);	ctx.performance().addAll(JvmConfigurationSuggestions.getSuggestions());	ctx.performance().addAll(OsConfigurationSuggestions.getSuggestions());	DiscoveryLocalJoinData joinData = ctx.discovery().localJoin();	IgniteInternalFuture<Boolean> transitionWaitFut = joinData.transitionWaitFuture();	boolean active;	if (transitionWaitFut != null) {	if (log.isInfoEnabled()) {	
join cluster while cluster state transition is in progress waiting when transition finish 

for (GridComponent comp : ctx) {	if (comp instanceof GridDiscoveryManager) continue;	if (comp instanceof GridIoManager) continue;	if (comp instanceof GridPluginComponent) continue;	if (!skipDaemon(comp)) {	try {	comp.onKernalStart(active);	}	catch (IgniteNeedReconnectException e) {	assert ctx.discovery().reconnectSupported();	
failed to start node components on node start will wait for reconnect 

}	}	for (PluginProvider provider : ctx.plugins().allProviders()) provider.onIgniteStart();	if (recon) reconnectState.waitFirstReconnect();	mBeansMgr.registerAllMBeans(utilityCachePool, execSvc, svcExecSvc, sysExecSvc, stripedExecSvc, p2pExecSvc, mgmtExecSvc, igfsExecSvc, dataStreamExecSvc, restExecSvc, affExecSvc, idxExecSvc, callbackExecSvc, qryExecSvc, schemaExecSvc, customExecSvcs);	notifyLifecycleBeans(AFTER_NODE_START);	}	catch (Throwable e) {	IgniteSpiVersionCheckException verCheckErr = X.cause(e, IgniteSpiVersionCheckException.class);	if (verCheckErr != null) U.error(log, verCheckErr.getMessage());	
grid startup routine has been interrupted will rollback 

}	}	for (PluginProvider provider : ctx.plugins().allProviders()) provider.onIgniteStart();	if (recon) reconnectState.waitFirstReconnect();	mBeansMgr.registerAllMBeans(utilityCachePool, execSvc, svcExecSvc, sysExecSvc, stripedExecSvc, p2pExecSvc, mgmtExecSvc, igfsExecSvc, dataStreamExecSvc, restExecSvc, affExecSvc, idxExecSvc, callbackExecSvc, qryExecSvc, schemaExecSvc, customExecSvcs);	notifyLifecycleBeans(AFTER_NODE_START);	}	catch (Throwable e) {	IgniteSpiVersionCheckException verCheckErr = X.cause(e, IgniteSpiVersionCheckException.class);	if (verCheckErr != null) U.error(log, verCheckErr.getMessage());	
got exception while starting will rollback startup routine 

boolean mandatory = cfg.getHadoopConfiguration() != null;	if (mandatory) {	if (cfg.isPeerClassLoadingEnabled()) throw new IgniteCheckedException("Hadoop module cannot be used with peer class loading enabled " + "(set IgniteConfiguration.peerClassLoadingEnabled to \"false\").");	HadoopProcessorAdapter res = IgniteComponentType.HADOOP.createIfInClassPath(ctx, true);	res.validateEnvironment();	return res;	}	else {	HadoopProcessorAdapter cmp = null;	if (!ctx.hadoopHelper().isNoOp() && cfg.isPeerClassLoadingEnabled()) {	
hadoop module is found in classpath but will not be started because peer class loading is enabled set igniteconfiguration peerclassloadingenabled to if you want to use hadoop module 

private void fillNodeAttributes(boolean notifyEnabled) throws IgniteCheckedException {	ctx.addNodeAttribute(ATTR_DATA_STREAMER_POOL_SIZE, configuration().getDataStreamerThreadPoolSize());	final String[] incProps = cfg.getIncludeProperties();	try {	for (Map.Entry<String, String> sysEntry : System.getenv().entrySet()) {	String name = sysEntry.getKey();	if (incProps == null || U.containsStringArray(incProps, name, true) || U.isVisorNodeStartProperty(name) || U.isVisorRequiredProperty(name)) ctx.addNodeAttribute(name, sysEntry.getValue());	}	
added environment properties to node attributes 

try {	for (Map.Entry<Object, Object> e : snapshot().entrySet()) {	String key = (String)e.getKey();	if (incProps == null || U.containsStringArray(incProps, key, true) || U.isVisorRequiredProperty(key)) {	Object val = ctx.nodeAttribute(key);	if (val != null && !val.equals(e.getValue())) U.warn(log, "System property will override environment variable with the same name: " + key);	ctx.addNodeAttribute(key, e.getValue());	}	}	ctx.addNodeAttribute(IgniteNodeAttributes.ATTR_UPDATE_NOTIFIER_ENABLED, notifyEnabled);	
added system properties to node attributes 

}	}	ctx.addNodeAttribute(IgniteNodeAttributes.ATTR_UPDATE_NOTIFIER_ENABLED, notifyEnabled);	}	catch (SecurityException e) {	throw new IgniteCheckedException("Failed to add system properties to node attributes due to security " + "violation: " + e.getMessage());	}	String ips = F.concat(U.allLocalIps(), ", ");	String macs = F.concat(U.allLocalMACs(), ", ");	if (log.isInfoEnabled()) {	
non loopback local ips n a 

}	}	ctx.addNodeAttribute(IgniteNodeAttributes.ATTR_UPDATE_NOTIFIER_ENABLED, notifyEnabled);	}	catch (SecurityException e) {	throw new IgniteCheckedException("Failed to add system properties to node attributes due to security " + "violation: " + e.getMessage());	}	String ips = F.concat(U.allLocalIps(), ", ");	String macs = F.concat(U.allLocalMACs(), ", ");	if (log.isInfoEnabled()) {	
enabled local macs n a 

}	ctx.addNodeAttribute(IgniteNodeAttributes.ATTR_UPDATE_NOTIFIER_ENABLED, notifyEnabled);	}	catch (SecurityException e) {	throw new IgniteCheckedException("Failed to add system properties to node attributes due to security " + "violation: " + e.getMessage());	}	String ips = F.concat(U.allLocalIps(), ", ");	String macs = F.concat(U.allLocalMACs(), ", ");	if (log.isInfoEnabled()) {	}	
ignite is starting on loopback address only nodes on the same physical computer can participate in topology ignite is starting on loopback address 

addSpiAttributes(cfg.getCollisionSpi());	addSpiAttributes(cfg.getDiscoverySpi());	addSpiAttributes(cfg.getFailoverSpi());	addSpiAttributes(cfg.getCommunicationSpi());	addSpiAttributes(cfg.getEventStorageSpi());	addSpiAttributes(cfg.getCheckpointSpi());	addSpiAttributes(cfg.getLoadBalancingSpi());	addSpiAttributes(cfg.getDeploymentSpi());	if (cfg.getUserAttributes() != null) {	for (Map.Entry<String, ?> e : cfg.getUserAttributes().entrySet()) {	
user or internal attribute has the same name as environment or system property and will take precedence 

private void startManager(GridManager mgr) throws IgniteCheckedException {	ctx.add(mgr);	try {	if (!skipDaemon(mgr)) mgr.start();	}	catch (IgniteCheckedException e) {	
failed to start manager 

private void ackConfigUrl() {	assert log != null;	
config url n a 

private void ackOsInfo() {	assert log != null;	if (log.isQuiet()) U.quiet(false, "OS: " + U.osString());	if (log.isInfoEnabled()) {	
os 

private void ackOsInfo() {	assert log != null;	if (log.isQuiet()) U.quiet(false, "OS: " + U.osString());	if (log.isInfoEnabled()) {	
os user user name 

private void ackLanguageRuntime() {	assert log != null;	if (log.isQuiet()) U.quiet(false, "VM information: " + U.jdkString());	if (log.isInfoEnabled()) {	
language runtime 

private void ackLanguageRuntime() {	assert log != null;	if (log.isQuiet()) U.quiet(false, "VM information: " + U.jdkString());	if (log.isInfoEnabled()) {	
vm information 

private void ackLanguageRuntime() {	assert log != null;	if (log.isQuiet()) U.quiet(false, "VM information: " + U.jdkString());	if (log.isInfoEnabled()) {	
vm total memory GB 

private void stop0(boolean cancel) {	gw.compareAndSet(null, new GridKernalGatewayImpl(igniteInstanceName));	GridKernalGateway gw = this.gw.get();	if (stopGuard.compareAndSet(false, true)) {	boolean firstStop = false;	GridKernalState state = gw.getState();	if (state == STARTED || state == DISCONNECTED) firstStop = true;	else if (state == STARTING) U.warn(log, "Attempt to stop starting grid. This operation " + "cannot be guaranteed to be successful.");	if (firstStop) {	
notifying lifecycle beans 

notifyLifecycleBeansEx(LifecycleEventType.BEFORE_NODE_STOP);	}	List<GridComponent> comps = ctx.components();	for (ListIterator<GridComponent> it = comps.listIterator(comps.size()); it.hasPrevious(); ) {	GridComponent comp = it.previous();	try {	if (!skipDaemon(comp)) comp.onKernalStop(cancel);	}	catch (Throwable e) {	errOnStop = true;	
failed to pre stop processor 

gw.writeUnlock();	}	GridCacheProcessor cache = ctx.cache();	if (cache != null) cache.blockGateways();	if (!mBeansMgr.unregisterAllMBeans()) errOnStop = true;	for (ListIterator<GridComponent> it = comps.listIterator(comps.size()); it.hasPrevious(); ) {	GridComponent comp = it.previous();	try {	if (!skipDaemon(comp)) {	comp.stop(cancel);	
component stopped 

if (!mBeansMgr.unregisterAllMBeans()) errOnStop = true;	for (ListIterator<GridComponent> it = comps.listIterator(comps.size()); it.hasPrevious(); ) {	GridComponent comp = it.previous();	try {	if (!skipDaemon(comp)) {	comp.stop(cancel);	}	}	catch (Throwable e) {	errOnStop = true;	
failed to stop component ignoring 

try {	U.onGridStop();	}	catch (InterruptedException ignored) {	Thread.currentThread().interrupt();	}	}	else {	if (log.isDebugEnabled()) {	if (gw.getState() == STOPPED) log.debug("Grid is already stopped. Nothing to do.");	
grid is being stopped by another thread aborting this stop sequence allowing other thread to finish 

private void ackDaemon() {	assert log != null;	
daemon mode on off 

private void ackSpis() {	assert log != null;	if (log.isDebugEnabled()) {	log.debug("+-------------+");	
start spi list 

private void ackSpis() {	assert log != null;	if (log.isDebugEnabled()) {	log.debug("+-------------+");	log.debug("+-------------+");	
grid checkpoint spi 

private void ackSpis() {	assert log != null;	if (log.isDebugEnabled()) {	log.debug("+-------------+");	log.debug("+-------------+");	
grid collision spi 

private void ackSpis() {	assert log != null;	if (log.isDebugEnabled()) {	log.debug("+-------------+");	log.debug("+-------------+");	
grid communication spi 

private void ackSpis() {	assert log != null;	if (log.isDebugEnabled()) {	log.debug("+-------------+");	log.debug("+-------------+");	
grid deployment spi 

private void ackSpis() {	assert log != null;	if (log.isDebugEnabled()) {	log.debug("+-------------+");	log.debug("+-------------+");	
grid discovery spi 

private void ackSpis() {	assert log != null;	if (log.isDebugEnabled()) {	log.debug("+-------------+");	log.debug("+-------------+");	
grid event storage spi 

private void ackSpis() {	assert log != null;	if (log.isDebugEnabled()) {	log.debug("+-------------+");	log.debug("+-------------+");	
grid failover spi 

private void ackSpis() {	assert log != null;	if (log.isDebugEnabled()) {	log.debug("+-------------+");	log.debug("+-------------+");	
grid load balancing spi 

private void ackVmArguments(RuntimeMXBean rtBean) {	assert log != null;	if (log.isInfoEnabled() && S.INCLUDE_SENSITIVE) {	log.info("IGNITE_HOME=" + cfg.getIgniteHome());	
vm arguments 

private void ackClassPaths(RuntimeMXBean rtBean) {	assert log != null;	if (log.isDebugEnabled()) {	try {	
boot class path 

private void ackClassPaths(RuntimeMXBean rtBean) {	assert log != null;	if (log.isDebugEnabled()) {	try {	
class path 

private void ackClassPaths(RuntimeMXBean rtBean) {	assert log != null;	if (log.isDebugEnabled()) {	try {	
library path 

}	}	for (GridCacheContext cctx : ctx.cache().context().cacheContexts()) {	cctx.gate().writeLock();	cctx.gate().writeUnlock();	}	ctx.gateway().writeLock();	ctx.gateway().writeUnlock();	if (err != null) {	reconnectFut.onDone(err);	
failed to reconnect will stop node 

final GridFutureAdapter reconnectDone = reconnectState.reconnectDone;	curReconnectFut.listen(new CI1<IgniteInternalFuture<?>>() {	try {	Object res = fut.get();	if (res == STOP_RECONNECT) return;	ctx.gateway().onReconnected();	reconnectState.firstReconnectFut.onDone();	}	catch (IgniteCheckedException e) {	if (!X.hasCause(e, IgniteNeedReconnectException.class, IgniteClientDisconnectedCheckedException.class)) {	
failed to reconnect will stop node 

ClusterNode locNode = discoMrg != null ? discoMrg.localNode() : null;	if (ctx != null && discoMrg != null && locNode != null) {	boolean client = ctx.clientNode();	UUID routerId = locNode instanceof TcpDiscoveryNode ? ((TcpDiscoveryNode)locNode).clientRouterNodeId() : null;	U.warn(ctx.cluster().diagnosticLog(), "Dumping debug info for node [id=" + locNode.id() + ", name=" + ctx.igniteInstanceName() + ", order=" + locNode.order() + ", topVer=" + discoMrg.topologyVersion() + ", client=" + client + (client && routerId != null ? ", routerId=" + routerId : "") + ']');	ctx.cache().context().exchange().dumpDebugInfo(null);	}	else U.warn(log, "Dumping debug info for node, context is not initialized [name=" + igniteInstanceName + ']');	}	catch (Exception e) {	
failed to dump debug info for node 

private <T> void registerMBean(String grp, String name, T impl, Class<T> itf) throws IgniteCheckedException {	assert !U.IGNITE_MBEANS_DISABLED;	try {	ObjectName objName = U.registerMBean( cfg.getMBeanServer(), cfg.getIgniteInstanceName(), grp, name, impl, itf);	
registered mbean 

private boolean unregisterMBean(ObjectName mbean) {	assert !U.IGNITE_MBEANS_DISABLED;	try {	cfg.getMBeanServer().unregisterMBean(mbean);	
unregistered mbean 

private boolean unregisterMBean(ObjectName mbean) {	assert !U.IGNITE_MBEANS_DISABLED;	try {	cfg.getMBeanServer().unregisterMBean(mbean);	return true;	}	catch (JMException e) {	
failed to unregister mbean 

========================= ignite sample_5039 =========================

public void testParseClientHandshake() throws Exception {	for (int splitPos = 1; splitPos < 5; splitPos++) {	
checking split position 

========================= ignite sample_263 =========================

private void cleanupQueue() {	long now = U.currentTimeMillis();	long queueOversize = evts.sizex() - expireCnt;	for (int i = 0; i < queueOversize && evts.sizex() > expireCnt; i++) {	Event expired = evts.poll();	
event expired by count 

long queueOversize = evts.sizex() - expireCnt;	for (int i = 0; i < queueOversize && evts.sizex() > expireCnt; i++) {	Event expired = evts.poll();	}	while (true) {	ConcurrentLinkedDeque8.Node<Event> node = evts.peekx();	if (node == null) break;	Event evt = node.item();	if (evt == null) continue;	if (now - evt.timestamp() < expireAgeMs) break;	
event expired by age 

========================= ignite sample_5717 =========================

if (locked != null) unlockEntries(locked, req.topologyVersion());	if (deleted != null) {	assert !deleted.isEmpty();	assert ctx.deferredDelete() : this;	for (IgniteBiTuple<GridDhtCacheEntry, GridCacheVersion> e : deleted) ctx.onDeferredDelete(e.get1(), e.get2());	}	if (ctx.shared().wal() != null) ctx.shared().wal().fsync(null);	}	}	catch (GridDhtInvalidPartitionException ignore) {	
caught invalid partition exception for cache entry will remap update request 

assert ctx.deferredDelete() : this;	for (IgniteBiTuple<GridDhtCacheEntry, GridCacheVersion> e : deleted) ctx.onDeferredDelete(e.get1(), e.get2());	}	if (ctx.shared().wal() != null) ctx.shared().wal().fsync(null);	}	}	catch (GridDhtInvalidPartitionException ignore) {	res.remapTopologyVersion(ctx.topology().lastTopologyChangeVersion());	}	catch (Throwable e) {	
unexpected exception during cache update 

Long updateIdx = req.updateCounter(i);	GridCacheOperation op = entryProcessor != null ? TRANSFORM : (val != null) ? UPDATE : DELETE;	long ttl = req.ttl(i);	long expireTime = req.conflictExpireTime(i);	GridCacheUpdateAtomicResult updRes = entry.innerUpdate( ver, nodeId, nodeId, op, op == TRANSFORM ? entryProcessor : val, op == TRANSFORM ? req.invokeArguments() : null, && writeThrough() && !req.skipStore(), req.keepBinary(), req.topologyVersion(), CU.empty0(), replicate ? DR_BACKUP : DR_NONE, ttl, expireTime, req.conflictVersion(i), false, intercept, req.subjectId(), taskName, prevVal, updateIdx, null);	if (updRes.removeVersion() != null) ctx.onDeferredDelete(entry, updRes.removeVersion());	entry.onUnlock();	break;	}	catch (GridCacheEntryRemovedException ignored) {	
got removed entry while updating backup value will retry 

}	catch (GridCacheEntryRemovedException ignored) {	entry = null;	}	finally {	if (entry != null) ctx.evicts().touch(entry, req.topologyVersion());	}	}	}	catch (NodeStoppingException e){	
failed to update key on backup local node is stopping 

}	catch (NodeStoppingException e){	return;	}	catch (GridDhtInvalidPartitionException ignored) {	}	catch (IgniteCheckedException|RuntimeException e) {	if(e instanceof RuntimeException && !X.hasCause(e, IgniteOutOfMemoryException.class)) throw (RuntimeException)e;	IgniteCheckedException err = new IgniteCheckedException("Failed to update key on backup node: " + key, e);	if (nearRes != null) nearRes.addFailedKey(key, err);	
failed to update key on backup node 

========================= ignite sample_3786 =========================

GridNearCacheEntry entry = savedEntries.get(info.key());	if (entry == null) entry = cache().entryExx(info.key(), topVer);	entry.loadedValue(tx, nodeId, info.value(), atomic ? info.version() : ver, info.version(), info.ttl(), info.expireTime(), true, !deserializeBinary, topVer, subjId);	}	CacheObject val = info.value();	KeyCacheObject key = info.key();	assert skipVals == (info.value() == null);	cctx.addResult(map, key, val, skipVals, keepCacheObjects, deserializeBinary, false, needVer ? info.version() : null, 0, 0);	}	catch (GridCacheEntryRemovedException ignore) {	
got removed entry while processing get response will not retry 

========================= ignite sample_3856 =========================

public static void kill(String igniteInstanceName) {	A.notNull(igniteInstanceName, "igniteInstanceName");	IgniteProcessProxy proxy = gridProxies.get(igniteInstanceName);	if (proxy == null) return;	if (proxy == null) return;	try {	proxy.getProcess().kill();	}	catch (Exception e) {	
exception while killing 

public static void killAll() {	for (IgniteProcessProxy ignite : gridProxies.values()) {	try {	ignite.getProcess().kill();	}	catch (Exception e) {	
killing failed 

========================= ignite sample_2744 =========================

for (int i = 0; i < 50; i++) {	int node = rnd.nextInt(0, NODES);	final Ignite ignite = ignite(node);	info("Running iteration on the node [idx=" + node + ", nodeId=" + ignite.cluster().localNode().id() + ']');	IgniteInternalFuture<?> fut = GridTestUtils.runAsync(new Callable<Void>() {	Thread.currentThread().setName("create-thread");	try {	ignite.createCache(new CacheConfiguration<>(CACHE_NAME));	}	catch (CacheException | IllegalStateException e) {	
expected error 

========================= ignite sample_849 =========================

map.put(nearKey(cache), 1);	map.put(primaryKey(cache), 1);	TestCommunicationSpi spi = (TestCommunicationSpi)ignite(0).configuration().getCommunicationSpi();	spi.sndFail = true;	try {	try {	cache.putAll(map);	fail("Put should fail.");	}	catch (CacheException e) {	
expected exception 

========================= ignite sample_1312 =========================

public static void setUpClass() {	if (CassandraHelper.useEmbeddedCassandra()) {	try {	CassandraHelper.startEmbeddedCassandra(LOGGER);	}	catch (Throwable e) {	throw new RuntimeException("Failed to start embedded Cassandra instance", e);	}	}	
testing admin connection to cassandra 

public static void setUpClass() {	if (CassandraHelper.useEmbeddedCassandra()) {	try {	CassandraHelper.startEmbeddedCassandra(LOGGER);	}	catch (Throwable e) {	throw new RuntimeException("Failed to start embedded Cassandra instance", e);	}	}	CassandraHelper.testAdminConnection();	
testing regular connection to cassandra 

if (CassandraHelper.useEmbeddedCassandra()) {	try {	CassandraHelper.startEmbeddedCassandra(LOGGER);	}	catch (Throwable e) {	throw new RuntimeException("Failed to start embedded Cassandra instance", e);	}	}	CassandraHelper.testAdminConnection();	CassandraHelper.testRegularConnection();	
dropping all artifacts from previous tests execution session 

try {	CassandraHelper.startEmbeddedCassandra(LOGGER);	}	catch (Throwable e) {	throw new RuntimeException("Failed to start embedded Cassandra instance", e);	}	}	CassandraHelper.testAdminConnection();	CassandraHelper.testRegularConnection();	CassandraHelper.dropTestKeyspaces();	
start tests execution 

try {	CassandraHelper.dropTestKeyspaces();	}	finally {	CassandraHelper.releaseCassandraResources();	if (CassandraHelper.useEmbeddedCassandra()) {	try {	CassandraHelper.stopEmbeddedCassandra();	}	catch (Throwable e) {	
failed to stop embedded cassandra instance 

Collection<Long> fakeLongKeys = TestsHelper.getKeys(longEntries);	fakeLongKeys.add(-1L);	fakeLongKeys.add(-2L);	fakeLongKeys.add(-3L);	fakeLongKeys.add(-4L);	Collection<String> fakeStrKeys = TestsHelper.getKeys(strEntries);	fakeStrKeys.add("-1");	fakeStrKeys.add("-2");	fakeStrKeys.add("-3");	fakeStrKeys.add("-4");	
running primitive strategy write tests 

Collection<Long> fakeLongKeys = TestsHelper.getKeys(longEntries);	fakeLongKeys.add(-1L);	fakeLongKeys.add(-2L);	fakeLongKeys.add(-3L);	fakeLongKeys.add(-4L);	Collection<String> fakeStrKeys = TestsHelper.getKeys(strEntries);	fakeStrKeys.add("-1");	fakeStrKeys.add("-2");	fakeStrKeys.add("-3");	fakeStrKeys.add("-4");	
running single write operation tests 

fakeLongKeys.add(-2L);	fakeLongKeys.add(-3L);	fakeLongKeys.add(-4L);	Collection<String> fakeStrKeys = TestsHelper.getKeys(strEntries);	fakeStrKeys.add("-1");	fakeStrKeys.add("-2");	fakeStrKeys.add("-3");	fakeStrKeys.add("-4");	store1.write(longEntries.iterator().next());	store2.write(strEntries.iterator().next());	
single write operation tests passed 

fakeLongKeys.add(-2L);	fakeLongKeys.add(-3L);	fakeLongKeys.add(-4L);	Collection<String> fakeStrKeys = TestsHelper.getKeys(strEntries);	fakeStrKeys.add("-1");	fakeStrKeys.add("-2");	fakeStrKeys.add("-3");	fakeStrKeys.add("-4");	store1.write(longEntries.iterator().next());	store2.write(strEntries.iterator().next());	
running bulk write operation tests 

fakeLongKeys.add(-4L);	Collection<String> fakeStrKeys = TestsHelper.getKeys(strEntries);	fakeStrKeys.add("-1");	fakeStrKeys.add("-2");	fakeStrKeys.add("-3");	fakeStrKeys.add("-4");	store1.write(longEntries.iterator().next());	store2.write(strEntries.iterator().next());	store1.writeAll(longEntries);	store2.writeAll(strEntries);	
bulk write operation tests passed 

fakeLongKeys.add(-4L);	Collection<String> fakeStrKeys = TestsHelper.getKeys(strEntries);	fakeStrKeys.add("-1");	fakeStrKeys.add("-2");	fakeStrKeys.add("-3");	fakeStrKeys.add("-4");	store1.write(longEntries.iterator().next());	store2.write(strEntries.iterator().next());	store1.writeAll(longEntries);	store2.writeAll(strEntries);	
primitive strategy write tests passed 

fakeLongKeys.add(-4L);	Collection<String> fakeStrKeys = TestsHelper.getKeys(strEntries);	fakeStrKeys.add("-1");	fakeStrKeys.add("-2");	fakeStrKeys.add("-3");	fakeStrKeys.add("-4");	store1.write(longEntries.iterator().next());	store2.write(strEntries.iterator().next());	store1.writeAll(longEntries);	store2.writeAll(strEntries);	
running primitive strategy read tests 

fakeLongKeys.add(-4L);	Collection<String> fakeStrKeys = TestsHelper.getKeys(strEntries);	fakeStrKeys.add("-1");	fakeStrKeys.add("-2");	fakeStrKeys.add("-3");	fakeStrKeys.add("-4");	store1.write(longEntries.iterator().next());	store2.write(strEntries.iterator().next());	store1.writeAll(longEntries);	store2.writeAll(strEntries);	
running single read operation tests 

fakeLongKeys.add(-4L);	Collection<String> fakeStrKeys = TestsHelper.getKeys(strEntries);	fakeStrKeys.add("-1");	fakeStrKeys.add("-2");	fakeStrKeys.add("-3");	fakeStrKeys.add("-4");	store1.write(longEntries.iterator().next());	store2.write(strEntries.iterator().next());	store1.writeAll(longEntries);	store2.writeAll(strEntries);	
running real keys read tests 

fakeStrKeys.add("-3");	fakeStrKeys.add("-4");	store1.write(longEntries.iterator().next());	store2.write(strEntries.iterator().next());	store1.writeAll(longEntries);	store2.writeAll(strEntries);	Long longVal = (Long)store1.load(longEntries.iterator().next().getKey());	if (!longEntries.iterator().next().getValue().equals(longVal)) throw new RuntimeException("Long values were incorrectly deserialized from Cassandra");	String strVal = (String)store2.load(strEntries.iterator().next().getKey());	if (!strEntries.iterator().next().getValue().equals(strVal)) throw new RuntimeException("String values were incorrectly deserialized from Cassandra");	
running fake keys read tests 

store1.writeAll(longEntries);	store2.writeAll(strEntries);	Long longVal = (Long)store1.load(longEntries.iterator().next().getKey());	if (!longEntries.iterator().next().getValue().equals(longVal)) throw new RuntimeException("Long values were incorrectly deserialized from Cassandra");	String strVal = (String)store2.load(strEntries.iterator().next().getKey());	if (!strEntries.iterator().next().getValue().equals(strVal)) throw new RuntimeException("String values were incorrectly deserialized from Cassandra");	longVal = (Long)store1.load(-1L);	if (longVal != null) throw new RuntimeException("Long value with fake key '-1' was found in Cassandra");	strVal = (String)store2.load("-1");	if (strVal != null) throw new RuntimeException("String value with fake key '-1' was found in Cassandra");	
single read operation tests passed 

store1.writeAll(longEntries);	store2.writeAll(strEntries);	Long longVal = (Long)store1.load(longEntries.iterator().next().getKey());	if (!longEntries.iterator().next().getValue().equals(longVal)) throw new RuntimeException("Long values were incorrectly deserialized from Cassandra");	String strVal = (String)store2.load(strEntries.iterator().next().getKey());	if (!strEntries.iterator().next().getValue().equals(strVal)) throw new RuntimeException("String values were incorrectly deserialized from Cassandra");	longVal = (Long)store1.load(-1L);	if (longVal != null) throw new RuntimeException("Long value with fake key '-1' was found in Cassandra");	strVal = (String)store2.load("-1");	if (strVal != null) throw new RuntimeException("String value with fake key '-1' was found in Cassandra");	
running bulk read operation tests 

store1.writeAll(longEntries);	store2.writeAll(strEntries);	Long longVal = (Long)store1.load(longEntries.iterator().next().getKey());	if (!longEntries.iterator().next().getValue().equals(longVal)) throw new RuntimeException("Long values were incorrectly deserialized from Cassandra");	String strVal = (String)store2.load(strEntries.iterator().next().getKey());	if (!strEntries.iterator().next().getValue().equals(strVal)) throw new RuntimeException("String values were incorrectly deserialized from Cassandra");	longVal = (Long)store1.load(-1L);	if (longVal != null) throw new RuntimeException("Long value with fake key '-1' was found in Cassandra");	strVal = (String)store2.load("-1");	if (strVal != null) throw new RuntimeException("String value with fake key '-1' was found in Cassandra");	
running real keys read tests 

String strVal = (String)store2.load(strEntries.iterator().next().getKey());	if (!strEntries.iterator().next().getValue().equals(strVal)) throw new RuntimeException("String values were incorrectly deserialized from Cassandra");	longVal = (Long)store1.load(-1L);	if (longVal != null) throw new RuntimeException("Long value with fake key '-1' was found in Cassandra");	strVal = (String)store2.load("-1");	if (strVal != null) throw new RuntimeException("String value with fake key '-1' was found in Cassandra");	Map longValues = store1.loadAll(TestsHelper.getKeys(longEntries));	if (!TestsHelper.checkCollectionsEqual(longValues, longEntries)) throw new RuntimeException("Long values were incorrectly deserialized from Cassandra");	Map strValues = store2.loadAll(TestsHelper.getKeys(strEntries));	if (!TestsHelper.checkCollectionsEqual(strValues, strEntries)) throw new RuntimeException("String values were incorrectly deserialized from Cassandra");	
running fake keys read tests 

strVal = (String)store2.load("-1");	if (strVal != null) throw new RuntimeException("String value with fake key '-1' was found in Cassandra");	Map longValues = store1.loadAll(TestsHelper.getKeys(longEntries));	if (!TestsHelper.checkCollectionsEqual(longValues, longEntries)) throw new RuntimeException("Long values were incorrectly deserialized from Cassandra");	Map strValues = store2.loadAll(TestsHelper.getKeys(strEntries));	if (!TestsHelper.checkCollectionsEqual(strValues, strEntries)) throw new RuntimeException("String values were incorrectly deserialized from Cassandra");	longValues = store1.loadAll(fakeLongKeys);	if (!TestsHelper.checkCollectionsEqual(longValues, longEntries)) throw new RuntimeException("Long values were incorrectly deserialized from Cassandra");	strValues = store2.loadAll(fakeStrKeys);	if (!TestsHelper.checkCollectionsEqual(strValues, strEntries)) throw new RuntimeException("String values were incorrectly deserialized from Cassandra");	
bulk read operation tests passed 

strVal = (String)store2.load("-1");	if (strVal != null) throw new RuntimeException("String value with fake key '-1' was found in Cassandra");	Map longValues = store1.loadAll(TestsHelper.getKeys(longEntries));	if (!TestsHelper.checkCollectionsEqual(longValues, longEntries)) throw new RuntimeException("Long values were incorrectly deserialized from Cassandra");	Map strValues = store2.loadAll(TestsHelper.getKeys(strEntries));	if (!TestsHelper.checkCollectionsEqual(strValues, strEntries)) throw new RuntimeException("String values were incorrectly deserialized from Cassandra");	longValues = store1.loadAll(fakeLongKeys);	if (!TestsHelper.checkCollectionsEqual(longValues, longEntries)) throw new RuntimeException("Long values were incorrectly deserialized from Cassandra");	strValues = store2.loadAll(fakeStrKeys);	if (!TestsHelper.checkCollectionsEqual(strValues, strEntries)) throw new RuntimeException("String values were incorrectly deserialized from Cassandra");	
primitive strategy read tests passed 

strVal = (String)store2.load("-1");	if (strVal != null) throw new RuntimeException("String value with fake key '-1' was found in Cassandra");	Map longValues = store1.loadAll(TestsHelper.getKeys(longEntries));	if (!TestsHelper.checkCollectionsEqual(longValues, longEntries)) throw new RuntimeException("Long values were incorrectly deserialized from Cassandra");	Map strValues = store2.loadAll(TestsHelper.getKeys(strEntries));	if (!TestsHelper.checkCollectionsEqual(strValues, strEntries)) throw new RuntimeException("String values were incorrectly deserialized from Cassandra");	longValues = store1.loadAll(fakeLongKeys);	if (!TestsHelper.checkCollectionsEqual(longValues, longEntries)) throw new RuntimeException("Long values were incorrectly deserialized from Cassandra");	strValues = store2.loadAll(fakeStrKeys);	if (!TestsHelper.checkCollectionsEqual(strValues, strEntries)) throw new RuntimeException("String values were incorrectly deserialized from Cassandra");	
running primitive strategy delete tests 

strVal = (String)store2.load("-1");	if (strVal != null) throw new RuntimeException("String value with fake key '-1' was found in Cassandra");	Map longValues = store1.loadAll(TestsHelper.getKeys(longEntries));	if (!TestsHelper.checkCollectionsEqual(longValues, longEntries)) throw new RuntimeException("Long values were incorrectly deserialized from Cassandra");	Map strValues = store2.loadAll(TestsHelper.getKeys(strEntries));	if (!TestsHelper.checkCollectionsEqual(strValues, strEntries)) throw new RuntimeException("String values were incorrectly deserialized from Cassandra");	longValues = store1.loadAll(fakeLongKeys);	if (!TestsHelper.checkCollectionsEqual(longValues, longEntries)) throw new RuntimeException("Long values were incorrectly deserialized from Cassandra");	strValues = store2.loadAll(fakeStrKeys);	if (!TestsHelper.checkCollectionsEqual(strValues, strEntries)) throw new RuntimeException("String values were incorrectly deserialized from Cassandra");	
deleting real keys 

Map strValues = store2.loadAll(TestsHelper.getKeys(strEntries));	if (!TestsHelper.checkCollectionsEqual(strValues, strEntries)) throw new RuntimeException("String values were incorrectly deserialized from Cassandra");	longValues = store1.loadAll(fakeLongKeys);	if (!TestsHelper.checkCollectionsEqual(longValues, longEntries)) throw new RuntimeException("Long values were incorrectly deserialized from Cassandra");	strValues = store2.loadAll(fakeStrKeys);	if (!TestsHelper.checkCollectionsEqual(strValues, strEntries)) throw new RuntimeException("String values were incorrectly deserialized from Cassandra");	store1.delete(longEntries.iterator().next().getKey());	store1.deleteAll(TestsHelper.getKeys(longEntries));	store2.delete(strEntries.iterator().next().getKey());	store2.deleteAll(TestsHelper.getKeys(strEntries));	
deleting fake keys 

strValues = store2.loadAll(fakeStrKeys);	if (!TestsHelper.checkCollectionsEqual(strValues, strEntries)) throw new RuntimeException("String values were incorrectly deserialized from Cassandra");	store1.delete(longEntries.iterator().next().getKey());	store1.deleteAll(TestsHelper.getKeys(longEntries));	store2.delete(strEntries.iterator().next().getKey());	store2.deleteAll(TestsHelper.getKeys(strEntries));	store1.delete(-1L);	store2.delete("-1");	store1.deleteAll(fakeLongKeys);	store2.deleteAll(fakeStrKeys);	
primitive strategy delete tests passed 

public void blobStrategyTest() {	CacheStore store1 = CacheStoreHelper.createCacheStore("longTypes", new ClassPathResource("org/apache/ignite/tests/persistence/blob/persistence-settings-1.xml"), CassandraHelper.getAdminDataSrc());	CacheStore store2 = CacheStoreHelper.createCacheStore("personTypes", new ClassPathResource("org/apache/ignite/tests/persistence/blob/persistence-settings-2.xml"), CassandraHelper.getAdminDataSrc());	CacheStore store3 = CacheStoreHelper.createCacheStore("personTypes", new ClassPathResource("org/apache/ignite/tests/persistence/blob/persistence-settings-3.xml"), CassandraHelper.getAdminDataSrc());	Collection<CacheEntryImpl<Long, Long>> longEntries = TestsHelper.generateLongsEntries();	Collection<CacheEntryImpl<Long, Person>> personEntries = TestsHelper.generateLongsPersonsEntries();	
running blob strategy write tests 

public void blobStrategyTest() {	CacheStore store1 = CacheStoreHelper.createCacheStore("longTypes", new ClassPathResource("org/apache/ignite/tests/persistence/blob/persistence-settings-1.xml"), CassandraHelper.getAdminDataSrc());	CacheStore store2 = CacheStoreHelper.createCacheStore("personTypes", new ClassPathResource("org/apache/ignite/tests/persistence/blob/persistence-settings-2.xml"), CassandraHelper.getAdminDataSrc());	CacheStore store3 = CacheStoreHelper.createCacheStore("personTypes", new ClassPathResource("org/apache/ignite/tests/persistence/blob/persistence-settings-3.xml"), CassandraHelper.getAdminDataSrc());	Collection<CacheEntryImpl<Long, Long>> longEntries = TestsHelper.generateLongsEntries();	Collection<CacheEntryImpl<Long, Person>> personEntries = TestsHelper.generateLongsPersonsEntries();	
running single write operation tests 

public void blobStrategyTest() {	CacheStore store1 = CacheStoreHelper.createCacheStore("longTypes", new ClassPathResource("org/apache/ignite/tests/persistence/blob/persistence-settings-1.xml"), CassandraHelper.getAdminDataSrc());	CacheStore store2 = CacheStoreHelper.createCacheStore("personTypes", new ClassPathResource("org/apache/ignite/tests/persistence/blob/persistence-settings-2.xml"), CassandraHelper.getAdminDataSrc());	CacheStore store3 = CacheStoreHelper.createCacheStore("personTypes", new ClassPathResource("org/apache/ignite/tests/persistence/blob/persistence-settings-3.xml"), CassandraHelper.getAdminDataSrc());	Collection<CacheEntryImpl<Long, Long>> longEntries = TestsHelper.generateLongsEntries();	Collection<CacheEntryImpl<Long, Person>> personEntries = TestsHelper.generateLongsPersonsEntries();	store1.write(longEntries.iterator().next());	store2.write(personEntries.iterator().next());	store3.write(personEntries.iterator().next());	
single write operation tests passed 

public void blobStrategyTest() {	CacheStore store1 = CacheStoreHelper.createCacheStore("longTypes", new ClassPathResource("org/apache/ignite/tests/persistence/blob/persistence-settings-1.xml"), CassandraHelper.getAdminDataSrc());	CacheStore store2 = CacheStoreHelper.createCacheStore("personTypes", new ClassPathResource("org/apache/ignite/tests/persistence/blob/persistence-settings-2.xml"), CassandraHelper.getAdminDataSrc());	CacheStore store3 = CacheStoreHelper.createCacheStore("personTypes", new ClassPathResource("org/apache/ignite/tests/persistence/blob/persistence-settings-3.xml"), CassandraHelper.getAdminDataSrc());	Collection<CacheEntryImpl<Long, Long>> longEntries = TestsHelper.generateLongsEntries();	Collection<CacheEntryImpl<Long, Person>> personEntries = TestsHelper.generateLongsPersonsEntries();	store1.write(longEntries.iterator().next());	store2.write(personEntries.iterator().next());	store3.write(personEntries.iterator().next());	
running bulk write operation tests 

CacheStore store2 = CacheStoreHelper.createCacheStore("personTypes", new ClassPathResource("org/apache/ignite/tests/persistence/blob/persistence-settings-2.xml"), CassandraHelper.getAdminDataSrc());	CacheStore store3 = CacheStoreHelper.createCacheStore("personTypes", new ClassPathResource("org/apache/ignite/tests/persistence/blob/persistence-settings-3.xml"), CassandraHelper.getAdminDataSrc());	Collection<CacheEntryImpl<Long, Long>> longEntries = TestsHelper.generateLongsEntries();	Collection<CacheEntryImpl<Long, Person>> personEntries = TestsHelper.generateLongsPersonsEntries();	store1.write(longEntries.iterator().next());	store2.write(personEntries.iterator().next());	store3.write(personEntries.iterator().next());	store1.writeAll(longEntries);	store2.writeAll(personEntries);	store3.writeAll(personEntries);	
bulk write operation tests passed 

CacheStore store2 = CacheStoreHelper.createCacheStore("personTypes", new ClassPathResource("org/apache/ignite/tests/persistence/blob/persistence-settings-2.xml"), CassandraHelper.getAdminDataSrc());	CacheStore store3 = CacheStoreHelper.createCacheStore("personTypes", new ClassPathResource("org/apache/ignite/tests/persistence/blob/persistence-settings-3.xml"), CassandraHelper.getAdminDataSrc());	Collection<CacheEntryImpl<Long, Long>> longEntries = TestsHelper.generateLongsEntries();	Collection<CacheEntryImpl<Long, Person>> personEntries = TestsHelper.generateLongsPersonsEntries();	store1.write(longEntries.iterator().next());	store2.write(personEntries.iterator().next());	store3.write(personEntries.iterator().next());	store1.writeAll(longEntries);	store2.writeAll(personEntries);	store3.writeAll(personEntries);	
blob strategy write tests passed 

CacheStore store2 = CacheStoreHelper.createCacheStore("personTypes", new ClassPathResource("org/apache/ignite/tests/persistence/blob/persistence-settings-2.xml"), CassandraHelper.getAdminDataSrc());	CacheStore store3 = CacheStoreHelper.createCacheStore("personTypes", new ClassPathResource("org/apache/ignite/tests/persistence/blob/persistence-settings-3.xml"), CassandraHelper.getAdminDataSrc());	Collection<CacheEntryImpl<Long, Long>> longEntries = TestsHelper.generateLongsEntries();	Collection<CacheEntryImpl<Long, Person>> personEntries = TestsHelper.generateLongsPersonsEntries();	store1.write(longEntries.iterator().next());	store2.write(personEntries.iterator().next());	store3.write(personEntries.iterator().next());	store1.writeAll(longEntries);	store2.writeAll(personEntries);	store3.writeAll(personEntries);	
running blob strategy read tests 

CacheStore store2 = CacheStoreHelper.createCacheStore("personTypes", new ClassPathResource("org/apache/ignite/tests/persistence/blob/persistence-settings-2.xml"), CassandraHelper.getAdminDataSrc());	CacheStore store3 = CacheStoreHelper.createCacheStore("personTypes", new ClassPathResource("org/apache/ignite/tests/persistence/blob/persistence-settings-3.xml"), CassandraHelper.getAdminDataSrc());	Collection<CacheEntryImpl<Long, Long>> longEntries = TestsHelper.generateLongsEntries();	Collection<CacheEntryImpl<Long, Person>> personEntries = TestsHelper.generateLongsPersonsEntries();	store1.write(longEntries.iterator().next());	store2.write(personEntries.iterator().next());	store3.write(personEntries.iterator().next());	store1.writeAll(longEntries);	store2.writeAll(personEntries);	store3.writeAll(personEntries);	
running single read operation tests 

store3.write(personEntries.iterator().next());	store1.writeAll(longEntries);	store2.writeAll(personEntries);	store3.writeAll(personEntries);	Long longVal = (Long)store1.load(longEntries.iterator().next().getKey());	if (!longEntries.iterator().next().getValue().equals(longVal)) throw new RuntimeException("Long values were incorrectly deserialized from Cassandra");	Person personVal = (Person)store2.load(personEntries.iterator().next().getKey());	if (!personEntries.iterator().next().getValue().equals(personVal)) throw new RuntimeException("Person values were incorrectly deserialized from Cassandra");	personVal = (Person)store3.load(personEntries.iterator().next().getKey());	if (!personEntries.iterator().next().getValue().equals(personVal)) throw new RuntimeException("Person values were incorrectly deserialized from Cassandra");	
single read operation tests passed 

store3.write(personEntries.iterator().next());	store1.writeAll(longEntries);	store2.writeAll(personEntries);	store3.writeAll(personEntries);	Long longVal = (Long)store1.load(longEntries.iterator().next().getKey());	if (!longEntries.iterator().next().getValue().equals(longVal)) throw new RuntimeException("Long values were incorrectly deserialized from Cassandra");	Person personVal = (Person)store2.load(personEntries.iterator().next().getKey());	if (!personEntries.iterator().next().getValue().equals(personVal)) throw new RuntimeException("Person values were incorrectly deserialized from Cassandra");	personVal = (Person)store3.load(personEntries.iterator().next().getKey());	if (!personEntries.iterator().next().getValue().equals(personVal)) throw new RuntimeException("Person values were incorrectly deserialized from Cassandra");	
running bulk read operation tests 

Person personVal = (Person)store2.load(personEntries.iterator().next().getKey());	if (!personEntries.iterator().next().getValue().equals(personVal)) throw new RuntimeException("Person values were incorrectly deserialized from Cassandra");	personVal = (Person)store3.load(personEntries.iterator().next().getKey());	if (!personEntries.iterator().next().getValue().equals(personVal)) throw new RuntimeException("Person values were incorrectly deserialized from Cassandra");	Map longValues = store1.loadAll(TestsHelper.getKeys(longEntries));	if (!TestsHelper.checkCollectionsEqual(longValues, longEntries)) throw new RuntimeException("Long values were incorrectly deserialized from Cassandra");	Map personValues = store2.loadAll(TestsHelper.getKeys(personEntries));	if (!TestsHelper.checkPersonCollectionsEqual(personValues, personEntries, false)) throw new RuntimeException("Person values were incorrectly deserialized from Cassandra");	personValues = store3.loadAll(TestsHelper.getKeys(personEntries));	if (!TestsHelper.checkPersonCollectionsEqual(personValues, personEntries, false)) throw new RuntimeException("Person values were incorrectly deserialized from Cassandra");	
bulk read operation tests passed 

Person personVal = (Person)store2.load(personEntries.iterator().next().getKey());	if (!personEntries.iterator().next().getValue().equals(personVal)) throw new RuntimeException("Person values were incorrectly deserialized from Cassandra");	personVal = (Person)store3.load(personEntries.iterator().next().getKey());	if (!personEntries.iterator().next().getValue().equals(personVal)) throw new RuntimeException("Person values were incorrectly deserialized from Cassandra");	Map longValues = store1.loadAll(TestsHelper.getKeys(longEntries));	if (!TestsHelper.checkCollectionsEqual(longValues, longEntries)) throw new RuntimeException("Long values were incorrectly deserialized from Cassandra");	Map personValues = store2.loadAll(TestsHelper.getKeys(personEntries));	if (!TestsHelper.checkPersonCollectionsEqual(personValues, personEntries, false)) throw new RuntimeException("Person values were incorrectly deserialized from Cassandra");	personValues = store3.loadAll(TestsHelper.getKeys(personEntries));	if (!TestsHelper.checkPersonCollectionsEqual(personValues, personEntries, false)) throw new RuntimeException("Person values were incorrectly deserialized from Cassandra");	
blob strategy read tests passed 

Person personVal = (Person)store2.load(personEntries.iterator().next().getKey());	if (!personEntries.iterator().next().getValue().equals(personVal)) throw new RuntimeException("Person values were incorrectly deserialized from Cassandra");	personVal = (Person)store3.load(personEntries.iterator().next().getKey());	if (!personEntries.iterator().next().getValue().equals(personVal)) throw new RuntimeException("Person values were incorrectly deserialized from Cassandra");	Map longValues = store1.loadAll(TestsHelper.getKeys(longEntries));	if (!TestsHelper.checkCollectionsEqual(longValues, longEntries)) throw new RuntimeException("Long values were incorrectly deserialized from Cassandra");	Map personValues = store2.loadAll(TestsHelper.getKeys(personEntries));	if (!TestsHelper.checkPersonCollectionsEqual(personValues, personEntries, false)) throw new RuntimeException("Person values were incorrectly deserialized from Cassandra");	personValues = store3.loadAll(TestsHelper.getKeys(personEntries));	if (!TestsHelper.checkPersonCollectionsEqual(personValues, personEntries, false)) throw new RuntimeException("Person values were incorrectly deserialized from Cassandra");	
running blob strategy delete tests 

Map personValues = store2.loadAll(TestsHelper.getKeys(personEntries));	if (!TestsHelper.checkPersonCollectionsEqual(personValues, personEntries, false)) throw new RuntimeException("Person values were incorrectly deserialized from Cassandra");	personValues = store3.loadAll(TestsHelper.getKeys(personEntries));	if (!TestsHelper.checkPersonCollectionsEqual(personValues, personEntries, false)) throw new RuntimeException("Person values were incorrectly deserialized from Cassandra");	store1.delete(longEntries.iterator().next().getKey());	store1.deleteAll(TestsHelper.getKeys(longEntries));	store2.delete(personEntries.iterator().next().getKey());	store2.deleteAll(TestsHelper.getKeys(personEntries));	store3.delete(personEntries.iterator().next().getKey());	store3.deleteAll(TestsHelper.getKeys(personEntries));	
blob strategy delete tests passed 

CacheStore store2 = CacheStoreHelper.createCacheStore("personTypes", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/persistence-settings-2.xml"), CassandraHelper.getAdminDataSrc());	CacheStore store3 = CacheStoreHelper.createCacheStore("personTypes", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/persistence-settings-3.xml"), CassandraHelper.getAdminDataSrc());	CacheStore store4 = CacheStoreHelper.createCacheStore("persons", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/persistence-settings-4.xml"), CassandraHelper.getAdminDataSrc());	CacheStore productStore = CacheStoreHelper.createCacheStore("product", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/product.xml"), CassandraHelper.getAdminDataSrc());	CacheStore orderStore = CacheStoreHelper.createCacheStore("order", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/order.xml"), CassandraHelper.getAdminDataSrc());	Collection<CacheEntryImpl<Long, Person>> entries1 = TestsHelper.generateLongsPersonsEntries();	Collection<CacheEntryImpl<PersonId, Person>> entries2 = TestsHelper.generatePersonIdsPersonsEntries();	Collection<CacheEntryImpl<PersonId, Person>> entries3 = TestsHelper.generatePersonIdsPersonsEntries();	Collection<CacheEntryImpl<Long, Product>> productEntries = TestsHelper.generateProductEntries();	Collection<CacheEntryImpl<Long, ProductOrder>> orderEntries = TestsHelper.generateOrderEntries();	
running pojo strategy write tests 

CacheStore store2 = CacheStoreHelper.createCacheStore("personTypes", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/persistence-settings-2.xml"), CassandraHelper.getAdminDataSrc());	CacheStore store3 = CacheStoreHelper.createCacheStore("personTypes", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/persistence-settings-3.xml"), CassandraHelper.getAdminDataSrc());	CacheStore store4 = CacheStoreHelper.createCacheStore("persons", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/persistence-settings-4.xml"), CassandraHelper.getAdminDataSrc());	CacheStore productStore = CacheStoreHelper.createCacheStore("product", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/product.xml"), CassandraHelper.getAdminDataSrc());	CacheStore orderStore = CacheStoreHelper.createCacheStore("order", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/order.xml"), CassandraHelper.getAdminDataSrc());	Collection<CacheEntryImpl<Long, Person>> entries1 = TestsHelper.generateLongsPersonsEntries();	Collection<CacheEntryImpl<PersonId, Person>> entries2 = TestsHelper.generatePersonIdsPersonsEntries();	Collection<CacheEntryImpl<PersonId, Person>> entries3 = TestsHelper.generatePersonIdsPersonsEntries();	Collection<CacheEntryImpl<Long, Product>> productEntries = TestsHelper.generateProductEntries();	Collection<CacheEntryImpl<Long, ProductOrder>> orderEntries = TestsHelper.generateOrderEntries();	
running single write operation tests 

Collection<CacheEntryImpl<PersonId, Person>> entries2 = TestsHelper.generatePersonIdsPersonsEntries();	Collection<CacheEntryImpl<PersonId, Person>> entries3 = TestsHelper.generatePersonIdsPersonsEntries();	Collection<CacheEntryImpl<Long, Product>> productEntries = TestsHelper.generateProductEntries();	Collection<CacheEntryImpl<Long, ProductOrder>> orderEntries = TestsHelper.generateOrderEntries();	store1.write(entries1.iterator().next());	store2.write(entries2.iterator().next());	store3.write(entries3.iterator().next());	store4.write(entries3.iterator().next());	productStore.write(productEntries.iterator().next());	orderStore.write(orderEntries.iterator().next());	
single write operation tests passed 

Collection<CacheEntryImpl<PersonId, Person>> entries2 = TestsHelper.generatePersonIdsPersonsEntries();	Collection<CacheEntryImpl<PersonId, Person>> entries3 = TestsHelper.generatePersonIdsPersonsEntries();	Collection<CacheEntryImpl<Long, Product>> productEntries = TestsHelper.generateProductEntries();	Collection<CacheEntryImpl<Long, ProductOrder>> orderEntries = TestsHelper.generateOrderEntries();	store1.write(entries1.iterator().next());	store2.write(entries2.iterator().next());	store3.write(entries3.iterator().next());	store4.write(entries3.iterator().next());	productStore.write(productEntries.iterator().next());	orderStore.write(orderEntries.iterator().next());	
running bulk write operation tests 

store3.write(entries3.iterator().next());	store4.write(entries3.iterator().next());	productStore.write(productEntries.iterator().next());	orderStore.write(orderEntries.iterator().next());	store1.writeAll(entries1);	store2.writeAll(entries2);	store3.writeAll(entries3);	store4.writeAll(entries3);	productStore.writeAll(productEntries);	orderStore.writeAll(orderEntries);	
bulk write operation tests passed 

store3.write(entries3.iterator().next());	store4.write(entries3.iterator().next());	productStore.write(productEntries.iterator().next());	orderStore.write(orderEntries.iterator().next());	store1.writeAll(entries1);	store2.writeAll(entries2);	store3.writeAll(entries3);	store4.writeAll(entries3);	productStore.writeAll(productEntries);	orderStore.writeAll(orderEntries);	
pojo strategy write tests passed 

store3.write(entries3.iterator().next());	store4.write(entries3.iterator().next());	productStore.write(productEntries.iterator().next());	orderStore.write(orderEntries.iterator().next());	store1.writeAll(entries1);	store2.writeAll(entries2);	store3.writeAll(entries3);	store4.writeAll(entries3);	productStore.writeAll(productEntries);	orderStore.writeAll(orderEntries);	
running pojo strategy read tests 

store3.write(entries3.iterator().next());	store4.write(entries3.iterator().next());	productStore.write(productEntries.iterator().next());	orderStore.write(orderEntries.iterator().next());	store1.writeAll(entries1);	store2.writeAll(entries2);	store3.writeAll(entries3);	store4.writeAll(entries3);	productStore.writeAll(productEntries);	orderStore.writeAll(orderEntries);	
running single read operation tests 

person = (Person)store2.load(entries2.iterator().next().getKey());	if (!entries2.iterator().next().getValue().equalsPrimitiveFields(person)) throw new RuntimeException("Person values were incorrectly deserialized from Cassandra");	person = (Person)store3.load(entries3.iterator().next().getKey());	if (!entries3.iterator().next().getValue().equals(person)) throw new RuntimeException("Person values were incorrectly deserialized from Cassandra");	person = (Person)store4.load(entries3.iterator().next().getKey());	if (!entries3.iterator().next().getValue().equals(person)) throw new RuntimeException("Person values were incorrectly deserialized from Cassandra");	Product product = (Product)productStore.load(productEntries.iterator().next().getKey());	if (!productEntries.iterator().next().getValue().equals(product)) throw new RuntimeException("Product values were incorrectly deserialized from Cassandra");	ProductOrder order = (ProductOrder)orderStore.load(orderEntries.iterator().next().getKey());	if (!orderEntries.iterator().next().getValue().equals(order)) throw new RuntimeException("Order values were incorrectly deserialized from Cassandra");	
single read operation tests passed 

person = (Person)store2.load(entries2.iterator().next().getKey());	if (!entries2.iterator().next().getValue().equalsPrimitiveFields(person)) throw new RuntimeException("Person values were incorrectly deserialized from Cassandra");	person = (Person)store3.load(entries3.iterator().next().getKey());	if (!entries3.iterator().next().getValue().equals(person)) throw new RuntimeException("Person values were incorrectly deserialized from Cassandra");	person = (Person)store4.load(entries3.iterator().next().getKey());	if (!entries3.iterator().next().getValue().equals(person)) throw new RuntimeException("Person values were incorrectly deserialized from Cassandra");	Product product = (Product)productStore.load(productEntries.iterator().next().getKey());	if (!productEntries.iterator().next().getValue().equals(product)) throw new RuntimeException("Product values were incorrectly deserialized from Cassandra");	ProductOrder order = (ProductOrder)orderStore.load(orderEntries.iterator().next().getKey());	if (!orderEntries.iterator().next().getValue().equals(order)) throw new RuntimeException("Order values were incorrectly deserialized from Cassandra");	
running bulk read operation tests 

persons = store2.loadAll(TestsHelper.getKeys(entries2));	if (!TestsHelper.checkPersonCollectionsEqual(persons, entries2, true)) throw new RuntimeException("Person values were incorrectly deserialized from Cassandra");	persons = store3.loadAll(TestsHelper.getKeys(entries3));	if (!TestsHelper.checkPersonCollectionsEqual(persons, entries3, false)) throw new RuntimeException("Person values were incorrectly deserialized from Cassandra");	persons = store4.loadAll(TestsHelper.getKeys(entries3));	if (!TestsHelper.checkPersonCollectionsEqual(persons, entries3, false)) throw new RuntimeException("Person values were incorrectly deserialized from Cassandra");	Map products = productStore.loadAll(TestsHelper.getKeys(productEntries));	if (!TestsHelper.checkProductCollectionsEqual(products, productEntries)) throw new RuntimeException("Product values were incorrectly deserialized from Cassandra");	Map orders = orderStore.loadAll(TestsHelper.getKeys(orderEntries));	if (!TestsHelper.checkOrderCollectionsEqual(orders, orderEntries)) throw new RuntimeException("Order values were incorrectly deserialized from Cassandra");	
bulk read operation tests passed 

persons = store2.loadAll(TestsHelper.getKeys(entries2));	if (!TestsHelper.checkPersonCollectionsEqual(persons, entries2, true)) throw new RuntimeException("Person values were incorrectly deserialized from Cassandra");	persons = store3.loadAll(TestsHelper.getKeys(entries3));	if (!TestsHelper.checkPersonCollectionsEqual(persons, entries3, false)) throw new RuntimeException("Person values were incorrectly deserialized from Cassandra");	persons = store4.loadAll(TestsHelper.getKeys(entries3));	if (!TestsHelper.checkPersonCollectionsEqual(persons, entries3, false)) throw new RuntimeException("Person values were incorrectly deserialized from Cassandra");	Map products = productStore.loadAll(TestsHelper.getKeys(productEntries));	if (!TestsHelper.checkProductCollectionsEqual(products, productEntries)) throw new RuntimeException("Product values were incorrectly deserialized from Cassandra");	Map orders = orderStore.loadAll(TestsHelper.getKeys(orderEntries));	if (!TestsHelper.checkOrderCollectionsEqual(orders, orderEntries)) throw new RuntimeException("Order values were incorrectly deserialized from Cassandra");	
pojo strategy read tests passed 

persons = store2.loadAll(TestsHelper.getKeys(entries2));	if (!TestsHelper.checkPersonCollectionsEqual(persons, entries2, true)) throw new RuntimeException("Person values were incorrectly deserialized from Cassandra");	persons = store3.loadAll(TestsHelper.getKeys(entries3));	if (!TestsHelper.checkPersonCollectionsEqual(persons, entries3, false)) throw new RuntimeException("Person values were incorrectly deserialized from Cassandra");	persons = store4.loadAll(TestsHelper.getKeys(entries3));	if (!TestsHelper.checkPersonCollectionsEqual(persons, entries3, false)) throw new RuntimeException("Person values were incorrectly deserialized from Cassandra");	Map products = productStore.loadAll(TestsHelper.getKeys(productEntries));	if (!TestsHelper.checkProductCollectionsEqual(products, productEntries)) throw new RuntimeException("Product values were incorrectly deserialized from Cassandra");	Map orders = orderStore.loadAll(TestsHelper.getKeys(orderEntries));	if (!TestsHelper.checkOrderCollectionsEqual(orders, orderEntries)) throw new RuntimeException("Order values were incorrectly deserialized from Cassandra");	
running pojo strategy delete tests 

store2.delete(entries2.iterator().next().getKey());	store2.deleteAll(TestsHelper.getKeys(entries2));	store3.delete(entries3.iterator().next().getKey());	store3.deleteAll(TestsHelper.getKeys(entries3));	store4.delete(entries3.iterator().next().getKey());	store4.deleteAll(TestsHelper.getKeys(entries3));	productStore.delete(productEntries.iterator().next().getKey());	productStore.deleteAll(TestsHelper.getKeys(productEntries));	orderStore.delete(orderEntries.iterator().next().getKey());	orderStore.deleteAll(TestsHelper.getKeys(orderEntries));	
pojo strategy delete tests passed 

public void pojoStrategySimpleObjectsTest() {	CacheStore store5 = CacheStoreHelper.createCacheStore("persons5", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/persistence-settings-5.xml"), CassandraHelper.getAdminDataSrc());	CacheStore store6 = CacheStoreHelper.createCacheStore("persons6", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/persistence-settings-6.xml"), CassandraHelper.getAdminDataSrc());	Collection<CacheEntryImpl<SimplePersonId, SimplePerson>> entries5 = TestsHelper.generateSimplePersonIdsPersonsEntries();	Collection<CacheEntryImpl<SimplePersonId, SimplePerson>> entries6 = TestsHelper.generateSimplePersonIdsPersonsEntries();	
running pojo strategy write tests for simple objects 

public void pojoStrategySimpleObjectsTest() {	CacheStore store5 = CacheStoreHelper.createCacheStore("persons5", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/persistence-settings-5.xml"), CassandraHelper.getAdminDataSrc());	CacheStore store6 = CacheStoreHelper.createCacheStore("persons6", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/persistence-settings-6.xml"), CassandraHelper.getAdminDataSrc());	Collection<CacheEntryImpl<SimplePersonId, SimplePerson>> entries5 = TestsHelper.generateSimplePersonIdsPersonsEntries();	Collection<CacheEntryImpl<SimplePersonId, SimplePerson>> entries6 = TestsHelper.generateSimplePersonIdsPersonsEntries();	
running single write operation tests 

public void pojoStrategySimpleObjectsTest() {	CacheStore store5 = CacheStoreHelper.createCacheStore("persons5", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/persistence-settings-5.xml"), CassandraHelper.getAdminDataSrc());	CacheStore store6 = CacheStoreHelper.createCacheStore("persons6", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/persistence-settings-6.xml"), CassandraHelper.getAdminDataSrc());	Collection<CacheEntryImpl<SimplePersonId, SimplePerson>> entries5 = TestsHelper.generateSimplePersonIdsPersonsEntries();	Collection<CacheEntryImpl<SimplePersonId, SimplePerson>> entries6 = TestsHelper.generateSimplePersonIdsPersonsEntries();	store5.write(entries5.iterator().next());	store6.write(entries6.iterator().next());	
single write operation tests passed 

public void pojoStrategySimpleObjectsTest() {	CacheStore store5 = CacheStoreHelper.createCacheStore("persons5", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/persistence-settings-5.xml"), CassandraHelper.getAdminDataSrc());	CacheStore store6 = CacheStoreHelper.createCacheStore("persons6", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/persistence-settings-6.xml"), CassandraHelper.getAdminDataSrc());	Collection<CacheEntryImpl<SimplePersonId, SimplePerson>> entries5 = TestsHelper.generateSimplePersonIdsPersonsEntries();	Collection<CacheEntryImpl<SimplePersonId, SimplePerson>> entries6 = TestsHelper.generateSimplePersonIdsPersonsEntries();	store5.write(entries5.iterator().next());	store6.write(entries6.iterator().next());	
running bulk write operation tests 

public void pojoStrategySimpleObjectsTest() {	CacheStore store5 = CacheStoreHelper.createCacheStore("persons5", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/persistence-settings-5.xml"), CassandraHelper.getAdminDataSrc());	CacheStore store6 = CacheStoreHelper.createCacheStore("persons6", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/persistence-settings-6.xml"), CassandraHelper.getAdminDataSrc());	Collection<CacheEntryImpl<SimplePersonId, SimplePerson>> entries5 = TestsHelper.generateSimplePersonIdsPersonsEntries();	Collection<CacheEntryImpl<SimplePersonId, SimplePerson>> entries6 = TestsHelper.generateSimplePersonIdsPersonsEntries();	store5.write(entries5.iterator().next());	store6.write(entries6.iterator().next());	store5.writeAll(entries5);	store6.writeAll(entries6);	
bulk write operation tests passed 

public void pojoStrategySimpleObjectsTest() {	CacheStore store5 = CacheStoreHelper.createCacheStore("persons5", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/persistence-settings-5.xml"), CassandraHelper.getAdminDataSrc());	CacheStore store6 = CacheStoreHelper.createCacheStore("persons6", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/persistence-settings-6.xml"), CassandraHelper.getAdminDataSrc());	Collection<CacheEntryImpl<SimplePersonId, SimplePerson>> entries5 = TestsHelper.generateSimplePersonIdsPersonsEntries();	Collection<CacheEntryImpl<SimplePersonId, SimplePerson>> entries6 = TestsHelper.generateSimplePersonIdsPersonsEntries();	store5.write(entries5.iterator().next());	store6.write(entries6.iterator().next());	store5.writeAll(entries5);	store6.writeAll(entries6);	
pojo strategy write tests for simple objects passed 

public void pojoStrategySimpleObjectsTest() {	CacheStore store5 = CacheStoreHelper.createCacheStore("persons5", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/persistence-settings-5.xml"), CassandraHelper.getAdminDataSrc());	CacheStore store6 = CacheStoreHelper.createCacheStore("persons6", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/persistence-settings-6.xml"), CassandraHelper.getAdminDataSrc());	Collection<CacheEntryImpl<SimplePersonId, SimplePerson>> entries5 = TestsHelper.generateSimplePersonIdsPersonsEntries();	Collection<CacheEntryImpl<SimplePersonId, SimplePerson>> entries6 = TestsHelper.generateSimplePersonIdsPersonsEntries();	store5.write(entries5.iterator().next());	store6.write(entries6.iterator().next());	store5.writeAll(entries5);	store6.writeAll(entries6);	
running pojo simple objects strategy read tests 

public void pojoStrategySimpleObjectsTest() {	CacheStore store5 = CacheStoreHelper.createCacheStore("persons5", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/persistence-settings-5.xml"), CassandraHelper.getAdminDataSrc());	CacheStore store6 = CacheStoreHelper.createCacheStore("persons6", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/persistence-settings-6.xml"), CassandraHelper.getAdminDataSrc());	Collection<CacheEntryImpl<SimplePersonId, SimplePerson>> entries5 = TestsHelper.generateSimplePersonIdsPersonsEntries();	Collection<CacheEntryImpl<SimplePersonId, SimplePerson>> entries6 = TestsHelper.generateSimplePersonIdsPersonsEntries();	store5.write(entries5.iterator().next());	store6.write(entries6.iterator().next());	store5.writeAll(entries5);	store6.writeAll(entries6);	
running single read operation tests 

Collection<CacheEntryImpl<SimplePersonId, SimplePerson>> entries5 = TestsHelper.generateSimplePersonIdsPersonsEntries();	Collection<CacheEntryImpl<SimplePersonId, SimplePerson>> entries6 = TestsHelper.generateSimplePersonIdsPersonsEntries();	store5.write(entries5.iterator().next());	store6.write(entries6.iterator().next());	store5.writeAll(entries5);	store6.writeAll(entries6);	SimplePerson person = (SimplePerson)store5.load(entries5.iterator().next().getKey());	if (!entries5.iterator().next().getValue().equalsPrimitiveFields(person)) throw new RuntimeException("SimplePerson values were incorrectly deserialized from Cassandra");	person = (SimplePerson)store6.load(entries6.iterator().next().getKey());	if (!entries6.iterator().next().getValue().equalsPrimitiveFields(person)) throw new RuntimeException("SimplePerson values were incorrectly deserialized from Cassandra");	
single read operation tests passed 

Collection<CacheEntryImpl<SimplePersonId, SimplePerson>> entries5 = TestsHelper.generateSimplePersonIdsPersonsEntries();	Collection<CacheEntryImpl<SimplePersonId, SimplePerson>> entries6 = TestsHelper.generateSimplePersonIdsPersonsEntries();	store5.write(entries5.iterator().next());	store6.write(entries6.iterator().next());	store5.writeAll(entries5);	store6.writeAll(entries6);	SimplePerson person = (SimplePerson)store5.load(entries5.iterator().next().getKey());	if (!entries5.iterator().next().getValue().equalsPrimitiveFields(person)) throw new RuntimeException("SimplePerson values were incorrectly deserialized from Cassandra");	person = (SimplePerson)store6.load(entries6.iterator().next().getKey());	if (!entries6.iterator().next().getValue().equalsPrimitiveFields(person)) throw new RuntimeException("SimplePerson values were incorrectly deserialized from Cassandra");	
running bulk read operation tests 

store5.writeAll(entries5);	store6.writeAll(entries6);	SimplePerson person = (SimplePerson)store5.load(entries5.iterator().next().getKey());	if (!entries5.iterator().next().getValue().equalsPrimitiveFields(person)) throw new RuntimeException("SimplePerson values were incorrectly deserialized from Cassandra");	person = (SimplePerson)store6.load(entries6.iterator().next().getKey());	if (!entries6.iterator().next().getValue().equalsPrimitiveFields(person)) throw new RuntimeException("SimplePerson values were incorrectly deserialized from Cassandra");	Map persons = store5.loadAll(TestsHelper.getKeys(entries5));	if (!TestsHelper.checkSimplePersonCollectionsEqual(persons, entries5, true)) throw new RuntimeException("SimplePerson values were incorrectly deserialized from Cassandra");	persons = store6.loadAll(TestsHelper.getKeys(entries6));	if (!TestsHelper.checkSimplePersonCollectionsEqual(persons, entries6, true)) throw new RuntimeException("SimplePerson values were incorrectly deserialized from Cassandra");	
bulk read operation tests passed 

store5.writeAll(entries5);	store6.writeAll(entries6);	SimplePerson person = (SimplePerson)store5.load(entries5.iterator().next().getKey());	if (!entries5.iterator().next().getValue().equalsPrimitiveFields(person)) throw new RuntimeException("SimplePerson values were incorrectly deserialized from Cassandra");	person = (SimplePerson)store6.load(entries6.iterator().next().getKey());	if (!entries6.iterator().next().getValue().equalsPrimitiveFields(person)) throw new RuntimeException("SimplePerson values were incorrectly deserialized from Cassandra");	Map persons = store5.loadAll(TestsHelper.getKeys(entries5));	if (!TestsHelper.checkSimplePersonCollectionsEqual(persons, entries5, true)) throw new RuntimeException("SimplePerson values were incorrectly deserialized from Cassandra");	persons = store6.loadAll(TestsHelper.getKeys(entries6));	if (!TestsHelper.checkSimplePersonCollectionsEqual(persons, entries6, true)) throw new RuntimeException("SimplePerson values were incorrectly deserialized from Cassandra");	
pojo strategy read tests for simple objects passed 

store5.writeAll(entries5);	store6.writeAll(entries6);	SimplePerson person = (SimplePerson)store5.load(entries5.iterator().next().getKey());	if (!entries5.iterator().next().getValue().equalsPrimitiveFields(person)) throw new RuntimeException("SimplePerson values were incorrectly deserialized from Cassandra");	person = (SimplePerson)store6.load(entries6.iterator().next().getKey());	if (!entries6.iterator().next().getValue().equalsPrimitiveFields(person)) throw new RuntimeException("SimplePerson values were incorrectly deserialized from Cassandra");	Map persons = store5.loadAll(TestsHelper.getKeys(entries5));	if (!TestsHelper.checkSimplePersonCollectionsEqual(persons, entries5, true)) throw new RuntimeException("SimplePerson values were incorrectly deserialized from Cassandra");	persons = store6.loadAll(TestsHelper.getKeys(entries6));	if (!TestsHelper.checkSimplePersonCollectionsEqual(persons, entries6, true)) throw new RuntimeException("SimplePerson values were incorrectly deserialized from Cassandra");	
running pojo strategy delete tests for simple objects 

person = (SimplePerson)store6.load(entries6.iterator().next().getKey());	if (!entries6.iterator().next().getValue().equalsPrimitiveFields(person)) throw new RuntimeException("SimplePerson values were incorrectly deserialized from Cassandra");	Map persons = store5.loadAll(TestsHelper.getKeys(entries5));	if (!TestsHelper.checkSimplePersonCollectionsEqual(persons, entries5, true)) throw new RuntimeException("SimplePerson values were incorrectly deserialized from Cassandra");	persons = store6.loadAll(TestsHelper.getKeys(entries6));	if (!TestsHelper.checkSimplePersonCollectionsEqual(persons, entries6, true)) throw new RuntimeException("SimplePerson values were incorrectly deserialized from Cassandra");	store5.delete(entries5.iterator().next().getKey());	store5.deleteAll(TestsHelper.getKeys(entries5));	store6.delete(entries6.iterator().next().getKey());	store6.deleteAll(TestsHelper.getKeys(entries6));	
pojo strategy delete tests for simple objects passed 

public void pojoStrategyTransactionTest() {	Map<Object, Object> sessionProps = U.newHashMap(1);	Transaction sessionTx = new TestTransaction();	CacheStore productStore = CacheStoreHelper.createCacheStore("product", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/product.xml"), CassandraHelper.getAdminDataSrc(), new TestCacheSession("product", sessionTx, sessionProps));	CacheStore orderStore = CacheStoreHelper.createCacheStore("order", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/order.xml"), CassandraHelper.getAdminDataSrc(), new TestCacheSession("order", sessionTx, sessionProps));	List<CacheEntryImpl<Long, Product>> productEntries = TestsHelper.generateProductEntries();	Map<Long, List<CacheEntryImpl<Long, ProductOrder>>> ordersPerProduct = TestsHelper.generateOrdersPerProductEntries(productEntries, 2);	Collection<Long> productIds =  TestsHelper.getProductIds(productEntries);	Collection<Long> orderIds =  TestsHelper.getOrderIds(ordersPerProduct);	
running pojo strategy transaction write tests 

public void pojoStrategyTransactionTest() {	Map<Object, Object> sessionProps = U.newHashMap(1);	Transaction sessionTx = new TestTransaction();	CacheStore productStore = CacheStoreHelper.createCacheStore("product", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/product.xml"), CassandraHelper.getAdminDataSrc(), new TestCacheSession("product", sessionTx, sessionProps));	CacheStore orderStore = CacheStoreHelper.createCacheStore("order", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/order.xml"), CassandraHelper.getAdminDataSrc(), new TestCacheSession("order", sessionTx, sessionProps));	List<CacheEntryImpl<Long, Product>> productEntries = TestsHelper.generateProductEntries();	Map<Long, List<CacheEntryImpl<Long, ProductOrder>>> ordersPerProduct = TestsHelper.generateOrdersPerProductEntries(productEntries, 2);	Collection<Long> productIds =  TestsHelper.getProductIds(productEntries);	Collection<Long> orderIds =  TestsHelper.getOrderIds(ordersPerProduct);	
running single write operation tests 

throw new RuntimeException("Single write operation test failed. Transaction was committed, but " + "no objects were persisted into Cassandra");	}	if (products.size() > 1 || orders.size() > 1) {	throw new RuntimeException("Single write operation test failed. There were committed more objects " + "into Cassandra than expected");	}	product1 = products.entrySet().iterator().next().getValue();	order1 = orders.entrySet().iterator().next().getValue();	if (!product.equals(product1) || !order.equals(order1)) {	throw new RuntimeException("Single write operation test failed. Transaction was committed, but " + "objects were incorrectly persisted/loaded to/from Cassandra");	}	
single write operation tests passed 

throw new RuntimeException("Single write operation test failed. Transaction was committed, but " + "no objects were persisted into Cassandra");	}	if (products.size() > 1 || orders.size() > 1) {	throw new RuntimeException("Single write operation test failed. There were committed more objects " + "into Cassandra than expected");	}	product1 = products.entrySet().iterator().next().getValue();	order1 = orders.entrySet().iterator().next().getValue();	if (!product.equals(product1) || !order.equals(order1)) {	throw new RuntimeException("Single write operation test failed. Transaction was committed, but " + "objects were incorrectly persisted/loaded to/from Cassandra");	}	
running bulk write operation tests 

}	}	for (Long productId : ordersPerProduct.keySet()) {	for (CacheEntryImpl<Long, ProductOrder> entry : ordersPerProduct.get(productId)) {	order = orders.get(entry.getKey());	if (!entry.getValue().equals(order)) {	throw new RuntimeException("Bulk write operation test failed. Transaction was committed, but " + "some objects were incorrectly persisted/loaded to/from Cassandra");	}	}	}	
bulk write operation tests passed 

}	}	for (Long productId : ordersPerProduct.keySet()) {	for (CacheEntryImpl<Long, ProductOrder> entry : ordersPerProduct.get(productId)) {	order = orders.get(entry.getKey());	if (!entry.getValue().equals(order)) {	throw new RuntimeException("Bulk write operation test failed. Transaction was committed, but " + "some objects were incorrectly persisted/loaded to/from Cassandra");	}	}	}	
pojo strategy transaction write tests passed 

}	}	for (Long productId : ordersPerProduct.keySet()) {	for (CacheEntryImpl<Long, ProductOrder> entry : ordersPerProduct.get(productId)) {	order = orders.get(entry.getKey());	if (!entry.getValue().equals(order)) {	throw new RuntimeException("Bulk write operation test failed. Transaction was committed, but " + "some objects were incorrectly persisted/loaded to/from Cassandra");	}	}	}	
running pojo strategy transaction delete tests 

}	}	for (Long productId : ordersPerProduct.keySet()) {	for (CacheEntryImpl<Long, ProductOrder> entry : ordersPerProduct.get(productId)) {	order = orders.get(entry.getKey());	if (!entry.getValue().equals(order)) {	throw new RuntimeException("Bulk write operation test failed. Transaction was committed, but " + "some objects were incorrectly persisted/loaded to/from Cassandra");	}	}	}	
running single delete tests 

productStore.sessionEnd(true);	orderStore.sessionEnd(true);	if (productStore.load(deletedProduct.getId()) != null || orderStore.load(deletedOrder.getId()) != null) {	throw new RuntimeException("Single delete operation test failed. Transaction was committed, but " + "objects were not deleted from Cassandra");	}	products = (Map<Long, Product>)productStore.loadAll(productIds);	orders = (Map<Long, ProductOrder>)orderStore.loadAll(orderIds);	if (products.get(deletedProduct.getId()) != null || orders.get(deletedOrder.getId()) != null) {	throw new RuntimeException("Single delete operation test failed. Transaction was committed, but " + "objects were not deleted from Cassandra");	}	
single delete tests passed 

productStore.sessionEnd(true);	orderStore.sessionEnd(true);	if (productStore.load(deletedProduct.getId()) != null || orderStore.load(deletedOrder.getId()) != null) {	throw new RuntimeException("Single delete operation test failed. Transaction was committed, but " + "objects were not deleted from Cassandra");	}	products = (Map<Long, Product>)productStore.loadAll(productIds);	orders = (Map<Long, ProductOrder>)orderStore.loadAll(orderIds);	if (products.get(deletedProduct.getId()) != null || orders.get(deletedOrder.getId()) != null) {	throw new RuntimeException("Single delete operation test failed. Transaction was committed, but " + "objects were not deleted from Cassandra");	}	
running bulk delete tests 

if (products == null || products.isEmpty() || orders == null || orders.isEmpty()) {	throw new RuntimeException("Bulk delete operation test failed. Transaction wasn't committed yet, but " + "objects were already deleted from Cassandra");	}	orderStore.sessionEnd(true);	productStore.sessionEnd(true);	products = (Map<Long, Product>)productStore.loadAll(productIds);	orders = (Map<Long, ProductOrder>)orderStore.loadAll(orderIds);	if ((products != null && !products.isEmpty()) || (orders != null && !orders.isEmpty())) {	throw new RuntimeException("Bulk delete operation test failed. Transaction was committed, but " + "objects were not deleted from Cassandra");	}	
bulk delete tests passed 

if (products == null || products.isEmpty() || orders == null || orders.isEmpty()) {	throw new RuntimeException("Bulk delete operation test failed. Transaction wasn't committed yet, but " + "objects were already deleted from Cassandra");	}	orderStore.sessionEnd(true);	productStore.sessionEnd(true);	products = (Map<Long, Product>)productStore.loadAll(productIds);	orders = (Map<Long, ProductOrder>)orderStore.loadAll(orderIds);	if ((products != null && !products.isEmpty()) || (orders != null && !orders.isEmpty())) {	throw new RuntimeException("Bulk delete operation test failed. Transaction was committed, but " + "objects were not deleted from Cassandra");	}	
pojo strategy transaction delete tests passed 

========================= ignite sample_6984 =========================

String ts = st.nextToken();	String hash = st.nextToken();	String s = ts + ':' + secretKey;	try {	MessageDigest md = MessageDigest.getInstance("SHA-1");	md.update(s.getBytes(UTF_8));	String compHash = encodeBase64(md.digest());	return hash.equalsIgnoreCase(compHash);	}	catch (NoSuchAlgorithmException e) {	
failed to check authentication signature 

========================= ignite sample_4956 =========================

private void undoLocks(boolean dist, boolean rollback) {	if (dist && tx == null) cctx.nearTx().removeLocks(lockVer, keys);	else {	if (rollback && tx != null) {	if (tx.setRollbackOnly()) {	
marked transaction as rollback only because locks could not be acquired 

private void undoLocks(boolean dist, boolean rollback) {	if (dist && tx == null) cctx.nearTx().removeLocks(lockVer, keys);	else {	if (rollback && tx != null) {	if (tx.setRollbackOnly()) {	}	
transaction was not marked rollback only while locks were not acquired 

private boolean filter(GridCacheEntryEx cached) {	try {	if (!cctx.isAll(cached, filter)) {	
filter didn t pass for entry will fail lock 

while (true) {	GridCacheEntryEx cached = entries.get(i);	try {	if (!locked(cached)) {	if (log.isDebugEnabled()) log.debug("Lock is still not acquired for entry (will keep waiting) [entry=" + cached + ", fut=" + this + ']');	return false;	}	break;	}	catch (GridCacheEntryRemovedException ignore) {	
got removed entry in onownerchanged method will retry 

if (!DONE_UPD.compareAndSet(this, 0, 1)) return false;	if (!success) undoLocks(distribute, true);	if (tx != null) {	cctx.tm().txContext(tx);	if (restoreTimeout && tx.trackTimeout()) {	boolean add = tx.addTimeoutHandler();	assert add;	}	}	if (super.onDone(success, err)) {	
completing future 

GridNearLockMapping map = null;	for (KeyCacheObject key : keys) {	GridNearLockMapping updated = map( key, map, topVer);	if (updated != map) {	mappings.add(updated);	if (tx != null && updated.node().isLocal()) tx.nearLocallyMapped(true);	}	map = updated;	}	if (isDone()) {	
abandoning re map because future is done 

GridNearLockRequest req = null;	Collection<KeyCacheObject> distributedKeys = new ArrayList<>(mappedKeys.size());	boolean explicit = false;	for (KeyCacheObject key : mappedKeys) {	IgniteTxKey txKey = cctx.txKey(key);	while (true) {	GridNearCacheEntry entry = null;	try {	entry = cctx.near().entryExx(key, topVer);	if (!cctx.isAll(entry, filter)) {	
entry being locked did not pass filter will not lock 

if (cand != null) {	if (tx == null && !cand.reentry()) cctx.mvcc().addExplicitLock(threadId,cand,topVer);	IgniteBiTuple<GridCacheVersion, CacheObject> val = entry.versionedValue();	if (val == null) {	GridDhtCacheEntry dhtEntry = dht().peekExx(key);	try {	if (dhtEntry != null) val = dhtEntry.versionedValue(topVer);	}	catch (GridCacheEntryRemovedException ignored) {	assert dhtEntry.obsolete() : dhtEntry;	
got removed exception for dht entry in map will ignore 

}	else {	if (timedOut) return;	explicit = tx != null && !entry.hasLockCandidate(tx.xidVersion());	}	if (explicit) tx.addKeyMapping(txKey, mapping.node());	break;	}	catch (GridCacheEntryRemovedException ignored) {	assert entry.obsolete() : "Got removed exception on non-obsolete entry: " + entry;	
got removed entry in lockasync method will retry 

synchronized (this) {	map = mappings.poll();	}	if (map == null) return;	final GridNearLockRequest req = map.request();	final Collection<KeyCacheObject> mappedKeys = map.distributedKeys();	final ClusterNode node = map.node();	if (filter != null && filter.length != 0) req.filter(filter, cctx);	if (node.isLocal()) {	req.miniId(-1);	
before locally locking near request 

tx.entry(cctx.txKey(k)).filters(pass ? CU.empty0() : CU.alwaysFalse0Arr());	}	if (record) {	if (cctx.events().isRecordable(EVT_CACHE_OBJECT_READ)) cctx.events().addEvent( entry.partition(), entry.key(), tx, null, EVT_CACHE_OBJECT_READ, newVal, newVal != null, oldVal, hasBytes, CU.subjectId(tx, cctx.shared()), null, inTx() ? tx.resolveTaskName() : null, keepBinary);	if (cctx.statisticsEnabled()) cctx.cache().metrics0().onRead(oldVal != null);	}	if (log.isDebugEnabled()) log.debug("Processed response for entry [res=" + res + ", entry=" + entry + ']');	break;	}	catch (GridCacheEntryRemovedException ignored) {	
failed to add candidates because entry was removed will renew 

========================= ignite sample_3840 =========================

private void doTestPartitionCounterOperation(CacheConfiguration<Object, Object> ccfg) throws Exception {	ignite(0).createCache(ccfg);	try {	long seed = System.currentTimeMillis();	Random rnd = new Random(seed);	
random seed 

========================= ignite sample_1484 =========================

private void manyClientsSequentially() throws Exception {	client = true;	List<Ignite> clients = new ArrayList<>();	final int CLIENTS = 50;	int idx = SRVS;	ThreadLocalRandom rnd = ThreadLocalRandom.current();	for (int i = 0; i < CLIENTS; i++) {	Ignite ignite = startGrid(idx++);	
started node 

ThreadLocalRandom rnd = ThreadLocalRandom.current();	for (int i = 0; i < CLIENTS; i++) {	Ignite ignite = startGrid(idx++);	assertTrue(ignite.configuration().isClientMode());	clients.add(ignite);	IgniteCache<Object, Object> cache = ignite.cache(DEFAULT_CACHE_NAME);	Integer key = rnd.nextInt(0, 1000);	cache.put(key, i);	assertNotNull(cache.get(key));	}	
all clients started 

private void checkNodes(int expCnt) {	assertEquals(expCnt, G.allGrids().size());	long topVer = -1L;	for (Ignite ignite : G.allGrids()) {	
check node 

final AtomicReference<Throwable> err = new AtomicReference<>();	final int THREADS = 50;	final CountDownLatch latch = new CountDownLatch(THREADS);	try {	IgniteInternalFuture<?> fut = GridTestUtils.runMultiThreadedAsync(new Callable<Object>() {	boolean counted = false;	try {	int nodeIdx = idx.getAndIncrement();	Thread.currentThread().setName("client-thread-node-" + nodeIdx);	try (Ignite ignite = startGrid(nodeIdx)) {	
started node 

cache.put(key, iter++);	assertNotNull(cache.get(key));	latch.countDown();	counted = true;	while (!stop.get() && err.get() == null) {	key = rnd.nextInt(0, 1000);	cache.put(key, iter++);	assertNotNull(cache.get(key));	Thread.sleep(1);	}	
stopping node 

key = rnd.nextInt(0, 1000);	cache.put(key, iter++);	assertNotNull(cache.get(key));	Thread.sleep(1);	}	}	return null;	}	catch (Throwable e) {	err.compareAndSet(null, e);	
unexpected error in client thread 

catch (Throwable e) {	err.compareAndSet(null, e);	throw e;	}	finally {	if (!counted) latch.countDown();	}	}	}, THREADS, "client-thread");	assertTrue(latch.await(getTestTimeout(), TimeUnit.MILLISECONDS));	
all clients started 

assertTrue(latch.await(getTestTimeout(), TimeUnit.MILLISECONDS));	Thread.sleep(10_000);	Throwable err0 = err.get();	if (err0 != null) throw err0;	boolean wait = GridTestUtils.waitForCondition(new GridAbsPredicate() {	try {	checkNodes(SRVS + THREADS);	return true;	}	catch (AssertionFailedError e) {	
check failed will retry 

========================= ignite sample_1191 =========================

if (prj == null) {	synchronized (mux) {	if ((prj = jobMetaPrj) == null) {	GridCacheAdapter<HadoopJobId, HadoopJobMetadata> sysCache = ctx.kernalContext().cache() .internalCache(CU.SYS_CACHE_HADOOP_MR);	assert sysCache != null;	mrPlanner = ctx.planner();	try {	ctx.kernalContext().resource().injectGeneric(mrPlanner);	}	catch (IgniteCheckedException e) {	
failed to inject resources 

if (log.isDebugEnabled()) log.debug("Submitting job metadata [jobId=" + jobId + ", meta=" + meta + ']');	long jobStart = U.currentTimeMillis();	HadoopPerformanceCounter perfCntr = HadoopPerformanceCounter.getCounter(meta.counters(), ctx.localNodeId());	perfCntr.clientSubmissionEvents(info);	perfCntr.onJobPrepare(jobPrepare);	perfCntr.onJobStart(jobStart);	if (jobMetaCache().getAndPutIfAbsent(jobId, meta) != null) throw new IgniteCheckedException("Failed to submit job. Job with the same ID already exists: " + jobId);	return completeFut;	}	catch (IgniteCheckedException e) {	
failed to submit job 

private void processNodeLeft(DiscoveryEvent evt) {	if (log.isDebugEnabled()) log.debug("Processing discovery event [locNodeId=" + ctx.localNodeId() + ", evt=" + evt + ']');	if (ctx.jobUpdateLeader()) {	boolean checkSetup = evt.eventNode().order() < ctx.localNodeOrder();	Iterable<IgniteCache.Entry<HadoopJobId, HadoopJobMetadata>> entries;	try {	entries = jobMetaCache().localEntries(OFFHEAP_PEEK_MODE);	}	catch (IgniteCheckedException e) {	
failed to get local entries 

if (ctx.kernalContext().discovery().node(nodeId) == null) {	int[] reducers = plan.reducers(nodeId);	if (cancelReducers == null) cancelReducers = new HashSet<>();	for (int rdc : reducers) cancelReducers.add(rdc);	}	}	if (cancelSplits != null || cancelReducers != null) jobMetaCache().invoke(meta.jobId(), new CancelJobProcessor(null, new IgniteCheckedException( "One or more nodes participating in map-reduce job execution failed."), cancelSplits, cancelReducers));	}	}	catch (IgniteCheckedException e) {	
failed to cancel job 

private void printPlan(HadoopJobId jobId, HadoopMapReducePlan plan) {	if (log.isInfoEnabled()) {	
plan for 

if (log.isDebugEnabled()) log.debug("Submitting COMMIT task for execution [locNodeId=" + locNodeId + ", jobId=" + jobId + ']');	ctx.taskExecutor().run(job, Collections.singletonList(info));	break;	}	Collection<HadoopTaskInfo> tasks = reducerTasks(plan.reducers(locNodeId), job);	if (tasks != null) ctx.taskExecutor().run(job, tasks);	break;	}	case PHASE_CANCELLING: {	if (state != null && state.onCancel()) {	
cancelling local task execution for job 

try {	String statWriterClsName = job.info().property(HadoopCommonUtils.JOB_COUNTER_WRITER_PROPERTY);	if (statWriterClsName != null) {	Class<?> cls = ldr.loadClass(statWriterClsName);	HadoopCounterWriter writer = (HadoopCounterWriter)cls.newInstance();	HadoopCounters cntrs = meta.counters();	writer.write(job, cntrs);	}	}	catch (Exception e) {	
can t write statistic due to 

========================= ignite sample_7291 =========================

public void testManyCachesAndNotManyPuts() throws Exception {	try {	IgniteEx ignite = startGrid(0);	ignite.active(true);	
saving initial data to caches 

GridCacheDatabaseSharedManager db = (GridCacheDatabaseSharedManager)(ignite.context().cache().context().database());	Collection<Integer> pageCntObserved = new ArrayList<>();	boolean checkpointWithLowNumOfPagesFound = false;	for (int i = 0; i < 10; i++) {	Random random = new Random();	int d = random.nextInt(PARTS) + PARTS;	int cIdx = random.nextInt(CACHES_IN_GRP);	int gIdx = random.nextInt(GROUPS);	String fullname = "dummyCache" + cIdx + "." + gIdx;	ignite.cache(fullname).put(d, d);	
put to cache value 

final int timeout = 5000;	try {	db.wakeupForCheckpoint("").get(timeout, TimeUnit.MILLISECONDS);	}	catch (IgniteFutureTimeoutCheckedException e) {	continue;	}	int currCpPages = waitForCurrentCheckpointPagesCounterUpdated(db, timeout);	if (currCpPages < 0) continue;	pageCntObserved.add(currCpPages);	
current cp pages 

========================= ignite sample_1715 =========================

}	finally {	localUpdate.unlock();	}	seq.set(newUpBound + 1);	cacheView.put(key, seq);	tx.commit();	return curLocVal;	}	catch (Error | Exception e) {	
failed to get and add 

========================= ignite sample_4281 =========================

if (!deps.isEmpty()) {	for (GridDeployment d : deps) {	if (!d.undeployed()) {	U.error(log, "Found more than one active deployment for the same resource " + "[cls=" + cls + ", depMode=" + depMode + ", dep=" + d + ']');	return null;	}	}	}	deps.addFirst(dep);	if (!cls.getName().equals(alias)) cache.put(cls.getName(), deps);	
created new deployment 

Collection<GridDeployment> doomed = new HashSet<>();	synchronized (mux) {	for (Iterator<ConcurrentLinkedDeque8<GridDeployment>> i1 = cache.values().iterator(); i1.hasNext();) {	ConcurrentLinkedDeque8<GridDeployment> deps = i1.next();	for (Iterator<GridDeployment> i2 = deps.iterator(); i2.hasNext();) {	GridDeployment dep = i2.next();	if (dep.classLoader() == ldr) {	dep.undeploy();	i2.remove();	doomed.add(dep);	
removed undeployed class 

========================= ignite sample_3544 =========================

if (cfg.getBalancer() instanceof GridClientTopologyListener) top.addTopologyListener((GridClientTopologyListener)cfg.getBalancer());	GridSslContextFactory factory = cfg.getSslContextFactory();	if (factory != null) {	try {	sslCtx = factory.createSslContext();	}	catch (SSLException e) {	throw new GridClientException("Failed to create client (unable to create SSL context, " + "check ssl context factory configuration): " + e.getMessage(), e);	}	}	
auto fetch for metrics is enabled without enabling caching for them 

if (cfg.getBalancer() instanceof GridClientTopologyListener) top.addTopologyListener((GridClientTopologyListener)cfg.getBalancer());	GridSslContextFactory factory = cfg.getSslContextFactory();	if (factory != null) {	try {	sslCtx = factory.createSslContext();	}	catch (SSLException e) {	throw new GridClientException("Failed to create client (unable to create SSL context, " + "check ssl context factory configuration): " + e.getMessage(), e);	}	}	
auto fetch for node attributes is enabled without enabling caching for them 

srvs = parseAddresses(cfg.getServers());	routers = parseAddresses(cfg.getRouters());	if (srvs.isEmpty() && routers.isEmpty()) throw new GridClientException("Servers addresses and routers addresses cannot both be empty " + "for client (please fix configuration and restart): " + this);	if (!srvs.isEmpty() && !routers.isEmpty()) throw new GridClientException("Servers addresses and routers addresses cannot both be provided " + "for client (please fix configuration and restart): " + this);	connMgr = createConnectionManager(id, sslCtx, cfg, routers, top, null, routerClient);	try {	tryInitTopology();	}	catch (GridClientException e) {	top.fail(e);	
failed to initialize topology on client start will retry in background 

========================= ignite sample_5185 =========================

private void scan(UriDeploymentScannerContext scanCtx) {	Collection<String> foundFiles = U.newHashSet(tstampCache.size());	long start = U.currentTimeMillis();	processHttp(foundFiles, scanCtx);	
http scanner time in ms 

assert sockFactory != null;	httpsConn.setSSLSocketFactory(sockFactory);	}	if (lastModified != null) conn.setIfModifiedSince(lastModified);	in = conn.getInputStream();	long rcvLastModified = conn.getLastModified();	if (in == null || lastModified != null && (lastModified == rcvLastModified || conn instanceof HttpURLConnection && ((HttpURLConnection)conn).getResponseCode() == HttpURLConnection.HTTP_NOT_MODIFIED)) continue;	tstampCache.put(url, rcvLastModified);	lastModified = rcvLastModified;	if (scanCtx.getLogger().isDebugEnabled()) {	
discovered deployment file or directory 

if (scanCtx.getLogger().isDebugEnabled()) {	}	file = scanCtx.createTempFile(fileName, scanCtx.getDeployDirectory());	file.deleteOnExit();	out = new FileOutputStream(file);	U.copy(in, out);	}	catch (IOException e) {	if (!scanCtx.isCancelled()) {	if (X.hasCause(e, ConnectException.class)) {	
failed to connect to http server connection refused 

file = scanCtx.createTempFile(fileName, scanCtx.getDeployDirectory());	file.deleteOnExit();	out = new FileOutputStream(file);	U.copy(in, out);	}	catch (IOException e) {	if (!scanCtx.isCancelled()) {	if (X.hasCause(e, ConnectException.class)) {	}	else if (X.hasCause(e, UnknownHostException.class)) {	
failed to connect to http server host is unknown 

file.deleteOnExit();	out = new FileOutputStream(file);	U.copy(in, out);	}	catch (IOException e) {	if (!scanCtx.isCancelled()) {	if (X.hasCause(e, ConnectException.class)) {	}	else if (X.hasCause(e, UnknownHostException.class)) {	}	
failed to save file 

assert sockFactory != null;	httpsConn.setSSLSocketFactory(sockFactory);	}	in = conn.getInputStream();	if (in == null) throw new IOException("Failed to open connection: " + U.hidePassword(url.toString()));	dom = tidy.parseDOM(in, null);	}	catch (IOException e) {	if (!scanCtx.isCancelled()) {	if (X.hasCause(e, ConnectException.class)) {	
failed to connect to http server connection refused 

}	in = conn.getInputStream();	if (in == null) throw new IOException("Failed to open connection: " + U.hidePassword(url.toString()));	dom = tidy.parseDOM(in, null);	}	catch (IOException e) {	if (!scanCtx.isCancelled()) {	if (X.hasCause(e, ConnectException.class)) {	}	else if (X.hasCause(e, UnknownHostException.class)) {	
failed to connect to http server host is unknown 

in = conn.getInputStream();	if (in == null) throw new IOException("Failed to open connection: " + U.hidePassword(url.toString()));	dom = tidy.parseDOM(in, null);	}	catch (IOException e) {	if (!scanCtx.isCancelled()) {	if (X.hasCause(e, ConnectException.class)) {	}	else if (X.hasCause(e, UnknownHostException.class)) {	}	
failed to get html page 

if (href != null && !href.isEmpty()) {	URL url = null;	try {	url = new URL(href);	}	catch (MalformedURLException e) {	try {	url = new URL(baseUrl.getProtocol(), baseUrl.getHost(), baseUrl.getPort(), href.charAt(0) == '/' ? href : baseUrl.getFile() + '/' + href);	}	catch (MalformedURLException e1) {	
skipping bad url 

========================= ignite sample_7911 =========================

private void executeSql(Ignite node, String cacheName, String sql) {	
executing ddl 

========================= ignite sample_7570 =========================

private boolean acquire() {	while (true) {	int cnt = activeCnt.get();	if (cnt == 0) {	
ipc io not acquired count was 

private boolean acquire() {	while (true) {	int cnt = activeCnt.get();	if (cnt == 0) {	return false;	}	if (activeCnt.compareAndSet(cnt, cnt + 1)) {	
ipc io acquired 

public void release() {	while (true) {	int cnt = activeCnt.get();	if (cnt == 0) {	
ipc io not released count was 

public void release() {	while (true) {	int cnt = activeCnt.get();	if (cnt == 0) {	return;	}	if (activeCnt.compareAndSet(cnt, cnt - 1)) {	if (cnt == 1) {	ipcCache.remove(endpointAddr, this);	
ipc io stopping as unused 

while (true) {	int cnt = activeCnt.get();	if (cnt == 0) {	return;	}	if (activeCnt.compareAndSet(cnt, cnt - 1)) {	if (cnt == 1) {	ipcCache.remove(endpointAddr, this);	stop();	}	
ipc io released 

private void stop() {	close0(null);	if (reader != null) {	try {	U.interrupt(reader);	U.join(reader);	reader = null;	}	catch (IgniteInterruptedCheckedException ignored) {	Thread.currentThread().interrupt();	
got interrupted while waiting for reader thread to shut down will return 

========================= ignite sample_7209 =========================

if (disconnected()) rmtNodes.clear();	for (TcpDiscoveryNode n : top) {	if (n.order() > 0) n.visible(true);	rmtNodes.put(n.id(), n);	}	topHist.clear();	nodeAdded = true;	if (msg.topologyHistory() != null) topHist.putAll(msg.topologyHistory());	}	else {	
discarding node added message with empty topology 

}	else {	}	}	else if (log.isDebugEnabled()) log.debug("Discarding node added message (this message has already been processed) " + "[msg=" + msg + ", locNode=" + locNode + ']');	}	else {	if (nodeAdded()) {	boolean topChanged = rmtNodes.putIfAbsent(newNodeId, node) == null;	if (topChanged) {	
added new node to topology 

boolean topChanged = rmtNodes.putIfAbsent(newNodeId, node) == null;	if (topChanged) {	DiscoveryDataPacket dataPacket = msg.gridDiscoveryData();	if (dataPacket != null && dataPacket.hasJoiningNodeData()) {	if (joining()) delayDiscoData.add(dataPacket);	else spi.onExchange(dataPacket, U.resolveClassLoader(spi.ignite().configuration()));	}	}	}	else {	
ignore topology message local node not added to topology 

if (spi.locNodeVer.equals(node.version())) node.version(spi.locNodeVer);	evt = true;	}	else {	if (log.isDebugEnabled()) log.debug("Skip node join event, node already joined [msg=" + msg + ", node=" + node + ']');	assert node.order() == topVer : node;	}	Collection<ClusterNode> top = updateTopologyHistory(topVer, msg);	assert top != null && top.contains(node) : "Topology does not contain node [msg=" + msg + ", node=" + node + ", top=" + top + ']';	if (state != CONNECTED) {	
discarding node add finished message join process is not finished 

assert top != null && top.contains(node) : "Topology does not contain node [msg=" + msg + ", node=" + node + ", top=" + top + ']';	if (state != CONNECTED) {	return;	}	if (evt) {	notifyDiscovery(EVT_NODE_JOINED, topVer, node, top);	spi.stats.onNodeJoined();	}	}	else {	
ignore topology message local node not added to topology 

private void processNodeLeftMessage(TcpDiscoveryNodeLeftMessage msg) {	if (getLocalNodeId().equals(msg.creatorNodeId())) {	
received node left message for local node 

else {	if (spi.getSpiContext().isStopping()) return;	if (nodeAdded()) {	TcpDiscoveryNode node = rmtNodes.remove(msg.creatorNodeId());	if (node == null) {	if (log.isDebugEnabled()) log.debug("Discarding node left message since node is not found [msg=" + msg + ']');	return;	}	Collection<ClusterNode> top = updateTopologyHistory(msg.topologyVersion(), msg);	if (state != CONNECTED) {	
discarding node left message join process is not finished 

return;	}	Collection<ClusterNode> top = updateTopologyHistory(msg.topologyVersion(), msg);	if (state != CONNECTED) {	return;	}	notifyDiscovery(EVT_NODE_LEFT, msg.topologyVersion(), node, top);	spi.stats.onNodeLeft();	}	else {	
ignore topology message local node not added to topology 

}	if (nodeAdded()) {	if (!getLocalNodeId().equals(msg.creatorNodeId())) {	TcpDiscoveryNode node = rmtNodes.remove(msg.failedNodeId());	if (node == null) {	if (log.isDebugEnabled()) log.debug("Discarding node failed message since node is not found [msg=" + msg + ']');	return;	}	Collection<ClusterNode> top = updateTopologyHistory(msg.topologyVersion(), msg);	if (state != CONNECTED) {	
discarding node failed message join process is not finished 

}	if (msg.warning() != null) {	ClusterNode creatorNode = rmtNodes.get(msg.creatorNodeId());	U.warn(log, "Received EVT_NODE_FAILED event with warning [" + "nodeInitiatedEvt=" + (creatorNode != null ? creatorNode : msg.creatorNodeId()) + ", msg=" + msg.warning() + ']');	}	notifyDiscovery(EVT_NODE_FAILED, msg.topologyVersion(), node, top);	spi.stats.onNodeFailed();	}	}	else {	
ignore topology message local node not added to topology 

private void processMetricsUpdateMessage(TcpDiscoveryMetricsUpdateMessage msg) {	if (spi.getSpiContext().isStopping()) return;	if (getLocalNodeId().equals(msg.creatorNodeId())) {	assert msg.senderNodeId() != null;	
received metrics response 

else {	if (joinLatch.getCount() > 0) {	if (msg.success()) {	for (TcpDiscoveryAbstractMessage pendingMsg : msg.pendingMessages()) {	if (log.isDebugEnabled()) log.debug("Process pending message on connect [msg=" + pendingMsg + ']');	processDiscoveryMessage(pendingMsg);	}	assert joinLatch.getCount() == 0 : msg;	}	}	
discarding reconnect message reconnect is completed 

if (msg.success()) {	for (TcpDiscoveryAbstractMessage pendingMsg : msg.pendingMessages()) {	if (log.isDebugEnabled()) log.debug("Process pending message on connect [msg=" + pendingMsg + ']');	processDiscoveryMessage(pendingMsg);	}	assert joinLatch.getCount() == 0 : msg;	}	}	}	}	
discarding reconnect message for another client 

DiscoverySpiListener lsnr = spi.lsnr;	if (lsnr != null) {	UUID nodeId = msg.creatorNodeId();	TcpDiscoveryNode node = nodeId.equals(getLocalNodeId()) ? locNode : rmtNodes.get(nodeId);	if (node != null && node.visible()) {	try {	DiscoverySpiCustomMessage msgObj = msg.message(spi.marshaller(), U.resolveClassLoader(spi.ignite().configuration()));	notifyDiscovery(EVT_DISCOVERY_CUSTOM_EVT, topVer, node, allVisibleNodes(), msgObj);	}	catch (Throwable e) {	
failed to unmarshal discovery custom message 

UUID nodeId = msg.creatorNodeId();	TcpDiscoveryNode node = nodeId.equals(getLocalNodeId()) ? locNode : rmtNodes.get(nodeId);	if (node != null && node.visible()) {	try {	DiscoverySpiCustomMessage msgObj = msg.message(spi.marshaller(), U.resolveClassLoader(spi.ignite().configuration()));	notifyDiscovery(EVT_DISCOVERY_CUSTOM_EVT, topVer, node, allVisibleNodes(), msgObj);	}	catch (Throwable e) {	}	}	
received metrics from unknown node 

assert nodeId != null;	assert metrics != null;	assert isLocDaemon || cacheMetrics != null;	TcpDiscoveryNode node = nodeId.equals(getLocalNodeId()) ? locNode : rmtNodes.get(nodeId);	if (node != null && node.visible()) {	node.setMetrics(metrics);	if (!isLocDaemon) node.setCacheMetrics(cacheMetrics);	node.lastUpdateTime(tstamp);	notifyDiscovery(EVT_NODE_METRICS_UPDATED, topVer, node, allVisibleNodes());	}	
received metrics from unknown node 

========================= ignite sample_5664 =========================

public void inject(GridDeployment dep, Class<?> depCls, Object target) throws IgniteCheckedException {	assert target != null;	
injecting resources target 

public void injectCacheName(Object obj, String cacheName) throws IgniteCheckedException {	assert obj != null;	
injecting cache name 

public boolean injectStoreSession(Object obj, CacheStoreSession ses) throws IgniteCheckedException {	assert obj != null;	
injecting cache store session 

public boolean injectFileSystem(Object obj, IgniteFileSystem igfs) throws IgniteCheckedException {	assert obj != null;	
injecting cache store session 

public void inject(Object obj, GridResourceIoc.AnnotationSet annSet, Object... params) throws IgniteCheckedException {	assert obj != null;	
injecting resources obj 

private void cleanup(Object obj, GridResourceIoc.AnnotationSet annSet) throws IgniteCheckedException {	assert obj != null;	
cleaning up resources 

public void inject(GridDeployment dep, Class<?> taskCls, ComputeJob job, ComputeTaskSession ses, GridJobContextImpl jobCtx) throws IgniteCheckedException {	
injecting resources job 

public void inject(GridDeployment dep, ComputeTask<?, ?> task, GridTaskSessionImpl ses, ComputeLoadBalancer balancer, ComputeTaskContinuousMapper mapper) throws IgniteCheckedException {	
injecting resources task 

========================= ignite sample_4833 =========================

assert size > 0 : "Size cannot be less than 1 byte";	log = parent.getLogger(IpcSharedMemorySpace.class);	opSize = size;	shmemPtr = IpcSharedMemoryUtils.allocateSystemResources(tokFileName, size, DEBUG && log.isDebugEnabled());	shmemId = IpcSharedMemoryUtils.sharedMemoryId(shmemPtr);	semId = IpcSharedMemoryUtils.semaphoreId(shmemPtr);	isReader = reader;	this.tokFileName = tokFileName;	this.readerPid = readerPid;	this.writerPid = writerPid;	
shared memory space has been created 

private void close0(boolean force) {	if (!closed.compareAndSet(false, true)) return;	IpcSharedMemoryUtils.ipcClose(shmemPtr);	lock.writeLock().lock();	try {	IpcSharedMemoryUtils.freeSystemResources(tokFileName, shmemPtr, force);	}	finally {	lock.writeLock().unlock();	}	
shared memory space has been closed 

========================= ignite sample_3421 =========================

else {	TestKey key1 = new TestKey(rnd.nextLong(KEY_RANGE));	TestKey key2 = new TestKey(key1.key() + 1);	cacheOperation(rnd, cache1, key1);	cacheOperation(rnd, cache2, key2);	}	tx.commit();	}	}	catch (CacheException | IgniteException e) {	
update error 

case 3: cache.get(key);	break;	default: assert false;	}	}	}, 10, "tx-thread");	long stopTime = System.currentTimeMillis() + 3 * 60_000;	long topVer = ignite0.cluster().topologyVersion();	boolean failed = false;	while (System.currentTimeMillis() < stopTime) {	
start node 

boolean failed = false;	while (System.currentTimeMillis() < stopTime) {	IgniteKernal ignite = (IgniteKernal)startGrid(GRID_CNT);	assertFalse(ignite.configuration().isClientMode());	topVer++;	IgniteInternalFuture<?> affFut = ignite.context().cache().context().exchange().affinityReadyFuture( new AffinityTopologyVersion(topVer));	try {	if (affFut != null) affFut.get(30_000);	}	catch (IgniteFutureTimeoutCheckedException ignored) {	
failed to wait for affinity future after start 

topVer++;	IgniteInternalFuture<?> affFut = ignite.context().cache().context().exchange().affinityReadyFuture( new AffinityTopologyVersion(topVer));	try {	if (affFut != null) affFut.get(30_000);	}	catch (IgniteFutureTimeoutCheckedException ignored) {	failed = true;	break;	}	Thread.sleep(500);	
stop node 

break;	}	Thread.sleep(500);	stopGrid(GRID_CNT);	topVer++;	affFut = ignite0.context().cache().context().exchange().affinityReadyFuture( new AffinityTopologyVersion(topVer));	try {	if (affFut != null) affFut.get(30_000);	}	catch (IgniteFutureTimeoutCheckedException ignored) {	
failed to wait for affinity future after stop 

========================= ignite sample_1109 =========================

private void testRandomOperation(final boolean jcacheApi, final boolean syncNtf, final boolean withFilter, final boolean asyncCallback, final boolean keepBinary) throws Exception {	if (keepBinary && !(getConfiguration().getMarshaller() == null || getConfiguration().getMarshaller().getClass() == BinaryMarshaller.class)) return;	runInAllDataModes(new TestRunnable() {	long seed = System.currentTimeMillis();	Random rnd = new Random(seed);	
random seed 

ContinuousQuery<Object, Object> qry = new ContinuousQuery<>();	final List<CacheEntryEvent<?, ?>> evts = new CopyOnWriteArrayList<>();	qry.setLocalListener(new CacheEntryUpdatedListener<Object, Object>() {	throws CacheEntryListenerException {	for (CacheEntryEvent<?, ?> e : events) evts.add(e);	}	});	Object key = key(1);	try (QueryCursor qryCur = cache.query(qry)) {	for (int i = 0; i < ITERATION_CNT; i++) {	
start iteration 

checkEvent(evts.get(5), EventType.UPDATED, value(4), value(3));	checkEvent(evts.get(6), REMOVED, null, value(4));	checkEvent(evts.get(7), CREATED, value(5), null);	checkEvent(evts.get(8), EventType.UPDATED, value(6), value(5));	cache.remove(key);	cache.remove(key);	while (evts.size() != 10) {	Thread.sleep(100);	}	evts.clear();	
finish iteration 

========================= ignite sample_1959 =========================

GridCacheEntryInfo info = entry.info();	if (info == null) {	assert entry.obsolete() : entry;	continue;	}	if (!info.isNew()) res.addInfo(info);	ctx.evicts().touch(entry, msg.topologyVersion());	break;	}	catch (GridCacheEntryRemovedException ignore) {	
got removed entry 

assert entry.obsolete() : entry;	continue;	}	if (!info.isNew()) res.addInfo(info);	ctx.evicts().touch(entry, msg.topologyVersion());	break;	}	catch (GridCacheEntryRemovedException ignore) {	}	catch (GridDhtInvalidPartitionException ignore) {	
local node is no longer an owner 

long ttl = CU.ttlForLoad(plc);	if (ttl == CU.TTL_ZERO) return;	CacheObject cacheVal = ctx.toCacheObject(val);	entry = entryEx(key);	entry.initialValue(cacheVal, ver, ttl, CU.EXPIRE_TIME_CALCULATE, false, topVer, replicate ? DR_LOAD : DR_NONE, false);	}	catch (IgniteCheckedException e) {	throw new IgniteException("Failed to put cache value: " + entry, e);	}	catch (GridCacheEntryRemovedException ignore) {	
got removed entry during loadcache will ignore 

throw new IgniteException("Failed to put cache value: " + entry, e);	}	catch (GridCacheEntryRemovedException ignore) {	}	finally {	if (entry != null) entry.context().evicts().touch(entry, topVer);	part.release();	ctx.shared().database().checkpointReadUnlock();	}	}	
will node load entry into cache partition is invalid 

catch (GridCacheEntryRemovedException ignore) {	}	finally {	if (entry != null) entry.context().evicts().touch(entry, topVer);	part.release();	ctx.shared().database().checkpointReadUnlock();	}	}	}	catch (GridDhtInvalidPartitionException e) {	
ignoring entry for partition that does not belong key val err 

else {	AffinityTopologyVersion topVer = ctx.shared().exchange().lastTopologyFuture().initialVersion();	assert topVer.compareTo(req.topologyVersion()) > 0 : "Wrong ready topology version for " + "invalid partitions response [topVer=" + topVer + ", req=" + req + ']';	res = new GridNearSingleGetResponse(ctx.cacheId(), req.futureId(), topVer, null, true, req.addDeploymentInfo());	}	}	catch (NodeStoppingException ignored) {	return;	}	catch (IgniteCheckedException e) {	
failed processing get request 

GridNearGetResponse res = new GridNearGetResponse(ctx.cacheId(), req.futureId(), req.miniId(), req.version(), req.deployInfo() != null);	GridDhtFuture<Collection<GridCacheEntryInfo>> fut = (GridDhtFuture<Collection<GridCacheEntryInfo>>)f;	try {	Collection<GridCacheEntryInfo> entries = fut.get();	res.entries(entries);	}	catch (NodeStoppingException ignored) {	return;	}	catch (IgniteCheckedException e) {	
failed processing get request 

for (IgniteBiTuple<KeyCacheObject, GridCacheVersion> t : e.getValue()) req.addNearEntry(t.get1(), t.get2());	}	}	}	for (Map.Entry<ClusterNode, GridCacheTtlUpdateRequest> req : reqMap.entrySet()) {	try {	ctx.io().send(req.getKey(), req.getValue(), ctx.ioPolicy());	}	catch (IgniteCheckedException e) {	if (e instanceof ClusterTopologyCheckedException) {	
failed to send ttc update request node left 

}	}	}	for (Map.Entry<ClusterNode, GridCacheTtlUpdateRequest> req : reqMap.entrySet()) {	try {	ctx.io().send(req.getKey(), req.getValue(), ctx.ioPolicy());	}	catch (IgniteCheckedException e) {	if (e instanceof ClusterTopologyCheckedException) {	}	
failed to send ttl update request 

GridCacheEntryEx entry = null;	try {	while (true) {	try {	entry = cache.entryEx(keys.get(i));	entry.unswap(false);	entry.updateTtl(vers.get(i), ttl);	break;	}	catch (GridCacheEntryRemovedException ignore) {	
got removed entry 

while (true) {	try {	entry = cache.entryEx(keys.get(i));	entry.unswap(false);	entry.updateTtl(vers.get(i), ttl);	break;	}	catch (GridCacheEntryRemovedException ignore) {	}	catch (GridDhtInvalidPartitionException e) {	
got griddhtinvalidpartitionexception 

catch (GridDhtInvalidPartitionException e) {	break;	}	}	}	finally {	if (entry != null) cache.context().evicts().touch(entry, AffinityTopologyVersion.NONE);	}	}	catch (IgniteCheckedException e) {	
failed to unswap entry 

========================= ignite sample_3821 =========================

private void clearCaches() {	for (int i = 0; i < NODES_CNT; i++) try {	grid(i).cache(PARTITIONED_CACHE_NAME).clear();	} catch (IgniteException e) {	
cache clear failed 

========================= ignite sample_351 =========================

IgnitePageStoreManager pageStore = shared.pageStore();	U.sleep(1_000);	dbMgr.enableCheckpoints(false).get();	ig.cache(cacheName).put(0, 0);	PageMemory mem = shared.database().dataRegion(null).pageMemory();	IgniteBiTuple<Map<FullPageId, Integer>, WALPointer> res;	try {	res = runCheckpointing(ig, (PageMemoryImpl)mem, pageStore, shared.wal(), shared.cache().cache(cacheName).context().cacheId());	}	catch (Throwable th) {	
error while running checkpointing 

========================= ignite sample_1723 =========================

Thread.sleep(ThreadLocalRandom.current().nextLong(500) + 1);	stopGrid(stopIdx.incrementAndGet());	return null;	}	}, 3, "stop-srv");	final AtomicInteger startIdx = new AtomicInteger(initNodes);	IgniteInternalFuture startFut = GridTestUtils.runMultiThreadedAsync(new Callable<Void>() {	ThreadLocalRandom rnd = ThreadLocalRandom.current();	int nodeIdx = startIdx.incrementAndGet();	if (rnd.nextInt(3) == 0) {	
start client 

return null;	}	}, 3, "stop-srv");	final AtomicInteger startIdx = new AtomicInteger(initNodes);	IgniteInternalFuture startFut = GridTestUtils.runMultiThreadedAsync(new Callable<Void>() {	ThreadLocalRandom rnd = ThreadLocalRandom.current();	int nodeIdx = startIdx.incrementAndGet();	if (rnd.nextInt(3) == 0) {	client.set(true);	}	
start server 

}, 3, "stop-srv");	final AtomicInteger startIdx = new AtomicInteger(initNodes);	IgniteInternalFuture startFut = GridTestUtils.runMultiThreadedAsync(new Callable<Void>() {	ThreadLocalRandom rnd = ThreadLocalRandom.current();	int nodeIdx = startIdx.incrementAndGet();	if (rnd.nextInt(3) == 0) {	client.set(true);	}	startGrid(nodeIdx);	if (rnd.nextBoolean()) {	
stop started node 

startGrid(srvs + i);	}	final int threads = 8;	final int initNodes = srvs + clients;	mergeExchangeWaitVersion(srv0, initNodes + threads);	final AtomicInteger idx = new AtomicInteger(initNodes);	IgniteInternalFuture fut = GridTestUtils.runMultiThreadedAsync(new Callable<Void>() {	ThreadLocalRandom rnd = ThreadLocalRandom.current();	int nodeIdx = idx.incrementAndGet();	if (rnd.nextInt(3) == 0) {	
start client 

final int threads = 8;	final int initNodes = srvs + clients;	mergeExchangeWaitVersion(srv0, initNodes + threads);	final AtomicInteger idx = new AtomicInteger(initNodes);	IgniteInternalFuture fut = GridTestUtils.runMultiThreadedAsync(new Callable<Void>() {	ThreadLocalRandom rnd = ThreadLocalRandom.current();	int nodeIdx = idx.incrementAndGet();	if (rnd.nextInt(3) == 0) {	client.set(true);	}	
start server 

public void testMergeStartStopRandomClientsServers() throws Exception {	for (int iter = 0; iter < 3; iter++) {	
iteration 

final AtomicInteger idx = new AtomicInteger(initNodes);	final ConcurrentHashSet<Integer> stopNodes = new ConcurrentHashSet<>();	IgniteInternalFuture fut = GridTestUtils.runMultiThreadedAsync(new Callable<Void>() {	ThreadLocalRandom rnd = ThreadLocalRandom.current();	if (rnd.nextBoolean()) {	Integer stopIdx;	for (;;) {	stopIdx = rnd.nextInt(initNodes - 1) + 1;	if (stopNodes.add(stopIdx)) break;	}	
stop node 

Integer stopIdx;	for (;;) {	stopIdx = rnd.nextInt(initNodes - 1) + 1;	if (stopNodes.add(stopIdx)) break;	}	stopGrid(getTestIgniteInstanceName(stopIdx), true, false);	}	else {	int nodeIdx = idx.incrementAndGet();	if (rnd.nextInt(5) == 0) {	
start client 

stopIdx = rnd.nextInt(initNodes - 1) + 1;	if (stopNodes.add(stopIdx)) break;	}	stopGrid(getTestIgniteInstanceName(stopIdx), true, false);	}	else {	int nodeIdx = idx.incrementAndGet();	if (rnd.nextInt(5) == 0) {	client.set(true);	}	
start server 

private void concurrentStart(final boolean withClients) throws Exception {	for (int i = 0; i < 5; i++) {	
iteration 

private IgniteInternalFuture startGridsAsync(Ignite node, int startIdx, int cnt) throws Exception {	GridCompoundFuture fut = new GridCompoundFuture();	for (int i = 0; i < cnt; i++) {	final CountDownLatch latch = new CountDownLatch(1);	node.events().localListen(new IgnitePredicate<Event>() {	
got event 

GridCompoundFuture fut = new GridCompoundFuture();	for (int i = 0; i < cnt; i++) {	final CountDownLatch latch = new CountDownLatch(1);	node.events().localListen(new IgnitePredicate<Event>() {	latch.countDown();	return false;	}	}, EventType.EVT_NODE_JOINED);	final int nodeIdx = startIdx + i;	IgniteInternalFuture fut0 = GridTestUtils.runAsync(new Callable() {	
start new node 

========================= ignite sample_1373 =========================

public void cancel() {	
cancelling grid runnable 

public void join() throws InterruptedException {	
joining grid runnable 

========================= ignite sample_3460 =========================

public void testStartFabricDefault() throws Exception {	try (Ignite ignite = Ignition.start("config/fabric/default-config.xml")) {	
started 

public void testStartDefault() throws Exception {	try (Ignite ignite = Ignition.start("config/default-config.xml")) {	
started 

========================= ignite sample_7970 =========================

private void requestPartitions(final RebalanceFuture fut, GridDhtPreloaderAssignments assigns) {	assert fut != null;	if (topologyChanged(fut)) {	fut.cancel();	return;	}	if (!ctx.kernalContext().grid().isRebalanceEnabled()) {	
cancel partition demand because rebalance disabled on current node 

ctx.io().sendOrderedMessage(node, rebalanceTopics.get(finalCnt), initD, grp.ioPolicy(), initD.timeout());	synchronized (fut) {	if (fut.isDone()) fut.cleanupRemoteContexts(node.id());	}	if (log.isDebugEnabled()) log.debug("Requested rebalancing [from node=" + node.id() + ", listener index=" + finalCnt + ", partitions count=" + sParts.get(finalCnt).size() + " (" + partitionsList(sParts.get(finalCnt)) + ")]");	}	}	catch (IgniteCheckedException e) {	ClusterTopologyCheckedException cause = e.getCause(ClusterTopologyCheckedException.class);	if (cause != null) log.warning("Failed to send initial demand request to node. " + e.getMessage());	
failed to send initial demand request to node 

}	if (log.isDebugEnabled()) log.debug("Requested rebalancing [from node=" + node.id() + ", listener index=" + finalCnt + ", partitions count=" + sParts.get(finalCnt).size() + " (" + partitionsList(sParts.get(finalCnt)) + ")]");	}	}	catch (IgniteCheckedException e) {	ClusterTopologyCheckedException cause = e.getCause(ClusterTopologyCheckedException.class);	if (cause != null) log.warning("Failed to send initial demand request to node. " + e.getMessage());	fut.cancel();	}	catch (Throwable th) {	
runtime error caught during initial demand request sending 

if (log.isDebugEnabled()) log.debug("Got entries for invalid partition during " + "preloading (will skip) [p=" + p + ", entry=" + entry + ']');	break;	}	if (grp.sharedGroup() && (cctx == null || cctx.cacheId() != entry.cacheId())) cctx = ctx.cacheContext(entry.cacheId());	if(cctx != null && cctx.statisticsEnabled()) cctx.cache().metrics0().onRebalanceKeyReceived();	}	if (last) {	if (supply.isClean(p)) part.updateCounter(supply.last().get(p));	top.own(part);	fut.partitionDone(id, p);	
finished rebalancing partition 

fut.partitionDone(id, p);	}	}	finally {	part.unlock();	part.release();	}	}	else {	if (last) fut.partitionDone(id, p);	
skipping rebalancing partition state is not moving 

part.unlock();	part.release();	}	}	else {	if (last) fut.partitionDone(id, p);	}	}	else {	fut.partitionDone(id, p);	
skipping rebalancing partition it does not belong on current node 

if (preloadPred == null || preloadPred.apply(entry)) {	if (cached.initialValue( entry.value(), entry.version(), entry.ttl(), entry.expireTime(), true, topVer, cctx.isDrEnabled() ? DR_PRELOAD : DR_NONE, false )) {	cctx.evicts().touch(cached, topVer);	if (cctx.events().isRecordable(EVT_CACHE_REBALANCE_OBJECT_LOADED) && !cached.isInternal()) cctx.events().addEvent(cached.partition(), cached.key(), cctx.localNodeId(), (IgniteUuid)null, null, EVT_CACHE_REBALANCE_OBJECT_LOADED, entry.value(), true, null, false, null, null, null, true);	}	else {	cctx.evicts().touch(cached, topVer);	if (log.isDebugEnabled()) log.debug("Rebalancing entry is already in cache (will ignore) [key=" + cached.key() + ", part=" + p + ']');	}	}	
rebalance predicate evaluated to false for entry will ignore 

}	}	finally {	cctx.shared().database().checkpointReadUnlock();	}	}	catch (GridCacheEntryRemovedException ignored) {	if (log.isDebugEnabled()) log.debug("Entry has been concurrently removed while rebalancing (will ignore) [key=" + cached.key() + ", part=" + p + ']');	}	catch (GridDhtInvalidPartitionException ignored) {	
partition became invalid during rebalancing will ignore 

if (node == null) return;	GridDhtPartitionDemandMessage d = new GridDhtPartitionDemandMessage( -1/* remove supply context signal */, this.topologyVersion(), grp.groupId());	d.timeout(grp.config().getRebalanceTimeout());	try {	for (int idx = 0; idx < ctx.gridConfig().getRebalanceThreadPoolSize(); idx++) {	d.topic(GridCachePartitionExchangeManager.rebalanceTopic(idx));	ctx.io().sendOrderedMessage(node, GridCachePartitionExchangeManager.rebalanceTopic(idx), d, grp.ioPolicy(), grp.config().getRebalanceTimeout());	}	}	catch (IgniteCheckedException ignored) {	
failed to send failover context cleanup request to node 

private void checkIsDone(boolean cancelled) {	if (remaining.isEmpty()) {	sendRebalanceFinishedEvent();	
completed rebalance future 

========================= ignite sample_3749 =========================

private IgniteInternalFuture startSeedNodeAsync() throws IgniteCheckedException {	return GridTestUtils.runAsync(new Callable<Object>() {	Ignite node = startGrid(0);	
creating cache 

private IgniteInternalFuture startSeedNodeAsync() throws IgniteCheckedException {	return GridTestUtils.runAsync(new Callable<Object>() {	Ignite node = startGrid(0);	IgniteCache<Integer, Integer> cache = node.getOrCreateCache(cacheConfig());	
created cache 

private IgniteInternalFuture startSeedNodeAsync() throws IgniteCheckedException {	return GridTestUtils.runAsync(new Callable<Object>() {	Ignite node = startGrid(0);	IgniteCache<Integer, Integer> cache = node.getOrCreateCache(cacheConfig());	Map<Integer, Integer> data = new HashMap<>(CACHE_SIZE);	for (int i = 0; i < CACHE_SIZE; i++) data.put(i, i);	
filling 

private IgniteInternalFuture startSeedNodeAsync() throws IgniteCheckedException {	return GridTestUtils.runAsync(new Callable<Object>() {	Ignite node = startGrid(0);	IgniteCache<Integer, Integer> cache = node.getOrCreateCache(cacheConfig());	Map<Integer, Integer> data = new HashMap<>(CACHE_SIZE);	for (int i = 0; i < CACHE_SIZE; i++) data.put(i, i);	cache.putAll(data);	
filled 

private IgniteInternalFuture startNodeAsync(final int nodeId) throws IgniteCheckedException {	return GridTestUtils.runAsync(new Callable<Object>() {	Ignite node = startGrid(nodeId);	
getting cache 

private IgniteInternalFuture startNodeAsync(final int nodeId) throws IgniteCheckedException {	return GridTestUtils.runAsync(new Callable<Object>() {	Ignite node = startGrid(nodeId);	IgniteCache<Integer, Integer> cache = node.getOrCreateCache(cacheConfig());	
got cache 

private IgniteInternalFuture startNodeAsync(final int nodeId) throws IgniteCheckedException {	return GridTestUtils.runAsync(new Callable<Object>() {	Ignite node = startGrid(nodeId);	IgniteCache<Integer, Integer> cache = node.getOrCreateCache(cacheConfig());	FILLED_LATCH.await();	
got filled 

========================= ignite sample_1133 =========================

if (readerArgs == null) readerArgs = new ReaderArguments(reader, msgId, topVer);	}	rdrFut = addReader ? e.addReader(reader, msgId, topVer) : null;	break;	}	catch (IgniteCheckedException err) {	onDone(err);	return;	}	catch (GridCacheEntryRemovedException ignore) {	
got removed entry when getting a dht value 

========================= ignite sample_3732 =========================

public void testManyAsyncOperations() throws Exception {	try (Ignite client = startGrid(gridCount())) {	assertTrue(client.configuration().isClientMode());	IgniteCache<Object, Object> cache = client.cache(DEFAULT_CACHE_NAME);	final int ASYNC_OPS = cache.getConfiguration(CacheConfiguration.class).getMaxConcurrentAsyncOperations();	
number of async operations 

public void testManyAsyncOperations() throws Exception {	try (Ignite client = startGrid(gridCount())) {	assertTrue(client.configuration().isClientMode());	IgniteCache<Object, Object> cache = client.cache(DEFAULT_CACHE_NAME);	final int ASYNC_OPS = cache.getConfiguration(CacheConfiguration.class).getMaxConcurrentAsyncOperations();	Map<Integer, byte[]> map = new HashMap<>();	for (int i = 0; i < 100; i++) map.put(i, new byte[128]);	for (int iter = 0; iter < 3; iter++) {	
iteration 

========================= ignite sample_1534 =========================

public void testStartStopSameCacheMultinode() throws Exception {	final AtomicInteger idx = new AtomicInteger();	IgniteInternalFuture<?> fut = GridTestUtils.runMultiThreadedAsync(new Callable<Object>() {	int node = idx.getAndIncrement();	Ignite ignite = ignite(node);	Thread.currentThread().setName("start-stop-" + ignite.name());	CacheConfiguration ccfg = new CacheConfiguration(DEFAULT_CACHE_NAME);	ccfg.setName("testStartStop");	for (int i = 0; i < 1000; i++) {	
start cache 

final AtomicInteger idx = new AtomicInteger();	IgniteInternalFuture<?> fut = GridTestUtils.runMultiThreadedAsync(new Callable<Object>() {	int node = idx.getAndIncrement();	Ignite ignite = ignite(node);	Thread.currentThread().setName("start-stop-" + ignite.name());	CacheConfiguration ccfg = new CacheConfiguration(DEFAULT_CACHE_NAME);	ccfg.setName("testStartStop");	for (int i = 0; i < 1000; i++) {	try (IgniteCache<Object, Object> cache = ignite.getOrCreateCache(ccfg)) {	}	
stopped cache 

========================= ignite sample_845 =========================

public void onPage(@Nullable UUID nodeId, @Nullable Collection<?> data, @Nullable Throwable err, boolean finished) {	if (isCancelled()) return;	
received query result page nodeId data err finished 

========================= ignite sample_4223 =========================

else if (affinityBackupFilter == null && backupFilter == null) res.add(next.get2());	}	}	if (res.size() < primaryAndBackups && nodes.size() >= primaryAndBackups && exclNeighbors) {	for (int i = 1; i < lst.size() && res.size() < primaryAndBackups; i++) {	IgniteBiTuple<Long, ClusterNode> next = lst.get(i);	ClusterNode node = next.get2();	if (!res.contains(node)) res.add(next.get2());	}	if (!exclNeighborsWarn) {	
affinity function excludeneighbors property is ignored because topology has no enough nodes to assign backups 

========================= ignite sample_516 =========================

switch (status) {	case STATUS_SUCCESS: return RestResult.success(res.getResponse());	default: return RestResult.fail(status, res.getError());	}	}	if (resp.code() == 401) return RestResult.fail(STATUS_AUTH_FAILED, "Failed to authenticate in cluster. " + "Please check agent\'s login and password or node port.");	if (resp.code() == 404) return RestResult.fail(STATUS_FAILED, "Failed connect to cluster.");	return RestResult.fail(STATUS_FAILED, "Failed to execute REST command: " + resp.message());	}	catch (ConnectException ignored) {	
failed connect to cluster please ensure that nodes have ignite rest http module in classpath was copied from libs optional to libs folder 

========================= ignite sample_7376 =========================

HadoopShuffleFinishRequest m = (HadoopShuffleFinishRequest)msg;	job(m.jobId()).onShuffleFinishRequest(src, m);	}	else if (msg instanceof HadoopShuffleFinishResponse) {	HadoopShuffleFinishResponse m = (HadoopShuffleFinishResponse)msg;	job(m.jobId()).onShuffleFinishResponse(src);	}	else throw new IllegalStateException("Unknown message type received to Hadoop shuffle [src=" + src + ", msg=" + msg + ']');	}	catch (IgniteCheckedException e) {	
message handling failed 

public void jobFinished(HadoopJobId jobId) {	HadoopShuffleJob job = jobs.remove(jobId);	if (job != null) {	try {	job.close();	}	catch (IgniteCheckedException e) {	
failed to close job 

========================= ignite sample_7233 =========================

private void execute() throws InterruptedException {	testStartTime = System.currentTimeMillis();	
test execution started 

private void execute() throws InterruptedException {	testStartTime = System.currentTimeMillis();	
warm up period started 

Map batchMap = new HashMap(TestsHelper.getBulkOperationSize());	int execTime = TestsHelper.getLoadTestsWarmupPeriod() + TestsHelper.getLoadTestsExecutionTime();	try {	while (true) {	if (System.currentTimeMillis() - testStartTime > execTime) break;	if (warmup && System.currentTimeMillis() - testStartTime > TestsHelper.getLoadTestsWarmupPeriod()) {	warmupFinishTime = System.currentTimeMillis();	startTime = warmupFinishTime;	statReportedTime = warmupFinishTime;	warmup = false;	
warm up period completed 

private void doWork(CacheEntryImpl entry) {	try {	process(cacheStore, entry);	updateMetrics(1);	}	catch (Throwable e) {	
failed to perform single operation 

private void doWork(Object key, Object val) {	try {	process(igniteCache, key, val);	updateMetrics(1);	}	catch (Throwable e) {	
failed to perform single operation 

private void doWork(Collection<CacheEntryImpl> entries) {	try {	process(cacheStore, entries);	updateMetrics(entries.size());	}	catch (Throwable e) {	
failed to perform batch operation 

private void doWork(Map entries) {	try {	process(igniteCache, entries);	updateMetrics(entries.size());	}	catch (Throwable e) {	
failed to perform batch operation 

private void reportStatistics() {	if (System.currentTimeMillis() - statReportedTime < 30000) return;	statReportedTime = System.currentTimeMillis();	int completed = warmup ? (int)(statReportedTime - warmupStartTime) * 100 / TestsHelper.getLoadTestsWarmupPeriod() : (int)(statReportedTime - startTime) * 100 / TestsHelper.getLoadTestsExecutionTime();	if (completed > 100) completed = 100;	if (warmup) {	
warm up messages processed speed msg sec completed 

private void reportStatistics() {	if (System.currentTimeMillis() - statReportedTime < 30000) return;	statReportedTime = System.currentTimeMillis();	int completed = warmup ? (int)(statReportedTime - warmupStartTime) * 100 / TestsHelper.getLoadTestsWarmupPeriod() : (int)(statReportedTime - startTime) * 100 / TestsHelper.getLoadTestsExecutionTime();	if (completed > 100) completed = 100;	if (warmup) {	}	else {	
messages processed speed msg sec completed errors 

========================= ignite sample_6971 =========================

rmv = markObsolete0(cctx.versions().next(this.ver), true, null);	return false;	}	}	finally {	unlockEntry();	}	if (filter != CU.empty0() && !cctx.isAll(this, filter)) return false;	}	catch (IgniteCheckedException e) {	
an exception was thrown while filter checking 

========================= ignite sample_3909 =========================

private void process(KeyCacheObject key, CacheObject val, GridCacheVersion ver, GridDhtCacheAdapter colocated) {	while (true) {	GridCacheEntryEx entry = null;	cctx.shared().database().checkpointReadLock();	try {	entry = colocated.entryEx(key, topVer);	entry.initialValue( val, ver, expiryPlc == null ? 0 : expiryPlc.forCreate(), expiryPlc == null ? 0 : toExpireTime(expiryPlc.forCreate()), false, topVer, GridDrType.DR_BACKUP, true);	break;	}	catch (GridCacheEntryRemovedException ignore) {	
got removed entry during postprocessing will retry 

GridCacheEntryEx entry = null;	cctx.shared().database().checkpointReadLock();	try {	entry = colocated.entryEx(key, topVer);	entry.initialValue( val, ver, expiryPlc == null ? 0 : expiryPlc.forCreate(), expiryPlc == null ? 0 : toExpireTime(expiryPlc.forCreate()), false, topVer, GridDrType.DR_BACKUP, true);	break;	}	catch (GridCacheEntryRemovedException ignore) {	}	catch (IgniteCheckedException e) {	
error saving backup value 

========================= ignite sample_4235 =========================

protected void doTestPartitionCounterOperation(CacheConfiguration<Object, Object> ccfg) throws Exception {	ignite(0).createCache(ccfg);	try {	long seed = System.currentTimeMillis();	Random rnd = new Random(seed);	
random seed 

========================= ignite sample_853 =========================

private void testFlushFromTheSameThread(boolean writeCoalescing) throws Exception {	delegate.setOperationDelay(50);	initStore(2, writeCoalescing);	Set<Integer> exp;	int start = store.getWriteBehindTotalCriticalOverflowCount();	try {	exp = runPutGetRemoveMultithreaded(5, CACHE_SIZE);	}	finally {	
done inserting shutting down the store 

int start = store.getWriteBehindTotalCriticalOverflowCount();	try {	exp = runPutGetRemoveMultithreaded(5, CACHE_SIZE);	}	finally {	shutdownStore();	}	delegate.setOperationDelay(0);	Map<Integer, String> map = delegate.getMap();	int end = store.getWriteBehindTotalCriticalOverflowCount();	
there are keys in store overflows detected 

========================= ignite sample_2027 =========================

private void checkPutAll() throws Exception {	
check putall 

private void checkPutAll() throws Exception {	Ignite ignite0 = grid(0);	IgniteCache<Integer, Integer> cache0 = ignite0.cache(DEFAULT_CACHE_NAME);	Affinity<Integer> aff = ignite0.affinity(DEFAULT_CACHE_NAME);	UUID id0 = ignite0.cluster().localNode().id();	Map<Integer, Integer> primaryKeys = new HashMap<>();	for (int i = 0; i < 10; i++) primaryKeys.put(key(ignite0, PRIMARY), 1);	
putall from primary 

UUID id0 = ignite0.cluster().localNode().id();	Map<Integer, Integer> primaryKeys = new HashMap<>();	for (int i = 0; i < 10; i++) primaryKeys.put(key(ignite0, PRIMARY), 1);	cache0.putAll(primaryKeys);	for (int i = 0; i < GRID_CNT; i++) {	for (Integer primaryKey : primaryKeys.keySet()) checkEntry(grid(i), primaryKey, 1, false);	}	if (backups > 0) {	Map<Integer, Integer> backupKeys = new HashMap<>();	for (int i = 0; i < 10; i++) backupKeys.put(key(ignite0, BACKUP), 2);	
putall from backup 

if (backups > 0) {	Map<Integer, Integer> backupKeys = new HashMap<>();	for (int i = 0; i < 10; i++) backupKeys.put(key(ignite0, BACKUP), 2);	cache0.putAll(backupKeys);	for (int i = 0; i < GRID_CNT; i++) {	for (Integer backupKey : backupKeys.keySet()) checkEntry(grid(i), backupKey, 2, false);	}	}	Map<Integer, Integer> nearKeys = new HashMap<>();	for (int i = 0; i < 30; i++) nearKeys.put(key(ignite0, NOT_PRIMARY_AND_BACKUP), 3);	
putall from near 

private void checkTransform() throws Exception {	
check transform 

private void checkTransform() throws Exception {	Ignite ignite0 = grid(0);	IgniteCache<Integer, Integer> cache0 = ignite0.cache(DEFAULT_CACHE_NAME);	Affinity<Object> aff = ignite0.affinity(DEFAULT_CACHE_NAME);	UUID id0 = ignite0.cluster().localNode().id();	Integer primaryKey = key(ignite0, PRIMARY);	
transform from primary 

private void checkTransform() throws Exception {	Ignite ignite0 = grid(0);	IgniteCache<Integer, Integer> cache0 = ignite0.cache(DEFAULT_CACHE_NAME);	Affinity<Object> aff = ignite0.affinity(DEFAULT_CACHE_NAME);	UUID id0 = ignite0.cluster().localNode().id();	Integer primaryKey = key(ignite0, PRIMARY);	cache0.invoke(primaryKey, new Processor(primaryKey));	for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), primaryKey, primaryKey, false);	if (backups > 0) {	Integer backupKey = key(ignite0, BACKUP);	
transform from backup 

UUID id0 = ignite0.cluster().localNode().id();	Integer primaryKey = key(ignite0, PRIMARY);	cache0.invoke(primaryKey, new Processor(primaryKey));	for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), primaryKey, primaryKey, false);	if (backups > 0) {	Integer backupKey = key(ignite0, BACKUP);	cache0.invoke(backupKey, new Processor(backupKey));	for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), backupKey, backupKey, false);	}	Integer nearKey = key(ignite0, NOT_PRIMARY_AND_BACKUP);	
transform from near 

private void checkTransformAll() throws Exception {	
check transformall 

private void checkTransformAll() throws Exception {	Ignite ignite0 = grid(0);	IgniteCache<Integer, Integer> cache0 = ignite0.cache(DEFAULT_CACHE_NAME);	Affinity<Object> aff = ignite0.affinity(DEFAULT_CACHE_NAME);	UUID id0 = ignite0.cluster().localNode().id();	Set<Integer> primaryKeys = new HashSet<>();	for (int i = 0; i < 10; i++) primaryKeys.add(key(ignite0, PRIMARY));	
transformall from primary 

UUID id0 = ignite0.cluster().localNode().id();	Set<Integer> primaryKeys = new HashSet<>();	for (int i = 0; i < 10; i++) primaryKeys.add(key(ignite0, PRIMARY));	cache0.invokeAll(primaryKeys, new Processor(1));	for (int i = 0; i < GRID_CNT; i++) {	for (Integer primaryKey : primaryKeys) checkEntry(grid(i), primaryKey, 1, false);	}	if (backups > 0) {	Set<Integer> backupKeys = new HashSet<>();	for (int i = 0; i < 10; i++) backupKeys.add(key(ignite0, BACKUP));	
transformall from backup 

if (backups > 0) {	Set<Integer> backupKeys = new HashSet<>();	for (int i = 0; i < 10; i++) backupKeys.add(key(ignite0, BACKUP));	cache0.invokeAll(backupKeys, new Processor(2));	for (int i = 0; i < GRID_CNT; i++) {	for (Integer backupKey : backupKeys) checkEntry(grid(i), backupKey, 2, false);	}	}	Set<Integer> nearKeys = new HashSet<>();	for (int i = 0; i < 30; i++) nearKeys.add(key(ignite0, NOT_PRIMARY_AND_BACKUP));	
transformall from near 

private void checkPut(int grid) throws Exception {	
check put grid 

private void checkPut(int grid) throws Exception {	Ignite ignite0 = grid(grid);	IgniteCache<Integer, Integer> cache0 = ignite0.cache(DEFAULT_CACHE_NAME);	Affinity<Integer> aff = ignite0.affinity(DEFAULT_CACHE_NAME);	UUID id0 = ignite0.cluster().localNode().id();	Integer primaryKey = key(ignite0, PRIMARY);	
put from primary 

private void checkPut(int grid) throws Exception {	Ignite ignite0 = grid(grid);	IgniteCache<Integer, Integer> cache0 = ignite0.cache(DEFAULT_CACHE_NAME);	Affinity<Integer> aff = ignite0.affinity(DEFAULT_CACHE_NAME);	UUID id0 = ignite0.cluster().localNode().id();	Integer primaryKey = key(ignite0, PRIMARY);	cache0.put(primaryKey, primaryKey);	for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), primaryKey, primaryKey, false);	if (backups > 0) {	Integer backupKey = key(ignite0, BACKUP);	
put from backup 

UUID id0 = ignite0.cluster().localNode().id();	Integer primaryKey = key(ignite0, PRIMARY);	cache0.put(primaryKey, primaryKey);	for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), primaryKey, primaryKey, false);	if (backups > 0) {	Integer backupKey = key(ignite0, BACKUP);	cache0.put(backupKey, backupKey);	for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), backupKey, backupKey, false);	}	Integer nearKey = key(ignite0, NOT_PRIMARY_AND_BACKUP);	
put from near 

private void checkRemove() throws Exception {	
check remove 

private void checkRemove() throws Exception {	Ignite ignite0 = grid(0);	IgniteCache<Integer, Integer> cache0 = ignite0.cache(DEFAULT_CACHE_NAME);	Integer primaryKey = key(ignite0, PRIMARY);	
put from primary 

private void checkRemove() throws Exception {	Ignite ignite0 = grid(0);	IgniteCache<Integer, Integer> cache0 = ignite0.cache(DEFAULT_CACHE_NAME);	Integer primaryKey = key(ignite0, PRIMARY);	cache0.put(primaryKey, primaryKey);	for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), primaryKey, primaryKey, false);	
remove from primary 

private void checkRemove() throws Exception {	Ignite ignite0 = grid(0);	IgniteCache<Integer, Integer> cache0 = ignite0.cache(DEFAULT_CACHE_NAME);	Integer primaryKey = key(ignite0, PRIMARY);	cache0.put(primaryKey, primaryKey);	for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), primaryKey, primaryKey, false);	cache0.remove(primaryKey);	for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), primaryKey, null, false);	if (backups > 0) {	Integer backupKey = key(ignite0, BACKUP);	
put from backup 

IgniteCache<Integer, Integer> cache0 = ignite0.cache(DEFAULT_CACHE_NAME);	Integer primaryKey = key(ignite0, PRIMARY);	cache0.put(primaryKey, primaryKey);	for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), primaryKey, primaryKey, false);	cache0.remove(primaryKey);	for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), primaryKey, null, false);	if (backups > 0) {	Integer backupKey = key(ignite0, BACKUP);	cache0.put(backupKey, backupKey);	for (int i = 0; i < GRID_CNT; i++) checkEntry(grid(i), backupKey, backupKey, false);	
remove from backup 

private void checkReaderEvict() throws Exception {	
check evict 

private void doStartGrids(int backups) throws Exception {	this.backups = backups;	startGrids(GRID_CNT);	awaitPartitionMapExchange();	
grids 

========================= ignite sample_1125 =========================

private void testRegistration(CacheConfiguration ccfg) throws Exception {	ExecutorService execSrv = newSingleThreadExecutor();	try {	final IgniteCache<Integer, String> cache = grid(0).getOrCreateCache(ccfg);	for (int i = 0; i < 10; i++) {	
start iteration 

for (int i = 0; i < 10; i++) {	final int i0 = i;	final AtomicBoolean stop = new AtomicBoolean(false);	final CountDownLatch latch = new CountDownLatch(1);	final int conQryCnt = 50;	Future<List<IgniteFuture<String>>> fut = execSrv.submit( new Callable<List<IgniteFuture<String>>>() {	int cnt = 0;	List<IgniteFuture<String>> futures = new ArrayList<>();	while (!stop.get()) {	futures.add(waitForKey(i0, cache, cnt));	
started cont query count 

========================= ignite sample_1963 =========================

public void testRecoveringOnCacheInitError() throws Exception {	failPageStoreDiskOperations = true;	diskSpaceBytes = 2 * PAGE_SIZE;	final IgniteEx grid = startGrid(0);	boolean failed = false;	try {	grid.active(true);	} catch (Exception expected) {	
expected cache error 

========================= ignite sample_1719 =========================

public void start() {	A.ensure(getSingleTupleExtractor() != null || getMultipleTupleExtractor() != null, "ZeroMq extractor.");	log = getIgnite().log();	if (isStarted) {	
attempted to start an already started zeromq streamer 

========================= ignite sample_6176 =========================

final CountDownLatch latch = new CountDownLatch(nodes.size());	final AtomicBoolean err = new AtomicBoolean();	for (String node : nodes) {	G.ignite(node).events().localListen(new IgnitePredicate<Event>() {	DiscoveryEvent disoEvt = (DiscoveryEvent)evt;	if (disoEvt.eventNode().id().equals(srvNodeId)) {	info("Expected node failed event: " + ((DiscoveryEvent) evt).eventNode());	latch.countDown();	}	else {	
unexpected node failed event 

========================= ignite sample_2565 =========================

public void testToStringPerformance() {	TestClass1 obj = new TestClass1();	IgniteLogger log = log();	obj.toStringAutomatic();	long start = System.currentTimeMillis();	for (int i = 0; i < 100000; i++) obj.toStringManual();	
manual tostring took ms 

public void testToStringPerformance() {	TestClass1 obj = new TestClass1();	IgniteLogger log = log();	obj.toStringAutomatic();	long start = System.currentTimeMillis();	for (int i = 0; i < 100000; i++) obj.toStringManual();	start = System.currentTimeMillis();	for (int i = 0; i < 100000; i++) obj.toStringAutomatic();	
automatic tostring took ms 

========================= ignite sample_691 =========================

try (Ignite client = startGrid("client")) {	IgniteCache<Integer, ValueObj> cache = client.cache(CACHE_NAME_2);	int count = 1_000;	for (int idx = 0; idx < count; ++idx) cache.put(idx, new ValueObj(count - idx - 1, 0));	long start = System.currentTimeMillis();	QueryCursor<List<?>> cursor = cache.query(new SqlFieldsQuery("select min(_key), max(_key) from ValueObj"));	List<List<?>> result = cursor.getAll();	assertEquals(1, result.size());	assertEquals(0, result.get(0).get(0));	assertEquals(count - 1, result.get(0).get(1));	
elapsed 

List<List<?>> result = cursor.getAll();	assertEquals(1, result.size());	assertEquals(0, result.get(0).get(0));	assertEquals(count - 1, result.get(0).get(1));	start = System.currentTimeMillis();	cursor = cache.query(new SqlFieldsQuery("select min(idxVal), max(idxVal) from ValueObj"));	result = cursor.getAll();	assertEquals(1, result.size());	assertEquals(0, result.get(0).get(0));	assertEquals(count - 1, result.get(0).get(1));	
elapsed 

result = cursor.getAll();	assertEquals(1, result.size());	assertEquals(0, result.get(0).get(0));	assertEquals(count - 1, result.get(0).get(1));	start = System.currentTimeMillis();	cursor = cache.query(new SqlFieldsQuery("select min(nonIdxVal), max(nonIdxVal) from ValueObj"));	result = cursor.getAll();	assertEquals(1, result.size());	assertEquals(0, result.get(0).get(0));	assertEquals(count - 1, result.get(0).get(1));	
elapsed 

========================= ignite sample_7623 =========================

private DatabaseMetadataDialect dialect(Connection conn) {	try {	String dbProductName = conn.getMetaData().getDatabaseProductName();	if ("Oracle".equals(dbProductName)) return new OracleMetadataDialect();	if (dbProductName.startsWith("DB2/")) return new DB2MetadataDialect();	if ("MySQL".equals(dbProductName)) return new MySQLMetadataDialect();	return new JdbcMetadataDialect();	}	catch (SQLException e) {	
failed to resolve dialect jdbcmetadatadialect will be used 

========================= ignite sample_7361 =========================

public boolean clearInternal( GridCacheVersion ver, GridCacheObsoleteEntryExtras extras ) throws IgniteCheckedException {	boolean rmv = false;	lockEntry();	try {	if (!markObsolete0(ver, false, extras)) {	
entry could not be marked obsolete it is still used or has readers 

public boolean clearInternal( GridCacheVersion ver, GridCacheObsoleteEntryExtras extras ) throws IgniteCheckedException {	boolean rmv = false;	lockEntry();	try {	if (!markObsolete0(ver, false, extras)) {	return false;	}	rdrs = ReaderId.EMPTY_ARRAY;	
entry has been marked obsolete 

========================= ignite sample_3772 =========================

public IgniteInternalFuture<IgniteInternalTx> commitDhtLocalAsync() {	
committing dht local tx 

========================= ignite sample_3764 =========================

public void onShuffleAck(HadoopShuffleAck ack) {	IgniteBiTuple<HadoopShuffleMessage, GridFutureAdapter<?>> tup = sentMsgs.get(ack.id());	if (tup != null) tup.get2().onDone();	
received shuffle ack for not registered shuffle id 

catch (GridClosureException e) {	if (fut != null) fut.onDone(U.unwrap(e));	}	if (fut != null) {	fut.listen(new IgniteInClosure<IgniteInternalFuture<?>>() {	try {	f.get();	sentMsgs.remove(msgId);	}	catch (IgniteCheckedException e) {	
failed to send message 

public IgniteInternalFuture<?> flush() throws IgniteCheckedException {	
flushing job on address 

public IgniteInternalFuture<?> flush() throws IgniteCheckedException {	flushed = true;	if (totalReducerCnt == 0) return new GridFinishedFuture<>();	if (!stripeMappers) {	U.await(ioInitLatch);	GridWorker snd0 = snd;	if (snd0 != null) {	
cancelling sender thread 

public IgniteInternalFuture<?> flush() throws IgniteCheckedException {	flushed = true;	if (totalReducerCnt == 0) return new GridFinishedFuture<>();	if (!stripeMappers) {	U.await(ioInitLatch);	GridWorker snd0 = snd;	if (snd0 != null) {	snd0.cancel();	try {	snd0.join();	
finished waiting for sending thread to complete on shuffle job flush 

if (snd0 != null) {	snd0.cancel();	try {	snd0.join();	}	catch (InterruptedException e) {	throw new IgniteInterruptedCheckedException(e);	}	}	collectUpdatesAndSend(true);	
finished sending collected updates to remote reducers 

if (log.isDebugEnabled()) log.debug("Sent shuffle finish request [jobId=" + job.id() + ", dest=" + dest + ", req=" + req + ']');	fut.add(rmtState.future());	sent = true;	}	if (sent) fut.markInitialized();	else return new GridFinishedFuture<>();	}	else {	for (IgniteBiTuple<HadoopShuffleMessage, GridFutureAdapter<?>> tup : sentMsgs.values()) fut.add(tup.get2());	fut.markInitialized();	
collected futures to compound futures for flush 

========================= ignite sample_7232 =========================

private Worker createWorker(Class clazz, Object cfg, long startPosition, long endPosition) {	try {	Class cfgCls = cfg instanceof Ignite ? Ignite.class : CacheStore.class;	Constructor ctor = clazz.getConstructor(cfgCls, long.class, long.class);	return (Worker)ctor.newInstance(cfg, startPosition, endPosition);	}	catch (Throwable e) {	
failed to instantiate worker of class 

private void printTestResultsHeader(String testName, List<String> failedWorkers) {	if (failedWorkers.isEmpty()) {	
test execution successfully completed 

private void printTestResultsHeader(String testName, List<String> failedWorkers) {	if (failedWorkers.isEmpty()) {	return;	}	if (failedWorkers.size() == TestsHelper.getLoadTestsThreadsCount()) {	
test execution totally failed 

return;	}	if (failedWorkers.size() == TestsHelper.getLoadTestsThreadsCount()) {	return;	}	String strFailedWorkers = "";	for (String workerName : failedWorkers) {	if (!strFailedWorkers.isEmpty()) strFailedWorkers += ", ";	strFailedWorkers += workerName;	}	
test execution completed but of workers failed failed workers 

========================= ignite sample_6970 =========================

srvSocks.add(srvSock);	new GridTestThread(new Runnable() {	try {	while(!Thread.currentThread().isInterrupted()) {	Socket clientSock = srvSock.accept();	connCnt.getAndIncrement();	new GridTestThread(workerFactory.newWorker(clientSock)).start();	}	}	catch (Exception e) {	
unexpected error 

public abstract void action(InputStream input, OutputStream output) throws IOException;	}	return new SocketWorker(clientSock) {	output.write("Some response".getBytes());	
test some response was sent to 

========================= ignite sample_2572 =========================

private void checkQueryPlan(IgniteCache<Object, Object> cache, boolean enforceJoinOrder, int expBatchedJoins, SqlFieldsQuery qry, String... expText) {	qry.setEnforceJoinOrder(enforceJoinOrder);	qry.setDistributedJoins(true);	String plan = queryPlan(cache, qry);	
plan 

========================= ignite sample_7635 =========================

}	assertTrue(found);	}	}	Ignite newIgnite = startGrid(GRID_CNT + 1);	Collection<Integer> locQueueContent = compute(newIgnite.cluster().forLocal()).call( new IgniteCallable<Collection<Integer>>() {	private Ignite grid;	Collection<Integer> values = new ArrayList<>();	grid.log().info("Running job [node=" + grid.cluster().localNode().id() + ", job=" + this + "]");	IgniteQueue<Integer> locQueue = grid.queue(queueName, QUEUE_CAPACITY, config(false));	
queue size 

========================= ignite sample_956 =========================

public void testMessageQueueLimit() throws Exception {	for (int i = 0; i < 3; i++) {	
iteration 

========================= ignite sample_1077 =========================

AddressReceiver rcvr = new AddressReceiver(mcastAddr, itf);	rcvr.start();	rcvrs.add(rcvr);	}	for (AddressReceiver rcvr : rcvrs) {	try {	rcvr.join();	ret.addAll(rcvr.addresses());	}	catch (InterruptedException ignore) {	
got interrupted while receiving address request 

sock.setSoTimeout(resWaitTime);	if (ttl != -1) sock.setTimeToLive(ttl);	reqPckt.setData(MSG_ADDR_REQ_DATA);	try {	sock.send(reqPckt);	}	catch (IOException e) {	sndErr = true;	if (!handleNetworkError(e)) break;	if (i < addrReqAttempts - 1) {	
failed to send multicast address request will retry in ms 

try {	sock.send(reqPckt);	}	catch (IOException e) {	sndErr = true;	if (!handleNetworkError(e)) break;	if (i < addrReqAttempts - 1) {	U.sleep(500);	}	else {	
failed to send multicast address request 

}	sndError = true;	continue;	}	long rcvEnd = U.currentTimeMillis() + resWaitTime;	try {	while (U.currentTimeMillis() < rcvEnd) {	sock.receive(resPckt);	byte[] data = resPckt.getData();	if (!U.bytesEqual(U.IGNITE_HEADER, 0, data, 0, U.IGNITE_HEADER.length)) {	
failed to verify message header 

sock.receive(resPckt);	byte[] data = resPckt.getData();	if (!U.bytesEqual(U.IGNITE_HEADER, 0, data, 0, U.IGNITE_HEADER.length)) {	continue;	}	AddressResponse addrRes;	try {	addrRes = new AddressResponse(data);	}	catch (IgniteCheckedException e) {	
failed to deserialize multicast response 

try {	addrRes = new AddressResponse(data);	}	catch (IgniteCheckedException e) {	continue;	}	rmtAddrs.addAll(addrRes.addresses());	}	}	catch (SocketTimeoutException ignored) {	
address receive timeout 

catch (IgniteCheckedException e) {	continue;	}	rmtAddrs.addAll(addrRes.addresses());	}	}	catch (SocketTimeoutException ignored) {	}	}	catch (IOException e) {	
failed to request nodes addresses 

catch (SocketTimeoutException ignored) {	}	}	catch (IOException e) {	}	finally {	U.close(sock);	}	if (i < addrReqAttempts - 1) U.sleep(200);	}	
received nodes addresses 

}	finally {	U.close(sock);	}	if (i < addrReqAttempts - 1) U.sleep(200);	}	if (rmtAddrs.isEmpty() && sndError) U.quietAndWarn(log, "Failed to send multicast message (is multicast enabled on this node?).");	return new T2<>(rmtAddrs, sndErr);	}	catch (IgniteInterruptedCheckedException ignored) {	
got interrupted while sending address request 

private boolean handleNetworkError(IOException e) {	if ("Network is unreachable".equals(e.getMessage()) && U.isMacOs()) {	
multicast does not work on mac os jvm loopback address configure external ip address for localhost configuration property 

private MulticastSocket createSocket() throws IOException {	MulticastSocket sock = new MulticastSocket(mcastPort);	sock.setLoopbackMode(false);	if (sockItf != null) sock.setInterface(sockItf);	
loopback mode is disabled which prevents nodes on the same machine from discovering each other 

========================= ignite sample_5673 =========================

U.sleep(200);	if (!fut.cancel(true)) {	BenchmarkUtils.println(cfg, "Failed to cancel Preload logger.");	return;	}	}	}	printCachesStatistics();	}	catch (Exception e) {	
failed to stop preload logger 

========================= ignite sample_6399 =========================

job.waitForCompletion(false);	assertEquals("job must end successfully", JobStatus.State.SUCCEEDED, job.getStatus().getState());	final Counters counters = job.getCounters();	assertNotNull("counters cannot be null", counters);	assertEquals("wrong counters count", 3, counters.countCounters());	assertEquals("wrong counter value", 15, counters.findCounter(TestCounter.COUNTER1).getValue());	assertEquals("wrong counter value", 3, counters.findCounter(TestCounter.COUNTER2).getValue());	assertEquals("wrong counter value", 3, counters.findCounter(TestCounter.COUNTER3).getValue());	}	catch (Throwable t) {	
unexpected exception 

========================= ignite sample_7137 =========================

public void clearLocally(Collection<KeyCacheObject> keys, boolean readers) {	if (F.isEmpty(keys)) return;	GridCacheVersion obsoleteVer = ctx.versions().next();	for (KeyCacheObject key : keys) {	GridCacheEntryEx e = peekEx(key);	try {	if (e != null) e.clear(obsoleteVer, readers);	}	catch (IgniteCheckedException ex) {	
failed to clearlocally entry will continue to clearlocally other entries 

public final void removeEntry(GridCacheEntryEx entry) {	boolean rmvd = map.removeEntry(entry);	if (log.isDebugEnabled()) {	
removed entry from cache 

public final void removeEntry(GridCacheEntryEx entry) {	boolean rmvd = map.removeEntry(entry);	if (log.isDebugEnabled()) {	
remove will not be done for key entry got replaced or removed 

private boolean evictx(K key, GridCacheVersion ver, @Nullable CacheEntryPredicate[] filter) {	KeyCacheObject cacheKey = ctx.toCacheKeyObject(key);	GridCacheEntryEx entry = peekEx(cacheKey);	if (entry == null) return true;	try {	return ctx.evicts().evict(entry, ver, true, filter);	}	catch (IgniteCheckedException ex) {	
failed to evict entry from cache 

}	}	if (res != null) {	ctx.addResult(map, key, res, skipVals, keepCacheObjects, deserializeBinary, true, needVer);	if (entry != null && (tx == null || (!tx.implicit() && tx.isolation() == READ_COMMITTED))) ctx.evicts().touch(entry, topVer);	if (keysSize == 1) return new GridFinishedFuture<>(map);	}	break;	}	catch (GridCacheEntryRemovedException ignored) {	
got removed entry in getallasync method will retry 

entry.unswap();	EntryGetResult verVal = entry.versionedValue( cacheVal, res.version(), null, expiry, readerArgs);	if (log.isDebugEnabled()) log.debug("Set value loaded from store into entry [" + "oldVer=" + res.version() + ", newVer=" + verVal.version() + ", " + "entry=" + entry + ']');	if (verVal.value() != null) {	ctx.addResult(map, key, verVal, skipVals, keepCacheObjects, deserializeBinary, true, needVer);	}	if (tx0 == null || (!tx0.implicit() && tx0.isolation() == READ_COMMITTED)) ctx.evicts().touch(entry, topVer);	break;	}	catch (GridCacheEntryRemovedException ignore) {	
got removed entry during getallasync will retry 

if (p != null && !p.apply(key.value(ctx.cacheObjectContext(), false), val)) return;	CacheObject cacheVal = ctx.toCacheObject(val);	GridCacheEntryEx entry = entryEx(key);	try {	entry.initialValue(cacheVal, ver, ttl, CU.EXPIRE_TIME_CALCULATE, false, topVer, replicate ? DR_LOAD : DR_NONE, true);	}	catch (IgniteCheckedException e) {	throw new IgniteException("Failed to put cache value: " + entry, e);	}	catch (GridCacheEntryRemovedException ignore) {	
got removed entry during loadcache will ignore 

return F.iterator(iterator(), new IgniteClosure<Cache.Entry<K, V>, Cache.Entry<K, V>>() {	private IgniteCacheExpiryPolicy expiryPlc = ctx.cache().expiryPolicy(opCtx != null ? opCtx.expiry() : null);	CacheOperationContext prev = ctx.gate().enter(opCtx);	try {	V val = localPeek(lazyEntry.getKey(), CachePeekModes.ONHEAP_ONLY, expiryPlc);	GridCacheVersion ver = null;	try {	ver = lazyEntry.unwrap(GridCacheVersion.class);	}	catch (IllegalArgumentException e) {	
failed to unwrap entry version information 

private <T> IgniteInternalFuture<T> asyncOp(final AsyncOp<T> op) {	try {	checkJta();	}	catch (IgniteCheckedException e) {	return new GridFinishedFuture<>(e);	}	
performing async op 

GridCacheVersion obsoleteVer = ctx.versions().next();	ctx.shared().database().checkpointReadLock();	try {	KeyCacheObject cacheKey = ctx.toCacheKeyObject(key);	GridCacheEntryEx entry = ctx.isNear() ? peekEx(cacheKey) : entryEx(cacheKey);	if (entry != null) return entry.clear(obsoleteVer, readers);	}	catch (GridDhtInvalidPartitionException ignored) {	}	catch (IgniteCheckedException ex) {	
failed to clearlocally entry for key 

========================= ignite sample_4170 =========================

protected void doTestPartitionsQuery(Ignite orig) {	IgniteCache<ClientKey, Client> cl = orig.cache("cl");	for (int regionId = 1; regionId <= PARTS_PER_REGION.length; regionId++) {	
running test queries for region 

protected void doTestPartitionsQuery(Ignite orig) {	IgniteCache<ClientKey, Client> cl = orig.cache("cl");	for (int regionId = 1; regionId <= PARTS_PER_REGION.length; regionId++) {	List<Integer> range = REGION_TO_PART_MAP.get(regionId);	int[] parts = createRange(range.get(0), range.get(1));	int off = rnd.nextInt(parts.length);	int p1 = parts[off], p2 = parts[(off + (1 + rnd.nextInt(parts.length-1))) % parts.length];	
parts 

========================= ignite sample_7475 =========================

public void onNodeLeft(UUID nodeId) {	if (compareAndSet(nodeId, null)) {	IgniteLogger log = cctx.logger(TxDeadlockDetection.class);	
failed to finish deadlock detection node left 

public void onResult(UUID nodeId, TxLocksResponse res) {	boolean set = compareAndSet(nodeId, null);	if (res != null && set) {	if (res.classError() != null) {	IgniteLogger log = cctx.kernalContext().log(this.getClass());	
failed to finish deadlock detection due to an error 

========================= ignite sample_3965 =========================

}	if (root instanceof BinaryObjectException && root.getMessage().startsWith("Unexpected field type")) {	log().info(sb.toString());	Field f = fields[unexpectedCnt];	Throwable t = ex;	assertTrue(t.getMessage(), t.getMessage().contains( "object [typeName=org.apache.ignite.internal.binary.BinaryObjectExceptionSelfTest$Value"));	t = t.getCause();	assertTrue(t.getMessage(), t.getMessage().contains("field [name=" + f.getName()));	++unexpectedCnt;	}	
ignored exception 

========================= ignite sample_2227 =========================

private void onDiscoDataReceived(DiscoveryData data) {	if (!ctx.isDaemon() && data != null) {	for (DiscoveryDataItem item : data.items) {	try {	if (item.prjPred != null) ctx.resource().injectGeneric(item.prjPred);	if ((item.prjPred == null || item.prjPred.apply(ctx.discovery().localNode())) && !locInfos.containsKey(item.routineId)) registerHandler(data.nodeId, item.routineId, item.hnd, item.bufSize, item.interval, item.autoUnsubscribe, false);	if (!item.autoUnsubscribe) locInfos.putIfAbsent(item.routineId, new LocalRoutineInfo( item.prjPred, item.hnd, item.bufSize, item.interval, item.autoUnsubscribe));	}	catch (IgniteCheckedException e) {	
failed to register continuous handler 

for (Map.Entry<UUID, LocalRoutineInfo> e : clientRoutineMap.entrySet()) {	UUID routineId = e.getKey();	LocalRoutineInfo info = e.getValue();	try {	if (info.prjPred != null) ctx.resource().injectGeneric(info.prjPred);	if (info.prjPred == null || info.prjPred.apply(ctx.discovery().localNode())) {	registerHandler(clientNodeId, routineId, info.hnd, info.bufSize, info.interval, info.autoUnsubscribe, false);	}	}	catch (IgniteCheckedException err) {	
failed to register continuous handler 

private void registerMessageListener(GridContinuousHandler hnd) {	if (hnd.orderedTopic() != null) {	ctx.io().addMessageListener(hnd.orderedTopic(), new GridMessageListener() {	GridContinuousMessage msg = (GridContinuousMessage)obj;	assert msg.type() == MSG_EVT_NOTIFICATION;	if (msg.data() == null && msg.dataBytes() != null) {	try {	msg.data(U.unmarshal(marsh, msg.dataBytes(), U.resolveClassLoader(ctx.config())));	}	catch (IgniteCheckedException e) {	
failed to process message ignoring 

try {	LocalRoutineInfo routine = locInfos.get(routineId);	if (routine != null) routine.hnd.notifyCallback(nodeId, routineId, (Collection<?>)msg.data(), ctx);	}	finally {	if (msg.futureId() != null) {	try {	sendWithRetries(nodeId, new GridContinuousMessage(MSG_EVT_ACK, null, msg.futureId(), null, false), null, null);	}	catch (IgniteCheckedException e) {	
failed to send event acknowledgment to node 

try {	Collection<Object> toSnd = batch.collect();	boolean msg = toSnd.iterator().next() instanceof Message;	CI1<IgniteException> ackC = new CI1<IgniteException>() {	if (e == null) info.hnd.onBatchAcknowledged(routineId, batch, ctx);	}	};	sendNotification(nodeId, routineId, null, toSnd, hnd.orderedTopic(), msg, ackC);	}	catch (ClusterTopologyCheckedException ignored) {	
failed to send notification to node is node alive 

boolean msg = toSnd.iterator().next() instanceof Message;	CI1<IgniteException> ackC = new CI1<IgniteException>() {	if (e == null) info.hnd.onBatchAcknowledged(routineId, batch, ctx);	}	};	sendNotification(nodeId, routineId, null, toSnd, hnd.orderedTopic(), msg, ackC);	}	catch (ClusterTopologyCheckedException ignored) {	}	catch (IgniteCheckedException e) {	
failed to send notification to node 

}	else ctx.io().sendToGridTopic(node, TOPIC_CONTINUOUS, msg, SYSTEM_POOL, ackC);	break;	}	catch (ClusterTopologyCheckedException | IgniteInterruptedCheckedException e) {	throw e;	}	catch (IgniteCheckedException e) {	if (!ctx.discovery().alive(node.id())) throw new ClusterTopologyCheckedException("Node left grid while sending message to: " + node.id(), e);	if (cnt == retryCnt) throw e;	
failed to send message to node will retry 

========================= ignite sample_4920 =========================

if (p == null) {	ComputeTaskSession ses = ctx.getTaskSession();	try {	p = (Integer)ses.getAttribute(taskPriAttrKey);	}	catch (ClassCastException e) {	LT.error(log, e, "Type of task session priority attribute '" + taskPriAttrKey + "' is not java.lang.Integer [type=" + ses.getAttribute(taskPriAttrKey).getClass() + ']');	}	if (p == null) {	if (log.isDebugEnabled()) {	
failed get priority from job context attribute and task session attribute will use default priority 

========================= ignite sample_5777 =========================

private void sendErrorResponse(ObjectOutput out, Exception err) {	try {	out.writeObject(new IpcSharedMemoryInitResponse(err));	}	catch (IOException e) {	
failed to send error response to client 

private void cleanupResources(File workTokDir) {	RandomAccessFile lockFile = null;	FileLock lock = null;	try {	lockFile = new RandomAccessFile(new File(workTokDir, LOCK_FILE_NAME), "rw");	lock = lockFile.getChannel().lock();	if (lock != null) processTokenDirectory(workTokDir);	
token directory is being processed concurrently 

private void cleanupResources(File workTokDir) {	RandomAccessFile lockFile = null;	FileLock lock = null;	try {	lockFile = new RandomAccessFile(new File(workTokDir, LOCK_FILE_NAME), "rw");	lock = lockFile.getChannel().lock();	if (lock != null) processTokenDirectory(workTokDir);	}	catch (OverlappingFileLockException ignored) {	
token directory is being processed concurrently 

lockFile = new RandomAccessFile(new File(workTokDir, LOCK_FILE_NAME), "rw");	lock = lockFile.getChannel().lock();	if (lock != null) processTokenDirectory(workTokDir);	}	catch (OverlappingFileLockException ignored) {	}	catch (FileLockInterruptionException ignored) {	Thread.currentThread().interrupt();	}	catch (IOException e) {	
failed to process directory 

private void processTokenDirectory(File workTokDir) {	for (File f : workTokDir.listFiles()) {	if (!f.isDirectory()) {	if (!f.getName().equals(LOCK_FILE_NAME)) {	
unexpected file 

private void processTokenDirectory(File workTokDir) {	for (File f : workTokDir.listFiles()) {	if (!f.isDirectory()) {	if (!f.getName().equals(LOCK_FILE_NAME)) {	}	continue;	}	if (f.equals(tokDir)) {	
skipping own token directory 

}	if (f.equals(tokDir)) {	continue;	}	String name = f.getName();	int pid;	try {	pid = Integer.parseInt(name.substring(name.lastIndexOf('-') + 1));	}	catch (NumberFormatException ignored) {	
failed to parse file name 

}	String name = f.getName();	int pid;	try {	pid = Integer.parseInt(name.substring(name.lastIndexOf('-') + 1));	}	catch (NumberFormatException ignored) {	continue;	}	if (IpcSharedMemoryUtils.alive(pid)) {	
skipping alive node 

int pid;	try {	pid = Integer.parseInt(name.substring(name.lastIndexOf('-') + 1));	}	catch (NumberFormatException ignored) {	continue;	}	if (IpcSharedMemoryUtils.alive(pid)) {	continue;	}	
possibly stale token folder 

continue;	}	if (IpcSharedMemoryUtils.alive(pid)) {	continue;	}	File[] shmemToks = f.listFiles();	if (shmemToks == null) return;	int rmvCnt = 0;	try {	for (File f0 : shmemToks) {	
processing token file 

}	if (IpcSharedMemoryUtils.alive(pid)) {	continue;	}	File[] shmemToks = f.listFiles();	if (shmemToks == null) return;	int rmvCnt = 0;	try {	for (File f0 : shmemToks) {	if (f0.isDirectory()) {	
unexpected directory 

}	File[] shmemToks = f.listFiles();	if (shmemToks == null) return;	int rmvCnt = 0;	try {	for (File f0 : shmemToks) {	if (f0.isDirectory()) {	}	String[] toks = f0.getName().split("-");	if (toks.length != 6) {	
unrecognized token file 

if (toks.length != 6) {	continue;	}	int pid0;	int size;	try {	pid0 = Integer.parseInt(toks[4]);	size = Integer.parseInt(toks[5]);	}	catch (NumberFormatException ignored) {	
failed to parse file name 

int pid0;	int size;	try {	pid0 = Integer.parseInt(toks[4]);	size = Integer.parseInt(toks[5]);	}	catch (NumberFormatException ignored) {	continue;	}	if (IpcSharedMemoryUtils.alive(pid0)) {	
skipping alive process 

try {	pid0 = Integer.parseInt(toks[4]);	size = Integer.parseInt(toks[5]);	}	catch (NumberFormatException ignored) {	continue;	}	if (IpcSharedMemoryUtils.alive(pid0)) {	continue;	}	
possibly stale token file 

size = Integer.parseInt(toks[5]);	}	catch (NumberFormatException ignored) {	continue;	}	if (IpcSharedMemoryUtils.alive(pid0)) {	continue;	}	IpcSharedMemoryUtils.freeSystemResources(f0.getAbsolutePath(), size);	if (f0.delete()) {	
deleted file 

continue;	}	if (IpcSharedMemoryUtils.alive(pid0)) {	continue;	}	IpcSharedMemoryUtils.freeSystemResources(f0.getAbsolutePath(), size);	if (f0.delete()) {	rmvCnt++;	}	else if (!f0.exists()) {	
file has been concurrently deleted 

if (IpcSharedMemoryUtils.alive(pid0)) {	continue;	}	IpcSharedMemoryUtils.freeSystemResources(f0.getAbsolutePath(), size);	if (f0.delete()) {	rmvCnt++;	}	else if (!f0.exists()) {	rmvCnt++;	}	
failed to delete file 

rmvCnt++;	}	else if (!f0.exists()) {	rmvCnt++;	}	}	}	finally {	if (rmvCnt == shmemToks.length) {	U.delete(f);	
deleted empty token directory 

========================= ignite sample_3420 =========================

boolean failed = true;	try {	svc.execute(new Runnable() {	boolean failed = true;	try {	bar.await();	long start = System.currentTimeMillis();	if (log.isInfoEnabled()) log.info("IO test started " + "[warmup=" + warmup + ", duration=" + duration + ", threads=" + threads + ", latencyLimit=" + latencyLimit + ", rangesCnt=" + rangesCnt + ", payLoadSize=" + payLoadSize + ", procFromNioThreads=" + procFromNioThread + ']' );	for (;;) {	if (!warmupFinished.get() && System.currentTimeMillis() - start > warmup) {	
io test warmup finished 

try {	bar.await();	long start = System.currentTimeMillis();	if (log.isInfoEnabled()) log.info("IO test started " + "[warmup=" + warmup + ", duration=" + duration + ", threads=" + threads + ", latencyLimit=" + latencyLimit + ", rangesCnt=" + rangesCnt + ", payLoadSize=" + payLoadSize + ", procFromNioThreads=" + procFromNioThread + ']' );	for (;;) {	if (!warmupFinished.get() && System.currentTimeMillis() - start > warmup) {	warmupFinished.set(true);	start = System.currentTimeMillis();	}	if (warmupFinished.get() && System.currentTimeMillis() - start > duration) {	
io test finished will wait for all threads to finish 

bar.await();	failed = false;	break;	}	if (log.isInfoEnabled()) log.info("IO test [opsCnt/sec=" + (cnt.sumThenReset() * 1000 / sleepDuration) + ", warmup=" + !warmupFinished.get() + ", elapsed=" + (System.currentTimeMillis() - start) + ']');	Thread.sleep(sleepDuration);	}	printIoTestResults(res);	}	catch (InterruptedException | BrokenBarrierException e) {	
io test failed 

assert nodeId != null;	IoTestThreadLocalNodeResults nodeRes = res0.get(nodeId);	if (nodeRes == null) res0.put(nodeId, nodeRes = new IoTestThreadLocalNodeResults(rangesCnt, latencyLimit));	nodeRes.onResult(msg);	}	}	bar.await();	failed = false;	}	catch (Exception e) {	
io test worker thread failed 

else processRegularMessage(nodeId, msg, plc, msgC);	break;	}	default: assert plc >= 0 : "Negative policy [plc=" + plc + ", msg=" + msg + ']';	if (isReservedGridIoPolicy(plc)) throw new IgniteCheckedException("Failed to process message with policy of reserved range. " + "[policy=" + plc + ']');	if (msg.isOrdered()) processOrderedMessage(nodeId, msg, plc, msgC);	else processRegularMessage(nodeId, msg, plc, msgC);	}	}	catch (IgniteCheckedException e) {	
failed to process message will ignore 

}	if (rmv) msgSetMap.remove(msg.topic(), map);	else {	assert set != null;	assert !isNew;	set.add(msg, msgC);	break;	}	}	if (isNew && ctx.discovery().node(nodeId) == null) {	
message is ignored as sender has left the grid 

map.remove(nodeId);	rmv = map.isEmpty();	}	if (rmv) msgSetMap.remove(msg.topic(), map);	return;	}	if (isNew && set.endTime() != Long.MAX_VALUE) ctx.timeout().addTimeoutObject(set);	final GridMessageListener lsnr = listenerGet0(msg.topic());	if (lsnr == null) {	if (closedTopics.contains(msg.topic())) {	
message is ignored as it came for the closed topic 

return;	}	if (isNew && set.endTime() != Long.MAX_VALUE) ctx.timeout().addTimeoutObject(set);	final GridMessageListener lsnr = listenerGet0(msg.topic());	if (lsnr == null) {	if (closedTopics.contains(msg.topic())) {	assert map != null;	msgSetMap.remove(msg.topic(), map);	}	else if (log.isDebugEnabled()) {	
received message for unknown listener messages will be kept until a listener is registered 

private void unwindMessageSet(GridCommunicationMessageSet msgSet, GridMessageListener lsnr) {	while (true) {	if (msgSet.reserve()) {	try {	msgSet.unwind(lsnr);	}	finally {	msgSet.release();	}	if (!msgSet.changed()) {	
message set has not been changed 

msgSet.unwind(lsnr);	}	finally {	msgSet.release();	}	if (!msgSet.changed()) {	break;	}	}	else {	
another thread owns reservation 

final GridMessageListener lsnrs0 = lsnrs;	try {	for (final GridCommunicationMessageSet msgSet : msgSets) {	pools.poolForPolicy(msgSet.policy()).execute( new Runnable() {	unwindMessageSet(msgSet, lsnrs0);	}	});	}	}	catch (RejectedExecutionException e) {	
failed to process delayed message due to execution rejection increase the upper bound on executor service provided in igniteconfiguration getpublicthreadpoolsize will attempt to process message in the listener thread instead 

========================= ignite sample_3565 =========================

protected void splitAndWait() throws InterruptedException, IgniteCheckedException {	
simulating split 

protected void unsplit() {	
restoring from split 

========================= ignite sample_1108 =========================

locNodeAddrs.addAll(locNode.socketAddresses());	if (extAddrs != null) {	locNodeAttrs.put(createSpiAttributeName(ATTR_EXT_ADDRS), extAddrs);	locNodeAddrs.addAll(extAddrs);	}	}	locNode.setAttributes(locNodeAttrs);	locNode.local(true);	DiscoverySpiListener lsnr = this.lsnr;	if (lsnr != null) lsnr.onLocalNodeInitialized(locNode);	
local node initialized 

protected Collection<InetSocketAddress> resolvedAddresses() throws IgniteSpiException {	List<InetSocketAddress> res = new ArrayList<>();	Collection<InetSocketAddress> addrs;	while (true) {	try {	addrs = registeredAddresses();	break;	}	catch (IgniteSpiException e) {	
failed to get registered addresses from ip finder on start retrying every ms 

throw new IgniteSpiException("Thread has been interrupted.", e);	}	}	for (InetSocketAddress addr : addrs) {	assert addr != null;	try {	InetSocketAddress resolved = addr.isUnresolved() ? new InetSocketAddress(InetAddress.getByName(addr.getHostName()), addr.getPort()) : addr;	if (locNodeAddrs == null || !locNodeAddrs.contains(resolved)) res.add(resolved);	}	catch (UnknownHostException ignored) {	
failed to resolve address from ip finder host is unknown 

throw new IgniteSpiException("Failed to create SSL context. SSL factory: " + ignite.configuration().getSslContextFactory(), e);	}	}	try {	locHost = U.resolveLocalHost(locAddr);	}	catch (IOException e) {	throw new IgniteSpiException("Unknown local address: " + locAddr, e);	}	if (log.isDebugEnabled()) {	
localHost 

throw new IgniteSpiException("Failed to create SSL context. SSL factory: " + ignite.configuration().getSslContextFactory(), e);	}	}	try {	locHost = U.resolveLocalHost(locAddr);	}	catch (IOException e) {	throw new IgniteSpiException("Unknown local address: " + locAddr, e);	}	if (log.isDebugEnabled()) {	
localPort 

throw new IgniteSpiException("Failed to create SSL context. SSL factory: " + ignite.configuration().getSslContextFactory(), e);	}	}	try {	locHost = U.resolveLocalHost(locAddr);	}	catch (IOException e) {	throw new IgniteSpiException("Unknown local address: " + locAddr, e);	}	if (log.isDebugEnabled()) {	
localPortRange 

throw new IgniteSpiException("Failed to create SSL context. SSL factory: " + ignite.configuration().getSslContextFactory(), e);	}	}	try {	locHost = U.resolveLocalHost(locAddr);	}	catch (IOException e) {	throw new IgniteSpiException("Unknown local address: " + locAddr, e);	}	if (log.isDebugEnabled()) {	
threadPri 

}	}	try {	locHost = U.resolveLocalHost(locAddr);	}	catch (IOException e) {	throw new IgniteSpiException("Unknown local address: " + locAddr, e);	}	if (log.isDebugEnabled()) {	if (!failureDetectionTimeoutEnabled()) {	
failure detection timeout is ignored because at least one of the parameters from this list has been set explicitly socktimeout acktimeout maxacktimeout reconnectcount 

}	}	try {	locHost = U.resolveLocalHost(locAddr);	}	catch (IOException e) {	throw new IgniteSpiException("Unknown local address: " + locAddr, e);	}	if (log.isDebugEnabled()) {	if (!failureDetectionTimeoutEnabled()) {	
networkTimeout 

}	}	try {	locHost = U.resolveLocalHost(locAddr);	}	catch (IOException e) {	throw new IgniteSpiException("Unknown local address: " + locAddr, e);	}	if (log.isDebugEnabled()) {	if (!failureDetectionTimeoutEnabled()) {	
sockTimeout 

}	}	try {	locHost = U.resolveLocalHost(locAddr);	}	catch (IOException e) {	throw new IgniteSpiException("Unknown local address: " + locAddr, e);	}	if (log.isDebugEnabled()) {	if (!failureDetectionTimeoutEnabled()) {	
ackTimeout 

}	}	try {	locHost = U.resolveLocalHost(locAddr);	}	catch (IOException e) {	throw new IgniteSpiException("Unknown local address: " + locAddr, e);	}	if (log.isDebugEnabled()) {	if (!failureDetectionTimeoutEnabled()) {	
maxAckTimeout 

}	}	try {	locHost = U.resolveLocalHost(locAddr);	}	catch (IOException e) {	throw new IgniteSpiException("Unknown local address: " + locAddr, e);	}	if (log.isDebugEnabled()) {	if (!failureDetectionTimeoutEnabled()) {	
reconnectCount 

}	try {	locHost = U.resolveLocalHost(locAddr);	}	catch (IOException e) {	throw new IgniteSpiException("Unknown local address: " + locAddr, e);	}	if (log.isDebugEnabled()) {	if (!failureDetectionTimeoutEnabled()) {	}	
failureDetectionTimeout 

}	try {	locHost = U.resolveLocalHost(locAddr);	}	catch (IOException e) {	throw new IgniteSpiException("Unknown local address: " + locAddr, e);	}	if (log.isDebugEnabled()) {	if (!failureDetectionTimeoutEnabled()) {	}	
ipFinder 

}	try {	locHost = U.resolveLocalHost(locAddr);	}	catch (IOException e) {	throw new IgniteSpiException("Unknown local address: " + locAddr, e);	}	if (log.isDebugEnabled()) {	if (!failureDetectionTimeoutEnabled()) {	}	
ipFinderCleanFreq 

}	try {	locHost = U.resolveLocalHost(locAddr);	}	catch (IOException e) {	throw new IgniteSpiException("Unknown local address: " + locAddr, e);	}	if (log.isDebugEnabled()) {	if (!failureDetectionTimeoutEnabled()) {	}	
metricsUpdateFreq 

}	try {	locHost = U.resolveLocalHost(locAddr);	}	catch (IOException e) {	throw new IgniteSpiException("Unknown local address: " + locAddr, e);	}	if (log.isDebugEnabled()) {	if (!failureDetectionTimeoutEnabled()) {	}	
statsPrintFreq 

try {	locHost = U.resolveLocalHost(locAddr);	}	catch (IOException e) {	throw new IgniteSpiException("Unknown local address: " + locAddr, e);	}	if (log.isDebugEnabled()) {	if (!failureDetectionTimeoutEnabled()) {	}	}	
network timeout is too low at least ms recommended 

========================= ignite sample_5665 =========================

private void initWriteThrottle() {	if (!(ctx.database() instanceof GridCacheDatabaseSharedManager)) {	
write throttle can t start unexpected class of database manager 

private long evictPage() throws IgniteCheckedException {	assert getWriteHoldCount() > 0;	if (!pageEvictWarned) {	pageEvictWarned = true;	
page evictions started this will affect storage performance consider increasing dataregionconfiguration setmaxsize 

========================= ignite sample_4131 =========================

final List<CacheEntryEvent<? extends QueryTestKey, ? extends QueryTestValue>> evts = new CopyOnWriteArrayList<>();	if (noOpFilterFactory() != null) qry.setRemoteFilterFactory(noOpFilterFactory());	qry.setLocalListener(new CacheEntryUpdatedListener<QueryTestKey, QueryTestValue>() {	? extends QueryTestValue>> events) throws CacheEntryListenerException {	for (CacheEntryEvent<? extends QueryTestKey, ? extends QueryTestValue> e : events) evts.add(e);	}	});	QueryTestKey key = new QueryTestKey(1);	try (QueryCursor qryCur = cache.query(qry)) {	for (int i = 0; i < ITERATION_CNT; i++) {	
start iteration 

checkSingleEvent(evts.get(7), CREATED, new QueryTestValue(5), null);	checkSingleEvent(evts.get(8), EventType.UPDATED, new QueryTestValue(6), new QueryTestValue(5));	evts.clear();	cache.remove(key);	cache.remove(key);	assert GridTestUtils.waitForCondition(new PA() {	return evts.size() == 1;	}	}, 5_000);	evts.clear();	
finish iteration 

if (noOpFilterFactory() != null) qry.setRemoteFilterFactory(noOpFilterFactory());	qry.setLocalListener(new CacheEntryUpdatedListener<QueryTestKey, QueryTestValue>() {	? extends QueryTestValue>> events) throws CacheEntryListenerException {	for (CacheEntryEvent<? extends QueryTestKey, ? extends QueryTestValue> e : events) evts.add(e);	}	});	Map<QueryTestKey, QueryTestValue> map = new TreeMap<>();	for (int i = 0; i < KEYS; i++) map.put(new QueryTestKey(i), new QueryTestValue(i));	try (QueryCursor qryCur = cache.query(qry)) {	for (int i = 0; i < ITERATION_CNT / 2; i++) {	
start iteration 

assertEquals(0, evts.size());	cache.invokeAll(map.keySet(), (EntryProcessor<QueryTestKey, QueryTestValue, ? extends Object>) (Object)new EntrySetValueProcessor(null, false));	cache.removeAll(map.keySet());	cache.removeAll(map.keySet());	assert GridTestUtils.waitForCondition(new PA() {	return evts.size() == KEYS;	}	}, 5_000);	checkEvents(evts, REMOVED);	evts.clear();	
finish iteration 

protected void doTestContinuousQuery(CacheConfiguration<Object, Object> ccfg, ContinuousDeploy deploy) throws Exception {	ignite(0).createCache(ccfg);	try {	long seed = System.currentTimeMillis();	Random rnd = new Random(seed);	
random seed 

========================= ignite sample_1919 =========================

public void testAffinitySimpleStopRandomNode() throws Exception {	final int ITERATIONS = 3;	for (int iter = 0; iter < 3; iter++) {	
iteration 

final int ITERATIONS = 3;	for (int iter = 0; iter < 3; iter++) {	final int NODES = 5;	for (int i = 0 ; i < NODES; i++) startServer(i, i + 1);	int majorVer = NODES;	checkAffinity(majorVer, topVer(majorVer, 1), true);	Set<Integer> stopOrder = new HashSet<>();	while (stopOrder.size() != NODES - 1) stopOrder.add(ThreadLocalRandom.current().nextInt(NODES));	int nodes = NODES;	for (Integer idx : stopOrder) {	
stop node 

final int MAX_CLIENTS = 10;	final int MAX_CACHES = 15;	List<String> srvs = new ArrayList<>();	List<String> clients = new ArrayList<>();	int srvIdx = 0;	int clientIdx = 0;	int cacheIdx = 0;	List<String> caches = new ArrayList<>();	long seed = System.currentTimeMillis();	Random rnd = new Random(seed);	
random seed 

Random rnd = new Random(seed);	long topVer = 0;	for (int i = 0; i < 100; i++) {	int op = i == 0 ? 0 : rnd.nextInt(7);	log.info("Iteration [iter=" + i + ", op=" + op + ']');	switch (op) {	case 0: {	if (srvs.size() < MAX_SRVS) {	srvIdx++;	String srvName = "server-" + srvIdx;	
start server 

int op = i == 0 ? 0 : rnd.nextInt(7);	log.info("Iteration [iter=" + i + ", op=" + op + ']');	switch (op) {	case 0: {	if (srvs.size() < MAX_SRVS) {	srvIdx++;	String srvName = "server-" + srvIdx;	if (rnd.nextBoolean()) {	cacheIdx++;	String cacheName = "join-cache-" + cacheIdx;	
cache for joining node 

String srvName = "server-" + srvIdx;	if (rnd.nextBoolean()) {	cacheIdx++;	String cacheName = "join-cache-" + cacheIdx;	cacheClosure(rnd, caches, cacheName, srvs, srvIdx);	}	else cacheClosure(rnd, caches, DEFAULT_CACHE_NAME, srvs, srvIdx);	startNode(srvName, ++topVer, false);	srvs.add(srvName);	}	
skip start server 

}	else cacheClosure(rnd, caches, DEFAULT_CACHE_NAME, srvs, srvIdx);	startNode(srvName, ++topVer, false);	srvs.add(srvName);	}	break;	}	case 1: {	if (srvs.size() > 1) {	String srvName = srvs.get(rnd.nextInt(srvs.size()));	
stop server 

srvs.add(srvName);	}	break;	}	case 1: {	if (srvs.size() > 1) {	String srvName = srvs.get(rnd.nextInt(srvs.size()));	stopNode(srvName, ++topVer);	srvs.remove(srvName);	}	
skip stop server 

String srvName = srvs.get(rnd.nextInt(srvs.size()));	stopNode(srvName, ++topVer);	srvs.remove(srvName);	}	break;	}	case 2: {	if (clients.size() < MAX_CLIENTS) {	clientIdx++;	String clientName = "client-" + clientIdx;	
start client 

}	break;	}	case 2: {	if (clients.size() < MAX_CLIENTS) {	clientIdx++;	String clientName = "client-" + clientIdx;	if (rnd.nextBoolean()) {	cacheIdx++;	String cacheName = "join-cache-" + cacheIdx;	
cache for joining node 

String clientName = "client-" + clientIdx;	if (rnd.nextBoolean()) {	cacheIdx++;	String cacheName = "join-cache-" + cacheIdx;	cacheClosure(rnd, caches, cacheName, srvs, srvIdx);	}	else cacheClosure(rnd, caches, DEFAULT_CACHE_NAME, srvs, srvIdx);	startNode(clientName, ++topVer, true);	clients.add(clientName);	}	
skip start client 

}	else cacheClosure(rnd, caches, DEFAULT_CACHE_NAME, srvs, srvIdx);	startNode(clientName, ++topVer, true);	clients.add(clientName);	}	break;	}	case 3: {	if (clients.size() > 1) {	String clientName = clients.get(rnd.nextInt(clients.size()));	
stop client 

clients.add(clientName);	}	break;	}	case 3: {	if (clients.size() > 1) {	String clientName = clients.get(rnd.nextInt(clients.size()));	stopNode(clientName, ++topVer);	clients.remove(clientName);	}	
skip stop client 

break;	}	case 4: {	if (caches.size() > 0) {	String cacheName = caches.get(rnd.nextInt(caches.size()));	Ignite node = randomNode(rnd, srvs, clients);	log.info("Destroy cache [cache=" + cacheName + ", node=" + node.name() + ']');	node.destroyCache(cacheName);	caches.remove(cacheName);	}	
skip destroy cache 

case 5: {	if (caches.size() < MAX_CACHES) {	cacheIdx++;	String cacheName = "cache-" + cacheIdx;	Ignite node = randomNode(rnd, srvs, clients);	log.info("Create cache [cache=" + cacheName + ", node=" + node.name() + ']');	node.createCache(randomCacheConfiguration(rnd, cacheName, srvs, srvIdx));	calculateAffinity(topVer);	caches.add(cacheName);	}	
skip create cache 

if (caches.size() > 0) {	for (int j = 0; j < 3; j++) {	String cacheName = caches.get(rnd.nextInt(caches.size()));	for (int k = 0; k < 3; k++) {	Ignite node = randomNode(rnd, srvs, clients);	log.info("Get/closes cache [cache=" + cacheName + ", node=" + node.name() + ']');	node.cache(cacheName).close();	}	}	}	
skip get close cache 

}	break;	}	default: fail();	}	IgniteKernal node = (IgniteKernal)grid(srvs.get(0));	checkAffinity(srvs.size() + clients.size(), node.context().cache().context().exchange().readyAffinityVersion(), false);	}	srvIdx++;	String srvName = "server-" + srvIdx;	
start server 

cache.getAndPut(key, rnd.nextInt(10));	cache.invoke(key + 1, new TestEntryProcessor(rnd.nextInt(10)));	cache.get(key + 2);	tx.commit();	}	}	}	}	}	catch (Exception e) {	
cache operation failed 

public void stopBlock() {	List<DiscoverySpiCustomMessage> msgs;	synchronized (this) {	msgs = new ArrayList<>(blockedMsgs);	blockCustomEvt = false;	blockedMsgs.clear();	}	for (DiscoverySpiCustomMessage msg : msgs) {	
resend blocked message 

========================= ignite sample_1082 =========================

protected final void clearSerializationCaches() {	try {	clearSerializationCache(Class.forName("java.io.ObjectInputStream$Caches"), "subclassAudits");	clearSerializationCache(Class.forName("java.io.ObjectOutputStream$Caches"), "subclassAudits");	clearSerializationCache(Class.forName("java.io.ObjectStreamClass$Caches"), "localDescs");	clearSerializationCache(Class.forName("java.io.ObjectStreamClass$Caches"), "reflectors");	}	catch (ClassNotFoundException e) {	
class not found 

protected final void clearSerializationCaches() {	try {	clearSerializationCache(Class.forName("java.io.ObjectInputStream$Caches"), "subclassAudits");	clearSerializationCache(Class.forName("java.io.ObjectOutputStream$Caches"), "subclassAudits");	clearSerializationCache(Class.forName("java.io.ObjectStreamClass$Caches"), "localDescs");	clearSerializationCache(Class.forName("java.io.ObjectStreamClass$Caches"), "reflectors");	}	catch (ClassNotFoundException e) {	}	catch (NoSuchFieldException e) {	
field not found 

clearSerializationCache(Class.forName("java.io.ObjectInputStream$Caches"), "subclassAudits");	clearSerializationCache(Class.forName("java.io.ObjectOutputStream$Caches"), "subclassAudits");	clearSerializationCache(Class.forName("java.io.ObjectStreamClass$Caches"), "localDescs");	clearSerializationCache(Class.forName("java.io.ObjectStreamClass$Caches"), "reflectors");	}	catch (ClassNotFoundException e) {	}	catch (NoSuchFieldException e) {	}	catch (IllegalAccessException e) {	
field can t be accessed 

========================= ignite sample_3545 =========================

private void checkQuery(String sql, IgniteCache<Object, Object> cache, int expSize, Object... args) {	String plan = (String)cache.query(new SqlFieldsQuery("explain " + sql)) .getAll().get(0).get(0);	
plan 

========================= ignite sample_7506 =========================

private void salvageTx(IgniteInternalTx tx, IgniteInternalTx.FinalizationStatus status) {	assert tx != null;	TransactionState state = tx.state();	if (state == ACTIVE || state == PREPARING || state == PREPARED || state == MARKED_ROLLBACK) {	if (!tx.markFinalizing(status)) {	
will not try to commit invalidate transaction could not mark finalized 

private void salvageTx(IgniteInternalTx tx, IgniteInternalTx.FinalizationStatus status) {	assert tx != null;	TransactionState state = tx.state();	if (state == ACTIVE || state == PREPARING || state == PREPARED || state == MARKED_ROLLBACK) {	if (!tx.markFinalizing(status)) {	return;	}	tx.salvageTx();	
invalidated transaction because originating node left grid 

public boolean onStarted(IgniteInternalTx tx) {	assert tx.state() == ACTIVE || tx.isRollbackOnly() : "Invalid transaction state [locId=" + cctx.localNodeId() + ", tx=" + tx + ']';	if (isCompleted(tx)) {	ConcurrentMap<GridCacheVersion, IgniteInternalTx> txIdMap = transactionMap(tx);	txIdMap.remove(tx.xidVersion(), tx);	
attempt to start a completed transaction will ignore 

public boolean onStarted(IgniteInternalTx tx) {	assert tx.state() == ACTIVE || tx.isRollbackOnly() : "Invalid transaction state [locId=" + cctx.localNodeId() + ", tx=" + tx + ']';	if (isCompleted(tx)) {	ConcurrentMap<GridCacheVersion, IgniteInternalTx> txIdMap = transactionMap(tx);	txIdMap.remove(tx.xidVersion(), tx);	return false;	}	
transaction started 

if (cached.detached()) continue;	try {	if (cached.obsolete() || cached.markObsoleteIfEmpty(tx.xidVersion())) cacheCtx.cache().removeEntry(cached);	if (!tx.near() && isNearEnabled(cacheCtx)) {	GridNearCacheAdapter near = cacheCtx.isNear() ? cacheCtx.near() : cacheCtx.dht().near();	GridNearCacheEntry e = near.peekExx(entry.key());	if (e != null && e.markObsoleteIfEmpty(null)) near.removeEntry(e);	}	}	catch (IgniteCheckedException e) {	
failed to remove obsolete entry from cache 

private void collectPendingVersions(GridDhtTxLocal dhtTxLoc) {	if (dhtTxLoc.needsCompletedVersions()) {	
checking for pending locks with version less then tx version 

private void collectPendingVersions(Iterable<IgniteTxEntry> entries, GridCacheVersion baseVer, Set<GridCacheVersion> vers) {	for (IgniteTxEntry txEntry : entries) {	GridCacheEntryEx cached = txEntry.cached();	try {	if (!cached.obsolete()) {	for (GridCacheMvccCandidate cand : cached.localCandidates()) {	if (!cand.owner() && cand.version().compareTo(baseVer) < 0) {	
adding candidate version to pending set 

try {	if (!cached.obsolete()) {	for (GridCacheMvccCandidate cand : cached.localCandidates()) {	if (!cand.owner() && cand.version().compareTo(baseVer) < 0) {	vers.add(cand.version());	}	}	}	}	catch (GridCacheEntryRemovedException ignored) {	
there are no pending locks for entry entry was deleted in transaction 

if (mapped != null) mappedVers.remove(mapped);	}	resetContext();	if (!tx.dht() && tx.local()) {	if (!tx.system()) cctx.txMetrics().onTxCommit();	tx.txState().onTxEnd(cctx, tx, true);	}	if (slowTxWarnTimeout > 0 && tx.local() && U.currentTimeMillis() - tx.startTime() > slowTxWarnTimeout) U.warn(log, "Slow transaction detected [tx=" + tx + ", slowTxWarnTimeout=" + slowTxWarnTimeout + ']') ;	if (log.isDebugEnabled()) log.debug("Committed from TM [locNodeId=" + cctx.localNodeId() + ", tx=" + tx + ']');	}	
did not commit from tm was already committed 

notifyEvictions(tx);	removeObsolete(tx);	if (clearThreadMap) clearThreadMap(tx);	if (!tx.alternateVersions().isEmpty()) for (GridCacheVersion ver : tx.alternateVersions()) idMap.remove(ver);	if (tx instanceof GridCacheMappedVersion) mappedVers.remove(((GridCacheMappedVersion)tx).mappedVersion());	resetContext();	if (!tx.dht() && tx.local()) {	if (!tx.system()) cctx.txMetrics().onTxRollback();	tx.txState().onTxEnd(cctx, tx, false);	}	
rolled back from tm 

removeObsolete(tx);	if (clearThreadMap) clearThreadMap(tx);	if (!tx.alternateVersions().isEmpty()) for (GridCacheVersion ver : tx.alternateVersions()) idMap.remove(ver);	if (tx instanceof GridCacheMappedVersion) mappedVers.remove(((GridCacheMappedVersion)tx).mappedVersion());	resetContext();	if (!tx.dht() && tx.local()) {	if (!tx.system()) cctx.txMetrics().onTxRollback();	tx.txState().onTxEnd(cctx, tx, false);	}	}	
did not rollback from tm was already rolled back 

public boolean onOwnerChanged(GridCacheEntryEx entry, GridCacheMvccCandidate owner) {	if (owner != null) {	IgniteTxAdapter tx = tx(owner.version());	if (tx == null) tx = nearTx(owner.version());	if (tx != null) {	if (!tx.local()) {	if (log.isDebugEnabled()) log.debug("Found transaction for owner changed event [owner=" + owner + ", entry=" + entry + ", tx=" + tx + ']');	tx.onOwnerChanged(entry, owner);	return true;	}	
ignoring local transaction for owner change event 

if (!entry1.tmLock(tx, timeout, serOrder, serReadVer, read)) {	for (IgniteTxEntry txEntry2 : entries) {	if (txEntry2 == txEntry1) break;	txUnlock(tx, txEntry2);	}	return false;	}	break;	}	catch (GridCacheEntryRemovedException ignored) {	
got removed entry in tm lockmultiple method will retry 

private void txUnlock(IgniteInternalTx tx, IgniteTxEntry txEntry) {	while (true) {	try {	txEntry.cached().txUnlock(tx);	break;	}	catch (GridCacheEntryRemovedException ignored) {	
got removed entry in tm txunlock method will retry 

GridCacheContext cacheCtx = txEntry.context();	while (true) {	try {	GridCacheEntryEx entry = txEntry.cached();	assert entry != null;	if (entry.detached()) break;	entry.txUnlock(tx);	break;	}	catch (GridCacheEntryRemovedException ignored) {	
got removed entry in tm unlockmultiple method will retry 

public IgniteInternalFuture<Boolean> txCommitted(GridCacheVersion xidVer) {	final GridFutureAdapter<Boolean> resFut = new GridFutureAdapter<>();	final IgniteInternalTx tx = cctx.tm().tx(xidVer);	if (tx != null) {	assert tx.near() && tx.local() : tx;	
found near transaction will wait for completion 

public IgniteInternalFuture<Boolean> txCommitted(GridCacheVersion xidVer) {	final GridFutureAdapter<Boolean> resFut = new GridFutureAdapter<>();	final IgniteInternalTx tx = cctx.tm().tx(xidVer);	if (tx != null) {	assert tx.near() && tx.local() : tx;	tx.finishFuture().listen(new CI1<IgniteInternalFuture<IgniteInternalTx>>() {	TransactionState state = tx.state();	
near transaction finished with state 

public void finishTxOnRecovery(final IgniteInternalTx tx, boolean commit) {	if (log.isDebugEnabled()) log.debug("Finishing prepared transaction [tx=" + tx + ", commit=" + commit + ']');	if (!tx.markFinalizing(RECOVERY_FINISH)) {	
will not try to commit prepared transaction could not mark finalized 

private void processFailedMessage(UUID nodeId, GridCacheMessage msg, Throwable err) throws IgniteCheckedException {	switch (msg.directType()) {	case -24: {	TxLocksRequest req = (TxLocksRequest)msg;	TxLocksResponse res = new TxLocksResponse();	res.futureId(req.futureId());	try {	cctx.gridIO().sendToGridTopic(nodeId, TOPIC_TX, res, SYSTEM_POOL);	}	catch (ClusterTopologyCheckedException e) {	
failed to send response node failed 

========================= ignite sample_3953 =========================

SelectedSelectionKeySet selectedKeySet = new SelectedSelectionKeySet();	Class<?> selectorImplCls = Class.forName("sun.nio.ch.SelectorImpl", false, U.gridClassLoader());	if (!selectorImplCls.isAssignableFrom(selector.getClass())) return;	Field selectedKeysField = selectorImplCls.getDeclaredField("selectedKeys");	Field publicSelectedKeysField = selectorImplCls.getDeclaredField("publicSelectedKeys");	selectedKeysField.setAccessible(true);	publicSelectedKeysField.setAccessible(true);	selectedKeysField.set(selector, selectedKeySet);	publicSelectedKeysField.set(selector, selectedKeySet);	selectedKeys = selectedKeySet;	
instrumented an optimized java util set into 

select = false;	}	long now = U.currentTimeMillis();	if (now - lastIdleCheck > 2000) {	lastIdleCheck = now;	checkIdle(selector.keys());	}	}	}	catch (ClosedByInterruptException e) {	
closing selector due to thread interruption 

catch (ClosedByInterruptException e) {	}	catch (ClosedSelectorException e) {	throw new IgniteCheckedException("Selector got closed while active.", e);	}	catch (IOException e) {	throw new IgniteCheckedException("Failed to select events on selector.", e);	}	finally {	if (selector.isOpen()) {	
closing all connected client sockets 

}	catch (IOException e) {	throw new IgniteCheckedException("Failed to select events on selector.", e);	}	finally {	if (selector.isOpen()) {	for (SelectionKey key : selector.keys()) {	GridNioKeyAttachment attach = (GridNioKeyAttachment)key.attachment();	if (attach != null && attach.hasSession()) close(attach.session(), null);	}	
closing nio selector 

private void processSelectedKeys(Set<SelectionKey> keys) throws ClosedByInterruptException {	
processing keys in client worker 

filterChain.onSessionOpened(ses);	fut.onDone(ses);	}	catch (IgniteCheckedException e) {	close(ses, e);	fut.onDone(e);	}	if (closed) ses.onServerStopped();	}	catch (ClosedChannelException e) {	
failed to register accepted socket channel to selector channel was closed 

}	catch (IgniteCheckedException e) {	close(ses, e);	fut.onDone(e);	}	if (closed) ses.onServerStopped();	}	catch (ClosedChannelException e) {	}	catch (IOException e) {	
failed to get socket addresses 

protected boolean close(final GridSelectorNioSessionImpl ses, @Nullable final IgniteCheckedException e) {	if (e != null) {	if (e.hasCause(IOException.class)) U.warn(log, "Closing NIO session because of unhandled exception [cls=" + e.getClass() + ", msg=" + e.getMessage() + ']');	
closing nio session because of unhandled exception 

private void closeSelector() {	if (selector.isOpen()) {	
closing all listening sockets 

private void closeSelector() {	if (selector.isOpen()) {	for (SelectionKey key : selector.keys()) U.close(key.channel(), log);	
closing nio selector 

private void processSelectedKeys(Set<SelectionKey> keys) throws IOException {	
processing keys in accept worker 

iter.remove();	if (!key.isValid()) continue;	if (key.isAcceptable()) {	ServerSocketChannel srvrCh = (ServerSocketChannel)key.channel();	SocketChannel sockCh = srvrCh.accept();	sockCh.configureBlocking(false);	sockCh.socket().setTcpNoDelay(tcpNoDelay);	sockCh.socket().setKeepAlive(true);	if (sockSndBuf > 0) sockCh.socket().setSendBufferSize(sockSndBuf);	if (sockRcvBuf > 0) sockCh.socket().setReceiveBufferSize(sockRcvBuf);	
accepted new client connection 

========================= ignite sample_3379 =========================

IgniteFuture<?> fut3 = cache.getAndPutAsync(1, 3);	assertFalse(fut1.isDone());	assertFalse(fut2.isDone());	assertFalse(fut3.isDone());	latch.countDown();	try {	fut1.get();	fail();	}	catch (CacheException e) {	
expected error 

assertFalse(fut1.isDone());	assertFalse(fut2.isDone());	assertFalse(fut3.isDone());	assertFalse(fut4.isDone());	latch.countDown();	try {	fut1.get();	fail();	}	catch (CacheException e) {	
expected error 

fut1.get();	fail();	}	catch (CacheException e) {	}	try {	fut2.get();	fail();	}	catch (CacheException e) {	
expected error 

fail();	}	catch (CacheException e) {	}	assertEquals(1, fut3.get());	try {	fut4.get();	fail();	}	catch (CacheException e) {	
expected error 

========================= ignite sample_1201 =========================

private void createCheckpointTable(Connection conn) throws SQLException {	Statement st = null;	try {	st = conn.createStatement();	st.executeUpdate(crtTblSql);	
successfully created checkpoint table 

}	delSt = conn.prepareStatement(delExpSql);	delSt.setTime(1, time);	delCnt = delSt.executeUpdate();	}	finally {	U.close(rs, log);	U.close(selSt, log);	U.close(delSt, log);	}	
successfully removed expired checkpoints from 

========================= ignite sample_5612 =========================

CacheOperationContext opCtx = cacheCtx.operationContextPerCall();	final Byte dataCenterId = opCtx != null ? opCtx.dataCenterId() : null;	KeyCacheObject cacheKey = cacheCtx.toCacheKeyObject(key);	boolean keepBinary = opCtx != null && opCtx.isKeepBinary();	final CacheEntryPredicate[] filters = CU.filterArray(filter);	final IgniteInternalFuture<Void> loadFut = enlistWrite( cacheCtx, entryTopVer, cacheKey, val, opCtx != null ? opCtx.expiry() : null, entryProcessor, invokeArgs, retval, filters, ret, opCtx != null && opCtx.skipStore(), keepBinary, opCtx != null && opCtx.recovery(), dataCenterId);	if (pessimistic()) {	assert loadFut == null || loadFut.isDone() : loadFut;	if (loadFut != null) loadFut.get();	final Collection<KeyCacheObject> enlisted = Collections.singleton(cacheKey);	
before acquiring transaction lock for put on key 

final IgniteInternalFuture<Void> loadFut = enlistWrite( cacheCtx, entryTopVer, cacheKey, val, opCtx != null ? opCtx.expiry() : null, entryProcessor, invokeArgs, retval, filters, ret, opCtx != null && opCtx.skipStore(), keepBinary, opCtx != null && opCtx.recovery(), dataCenterId);	if (pessimistic()) {	assert loadFut == null || loadFut.isDone() : loadFut;	if (loadFut != null) loadFut.get();	final Collection<KeyCacheObject> enlisted = Collections.singleton(cacheKey);	long timeout = remainingTime();	if (timeout == -1) return new GridFinishedFuture<>(timeoutException());	IgniteInternalFuture<Boolean> fut = cacheCtx.cache().txLockAsync(enlisted, timeout, this, retval, isolation, isInvalidate(), -1L, -1L);	PLC1<GridCacheReturn> plc1 = new PLC1<GridCacheReturn>(ret) {	throws IgniteCheckedException {	
acquired transaction lock for put on keys 

if (pessimistic()) {	assert loadFut == null || loadFut.isDone() : loadFut;	if (loadFut != null) {	try {	loadFut.get();	}	catch (IgniteCheckedException e) {	return new GridFinishedFuture(e);	}	}	
before acquiring transaction lock for put on keys 

}	catch (IgniteCheckedException e) {	return new GridFinishedFuture(e);	}	}	long timeout = remainingTime();	if (timeout == -1) return new GridFinishedFuture<>(timeoutException());	IgniteInternalFuture<Boolean> fut = cacheCtx.cache().txLockAsync(enlisted, timeout, this, retval, isolation, isInvalidate(), -1L, -1L);	PLC1<GridCacheReturn> plc1 = new PLC1<GridCacheReturn>(ret) {	throws IgniteCheckedException {	
acquired transaction lock for put on keys 

}	}	}	else {	if (retval && !transform) ret.set(cacheCtx, old, true, keepBinary);	else ret.success(true);	}	break;	}	catch (GridCacheEntryRemovedException ignore) {	
got removed entry in transaction method 

}	else keys0 = keys;	CacheOperationContext opCtx = cacheCtx.operationContextPerCall();	final Byte dataCenterId;	if (opCtx != null && opCtx.hasDataCenterId()) {	assert drMap == null : drMap;	dataCenterId = opCtx.dataCenterId();	}	else dataCenterId = null;	assert keys0 != null;	
called removeallasync tx keys implicit retval 

return new GridFinishedFuture<>(ret.success(true));	}	init();	final Collection<KeyCacheObject> enlisted = new ArrayList<>();	ExpiryPolicy plc;	final CacheEntryPredicate[] filters = CU.filterArray(filter);	if (!F.isEmpty(filters)) plc = opCtx != null ? opCtx.expiry() : null;	else plc = null;	final boolean keepBinary = opCtx != null && opCtx.isKeepBinary();	final IgniteInternalFuture<Void> loadFut = enlistWrite( cacheCtx, entryTopVer, keys0, plc, retval, filters, ret, enlisted, null, drMap, opCtx != null && opCtx.skipStore(), singleRmv, keepBinary, opCtx != null && opCtx.recovery(), dataCenterId );	
remove keys 

if (pessimistic()) {	assert loadFut == null || loadFut.isDone() : loadFut;	if (loadFut != null) {	try {	loadFut.get();	}	catch (IgniteCheckedException e) {	return new GridFinishedFuture<>(e);	}	}	
before acquiring transaction lock for remove on keys 

}	catch (IgniteCheckedException e) {	return new GridFinishedFuture<>(e);	}	}	long timeout = remainingTime();	if (timeout == -1) return new GridFinishedFuture<>(timeoutException());	IgniteInternalFuture<Boolean> fut = cacheCtx.cache().txLockAsync(enlisted, timeout, this, false, retval, isolation, isInvalidate(), -1L, -1L);	PLC1<GridCacheReturn> plc1 = new PLC1<GridCacheReturn>(ret) {	throws IgniteCheckedException {	
acquired transaction lock for remove on keys 

if (single && missed.isEmpty()) return new GridFinishedFuture<>(retMap);	if (pessimistic() && !readCommitted() && !skipVals) {	if (expiryPlc == null) expiryPlc = cacheCtx.expiry();	long accessTtl = expiryPlc != null ? CU.toTtl(expiryPlc.getExpiryForAccess()) : CU.TTL_NOT_CHANGED;	long createTtl = expiryPlc != null ? CU.toTtl(expiryPlc.getExpiryForCreation()) : CU.TTL_NOT_CHANGED;	long timeout = remainingTime();	if (timeout == -1) return new GridFinishedFuture<>(timeoutException());	IgniteInternalFuture<Boolean> fut = cacheCtx.cache().txLockAsync(lockKeys, timeout, this, true, true, isolation, isInvalidate(), createTtl, accessTtl);	final ExpiryPolicy expiryPlc0 = expiryPlc;	PLC2<Map<K, V>> plc2 = new PLC2<Map<K, V>>() {	
acquired transaction lock for read on keys 

if (val != null) {	missed.remove(cacheKey);	txEntry.setAndMarkValid(val);	if (!F.isEmpty(txEntry.entryProcessors())) val = txEntry.applyEntryProcessors(val);	cacheCtx.addResult(retMap, cacheKey, val, skipVals, keepCacheObjects, deserializeBinary, false, getRes, readVer, 0, 0, needVer);	if (readVer != null) txEntry.entryReadVersion(readVer);	}	break;	}	catch (GridCacheEntryRemovedException ignore) {	
got removed exception in get postlock will retry 

txEntry.markValid();	if (needReadVer) {	assert readVer != null;	txEntry.entryReadVersion(readVer);	}	}	}	break;	}	catch (GridCacheEntryRemovedException ignored) {	
got removed entry in transaction getallasync will retry 

try {	EntryGetResult res = entry.innerGetVersioned( null, this, CU.subjectId(this, cctx), null, resolveTaskName(), expiryPlc0, txEntry == null ? keepBinary : txEntry.keepBinary(), null);	if (res == null) {	if (misses == null) misses = new LinkedHashMap<>();	misses.put(key, entry.version());	}	else c.apply(key, skipVals ? true : res.value(), res.version());	break;	}	catch (GridCacheEntryRemovedException ignore) {	
got removed entry will retry 

try {	cacheCtx.shared().database().ensureFreeSpace(cacheCtx.dataRegion());	EntryGetResult verVal = entry.versionedValue(cacheVal, ver, null, null, null);	if (log.isDebugEnabled()) {	log.debug("Set value loaded from store into entry [" + "oldVer=" + ver + ", newVer=" + verVal.version() + ", entry=" + entry + ']');	}	ver = verVal.version();	break;	}	catch (GridCacheEntryRemovedException ignore) {	
got removed entry will retry 

public void suspend() throws IgniteCheckedException {	
suspend near local tx 

public void resume() throws IgniteCheckedException {	
resume near local tx 

public IgniteInternalFuture<IgniteInternalTx> commitNearTxLocalAsync() {	
committing near local tx 

private IgniteInternalFuture<IgniteInternalTx> rollbackNearTxLocalAsync(final boolean onTimeout) {	
rolling back near tx 

private IgniteInternalFuture<IgniteInternalTx> chainFinishFuture(final NearTxFinishFuture fut, final boolean commit) {	assert fut != null;	if (fut.commit() != commit) {	final GridNearTxLocal tx = this;	if (!commit) {	final GridNearTxFinishFuture rollbackFut = new GridNearTxFinishFuture<>(cctx, this, false);	fut.listen(new IgniteInClosure<IgniteInternalFuture<IgniteInternalTx>>() {	if (FINISH_FUT_UPD.compareAndSet(tx, fut, rollbackFut)) {	if (tx.state() == COMMITTED) {	
failed to rollback transaction is already committed 

setRollbackOnly();	fut.onError(new IgniteTxRollbackCheckedException("Failed to prepare transaction: " + this, e));	try {	rollback();	}	catch (IgniteTxOptimisticCheckedException e1) {	if (log.isDebugEnabled()) log.debug("Failed optimistically to prepare transaction [tx=" + this + ", e=" + e1 + ']');	fut.onError(e);	}	catch (IgniteCheckedException e1) {	
failed to rollback transaction 

public IgniteInternalFuture<IgniteInternalTx> commitAsyncLocal() {	
committing colocated tx locally 

final GridDhtTxFinishFuture fut = new GridDhtTxFinishFuture<>(cctx, this, true);	cctx.mvcc().addFuture(fut, fut.futureId());	if (prep == null || prep.isDone()) {	assert prep != null || optimistic();	IgniteCheckedException err = null;	try {	if (prep != null) prep.get();	}	catch (IgniteCheckedException e) {	err = e;	
failed to prepare transaction 

if (err != null) fut.rollbackOnError(err);	else fut.finish(true);	}	else prep.listen(new CI1<IgniteInternalFuture<?>>() {	IgniteCheckedException err = null;	try {	f.get();	}	catch (IgniteCheckedException e) {	err = e;	
failed to prepare transaction 

public IgniteInternalFuture<IgniteInternalTx> rollbackAsyncLocal() {	
rolling back colocated tx locally 

assert pessimistic();	try {	checkValid();	}	catch (IgniteCheckedException e) {	return new GridFinishedFuture<>(e);	}	final GridCacheReturn ret = new GridCacheReturn(localResult(), false);	if (F.isEmpty(keys)) return new GridFinishedFuture<>(ret);	init();	
before acquiring transaction lock on keys 

catch (IgniteCheckedException e) {	return new GridFinishedFuture<>(e);	}	final GridCacheReturn ret = new GridCacheReturn(localResult(), false);	if (F.isEmpty(keys)) return new GridFinishedFuture<>(ret);	init();	long timeout = remainingTime();	if (timeout == -1) return new GridFinishedFuture<>(timeoutException());	IgniteInternalFuture<Boolean> fut = cacheCtx.colocated().lockAllAsyncInternal(keys, timeout, this, isInvalidate(), read, retval, isolation, createTtl, accessTtl, CU.empty0(), skipStore, keepBinary);	return new GridEmbeddedFuture<>( fut, new PLC1<GridCacheReturn>(ret, false) {	
acquired transaction lock on keys 

private <K, V> IgniteInternalFuture<Map<K, V>> checkMissed( final GridCacheContext cacheCtx, final AffinityTopologyVersion topVer, final Map<K, V> map, final Map<KeyCacheObject, GridCacheVersion> missedMap, final boolean deserializeBinary, final boolean skipVals, final boolean keepCacheObjects, final boolean skipStore, final boolean recovery, final boolean needVer, final ExpiryPolicy expiryPlc ) {	
loading missed values for missed map 

========================= ignite sample_3841 =========================

env.put("IGNITE_TCP_DISCOVERY_ADDRESSES", getAddress(c.getNodeId().getHost()));	if (props.jvmOpts() != null && !props.jvmOpts().isEmpty()) env.put("JVM_OPTS", props.jvmOpts());	ctx.setEnvironment(env);	Map<String, LocalResource> resources = new HashMap<>();	resources.put("ignite", IgniteYarnUtils.setupFile(ignitePath, fs, LocalResourceType.ARCHIVE));	resources.put("ignite-config.xml", IgniteYarnUtils.setupFile(cfgPath, fs, LocalResourceType.FILE));	if (props.licencePath() != null) resources.put("gridgain-license.xml", IgniteYarnUtils.setupFile(new Path(props.licencePath()), fs, LocalResourceType.FILE));	if (props.userLibs() != null) resources.put("libs", IgniteYarnUtils.setupFile(new Path(props.userLibs()), fs, LocalResourceType.FILE));	ctx.setLocalResources(resources);	ctx.setCommands( Collections.singletonList( (props.licencePath() != null ? "cp gridgain-license.xml ./ignite/*/ || true && " : "") + "cp -r ./libs/* ./ignite/*/libs/ || true && " + "./ignite/*/bin/ignite.sh " + "./ignite-config.xml" + " -J-Xmx" + ((int)props.memoryPerNode()) + "m" + " -J-Xms" + ((int)props.memoryPerNode()) + "m" + IgniteYarnUtils.YARN_LOG_OUT ));	
launching container 

resources.put("ignite", IgniteYarnUtils.setupFile(ignitePath, fs, LocalResourceType.ARCHIVE));	resources.put("ignite-config.xml", IgniteYarnUtils.setupFile(cfgPath, fs, LocalResourceType.FILE));	if (props.licencePath() != null) resources.put("gridgain-license.xml", IgniteYarnUtils.setupFile(new Path(props.licencePath()), fs, LocalResourceType.FILE));	if (props.userLibs() != null) resources.put("libs", IgniteYarnUtils.setupFile(new Path(props.userLibs()), fs, LocalResourceType.FILE));	ctx.setLocalResources(resources);	ctx.setCommands( Collections.singletonList( (props.licencePath() != null ? "cp gridgain-license.xml ./ignite/*/ || true && " : "") + "cp -r ./libs/* ./ignite/*/libs/ || true && " + "./ignite/*/bin/ignite.sh " + "./ignite-config.xml" + " -J-Xmx" + ((int)props.memoryPerNode()) + "m" + " -J-Xms" + ((int)props.memoryPerNode()) + "m" + IgniteYarnUtils.YARN_LOG_OUT ));	nmClient.startContainer(c, ctx);	containers.put(c.getId(), new IgniteContainer( c.getId(), c.getNodeId(), c.getResource().getVirtualCores(), c.getResource().getMemory()));	}	catch (Exception ex) {	
error launching container 

public synchronized void onContainersCompleted(List<ContainerStatus> statuses) {	for (ContainerStatus status : statuses) {	containers.remove(status.getContainerId());	
container completed container id state 

public synchronized void onNodesUpdated(List<NodeReport> updated) {	for (NodeReport node : updated) {	if (node.getNodeState().isUnusable()) {	for (IgniteContainer cont : containers.values()) {	if (cont.nodeId().equals(node.getNodeId())) {	containers.remove(cont.id());	
node is unusable node state 

public synchronized void onNodesUpdated(List<NodeReport> updated) {	for (NodeReport node : updated) {	if (node.getNodeState().isUnusable()) {	for (IgniteContainer cont : containers.values()) {	if (cont.nodeId().equals(node.getNodeId())) {	containers.remove(cont.id());	}	}	
node is unusable node state 

public void run() throws Exception {	rmClient.registerApplicationMaster("", 0, "");	
application master registered 

try {	while (!nmClient.isInState(Service.STATE.STOPPED)) {	int runningCnt = containers.size();	if (runningCnt < props.instances() && checkAvailableResource()) {	Resource capability = Records.newRecord(Resource.class);	capability.setMemory((int)props.totalMemoryPerNode());	capability.setVirtualCores((int)props.cpusPerNode());	for (int i = 0; i < props.instances() - runningCnt; ++i) {	AMRMClient.ContainerRequest containerAsk = new AMRMClient.ContainerRequest(capability, null, null, priority);	rmClient.addContainerRequest(containerAsk);	
making request memory cpu 

for (int i = 0; i < props.instances() - runningCnt; ++i) {	AMRMClient.ContainerRequest containerAsk = new AMRMClient.ContainerRequest(capability, null, null, priority);	rmClient.addContainerRequest(containerAsk);	}	}	TimeUnit.MILLISECONDS.sleep(schedulerTimeout);	}	}	catch (InterruptedException ignored) {	rmClient.unregisterApplicationMaster(FinalApplicationStatus.KILLED, "", "");	
application master killed 

========================= ignite sample_6205 =========================

public void testStartNodes() throws Exception {	for (int i = 0; i < ITERATIONS; i++) {	try {	
iteration 

========================= ignite sample_1190 =========================

public static void main(String[] args) {	try {	CassandraHelper.startEmbeddedCassandra(LOGGER);	}	catch (Throwable e) {	throw new RuntimeException("Failed to start embedded Cassandra instance", e);	}	
testing admin connection to cassandra 

public static void main(String[] args) {	try {	CassandraHelper.startEmbeddedCassandra(LOGGER);	}	catch (Throwable e) {	throw new RuntimeException("Failed to start embedded Cassandra instance", e);	}	CassandraHelper.testAdminConnection();	
testing regular connection to cassandra 

public static void main(String[] args) {	try {	CassandraHelper.startEmbeddedCassandra(LOGGER);	}	catch (Throwable e) {	throw new RuntimeException("Failed to start embedded Cassandra instance", e);	}	CassandraHelper.testAdminConnection();	CassandraHelper.testRegularConnection();	
dropping all artifacts from previous tests execution session 

========================= ignite sample_6985 =========================

codec, sslFilter };	}	else filters = new GridNioFilter[] { codec };	srv = GridNioServer.<GridClientMessage>builder() .address(hostAddr) .port(port) .listener(lsnr) .logger(log) .selectorCount(cfg.getSelectorCount()) .igniteInstanceName(ctx.igniteInstanceName()) .serverName("tcp-rest") .tcpNoDelay(cfg.isNoDelay()) .directBuffer(cfg.isDirectBuffer()) .byteOrder(ByteOrder.nativeOrder()) .socketSendBufferSize(cfg.getSendBufferSize()) .socketReceiveBufferSize(cfg.getReceiveBufferSize()) .sendQueueLimit(cfg.getSendQueueLimit()) .filters(filters) .directMode(false) .build();	srv.idleTimeout(cfg.getIdleTimeout());	srv.start();	ctx.ports().registerPort(port, IgnitePortProtocol.TCP, getClass());	return true;	}	catch (IgniteCheckedException e) {	
failed to start protocol on port 

========================= ignite sample_4957 =========================

public void testZeroOnCreate() throws Exception {	factory = CreatedExpiryPolicy.factoryOf(Duration.ZERO);	startGrids();	for (final Integer key : keys()) {	
test zero duration on create key 

public void testZeroOnUpdate() throws Exception {	factory = new FactoryBuilder.SingletonFactory<>(new TestPolicy(null, 0L, null));	startGrids();	for (final Integer key : keys()) {	
test zero duration on update key 

public void testZeroOnAccess() throws Exception {	factory = new FactoryBuilder.SingletonFactory<>(new TestPolicy(null, null, 0L));	startGrids();	for (final Integer key : keys()) {	
test zero duration on access key 

public void testEternal() throws Exception {	factory = EternalExpiryPolicy.factoryOf();	ExpiryPolicy plc = factory.create();	assertTrue(plc.getExpiryForCreation().isEternal());	assertNull(plc.getExpiryForUpdate());	assertNull(plc.getExpiryForAccess());	startGrids();	for (final Integer key : keys()) {	
test eternalpolicy key 

public void testNullFactory() throws Exception {	factory = null;	startGrids();	for (final Integer key : keys()) {	
test eternalpolicy key 

private void createUpdate(Integer key, @Nullable TransactionConcurrency txConcurrency) throws Exception {	IgniteCache<Integer, Integer> cache = jcache();	for (int i = 0; i < 3; i++) {	
iteration 

private void nearReaderUpdate() throws Exception {	
test near reader update 

========================= ignite sample_1885 =========================

public DataStreamerImpl<K, V> dataStreamer(@Nullable String cacheName) {	if (!busyLock.enterBusy()) throw new IllegalStateException("Failed to create data streamer (grid is stopping).");	try {	final DataStreamerImpl<K, V> ldr = new DataStreamerImpl<>(ctx, cacheName, flushQ);	ldrs.add(ldr);	ldr.internalFuture().listen(new CI1<IgniteInternalFuture<?>>() {	boolean b = ldrs.remove(ldr);	assert b : "Loader has not been added to set: " + ldr;	
loader has been completed 

private void processRequest(final UUID nodeId, final DataStreamerRequest req) {	if (!busyLock.enterBusy()) {	
ignoring data load request node is stopping 

private void processRequest(final UUID nodeId, final DataStreamerRequest req) {	if (!busyLock.enterBusy()) {	return;	}	try {	
processing data load request 

catch (Exception e) {	U.error(log, "Failed to marshal error [err=" + err + ", marshErr=" + e + ']', e);	errBytes = marshErrBytes;	}	DataStreamerResponse res = new DataStreamerResponse(reqId, errBytes, forceLocDep);	try {	ctx.io().sendToCustomTopic(nodeId, resTopic, res, threadIoPolicy());	}	catch (IgniteCheckedException e) {	if (ctx.discovery().alive(nodeId)) U.error(log, "Failed to respond to node [nodeId=" + nodeId + ", res=" + res + ']', e);	
node has left the grid 

========================= ignite sample_4557 =========================

if (missedRsrcs != null && missedRsrcs.contains(path)) throw new ClassNotFoundException("Failed to peer load class [class=" + name + ", nodeClsLdrIds=" + nodeLdrMap + ", parentClsLoader=" + getParent() + ']');	nodeListCp = singleNode ? nodeList : new LinkedList<>(nodeList);	nodeLdrMapCp = singleNode ? nodeLdrMap : new HashMap<>(nodeLdrMap);	}	IgniteCheckedException err = null;	for (UUID nodeId : nodeListCp) {	if (nodeId.equals(ctx.discovery().localNode().id())) continue;	IgniteUuid ldrId = nodeLdrMapCp.get(nodeId);	ClusterNode node = ctx.discovery().node(nodeId);	if (node == null) {	
found inactive node in class loader will skip 

}	if (res.success()) return res.byteSource();	if (log.isDebugEnabled()) log.debug("Failed to find class on remote node [class=" + name + ", nodeId=" + node.id() + ", clsLdrId=" + ldrId + ", reason=" + res.errorMessage() + ']');	synchronized (mux) {	if (missedRsrcs != null) missedRsrcs.add(path);	}	throw new ClassNotFoundException("Failed to peer load class [class=" + name + ", nodeClsLdrs=" + nodeLdrMapCp + ", parentClsLoader=" + getParent() + ", reason=" + res.errorMessage() + ']');	}	catch (IgniteCheckedException e) {	if (Thread.currentThread().isInterrupted()) {	
failed to find class probably due to task job cancellation 

========================= ignite sample_3553 =========================

log = getIgnite().log();	if (persistence == null) client = new MqttClient(brokerUrl, clientId);	else client = new MqttClient(brokerUrl, clientId, persistence);	client.setCallback(this);	stopped = false;	Retryer<Void> retrier = RetryerBuilder.<Void>newBuilder() .retryIfResult(new Predicate<Void>() {	return !client.isConnected() && !stopped;	}	}) .retryIfException().retryIfRuntimeException() .withWaitStrategy(retryWaitStrategy) .withStopStrategy(retryStopStrategy) .build();	connectionRetrier = new MqttConnectionRetrier(retrier);	
starting mqtt streamer 

if (client.isConnected()) return null;	if (stopped) return null;	if (connectOptions == null) client.connect();	else client.connect(connectOptions);	if (qualitiesOfService.isEmpty()) client.subscribe(topics.toArray(new String[0]));	else {	int[] qoses = new int[qualitiesOfService.size()];	for (int i = 0; i < qualitiesOfService.size(); i++) qoses[i] = qualitiesOfService.get(i);	client.subscribe(topics.toArray(new String[0]), qoses);	}	
mqtt streamer re connected and subscribed 

========================= ignite sample_7843 =========================

for (int k = 0; k < 10; k++) checkVersionIncrease(caches.get(i), cachesVers.get(i));	}	int nodeIdx = 1;	for (int n = 0; n < 10; n++) {	startGrid(nodeIdx++);	for (int i = 0; i < caches.size(); i++) checkVersionIncrease(caches.get(i), cachesVers.get(i));	awaitPartitionMapExchange();	for (int i = 0; i < caches.size(); i++) checkVersionIncrease(caches.get(i), cachesVers.get(i));	}	for (int n = 1; n < nodeIdx; n++) {	
stop node 

========================= ignite sample_1978 =========================

private void cancelFuture(IgniteInternalFuture<?> fut) {	try {	fut.cancel();	}	catch (IgniteCheckedException e) {	
failed to cancel task 

========================= ignite sample_5043 =========================

startGrids(2);	final AtomicBoolean stop = new AtomicBoolean();	final ConcurrentHashSet<String> allTypes = new ConcurrentHashSet<>();	IgniteInternalFuture<?> fut;	try {	fut = GridTestUtils.runMultiThreadedAsync(new Callable<Object>() {	IgniteBinary binaries = ignite(0).binary();	IgniteCache<Object, Object> cache = ignite(0).cache(DEFAULT_CACHE_NAME).withKeepBinary();	ThreadLocalRandom rnd = ThreadLocalRandom.current();	for (int i = 0; i < 1000; i++) {	
iteration 

builder.setField("f0", i);	cache.put(i, builder.build());	}	client = true;	final CyclicBarrier barrier = new CyclicBarrier(6);	final AtomicInteger startIdx = new AtomicInteger(4);	IgniteInternalFuture<?> fut = GridTestUtils.runMultiThreadedAsync(new Callable<Object>() {	barrier.await();	Ignite ignite = startGrid(startIdx.getAndIncrement());	assertTrue(ignite.configuration().isClientMode());	
started node 

========================= ignite sample_1846 =========================

IgniteInternalFuture<Boolean> fut = adapter.lockAsync(key, 0);	try {	fut.get(30_000);	U.sleep(1);	}	catch (IgniteFutureTimeoutException e) {	info("Entry: " + adapter.peekEx(key));	fail("Lock timeout [fut=" + fut + ", err=" + e + ']');	}	catch (Exception e) {	
error 

========================= ignite sample_1145 =========================

if (grid(i).affinity(DEFAULT_CACHE_NAME).isPrimaryOrBackup(grid(i).localNode(), key)) {	c0 = ((IgniteKernal)grid(i)).internalCache(DEFAULT_CACHE_NAME);	if (c0.isNear()) c0 = c0.context().near().dht();	GridCacheEntryEx curEntry = c0.entryEx(key);	curEntry.unswap();	assertTrue(curEntry.expireTime() >= startTime);	expireTimes[i] = curEntry.expireTime();	}	}	U.sleep(100);	
put 

}	}	U.sleep(100);	tx = inTx ? grid(0).transactions().txStart() : null;	try {	c.put(key, 4);	}	finally {	if (tx != null) tx.commit();	}	
put done 

========================= ignite sample_1796 =========================

protected final void postLockWrite( GridCacheContext cacheCtx, Iterable<KeyCacheObject> keys, GridCacheReturn ret, boolean rmv, boolean retval, boolean read, long accessTtl, CacheEntryPredicate[] filter, boolean computeInvoke ) throws IgniteCheckedException {	for (KeyCacheObject k : keys) {	IgniteTxEntry txEntry = entry(cacheCtx.txKey(k));	if (txEntry == null) throw new IgniteCheckedException("Transaction entry is null (most likely collection of keys passed into cache " + "operation was changed before operation completed) [missingKey=" + k + ", tx=" + this + ']');	while (true) {	GridCacheEntryEx cached = txEntry.cached();	try {	assert cached.detached() || cached.lockedByThread(threadId) || isRollbackOnly() : "Transaction lock is not acquired [entry=" + cached + ", tx=" + this + ", nodeId=" + cctx.localNodeId() + ", threadId=" + threadId + ']';	
post lock write entry 

}	}	else ret.value(cacheCtx, v, txEntry.keepBinary());	}	boolean pass = F.isEmpty(filter) || cacheCtx.isAll(cached, filter);	ret.success(pass && (!retval ? !rmv || cached.hasValue() || v != null : !rmv || v != null));	if (onePhaseCommit()) txEntry.filtersPassed(pass);	boolean updateTtl = read;	if (pass) {	txEntry.markValid();	
filter passed in post lock for key 

if (updateTtl) {	if (!read) {	ExpiryPolicy expiryPlc = cacheCtx.expiryForTxEntry(txEntry);	if (expiryPlc != null) txEntry.ttl(CU.toTtl(expiryPlc.getExpiryForAccess()));	}	else txEntry.ttl(accessTtl);	}	break;	}	catch (GridCacheEntryRemovedException ignore) {	
got removed entry in putallasync method will retry 

old.cached(entry);	old.filters(filter);	old.skipStore(skipStore);	old.keepBinary(keepBinary);	if (drTtl >= 0L) {	assert drExpireTime >= 0L;	entryTtlDr(key, drTtl, drExpireTime);	}	else entryExpiry(key, expiryPlc);	txEntry = old;	
updated transaction entry 

}	else entryExpiry(key, expiryPlc);	txEntry = old;	}	else {	boolean hasDrTtl = drTtl >= 0;	txEntry = new IgniteTxEntry(entry.context(), this, op, val, EntryProcessorResourceInjectorProxy.wrap(cctx.kernalContext(), entryProcessor), invokeArgs, hasDrTtl ? drTtl : -1L, entry, filter, drVer, skipStore, keepBinary, addReader);	txEntry.conflictExpireTime(drExpireTime);	if (!hasDrTtl) txEntry.expiry(expiryPlc);	txState.addEntry(txEntry);	
created transaction entry 

if (!hasDrTtl) txEntry.expiry(expiryPlc);	txState.addEntry(txEntry);	}	txEntry.filtersSet(filtersSet);	while (true) {	try {	updateExplicitVersion(txEntry, entry);	return txEntry;	}	catch (GridCacheEntryRemovedException ignore) {	
got removed entry in transaction newentry method will retry 

========================= ignite sample_3969 =========================

final IgniteCache<Integer, Company> co = grid.cache("co");	try {	runQuery(grid, new Runnable() {	if (rnd.nextBoolean()) co.get(rnd.nextInt(COMPANY_CNT));	assertEquals(pRes, cache.query(qry).getAll());	}	});	} catch (CacheException e) {	if (e.getCause() instanceof IgniteInterruptedCheckedException || e.getCause() instanceof InterruptedException || e.getCause() instanceof ClusterTopologyException || e.getCause() instanceof TransactionTimeoutException || e.getCause() instanceof TransactionException) continue;	if (e.getCause() instanceof QueryCancelledException) fail("Retry is expected");	
on large page size must retry 

break;	}	if (!(th instanceof CacheException)) continue;	if (th.getMessage() != null && th.getMessage().startsWith("Failed to fetch data from node:")) {	failedOnRemoteFetch = true;	break;	}	}	if (failedOnInterruption) continue;	if (!failedOnRemoteFetch) {	
must fail inside of gridresultpage fetchnextpage or subclass 

========================= ignite sample_7479 =========================

public void testConcurrentJoin() throws Exception {	for (int iter = 0; iter < ITERATIONS; iter++) {	
iteration 

========================= ignite sample_999 =========================

public void run() {	processMessage((T)message);	if (batched) {	if (batchClosureSize <= 0) return;	else if (counter.incrementAndGet() >= batchClosureSize) {	try {	session.commit();	counter.set(0);	}	catch (Exception e) {	
could not commit jms session upon completion of batch 

}	catch (Exception e) {	}	}	}	else if (transacted) {	try {	session.commit();	}	catch (JMSException e) {	
could not commit jms session non batched 

========================= ignite sample_6268 =========================

public void testMultinodeCacheStart() throws Exception {	for (int i = 0; i < 10; i++) {	
iteration 

private void oldestNotAffinityNode1(final CacheConfiguration ccfg) throws Exception {	
test with cache 

private void oldestNotAffinityNode2(final CacheConfiguration ccfg) throws Exception {	
test with cache 

private void notAffinityNode1(final CacheConfiguration ccfg) throws Exception {	
test with cache 

private void notAffinityNode2(final CacheConfiguration ccfg) throws Exception {	
test with cache 

========================= ignite sample_825 =========================

int node = nodeIdx.getAndIncrement();	Ignite ignite = startGrid(node);	IgniteCache<Integer, Integer> cache = ignite.cache(DEFAULT_CACHE_NAME);	for (int i = 0; i < 1000; i++) incrementTx(ignite, cache, incMap);	return null;	}	}, START_NODES, "start-thread");	IgniteInternalFuture<?> txFut = updateFuture(NODES, incMap, fut);	fut.get();	txFut.get();	
first updates 

cache.put(key, val);	keys.add(key);	}	if (singleKey) break;	}	tx.commit();	for (Integer key : keys) incMap.get(key).incrementAndGet();	}	}	catch (Exception e) {	
tx failed 

========================= ignite sample_1554 =========================

private boolean sendQueryResponse(UUID nodeId, GridCacheQueryResponse res, long timeout) {	ClusterNode node = cctx.node(nodeId);	if (node == null) return false;	int attempt = 1;	IgniteCheckedException err = null;	while (!Thread.currentThread().isInterrupted()) {	try {	
send query response 

private void processQueryResponse(UUID sndId, GridCacheQueryResponse res) {	
received query response 

========================= ignite sample_4185 =========================

for (int i = 0; i < retries; i++) {	try {	cached = cache.get(sesId);	break;	}	catch (CacheException | IgniteException | IllegalStateException e) {	handleLoadSessionException(sesId, i, e);	}	}	if (cached != null) {	
using cached session for id 

break;	}	catch (CacheException | IgniteException | IllegalStateException e) {	handleLoadSessionException(sesId, i, e);	}	}	if (cached != null) {	if (cached.isNew()) cached = new WebSession(cached.getId(), cached, false);	}	else {	
cached session was invalidated and doesn t exist 

try {	final WebSessionEntity entity = binaryCache.get(sesId);	if (entity != null) cached = new WebSessionV2(sesId, httpReq.getSession(false), false, ctx, entity, marshaller);	break;	}	catch (CacheException | IgniteException | IllegalStateException e) {	handleLoadSessionException(sesId, i, e);	}	}	if (cached != null) {	
using cached session for id 

if (entity != null) cached = new WebSessionV2(sesId, httpReq.getSession(false), false, ctx, entity, marshaller);	break;	}	catch (CacheException | IgniteException | IllegalStateException e) {	handleLoadSessionException(sesId, i, e);	}	}	if (cached != null) {	}	else {	
cached session was invalidated and doesn t exist 

private void handleLoadSessionException(final String sesId, final int tryCnt, final RuntimeException e) {	if (log.isDebugEnabled()) log.debug(e.getMessage());	if (tryCnt == retries - 1) throw new IgniteException("Failed to handle request [session= " + sesId + "]", e);	else {	
failed to handle request will retry 

private WebSession createSession(HttpSession ses, String sesId) {	WebSession cached = new WebSession(sesId, ses, true);	cached.genSes(ses);	
session created 

private void handleCreateSessionException(final String sesId, final int tryCnt, final RuntimeException e) {	if (log.isDebugEnabled()) log.debug(e.getMessage());	if (tryCnt == retries - 1) throw new IgniteException("Failed to save session: " + sesId, e);	else {	
failed to save session will retry 

private WebSessionV2 createSessionV2(HttpServletRequest httpReq) throws IOException {	final HttpSession ses = httpReq.getSession(true);	final String sesId = transformSessionId(ses.getId());	
session created 

public void destroySession(String sesId) {	assert sesId != null;	for (int i = 0; i < retries; i++) {	try {	
session destroyed 

public void destroySession(String sesId) {	assert sesId != null;	for (int i = 0; i < retries; i++) {	try {	}	catch (CacheException | IgniteException | IllegalStateException e) {	if (i == retries - 1) {	U.warn(log, "Failed to remove session [sesId=" + sesId + ", retries=" + retries + ']');	}	else {	
failed to remove session will retry 

private void handleAttributeUpdateException(final String sesId, final int tryCnt, final RuntimeException e) {	if (tryCnt == retries - 1) {	U.error(log, "Failed to apply updates for session (maximum number of retries exceeded) [sesId=" + sesId + ", retries=" + retries + ']', e);	}	else {	
failed to apply updates for session will retry 

========================= ignite sample_6157 =========================

for (int f0 = 0; f0 < files; f0++) {	final IgfsOutputStream os = outStreams[f0];	assert os != null;	final int f = f0;	int att = doWithRetries(1, new Callable<Void>() {	IgfsOutputStream ios = os;	try {	writeChunks0(igfs0, ios, f);	}	catch (IOException ioe) {	
attempt to append the data to existing stream failed 

========================= ignite sample_2114 =========================

res = new IgniteBiTuple<>(true, computedMap);	}	else computedMap = (Map<K, EntryProcessorResult>)res.get2();	computedMap.put(key, t.get3());	}	}	else if (res == null) res = new T2(t.get1(), t.get2());	break;	}	catch (GridCacheEntryRemovedException ignored) {	
got removed entry while updating will retry 

}	else if (res == null) res = new T2(t.get1(), t.get2());	break;	}	catch (GridCacheEntryRemovedException ignored) {	entry = null;	}	catch (IgniteCheckedException e) {	if (err == null) err = partialUpdateException();	err.add(F.asList(key), e);	
failed to update key 

========================= ignite sample_3928 =========================

public void testSimpleRebalancing() throws Exception {	IgniteKernal ignite = (IgniteKernal)startGrid(0);	generateData(ignite, 0, 0);	
preloading started 

awaitPartitionMapExchange(true, true, null, true);	checkPartitionMapExchangeFinished();	checkPartitionMapMessagesAbsent();	stopGrid(2);	waitForRebalancing(1, 5);	awaitPartitionMapExchange(true, true, null, true);	checkPartitionMapExchangeFinished();	checkPartitionMapMessagesAbsent();	long spend = (System.currentTimeMillis() - start) / 1000;	checkData(grid(1), 0, 0);	
spend seconds to rebalance entries 

public void testLoadRebalancing() throws Exception {	final Ignite ignite = startGrid(0);	startGrid(1);	generateData(ignite, CACHE_NAME_DHT_PARTITIONED, 0, 0);	
preloading started 

GridDhtPartitionMap pMap = remoteTop.partitionMap(true).get(((IgniteKernal)g).getLocalNodeId());	assertEquals(pMap.size(), locs.size());	for (Map.Entry entry : pMap.entrySet()) {	assertTrue("Wrong remote partition state part=" + entry.getKey() + ", should be OWNING [state=" + entry.getValue() + "], node=" + remote.name() + " cache=" + c.getName(), entry.getValue() == GridDhtPartitionState.OWNING);	}	for (GridDhtLocalPartition loc : locs) assertTrue(pMap.containsKey(loc.id()));	}	}	}	}	
checkpartitionmapexchangefinished finished 

protected void checkPartitionMapMessagesAbsent() throws Exception {	map.clear();	record = true;	
checking griddhtpartitions message absent it will take seconds 

public void testComplexRebalancing() throws Exception {	final Ignite ignite = startGrid(0);	generateData(ignite, 0, 0);	
preloading started 

========================= ignite sample_1016 =========================

private void classCacheUpdateFailover(boolean stopSrv) throws Exception {	cache = true;	startGridsMultiThreaded(2);	cache = stopSrv;	IgniteCache<Integer, Object> cache0 = ignite(0).cache(DEFAULT_CACHE_NAME);	for (int i = 0; i < 20; i++) {	
iteration 

========================= ignite sample_2285 =========================

}	}	if (res.size() < primaryAndBackups && nodes.size() >= primaryAndBackups && exclNeighbors) {	it = sortedNodes.iterator();	it.next();	while (it.hasNext() && res.size() < primaryAndBackups) {	ClusterNode node = it.next();	if (!res.contains(node)) res.add(node);	}	if (!exclNeighborsWarn) {	
affinity function excludeneighbors property is ignored because topology has no enough nodes to assign backups affinity function excludeneighbors property is ignored because topology has no enough nodes to assign backups 

========================= ignite sample_2905 =========================

private void spiStop0(boolean disconnect) throws IgniteSpiException {	if (log.isDebugEnabled()) {	
disconnecting spi 

private void spiStop0(boolean disconnect) throws IgniteSpiException {	if (log.isDebugEnabled()) {	
preparing to start local node stop procedure 

try {	mux.wait(timeout);	timeout = threshold - U.currentTimeMillis();	}	catch (InterruptedException ignored) {	Thread.currentThread().interrupt();	break;	}	}	if (spiState == LEFT) {	
verification for local node leave has been received from coordinator continuing stop procedure 

timeout = threshold - U.currentTimeMillis();	}	catch (InterruptedException ignored) {	Thread.currentThread().interrupt();	break;	}	}	if (spiState == LEFT) {	}	else if (log.isInfoEnabled()) {	
no verification for local node leave has been received from coordinator will stop node anyway 

try {	if (addr.isUnresolved()) addr = new InetSocketAddress(InetAddress.getByName(addr.getHostName()), addr.getPort());	long tstamp = U.currentTimeMillis();	sock = spi.createSocket();	fut.sock = sock;	sock = spi.openSocket(sock, addr, timeoutHelper);	openedSock = true;	spi.writeToSocket(sock, new TcpDiscoveryPingRequest(locNodeId, clientNodeId), timeoutHelper.nextTimeoutChunk(spi.getSocketTimeout()));	TcpDiscoveryPingResponse res = spi.readMessage(sock, null, timeoutHelper.nextTimeoutChunk( spi.getAckTimeout()));	if (locNodeId.equals(res.creatorNodeId())) {	
ping response from local node 

noResStart = 0;	switch (res) {	case RES_WAIT: retry = true;	break;	case RES_OK: if (log.isDebugEnabled()) log.debug("Join request message has been sent to address [addr=" + addr + ", req=" + joinReq + ']');	return true;	default: if (res == RES_CONTINUE_JOIN) {	if (!fromAddrs.contains(addr)) retry = true;	}	else {	
unexpected response to join request 

errs.add(e);	if (log.isDebugEnabled()) {	IOException ioe = X.cause(e, IOException.class);	log.debug("Failed to send join request message [addr=" + addr + ", msg=" + (ioe != null ? ioe.getMessage() : e.getMessage()) + ']');	onException("Failed to send join request message [addr=" + addr + ", msg=" + (ioe != null ? ioe.getMessage() : e.getMessage()) + ']', ioe);	}	noResAddrs.add(addr);	}	}	if (retry) {	
concurrent discovery spi start has been detected local node should wait 

throw new IgniteSpiException("Thread has been interrupted.", e);	}	}	else if (!spi.ipFinder.isShared() && !ipFinderHasLocAddr) {	IgniteCheckedException e = null;	if (!errs.isEmpty()) {	e = new IgniteCheckedException("Multiple connection attempts failed.");	for (Exception err : errs) e.addSuppressed(err);	}	if (e != null && X.hasCause(e, ConnectException.class)) {	
failed to connect to any address from ip finder make sure ip finder addresses are correct and firewalls are disabled on all host machines 

private void initConnectionCheckFrequency() {	if (spi.failureDetectionTimeoutEnabled()) connCheckThreshold = spi.failureDetectionTimeout();	else connCheckThreshold = Math.min(spi.getSocketTimeout(), spi.metricsUpdateFreq);	for (int i = 3; i > 0; i--) {	connCheckFreq = connCheckThreshold / i;	if (connCheckFreq > 10) break;	}	assert connCheckFreq > 0;	
connection check frequency is calculated 

private void sendMessageToClients(TcpDiscoveryAbstractMessage msg) {	if (redirectToClients(msg)) {	if (spi.ensured(msg)) msgHist.add(msg);	byte[] msgBytes = null;	for (ClientMessageWorker clientMsgWorker : clientMsgWorkers.values()) {	if (msgBytes == null) {	try {	msgBytes = U.marshal(spi.marshaller(), msg);	}	catch (IgniteCheckedException e) {	
failed to marshal message 

if (msg instanceof TcpDiscoveryNodeAddedMessage) {	TcpDiscoveryNodeAddedMessage nodeAddedMsg = (TcpDiscoveryNodeAddedMessage)msg;	TcpDiscoveryNode node = nodeAddedMsg.node();	if (clientMsgWorker.clientNodeId.equals(node.id())) {	try {	msg0 = U.unmarshal(spi.marshaller(), msgBytes, U.resolveClassLoader(spi.ignite().configuration()));	prepareNodeAddedMessage(msg0, clientMsgWorker.clientNodeId, null, null, null);	msgBytes0 = null;	}	catch (IgniteCheckedException e) {	
failed to create message copy 

failedNodes = U.arrayList(ServerImpl.this.failedNodes.keySet());	state = spiState;	}	Collection<Throwable> errs = null;	boolean sent = false;	boolean newNextNode = false;	UUID locNodeId = getLocalNodeId();	while (true) {	TcpDiscoveryNode newNext = ring.nextNode(failedNodes);	if (newNext == null) {	
no next node in topology 

sock = null;	next = newNext;	newNextNode = true;	}	else if (log.isTraceEnabled()) log.trace("Next node remains the same [nextId=" + next.id() + ", nextOrder=" + next.internalOrder() + ']');	final boolean sameHost = U.sameMacs(locNode, next);	List<InetSocketAddress> locNodeAddrs = U.arrayList(locNode.socketAddresses());	addr: for (InetSocketAddress addr : spi.getNodeAddresses(next, sameHost)) {	long ackTimeout0 = spi.getAckTimeout();	if (locNodeAddrs.contains(addr)){	
skip to send message to the local node probably remote node has the same loopback address that local node 

boolean success = false;	boolean openSock = false;	try {	long tstamp = U.currentTimeMillis();	sock = spi.openSocket(addr, timeoutHelper);	out = spi.socketStream(sock);	openSock = true;	spi.writeToSocket(sock, out, new TcpDiscoveryHandshakeRequest(locNodeId), timeoutHelper.nextTimeoutChunk(spi.getSocketTimeout()));	TcpDiscoveryHandshakeResponse res = spi.readMessage(sock, null, timeoutHelper.nextTimeoutChunk(ackTimeout0));	if (locNodeId.equals(res.creatorNodeId())) {	
handshake response from local node 

else {	if (nextOrder != next.internalOrder()) {	boolean nextNew = (msg instanceof TcpDiscoveryNodeAddedMessage && ((TcpDiscoveryNodeAddedMessage)msg).node().id().equals(nextId));	if (!nextNew) nextNew = hasPendingAddMessage(nextId);	if (!nextNew) {	if (log.isDebugEnabled()) log.debug("Failed to restore ring because next node order received " + "is not as expected [expected=" + next.internalOrder() + ", rcvd=" + nextOrder + ", id=" + next.id() + ']');	if (debugMode) debugLog(msg, "Failed to restore ring because next node order " + "received is not as expected [expected=" + next.internalOrder() + ", rcvd=" + nextOrder + ", id=" + next.id() + ']');	break;	}	}	
initialized connection with next node 

if (!spi.failureDetectionTimeoutEnabled() && ++reconCnt == spi.getReconnectCount()) break;	if (timeoutHelper.checkFailureTimeoutReached(e)) break;	else if (!spi.failureDetectionTimeoutEnabled() && (e instanceof SocketTimeoutException || X.hasCause(e, SocketTimeoutException.class))) {	ackTimeout0 *= 2;	if (!checkAckTimeout(ackTimeout0)) break;	}	continue;	}	finally {	if (!success) {	
closing socket to next 

SecurityUtils.serializeVersion(1);	long tstamp = U.currentTimeMillis();	if (timeoutHelper == null) timeoutHelper = new IgniteSpiOperationTimeoutHelper(spi, true);	if (!failedNodes.isEmpty()) {	for (TcpDiscoveryNode failedNode : failedNodes) {	assert !failedNode.equals(next) : failedNode;	msg.addFailedNode(failedNode.id());	}	}	boolean latencyCheck = msg instanceof TcpDiscoveryRingLatencyCheckMessage;	
latency check message has been written to socket 

if (!failedNodes.isEmpty()) {	for (TcpDiscoveryNode failedNode : failedNodes) {	assert !failedNode.equals(next) : failedNode;	msg.addFailedNode(failedNode.id());	}	}	boolean latencyCheck = msg instanceof TcpDiscoveryRingLatencyCheckMessage;	spi.writeToSocket(newNextNode ? newNext : next, sock, out, msg, timeoutHelper.nextTimeoutChunk(spi.getSocketTimeout()));	long tstamp0 = U.currentTimeMillis();	int res = spi.readReceipt(sock, timeoutHelper.nextTimeoutChunk(ackTimeout0));	
latency check message has been acked 

if (++reconCnt == spi.getReconnectCount()) break;	else if (e instanceof SocketTimeoutException || X.hasCause(e, SocketTimeoutException.class)) {	ackTimeout0 *= 2;	if (!checkAckTimeout(ackTimeout0)) break;	}	}	}	finally {	forceSndPending = false;	if (!sent) {	
closing socket to next not sent 

next = null;	errs = null;	}	else break;	}	synchronized (mux) {	failedNodes.removeAll(ServerImpl.this.failedNodes.keySet());	}	if (!failedNodes.isEmpty()) {	if (state == CONNECTED) {	
message has not been sent 

next = null;	errs = null;	}	else break;	}	synchronized (mux) {	failedNodes.removeAll(ServerImpl.this.failedNodes.keySet());	}	if (!failedNodes.isEmpty()) {	if (state == CONNECTED) {	
detected failed nodes 

}	synchronized (mux) {	for (TcpDiscoveryNode failedNode : failedNodes) {	if (!ServerImpl.this.failedNodes.containsKey(failedNode)) ServerImpl.this.failedNodes.put(failedNode, locNodeId);	}	for (TcpDiscoveryNode failedNode : failedNodes) failedNodesMsgSent.add(failedNode.id());	}	for (TcpDiscoveryNode n : failedNodes) msgWorker.addMessage(new TcpDiscoveryNodeFailedMessage(locNodeId, n.id(), n.internalOrder()));	if (!sent) {	assert next == null : next;	
pending messages will be resent to local node 

for (TcpDiscoveryAbstractMessage pendingMsg : pendingMsgs) {	prepareNodeAddedMessage(pendingMsg, locNodeId, pendingMsgs.msgs, pendingMsgs.discardId, pendingMsgs.customDiscardId);	pendingMsg.senderNodeId(locNodeId);	msgWorker.addMessage(pendingMsg);	if (log.isDebugEnabled()) log.debug("Pending message has been sent to local node [msg=" + msg.id() + ", pendingMsgId=" + pendingMsg + ']');	if (debugMode) {	debugLog(msg, "Pending message has been sent to local node [msg=" + msg.id() + ", pendingMsgId=" + pendingMsg + ']');	}	}	}	
local node has detected failed nodes and started cluster wide procedure to speed up failure detection please see failure detection section under javadoc for tcpdiscoveryspi 

private void registerPendingMessage(TcpDiscoveryAbstractMessage msg) {	assert msg != null;	if (spi.ensured(msg)) {	pendingMsgs.add(msg);	spi.stats.onPendingMessageRegistered();	
pending message has been registered 

onException("Failed to send loopback problem message to node " + "[node=" + node + ", err=" + e.getMessage() + ']', e);	}	return;	}	}	if (isLocalNodeCoordinator()) {	TcpDiscoveryNode existingNode = ring.node(node.id());	if (existingNode != null) {	if (!node.socketAddresses().equals(existingNode.socketAddresses())) {	if (!pingNode(existingNode)) {	
sending node failed message for existing node 

if (log.isDebugEnabled()) log.debug("Send reconnect message to already joined client " + "[clientNode=" + existingNode + ", msg=" + reconMsg + ']');	if (getLocalNodeId().equals(node.clientRouterNodeId())) {	ClientMessageWorker wrk = clientMsgWorkers.get(node.id());	if (wrk != null) wrk.addMessage(reconMsg);	else if (log.isDebugEnabled()) log.debug("Failed to find client message worker " + "[clientNode=" + existingNode + ", msg=" + reconMsg + ']');	}	else {	if (sendMessageToRemotes(reconMsg)) sendMessageAcrossRing(reconMsg);	}	}	
ignoring join request message since node is already in topology 

SecurityUtils.restoreDefaultSerializeVersion();	}	if (!permissionsEqual(coordSubj.subject().permissions(), subj.subject().permissions())) {	LT.warn(log, "Authentication failed [nodeId=" + node.id() + ", addrs=" + U.addressesAsString(node) + ']', "Authentication failed [nodeId=" + U.id8(node.id()) + ", addrs=" + U.addressesAsString(node) + ']');	if (log.isDebugEnabled()) log.debug("Authentication failed [nodeId=" + node.id() + ", addrs=" + U.addressesAsString(node));	}	else authFailed = false;	}	}	catch (IgniteException | IgniteCheckedException e) {	
failed to verify node permissions consistency will drop the node 

SecurityContext locCrd = unmarshalWithSecurityVersion(locSubj, ver);	if (!permissionsEqual(locCrd.subject().permissions(), rmCrd.subject().permissions())) {	LT.warn(log, "Failed to authenticate local node " + "(local authentication result is different from rest of topology) " + "[nodeId=" + node.id() + ", addrs=" + U.addressesAsString(node) + ']', "Authentication failed [nodeId=" + U.id8(node.id()) + ", addrs=" + U.addressesAsString(node) + ']');	joinRes.set(authFail);	spiState = AUTH_FAILED;	mux.notifyAll();	return;	}	}	catch (IgniteCheckedException e) {	
failed to verify node permissions consistency will drop the node 

for (TcpDiscoveryNode n : top) {	assert n.internalOrder() < node.internalOrder() : "Invalid node [topNode=" + n + ", added=" + node + ']';	n.visible(true);	}	synchronized (mux) {	joiningNodes.clear();	}	locNode.setAttributes(node.attributes());	locNode.visible(true);	ring.restoreTopology(top, node.internalOrder());	
restored topology from node added message 

dataPacket = msg.gridDiscoveryData();	topHist.clear();	topHist.putAll(msg.topologyHistory());	pendingMsgs.reset(msg.messages(), msg.discardedMessageId(), msg.discardedCustomMessageId());	msg.messages(null, null, null);	msg.topology(null);	msg.topologyHistory(null);	msg.clearDiscoveryData();	}	else {	
discarding node added message with empty topology 

private void processNodeAddFinishedMessage(TcpDiscoveryNodeAddFinishedMessage msg) {	assert msg != null;	UUID nodeId = msg.nodeId();	assert nodeId != null;	TcpDiscoveryNode node = ring.node(nodeId);	if (node == null) {	if (log.isDebugEnabled()) log.debug("Discarding node add finished message since node is not found " + "[msg=" + msg + ']');	return;	}	
node to finish add 

private void processRingLatencyCheckMessage(TcpDiscoveryRingLatencyCheckMessage msg) {	assert msg != null;	if (msg.maxHopsReached()) {	if (log.isInfoEnabled()) log.info("Latency check has been discarded (max hops reached) [id=" + msg.id() + ", maxHops=" + msg.maxHops() + ']');	return;	}	
latency check processing 

private void processRingLatencyCheckMessage(TcpDiscoveryRingLatencyCheckMessage msg) {	assert msg != null;	if (msg.maxHopsReached()) {	if (log.isInfoEnabled()) log.info("Latency check has been discarded (max hops reached) [id=" + msg.id() + ", maxHops=" + msg.maxHops() + ']');	return;	}	if (sendMessageToRemotes(msg)) sendMessageAcrossRing(msg);	else {	
latency check has been discarded no remote nodes 

private void processNodeLeftMessage(TcpDiscoveryNodeLeftMessage msg) {	assert msg != null;	UUID locNodeId = getLocalNodeId();	UUID leavingNodeId = msg.creatorNodeId();	if (locNodeId.equals(leavingNodeId)) {	if (msg.senderNodeId() == null) {	synchronized (mux) {	
starting local node stop procedure 

spiState = STOPPING;	mux.notifyAll();	}	}	if (msg.verified() || !ring.hasRemoteNodes() || msg.senderNodeId() != null) {	if (spi.ipFinder.isShared() && !ring.hasRemoteNodes()) {	try {	spi.ipFinder.unregisterAddresses( U.resolveAddresses(spi.getAddressResolver(), locNode.socketAddresses()));	}	catch (IgniteSpiException e) {	
failed to unregister local node address from ip finder 

spiState = LEFT;	mux.notifyAll();	}	}	return;	}	sendMessageAcrossRing(msg);	return;	}	if (ring.node(msg.senderNodeId()) == null) {	
discarding node left message since sender node is not in topology 

if (ring.node(msg.senderNodeId()) == null) {	return;	}	TcpDiscoveryNode leavingNode = ring.node(leavingNodeId);	if (leavingNode != null) {	synchronized (mux) {	leavingNodes.add(leavingNode);	}	}	else {	
discarding node left message since node was not found 

spi.stats.onRingMessageReceived(msg);	addMessage(new TcpDiscoveryDiscardMessage(locNodeId, msg.id(), false));	return;	}	msg.verify(locNodeId);	}	if (msg.verified() && !locNodeId.equals(leavingNodeId)) {	TcpDiscoveryNode leftNode = ring.removeNode(leavingNodeId);	interruptPing(leavingNode);	assert leftNode != null : msg;	
removed node from topology 

if (log.isDebugEnabled()) log.debug("Topology version has been updated: [ring=" + ring + ", msg=" + msg + ']');	lastMsg = msg;	}	if (msg.client()) {	ClientMessageWorker wrk = clientMsgWorkers.remove(leavingNodeId);	if (wrk != null) wrk.addMessage(msg);	}	else if (leftNode.equals(next) && sock != null) {	try {	spi.writeToSocket(sock, out, msg, spi.failureDetectionTimeoutEnabled() ? spi.failureDetectionTimeout() : spi.getSocketTimeout());	
sent verified node left message to leaving node 

if (sendMessageToRemotes(msg)) {	try {	sendMessageAcrossRing(msg);	}	finally {	forceSndPending = false;	}	}	else {	forceSndPending = false;	
unable to send message across the ring topology has no remote nodes 

private void processNodeFailedMessage(TcpDiscoveryNodeFailedMessage msg) {	assert msg != null;	UUID sndId = msg.senderNodeId();	if (sndId != null) {	TcpDiscoveryNode sndNode = ring.node(sndId);	if (sndNode == null) {	
discarding node failed message sent from unknown node 

return;	}	else {	boolean contains;	UUID creatorId = msg.creatorNodeId();	assert creatorId != null : msg;	synchronized (mux) {	contains = failedNodes.containsKey(sndNode) || ring.node(creatorId) == null;	}	if (contains) {	
discarding node failed message sent from node which is about to fail 

if (failedNode != null) {	assert !failedNode.isLocal() || !msg.verified() : msg;	boolean skipUpdateFailedNodes = msg.force() && !msg.verified();	if (!skipUpdateFailedNodes) {	synchronized (mux) {	if (!failedNodes.containsKey(failedNode)) failedNodes.put(failedNode, msg.senderNodeId() != null ? msg.senderNodeId() : getLocalNodeId());	}	}	}	else {	
discarding node failed message since node was not found 

U.warn(log, "Received EVT_NODE_FAILED event with warning [" + "nodeInitiatedEvt=" + (creatorNode != null ? creatorNode : msg.creatorNodeId()) + ", msg=" + msg.warning() + ']');	}	synchronized (mux) {	joiningNodes.remove(failedNode.id());	}	notifyDiscovery(EVT_NODE_FAILED, topVer, failedNode);	spi.stats.onNodeFailed();	}	if (sendMessageToRemotes(msg)) sendMessageAcrossRing(msg);	else {	
unable to send message across the ring topology has no remote nodes 

private void processStatusCheckMessage(final TcpDiscoveryStatusCheckMessage msg) {	assert msg != null;	UUID locNodeId = getLocalNodeId();	if (msg.failedNodeId() != null) {	if (locNodeId.equals(msg.failedNodeId())) {	
status check message discarded suspect node is local node 

private void processStatusCheckMessage(final TcpDiscoveryStatusCheckMessage msg) {	assert msg != null;	UUID locNodeId = getLocalNodeId();	if (msg.failedNodeId() != null) {	if (locNodeId.equals(msg.failedNodeId())) {	return;	}	if (locNodeId.equals(msg.creatorNodeId()) && msg.senderNodeId() != null) {	
status check message discarded local node is the sender of the status message 

assert msg != null;	UUID locNodeId = getLocalNodeId();	if (msg.failedNodeId() != null) {	if (locNodeId.equals(msg.failedNodeId())) {	return;	}	if (locNodeId.equals(msg.creatorNodeId()) && msg.senderNodeId() != null) {	return;	}	if (isLocalNodeCoordinator() && ring.node(msg.creatorNodeId()) == null) {	
status check message discarded creator node is not in topology 

else {	if (isLocalNodeCoordinator() && !locNodeId.equals(msg.creatorNodeId())) {	if (ring.node(msg.creatorNodeId()) != null) {	msg.status(STATUS_OK);	sendMessageAcrossRing(msg);	}	else {	msg.status(STATUS_RECON);	utilityPool.execute(new Runnable() {	if (spiState == DISCONNECTED) {	
ignoring status check request spi is already disconnected 

log.debug("Failed to respond to status check message (did the node stop?)" + "[recipient=" + msg0.creatorNodeId() + ", status=" + msg0.status() + ']');	}	}	}	}	});	}	return;	}	if (locNodeId.equals(msg.creatorNodeId()) && msg.senderNodeId() == null && U.currentTimeMillis() - locNode.lastUpdateTime() < spi.metricsUpdateFreq) {	
status check message discarded local node receives updates 

}	}	});	}	return;	}	if (locNodeId.equals(msg.creatorNodeId()) && msg.senderNodeId() == null && U.currentTimeMillis() - locNode.lastUpdateTime() < spi.metricsUpdateFreq) {	return;	}	if (locNodeId.equals(msg.creatorNodeId()) && msg.senderNodeId() == null && spiStateCopy() != CONNECTED) {	
status check message discarded local node is not connected to topology 

}	if (locNodeId.equals(msg.creatorNodeId()) && msg.senderNodeId() == null && U.currentTimeMillis() - locNode.lastUpdateTime() < spi.metricsUpdateFreq) {	return;	}	if (locNodeId.equals(msg.creatorNodeId()) && msg.senderNodeId() == null && spiStateCopy() != CONNECTED) {	return;	}	if (locNodeId.equals(msg.creatorNodeId()) && msg.senderNodeId() != null) {	if (spiStateCopy() != CONNECTED) return;	if (msg.status() == STATUS_OK) {	
received ok status response from coordinator 

return;	}	if (locNodeId.equals(msg.creatorNodeId()) && msg.senderNodeId() == null && spiStateCopy() != CONNECTED) {	return;	}	if (locNodeId.equals(msg.creatorNodeId()) && msg.senderNodeId() != null) {	if (spiStateCopy() != CONNECTED) return;	if (msg.status() == STATUS_OK) {	}	else if (msg.status() == STATUS_RECON) {	
node is out of topology probably due to short time network problems 

return;	}	if (locNodeId.equals(msg.creatorNodeId()) && msg.senderNodeId() != null) {	if (spiStateCopy() != CONNECTED) return;	if (msg.status() == STATUS_OK) {	}	else if (msg.status() == STATUS_RECON) {	notifyDiscovery(EVT_NODE_SEGMENTED, ring.topologyVersion(), locNode);	return;	}	
status value was not updated in status response 

private void processMetricsUpdateMessage(TcpDiscoveryMetricsUpdateMessage msg) {	assert msg != null;	assert !msg.client();	UUID locNodeId = getLocalNodeId();	if (ring.node(msg.creatorNodeId()) == null) {	if (log.isDebugEnabled()) log.debug("Discarding metrics update message issued by unknown node [msg=" + msg + ", ring=" + ring + ']');	return;	}	if (isLocalNodeCoordinator() && !locNodeId.equals(msg.creatorNodeId())) {	
discarding metrics update message issued by non coordinator node 

assert !msg.client();	UUID locNodeId = getLocalNodeId();	if (ring.node(msg.creatorNodeId()) == null) {	if (log.isDebugEnabled()) log.debug("Discarding metrics update message issued by unknown node [msg=" + msg + ", ring=" + ring + ']');	return;	}	if (isLocalNodeCoordinator() && !locNodeId.equals(msg.creatorNodeId())) {	return;	}	if (!isLocalNodeCoordinator() && locNodeId.equals(msg.creatorNodeId())) {	
discarding metrics update message issued by local node node is no more coordinator 

if (log.isDebugEnabled()) log.debug("Discarding metrics update message issued by unknown node [msg=" + msg + ", ring=" + ring + ']');	return;	}	if (isLocalNodeCoordinator() && !locNodeId.equals(msg.creatorNodeId())) {	return;	}	if (!isLocalNodeCoordinator() && locNodeId.equals(msg.creatorNodeId())) {	return;	}	if (locNodeId.equals(msg.creatorNodeId()) && !hasMetrics(msg, locNodeId) && msg.senderNodeId() != null) {	
discarding metrics update message that has made two passes 

private void updateMetrics(UUID nodeId, ClusterMetrics metrics, Map<Integer, CacheMetrics> cacheMetrics, long tstamp) {	assert nodeId != null;	assert metrics != null;	TcpDiscoveryNode node = ring.node(nodeId);	if (node != null) {	node.setMetrics(metrics);	node.setCacheMetrics(cacheMetrics);	node.lastUpdateTime(tstamp);	notifyDiscovery(EVT_NODE_METRICS_UPDATED, ring.topologyVersion(), node);	}	
received metrics from unknown node 

private void processClientPingRequest(final TcpDiscoveryClientPingRequest msg) {	utilityPool.execute(new Runnable() {	if (spiState == DISCONNECTED) {	
ignoring ping request spi is already disconnected 

private void processClientPingRequest(final TcpDiscoveryClientPingRequest msg) {	utilityPool.execute(new Runnable() {	if (spiState == DISCONNECTED) {	return;	}	final ClientMessageWorker worker = clientMsgWorkers.get(msg.creatorNodeId());	if (worker == null) {	
ping request from dead client node will be skipped 

msg.message(null, msg.messageBytes());	}	else {	addMessage(new TcpDiscoveryDiscardMessage(getLocalNodeId(), msg.id(), true));	spi.stats.onRingMessageReceived(msg);	DiscoverySpiCustomMessage msgObj = null;	try {	msgObj = msg.message(spi.marshaller(), U.resolveClassLoader(spi.ignite().configuration()));	}	catch (Throwable e) {	
failed to unmarshal discovery custom message 

}	if (msgObj != null) {	DiscoverySpiCustomMessage nextMsg = msgObj.ackMessage();	if (nextMsg != null) {	try {	TcpDiscoveryCustomEventMessage ackMsg = new TcpDiscoveryCustomEventMessage( getLocalNodeId(), nextMsg, U.marshal(spi.marshaller(), nextMsg));	ackMsg.topologyVersion(msg.topologyVersion());	processCustomMessage(ackMsg);	}	catch (IgniteCheckedException e) {	
failed to marshal discovery custom message 

}	if (!failedNodesMsgSent.isEmpty()) {	for (Iterator<UUID> it = failedNodesMsgSent.iterator(); it.hasNext(); ) {	UUID nodeId = it.next();	if (ring.node(nodeId) == null) it.remove();	}	}	}	if (msgs != null) {	for (TcpDiscoveryNodeFailedMessage msg : msgs) {	
added node failed message for node from failed nodes list 

Collection<ClusterNode> snapshot = hist.get(msg.topologyVersion());	if (lsnr != null && (spiState == CONNECTED || spiState == DISCONNECTING)) {	TcpDiscoveryNode node = ring.node(msg.creatorNodeId());	if (node != null) {	try {	DiscoverySpiCustomMessage msgObj = msg.message(spi.marshaller(), U.resolveClassLoader(spi.ignite().configuration()));	lsnr.onDiscovery(DiscoveryCustomEvent.EVT_DISCOVERY_CUSTOM_EVT, msg.topologyVersion(), node, snapshot, hist, msgObj);	if (msgObj.isMutable()) msg.message(msgObj, U.marshal(spi.marshaller(), msgObj));	}	catch (Throwable e) {	
failed to unmarshal discovery custom message 

private void processClientMetricsUpdateMessage(TcpDiscoveryClientMetricsUpdateMessage msg) {	assert msg.client();	ClientMessageWorker wrk = clientMsgWorkers.get(msg.creatorNodeId());	if (wrk != null) wrk.metrics(msg.metrics());	
received client metrics update message from unknown client node 

========================= ignite sample_5686 =========================

public static CountDownLatch tryStart() {	if (initGuard.compareAndSet(false, true)) {	
demo starting embedded nodes for demo 

String jettyHost = jettyAddrs.iterator().next();	Integer jettyPort = ignite.localNode().attribute(ATTR_REST_JETTY_PORT);	if (F.isEmpty(jettyHost) || jettyPort == null) throw new IgniteException("DEMO: Failed to start Jetty REST handler on embedded node");	log.info("DEMO: Started embedded node for demo purpose [TCP binary port={}, Jetty REST port={}]", port, jettyPort);	demoUrl = String.format("http: initLatch.countDown();	}	}	catch (Throwable e) {	if (idx == 0) {	basePort.getAndAdd(50);	
demo failed to start embedded node 

Integer jettyPort = ignite.localNode().attribute(ATTR_REST_JETTY_PORT);	if (F.isEmpty(jettyHost) || jettyPort == null) throw new IgniteException("DEMO: Failed to start Jetty REST handler on embedded node");	log.info("DEMO: Started embedded node for demo purpose [TCP binary port={}, Jetty REST port={}]", port, jettyPort);	demoUrl = String.format("http: initLatch.countDown();	}	}	catch (Throwable e) {	if (idx == 0) {	basePort.getAndAdd(50);	}	
demo failed to start embedded node 

}	}	catch (Throwable e) {	if (idx == 0) {	basePort.getAndAdd(50);	}	}	finally {	if (idx == NODE_CNT) {	deployServices(ignite.services(ignite.cluster().forServers()));	
demo all embedded nodes for demo successfully started 

========================= ignite sample_7341 =========================

SplitInfo bi = ignite.compute().affinityCall(ProjectionsCache.CACHE_NAME, input.affinityKey(bestFeatureIdx, ignite), () -> {	TrainingContext<ContinuousRegionInfo> ctx = ContextCache.getOrCreate(ignite).get(uuid);	Ignite ignite = Ignition.localIgnite();	RegionKey key = ProjectionsCache.key(bestFeatureIdx, regIdx / BLOCK_SIZE, input.affinityKey(bestFeatureIdx, Ignition.localIgnite()), uuid);	RegionProjection reg = ProjectionsCache.getOrCreate(ignite).localPeek(key).get(regIdx % BLOCK_SIZE);	return ctx.featureProcessor(bestFeatureIdx).findBestSplit(reg, ctx.values(bestFeatureIdx, ignite), ctx.labels(), regIdx);	});	long findBestSplit = System.currentTimeMillis() - before;	IndexAndSplitInfo best = new IndexAndSplitInfo(bestFeatureIdx, bi);	regsCnt++;	
globally best idx time calculate best fi regs 

});	SplitNode sn = best.info.createSplitNode(best.featureIdx);	TreeTip tipToSplit = tips.get(ind);	tipToSplit.leafSetter.accept(sn);	tipToSplit.leafSetter = sn::setLeft;	int d = tipToSplit.depth++;	tips.add(new TreeTip(sn::setRight, d));	if (d > curDepth) {	curDepth = d;	if (log.isDebugEnabled()) {	
depth 

});	SplitNode sn = best.info.createSplitNode(best.featureIdx);	TreeTip tipToSplit = tips.get(ind);	tipToSplit.leafSetter.accept(sn);	tipToSplit.leafSetter = sn::setLeft;	int d = tipToSplit.depth++;	tips.add(new TreeTip(sn::setRight, d));	if (d > curDepth) {	curDepth = d;	if (log.isDebugEnabled()) {	
cache size 

if (rightBlock == null) {	List<RegionProjection> newBlock = new ArrayList<>(BLOCK_SIZE);	newBlock.add(right);	return Stream.of(new CacheEntryImpl<>(k, leftBlock), new CacheEntryImpl<>(rightKey, newBlock));	}	else {	rightBlock.add(right);	return rightBlock.equals(k) ? Stream.of(new CacheEntryImpl<>(k, leftBlock)) : Stream.of(new CacheEntryImpl<>(k, leftBlock), new CacheEntryImpl<>(rightKey, rightBlock));	}	}, bestRegsKeys);	
update of projections cache time 

newBlock.add(right);	return Stream.of(new CacheEntryImpl<>(k, leftBlock), new CacheEntryImpl<>(rightKey, newBlock));	}	else {	rightBlock.add(right);	return rightBlock.equals(k) ? Stream.of(new CacheEntryImpl<>(k, leftBlock)) : Stream.of(new CacheEntryImpl<>(k, leftBlock), new CacheEntryImpl<>(rightKey, rightBlock));	}	}, bestRegsKeys);	before = System.currentTimeMillis();	updateSplitCache(ind, rc, featuresCnt, ig -> i -> input.affinityKey(i, ig), uuid);	
update of split cache time 

========================= ignite sample_6601 =========================

private void close(GridClientConnectionCloseReason reason, boolean waitCompletion, @Nullable Throwable cause) {	synchronized (this) {	if (closeReason != null) return;	closeReason = reason;	}	try {	if (waitCompletion && !pendingReqs.isEmpty() && ses.closeTime() == 0) closedLatch.await();	}	catch (InterruptedException ignored) {	
interrupted while waiting for all requests to be processed all pending requests will be failed 

catch (InterruptedException ignored) {	Thread.currentThread().interrupt();	}	if (pingTask != null) pingTask.cancel(false);	if (ses != null) ses.close();	for (Iterator<TcpClientFuture> it = pendingReqs.values().iterator(); it.hasNext(); ) {	GridClientFutureAdapter fut = it.next();	fut.onDone(getCloseReasonAsException(closeReason, cause));	it.remove();	}	
client tcp connection closed 

========================= ignite sample_5168 =========================

protected final <T extends IgniteSpiManagementMBean> void registerMBean(String igniteInstanceName, T impl, Class<T> mbeanItf ) throws IgniteSpiException {	if(ignite == null || U.IGNITE_MBEANS_DISABLED) return;	MBeanServer jmx = ignite.configuration().getMBeanServer();	assert mbeanItf == null || mbeanItf.isInterface();	assert jmx != null;	try {	spiMBean = U.registerMBean(jmx, igniteInstanceName, "SPIs", getName(), impl, mbeanItf);	
registered spi mbean 

protected final void unregisterMBean() throws IgniteSpiException {	if (spiMBean != null && ignite != null) {	assert !U.IGNITE_MBEANS_DISABLED;	MBeanServer jmx = ignite.configuration().getMBeanServer();	assert jmx != null;	try {	jmx.unregisterMBean(spiMBean);	
unregistered spi mbean 

========================= ignite sample_5754 =========================

protected void onError(Throwable e) {	
listener operation failed 

private void checkReversing() {	
suspect logic reversing listener return status was true then false now true again 

========================= ignite sample_5979 =========================

assertEquals(1, staticCache.get(1));	CacheConfiguration<Object, Object> ccfg = new CacheConfiguration<>(DEFAULT_CACHE_NAME);	ccfg.setWriteSynchronizationMode(FULL_SYNC);	ccfg.setName("nearCache");	final IgniteCache<Object, Object> nearCache = client.getOrCreateCache(ccfg, new NearCacheConfiguration<>());	nearCache.put(1, 1);	assertEquals(1, nearCache.localPeek(1));	cache.put(1, 1);	final CountDownLatch disconnectLatch = new CountDownLatch(1);	final CountDownLatch reconnectLatch = new CountDownLatch(1);	
block reconnect 

cache.put(1, 1);	final CountDownLatch disconnectLatch = new CountDownLatch(1);	final CountDownLatch reconnectLatch = new CountDownLatch(1);	clientSpi.writeLatch = new CountDownLatch(1);	final AtomicReference<IgniteInternalFuture> blockPutRef = new AtomicReference<>();	client.events().localListen(new IgnitePredicate<Event>() {	if (evt.type() == EVT_CLIENT_NODE_DISCONNECTED) {	info("Disconnected: " + evt);	assertEquals(1, reconnectLatch.getCount());	blockPutRef.set(GridTestUtils.runAsync(new Callable() {	
start put 

client.events().localListen(new IgnitePredicate<Event>() {	if (evt.type() == EVT_CLIENT_NODE_DISCONNECTED) {	info("Disconnected: " + evt);	assertEquals(1, reconnectLatch.getCount());	blockPutRef.set(GridTestUtils.runAsync(new Callable() {	try {	cache.put(2, 2);	fail();	}	catch (CacheException e) {	
expected exception 

blockPutRef.set(GridTestUtils.runAsync(new Callable() {	try {	cache.put(2, 2);	fail();	}	catch (CacheException e) {	IgniteClientDisconnectedException e0 = (IgniteClientDisconnectedException)e.getCause();	e0.reconnectFuture().get();	}	cache.put(2, 2);	
finish put 

IgniteCache<Object, Object> cache = client.getOrCreateCache(ccfg);	final IgniteTransactions txs = client.transactions();	final Transaction tx = txs.txStart(OPTIMISTIC, REPEATABLE_READ);	cache.put(1, 1);	reconnectClientNode(client, srv, new Runnable() {	try {	tx.commit();	fail();	}	catch (IgniteClientDisconnectedException e) {	
expected error 

fail();	}	catch (IgniteClientDisconnectedException e) {	assertNotNull(e.reconnectFuture());	}	try {	txs.txStart();	fail();	}	catch (IgniteClientDisconnectedException e) {	
expected error 

private void reconnectTransactionInProgress1(IgniteEx client, final TransactionConcurrency txConcurrency, final IgniteCache<Object, Object> cache) throws Exception {	Ignite srv = clientRouter(client);	final TestTcpDiscoverySpi clientSpi = spi(client);	final TestTcpDiscoverySpi srvSpi = spi(srv);	final CountDownLatch disconnectLatch = new CountDownLatch(1);	final CountDownLatch reconnectLatch = new CountDownLatch(1);	
block reconnect 

int g = SRV_CNT + i;	IgniteEx client = startGrid(g);	info(">>>>> Started client: " + g);	addListener(client, disconnectLatch, reconnectLatch);	IgniteCache cache = client.getOrCreateCache(new CacheConfiguration<>(DEFAULT_CACHE_NAME));	assertNotNull(cache);	caches.add(cache);	}	for (int i = 0; i < SRV_CNT; i++) stopGrid(i);	assertTrue(disconnectLatch.await(30_000, MILLISECONDS));	
restart servers 

srvCommSpi.blockMessages(msgToBlock, client.localNode().id());	}	IgniteInternalFuture<?> fut = GridTestUtils.runAsync(new Callable<Object>() {	IgniteClientDisconnectedException e0 = null;	try {	assertEquals(id, client.localNode().id());	c.apply(cache);	fail();	}	catch (IgniteClientDisconnectedException e) {	
expected exception 

IgniteClientDisconnectedException e0 = null;	try {	assertEquals(id, client.localNode().id());	c.apply(cache);	fail();	}	catch (IgniteClientDisconnectedException e) {	e0 = e;	}	catch (CacheException e) {	
expected exception 

========================= ignite sample_2300 =========================

try {	if (put) {	boolean failed = false;	if (txConcurrency != null) {	try (Transaction tx = txs.txStart(txConcurrency, txIsolation)) {	sndCache0.put(key, i);	tx.commit();	}	catch (CacheException | IgniteException e) {	if (!X.hasCause(e, ClusterTopologyCheckedException.class)) {	
unexpected error 

}	else {	boolean failed = false;	if (txConcurrency != null) {	try (Transaction tx = txs.txStart(txConcurrency, txIsolation)) {	sndCache0.remove(key);	tx.commit();	}	catch (CacheException | IgniteException e) {	if (!X.hasCause(e, ClusterTopologyCheckedException.class)) {	
unexpected error 

catch (CacheException e) {	if (put) log.error("Put failed [key=" + key + ", val=" + i + ']', e);	else log.error("Remove failed [key=" + key + ']', e);	errCntr.incrementAndGet();	}	}	}	cntr.addAndGet(100);	CyclicBarrier barrier = cmp.get();	if (barrier != null) {	
wait data check 

if (put) log.error("Put failed [key=" + key + ", val=" + i + ']', e);	else log.error("Remove failed [key=" + key + ']', e);	errCntr.incrementAndGet();	}	}	}	cntr.addAndGet(100);	CyclicBarrier barrier = cmp.get();	if (barrier != null) {	barrier.await(60_000, TimeUnit.MILLISECONDS);	
finished wait data check 

protected IgniteInternalFuture createAndRunConcurrentAction(final AtomicBoolean stop, final AtomicReference<CyclicBarrier> cmp) {	return GridTestUtils.runAsync(new Callable<Void>() {	Thread.currentThread().setName("restart-thread");	while (!stop.get()) {	U.sleep(random(KILL_DELAY.get1(), KILL_DELAY.get2()));	killAndRestart(stop, random(1, GRID_CNT + 1));	CyclicBarrier barrier = cmp.get();	if (barrier != null) {	
wait data check 

protected IgniteInternalFuture createAndRunConcurrentAction(final AtomicBoolean stop, final AtomicReference<CyclicBarrier> cmp) {	return GridTestUtils.runAsync(new Callable<Void>() {	Thread.currentThread().setName("restart-thread");	while (!stop.get()) {	U.sleep(random(KILL_DELAY.get1(), KILL_DELAY.get2()));	killAndRestart(stop, random(1, GRID_CNT + 1));	CyclicBarrier barrier = cmp.get();	if (barrier != null) {	barrier.await(60_000, TimeUnit.MILLISECONDS);	
finished wait data check 

protected void killAndRestart(AtomicBoolean stop, int nodeIdx) throws Exception {	if (stop.get()) return;	
killing node 

protected void killAndRestart(AtomicBoolean stop, int nodeIdx) throws Exception {	if (stop.get()) return;	stopGrid(nodeIdx);	U.sleep(random(START_DELAY.get1(), START_DELAY.get2()));	
restarting node 

========================= ignite sample_1605 =========================

Service svc = svcCtx.service();	if (svc != null) return mtd.invoke(svc, args);	}	}	else {	if (node.version().compareTo(SVC_POOL_SINCE_VER) >= 0) ctx.task().setThreadContext(TC_IO_POLICY, GridIoPolicy.SERVICE_POOL);	return ctx.closure().callAsyncNoFailover( GridClosureCallMode.BROADCAST, new ServiceProxyCallable(mtd.getName(), name, mtd.getParameterTypes(), args), Collections.singleton(node), false, waitTimeout, true).get();	}	}	catch (GridServiceNotFoundException | ClusterTopologyCheckedException e) {	
service was not found or topology changed will retry 

========================= ignite sample_4811 =========================

if (initGuard.compareAndSet(false, true)) try {	if (cred == null && credProvider == null) throw new IgniteSpiException("AWS credentials are not set.");	if (cfg == null) U.warn(log, "Amazon client configuration is not set (will use default).");	if (F.isEmpty(bucketName)) throw new IgniteSpiException("Bucket name is null or empty (provide bucket name and restart).");	objMetadata.setContentLength(ENTRY_CONTENT.length);	if (!F.isEmpty(sseAlg)) objMetadata.setSSEAlgorithm(sseAlg);	s3 = createAmazonS3Client();	if (!s3.doesBucketExist(bucketName)) {	try {	s3.createBucket(bucketName);	
created bucket 

========================= ignite sample_7401 =========================

lockKeys.remove(key);	checkLocks();	}	protected void onAllKeysAdded() {	allKeysAdded = true;	checkLocks();	}	private boolean checkLocks() {	boolean locked = lockKeys.isEmpty();	if (locked && allKeysAdded) {	
all locks are acquired for near prepare future 

========================= ignite sample_3854 =========================

doTestDeadlock(2, false, false, false, startKey);	doTestDeadlock(2, false, false, true, startKey);	doTestDeadlock(3, false, true, true, startKey);	doTestDeadlock(3, false, false, false, startKey);	doTestDeadlock(3, false, false, true, startKey);	doTestDeadlock(4, false, true, true, startKey);	doTestDeadlock(4, false, false, false, startKey);	doTestDeadlock(4, false, false, true, startKey);	}	catch (Exception e) {	
unexpected exception 

k = incrementKey(o, + 13);	involvedKeys.add(k);	entries.put(k, 2);	}	log.info(">>> Performs put [node=" + ((IgniteKernal)ignite).localNode() + ", tx=" + tx + ", entries=" + entries + ']');	cache.putAll(entries);	tx.commit();	}	catch (Throwable e) {	if (hasCause(e, TransactionTimeoutException.class) && hasCause(e, TransactionDeadlockException.class) ) {	
at least one stack trace should contain 

========================= ignite sample_1613 =========================

private void applyPluginsData(UUID nodeId, Map<String, Serializable> pluginsData) {	for (Map.Entry<String, Serializable> e : pluginsData.entrySet()) {	PluginProvider provider = plugins.get(e.getKey());	if (provider != null) provider.receiveDiscoveryData(nodeId, e.getValue());	
received discovery data for unknown plugin 

========================= ignite sample_4554 =========================

changed = checkGarDirectoryChanged(file, dirLastModified);	lastMod = dirLastModified.get();	}	else {	lastMod = tstampCache.get(file);	changed = lastMod == null || lastMod != file.lastModified();	lastMod = file.lastModified();	}	if (changed) {	tstampCache.put(file, lastMod);	
discovered deployment file or directory 

if (file.isDirectory()) {	cpFile = new File(cpFile.getParent(), "dir_" + cpFile.getName());	cpFile.deleteOnExit();	}	U.copy(file, cpFile, true);	String fileUri = getFileUri(file.getAbsolutePath());	assert lastMod != null;	ctx.getListener().onNewOrUpdatedFile(cpFile, fileUri, lastMod);	}	catch (IOException e) {	
error saving file 

========================= ignite sample_7905 =========================

public void testConcurrentStart() throws Exception {	for (int i = 0; i < ITERATIONS; i++) {	
iteration 

public void testConcurrentStart() throws Exception {	for (int i = 0; i < ITERATIONS; i++) {	long start = System.currentTimeMillis();	startGridsMultiThreaded(10, false);	startGridsMultiThreaded(10, 10);	awaitPartitionMapExchange();	stopAllGrids();	
iteration finished time 

========================= ignite sample_1214 =========================

private void prepareProcess(HadoopPrepareForJobRequest req) {	if (initGuard.compareAndSet(false, true)) {	try {	
initializing external hadoop task 

throw new IgniteException("Failed to load job class: " + HadoopCommonUtils.JOB_CLS_NAME, e);	}	job = req.jobInfo().createJob(jobCls, req.jobId(), log, null, new HadoopHelperImpl());	job.initialize(true, nodeDesc.processId());	shuffleJob = new HadoopShuffleJob<>(comm.localProcessDescriptor(), log, job, mem, req.totalReducerCount(), req.localReducers(), 0, false);	initializeExecutors();	if (log.isDebugEnabled()) log.debug("External process initialized [initWaitTime=" + (U.currentTimeMillis() - startTime) + ']');	initFut.onDone();	}	catch (IgniteCheckedException e) {	
failed to initialize process 

job.initialize(true, nodeDesc.processId());	shuffleJob = new HadoopShuffleJob<>(comm.localProcessDescriptor(), log, job, mem, req.totalReducerCount(), req.localReducers(), 0, false);	initializeExecutors();	if (log.isDebugEnabled()) log.debug("External process initialized [initWaitTime=" + (U.currentTimeMillis() - startTime) + ']');	initFut.onDone();	}	catch (IgniteCheckedException e) {	initFut.onDone(e);	}	}	
duplicate initialize process request received will ignore 

private void runTasks(final HadoopTaskExecutionRequest req) {	
will wait for process initialization future completion 

initFut.listen(new CI1<IgniteInternalFuture<?>>() {	try {	f.get();	boolean set = pendingTasks.compareAndSet(0, req.tasks().size());	assert set;	HadoopTaskInfo info = F.first(req.tasks());	assert info != null;	int size = info.type() == MAP ? concMappers : concReducers;	if (log.isDebugEnabled()) log.debug("Set executor service size for task type [type=" + info.type() + ", size=" + size + ']');	for (HadoopTaskInfo taskInfo : req.tasks()) {	
submitted task for external execution 

private void shutdown() {	if (execSvc != null) execSvc.shutdown(5000);	if (msgExecSvc != null) msgExecSvc.shutdownNow();	try {	job.dispose(true);	}	catch (IgniteCheckedException e) {	
failed to dispose job 

private void notifyTaskFinished(final HadoopTaskInfo taskInfo, final HadoopTaskStatus status, boolean flush) {	final HadoopTaskState state = status.state();	final Throwable err = status.failCause();	if (!flush) {	try {	if (log.isDebugEnabled()) log.debug("Sending notification to parent node [taskInfo=" + taskInfo + ", state=" + state + ", err=" + err + ']');	comm.sendMessage(nodeDesc, new HadoopTaskFinishedMessage(taskInfo, status));	}	catch (IgniteCheckedException e) {	
failed to send message to parent node will terminate child process 

========================= ignite sample_7285 =========================

public void testStopMultithreaded() throws Exception {	try {	startGrid(0);	for (int i = 0; i < 5; i++) {	
iteration 

catch (Exception ignore) {	}	}	return null;	}	}, 2, "tx-thread");	Thread.sleep(3000);	final AtomicInteger nodeIdx = new AtomicInteger(1);	GridTestUtils.runMultiThreaded(new Callable<Void>() {	int idx = nodeIdx.getAndIncrement();	
stop node 

tx.commit();	}	}	else {	readyLatch.countDown();	stopLatch.await();	cache.put(key, key);	}	}	catch (CacheException | IgniteException | IllegalStateException e) {	
ignore error 

========================= ignite sample_1516 =========================

private void checkBackupConsistency(CacheConfiguration<Integer, Integer> ccfg, TransactionConcurrency txConcurrency, TransactionIsolation txIsolation) throws Exception {	IgniteCache<Integer, Integer> cache = grid(0).getOrCreateCache(ccfg);	int nodeIdx = ThreadLocalRandom.current().nextInt(NODES);	try {	for (int i = 0; i < ITERATION_CNT; i++) {	
iteration 

private void checkBackupConsistencyGetAll(CacheConfiguration<Integer, Integer> ccfg, TransactionConcurrency txConcurrency, TransactionIsolation txIsolation) throws Exception {	IgniteCache<Integer, Integer> cache = grid(0).getOrCreateCache(ccfg);	int nodeIdx = ThreadLocalRandom.current().nextInt(NODES);	try {	for (int i = 0; i < ITERATION_CNT; i++) {	
iteration 

========================= ignite sample_1900 =========================

}	}	}	});	cache.query(qry);	nodesEvts.add(allEvts);	}	final AtomicInteger keyVal = new AtomicInteger();	for (int i = 0; i < NODES; i++) {	Ignite ignite = ignite(i);	
test with node 

========================= ignite sample_867 =========================

private void registerMetricsMBean( DataRegionMetricsImpl memMetrics, DataRegionConfiguration dataRegionCfg, IgniteConfiguration cfg ) {	assert !U.IGNITE_MBEANS_DISABLED;	try {	U.registerMBean( cfg.getMBeanServer(), cfg.getIgniteInstanceName(), "DataRegionMetrics", dataRegionCfg.getName(), new DataRegionMetricsMXBeanImpl(memMetrics, dataRegionCfg), DataRegionMetricsMXBean.class);	}	catch (Throwable e) {	
failed to register mbean for dataregionmetrics with name 

protected void addDataRegion( DataStorageConfiguration dataStorageCfg, DataRegionConfiguration dataRegionCfg, boolean trackable ) throws IgniteCheckedException {	String dataRegionName = dataRegionCfg.getName();	String dfltMemPlcName = dataStorageCfg.getDefaultDataRegionConfiguration().getName();	if (dfltMemPlcName == null) dfltMemPlcName = DFLT_DATA_REG_DEFAULT_NAME;	DataRegionMetricsImpl memMetrics = new DataRegionMetricsImpl(dataRegionCfg, fillFactorProvider(dataRegionCfg));	DataRegion memPlc = initMemory(dataStorageCfg, dataRegionCfg, memMetrics, trackable);	dataRegionMap.put(dataRegionName, memPlc);	memMetricsMap.put(dataRegionName, memMetrics);	if (dataRegionName.equals(dfltMemPlcName)) dfltDataRegion = memPlc;	
data region with name default isn t used as a default please check memory policies configuration 

private void unregisterMBean(String name) {	if(U.IGNITE_MBEANS_DISABLED) return;	IgniteConfiguration cfg = cctx.gridConfig();	try {	cfg.getMBeanServer().unregisterMBean( U.makeMBeanName( cfg.getIgniteInstanceName(), "DataRegionMetrics", name ));	}	catch (InstanceNotFoundException ignored) {	}	catch (Throwable e) {	
failed to unregister mbean for memory metrics 

========================= ignite sample_4128 =========================

private void startEmbeddedCassandra() {	if (log != null) {	log.info("-------------------------------");	
starting embedded cassandra 

if (cassandraCfgFile != null) System.setProperty(CASSANDRA_CONFIG_PROP, FILE_PREFIX + cassandraCfgFile);	embeddedCassandraDaemon = new CassandraDaemon(true);	embeddedCassandraDaemon.init(null);	embeddedCassandraDaemon.start();	}	catch (Exception e) {	throw new RuntimeException("Failed to start embedded Cassandra", e);	}	if (log != null) {	log.info("------------------------------");	
embedded cassandra started 

private void stopEmbeddedCassandra() {	if (log != null) {	log.info("-------------------------------");	
stopping embedded cassandra 

if (embeddedCassandraDaemon != null) {	try {	embeddedCassandraDaemon.deactivate();	}	catch (Throwable e) {	throw new RuntimeException("Failed to stop embedded Cassandra", e);	}	}	if (log != null) {	log.info("------------------------------");	
embedded cassandra stopped 

========================= ignite sample_6953 =========================

cancelAllJobs = true;	IgniteInternalFuture<Long> affFut = null;	try {	affFut = GridTestUtils.runMultiThreadedAsync(new Runnable() {	while (System.currentTimeMillis() < endTime) {	int n = 0;	try {	for (final int orgId : orgIds) {	if (System.currentTimeMillis() >= endTime) break;	n = jobNum.getAndIncrement();	
job submitted 

while (System.currentTimeMillis() < endTime) {	int n = 0;	try {	for (final int orgId : orgIds) {	if (System.currentTimeMillis() >= endTime) break;	n = jobNum.getAndIncrement();	grid(0).compute().affinityRun( Arrays.asList(Organization.class.getSimpleName(), Person.class.getSimpleName()), new Integer(orgId), new TestRun(n));	}	}	catch (Exception e) {	
job failed 

========================= ignite sample_7495 =========================

BigDecimal rate = deposit.field(MARGIN_RATE);	BigDecimal expBalance;	SqlFieldsQuery findDepositHist = new SqlFieldsQuery(DEPOSIT_OPERATION_COUNT_SQL);	try (QueryCursor cursor1 = histCache.query(findDepositHist.setArgs(depositKey))) {	Long cnt = (Long)((ArrayList)cursor1.iterator().next()).get(0);	expBalance = startBalance.multiply(rate.add(BigDecimal.ONE).pow(cnt.intValue()));	}	expBalance = expBalance.setScale(2, BigDecimal.ROUND_DOWN);	balance = balance.setScale(2, BigDecimal.ROUND_DOWN);	if (checkBalance && !expBalance.equals(balance)) {	
deposit has incorrect balance when expected 

========================= ignite sample_7616 =========================

ses.update(e);	ses.save(new Entity(2, "name-2"));	ses.delete(ses.load(Entity2.class, 0));	Entity2 e2 = (Entity2)ses.load(Entity2.class, 1);	e2.setName("name-2");	ses.update(e2);	tx.commit();	fail("Commit must fail.");	}	catch (ConstraintViolationException e) {	
expected exception 

if (accessType == AccessType.READ_ONLY) return;	e0.setVersion(ver - 1);	ses = sesFactory1.openSession();	Transaction tx = ses.beginTransaction();	try {	ses.update(e0);	tx.commit();	fail("Commit must fail.");	}	catch (StaleObjectStateException e) {	
expected exception 

assertNaturalIdCache(sesFactory1, nameToId, "name-1-changed1");	ses = sesFactory1.openSession();	Transaction tx = ses.beginTransaction();	try {	Entity e1 = (Entity)ses.load(Entity.class, 1);	e1.setName("name-0");	tx.commit();	fail("Commit must fail.");	}	catch (ConstraintViolationException e) {	
expected exception 

assertEntityCache(ENTITY_NAME, sesFactory1, idToName, 100);	ses = sesFactory1.openSession();	Transaction tx = ses.beginTransaction();	try {	ses.save(new Entity(3, "name-3"));	ses.save(new Entity(0, "name-0"));	tx.commit();	fail("Commit must fail.");	}	catch (ConstraintViolationException e) {	
expected exception 

Entity e0 = (Entity)ses.load(Entity.class, 0);	Entity e1 = (Entity)ses.load(Entity.class, 1);	e0.setName("name-10");	e1.setName("name-2");	ses.update(e0);	ses.update(e1);	tx.commit();	fail("Commit must fail.");	}	catch (ConstraintViolationException e) {	
expected exception 

tx = ses.beginTransaction();	try {	ses.save(new Entity(3, "name-3"));	Entity e1 = (Entity)ses.load(Entity.class, 1);	e1.setName("name-10");	ses.delete(ses.load(Entity.class, 0));	tx.commit();	fail("Commit must fail.");	}	catch (ConstraintViolationException e) {	
expected exception 

ses = sesFactory1.openSession();	tx = ses.beginTransaction();	try {	ses.delete(ses.load(Entity.class, 1));	idToName.remove(1);	ses.delete(ses.load(Entity.class, 0));	tx.commit();	fail("Commit must fail.");	}	catch (ConstraintViolationException e) {	
expected exception 

========================= ignite sample_430 =========================

srv = false;	log = null;	}	if (srv) {	ioLsnr = new GridMessageListener() {	if (msg instanceof WalStateAckMessage) {	WalStateAckMessage msg0 = (WalStateAckMessage) msg;	msg0.senderNodeId(nodeId);	onAck(msg0);	}	
unexpected io message will ignore 

public void onCachesInfoCollected() {	if (!srv) return;	synchronized (mux) {	for (CacheGroupDescriptor grpDesc : cacheProcessor().cacheGroupDescriptors().values()) {	WalStateProposeMessage msg = grpDesc.nextWalChangeRequest();	if (msg != null) {	
processing wal state message on start 

fut.listen(new IgniteInClosure<IgniteInternalFuture<Boolean>>() {	synchronized (mux) {	userFuts.remove(opId);	}	}	});	WalStateProposeMessage msg = new WalStateProposeMessage(opId, grpDesc.groupId(), grpDesc.deploymentId(), cctx.localNodeId(), caches, enabled);	userFuts.put(opId, fut);	try {	cctx.discovery().sendCustomEvent(msg);	
initiated wal state change operation 

public void onProposeDiscovery(WalStateProposeMessage msg) {	if (isDuplicate(msg)) return;	synchronized (mux) {	if (disconnected) return;	if (validateProposeDiscovery(msg)) {	
wal state change message is valid will continue processing 

synchronized (mux) {	if (disconnected) return;	if (validateProposeDiscovery(msg)) {	CacheGroupDescriptor grpDesc = cacheProcessor().cacheGroupDescriptors().get(msg.groupId());	assert grpDesc != null;	IgnitePredicate<ClusterNode> nodeFilter = grpDesc.config().getNodeFilter();	boolean affNode = srv && (nodeFilter == null || nodeFilter.apply(cctx.localNode()));	msg.affinityNode(affNode);	if (grpDesc.addWalChangeRequest(msg)) {	msg.exchangeMessage(msg);	
wal state change message will be processed in exchange thread 

if (validateProposeDiscovery(msg)) {	CacheGroupDescriptor grpDesc = cacheProcessor().cacheGroupDescriptors().get(msg.groupId());	assert grpDesc != null;	IgnitePredicate<ClusterNode> nodeFilter = grpDesc.config().getNodeFilter();	boolean affNode = srv && (nodeFilter == null || nodeFilter.apply(cctx.localNode()));	msg.affinityNode(affNode);	if (grpDesc.addWalChangeRequest(msg)) {	msg.exchangeMessage(msg);	}	else {	
wal state change message is added to pending set and will be processed later 

IgnitePredicate<ClusterNode> nodeFilter = grpDesc.config().getNodeFilter();	boolean affNode = srv && (nodeFilter == null || nodeFilter.apply(cctx.localNode()));	msg.affinityNode(affNode);	if (grpDesc.addWalChangeRequest(msg)) {	msg.exchangeMessage(msg);	}	else {	}	}	else {	
wal state change message is invalid will ignore 

private void sendFinishMessage(WalStateFinishMessage finishMsg) {	try {	cctx.discovery().sendCustomEvent(finishMsg);	}	catch (Exception e) {	
failed to send wal mode change finish message due to unexpected exception 

private boolean isDuplicate(WalStateAbstractMessage msg) {	T2<UUID, Boolean> key;	if (msg instanceof WalStateProposeMessage) key = new T2<>(msg.operationId(), true);	else {	assert msg instanceof WalStateFinishMessage;	key = new T2<>(msg.operationId(), false);	}	if (!discoMsgIdHist.add(key)) {	
received duplicate wal mode change discovery message will ignore 

========================= ignite sample_4240 =========================

clientFlagGlobal = true;	startGridsMultiThreaded(srvs, clients);	final BlockingQueue<Integer> clientStopIdxs = new LinkedBlockingQueue<>();	for (int i = srvs; i < srvs + clients; i++) clientStopIdxs.add(i);	final AtomicInteger clientStartIdx = new AtomicInteger(9000);	clientFut = multithreadedAsync( new Callable<Object>() {	try {	clientFlagPerThread.set(true);	while (!done.get() && error.get() == null) {	Integer stopIdx = clientStopIdxs.take();	
stop client 

for (int i = srvs; i < srvs + clients; i++) clientStopIdxs.add(i);	final AtomicInteger clientStartIdx = new AtomicInteger(9000);	clientFut = multithreadedAsync( new Callable<Object>() {	try {	clientFlagPerThread.set(true);	while (!done.get() && error.get() == null) {	Integer stopIdx = clientStopIdxs.take();	stopGrid(stopIdx);	while (!done.get() && error.get() == null) {	int startIdx = clientStartIdx.getAndIncrement();	
start client 

int startIdx = clientStartIdx.getAndIncrement();	UUID id = UUID.randomUUID();	nodeId.set(id);	try {	Ignite ignite = startGrid(startIdx);	assertTrue(ignite.configuration().isClientMode());	clientStopIdxs.add(startIdx);	break;	}	catch (Exception e) {	
client disconnected 

int startIdx = clientStartIdx.getAndIncrement();	UUID id = UUID.randomUUID();	nodeId.set(id);	try {	Ignite ignite = startGrid(startIdx);	assertTrue(ignite.configuration().isClientMode());	clientStopIdxs.add(startIdx);	break;	}	catch (Exception e) {	
client failed to start 

UUID id = UUID.randomUUID();	nodeId.set(id);	try {	Ignite ignite = startGrid(startIdx);	assertTrue(ignite.configuration().isClientMode());	clientStopIdxs.add(startIdx);	break;	}	catch (Exception e) {	else {	
client failed 

}	catch (Exception e) {	else {	else throw e;	}	}	}	}	}	catch (Throwable e) {	
unexpected error 

final BlockingQueue<Integer> srvStopIdxs = new LinkedBlockingQueue<>();	for (int i = 0; i < srvs; i++) srvStopIdxs.add(i);	final AtomicInteger srvStartIdx = new AtomicInteger(srvs + clients);	IgniteInternalFuture<?> srvFut = multithreadedAsync( new Callable<Object>() {	try {	clientFlagPerThread.set(false);	while (!done.get() && error.get() == null) {	int stopIdx = srvStopIdxs.take();	U.sleep(50);	Thread.currentThread().setName("stop-server-" + getTestIgniteInstanceName(stopIdx));	
stop server 

IgniteInternalFuture<?> srvFut = multithreadedAsync( new Callable<Object>() {	try {	clientFlagPerThread.set(false);	while (!done.get() && error.get() == null) {	int stopIdx = srvStopIdxs.take();	U.sleep(50);	Thread.currentThread().setName("stop-server-" + getTestIgniteInstanceName(stopIdx));	stopGrid(stopIdx);	int startIdx = srvStartIdx.getAndIncrement();	Thread.currentThread().setName("start-server-" + getTestIgniteInstanceName(startIdx));	
start server 

Thread.currentThread().setName("stop-server-" + getTestIgniteInstanceName(stopIdx));	stopGrid(stopIdx);	int startIdx = srvStartIdx.getAndIncrement();	Thread.currentThread().setName("start-server-" + getTestIgniteInstanceName(startIdx));	try {	Ignite ignite = startGrid(startIdx);	assertFalse(ignite.configuration().isClientMode());	srvStopIdxs.add(startIdx);	}	catch (IgniteCheckedException e) {	
failed to start 

try {	Ignite ignite = startGrid(startIdx);	assertFalse(ignite.configuration().isClientMode());	srvStopIdxs.add(startIdx);	}	catch (IgniteCheckedException e) {	}	}	}	catch (Throwable e) {	
unexpected error 

public void testMultipleStartOnCoordinatorStop() throws Exception{	for (int k = 0; k < 3; k++) {	
iteration 

final int JOIN_NODES = 10;	startGrids(START_NODES);	final CyclicBarrier barrier = new CyclicBarrier(JOIN_NODES + 1);	final AtomicInteger startIdx = new AtomicInteger(START_NODES);	IgniteInternalFuture<?> fut = GridTestUtils.runMultiThreadedAsync(new Callable<Object>() {	int idx = startIdx.getAndIncrement();	Thread.currentThread().setName("start-thread-" + idx);	barrier.await();	Ignite ignite = startGrid(idx);	assertFalse(ignite.configuration().isClientMode());	
started node 

public void _testCustomEventOnJoinCoordinatorStop() throws Exception {	for (int k = 0; k < 10; k++) {	
iteration 

}	});	try {	final CyclicBarrier barrier = new CyclicBarrier(JOIN_NODES + 1);	IgniteInternalFuture<?> fut2 = GridTestUtils.runMultiThreadedAsync(new Callable<Object>() {	int idx = startIdx.getAndIncrement();	Thread.currentThread().setName("start-thread-" + idx);	barrier.await();	Ignite ignite = startGrid(idx);	assertFalse(ignite.configuration().isClientMode());	
started node 

public void _testClientContinuousQueryCoordinatorStop() throws Exception {	for (int k = 0; k < 10; k++) {	
iteration 

ignite(0).createCache(new CacheConfiguration<>(DEFAULT_CACHE_NAME));	final AtomicInteger startIdx = new AtomicInteger(START_NODES);	final CyclicBarrier barrier = new CyclicBarrier(JOIN_NODES + 1);	clientFlagGlobal = true;	IgniteInternalFuture<?> fut = GridTestUtils.runMultiThreadedAsync(new Callable<Object>() {	int idx = startIdx.getAndIncrement();	Thread.currentThread().setName("start-thread-" + idx);	barrier.await();	Ignite ignite = startGrid(idx);	assertTrue(ignite.configuration().isClientMode());	
started node 

}	});	QueryCursor<Cache.Entry<Object, Object>> cur = cache.query(qry);	cur.close();	}	GridTestUtils.invoke(ignite.configuration().getDiscoverySpi(), "simulateNodeFailure");	ignite.close();	}	}	catch (Exception e) {	
unexpected error 

========================= ignite sample_2547 =========================

private void testUniqueName(final boolean singleGrid) throws Exception {	final String name = IgniteUuid.randomUuid().toString();	final int DS_TYPES = 6;	final int THREADS = DS_TYPES * 3;	for (int iter = 0; iter < 20; iter++) {	
iteration 

final CyclicBarrier barrier = new CyclicBarrier(THREADS);	for (int i = 0; i < THREADS; i++) {	final int idx = i;	IgniteInternalFuture<Object> fut = GridTestUtils.runAsync(new Callable<Object>() {	try {	Thread.currentThread().setName("test thread-" + idx);	barrier.await();	Ignite ignite = singleGrid ? ignite(0) : ignite(idx % gridCount());	Object res;	switch (idx % DS_TYPES) {	
create atomic long grid 

final int idx = i;	IgniteInternalFuture<Object> fut = GridTestUtils.runAsync(new Callable<Object>() {	try {	Thread.currentThread().setName("test thread-" + idx);	barrier.await();	Ignite ignite = singleGrid ? ignite(0) : ignite(idx % gridCount());	Object res;	switch (idx % DS_TYPES) {	res = ignite.atomicLong(name, 0, true);	break;	
create atomic sequence grid 

try {	Thread.currentThread().setName("test thread-" + idx);	barrier.await();	Ignite ignite = singleGrid ? ignite(0) : ignite(idx % gridCount());	Object res;	switch (idx % DS_TYPES) {	res = ignite.atomicLong(name, 0, true);	break;	res = ignite.atomicSequence(name, 0, true);	break;	
create atomic stamped grid 

barrier.await();	Ignite ignite = singleGrid ? ignite(0) : ignite(idx % gridCount());	Object res;	switch (idx % DS_TYPES) {	res = ignite.atomicLong(name, 0, true);	break;	res = ignite.atomicSequence(name, 0, true);	break;	res = ignite.atomicStamped(name, 0, true, true);	break;	
create atomic reference grid 

Object res;	switch (idx % DS_TYPES) {	res = ignite.atomicLong(name, 0, true);	break;	res = ignite.atomicSequence(name, 0, true);	break;	res = ignite.atomicStamped(name, 0, true, true);	break;	res = ignite.atomicReference(name, null, true);	break;	
create queue grid 

res = ignite.atomicLong(name, 0, true);	break;	res = ignite.atomicSequence(name, 0, true);	break;	res = ignite.atomicStamped(name, 0, true, true);	break;	res = ignite.atomicReference(name, null, true);	break;	res = ignite.queue(name, 0, config(false));	break;	
create set grid 

break;	res = ignite.atomicReference(name, null, true);	break;	res = ignite.queue(name, 0, config(false));	break;	res = ignite.set(name, config(false));	break;	default: fail();	return null;	}	
thread created 

res = ignite.queue(name, 0, config(false));	break;	res = ignite.set(name, config(false));	break;	default: fail();	return null;	}	return res;	}	catch (IgniteException e) {	
failed 

}	});	futs.add(fut);	}	Closeable dataStructure = null;	int createdCnt = 0;	for (IgniteInternalFuture<Object> fut : futs) {	Object res = fut.get();	if (res instanceof IgniteException || res instanceof IgniteCheckedException) continue;	assertTrue("Unexpected object: " + res, res instanceof IgniteAtomicLong || res instanceof IgniteAtomicSequence || res instanceof IgniteAtomicReference || res instanceof IgniteAtomicStamped || res instanceof IgniteCountDownLatch || res instanceof IgniteQueue || res instanceof IgniteSet || res instanceof IgniteSemaphore || res instanceof IgniteLock);	
data structure created 

========================= ignite sample_934 =========================

public void testMarkerFiltering() throws Exception {	Slf4jLogger log = new Slf4jLogger(LoggerFactory.getLogger(Slf4jLoggerMarkerTest.class));	
ignore me ignored error 

public void testMarkerFiltering() throws Exception {	Slf4jLogger log = new Slf4jLogger(LoggerFactory.getLogger(Slf4jLoggerMarkerTest.class));	
ignore me ignored warning 

public void testMarkerFiltering() throws Exception {	Slf4jLogger log = new Slf4jLogger(LoggerFactory.getLogger(Slf4jLoggerMarkerTest.class));	
ignore me ignored info 

public void testMarkerFiltering() throws Exception {	Slf4jLogger log = new Slf4jLogger(LoggerFactory.getLogger(Slf4jLoggerMarkerTest.class));	
ignore me ignored debug 

public void testMarkerFiltering() throws Exception {	Slf4jLogger log = new Slf4jLogger(LoggerFactory.getLogger(Slf4jLoggerMarkerTest.class));	
ignore me ignored trace 

public void testMarkerFiltering() throws Exception {	Slf4jLogger log = new Slf4jLogger(LoggerFactory.getLogger(Slf4jLoggerMarkerTest.class));	
accept me accepted error 

public void testMarkerFiltering() throws Exception {	Slf4jLogger log = new Slf4jLogger(LoggerFactory.getLogger(Slf4jLoggerMarkerTest.class));	
accept me accepted warning 

public void testMarkerFiltering() throws Exception {	Slf4jLogger log = new Slf4jLogger(LoggerFactory.getLogger(Slf4jLoggerMarkerTest.class));	
accept me accepted info 

public void testMarkerFiltering() throws Exception {	Slf4jLogger log = new Slf4jLogger(LoggerFactory.getLogger(Slf4jLoggerMarkerTest.class));	
accept me accepted debug 

public void testMarkerFiltering() throws Exception {	Slf4jLogger log = new Slf4jLogger(LoggerFactory.getLogger(Slf4jLoggerMarkerTest.class));	
accept me accepted trace 

========================= ignite sample_6103 =========================

public static Connection testDrive() throws SQLException {	if (initLatch.compareAndSet(false, true)) {	
demo prepare in memory database 

public static Connection testDrive() throws SQLException {	if (initLatch.compareAndSet(false, true)) {	try {	Class.forName("org.h2.Driver");	Connection conn = DriverManager.getConnection("jdbc:h2:mem:demo-db;DB_CLOSE_DELAY=-1", "sa", "");	File sqlScript = resolvePath("demo/db-init.sql");	RunScript.execute(conn, new FileReader(sqlScript));	
demo sample tables created 

public static Connection testDrive() throws SQLException {	if (initLatch.compareAndSet(false, true)) {	try {	Class.forName("org.h2.Driver");	Connection conn = DriverManager.getConnection("jdbc:h2:mem:demo-db;DB_CLOSE_DELAY=-1", "sa", "");	File sqlScript = resolvePath("demo/db-init.sql");	RunScript.execute(conn, new FileReader(sqlScript));	conn.close();	Server.createTcpServer("-tcpDaemon").start();	
demo tcpserver stared 

public static Connection testDrive() throws SQLException {	if (initLatch.compareAndSet(false, true)) {	try {	Class.forName("org.h2.Driver");	Connection conn = DriverManager.getConnection("jdbc:h2:mem:demo-db;DB_CLOSE_DELAY=-1", "sa", "");	File sqlScript = resolvePath("demo/db-init.sql");	RunScript.execute(conn, new FileReader(sqlScript));	conn.close();	Server.createTcpServer("-tcpDaemon").start();	
demo jdbc url for test drive metadata load jdbc mem demo db 

if (initLatch.compareAndSet(false, true)) {	try {	Class.forName("org.h2.Driver");	Connection conn = DriverManager.getConnection("jdbc:h2:mem:demo-db;DB_CLOSE_DELAY=-1", "sa", "");	File sqlScript = resolvePath("demo/db-init.sql");	RunScript.execute(conn, new FileReader(sqlScript));	conn.close();	Server.createTcpServer("-tcpDaemon").start();	}	catch (ClassNotFoundException e) {	
demo failed to load driver 

Connection conn = DriverManager.getConnection("jdbc:h2:mem:demo-db;DB_CLOSE_DELAY=-1", "sa", "");	File sqlScript = resolvePath("demo/db-init.sql");	RunScript.execute(conn, new FileReader(sqlScript));	conn.close();	Server.createTcpServer("-tcpDaemon").start();	}	catch (ClassNotFoundException e) {	throw new SQLException("Failed to load H2 driver", e);	}	catch (SQLException e) {	
demo failed to start test drive for metadata 

conn.close();	Server.createTcpServer("-tcpDaemon").start();	}	catch (ClassNotFoundException e) {	throw new SQLException("Failed to load H2 driver", e);	}	catch (SQLException e) {	throw e;	}	catch (FileNotFoundException | NullPointerException e) {	
demo failed to find demo database init script file demo db init sql 

========================= ignite sample_7342 =========================

assert curDelegate != null;	close = curDelegate.doomed;	return clo.apply(curDelegate.hadoop, curDelegate.hndResp);	}	catch (HadoopIgfsCommunicationException e) {	if (curDelegate != null && !curDelegate.doomed) {	delegateRef.compareAndSet(curDelegate, null);	close = true;	force = true;	}	
failed to send message to a server 

========================= ignite sample_7207 =========================

msg = it.next();	try {	addMessage(msg);	}	catch (Exception e) {	U.error(log, "Message is ignored due to an error [msg=" + msg + ']', e);	}	}	}	catch (Exception e) {	
message can t be consumed from stream retry after ms 

public void stop() {	stopped = true;	if (consumer != null) consumer.shutdown();	if (executor != null) {	executor.shutdown();	try {	
timed out waiting for consumer threads to shut down exiting uncleanly 

public void stop() {	stopped = true;	if (consumer != null) consumer.shutdown();	if (executor != null) {	executor.shutdown();	try {	}	catch (InterruptedException ignored) {	
interrupted during shutdown exiting uncleanly 

========================= ignite sample_6932 =========================

public void testLockAndConcurrentTimeout() throws Exception {	startClient();	for (Ignite node : G.allGrids()) {	
test with node 

public void testRandomMixedTxConfigurations() throws Exception {	final Ignite client = startClient();	final AtomicBoolean stop = new AtomicBoolean();	final long seed = System.currentTimeMillis();	final Random r = new Random(seed);	
using seed 

case 4: break;	default: fail();	}	}	catch (Throwable t) {	saved = t;	}	Collection set = U.field(near.context().cache().context().time(), "timeoutObjs");	for (Object obj : set) {	if (obj.getClass().isAssignableFrom(GridNearTxLocal.class)) {	
last saved exception 

}	finally {	blocked.countDown();	}	U.awaitQuiet(unblocked);	try {	near.cache(CACHE_NAME).put(0, 0);	fail();	}	catch (CacheException e) {	
expecting error 

near.cache(CACHE_NAME).put(0, 0);	fail();	}	catch (CacheException e) {	}	try {	tx.commit();	fail();	}	catch (IgniteException e) {	
expecting error 

private void waitingTxUnblockedOnThreadDeath0(final Ignite near, final Ignite other, final int recordsCnt, final long timeout) throws Exception {	info("Start test [node1=" + near.name() + ", node2=" + other.name() + ']');	final CountDownLatch blocked = new CountDownLatch(1);	IgniteInternalFuture<?> fut1 = multithreadedAsync(new Runnable() {	near.transactions().txStart(PESSIMISTIC, REPEATABLE_READ, timeout, recordsCnt);	try {	for (int i = 0; i < recordsCnt; i++) near.cache(CACHE_NAME).put(i, i);	
locked all records 

private void waitingTxUnblockedOnThreadDeath0(final Ignite near, final Ignite other, final int recordsCnt, final long timeout) throws Exception {	info("Start test [node1=" + near.name() + ", node2=" + other.name() + ']');	final CountDownLatch blocked = new CountDownLatch(1);	IgniteInternalFuture<?> fut1 = multithreadedAsync(new Runnable() {	near.transactions().txStart(PESSIMISTIC, REPEATABLE_READ, timeout, recordsCnt);	try {	for (int i = 0; i < recordsCnt; i++) near.cache(CACHE_NAME).put(i, i);	}	catch (Exception e) {	
failed to locked all records 

========================= ignite sample_1609 =========================

case 8: obj = new TestClass9(); break;	case 9: obj = new TestClass10(); break;	default: fail();	}	data.put(i, obj);	dataBytes.put(i, ignite.configuration().getMarshaller().marshal(obj));	}	ignite.cache(DEFAULT_CACHE_NAME).putAll(data);	stopGrid(0);	for (int i = 0; i < 3; i++) {	
iteration 

========================= ignite sample_851 =========================

public void runLocalWithThreadPolicy(IgniteThread thread, Runnable c) {	assert thread.stripe() >= 0 || thread.policy() != GridIoPolicy.UNDEFINED : thread;	if (thread.stripe() >= 0) ctx.getStripedExecutorService().execute(thread.stripe(), c);	else {	try {	ctx.pools().poolForPolicy(thread.policy()).execute(c);	}	catch (IgniteCheckedException e) {	
failed to get pool for policy 

if (!(c instanceof GridPlainRunnable)) ctx.resource().inject(ctx.deploy().getDeployment(c.getClass().getName()), c.getClass(), c);	final ClassLoader ldr = Thread.currentThread().getContextClassLoader();	final GridWorkerFuture fut = new GridWorkerFuture();	GridWorker w = new GridWorker(ctx.igniteInstanceName(), "closure-proc-worker", log) {	try {	if (ldr != null) U.wrapThreadLoader(ldr, c);	else c.run();	fut.onDone();	}	catch (Throwable e) {	
closure execution failed with error 

public IgniteInternalFuture<?> runLocalSafe(Runnable c, byte plc) {	try {	return runLocal(c, plc);	}	catch (Throwable e) {	if (e instanceof Error) {	
closure execution failed with error 

throw (Error)e;	}	if (e.getCause() instanceof RejectedExecutionException) {	U.warn(log, "Closure execution has been rejected (will execute in the same thread) [plc=" + plc + ", closure=" + c + ']');	try {	c.run();	return new GridFinishedFuture();	}	catch (Throwable t) {	if (t instanceof Error) {	
closure execution failed with error 

try {	if (!(c instanceof GridPlainCallable)) ctx.resource().inject(ctx.deploy().getDeployment(c.getClass().getName()), c.getClass(), c);	final ClassLoader ldr = Thread.currentThread().getContextClassLoader();	final GridWorkerFuture<R> fut = new GridWorkerFuture<>();	GridWorker w = new GridWorker(ctx.igniteInstanceName(), "closure-proc-worker", log) {	try {	if (ldr != null) fut.onDone(U.wrapThreadLoader(ldr, c));	else fut.onDone(c.call());	}	catch (Throwable e) {	
closure execution failed with error 

========================= ignite sample_3622 =========================

public void testOneNodeZeroIpv4Address() throws Exception {	startRestNode("grid1", WILDCARD_IP, defaultRestPort());	Collection<String> addrs = new LinkedList<>();	addrs.add(LOOPBACK_IP);	Collection<String> nonLoopbackAddrs = U.allLocalIps();	assertNotNull(nonLoopbackAddrs);	addrs.addAll(F.view(nonLoopbackAddrs, new IpV4AddressPredicate()));	for (String addr : addrs) {	
trying address 

========================= ignite sample_282 =========================

private void clusterConnect(Collection<UUID> nids) {	
connection successfully established to cluster with nodes 

private void clusterDisconnect() {	if (top == null) return;	top = null;	
connection to cluster was lost 

========================= ignite sample_7374 =========================

protected void checkCache(IgniteEx ignite, IgniteCache cache) {	
start cache validation 

}	}	assert notEqualsCacheVals.size() == notEqualsLocMapVals.size() : "Invalid state " + "[cacheMapVals=" + notEqualsCacheVals + ", mapVals=" + notEqualsLocMapVals + "]";	if (!notEqualsCacheVals.isEmpty()) {	for (Map.Entry<String, Long> eLocMap : notEqualsLocMapVals.entrySet()) {	String key = eLocMap.getKey();	Long mapVal = eLocMap.getValue();	Long cacheVal = notEqualsCacheVals.get(key);	U.error(log, "Got different values [key='" + key + "', cacheVal=" + cacheVal + ", localMapVal=" + mapVal + "]");	}	
local driver map contant 

}	}	assert notEqualsCacheVals.size() == notEqualsLocMapVals.size() : "Invalid state " + "[cacheMapVals=" + notEqualsCacheVals + ", mapVals=" + notEqualsLocMapVals + "]";	if (!notEqualsCacheVals.isEmpty()) {	for (Map.Entry<String, Long> eLocMap : notEqualsLocMapVals.entrySet()) {	String key = eLocMap.getKey();	Long mapVal = eLocMap.getValue();	Long cacheVal = notEqualsCacheVals.get(key);	U.error(log, "Got different values [key='" + key + "', cacheVal=" + cacheVal + ", localMapVal=" + mapVal + "]");	}	
cache content 

}	for (int k2 = 0; k2 < RANGE; k2++) {	for (int i2 = 0; i2 < KEYS_CNT; i2++) {	String key2 = "key-" + k2 + "-" + i2;	Long val = (Long)cache.get(key2);	if (val != null) log.info("Entry [key=" + key2 + ", val=" + val + "]");	}	}	throw new IllegalStateException("Cache and local map are in inconsistent state [badKeys=" + notEqualsCacheVals.keySet() + ']');	}	
cache validation successfully finished in sec 

========================= ignite sample_1023 =========================

public void testEntrySet() throws Exception {	for (int i = 0; i < 10; i++) {	
iteration 

public void testEntrySet() throws Exception {	for (int i = 0; i < 10; i++) {	final AtomicInteger cacheIdx = new AtomicInteger(0);	GridTestUtils.runMultiThreaded(new Callable<Void>() {	int idx = cacheIdx.getAndIncrement();	
use cache 

========================= ignite sample_1189 =========================

private void init() throws IgniteException {	if (initGuard.compareAndSet(false, true)) {	
initializing cache store 

private void init() throws IgniteException {	if (initGuard.compareAndSet(false, true)) {	try {	if (sesFactory != null) return;	if (!F.isEmpty(hibernateCfgPath)) {	try {	URL url = new URL(hibernateCfgPath);	sesFactory = new Configuration().configure(url).buildSessionFactory();	
configured session factory using url 

if (initGuard.compareAndSet(false, true)) {	try {	if (sesFactory != null) return;	if (!F.isEmpty(hibernateCfgPath)) {	try {	URL url = new URL(hibernateCfgPath);	sesFactory = new Configuration().configure(url).buildSessionFactory();	return;	}	catch (MalformedURLException e) {	
caught malformed url exception 

try {	URL url = new URL(hibernateCfgPath);	sesFactory = new Configuration().configure(url).buildSessionFactory();	return;	}	catch (MalformedURLException e) {	}	File cfgFile = new File(hibernateCfgPath);	if (cfgFile.exists()) {	sesFactory = new Configuration().configure(cfgFile).buildSessionFactory();	
configured session factory using file 

return;	}	catch (MalformedURLException e) {	}	File cfgFile = new File(hibernateCfgPath);	if (cfgFile.exists()) {	sesFactory = new Configuration().configure(cfgFile).buildSessionFactory();	return;	}	sesFactory = new Configuration().configure(hibernateCfgPath).buildSessionFactory();	
configured session factory using classpath resource 

}	File cfgFile = new File(hibernateCfgPath);	if (cfgFile.exists()) {	sesFactory = new Configuration().configure(cfgFile).buildSessionFactory();	return;	}	sesFactory = new Configuration().configure(hibernateCfgPath).buildSessionFactory();	}	else {	if (hibernateProps == null) {	
no hibernate configuration has been provided for store will use default 

hibernateProps = new Properties();	hibernateProps.setProperty("hibernate.connection.url", DFLT_CONN_URL);	hibernateProps.setProperty("hibernate.show_sql", DFLT_SHOW_SQL);	hibernateProps.setProperty("hibernate.hbm2ddl.auto", DFLT_HBM2DDL_AUTO);	}	Configuration cfg = new Configuration();	cfg.setProperties(hibernateProps);	assert resourceAvailable(MAPPING_RESOURCE) : MAPPING_RESOURCE;	cfg.addResource(MAPPING_RESOURCE);	sesFactory = cfg.buildSessionFactory();	
configured session factory using properties 

private boolean resourceAvailable(String name) {	InputStream cfgStream = Thread.currentThread().getContextClassLoader().getResourceAsStream(name);	if (cfgStream == null) {	
classpath resource not found 

private boolean resourceAvailable(String name) {	InputStream cfgStream = Thread.currentThread().getContextClassLoader().getResourceAsStream(name);	if (cfgStream == null) {	return false;	}	try {	cfgStream.read();	return true;	}	catch (IOException e) {	
failed to read classpath resource 

========================= ignite sample_451 =========================

public void testConcurrentStartStop() throws Exception {	awaitPartitionMapExchange();	int minorVer = ignite(0).configuration().isLateAffinityAssignment() ? 1 : 0;	checkTopologyVersion(new AffinityTopologyVersion(NODES, minorVer));	for (int i = 0; i < 5; i++) {	
iteration 

========================= ignite sample_2064 =========================

public void testLoadCacheWithDataStreamer() throws Exception {	configured = true;	try {	IgniteInClosure<Ignite> f = new IgniteInClosure<Ignite>() {	try (IgniteDataStreamer<Integer, String> dataStreamer = grid.dataStreamer(DEFAULT_CACHE_NAME)) {	dataStreamer.allowOverwrite(allowOverwrite);	for (int i = 0; i < KEYS_CNT; i++) dataStreamer.addData(i, Integer.toString(i));	}	
data loaded 

========================= ignite sample_1218 =========================

private void warnFirstEvict() {	synchronized (this) {	if (firstEvictWarn) return;	firstEvictWarn = true;	}	
evictions started cache may have reached its capacity you may wish to increase maxsize on eviction policy being used for cache evictions started cache may have reached its capacity 

private void notifyPolicy(GridCacheEntryEx e) {	assert plcEnabled;	assert plc != null;	assert !e.isInternal() : "Invalid entry for policy notification: " + e;	
notifying eviction policy with entry 

========================= ignite sample_3675 =========================

stmt.addBatch("select * from Person");	stmt.addBatch("insert into Person (_key, id, firstName, lastName, age) values " + generateValues(100, 7));	try {	stmt.executeBatch();	fail("BatchUpdateException must be thrown");	} catch(BatchUpdateException e) {	int [] updCnts = e.getUpdateCounts();	assertEquals("Invalid update counts size", BATCH_SIZE, updCnts.length);	for (int i = 0; i < BATCH_SIZE; ++i) assertEquals("Invalid update count", i != FAILED_IDX ? i + 1 : Statement.EXECUTE_FAILED, updCnts[i]);	if (!e.getMessage().contains("Given statement type does not match that declared by JDBC driver")) {	
invalid exception 

stmt.addBatch("insert into Person (_key, id, firstName, lastName, age) values (4444, 'fail', 1, 1, 1)");	stmt.addBatch("insert into Person (_key, id, firstName, lastName, age) values " + generateValues(100, 7));	try {	stmt.executeBatch();	fail("BatchUpdateException must be thrown");	} catch(BatchUpdateException e) {	int [] updCnts = e.getUpdateCounts();	assertEquals("Invalid update counts size", BATCH_SIZE, updCnts.length);	for (int i = 0; i < BATCH_SIZE; ++i) assertEquals("Invalid update count: " + i, i != FAILED_IDX ? i + 1 : Statement.EXECUTE_FAILED, updCnts[i]);	if (!e.getMessage().contains("Value conversion failed")) {	
invalid exception 

stmt.addBatch("merge into Person (_key, id, firstName, lastName, age) values (4444, 'FAIL', 1, 1, 1)");	stmt.addBatch("merge into Person (_key, id, firstName, lastName, age) values " + generateValues(100, 7));	try {	stmt.executeBatch();	fail("BatchUpdateException must be thrown");	} catch(BatchUpdateException e) {	int [] updCnts = e.getUpdateCounts();	assertEquals("Invalid update counts size", BATCH_SIZE, updCnts.length);	for (int i = 0; i < BATCH_SIZE; ++i) assertEquals("Invalid update count: " + i, i != FAILED_IDX ? i + 1 : Statement.EXECUTE_FAILED, updCnts[i]);	if (!e.getMessage().contains("Value conversion failed")) {	
invalid exception 

stmt.addBatch("insert into Person (_key, id, firstName, lastName, age) values ('p0', 0, 'Name0', 'Lastname0', 20)");	stmt.addBatch("insert into Person (_key, id, firstName, lastName, age) values " + generateValues(++idx, 7));	try {	stmt.executeBatch();	fail("BatchUpdateException must be thrown");	} catch(BatchUpdateException e) {	int [] updCnts = e.getUpdateCounts();	assertEquals("Invalid update counts size", BATCH_SIZE, updCnts.length);	for (int i = 0; i < BATCH_SIZE; ++i) assertEquals("Invalid update count: " + i, i != FAILED_IDX ? i + 1 : Statement.EXECUTE_FAILED, updCnts[i]);	if (!e.getMessage().contains("Failed to INSERT some keys because they are already in cache [keys=[p0]")) {	
invalid exception 

stmt.addBatch("merge into Person (_key, id, firstName, lastName, age) values ('p3', 3, 'Name3', 'Lastname3', 40)");	stmt.addBatch("update Person set id = 'FAIL' where age >= 30");	stmt.addBatch("merge into Person (_key, id, firstName, lastName, age) values ('p0', 2, 'Name2', 'Lastname2', 50)");	stmt.addBatch("delete from Person where FAIL <= 40");	try {	stmt.executeBatch();	fail("BatchUpdateException must be thrown");	} catch(BatchUpdateException e) {	int[] updCnts = e.getUpdateCounts();	if (!e.getMessage().contains("Value conversion failed")) {	
invalid exception 

pstmt.addBatch();	try {	pstmt.executeBatch();	fail("BatchUpdateException must be thrown");	}	catch(BatchUpdateException e) {	int [] updCnts = e.getUpdateCounts();	assertEquals("Invalid update counts size", BATCH_SIZE, updCnts.length);	for (int i = 0; i < BATCH_SIZE; ++i) assertEquals("Invalid update count", i != FAILED_IDX ? 1 : Statement.EXECUTE_FAILED, updCnts[i]);	if (!e.getMessage().contains("Value conversion failed")) {	
invalid exception 

pstmt.addBatch();	try {	int[] res = pstmt.executeBatch();	fail("BatchUpdateException must be thrown res=" + Arrays.toString(res));	}	catch(BatchUpdateException e) {	int [] updCnts = e.getUpdateCounts();	assertEquals("Invalid update counts size", BATCH_SIZE, updCnts.length);	for (int i = 0; i < BATCH_SIZE; ++i) assertEquals("Invalid update count", i != FAILED_IDX ? 1 : Statement.EXECUTE_FAILED, updCnts[i]);	if (!e.getMessage().contains("Value conversion failed")) {	
invalid exception 

pstmt.addBatch();	try {	int[] res = pstmt.executeBatch();	fail("BatchUpdateException must be thrown res=" + Arrays.toString(res));	}	catch(BatchUpdateException e) {	int [] updCnts = e.getUpdateCounts();	assertEquals("Invalid update counts size", BATCH_SIZE, updCnts.length);	for (int i = 0; i < BATCH_SIZE; ++i) assertEquals("Invalid update count", i != FAILED_IDX ? 1 : Statement.EXECUTE_FAILED, updCnts[i]);	if (!e.getMessage().contains("Data conversion error converting \"FAIL\"")) {	
invalid exception 

pstmt.addBatch();	try {	int[] res = pstmt.executeBatch();	fail("BatchUpdateException must be thrown res=" + Arrays.toString(res));	}	catch(BatchUpdateException e) {	int [] updCnts = e.getUpdateCounts();	assertEquals("Invalid update counts size", BATCH_SIZE, updCnts.length);	for (int i = 0; i < BATCH_SIZE; ++i) assertEquals("Invalid update count", i != FAILED_IDX ? 1 : Statement.EXECUTE_FAILED, updCnts[i]);	if (!e.getMessage().contains("Data conversion error converting \"FAIL\"")) {	
invalid exception 

========================= ignite sample_390 =========================

private void testTxPutRemove(TransactionConcurrency concurrency, TransactionIsolation isolation) throws Exception {	log.info("Test tx put/remove [concurrency=" + concurrency + ", isolation=" + isolation + ']');	IgniteCache<Integer, Integer> cache = jcache(0);	List<Integer> keys = testKeys(cache, 3);	Integer key1 = keys.get(0);	Integer key2 = keys.get(1);	Integer key3 = keys.get(2);	try (Transaction tx = startTx(concurrency, isolation)) {	
do tx 

private void testTxPutRemove(TransactionConcurrency concurrency, TransactionIsolation isolation) throws Exception {	log.info("Test tx put/remove [concurrency=" + concurrency + ", isolation=" + isolation + ']');	IgniteCache<Integer, Integer> cache = jcache(0);	List<Integer> keys = testKeys(cache, 3);	Integer key1 = keys.get(0);	Integer key2 = keys.get(1);	Integer key3 = keys.get(2);	try (Transaction tx = startTx(concurrency, isolation)) {	cache.put(key1, key1);	
do tx 

private void testTxPutRemove(TransactionConcurrency concurrency, TransactionIsolation isolation) throws Exception {	log.info("Test tx put/remove [concurrency=" + concurrency + ", isolation=" + isolation + ']');	IgniteCache<Integer, Integer> cache = jcache(0);	List<Integer> keys = testKeys(cache, 3);	Integer key1 = keys.get(0);	Integer key2 = keys.get(1);	Integer key3 = keys.get(2);	try (Transaction tx = startTx(concurrency, isolation)) {	cache.put(key1, key1);	cache.put(key2, key2);	
do tx remove 

Integer key1 = keys.get(0);	Integer key2 = keys.get(1);	Integer key3 = keys.get(2);	try (Transaction tx = startTx(concurrency, isolation)) {	cache.put(key1, key1);	cache.put(key2, key2);	cache.remove(key3);	expData.add(new ExpectedData(true, "writeAll", new HashMap<>(), DEFAULT_CACHE_NAME));	expData.add(new ExpectedData(true, "delete", F.<Object, Object>asMap(0, "writeAll"), DEFAULT_CACHE_NAME));	expData.add(new ExpectedData(true, "sessionEnd", F.<Object, Object>asMap(0, "writeAll", 1, "delete"), DEFAULT_CACHE_NAME));	
do tx commit 

private void testTxPut(IgniteCache<Object, Object> cache, TransactionConcurrency concurrency, TransactionIsolation isolation) throws Exception {	log.info("Test tx put [concurrency=" + concurrency + ", isolation=" + isolation + ']');	List<Integer> keys = testKeys(cache, 3);	Integer key1 = keys.get(0);	try (Transaction tx = startTx(concurrency, isolation)) {	
do tx get 

private void testTxPut(IgniteCache<Object, Object> cache, TransactionConcurrency concurrency, TransactionIsolation isolation) throws Exception {	log.info("Test tx put [concurrency=" + concurrency + ", isolation=" + isolation + ']');	List<Integer> keys = testKeys(cache, 3);	Integer key1 = keys.get(0);	try (Transaction tx = startTx(concurrency, isolation)) {	expData.add(new ExpectedData(false, "load", new HashMap(), cache.getName()));	expData.add(new ExpectedData(true, "sessionEnd", F.<Object, Object>asMap(0, "load"), cache.getName()));	cache.get(key1);	expData.clear();	
do tx put 

List<Integer> keys = testKeys(cache, 3);	Integer key1 = keys.get(0);	try (Transaction tx = startTx(concurrency, isolation)) {	expData.add(new ExpectedData(false, "load", new HashMap(), cache.getName()));	expData.add(new ExpectedData(true, "sessionEnd", F.<Object, Object>asMap(0, "load"), cache.getName()));	cache.get(key1);	expData.clear();	cache.put(key1, key1);	expData.add(new ExpectedData(true, "write", new HashMap<>(), cache.getName()));	expData.add(new ExpectedData(true, "sessionEnd", F.<Object, Object>asMap(0, "write"), cache.getName()));	
do tx commit 

expData.clear();	cache.put(key1, key1);	expData.add(new ExpectedData(true, "write", new HashMap<>(), cache.getName()));	expData.add(new ExpectedData(true, "sessionEnd", F.<Object, Object>asMap(0, "write"), cache.getName()));	tx.commit();	}	assertEquals(0, expData.size());	Integer key2 = keys.get(1);	Integer key3 = keys.get(2);	try (Transaction tx = startTx(concurrency, isolation)) {	
do tx 

cache.put(key1, key1);	expData.add(new ExpectedData(true, "write", new HashMap<>(), cache.getName()));	expData.add(new ExpectedData(true, "sessionEnd", F.<Object, Object>asMap(0, "write"), cache.getName()));	tx.commit();	}	assertEquals(0, expData.size());	Integer key2 = keys.get(1);	Integer key3 = keys.get(2);	try (Transaction tx = startTx(concurrency, isolation)) {	cache.put(key2, key2);	
do tx 

tx.commit();	}	assertEquals(0, expData.size());	Integer key2 = keys.get(1);	Integer key3 = keys.get(2);	try (Transaction tx = startTx(concurrency, isolation)) {	cache.put(key2, key2);	cache.put(key3, key3);	expData.add(new ExpectedData(true, "writeAll", new HashMap<>(), cache.getName()));	expData.add(new ExpectedData(true, "sessionEnd", F.<Object, Object>asMap(0, "writeAll"), cache.getName()));	
do tx commit 

private void testTxRemove(TransactionConcurrency concurrency, TransactionIsolation isolation) throws Exception {	log.info("Test tx remove [concurrency=" + concurrency + ", isolation=" + isolation + ']');	IgniteCache<Integer, Integer> cache = jcache(0);	List<Integer> keys = testKeys(cache, 3);	Integer key1 = keys.get(0);	try (Transaction tx = startTx(concurrency, isolation)) {	
do tx get 

private void testTxRemove(TransactionConcurrency concurrency, TransactionIsolation isolation) throws Exception {	log.info("Test tx remove [concurrency=" + concurrency + ", isolation=" + isolation + ']');	IgniteCache<Integer, Integer> cache = jcache(0);	List<Integer> keys = testKeys(cache, 3);	Integer key1 = keys.get(0);	try (Transaction tx = startTx(concurrency, isolation)) {	cache.get(key1);	
do tx remove 

private void testTxRemove(TransactionConcurrency concurrency, TransactionIsolation isolation) throws Exception {	log.info("Test tx remove [concurrency=" + concurrency + ", isolation=" + isolation + ']');	IgniteCache<Integer, Integer> cache = jcache(0);	List<Integer> keys = testKeys(cache, 3);	Integer key1 = keys.get(0);	try (Transaction tx = startTx(concurrency, isolation)) {	cache.get(key1);	cache.remove(key1, key1);	expData.add(new ExpectedData(true, "delete", new HashMap<>(), DEFAULT_CACHE_NAME));	expData.add(new ExpectedData(true, "sessionEnd", F.<Object, Object>asMap(0, "delete"), DEFAULT_CACHE_NAME));	
do tx commit 

cache.get(key1);	cache.remove(key1, key1);	expData.add(new ExpectedData(true, "delete", new HashMap<>(), DEFAULT_CACHE_NAME));	expData.add(new ExpectedData(true, "sessionEnd", F.<Object, Object>asMap(0, "delete"), DEFAULT_CACHE_NAME));	tx.commit();	}	assertEquals(0, expData.size());	Integer key2 = keys.get(1);	Integer key3 = keys.get(2);	try (Transaction tx = startTx(concurrency, isolation)) {	
do tx 

cache.remove(key1, key1);	expData.add(new ExpectedData(true, "delete", new HashMap<>(), DEFAULT_CACHE_NAME));	expData.add(new ExpectedData(true, "sessionEnd", F.<Object, Object>asMap(0, "delete"), DEFAULT_CACHE_NAME));	tx.commit();	}	assertEquals(0, expData.size());	Integer key2 = keys.get(1);	Integer key3 = keys.get(2);	try (Transaction tx = startTx(concurrency, isolation)) {	cache.remove(key2, key2);	
do tx 

tx.commit();	}	assertEquals(0, expData.size());	Integer key2 = keys.get(1);	Integer key3 = keys.get(2);	try (Transaction tx = startTx(concurrency, isolation)) {	cache.remove(key2, key2);	cache.remove(key3, key3);	expData.add(new ExpectedData(true, "deleteAll", new HashMap<>(), DEFAULT_CACHE_NAME));	expData.add(new ExpectedData(true, "sessionEnd", F.<Object, Object>asMap(0, "deleteAll"), DEFAULT_CACHE_NAME));	
do tx commit 

========================= ignite sample_1424 =========================

public void testCacheAndNodeStop() throws Exception {	final Ignite ignite = startGrid(0);	for (int i = 0; i < 3; i++) {	
iteration 

========================= ignite sample_1977 =========================

private void testLatch(Ignite creator, final Ignite other) throws Exception {	assertNull(creator.countDownLatch("latch1", 1, true, false));	assertNull(other.countDownLatch("latch1", 1, true, false));	try (IgniteCountDownLatch latch = creator.countDownLatch("latch1", 1, true, true)) {	assertNotNull(latch);	assertEquals(1, latch.count());	IgniteInternalFuture<?> fut = GridTestUtils.runAsync(new Callable<Object>() {	U.sleep(1000);	IgniteCountDownLatch latch0 = other.countDownLatch("latch1", 1, true, false);	assertEquals(1, latch0.count());	
count down latch 

assertEquals(1, latch.count());	IgniteInternalFuture<?> fut = GridTestUtils.runAsync(new Callable<Object>() {	U.sleep(1000);	IgniteCountDownLatch latch0 = other.countDownLatch("latch1", 1, true, false);	assertEquals(1, latch0.count());	latch0.countDown();	assertEquals(0, latch0.count());	return null;	}	});	
await latch 

IgniteInternalFuture<?> fut = GridTestUtils.runAsync(new Callable<Object>() {	U.sleep(1000);	IgniteCountDownLatch latch0 = other.countDownLatch("latch1", 1, true, false);	assertEquals(1, latch0.count());	latch0.countDown();	assertEquals(0, latch0.count());	return null;	}	});	assertTrue(latch.await(5000, TimeUnit.MILLISECONDS));	
finished wait 

private void testSemaphore(Ignite creator, final Ignite other) throws Exception {	assertNull(creator.semaphore("semaphore1", 1, true, false));	assertNull(other.semaphore("semaphore1", 1, true, false));	try (IgniteSemaphore semaphore = creator.semaphore("semaphore1", -1, true, true)) {	assertNotNull(semaphore);	assertEquals(-1, semaphore.availablePermits());	IgniteInternalFuture<?> fut = GridTestUtils.runAsync(new Callable<Object>() {	U.sleep(1000);	IgniteSemaphore semaphore0 = other.semaphore("semaphore1", -1, true, false);	assertEquals(-1, semaphore0.availablePermits());	
release semaphore 

assertNotNull(semaphore);	assertEquals(-1, semaphore.availablePermits());	IgniteInternalFuture<?> fut = GridTestUtils.runAsync(new Callable<Object>() {	U.sleep(1000);	IgniteSemaphore semaphore0 = other.semaphore("semaphore1", -1, true, false);	assertEquals(-1, semaphore0.availablePermits());	semaphore0.release(2);	return null;	}	});	
acquire semaphore 

assertEquals(-1, semaphore.availablePermits());	IgniteInternalFuture<?> fut = GridTestUtils.runAsync(new Callable<Object>() {	U.sleep(1000);	IgniteSemaphore semaphore0 = other.semaphore("semaphore1", -1, true, false);	assertEquals(-1, semaphore0.availablePermits());	semaphore0.release(2);	return null;	}	});	assertTrue(semaphore.tryAcquire(1, 5000, TimeUnit.MILLISECONDS));	
finished wait 

try (IgniteLock lock = creator.reentrantLock("lock1", true, false, true)) {	assertNotNull(lock);	assertFalse(lock.isLocked());	final Semaphore semaphore = new Semaphore(0);	IgniteInternalFuture<?> fut = GridTestUtils.runAsync(new Callable<Object>() {	IgniteLock lock0 = other.reentrantLock("lock1", true, false, false);	lock0.lock();	assertTrue(lock0.isLocked());	semaphore.release();	U.sleep(1000);	
release reentrant lock 

IgniteLock lock0 = other.reentrantLock("lock1", true, false, false);	lock0.lock();	assertTrue(lock0.isLocked());	semaphore.release();	U.sleep(1000);	lock0.unlock();	return null;	}	});	semaphore.acquire();	
try acquire lock 

lock0.lock();	assertTrue(lock0.isLocked());	semaphore.release();	U.sleep(1000);	lock0.unlock();	return null;	}	});	semaphore.acquire();	assertTrue(lock.tryLock(5000, TimeUnit.MILLISECONDS));	
finished wait 

assertNull(creator.queue("q1", 0, null));	assertNull(other.queue("q1", 0, null));	try (IgniteQueue<Integer> queue = creator.queue("q1", 0, new CollectionConfiguration())) {	assertNotNull(queue);	queue.add(1);	assertEquals(1, queue.poll().intValue());	IgniteInternalFuture<?> fut = GridTestUtils.runAsync(new Callable<Object>() {	U.sleep(1000);	IgniteQueue<Integer> queue0 = other.queue("q1", 0, null);	assertEquals(0, queue0.size());	
add in queue 

queue.add(1);	assertEquals(1, queue.poll().intValue());	IgniteInternalFuture<?> fut = GridTestUtils.runAsync(new Callable<Object>() {	U.sleep(1000);	IgniteQueue<Integer> queue0 = other.queue("q1", 0, null);	assertEquals(0, queue0.size());	queue0.add(2);	return null;	}	});	
try take 

assertEquals(1, queue.poll().intValue());	IgniteInternalFuture<?> fut = GridTestUtils.runAsync(new Callable<Object>() {	U.sleep(1000);	IgniteQueue<Integer> queue0 = other.queue("q1", 0, null);	assertEquals(0, queue0.size());	queue0.add(2);	return null;	}	});	assertEquals(2, queue.take().intValue());	
finished take 

========================= ignite sample_955 =========================

public void testBalance2() throws Exception {	System.setProperty(IgniteSystemProperties.IGNITE_IO_BALANCE_PERIOD, "1000");	try {	startGridsMultiThreaded(5);	client = true;	startGridsMultiThreaded(5, 5);	for (int i = 0; i < 5; i++) {	
iteration 

========================= ignite sample_754 =========================

private void processEventInactive(DiscoveryEvent evt, DiscoCache cache) {	cctx.cache().localJoinCachesContext();	
ignore event cluster is inactive 

else if (customMsg instanceof WalStateAbstractMessage && ((WalStateAbstractMessage)customMsg).needExchange()) {	exchId = exchangeId(n.id(), affinityTopologyVersion(evt), evt);	exchFut = exchangeFuture(exchId, evt, null, null, null);	}	else {	CachePartitionExchangeWorkerTask task = cctx.cache().exchangeTaskForCustomDiscoveryMessage(customMsg);	if (task != null) exchWorker.addCustomTask(task);	}	}	if (exchId != null) {	
discovery event will start exchange 

else {	CachePartitionExchangeWorkerTask task = cctx.cache().exchangeTaskForCustomDiscoveryMessage(customMsg);	if (task != null) exchWorker.addCustomTask(task);	}	}	if (exchId != null) {	exchFut.onEvent(exchId, evt, cache);	addFuture(exchFut);	}	else {	
do not start exchange for discovery event 

private boolean enterBusy() {	if (busyLock.readLock().tryLock()) return true;	
failed to enter to busy state exchange manager is stopping 

if (oldest.id().equals(cctx.localNodeId())) {	for (CacheGroupContext grp : cctx.cache().cacheGroups()) {	if (!grp.isLocal()) {	GridDhtPartitionTopology top = grp.topology();	if (top != null) cctx.affinity().checkRebalanceState(top, grp.groupId());	}	}	GridDhtPartitionsExchangeFuture lastFut = lastInitializedFut;	AffinityTopologyVersion rmtTopVer = lastFut != null ? (lastFut.isDone() ? lastFut.topologyVersion() : lastFut.initialVersion()) : AffinityTopologyVersion.NONE;	Collection<ClusterNode> rmts = cctx.discovery().remoteAliveNodesWithCaches(rmtTopVer);	
refreshing partitions from oldest node 

GridDhtPartitionTopology top = grp.topology();	if (top != null) cctx.affinity().checkRebalanceState(top, grp.groupId());	}	}	GridDhtPartitionsExchangeFuture lastFut = lastInitializedFut;	AffinityTopologyVersion rmtTopVer = lastFut != null ? (lastFut.isDone() ? lastFut.topologyVersion() : lastFut.initialVersion()) : AffinityTopologyVersion.NONE;	Collection<ClusterNode> rmts = cctx.discovery().remoteAliveNodesWithCaches(rmtTopVer);	sendAllPartitions(rmts, rmtTopVer);	}	else {	
refreshing local partitions from non oldest node 

public void dumpDebugInfo(@Nullable GridDhtPartitionsExchangeFuture exchFut) throws Exception {	AffinityTopologyVersion exchTopVer = exchFut != null ? exchFut.initialVersion() : null;	
ready affinity version 

public void dumpDebugInfo(@Nullable GridDhtPartitionsExchangeFuture exchFut) throws Exception {	AffinityTopologyVersion exchTopVer = exchFut != null ? exchFut.initialVersion() : null;	
last exchange future 

int cnt = 0;	for (AffinityReadyFuture fut : readyFuts.values()) {	U.warn(diagnosticLog, ">>> " + fut);	if (++cnt == 5) break;	}	}	IgniteDiagnosticPrepareContext diagCtx = cctx.kernalContext().cluster().diagnosticEnabled() ? new IgniteDiagnosticPrepareContext(cctx.localNodeId()) : null;	if (diagCtx != null && exchFut != null) exchFut.addDiagnosticRequest(diagCtx);	ExchangeFutureSet exchFuts = this.exchFuts;	if (exchFuts != null) {	
last exchange futures total 

}	}	if (diagCtx != null && !diagCtx.empty()) {	try {	cctx.kernalContext().closure().runLocal(new Runnable() {	diagCtx.send(cctx.kernalContext(), null);	}	}, SYSTEM_POOL);	}	catch (IgniteCheckedException e) {	
failed to submit diagnostic closure 

public void dumpLongRunningOperations(long timeout) {	try {	GridDhtPartitionsExchangeFuture lastFut = lastInitializedFut;	if (lastFut != null && !lastFut.isDone()) return;	if (U.currentTimeMillis() < nextLongRunningOpsDumpTime) return;	if (dumpLongRunningOperations0(timeout)) {	nextLongRunningOpsDumpTime = U.currentTimeMillis() + nextDumpTimeout(longRunningOpsDumpStep++, timeout);	if (IgniteSystemProperties.getBoolean(IGNITE_THREAD_DUMP_ON_EXCHANGE_TIMEOUT, false)) {	
found long running cache operations dump threads 

try {	GridDhtPartitionsExchangeFuture lastFut = lastInitializedFut;	if (lastFut != null && !lastFut.isDone()) return;	if (U.currentTimeMillis() < nextLongRunningOpsDumpTime) return;	if (dumpLongRunningOperations0(timeout)) {	nextLongRunningOpsDumpTime = U.currentTimeMillis() + nextDumpTimeout(longRunningOpsDumpStep++, timeout);	if (IgniteSystemProperties.getBoolean(IGNITE_THREAD_DUMP_ON_EXCHANGE_TIMEOUT, false)) {	U.dumpThreads(diagnosticLog);	}	if (IgniteSystemProperties.getBoolean(IGNITE_IO_DUMP_ON_TIMEOUT, false)) {	
found long running cache operations dump io statistics 

}	if (IgniteSystemProperties.getBoolean(IGNITE_IO_DUMP_ON_TIMEOUT, false)) {	if (IgniteSystemProperties.getBoolean(IgniteSystemProperties.IGNITE_IO_DUMP_ON_TIMEOUT, false)) cctx.gridIO().dumpStats();}	}	else {	nextLongRunningOpsDumpTime = 0;	longRunningOpsDumpStep = 0;	}	}	catch (Exception e) {	
failed to dump debug information 

public boolean mergeExchangesOnCoordinator(GridDhtPartitionsExchangeFuture curFut) {	if (IGNITE_EXCHANGE_MERGE_DELAY > 0) {	try {	U.sleep(IGNITE_EXCHANGE_MERGE_DELAY);	}	catch (IgniteInterruptedCheckedException e) {	
failed to wait for exchange merge thread interrupted 

if (log.isInfoEnabled()) {	log.info("Exchange merge test, waiting for version [exch=" + curFut.initialVersion() + ", waitVer=" + exchMergeTestWaitVer + ']');	}	long end = U.currentTimeMillis() + 10_000;	while (U.currentTimeMillis() < end) {	boolean found = false;	for (CachePartitionExchangeWorkerTask task : exchWorker.futQ) {	if (task instanceof GridDhtPartitionsExchangeFuture) {	GridDhtPartitionsExchangeFuture fut = (GridDhtPartitionsExchangeFuture)task;	if (exchMergeTestWaitVer.equals(fut.initialVersion())) {	
exchange merge test found awaited version 

}	this.exchMergeTestWaitVer = null;	}	synchronized (curFut.mutex()) {	int awaited = 0;	for (CachePartitionExchangeWorkerTask task : exchWorker.futQ) {	if (task instanceof GridDhtPartitionsExchangeFuture) {	GridDhtPartitionsExchangeFuture fut = (GridDhtPartitionsExchangeFuture)task;	DiscoveryEvent evt = fut.firstEvent();	if (evt.type() == EVT_DISCOVERY_CUSTOM_EVT) {	
stop merge custom event found 

int awaited = 0;	for (CachePartitionExchangeWorkerTask task : exchWorker.futQ) {	if (task instanceof GridDhtPartitionsExchangeFuture) {	GridDhtPartitionsExchangeFuture fut = (GridDhtPartitionsExchangeFuture)task;	DiscoveryEvent evt = fut.firstEvent();	if (evt.type() == EVT_DISCOVERY_CUSTOM_EVT) {	break;	}	ClusterNode node = evt.eventNode();	if (!curFut.context().supportsMergeExchanges(node)) {	
stop merge node does not support merge 

GridDhtPartitionsExchangeFuture fut = (GridDhtPartitionsExchangeFuture)task;	DiscoveryEvent evt = fut.firstEvent();	if (evt.type() == EVT_DISCOVERY_CUSTOM_EVT) {	break;	}	ClusterNode node = evt.eventNode();	if (!curFut.context().supportsMergeExchanges(node)) {	break;	}	if (evt.type() == EVT_NODE_JOINED && cctx.cache().hasCachesReceivedFromJoin(node)) {	
stop merge received caches from node 

if (log.isInfoEnabled()) {	log.info("Merge exchange future [curFut=" + curFut.initialVersion() + ", mergedFut=" + fut.initialVersion() + ", evt=" + IgniteUtils.gridEventName(fut.firstEvent().type()) + ", evtNode=" + fut.firstEvent().eventNode().id() + ", evtNodeClient=" + CU.clientNode(fut.firstEvent().eventNode())+ ']');	}	curFut.context().events().addEvent(fut.initialVersion(), fut.firstEvent(), fut.firstEventCache());	if (evt.type() == EVT_NODE_JOINED) {	if (fut.mergeJoinExchange(curFut)) awaited++;	}	}	else {	if (!task.skipForExchangeMerge()) {	
stop merge custom task found 

========================= ignite sample_3874 =========================

public AlignedBuffersDirectFileIOFactory( final IgniteLogger log, final File storePath, final int pageSize, final FileIOFactory backupFactory) {	this.log = log;	this.pageSize = pageSize;	this.backupFactory = backupFactory;	useBackupFactory = true;	fsBlockSize = IgniteNativeIoLib.getFsBlockSize(storePath.getAbsolutePath(), log);	if(!IgniteSystemProperties.getBoolean(IgniteSystemProperties.IGNITE_DIRECT_IO_ENABLED, true)) {	
direct io is explicitly disabled by system property 

========================= ignite sample_7837 =========================

initCachesData();	List<String> cacheNames = new ArrayList<>();	cacheNames.add(personCacheType.cacheName);	cacheNames.add(orgCacheType.cacheName);	cacheNames.add(accCacheType.cacheName);	for (int i = 0; i < NODES; i++) {	Ignite testNode = ignite(i);	log.info("Test node [idx=" + i + ", isClient=" + testNode.configuration().isClientMode() + "]");	for (String cacheName : cacheNames) {	cache = testNode.cache(cacheName);	
use cache 

========================= ignite sample_7499 =========================

public void checkWeakQueue() {	for (Reference itRef = refQueue.poll(); itRef != null;	itRef = refQueue.poll()) {	try {	WeakReference weakRef = (WeakReference)itRef;	AutoCloseable rsrc = refs.remove(weakRef);	if (rsrc != null) rsrc.close();	}	catch (Exception e) {	
failed to close iterator 

public void clearQueries(){	for (AutoCloseable rsrc : refs.values()) {	try {	rsrc.close();	}	catch (Exception e) {	
failed to close iterator 

========================= ignite sample_4165 =========================

try {	assert msg != null;	ClusterNode node = ctx.discovery().node(nodeId);	if (node == null) return;	boolean processed = true;	if (msg instanceof GridH2QueryRequest) onQueryRequest(node, (GridH2QueryRequest)msg);	else if (msg instanceof GridQueryNextPageRequest) onNextPageRequest(node, (GridQueryNextPageRequest)msg);	else if (msg instanceof GridQueryCancelRequest) onCancel(node, (GridQueryCancelRequest)msg);	else if (msg instanceof GridH2DmlRequest) onDmlRequest(node, (GridH2DmlRequest)msg);	else processed = false;	
processed request 

ClusterNode node = ctx.discovery().node(nodeId);	if (node == null) return;	boolean processed = true;	if (msg instanceof GridH2QueryRequest) onQueryRequest(node, (GridH2QueryRequest)msg);	else if (msg instanceof GridQueryNextPageRequest) onNextPageRequest(node, (GridQueryNextPageRequest)msg);	else if (msg instanceof GridQueryCancelRequest) onCancel(node, (GridQueryCancelRequest)msg);	else if (msg instanceof GridH2DmlRequest) onDmlRequest(node, (GridH2DmlRequest)msg);	else processed = false;	}	catch(Throwable th) {	
failed to process message 

private void sendError(ClusterNode node, long qryReqId, Throwable err) {	try {	GridQueryFailResponse msg = new GridQueryFailResponse(qryReqId, err);	if (node.isLocal()) {	
failed to run map query on local node 

private void sendError(ClusterNode node, long qryReqId, Throwable err) {	try {	GridQueryFailResponse msg = new GridQueryFailResponse(qryReqId, err);	if (node.isLocal()) {	h2.reduceQueryExecutor().onMessage(ctx.localNodeId(), msg);	}	else ctx.io().sendToGridTopic(node, GridTopic.TOPIC_QUERY, msg, QUERY_POOL);	}	catch (Exception e) {	e.addSuppressed(err);	
failed to send error message 

try {	GridH2DmlResponse rsp = new GridH2DmlResponse(reqId, updResult == null ? 0 : updResult.counter(), updResult == null ? null : updResult.errorKeys(), error);	if (log.isDebugEnabled()) log.debug("Sending: [localNodeId=" + ctx.localNodeId() + ", node=" + node.id() + ", msg=" + rsp + "]");	if (node.isLocal()) h2.reduceQueryExecutor().onMessage(ctx.localNodeId(), rsp);	else {	rsp.marshall(ctx.config().getMarshaller());	ctx.io().sendToGridTopic(node, GridTopic.TOPIC_QUERY, rsp, QUERY_POOL);	}	}	catch (Exception e) {	
failed to send message 

if (MapQueryLazyWorker.currentWorker() != null) releaseReservations();	}	}	try {	boolean loc = node.isLocal();	GridQueryNextPageResponse msg = new GridQueryNextPageResponse(qr.queryRequestId(), segmentId, qry, page, page == 0 ? res.rowCount() : -1, res.columnCount(), loc ? null : toMessages(rows, new ArrayList<Message>(res.columnCount())), loc ? rows : null, last);	if (loc) h2.reduceQueryExecutor().onMessage(ctx.localNodeId(), msg);	else ctx.io().sendToGridTopic(node, GridTopic.TOPIC_QUERY, msg, QUERY_POOL);	}	catch (IgniteCheckedException e) {	
failed to send message 

private void sendRetry(ClusterNode node, long reqId, int segmentId) {	try {	boolean loc = node.isLocal();	GridQueryNextPageResponse msg = new GridQueryNextPageResponse(reqId, segmentId, loc ? null : Collections.<Message>emptyList(), loc ? Collections.<Value[]>emptyList() : null, false);	msg.retry(h2.readyTopologyVersion());	if (loc) h2.reduceQueryExecutor().onMessage(ctx.localNodeId(), msg);	else ctx.io().sendToGridTopic(node, GridTopic.TOPIC_QUERY, msg, QUERY_POOL);	}	catch (Exception e) {	
failed to send retry message 

========================= ignite sample_7749 =========================

public void testStopWhenDisconnected() throws Exception {	clientMode = true;	Ignite client = startGrid(serverCount());	assertTrue(client.cluster().localNode().isClient());	Ignite srv = clientRouter(client);	TestTcpDiscoverySpi srvSpi = spi(srv);	final CountDownLatch disconnectLatch = new CountDownLatch(1);	final CountDownLatch reconnectLatch = new CountDownLatch(1);	final TestTcpDiscoverySpi clientSpi = spi(client);	
block reconnect 

========================= ignite sample_650 =========================

public void invalidate(@NotNull GridKernalContext ctx, @NotNull Throwable error) {	if (ctx.invalidated()) return;	ctx.invalidate();	final String gridName = ctx.igniteInstanceName();	final IgniteLogger logger = ctx.log(getClass());	
critical error with is happened all further operations will be failed and local node will be stopped 

========================= ignite sample_3589 =========================

private void scheduleNextInactivityPeriodElapsedCheck() {	final long lastRecMs = lastRecordLoggedMs.get();	final long nextPossibleAutoArchive = (lastRecMs <= 0 ? U.currentTimeMillis() : lastRecMs) + walAutoArchiveAfterInactivity;	
schedule wal rollover check at 

if (lastRecMs == 0) return;	final long elapsedMs = U.currentTimeMillis() - lastRecMs;	if (elapsedMs <= walAutoArchiveAfterInactivity) return;	if (!lastRecordLoggedMs.compareAndSet(lastRecMs, 0)) return;	final FileWriteHandle handle = currentHandle();	try {	handle.buf.close();	rollOver(handle);	}	catch (IgniteCheckedException e) {	
unable to perform segment rollover 

private FileWriteHandle initNextWriteHandle(FileWriteHandle cur) throws StorageException, IgniteCheckedException {	try {	File nextFile = pollNextFile(cur.idx);	
switching to a new wal segment 

private void deleteObsoleteRawSegments() {	FileDescriptor[] descs = scan(walArchiveDir.listFiles(WAL_SEGMENT_FILE_FILTER));	FileArchiver archiver0 = archiver;	for (FileDescriptor desc : descs) {	if (archiver0 != null && archiver0.reserved(desc.idx)) return;	if (desc.idx < lastCompressedIdx) {	
failed to remove obsolete wal segment make sure the process has enough rights exists 

private void doFlush() {	FileWriteHandle hnd = currentHandle();	try {	hnd.flush(null);	}	catch (Exception e) {	
failed to flush wal record queue 

========================= ignite sample_4077 =========================

public static void main(String[] args) {	try {	
ignite load tests execution started 

public static void main(String[] args) {	try {	LoadTestDriver driver = new IgnitePersistentStoreLoadTest();	driver.runTest("WRITE", WriteWorker.class, WriteWorker.LOGGER_NAME);	driver.runTest("BULK_WRITE", BulkWriteWorker.class, BulkWriteWorker.LOGGER_NAME);	driver.runTest("READ", ReadWorker.class, ReadWorker.LOGGER_NAME);	driver.runTest("BULK_READ", BulkReadWorker.class, BulkReadWorker.LOGGER_NAME);	
ignite load tests execution completed 

public static void main(String[] args) {	try {	LoadTestDriver driver = new IgnitePersistentStoreLoadTest();	driver.runTest("WRITE", WriteWorker.class, WriteWorker.LOGGER_NAME);	driver.runTest("BULK_WRITE", BulkWriteWorker.class, BulkWriteWorker.LOGGER_NAME);	driver.runTest("READ", ReadWorker.class, ReadWorker.LOGGER_NAME);	driver.runTest("BULK_READ", BulkReadWorker.class, BulkReadWorker.LOGGER_NAME);	}	catch (Throwable e) {	
ignite load tests execution failed 

========================= ignite sample_6949 =========================

protected static void awaitPartitionMapExchange(Ignite ignite) throws Exception {	IgniteLogger log = ignite.log();	
waiting for finishing of a partition exchange on node 

protected static void awaitPartitionMapExchange(Ignite ignite) throws Exception {	IgniteLogger log = ignite.log();	IgniteKernal kernal = (IgniteKernal)ignite;	while (true) {	boolean partitionsExchangeFinished = true;	for (IgniteInternalCache<?, ?> cache : kernal.cachesx(null)) {	
checking cache 

IgniteLogger log = ignite.log();	IgniteKernal kernal = (IgniteKernal)ignite;	while (true) {	boolean partitionsExchangeFinished = true;	for (IgniteInternalCache<?, ?> cache : kernal.cachesx(null)) {	GridCacheAdapter<?, ?> c = kernal.internalCache(cache.name());	if (!(c instanceof GridDhtCacheAdapter)) break;	GridDhtCacheAdapter<?, ?> dht = (GridDhtCacheAdapter<?, ?>)c;	GridDhtPartitionFullMap partMap = dht.topology().partitionMap(true);	for (Map.Entry<UUID, GridDhtPartitionMap> e : partMap.entrySet()) {	
checking node 

========================= ignite sample_6363 =========================

final AtomicBoolean done = new AtomicBoolean();	IgniteInternalFuture<?> fut = GridTestUtils.runAsync(new Callable<Void>() {	U.sleep(500);	stopGrid(grid.name());	done.set(true);	return null;	}	});	while (!done.get()) cache.putAll(vals);	fut.get();	
update one more time 

checkEvents(cache, new RemoveListenerFactory(), key, false, false, true, false);	log.info("Check expire event [key=" + key + ']');	checkEvents(cache, new ExpireListenerFactory(), key, false, false, false, true);	log.info("Check create/update events [key=" + key + ']');	checkEvents(cache, new CreateUpdateListenerFactory(), key, true, true, false, false);	log.info("Check create/update/remove/expire events [key=" + key + ']');	checkEvents(cache, new CreateUpdateRemoveExpireListenerFactory(), key, true, true, true, true);	}	CacheEntryListenerConfiguration<Object, Object> lsnrCfg = new MutableCacheEntryListenerConfiguration<>( new CreateUpdateRemoveExpireListenerFactory(), new TestFilterFactory(), true, false );	cache.registerCacheEntryListener(lsnrCfg);	
check filter 

checkEvents(cache, lsnrCfg, key, true, true, true, true, true);	}	finally {	stopGrid(gridCount());	}	lsnrCfg = new MutableCacheEntryListenerConfiguration<>( new CreateUpdateRemoveExpireListenerFactory(), new TestFilterFactory(), true, false );	grid = startGrid(gridCount());	try {	awaitPartitionMapExchange();	IgniteCache<Object, Object> cache = grid.cache(DEFAULT_CACHE_NAME);	
check filter for listener in configuration 

========================= ignite sample_892 =========================

private void checkQuery(String sql, IgniteCache<Object, Object> cache, int expSize, Object... args) {	
execute query 

private void checkQuery(String sql, IgniteCache<Object, Object> cache, boolean enforceJoinOrder, int expSize, Object... args) {	SqlFieldsQuery qry = new SqlFieldsQuery(sql);	qry.setDistributedJoins(true);	qry.setEnforceJoinOrder(enforceJoinOrder);	qry.setArgs(args);	
plan 

========================= ignite sample_7436 =========================

for (GridCacheVersionedFuture<?> fut : futColCp) {	if (!fut.isDone()) {	final GridCacheVersionedFuture<Boolean> mvccFut = (GridCacheVersionedFuture<Boolean>)fut;	if (mvccFut.onOwnerChanged(entry, owner)) return;	}	}	}	}	if (log.isDebugEnabled()) log.debug("Lock future not found for owner change callback (will try transaction futures) [owner=" + owner + ", entry=" + entry + ']');	if (cctx.tm().onOwnerChanged(entry, owner)) {	
found transaction for changed owner 

if (!fut.isDone()) {	final GridCacheVersionedFuture<Boolean> mvccFut = (GridCacheVersionedFuture<Boolean>)fut;	if (mvccFut.onOwnerChanged(entry, owner)) return;	}	}	}	}	if (log.isDebugEnabled()) log.debug("Lock future not found for owner change callback (will try transaction futures) [owner=" + owner + ", entry=" + entry + ']');	if (cctx.tm().onOwnerChanged(entry, owner)) {	}	
failed to find transaction for changed owner 

public boolean removeVersionedFuture(GridCacheVersionedFuture<?> fut) {	if (!fut.trackable()) return true;	Collection<GridCacheVersionedFuture<?>> cur = verFuts.get(fut.version());	if (cur == null) return false;	boolean rmv, empty;	synchronized (cur) {	rmv = cur.remove(fut);	empty = cur.isEmpty();	}	if (rmv) {	
removed future from future map 

if (!fut.trackable()) return true;	Collection<GridCacheVersionedFuture<?>> cur = verFuts.get(fut.version());	if (cur == null) return false;	boolean rmv, empty;	synchronized (cur) {	rmv = cur.remove(fut);	empty = cur.isEmpty();	}	if (rmv) {	}	
attempted to remove a non registered future has it been already removed 

if (!fut.trackable()) return true;	Collection<GridCacheVersionedFuture<?>> cur = verFuts.get(fut.version());	if (cur == null) return false;	boolean rmv, empty;	synchronized (cur) {	rmv = cur.remove(fut);	empty = cur.isEmpty();	}	if (rmv) {	}	
removed future list from futures map for lock version 

public boolean addRemoved(GridCacheContext cacheCtx, GridCacheVersion ver) {	if (cacheCtx.isNear() || cacheCtx.isLocal()) return true;	boolean ret = rmvLocks.add(ver);	
added removed lock version 

if (cacheCtx.isNear() || cand.singleImplicit()) return true;	Deque<GridCacheMvccCandidate> queue = pending.get();	if (queue == null) pending.set(queue = new ArrayDeque<>());	GridCacheMvccCandidate prev = null;	if (!queue.isEmpty()) prev = queue.getLast();	queue.add(cand);	if (prev != null) {	prev.next(cand);	cand.previous(prev);	}	
linked new candidate 

public void recheckPendingLocks() {	
rechecking pending locks for completion 

========================= ignite sample_3926 =========================

if (grid(i).affinity(DEFAULT_CACHE_NAME).isPrimaryOrBackup(grid(i).localNode(), key)) {	IgnitePair<Long> curEntryTtl = entryTtl(jcache(i), key);	assertNotNull(curEntryTtl.get1());	assertNotNull(curEntryTtl.get2());	assertEquals(ttl, (long)curEntryTtl.get1());	assertTrue(curEntryTtl.get2() > startTime);	expireTimes[i] = curEntryTtl.get2();	}	}	U.sleep(100);	
put 

}	U.sleep(100);	tx = inTx ? transactions().txStart() : null;	try {	jcache().put(key, 4);	if (tx != null) tx.commit();	}	finally {	if (tx != null) tx.close();	}	
put done 

for (int i = 0; i < secs; i++) {	try {	cache.size();	checkIteratorsCleared();	}	catch (AssertionFailedError e) {	if (i == 9) {	for (int j = 0; j < gridCount(); j++) executeOnLocalOrRemoteJvm(j, new PrintIteratorStateTask());	throw e;	}	
iterators not cleared will wait 

========================= ignite sample_1448 =========================

private void checkQuery(String sql, IgniteCache<Object, Object> cache, boolean enforceJoinOrder, int expSize) {	String plan = (String)cache.query(new SqlFieldsQuery("explain " + sql) .setDistributedJoins(true) .setEnforceJoinOrder(enforceJoinOrder)) .getAll().get(0).get(0);	
plan 

========================= ignite sample_7524 =========================

public void testRecoveryNoPageLost3() throws Exception {	try {	pageSize = 1024;	checkpointFreq = 100L;	extraCcfg = new CacheConfiguration(CACHE2_NAME);	extraCcfg.setAffinity(new RendezvousAffinityFunction(false, 32));	List<Integer> pages = null;	for (int iter = 0; iter < 5; iter++) {	
start node 

private void recoveryNoPageLost(boolean checkpoint) throws Exception {	try {	pageSize = 1024;	extraCcfg = new CacheConfiguration(CACHE2_NAME);	extraCcfg.setAffinity(new RendezvousAffinityFunction(false, 32));	List<Integer> pages = null;	AtomicInteger cnt = new AtomicInteger();	for (int iter = 0; iter < 5; iter++) {	
start node 

========================= ignite sample_1736 =========================

IgniteCache<Object, Object> clientCache = nearCache ? client.createNearCache(cacheName, new NearCacheConfiguration<>()) : client.cache(cacheName);	fail = true;	final CountDownLatch futLatch = new CountDownLatch(1);	clientCache.getAsync("key").listen(new IgniteInClosure<IgniteFuture<Object>>() {	assertTrue(fut.isDone());	try {	fut.get();	fail();	}	catch (CacheException e) {	
expected error 

========================= ignite sample_1389 =========================

}	final boolean sndOldVal = !cacheCtx.isLocal() && !cacheCtx.topology().rebalanceFinished(tx.topologyVersion());	if (sndOldVal) {	if (oldVal == null && !readOld) {	oldVal = cached.innerGet( null, tx, }	if (oldVal != null) oldVal.prepareMarshal(cacheCtx.cacheObjectContext());	txEntry.oldValue(oldVal);	}	}	catch (IgniteCheckedException e) {	
failed to get result value for cache entry 

private void readyLocks() {	
marking all local candidates as ready 

}	}	while (true) {	try {	assert txEntry.explicitVersion() == null || entry.lockedBy(txEntry.explicitVersion());	CacheLockCandidates owners = entry.readyLock(tx.xidVersion());	if (log.isDebugEnabled()) log.debug("Current lock owners for entry [owner=" + owners + ", entry=" + entry + ']');	break;	}	catch (GridCacheEntryRemovedException ignored) {	
got removed entry in future onallreplies method will retry 

private void prepare0() {	try {	if (tx.serializable() && tx.optimistic()) {	IgniteCheckedException err0;	try {	err0 = checkReadConflict(req.writes());	if (err0 == null) err0 = checkReadConflict(req.reads());	}	catch (IgniteCheckedException e) {	
failed to check entry version 

Collection<UUID> readers = cached.readers();	if (!F.isEmpty(readers)) {	for (UUID readerId : readers) {	if (readerId.equals(tx.nearNodeId())) continue;	ClusterNode readerNode = cctx.discovery().node(readerId);	if (readerNode == null || canSkipNearReader(dht, readerNode, dhtNodes)) continue;	if (log.isDebugEnabled()) log.debug("Mapping entry to near node [node=" + readerNode + ", entry=" + entry + ']');	addMapping(entry, readerNode, nearMap);	}	}	
entry has no near readers 

========================= ignite sample_3822 =========================

final Ignite client = grid(serverCount());	final IgniteCache<Integer, Person> cache = client.cache(DEFAULT_CACHE_NAME);	assertNotNull(cache);	reconnectFailover(new Callable<Void>() {	SqlQuery<Integer, Person> sqlQry = new SqlQuery<>(Person.class, "where id > 1");	try {	assertEquals(9999, cache.query(sqlQry).getAll().size());	}	catch (CacheException e) {	if (e.getCause() instanceof IgniteClientDisconnectedException) throw e;	
ignore error 

}	try {	SqlFieldsQuery fieldsQry = new SqlFieldsQuery("select avg(p.id) from Person p");	List<List<?>> res = cache.query(fieldsQry).getAll();	assertEquals(1, res.size());	Integer avg = (Integer)res.get(0).get(0);	assertEquals(5_000, avg.intValue());	}	catch (CacheException e) {	if (e.getCause() instanceof IgniteClientDisconnectedException) throw e;	
ignore error 

========================= ignite sample_7532 =========================

private void singleKeyCommit(CacheConfiguration<Object, Object> ccfg) throws Exception {	Ignite ignite = ignite(0);	IgniteCache<Object, Object> cache = ignite.createCache(ccfg);	try {	ignite(NODES - 1).createNearCache(ccfg.getName(), new NearCacheConfiguration<>());	for (int i = 1; i < NODES; i++) {	Ignite node = ignite(i);	
test node 

private void checkWaitPrimaryResponse(CacheConfiguration<Object, Object> ccfg) throws Exception {	Ignite ignite = ignite(0);	IgniteCache<Object, Object> cache = ignite.createCache(ccfg);	try {	ignite(NODES - 1).createNearCache(ccfg.getName(), new NearCacheConfiguration<>());	for (int i = 1; i < NODES; i++) {	Ignite node = ignite(i);	
test node 

private void checkOnePhaseMessages(CacheConfiguration<Object, Object> ccfg) throws Exception {	Ignite ignite = ignite(0);	IgniteCache<Object, Object> cache = ignite.createCache(ccfg);	try {	for (int i = 1; i < NODES; i++) {	Ignite node = ignite(i);	
test node 

========================= ignite sample_997 =========================

protected void clearEntry(GridCacheEntryEx e) {	ctx.shared().database().checkpointReadLock();	try {	e.clear(obsoleteVer, readers);	}	catch (IgniteCheckedException ex) {	
failed to clearlocally entry from cache will continue to clearlocally other entries 

========================= ignite sample_4172 =========================

private void checkNodes(int expCnt) {	assertEquals(expCnt, G.allGrids().size());	long topVer = -1L;	for (Ignite ignite : G.allGrids()) {	
check node 

========================= ignite sample_1370 =========================

private void freeDirectBuffers() {	ConcurrentHashMap8<Long, Thread> buffers = managedBuffers;	if (buffers == null) return;	managedBuffers = null;	
direct io buffers to be freed 

FileChannel ch = U.field(chIo, "ch");	FileDescriptor fd = U.field(ch, "fd");	int fdVal = U.field(fd, "fd");	int retVal = IgniteNativeIoLib.posix_fadvise(fdVal, 0, size, IgniteNativeIoLib.POSIX_FADV_DONTNEED);	if (retVal != 0) {	U.warn(log, "Unable to apply fadvice on WAL file descriptor [fd=" + fdVal + "]:" + IgniteNativeIoLib.strerror(retVal));	}	}	}	catch (Exception e) {	
unable to advice on wal file descriptor 

========================= ignite sample_7836 =========================

ctx.cache().onDiscoveryEvent(type, customMsg, node, nextTopVer, ctx.state().clusterState());	if (type == EVT_DISCOVERY_CUSTOM_EVT) {	for (Class cls = customMsg.getClass(); cls != null; cls = cls.getSuperclass()) {	List<CustomEventListener<DiscoveryCustomMessage>> list = customEvtLsnrs.get(cls);	if (list != null) {	for (CustomEventListener<DiscoveryCustomMessage> lsnr : list) {	try {	lsnr.onCustomEvent(nextTopVer, node, customMsg);	}	catch (Exception e) {	
failed to notify direct custom event listener 

private void checkSegmentOnStart() throws IgniteCheckedException {	assert hasRslvrs;	
starting network segment check 

private void checkSegmentOnStart() throws IgniteCheckedException {	assert hasRslvrs;	while (true) {	if (ctx.segmentation().isValidSegment()) break;	if (ctx.config().isWaitForSegmentOnStart()) {	
failed to check network segment retrying every ms 

private void checkSegmentOnStart() throws IgniteCheckedException {	assert hasRslvrs;	while (true) {	if (ctx.segmentation().isValidSegment()) break;	if (ctx.config().isWaitForSegmentOnStart()) {	U.sleep(2000);	}	else throw new IgniteCheckedException("Failed to check network segment.");	}	
finished network segment check successfully 

if (n.version().compareToIgnoreTimestamp(SERVICE_PERMISSIONS_SINCE) >= 0 && ctx.security().enabled() ) {	Boolean rmtSecurityCompatibilityEnabled = n.attribute(ATTR_SECURITY_COMPATIBILITY_MODE);	if (!F.eq(locSecurityCompatibilityEnabled, rmtSecurityCompatibilityEnabled)) {	throw new IgniteCheckedException("Local node's " + IGNITE_SECURITY_COMPATIBILITY_MODE + " property value differs from remote node's value " + "(to make sure all nodes in topology have identical Ignite security compatibility mode enabled, " + "configure system property explicitly) " + "[locSecurityCompatibilityEnabled=" + locSecurityCompatibilityEnabled + ", rmtSecurityCompatibilityEnabled=" + rmtSecurityCompatibilityEnabled + ", locNodeAddrs=" + U.addressesAsString(locNode) + ", rmtNodeAddrs=" + U.addressesAsString(n) + ", locNodeId=" + locNode.id() + ", rmtNode=" + U.toShortString(n) + "]");	}	}	if (n.version().compareToIgnoreTimestamp(SERVICE_PERMISSIONS_SINCE) < 0 && ctx.security().enabled() && (locSecurityCompatibilityEnabled == null || !locSecurityCompatibilityEnabled)) {	throw new IgniteCheckedException("Remote node does not support service security permissions. " + "To be able to join to it, local node must be started with " + IGNITE_SECURITY_COMPATIBILITY_MODE + " system property set to \"true\". " + "[locSecurityCompatibilityEnabled=" + locSecurityCompatibilityEnabled + ", locNodeAddrs=" + U.addressesAsString(locNode) + ", rmtNodeAddrs=" + U.addressesAsString(n) + ", locNodeId=" + locNode.id() + ", rmtNodeId=" + n.id() + ", " + ", rmtNodeVer" + n.version() + ", rmtNode=" + U.toShortString(n) + "]");	}	}	
finished node attributes consistency check 

case EVT_NODE_JOINED: {	assert !discoOrdered || topVer.topologyVersion() == node.order() : "Invalid topology version [topVer=" + topVer + ", node=" + node + ']';	try {	checkAttributes(F.asList(node));	}	catch (IgniteCheckedException e) {	U.warn(log, e.getMessage());	}	if (!isDaemon) {	if (!isLocDaemon) {	
added new node to topology 

try {	checkAttributes(F.asList(node));	}	catch (IgniteCheckedException e) {	U.warn(log, e.getMessage());	}	if (!isDaemon) {	if (!isLocDaemon) {	ackTopology(topVer.topologyVersion(), true);	}	
added new node to topology 

checkAttributes(F.asList(node));	}	catch (IgniteCheckedException e) {	U.warn(log, e.getMessage());	}	if (!isDaemon) {	if (!isLocDaemon) {	ackTopology(topVer.topologyVersion(), true);	}	}	
added new daemon node to topology 

if (!isLocDaemon) {	ackTopology(topVer.topologyVersion(), true);	}	}	break;	}	case EVT_NODE_LEFT: {	if (hasRslvrs) segChkWrk.scheduleSegmentCheck();	if (!isDaemon) {	if (!isLocDaemon) {	
node left topology 

}	}	break;	}	case EVT_NODE_LEFT: {	if (hasRslvrs) segChkWrk.scheduleSegmentCheck();	if (!isDaemon) {	if (!isLocDaemon) {	ackTopology(topVer.topologyVersion(), true);	}	
node left topology 

}	break;	}	case EVT_NODE_LEFT: {	if (hasRslvrs) segChkWrk.scheduleSegmentCheck();	if (!isDaemon) {	if (!isLocDaemon) {	ackTopology(topVer.topologyVersion(), true);	}	}	
daemon node left topology 

if (!isLocDaemon) {	ackTopology(topVer.topologyVersion(), true);	}	}	break;	}	case EVT_CLIENT_NODE_DISCONNECTED: {	break;	}	case EVT_CLIENT_NODE_RECONNECTED: {	
client node reconnected to topology 

break;	}	case EVT_CLIENT_NODE_RECONNECTED: {	if (!isLocDaemon) ackTopology(topVer.topologyVersion(), true);	break;	}	case EVT_NODE_FAILED: {	if (hasRslvrs) segChkWrk.scheduleSegmentCheck();	if (!isDaemon) {	if (!isLocDaemon) {	
node failed 

case EVT_CLIENT_NODE_RECONNECTED: {	if (!isLocDaemon) ackTopology(topVer.topologyVersion(), true);	break;	}	case EVT_NODE_FAILED: {	if (hasRslvrs) segChkWrk.scheduleSegmentCheck();	if (!isDaemon) {	if (!isLocDaemon) {	ackTopology(topVer.topologyVersion(), true);	}	
node failed 

if (!isLocDaemon) ackTopology(topVer.topologyVersion(), true);	break;	}	case EVT_NODE_FAILED: {	if (hasRslvrs) segChkWrk.scheduleSegmentCheck();	if (!isDaemon) {	if (!isLocDaemon) {	ackTopology(topVer.topologyVersion(), true);	}	}	
daemon node failed 

assert F.eqNodes(localNode(), node);	if (nodeSegFired) {	if (log.isDebugEnabled()) {	log.debug("Ignored node segmented event [type=EVT_NODE_SEGMENTED, " + "node=" + node + ']');	}	return;	}	nodeSegFired = true;	lastLoggedTop.set(0);	segmented = true;	
local node segmented 

assert F.eqNodes(localNode(), node);	if (nodeSegFired) {	if (log.isDebugEnabled()) {	log.debug("Ignored node segmented event [type=EVT_NODE_SEGMENTED, " + "node=" + node + ']');	}	return;	}	nodeSegFired = true;	lastLoggedTop.set(0);	segmented = true;	
local node segmented 

private void onSegmentation() {	SegmentationPolicy segPlc = ctx.config().getSegmentationPolicy();	try {	getSpi().disconnect();	}	catch (IgniteSpiException e) {	
failed to disconnect discovery spi 

private void onSegmentation() {	SegmentationPolicy segPlc = ctx.config().getSegmentationPolicy();	try {	getSpi().disconnect();	}	catch (IgniteSpiException e) {	}	switch (segPlc) {	
restarting jvm according to configured segmentation policy 

private void onSegmentation() {	SegmentationPolicy segPlc = ctx.config().getSegmentationPolicy();	try {	getSpi().disconnect();	}	catch (IgniteSpiException e) {	}	switch (segPlc) {	restartJvm();	break;	
stopping local node according to configured segmentation policy 

========================= ignite sample_3526 =========================

entry.unswap(false);	if (!cctx.isAll(entry, filter)) {	onFailed();	return false;	}	GridCacheMvccCandidate cand = addEntry(entry);	if (cand == null && isDone()) return false;	break;	}	catch (GridCacheEntryRemovedException ignored) {	
got removed entry in lockasync method will retry 

private @Nullable GridCacheMvccCandidate addEntry(GridLocalCacheEntry entry) throws GridCacheEntryRemovedException {	GridCacheMvccCandidate c = entry.addLocal( threadId, lockVer, null, null, timeout, !inTx(), inTx(), implicitSingle(), false );	entries.add(entry);	if (c == null && timeout < 0) {	
failed to acquire lock with negative timeout 

private void undoLocks() {	for (GridLocalCacheEntry e : entries) {	try {	e.removeLock(lockVer);	}	catch (GridCacheEntryRemovedException ignore) {	
got removed entry while undoing locks 

private boolean filter(GridCacheEntryEx cached) {	try {	if (!cctx.isAll(cached, filter)) {	
filter didn t pass for entry will fail lock 

private void onComplete(boolean success) {	if (!success) undoLocks();	if (onDone(success, err)) {	
completing future 

========================= ignite sample_3930 =========================

if (cfg.getConsistentId() != null) {	return new PdsFolderSettings(pstStoreBasePath, cfg.getConsistentId());	}	final String subFolder = U.maskForFileName(consistentId.toString());	final GridCacheDatabaseSharedManager.FileLockHolder oldStyleFolderLockHolder = tryLock(new File(pstStoreBasePath, subFolder));	if (oldStyleFolderLockHolder != null) return new PdsFolderSettings(pstStoreBasePath, subFolder, consistentId, oldStyleFolderLockHolder, true);	final File[] oldStyleFolders = pstStoreBasePath.listFiles(DB_SUBFOLDERS_OLD_STYLE_FILTER);	if (oldStyleFolders != null && oldStyleFolders.length != 0) {	for (File folder : oldStyleFolders) {	final String path = getPathDisplayableInfo(folder);	
there is other non empty storage folder under storage base directory 

if (oldStyleFolderLockHolder != null) return new PdsFolderSettings(pstStoreBasePath, subFolder, consistentId, oldStyleFolderLockHolder, true);	final File[] oldStyleFolders = pstStoreBasePath.listFiles(DB_SUBFOLDERS_OLD_STYLE_FILTER);	if (oldStyleFolders != null && oldStyleFolders.length != 0) {	for (File folder : oldStyleFolders) {	final String path = getPathDisplayableInfo(folder);	}	}	for (FolderCandidate next : getNodeIndexSortedCandidates(pstStoreBasePath)) {	final GridCacheDatabaseSharedManager.FileLockHolder fileLockHolder = tryLock(next.subFolderFile());	if (fileLockHolder != null) {	
successfully locked persistence storage folder 

private GridCacheDatabaseSharedManager.FileLockHolder tryLock(File dbStoreDirWithSubdirectory) {	if (!dbStoreDirWithSubdirectory.exists()) return null;	final String path = dbStoreDirWithSubdirectory.getAbsolutePath();	final GridCacheDatabaseSharedManager.FileLockHolder fileLockHolder = new GridCacheDatabaseSharedManager.FileLockHolder(path, ctx, log);	try {	fileLockHolder.tryLock(1000);	return fileLockHolder;	}	catch (IgniteCheckedException e) {	U.closeQuiet(fileLockHolder);	
unable to acquire lock to file reason 

========================= ignite sample_4125 =========================

private void testGet(String key, boolean needVer) throws Exception {	interceptor.retInterceptor = new NullGetInterceptor();	
get 

private void testGet(String key, boolean needVer) throws Exception {	interceptor.retInterceptor = new NullGetInterceptor();	IgniteCache<String, Integer> cache = jcache(0);	assertEquals(null, needVer ? cache.getEntry(key) : cache.get(key));	assertEquals(1, interceptor.invokeCnt.get());	assertEquals(0, interceptor.getMap.size());	interceptor.reset();	interceptor.retInterceptor = new OneGetInterceptor();	
get 

interceptor.reset();	interceptor.retInterceptor = new OneGetInterceptor();	assertEquals((Integer)1, needVer ? cache.getEntry(key).getValue() : cache.get(key));	assertEquals(1, interceptor.invokeCnt.get());	assertEquals(0, interceptor.getMap.size());	interceptor.reset();	interceptor.disabled = true;	cache.put(key, 100);	interceptor.disabled = false;	interceptor.retInterceptor = new NullGetInterceptor();	
get 

cache.put(key, 100);	interceptor.disabled = false;	interceptor.retInterceptor = new NullGetInterceptor();	assertEquals(null, needVer ? cache.getEntry(key) : cache.get(key));	assertEquals(1, interceptor.invokeCnt.get());	assertEquals(1, interceptor.getMap.size());	assertEquals(100, interceptor.getMap.get(key));	checkCacheValue(key, 100);	interceptor.reset();	interceptor.retInterceptor = new GetIncrementInterceptor();	
get 

checkCacheValue(key, 100);	interceptor.reset();	interceptor.retInterceptor = new GetIncrementInterceptor();	assertEquals((Integer)101, needVer ? cache.getEntry(key).getValue() : cache.get(key));	assertEquals(1, interceptor.invokeCnt.get());	assertEquals(1, interceptor.getMap.size());	assertEquals(100, interceptor.getMap.get(key));	checkCacheValue(key, 100);	interceptor.reset();	interceptor.retInterceptor = new GetIncrementInterceptor();	
getasync 

private void testCancelUpdate(String key, Operation op) throws Exception {	CacheInterceptor retInterceptor = new NullPutInterceptor();	interceptor.retInterceptor = retInterceptor;	
update 

IgniteBiTuple t = interceptor.beforePutMap.get(key);	assertEquals(null, t.get1());	assertEquals(1, t.get2());	interceptor.reset();	interceptor.disabled = true;	clearCaches();	jcache(0).put(key, 1);	checkCacheValue(key, 1);	interceptor.disabled = false;	interceptor.retInterceptor = retInterceptor;	
update 

private void testModifyUpdate(String key, Operation op) throws Exception {	CacheInterceptor retInterceptor = new PutIncrementInterceptor();	interceptor.retInterceptor = retInterceptor;	
update 

update(0, op, key, 1, null);	checkCacheValue(key, 2);	assertEquals(1, interceptor.beforePutMap.size());	IgniteBiTuple t = interceptor.beforePutMap.get(key);	assertEquals(null, t.get1());	assertEquals(1, t.get2());	assertEquals(1, interceptor.afterPutMap.size());	assertEquals(2, interceptor.afterPutMap.get(key));	interceptor.reset();	interceptor.retInterceptor = retInterceptor;	
update 

private void testCancelRemove(String key, Operation op) throws Exception {	interceptor.retInterceptor = new BeforeRemoveInterceptor(new IgniteBiTuple(true, null));	
remove 

private void testCancelRemove(String key, Operation op) throws Exception {	interceptor.retInterceptor = new BeforeRemoveInterceptor(new IgniteBiTuple(true, null));	remove(0, op, key, null, null);	checkCacheValue(key, null);	assertEquals(0, interceptor.beforeRmvMap.size());	assertEquals(null, interceptor.beforeRmvMap.get(key));	
remove 

assertEquals(0, interceptor.beforeRmvMap.size());	assertEquals(null, interceptor.beforeRmvMap.get(key));	interceptor.reset();	interceptor.disabled = true;	clearCaches();	jcache(0).put(key, 1);	checkCacheValue(key, 1);	interceptor.reset();	interceptor.disabled = false;	interceptor.retInterceptor = new BeforeRemoveInterceptor(new IgniteBiTuple(true, null));	
remove 

checkCacheValue(key, 1);	interceptor.reset();	interceptor.disabled = false;	interceptor.retInterceptor = new BeforeRemoveInterceptor(new IgniteBiTuple(true, null));	remove(0, op, key, 1, null);	checkCacheValue(key, 1);	assertEquals(1, interceptor.beforeRmvMap.size());	assertEquals(1, interceptor.beforeRmvMap.get(key));	interceptor.reset();	interceptor.retInterceptor = new BeforeRemoveInterceptor(new IgniteBiTuple(true, 1000));	
remove 

private void testRemove(String key, Operation op) throws Exception {	interceptor.retInterceptor = new BeforeRemoveInterceptor( new IgniteBiTuple(false, null));	
remove 

private void testRemove(String key, Operation op) throws Exception {	interceptor.retInterceptor = new BeforeRemoveInterceptor( new IgniteBiTuple(false, null));	remove(0, op, key, null, null);	checkCacheValue(key, null);	assertEquals(0, interceptor.beforeRmvMap.size());	assertEquals(0, interceptor.afterRmvMap.size());	
remove 

assertEquals(0, interceptor.beforeRmvMap.size());	assertEquals(0, interceptor.afterRmvMap.size());	interceptor.reset();	interceptor.disabled = true;	clearCaches();	jcache(0).put(key, 1);	checkCacheValue(key, 1);	interceptor.reset();	interceptor.disabled = false;	interceptor.retInterceptor = new BeforeRemoveInterceptor(new IgniteBiTuple(false, null));	
remove 

assertEquals(1, interceptor.beforeRmvMap.get(key));	assertEquals(1, interceptor.afterRmvMap.size());	assertEquals(1, interceptor.afterRmvMap.get(key));	interceptor.disabled = true;	clearCaches();	jcache(0).put(key, 2);	checkCacheValue(key, 2);	interceptor.reset();	interceptor.disabled = false;	interceptor.retInterceptor = new BeforeRemoveInterceptor(new IgniteBiTuple(false, 1000));	
remove 

}	else {	List<String> keys = primaryKeys(0, 2);	key1 = keys.get(0);	key2 = keys.get(1);	key3 = backupKey(0);	}	map.put(key1, 1);	map.put(key2, 2);	map.put(key3, 3);	
batch update 

assertBeforePutValue(key3, null, 3);	assertEquals(3, interceptor.afterPutMap.size());	assertEquals(2, interceptor.afterPutMap.get(key1));	assertEquals(3, interceptor.afterPutMap.get(key2));	assertEquals(4, interceptor.afterPutMap.get(key3));	interceptor.reset();	interceptor.retInterceptor = new BatchPutInterceptor1(key1);	map.put(key1, 100);	map.put(key2, 200);	map.put(key3, 300);	
batch update 

else {	List<String> keys = primaryKeys(0, 2);	key1 = keys.get(0);	key2 = keys.get(1);	key3 = backupKey(0);	}	map.put(key1, 1);	map.put(key2, 2);	map.put(key3, 3);	interceptor.retInterceptor = new BeforeRemoveInterceptor(new IgniteBiTuple(false, 999));	
batch remove 

checkCacheValue(key1, null);	checkCacheValue(key2, null);	checkCacheValue(key3, null);	assertEquals(0, interceptor.beforeRmvMap.size());	assertEquals(0, interceptor.afterRmvMap.size());	interceptor.disabled = true;	jcache(0).putAll(map);	interceptor.disabled = false;	interceptor.reset();	interceptor.retInterceptor = new BeforeRemoveInterceptor(new IgniteBiTuple(false, 999));	
batch remove 

assertEquals(3, interceptor.beforeRmvMap.get(key3));	assertEquals(3, interceptor.afterRmvMap.size());	assertEquals(1, interceptor.afterRmvMap.get(key1));	assertEquals(2, interceptor.afterRmvMap.get(key2));	assertEquals(3, interceptor.afterRmvMap.get(key3));	interceptor.disabled = true;	jcache(0).putAll(map);	interceptor.disabled = false;	interceptor.reset();	interceptor.retInterceptor = new BatchRemoveInterceptor(key1);	
batch remove 

========================= ignite sample_1777 =========================

public static void main(String[] args) throws Exception {	
starting apache ignite web console agent 

public static void main(String[] args) throws Exception {	final AgentConfiguration cfg = new AgentConfiguration();	JCommander jCommander = new JCommander(cfg);	String osName = System.getProperty("os.name").toLowerCase();	jCommander.setProgramName("ignite-web-agent." + (osName.contains("win") ? "bat" : "sh"));	try {	jCommander.parse(args);	}	catch (ParameterException pe) {	
failed to parse command line parameters 

return;	}	String prop = cfg.configPath();	AgentConfiguration propCfg = new AgentConfiguration();	try {	File f = AgentUtils.resolvePath(prop);	if (f == null) log.warn("Failed to find agent property file: {}", prop);	else propCfg.load(f.toURI().toURL());	}	catch (IOException e) {	
failed to load agent property file 

System.out.println();	System.out.println("Agent configuration:");	System.out.println(cfg);	System.out.println();	if (cfg.tokens() == null) {	String webHost;	try {	webHost = new URI(cfg.serverUri()).getHost();	}	catch (URISyntaxException e) {	
failed to parse ignite web console uri 

========================= ignite sample_7359 =========================

protected void checkSession(String mtd) {	assertNotNull(ignite);	CacheStoreSession ses = session();	assertNotNull(ses);	
cache 

========================= ignite sample_1425 =========================

public void join() {	U.join(scannerThread, log);	
grid uri deployment scanner stopped 

========================= ignite sample_7908 =========================

throw new GridClientException("Failed to obtain client credentials.", e);	}	GridClientConnection conn;	if (cfg.getProtocol() == GridClientProtocol.TCP) {	GridClientMarshaller marsh = cfg.getMarshaller();	try {	conn = new GridClientNioTcpConnection(srv, clientId, addr, sslCtx, pingExecutor, cfg.getConnectTimeout(), cfg.getPingInterval(), cfg.getPingTimeout(), cfg.isTcpNoDelay(), marsh, marshId, top, cred, keepBinariesThreadLocal());	}	catch (GridClientException e) {	if (marsh instanceof GridClientZipOptimizedMarshaller) {	
failed to connect with gridclientzipoptimizedmarshaller trying to fallback to default marshaller 

========================= ignite sample_5173 =========================

public void testAtomicSequenceInitialization() throws Exception {	int threadCnt = 3;	final AtomicInteger idx = new AtomicInteger(gridCount());	IgniteInternalFuture<?> fut = GridTestUtils.runMultiThreadedAsync(new CA() {	int id = idx.getAndIncrement();	try {	
start node 

========================= ignite sample_954 =========================

for (CacheConfiguration<Object, Object> ccfg : cacheConfigurations()) {	ccfg.setCacheStoreFactory(new TestStoreFactory());	ccfg.setReadThrough(true);	boolean near = (ccfg.getNearConfiguration() != null);	log.info("Test cache [mode=" + ccfg.getCacheMode() + ", atomicity=" + ccfg.getAtomicityMode() + ", backups=" + ccfg.getBackups() + ", near=" + near + "]");	ignite(0).createCache(ccfg);	awaitPartitionMapExchange();	try {	for (int i = 0; i < NODES; i++) {	Ignite ignite = ignite(i);	
check node 

}	try (Ignite newNode = startGrid(NODES)) {	IgniteCache<Integer, Integer> cache = newNode.cache(ccfg.getName());	TestRecordingCommunicationSpi newNodeSpi = recordGetRequests(newNode, near);	Integer key = backupKey(cache);	assertNull(cache.get(key));	List<Object> msgs = newNodeSpi.recordedMessages(false);	assertEquals(1, msgs.size());	for (int i = 0; i < NODES; i++) {	Ignite ignite = ignite(i);	
check node 

private void checkLocalRead(int nodes, CacheConfiguration<Object, Object> ccfg) throws Exception {	for (int i = 0; i < nodes; i++) {	Ignite ignite = ignite(i);	
check node 

========================= ignite sample_1219 =========================

public void testSingleGetMessage() throws Exception {	assertFalse(ignite(0).configuration().isClientMode());	assertTrue(ignite(SRVS).configuration().isClientMode());	List<CacheConfiguration<Integer, Integer>> ccfgs = cacheConfigurations();	for (int i = 0; i < ccfgs.size(); i++) {	CacheConfiguration<Integer, Integer> ccfg = ccfgs.get(i);	ccfg.setName("cache-" + i);	
test cache 

========================= ignite sample_1022 =========================

int newVal = cntr - 1;	if (putCntr) {	if (DEBUG) ignite.log().info("Before item put counter [retry=" + retry + ", isCntrPrimary=" + isCntrPrimary + ", cur=" + cntr + ", new=" + newVal + ", nearEntry=" + nearEntry(locId, RMVD_CNTR_KEY) + (isCntrPrimary ? ", dhtEntry=" + dhtEntry(locId, RMVD_CNTR_KEY) : "") + ']');	cache.put(RMVD_CNTR_KEY, newVal);	}	while (true) {	SqlQuery<String, Integer> qry = new SqlQuery<>(Integer.class, "_key != 'RMVD_CNTR_KEY' and _val >= 0");	if (DEBUG) ignite.log().info("Before executing query [retry=" + retry + ", locId=" + locId + ", txId=" + tx.xid() + ']');	Cache.Entry<String, Integer> entry = F.first(cache.query(qry).getAll());	if (entry == null) {	
queue is empty 

if (DEBUG) ignite.log().info("Before item remove [retry=" + retry + ", key=" + itemKey + ", cur=" + cntr + ", nearEntry=" + nearEntry(locId, itemKey) + ", dhtEntry=" + dhtEntry(itemPrimaryId, itemKey) + ']');	assert cache.remove(itemKey) : "Failed to remove key [locId=" + locId + ", primaryId=" + itemPrimaryId + ", key=" + itemKey + ']';	if (DEBUG) info("After item remove item [retry=" + retry + ", key=" + itemKey + ", cur=" + cntr + ", new=" + newVal + ", nearEntry=" + nearEntry(locId, itemKey) + ", dhtEntry=" + dhtEntry(itemPrimaryId, itemKey) + ']');	break;	}	else cache.remove(itemKey);	}	tx.commit();	}	catch (Error e) {	
error in test 

if (putCntr) {	if (DEBUG) ignite.log().info("Before item put counter [retry=" + retry + ", isCntrPrimary=" + isCntrPrimary + ", cur=" + cntr + ", new=" + newVal + ", nearEntry=" + nearEntry(locId, RMVD_CNTR_KEY) + (isCntrPrimary ? ", dhtEntry=" + dhtEntry(locId, RMVD_CNTR_KEY) : "") + ']');	cache.put(RMVD_CNTR_KEY, newVal);	}	if (DEBUG) ignite.log().info("Before item remove item [retry=" + retry + ", key=" + itemKey + ", cur=" + cntr + ", new=" + newVal + ", nearEntry=" + nearEntry(locId, itemKey) + ", dhtEntry=" + dhtEntry(itemPrimaryId, itemKey) + ']');	assertTrue(cache.remove(itemKey));	if (DEBUG) info("After item put item [retry=" + retry + ", key=" + itemKey + ", cur=" + cntr + ", new=" + newVal + ", nearEntry=" + nearEntry(locId, itemKey) + ", dhtEntry=" + dhtEntry(itemPrimaryId, itemKey) + ']');	tx.commit();	}	catch (Error e) {	
error in test 

========================= ignite sample_1864 =========================

public void testDefaultClosure() throws Exception {	Set<String> srvNames = new HashSet<>(NODES_CNT - 1);	for (int i = 1; i < NODES_CNT; ++i) srvNames.add(getTestIgniteInstanceName(i));	for (int i = 0 ; i < NODES_CNT; i++) {	
iteration 

public void testClientClosure() throws Exception {	for (int i = 0 ; i < NODES_CNT; i++) {	
iteration 

public void testCustomClosure() throws Exception {	for (int i = 0 ; i < NODES_CNT; i++) {	
iteration 

public void testDefaultService() throws Exception {	UUID clientNodeId = grid(0).cluster().localNode().id();	for (int i = 0 ; i < NODES_CNT; i++) {	
iteration 

public void testClientService() throws Exception {	UUID clientNodeId = grid(0).cluster().localNode().id();	for (int i = 0 ; i < NODES_CNT; i++) {	
iteration 

========================= ignite sample_2177 =========================

private void readyLocks() {	
marking local locks as ready for dht lock future 

onFailed(false);	return;	}	}	if (log.isDebugEnabled()) {	log.debug("Current lock owners [entry=" + entry + ", owners=" + owners + ", fut=" + this + ']');	}	break;	}	catch (GridCacheEntryRemovedException ignored) {	
failed to ready lock because entry was removed will renew 

private boolean filter(GridCacheEntryEx cached) {	try {	if (!cctx.isAll(cached, filter)) {	
filter didn t pass for entry will fail lock 

cctx.tm().txContext(tx);	set = cctx.tm().setTxTopologyHint(tx.topologyVersionSnapshot());	}	try {	if (err == null && !stopping) loadMissingFromStore();	}	finally {	if (set) cctx.tm().setTxTopologyHint(null);	}	if (super.onDone(success, err)) {	
completing future 

private void map(Iterable<GridDhtCacheEntry> entries) {	synchronized (this) {	if (mapped) return;	mapped = true;	}	try {	
mapping entry for dht lock future 

try {	cctx.dhtMap( nearNodeId, topVer, entry, tx == null ? lockVer : null, log, dhtMap, null);	GridCacheMvccCandidate cand = entry.candidate(lockVer);	if (cand == null) {	onFailed(false);	return;	}	break;	}	catch (GridCacheEntryRemovedException ignore) {	
got removed entry when mapping dht lock future will retry 

catch (GridCacheEntryRemovedException ignore) {	entry = cctx.dht().entryExx(entry.key(), topVer);	}	}	}	catch (GridDhtInvalidPartitionException e) {	assert false : "DHT lock should never get invalid partition [err=" + e + ", fut=" + this + ']';	}	}	if (isDone()) {	
mapping won t proceed because future is done 

private GridDhtCacheEntry addOwned(GridDhtLockRequest req, GridDhtCacheEntry e) {	while (true) {	try {	GridCacheMvccCandidate added = e.candidate(lockVer);	assert added != null;	assert added.dhtLocal();	if (added.ownerVersion() != null) req.owned(e.key(), added.ownerVersion());	break;	}	catch (GridCacheEntryRemovedException ignore) {	
got removed entry when creating dht lock request will retry 

========================= ignite sample_3766 =========================

public void testAddCacheConfigurationMultinode() throws Exception {	addTemplate = true;	final int GRID_CNT = 3;	startGridsMultiThreaded(GRID_CNT);	for (int i = 0; i < 10; i++) {	
iteration 

public void testAddCacheConfigurationMultinode() throws Exception {	addTemplate = true;	final int GRID_CNT = 3;	startGridsMultiThreaded(GRID_CNT);	for (int i = 0; i < 10; i++) {	final AtomicInteger idx = new AtomicInteger();	final int iter = i;	GridTestUtils.runMultiThreaded(new Callable<Void>() {	int node = idx.getAndIncrement() % GRID_CNT;	Ignite ignite = grid(node);	
add configuration using node 

public void testNoPartitionExchangeForTemplate() throws Exception{	final int GRID_CNT = 3;	startGridsMultiThreaded(GRID_CNT);	final CountDownLatch evtLatch = new CountDownLatch(1);	
add templates 

public void testNoPartitionExchangeForTemplate() throws Exception{	final int GRID_CNT = 3;	startGridsMultiThreaded(GRID_CNT);	final CountDownLatch evtLatch = new CountDownLatch(1);	for (int i = 0; i < GRID_CNT; i++) {	Ignite ignite = ignite(i);	ignite.events().localListen(new IgnitePredicate<Event>() {	
event 

========================= ignite sample_850 =========================

private void addCommandHandler(GridRedisCommandHandler hnd) {	assert !handlers.containsValue(hnd);	
added redis command handler 

========================= ignite sample_4959 =========================

for (int i = 0; i < 10; i++) futs.add(compute.executeAsync(taskName, taskArg));	GridClientFactory.stop(c.id(), true);	info(">>> Completed stop request.");	int failed = 0;	for (GridClientFuture<Object> fut : futs) {	try {	assertEquals(17, fut.get());	}	catch (GridClientException e) {	failed++;	
task execution failed 

========================= ignite sample_295 =========================

try {	awaitPartitionMapExchange();	doTestDeadlock(3, true, true, startKey);	doTestDeadlock(3, false, false, startKey);	doTestDeadlock(3, false, true, startKey);	doTestDeadlock(4, true, true, startKey);	doTestDeadlock(4, false, false, startKey);	doTestDeadlock(4, false, true, startKey);	}	catch (Throwable e) {	
unexpected exception 

private void doTestDeadlocksTxOnPrimary(IgniteCache cache, Object startKey) {	try {	awaitPartitionMapExchange();	doTestDeadlock(3, false, false, startKey, true);	doTestDeadlock(4, false, false, startKey, true);	}	catch (Throwable e) {	
unexpected exception 

k = incrementKey(o, 13);	involvedKeys.add(k);	entries.put(k, 2);	}	entries.put(key, 0);	log.info(">>> Performs put [node=" + ((IgniteKernal)ignite).localNode().id() + ", tx=" + tx.xid() + ", entries=" + entries + ']');	cache.putAll(entries);	tx.commit();	}	catch (Throwable e) {	
expected exception 

log.info(">>> Performs put [node=" + ((IgniteKernal)ignite).localNode().id() + ", tx=" + tx.xid() + ", entries=" + entries + ']');	cache.putAll(entries);	tx.commit();	}	catch (Throwable e) {	String stackTrace = X.getFullStackTrace(e);	log.info(stackTrace);	assertTrue("DeadlockDetection hasn't executed at "+ (threadNum - 1) + " node.", stackTrace.contains(TxDeadlockDetection.class.getName()));	if (hasCause(e, TransactionTimeoutException.class) && hasCause(e, TransactionDeadlockException.class)) {	if (deadlockErr.compareAndSet(null, cause(e, TransactionDeadlockException.class))) {	
at least one stack trace should contain 

========================= ignite sample_1608 =========================

private IgniteInternalFuture<IgniteDiagnosticInfo> sendDiagnosticMessage(UUID nodeId, IgniteClosure<GridKernalContext, IgniteDiagnosticInfo> c) {	try {	IgniteDiagnosticMessage msg = IgniteDiagnosticMessage.createRequest(marsh, c, diagFutId.getAndIncrement());	InternalDiagnosticFuture fut = new InternalDiagnosticFuture(nodeId, msg.futureId());	diagnosticFuturesMap().put(msg.futureId(), fut);	ctx.io().sendToGridTopic(nodeId, TOPIC_INTERNAL_DIAGNOSTIC, msg, GridIoPolicy.SYSTEM_POOL);	return fut;	}	catch (Exception e) {	
failed to send diagnostic message 

========================= ignite sample_3635 =========================

private File initFolder() throws IgniteSpiException {	if (initGuard.compareAndSet(false, true)) {	if (path == null) throw new IgniteSpiException("Shared file system path is null " + "(it should be configured via setPath(..) configuration property).");	
default local computer only share is used by ip finder 

========================= ignite sample_5684 =========================

IgniteException ex = new IgniteException("Failed to lock partitions " + "[jobId=" + ses.getJobId() + ", ses=" + ses + ']', e);	U.error(log, "Failed to lock partitions [jobId=" + ses.getJobId() + ", ses=" + ses + ']', e);;	finishJob(null, ex, true);	return;	}	}	if (isCancelled()) super.cancel();	if (!skipNtf) {	if (holdLsnr.onUnheld(this)) held.decrementAndGet();	else {	
ignoring job execution job was not held 

try {	if (internal && ctx.config().isPeerClassLoadingEnabled()) ctx.job().internal(true);	return job.execute();	}	finally {	if (internal && ctx.config().isPeerClassLoadingEnabled()) ctx.job().internal(false);	}	}	});	if (log.isDebugEnabled()) {	
job execution has successfully finished job res 

public void cancel(boolean sys) {	try {	super.cancel();	final ComputeJob job0 = job;	if (sys) sysCancelled = true;	if (job0 != null) {	
cancelling job 

========================= ignite sample_4703 =========================

public void start() {	A.ensure(getSingleTupleExtractor() != null || getMultipleTupleExtractor() != null, "tupleExtractor (single or multiple)");	A.notNull(getStreamer(), "streamer");	A.notNull(getIgnite(), "ignite");	A.ensure(threads > 0, "threads > 0");	log = getIgnite().log();	GridNioServerListener<byte[]> lsnr = new GridNioServerListenerAdapter<byte[]>() {	assert ses.accepted();	
accepted connection 

public void stop() {	if (srv != null) srv.stop();	
socket streaming server stopped 

========================= ignite sample_3029 =========================

List<CA> q;	synchronized (undeploys) {	q = undeploys.remove(ctx.name());	}	if (q == null) return;	int cnt = 0;	for (CA c : q) {	c.apply();	cnt++;	}	
unwound undeploys count 

try {	CacheObject v = entry.peek(null);	key0 = key.value(cache.context().cacheObjectContext(), false);	assert key0 != null : "Key cannot be null for cache entry: " + e;	val0 = CU.value(v, cache.context(), false);	}	catch (GridCacheEntryRemovedException ignore) {	return false;	}	catch (BinaryInvalidTypeException ex) {	
an attempt to undeploy cache with binary objects 

========================= ignite sample_3880 =========================

private void checkEntryProcessCall(Integer key, IgniteCache<Integer, TestValue> cache, @Nullable TransactionConcurrency concurrency, @Nullable TransactionConcurrency concurrency, @Nullable TransactionIsolation isolation, @Nullable TransactionIsolation isolation, int expCallCnt) {	Ignite ignite = cache.unwrap(Ignite.class);	ClusterNode primary = ignite.affinity(cache.getName()).mapKeyToNode(key);	assertNotNull(primary);	log.info("Check call [key=" + key + ", primary=" + primary.attribute(ATTR_IGNITE_INSTANCE_NAME) + ", concurrency=" + concurrency + ", isolation=" + isolation + "]");	Transaction tx;	TestReturnValue retVal;	
invoke 

log.info("Check call [key=" + key + ", primary=" + primary.attribute(ATTR_IGNITE_INSTANCE_NAME) + ", concurrency=" + concurrency + ", isolation=" + isolation + "]");	Transaction tx;	TestReturnValue retVal;	callCnt.set(0);	tx = startTx(cache, concurrency, isolation);	retVal = cache.invoke(key, new TestEntryProcessor(OP_UPDATE), new TestValue(Integer.MIN_VALUE));	if (tx != null) tx.commit();	assertEquals(expCallCnt, callCnt.get());	checkReturnValue(retVal, "null");	checkCacheValue(cache.getName(), key, new TestValue(0));	
invoke 

assertEquals(expCallCnt, callCnt.get());	checkReturnValue(retVal, "null");	checkCacheValue(cache.getName(), key, new TestValue(0));	callCnt.set(0);	tx = startTx(cache, concurrency, isolation);	retVal = cache.invoke(key, new TestEntryProcessor(OP_GET), new TestValue(Integer.MIN_VALUE));	if (tx != null) tx.commit();	assertEquals(expCallCnt, callCnt.get());	checkReturnValue(retVal, "0");	checkCacheValue(cache.getName(), key, new TestValue(0));	
invoke 

assertEquals(expCallCnt, callCnt.get());	checkReturnValue(retVal, "0");	checkCacheValue(cache.getName(), key, new TestValue(0));	callCnt.set(0);	tx = startTx(cache, concurrency, isolation);	retVal = cache.invoke(key, new TestEntryProcessor(OP_UPDATE), new TestValue(Integer.MIN_VALUE));	if (tx != null) tx.commit();	assertEquals(expCallCnt, callCnt.get());	checkReturnValue(retVal, "0");	checkCacheValue(cache.getName(), key, new TestValue(1));	
invoke 

========================= ignite sample_1385 =========================

public void testJobRun() throws Exception {	for (int i = 0; i < 5; i++) {	U.resolveWorkDirectory(U.defaultWorkDirectory(), "marshaller", true);	
iteration 

========================= ignite sample_1805 =========================

private void testEntityCacheReadWrite(AccessType accessType) throws Exception {	
test access type 

========================= ignite sample_431 =========================

ClassLoader clsLdr = GridUriDeploymentClassLoaderFactory.create(U.gridClassLoader(), file, log);	List<Class<? extends ComputeTask<?, ?>>> tasks = doc.getTasks(clsLdr);	List<Class<? extends ComputeTask<?, ?>>> validTasks = null;	if (!F.isEmpty(tasks)) {	validTasks = new ArrayList<>();	for (Class<? extends ComputeTask<?, ?>> task : tasks) {	if (!isAllowedTaskClass(task)) {	U.warn(log, "Failed to load task. Task should be public none-abstract class " + "(might be inner static one) that implements ComputeTask interface [taskCls=" + task + ']');	}	else {	
found grid deployment task 

========================= ignite sample_7894 =========================

idxCls = null;	}	else idx = INDEXING.inClassPath() ? U.<GridQueryIndexing>newInstance(INDEXING.className()) : null;	valCtx = new CacheQueryObjectValueContext(ctx);	ioLsnr = new GridMessageListener() {	if (msg instanceof SchemaOperationStatusMessage) {	SchemaOperationStatusMessage msg0 = (SchemaOperationStatusMessage)msg;	msg0.senderNodeId(nodeId);	processStatusMessage(msg0);	}	
unsupported io message 

}	else assert false;	}	}	}	break;	}	}	registerCache0(cacheName, schemaName, cctx, cands);	if (!mustDeserializeClss.isEmpty()) {	
some classes in query configuration cannot be written in binary format because they either implement externalizable interface or have writeobject readobject methods instances of these classes will be deserialized in order to build indexes please ensure that all nodes have these classes in classpath to enable binary serialization either implement interface or set explicit serializer using binarytypeconfiguration setserializer method 

Map.Entry<QueryIndexKey, QueryIndexDescriptorImpl> idxEntry = idxIt.next();	if (F.eq(cacheName, idxEntry.getValue().typeDescriptor().cacheName())) idxIt.remove();	}	for (SchemaOperation op : schemaOps.values()) {	if (op.started()) op.manager().worker().cancel();	}	try {	idx.unregisterCache(cctx, destroy);	}	catch (Exception e) {	
failed to clear indexing on cache unregister will ignore 

========================= ignite sample_4892 =========================

private void populateCacheEmployee() {	
demo start employees population with data 

for (int i = 0, n = 1; i < DEP_CNT; i++, n++) {	cacheDepartment.put(i, new Department(n, rnd.nextInt(CNTR_CNT), "Department #" + n));	double r = rnd.nextDouble();	cacheEmployee.put(i, new Employee(i, rnd.nextInt(DEP_CNT), null, "First name manager #" + n, "Last name manager #" + n, "Email manager #" + n, "Phone number manager #" + n, new java.sql.Date((long)(r * range)), "Job manager #" + n, 1000 + AgentDemoUtils.round(r * 4000, 2)));	}	for (int i = 0, n = 1; i < EMPL_CNT; i++, n++) {	Integer depId = rnd.nextInt(DEP_CNT);	double r = rnd.nextDouble();	cacheEmployee.put(i, new Employee(i, depId, depId, "First name employee #" + n, "Last name employee #" + n, "Email employee #" + n, "Phone number employee #" + n, new java.sql.Date((long)(r * range)), "Job employee #" + n, 500 + AgentDemoUtils.round(r * 2000, 2)));	}	
demo finished employees population 

private void populateCacheCar() {	
demo start cars population 

private void populateCacheCar() {	IgniteCache<Integer, Parking> cacheParking = ignite.cache(PARKING_CACHE_NAME);	for (int i = 0, n = 1; i < PARK_CNT; i++, n++) cacheParking.put(i, new Parking(i, "Parking #" + n, n * 10));	IgniteCache<Integer, Car> cacheCar = ignite.cache(CAR_CACHE_NAME);	for (int i = 0, n = 1; i < CAR_CNT; i++, n++) cacheCar.put(i, new Car(i, rnd.nextInt(PARK_CNT), "Car #" + n));	
demo finished cars population 

========================= ignite sample_7355 =========================

public static void setUpClass() {	if (CassandraHelper.useEmbeddedCassandra()) {	try {	CassandraHelper.startEmbeddedCassandra(LOGGER);	}	catch (Throwable e) {	throw new RuntimeException("Failed to start embedded Cassandra instance", e);	}	}	
testing admin connection to cassandra 

public static void setUpClass() {	if (CassandraHelper.useEmbeddedCassandra()) {	try {	CassandraHelper.startEmbeddedCassandra(LOGGER);	}	catch (Throwable e) {	throw new RuntimeException("Failed to start embedded Cassandra instance", e);	}	}	CassandraHelper.testAdminConnection();	
testing regular connection to cassandra 

if (CassandraHelper.useEmbeddedCassandra()) {	try {	CassandraHelper.startEmbeddedCassandra(LOGGER);	}	catch (Throwable e) {	throw new RuntimeException("Failed to start embedded Cassandra instance", e);	}	}	CassandraHelper.testAdminConnection();	CassandraHelper.testRegularConnection();	
dropping all artifacts from previous tests execution session 

try {	CassandraHelper.startEmbeddedCassandra(LOGGER);	}	catch (Throwable e) {	throw new RuntimeException("Failed to start embedded Cassandra instance", e);	}	}	CassandraHelper.testAdminConnection();	CassandraHelper.testRegularConnection();	CassandraHelper.dropTestKeyspaces();	
start tests execution 

try {	CassandraHelper.dropTestKeyspaces();	}	finally {	CassandraHelper.releaseCassandraResources();	if (CassandraHelper.useEmbeddedCassandra()) {	try {	CassandraHelper.stopEmbeddedCassandra();	}	catch (Throwable e) {	
failed to stop embedded cassandra instance 

public void primitiveStrategyTest() {	Ignition.stopAll(true);	Map<Long, Long> longMap = TestsHelper.generateLongsMap();	Map<String, String> strMap = TestsHelper.generateStringsMap();	
running primitive strategy write tests 

public void primitiveStrategyTest() {	Ignition.stopAll(true);	Map<Long, Long> longMap = TestsHelper.generateLongsMap();	Map<String, String> strMap = TestsHelper.generateStringsMap();	try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/primitive/ignite-config.xml")) {	IgniteCache<Long, Long> longCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Long>("cache1"));	IgniteCache<String, String> strCache = ignite.getOrCreateCache(new CacheConfiguration<String, String>("cache2"));	
running single operation write tests 

public void primitiveStrategyTest() {	Ignition.stopAll(true);	Map<Long, Long> longMap = TestsHelper.generateLongsMap();	Map<String, String> strMap = TestsHelper.generateStringsMap();	try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/primitive/ignite-config.xml")) {	IgniteCache<Long, Long> longCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Long>("cache1"));	IgniteCache<String, String> strCache = ignite.getOrCreateCache(new CacheConfiguration<String, String>("cache2"));	longCache.put(1L, 1L);	strCache.put("1", "1");	
single operation write tests passed 

public void primitiveStrategyTest() {	Ignition.stopAll(true);	Map<Long, Long> longMap = TestsHelper.generateLongsMap();	Map<String, String> strMap = TestsHelper.generateStringsMap();	try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/primitive/ignite-config.xml")) {	IgniteCache<Long, Long> longCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Long>("cache1"));	IgniteCache<String, String> strCache = ignite.getOrCreateCache(new CacheConfiguration<String, String>("cache2"));	longCache.put(1L, 1L);	strCache.put("1", "1");	
running bulk operation write tests 

Ignition.stopAll(true);	Map<Long, Long> longMap = TestsHelper.generateLongsMap();	Map<String, String> strMap = TestsHelper.generateStringsMap();	try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/primitive/ignite-config.xml")) {	IgniteCache<Long, Long> longCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Long>("cache1"));	IgniteCache<String, String> strCache = ignite.getOrCreateCache(new CacheConfiguration<String, String>("cache2"));	longCache.put(1L, 1L);	strCache.put("1", "1");	longCache.putAll(longMap);	strCache.putAll(strMap);	
bulk operation write tests passed 

Map<Long, Long> longMap = TestsHelper.generateLongsMap();	Map<String, String> strMap = TestsHelper.generateStringsMap();	try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/primitive/ignite-config.xml")) {	IgniteCache<Long, Long> longCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Long>("cache1"));	IgniteCache<String, String> strCache = ignite.getOrCreateCache(new CacheConfiguration<String, String>("cache2"));	longCache.put(1L, 1L);	strCache.put("1", "1");	longCache.putAll(longMap);	strCache.putAll(strMap);	}	
primitive strategy write tests passed 

try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/primitive/ignite-config.xml")) {	IgniteCache<Long, Long> longCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Long>("cache1"));	IgniteCache<String, String> strCache = ignite.getOrCreateCache(new CacheConfiguration<String, String>("cache2"));	longCache.put(1L, 1L);	strCache.put("1", "1");	longCache.putAll(longMap);	strCache.putAll(strMap);	}	Ignition.stopAll(true);	try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/primitive/ignite-config.xml")) {	
running primitive strategy read tests 

IgniteCache<String, String> strCache = ignite.getOrCreateCache(new CacheConfiguration<String, String>("cache2"));	longCache.put(1L, 1L);	strCache.put("1", "1");	longCache.putAll(longMap);	strCache.putAll(strMap);	}	Ignition.stopAll(true);	try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/primitive/ignite-config.xml")) {	IgniteCache<Long, Long> longCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Long>("cache1"));	IgniteCache<String, String> strCache = ignite.getOrCreateCache(new CacheConfiguration<String, String>("cache2"));	
running single operation read tests 

strCache.putAll(strMap);	}	Ignition.stopAll(true);	try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/primitive/ignite-config.xml")) {	IgniteCache<Long, Long> longCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Long>("cache1"));	IgniteCache<String, String> strCache = ignite.getOrCreateCache(new CacheConfiguration<String, String>("cache2"));	Long longVal = longCache.get(1L);	if (!longVal.equals(longMap.get(1L))) throw new RuntimeException("Long value was incorrectly deserialized from Cassandra");	String strVal = strCache.get("1");	if (!strVal.equals(strMap.get("1"))) throw new RuntimeException("String value was incorrectly deserialized from Cassandra");	
single operation read tests passed 

strCache.putAll(strMap);	}	Ignition.stopAll(true);	try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/primitive/ignite-config.xml")) {	IgniteCache<Long, Long> longCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Long>("cache1"));	IgniteCache<String, String> strCache = ignite.getOrCreateCache(new CacheConfiguration<String, String>("cache2"));	Long longVal = longCache.get(1L);	if (!longVal.equals(longMap.get(1L))) throw new RuntimeException("Long value was incorrectly deserialized from Cassandra");	String strVal = strCache.get("1");	if (!strVal.equals(strMap.get("1"))) throw new RuntimeException("String value was incorrectly deserialized from Cassandra");	
running bulk operation read tests 

IgniteCache<Long, Long> longCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Long>("cache1"));	IgniteCache<String, String> strCache = ignite.getOrCreateCache(new CacheConfiguration<String, String>("cache2"));	Long longVal = longCache.get(1L);	if (!longVal.equals(longMap.get(1L))) throw new RuntimeException("Long value was incorrectly deserialized from Cassandra");	String strVal = strCache.get("1");	if (!strVal.equals(strMap.get("1"))) throw new RuntimeException("String value was incorrectly deserialized from Cassandra");	Map<Long, Long> longMap1 = longCache.getAll(longMap.keySet());	if (!TestsHelper.checkMapsEqual(longMap, longMap1)) throw new RuntimeException("Long values batch was incorrectly deserialized from Cassandra");	Map<String, String> strMap1 = strCache.getAll(strMap.keySet());	if (!TestsHelper.checkMapsEqual(strMap, strMap1)) throw new RuntimeException("String values batch was incorrectly deserialized from Cassandra");	
bulk operation read tests passed 

IgniteCache<Long, Long> longCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Long>("cache1"));	IgniteCache<String, String> strCache = ignite.getOrCreateCache(new CacheConfiguration<String, String>("cache2"));	Long longVal = longCache.get(1L);	if (!longVal.equals(longMap.get(1L))) throw new RuntimeException("Long value was incorrectly deserialized from Cassandra");	String strVal = strCache.get("1");	if (!strVal.equals(strMap.get("1"))) throw new RuntimeException("String value was incorrectly deserialized from Cassandra");	Map<Long, Long> longMap1 = longCache.getAll(longMap.keySet());	if (!TestsHelper.checkMapsEqual(longMap, longMap1)) throw new RuntimeException("Long values batch was incorrectly deserialized from Cassandra");	Map<String, String> strMap1 = strCache.getAll(strMap.keySet());	if (!TestsHelper.checkMapsEqual(strMap, strMap1)) throw new RuntimeException("String values batch was incorrectly deserialized from Cassandra");	
primitive strategy read tests passed 

IgniteCache<Long, Long> longCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Long>("cache1"));	IgniteCache<String, String> strCache = ignite.getOrCreateCache(new CacheConfiguration<String, String>("cache2"));	Long longVal = longCache.get(1L);	if (!longVal.equals(longMap.get(1L))) throw new RuntimeException("Long value was incorrectly deserialized from Cassandra");	String strVal = strCache.get("1");	if (!strVal.equals(strMap.get("1"))) throw new RuntimeException("String value was incorrectly deserialized from Cassandra");	Map<Long, Long> longMap1 = longCache.getAll(longMap.keySet());	if (!TestsHelper.checkMapsEqual(longMap, longMap1)) throw new RuntimeException("Long values batch was incorrectly deserialized from Cassandra");	Map<String, String> strMap1 = strCache.getAll(strMap.keySet());	if (!TestsHelper.checkMapsEqual(strMap, strMap1)) throw new RuntimeException("String values batch was incorrectly deserialized from Cassandra");	
running primitive strategy delete tests 

String strVal = strCache.get("1");	if (!strVal.equals(strMap.get("1"))) throw new RuntimeException("String value was incorrectly deserialized from Cassandra");	Map<Long, Long> longMap1 = longCache.getAll(longMap.keySet());	if (!TestsHelper.checkMapsEqual(longMap, longMap1)) throw new RuntimeException("Long values batch was incorrectly deserialized from Cassandra");	Map<String, String> strMap1 = strCache.getAll(strMap.keySet());	if (!TestsHelper.checkMapsEqual(strMap, strMap1)) throw new RuntimeException("String values batch was incorrectly deserialized from Cassandra");	longCache.remove(1L);	longCache.removeAll(longMap.keySet());	strCache.remove("1");	strCache.removeAll(strMap.keySet());	
primitive strategy delete tests passed 

public void blobStrategyTest() {	Ignition.stopAll(true);	Map<Long, Long> longMap = TestsHelper.generateLongsMap();	Map<Long, Person> personMap = TestsHelper.generateLongsPersonsMap();	
running blob strategy write tests 

public void blobStrategyTest() {	Ignition.stopAll(true);	Map<Long, Long> longMap = TestsHelper.generateLongsMap();	Map<Long, Person> personMap = TestsHelper.generateLongsPersonsMap();	try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/blob/ignite-config.xml")) {	IgniteCache<Long, Long> longCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Long>("cache1"));	IgniteCache<Long, Person> personCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Person>("cache2"));	
running single operation write tests 

public void blobStrategyTest() {	Ignition.stopAll(true);	Map<Long, Long> longMap = TestsHelper.generateLongsMap();	Map<Long, Person> personMap = TestsHelper.generateLongsPersonsMap();	try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/blob/ignite-config.xml")) {	IgniteCache<Long, Long> longCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Long>("cache1"));	IgniteCache<Long, Person> personCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Person>("cache2"));	longCache.put(1L, 1L);	personCache.put(1L, TestsHelper.generateRandomPerson(1L));	
single operation write tests passed 

public void blobStrategyTest() {	Ignition.stopAll(true);	Map<Long, Long> longMap = TestsHelper.generateLongsMap();	Map<Long, Person> personMap = TestsHelper.generateLongsPersonsMap();	try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/blob/ignite-config.xml")) {	IgniteCache<Long, Long> longCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Long>("cache1"));	IgniteCache<Long, Person> personCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Person>("cache2"));	longCache.put(1L, 1L);	personCache.put(1L, TestsHelper.generateRandomPerson(1L));	
running bulk operation write tests 

Ignition.stopAll(true);	Map<Long, Long> longMap = TestsHelper.generateLongsMap();	Map<Long, Person> personMap = TestsHelper.generateLongsPersonsMap();	try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/blob/ignite-config.xml")) {	IgniteCache<Long, Long> longCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Long>("cache1"));	IgniteCache<Long, Person> personCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Person>("cache2"));	longCache.put(1L, 1L);	personCache.put(1L, TestsHelper.generateRandomPerson(1L));	longCache.putAll(longMap);	personCache.putAll(personMap);	
bulk operation write tests passed 

Map<Long, Long> longMap = TestsHelper.generateLongsMap();	Map<Long, Person> personMap = TestsHelper.generateLongsPersonsMap();	try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/blob/ignite-config.xml")) {	IgniteCache<Long, Long> longCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Long>("cache1"));	IgniteCache<Long, Person> personCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Person>("cache2"));	longCache.put(1L, 1L);	personCache.put(1L, TestsHelper.generateRandomPerson(1L));	longCache.putAll(longMap);	personCache.putAll(personMap);	}	
blob strategy write tests passed 

try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/blob/ignite-config.xml")) {	IgniteCache<Long, Long> longCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Long>("cache1"));	IgniteCache<Long, Person> personCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Person>("cache2"));	longCache.put(1L, 1L);	personCache.put(1L, TestsHelper.generateRandomPerson(1L));	longCache.putAll(longMap);	personCache.putAll(personMap);	}	Ignition.stopAll(true);	try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/blob/ignite-config.xml")) {	
running blob strategy read tests 

IgniteCache<Long, Person> personCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Person>("cache2"));	longCache.put(1L, 1L);	personCache.put(1L, TestsHelper.generateRandomPerson(1L));	longCache.putAll(longMap);	personCache.putAll(personMap);	}	Ignition.stopAll(true);	try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/blob/ignite-config.xml")) {	IgniteCache<Long, Long> longCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Long>("cache1"));	IgniteCache<Long, Person> personCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Person>("cache2"));	
running single operation read tests 

personCache.putAll(personMap);	}	Ignition.stopAll(true);	try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/blob/ignite-config.xml")) {	IgniteCache<Long, Long> longCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Long>("cache1"));	IgniteCache<Long, Person> personCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Person>("cache2"));	Long longVal = longCache.get(1L);	if (!longVal.equals(longMap.get(1L))) throw new RuntimeException("Long value was incorrectly deserialized from Cassandra");	Person person = personCache.get(1L);	if (!person.equals(personMap.get(1L))) throw new RuntimeException("Person value was incorrectly deserialized from Cassandra");	
single operation read tests passed 

personCache.putAll(personMap);	}	Ignition.stopAll(true);	try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/blob/ignite-config.xml")) {	IgniteCache<Long, Long> longCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Long>("cache1"));	IgniteCache<Long, Person> personCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Person>("cache2"));	Long longVal = longCache.get(1L);	if (!longVal.equals(longMap.get(1L))) throw new RuntimeException("Long value was incorrectly deserialized from Cassandra");	Person person = personCache.get(1L);	if (!person.equals(personMap.get(1L))) throw new RuntimeException("Person value was incorrectly deserialized from Cassandra");	
running bulk operation read tests 

IgniteCache<Long, Long> longCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Long>("cache1"));	IgniteCache<Long, Person> personCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Person>("cache2"));	Long longVal = longCache.get(1L);	if (!longVal.equals(longMap.get(1L))) throw new RuntimeException("Long value was incorrectly deserialized from Cassandra");	Person person = personCache.get(1L);	if (!person.equals(personMap.get(1L))) throw new RuntimeException("Person value was incorrectly deserialized from Cassandra");	Map<Long, Long> longMap1 = longCache.getAll(longMap.keySet());	if (!TestsHelper.checkMapsEqual(longMap, longMap1)) throw new RuntimeException("Long values batch was incorrectly deserialized from Cassandra");	Map<Long, Person> personMap1 = personCache.getAll(personMap.keySet());	if (!TestsHelper.checkPersonMapsEqual(personMap, personMap1, false)) throw new RuntimeException("Person values batch was incorrectly deserialized from Cassandra");	
bulk operation read tests passed 

IgniteCache<Long, Long> longCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Long>("cache1"));	IgniteCache<Long, Person> personCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Person>("cache2"));	Long longVal = longCache.get(1L);	if (!longVal.equals(longMap.get(1L))) throw new RuntimeException("Long value was incorrectly deserialized from Cassandra");	Person person = personCache.get(1L);	if (!person.equals(personMap.get(1L))) throw new RuntimeException("Person value was incorrectly deserialized from Cassandra");	Map<Long, Long> longMap1 = longCache.getAll(longMap.keySet());	if (!TestsHelper.checkMapsEqual(longMap, longMap1)) throw new RuntimeException("Long values batch was incorrectly deserialized from Cassandra");	Map<Long, Person> personMap1 = personCache.getAll(personMap.keySet());	if (!TestsHelper.checkPersonMapsEqual(personMap, personMap1, false)) throw new RuntimeException("Person values batch was incorrectly deserialized from Cassandra");	
blob strategy read tests passed 

IgniteCache<Long, Long> longCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Long>("cache1"));	IgniteCache<Long, Person> personCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Person>("cache2"));	Long longVal = longCache.get(1L);	if (!longVal.equals(longMap.get(1L))) throw new RuntimeException("Long value was incorrectly deserialized from Cassandra");	Person person = personCache.get(1L);	if (!person.equals(personMap.get(1L))) throw new RuntimeException("Person value was incorrectly deserialized from Cassandra");	Map<Long, Long> longMap1 = longCache.getAll(longMap.keySet());	if (!TestsHelper.checkMapsEqual(longMap, longMap1)) throw new RuntimeException("Long values batch was incorrectly deserialized from Cassandra");	Map<Long, Person> personMap1 = personCache.getAll(personMap.keySet());	if (!TestsHelper.checkPersonMapsEqual(personMap, personMap1, false)) throw new RuntimeException("Person values batch was incorrectly deserialized from Cassandra");	
running blob strategy delete tests 

Person person = personCache.get(1L);	if (!person.equals(personMap.get(1L))) throw new RuntimeException("Person value was incorrectly deserialized from Cassandra");	Map<Long, Long> longMap1 = longCache.getAll(longMap.keySet());	if (!TestsHelper.checkMapsEqual(longMap, longMap1)) throw new RuntimeException("Long values batch was incorrectly deserialized from Cassandra");	Map<Long, Person> personMap1 = personCache.getAll(personMap.keySet());	if (!TestsHelper.checkPersonMapsEqual(personMap, personMap1, false)) throw new RuntimeException("Person values batch was incorrectly deserialized from Cassandra");	longCache.remove(1L);	longCache.removeAll(longMap.keySet());	personCache.remove(1L);	personCache.removeAll(personMap.keySet());	
blob strategy delete tests passed 

IgniteCache<Long, PojoPerson> personCache = ignite.getOrCreateCache("cache2");	assert ignite.configuration().getMarshaller() instanceof BinaryMarshaller;	personCache.put(1L, new PojoPerson(1, "name"));	assert personCache.withKeepBinary().get(1L) instanceof BinaryObject;	}	Ignition.stopAll(true);	try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/loadall_blob/ignite-config.xml")) {	IgniteCache<Long, PojoPerson> personCache = ignite.getOrCreateCache("cache2");	personCache.loadCache(null, null);	PojoPerson person = personCache.get(1L);	
loadcache tests passed 

public void pojoStrategyTest() {	Ignition.stopAll(true);	
running pojo strategy write tests 

Map<Long, ProductOrder> ordersMap = TestsHelper.generateOrdersMap();	Product product = TestsHelper.generateRandomProduct(-1L);	ProductOrder order = TestsHelper.generateRandomOrder(-1L);	try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/pojo/ignite-config.xml")) {	IgniteCache<Long, Person> personCache1 = ignite.getOrCreateCache(new CacheConfiguration<Long, Person>("cache1"));	IgniteCache<PersonId, Person> personCache2 = ignite.getOrCreateCache(new CacheConfiguration<PersonId, Person>("cache2"));	IgniteCache<PersonId, Person> personCache3 = ignite.getOrCreateCache(new CacheConfiguration<PersonId, Person>("cache3"));	IgniteCache<PersonId, Person> personCache4 = ignite.getOrCreateCache(new CacheConfiguration<PersonId, Person>("cache4"));	IgniteCache<Long, Product> productCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Product>("product"));	IgniteCache<Long, ProductOrder> orderCache = ignite.getOrCreateCache(new CacheConfiguration<Long, ProductOrder>("order"));	
running single operation write tests 

IgniteCache<Long, Product> productCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Product>("product"));	IgniteCache<Long, ProductOrder> orderCache = ignite.getOrCreateCache(new CacheConfiguration<Long, ProductOrder>("order"));	personCache1.put(1L, TestsHelper.generateRandomPerson(1L));	PersonId id = TestsHelper.generateRandomPersonId();	personCache2.put(id, TestsHelper.generateRandomPerson(id.getPersonNumber()));	id = TestsHelper.generateRandomPersonId();	personCache3.put(id, TestsHelper.generateRandomPerson(id.getPersonNumber()));	personCache4.put(id, TestsHelper.generateRandomPerson(id.getPersonNumber()));	productCache.put(product.getId(), product);	orderCache.put(order.getId(), order);	
single operation write tests passed 

IgniteCache<Long, Product> productCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Product>("product"));	IgniteCache<Long, ProductOrder> orderCache = ignite.getOrCreateCache(new CacheConfiguration<Long, ProductOrder>("order"));	personCache1.put(1L, TestsHelper.generateRandomPerson(1L));	PersonId id = TestsHelper.generateRandomPersonId();	personCache2.put(id, TestsHelper.generateRandomPerson(id.getPersonNumber()));	id = TestsHelper.generateRandomPersonId();	personCache3.put(id, TestsHelper.generateRandomPerson(id.getPersonNumber()));	personCache4.put(id, TestsHelper.generateRandomPerson(id.getPersonNumber()));	productCache.put(product.getId(), product);	orderCache.put(order.getId(), order);	
running bulk operation write tests 

personCache3.put(id, TestsHelper.generateRandomPerson(id.getPersonNumber()));	personCache4.put(id, TestsHelper.generateRandomPerson(id.getPersonNumber()));	productCache.put(product.getId(), product);	orderCache.put(order.getId(), order);	personCache1.putAll(personMap1);	personCache2.putAll(personMap2);	personCache3.putAll(personMap2);	personCache4.putAll(personMap2);	productCache.putAll(productsMap);	orderCache.putAll(ordersMap);	
bulk operation write tests passed 

personCache4.put(id, TestsHelper.generateRandomPerson(id.getPersonNumber()));	productCache.put(product.getId(), product);	orderCache.put(order.getId(), order);	personCache1.putAll(personMap1);	personCache2.putAll(personMap2);	personCache3.putAll(personMap2);	personCache4.putAll(personMap2);	productCache.putAll(productsMap);	orderCache.putAll(ordersMap);	}	
pojo strategy write tests passed 

orderCache.put(order.getId(), order);	personCache1.putAll(personMap1);	personCache2.putAll(personMap2);	personCache3.putAll(personMap2);	personCache4.putAll(personMap2);	productCache.putAll(productsMap);	orderCache.putAll(ordersMap);	}	Ignition.stopAll(true);	try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/pojo/ignite-config.xml")) {	
running pojo strategy read tests 

orderCache.putAll(ordersMap);	}	Ignition.stopAll(true);	try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/pojo/ignite-config.xml")) {	IgniteCache<Long, Person> personCache1 = ignite.getOrCreateCache(new CacheConfiguration<Long, Person>("cache1"));	IgniteCache<PersonId, Person> personCache2 = ignite.getOrCreateCache(new CacheConfiguration<PersonId, Person>("cache2"));	IgniteCache<PersonId, Person> personCache3 = ignite.getOrCreateCache(new CacheConfiguration<PersonId, Person>("cache3"));	IgniteCache<PersonId, Person> personCache4 = ignite.getOrCreateCache(new CacheConfiguration<PersonId, Person>("cache4"));	IgniteCache<Long, Product> productCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Product>("product"));	IgniteCache<Long, ProductOrder> orderCache = ignite.getOrCreateCache(new CacheConfiguration<Long, ProductOrder>("order"));	
running single operation read tests 

person = personCache2.get(id);	if (!person.equalsPrimitiveFields(personMap2.get(id))) throw new RuntimeException("Person value was incorrectly deserialized from Cassandra");	person = personCache3.get(id);	if (!person.equals(personMap2.get(id))) throw new RuntimeException("Person value was incorrectly deserialized from Cassandra");	person = personCache4.get(id);	if (!person.equals(personMap2.get(id))) throw new RuntimeException("Person value was incorrectly deserialized from Cassandra");	Product product1 = productCache.get(product.getId());	if (!product.equals(product1)) throw new RuntimeException("Product value was incorrectly deserialized from Cassandra");	ProductOrder order1 = orderCache.get(order.getId());	if (!order.equals(order1)) throw new RuntimeException("Order value was incorrectly deserialized from Cassandra");	
single operation read tests passed 

person = personCache2.get(id);	if (!person.equalsPrimitiveFields(personMap2.get(id))) throw new RuntimeException("Person value was incorrectly deserialized from Cassandra");	person = personCache3.get(id);	if (!person.equals(personMap2.get(id))) throw new RuntimeException("Person value was incorrectly deserialized from Cassandra");	person = personCache4.get(id);	if (!person.equals(personMap2.get(id))) throw new RuntimeException("Person value was incorrectly deserialized from Cassandra");	Product product1 = productCache.get(product.getId());	if (!product.equals(product1)) throw new RuntimeException("Product value was incorrectly deserialized from Cassandra");	ProductOrder order1 = orderCache.get(order.getId());	if (!order.equals(order1)) throw new RuntimeException("Order value was incorrectly deserialized from Cassandra");	
running bulk operation read tests 

Map<PersonId, Person> persons2 = personCache2.getAll(personMap2.keySet());	if (!TestsHelper.checkPersonMapsEqual(persons2, personMap2, true)) throw new RuntimeException("Person values batch was incorrectly deserialized from Cassandra");	Map<PersonId, Person> persons3 = personCache3.getAll(personMap2.keySet());	if (!TestsHelper.checkPersonMapsEqual(persons3, personMap2, false)) throw new RuntimeException("Person values batch was incorrectly deserialized from Cassandra");	Map<PersonId, Person> persons4 = personCache4.getAll(personMap2.keySet());	if (!TestsHelper.checkPersonMapsEqual(persons4, personMap2, false)) throw new RuntimeException("Person values batch was incorrectly deserialized from Cassandra");	Map<Long, Product> productsMap1 = productCache.getAll(productsMap.keySet());	if (!TestsHelper.checkProductMapsEqual(productsMap, productsMap1)) throw new RuntimeException("Product values batch was incorrectly deserialized from Cassandra");	Map<Long, ProductOrder> ordersMap1 = orderCache.getAll(ordersMap.keySet());	if (!TestsHelper.checkOrderMapsEqual(ordersMap, ordersMap1)) throw new RuntimeException("Order values batch was incorrectly deserialized from Cassandra");	
bulk operation read tests passed 

Map<PersonId, Person> persons2 = personCache2.getAll(personMap2.keySet());	if (!TestsHelper.checkPersonMapsEqual(persons2, personMap2, true)) throw new RuntimeException("Person values batch was incorrectly deserialized from Cassandra");	Map<PersonId, Person> persons3 = personCache3.getAll(personMap2.keySet());	if (!TestsHelper.checkPersonMapsEqual(persons3, personMap2, false)) throw new RuntimeException("Person values batch was incorrectly deserialized from Cassandra");	Map<PersonId, Person> persons4 = personCache4.getAll(personMap2.keySet());	if (!TestsHelper.checkPersonMapsEqual(persons4, personMap2, false)) throw new RuntimeException("Person values batch was incorrectly deserialized from Cassandra");	Map<Long, Product> productsMap1 = productCache.getAll(productsMap.keySet());	if (!TestsHelper.checkProductMapsEqual(productsMap, productsMap1)) throw new RuntimeException("Product values batch was incorrectly deserialized from Cassandra");	Map<Long, ProductOrder> ordersMap1 = orderCache.getAll(ordersMap.keySet());	if (!TestsHelper.checkOrderMapsEqual(ordersMap, ordersMap1)) throw new RuntimeException("Order values batch was incorrectly deserialized from Cassandra");	
pojo strategy read tests passed 

Map<PersonId, Person> persons2 = personCache2.getAll(personMap2.keySet());	if (!TestsHelper.checkPersonMapsEqual(persons2, personMap2, true)) throw new RuntimeException("Person values batch was incorrectly deserialized from Cassandra");	Map<PersonId, Person> persons3 = personCache3.getAll(personMap2.keySet());	if (!TestsHelper.checkPersonMapsEqual(persons3, personMap2, false)) throw new RuntimeException("Person values batch was incorrectly deserialized from Cassandra");	Map<PersonId, Person> persons4 = personCache4.getAll(personMap2.keySet());	if (!TestsHelper.checkPersonMapsEqual(persons4, personMap2, false)) throw new RuntimeException("Person values batch was incorrectly deserialized from Cassandra");	Map<Long, Product> productsMap1 = productCache.getAll(productsMap.keySet());	if (!TestsHelper.checkProductMapsEqual(productsMap, productsMap1)) throw new RuntimeException("Product values batch was incorrectly deserialized from Cassandra");	Map<Long, ProductOrder> ordersMap1 = orderCache.getAll(ordersMap.keySet());	if (!TestsHelper.checkOrderMapsEqual(ordersMap, ordersMap1)) throw new RuntimeException("Order values batch was incorrectly deserialized from Cassandra");	
running pojo strategy delete tests 

personCache2.remove(id);	personCache2.removeAll(personMap2.keySet());	personCache3.remove(id);	personCache3.removeAll(personMap2.keySet());	personCache4.remove(id);	personCache4.removeAll(personMap2.keySet());	productCache.remove(product.getId());	productCache.removeAll(productsMap.keySet());	orderCache.remove(order.getId());	orderCache.removeAll(ordersMap.keySet());	
pojo strategy delete tests passed 

public void pojoStrategySimpleObjectsTest() {	Ignition.stopAll(true);	
running pojo strategy write tests for simple objects 

public void pojoStrategySimpleObjectsTest() {	Ignition.stopAll(true);	Map<SimplePersonId, SimplePerson> personMap5 = TestsHelper.generateSimplePersonIdsPersonsMap();	Map<SimplePersonId, SimplePerson> personMap6 = TestsHelper.generateSimplePersonIdsPersonsMap();	try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/pojo/ignite-config.xml")) {	IgniteCache<SimplePersonId, SimplePerson> personCache5 = ignite.getOrCreateCache(new CacheConfiguration<SimplePersonId, SimplePerson>("cache5"));	IgniteCache<SimplePersonId, SimplePerson> personCache6 = ignite.getOrCreateCache(new CacheConfiguration<SimplePersonId, SimplePerson>("cache6"));	
running single operation write tests 

public void pojoStrategySimpleObjectsTest() {	Ignition.stopAll(true);	Map<SimplePersonId, SimplePerson> personMap5 = TestsHelper.generateSimplePersonIdsPersonsMap();	Map<SimplePersonId, SimplePerson> personMap6 = TestsHelper.generateSimplePersonIdsPersonsMap();	try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/pojo/ignite-config.xml")) {	IgniteCache<SimplePersonId, SimplePerson> personCache5 = ignite.getOrCreateCache(new CacheConfiguration<SimplePersonId, SimplePerson>("cache5"));	IgniteCache<SimplePersonId, SimplePerson> personCache6 = ignite.getOrCreateCache(new CacheConfiguration<SimplePersonId, SimplePerson>("cache6"));	SimplePersonId id = TestsHelper.generateRandomSimplePersonId();	personCache5.put(id, TestsHelper.generateRandomSimplePerson(id.personNum));	personCache6.put(id, TestsHelper.generateRandomSimplePerson(id.personNum));	
single operation write tests passed 

public void pojoStrategySimpleObjectsTest() {	Ignition.stopAll(true);	Map<SimplePersonId, SimplePerson> personMap5 = TestsHelper.generateSimplePersonIdsPersonsMap();	Map<SimplePersonId, SimplePerson> personMap6 = TestsHelper.generateSimplePersonIdsPersonsMap();	try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/pojo/ignite-config.xml")) {	IgniteCache<SimplePersonId, SimplePerson> personCache5 = ignite.getOrCreateCache(new CacheConfiguration<SimplePersonId, SimplePerson>("cache5"));	IgniteCache<SimplePersonId, SimplePerson> personCache6 = ignite.getOrCreateCache(new CacheConfiguration<SimplePersonId, SimplePerson>("cache6"));	SimplePersonId id = TestsHelper.generateRandomSimplePersonId();	personCache5.put(id, TestsHelper.generateRandomSimplePerson(id.personNum));	personCache6.put(id, TestsHelper.generateRandomSimplePerson(id.personNum));	
running bulk operation write tests 

Map<SimplePersonId, SimplePerson> personMap5 = TestsHelper.generateSimplePersonIdsPersonsMap();	Map<SimplePersonId, SimplePerson> personMap6 = TestsHelper.generateSimplePersonIdsPersonsMap();	try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/pojo/ignite-config.xml")) {	IgniteCache<SimplePersonId, SimplePerson> personCache5 = ignite.getOrCreateCache(new CacheConfiguration<SimplePersonId, SimplePerson>("cache5"));	IgniteCache<SimplePersonId, SimplePerson> personCache6 = ignite.getOrCreateCache(new CacheConfiguration<SimplePersonId, SimplePerson>("cache6"));	SimplePersonId id = TestsHelper.generateRandomSimplePersonId();	personCache5.put(id, TestsHelper.generateRandomSimplePerson(id.personNum));	personCache6.put(id, TestsHelper.generateRandomSimplePerson(id.personNum));	personCache5.putAll(personMap5);	personCache6.putAll(personMap6);	
bulk operation write tests passed 

Map<SimplePersonId, SimplePerson> personMap6 = TestsHelper.generateSimplePersonIdsPersonsMap();	try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/pojo/ignite-config.xml")) {	IgniteCache<SimplePersonId, SimplePerson> personCache5 = ignite.getOrCreateCache(new CacheConfiguration<SimplePersonId, SimplePerson>("cache5"));	IgniteCache<SimplePersonId, SimplePerson> personCache6 = ignite.getOrCreateCache(new CacheConfiguration<SimplePersonId, SimplePerson>("cache6"));	SimplePersonId id = TestsHelper.generateRandomSimplePersonId();	personCache5.put(id, TestsHelper.generateRandomSimplePerson(id.personNum));	personCache6.put(id, TestsHelper.generateRandomSimplePerson(id.personNum));	personCache5.putAll(personMap5);	personCache6.putAll(personMap6);	}	
pojo strategy write tests for simple objects passed 

IgniteCache<SimplePersonId, SimplePerson> personCache5 = ignite.getOrCreateCache(new CacheConfiguration<SimplePersonId, SimplePerson>("cache5"));	IgniteCache<SimplePersonId, SimplePerson> personCache6 = ignite.getOrCreateCache(new CacheConfiguration<SimplePersonId, SimplePerson>("cache6"));	SimplePersonId id = TestsHelper.generateRandomSimplePersonId();	personCache5.put(id, TestsHelper.generateRandomSimplePerson(id.personNum));	personCache6.put(id, TestsHelper.generateRandomSimplePerson(id.personNum));	personCache5.putAll(personMap5);	personCache6.putAll(personMap6);	}	Ignition.stopAll(true);	try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/pojo/ignite-config.xml")) {	
running pojo strategy read tests for simple objects 

SimplePersonId id = TestsHelper.generateRandomSimplePersonId();	personCache5.put(id, TestsHelper.generateRandomSimplePerson(id.personNum));	personCache6.put(id, TestsHelper.generateRandomSimplePerson(id.personNum));	personCache5.putAll(personMap5);	personCache6.putAll(personMap6);	}	Ignition.stopAll(true);	try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/pojo/ignite-config.xml")) {	IgniteCache<SimplePersonId, SimplePerson> personCache5 = ignite.getOrCreateCache(new CacheConfiguration<SimplePersonId, SimplePerson>("cache5"));	IgniteCache<SimplePersonId, SimplePerson> personCache6 = ignite.getOrCreateCache(new CacheConfiguration<SimplePersonId, SimplePerson>("cache6"));	
running single operation read tests 

Ignition.stopAll(true);	try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/pojo/ignite-config.xml")) {	IgniteCache<SimplePersonId, SimplePerson> personCache5 = ignite.getOrCreateCache(new CacheConfiguration<SimplePersonId, SimplePerson>("cache5"));	IgniteCache<SimplePersonId, SimplePerson> personCache6 = ignite.getOrCreateCache(new CacheConfiguration<SimplePersonId, SimplePerson>("cache6"));	SimplePersonId id = personMap5.keySet().iterator().next();	SimplePerson person = personCache5.get(id);	if (!person.equalsPrimitiveFields(personMap5.get(id))) throw new RuntimeException("SimplePerson value was incorrectly deserialized from Cassandra");	id = personMap6.keySet().iterator().next();	person = personCache6.get(id);	if (!person.equals(personMap6.get(id))) throw new RuntimeException("SimplePerson value was incorrectly deserialized from Cassandra");	
single operation read tests passed 

Ignition.stopAll(true);	try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/pojo/ignite-config.xml")) {	IgniteCache<SimplePersonId, SimplePerson> personCache5 = ignite.getOrCreateCache(new CacheConfiguration<SimplePersonId, SimplePerson>("cache5"));	IgniteCache<SimplePersonId, SimplePerson> personCache6 = ignite.getOrCreateCache(new CacheConfiguration<SimplePersonId, SimplePerson>("cache6"));	SimplePersonId id = personMap5.keySet().iterator().next();	SimplePerson person = personCache5.get(id);	if (!person.equalsPrimitiveFields(personMap5.get(id))) throw new RuntimeException("SimplePerson value was incorrectly deserialized from Cassandra");	id = personMap6.keySet().iterator().next();	person = personCache6.get(id);	if (!person.equals(personMap6.get(id))) throw new RuntimeException("SimplePerson value was incorrectly deserialized from Cassandra");	
running bulk operation read tests 

SimplePersonId id = personMap5.keySet().iterator().next();	SimplePerson person = personCache5.get(id);	if (!person.equalsPrimitiveFields(personMap5.get(id))) throw new RuntimeException("SimplePerson value was incorrectly deserialized from Cassandra");	id = personMap6.keySet().iterator().next();	person = personCache6.get(id);	if (!person.equals(personMap6.get(id))) throw new RuntimeException("SimplePerson value was incorrectly deserialized from Cassandra");	Map<SimplePersonId, SimplePerson> persons5 = personCache5.getAll(personMap5.keySet());	if (!TestsHelper.checkSimplePersonMapsEqual(persons5, personMap5, true)) throw new RuntimeException("SimplePerson values batch was incorrectly deserialized from Cassandra");	Map<SimplePersonId, SimplePerson> persons6 = personCache6.getAll(personMap6.keySet());	if (!TestsHelper.checkSimplePersonMapsEqual(persons6, personMap6, false)) throw new RuntimeException("SimplePerson values batch was incorrectly deserialized from Cassandra");	
bulk operation read tests passed 

SimplePersonId id = personMap5.keySet().iterator().next();	SimplePerson person = personCache5.get(id);	if (!person.equalsPrimitiveFields(personMap5.get(id))) throw new RuntimeException("SimplePerson value was incorrectly deserialized from Cassandra");	id = personMap6.keySet().iterator().next();	person = personCache6.get(id);	if (!person.equals(personMap6.get(id))) throw new RuntimeException("SimplePerson value was incorrectly deserialized from Cassandra");	Map<SimplePersonId, SimplePerson> persons5 = personCache5.getAll(personMap5.keySet());	if (!TestsHelper.checkSimplePersonMapsEqual(persons5, personMap5, true)) throw new RuntimeException("SimplePerson values batch was incorrectly deserialized from Cassandra");	Map<SimplePersonId, SimplePerson> persons6 = personCache6.getAll(personMap6.keySet());	if (!TestsHelper.checkSimplePersonMapsEqual(persons6, personMap6, false)) throw new RuntimeException("SimplePerson values batch was incorrectly deserialized from Cassandra");	
pojo strategy read tests for simple objects passed 

SimplePersonId id = personMap5.keySet().iterator().next();	SimplePerson person = personCache5.get(id);	if (!person.equalsPrimitiveFields(personMap5.get(id))) throw new RuntimeException("SimplePerson value was incorrectly deserialized from Cassandra");	id = personMap6.keySet().iterator().next();	person = personCache6.get(id);	if (!person.equals(personMap6.get(id))) throw new RuntimeException("SimplePerson value was incorrectly deserialized from Cassandra");	Map<SimplePersonId, SimplePerson> persons5 = personCache5.getAll(personMap5.keySet());	if (!TestsHelper.checkSimplePersonMapsEqual(persons5, personMap5, true)) throw new RuntimeException("SimplePerson values batch was incorrectly deserialized from Cassandra");	Map<SimplePersonId, SimplePerson> persons6 = personCache6.getAll(personMap6.keySet());	if (!TestsHelper.checkSimplePersonMapsEqual(persons6, personMap6, false)) throw new RuntimeException("SimplePerson values batch was incorrectly deserialized from Cassandra");	
running pojo strategy delete tests for simple objects 

person = personCache6.get(id);	if (!person.equals(personMap6.get(id))) throw new RuntimeException("SimplePerson value was incorrectly deserialized from Cassandra");	Map<SimplePersonId, SimplePerson> persons5 = personCache5.getAll(personMap5.keySet());	if (!TestsHelper.checkSimplePersonMapsEqual(persons5, personMap5, true)) throw new RuntimeException("SimplePerson values batch was incorrectly deserialized from Cassandra");	Map<SimplePersonId, SimplePerson> persons6 = personCache6.getAll(personMap6.keySet());	if (!TestsHelper.checkSimplePersonMapsEqual(persons6, personMap6, false)) throw new RuntimeException("SimplePerson values batch was incorrectly deserialized from Cassandra");	personCache5.remove(id);	personCache5.removeAll(personMap5.keySet());	personCache6.remove(id);	personCache6.removeAll(personMap6.keySet());	
pojo strategy delete tests for simple objects passed 

public void loadCacheTest() {	Ignition.stopAll(true);	
running loadcache test 

public void loadCacheTest() {	Ignition.stopAll(true);	
filling cassandra table with test data 

public void loadCacheTest() {	Ignition.stopAll(true);	CacheStore store = CacheStoreHelper.createCacheStore("personTypes", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/persistence-settings-3.xml"), CassandraHelper.getAdminDataSrc());	Collection<CacheEntryImpl<PersonId, Person>> entries = TestsHelper.generatePersonIdsPersonsEntries();	store.writeAll(entries);	
cassandra table filled with test data 

public void loadCacheTest() {	Ignition.stopAll(true);	CacheStore store = CacheStoreHelper.createCacheStore("personTypes", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/persistence-settings-3.xml"), CassandraHelper.getAdminDataSrc());	Collection<CacheEntryImpl<PersonId, Person>> entries = TestsHelper.generatePersonIdsPersonsEntries();	store.writeAll(entries);	
running loadcache test 

public void loadCacheTest() {	Ignition.stopAll(true);	CacheStore store = CacheStoreHelper.createCacheStore("personTypes", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/persistence-settings-3.xml"), CassandraHelper.getAdminDataSrc());	Collection<CacheEntryImpl<PersonId, Person>> entries = TestsHelper.generatePersonIdsPersonsEntries();	store.writeAll(entries);	try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/pojo/ignite-config.xml")) {	CacheConfiguration<PersonId, Person> ccfg = new CacheConfiguration<>("cache3");	IgniteCache<PersonId, Person> personCache3 = ignite.getOrCreateCache(ccfg);	int size = personCache3.size(CachePeekMode.ALL);	
initial cache size 

public void loadCacheTest() {	Ignition.stopAll(true);	CacheStore store = CacheStoreHelper.createCacheStore("personTypes", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/persistence-settings-3.xml"), CassandraHelper.getAdminDataSrc());	Collection<CacheEntryImpl<PersonId, Person>> entries = TestsHelper.generatePersonIdsPersonsEntries();	store.writeAll(entries);	try (Ignite ignite = Ignition.start("org/apache/ignite/tests/persistence/pojo/ignite-config.xml")) {	CacheConfiguration<PersonId, Person> ccfg = new CacheConfiguration<>("cache3");	IgniteCache<PersonId, Person> personCache3 = ignite.getOrCreateCache(ccfg);	int size = personCache3.size(CachePeekMode.ALL);	
loading cache data from cassandra table 

size = personCache3.size(CachePeekMode.ALL);	Assert.assertEquals("Cache data was incorrectly loaded from Cassandra table by '" + qry + "'", 3, size);	personCache3.clear();	personCache3.loadCache(null, new SimpleStatement(qry));	size = personCache3.size(CachePeekMode.ALL);	Assert.assertEquals("Cache data was incorrectly loaded from Cassandra table by statement", 3, size);	personCache3.clear();	personCache3.loadCache(null);	size = personCache3.size(CachePeekMode.ALL);	Assert.assertEquals("Cache data was incorrectly loaded from Cassandra. " + "Expected number of records is " + TestsHelper.getBulkOperationSize() + ", but loaded number of records is " + size, TestsHelper.getBulkOperationSize(), size);	
cache data loaded from cassandra table 

Assert.assertEquals("Cache data was incorrectly loaded from Cassandra table by '" + qry + "'", 3, size);	personCache3.clear();	personCache3.loadCache(null, new SimpleStatement(qry));	size = personCache3.size(CachePeekMode.ALL);	Assert.assertEquals("Cache data was incorrectly loaded from Cassandra table by statement", 3, size);	personCache3.clear();	personCache3.loadCache(null);	size = personCache3.size(CachePeekMode.ALL);	Assert.assertEquals("Cache data was incorrectly loaded from Cassandra. " + "Expected number of records is " + TestsHelper.getBulkOperationSize() + ", but loaded number of records is " + size, TestsHelper.getBulkOperationSize(), size);	}	
loadcache test passed 

private void pojoStrategyTransactionTest(Ignite ignite, TransactionConcurrency concurrency, TransactionIsolation isolation) {	LOGGER.info("-----------------------------------------------------------------------------------");	
running pojo transaction tests using concurrency and isolation level 

CacheStore orderStore = CacheStoreHelper.createCacheStore("order", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/order.xml"), CassandraHelper.getAdminDataSrc());	Map<Long, Product> productsMap = TestsHelper.generateProductsMap(5);	Map<Long, Product> productsMap1;	Map<Long, ProductOrder> ordersMap = TestsHelper.generateOrdersMap(5);	Map<Long, ProductOrder> ordersMap1;	Product product = TestsHelper.generateRandomProduct(-1L);	ProductOrder order = TestsHelper.generateRandomOrder(-1L, -1L, new Date());	IgniteTransactions txs = ignite.transactions();	IgniteCache<Long, Product> productCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Product>("product"));	IgniteCache<Long, ProductOrder> orderCache = ignite.getOrCreateCache(new CacheConfiguration<Long, ProductOrder>("order"));	
running pojo strategy write tests 

CacheStore orderStore = CacheStoreHelper.createCacheStore("order", new ClassPathResource("org/apache/ignite/tests/persistence/pojo/order.xml"), CassandraHelper.getAdminDataSrc());	Map<Long, Product> productsMap = TestsHelper.generateProductsMap(5);	Map<Long, Product> productsMap1;	Map<Long, ProductOrder> ordersMap = TestsHelper.generateOrdersMap(5);	Map<Long, ProductOrder> ordersMap1;	Product product = TestsHelper.generateRandomProduct(-1L);	ProductOrder order = TestsHelper.generateRandomOrder(-1L, -1L, new Date());	IgniteTransactions txs = ignite.transactions();	IgniteCache<Long, Product> productCache = ignite.getOrCreateCache(new CacheConfiguration<Long, Product>("product"));	IgniteCache<Long, ProductOrder> orderCache = ignite.getOrCreateCache(new CacheConfiguration<Long, ProductOrder>("order"));	
running single operation write tests 

U.closeQuiet(tx);	}	Product product1 = (Product)productStore.load(product.getId());	ProductOrder order1 = (ProductOrder)orderStore.load(order.getId());	if (product1 == null || order1 == null) {	throw new RuntimeException("Single write operation test failed. Transaction was committed, but " + "no objects were persisted into Cassandra");	}	if (!product.equals(product1) || !order.equals(order1)) {	throw new RuntimeException("Single write operation test failed. Transaction was committed, but " + "objects were incorrectly persisted/loaded to/from Cassandra");	}	
single operation write tests passed 

U.closeQuiet(tx);	}	Product product1 = (Product)productStore.load(product.getId());	ProductOrder order1 = (ProductOrder)orderStore.load(order.getId());	if (product1 == null || order1 == null) {	throw new RuntimeException("Single write operation test failed. Transaction was committed, but " + "no objects were persisted into Cassandra");	}	if (!product.equals(product1) || !order.equals(order1)) {	throw new RuntimeException("Single write operation test failed. Transaction was committed, but " + "objects were incorrectly persisted/loaded to/from Cassandra");	}	
running bulk operation write tests 

if (!entry.getValue().equals(product)) {	throw new RuntimeException("Bulk write operation test failed. Transaction was committed, but " + "some objects were incorrectly persisted/loaded to/from Cassandra");	}	}	for (Map.Entry<Long, ProductOrder> entry : ordersMap.entrySet()) {	order = ordersMap1.get(entry.getKey());	if (!entry.getValue().equals(order)) {	throw new RuntimeException("Bulk write operation test failed. Transaction was committed, but " + "some objects were incorrectly persisted/loaded to/from Cassandra");	}	}	
bulk operation write tests passed 

if (!entry.getValue().equals(product)) {	throw new RuntimeException("Bulk write operation test failed. Transaction was committed, but " + "some objects were incorrectly persisted/loaded to/from Cassandra");	}	}	for (Map.Entry<Long, ProductOrder> entry : ordersMap.entrySet()) {	order = ordersMap1.get(entry.getKey());	if (!entry.getValue().equals(order)) {	throw new RuntimeException("Bulk write operation test failed. Transaction was committed, but " + "some objects were incorrectly persisted/loaded to/from Cassandra");	}	}	
pojo strategy write tests passed 

if (!entry.getValue().equals(product)) {	throw new RuntimeException("Bulk write operation test failed. Transaction was committed, but " + "some objects were incorrectly persisted/loaded to/from Cassandra");	}	}	for (Map.Entry<Long, ProductOrder> entry : ordersMap.entrySet()) {	order = ordersMap1.get(entry.getKey());	if (!entry.getValue().equals(order)) {	throw new RuntimeException("Bulk write operation test failed. Transaction was committed, but " + "some objects were incorrectly persisted/loaded to/from Cassandra");	}	}	
running pojo strategy delete tests 

if (!entry.getValue().equals(product)) {	throw new RuntimeException("Bulk write operation test failed. Transaction was committed, but " + "some objects were incorrectly persisted/loaded to/from Cassandra");	}	}	for (Map.Entry<Long, ProductOrder> entry : ordersMap.entrySet()) {	order = ordersMap1.get(entry.getKey());	if (!entry.getValue().equals(order)) {	throw new RuntimeException("Bulk write operation test failed. Transaction was committed, but " + "some objects were incorrectly persisted/loaded to/from Cassandra");	}	}	
running single delete tests 

throw new RuntimeException("Single delete operation test failed. Transaction wasn't committed yet, but " + "objects were already deleted from Cassandra");	}	tx.commit();	}	finally {	U.closeQuiet(tx);	}	if (productStore.load(-1L) != null || orderStore.load(-1L) != null) {	throw new RuntimeException("Single delete operation test failed. Transaction was committed, but " + "objects were not deleted from Cassandra");	}	
single delete tests passed 

throw new RuntimeException("Single delete operation test failed. Transaction wasn't committed yet, but " + "objects were already deleted from Cassandra");	}	tx.commit();	}	finally {	U.closeQuiet(tx);	}	if (productStore.load(-1L) != null || orderStore.load(-1L) != null) {	throw new RuntimeException("Single delete operation test failed. Transaction was committed, but " + "objects were not deleted from Cassandra");	}	
running bulk delete tests 

tx.commit();	}	finally {	U.closeQuiet(tx);	}	productsMap1 = (Map<Long, Product>)productStore.loadAll(productsMap.keySet());	ordersMap1 = (Map<Long, ProductOrder>)orderStore.loadAll(ordersMap.keySet());	if ((productsMap1 != null && !productsMap1.isEmpty()) || (ordersMap1 != null && !ordersMap1.isEmpty())) {	throw new RuntimeException("Bulk delete operation test failed. Transaction was committed, but " + "objects were not deleted from Cassandra");	}	
bulk delete tests passed 

tx.commit();	}	finally {	U.closeQuiet(tx);	}	productsMap1 = (Map<Long, Product>)productStore.loadAll(productsMap.keySet());	ordersMap1 = (Map<Long, ProductOrder>)orderStore.loadAll(ordersMap.keySet());	if ((productsMap1 != null && !productsMap1.isEmpty()) || (ordersMap1 != null && !ordersMap1.isEmpty())) {	throw new RuntimeException("Bulk delete operation test failed. Transaction was committed, but " + "objects were not deleted from Cassandra");	}	
pojo strategy delete tests passed 

}	finally {	U.closeQuiet(tx);	}	productsMap1 = (Map<Long, Product>)productStore.loadAll(productsMap.keySet());	ordersMap1 = (Map<Long, ProductOrder>)orderStore.loadAll(ordersMap.keySet());	if ((productsMap1 != null && !productsMap1.isEmpty()) || (ordersMap1 != null && !ordersMap1.isEmpty())) {	throw new RuntimeException("Bulk delete operation test failed. Transaction was committed, but " + "objects were not deleted from Cassandra");	}	LOGGER.info("-----------------------------------------------------------------------------------");	
passed pojo transaction tests for concurrency and isolation level 

========================= ignite sample_6952 =========================

private void txStreamerLoad(Ignite ignite, Integer key, String cacheName, boolean allowOverwrite) throws Exception {	IgniteCache<Integer, Integer> cache = ignite.cache(cacheName);	
test key 

public void testTxLoadFromStore() throws Exception {	Ignite ignite0 = ignite(0);	final IgniteTransactions txs = ignite0.transactions();	for (CacheConfiguration<Integer, Integer> ccfg : cacheConfigurations()) {	if (ccfg.getCacheStoreFactory() == null) continue;	logCacheInfo(ccfg);	try {	IgniteCache<Integer, Integer> cache = ignite0.createCache(ccfg);	List<Integer> keys = testKeys(cache);	for (Integer key : keys) {	
test key 

public void testTxCommitReadOnly1() throws Exception {	Ignite ignite0 = ignite(0);	final IgniteTransactions txs = ignite0.transactions();	for (CacheConfiguration<Integer, Integer> ccfg : cacheConfigurations()) {	logCacheInfo(ccfg);	try {	IgniteCache<Integer, Integer> cache = ignite0.createCache(ccfg);	List<Integer> keys = testKeys(cache);	for (Integer key : keys) {	
test key 

public void testTxCommitReadOnly2() throws Exception {	Ignite ignite0 = ignite(0);	final IgniteTransactions txs = ignite0.transactions();	for (CacheConfiguration<Integer, Integer> ccfg : cacheConfigurations()) {	logCacheInfo(ccfg);	try {	IgniteCache<Integer, Integer> cache = ignite0.createCache(ccfg);	List<Integer> keys = testKeys(cache);	for (final Integer key : keys) {	
test key 

Ignite ignite1 = ignite(1);	final IgniteTransactions txs0 = ignite0.transactions();	final IgniteTransactions txs1 = ignite1.transactions();	for (CacheConfiguration<Integer, Integer> ccfg : cacheConfigurations()) {	logCacheInfo(ccfg);	try {	IgniteCache<Integer, Integer> cache0 = ignite0.createCache(ccfg);	IgniteCache<Integer, Integer> cache1 = ignite1.cache(ccfg.getName());	List<Integer> keys = testKeys(cache0);	for (Integer key : keys) {	
test key 

Ignite ignite1 = ignite(1);	final IgniteTransactions txs0 = ignite0.transactions();	final IgniteTransactions txs1 = ignite1.transactions();	for (CacheConfiguration<Integer, Integer> ccfg : cacheConfigurations()) {	logCacheInfo(ccfg);	try {	IgniteCache<Integer, Integer> cache0 = ignite0.createCache(ccfg);	IgniteCache<Integer, Integer> cache1 = ignite1.cache(ccfg.getName());	List<Integer> keys = testKeys(cache0);	for (Integer key : keys) {	
test key 

private void txConflictRead(boolean noVal, boolean needVer) throws Exception {	Ignite ignite0 = ignite(0);	final IgniteTransactions txs = ignite0.transactions();	for (CacheConfiguration<Integer, Integer> ccfg : cacheConfigurations()) {	logCacheInfo(ccfg);	try {	IgniteCache<Integer, Integer> cache = ignite0.createCache(ccfg);	List<Integer> keys = testKeys(cache);	for (Integer key : keys) {	
test key 

}	else {	Integer val = cache.get(key);	assertEquals(expVal, val);	}	updateKey(cache, key, 1);	tx.commit();	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

private void txConflictReadWrite(boolean noVal, boolean rmv, boolean needVer) throws Exception {	Ignite ignite0 = ignite(0);	final IgniteTransactions txs = ignite0.transactions();	for (CacheConfiguration<Integer, Integer> ccfg : cacheConfigurations()) {	logCacheInfo(ccfg);	try {	IgniteCache<Integer, Integer> cache = ignite0.createCache(ccfg);	List<Integer> keys = testKeys(cache);	for (Integer key : keys) {	
test key 

assertEquals(expVal, val);	}	updateKey(cache, key, 1);	if (rmv) cache.remove(key);	else cache.put(key, 2);	tx.commit();	}	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

private void txConflictGetAndPut(boolean noVal, boolean rmv) throws Exception {	Ignite ignite0 = ignite(0);	final IgniteTransactions txs = ignite0.transactions();	for (CacheConfiguration<Integer, Integer> ccfg : cacheConfigurations()) {	logCacheInfo(ccfg);	try {	IgniteCache<Integer, Integer> cache = ignite0.createCache(ccfg);	List<Integer> keys = testKeys(cache);	for (Integer key : keys) {	
test key 

try {	try (Transaction tx = txs.txStart(OPTIMISTIC, SERIALIZABLE)) {	Integer val = rmv ? cache.getAndRemove(key) : cache.getAndPut(key, 2);	assertEquals(expVal, val);	updateKey(cache, key, 1);	tx.commit();	}	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

private void txConflictInvoke(boolean noVal, boolean rmv) throws Exception {	Ignite ignite0 = ignite(0);	final IgniteTransactions txs = ignite0.transactions();	for (CacheConfiguration<Integer, Integer> ccfg : cacheConfigurations()) {	logCacheInfo(ccfg);	try {	IgniteCache<Integer, Integer> cache = ignite0.createCache(ccfg);	List<Integer> keys = testKeys(cache);	for (Integer key : keys) {	
test key 

try {	try (Transaction tx = txs.txStart(OPTIMISTIC, SERIALIZABLE)) {	Integer val = cache.invoke(key, new SetValueProcessor(rmv ? null : 2));	assertEquals(expVal, val);	updateKey(cache, key, 1);	tx.commit();	}	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

Ignite ignite0 = ignite(0);	for (CacheConfiguration<Integer, Integer> ccfg : cacheConfigurations()) {	logCacheInfo(ccfg);	try {	IgniteCache<Integer, Integer> cache0 = ignite0.createCache(ccfg);	final Integer key1 = primaryKey(ignite(0).cache(cache0.getName()));	final Integer key2 = primaryKey(ignite(1).cache(cache0.getName()));	Map<Integer, Integer> vals = new HashMap<>();	int newVal = 0;	for (Ignite ignite : G.allGrids()) {	
test node 

assertEquals(vals.get(key1), res1.get());	EntryProcessorResult<Integer> res2 = res.get(key2);	assertNotNull(res2);	assertEquals(vals.get(key2), res2.get());	updateKey(cache0, key1, -1);	tx.commit();	}	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

public void testTxConflictPutIfAbsent() throws Exception {	Ignite ignite0 = ignite(0);	final IgniteTransactions txs = ignite0.transactions();	for (CacheConfiguration<Integer, Integer> ccfg : cacheConfigurations()) {	logCacheInfo(ccfg);	try {	IgniteCache<Integer, Integer> cache = ignite0.createCache(ccfg);	List<Integer> keys = testKeys(cache);	for (Integer key : keys) {	
test key 

try {	try (Transaction tx = txs.txStart(OPTIMISTIC, SERIALIZABLE)) {	boolean put = cache.putIfAbsent(key, 2);	assertTrue(put);	updateKey(cache, key, 1);	tx.commit();	}	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

try {	try (Transaction tx = txs.txStart(OPTIMISTIC, SERIALIZABLE)) {	boolean put = cache.putIfAbsent(key, 2);	assertFalse(put);	updateKey(cache, key, 3);	tx.commit();	}	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

public void testTxConflictGetAndPutIfAbsent() throws Exception {	Ignite ignite0 = ignite(0);	final IgniteTransactions txs = ignite0.transactions();	for (CacheConfiguration<Integer, Integer> ccfg : cacheConfigurations()) {	logCacheInfo(ccfg);	try {	IgniteCache<Integer, Integer> cache = ignite0.createCache(ccfg);	List<Integer> keys = testKeys(cache);	for (Integer key : keys) {	
test key 

try {	try (Transaction tx = txs.txStart(OPTIMISTIC, SERIALIZABLE)) {	Object old = cache.getAndPutIfAbsent(key, 2);	assertNull(old);	updateKey(cache, key, 1);	tx.commit();	}	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

try {	try (Transaction tx = txs.txStart(OPTIMISTIC, SERIALIZABLE)) {	Object old = cache.getAndPutIfAbsent(key, 4);	assertEquals(2, old);	updateKey(cache, key, 3);	tx.commit();	}	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

public void testTxConflictReplace() throws Exception {	Ignite ignite0 = ignite(0);	final IgniteTransactions txs = ignite0.transactions();	for (CacheConfiguration<Integer, Integer> ccfg : cacheConfigurations()) {	logCacheInfo(ccfg);	try {	IgniteCache<Integer, Integer> cache = ignite0.createCache(ccfg);	List<Integer> keys = testKeys(cache);	for (final Integer key : keys) {	
test key 

try {	try (Transaction tx = txs.txStart(OPTIMISTIC, SERIALIZABLE)) {	boolean replace = cache.replace(key, 2);	assertFalse(replace);	updateKey(cache, key, 1);	tx.commit();	}	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

try {	try (Transaction tx = txs.txStart(OPTIMISTIC, SERIALIZABLE)) {	boolean replace = cache.replace(key, 2);	assertFalse(replace);	updateKey(cache, key, 3);	tx.commit();	}	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

cache.remove(key);	return null;	}	}	);	tx.commit();	}	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

public void testTxConflictGetAndReplace() throws Exception {	Ignite ignite0 = ignite(0);	final IgniteTransactions txs = ignite0.transactions();	for (CacheConfiguration<Integer, Integer> ccfg : cacheConfigurations()) {	logCacheInfo(ccfg);	try {	IgniteCache<Integer, Integer> cache = ignite0.createCache(ccfg);	List<Integer> keys = testKeys(cache);	for (final Integer key : keys) {	
test key 

try {	try (Transaction tx = txs.txStart(OPTIMISTIC, SERIALIZABLE)) {	Object old = cache.getAndReplace(key, 2);	assertNull(old);	updateKey(cache, key, 1);	tx.commit();	}	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

try {	try (Transaction tx = txs.txStart(OPTIMISTIC, SERIALIZABLE)) {	Object old = cache.getAndReplace(key, 2);	assertNull(old);	updateKey(cache, key, 3);	tx.commit();	}	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

cache.remove(key);	return null;	}	}	);	tx.commit();	}	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

public void testTxConflictRemoveWithOldValue() throws Exception {	Ignite ignite0 = ignite(0);	final IgniteTransactions txs = ignite0.transactions();	for (CacheConfiguration<Integer, Integer> ccfg : cacheConfigurations()) {	logCacheInfo(ccfg);	try {	IgniteCache<Integer, Integer> cache = ignite0.createCache(ccfg);	List<Integer> keys = testKeys(cache);	for (final Integer key : keys) {	
test key 

try {	try (Transaction tx = txs.txStart(OPTIMISTIC, SERIALIZABLE)) {	boolean rmv = cache.remove(key, 2);	assertFalse(rmv);	updateKey(cache, key, 1);	tx.commit();	}	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

try {	try (Transaction tx = txs.txStart(OPTIMISTIC, SERIALIZABLE)) {	boolean rmv = cache.remove(key, 2);	assertTrue(rmv);	updateKey(cache, key, 3);	tx.commit();	}	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

cache.remove(key);	return null;	}	}	);	tx.commit();	}	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

public void testTxConflictCasReplace() throws Exception {	Ignite ignite0 = ignite(0);	final IgniteTransactions txs = ignite0.transactions();	for (CacheConfiguration<Integer, Integer> ccfg : cacheConfigurations()) {	logCacheInfo(ccfg);	try {	IgniteCache<Integer, Integer> cache = ignite0.createCache(ccfg);	List<Integer> keys = testKeys(cache);	for (final Integer key : keys) {	
test key 

try {	try (Transaction tx = txs.txStart(OPTIMISTIC, SERIALIZABLE)) {	boolean replace = cache.replace(key, 1, 2);	assertFalse(replace);	updateKey(cache, key, 1);	tx.commit();	}	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

try {	try (Transaction tx = txs.txStart(OPTIMISTIC, SERIALIZABLE)) {	boolean replace = cache.replace(key, 2, 1);	assertTrue(replace);	updateKey(cache, key, 3);	tx.commit();	}	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

cache.remove(key);	return null;	}	}	);	tx.commit();	}	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

private void txConflictRemoveReturnBoolean(boolean noVal) throws Exception {	Ignite ignite0 = ignite(0);	final IgniteTransactions txs = ignite0.transactions();	for (CacheConfiguration<Integer, Integer> ccfg : cacheConfigurations()) {	logCacheInfo(ccfg);	try {	IgniteCache<Integer, Integer> cache = ignite0.createCache(ccfg);	List<Integer> keys = testKeys(cache);	for (final Integer key : keys) {	
test key 

try {	try (Transaction tx = txs.txStart(OPTIMISTIC, SERIALIZABLE)) {	boolean res = cache.remove(key);	assertFalse(res);	updateKey(cache, key, -1);	tx.commit();	}	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

cache.remove(key);	return null;	}	}	);	tx.commit();	}	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

Object val = cache.get(key);	assertEquals(1, val);	boolean res = cache.remove(key);	assertTrue(res);	updateKey(cache, key, 2);	tx.commit();	}	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

private void txNoConflictUpdate(boolean noVal, boolean rmv, boolean getAfterUpdate) throws Exception {	Ignite ignite0 = ignite(0);	final IgniteTransactions txs = ignite0.transactions();	for (CacheConfiguration<Integer, Integer> ccfg : cacheConfigurations()) {	logCacheInfo(ccfg);	try {	IgniteCache<Integer, Integer> cache = ignite0.createCache(ccfg);	List<Integer> keys = testKeys(cache);	for (Integer key : keys) {	
test key 

private void txNoConflictContainsKey(boolean noVal) throws Exception {	Ignite ignite0 = ignite(0);	final IgniteTransactions txs = ignite0.transactions();	for (CacheConfiguration<Integer, Integer> ccfg : cacheConfigurations()) {	logCacheInfo(ccfg);	try {	IgniteCache<Integer, Integer> cache = ignite0.createCache(ccfg);	List<Integer> keys = testKeys(cache);	for (Integer key : keys) {	
test key 

public void testTxRollbackIfLocked1() throws Exception {	Ignite ignite0 = ignite(0);	IgniteTransactions txs = ignite0.transactions();	for (CacheConfiguration<Integer, Integer> ccfg : cacheConfigurations()) {	logCacheInfo(ccfg);	try {	IgniteCache<Integer, Integer> cache = ignite0.createCache(ccfg);	List<Integer> keys = testKeys(cache);	for (Integer key : keys) {	
test key 

logCacheInfo(ccfg);	try {	IgniteCache<Integer, Integer> cache = ignite0.createCache(ccfg);	List<Integer> keys = testKeys(cache);	for (Integer key : keys) {	CountDownLatch latch = new CountDownLatch(1);	IgniteInternalFuture<?> fut = lockKey(latch, cache, key);	try {	try (Transaction tx = txs.txStart(OPTIMISTIC, SERIALIZABLE)) {	cache.put(key, 2);	
Commit 

CountDownLatch latch = new CountDownLatch(1);	IgniteInternalFuture<?> fut = lockKey(latch, cache, key);	try {	try (Transaction tx = txs.txStart(OPTIMISTIC, SERIALIZABLE)) {	cache.put(key, 2);	tx.commit();	}	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

IgniteInternalFuture<?> fut = lockKey(latch, cache, key1);	try {	try (Transaction tx = txs.txStart(OPTIMISTIC, SERIALIZABLE)) {	cache.put(key1, 2);	cache.put(key2, 2);	tx.commit();	}	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

IgniteInternalFuture<?> fut = lockKey(latch, cache, readKey);	try {	try (Transaction tx = ignite.transactions().txStart(OPTIMISTIC, SERIALIZABLE)) {	cache.put(writeKey, writeKey);	cache.get(readKey);	tx.commit();	}	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

try {	try (Transaction tx = ignite.transactions().txStart(OPTIMISTIC, SERIALIZABLE)) {	cache.getAndPut(writeKey, writeKey);	cache.get(readKey);	updateKey(cache, writeKey, writeKey + readKey);	tx.commit();	}	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

continue;	}	break;	}	assertEquals(ACCOUNTS * VAL_PER_ACCOUNT, sum);	}	return null;	}	catch (Throwable e) {	stop.set(true);	
unexpected error 

catch (TransactionOptimisticException ignored) {	continue;	}	break;	}	}	return null;	}	catch (Throwable e) {	stop.set(true);	
unexpected error 

try (Transaction tx = txs.txStart(OPTIMISTIC, SERIALIZABLE)) {	cache.put(key1, key1);	cache.put(key2, key2);	cache.put(key3, key3);	fut = lockKey(latch, near ? cache : cache0, key2);	tx.commit();	}	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

try (Transaction tx = txs.txStart(OPTIMISTIC, SERIALIZABLE)) {	cache.get(key1);	cache.get(key2);	cache.get(key3);	updateKey(near ? cache : cache0, key2, -2);	tx.commit();	}	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

try {	CacheConfiguration<Integer, Integer> ccfg1 = cacheConfiguration(PARTITIONED, FULL_SYNC, 1, false, false);	ccfg1.setName(CACHE1);	ignite0.createCache(ccfg1);	CacheConfiguration<Integer, Integer> ccfg2 = cacheConfiguration(PARTITIONED, FULL_SYNC, 1, false, false);	ccfg2.setName(CACHE2);	ignite0.createCache(ccfg2);	Integer newVal = 0;	List<Integer> keys = testKeys(ignite0.<Integer, Integer>cache(CACHE1));	for (Ignite ignite : G.allGrids()) {	
test node 

IgniteInternalFuture<?> fut = lockKey(latch, cache1, key1);	try {	try (Transaction tx = txs.txStart(OPTIMISTIC, SERIALIZABLE)) {	cache1.put(key1, newVal + 1);	cache2.put(key1, newVal + 1);	tx.commit();	}	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

fut = lockKey(latch, cache1, key1);	try {	try (Transaction tx = txs.txStart(OPTIMISTIC, SERIALIZABLE)) {	cache1.put(key1, newVal + 1);	cache2.put(key2, newVal + 1);	tx.commit();	}	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

assertEquals(1, val1);	assertEquals(newVal, val2);	updateKey(cache2, key2, 1);	cache1.put(key1, newVal + 1);	cache2.put(key2, newVal + 1);	tx.commit();	}	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

Object val1 = cache1.get(key1);	Object val2 = cache2.get(key2);	assertEquals(newVal, val1);	assertEquals(newVal, val2);	updateKey(cache2, key2, newVal);	tx.commit();	}	fail();	}	catch (TransactionOptimisticException e) {	
expected exception 

public void testRandomOperations() throws Exception {	Ignite ignite0 = ignite(0);	long stopTime = U.currentTimeMillis() + getTestTimeout() - 30_000;	for (CacheConfiguration<Integer, Integer> ccfg : cacheConfigurations()) {	logCacheInfo(ccfg);	try {	IgniteCache<Integer, Integer> cache0 = ignite0.createCache(ccfg);	ThreadLocalRandom rnd = ThreadLocalRandom.current();	for (Ignite ignite : G.allGrids()) {	
test node 

final AtomicInteger cntr = new AtomicInteger();	final Integer key = i;	final AtomicInteger threadIdx = new AtomicInteger();	final int THREADS = 10;	final CyclicBarrier barrier = new CyclicBarrier(THREADS);	GridTestUtils.runMultiThreadedAsync(new Callable<Void>() {	int idx = threadIdx.getAndIncrement() % caches.size();	IgniteCache<Integer, Integer> cache = caches.get(idx);	Ignite ignite = cache.unwrap(Ignite.class);	IgniteTransactions txs = ignite.transactions();	
started update thread 

final AtomicInteger threadIdx = new AtomicInteger();	final int THREADS = 10;	final CyclicBarrier barrier = new CyclicBarrier(THREADS);	final ConcurrentSkipListSet<Integer> vals1 = new ConcurrentSkipListSet<>();	final ConcurrentSkipListSet<Integer> vals2 = new ConcurrentSkipListSet<>();	GridTestUtils.runMultiThreadedAsync(new Callable<Void>() {	int idx = threadIdx.getAndIncrement() % caches.size();	IgniteCache<Integer, Integer> cache = caches.get(idx);	Ignite ignite = cache.unwrap(Ignite.class);	IgniteTransactions txs = ignite.transactions();	
started update thread 

final Integer key = i;	final AtomicInteger threadIdx = new AtomicInteger();	final int THREADS = 10;	final CyclicBarrier barrier = new CyclicBarrier(THREADS);	final IgniteInternalFuture<?> updateFut = GridTestUtils.runMultiThreadedAsync(new Callable<Void>() {	int thread = threadIdx.getAndIncrement();	int idx = thread % caches.size();	IgniteCache<Integer, Integer> cache = caches.get(idx);	Ignite ignite = cache.unwrap(Ignite.class);	IgniteTransactions txs = ignite.transactions();	
started update thread 

final AtomicInteger idx = new AtomicInteger();	final int THREADS = 20;	final long testTime = 30_000;	final long stopTime = System.currentTimeMillis() + testTime;	IgniteInternalFuture<?> nonSerFut = null;	if (nonSer) {	nonSerFut = runMultiThreadedAsync(new Callable<Void>() {	int nodeIdx = idx.getAndIncrement() % clients.size();	Ignite node = clients.get(nodeIdx);	Thread.currentThread().setName("update-pessimistic-" + node.name());	
pessimistic tx thread 

}	}	return null;	}	}, 10, "non-ser-thread");	}	final IgniteInternalFuture<?> fut = runMultiThreadedAsync(new Callable<Void>() {	int nodeIdx = idx.getAndIncrement() % clients.size();	Ignite node = clients.get(nodeIdx);	Thread.currentThread().setName("update-" + node.name());	
tx thread 

break;	}	catch (TransactionOptimisticException ignore) {	}	catch (IgniteException | CacheException e) {	assertTrue("Unexpected exception [err=" + e + ", cause=" + e.getCause() + ']', restart && X.hasCause(e, ClusterTopologyCheckedException.class));	}	}	}	catch (Throwable e) {	
unexpected error 

GridTestUtils.runMultiThreadedAsync(new Callable<Void>() {	IgniteCache<Integer, Integer> cache = ignite.cache(cacheName);	IgniteTransactions txs = cache.unwrap(Ignite.class).transactions();	try (Transaction tx = txs.txStart(OPTIMISTIC, SERIALIZABLE)) {	cache.putAll(keys);	barrier.await();	tx.commit();	commitCntr.incrementAndGet();	}	catch (TransactionOptimisticException e) {	
optimistic error 

IgniteInternalFuture<?> fut = GridTestUtils.runAsync(new Callable<Void>() {	IgniteTransactions txs = cache.unwrap(Ignite.class).transactions();	try (Transaction tx = txs.txStart(OPTIMISTIC, SERIALIZABLE)) {	assertTrue(cache.get(key).equals(val));	readLatch.countDown();	writeLatch.await(10, TimeUnit.SECONDS);	try {	tx.commit();	}	catch (TransactionOptimisticException e) {	
expected exception 

if (FAST) return;	assert updateNodes.size() > 0;	final Ignite srv = ignite(1);	final String cacheName = srv.createCache(cacheConfiguration(PARTITIONED, FULL_SYNC, 1, false, false)).getName();	try {	final int KEYS = 100;	final AtomicBoolean finished = new AtomicBoolean();	IgniteInternalFuture<?> fut = restart ? restartFuture(finished, null) : null;	try {	for (int i = 0; i < 10; i++) {	
iteration 

IgniteInternalFuture<?> fut = restart ? restartFuture(finished, null) : null;	try {	for (int i = 0; i < 10; i++) {	final long stopTime = U.currentTimeMillis() + 10_000;	final AtomicInteger idx = new AtomicInteger();	IgniteInternalFuture<?> nonSerFut = null;	if (nonSer) {	nonSerFut = GridTestUtils.runMultiThreadedAsync(new Callable<Void>() {	int nodeIdx = idx.getAndIncrement() % updateNodes.size();	Ignite node = updateNodes.get(nodeIdx);	
non serializable tx thread 

}	});	}	return null;	}	}, 5, "non-ser-thread");	}	IgniteInternalFuture<?> updateFut = GridTestUtils.runMultiThreadedAsync(new Callable<Void>() {	int nodeIdx = idx.getAndIncrement() % updateNodes.size();	Ignite node = updateNodes.get(nodeIdx);	
tx thread 

}	}	else cache.putAll(map);	tx.commit();	}	}	}	catch (TransactionOptimisticException ignore) {	}	catch (Throwable e) {	
unexpected error 

private IgniteInternalFuture<?> lockKey( final CountDownLatch releaseLatch, final IgniteCache<Integer, Integer> cache, final Integer key) throws Exception {	final CountDownLatch lockLatch = new CountDownLatch(1);	IgniteInternalFuture<?> fut = GridTestUtils.runAsync(new Callable<Void>() {	IgniteTransactions txs = cache.unwrap(Ignite.class).transactions();	try (Transaction tx = txs.txStart(PESSIMISTIC, REPEATABLE_READ)) {	cache.put(key, 1);	
locked key 

private IgniteInternalFuture<?> lockKey( final CountDownLatch releaseLatch, final IgniteCache<Integer, Integer> cache, final Integer key) throws Exception {	final CountDownLatch lockLatch = new CountDownLatch(1);	IgniteInternalFuture<?> fut = GridTestUtils.runAsync(new Callable<Void>() {	IgniteTransactions txs = cache.unwrap(Ignite.class).transactions();	try (Transaction tx = txs.txStart(PESSIMISTIC, REPEATABLE_READ)) {	cache.put(key, 1);	lockLatch.countDown();	assertTrue(releaseLatch.await(100000, SECONDS));	
commit tx 

========================= ignite sample_2069 =========================

fut.onDone(new IgniteCheckedException("Failed to start external process (grid is stopping)."));	return;	}	try {	HadoopExternalTaskMetadata startMeta = buildTaskMeta();	if (log.isDebugEnabled()) log.debug("Created hadoop child process metadata for job [job=" + job + ", childProcId=" + childProcId + ", taskMeta=" + startMeta + ']');	Process proc = startJavaProcess(childProcId, startMeta, job, ctx.kernalContext().config().getWorkDirectory());	BufferedReader rdr = new BufferedReader(new InputStreamReader(proc.getInputStream()));	String line;	while ((line = rdr.readLine()) != null) {	
tracing process output 

String javaHome = System.getProperty("java.home");	if (javaHome == null) javaHome = System.getenv("JAVA_HOME");	if (javaHome == null) throw new IgniteCheckedException("Failed to locate JAVA_HOME.");	javaCmd = javaHome + File.separator + "bin" + File.separator + (U.isWindows() ? "java.exe" : "java");	try {	Process proc = new ProcessBuilder(javaCmd, "-version").redirectErrorStream(true).start();	Collection<String> out = readProcessOutput(proc);	int res = proc.waitFor();	if (res != 0) throw new IgniteCheckedException("Failed to execute 'java -version' command (process finished with nonzero " + "code) [exitCode=" + res + ", javaCmd='" + javaCmd + "', msg=" + F.first(out) + ']');	if (log.isInfoEnabled()) {	
will use java for external task execution 

private Process startJavaProcess(UUID childProcId, HadoopExternalTaskMetadata startMeta, HadoopJobEx job, String igniteWorkDir) throws Exception {	String outFldr = jobWorkFolder(job.id()) + File.separator + childProcId;	
will write process log output to 

========================= ignite sample_7284 =========================

private void concurrentClientsStart(boolean createCache) throws Exception {	for (int i = 0; i < ITERATIONS; i++) {	try {	
iteration 

========================= ignite sample_1079 =========================

private void processFragmentizerRequest(IgfsFragmentizerRequest req) throws IgniteCheckedException {	req.finishUnmarshal(igfsCtx.kernalContext().config().getMarshaller(), null);	Collection<IgfsFileAffinityRange> ranges = req.fragmentRanges();	IgniteUuid fileId = req.fileId();	IgfsEntryInfo fileInfo = igfsCtx.meta().info(fileId);	if (fileInfo == null) {	
failed to find file info for fragmentizer request 

assert old == null;	for (Map.Entry<UUID, Collection<IgfsFileAffinityRange>> entry : grpMap.entrySet()) {	UUID nodeId = entry.getKey();	IgfsFragmentizerRequest msg = new IgfsFragmentizerRequest(fileInfo.id(), entry.getValue());	try {	if (log.isDebugEnabled()) log.debug("Sending fragmentizer request to remote node [nodeId=" + nodeId + ", fileId=" + fileInfo.id() + ", msg=" + msg + ']');	sendWithRetries(nodeId, msg);	}	catch (IgniteCheckedException e) {	if (e.hasCause(ClusterTopologyCheckedException.class)) {	
failed to send fragmentizer request to remote node node left grid 

sendWithRetries(nodeId, msg);	}	catch (IgniteCheckedException e) {	if (e.hasCause(ClusterTopologyCheckedException.class)) {	}	else U.error(log, "Failed to send fragmentizer request to remote node [nodeId=" + nodeId + ", msg=" + msg + ']', e);	nodeIds.remove(nodeId);	}	}	if (nodeIds.isEmpty()) {	
got empty wait set for fragmentized file 

private void sendResponse(UUID nodeId, IgfsCommunicationMessage msg) {	try {	sendWithRetries(nodeId, msg);	}	catch (IgniteCheckedException e) {	if (e.hasCause(ClusterTopologyCheckedException.class)) {	
failed to send sync response to igfs fragmentizer coordinator originating node left the grid 

private void sendResponse(UUID nodeId, IgfsCommunicationMessage msg) {	try {	sendWithRetries(nodeId, msg);	}	catch (IgniteCheckedException e) {	if (e.hasCause(ClusterTopologyCheckedException.class)) {	}	
failed to send sync response to igfs fragmentizer coordinator 

========================= ignite sample_4781 =========================

public void testPut() throws Exception {	try {	Ignite g = G.start("examples/config/example-cache.xml");	
topology is too small for this test run with remote nodes or more having large number of backup nodes 

========================= ignite sample_1096 =========================

public void testConnectionStatusWithBrokerDisconnection() throws Exception {	fail("https: streamer.setSingleTupleExtractor(singleTupleExtractor());	streamer.setTopic(SINGLE_TOPIC_NAME);	streamer.setBlockUntilConnected(true);	streamer.setRetryWaitStrategy(WaitStrategies.noWait());	streamer.start();	for (int i = 0; i < 5; i++) {	
iteration 

========================= ignite sample_7841 =========================

public void testMarkerFiltering() throws Exception {	Log4J2Logger log = new Log4J2Logger(LOG_CONFIG);	
ignore me ignored error 

public void testMarkerFiltering() throws Exception {	Log4J2Logger log = new Log4J2Logger(LOG_CONFIG);	
ignore me ignored warning 

public void testMarkerFiltering() throws Exception {	Log4J2Logger log = new Log4J2Logger(LOG_CONFIG);	
ignore me ignored info 

public void testMarkerFiltering() throws Exception {	Log4J2Logger log = new Log4J2Logger(LOG_CONFIG);	
ignore me ignored debug 

public void testMarkerFiltering() throws Exception {	Log4J2Logger log = new Log4J2Logger(LOG_CONFIG);	
ignore me ignored trace 

public void testMarkerFiltering() throws Exception {	Log4J2Logger log = new Log4J2Logger(LOG_CONFIG);	
accept me accepted error 

public void testMarkerFiltering() throws Exception {	Log4J2Logger log = new Log4J2Logger(LOG_CONFIG);	
accept me accepted warning 

public void testMarkerFiltering() throws Exception {	Log4J2Logger log = new Log4J2Logger(LOG_CONFIG);	
accept me accepted info 

public void testMarkerFiltering() throws Exception {	Log4J2Logger log = new Log4J2Logger(LOG_CONFIG);	
accept me accepted debug 

public void testMarkerFiltering() throws Exception {	Log4J2Logger log = new Log4J2Logger(LOG_CONFIG);	
accept me accepted trace 

========================= ignite sample_6138 =========================

GridDhtTxMapping txMapping = new GridDhtTxMapping();	Map<UUID, GridDistributedTxMapping> mappings = new HashMap<>();	boolean hasNearCache = false;	for (IgniteTxEntry write : writes) {	map(write, topVer, mappings, txMapping, remap, topLocked);	if (write.context().isNear()) hasNearCache = true;	}	for (IgniteTxEntry read : reads) map(read, topVer, mappings, txMapping, remap, topLocked);	if (keyLockFut != null) keyLockFut.onAllKeysAdded();	if (isDone()) {	
abandoning re map because future is done 

========================= ignite sample_3855 =========================

protected void stopGrid(int idx) {	Ignite remote = grid(idx);	assert remote instanceof IgniteProcessProxy : remote;	IgniteProcessProxy proc = (IgniteProcessProxy) remote;	int pid = proc.getProcess().getPid();	try {	
killing grid id d with pid d 

protected void stopGrid(int idx) {	Ignite remote = grid(idx);	assert remote instanceof IgniteProcessProxy : remote;	IgniteProcessProxy proc = (IgniteProcessProxy) remote;	int pid = proc.getProcess().getPid();	try {	IgniteProcessProxy.kill(proc.name());	
grid id d with pid d stopped 

========================= ignite sample_2751 =========================

public Option[] baseConfig() {	
org apache karaf apache karaf tar gz karafVersion target paxexam unpack etc jre properties jre sun nio ch etc jre properties jre sun nio ch org apache ignite ignite osgi karaf projectVersion xml features projectVersion projectVersion 

========================= ignite sample_6179 =========================

public static void main(String args[]) {	SparkConf sparkConf = new SparkConf() .setAppName("JavaIgniteRDDExample") .setMaster("local") .set("spark.executor.instances", "2");	JavaSparkContext sparkContext = new JavaSparkContext(sparkConf);	Logger.getRootLogger().setLevel(Level.ERROR);	
org apache ignite 

========================= ignite sample_221 =========================

private List<List<?>> checkQuery(String sql, IgniteCache<Object, Object> cache, boolean enforceJoinOrder, int expSize, Object... args) {	SqlFieldsQuery qry = new SqlFieldsQuery(sql);	qry.setDistributedJoins(true);	qry.setEnforceJoinOrder(enforceJoinOrder);	qry.setArgs(args);	
plan 

========================= ignite sample_7504 =========================

IgniteNamedInstance grid = start0( new GridStartContext(cfg, springCfgUrl, springCtx == null ? cfgMap.get2() : springCtx), true).get1();	if (grid != null) grids.add(grid);	}	}	catch (IgniteCheckedException e) {	for (IgniteNamedInstance grid : grids) {	try {	grid.stop(true);	}	catch (Exception e1) {	
error when stopping grid 

boolean started = false;	try {	IgniteKernal grid0 = new IgniteKernal(startCtx.springContext());	grid = grid0;	grid0.start( myCfg, utilityCacheExecSvc, execSvc, svcExecSvc, sysExecSvc, stripedExecSvc, p2pExecSvc, mgmtExecSvc, igfsExecSvc, dataStreamerExecSvc, restExecSvc, affExecSvc, idxExecSvc, callbackExecSvc, qryExecSvc, schemaExecSvc, customExecSvcs, new CA() {	startLatch.countDown();	}	}	);	state = STARTED;	
grid factory started ok 

catch (IllegalArgumentException e) {	throw new IgniteCheckedException("Failed to override deployment mode using system property " + "(are there any misspellings?)" + "[name=" + IGNITE_DEP_MODE_OVERRIDE + ", value=" + depModeName + ']', e);	}	}	}	if (myCfg.getUserAttributes() == null) myCfg.setUserAttributes(Collections.<String, Object>emptyMap());	if (myCfg.getMBeanServer() == null && !U.IGNITE_MBEANS_DISABLED) myCfg.setMBeanServer(ManagementFactory.getPlatformMBeanServer());	Marshaller marsh = myCfg.getMarshaller();	if (marsh == null) {	if (!BinaryMarshaller.available()) {	
optimizedmarshaller is not supported on this jvm only recent and versions hotspot vms are supported to enable fast marshalling upgrade to recent or hotspot vm release switching to standard jdk marshalling object serialization performance will be significantly slower to enable fast marshalling upgrade to recent or hotspot vm release 

}	catch (IllegalStateException e) {	if (log != null && log.isDebugEnabled()) log.debug("Shutdown is in progress (ignoring): " + e.getMessage());	}	unregisterFactoryMBean();	try {	grid0.stop(cancel);	if (log != null && log.isDebugEnabled()) log.debug("Ignite instance stopped ok: " + name);	}	catch (Throwable e) {	
failed to properly stop grid instance due to undeclared exception 

GridMBeanServerData data = mbeans.get(srv);	if (data == null) {	try {	IgnitionMXBean mbean = new IgnitionMXBeanAdapter();	ObjectName objName = U.makeMBeanName( null, "Kernal", Ignition.class.getSimpleName() );	if (!srv.queryMBeans(objName, null).isEmpty()) throw new IgniteCheckedException("MBean was already registered: " + objName);	else {	objName = U.registerMBean( srv, null, "Kernal", Ignition.class.getSimpleName(), mbean, IgnitionMXBean.class );	data = new GridMBeanServerData(objName);	mbeans.put(srv, data);	
registered mbean 

synchronized (mbeans) {	Iterator<Entry<MBeanServer, GridMBeanServerData>> iter = mbeans.entrySet().iterator();	while (iter.hasNext()) {	Entry<MBeanServer, GridMBeanServerData> entry = iter.next();	if (entry.getValue().containsIgniteInstance(name)) {	GridMBeanServerData data = entry.getValue();	assert data != null;	if (data.getCounter() == 1) {	try {	entry.getKey().unregisterMBean(data.getMbean());	
unregistered mbean 

while (iter.hasNext()) {	Entry<MBeanServer, GridMBeanServerData> entry = iter.next();	if (entry.getValue().containsIgniteInstance(name)) {	GridMBeanServerData data = entry.getValue();	assert data != null;	if (data.getCounter() == 1) {	try {	entry.getKey().unregisterMBean(data.getMbean());	}	catch (JMException e) {	
failed to unregister mbean 

========================= ignite sample_5539 =========================

private boolean casState(long state, GridDhtPartitionState toState) {	if (grp.persistenceEnabled() && grp.walEnabled()) {	synchronized (this) {	boolean update = this.state.compareAndSet(state, setPartState(state, toState));	if (update) try {	ctx.wal().log(new PartitionMetaStateRecord(grp.groupId(), id, toState, updateCounter()));	}	catch (IgniteCheckedException e) {	
error while writing to log 

public IgniteInternalFuture<?> rent(boolean updateSeq) {	long state = this.state.get();	GridDhtPartitionState partState = getPartState(state);	if (partState == RENTING || partState == EVICTED) return rent;	shouldBeRenting = true;	if (getReservations(state) == 0 && casState(state, RENTING)) {	shouldBeRenting = false;	
moved partition to renting state 

public void tryEvictAsync(boolean updateSeq) {	long state = this.state.get();	GridDhtPartitionState partState = getPartState(state);	if (isEmpty() && !grp.queriesEnabled() && getSize(state) == 0 && partState == RENTING && getReservations(state) == 0 && !groupReserved() && casState(state, EVICTED)) {	
evicted partition 

public void tryEvict() throws NodeStoppingException {	long state = this.state.get();	GridDhtPartitionState partState = getPartState(state);	if (partState != RENTING || getReservations(state) != 0 || groupReserved()) return;	if (addEvicting()) {	try {	clearAll();	if (isEmpty() && getSize(state) == 0 && casState(state, EVICTED)) {	
evicted partition 

catch (GridDhtInvalidPartitionException e) {	assert isEmpty() && state() == EVICTED : "Invalid error [e=" + e + ", part=" + this + ']';	break;	}	finally {	ctx.database().checkpointReadUnlock();	}	}	}	catch (NodeStoppingException e) {	
failed to get iterator for evicted partition 

finally {	ctx.database().checkpointReadUnlock();	}	}	}	catch (NodeStoppingException e) {	rent.onDone(e);	throw e;	}	catch (IgniteCheckedException e) {	
failed to get iterator for evicted partition 

grp.addCacheEvent(cached.partition(), cached.key(), ctx.localNodeId(), EVT_CACHE_REBALANCE_OBJECT_UNLOADED, null, false, cached.rawGet(), cached.hasValue(), false);	}	}	}	}	catch (GridDhtInvalidPartitionException e) {	assert isEmpty() && state() == EVICTED : "Invalid error [e=" + e + ", part=" + this + ']';	break;	}	catch (NodeStoppingException e) {	
failed to clear cache entry for evicted partition 

}	catch (GridDhtInvalidPartitionException e) {	assert isEmpty() && state() == EVICTED : "Invalid error [e=" + e + ", part=" + this + ']';	break;	}	catch (NodeStoppingException e) {	rent.onDone(e);	throw e;	}	catch (IgniteCheckedException e) {	
failed to clear cache entry for evicted partition 

========================= ignite sample_3814 =========================

assert id != null;	if (!cctx.isLocal()) {	while (true) {	AffinityTopologyVersion topVer = cctx.topologyVersionFuture().get();	Collection<ClusterNode> nodes = CU.affinityNodes(cctx, topVer);	try {	cctx.closures().callAsyncNoFailover(BROADCAST, new BlockSetCallable(cctx.name(), id), nodes, true, 0, false).get();	}	catch (IgniteCheckedException e) {	if (e.hasCause(ClusterTopologyCheckedException.class)) {	
removesetdata job failed will retry 

AffinityTopologyVersion topVer = cctx.topologyVersionFuture().get();	Collection<ClusterNode> nodes = CU.affinityNodes(cctx, topVer);	try {	cctx.closures().callAsyncNoFailover(BROADCAST, new BlockSetCallable(cctx.name(), id), nodes, true, 0, false).get();	}	catch (IgniteCheckedException e) {	if (e.hasCause(ClusterTopologyCheckedException.class)) {	continue;	}	else if (!pingNodes(nodes)) {	
removesetdata job failed and set data node left will retry 

else if (!pingNodes(nodes)) {	continue;	}	else throw e;	}	try {	cctx.closures().callAsyncNoFailover(BROADCAST, new RemoveSetDataCallable(cctx.name(), id, topVer), nodes, true, 0, false).get();	}	catch (IgniteCheckedException e) {	if (e.hasCause(ClusterTopologyCheckedException.class)) {	
removesetdata job failed will retry 

else throw e;	}	try {	cctx.closures().callAsyncNoFailover(BROADCAST, new RemoveSetDataCallable(cctx.name(), id, topVer), nodes, true, 0, false).get();	}	catch (IgniteCheckedException e) {	if (e.hasCause(ClusterTopologyCheckedException.class)) {	continue;	}	else if (!pingNodes(nodes)) {	
removesetdata job failed and set data node left will retry 

========================= ignite sample_3720 =========================

ClusterTopologyException cause = X.cause(e, ClusterTopologyException.class);	assert cause != null;	cause.retryReadyFuture().get();	}	else throw e;	}	}	return null;	}	catch (Throwable e) {	
unexpected error 

========================= ignite sample_2561 =========================

public void testInternalQuery() throws Exception {	CacheConfiguration<Object, Object> ccfg = cacheConfiguration(REPLICATED, 1, ATOMIC, false);	final IgniteCache<Object, Object> cache = grid(0).createCache(ccfg);	UUID uuid = null;	try {	for (int i = 0; i < 10; i++) cache.put(i, i);	final CountDownLatch latch = new CountDownLatch(5);	CacheEntryUpdatedListener lsnr = new CacheEntryUpdatedListener() {	for (Object evt : iterable) {	latch.countDown();	
received event 

========================= ignite sample_1930 =========================

assertParameter(cc.getWriteBehindFlushThreadCount() > 0, "writeBehindFlushThreadCount > 0");	if (cc.getWriteBehindFlushSize() == 0 && cc.getWriteBehindFlushFrequency() == 0) throw new IgniteCheckedException("Cannot set both 'writeBehindFlushFrequency' and " + "'writeBehindFlushSize' parameters to 0 for cache: " + U.maskName(cc.getName()));	}	if (cc.isReadThrough() && cfgStore == null) throw new IgniteCheckedException("Cannot enable read-through (loader or store is not provided) " + "for cache: " + U.maskName(cc.getName()));	if (cc.isWriteThrough() && cfgStore == null) throw new IgniteCheckedException("Cannot enable write-through (writer or store is not provided) " + "for cache: " + U.maskName(cc.getName()));	long delay = cc.getRebalanceDelay();	if (delay != 0) {	if (cc.getCacheMode() != PARTITIONED) U.warn(log, "Rebalance delay is supported only for partitioned caches (will ignore): " + (cc.getName()), "Will ignore rebalance delay for cache: " + U.maskName(cc.getName()));	else if (cc.getRebalanceMode() == SYNC) {	if (delay < 0) {	
ignoring sync rebalance mode with manual rebalance start node will not wait for rebalancing to be finished node will not wait for rebalance in sync mode 

}	if (cc.isReadThrough() && cfgStore == null) throw new IgniteCheckedException("Cannot enable read-through (loader or store is not provided) " + "for cache: " + U.maskName(cc.getName()));	if (cc.isWriteThrough() && cfgStore == null) throw new IgniteCheckedException("Cannot enable write-through (writer or store is not provided) " + "for cache: " + U.maskName(cc.getName()));	long delay = cc.getRebalanceDelay();	if (delay != 0) {	if (cc.getCacheMode() != PARTITIONED) U.warn(log, "Rebalance delay is supported only for partitioned caches (will ignore): " + (cc.getName()), "Will ignore rebalance delay for cache: " + U.maskName(cc.getName()));	else if (cc.getRebalanceMode() == SYNC) {	if (delay < 0) {	}	else {	
using sync rebalance mode with rebalance delay node will wait until rebalancing is initiated for ms for cache node will wait until rebalancing is initiated for ms for cache 

private void cleanup(CacheGroupContext grp) {	CacheConfiguration cfg = grp.config();	for (Object obj : grp.configuredUserObjects()) cleanup(cfg, obj, false);	if (!grp.systemCache()) {	try {	ctx.config().getMBeanServer().unregisterMBean(U.makeMBeanName(ctx.igniteInstanceName(), CACHE_GRP_METRICS_MBEAN_GRP, grp.cacheOrGroupName()));	}	catch (Throwable e) {	
failed to unregister mbean for cache group 

private void cleanup(CacheConfiguration cfg, @Nullable Object rsrc, boolean near) {	if (rsrc != null) {	unregisterMbean(rsrc, cfg.getName(), near);	try {	ctx.resource().cleanupGeneric(rsrc);	}	catch (IgniteCheckedException e) {	
failed to cleanup resource 

GridCacheContext<?, ?> cacheCtx = cache.context();	CacheConfiguration cfg = cacheCtx.config();	if (cfg.isStoreKeepBinary() && cfg.isStoreKeepBinary() != CacheConfiguration.DFLT_STORE_KEEP_BINARY && !(ctx.config().getMarshaller() instanceof BinaryMarshaller)) U.warn(log, "CacheConfiguration.isStoreKeepBinary() configuration property will be ignored because " + "BinaryMarshaller is not used");	for (GridCacheManager mgr : F.view(cacheCtx.managers(), F.notContains(dhtExcludes(cacheCtx)))) mgr.start(cacheCtx);	cacheCtx.initConflictResolver();	if (cfg.getCacheMode() != LOCAL && GridCacheUtils.isNearEnabled(cfg)) {	GridCacheContext<?, ?> dhtCtx = cacheCtx.near().dht().context();	for (GridCacheManager mgr : dhtManagers(dhtCtx)) mgr.start(dhtCtx);	dhtCtx.initConflictResolver();	dhtCtx.cache().start();	
started dht cache 

private void onKernalStart(GridCacheAdapter<?, ?> cache) throws IgniteCheckedException {	GridCacheContext<?, ?> ctx = cache.context();	if (isNearEnabled(ctx)) {	GridDhtCacheAdapter dht = ctx.near().dht();	GridCacheContext<?, ?> dhtCtx = dht.context();	for (GridCacheManager mgr : dhtManagers(dhtCtx)) mgr.onKernalStart();	dht.onKernalStart();	
executed onkernalstart callback for dht cache 

CacheGroupContext grp = new CacheGroupContext(sharedCtx, desc.groupId(), desc.receivedFrom(), cacheType, cfg, affNode, memPlc, cacheObjCtx, freeList, reuseList, exchTopVer, desc.walEnabled() );	for (Object obj : grp.configuredUserObjects()) prepare(cfg, obj, false);	U.startLifecycleAware(grp.configuredUserObjects());	grp.start();	CacheGroupContext old = cacheGrps.put(desc.groupId(), grp);	if (!grp.systemCache()) {	try {	U.registerMBean(ctx.config().getMBeanServer(), ctx.igniteInstanceName(), CACHE_GRP_METRICS_MBEAN_GRP, grp.cacheOrGroupName(), grp.mxBean(), CacheGroupMetricsMXBean.class);	}	catch (Throwable e) {	
failed to register mbean for cache group 

if (exchActions.systemCachesStarting() && exchActions.stateChangeRequest() == null) {	ctx.dataStructures().restoreStructuresState(ctx);	ctx.service().updateUtilityCache();	}	if (err == null) {	if (exchActions.cacheStopRequests().size() > 0) {	try {	sharedCtx.database().waitForCheckpoint("caches stop");	}	catch (IgniteCheckedException e) {	
failed to wait for checkpoint finish during cache stop 

IgniteTxManager tm = new IgniteTxManager();	GridCacheMvccManager mvccMgr = new GridCacheMvccManager();	GridCacheVersionManager verMgr = new GridCacheVersionManager();	GridCacheDeploymentManager depMgr = new GridCacheDeploymentManager();	GridCachePartitionExchangeManager exchMgr = new GridCachePartitionExchangeManager();	IgniteCacheDatabaseSharedManager dbMgr;	IgnitePageStoreManager pageStoreMgr = null;	IgniteWriteAheadLogManager walMgr = null;	if (CU.isPersistenceEnabled(ctx.config()) && !ctx.clientNode()) {	if (ctx.clientNode()) {	
persistent store is not supported on client nodes persistent store s configuration will be ignored 

public IgniteInternalFuture<?> resetCacheState(Collection<String> cacheNames) {	checkEmptyTransactions();	if (F.isEmpty(cacheNames)) cacheNames = cachesInfo.registeredCaches().keySet();	Collection<DynamicCacheChangeRequest> reqs = new ArrayList<>(cacheNames.size());	for (String cacheName : cacheNames) {	DynamicCacheDescriptor desc = cacheDescriptor(cacheName);	if (desc == null) {	
failed to find cache for reset lost partition request cache does not exist 

public <K, V> IgniteInternalCache<K, V> cache(String name) {	assert name != null;	
getting cache for name 

public <K, V> IgniteInternalCache<K, V> getOrStartCache(String name, CacheConfiguration ccfg) throws IgniteCheckedException {	assert name != null;	
getting cache for name 

public <K, V> IgniteInternalCache<K, V> publicCache(String name) {	assert name != null;	
getting public cache for name 

public <K, V> GridCacheAdapter<K, V> internalCache(String name) {	assert name != null;	
getting internal cache adapter 

needToUnregister = true;	break;	}	}	}	if (needToUnregister) {	try {	srvr.unregisterMBean(U.makeMBeanName(ctx.igniteInstanceName(), cacheName, o.getClass().getName()));	}	catch (Throwable e) {	
failed to unregister mbean for component 

========================= ignite sample_4231 =========================

int iterCnt = 20;	int keyCnt = 5000;	for (int i = 0; i < iterCnt; i++) {	int j = rnd.nextInt(keyCnt);	if (rnd.nextBoolean()) cache.put(j, j);	else cache.remove(j);	if (i != 0 && i % 1000 == 0) info("Stats [iterCnt=" + i + ", size=" + cache.size() + ']');	}	FifoEvictionPolicy<Integer, Integer> plc0 = (FifoEvictionPolicy<Integer, Integer>)this.plc;	if (!plc0.queue().isEmpty()) {	
policy queue item 

========================= ignite sample_2035 =========================

U.sleep(500);	stopGrid(NODES_CNT);	}	catch (Exception ignored) {	}	}	}	}, 1, "restart-thread");	long stopTime = System.currentTimeMillis() + 2 * 60_000L;	for (int i = 0; System.currentTimeMillis() < stopTime; i++) {	
iteration 

public void testNoDeadlock() throws Exception {	for (int i = 2; i <= 10; i++) {	final int threads = i;	
test with transactions 

========================= ignite sample_1619 =========================

DiscoCache curDiscoCache = cctx.discovery().discoCache();	DiscoCache discoCache = exchFut.events().discoveryCache();	List<ClusterNode> nodes = new ArrayList<>();	synchronized (this) {	for (ClusterNode node : discoCache.allNodes()) {	if (!node.isLocal() && cctx.discovery().alive(node)) {	awaited.add(node.id());	nodes.add(node);	}	else if (!node.isLocal()) {	
init new coordinator future will skip remote node 

GridDhtPartitionsSingleRequest req = GridDhtPartitionsSingleRequest.restoreStateRequest(exchFut.exchangeId(), exchFut.exchangeId());	for (ClusterNode node : nodes) {	try {	GridDhtPartitionsSingleRequest sndReq = req;	if (joinedNodes.containsKey(node.id())) {	sndReq = GridDhtPartitionsSingleRequest.restoreStateRequest( joinedNodes.get(node.id()), exchFut.exchangeId());	}	cctx.io().send(node, sndReq, GridIoPolicy.SYSTEM_POOL);	}	catch (ClusterTopologyCheckedException e) {	
failed to send partitions request node failed 

========================= ignite sample_3761 =========================

pool.schedule(new Runnable() {	try {	U.await(latch);	demoStartCb.call();	refreshTask = pool.scheduleWithFixedDelay(new Runnable() {	try {	RestResult top = restExecutor.topology(true, true);	client.emit(EVENT_DEMO_TOPOLOGY, toJSON(top));	}	catch (IOException e) {	
lost connection to the demo cluster 

========================= ignite sample_7372 =========================

if (grid(i).affinity(cacheName()).isPrimaryOrBackup(grid(i).localNode(), key)) {	IgnitePair<Long> curEntryTtl = entryTtl(jcache(i), key);	assertNotNull(curEntryTtl.get1());	assertNotNull(curEntryTtl.get2());	assertEquals(ttl, (long)curEntryTtl.get1());	assertTrue(curEntryTtl.get2() > startTime);	expireTimes[i] = curEntryTtl.get2();	}	}	U.sleep(100);	
put 

}	U.sleep(100);	tx = inTx ? transactions().txStart() : null;	try {	jcache().put(key, 4);	if (tx != null) tx.commit();	}	finally {	if (tx != null) tx.close();	}	
put done 

assert iter.hasNext();	}	System.gc();	for (int i = 0; i < 10; i++) {	try {	cache.size();	checkIteratorsCleared();	}	catch (AssertionFailedError e) {	if (i == 9) throw e;	
set iterators not cleared will wait 

========================= ignite sample_904 =========================

if (msg instanceof NodeIdMessage) {	sndId = U.bytesToUuid(((NodeIdMessage)msg).nodeIdBytes(), 0);	connKey = new ConnectionKey(sndId, 0, -1);	}	else {	assert msg instanceof HandshakeMessage : msg;	HandshakeMessage msg0 = (HandshakeMessage)msg;	sndId = ((HandshakeMessage)msg).nodeId();	connKey = new ConnectionKey(sndId, msg0.connectionIndex(), msg0.connectCount());	}	
remote node id received 

assert connKey != null && connKey.connectionIndex() >= 0 : connKey;	assert !usePairedConnections(node);	recovery.onHandshake(rcvCnt);	ses.inRecoveryDescriptor(recovery);	ses.outRecoveryDescriptor(recovery);	nioSrvr.resend(ses);	try {	if (sndRes) nioSrvr.sendSystem(ses, new RecoveryLastReceivedMessage(recovery.received()));	}	catch (IgniteCheckedException e) {	
failed to send message 

private void connectedNew( GridNioRecoveryDescriptor recovery, GridNioSession ses, boolean sndRes) {	try {	ses.inRecoveryDescriptor(recovery);	if (sndRes) nioSrvr.sendSystem(ses, new RecoveryLastReceivedMessage(recovery.received()));	recovery.onConnected();	}	catch (IgniteCheckedException e) {	
failed to send message 

StringBuilder sb = new StringBuilder();	dumpInfo(sb, null);	U.warn(log, sb.toString());	GridNioServer<Message> nioSrvr = this.nioSrvr;	if (nioSrvr != null) {	nioSrvr.dumpStats().listen(new CI1<IgniteInternalFuture<String>>() {	try {	U.warn(log, fut.get());	}	catch (Exception e) {	
failed to dump nio server statistics 

else {	buf = ByteBuffer.allocate(NodeIdMessage.MESSAGE_FULL_SIZE);	for (int i = 0; i < NodeIdMessage.MESSAGE_FULL_SIZE; ) {	int read = ch.read(buf);	if (read == -1) throw new HandshakeException("Failed to read remote node ID (connection closed).");	i += read;	}	}	UUID rmtNodeId0 = U.bytesToUuid(buf.array(), Message.DIRECT_TYPE_SIZE);	if (!rmtNodeId.equals(rmtNodeId0)) throw new HandshakeException("Remote node ID is not as expected [expected=" + rmtNodeId + ", rcvd=" + rmtNodeId0 + ']');	
received remote node id 

}	rcvCnt = buf.getLong(Message.DIRECT_TYPE_SIZE);	}	if (log.isDebugEnabled()) log.debug("Received handshake message [rmtNode=" + rmtNodeId + ", rcvCnt=" + rcvCnt + ']');	if (rcvCnt == -1) {	if (log.isDebugEnabled()) log.debug("Connection rejected, will retry client creation [rmtNode=" + rmtNodeId + ']');	}	}	}	catch (IOException e) {	
failed to read from channel 

private GridNioRecoveryDescriptor recoveryDescriptor( ConcurrentMap<ConnectionKey, GridNioRecoveryDescriptor> recoveryDescs, boolean pairedConnections, ClusterNode node, ConnectionKey key) {	GridNioRecoveryDescriptor recovery = recoveryDescs.get(key);	if (recovery == null) {	if (log.isDebugEnabled()) log.debug("Missing recovery descriptor for the node (will create a new one) " + "[locNodeId=" + getLocalNode().id() + ", key=" + key + ", rmtNode=" + node + ']');	int maxSize = Math.max(msgQueueLimit, ackSndThreshold);	int queueLimit = unackedMsgsBufSize != 0 ? unackedMsgsBufSize : (maxSize * 128);	GridNioRecoveryDescriptor old = recoveryDescs.putIfAbsent(key, recovery = new GridNioRecoveryDescriptor(pairedConnections, queueLimit, node, log));	if (old != null) {	recovery = old;	
will use existing recovery descriptor 

private void processIdle() {	cleanupRecovery();	for (Map.Entry<UUID, GridCommunicationClient[]> e : clients.entrySet()) {	UUID nodeId = e.getKey();	for (GridCommunicationClient client : e.getValue()) {	if (client == null) continue;	ClusterNode node = getSpiContext().node(nodeId);	if (node == null) {	
forcing close of non existent node connection 

if (!usePairedConnections(node) && client instanceof GridTcpNioCommunicationClient) {	recovery = recoveryDescs.get(new ConnectionKey(node.id(), client.connectionIndex(), -1));	if (recovery != null && recovery.lastAcknowledged() != recovery.received()) {	RecoveryLastReceivedMessage msg = new RecoveryLastReceivedMessage(recovery.received());	if (log.isDebugEnabled()) log.debug("Send recovery acknowledgement on timeout [rmtNode=" + nodeId + ", rcvCnt=" + msg.received() + ']');	try {	nioSrvr.sendSystem(((GridTcpNioCommunicationClient)client).session(), msg);	recovery.lastAcknowledged(msg.received());	}	catch (IgniteCheckedException err) {	
failed to send message 

}	catch (IgniteCheckedException err) {	}	continue;	}	}	long idleTime = client.getIdleTime();	if (idleTime >= idleConnTimeout) {	if (recovery == null && usePairedConnections(node)) recovery = outRecDescs.get(new ConnectionKey(node.id(), client.connectionIndex(), -1));	if (recovery != null && recovery.nodeAlive(getSpiContext().node(nodeId)) && !recovery.messagesRequests().isEmpty()) {	
node connection is idle but there are unacknowledged messages will wait 

}	continue;	}	}	long idleTime = client.getIdleTime();	if (idleTime >= idleConnTimeout) {	if (recovery == null && usePairedConnections(node)) recovery = outRecDescs.get(new ConnectionKey(node.id(), client.connectionIndex(), -1));	if (recovery != null && recovery.nodeAlive(getSpiContext().node(nodeId)) && !recovery.messagesRequests().isEmpty()) {	continue;	}	
closing idle node connection 

if (recovery != null && recovery.lastAcknowledged() != recovery.received()) {	RecoveryLastReceivedMessage msg = new RecoveryLastReceivedMessage(recovery.received());	if (log.isDebugEnabled()) {	log.debug("Send recovery acknowledgement on timeout [rmtNode=" + recovery.node().id() + ", rcvCnt=" + msg.received() + ", lastAcked=" + recovery.lastAcknowledged() + ']');	}	try {	nioSrvr.sendSystem(ses, msg);	recovery.lastAcknowledged(msg.received());	}	catch (IgniteCheckedException e) {	
failed to send message 

if (recoveryDesc.nodeAlive(getSpiContext().node(node.id())) && getSpiContext().pingNode(node.id())) {	if (log.isDebugEnabled()) log.debug("Recovery reconnect failed, will retry " + "[rmtNode=" + recoveryDesc.node().id() + ", err=" + e + ']');	addProcessDisconnectRequest(sesInfo);	}	else {	if (log.isDebugEnabled()) log.debug("Recovery reconnect failed, " + "node left [rmtNode=" + recoveryDesc.node().id() + ", err=" + e + ']');	onException("Recovery reconnect failed, node left [rmtNode=" + recoveryDesc.node().id() + "]", e);	}	}	catch (IgniteClientDisconnectedException ignored) {	
failed to ping node client disconnected 

========================= ignite sample_5748 =========================

private void waitSetResourcesCleared() throws IgniteCheckedException {	final int MAX_CHECK = 5;	for (int i = 0; i < MAX_CHECK; i++) {	try {	assertSetResourcesCleared();	return;	}	catch (AssertionFailedError e) {	if (i == MAX_CHECK - 1) throw e;	
set resources not cleared will wait more 

for (int i = 0; i < 5000; i++) assertTrue(set0.add(i));	createIterators(set0);	System.gc();	for (int i = 0; i < 10; i++) {	try {	set0.size();	assertSetIteratorsCleared();	}	catch (AssertionFailedError e) {	if (i == 9) throw e;	
set iterators not cleared will wait 

}	createIterators(set0);	int idx = gridCount() > 1 ? 1 : 0;	grid(idx).set(SET_NAME, null).close();	for (int i = 0; i < 10; i++) {	try {	assertSetIteratorsCleared();	}	catch (AssertionFailedError e) {	if (i == 9) throw e;	
set iterators not cleared will wait 

final AtomicInteger val = new AtomicInteger(10_000);	IgniteInternalFuture<?> fut;	try {	fut = GridTestUtils.runMultiThreadedAsync(new Callable<Object>() {	try {	while (!stop.get()) {	for (Set<Integer> set : sets) set.add(val.incrementAndGet());	}	}	catch (IllegalStateException e) {	
set removed 

========================= ignite sample_928 =========================

if (log == null) {	msgLog = cctx.txRecoveryMessageLogger();	log = U.logger(cctx.kernalContext(), logRef, GridCacheTxRecoveryFuture.class);	}	nodes = new GridLeanMap<>();	UUID locNodeId = cctx.localNodeId();	for (Map.Entry<UUID, Collection<UUID>> e : tx.transactionNodes().entrySet()) {	if (!locNodeId.equals(e.getKey()) && !failedNodeIds.contains(e.getKey()) && !nodes.containsKey(e.getKey())) {	ClusterNode node = cctx.discovery().node(e.getKey());	if (node != null) nodes.put(node.id(), node);	
transaction node left will ignore 

UUID locNodeId = cctx.localNodeId();	for (Map.Entry<UUID, Collection<UUID>> e : tx.transactionNodes().entrySet()) {	if (!locNodeId.equals(e.getKey()) && !failedNodeIds.contains(e.getKey()) && !nodes.containsKey(e.getKey())) {	ClusterNode node = cctx.discovery().node(e.getKey());	if (node != null) nodes.put(node.id(), node);	}	for (UUID nodeId : e.getValue()) {	if (!locNodeId.equals(nodeId) && !failedNodeIds.contains(nodeId) && !nodes.containsKey(nodeId)) {	ClusterNode node = cctx.discovery().node(nodeId);	if (node != null) nodes.put(node.id(), node);	
transaction node left will ignore 

========================= ignite sample_3872 =========================

private void undoLocks(boolean dist, boolean rollback) {	if (dist && tx == null) cctx.colocated().removeLocks(threadId, lockVer, keys);	else {	if (rollback && tx != null) {	if (tx.setRollbackOnly()) {	
marked transaction as rollback only because locks could not be acquired 

private void undoLocks(boolean dist, boolean rollback) {	if (dist && tx == null) cctx.colocated().removeLocks(threadId, lockVer, keys);	else {	if (rollback && tx != null) {	if (tx.setRollbackOnly()) {	}	
transaction was not marked rollback only while locks were not acquired 

if (!DONE_UPD.compareAndSet(this, 0, 1)) return false;	if (!success) undoLocks(distribute, true);	if (tx != null) {	cctx.tm().txContext(tx);	if (restoreTimeout && tx.trackTimeout()) {	boolean add = tx.addTimeoutHandler();	assert add;	}	}	if (super.onDone(success, err)) {	
completing future 

GridNearLockMapping map = null;	for (KeyCacheObject key : keys) {	GridNearLockMapping updated = map(key, map, topVer);	if (updated != map) {	mappings.add(updated);	if (tx != null && updated.node().isLocal()) tx.colocatedLocallyMapped(true);	}	map = updated;	}	if (isDone()) {	
abandoning re map because future is done 

entry = cctx.colocated().entryExx(key, topVer, true);	txEntry.cached(entry);	}	}	}	boolean explicit;	while (true) {	try {	if (entry == null) entry = cctx.colocated().entryExx(key, topVer, true);	if (!cctx.isAll(entry, filter)) {	
entry being locked did not pass filter will not lock 

}	distributedKeys.add(key);	if (tx != null) tx.addKeyMapping(txKey, mapping.node());	req.addKeyBytes( key, retval, dhtVer, cctx);	}	explicit = inTx() && cand == null;	if (explicit) tx.addKeyMapping(txKey, mapping.node());	break;	}	catch (GridCacheEntryRemovedException ignored) {	
got removed entry in lockasync method will retry 

private void lockLocally( final Collection<KeyCacheObject> keys, AffinityTopologyVersion topVer ) {	
before locally locking keys 

private boolean addLocalKey( KeyCacheObject key, AffinityTopologyVersion topVer, Collection<KeyCacheObject> distributedKeys ) throws IgniteCheckedException {	GridDistributedCacheEntry entry = cctx.colocated().entryExx(key, topVer, false);	assert !entry.detached();	if (!cctx.isAll(entry, filter)) {	
entry being locked did not pass filter will not lock 

========================= ignite sample_3825 =========================

return;	}	if (!authChecker.apply(req.getHeader("X-Signature"))) {	res.setStatus(HttpServletResponse.SC_UNAUTHORIZED);	return;	}	GridRestResponse cmdRes;	Map<String, Object> params = parameters(req);	try {	GridRestRequest cmdReq = createRequest(cmd, params, req);	
initialized command request 

U.error(log, "Failed to process HTTP request [action=" + act + ", req=" + req + ']', e);	if (e instanceof Error) throw (Error)e;	cmdRes = new GridRestResponse(STATUS_FAILED, e.getMessage());	}	try {	ServletOutputStream os = res.getOutputStream();	try {	jsonMapper.writeValue(os, cmdRes);	}	catch (JsonProcessingException e) {	
failed to convert response to json 

ServletOutputStream os = res.getOutputStream();	try {	jsonMapper.writeValue(os, cmdRes);	}	catch (JsonProcessingException e) {	jsonMapper.writeValue(os, F.asMap("successStatus", STATUS_FAILED, "error", e.getMessage()));	}	if (log.isDebugEnabled()) log.debug("Processed HTTP request [action=" + act + ", jsonRes=" + cmdRes + ", req=" + req + ']');	}	catch (IOException e) {	
failed to send http response 

========================= ignite sample_7866 =========================

private void init() {	if (!initGuard.compareAndSet(false, true)) return;	String sysPropZkConnString = System.getProperty(PROP_ZK_CONNECTION_STRING);	if (sysPropZkConnString != null && sysPropZkConnString.trim().length() > 0) zkConnectionString = sysPropZkConnString;	
initializing zookeeper ip finder 

========================= ignite sample_6261 =========================

public void testQueueRemoveMultithreadBounded() throws Exception {	final String queueName = UUID.randomUUID().toString();	final IgniteQueue<String> queue = grid(0).queue(queueName, QUEUE_CAPACITY, config(false));	final CountDownLatch putLatch = new CountDownLatch(THREAD_NUM);	final CountDownLatch clearLatch = new CountDownLatch(THREAD_NUM);	IgniteInternalFuture<?> offerFut = GridTestUtils.runMultiThreadedAsync(new Callable<Void>() {	
thread has been started 

IgniteInternalFuture<?> offerFut = GridTestUtils.runMultiThreadedAsync(new Callable<Void>() {	try {	for (int i = 0; i < (QUEUE_CAPACITY * THREAD_NUM); i++) queue.offer("anything", 3, TimeUnit.MINUTES);	fail("Queue failed");	}	catch (IgniteException | IllegalStateException e) {	putLatch.countDown();	assert e.getMessage().contains("removed");	assert queue.removed();	}	
thread has been stopped 

========================= ignite sample_990 =========================

public void testQueryCancelsOnGridShutdown() throws Exception {	try (Ignite client = startGrid("client")) {	IgniteCache<Object, Object> cache = client.cache(DEFAULT_CACHE_NAME);	assertEquals(0, cache.localSize());	int p = 1;	for (int i = 1; i <= CACHE_SIZE; i++) {	char[] tmp = new char[VAL_SIZE];	Arrays.fill(tmp, ' ');	cache.put(i, new String(tmp));	if (i / (float)CACHE_SIZE >= p / 10f) {	
loaded of 

========================= ignite sample_7470 =========================

private boolean checkDeployed(Ignite ignite, String taskName) {	Map<String, Class<? extends ComputeTask<?, ?>>> locTasks = ignite.compute().localTasks();	
local tasks found 

========================= ignite sample_2222 =========================

}	}	cache.putAll(map);	Map<Integer, Integer> res = cache.getAll(map.keySet());	assertEquals(map, res);	}	catch (IgniteClientDisconnectedException e) {	throw e;	}	catch (IgniteException e) {	
ignore error 

Map<Integer, Integer> res = cache.getAll(map.keySet());	assertEquals(map, res);	}	catch (IgniteClientDisconnectedException e) {	throw e;	}	catch (IgniteException e) {	}	catch (CacheException e) {	if (e.getCause() instanceof IgniteClientDisconnectedException) throw e;	
ignore error 

========================= ignite sample_656 =========================

try {	fut.onDone(f.get());	}	catch (IgniteCheckedException e) {	fut.onDone(e);	}	}	});	}	catch (Throwable e) {	
client request execution failed with error 

private IgniteInternalFuture<GridRestResponse> handleRequest(final GridRestRequest req) {	if (startLatch.getCount() > 0) {	try {	startLatch.await();	}	catch (InterruptedException e) {	return new GridFinishedFuture<>(new IgniteCheckedException("Failed to handle request " + "(protocol handler was interrupted when awaiting grid start).", e));	}	}	
received request from client 

IgniteInternalFuture<GridRestResponse> res = hnd == null ? null : hnd.handleAsync(req);	if (res == null) return new GridFinishedFuture<>( new IgniteCheckedException("Failed to find registered handler for command: " + req.command()));	return res.chain(new C1<IgniteInternalFuture<GridRestResponse>, GridRestResponse>() {	GridRestResponse res;	boolean failed = false;	try {	res = f.get();	}	catch (Exception e) {	failed = true;	
failed to handle request 

private void addHandler(GridRestCommandHandler hnd) {	assert !handlers.containsValue(hnd);	
added rest command handler 

private void startHttpProtocol() throws IgniteCheckedException {	try {	Class<?> cls = Class.forName(HTTP_PROTO_CLS);	Constructor<?> ctor = cls.getConstructor(GridKernalContext.class);	GridRestProtocol proto = (GridRestProtocol)ctor.newInstance(ctx);	startProtocol(proto);	}	catch (ClassNotFoundException ignored) {	
failed to initialize http rest protocol consider adding ignite rest http module to classpath 

private void startProtocol(GridRestProtocol proto) throws IgniteCheckedException {	assert proto != null;	assert !protos.contains(proto);	protos.add(proto);	proto.start(protoHnd);	
added rest protocol 

========================= ignite sample_4925 =========================

private void load0( Collection<? extends DataStreamerEntry> entries, final GridFutureAdapter<Object> resFut, @Nullable final Collection<KeyCacheObjectWrapper> activeKeys, @Nullable final Collection<KeyCacheObjectWrapper> activeKeys, final int remaps ) {	try {	assert entries != null;	final boolean remap = remaps > 0;	if (!remap) {	acquireRemapSemaphore();	}	if (!isWarningPrinted) {	synchronized (this) {	if (!allowOverwrite() && !isWarningPrinted) {	
data streamer will not overwrite existing cache entries for better performance to change set allowoverwrite to true 

for (IgniteInternalFuture fut = q.poll(); fut != null; fut = q.poll()) {	try {	if (timeout == DFLT_UNLIMIT_TIMEOUT) fut.get();	else {	long timeRemain = timeout - U.currentTimeMillis() + startTimeMillis;	if (timeRemain <= 0) throw new IgniteDataStreamerTimeoutException("Data streamer exceeded timeout on flush.");	fut.get(timeRemain);	}	}	catch (IgniteClientDisconnectedCheckedException e) {	
failed to flush buffer 

else {	long timeRemain = timeout - U.currentTimeMillis() + startTimeMillis;	if (timeRemain <= 0) throw new IgniteDataStreamerTimeoutException("Data streamer exceeded timeout on flush.");	fut.get(timeRemain);	}	}	catch (IgniteClientDisconnectedCheckedException e) {	throw CU.convertToCacheException(e);	}	catch (IgniteFutureTimeoutCheckedException e) {	
failed to flush buffer 

fut.get(timeRemain);	}	}	catch (IgniteClientDisconnectedCheckedException e) {	throw CU.convertToCacheException(e);	}	catch (IgniteFutureTimeoutCheckedException e) {	throw new IgniteDataStreamerTimeoutException("Data streamer exceeded timeout on flush.", e);	}	catch (IgniteCheckedException e) {	
failed to flush buffer 

private void incrementActiveTasks() throws IgniteInterruptedCheckedException {	if (timeout == DFLT_UNLIMIT_TIMEOUT) U.acquire(sem);	else if (!U.tryAcquire(sem, timeout, TimeUnit.MILLISECONDS)) {	
failed to add parallel operation 

CacheObject val = e.getValue();	if (val != null) val.prepareMarshal(cacheObjCtx);	}	if (updaterBytes == null) {	assert rcvr != null;	updaterBytes = U.marshal(ctx, rcvr);	}	if (topicBytes == null) topicBytes = U.marshal(ctx, topic);	}	catch (IgniteCheckedException e) {	
failed to marshal request will not be sent 

}	GridDeployment dep = null;	GridPeerDeployAware jobPda0 = jobPda;	if (ctx.deploy().enabled() && jobPda0 != null) {	try {	dep = ctx.deploy().deploy(jobPda0.deployClass(), jobPda0.classLoader());	GridCacheAdapter<Object, Object> cache = ctx.cache().internalCache(cacheName);	if (cache != null) cache.context().deploy().onEnter();	}	catch (IgniteCheckedException e) {	
failed to deploy class request will not be sent 

========================= ignite sample_4564 =========================

private void registerMBean() {	if (U.IGNITE_MBEANS_DISABLED) return;	try {	ObjectName objName = U.registerMBean( ManagementFactory.getPlatformMBeanServer(), "Router", "TCP Router " + id, getClass().getSimpleName(), this, GridTcpRouterMBean.class);	
registered mbean 

private void registerMBean() {	if (U.IGNITE_MBEANS_DISABLED) return;	try {	ObjectName objName = U.registerMBean( ManagementFactory.getPlatformMBeanServer(), "Router", "TCP Router " + id, getClass().getSimpleName(), this, GridTcpRouterMBean.class);	mbeanName = objName;	}	catch (JMException e) {	
failed to register mbean 

private void unregisterMBean() {	if (mbeanName == null) return;	assert !U.IGNITE_MBEANS_DISABLED;	try {	ManagementFactory.getPlatformMBeanServer().unregisterMBean(mbeanName);	
unregistered mbean 

private void unregisterMBean() {	if (mbeanName == null) return;	assert !U.IGNITE_MBEANS_DISABLED;	try {	ManagementFactory.getPlatformMBeanServer().unregisterMBean(mbeanName);	}	catch (JMException e) {	
failed to unregister mbean 

sslFilter.wantClientAuth(wantClientAuth);	sslFilter.needClientAuth(needClientAuth);	filters = new GridNioFilter[] { codec, sslFilter };	}	else filters = new GridNioFilter[] { codec };	srv = GridNioServer.<GridClientMessage>builder() .address(hostAddr) .port(port) .listener(lsnr) .logger(log) .selectorCount(Runtime.getRuntime().availableProcessors()) .igniteInstanceName(igniteInstanceName) .serverName("router") .tcpNoDelay(tcpNoDelay) .directBuffer(false) .byteOrder(ByteOrder.nativeOrder()) .socketSendBufferSize(0) .socketReceiveBufferSize(0) .sendQueueLimit(0) .filters(filters) .idleTimeout(cfg.getIdleTimeout()) .build();	srv.start();	return true;	}	catch (IgniteCheckedException e) {	
failed to start tcp router protocol on port 

========================= ignite sample_5230 =========================

GridCacheEntryEx entry = null;	AffinityTopologyVersion topVer = ctx.affinity().affinityTopologyVersion();	try {	entry = entryEx(key, topVer);	GridCacheOperation op = val != null ? UPDATE : DELETE;	GridCacheUpdateAtomicResult updRes = entry.innerUpdate( ver, nodeId, nodeId, op, val, null, keepBinary, topVer, CU.empty0(), DR_NONE, ttl, expireTime, null, false, false, subjId, taskName, null, null, null);	if (updRes.removeVersion() != null) ctx.onDeferredDelete(entry, updRes.removeVersion());	break;	}	catch (GridCacheEntryRemovedException ignored) {	
got removed entry while updating near cache value will retry 

========================= ignite sample_3864 =========================

private void affinityCallRestartNode() throws Exception {	final int ITERS = 10;	for (int i = 0; i < ITERS; i++) {	
iteration 

========================= ignite sample_894 =========================

public void testEntryUpdate() throws Exception {	try (Ignite ignite = startGrid()) {	for (int i = 0; i < CACHE_COUNT; ++i) {	IgniteCache<Long, byte[]> cache = ignite.cache(CACHE_PREFIX + i);	cache.put(0L, new byte[PAGE_SIZE * 2]);	}	IgniteCompute compute = ignite.compute().withAsync();	long endTime = System.currentTimeMillis() + WAIT_TIMEOUT;	int iter = 0;	while (System.currentTimeMillis() < endTime) {	
iteration 

========================= ignite sample_528 =========================

public void testInvoke() throws Exception {	IgniteCache<Integer, Integer> cache = grid(0).cache(DEFAULT_CACHE_NAME);	final Integer key = primaryKey(cache);	cache.put(key, 0);	final int THREADS = gridCount();	final int ITERATIONS_PER_THREAD = 1000;	Integer expVal = 0;	for (int i = 0; i < iterations(); i++) {	
iteration 

========================= ignite sample_1438 =========================

TestCommunicationSpi spi = (TestCommunicationSpi)ignite2.configuration().getCommunicationSpi();	spi.blockMessages(GridNearLockRequest.class, ignite0.localNode().id());	spi.blockMessages(GridNearLockRequest.class, ignite1.localNode().id());	final IgniteCache<Integer, Integer> cache = ignite2.cache(DEFAULT_CACHE_NAME);	final CountDownLatch lockedLatch = new CountDownLatch(1);	final CountDownLatch unlockLatch = new CountDownLatch(1);	IgniteInternalFuture<Lock> lockFut = GridTestUtils.runAsync(new Callable<Lock>() {	Thread.currentThread().setName("put-thread");	Lock lock = cache.lockAll(keys);	lock.lock();	
Locked 

}	finally {	cache0.context().evicts().touch(entry, cache0.context().affinity().affinityTopologyVersion());	}	}	else assertNull("Unexpected non-null value for " + node.name(), val0);	}	}	}	catch (AssertionError e) {	
check failed will retry 

try (Transaction tx = txs.txStart(concurrency, isolation)) {	cache.putAll(map);	tx.commit();	}	}	else cache.putAll(map);	putKeys.addAll(map.keySet());	}	}	catch (CacheException | IgniteException e) {	
operation failed ignore 

if (updateBarrier != null) updateBarrier.await();	}	return null;	}	}, THREADS, "update-thread");	long stopTime = System.currentTimeMillis() + 60_000;	while (System.currentTimeMillis() < stopTime) {	boolean restartClient = ThreadLocalRandom.current().nextBoolean();	Integer idx = null;	if (restartClient) {	
start client node 

boolean restartClient = ThreadLocalRandom.current().nextBoolean();	Integer idx = null;	if (restartClient) {	client = true;	IgniteEx ignite = startGrid(SRV_CNT + CLIENT_CNT);	IgniteCache<Integer, Integer> cache = ignite.cache(DEFAULT_CACHE_NAME);	assertNotNull(cache);	}	else {	idx = ThreadLocalRandom.current().nextInt(0, SRV_CNT);	
stop server node 

stopGrid(idx);	}	updateBarrier = new CyclicBarrier(THREADS + 1, new Runnable() {	updateBarrier = null;	}	});	try {	updateBarrier.await(30_000, TimeUnit.MILLISECONDS);	}	catch (TimeoutException ignored) {	
failed to wait for update 

}	catch (TimeoutException ignored) {	for (Ignite ignite : G.allGrids()) ((IgniteKernal)ignite).dumpDebugInfo();	U.dumpThreads(log);	CyclicBarrier barrier0 = updateBarrier;	if (barrier0 != null) barrier0.reset();	fail("Failed to wait for update.");	}	U.sleep(500);	if (restartClient) {	
stop client node 

U.dumpThreads(log);	CyclicBarrier barrier0 = updateBarrier;	if (barrier0 != null) barrier0.reset();	fail("Failed to wait for update.");	}	U.sleep(500);	if (restartClient) {	stopGrid(SRV_CNT + CLIENT_CNT);	}	else {	
start server node 

========================= ignite sample_1204 =========================

private void exportOsgiService(Ignite ignite) {	Dictionary<String, String> dict = new Hashtable<>();	if (ignite.name() != null) dict.put(OSGI_SERVICE_PROP_IGNITE_NAME, ignite.name());	bundleCtx.registerService(Ignite.class, ignite, dict);	
exported osgi service for ignite with properties 

========================= ignite sample_6191 =========================

amContainer.setCommands( Collections.singletonList( Environment.JAVA_HOME.$() + "/bin/java -Xmx512m " + ApplicationMaster.class.getName() + IgniteYarnUtils.SPACE + ignite.toUri() + IgniteYarnUtils.YARN_LOG_OUT ) );	LocalResource appMasterJar = IgniteYarnUtils.setupFile(appJar, fs, LocalResourceType.FILE);	amContainer.setLocalResources(Collections.singletonMap(IgniteYarnUtils.JAR_NAME, appMasterJar));	Map<String, String> appMasterEnv = props.toEnvs();	setupAppMasterEnv(appMasterEnv, conf);	amContainer.setEnvironment(appMasterEnv);	if (UserGroupInformation.isSecurityEnabled()) {	Credentials creds = new Credentials();	String tokRenewer = conf.get(YarnConfiguration.RM_PRINCIPAL);	if (tokRenewer == null || tokRenewer.length() == 0) throw new IOException("Master Kerberos principal for the RM is not set.");	
found rm principal 

Resource capability = Records.newRecord(Resource.class);	capability.setMemory(512);	capability.setVirtualCores(1);	ApplicationSubmissionContext appContext = app.getApplicationSubmissionContext();	appContext.setApplicationName("ignition");	appContext.setAMContainerSpec(amContainer);	appContext.setResource(capability);	appContext.setQueue("default");	ApplicationId appId = appContext.getApplicationId();	yarnClient.submitApplication(appContext);	
submitted application application id 

ApplicationId appId = appContext.getApplicationId();	yarnClient.submitApplication(appContext);	ApplicationReport appReport = yarnClient.getApplicationReport(appId);	YarnApplicationState appState = appReport.getYarnApplicationState();	while (appState == YarnApplicationState.NEW || appState == YarnApplicationState.NEW_SAVING || appState == YarnApplicationState.SUBMITTED || appState == YarnApplicationState.ACCEPTED) {	TimeUnit.SECONDS.sleep(1L);	appReport = yarnClient.getApplicationReport(appId);	if (appState != YarnApplicationState.ACCEPTED && appReport.getYarnApplicationState() == YarnApplicationState.ACCEPTED) log.log(Level.INFO, "Application {0} is ACCEPTED.", appId);	appState = appReport.getYarnApplicationState();	}	
application is 

========================= ignite sample_6200 =========================

private void startNameServer() throws Exception {	NamesrvConfig namesrvConfig = new NamesrvConfig();	NettyServerConfig nameServerNettyServerConfig = new NettyServerConfig();	namesrvConfig.setKvConfigPath(System.getProperty("java.io.tmpdir") + separator + "namesrv" + separator + "kvConfig.json");	nameServerNettyServerConfig.setListenPort(NAME_SERVER_PORT);	nameSrv = new NamesrvController(namesrvConfig, nameServerNettyServerConfig);	nameSrv.initialize();	nameSrv.start();	
started nameserver at 

brokerCfg.setBrokerClusterName(TEST_CLUSTER);	brokerCfg.setBrokerIP1(TEST_IP);	brokerCfg.setNamesrvAddr(TEST_IP + ":" + NAME_SERVER_PORT);	storeCfg.setStorePathRootDir(System.getProperty("java.io.tmpdir") + separator + "store-" + UUID.randomUUID());	storeCfg.setStorePathCommitLog(System.getProperty("java.io.tmpdir") + separator + "commitlog");	storeCfg.setHaListenPort(HA_PORT);	nettySrvCfg.setListenPort(BROKER_PORT);	broker = new BrokerController(brokerCfg, nettySrvCfg, new NettyClientConfig(), storeCfg);	broker.initialize();	broker.start();	
started broker at 

========================= ignite sample_7942 =========================

public void testStartNodes() throws Exception {	long stopTime = System.currentTimeMillis() + 2 * 60_000;	try {	int iter = 0;	while (System.currentTimeMillis() < stopTime && iter < 5) {	
iteration 

========================= ignite sample_1195 =========================

private void prepareSingle(IgniteTxEntry write, boolean topLocked, boolean remap) {	write.clearEntryReadVersion();	AffinityTopologyVersion topVer = tx.topologyVersion();	assert topVer.topologyVersion() > 0;	txMapping = new GridDhtTxMapping();	GridDistributedTxMapping mapping = map(write, topVer, null, topLocked, remap);	if (isDone()) {	
abandoning re map because future is done 

GridDistributedTxMapping prev = map.put(key, updated);	if (prev != null) prev.last(false);	if (updated.primary().isLocal()) {	if (write.context().isNear()) tx.nearLocallyMapped(true);	else if (write.context().isColocated()) tx.colocatedLocallyMapped(true);	}	cur = updated;	}	}	if (isDone()) {	
abandoning re map because future is done 

========================= ignite sample_3844 =========================

private void cacheOp(IgniteCache<Integer, Integer> cache) {	boolean b = cache.putIfAbsent(42, 42);	
putifabsent 

private void cacheOp(IgniteCache<Integer, Integer> cache) {	boolean b = cache.putIfAbsent(42, 42);	Integer val = cache.get(42);	
get 

========================= ignite sample_1480 =========================

int global = globalCntrMultiNode.get();	assert curCntr >= global : invalid("Counter mismatch", near, false, curCntr, global);	int newCntr = curCntr + 1;	if (DEBUG) log.info("Setting global counter [old=" + global + ", new=" + newCntr + ']');	assert globalCntrMultiNode.compareAndSet(global, newCntr) : invalid("Invalid global counter", near, false, newCntr, global);	int prev = c.getAndPut(CNTR_KEY, newCntr);	if (DEBUG) log.info("Put new value [igniteInstanceName=" + near.name() + ", primary=false, prev=" + prev + ", newCntr=" + newCntr + ']');	assert curCntr == prev : invalid("Counter mismatch", near, false, curCntr, prev);	assertTrue(lockedMultiNode.compareAndSet(true, false));	tx.commit();	
committed tx 

int newCntr = curCntr + 1;	if (DEBUG) log.info("Setting global counter [old=" + global + ", new=" + newCntr + ']');	assert globalCntrMultiNode.compareAndSet(global, newCntr) : invalid("Invalid global counter", pri, true, newCntr, global);	int prev = c.getAndPut(CNTR_KEY, newCntr);	if (DEBUG) {	log.info("Put new value [igniteInstanceName=" + pri.name() + ", primary=true, prev=" + prev + ", newCntr=" + newCntr + ']');	}	assert curCntr == prev : invalid("Counter mismatch", pri, true, curCntr, prev);	assertTrue(lockedMultiNode.compareAndSet(true, false));	tx.commit();	
committed tx 

========================= ignite sample_1253 =========================

Ignite client = ignite(SRV_CNT);	assertTrue(client.configuration().isClientMode());	final IgniteCache<Integer, Integer> clientCache = client.createNearCache(DEFAULT_CACHE_NAME, new NearCacheConfiguration<Integer, Integer>());	final AtomicBoolean stop = new AtomicBoolean();	IgniteInternalFuture<?> restartFut = null;	final Integer key0 = primaryKey(ignite(SRV_CNT - 1).cache(DEFAULT_CACHE_NAME));	if (restart) {	restartFut = GridTestUtils.runAsync(new Callable<Void>() {	while (!stop.get()) {	Thread.sleep(300);	
stop node 

final IgniteCache<Integer, Integer> clientCache = client.createNearCache(DEFAULT_CACHE_NAME, new NearCacheConfiguration<Integer, Integer>());	final AtomicBoolean stop = new AtomicBoolean();	IgniteInternalFuture<?> restartFut = null;	final Integer key0 = primaryKey(ignite(SRV_CNT - 1).cache(DEFAULT_CACHE_NAME));	if (restart) {	restartFut = GridTestUtils.runAsync(new Callable<Void>() {	while (!stop.get()) {	Thread.sleep(300);	stopGrid(SRV_CNT - 1);	Thread.sleep(300);	
start node 

========================= ignite sample_1316 =========================

for (Integer key : keys) {	info("Put " + key);	cache.put(key, 2);	}	info("Commit.");	tx.commit();	}	fail("Transaction should fail.");	}	catch (IgniteException e) {	
expected exception 

private void checkValue(final Integer key, boolean putBefore) throws Exception {	store.forceFail(false);	info("Check key: " + key);	for (int i = 0; i < gridCount(); i++) {	IgniteKernal grid = (IgniteKernal)grid(i);	GridCacheAdapter cache = grid.internalCache(DEFAULT_CACHE_NAME);	GridCacheMapEntry entry = cache.map().getEntry(cache.context(), cache.context().toCacheKeyObject(key));	
entry 

IgniteKernal grid = (IgniteKernal)grid(i);	GridCacheAdapter cache = grid.internalCache(DEFAULT_CACHE_NAME);	GridCacheMapEntry entry = cache.map().getEntry(cache.context(), cache.context().toCacheKeyObject(key));	if (entry != null) {	assertFalse("Unexpected entry for grid [idx=" + i + ", entry=" + entry + ']', entry.lockedByAny());	assertEquals("Unexpected entry for grid [idx=" + i + ", entry=" + entry + ']', putBefore, entry.hasValue());	assertEquals("Unexpected entry for grid [idx=" + i + ", entry=" + entry + ']', putBefore ? 1 : null, entry.rawGet().value(cache.ctx.cacheObjectContext(), false));	}	if (cache.isNear()) {	entry = ((GridNearCacheAdapter)cache).dht().map().getEntry(cache.context(), cache.context().toCacheKeyObject(key));	
dht entry 

========================= ignite sample_1992 =========================

}	}	assertNotNull(key0);	final Integer key1 = key0;	final Integer key2 = primaryKey(cache2);	final Collection<ClusterNode> key1Nodes = aff.mapKeyToPrimaryAndBackups(key1);	final Collection<ClusterNode> key2Nodes = aff.mapKeyToPrimaryAndBackups(key2);	TestCommunicationSpi commSpi = (TestCommunicationSpi)ignite(0).configuration().getCommunicationSpi();	IgniteTransactions txs = ignite(0).transactions();	try (Transaction tx = txs.txStart(optimistic ? OPTIMISTIC : PESSIMISTIC, REPEATABLE_READ)) {	
put 

}	assertNotNull(key0);	final Integer key1 = key0;	final Integer key2 = primaryKey(cache2);	final Collection<ClusterNode> key1Nodes = aff.mapKeyToPrimaryAndBackups(key1);	final Collection<ClusterNode> key2Nodes = aff.mapKeyToPrimaryAndBackups(key2);	TestCommunicationSpi commSpi = (TestCommunicationSpi)ignite(0).configuration().getCommunicationSpi();	IgniteTransactions txs = ignite(0).transactions();	try (Transaction tx = txs.txStart(optimistic ? OPTIMISTIC : PESSIMISTIC, REPEATABLE_READ)) {	cache0.put(key1, key1);	
put 

assertNotNull(key0);	final Integer key1 = key0;	final Integer key2 = primaryKey(cache2);	final Collection<ClusterNode> key1Nodes = aff.mapKeyToPrimaryAndBackups(key1);	final Collection<ClusterNode> key2Nodes = aff.mapKeyToPrimaryAndBackups(key2);	TestCommunicationSpi commSpi = (TestCommunicationSpi)ignite(0).configuration().getCommunicationSpi();	IgniteTransactions txs = ignite(0).transactions();	try (Transaction tx = txs.txStart(optimistic ? OPTIMISTIC : PESSIMISTIC, REPEATABLE_READ)) {	cache0.put(key1, key1);	cache0.put(key2, key2);	
start prepare 

final Collection<ClusterNode> key2Nodes = aff.mapKeyToPrimaryAndBackups(key2);	TestCommunicationSpi commSpi = (TestCommunicationSpi)ignite(0).configuration().getCommunicationSpi();	IgniteTransactions txs = ignite(0).transactions();	try (Transaction tx = txs.txStart(optimistic ? OPTIMISTIC : PESSIMISTIC, REPEATABLE_READ)) {	cache0.put(key1, key1);	cache0.put(key2, key2);	GridNearTxLocal txEx = ((TransactionProxyImpl)tx).tx();	commSpi.blockMessages(ignite(2).cluster().localNode().id());	IgniteInternalFuture<?> prepFut = txEx.prepareNearTxLocal();	waitPrepared(ignite(1));	
stop one primary node 

cache0.put(key2, key2);	GridNearTxLocal txEx = ((TransactionProxyImpl)tx).tx();	commSpi.blockMessages(ignite(2).cluster().localNode().id());	IgniteInternalFuture<?> prepFut = txEx.prepareNearTxLocal();	waitPrepared(ignite(1));	stopGrid(1);	U.sleep(1000);	commSpi.stopBlock();	prepFut.get(10_000);	if (rollback) {	
rollback 

IgniteInternalFuture<?> prepFut = txEx.prepareNearTxLocal();	waitPrepared(ignite(1));	stopGrid(1);	U.sleep(1000);	commSpi.stopBlock();	prepFut.get(10_000);	if (rollback) {	tx.rollback();	}	else {	
commit 

tx.commit();	}	}	GridTestUtils.waitForCondition(new GridAbsPredicate() {	try {	checkKey(key1, rollback ? null : key1Nodes);	checkKey(key2, rollback ? null : key2Nodes);	return true;	}	catch (AssertionError e) {	
check failed 

int backups = cache0.getConfiguration(CacheConfiguration.class).getBackups();	final Collection<ClusterNode> key1Nodes = (locBackupKey && backups < 2) ? null : aff.mapKeyToPrimaryAndBackups(key1);	final Collection<ClusterNode> key2Nodes = aff.mapKeyToPrimaryAndBackups(key2);	TestCommunicationSpi commSpi = (TestCommunicationSpi)ignite(0).configuration().getCommunicationSpi();	IgniteTransactions txs = ignite(0).transactions();	Transaction tx = txs.txStart(optimistic ? OPTIMISTIC : PESSIMISTIC, REPEATABLE_READ);	log.info("Put key1 [key1=" + key1 + ", nodes=" + U.nodeIds(aff.mapKeyToPrimaryAndBackups(key1)) + ']');	cache0.put(key1, key1);	log.info("Put key2 [key2=" + key2 + ", nodes=" + U.nodeIds(aff.mapKeyToPrimaryAndBackups(key2)) + ']');	cache0.put(key2, key2);	
start prepare 

IgniteTransactions txs = ignite(0).transactions();	Transaction tx = txs.txStart(optimistic ? OPTIMISTIC : PESSIMISTIC, REPEATABLE_READ);	log.info("Put key1 [key1=" + key1 + ", nodes=" + U.nodeIds(aff.mapKeyToPrimaryAndBackups(key1)) + ']');	cache0.put(key1, key1);	log.info("Put key2 [key2=" + key2 + ", nodes=" + U.nodeIds(aff.mapKeyToPrimaryAndBackups(key2)) + ']');	cache0.put(key2, key2);	GridNearTxLocal txEx = ((TransactionProxyImpl)tx).tx();	commSpi.blockMessages(ignite(2).cluster().localNode().id());	IgniteInternalFuture<?> prepFut = txEx.prepareNearTxLocal();	waitPrepared(ignite(1));	
stop one primary node 

GridNearTxLocal txEx = ((TransactionProxyImpl)tx).tx();	commSpi.blockMessages(ignite(2).cluster().localNode().id());	IgniteInternalFuture<?> prepFut = txEx.prepareNearTxLocal();	waitPrepared(ignite(1));	stopGrid(1);	U.sleep(1000);	if (!rollback) {	commSpi.stopBlock();	prepFut.get(10_000);	}	
stop originating node 

commSpi.stopBlock();	prepFut.get(10_000);	}	stopGrid(0);	GridTestUtils.waitForCondition(new GridAbsPredicate() {	try {	checkKey(key1, rollback ? null : key1Nodes);	checkKey(key2, rollback ? null : key2Nodes);	return true;	} catch (AssertionError e) {	
check failed 

========================= ignite sample_1091 =========================

private void rollbackCurrentTx() {	try {	TxContext ctx = txCtx.get();	if (ctx != null) {	txCtx.remove();	GridNearTxLocal tx = cache.tx();	if (tx != null) tx.proxy().rollback();	}	}	catch (IgniteException e) {	
failed to rollback cache transaction 

========================= ignite sample_6168 =========================

CacheConfiguration ccfg = cacheConfiguration();	ccfg.setName("cache-2");	ccfg.setBackups(2);	ignite(0).createCache(ccfg);	cacheName = ccfg.getName();	}	try {	int NODES = GRID_CNT + 1;	final int RESTART_IDX = GRID_CNT + 1;	for (int iter = 0; iter < 10; iter++) {	
iteration 

========================= ignite sample_1510 =========================

public void testPutConsistencyMultithreaded() throws Exception {	if (nearEnabled()) fail("https: for (int i = 0; i < 20; i++) {	
iteration 

if (nearEnabled()) fail("https: for (int i = 0; i < 20; i++) {	final int range = 100;	final int iterCnt = 100;	final AtomicInteger threadId = new AtomicInteger();	final AtomicInteger iters = new AtomicInteger();	multithreadedAsync(new Callable<Object>() {	Random rnd = new Random();	int g = threadId.getAndIncrement();	Ignite ignite = grid(g);	IgniteCache<Object, Object> cache = ignite.cache(DEFAULT_CACHE_NAME);	
update thread 

public void testPutRemoveConsistencyMultithreaded() throws Exception {	if (nearEnabled()) fail("https: for (int i = 0; i < 10; i++) {	
iteration 

for (int g = 0; g < gridCount(); g++) {	Ignite ignite = grid(g);	Long val = (Long)ignite.cache(DEFAULT_CACHE_NAME).localPeek(i);	if (firstVal == null && val != null) firstVal = val;	if (val != null) {	if (!firstVal.equals(val)) {	invalidVal = true;	boolean primary = aff.isPrimary(ignite.cluster().localNode(), i);	boolean backup = aff.isBackup(ignite.cluster().localNode(), i);	log.error("Invalid value detected [key=" + i + ", val=" + val + ", firstVal=" + firstVal + ", node=" + g + ", primary=" + primary + ", backup=" + backup + ']');	
all values 

========================= ignite sample_1793 =========================

private Map<IgniteUuid, IgfsEntryInfo> lockIds(Collection<IgniteUuid> fileIds) throws IgniteCheckedException {	assert isSorted(fileIds);	validTxState(true);	
locking file ids 

private Map<IgniteUuid, IgfsEntryInfo> lockIds(Collection<IgniteUuid> fileIds) throws IgniteCheckedException {	assert isSorted(fileIds);	validTxState(true);	Map<IgniteUuid, IgfsEntryInfo> map = getInfos(fileIds);	
locked file ids 

public IgfsEntryInfo reserveSpace(IgniteUuid fileId, long space, IgfsFileAffinityRange affRange) throws IgniteCheckedException {	validTxState(false);	if (busyLock.enterBusy()) {	try {	
reserve file space 

========================= ignite sample_4745 =========================

queue.add(thisNode);	condMap.put(lastCond, queue);	}	val.setConditionMap(condMap);	cacheView.put(key, val);	tx.commit();	return true;	}	catch (Exception e) {	if (interruptAll) {	
node is stopped or lock is broken in non failover safe mode aborting transaction 

}	val.setConditionMap(condMap);	cacheView.put(key, val);	tx.commit();	return true;	}	catch (Exception e) {	if (interruptAll) {	return true;	}	
failed to release 

private void initializeReentrantLock() throws IgniteCheckedException {	if (initGuard.compareAndSet(false, true)) {	try {	sync = retryTopologySafe(new Callable<Sync>() {	try (GridNearTxLocal tx = CU.txStartInternal(ctx, cacheView, PESSIMISTIC, REPEATABLE_READ)) {	GridCacheLockState val = cacheView.get(key);	if (val == null) {	
failed to find reentrant lock with given name 

try (GridNearTxLocal tx = CU.txStartInternal(ctx, cacheView, PESSIMISTIC, REPEATABLE_READ)) {	GridCacheLockState val = cacheView.get(key);	if (val == null) {	return null;	}	tx.rollback();	return new Sync(val);	}	}	});	
initialized internal sync structure 

========================= ignite sample_4315 =========================

protected void beginNodesRestart() {	stopRestartThread.set(false);	nodeRestartFut = GridTestUtils.runAsync(new Callable<Void>() {	int restartGrid = GRID_CNT - RESTARTED_NODE_CNT;	while (!stopRestartThread.get() && System.currentTimeMillis() < endTime) {	
restart grid 

========================= ignite sample_7539 =========================

private Connection connectionForThread(@Nullable String schema) throws IgniteCheckedException {	H2ConnectionWrapper c = connCache.get();	if (c == null) throw new IgniteCheckedException("Failed to get DB connection for thread (check log for details).");	if (schema != null && !F.eq(c.schema(), schema)) {	Statement stmt = null;	try {	stmt = c.connection().createStatement();	stmt.executeUpdate("SET SCHEMA " + H2Utils.withQuotes(schema));	
set schema 

private void createSchema(String schema) throws IgniteCheckedException {	executeStatement("INFORMATION_SCHEMA", "CREATE SCHEMA IF NOT EXISTS " + H2Utils.withQuotes(schema));	conns.remove(Thread.currentThread());	
created schema for index database 

private void dropSchema(String schema) throws IgniteCheckedException {	executeStatement("INFORMATION_SCHEMA", "DROP SCHEMA IF EXISTS " + H2Utils.withQuotes(schema));	
dropped schema for index database 

private void dropTable(H2TableDescriptor tbl) throws IgniteCheckedException {	assert tbl != null;	
removing query index table 

private void dropTable(H2TableDescriptor tbl) throws IgniteCheckedException {	assert tbl != null;	Connection c = connectionForThread(tbl.schemaName());	Statement stmt = null;	try {	stmt = c.createStatement();	String sql = "DROP TABLE IF EXISTS " + tbl.fullTableName();	
dropping database index table with sql 

private FieldsQueryCursor<List<?>> doRunDistributedQuery(String schemaName, SqlFieldsQuery qry, GridCacheTwoStepQuery twoStepQry, List<GridQueryFieldMetadata> meta, boolean keepBinary, GridQueryCancel cancel) {	
parsed query into two step query 

assert tbl != null;	String keyType = dbTypeFromClass(tbl.type().keyClass());	String valTypeStr = dbTypeFromClass(tbl.type().valueClass());	SB sql = new SB();	String keyValVisibility = tbl.type().fields().isEmpty() ? " VISIBLE" : " INVISIBLE";	sql.a("CREATE TABLE ").a(tbl.fullTableName()).a(" (") .a(KEY_FIELD_NAME).a(' ').a(keyType).a(keyValVisibility).a(" NOT NULL");	sql.a(',').a(VAL_FIELD_NAME).a(' ').a(valTypeStr).a(keyValVisibility);	sql.a(',').a(VER_FIELD_NAME).a(" OTHER INVISIBLE");	for (Map.Entry<String, Class<?>> e : tbl.type().fields().entrySet()) sql.a(',').a(H2Utils.withQuotes(e.getKey())).a(' ').a(dbTypeFromClass(e.getValue())) .a(tbl.type().property(e.getKey()).notNull()? " NOT NULL" : "");	sql.a(')');	
creating db table with sql 

locNodeHnd.apply(finalLocNode, finalMsg);	}	finally {	busyLock.leaveBusy();	}	}	}, plc).listen(logger);	}	catch (IgniteCheckedException e) {	ok = false;	
failed to execute query locally 

========================= ignite sample_7755 =========================

private void createDestroyCaches(int srvs) throws Exception {	startGridsMultiThreaded(srvs);	checkCacheDiscoveryDataConsistent();	Ignite srv0 = ignite(0);	for (int i = 0; i < srvs; i++) checkCacheGroup(i, GROUP1, false);	for (int iter = 0; iter < 3; iter++) {	
iteration 

final int NODES = 4;	for (int i = 0; i < NODES; i++) {	ccfgs = cacheConfigurations(CACHES, GROUP1, "testCache1-");	client = i == NODES - 1;	startGrid(i);	}	Ignite client = ignite(NODES - 1);	client.createCaches(Arrays.asList(cacheConfigurations(CACHES, GROUP2, "testCache2-")));	checkCacheDiscoveryDataConsistent();	for (int i = 0; i < NODES; i++) {	
check node 

Ignite client = ignite(NODES - 1);	client.createCaches(Arrays.asList(cacheConfigurations(CACHES, GROUP2, "testCache2-")));	checkCacheDiscoveryDataConsistent();	for (int i = 0; i < NODES; i++) {	for (int c = 0; c < 10; c++) {	int cache = ThreadLocalRandom.current().nextInt(CACHES);	checkCache(i, "testCache1-" + cache, 1);	checkCache(i, "testCache2-" + cache, 1);	}	}	
stop nodes 

if (reverse) {	for (int i = keys.size() - 1; i >= 0; i--) map.put(keys.get(i), 2);	}	else {	for (Integer key : keys) map.put(key, 1);	}	while (!stop.get()) cache.putAll(map);	}	catch (Exception e) {	err.set(true);	
unexpected error 

catch (Exception e) {	if (X.hasCause(e, CacheStoppedException.class)) {	continue;	}	throw e;	}	}	}	catch (Exception e) {	err.set(true);	
unexpected error 

for (int i = 0; i < CACHES; i++) {	CacheAtomicityMode atomicityMode = i % 2 == 0 ? ATOMIC : TRANSACTIONAL;	caches.set(i, clientNode.createCache(cacheConfiguration(GROUP1, "c" + i, PARTITIONED, atomicityMode, 0, false)));	}	final AtomicBoolean stop = new AtomicBoolean();	final AtomicInteger cacheCntr = new AtomicInteger();	try {	for (int i = 0; i < 10; i++) {	stop.set(false);	final AtomicReference<Exception> err = new AtomicReference<>();	
iteration 

final AtomicInteger cacheCntr = new AtomicInteger();	try {	for (int i = 0; i < 10; i++) {	stop.set(false);	final AtomicReference<Exception> err = new AtomicReference<>();	IgniteInternalFuture<?> restartFut = GridTestUtils.runAsync(new Runnable() {	try {	ThreadLocalRandom rnd = ThreadLocalRandom.current();	while (!stop.get()) {	int node = rnd.nextInt(SRVS);	
stop node 

for (int i = 0; i < 10; i++) {	stop.set(false);	final AtomicReference<Exception> err = new AtomicReference<>();	IgniteInternalFuture<?> restartFut = GridTestUtils.runAsync(new Runnable() {	try {	ThreadLocalRandom rnd = ThreadLocalRandom.current();	while (!stop.get()) {	int node = rnd.nextInt(SRVS);	stopGrid(node);	U.sleep(500);	
start node 

U.sleep(500);	startGrid(node);	try {	if (rnd.nextBoolean()) awaitPartitionMapExchange();	}	catch (Exception ignore) {	}	}	}	catch (Exception e){	
unexpected error 

}	}	});	IgniteInternalFuture<?> cacheFut = GridTestUtils.runAsync(new Runnable() {	try {	ThreadLocalRandom rnd = ThreadLocalRandom.current();	while (!stop.get()) {	int idx = rnd.nextInt(CACHES);	IgniteCache cache = caches.get(idx);	if (cache != null && caches.compareAndSet(idx, cache, null)) {	
destroy cache 

if (cache != null && caches.compareAndSet(idx, cache, null)) {	clientNode.destroyCache(cache.getName());	CacheAtomicityMode atomicityMode = rnd.nextBoolean() ? ATOMIC : TRANSACTIONAL;	String name = "newName-" + cacheCntr.incrementAndGet();	cache = clientNode.createCache( cacheConfiguration(GROUP1, name, PARTITIONED, atomicityMode, 0, false));	caches.set(idx, cache);	}	}	}	catch (Exception e){	
unexpected error 

}	throw e;	}	finally {	caches.set(idx, cache);	}	}	}	catch (Exception e) {	err.set(e);	
unexpected error 

========================= ignite sample_1394 =========================

while (System.currentTimeMillis() < stopTime) {	for (int i = 0; i < 10_000; i++) {	try {	try (Transaction tx = txs.txStart(PESSIMISTIC, REPEATABLE_READ)) {	txCache.put(0, 0);	cache.put(i, i);	tx.commit();	}	}	catch (IgniteException | CacheException e) {	
ignore exception 

========================= ignite sample_1101 =========================

public void testBackupQueue() throws Exception {	final CacheEventListener lsnr = new CacheEventListener();	ContinuousQuery<Object, Object> qry = new ContinuousQuery<>();	qry.setLocalListener(lsnr);	qry.setRemoteFilterFactory(new AlwaysFalseFilterFactory());	try (QueryCursor<?> ignore = grid(0).cache(CACHE_NAME).query(qry)) {	for (int i = 0; i < KEYS_COUNT; i++) {	
put key 

public void testBackupQueue() throws Exception {	final CacheEventListener lsnr = new CacheEventListener();	ContinuousQuery<Object, Object> qry = new ContinuousQuery<>();	qry.setLocalListener(lsnr);	qry.setRemoteFilterFactory(new AlwaysFalseFilterFactory());	try (QueryCursor<?> ignore = grid(0).cache(CACHE_NAME).query(qry)) {	for (int i = 0; i < KEYS_COUNT; i++) {	for (int j = 0; j < 100; j++) grid(j % GRID_COUNT).cache(CACHE_NAME).put(i, new byte[1024 * 50]);	}	
finish 

public void testManyQueryBackupQueue() throws Exception {	List<QueryCursor> qryCursors = new ArrayList<>();	for (int i = 0; i < QUERY_COUNT; i++) {	ContinuousQuery<Object, Object> qry = new ContinuousQuery<>();	qry.setLocalListener(new CacheEventListener());	qry.setRemoteFilterFactory(new AlwaysFalseFilterFactory());	qryCursors.add(grid(0).cache(CACHE_NAME).query(qry));	}	for (int i = 0; i < KEYS_COUNT; i++) {	
put key 

awaitPartitionMapExchange();	List<QueryCursor> qryCursors = new ArrayList<>();	for (int i = 0; i < QUERY_COUNT; i++) {	ContinuousQuery<Object, Object> qry = new ContinuousQuery<>();	qry.setLocalListener(new CacheEventListener());	qry.setRemoteFilterFactory(new AlwaysFalseFilterFactory());	qry.setAutoUnsubscribe(false);	qryCursors.add(client.cache(CACHE_NAME).query(qry));	}	for (int i = 0; i < KEYS_COUNT; i++) {	
put key 

}	for (int i = 0; i < KEYS_COUNT; i++) {	grid(i % GRID_COUNT).cache(CACHE_NAME).put(i, new byte[1024 * 50]);	}	int size = backupQueueSize();	assertTrue(size > 0);	assertTrue(size <= BACKUP_ACK_THRESHOLD * QUERY_COUNT * /* partition count */1024);	stopGrid(GRID_COUNT);	awaitPartitionMapExchange();	for (int i = 0; i < KEYS_COUNT; i++) {	
put key 

========================= ignite sample_1947 =========================

private void notifyListenersOnClose() {	for (HadoopIgfsStreamEventListener lsnr : lsnrs.values()) {	try {	lsnr.onClose();	}	catch (IgniteCheckedException e) {	
failed to notify stream event listener 

========================= ignite sample_7190 =========================

}	}, threadCnt );	fut.get();	Collection<EvictableEntry<Integer, Integer>> queue = internalQueue(plc);	info("Test results [threadCnt=" + threadCnt + ", iterCnt=" + ITERATION_CNT + ", cacheSize=" + cache.size() + ", internalQueueSize" + queue.size() + ", duration=" + (System.currentTimeMillis() - start) + ']');	boolean detached = false;	for (Cache.Entry<Integer, Integer> e : queue) {	Integer rmv = cache.getAndRemove(e.getKey());	CacheEvictableEntryImpl unwrapped = e.unwrap(CacheEvictableEntryImpl.class);	if (rmv == null && (unwrapped.meta() != null || unwrapped.isCached())) {	
detached entry 

CacheEvictableEntryImpl unwrapped = e.unwrap(CacheEvictableEntryImpl.class);	if (rmv == null && (unwrapped.meta() != null || unwrapped.isCached())) {	detached = true;	}	else info("Entry removed: " + rmv);	}	if (detached) fail("Eviction policy contains keys that are not present in cache");	if (!(cache.localSize(CachePeekMode.ONHEAP) == 0)) {	boolean zombies = false;	for (Cache.Entry<Integer, Integer> e : cache.localEntries(CachePeekMode.ONHEAP)) {	
zombie entry 

========================= ignite sample_2054 =========================

throw new CacheException("Failed access to metadata for detect database dialect.", e);	}	finally {	U.closeQuiet(conn);	}	if ("H2".equals(dbProductName)) return new H2Dialect();	if ("MySQL".equals(dbProductName)) return new MySQLDialect();	if (dbProductName.startsWith("Microsoft SQL Server")) return new SQLServerDialect();	if ("Oracle".equals(dbProductName)) return new OracleDialect();	if (dbProductName.startsWith("DB2/")) return new DB2Dialect();	
failed to resolve dialect basicjdbcdialect will be used 

========================= ignite sample_2957 =========================

private void getAfterStop(CacheMode cacheMode, @Nullable NearCacheConfiguration nearCfg) throws Exception {	this.cacheMode = cacheMode;	this.nearCfg = nearCfg;	startGrids();	IgniteCache<Integer, Integer> cache0 = jcache(0);	IgniteCache<Integer, Integer> cache1 = jcache(1);	Integer key0 = primaryKey(cache0);	Integer key1 = primaryKey(cache1);	try (Transaction tx = ignite(0).transactions().txStart()) {	
put 

private void getAfterStop(CacheMode cacheMode, @Nullable NearCacheConfiguration nearCfg) throws Exception {	this.cacheMode = cacheMode;	this.nearCfg = nearCfg;	startGrids();	IgniteCache<Integer, Integer> cache0 = jcache(0);	IgniteCache<Integer, Integer> cache1 = jcache(1);	Integer key0 = primaryKey(cache0);	Integer key1 = primaryKey(cache1);	try (Transaction tx = ignite(0).transactions().txStart()) {	cache0.put(key0, key0);	
stop node 

this.cacheMode = cacheMode;	this.nearCfg = nearCfg;	startGrids();	IgniteCache<Integer, Integer> cache0 = jcache(0);	IgniteCache<Integer, Integer> cache1 = jcache(1);	Integer key0 = primaryKey(cache0);	Integer key1 = primaryKey(cache1);	try (Transaction tx = ignite(0).transactions().txStart()) {	cache0.put(key0, key0);	stopGrid(3);	
get 

this.nearCfg = nearCfg;	startGrids();	IgniteCache<Integer, Integer> cache0 = jcache(0);	IgniteCache<Integer, Integer> cache1 = jcache(1);	Integer key0 = primaryKey(cache0);	Integer key1 = primaryKey(cache1);	try (Transaction tx = ignite(0).transactions().txStart()) {	cache0.put(key0, key0);	stopGrid(3);	cache0.get(key1);	
commit 

========================= ignite sample_1228 =========================

public void setAttributes(GridJobSessionImpl ses, Map<?, ?> attrs) throws IgniteCheckedException {	assert ses.isFullSupport();	long timeout = ses.getEndTime() - U.currentTimeMillis();	if (timeout <= 0) {	
task execution timed out remote session attributes won t be set 

public void setAttributes(GridJobSessionImpl ses, Map<?, ?> attrs) throws IgniteCheckedException {	assert ses.isFullSupport();	long timeout = ses.getEndTime() - U.currentTimeMillis();	if (timeout <= 0) {	return;	}	
setting session attribute s from job 

GridJobSiblingsResponse res = null;	if (!(msg instanceof GridJobSiblingsResponse)) err = "Received unexpected message: " + msg;	else if (!nodeId.equals(taskNodeId)) err = "Received job siblings response from unexpected node [taskNodeId=" + taskNodeId + ", nodeId=" + nodeId + ']';	else {	res = (GridJobSiblingsResponse)msg;	if (res.jobSiblings() == null) {	try {	res.unmarshalSiblings(marsh);	}	catch (IgniteCheckedException e) {	
failed to unmarshal job siblings 

private boolean cancelPassiveJob(GridJobWorker job) {	assert !jobAlwaysActivate;	if (passiveJobs.remove(job.getJobId(), job)) {	
job has been cancelled before activation 

private void handleCollisions() {	assert !jobAlwaysActivate;	if (handlingCollision.get()) {	
skipping recursive collision handling 

private void handleCollisions() {	assert !jobAlwaysActivate;	if (handlingCollision.get()) {	return;	}	handlingCollision.set(Boolean.TRUE);	try {	
before handling collisions 

public void processJobExecuteRequest(ClusterNode node, final GridJobExecuteRequest req) {	if (log.isDebugEnabled()) log.debug("Received job request message [req=" + req + ", nodeId=" + node.id() + ']');	PartitionsReservation partsReservation = null;	if (req.getCacheIds() != null) {	assert req.getPartition() >= 0 : req;	assert !F.isEmpty(req.getCacheIds()) : req;	partsReservation = new PartitionsReservation(req.getCacheIds(), req.getPartition(), req.getTopVer());	}	GridJobWorker job = null;	if (!rwLock.tryReadLock()) {	
received job execution request while stopping this node will ignore 

}	GridJobWorker job = null;	if (!rwLock.tryReadLock()) {	return;	}	try {	long endTime = req.getCreateTime() + req.getTimeout();	if (endTime < 0) endTime = Long.MAX_VALUE;	GridDeployment tmpDep = req.isForceLocalDeployment() ? ctx.deploy().getLocalDeployment(req.getTaskClassName()) : ctx.deploy().getGlobalDeployment( req.getDeploymentMode(), req.getTaskName(), req.getTaskClassName(), req.getUserVersion(), node.id(), req.getClassLoaderId(), req.getLoaderParticipants(), null);	if (tmpDep == null) {	
checking local tasks 

if (tmpDep == null) {	for (Map.Entry<String, GridDeployment> d : ctx.task().getUsedDeploymentMap().entrySet()) {	if (d.getValue().classLoaderId().equals(req.getClassLoaderId())) {	assert d.getValue().local();	tmpDep = d.getValue();	break;	}	}	}	final GridDeployment dep = tmpDep;	
deployment 

if (sysCancelled == null) sysCancelled = cancelReqs.get(jobWorker.getJobId());	if (sysCancelled != null) {	activeJobs.remove(jobWorker.getJobId(), jobWorker);	IgniteException e2 = new ComputeExecutionRejectedException( "Job was cancelled before execution [jobSes=" + jobWorker. getSession() + ", job=" + jobWorker.getJob() + ']');	jobWorker.finishJob(null, e2, !sysCancelled);	return false;	}	if (ctx.discovery().node(jobWorker.getTaskNode().id()) == null && activeJobs.remove(jobWorker.getJobId(), jobWorker)) {	cancelledJobs.put(jobWorker.getJobId(), jobWorker);	if (!jobWorker.onMasterNodeLeft()) {	
job is being cancelled because master task node left grid as there is no one waiting for results job will not be failed over 

private boolean executeAsync(GridJobWorker jobWorker) {	try {	if (jobWorker.executorName() != null) {	Executor customExec = ctx.pools().customExecutor(jobWorker.executorName());	if (customExec != null) customExec.execute(jobWorker);	else {	
custom executor doesn t exist local job will be processed in default thread pool 

private void processTaskSessionRequest(UUID nodeId, GridTaskSessionRequest req) {	if (!rwLock.tryReadLock()) {	
received job session request while stopping grid will ignore 

private void processTaskSessionRequest(UUID nodeId, GridTaskSessionRequest req) {	if (!rwLock.tryReadLock()) {	return;	}	try {	GridTaskSessionImpl ses = ctx.session().getSession(req.getSessionId());	if (ses == null) {	
received job session request for non existing session 

Map<?, ?> attrs = loc ? req.getAttributes() : (Map<?, ?>)U.unmarshal(marsh, req.getAttributesBytes(), U.resolveClassLoader(ses.getClassLoader(), ctx.config()));	if (ctx.event().isRecordable(EVT_TASK_SESSION_ATTR_SET)) {	Event evt = new TaskEvent( ctx.discovery().localNode(), "Changed attributes: " + attrs, EVT_TASK_SESSION_ATTR_SET, ses.getId(), ses.getTaskName(), ses.getTaskClassName(), false, null);	ctx.event().record(evt);	}	synchronized (ses) {	ses.setInternal(attrs);	}	}	catch (IgniteCheckedException e) {	
failed to deserialize session attributes 

========================= ignite sample_4704 =========================

private int executeHadoopCmd(String... args) throws Exception {	ProcessBuilder procBuilder = createProcessBuilder();	List<String> cmd = new ArrayList<>();	cmd.add(hadoopHome + "/bin/hadoop");	cmd.addAll(Arrays.asList(args));	procBuilder.command(cmd);	
execute 

List<String> cmd = new ArrayList<>();	procBuilder.command(cmd);	cmd.add(hiveHome + "/bin/hive");	cmd.add("--hiveconf");	cmd.add("hive.rpc.query.plan=true");	cmd.add("--hiveconf");	cmd.add("javax.jdo.option.ConnectionURL=jdbc:derby:" + testWorkDir.getAbsolutePath() + "/metastore_db;" + "databaseName=metastore_db;create=true");	cmd.add("-e");	cmd.add(qry);	procBuilder.command(cmd);	
execute 

========================= ignite sample_7082 =========================

public void testTaskNodeRestart() throws Exception {	final AtomicBoolean finished = new AtomicBoolean();	final AtomicInteger stopIdx = new AtomicInteger();	IgniteInternalFuture<?> restartFut = GridTestUtils.runMultiThreadedAsync(new Callable<Object>() {	int idx = stopIdx.getAndIncrement();	int node = NODES + idx;	while (!finished.get()) {	
start node 

public void testTaskNodeRestart() throws Exception {	final AtomicBoolean finished = new AtomicBoolean();	final AtomicInteger stopIdx = new AtomicInteger();	IgniteInternalFuture<?> restartFut = GridTestUtils.runMultiThreadedAsync(new Callable<Object>() {	int idx = stopIdx.getAndIncrement();	int node = NODES + idx;	while (!finished.get()) {	startGrid(node);	U.sleep(300);	
stop node 

========================= ignite sample_2280 =========================

public void testNodeFailedAtRebalancing() throws Exception {	Ignite ignite = startGrid(0);	generateData(ignite, 0, 0);	
preloading started 

========================= ignite sample_1013 =========================

public void testNearOnlyPutMultithreaded() throws Exception {	final Ignite ignite1 = ignite(1);	assertTrue(ignite1.configuration().isClientMode());	ignite1.createNearCache(DEFAULT_CACHE_NAME, new NearCacheConfiguration<>());	final Integer key = 1;	final AtomicInteger idx = new AtomicInteger();	IgniteCache<Integer, Integer> cache0 = ignite(0).cache(DEFAULT_CACHE_NAME);	IgniteCache<Integer, Integer> cache1 = ignite1.cache(DEFAULT_CACHE_NAME);	for (int i = 0; i < 5; i++) {	
iteration 

private void txMultithreaded(final boolean optimistic) throws Exception {	final Ignite ignite1 = ignite(1);	assertTrue(ignite1.configuration().isClientMode());	ignite1.createNearCache(DEFAULT_CACHE_NAME, new NearCacheConfiguration<>());	final AtomicInteger idx = new AtomicInteger();	final Integer key = 1;	IgniteCache<Integer, Integer> cache0 = ignite(0).cache(DEFAULT_CACHE_NAME);	IgniteCache<Integer, Integer> cache1 = ignite1.cache(DEFAULT_CACHE_NAME);	for (int i = 0; i < 5; i++) {	
iteration 

========================= ignite sample_1291 =========================

private int[] cacheEvents(String evtPropsStr) throws Exception {	String[] evtStr = evtPropsStr.split("\\s*,\\s*");	if (evtStr.length == 0) return EventType.EVTS_CACHE;	int[] evts = new int[evtStr.length];	try {	for (int i = 0; i < evtStr.length; i++) evts[i] = CacheEvt.valueOf(evtStr[i].toUpperCase()).getId();	}	catch (Exception e) {	
failed to recognize the provided cache event 

========================= ignite sample_6942 =========================

}));	}	long total = 0;	List<Collection<Long>> cols = new ArrayList<>(THREADS);	for (IgniteInternalFuture<Collection<Long>> fut : futs) {	Collection<Long> col = fut.get();	assertEquals(ITERATIONS, col.size());	total += col.size();	cols.add(col);	}	
cache value 

========================= ignite sample_1507 =========================

tweetStreamProcessor = Executors.newFixedThreadPool(threadsCount);	for (int i = 0; i < threadsCount; i++) {	Callable<Boolean> task = new Callable<Boolean>() {	public Boolean call() {	while (true) {	try {	String tweet = tweetQueue.take();	addMessage(tweet);	}	catch (InterruptedException e) {	
tweets transformation was interrupted 

========================= ignite sample_460 =========================

public void cancelInternalQuery(UUID routineId) {	try {	cctx.kernalContext().continuous().stopRoutine(routineId).get();	}	catch (IgniteCheckedException | IgniteException e) {	
failed to stop internal continuous query 

hnd.internal(internal);	hnd.keepBinary(keepBinary);	hnd.localCache(cctx.isLocal());	IgnitePredicate<ClusterNode> pred = (loc || cctx.config().getCacheMode() == CacheMode.LOCAL) ? F.nodeForNodeId(cctx.localNodeId()) : cctx.group().nodeFilter();	assert pred != null : cctx.config();	UUID id = cctx.kernalContext().continuous().startRoutine( hnd, internal && loc, bufSize, timeInterval, autoUnsubscribe, pred).get();	try {	if (hnd.isQuery() && cctx.userCache() && !onStart) hnd.waitTopologyFuture(cctx.kernalContext());	}	catch (IgniteCheckedException e) {	
failed to start continuous query 

========================= ignite sample_4210 =========================

}	}	boolean more = cctx.offheap().expire(dhtCtx, expireC, amount);	if (more) return true;	if (amount != -1 && pendingEntries != null) {	EntryWrapper e = pendingEntries.firstx();	return e != null && e.expireTime <= now;	}	}	catch (IgniteCheckedException e) {	
failed to process entry expiration 

========================= ignite sample_3701 =========================

prop.igniteWorkDir = getStringProperty(IGNITE_WORK_DIR, props, DEFAULT_IGNITE_WORK_DIR);	prop.igniteCfg = getStringProperty(IGNITE_CONFIG_XML, props, null);	prop.userLibs = getStringProperty(IGNITE_USERS_LIBS, props, null);	String ptrn = getStringProperty(IGNITE_HOSTNAME_CONSTRAINT, props, null);	prop.idleTimeout = getLongProperty(IGNITE_HTTP_SERVER_IDLE_TIMEOUT, props, IGNITE_HTTP_SERVER_IDLE_TIMEOUT_DEFAULT);	if (ptrn != null) {	try {	prop.hostnameConstraint = Pattern.compile(ptrn);	}	catch (PatternSyntaxException e) {	
ignite hostname constraint has invalid pattern it will be ignore 

========================= ignite sample_6446 =========================

private void initializeLatch() throws IgniteCheckedException {	if (initGuard.compareAndSet(UNINITIALIZED_LATCH_STATE, CREATING_LATCH_STATE)) {	try {	internalLatch = retryTopologySafe(new Callable<CountDownLatch>() {	try (GridNearTxLocal tx = CU.txStartInternal(ctx, cacheView, PESSIMISTIC, REPEATABLE_READ)) {	GridCacheCountDownLatchValue val = cacheView.get(key);	if (val == null) {	
failed to find count down latch with given name 

return new CountDownLatch(val.get());	}	}	});	synchronized (initGuard) {	if (lastLatchVal != null) {	while (internalLatch.getCount() > lastLatchVal) internalLatch.countDown();	}	initGuard.set(READY_LATCH_STATE);	}	
initialized internal latch 

========================= ignite sample_4302 =========================

default: assert false : "Unexpected operation: " + operation;	}	threwEx = false;	return true;	}	finally {	if (initSes && storeMgr != null) storeMgr.writeBehindSessionEnd(threwEx);	}	}	catch (Exception e) {	
unable to update underlying store 

wakeUp();	flusherWriterLock.lock();	try {	while (queue.sizex() >= flusherCacheCriticalSize && !stopping.get()) {	if (cacheFlushFreq > 0) flusherWriterCanWrite.await(cacheFlushFreq, TimeUnit.MILLISECONDS);	else flusherWriterCanWrite.await();	}	cacheTotalOverflowCntr.incrementAndGet();	}	catch (InterruptedException e) {	
caught interrupted exception 

========================= ignite sample_4247 =========================

startGrids(2);	client = true;	final int CLIENT_ID = 3;	Ignite clientNode = startGrid(CLIENT_ID);	client = false;	final CacheEventListener lsnr = new CacheEventListener();	ContinuousQuery<Object, Object> qry = new ContinuousQuery<>();	qry.setLocalListener(lsnr);	QueryCursor<?> cur = clientNode.cache(DEFAULT_CACHE_NAME).query(qry);	for (int i = 0; i < 10; i++) {	
start iteration 

public void testNodeJoinsRestartQuery() throws Exception {	startGrids(2);	client = true;	final int CLIENT_ID = 3;	Ignite clientNode = startGrid(CLIENT_ID);	client = false;	for (int i = 0; i < 10; i++) {	
start iteration 

========================= ignite sample_1946 =========================

private void testEntityCacheReadWrite(AccessType accessType) throws Exception {	
test access type 

========================= ignite sample_6112 =========================

public void testRandom() throws Exception {	ThreadLocalRandom rnd = ThreadLocalRandom.current();	final int NODE_CNT = 10;	for (int iter = 0; iter < 1; iter++) {	
iteration 

for (int i = 0; i < NODE_CNT; i++) {	cache1 = rnd.nextBoolean();	cache2 = rnd.nextBoolean();	log.info("Start node [idx=" + i + ", cache1=" + cache1 + ", cache2=" + cache2 + ']');	startGrid(i);	awaitPartitionMapExchange();	}	LinkedHashSet<Integer> stopSeq = new LinkedHashSet<>();	while (stopSeq.size() != NODE_CNT) stopSeq.add(rnd.nextInt(0, NODE_CNT));	for (Integer idx : stopSeq) {	
stop node 

========================= ignite sample_1511 =========================

private void checkAwaitMessageType(Message obj, UUID srcNodeId) {	try {	GridIoMessage plainMsg = (GridIoMessage)obj;	Object msg = plainMsg.message();	if (delayedMsgCls.isAssignableFrom(msg.getClass())) {	info(getSpiContext().localNode().id() + " received message from " + srcNodeId);	U.sleep(delayTime);	}	}	catch (IgniteCheckedException e) {	
cannot process incoming message 

========================= ignite sample_2628 =========================

protected void checkCache(IgniteEx ignite, IgniteCache cache) throws Exception {	
start cache validation 

for (Map.Entry<String, AtomicLong> e : nextValMap.entrySet()) {	String key = e.getKey();	Set set = (Set)cache.get(key);	if (set == null || e.getValue() == null || !Objects.equals(e.getValue().get(), (long)set.size())) badCacheEntries.put(key, set);	}	if (!badCacheEntries.isEmpty()) {	for (Map.Entry<String, Set> e : badCacheEntries.entrySet()) {	String key = e.getKey();	U.error(log, "Got unexpected set size [key='" + key + "', expSize=" + nextValMap.get(key) + ", cacheVal=" + e.getValue() + "]");	}	
next values map contant 

String key = e.getKey();	Set set = (Set)cache.get(key);	if (set == null || e.getValue() == null || !Objects.equals(e.getValue().get(), (long)set.size())) badCacheEntries.put(key, set);	}	if (!badCacheEntries.isEmpty()) {	for (Map.Entry<String, Set> e : badCacheEntries.entrySet()) {	String key = e.getKey();	U.error(log, "Got unexpected set size [key='" + key + "', expSize=" + nextValMap.get(key) + ", cacheVal=" + e.getValue() + "]");	}	for (Map.Entry<String, AtomicLong> e : nextValMap.entrySet()) log.info("Map Entry [key=" + e.getKey() + ", val=" + e.getValue() + "]");	
cache content 

U.error(log, "Got unexpected set size [key='" + key + "', expSize=" + nextValMap.get(key) + ", cacheVal=" + e.getValue() + "]");	}	for (Map.Entry<String, AtomicLong> e : nextValMap.entrySet()) log.info("Map Entry [key=" + e.getKey() + ", val=" + e.getValue() + "]");	for (int k2 = 0; k2 < RANGE; k2++) {	String key2 = "key-" + k2;	Object val = cache.get(key2);	if (val != null) log.info("Cache Entry [key=" + key2 + ", val=" + val + "]");	}	fail("Cache and local map are in inconsistent state [badKeys=" + badCacheEntries.keySet() + ']');	}	
clearing all data 

for (Map.Entry<String, AtomicLong> e : nextValMap.entrySet()) log.info("Map Entry [key=" + e.getKey() + ", val=" + e.getValue() + "]");	for (int k2 = 0; k2 < RANGE; k2++) {	String key2 = "key-" + k2;	Object val = cache.get(key2);	if (val != null) log.info("Cache Entry [key=" + key2 + ", val=" + val + "]");	}	fail("Cache and local map are in inconsistent state [badKeys=" + badCacheEntries.keySet() + ']');	}	cache.removeAll();	nextValMap.clear();	
cache validation successfully finished in sec 

========================= ignite sample_1356 =========================

private void fireDone() {	assert isDone();	DoneCallback cb;	Error err = null;	while ((cb = cbs.poll()) != null) try {	cb.proceed();	}	catch (Error e) {	if (err == null) err = e;	
failed to notify future callback due to unhandled error 

========================= ignite sample_5179 =========================

private boolean isDeadClassLoader(GridDeploymentMetadata meta) {	assert Thread.holdsLock(mux);	if (deadClsLdrs.contains(meta.classLoaderId())) {	
ignoring request for obsolete class loader 

private void checkRedeploy(GridDeploymentMetadata meta) {	assert Thread.holdsLock(mux);	for (List<SharedDeployment> deps : cache.values()) {	for (SharedDeployment dep : deps) {	if (!dep.undeployed() && !dep.pendingUndeploy()) {	long undeployTimeout = ctx.config().getNetworkTimeout();	if (!dep.hasParticipants() && dep.deployMode() == CONTINUOUS && dep.existingDeployedClass(meta.className()) != null && !meta.userVersion().equals(dep.userVersion())) {	dep.onUndeployScheduled();	
deployment was scheduled for undeploy 

assert undep.pendingUndeploy();	if (!undep.undeployed()) {	undep.undeploy();	undep.onRemoved();	rmv = true;	Collection<SharedDeployment> deps = cache.get(undep.userVersion());	if (deps != null) {	for (Iterator<SharedDeployment> i = deps.iterator(); i.hasNext();) if (i.next() == undep) i.remove();	if (deps.isEmpty()) cache.remove(undep.userVersion());	}	
undeployed class loader due to deployment mode change user version change or hot redeployment 

clsLdr = new GridDeploymentClassLoader( ldrId, meta.userVersion(), meta.deploymentMode(), false, ctx, ctx.config().getClassLoader() != null ? ctx.config().getClassLoader() : U.gridClassLoader(), meta.classLoaderId(), meta.senderNodeId(), comm, ctx.config().getNetworkTimeout(), log, ctx.config().getPeerClassLoadingLocalClassPathExclude(), ctx.config().getPeerClassLoadingMissedResourcesCacheSize(), meta.deploymentMode() == CONTINUOUS /* enable class byte cache in CONTINUOUS mode */, false);	if (meta.participants() != null) for (Map.Entry<UUID, IgniteUuid> e : meta.participants().entrySet()) clsLdr.register(e.getKey(), e.getValue());	if (log.isDebugEnabled()) log.debug("Created class loader in CONTINUOUS mode or without participants " + "[ldr=" + clsLdr + ", meta=" + meta + ']');	}	else {	assert meta.deploymentMode() == SHARED;	clsLdr = new GridDeploymentClassLoader( ldrId, meta.userVersion(), meta.deploymentMode(), false, ctx, U.gridClassLoader(), meta.participants(), comm, ctx.config().getNetworkTimeout(), log, ctx.config().getPeerClassLoadingLocalClassPathExclude(), ctx.config().getPeerClassLoadingMissedResourcesCacheSize(), false, false);	if (log.isDebugEnabled()) log.debug("Created classloader in SHARED mode with participants " + "[ldr=" + clsLdr + ", meta=" + meta + ']');	}	SharedDeployment dep = new SharedDeployment(meta.deploymentMode(), clsLdr, ldrId, meta.userVersion(), meta.alias());	
created new deployment 

else {	assert meta.deploymentMode() == SHARED;	clsLdr = new GridDeploymentClassLoader( ldrId, meta.userVersion(), meta.deploymentMode(), false, ctx, U.gridClassLoader(), meta.participants(), comm, ctx.config().getNetworkTimeout(), log, ctx.config().getPeerClassLoadingLocalClassPathExclude(), ctx.config().getPeerClassLoadingMissedResourcesCacheSize(), false, false);	if (log.isDebugEnabled()) log.debug("Created classloader in SHARED mode with participants " + "[ldr=" + clsLdr + ", meta=" + meta + ']');	}	SharedDeployment dep = new SharedDeployment(meta.deploymentMode(), clsLdr, ldrId, meta.userVersion(), meta.alias());	if (isCache) {	List<SharedDeployment> deps = F.addIfAbsent(cache, meta.userVersion(), new LinkedList<SharedDeployment>());	assert deps != null;	deps.add(dep);	
added deployment to cache 

========================= ignite sample_3557 =========================

}	else return fut;	}	List<StoredCacheData> storedCfgs = null;	if (activate && CU.isPersistenceEnabled(ctx.config())) {	try {	Map<String, StoredCacheData> cfgs = ctx.cache().context().pageStore().readCacheConfigurations();	if (!F.isEmpty(cfgs)) storedCfgs = new ArrayList<>(cfgs.values());	}	catch (IgniteCheckedException e) {	
failed to read stored cache configurations 

return startedFut;	}	}	ChangeGlobalStateMessage msg = new ChangeGlobalStateMessage(startedFut.requestId, ctx.localNodeId(), storedCfgs, activate, blt, forceChangeBaselineTopology, System.currentTimeMillis());	try {	if (log.isInfoEnabled()) U.log(log, "Sending " + prettyStr(activate) + " request with BaselineTopology " + blt);	ctx.discovery().sendCustomEvent(msg);	if (ctx.isStopping()) startedFut.onDone(new IgniteCheckedException("Failed to execute " + prettyStr(activate) + " request, " + "node is stopping."));	}	catch (IgniteCheckedException e) {	
failed to send global state change request 

========================= ignite sample_3629 =========================

log.info(">>> Performs put [node=" + ((IgniteKernal)ignite).localNode() + ", tx=" + tx + ", key=" + key1 + ", cache=" + cache1.getName() + ']');	cache1.put(key1, 0);	barrier.await();	int key2 = primaryKey(cache2);	log.info(">>> Performs put [node=" + ((IgniteKernal)ignite).localNode() + ", tx=" + tx + ", key=" + key2 + ", cache=" + cache2.getName() + ']');	cache2.put(key2, 1);	tx.commit();	}	catch (Throwable e) {	if (hasCause(e, TransactionTimeoutException.class) && hasCause(e, TransactionDeadlockException.class) ) {	
at least one stack trace should contain 

========================= ignite sample_1614 =========================

int threadsNum = 10;	GridTestUtils.runMultiThreaded(new Runnable() {	try {	JobStealingResult res = ignite.compute().execute(new JobStealingTask(2), null);	info("Task result: " + res);	stolen.addAndGet(res.stolen);	noneStolen.addAndGet(res.nonStolen);	nodes.addAll(res.nodes);	}	catch (IgniteException e) {	
failed to execute task 

try {	final IgniteCompute compute = ignite.compute().withAsync();	compute.execute(new JobStealingTask(jobsPerTask), null);	JobStealingResult res = (JobStealingResult)compute.future().get();	info("Task result: " + res);	stolen.addAndGet(res.stolen);	noneStolen.addAndGet(res.nonStolen);	nodes.addAll(res.nodes);	}	catch (IgniteException e) {	
failed to execute task 

========================= ignite sample_773 =========================

private void testJoinQuery(CacheMode cacheMode, int backups, final boolean affKey, boolean includeAffKey) {	CacheConfiguration ccfg = cacheConfiguration(cacheMode, backups, affKey, includeAffKey);	log.info("Test cache [mode=" + cacheMode + ", backups=" + backups + ']');	IgniteCache cache = ignite(0).createCache(ccfg);	try {	final PutData putData = putData(cache, affKey);	for (int i = 0; i < NODES; i++) {	
test node 

========================= ignite sample_7543 =========================

public static File getAgentHome() {	try {	ProtectionDomain domain = AgentLauncher.class.getProtectionDomain();	if (domain == null || domain.getCodeSource() == null || domain.getCodeSource().getLocation() == null) {	
failed to resolve agent jar location 

try {	ProtectionDomain domain = AgentLauncher.class.getProtectionDomain();	if (domain == null || domain.getCodeSource() == null || domain.getCodeSource().getLocation() == null) {	return null;	}	URI classesUri = domain.getCodeSource().getLocation().toURI();	boolean win = System.getProperty("os.name").toLowerCase().contains("win");	if (win && classesUri.getAuthority() != null) classesUri = new URI(classesUri.toString().replace("file: return new File(classesUri).getParentFile();	}	catch (URISyntaxException | SecurityException ignored) {	
failed to resolve agent jar location 

========================= ignite sample_7369 =========================

prop.igniteLocalWorkDir = getStringProperty(IGNITE_LOCAL_WORK_DIR, props, DEFAULT_IGNITE_LOCAL_WORK_DIR);	prop.igniteReleasesDir = getStringProperty(IGNITE_RELEASES_DIR, props, DEFAULT_IGNITE_RELEASES_DIR);	prop.igniteCfg = getStringProperty(IGNITE_CONFIG_XML, props, null);	prop.userLibs = getStringProperty(IGNITE_USERS_LIBS, props, null);	String pattern = getStringProperty(IGNITE_HOSTNAME_CONSTRAINT, props, null);	if (pattern != null) {	try {	prop.hostnameConstraint = Pattern.compile(pattern);	}	catch (PatternSyntaxException e) {	
ignite hostname constraint has invalid pattern it will be ignore 

========================= ignite sample_6201 =========================

private void startStopIgnites() throws Exception {	for (int attempt = 0; attempt < 3; ++attempt) {	
iteration 

========================= ignite sample_2079 =========================

int lastGridIdx = GRID_CNT - 1;	IgniteFuture<?> fut = taskStarter.apply(grid(lastGridIdx).cluster().forPredicate(excludeLastPredicate()));	jobLatch.await();	stopGrid(lastGridIdx, true);	latch.countDown();	assert invokeLatch.await(5000, MILLISECONDS);	try {	fut.get();	}	catch (IgniteException e) {	
task failed 

========================= ignite sample_2215 =========================

assert dataChunk.size() == DATA_CHUNK_SIZE;	log.info("Pushing data chunk [chunkNo=" + chunkCntr + "]");	ComputeTaskFuture<Void> fut = comp.executeAsync( new GridCachePutAllTask( runningWorkers.get(rnd.nextInt(runningWorkers.size())).cluster().localNode().id(), CACHE_NAME), dataChunk);	resQueue.put(fut);	fut.listen(new CI1<IgniteFuture<Void>>() {	ComputeTaskFuture<?> taskFut = (ComputeTaskFuture<?>)f;	try {	taskFut.get();	}	catch (IgniteException e) {	
job failed 

stopGrid(victim.name());	failoverPushGap = FAILOVER_PUSH_GAP;	}	}	}	}	}	inputExhausted.set(true);	if (resQueue.isEmpty()) emptyLatch.countDown();	assert chunkCntr == TEST_MAP_SIZE / DATA_CHUNK_SIZE;	
waiting for empty queue 

chunkCntr++;	log.info("Pushing data chunk [chunkNo=" + chunkCntr + "]");	ComputeTaskFuture<Void> fut = comp.executeAsync(new GridCachePutAllTask(nodeId, CACHE_NAME), data);	resQueue.put(fut);	fut.listen(new CI1<IgniteFuture<Void>>() {	ComputeTaskFuture<?> taskFut = (ComputeTaskFuture<?>)f;	try {	taskFut.get();	}	catch (IgniteException e) {	
job failed 

}	for (Map.Entry<UUID, Collection<Integer>> entry : dataChunks.entrySet()) {	ComputeTaskFuture<Void> fut = comp.executeAsync(new GridCachePutAllTask(entry.getKey(), CACHE_NAME), entry.getValue());	resQueue.put(fut);	fut.listen(new CI1<IgniteFuture<Void>>() {	ComputeTaskFuture<?> taskFut = (ComputeTaskFuture<?>)f;	try {	taskFut.get();	}	catch (IgniteException e) {	
job failed 

========================= ignite sample_1593 =========================

private void initializeSemaphore() throws IgniteCheckedException {	if (!initGuard.get() && initGuard.compareAndSet(false, true)) {	try {	sync = retryTopologySafe(new Callable<Sync>() {	try (GridNearTxLocal tx = CU.txStartInternal(ctx, cacheView, PESSIMISTIC, REPEATABLE_READ)) {	GridCacheSemaphoreState val = cacheView.get(key);	if (val == null) {	
failed to find semaphore with given name 

final int cnt = val.getCount();	Map<UUID, Integer> waiters = val.getWaiters();	final boolean failoverSafe = val.isFailoverSafe();	tx.commit();	Sync sync = new Sync(cnt, waiters, failoverSafe);	sync.setBroken(val.isBroken());	return sync;	}	}	});	
initialized internal sync structure 

========================= ignite sample_4284 =========================

private IgniteInternalFuture<GridCacheReturn> obtainLockAsync( final GridCacheContext cacheCtx, GridCacheReturn ret, final Collection<KeyCacheObject> passedKeys, final boolean read, final boolean needRetVal, final long createTtl, final long accessTtl, boolean skipStore, boolean keepBinary) {	if (log.isDebugEnabled()) log.debug("Before acquiring transaction lock on keys [keys=" + passedKeys + ']');	if (passedKeys.isEmpty()) return new GridFinishedFuture<>(ret);	GridDhtTransactionalCacheAdapter<?, ?> dhtCache = cacheCtx.isNear() ? cacheCtx.nearTx().dht() : cacheCtx.dhtTx();	long timeout = remainingTime();	if (timeout == -1) return new GridFinishedFuture<>(timeoutException());	IgniteInternalFuture<Boolean> fut = dhtCache.lockAllAsyncInternal(passedKeys, timeout, this, isInvalidate(), read, needRetVal, isolation, createTtl, accessTtl, CU.empty0(), skipStore, keepBinary);	return new GridEmbeddedFuture<>( fut, new PLC1<GridCacheReturn>(ret) {	
acquired transaction lock on keys 

========================= ignite sample_3812 =========================

for (KeyCacheObject key : keys) {	while (true) {	GridDistributedCacheEntry entry = peekExx(key);	try {	if (entry != null) {	entry.doneRemote( req.version(), req.version(), null, req.committedVersions(), req.rolledbackVersions(), if (entry.removeLock(req.version())) {	if (log.isDebugEnabled()) log.debug("Removed lock [lockId=" + req.version() + ", key=" + key + ']');	evictNearEntry(entry, obsoleteVer, topVer);	}	else {	
received unlock request for unknown candidate added to cancelled locks set 

try {	if (entry != null) {	entry.doneRemote( req.version(), req.version(), null, req.committedVersions(), req.rolledbackVersions(), if (entry.removeLock(req.version())) {	if (log.isDebugEnabled()) log.debug("Removed lock [lockId=" + req.version() + ", key=" + key + ']');	evictNearEntry(entry, obsoleteVer, topVer);	}	else {	}	ctx.evicts().touch(entry, topVer);	}	
received unlock request for entry that could not be found 

GridCacheMvccCandidate cand = entry.candidate(ver);	if (cand != null) {	if (map == null) {	Collection<ClusterNode> affNodes = CU.affinityNodes(ctx, cand.topologyVersion());	if (F.isEmpty(affNodes)) return;	keyCnt = (int)Math.ceil((double)keys.size() / affNodes.size());	map = U.newHashMap(affNodes.size());	}	ClusterNode primary = ctx.affinity().primaryByKey(key, cand.topologyVersion());	if (primary == null) {	
failed to unlock key all partition nodes left the grid 

for (Map.Entry<ClusterNode, GridNearUnlockRequest> mapping : map.entrySet()) {	ClusterNode n = mapping.getKey();	GridDistributedUnlockRequest req = mapping.getValue();	if (!F.isEmpty(req.keys())) {	req.completedVersions(committed, rolledback);	ctx.io().send(n, req, ctx.ioPolicy());	}	}	}	catch (IgniteCheckedException ex) {	
failed to unlock the lock for keys 

========================= ignite sample_3865 =========================

private void notifyListener(IgniteInClosure<? super IgniteInternalFuture<R>> lsnr) {	assert lsnr != null;	try {	lsnr.apply(this);	}	catch (IllegalStateException e) {	U.error(logger(), "Failed to notify listener (is grid stopped?) [fut=" + this + ", lsnr=" + lsnr + ", err=" + e.getMessage() + ']', e);	}	catch (RuntimeException | Error e) {	
failed to notify listener 

========================= ignite sample_3473 =========================

private void doTransformResourceInjection(IgniteCache<String, Integer> cache) throws Exception {	final Collection<ResourceType> required = Arrays.asList( ResourceType.SPRING_APPLICATION_CONTEXT, ResourceType.SPRING_BEAN);	Integer flags = cache.invoke(UUID.randomUUID().toString(), new SpringResourceInjectionEntryProcessor());	assertTrue("Processor result is null", flags != null);	
injection flag 

========================= ignite sample_7963 =========================

private void checkJoin(List<Map<Integer, Integer>> cachesData, List<T2<CacheMode, Integer>> allModes, Stack<T2<CacheMode, Integer>> modes, int caches) throws Exception {	if (modes.size() == caches) {	List<CacheConfiguration> ccfgs = new ArrayList<>();	for (int i = 0; i < modes.size(); i++) {	T2<CacheMode, Integer> mode = modes.get(i);	CacheConfiguration ccfg = configuration("cache" + i, mode.get1(), mode.get2());	ccfgs.add(ccfg);	}	
check configurations 

========================= ignite sample_7534 =========================

GridCacheMvccCandidate lock = ctx.mvcc().removeExplicitLock(threadId, txKey, ver);	if (lock != null) {	AffinityTopologyVersion topVer = lock.topologyVersion();	if (map == null) {	Collection<ClusterNode> affNodes = CU.affinityNodes(ctx, topVer);	keyCnt = (int)Math.ceil((double)keys.size() / affNodes.size());	map = U.newHashMap(affNodes.size());	}	ClusterNode primary = ctx.affinity().primaryByKey(key, topVer);	if (primary == null) {	
failed to remove locks all partition nodes left the grid 

catch (ClusterTopologyCheckedException e) {	if (log.isDebugEnabled()) log.debug("Failed to send unlock request (node has left the grid) [keys=" + req.keys() + ", n=" + n + ", e=" + e + ']');	}	catch (IgniteCheckedException e) {	U.error(log, "Failed to send unlock request [keys=" + req.keys() + ", n=" + n + ']', e);	}	}	}	}	catch (IgniteCheckedException ex) {	
failed to unlock the lock for keys 

for (KeyCacheObject key : keys) {	if (timedout) break;	while (true) {	GridDhtCacheEntry entry = entryExx(key, topVer);	try {	fut.addEntry(key == null ? null : entry);	if (fut.isDone()) timedout = true;	break;	}	catch (GridCacheEntryRemovedException ignore) {	
got removed entry when adding lock will retry 

========================= ignite sample_3824 =========================

private void delete(IgniteUuid trashId) {	IgfsEntryInfo info = null;	try {	info = meta.info(trashId);	}	catch (ClusterTopologyServerNotFoundException ignore) {	}	catch (IgniteCheckedException e) {	
cannot obtain trash directory info is node stopping 

private void delete(IgniteUuid trashId) {	IgfsEntryInfo info = null;	try {	info = meta.info(trashId);	}	catch (ClusterTopologyServerNotFoundException ignore) {	}	catch (IgniteCheckedException e) {	
cannot obtain trash directory info 

if (!cancelled) {	if (delete(trashId, entry.getKey(), fileId)) {	if (log.isDebugEnabled()) log.debug("Sending delete confirmation message [name=" + entry.getKey() + ", fileId=" + fileId + ']');	}	}	else break;	}	catch (IgniteInterruptedCheckedException ignored) {	}	catch (IgniteCheckedException e) {	
failed to delete entry from the trash directory 

========================= ignite sample_4748 =========================

if (httpSrv.isStarted()) {	for (Connector con : httpSrv.getConnectors()) {	int connPort = ((NetworkConnector)con).getPort();	if (connPort > 0) ctx.ports().registerPort(connPort, TCP, getClass());	}	return true;	}	return  false;	}	catch (SocketException ignore) {	
failed to bind http server to configured port 

}	return true;	}	return  false;	}	catch (SocketException ignore) {	stopJetty();	return false;	}	catch (MultiException e) {	
caught multi exception 

return true;	}	return  false;	}	catch (SocketException ignore) {	stopJetty();	return false;	}	catch (MultiException e) {	for (Object obj : e.getThrowables()) if (!(obj instanceof SocketException)) throw new IgniteCheckedException("Failed to start Jetty HTTP server.", e);	
failed to bind http server to configured port 

boolean interrupted = Thread.interrupted();	try {	httpSrv.stop();	}	finally {	if (interrupted) Thread.currentThread().interrupt();	}	}	}	catch (InterruptedException ignored) {	
thread has been interrupted 

}	finally {	if (interrupted) Thread.currentThread().interrupt();	}	}	}	catch (InterruptedException ignored) {	Thread.currentThread().interrupt();	}	catch (Exception e) {	
failed to stop jetty http server 

========================= ignite sample_7867 =========================

checkAwait();	checkCountDown();	IgniteCountDownLatch latch1 = grid(0).countDownLatch("latch", 2, false, true);	assertEquals(2, latch1.count());	IgniteFuture<Object> fut = grid(0).compute().callAsync(new IgniteCallable<Object>() {	private Ignite ignite;	private IgniteLogger log;	IgniteInternalFuture<?> fut = GridTestUtils.runMultiThreadedAsync( new Callable<Object>() {	IgniteCountDownLatch latch = ignite.countDownLatch("latch", 2, false, true);	assert latch != null && latch.count() == 2;	
thread is going to wait on latch 

checkCountDown();	IgniteCountDownLatch latch1 = grid(0).countDownLatch("latch", 2, false, true);	assertEquals(2, latch1.count());	IgniteFuture<Object> fut = grid(0).compute().callAsync(new IgniteCallable<Object>() {	private Ignite ignite;	private IgniteLogger log;	IgniteInternalFuture<?> fut = GridTestUtils.runMultiThreadedAsync( new Callable<Object>() {	IgniteCountDownLatch latch = ignite.countDownLatch("latch", 2, false, true);	assert latch != null && latch.count() == 2;	assert latch.await(1, MINUTES);	
thread is again runnable 

========================= ignite sample_991 =========================

private void doJob() throws InterruptedException {	if (emitAttrs) {	for (int i = 0; i < EMIT_SEQUENCE_LENGTH; i++) {	try {	taskSes.setAttribute(String.valueOf(i), i);	}	catch (IgniteException e) {	
set attribute failed 

========================= ignite sample_2389 =========================

public void testSendReceiveMessage() throws Exception {	final Collection<Object> rcvMsgs = new GridConcurrentHashSet<>();	final AtomicBoolean error = new AtomicBoolean(false);	final CountDownLatch rcvLatch = new CountDownLatch(3);	ignite1.message().localListen(null, new P2<UUID, Object>() {	try {	log.info("Received new message [msg=" + msg + ", senderNodeId=" + nodeId + ']');	if (!nodeId.equals(ignite2.cluster().localNode().id())) {	
unexpected sender node 

public void testSendReceiveMessageWithStringTopic() throws Exception {	final Collection<Object> rcvMsgs = new GridConcurrentHashSet<>();	final AtomicBoolean error = new AtomicBoolean(false);	final CountDownLatch rcvLatch = new CountDownLatch(3);	ignite1.message().localListen(S_TOPIC_1, new P2<UUID, Object>() {	try {	log.info("Received new message [msg=" + msg + ", senderNodeId=" + nodeId + ", topic=" + S_TOPIC_1 + ']');	if (!nodeId.equals(ignite1.cluster().localNode().id())) {	
unexpected sender node 

final AtomicBoolean error = new AtomicBoolean(false);	final CountDownLatch rcvLatch = new CountDownLatch(3);	ignite1.message().localListen(S_TOPIC_1, new P2<UUID, Object>() {	try {	log.info("Received new message [msg=" + msg + ", senderNodeId=" + nodeId + ", topic=" + S_TOPIC_1 + ']');	if (!nodeId.equals(ignite1.cluster().localNode().id())) {	error.set(true);	return false;	}	if (!MSG_1.equals(msg)) {	
unexpected message for topic 

public void testSendReceiveMessageWithEnumTopic() throws Exception {	final Collection<Object> rcvMsgs = new GridConcurrentHashSet<>();	final AtomicBoolean error = new AtomicBoolean(false);	final CountDownLatch rcvLatch = new CountDownLatch(3);	ignite1.message().localListen(TestTopic.TOPIC_1, new P2<UUID, Object>() {	try {	log.info("Received new message [msg=" + msg + ", senderNodeId=" + nodeId + ", topic=" + TestTopic.TOPIC_1 + ']');	if (!nodeId.equals(ignite1.cluster().localNode().id())) {	
unexpected sender node 

final AtomicBoolean error = new AtomicBoolean(false);	final CountDownLatch rcvLatch = new CountDownLatch(3);	ignite1.message().localListen(TestTopic.TOPIC_1, new P2<UUID, Object>() {	try {	log.info("Received new message [msg=" + msg + ", senderNodeId=" + nodeId + ", topic=" + TestTopic.TOPIC_1 + ']');	if (!nodeId.equals(ignite1.cluster().localNode().id())) {	error.set(true);	return false;	}	if (!MSG_1.equals(msg)) {	
unexpected message for topic 

public void testRemoteListenOrderedMessages() throws Exception {	List<TestMessage> msgs = Arrays.asList( new TestMessage(MSG_1), new TestMessage(MSG_2, 3000), new TestMessage(MSG_3));	final Collection<Object> rcvMsgs = new ConcurrentLinkedDeque<>();	final AtomicBoolean error = new AtomicBoolean(false);	rcvLatch = new CountDownLatch(3);	ignite2.message().remoteListen(S_TOPIC_1, new P2<UUID, Object>() {	try {	log.info("Received new message [msg=" + msg + ", senderNodeId=" + nodeId + ']');	if (!nodeId.equals(ignite1.cluster().localNode().id())) {	
unexpected sender node 

public void testRemoteListenWithIntTopic() throws Exception {	final Collection<Object> rcvMsgs = new GridConcurrentHashSet<>();	final AtomicBoolean error = new AtomicBoolean(false);	rcvLatch = new CountDownLatch(3);	ignite2.message().remoteListen(I_TOPIC_1, new P2<UUID, Object>() {	private transient Ignite g;	assertEquals(ignite2, g);	try {	log.info("Received new message [msg=" + msg + ", senderNodeId=" + nodeId + ", topic=" + I_TOPIC_1 + ']');	if (!nodeId.equals(ignite1.cluster().localNode().id())) {	
unexpected sender node 

ignite2.message().remoteListen(I_TOPIC_1, new P2<UUID, Object>() {	private transient Ignite g;	assertEquals(ignite2, g);	try {	log.info("Received new message [msg=" + msg + ", senderNodeId=" + nodeId + ", topic=" + I_TOPIC_1 + ']');	if (!nodeId.equals(ignite1.cluster().localNode().id())) {	error.set(true);	return false;	}	if (!MSG_1.equals(msg)) {	
unexpected message for topic 

public void testSendMessageWithExternalClassLoader() throws Exception {	URL[] urls = new URL[] {new URL(GridTestProperties.getProperty("p2p.uri.cls"))};	ClassLoader extLdr = new URLClassLoader(urls);	Class rcCls = extLdr.loadClass(EXT_RESOURCE_CLS_NAME);	final AtomicBoolean error = new AtomicBoolean(false);	final CountDownLatch rcvLatch = new CountDownLatch(1);	ignite2.message().remoteListen(S_TOPIC_1, new P2<UUID, Object>() {	try {	log.info("Received new message [msg=" + msg + ", senderNodeId=" + nodeId + ']');	if (!nodeId.equals(ignite1.cluster().localNode().id())) {	
unexpected sender node 

public void stopBlock() {	List<DiscoverySpiCustomMessage> msgs;	synchronized (this) {	msgs = new ArrayList<>(blockedMsgs);	blockCustomEvt = false;	blockedMsgs.clear();	}	for (DiscoverySpiCustomMessage msg : msgs) {	
resend blocked message 

========================= ignite sample_2809 =========================

updateCache(tester, currVal, invoke, false, keys);	updateCache(tester, currVal + 1, invoke, true, keys);	invoke = !invoke;	currVal++;	synchronized (lastUpdateTs) {	lastUpdateTs.set(System.currentTimeMillis());	lastUpdateTs.notifyAll();	}	}	catch (Throwable e) {	
update failed 

========================= ignite sample_1376 =========================

public void addRebalanceEvent(int part, int type, ClusterNode discoNode, int discoType, long discoTs) {	assert discoNode != null;	assert type > 0;	assert discoType > 0;	assert discoTs > 0;	
added event without checking if event is recordable 

public void addUnloadEvent(int part) {	
added event without checking if event is recordable 

========================= ignite sample_4174 =========================

public void testStartNodes() throws Exception {	for (int i = 0; i < iters; i++) {	try {	
iteration 

cache.put(ignite.cluster().localNode().id(), UUID.randomUUID());	while (!stopped) {	int val = Math.abs(rnd.nextInt(100));	if (val >= 0 && val < 40) cache.containsKey(ignite.cluster().localNode().id());	else if (val >= 40 && val < 80) cache.get(ignite.cluster().localNode().id());	else cache.put(ignite.cluster().localNode().id(), UUID.randomUUID());	Thread.sleep(50);	}	}	catch (Exception e) {	
unexpected error 

========================= ignite sample_1233 =========================

if (retval != null) {	for (Map.Entry<String, Object> e : retval.entrySet()) {	if (ctx.hasNodeAttribute(e.getKey())) throw new IgniteCheckedException("SPI attribute collision for attribute [spi=" + spi + ", attr=" + e.getKey() + ']' + ". Attribute set by one SPI implementation has the same name (name collision) as " + "attribute set by other SPI implementation. Such overriding is not allowed. " + "Please check your Ignite configuration and/or SPI implementation to avoid " + "attribute name collisions.");	ctx.addNodeAttribute(e.getKey(), e.getValue());	}	}	}	catch (IgniteSpiException e) {	throw new IgniteCheckedException("Failed to get SPI attributes.", e);	}	
starting spi 

if (ctx.hasNodeAttribute(e.getKey())) throw new IgniteCheckedException("SPI attribute collision for attribute [spi=" + spi + ", attr=" + e.getKey() + ']' + ". Attribute set by one SPI implementation has the same name (name collision) as " + "attribute set by other SPI implementation. Such overriding is not allowed. " + "Please check your Ignite configuration and/or SPI implementation to avoid " + "attribute name collisions.");	ctx.addNodeAttribute(e.getKey(), e.getValue());	}	}	}	catch (IgniteSpiException e) {	throw new IgniteCheckedException("Failed to get SPI attributes.", e);	}	if (names.contains(spi.getName())) throw new IgniteCheckedException("Duplicate SPI name (need to explicitly configure 'setName()' property): " + spi.getName());	names.add(spi.getName());	
starting spi implementation 

if (names.contains(spi.getName())) throw new IgniteCheckedException("Duplicate SPI name (need to explicitly configure 'setName()' property): " + spi.getName());	names.add(spi.getName());	onBeforeSpiStart();	try {	spi.spiStart(ctx.igniteInstanceName());	}	catch (IgniteSpiException e) {	throw new IgniteCheckedException("Failed to start SPI: " + spi, e);	}	onAfterSpiStart();	
spi module started ok 

protected final void stopSpi() throws IgniteCheckedException {	for (T spi : spis) {	if (spiMap.remove(spi) == null) {	
will not stop spi since it has not been started by this manager 

protected final void stopSpi() throws IgniteCheckedException {	for (T spi : spis) {	if (spiMap.remove(spi) == null) {	continue;	}	
stopping spi 

protected final void stopSpi() throws IgniteCheckedException {	for (T spi : spis) {	if (spiMap.remove(spi) == null) {	continue;	}	try {	spi.spiStop();	
spi module stopped ok 

spi.spiStop();	}	catch (IgniteSpiException e) {	throw new IgniteCheckedException("Failed to stop SPI: " + spi, e);	}	try {	cleanup(spi);	ctx.resource().cleanup(spi);	}	catch (IgniteCheckedException e) {	
failed to remove injected resources from spi ignoring 

========================= ignite sample_3520 =========================

private void checkFsOpenWithAllNodesTypes() throws Exception {	for (int i = 0; i < nodesTypes.length; ++i) {	
begin test case for nodes 

========================= ignite sample_7133 =========================

if (cctx.kernalContext().isStopping()) {	if (log.isDebugEnabled()) log.debug(msg0.toString());	}	else {	U.error(log, msg0.toString());	try {	cacheMsg.onClassError(new IgniteCheckedException("Failed to find message handler for message: " + cacheMsg));	processFailedMessage(nodeId, cacheMsg, c, plc);	}	catch (Exception e) {	
failed to process failed message 

cnt++;	cctx.gridIO().sendToGridTopic(node, TOPIC_CACHE, msg, plc);	return;	}	catch (ClusterTopologyCheckedException e) {	throw e;	}	catch (IgniteCheckedException e) {	if (!cctx.discovery().alive(node.id()) || !cctx.discovery().pingNode(node.id())) throw new ClusterTopologyCheckedException("Node left grid while sending message to: " + node.id(), e);	if (cnt == retryCnt || cctx.kernalContext().isStopping()) throw e;	
failed to send message to node will retry 

cctx.gridIO().sendOrderedMessage(node, topic, msg, plc, timeout, false);	if (log.isDebugEnabled()) log.debug("Sent ordered cache message [topic=" + topic + ", msg=" + msg + ", nodeId=" + node.id() + ']');	return;	}	catch (ClusterTopologyCheckedException e) {	throw e;	}	catch (IgniteCheckedException e) {	if (cctx.discovery().node(node.id()) == null) throw new ClusterTopologyCheckedException("Node left grid while sending ordered message to: " + node.id(), e);	if (cnt == retryCnt) throw e;	
failed to send message to node will retry 

========================= ignite sample_3936 =========================

private void unRegistrateMetricsMBean() {	if (persistenceMetricsMbeanName == null) return;	assert !U.IGNITE_MBEANS_DISABLED;	try {	cctx.kernalContext().config().getMBeanServer().unregisterMBean(persistenceMetricsMbeanName);	persistenceMetricsMbeanName = null;	}	catch (Throwable e) {	
failed to unregister mbean 

private void shutdownCheckpointer(boolean cancel) {	Checkpointer cp = checkpointer;	if (cp != null) {	if (cancel) cp.shutdownNow();	else cp.cancel();	try {	U.join(cp);	checkpointer = null;	}	catch (IgniteInterruptedCheckedException ignore) {	
was interrupted while waiting for checkpointer shutdown will not wait for checkpoint to finish 

if (idx < 0) partToCheckPntEntry.remove(partId);	else {	if (partToCheckPntEntry.containsKey(partId)) continue;	partToCheckPntEntry.put(partId, chpEntry);	}	}	}	}	catch (IgniteCheckedException ex) {	String msg = chpEntry != null ? ", chpId=" + chpEntry.cpId + " ptr=" + chpEntry.cpMark + " ts=" + chpEntry.cpTs : "";	
failed to read checkpoint entry 

UUID startId = CheckpointStatus.NULL_UUID;	UUID endId = CheckpointStatus.NULL_UUID;	File startFile = null;	File endFile = null;	WALPointer startPtr = CheckpointStatus.NULL_PTR;	WALPointer endPtr = CheckpointStatus.NULL_PTR;	File dir = cpDir;	if (!dir.exists()) {	File[] files = dir.listFiles();	if (files != null && files.length > 0) {	
read checkpoint status cpdir exists is false cpdir listfiles is 

UUID endId = CheckpointStatus.NULL_UUID;	File startFile = null;	File endFile = null;	WALPointer startPtr = CheckpointStatus.NULL_PTR;	WALPointer endPtr = CheckpointStatus.NULL_PTR;	File dir = cpDir;	if (!dir.exists()) {	File[] files = dir.listFiles();	if (files != null && files.length > 0) {	}	
read checkpoint status cpdir exists is false files exists cpdir is true 

UUID endId = CheckpointStatus.NULL_UUID;	File startFile = null;	File endFile = null;	WALPointer startPtr = CheckpointStatus.NULL_PTR;	WALPointer endPtr = CheckpointStatus.NULL_PTR;	File dir = cpDir;	if (!dir.exists()) {	File[] files = dir.listFiles();	if (files != null && files.length > 0) {	}	
read checkpoint status checkpoint directory is not found 

if (printCheckpointStats) {	if (log.isInfoEnabled()) log.info(String.format("Checkpoint finished [cpId=%s, pages=%d, markPos=%s, " + "walSegmentsCleared=%d, markDuration=%dms, pagesWrite=%dms, fsync=%dms, " + "total=%dms]", chp.cpEntry.checkpointId(), chp.pagesSize, chp.cpEntry.checkpointMark(), chp.walFilesDeleted, tracker.markDuration(), tracker.pagesWriteDuration(), tracker.fsyncDuration(), tracker.totalDuration()));	}	persStoreMetrics.onCheckpoint( tracker.lockWaitDuration(), tracker.markDuration(), tracker.pagesWriteDuration(), tracker.fsyncDuration(), tracker.totalDuration(), chp.pagesSize, tracker.dataPagesWritten(), tracker.cowPagesWritten());	}	else {	persStoreMetrics.onCheckpoint( tracker.lockWaitDuration(), tracker.markDuration(), tracker.pagesWriteDuration(), tracker.fsyncDuration(), tracker.totalDuration(), chp.pagesSize, tracker.dataPagesWritten(), tracker.cowPagesWritten());	}	}	catch (IgniteCheckedException e) {	
failed to create checkpoint 

private void onWalTruncated(WALPointer ptr) {	FileWALPointer highBound = (FileWALPointer)ptr;	List<CheckpointEntry> cpToRemove = new ArrayList<>();	for (CheckpointEntry cpEntry : histMap.values()) {	FileWALPointer cpPnt = (FileWALPointer)cpEntry.checkpointMark();	if (highBound.compareTo(cpPnt) <= 0) break;	if (cctx.wal().reserved(cpEntry.checkpointMark())) {	
could not clear historymap due to wal reservation on cpentry history map size is 

private void onCheckpointFinished(Checkpoint chp) {	int deleted = 0;	boolean dropWal = persistenceCfg.getWalHistorySize() != Integer.MAX_VALUE;	while (histMap.size() > maxCpHistMemSize) {	Map.Entry<Long, CheckpointEntry> entry = histMap.firstEntry();	CheckpointEntry cpEntry = entry.getValue();	if (cctx.wal().reserved(cpEntry.checkpointMark())) {	
could not clear historymap due to wal reservation on cpentry history map size is 

private boolean removeCheckpointFiles(CheckpointEntry cpEntry) {	File startFile = new File(cpDir.getAbsolutePath(), cpEntry.startFile());	File endFile = new File(cpDir.getAbsolutePath(), cpEntry.endFile());	boolean rmvdStart = !startFile.exists() || startFile.delete();	boolean rmvdEnd = !endFile.exists() || endFile.delete();	boolean fail = !rmvdStart || !rmvdEnd;	if (fail) {	U.warn(log, "Failed to remove stale checkpoint files [startFile=" + startFile.getAbsolutePath() + ", endFile=" + endFile.getAbsolutePath() + ']');	if (histMap.size() > 2 * maxCpHistMemSize) {	
too many stale checkpoint entries in the map will truncate wal archive anyway 

for (int i = 0; i < lockWaitTimeMillis; i += 1000) {	try {	lock = ch.tryLock(0, 1, false);	if (lock != null && lock.isValid()) {	writeContent(sb.toString());	return;	}	}	catch (OverlappingFileLockException ignore) {	if (content == null) content = readContent();	
failed to acquire file lock local nodeid already locked by will try again in 

========================= ignite sample_4123 =========================

public void download(GridFutureAdapter<?> fut){	this.finishFut = fut;	final ServerSocketChannel ch = serverChannel;	fut.listen(new IgniteInClosureX<IgniteInternalFuture<?>>() {	try {	if (log != null && log.isInfoEnabled()) log.info("Server socket closed " + ch.getLocalAddress());	ch.close();	}	catch (Exception ex) {	
fail close socket 

========================= ignite sample_4066 =========================

private GridClientResponse makeFailureResponse(GridClientException e, UUID clientId, Long reqId) {	
failed to process message on router 

========================= ignite sample_5227 =========================

public boolean handshake() throws IgniteCheckedException, SSLException {	
entered handshake handshake status 

handshakeStatus = runTasks();	break;	}	case NEED_UNWRAP: {	Status status = unwrapHandshake();	handshakeStatus = sslEngine.getHandshakeStatus();	if (status == BUFFER_UNDERFLOW && sslEngine.isInboundDone()) loop = false;	break;	}	case NEED_WRAP: {	
output net buffer has unsent bytes during handshake will clear 

}	handshakeStatus = res.getHandshakeStatus();	if (log.isDebugEnabled()) log.debug("Wrapped handshake data [status=" + res.getStatus() + ", handshakeStatus=" + handshakeStatus + ']');	break;	}	default: {	throw new IllegalStateException("Invalid handshake status in handshake method [handshakeStatus=" + handshakeStatus + ']');	}	}	}	
leaved handshake handshake status 

public ByteBuffer encrypt(ByteBuffer src) throws SSLException {	assert handshakeFinished;	outNetBuf.clear();	while (src.hasRemaining()) {	int outNetRemaining = outNetBuf.capacity() - outNetBuf.position();	if (outNetRemaining < src.remaining() * 2) {	outNetBuf = expandBuffer(outNetBuf, Math.max( outNetBuf.position() + src.remaining() * 2, outNetBuf.capacity() * 2));	
expanded output net buffer 

appBuf = expandBuffer(appBuf, inNetBuf.capacity() * 2);	if (log.isDebugEnabled()) log.debug("Expanded buffers [inNetBufCapacity=" + inNetBuf.capacity() + ", appBufCapacity=" + appBuf.capacity() + ']');	}	inNetBuf.put(buf);	if (!handshakeFinished) handshake();	else unwrapData();	if (isInboundDone()) {	int newPosition = buf.position() - inNetBuf.position();	if (newPosition >= 0) {	buf.position(newPosition);	
got unread bytes after receiving close notify message will ignore 

private void unwrapData() throws IgniteCheckedException, SSLException {	
unwrapping received data 

private HandshakeStatus runTasks() {	Runnable runnable;	while ((runnable = sslEngine.getDelegatedTask()) != null) {	
running ssl engine task 

private HandshakeStatus runTasks() {	Runnable runnable;	while ((runnable = sslEngine.getDelegatedTask()) != null) {	runnable.run();	}	
finished running ssl engine tasks handshakestatus 

========================= ignite sample_3367 =========================

final GridCacheSharedContext<Object, Object> clientCtx = ((IgniteKernal)client).context().cache().context();	final CountDownLatch latch = new CountDownLatch(1);	final AtomicBoolean res = new AtomicBoolean();	clientCtx.gridIO().addMessageListener(TOPIC, new GridMessageListener() {	if (msg instanceof TxLocksResponse) {	try {	((TxLocksResponse)msg).finishUnmarshal(clientCtx, clientCtx.deploy().globalLoader());	res.set(true);	}	catch (Exception e) {	
message unmarshal failed 

========================= ignite sample_1618 =========================

private void init() throws IgniteException {	if (initGuard.compareAndSet(false, true)) {	
initializing cache store 

private void init() throws IgniteException {	if (initGuard.compareAndSet(false, true)) {	try {	if (sesFactory != null) return;	if (!F.isEmpty(hibernateCfgPath)) {	try {	URL url = new URL(hibernateCfgPath);	sesFactory = new Configuration().configure(url).buildSessionFactory();	
configured session factory using url 

if (initGuard.compareAndSet(false, true)) {	try {	if (sesFactory != null) return;	if (!F.isEmpty(hibernateCfgPath)) {	try {	URL url = new URL(hibernateCfgPath);	sesFactory = new Configuration().configure(url).buildSessionFactory();	return;	}	catch (MalformedURLException e) {	
caught malformed url exception 

try {	URL url = new URL(hibernateCfgPath);	sesFactory = new Configuration().configure(url).buildSessionFactory();	return;	}	catch (MalformedURLException e) {	}	File cfgFile = new File(hibernateCfgPath);	if (cfgFile.exists()) {	sesFactory = new Configuration().configure(cfgFile).buildSessionFactory();	
configured session factory using file 

return;	}	catch (MalformedURLException e) {	}	File cfgFile = new File(hibernateCfgPath);	if (cfgFile.exists()) {	sesFactory = new Configuration().configure(cfgFile).buildSessionFactory();	return;	}	sesFactory = new Configuration().configure(hibernateCfgPath).buildSessionFactory();	
configured session factory using classpath resource 

}	File cfgFile = new File(hibernateCfgPath);	if (cfgFile.exists()) {	sesFactory = new Configuration().configure(cfgFile).buildSessionFactory();	return;	}	sesFactory = new Configuration().configure(hibernateCfgPath).buildSessionFactory();	}	else {	if (hibernateProps == null) {	
no hibernate configuration has been provided for store will use default 

hibernateProps = new Properties();	hibernateProps.setProperty("hibernate.connection.url", DFLT_CONN_URL);	hibernateProps.setProperty("hibernate.show_sql", DFLT_SHOW_SQL);	hibernateProps.setProperty("hibernate.hbm2ddl.auto", DFLT_HBM2DDL_AUTO);	}	Configuration cfg = new Configuration();	cfg.setProperties(hibernateProps);	assert resourceAvailable(MAPPING_RESOURCE) : MAPPING_RESOURCE;	cfg.addResource(MAPPING_RESOURCE);	sesFactory = cfg.buildSessionFactory();	
configured session factory using properties 

private boolean resourceAvailable(String name) {	InputStream cfgStream = Thread.currentThread().getContextClassLoader().getResourceAsStream(name);	if (cfgStream == null) {	
classpath resource not found 

private boolean resourceAvailable(String name) {	InputStream cfgStream = Thread.currentThread().getContextClassLoader().getResourceAsStream(name);	if (cfgStream == null) {	return false;	}	try {	cfgStream.read();	return true;	}	catch (IOException e) {	
failed to read classpath resource 

========================= ignite sample_6132 =========================

for (GridServiceDeploymentFuture fut : failingFuts) {	Exception cause = new Exception("Test error");	causes.add(cause);	fut.onDone(cause);	}	try {	compFut.get(100);	fail("Should never reach here.");	}	catch (IgniteFutureTimeoutCheckedException e) {	
expected exception 

fail("Should never reach here.");	}	catch (IgniteFutureTimeoutCheckedException e) {	}	for (GridFutureAdapter<Object> fut : futs) fut.onDone();	try {	compFut.get();	fail("Should never reach here.");	}	catch (IgniteCheckedException ce) {	
expected exception 

========================= ignite sample_2171 =========================

keys.add(i);	if (keys.size() == KEYS) break;	}	}	assertEquals(KEYS, keys.size());	final int THREADS = 10;	final int UPDATES = 1000;	final List<IgniteCache<Object, Object>> srvCaches = new ArrayList<>();	for (String cacheName : caches) srvCaches.add(srv.cache(cacheName));	for (int i = 0; i < 15; i++) {	
iteration 

GridTestUtils.runMultiThreaded(new Callable<Void>() {	ThreadLocalRandom rnd = ThreadLocalRandom.current();	for (int i = 0; i < UPDATES; i++) {	for (int c = 0; c < srvCaches.size(); c++) srvCaches.get(c).put(keys.get(rnd.nextInt(KEYS)), i);	}	return null;	}	}, THREADS, "update");	for (final AtomicInteger evtCnt : cntrs) {	GridTestUtils.waitForCondition(new GridAbsPredicate() {	
events 

keys.add(i);	if (keys.size() == KEYS) break;	}	}	assertEquals(KEYS, keys.size());	final int THREADS = 10;	final int UPDATES = 1000;	final List<IgniteCache<Object, Object>> srvCaches = new ArrayList<>();	for (String cacheName : caches) srvCaches.add(srv.cache(cacheName));	for (int i = 0; i < 5; i++) {	
iteration 

========================= ignite sample_1931 =========================

public void testSkipPreload() throws Exception {	Ignite ignite0 = startGrid(0);	final CountDownLatch evtLatch0 = new CountDownLatch(1);	ignite0.events().localListen(new IgnitePredicate<Event>() {	
rebalance event 

Ignite ignite1 = startGrid(1);	waitForTopologyUpdate(2, new AffinityTopologyVersion(2, 1));	TestCommunicationSpi spi1 = (TestCommunicationSpi)ignite1.configuration().getCommunicationSpi();	assertEquals(0, spi0.partitionsSingleMessages());	assertEquals(2, spi0.partitionsFullMessages());	assertEquals(2, spi1.partitionsSingleMessages());	assertEquals(0, spi1.partitionsFullMessages());	spi0.reset();	spi1.reset();	client = true;	
start client 

TestCommunicationSpi spi2 = (TestCommunicationSpi)ignite2.configuration().getCommunicationSpi();	assertEquals(0, spi0.partitionsSingleMessages());	assertEquals(1, spi0.partitionsFullMessages());	assertEquals(0, spi1.partitionsSingleMessages());	assertEquals(0, spi1.partitionsFullMessages());	assertEquals(1, spi2.partitionsSingleMessages());	assertEquals(0, spi2.partitionsFullMessages());	spi0.reset();	spi1.reset();	spi2.reset();	
start client 

assertEquals(0, spi1.partitionsSingleMessages());	assertEquals(0, spi1.partitionsFullMessages());	assertEquals(0, spi2.partitionsSingleMessages());	assertEquals(0, spi2.partitionsFullMessages());	assertEquals(1, spi3.partitionsSingleMessages());	assertEquals(0, spi3.partitionsFullMessages());	spi0.reset();	spi1.reset();	spi2.reset();	spi3.reset();	
start one more server node 

assertEquals(2, spi2.partitionsSingleMessages());	assertEquals(0, spi2.partitionsFullMessages());	assertEquals(2, spi3.partitionsSingleMessages());	assertEquals(0, spi3.partitionsFullMessages());	assertEquals(2, spi4.partitionsSingleMessages());	assertEquals(0, spi4.partitionsFullMessages());	spi0.reset();	spi1.reset();	spi2.reset();	spi3.reset();	
stop server node 

assertEquals(1, spi1.partitionsSingleMessages());	assertEquals(0, spi1.partitionsFullMessages());	assertEquals(1, spi2.partitionsSingleMessages());	assertEquals(0, spi2.partitionsFullMessages());	assertEquals(1, spi3.partitionsSingleMessages());	assertEquals(0, spi3.partitionsFullMessages());	}	spi0.reset();	spi1.reset();	spi2.reset();	
stop client 

ignite3.close();	waitForTopologyUpdate(3, 7);	assertEquals(0, spi0.partitionsSingleMessages());	assertEquals(0, spi0.partitionsFullMessages());	assertEquals(0, spi1.partitionsSingleMessages());	assertEquals(0, spi1.partitionsFullMessages());	assertEquals(0, spi2.partitionsSingleMessages());	assertEquals(0, spi2.partitionsFullMessages());	spi0.reset();	spi1.reset();	
stop client 

assertEquals(0, spi2.partitionsSingleMessages());	assertEquals(0, spi2.partitionsFullMessages());	spi0.reset();	spi1.reset();	ignite2.close();	waitForTopologyUpdate(2, 8);	assertEquals(0, spi0.partitionsSingleMessages());	assertEquals(0, spi0.partitionsFullMessages());	assertEquals(0, spi1.partitionsSingleMessages());	assertEquals(0, spi1.partitionsFullMessages());	
stop server node 

========================= ignite sample_1198 =========================

}	else {	entry.cached(cached);	txState.addWriteEntry(entry.txKey(), entry);	addExplicit(entry);	return true;	}	}	catch (GridCacheEntryRemovedException ignore) {	evicted.add(entry.txKey());	
got removed entry when adding to remote transaction will ignore 

}	else {	IgniteTxEntry txEntry = new IgniteTxEntry(cacheCtx, this, op, val, -1L, -1L, cached, drVer, skipStore, keepBinary);	txState.addWriteEntry(key, txEntry);	return true;	}	}	}	catch (GridCacheEntryRemovedException ignore) {	evicted.add(key);	
got removed entry when adding reads to remote transaction will ignore 

========================= ignite sample_3849 =========================

public void testNodeRestartRandom() throws Exception {	final int NODES = 5;	Ignite ignite = startGridsMultiThreaded(NODES);	ignite.services().deploy(serviceConfiguration());	for (int i = 0; i < 30; i++) {	
iteration 

========================= ignite sample_2167 =========================

set = oldVal != null ? oldVal : set;	}	if (!set.add(e.getValue())) doubleNtfFail.set(false);	}	}	};	for (int i = 0; i < nodesCnt; i++) {	ContinuousQuery<Integer, Integer> qry = new ContinuousQuery<>();	qry.setLocalListener(lsnr);	Ignite ignite = grid(i);	
try to start cq on node 

}	if (!set.add(e.getValue())) doubleNtfFail.set(false);	}	}	};	for (int i = 0; i < nodesCnt; i++) {	ContinuousQuery<Integer, Integer> qry = new ContinuousQuery<>();	qry.setLocalListener(lsnr);	Ignite ignite = grid(i);	qryCursors.add(ignite.cache(ccfg.getName()).query(qry));	
cq started on node 

========================= ignite sample_1966 =========================

assert ignite.compute().localTasks().get(GridDeploymentTestTask.class.getName()) != null;	ignite.compute().undeployTask(GridDeploymentTestTask.class.getName());	final CyclicBarrier barrier = new CyclicBarrier(THREAD_CNT, new Runnable() {	private int iterCnt;	try {	ignite.compute().undeployTask(GridDeploymentTestTask.class.getName());	assert ignite.compute().localTasks().get(GridDeploymentTestTask.class.getName()) == null;	if (++iterCnt % 100 == 0) info("Iterations count: " + iterCnt);	}	catch (IgniteException e) {	
failed to undeploy task message 

});	GridTestUtils.runMultiThreaded(new Callable<Object>() {	try {	for (int i = 0; i < EXEC_CNT; i++) {	barrier.await(2000, MILLISECONDS);	ignite.compute().localDeployTask(GridDeploymentTestTask.class, GridDeploymentTestTask.class.getClassLoader());	assert ignite.compute().localTasks().get(GridDeploymentTestTask.class.getName()) != null;	}	}	catch (Exception e) {	
test failed 

========================= ignite sample_2219 =========================


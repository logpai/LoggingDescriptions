public synchronized void getCurrentValue(Writable val) throws IOException {	if (val instanceof Configurable) {	((Configurable) val).setConf(this.conf);	}	seekToCurrentValue();	if (!blockCompressed) {	val.readFields(valIn);	if (valIn.read() > 0) {	
available bytes 

if (!blockCompressed) {	val.readFields(valIn);	if (valIn.read() > 0) {	throw new IOException(val+" read "+(valBuffer.getPosition()-keyLength) + " bytes, should read " + (valBuffer.getLength()-keyLength));	}	} else {	int valLength = WritableUtils.readVInt(valLenIn);	val.readFields(valIn);	--noBufferedValues;	if ((valLength < 0) && LOG.isDebugEnabled()) {	
is a zero length value 

public synchronized Object getCurrentValue(Object val) throws IOException {	if (val instanceof Configurable) {	((Configurable) val).setConf(this.conf);	}	seekToCurrentValue();	if (!blockCompressed) {	val = deserializeValue(val);	if (valIn.read() > 0) {	
available bytes 

if (!blockCompressed) {	val = deserializeValue(val);	if (valIn.read() > 0) {	throw new IOException(val+" read "+(valBuffer.getPosition()-keyLength) + " bytes, should read " + (valBuffer.getLength()-keyLength));	}	} else {	int valLength = WritableUtils.readVInt(valLenIn);	val = deserializeValue(val);	--noBufferedValues;	if ((valLength < 0) && LOG.isDebugEnabled()) {	
is a zero length value 

private void handleChecksumException(ChecksumException e) throws IOException {	if (this.conf.getBoolean( IO_SKIP_CHECKSUM_ERRORS_KEY, IO_SKIP_CHECKSUM_ERRORS_DEFAULT)) {	
bad checksum at skipping entries 

private int sortPass(boolean deleteInput) throws IOException {	if(LOG.isDebugEnabled()) {	
running sort pass 

int keyLength = rawKeys.getLength() - keyOffset;	if (count == keyOffsets.length) grow();	keyOffsets[count] = keyOffset;	pointers[count] = count;	keyLengths[count] = keyLength;	rawValues[count] = rawValue;	bytesProcessed += recordLength;	count++;	}	if(LOG.isDebugEnabled()) {	
flushing segment 

private int mergePass(Path tmpDir) throws IOException {	if(LOG.isDebugEnabled()) {	
running merge pass 

factor = origFactor;	return this;	} else {	long approxOutputSize = 0;	for (SegmentDescriptor s : segmentsToMerge) {	approxOutputSize += s.segmentLength + ChecksumFileSystem.getApproxChkSumLength( s.segmentLength);	}	Path tmpFilename = new Path(tmpDir, "intermediate").suffix("." + passNo);	Path outputFile =  lDirAlloc.getLocalPathForWrite( tmpFilename.toString(), approxOutputSize, conf);	if(LOG.isDebugEnabled()) {	
writing intermediate results to 

========================= hadoop sample_3923 =========================

cluster = new MiniDFSCluster.Builder(conf) .nnTopology(topology) .numDataNodes(0) .build();	cluster.waitActive();	nn0 = cluster.getNameNode(0);	fs = HATestUtil.configureFailoverFs(cluster, conf);	cluster.transitionToActive(0);	fs = cluster.getFileSystem(0);	editLog = nn0.getNamesystem().getEditLog();	++retryCount;	break;	} catch (BindException e) {	
set up minidfscluster failed due to port conflicts retry times 

========================= hadoop sample_7433 =========================

public void testBlockMissingException() throws Exception {	
test testblockmissingexception started 

conf = new HdfsConfiguration();	conf.setInt(HdfsClientConfigKeys.Retry.WINDOW_BASE_KEY, 10);	try {	dfs = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DATANODES).build();	dfs.waitActive();	fileSys = dfs.getFileSystem();	Path file1 = new Path("/user/dhruba/raidtest/file1");	createOldFile(fileSys, file1, 1, numBlocks, blockSize);	LocatedBlocks locations = null;	locations = fileSys.dfs.getNamenode().getBlockLocations(file1.toString(), 0, numBlocks * blockSize);	
remove first block of file 

Path file1 = new Path("/user/dhruba/raidtest/file1");	createOldFile(fileSys, file1, 1, numBlocks, blockSize);	LocatedBlocks locations = null;	locations = fileSys.dfs.getNamenode().getBlockLocations(file1.toString(), 0, numBlocks * blockSize);	dfs.corruptBlockOnDataNodesByDeletingBlockFile( locations.get(0).getBlock());	validateFile(fileSys, file1);	} finally {	if (fileSys != null) fileSys.close();	if (dfs != null) dfs.shutdown();	}	
test testblockmissingexception completed 

========================= hadoop sample_7599 =========================

public boolean canDelete(String queueName) {	SchedulerQueue<T> queue = queueManager.getQueue(queueName);	if (queue == null) {	
the specified queue does not exist 

public boolean canDelete(String queueName) {	SchedulerQueue<T> queue = queueManager.getQueue(queueName);	if (queue == null) {	return false;	}	if (queue.getState() == QueueState.STOPPED){	return true;	}	
need to stop the specific queue first 

========================= hadoop sample_879 =========================

private TopN getTopUsersForMetric(long time, String metricName, RollingWindowMap rollingWindows) {	TopN topN = new TopN(topUsersCnt);	Iterator<Map.Entry<String, RollingWindow>> iterator = rollingWindows.entrySet().iterator();	while (iterator.hasNext()) {	Map.Entry<String, RollingWindow> entry = iterator.next();	String userName = entry.getKey();	RollingWindow aWindow = entry.getValue();	long windowSum = aWindow.getSum(time);	if (windowSum == 0) {	
gc window of metric username 

Iterator<Map.Entry<String, RollingWindow>> iterator = rollingWindows.entrySet().iterator();	while (iterator.hasNext()) {	Map.Entry<String, RollingWindow> entry = iterator.next();	String userName = entry.getKey();	RollingWindow aWindow = entry.getValue();	long windowSum = aWindow.getSum(time);	if (windowSum == 0) {	iterator.remove();	continue;	}	
offer window of metric username sum 

Map.Entry<String, RollingWindow> entry = iterator.next();	String userName = entry.getKey();	RollingWindow aWindow = entry.getValue();	long windowSum = aWindow.getSum(time);	if (windowSum == 0) {	iterator.remove();	continue;	}	topN.offer(new NameValuePair(userName, windowSum));	}	
topn users size for command is 

========================= hadoop sample_7960 =========================

public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException {	try {	String hostname;	try {	String address = request.getRemoteAddr();	if (address != null) {	hostname = InetAddress.getByName(address).getCanonicalHostName();	} else {	
request remote address is null 

try {	String hostname;	try {	String address = request.getRemoteAddr();	if (address != null) {	hostname = InetAddress.getByName(address).getCanonicalHostName();	} else {	hostname = "???";	}	} catch (UnknownHostException ex) {	
request remote address could not be resolved 

========================= hadoop sample_6737 =========================

public boolean handle(DomainSocket sock) {	assert(lock.isHeldByCurrentThread());	try {	kicked = false;	if (LOG.isTraceEnabled()) {	
notificationhandler doing a read on 

public boolean handle(DomainSocket sock) {	assert(lock.isHeldByCurrentThread());	try {	kicked = false;	if (LOG.isTraceEnabled()) {	}	if (sock.getInputStream().read() == -1) {	if (LOG.isTraceEnabled()) {	
notificationhandler got eof on 

try {	kicked = false;	if (LOG.isTraceEnabled()) {	}	if (sock.getInputStream().read() == -1) {	if (LOG.isTraceEnabled()) {	}	throw new EOFException();	}	if (LOG.isTraceEnabled()) {	
notificationhandler read succeeded on 

if (sock.getInputStream().read() == -1) {	if (LOG.isTraceEnabled()) {	}	throw new EOFException();	}	if (LOG.isTraceEnabled()) {	}	return false;	} catch (IOException e) {	if (LOG.isTraceEnabled()) {	
notificationhandler setting closed to true for 

if (loadingFailureReason != null) {	throw new UnsupportedOperationException(loadingFailureReason);	}	Preconditions.checkArgument(interruptCheckPeriodMs > 0);	this.interruptCheckPeriodMs = interruptCheckPeriodMs;	notificationSockets = DomainSocket.socketpair();	watcherThread.setDaemon(true);	watcherThread.setName(src + " DomainSocketWatcher");	watcherThread .setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {	public void uncaughtException(Thread thread, Throwable t) {	
terminating on unexpected exception 

public void close() throws IOException {	lock.lock();	try {	if (closed) return;	if (LOG.isDebugEnabled()) {	
closing 

private void kick() {	assert(lock.isHeldByCurrentThread());	if (kicked) {	return;	}	try {	notificationSockets[0].getOutputStream().write(0);	kicked = true;	} catch (IOException e) {	if (!closed) {	
error writing to notificationsockets 

private boolean sendCallback(String caller, TreeMap<Integer, Entry> entries, FdSet fdSet, int fd) {	if (LOG.isTraceEnabled()) {	
starting sendcallback for fd 

private boolean sendCallback(String caller, TreeMap<Integer, Entry> entries, FdSet fdSet, int fd) {	if (LOG.isTraceEnabled()) {	}	Entry entry = entries.get(fd);	Preconditions.checkNotNull(entry, this + ": fdSet contained " + fd + ", which we were " + "not tracking.");	DomainSocket sock = entry.getDomainSocket();	if (entry.getHandler().handle(sock)) {	if (LOG.isTraceEnabled()) {	
closing fd at the request of the handler 

if (LOG.isTraceEnabled()) {	}	Entry entry = entries.get(fd);	Preconditions.checkNotNull(entry, this + ": fdSet contained " + fd + ", which we were " + "not tracking.");	DomainSocket sock = entry.getDomainSocket();	if (entry.getHandler().handle(sock)) {	if (LOG.isTraceEnabled()) {	}	if (toRemove.remove(fd) != null) {	if (LOG.isTraceEnabled()) {	
sendcallback processed fd in toremove 

try {	sock.refCount.unreferenceCheckClosed();	} catch (IOException e) {	Preconditions.checkArgument(false, this + ": file descriptor " + sock.fd + " was closed while " + "still in the poll(2) loop.");	}	IOUtils.cleanupWithLogger(LOG, sock);	fdSet.remove(fd);	return true;	} else {	if (LOG.isTraceEnabled()) {	
sendcallback not closing fd 

sendCallbackAndRemove("getAndClearReadableFds", entries, fdSet, fd);	}	if (!(toAdd.isEmpty() && toRemove.isEmpty())) {	for (Iterator<Entry> iter = toAdd.iterator(); iter.hasNext(); ) {	Entry entry = iter.next();	iter.remove();	DomainSocket sock = entry.getDomainSocket();	Entry prevEntry = entries.put(sock.fd, entry);	Preconditions.checkState(prevEntry == null, this + ": tried to watch a file descriptor that we " + "were already watching: " + sock);	if (LOG.isTraceEnabled()) {	
adding fd 

}	while (true) {	Map.Entry<Integer, DomainSocket> entry = toRemove.firstEntry();	if (entry == null) break;	sendCallbackAndRemove("handlePendingRemovals", entries, fdSet, entry.getValue().fd);	}	processedCond.signalAll();	}	if (closed) {	if (LOG.isDebugEnabled()) {	
thread terminating 

}	if (Thread.interrupted()) {	throw new InterruptedException();	}	} finally {	lock.unlock();	}	doPoll0(interruptCheckPeriodMs, fdSet);	}	} catch (InterruptedException e) {	
terminating on interruptedexception 

if (Thread.interrupted()) {	throw new InterruptedException();	}	} finally {	lock.unlock();	}	doPoll0(interruptCheckPeriodMs, fdSet);	}	} catch (InterruptedException e) {	} catch (Throwable e) {	
terminating on exception 

private void addNotificationSocket(final TreeMap<Integer, Entry> entries, FdSet fdSet) {	entries.put(notificationSockets[1].fd, new Entry(notificationSockets[1], new NotificationHandler()));	try {	notificationSockets[1].refCount.reference();	} catch (IOException e) {	throw new RuntimeException(e);	}	fdSet.add(notificationSockets[1].fd);	if (LOG.isTraceEnabled()) {	
adding notificationsocket connected to 

========================= hadoop sample_3535 =========================

public N removeNode(NodeId nodeId) {	writeLock.lock();	try {	N node = nodes.remove(nodeId);	if (node == null) {	
attempting to remove a non existent node 

writeLock.lock();	try {	N node = nodes.remove(nodeId);	if (node == null) {	return null;	}	nodeNameToNodeMap.remove(node.getNodeName());	String rackName = node.getRackName();	List<N> nodesList = nodesPerRack.get(rackName);	if (nodesList == null) {	
attempting to remove node from an empty rack 

public List<N> getNodesByResourceName(final String resourceName) {	Preconditions.checkArgument( resourceName != null && !resourceName.isEmpty());	List<N> retNodes = new ArrayList<>();	if (ResourceRequest.ANY.equals(resourceName)) {	retNodes.addAll(getAllNodes());	} else if (nodeNameToNodeMap.containsKey(resourceName)) {	retNodes.add(nodeNameToNodeMap.get(resourceName));	} else if (nodesPerRack.containsKey(resourceName)) {	retNodes.addAll(nodesPerRack.get(resourceName));	} else {	
could not find a node matching given resourcename 

List<NodeId> retNodes = new ArrayList<>();	if (ResourceRequest.ANY.equals(resourceName)) {	retNodes.addAll(getAllNodeIds());	} else if (nodeNameToNodeMap.containsKey(resourceName)) {	retNodes.add(nodeNameToNodeMap.get(resourceName).getNodeID());	} else if (nodesPerRack.containsKey(resourceName)) {	for (N node : nodesPerRack.get(resourceName)) {	retNodes.add(node.getNodeID());	}	} else {	
could not find a node matching given resourcename 

========================= hadoop sample_896 =========================

protected boolean isValidRequestor(HttpServletRequest request, Configuration conf) throws IOException {	String remotePrincipal = request.getUserPrincipal().getName();	String remoteShortName = request.getRemoteUser();	if (remotePrincipal == null) {	
received null remoteuser while authorizing access to GetJournalEditServlet 

protected boolean isValidRequestor(HttpServletRequest request, Configuration conf) throws IOException {	String remotePrincipal = request.getUserPrincipal().getName();	String remoteShortName = request.getRemoteUser();	if (remotePrincipal == null) {	return false;	}	if (LOG.isDebugEnabled()) {	
validating request made by this user is 

if (remotePrincipal == null) {	return false;	}	if (LOG.isDebugEnabled()) {	}	Set<String> validRequestors = new HashSet<String>();	validRequestors.addAll(DFSUtil.getAllNnPrincipals(conf));	try {	validRequestors.add( SecurityUtil.getServerPrincipal(conf .get(DFSConfigKeys.DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY), SecondaryNameNode.getHttpAddress(conf).getHostName()));	} catch (Exception e) {	
secondarynamenode principal could not be added 

}	Set<String> validRequestors = new HashSet<String>();	validRequestors.addAll(DFSUtil.getAllNnPrincipals(conf));	try {	validRequestors.add( SecurityUtil.getServerPrincipal(conf .get(DFSConfigKeys.DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY), SecondaryNameNode.getHttpAddress(conf).getHostName()));	} catch (Exception e) {	String msg = String.format( "SecondaryNameNode principal not considered, %s = %s, %s = %s", DFSConfigKeys.DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY, conf.get(DFSConfigKeys.DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY), DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY, conf.get(DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY, DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_DEFAULT));	LOG.warn(msg);	}	for (String v : validRequestors) {	
isvalidrequestor is comparing to valid requestor 

Set<String> validRequestors = new HashSet<String>();	validRequestors.addAll(DFSUtil.getAllNnPrincipals(conf));	try {	validRequestors.add( SecurityUtil.getServerPrincipal(conf .get(DFSConfigKeys.DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY), SecondaryNameNode.getHttpAddress(conf).getHostName()));	} catch (Exception e) {	String msg = String.format( "SecondaryNameNode principal not considered, %s = %s, %s = %s", DFSConfigKeys.DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY, conf.get(DFSConfigKeys.DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY), DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY, conf.get(DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY, DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_DEFAULT));	LOG.warn(msg);	}	for (String v : validRequestors) {	if (v != null && v.equals(remotePrincipal)) {	
isvalidrequestor is allowing 

} catch (Exception e) {	String msg = String.format( "SecondaryNameNode principal not considered, %s = %s, %s = %s", DFSConfigKeys.DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY, conf.get(DFSConfigKeys.DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY), DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY, conf.get(DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY, DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_DEFAULT));	LOG.warn(msg);	}	for (String v : validRequestors) {	if (v != null && v.equals(remotePrincipal)) {	return true;	}	}	if (remoteShortName.equals( UserGroupInformation.getLoginUser().getShortUserName())) {	
isvalidrequestor is allowing other jn principal 

LOG.warn(msg);	}	for (String v : validRequestors) {	if (v != null && v.equals(remotePrincipal)) {	return true;	}	}	if (remoteShortName.equals( UserGroupInformation.getLoginUser().getShortUserName())) {	return true;	}	
isvalidrequestor is rejecting 

private boolean checkRequestorOrSendError(Configuration conf, HttpServletRequest request, HttpServletResponse response) throws IOException {	if (UserGroupInformation.isSecurityEnabled() && !isValidRequestor(request, conf)) {	response.sendError(HttpServletResponse.SC_FORBIDDEN, "Only Namenode and another JournalNode may access this servlet");	
received non nn jn request for edits from 

private boolean checkStorageInfoOrSendError(JNStorage storage, HttpServletRequest request, HttpServletResponse response) throws IOException {	int myNsId = storage.getNamespaceID();	String myClusterId = storage.getClusterID();	String theirStorageInfoString = StringEscapeUtils.escapeHtml( request.getParameter(STORAGEINFO_PARAM));	if (theirStorageInfoString != null) {	int theirNsId = StorageInfo.getNsIdFromColonSeparatedString( theirStorageInfoString);	String theirClusterId = StorageInfo.getClusterIdFromColonSeparatedString( theirStorageInfoString);	if (myNsId != theirNsId || !myClusterId.equals(theirClusterId)) {	String msg = "This node has namespaceId '" + myNsId + " and clusterId '" + myClusterId + "' but the requesting node expected '" + theirNsId + "' and '" + theirClusterId + "'";	response.sendError(HttpServletResponse.SC_FORBIDDEN, msg);	
received an invalid request file transfer request from 

========================= hadoop sample_8379 =========================

fileSummaryFromDFS = hdfs.getContentSummary(file1OnParentDir);	symLinkSummaryFromDFS = hdfs.getContentSummary(link1);	hdfs.createSymlink(childDir1, link2, true);	symLinkSummaryForDirContainsFromDFS = hdfs.getContentSummary(new Path( "/dirForLinks"));	hdfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_ENTER, false);	hdfs.saveNamespace();	originalFsimage = FSImageTestUtil.findLatestImageFile(FSImageTestUtil .getFSImage(cluster.getNameNode()).getStorage().getStorageDir(0));	if (originalFsimage == null) {	throw new RuntimeException("Didn't generate or can't find fsimage");	}	
original fs image file is 

========================= hadoop sample_7126 =========================

int i = 0;	long offset = out.getPos();	int maxBlockLocations = conf.getInt(MRConfig.MAX_BLOCK_LOCATIONS_KEY, MRConfig.MAX_BLOCK_LOCATIONS_DEFAULT);	for(org.apache.hadoop.mapred.InputSplit split: splits) {	long prevLen = out.getPos();	Text.writeString(out, split.getClass().getName());	split.write(out);	long currLen = out.getPos();	String[] locations = split.getLocations();	if (locations.length > maxBlockLocations) {	
max block location exceeded for split splitsize maxsize 

========================= hadoop sample_4835 =========================

private void recover() throws IOException {	if (stateStore.canRecover()) {	RecoveredLogDeleterState state = stateStore.loadLogDeleterState();	long now = System.currentTimeMillis();	for (Map.Entry<ApplicationId, LogDeleterProto> entry : state.getLogDeleterMap().entrySet()) {	ApplicationId appId = entry.getKey();	LogDeleterProto proto = entry.getValue();	long deleteDelayMsec = proto.getDeletionTime() - now;	if (LOG.isDebugEnabled()) {	
scheduling deletion of logs in msec 

public void run() {	List<Path> localAppLogDirs = new ArrayList<Path>();	FileContext lfs = getLocalFileContext(getConfig());	for (String rootLogDir : dirsHandler.getLogDirsForCleanup()) {	Path logDir = new Path(rootLogDir, applicationId.toString());	try {	lfs.getFileStatus(logDir);	localAppLogDirs.add(logDir);	} catch (UnsupportedFileSystemException ue) {	
unsupported file system used for log dir 

}	}	NonAggregatingLogHandler.this.dispatcher.getEventHandler().handle( new ApplicationEvent(this.applicationId, ApplicationEventType.APPLICATION_LOG_HANDLING_FINISHED));	if (localAppLogDirs.size() > 0) {	FileDeletionTask deletionTask = new FileDeletionTask( NonAggregatingLogHandler.this.delService, user, null, localAppLogDirs);	NonAggregatingLogHandler.this.delService.delete(deletionTask);	}	try {	NonAggregatingLogHandler.this.stateStore.removeLogDeleter( this.applicationId);	} catch (IOException e) {	
error removing log deletion state 

========================= hadoop sample_1764 =========================

public static int getCacheDepth(Configuration conf) {	int cacheDepth = conf.getInt(YarnConfiguration.SHARED_CACHE_NESTED_LEVEL, YarnConfiguration.DEFAULT_SHARED_CACHE_NESTED_LEVEL);	if (cacheDepth <= 0) {	
specified cache depth was less than or equal to zero using default value instead default specified 

========================= hadoop sample_1334 =========================

public Allocation allocate(ApplicationAttemptId applicationAttemptId, List<ResourceRequest> ask, List<ContainerId> release, List<String> blacklistAdditions, List<String> blacklistRemovals, ContainerUpdates updateRequests) {	FifoAppAttempt application = getApplicationAttempt(applicationAttemptId);	if (application == null) {	
calling allocate on removed or non existent application 

public Allocation allocate(ApplicationAttemptId applicationAttemptId, List<ResourceRequest> ask, List<ContainerId> release, List<String> blacklistAdditions, List<String> blacklistRemovals, ContainerUpdates updateRequests) {	FifoAppAttempt application = getApplicationAttempt(applicationAttemptId);	if (application == null) {	return EMPTY_ALLOCATION;	}	if (!application.getApplicationAttemptId().equals(applicationAttemptId)) {	
calling allocate on previous or removed or non existent application attempt 

if (application == null) {	return EMPTY_ALLOCATION;	}	if (!application.getApplicationAttemptId().equals(applicationAttemptId)) {	return EMPTY_ALLOCATION;	}	normalizeRequests(ask);	releaseContainers(release, application);	synchronized (application) {	if (application.isStopped()) {	
calling allocate on a stopped application 

public synchronized void addApplication(ApplicationId applicationId, String queue, String user, boolean isAppRecovering) {	SchedulerApplication<FifoAppAttempt> application = new SchedulerApplication<>(DEFAULT_QUEUE, user);	applications.put(applicationId, application);	metrics.submitApp(user);	
accepted application from user currently num of applications 

public synchronized void addApplication(ApplicationId applicationId, String queue, String user, boolean isAppRecovering) {	SchedulerApplication<FifoAppAttempt> application = new SchedulerApplication<>(DEFAULT_QUEUE, user);	applications.put(applicationId, application);	metrics.submitApp(user);	if (isAppRecovering) {	if (LOG.isDebugEnabled()) {	
is recovering skip notifying app accepted 

private synchronized void doneApplication(ApplicationId applicationId, RMAppState finalState) {	SchedulerApplication<FifoAppAttempt> application = applications.get(applicationId);	if (application == null){	
couldn t find application 

private synchronized void doneApplicationAttempt( ApplicationAttemptId applicationAttemptId, RMAppAttemptState rmAppAttemptFinalState, boolean keepContainers) throws IOException {	FifoAppAttempt attempt = getApplicationAttempt(applicationAttemptId);	SchedulerApplication<FifoAppAttempt> application = applications.get(applicationAttemptId.getApplicationId());	if (application == null || attempt == null) {	throw new IOException("Unknown application " + applicationAttemptId + " has completed!");	}	for (RMContainer container : attempt.getLiveContainers()) {	if (keepContainers && container.getState().equals(RMContainerState.RUNNING)) {	
skip killing 

private void assignContainers(FiCaSchedulerNode node) {	LOG.debug("assignContainers:" + " node=" + node.getRMNode().getNodeAddress() + " #applications=" + applications.size());	for (Map.Entry<ApplicationId, SchedulerApplication<FifoAppAttempt>> e : applications .entrySet()) {	FifoAppAttempt application = e.getValue().getCurrentAppAttempt();	if (application == null) {	continue;	}	
pre assigncontainers 

for (SchedulerRequestKey schedulerKey : application.getSchedulerKeys()) {	int maxContainers = getMaxAllocatableContainers(application, schedulerKey, node, NodeType.OFF_SWITCH);	if (maxContainers > 0) {	int assignedContainers = assignContainersOnNode(node, application, schedulerKey);	if (assignedContainers == 0) {	break;	}	}	}	}	
post assigncontainers 

case APP_ATTEMPT_ADDED: {	AppAttemptAddedSchedulerEvent appAttemptAddedEvent = (AppAttemptAddedSchedulerEvent) event;	addApplicationAttempt(appAttemptAddedEvent.getApplicationAttemptId(), appAttemptAddedEvent.getTransferStateFromPreviousAttempt(), appAttemptAddedEvent.getIsAttemptRecovering());	}	break;	case APP_ATTEMPT_REMOVED: {	AppAttemptRemovedSchedulerEvent appAttemptRemovedEvent = (AppAttemptRemovedSchedulerEvent) event;	try {	doneApplicationAttempt( appAttemptRemovedEvent.getApplicationAttemptID(), appAttemptRemovedEvent.getFinalAttemptState(), appAttemptRemovedEvent.getKeepContainersAcrossAppAttempts());	} catch(IOException ie) {	
unable to remove application 

}	break;	case RELEASE_CONTAINER: {	if (!(event instanceof ReleaseContainerEvent)) {	throw new RuntimeException("Unexpected event type: " + event);	}	RMContainer container = ((ReleaseContainerEvent) event).getContainer();	completedContainer(container, SchedulerUtils.createAbnormalContainerStatus( container.getContainerId(), SchedulerUtils.RELEASED_CONTAINER), RMContainerEventType.RELEASED);	}	break;	
invalid eventtype ignoring 

protected synchronized void completedContainerInternal( RMContainer rmContainer, ContainerStatus containerStatus, RMContainerEventType event) {	Container container = rmContainer.getContainer();	FifoAppAttempt application = getCurrentAttemptForContainer(container.getId());	ApplicationId appId = container.getId().getApplicationAttemptId().getApplicationId();	FiCaSchedulerNode node = (FiCaSchedulerNode) getNode(container.getNodeId());	if (application == null) {	
unknown application released container on node with event 

Container container = rmContainer.getContainer();	FifoAppAttempt application = getCurrentAttemptForContainer(container.getId());	ApplicationId appId = container.getId().getApplicationAttemptId().getApplicationId();	FiCaSchedulerNode node = (FiCaSchedulerNode) getNode(container.getNodeId());	if (application == null) {	return;	}	application.containerCompleted(rmContainer, containerStatus, event, RMNodeLabelsManager.NO_LABEL);	node.releaseContainer(rmContainer.getContainerId(), false);	Resources.subtractFrom(usedResource, container.getResource());	
application attempt released container on node with event 

========================= hadoop sample_984 =========================

public Response toResponse(Exception e) {	if (LOG.isTraceEnabled()) {	
got excepition 

s = Response.Status.BAD_REQUEST;	} else if (e instanceof IllegalArgumentException) {	s = Response.Status.BAD_REQUEST;	} else if (e instanceof NumberFormatException) {	s = Response.Status.BAD_REQUEST;	} else if (e instanceof BadRequestException) {	s = Response.Status.BAD_REQUEST;	} else if (e instanceof WebApplicationException && e.getCause() instanceof UnmarshalException) {	s = Response.Status.BAD_REQUEST;	} else {	
internal server error 

========================= hadoop sample_2236 =========================

private void setUpClusterMetrics(ClusterMetricsInfo metrics, long seed) {	
using seed 

========================= hadoop sample_1949 =========================

public void testResourceAllocation() throws IOException, YarnException, InterruptedException {	
start testresourceallocation 

application.addTask(t1);	final int memory2 = 2048;	Resource capability2 = Resources.createResource(memory2, 1);	Priority priority0 = Priority.newInstance(0);	application.addResourceRequestSpec(priority0, capability2);	application.schedule();	nodeUpdate(nm1);	((AbstractYarnScheduler)resourceManager.getResourceScheduler()).update();	application.schedule();	checkResourceUsage(nm1, nm2);	
adding new tasks 

Task t2 = new Task(application, priority1, new String[] {host1, host2});	application.addTask(t2);	Task t3 = new Task(application, priority0, new String[] {ResourceRequest.ANY});	application.addTask(t3);	application.schedule();	checkResourceUsage(nm1, nm2);	nodeUpdate(nm2);	nodeUpdate(nm2);	nodeUpdate(nm1);	nodeUpdate(nm1);	
trying to allocate 

Task t3 = new Task(application, priority0, new String[] {ResourceRequest.ANY});	application.addTask(t3);	application.schedule();	checkResourceUsage(nm1, nm2);	nodeUpdate(nm2);	nodeUpdate(nm2);	nodeUpdate(nm1);	nodeUpdate(nm1);	application.schedule();	checkResourceUsage(nm1, nm2);	
finishing up tasks 

nodeUpdate(nm1);	nodeUpdate(nm1);	application.schedule();	checkResourceUsage(nm1, nm2);	application.finishTask(t1);	application.finishTask(t2);	application.finishTask(t3);	AppAttemptRemovedSchedulerEvent appRemovedEvent1 = new AppAttemptRemovedSchedulerEvent( application.getApplicationAttemptId(), RMAppAttemptState.FINISHED, false);	resourceManager.getResourceScheduler().handle(appRemovedEvent1);	checkResourceUsage(nm1, nm2);	
end testresourceallocation 

}	};	Configuration conf = new YarnConfiguration();	conf.set(filterInitializerConfKey, filterInitializer);	conf.set("hadoop.security.authentication", "kerberos");	conf.set("hadoop.http.authentication.type", "kerberos");	try {	try {	UserGroupInformation.setConfiguration(conf);	} catch (Exception e) {	
got expected exception 

========================= hadoop sample_438 =========================

public synchronized Client getClient(Configuration conf, SocketFactory factory, Class<? extends Writable> valueClass) {	Client client = clients.get(factory);	if (client == null) {	client = new Client(valueClass, conf, factory);	clients.put(factory, client);	} else {	client.incCount();	}	if (Client.LOG.isDebugEnabled()) {	
getting client out of cache 

public void stopClient(Client client) {	if (Client.LOG.isDebugEnabled()) {	
stopping client from cache 

public void stopClient(Client client) {	if (Client.LOG.isDebugEnabled()) {	}	synchronized (this) {	client.decCount();	if (client.isZeroReference()) {	if (Client.LOG.isDebugEnabled()) {	
removing client from cache 

synchronized (this) {	client.decCount();	if (client.isZeroReference()) {	if (Client.LOG.isDebugEnabled()) {	}	clients.remove(client.getSocketFactory());	}	}	if (client.isZeroReference()) {	if (Client.LOG.isDebugEnabled()) {	
stopping actual client because no more references remain 

========================= hadoop sample_4024 =========================

logRequest.setAppOwner(appOwner);	logRequest.setContainerId(containerId.toString());	logRequest.setNodeId(nmContext.getNodeId().toString());	logRequest.setBytes(bytes);	Set<String> logTypes = new HashSet<>();	logTypes.add(outputFileName);	logRequest.setLogTypes(logTypes);	factory.getFileControllerForRead(appId, appOwner) .readAggregatedLogs(logRequest, os);	} catch (IOException ex) {	if (LOG.isDebugEnabled()) {	
can not access the aggregated log for the container 

========================= hadoop sample_1699 =========================

public RegisterApplicationMasterResponse registerApplicationMaster( final RegisterApplicationMasterRequest request) throws YarnException, IOException {	
forwarding registration request to the real yarn rm 

public AllocateResponse allocate(final AllocateRequest request) throws YarnException, IOException {	if (LOG.isDebugEnabled()) {	
forwarding allocate request to the real yarn rm 

public DistributedSchedulingAllocateResponse allocateForDistributedScheduling( DistributedSchedulingAllocateRequest request) throws YarnException, IOException {	if (LOG.isDebugEnabled()) {	
forwarding allocatefordistributedscheduling request to the real yarn rm 

public FinishApplicationMasterResponse finishApplicationMaster( final FinishApplicationMasterRequest request) throws YarnException, IOException {	
forwarding finish application request to the real yarn resource manager 

========================= hadoop sample_1733 =========================

public void testHistoryParsing() throws Exception {	
starting testhistoryparsing 

public void testHistoryParsing() throws Exception {	try {	checkHistoryParsing(2, 1, 2);	} finally {	
finished testhistoryparsing 

public void testHistoryParsingWithParseErrors() throws Exception {	
starting testhistoryparsingwithparseerrors 

public void testHistoryParsingWithParseErrors() throws Exception {	try {	checkHistoryParsing(3, 0, 2);	} finally {	
finished testhistoryparsingwithparseerrors 

private void checkHistoryParsing(final int numMaps, final int numReduces, final int numSuccessfulMaps) throws Exception {	Configuration conf = new Configuration();	conf.set(MRJobConfig.USER_NAME, System.getProperty("user.name"));	long amStartTimeEst = System.currentTimeMillis();	conf.setClass( NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY, MyResolver.class, DNSToSwitchMapping.class);	RackResolver.init(conf);	MRApp app = new MRAppWithHistory(numMaps, numReduces, true, this.getClass() .getName(), true);	app.submit(conf);	Job job = app.getContext().getAllJobs().values().iterator().next();	JobId jobId = job.getID();	
jobid is 

app.submit(conf);	Job job = app.getContext().getAllJobs().values().iterator().next();	JobId jobId = job.getID();	app.waitForState(job, JobState.SUCCEEDED);	app.waitForState(Service.STATE.STOPPED);	String jobhistoryDir = JobHistoryUtils .getHistoryIntermediateDoneDirForUser(conf);	FileContext fc = null;	try {	fc = FileContext.getFileContext(conf);	} catch (IOException ioe) {	
can not get filecontext 

Assert.assertEquals("Status does not match", "SUCCEEDED", jobSummaryElements.get("status"));	}	JobHistory jobHistory = new JobHistory();	jobHistory.init(conf);	HistoryFileInfo fileInfo = jobHistory.getJobFileInfo(jobId);	JobInfo jobInfo;	long numFinishedMaps;	synchronized (fileInfo) {	Path historyFilePath = fileInfo.getHistoryFile();	FSDataInputStream in = null;	
jobhistoryfile is 

jobHistory.init(conf);	HistoryFileInfo fileInfo = jobHistory.getJobFileInfo(jobId);	JobInfo jobInfo;	long numFinishedMaps;	synchronized (fileInfo) {	Path historyFilePath = fileInfo.getHistoryFile();	FSDataInputStream in = null;	try {	in = fc.open(fc.makeQualified(historyFilePath));	} catch (IOException ioe) {	
can not open history file 

public void testHistoryParsingForFailedAttempts() throws Exception {	
starting testhistoryparsingforfailedattempts 

JobHistoryParser parser;	JobInfo jobInfo;	synchronized (fileInfo) {	Path historyFilePath = fileInfo.getHistoryFile();	FSDataInputStream in = null;	FileContext fc = null;	try {	fc = FileContext.getFileContext(conf);	in = fc.open(fc.makeQualified(historyFilePath));	} catch (IOException ioe) {	
can not open history file 

for (TaskAttempt taskAttempt : task.getAttempts().values()) {	TaskAttemptInfo taskAttemptInfo = taskInfo.getAllTaskAttempts().get( TypeConverter.fromYarn((taskAttempt.getID())));	Assert.assertEquals("rack-name is incorrect", taskAttemptInfo.getRackname(), RACK_NAME);	if (taskAttemptInfo.getTaskStatus().equals("FAILED")) {	noOffailedAttempts++;	}	}	}	Assert.assertEquals("No of Failed tasks doesn't match.", 2, noOffailedAttempts);	} finally {	
finished testhistoryparsingforfailedattempts 

public void testCountersForFailedTask() throws Exception {	
starting testcountersforfailedtask 

JobHistoryParser parser;	JobInfo jobInfo;	synchronized (fileInfo) {	Path historyFilePath = fileInfo.getHistoryFile();	FSDataInputStream in = null;	FileContext fc = null;	try {	fc = FileContext.getFileContext(conf);	in = fc.open(fc.makeQualified(historyFilePath));	} catch (IOException ioe) {	
can not open history file 

Assert.assertNotNull("completed task report has null counters", ct .getReport().getCounters());	}	final List<String> originalDiagnostics = job.getDiagnostics();	final String historyError = jobInfo.getErrorInfo();	assertTrue("No original diagnostics for a failed job", originalDiagnostics != null && !originalDiagnostics.isEmpty());	assertNotNull("No history error info for a failed job ", historyError);	for (String diagString : originalDiagnostics) {	assertTrue(historyError.contains(diagString));	}	} finally {	
finished testcountersforfailedtask 

public void testDiagnosticsForKilledJob() throws Exception {	
starting testdiagnosticsforkilledjob 

JobHistoryParser parser;	JobInfo jobInfo;	synchronized (fileInfo) {	Path historyFilePath = fileInfo.getHistoryFile();	FSDataInputStream in = null;	FileContext fc = null;	try {	fc = FileContext.getFileContext(conf);	in = fc.open(fc.makeQualified(historyFilePath));	} catch (IOException ioe) {	
can not open history file 

assertNull("Caught an expected exception " + parseException, parseException);	final List<String> originalDiagnostics = job.getDiagnostics();	final String historyError = jobInfo.getErrorInfo();	assertTrue("No original diagnostics for a failed job", originalDiagnostics != null && !originalDiagnostics.isEmpty());	assertNotNull("No history error info for a failed job ", historyError);	for (String diagString : originalDiagnostics) {	assertTrue(historyError.contains(diagString));	}	assertTrue("No killed message in diagnostics", historyError.contains(JobImpl.JOB_KILLED_DIAG));	} finally {	
finished testdiagnosticsforkilledjob 

public void testScanningOldDirs() throws Exception {	
starting testscanningolddirs 

public void testScanningOldDirs() throws Exception {	try {	Configuration conf = new Configuration();	conf.setClass( NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY, MyResolver.class, DNSToSwitchMapping.class);	RackResolver.init(conf);	MRApp app = new MRAppWithHistory(1, 1, true, this.getClass().getName(), true);	app.submit(conf);	Job job = app.getContext().getAllJobs().values().iterator().next();	JobId jobId = job.getID();	
jobid is 

Assert.assertTrue(!fileInfo.didMoveFail());	msecToSleep -= msecPerSleep;	Thread.sleep(msecPerSleep);	}	Assert.assertTrue("Timeout waiting for history move", msecToSleep > 0);	fileInfo = hfm.getFileInfo(jobId);	hfm.stop();	Assert.assertNotNull("Unable to locate old job history", fileInfo);	Assert.assertTrue("HistoryFileManager not shutdown properly", hfm.moveToDoneExecutor.isTerminated());	} finally {	
finished testscanningolddirs 

public void testDeleteFileInfo() throws Exception {	
starting testdeletefileinfo 

}	Assert.assertNotNull(hfm.jobListCache.values());	hfm.clean();	Assert.assertFalse(fileInfo.isDeleted());	hfm.setMaxHistoryAge(-1);	hfm.clean();	hfm.stop();	Assert.assertTrue("Thread pool shutdown", hfm.moveToDoneExecutor.isTerminated());	Assert.assertTrue("file should be deleted ", fileInfo.isDeleted());	} finally {	
finished testdeletefileinfo 

public void testJobHistoryMethods() throws Exception {	
starting testjobhistorymethods 

Assert.assertEquals(1, jobHistory.getAllJobs(app.getAppID()).size());	JobsInfo jobsinfo = jobHistory.getPartialJobs(0L, 10L, null, "default", 0L, System.currentTimeMillis() + 1, 0L, System.currentTimeMillis() + 1, JobState.SUCCEEDED);	Assert.assertEquals(1, jobsinfo.getJobs().size());	Assert.assertNotNull(jobHistory.getApplicationAttemptId());	Assert.assertEquals("application_0_0000", jobHistory.getApplicationID() .toString());	Assert .assertEquals("Job History Server", jobHistory.getApplicationName());	Assert.assertNull(jobHistory.getEventHandler());	Assert.assertNull(jobHistory.getClock());	Assert.assertNull(jobHistory.getClusterInfo());	} finally {	
finished testjobhistorymethods 

public void testTaskAttemptUnsuccessfulCompletionWithoutCounters203() throws IOException {	Path histPath = new Path(getClass().getClassLoader().getResource( "job_2.0.3-alpha-FAILED.jhist").getFile());	JobHistoryParser parser = new JobHistoryParser(FileSystem.getLocal (new Configuration()), histPath);	JobInfo jobInfo = parser.parse();	
job info 

public void testTaskAttemptUnsuccessfulCompletionWithoutCounters240() throws IOException {	Path histPath = new Path(getClass().getClassLoader().getResource( "job_2.4.0-FAILED.jhist").getFile());	JobHistoryParser parser = new JobHistoryParser(FileSystem.getLocal (new Configuration()), histPath);	JobInfo jobInfo = parser.parse();	
job info 

public void testTaskAttemptUnsuccessfulCompletionWithoutCounters0239() throws IOException {	Path histPath = new Path(getClass().getClassLoader().getResource( "job_0.23.9-FAILED.jhist").getFile());	JobHistoryParser parser = new JobHistoryParser(FileSystem.getLocal (new Configuration()), histPath);	JobInfo jobInfo = parser.parse();	
job info 

========================= hadoop sample_5320 =========================

Configuration conf = new Configuration();	conf.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLOCK_SIZE);	conf.setInt(DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_KEY, 1000);	FSDataOutputStream stm = null;	MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf) .nnTopology(MiniDFSNNTopology.simpleHATopology()) .numDataNodes(3) .build();	try {	int sizeWritten = 0;	cluster.waitActive();	cluster.transitionToActive(0);	Thread.sleep(500);	
starting with nn active 

try {	int sizeWritten = 0;	cluster.waitActive();	cluster.transitionToActive(0);	Thread.sleep(500);	FileSystem fs = HATestUtil.configureFailoverFs(cluster, conf);	stm = fs.create(TEST_PATH);	AppendTestUtil.write(stm, 0, BLOCK_AND_A_HALF);	sizeWritten += BLOCK_AND_A_HALF;	stm.hflush();	
failing over to nn 

private void doTestWriteOverFailoverWithDnFail(TestScenario scenario) throws Exception {	Configuration conf = new Configuration();	conf.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLOCK_SIZE);	FSDataOutputStream stm = null;	MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf) .nnTopology(MiniDFSNNTopology.simpleHATopology()) .numDataNodes(5) .build();	try {	cluster.waitActive();	cluster.transitionToActive(0);	Thread.sleep(500);	
starting with nn active 

FSDataOutputStream stm = null;	MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf) .nnTopology(MiniDFSNNTopology.simpleHATopology()) .numDataNodes(5) .build();	try {	cluster.waitActive();	cluster.transitionToActive(0);	Thread.sleep(500);	FileSystem fs = HATestUtil.configureFailoverFs(cluster, conf);	stm = fs.create(TEST_PATH);	AppendTestUtil.write(stm, 0, BLOCK_AND_A_HALF);	stm.hflush();	
failing over to nn 

Thread.sleep(500);	FileSystem fs = HATestUtil.configureFailoverFs(cluster, conf);	stm = fs.create(TEST_PATH);	AppendTestUtil.write(stm, 0, BLOCK_AND_A_HALF);	stm.hflush();	scenario.run(cluster);	assertTrue(fs.exists(TEST_PATH));	cluster.stopDataNode(0);	AppendTestUtil.write(stm, BLOCK_AND_A_HALF, BLOCK_AND_A_HALF);	stm.hflush();	
failing back to nn 

public void testLeaseRecoveryAfterFailover() throws Exception {	final Configuration conf = new Configuration();	conf.setBoolean(DFSConfigKeys.DFS_PERMISSIONS_ENABLED_KEY, false);	conf.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLOCK_SIZE);	FSDataOutputStream stm = null;	final MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf) .nnTopology(MiniDFSNNTopology.simpleHATopology()) .numDataNodes(3) .build();	try {	cluster.waitActive();	cluster.transitionToActive(0);	Thread.sleep(500);	
starting with nn active 

FSDataOutputStream stm = null;	final MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf) .nnTopology(MiniDFSNNTopology.simpleHATopology()) .numDataNodes(3) .build();	try {	cluster.waitActive();	cluster.transitionToActive(0);	Thread.sleep(500);	FileSystem fs = HATestUtil.configureFailoverFs(cluster, conf);	stm = fs.create(TEST_PATH);	AppendTestUtil.write(stm, 0, BLOCK_AND_A_HALF);	stm.hflush();	
failing over to nn 

public void testFailoverRightBeforeCommitSynchronization() throws Exception {	final Configuration conf = new Configuration();	conf.setBoolean(DFSConfigKeys.DFS_PERMISSIONS_ENABLED_KEY, false);	conf.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLOCK_SIZE);	FSDataOutputStream stm = null;	final MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf) .nnTopology(MiniDFSNNTopology.simpleHATopology()) .numDataNodes(3) .build();	try {	cluster.waitActive();	cluster.transitionToActive(0);	Thread.sleep(500);	
starting with nn active 

cluster.waitActive();	cluster.transitionToActive(0);	Thread.sleep(500);	FileSystem fs = HATestUtil.configureFailoverFs(cluster, conf);	stm = fs.create(TEST_PATH);	AppendTestUtil.write(stm, 0, BLOCK_SIZE / 2);	stm.hflush();	NameNode nn0 = cluster.getNameNode(0);	ExtendedBlock blk = DFSTestUtil.getFirstBlock(fs, TEST_PATH);	DatanodeDescriptor expectedPrimary = DFSTestUtil.getExpectedPrimaryNode(nn0, blk);	
expecting block recovery to be triggered on dn 

stm.hflush();	NameNode nn0 = cluster.getNameNode(0);	ExtendedBlock blk = DFSTestUtil.getFirstBlock(fs, TEST_PATH);	DatanodeDescriptor expectedPrimary = DFSTestUtil.getExpectedPrimaryNode(nn0, blk);	DataNode primaryDN = cluster.getDataNode(expectedPrimary.getIpcPort());	DatanodeProtocolClientSideTranslatorPB nnSpy = InternalDataNodeTestUtils.spyOnBposToNN(primaryDN, nn0);	DelayAnswer delayer = new DelayAnswer(LOG);	Mockito.doAnswer(delayer).when(nnSpy).commitBlockSynchronization( Mockito.eq(blk), Mockito.anyInt(), Mockito.anyLong(), Mockito.eq(true), Mockito.eq(false), (DatanodeID[]) Mockito.anyObject(), (String[]) Mockito.anyObject());	DistributedFileSystem fsOtherUser = createFsAsOtherUser(cluster, conf);	assertFalse(fsOtherUser.recoverLease(TEST_PATH));	
waiting for commitblocksynchronization call from primary 

NameNode nn0 = cluster.getNameNode(0);	ExtendedBlock blk = DFSTestUtil.getFirstBlock(fs, TEST_PATH);	DatanodeDescriptor expectedPrimary = DFSTestUtil.getExpectedPrimaryNode(nn0, blk);	DataNode primaryDN = cluster.getDataNode(expectedPrimary.getIpcPort());	DatanodeProtocolClientSideTranslatorPB nnSpy = InternalDataNodeTestUtils.spyOnBposToNN(primaryDN, nn0);	DelayAnswer delayer = new DelayAnswer(LOG);	Mockito.doAnswer(delayer).when(nnSpy).commitBlockSynchronization( Mockito.eq(blk), Mockito.anyInt(), Mockito.anyLong(), Mockito.eq(true), Mockito.eq(false), (DatanodeID[]) Mockito.anyObject(), (String[]) Mockito.anyObject());	DistributedFileSystem fsOtherUser = createFsAsOtherUser(cluster, conf);	assertFalse(fsOtherUser.recoverLease(TEST_PATH));	delayer.waitForCall();	
failing over to nn 

public void testPipelineRecoveryStress() throws Exception {	
hdfs debug data begin 

public void testPipelineRecoveryStress() throws Exception {	String[][] scmds = new String[][] {	{"/bin/sh", "-c", "ulimit -a"}, {"hostname"}, {"ifconfig", "-a"}	};	for (String[] scmd: scmds) {	String scmd_str = StringUtils.join(" ", scmd);	try {	ShellCommandExecutor sce = new ShellCommandExecutor(scmd);	sce.execute();	
output 

public void testPipelineRecoveryStress() throws Exception {	String[][] scmds = new String[][] {	{"/bin/sh", "-c", "ulimit -a"}, {"hostname"}, {"ifconfig", "-a"}	};	for (String[] scmd: scmds) {	String scmd_str = StringUtils.join(" ", scmd);	try {	ShellCommandExecutor sce = new ShellCommandExecutor(scmd);	sce.execute();	} catch (IOException e) {	
error when running 

{"/bin/sh", "-c", "ulimit -a"}, {"hostname"}, {"ifconfig", "-a"}	};	for (String[] scmd: scmds) {	String scmd_str = StringUtils.join(" ", scmd);	try {	ShellCommandExecutor sce = new ShellCommandExecutor(scmd);	sce.execute();	} catch (IOException e) {	}	}	
hdfs debug data end 

try {	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	boolean success;	try {	success = ((DistributedFileSystem)fsOtherUser) .recoverLease(testPath);	} catch (IOException e) {	throw new RuntimeException(e);	}	if (!success) {	
waiting to recover lease successfully 

========================= hadoop sample_7472 =========================

public Token<RMDelegationTokenIdentifier> selectToken(Text service, Collection<Token<? extends TokenIdentifier>> tokens) {	if (service == null) {	return null;	}	
looking for a token with service 

public Token<RMDelegationTokenIdentifier> selectToken(Text service, Collection<Token<? extends TokenIdentifier>> tokens) {	if (service == null) {	return null;	}	for (Token<? extends TokenIdentifier> token : tokens) {	
token kind is and the token s service name is 

========================= hadoop sample_2220 =========================

}	} catch (IOException ioe) {	checkInterrupted(ioe);	if (ioe instanceof RemoteException) {	if (((RemoteException) ioe).unwrapRemoteException() instanceof ReplicaNotFoundException) {	replicaNotFoundCount--;	} else if (((RemoteException) ioe).unwrapRemoteException() instanceof RetriableException) {	retryList.add(datanode);	}	}	
failed to getreplicavisiblelength from datanode for block 

this.currentLocatedBlock = targetBlock;	long offsetIntoBlock = target - targetBlock.getStartOffset();	DNAddrPair retval = chooseDataNode(targetBlock, null);	chosenNode = retval.info;	InetSocketAddress targetAddr = retval.addr;	StorageType storageType = retval.storageType;	targetBlock = retval.block;	try {	blockReader = getBlockReader(targetBlock, offsetIntoBlock, targetBlock.getBlockSize() - offsetIntoBlock, targetAddr, storageType, chosenNode);	if(connectFailedOnce) {	
successfully connected to for 

StorageType storageType = retval.storageType;	targetBlock = retval.block;	try {	blockReader = getBlockReader(targetBlock, offsetIntoBlock, targetBlock.getBlockSize() - offsetIntoBlock, targetAddr, storageType, chosenNode);	if(connectFailedOnce) {	}	return chosenNode;	} catch (IOException ex) {	checkInterrupted(ex);	if (ex instanceof InvalidEncryptionKeyException && refetchEncryptionKey > 0) {	
will fetch a new encryption key and retry encryption key was invalid when connecting to 

} catch (IOException ex) {	checkInterrupted(ex);	if (ex instanceof InvalidEncryptionKeyException && refetchEncryptionKey > 0) {	refetchEncryptionKey--;	dfsClient.clearDataEncryptionKey();	} else if (refetchToken > 0 && tokenRefetchNeeded(ex, targetAddr)) {	refetchToken--;	fetchBlockAt(target);	} else {	connectFailedOnce = true;	
failed to connect to for block add to deadnodes and continue 

private void checkInterrupted(IOException e) throws IOException {	if (Thread.currentThread().isInterrupted() && (e instanceof ClosedByInterruptException || e instanceof InterruptedIOException)) {	
the reading thread has been interrupted 

public synchronized void close() throws IOException {	if (!closed.compareAndSet(false, true)) {	
dfsinputstream has been closed already 

dfsClient.checkOpen();	if ((extendedReadBuffers != null) && (!extendedReadBuffers.isEmpty())) {	final StringBuilder builder = new StringBuilder();	extendedReadBuffers.visitAll(new IdentityHashStore.Visitor<ByteBuffer, Object>() {	private String prefix = "";	public void accept(ByteBuffer k, Object v) {	builder.append(prefix).append(k);	prefix = ", ";	}	});	
closing file but there are still unreleased bytebuffers allocated by read please release 

public int doRead(BlockReader blockReader, int off, int len) throws IOException {	int oldpos = buf.position();	int oldlimit = buf.limit();	boolean success = false;	try {	int ret = blockReader.read(buf);	success = true;	updateReadStatistics(readStatistics, ret, blockReader);	dfsClient.updateFileSystemReadStats(blockReader.getNetworkDistance(), ret);	if (ret == 0) {	
zero 

private synchronized int readBuffer(ReaderStrategy reader, int off, int len, Map<ExtendedBlock, Set<DatanodeInfo>> corruptedBlockMap) throws IOException {	IOException ioe;	boolean retryCurrentNode = true;	while (true) {	try {	return reader.doRead(blockReader, off, len);	} catch ( ChecksumException ce ) {	
found checksum error for from at 

boolean retryCurrentNode = true;	while (true) {	try {	return reader.doRead(blockReader, off, len);	} catch ( ChecksumException ce ) {	ioe = ce;	retryCurrentNode = false;	addIntoCorruptedBlockMap(getCurrentBlock(), currentNode, corruptedBlockMap);	} catch ( IOException e ) {	if (!retryCurrentNode) {	
exception while reading from of from 

pos += result;	} else {	throw new IOException("Unexpected EOS from the reader");	}	return result;	} catch (ChecksumException ce) {	throw ce;	} catch (IOException e) {	checkInterrupted(e);	if (retries == 1) {	
dfs read 

private LocatedBlock refetchLocations(LocatedBlock block, Collection<DatanodeInfo> ignoredNodes) throws IOException {	String errMsg = getBestNodeDNAddrPairErrorString(block.getLocations(), deadNodes, ignoredNodes);	String blockInfo = block.getBlock() + " file=" + src;	if (failures >= dfsClient.getConf().getMaxBlockAcquireFailures()) {	String description = "Could not obtain block: " + blockInfo;	
throwing a blockmissingexception 

private LocatedBlock refetchLocations(LocatedBlock block, Collection<DatanodeInfo> ignoredNodes) throws IOException {	String errMsg = getBestNodeDNAddrPairErrorString(block.getLocations(), deadNodes, ignoredNodes);	String blockInfo = block.getBlock() + " file=" + src;	if (failures >= dfsClient.getConf().getMaxBlockAcquireFailures()) {	String description = "Could not obtain block: " + blockInfo;	throw new BlockMissingException(src, description, block.getStartOffset());	}	DatanodeInfo[] nodes = block.getLocations();	if (nodes == null || nodes.length == 0) {	
no node available for 

private LocatedBlock refetchLocations(LocatedBlock block, Collection<DatanodeInfo> ignoredNodes) throws IOException {	String errMsg = getBestNodeDNAddrPairErrorString(block.getLocations(), deadNodes, ignoredNodes);	String blockInfo = block.getBlock() + " file=" + src;	if (failures >= dfsClient.getConf().getMaxBlockAcquireFailures()) {	String description = "Could not obtain block: " + blockInfo;	throw new BlockMissingException(src, description, block.getStartOffset());	}	DatanodeInfo[] nodes = block.getLocations();	if (nodes == null || nodes.length == 0) {	}	
could not obtain from any node will get new block locations from namenode and retry 

if (failures >= dfsClient.getConf().getMaxBlockAcquireFailures()) {	String description = "Could not obtain block: " + blockInfo;	throw new BlockMissingException(src, description, block.getStartOffset());	}	DatanodeInfo[] nodes = block.getLocations();	if (nodes == null || nodes.length == 0) {	}	try {	final int timeWindow = dfsClient.getConf().getTimeWindow();	double waitTime = timeWindow * failures + timeWindow * (failures + 1) * ThreadLocalRandom.current().nextDouble();	
dfs choosedatanode got ioexception will wait for msec 

}	break;	}	}	}	if (chosenNode == null) {	DFSClient.LOG.warn("No live nodes contain block " + block.getBlock() + " after checking nodes = " + Arrays.toString(nodes) + ", ignoredNodes = " + ignoredNodes);	return null;	}	final String dnAddr = chosenNode.getXferAddr(dfsClient.getConf().isConnectToDnViaHostname());	
connecting to datanode 

Future<ByteBuffer> firstRequest = hedgedService .submit(getFromDataNodeCallable);	futures.add(firstRequest);	Future<ByteBuffer> future = null;	try {	future = hedgedService.poll( conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);	if (future != null) {	ByteBuffer result = future.get();	System.arraycopy(result.array(), result.position(), buf, offset, len);	return;	}	
waited ms to read from spawning hedged read 

if (chosenNode != null) {	block = chosenNode.block;	bb = ByteBuffer.allocate(len);	Callable<ByteBuffer> getFromDataNodeCallable = getFromOneDataNode( chosenNode, block, start, end, bb, corruptedBlockMap, hedgedReadId++);	Future<ByteBuffer> oneMoreRequest = hedgedService .submit(getFromDataNodeCallable);	futures.add(oneMoreRequest);	} else {	refetch = true;	}	} catch (IOException ioe) {	
failed getting node for hedged read 

protected static boolean tokenRefetchNeeded(IOException ex, InetSocketAddress targetAddr) {	if (ex instanceof InvalidBlockTokenException || ex instanceof InvalidToken) {	
access token was invalid when connecting to 

try {	pos += blockReader.skip(diff);	if (pos == targetPos) {	done = true;	} else {	String errMsg = "BlockReader failed to seek to " + targetPos + ". Instead, it seeked to " + pos + ".";	DFSClient.LOG.warn(errMsg);	throw new IOException(errMsg);	}	} catch (IOException e) {	
exception while seek to from of from 

protected void closeCurrentBlockReaders() {	if (blockReader == null) return;	try {	blockReader.close();	} catch (IOException e) {	
error closing blockreader 

long length31 = Integer.MAX_VALUE - blockPos;	if (length31 <= 0) {	DFSClient.LOG.debug("Unable to perform a zero-copy read from offset {} " + " of {}; 31-bit MappedByteBuffer limit exceeded.  blockPos={}, " + "curEnd={}", curPos, src, blockPos, curEnd);	return null;	}	length = (int)length31;	DFSClient.LOG.debug("Reducing read length from {} to {} to avoid 31-bit " + "limit.  blockPos={}; curPos={}; curEnd={}", maxLength, length, blockPos, curPos, curEnd);	}	final ClientMmap clientMmap = blockReader.getClientMmap(opts);	if (clientMmap == null) {	
unable to perform a zero copy read from offset of blockreader getclientmmap returned null 

========================= hadoop sample_7022 =========================

private static RandomDatum[] generate(int count) {	if(LOG.isDebugEnabled()) {	
generating records in debug 

private static void writeTest(FileSystem fs, RandomDatum[] data, String file) throws IOException {	Configuration conf = new Configuration();	MapFile.delete(fs, file);	if(LOG.isDebugEnabled()) {	
creating with debug 

private static void readTest(FileSystem fs, RandomDatum[] data, String file, Configuration conf) throws IOException {	RandomDatum v = new RandomDatum();	if(LOG.isDebugEnabled()) {	
reading debug 

throw new RuntimeException("wrong value at " + i);	}	}	for (int i = data.length-1; i >= 0; i--) {	reader.get(i, v);	if (!v.equals(data[i])) {	throw new RuntimeException("wrong value at " + i);	}	}	if(LOG.isDebugEnabled()) {	
done reading debug 

========================= hadoop sample_3061 =========================

protected void emitMetric(String groupName, String name, String type, String value, GangliaConf gConf, GangliaSlope gSlope) throws IOException {	if (name == null) {	
metric was emitted with no name 

protected void emitMetric(String groupName, String name, String type, String value, GangliaConf gConf, GangliaSlope gSlope) throws IOException {	if (name == null) {	return;	} else if (value == null) {	
metric name was emitted with a null value 

protected void emitMetric(String groupName, String name, String type, String value, GangliaConf gConf, GangliaSlope gSlope) throws IOException {	if (name == null) {	return;	} else if (value == null) {	return;	} else if (type == null) {	
metric name value has no type 

protected void emitMetric(String groupName, String name, String type, String value, GangliaConf gConf, GangliaSlope gSlope) throws IOException {	if (name == null) {	return;	} else if (value == null) {	return;	} else if (type == null) {	return;	}	if (LOG.isDebugEnabled()) {	
emitting metric type value slope from hostname 

========================= hadoop sample_3431 =========================

public void startDecommission(DatanodeDescriptor node) {	if (!node.isDecommissionInProgress() && !node.isDecommissioned()) {	hbManager.startDecommission(node);	if (node.isDecommissionInProgress()) {	for (DatanodeStorageInfo storage : node.getStorageInfos()) {	
starting decommission of with blocks 

public void startDecommission(DatanodeDescriptor node) {	if (!node.isDecommissionInProgress() && !node.isDecommissioned()) {	hbManager.startDecommission(node);	if (node.isDecommissionInProgress()) {	for (DatanodeStorageInfo storage : node.getStorageInfos()) {	}	node.getLeavingServiceStatus().setStartTime(monotonicNow());	pendingNodes.add(node);	}	} else {	
startdecommission node in nothing to do 

public void stopDecommission(DatanodeDescriptor node) {	if (node.isDecommissionInProgress() || node.isDecommissioned()) {	hbManager.stopDecommission(node);	if (node.isAlive()) {	blockManager.processExtraRedundancyBlocksOnInService(node);	}	pendingNodes.remove(node);	outOfServiceNodeBlocks.remove(node);	} else {	
stopdecommission node in nothing to do 

public void startMaintenance(DatanodeDescriptor node, long maintenanceExpireTimeInMS) {	node.setMaintenanceExpireTimeInMS(maintenanceExpireTimeInMS);	if (!node.isMaintenance()) {	hbManager.startMaintenance(node);	if (node.isEnteringMaintenance()) {	for (DatanodeStorageInfo storage : node.getStorageInfos()) {	
starting maintenance of with blocks 

node.setMaintenanceExpireTimeInMS(maintenanceExpireTimeInMS);	if (!node.isMaintenance()) {	hbManager.startMaintenance(node);	if (node.isEnteringMaintenance()) {	for (DatanodeStorageInfo storage : node.getStorageInfos()) {	}	node.getLeavingServiceStatus().setStartTime(monotonicNow());	}	pendingNodes.add(node);	} else {	
startmaintenance node in nothing to do 

if (node.isMaintenance()) {	hbManager.stopMaintenance(node);	if (!node.isAlive()) {	blockManager.removeBlocksAssociatedTo(node);	} else {	blockManager.processExtraRedundancyBlocksOnInService(node);	}	pendingNodes.remove(node);	outOfServiceNodeBlocks.remove(node);	} else {	
stopmaintenance node in nothing to do 

private void setDecommissioned(DatanodeDescriptor dn) {	dn.setDecommissioned();	
decommissioning complete for node 

private void setInMaintenance(DatanodeDescriptor dn) {	dn.setInMaintenance();	
node has entered maintenance mode 

private boolean isSufficientlyReplicated(BlockInfo block, BlockCollection bc, NumberReplicas numberReplicas, boolean isDecommission) {	if (blockManager.hasEnoughEffectiveReplicas(block, numberReplicas, 0)) {	
block does not need replication 

}	final int numExpected = blockManager.getExpectedLiveRedundancyNum(block, numberReplicas);	final int numLive = numberReplicas.liveReplicas();	LOG.trace("Block {} numExpected={}, numLive={}", block, numExpected, numLive);	if (isDecommission && numExpected > numLive) {	if (bc.isUnderConstruction() && block.equals(bc.getLastBlock())) {	if (numLive >= blockManager.minReplication) {	LOG.trace("UC block {} sufficiently-replicated since numLive ({}) " + ">= minR ({})", block, numLive, blockManager.minReplication);	return true;	} else {	
uc block insufficiently replicated since numlive minr 

private boolean exceededNumBlocksPerCheck() {	
processed blocks so far this tick 

private boolean exceededNumNodesPerCheck() {	
processed nodes so far this tick 

public void run() {	if (!namesystem.isRunning()) {	
namesystem is not running skipping decommissioning maintenance checks 

numBlocksCheckedPerLock = 0;	numNodesChecked = 0;	namesystem.writeLock();	try {	processPendingNodes();	check();	} finally {	namesystem.writeUnlock();	}	if (numBlocksChecked + numNodesChecked > 0) {	
checked blocks and nodes this tick 

boolean fullScan = false;	if (dn.isMaintenance() && dn.maintenanceExpired()) {	stopMaintenance(dn);	toRemove.add(dn);	continue;	}	if (dn.isInMaintenance()) {	continue;	}	if (blocks == null) {	
newly added node doing full scan to find insufficiently replicated blocks 

continue;	}	if (dn.isInMaintenance()) {	continue;	}	if (blocks == null) {	blocks = handleInsufficientlyReplicated(dn);	outOfServiceNodeBlocks.put(dn, blocks);	fullScan = true;	} else {	
processing node 

}	if (blocks == null) {	blocks = handleInsufficientlyReplicated(dn);	outOfServiceNodeBlocks.put(dn, blocks);	fullScan = true;	} else {	pruneSufficientlyReplicated(dn, blocks);	}	if (blocks.size() == 0) {	if (!fullScan) {	
node has finished replicating current set of blocks checking with the full block map 

final boolean isHealthy = blockManager.isNodeHealthyForDecommissionOrMaintenance(dn);	if (blocks.size() == 0 && isHealthy) {	if (dn.isDecommissionInProgress()) {	setDecommissioned(dn);	toRemove.add(dn);	} else if (dn.isEnteringMaintenance()) {	setInMaintenance(dn);	} else {	Preconditions.checkState(false, "A node is in an invalid state!");	}	
node is sufficiently replicated and healthy marked as 

if (blocks.size() == 0 && isHealthy) {	if (dn.isDecommissionInProgress()) {	setDecommissioned(dn);	toRemove.add(dn);	} else if (dn.isEnteringMaintenance()) {	setInMaintenance(dn);	} else {	Preconditions.checkState(false, "A node is in an invalid state!");	}	} else {	
node healthy it needs to replicate more blocks is still in progress is isn t 

setDecommissioned(dn);	toRemove.add(dn);	} else if (dn.isEnteringMaintenance()) {	setInMaintenance(dn);	} else {	Preconditions.checkState(false, "A node is in an invalid state!");	}	} else {	}	} else {	
node still has blocks to replicate before it is a candidate to finish 

private void processBlocksInternal( final DatanodeDescriptor datanode, final Iterator<BlockInfo> it, final List<BlockInfo> insufficientlyReplicated, boolean pruneSufficientlyReplicated) {	boolean firstReplicationLog = true;	int underReplicatedBlocks = 0;	int outOfServiceOnlyReplicas = 0;	int underReplicatedInOpenFiles = 0;	while (it.hasNext()) {	if (insufficientlyReplicated == null && numBlocksCheckedPerLock >= numBlocksPerCheck) {	namesystem.writeUnlock();	try {	
yielded lock during decommission maintenance check 

} catch (InterruptedException ignored) {	return;	}	numBlocksCheckedPerLock = 0;	namesystem.writeLock();	}	numBlocksChecked++;	numBlocksCheckedPerLock++;	final BlockInfo block = it.next();	if (blockManager.blocksMap.getStoredBlock(block) == null) {	
removing unknown block 

========================= hadoop sample_8338 =========================

sm.startThreads();	final Server server = new RPC.Builder(conf) .setProtocol(ClientProtocol.class).setInstance(mockNN) .setBindAddress(ADDRESS).setPort(0).setNumHandlers(5).setVerbose(true) .setSecretManager(sm).build();	server.start();	final UserGroupInformation current = UserGroupInformation.getCurrentUser();	final InetSocketAddress addr = NetUtils.getConnectAddress(server);	String user = current.getUserName();	Text owner = new Text(user);	DelegationTokenIdentifier dtId = new DelegationTokenIdentifier(owner, owner, null);	Token<DelegationTokenIdentifier> token = new Token<DelegationTokenIdentifier>( dtId, sm);	SecurityUtil.setTokenService(token, addr);	
service for token is 

========================= hadoop sample_7167 =========================

this.superGroup = this.conf.get( DFSConfigKeys.DFS_PERMISSIONS_SUPERUSERGROUP_KEY, DFSConfigKeys.DFS_PERMISSIONS_SUPERUSERGROUP_DEFAULT);	int handlerCount = this.conf.getInt(DFS_ROUTER_HANDLER_COUNT_KEY, DFS_ROUTER_HANDLER_COUNT_DEFAULT);	int readerCount = this.conf.getInt(DFS_ROUTER_READER_COUNT_KEY, DFS_ROUTER_READER_COUNT_DEFAULT);	int handlerQueueSize = this.conf.getInt(DFS_ROUTER_HANDLER_QUEUE_SIZE_KEY, DFS_ROUTER_HANDLER_QUEUE_SIZE_DEFAULT);	int readerQueueSize = this.conf.getInt(DFS_ROUTER_READER_QUEUE_SIZE_KEY, DFS_ROUTER_READER_QUEUE_SIZE_DEFAULT);	this.conf.setInt( CommonConfigurationKeys.IPC_SERVER_RPC_READ_CONNECTION_QUEUE_SIZE_KEY, readerQueueSize);	RPC.setProtocolEngine(this.conf, ClientNamenodeProtocolPB.class, ProtobufRpcEngine.class);	ClientNamenodeProtocolServerSideTranslatorPB clientProtocolServerTranslator = new ClientNamenodeProtocolServerSideTranslatorPB(this);	BlockingService clientNNPbService = ClientNamenodeProtocol .newReflectiveBlockingService(clientProtocolServerTranslator);	InetSocketAddress confRpcAddress = conf.getSocketAddr( DFSConfigKeys.DFS_ROUTER_RPC_BIND_HOST_KEY, DFSConfigKeys.DFS_ROUTER_RPC_ADDRESS_KEY, DFSConfigKeys.DFS_ROUTER_RPC_ADDRESS_DEFAULT, DFSConfigKeys.DFS_ROUTER_RPC_PORT_DEFAULT);	
rpc server binding to with handlers for router 

protected void serviceInit(Configuration configuration) throws Exception {	this.conf = configuration;	if (this.rpcMonitor == null) {	
cannot instantiate router rpc metrics class 

protected void serviceStart() throws Exception {	if (this.rpcServer != null) {	this.rpcServer.start();	
router rpc up at 

private void checkOperation(OperationCategory op) throws StandbyException {	if (rpcMonitor != null) {	rpcMonitor.startOp();	}	if (LOG.isDebugEnabled()) {	String methodName = getMethodName();	
proxying operation 

public boolean mkdirs(String src, FsPermission masked, boolean createParent) throws IOException {	checkOperation(OperationCategory.WRITE);	final List<RemoteLocation> locations = getLocationsForPath(src, true);	if (locations.size() > 1) {	try {	HdfsFileStatus fileStatus = getFileInfo(src);	if (fileStatus != null) {	return true;	}	} catch (IOException ioe) {	
error requesting file info for path while proxing mkdirs 

Map<String, HdfsFileStatus> nnListing = new TreeMap<>();	int totalRemainingEntries = 0;	int remainingEntries = 0;	boolean namenodeListingExists = false;	if (listings != null) {	String lastName = null;	for (Entry<RemoteLocation, Object> entry : listings.entrySet()) {	RemoteLocation location = entry.getKey();	DirectoryListing listing = (DirectoryListing) entry.getValue();	if (listing == null) {	
cannot get listing from 

Map<FederationNamespaceInfo, Object> results = rpcClient.invokeConcurrent(nss, method, true, false, timeOutMs);	for (Entry<FederationNamespaceInfo, Object> entry : results.entrySet()) {	FederationNamespaceInfo ns = entry.getKey();	DatanodeInfo[] result = (DatanodeInfo[]) entry.getValue();	for (DatanodeInfo node : result) {	String nodeId = node.getXferAddr();	if (!datanodesMap.containsKey(nodeId)) {	node.setNetworkLocation( NodeBase.PATH_SEPARATOR_STR + ns.getNameserviceId() + node.getNetworkLocation());	datanodesMap.put(nodeId, node);	} else {	
is in multiple subclusters 

final List<String> children = subclusterResolver.getMountPoints(path);	if (children != null) {	for (String child : children) {	Path childPath = new Path(path, child);	try {	ContentSummary mountSummary = getContentSummary(childPath.toString());	if (mountSummary != null) {	summaries.add(mountSummary);	}	} catch (Exception e) {	
cannot get content summary for mount 

long modTime = date;	long accessTime = date;	FsPermission permission = FsPermission.getDirDefault();	String owner = this.superUser;	String group = this.superGroup;	try {	UserGroupInformation ugi = getRemoteUser();	owner = ugi.getUserName();	group = ugi.getPrimaryGroupName();	} catch (IOException e) {	
cannot get the remote user 

========================= hadoop sample_8292 =========================

try {	dtCancelThread.join(1000);	} catch (InterruptedException e) {	e.printStackTrace();	}	if (tokenKeepAliveEnabled && delayedRemovalThread != null) {	delayedRemovalThread.interrupt();	try {	delayedRemovalThread.join(1000);	} catch (InterruptedException e) {	
interrupted while joining on delayed removal thread 

public void cancelToken(Token<?> token, Configuration conf) {	TokenWithConf tokenWithConf = new TokenWithConf(token, conf);	while (!queue.offer(tokenWithConf)) {	
unable to add token for cancellation will retry 

public void run() {	TokenWithConf tokenWithConf = null;	while (true) {	try {	tokenWithConf = queue.take();	final TokenWithConf current = tokenWithConf;	if (LOG.isDebugEnabled()) {	
cancelling token 

final TokenWithConf current = tokenWithConf;	if (LOG.isDebugEnabled()) {	}	UserGroupInformation.getLoginUser() .doAs(new PrivilegedExceptionAction<Void>(){	public Void run() throws Exception {	current.token.cancel(current.conf);	return null;	}	});	} catch (IOException e) {	
failed to cancel token 

if (LOG.isDebugEnabled()) {	}	UserGroupInformation.getLoginUser() .doAs(new PrivilegedExceptionAction<Void>(){	public Void run() throws Exception {	current.token.cancel(current.conf);	return null;	}	});	} catch (IOException e) {	} catch (RuntimeException e) {	
failed to cancel token 

public Void run() throws Exception {	current.token.cancel(current.conf);	return null;	}	});	} catch (IOException e) {	} catch (RuntimeException e) {	} catch (InterruptedException ie) {	return;	} catch (Throwable t) {	
got exception exiting 

LOG.debug("Registering tokens for renewal for:" + " appId = " + applicationId);	}	Collection<Token<?>> tokens = ts.getAllTokens();	long now = System.currentTimeMillis();	appTokens.put(applicationId, Collections.synchronizedSet(new HashSet<DelegationTokenToRenew>()));	Set<DelegationTokenToRenew> tokenList = new HashSet<DelegationTokenToRenew>();	boolean hasHdfsToken = false;	for (Token<?> token : tokens) {	if (token.isManaged()) {	if (token.getKind().equals(HDFS_DELEGATION_KIND)) {	
found existing hdfs token 

Token<?> token = dttr.token;	try {	requestNewHdfsDelegationTokenIfNeeded(dttr);	if (!dttr.isTimerCancelled()) {	renewToken(dttr);	setTimerForTokenRenewal(dttr);	} else {	LOG.info("The token was removed already. Token = [" +dttr +"]");	}	} catch (Exception e) {	
exception renewing token not rescheduled 

continue;	}	Iterator<DelegationTokenToRenew> iter = tokenSet.iterator();	synchronized (tokenSet) {	while (iter.hasNext()) {	DelegationTokenToRenew t = iter.next();	if (t.token.getKind().equals(HDFS_DELEGATION_KIND)) {	iter.remove();	allTokens.remove(t.token);	t.cancelTimer();	
removed expiring token 

private void requestNewHdfsDelegationTokenAsProxyUser( Collection<ApplicationId> referringAppIds, String user, boolean shouldCancelAtEnd) throws IOException, InterruptedException {	if (!hasProxyUserPrivileges) {	
rm proxy user privilege is not enabled skip requesting hdfs tokens 

private void requestNewHdfsDelegationTokenAsProxyUser( Collection<ApplicationId> referringAppIds, String user, boolean shouldCancelAtEnd) throws IOException, InterruptedException {	if (!hasProxyUserPrivileges) {	return;	}	Credentials credentials = new Credentials();	Token<?>[] newTokens = obtainSystemTokensForUser(user, credentials);	
received new tokens for received tokens 

Token<?>[] newTokens = obtainSystemTokensForUser(user, credentials);	if (newTokens.length > 0) {	for (Token<?> token : newTokens) {	if (token.isManaged()) {	DelegationTokenToRenew tokenToRenew = new DelegationTokenToRenew(referringAppIds, token, getConfig(), Time.now(), shouldCancelAtEnd, user);	renewToken(tokenToRenew);	setTimerForTokenRenewal(tokenToRenew);	for (ApplicationId applicationId : referringAppIds) {	appTokens.get(applicationId).add(tokenToRenew);	}	
received new token 

private void cancelToken(DelegationTokenToRenew t) {	if(t.shouldCancelAtEnd) {	dtCancelThread.cancelToken(t.token, t.conf);	} else {	
did not cancel 

}	}	for (ApplicationId appId : toCancel) {	removeApplicationFromRenewal(appId);	delayedRemovalMap.remove(appId);	}	synchronized (this) {	try {	wait(waitTimeMs);	} catch (InterruptedException e) {	
delayed deletion thread interrupted shutting it down 

private void handleDTRenewerAppSubmitEvent( DelegationTokenRenewerAppSubmitEvent event) {	try {	DelegationTokenRenewer.this.handleAppSubmitEvent(event);	rmContext.getDispatcher().getEventHandler() .handle(new RMAppEvent(event.getApplicationId(), RMAppEventType.START));	} catch (Throwable t) {	
unable to add the application to the delegation token renewer 

private void handleDTRenewerAppRecoverEvent( DelegationTokenRenewerAppRecoverEvent event) {	try {	DelegationTokenRenewer.this.handleAppSubmitEvent(event);	} catch (Throwable t) {	
unable to add the application to the delegation token renewer on recovery 

========================= hadoop sample_707 =========================

public static void setup() throws InterruptedException, IOException {	if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {	
mrappjar not found not running test 

public static void tearDown() {	if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {	
mrappjar not found not running test 

public void testDefaultProfiler() throws Exception {	
starting testdefaultprofiler 

public void testDifferentProfilers() throws Exception {	
starting testdefaultprofiler 

private void testProfilerInternal(boolean useDefault) throws Exception {	if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {	
mrappjar not found not running test 

final JobId jobId = TypeConverter.toYarn(job.getJobID());	final ApplicationId appID = jobId.getAppId();	int pollElapsed = 0;	while (true) {	Thread.sleep(1000);	pollElapsed += 1000;	if (TERMINAL_RM_APP_STATES.contains( mrCluster.getResourceManager().getRMContext().getRMApps().get(appID) .getState())) {	break;	}	if (pollElapsed >= 60000) {	
application did not reach terminal state within seconds 

========================= hadoop sample_5623 =========================

while (bytesLeft > 0) {	int n = input.readWithChecksum(buf, 0, (int) Math.min(bytesLeft, BYTES_TO_READ));	if (n < 0) {	throw new IOException("read past end of stream reading " + getMapId());	}	disk.write(buf, 0, n);	bytesLeft -= n;	metrics.inputBytes(n);	reporter.progress();	}	
read bytes from map output for 

public void abort() {	try {	fs.delete(tmpOutputPath, false);	} catch (IOException ie) {	
failure to clean up 

========================= hadoop sample_4931 =========================

protected T getProxyInternal() {	try {	final InetSocketAddress rmAddress = rmProxy.getRMAddress(conf, protocol);	return rmProxy.getProxy(conf, protocol, rmAddress);	} catch (IOException ioe) {	
unable to create proxy to the resourcemanager 

public synchronized void performFailover(T currentProxy) {	currentProxyIndex = (currentProxyIndex + 1) % rmServiceIds.length;	conf.set(YarnConfiguration.RM_HA_ID, rmServiceIds[currentProxyIndex]);	
failing over to 

========================= hadoop sample_2559 =========================

public DirectoryCollection(String[] dirs, float utilizationPercentageCutOffHigh, float utilizationPercentageCutOffLow, long utilizationSpaceCutOff) {	conf = new YarnConfiguration();	try {	diskValidator = DiskValidatorFactory.getInstance( conf.get(YarnConfiguration.DISK_VALIDATOR, YarnConfiguration.DEFAULT_DISK_VALIDATOR));	
disk validator is loaded 

========================= hadoop sample_1725 =========================

StaticMapping.addNodeToRack(hosts[invalidIdx], racks[validIdx]);	
datanode came up with network location 

StaticMapping.addNodeToRack(hosts[invalidIdx], racks[validIdx]);	cluster.restartDataNode(invalidIdx);	Thread.sleep(5000);	while (true) {	info = nn.getDatanodeReport(DatanodeReportType.LIVE);	if (info.length == 2) {	break;	}	if (info.length == 0) {	
got no valid dns 

StaticMapping.addNodeToRack(hosts[invalidIdx], racks[validIdx]);	cluster.restartDataNode(invalidIdx);	Thread.sleep(5000);	while (true) {	info = nn.getDatanodeReport(DatanodeReportType.LIVE);	if (info.length == 2) {	break;	}	if (info.length == 0) {	} else if (info.length == 1) {	
got one valid dn at 

========================= hadoop sample_7075 =========================

private Object runTool(String ... args) throws Exception {	errOutBytes.reset();	outBytes.reset();	
running dfshaadmin 

private Object runTool(String ... args) throws Exception {	errOutBytes.reset();	outBytes.reset();	int ret = tool.run(args);	errOutput = new String(errOutBytes.toByteArray(), Charsets.UTF_8);	output = new String(outBytes.toByteArray(), Charsets.UTF_8);	
err output output 

========================= hadoop sample_7130 =========================

assertNotNull(volumeDirectoryToRemove);	datanodeToRemoveStorageFrom.shutdown();	FileUtil.fullyDelete(new File(volumeDirectoryToRemove));	FileOutputStream fos = new FileOutputStream(volumeDirectoryToRemove);	try {	fos.write(1);	} finally {	fos.close();	}	cluster.restartDataNode(datanodeToRemoveStorageFromIdx);	
waiting for the datanode to remove 

fos.close();	}	cluster.restartDataNode(datanodeToRemoveStorageFromIdx);	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	final DatanodeDescriptor dnDescriptor = cluster.getNamesystem().getBlockManager().getDatanodeManager(). getDatanode(datanodeToRemoveStorageFrom.getDatanodeUuid());	assertNotNull(dnDescriptor);	DatanodeStorageInfo[] infos = dnDescriptor.getStorageInfos();	for (DatanodeStorageInfo info : infos) {	if (info.getStorageID().equals(storageIdToRemove)) {	
still found storage on 

volumeRefs.close();	}	final ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, TEST_PATH);	cluster.restartDataNodes();	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	cluster.getNamesystem().writeLock();	try {	Iterator<DatanodeStorageInfo> storageInfoIter = cluster.getNamesystem().getBlockManager(). getStorages(block.getLocalBlock()).iterator();	if (!storageInfoIter.hasNext()) {	
expected to find a storage for but nothing was found continuing to wait 

GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	cluster.getNamesystem().writeLock();	try {	Iterator<DatanodeStorageInfo> storageInfoIter = cluster.getNamesystem().getBlockManager(). getStorages(block.getLocalBlock()).iterator();	if (!storageInfoIter.hasNext()) {	return false;	}	DatanodeStorageInfo info = storageInfoIter.next();	if (!newStorageId.equals(info.getStorageID())) {	
expected to be in storage id but it was in continuing to wait 

cluster.getNamesystem().writeLock();	try {	Iterator<DatanodeStorageInfo> storageInfoIter = cluster.getNamesystem().getBlockManager(). getStorages(block.getLocalBlock()).iterator();	if (!storageInfoIter.hasNext()) {	return false;	}	DatanodeStorageInfo info = storageInfoIter.next();	if (!newStorageId.equals(info.getStorageID())) {	return false;	}	
successfully found in be in storage id 

========================= hadoop sample_7560 =========================

private Map<String, String> load() {	Map<String, String> loadMap = new HashMap<String, String>();	String filename = getConf().get(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY, null);	if (StringUtils.isBlank(filename)) {	
not configured 

}	try (BufferedReader reader = new BufferedReader(new InputStreamReader( new FileInputStream(filename), StandardCharsets.UTF_8))) {	String line = reader.readLine();	while (line != null) {	line = line.trim();	if (line.length() != 0 && line.charAt(0) != '#') {	String[] columns = line.split("\\s+");	if (columns.length == 2) {	loadMap.put(columns[0], columns[1]);	} else {	
line does not have two columns ignoring 

if (line.length() != 0 && line.charAt(0) != '#') {	String[] columns = line.split("\\s+");	if (columns.length == 2) {	loadMap.put(columns[0], columns[1]);	} else {	}	}	line = reader.readLine();	}	} catch (Exception e) {	
cannot be read 

public synchronized List<String> resolve(List<String> names) {	if (map == null) {	map = load();	if (map == null) {	
failed to read topology table will be used for all nodes 

public void reloadCachedMappings() {	Map<String, String> newMap = load();	if (newMap == null) {	
failed to reload the topology table the cached mappings will not be cleared 

========================= hadoop sample_3523 =========================

public void flushAndSync(boolean durable) throws IOException {	if (fp == null) {	throw new IOException("Trying to use aborted output stream");	}	if (doubleBuf.isFlushed()) {	
nothing to flush 

long total = 0;	long fillCapacity = fill.capacity();	while (need > 0) {	fill.position(0);	IOUtils.writeFully(fc, fill, size);	need -= fillCapacity;	size += fillCapacity;	total += fillCapacity;	}	if(LOG.isDebugEnabled()) {	
preallocated bytes at the end of the edit log offset 

========================= hadoop sample_7999 =========================

public void tearDown() throws IOException {	if (dn != null) {	try {	dn.shutdown();	} catch (Exception e) {	
cannot close 

========================= hadoop sample_7219 =========================

public CallQueueManager(Class<? extends BlockingQueue<E>> backingClass, Class<? extends RpcScheduler> schedulerClass, boolean clientBackOffEnabled, int maxQueueSize, String namespace, Configuration conf) {	int priorityLevels = parseNumLevels(namespace, conf);	this.scheduler = createScheduler(schedulerClass, priorityLevels, namespace, conf);	BlockingQueue<E> bq = createCallQueueInstance(backingClass, priorityLevels, maxQueueSize, namespace, conf);	this.clientBackOffEnabled = clientBackOffEnabled;	this.putRef = new AtomicReference<BlockingQueue<E>>(bq);	this.takeRef = new AtomicReference<BlockingQueue<E>>(bq);	
using callqueue queuecapacity scheduler 

private static int parseNumLevels(String ns, Configuration conf) {	int retval = conf.getInt(ns + "." + FairCallQueue.IPC_CALLQUEUE_PRIORITY_LEVELS_KEY, 0);	if (retval == 0) {	retval = conf.getInt(ns + "." + CommonConfigurationKeys.IPC_SCHEDULER_PRIORITY_LEVELS_KEY, CommonConfigurationKeys.IPC_SCHEDULER_PRIORITY_LEVELS_DEFAULT_KEY);	} else {	
is deprecated please use 

public synchronized void swapQueue( Class<? extends RpcScheduler> schedulerClass, Class<? extends BlockingQueue<E>> queueClassToUse, int maxSize, String ns, Configuration conf) {	int priorityLevels = parseNumLevels(ns, conf);	this.scheduler.stop();	RpcScheduler newScheduler = createScheduler(schedulerClass, priorityLevels, ns, conf);	BlockingQueue<E> newQ = createCallQueueInstance(queueClassToUse, priorityLevels, maxSize, ns, conf);	BlockingQueue<E> oldQ = putRef.get();	putRef.set(newQ);	while (!queueIsReallyEmpty(oldQ)) {}	takeRef.set(newQ);	this.scheduler = newScheduler;	
old queue replacement 

========================= hadoop sample_4065 =========================

public void testSquashCGroupOperationsWithInvalidOperations() {	List<PrivilegedOperation> ops = new ArrayList<>();	ops.add(opTasksNone);	ops.add(opDisallowed);	try {	PrivilegedOperationExecutor.squashCGroupOperations(ops);	Assert.fail("Expected squash operation to fail with an exception!");	} catch (PrivilegedOperationException e) {	
caught expected exception 

Assert.fail("Expected squash operation to fail with an exception!");	} catch (PrivilegedOperationException e) {	}	ops.clear();	ops.add(opTasksNone);	ops.add(opTasksInvalid);	try {	PrivilegedOperationExecutor.squashCGroupOperations(ops);	Assert.fail("Expected squash operation to fail with an exception!");	} catch (PrivilegedOperationException e) {	
caught expected exception 

ops.add(opTasks1);	ops.add(opTasksNone);	ops.add(opTasks2);	ops.add(opTasks3);	try {	PrivilegedOperation op = PrivilegedOperationExecutor .squashCGroupOperations(ops);	String expected = new StringBuffer (PrivilegedOperation.CGROUP_ARG_PREFIX) .append(cGroupTasks1).append(PrivilegedOperation .LINUX_FILE_PATH_SEPARATOR) .append(cGroupTasks2).append(PrivilegedOperation .LINUX_FILE_PATH_SEPARATOR) .append(cGroupTasks3).toString();	Assert.assertEquals(1, op.getArguments().size());	Assert.assertEquals(expected, op.getArguments().get(0));	} catch (PrivilegedOperationException e) {	
caught unexpected exception 

========================= hadoop sample_1637 =========================

lease = new SelfRenewingLease(blob, true);	accessCondition.setLeaseID(lease.getLeaseID());	} catch (StorageException ex) {	if (ex.getErrorCode().equals(StorageErrorCodeStrings.BLOB_NOT_FOUND)) {	blobExist = false;	}	else if (ex.getErrorCode().equals( StorageErrorCodeStrings.LEASE_ALREADY_PRESENT)) {	throw new AzureException( "Unable to set Append lease on the Blob: " + ex, ex);	}	else {	
encountered storage exception storageexception errorcode 

public synchronized void close() throws IOException {	
close 

public synchronized void close() throws IOException {	if (closed) {	return;	}	flush();	ioThreadPool.shutdown();	try {	if (!ioThreadPool.awaitTermination(CLOSE_UPLOAD_DELAY, TimeUnit.MINUTES)) {	
time out occured while close is waiting for io request to finish in append for blob 

NativeAzureFileSystemHelper.logAllLiveStackTraces();	throw new AzureException("Timed out waiting for IO requests to finish");	}	} catch(InterruptedException ex) {	Thread.currentThread().interrupt();	}	if (firstError.get() == null && blobExist) {	try {	lease.free();	} catch (StorageException ex) {	
lease free update blob encountered storage exception error code 

private void writeBlockRequestInternal(String blockId, ByteBuffer dataPayload, boolean bufferPoolBuffer) {	IOException lastLocalException = null;	int uploadRetryAttempts = 0;	while (uploadRetryAttempts < MAX_BLOCK_UPLOAD_RETRIES) {	try {	long startTime = System.nanoTime();	blob.uploadBlock(blockId, accessCondition, new ByteArrayInputStream( dataPayload.array()), dataPayload.position(), new BlobRequestOptions(), opContext);	
upload block finished for ms block 

private void writeBlockRequestInternal(String blockId, ByteBuffer dataPayload, boolean bufferPoolBuffer) {	IOException lastLocalException = null;	int uploadRetryAttempts = 0;	while (uploadRetryAttempts < MAX_BLOCK_UPLOAD_RETRIES) {	try {	long startTime = System.nanoTime();	blob.uploadBlock(blockId, accessCondition, new ByteArrayInputStream( dataPayload.array()), dataPayload.position(), new BlobRequestOptions(), opContext);	break;	} catch(Exception ioe) {	
encountered exception during uploading block for blob exception 

private void writeBlockListRequestInternal() {	IOException lastLocalException = null;	int uploadRetryAttempts = 0;	while (uploadRetryAttempts < MAX_BLOCK_UPLOAD_RETRIES) {	try {	long startTime = System.nanoTime();	blob.commitBlockList(blockEntries, accessCondition, new BlobRequestOptions(), opContext);	
upload block list took ms for blob 

private void writeBlockListRequestInternal() {	IOException lastLocalException = null;	int uploadRetryAttempts = 0;	while (uploadRetryAttempts < MAX_BLOCK_UPLOAD_RETRIES) {	try {	long startTime = System.nanoTime();	blob.commitBlockList(blockEntries, accessCondition, new BlobRequestOptions(), opContext);	break;	} catch(Exception ioe) {	
encountered exception during uploading block for blob exception 

public void execute() throws InterruptedException, IOException {	if (committedBlobLength.get() >= getCommandBlobOffset()) {	
commit already applied for 

public void execute() throws InterruptedException, IOException {	if (committedBlobLength.get() >= getCommandBlobOffset()) {	return;	}	if (lastBlock == null) {	
nothing to commit for 

public void execute() throws InterruptedException, IOException {	if (committedBlobLength.get() >= getCommandBlobOffset()) {	return;	}	if (lastBlock == null) {	return;	}	
active commands for 

break;	}	}	uploadingSemaphore.acquire(MAX_NUMBER_THREADS_IN_THREAD_POOL);	BlockEntry uncommittedBlock;	do  {	uncommittedBlock = uncommittedBlockEntries.poll();	blockEntries.add(uncommittedBlock);	} while (uncommittedBlock != lastBlock);	if (blockEntries.size() > activateCompactionBlockCount) {	
block compaction activated with blocks for 

}	uploadingSemaphore.acquire(MAX_NUMBER_THREADS_IN_THREAD_POOL);	BlockEntry uncommittedBlock;	do  {	uncommittedBlock = uncommittedBlockEntries.poll();	blockEntries.add(uncommittedBlock);	} while (uncommittedBlock != lastBlock);	if (blockEntries.size() > activateCompactionBlockCount) {	long startCompaction = System.nanoTime();	blockCompaction();	
block compaction finished for ms with blocks for 

maxSegmentEnd = segmentEnd;	maxSegmentOffsetBegin = segmentOffsetBegin;	maxSegmentOffsetEnd = segmentOffsetEnd - block.getSize();	}	}	segmentBegin = segmentEnd - 1;	segmentOffsetBegin = segmentOffsetEnd - block.getSize();	}	}	if (maxSegmentEnd - maxSegmentBegin > 1) {	
block compaction blocks for 

segmentBegin = segmentEnd - 1;	segmentOffsetBegin = segmentOffsetEnd - block.getSize();	}	}	if (maxSegmentEnd - maxSegmentBegin > 1) {	ByteArrayOutputStreamInternal blockOutputStream = new ByteArrayOutputStreamInternal(maxBlockSize.get());	try {	long length = maxSegmentOffsetEnd - maxSegmentOffsetBegin;	blob.downloadRange(maxSegmentOffsetBegin, length, blockOutputStream, new BlobRequestOptions(), opContext);	} catch(StorageException ex) {	
storage exception encountered during block compaction phase storage exception error code 

public void run() {	try {	command.dump();	long startTime = System.nanoTime();	command.execute();	command.setCompleted();	
command finished for ms 

public void run() {	try {	command.dump();	long startTime = System.nanoTime();	command.execute();	command.setCompleted();	} catch (InterruptedException ie) {	Thread.currentThread().interrupt();	} catch (Exception ex) {	
encountered exception during execution of command for blob exception 

========================= hadoop sample_6408 =========================

sanitizeEnv(environment, containerWorkDir, appDirs, userLocalDirs, containerLogDirs, localResources, nmPrivateClasspathJarDir);	exec.writeLaunchEnv(containerScriptOutStream, environment, localResources, launchContext.getCommands(), new Path(containerLogDirs.get(0)), user);	tokensOutStream = lfs.create(nmPrivateTokensPath, EnumSet.of(CREATE, OVERWRITE));	Credentials creds = container.getCredentials();	creds.writeTokenStorageToStream(tokensOutStream);	} finally {	IOUtils.cleanupWithLogger(LOG, containerScriptOutStream, tokensOutStream);	}	ret = launchContainer(new ContainerStartContext.Builder() .setContainer(container) .setLocalizedResources(localResources) .setNmPrivateContainerScriptPath(nmPrivateContainerScriptPath) .setNmPrivateTokensPath(nmPrivateTokensPath) .setUser(user) .setAppId(appIdStr) .setContainerWorkDir(containerWorkDir) .setLocalDirs(localDirs) .setLogDirs(logDirs) .setFilecacheDirs(filecacheDirs) .setUserLocalDirs(userLocalDirs) .setContainerLocalDirs(containerLocalDirs) .setContainerLogDirs(containerLogDirs) .build());	} catch (ConfigurationException e) {	
failed to launch container due to configuration error 

creds.writeTokenStorageToStream(tokensOutStream);	} finally {	IOUtils.cleanupWithLogger(LOG, containerScriptOutStream, tokensOutStream);	}	ret = launchContainer(new ContainerStartContext.Builder() .setContainer(container) .setLocalizedResources(localResources) .setNmPrivateContainerScriptPath(nmPrivateContainerScriptPath) .setNmPrivateTokensPath(nmPrivateTokensPath) .setUser(user) .setAppId(appIdStr) .setContainerWorkDir(containerWorkDir) .setLocalDirs(localDirs) .setLogDirs(logDirs) .setFilecacheDirs(filecacheDirs) .setUserLocalDirs(userLocalDirs) .setContainerLocalDirs(containerLocalDirs) .setContainerLogDirs(containerLogDirs) .build());	} catch (ConfigurationException e) {	dispatcher.getEventHandler().handle(new ContainerExitEvent( containerID, ContainerEventType.CONTAINER_EXITED_WITH_FAILURE, ret, e.getMessage()));	context.getNodeStatusUpdater().reportException(e);	return ret;	} catch (Throwable e) {	
failed to launch container 

protected int launchContainer(ContainerStartContext ctx) throws IOException, ConfigurationException {	ContainerId containerId = container.getContainerId();	if (container.isMarkedForKilling()) {	
container not launched as it has already been marked for killing 

protected int launchContainer(ContainerStartContext ctx) throws IOException, ConfigurationException {	ContainerId containerId = container.getContainerId();	if (container.isMarkedForKilling()) {	this.killedBeforeStart = true;	return ExitCode.TERMINATED.getExitCode();	}	dispatcher.getEventHandler().handle(new ContainerEvent( containerId, ContainerEventType.CONTAINER_LAUNCHED));	context.getNMStateStore().storeContainerLaunched(containerId);	if (!containerAlreadyLaunched.compareAndSet(false, true)) {	
container not launched as cleanup already called 

protected void setContainerCompletedStatus(int exitCode) {	ContainerId containerId = container.getContainerId();	completed.set(true);	exec.deactivateContainer(containerId);	try {	if (!container.shouldRetry(exitCode)) {	context.getNMStateStore().storeContainerCompleted(containerId, exitCode);	}	} catch (IOException e) {	
unable to set exit code for container 

protected void handleContainerExitWithFailure(ContainerId containerID, int ret, Path containerLogDir, StringBuilder diagnosticInfo) {	
container launch failed 

try {	fileSystem = FileSystem.getLocal(conf).getRaw();	FileStatus preLaunchErrorFileStatus = fileSystem .getFileStatus(new Path(containerLogDir, ContainerLaunch.CONTAINER_PRE_LAUNCH_STDERR));	Path errorFile = preLaunchErrorFileStatus.getPath();	long fileSize = preLaunchErrorFileStatus.getLen();	diagnosticInfo.append("Error file: ") .append(ContainerLaunch.CONTAINER_PRE_LAUNCH_STDERR).append(".\n");	;	byte[] tailBuffer = tailFile(errorFile, fileSize, tailSizeInBytes);	diagnosticInfo.append("Last ").append(tailSizeInBytes) .append(" bytes of ").append(errorFile.getName()).append(" :\n") .append(new String(tailBuffer, StandardCharsets.UTF_8));	} catch (IOException e) {	
failed to get tail of the container s prelaunch error log file 

fileSize = errorFileStatuses[i].getLen();	}	}	diagnosticInfo.append("Error files: ") .append(StringUtils.join(", ", errorFileNames)).append(".\n");	}	byte[] tailBuffer = tailFile(errorFile, fileSize, tailSizeInBytes);	String tailBufferMsg = new String(tailBuffer, StandardCharsets.UTF_8);	diagnosticInfo.append("Last ").append(tailSizeInBytes) .append(" bytes of ").append(errorFile.getName()).append(" :\n") .append(tailBufferMsg).append("\n") .append(analysesErrorMsgOfContainerExitWithFailure(tailBufferMsg));	}	} catch (IOException e) {	
failed to get tail of the container s error log file 

public void cleanupContainer() throws IOException {	ContainerId containerId = container.getContainerId();	String containerIdStr = containerId.toString();	
cleaning up container 

public void cleanupContainer() throws IOException {	ContainerId containerId = container.getContainerId();	String containerIdStr = containerId.toString();	try {	context.getNMStateStore().storeContainerKilled(containerId);	} catch (IOException e) {	
unable to mark container killed in store 

public void cleanupContainer() throws IOException {	ContainerId containerId = container.getContainerId();	String containerIdStr = containerId.toString();	try {	context.getNMStateStore().storeContainerKilled(containerId);	} catch (IOException e) {	}	boolean alreadyLaunched = !containerAlreadyLaunched.compareAndSet(false, true);	if (!alreadyLaunched) {	
container not launched no cleanup needed to be done 

String containerIdStr = containerId.toString();	try {	context.getNMStateStore().storeContainerKilled(containerId);	} catch (IOException e) {	}	boolean alreadyLaunched = !containerAlreadyLaunched.compareAndSet(false, true);	if (!alreadyLaunched) {	return;	}	if (LOG.isDebugEnabled()) {	
marking container as inactive 

LOG.debug("Getting pid for container " + containerIdStr + " to kill" + " from pid file " + (pidFilePath != null ? pidFilePath.toString() : "null"));	}	try {	String processId = null;	if (pidFilePath != null) {	processId = getContainerPid(pidFilePath);	}	if (processId != null) {	String user = container.getUser();	if (LOG.isDebugEnabled()) {	
sending signal to pid as user for container 

public void signalContainer(SignalContainerCommand command) throws IOException {	ContainerId containerId = container.getContainerTokenIdentifier().getContainerID();	String containerIdStr = containerId.toString();	String user = container.getUser();	Signal signal = translateCommandToSignal(command);	if (signal.equals(Signal.NULL)) {	
ignore signal command 

public void signalContainer(SignalContainerCommand command) throws IOException {	ContainerId containerId = container.getContainerTokenIdentifier().getContainerID();	String containerIdStr = containerId.toString();	String user = container.getUser();	Signal signal = translateCommandToSignal(command);	if (signal.equals(Signal.NULL)) {	return;	}	
sending signal to container 

public void signalContainer(SignalContainerCommand command) throws IOException {	ContainerId containerId = container.getContainerTokenIdentifier().getContainerID();	String containerIdStr = containerId.toString();	String user = container.getUser();	Signal signal = translateCommandToSignal(command);	if (signal.equals(Signal.NULL)) {	return;	}	boolean alreadyLaunched = !containerAlreadyLaunched.compareAndSet(false, true);	if (!alreadyLaunched) {	
container not launched not sending the signal 

if (LOG.isDebugEnabled()) {	LOG.debug("Getting pid for container " + containerIdStr + " to send signal to from pid file " + (pidFilePath != null ? pidFilePath.toString() : "null"));	}	try {	String processId = null;	if (pidFilePath != null) {	processId = getContainerPid(pidFilePath);	}	if (processId != null) {	if (LOG.isDebugEnabled()) {	
sending signal to pid as user for container 

public void pauseContainer() throws IOException {	ContainerId containerId = container.getContainerId();	String containerIdStr = containerId.toString();	
pausing the container 

public void pauseContainer() throws IOException {	ContainerId containerId = container.getContainerId();	String containerIdStr = containerId.toString();	if (!shouldPauseContainer.compareAndSet(false, true)) {	
container not paused as resume already called 

String containerIdStr = containerId.toString();	if (!shouldPauseContainer.compareAndSet(false, true)) {	return;	}	try {	exec.pauseContainer(container);	dispatcher.getEventHandler().handle(new ContainerEvent( containerId, ContainerEventType.CONTAINER_PAUSED));	try {	this.context.getNMStateStore().storeContainerPaused( container.getContainerId());	} catch (IOException e) {	
could not store container state the container has been paused 

public void resumeContainer() throws IOException {	ContainerId containerId = container.getContainerId();	String containerIdStr = containerId.toString();	
resuming the container 

public void resumeContainer() throws IOException {	ContainerId containerId = container.getContainerId();	String containerIdStr = containerId.toString();	boolean alreadyPaused = !shouldPauseContainer.compareAndSet(false, true);	if (!alreadyPaused) {	
container not paused no resume necessary 

boolean alreadyPaused = !shouldPauseContainer.compareAndSet(false, true);	if (!alreadyPaused) {	return;	}	try {	exec.resumeContainer(container);	dispatcher.getEventHandler().handle(new ContainerEvent( containerId, ContainerEventType.CONTAINER_RESUMED));	try {	this.context.getNMStateStore().removeContainerPaused( container.getContainerId());	} catch (IOException e) {	
could not store container state the container has been resumed 

private String getContainerPid(Path pidFilePath) throws Exception {	String containerIdStr = container.getContainerId().toString();	String processId = null;	if (LOG.isDebugEnabled()) {	
accessing pid for container from pid file 

String containerIdStr = container.getContainerId().toString();	String processId = null;	if (LOG.isDebugEnabled()) {	}	int sleepCounter = 0;	final int sleepInterval = 100;	while (true) {	processId = ProcessIdFileReader.getProcessId(pidFilePath);	if (processId != null) {	if (LOG.isDebugEnabled()) {	
got pid for container 

int sleepCounter = 0;	final int sleepInterval = 100;	while (true) {	processId = ProcessIdFileReader.getProcessId(pidFilePath);	if (processId != null) {	if (LOG.isDebugEnabled()) {	}	break;	}	else if ((sleepCounter*sleepInterval) > maxKillWaitTime) {	
could not get pid for waited for ms 

========================= hadoop sample_1772 =========================

private List<Queue> createQueues(Configuration conf) {	String[] queueNameValues = conf.getStrings( MAPRED_QUEUE_NAMES_KEY);	List<Queue> list = new ArrayList<Queue>();	for (String name : queueNameValues) {	try {	Map<String, AccessControlList> acls = getQueueAcls( name, conf);	QueueState state = getQueueState(name, conf);	Queue q = new Queue(name, acls, state);	list.add(q);	} catch (Throwable t) {	
not able to initialize queue 

private boolean deprecatedConf(Configuration conf) {	String[] queues = null;	String queueNameValues = getQueueNames(conf);	if (queueNameValues == null) {	return false;	} else {	
configuring mapred queue names key in mapred site xml or hadoop site xml is deprecated and will overshadow remove this property and configure queue hierarchy in 

return false;	} else {	queues = conf.getStrings(MAPRED_QUEUE_NAMES_KEY);	}	if (queues != null) {	for (String queue : queues) {	for (QueueACL qAcl : QueueACL.values()) {	String key = toFullPropertyName(queue, qAcl.getAclName());	String aclString = conf.get(key);	if (aclString != null) {	
configuring queue acls in mapred site xml or hadoop site xml is deprecated configure queue acls in 

========================= hadoop sample_4823 =========================

static boolean canRollBack(StorageDirectory sd, StorageInfo storage, StorageInfo prevStorage, int targetLayoutVersion) throws IOException {	File prevDir = sd.getPreviousDir();	if (!prevDir.exists()) {	
storage directory does not contain previous fs state 

static void doFinalize(StorageDirectory sd) throws IOException {	File prevDir = sd.getPreviousDir();	if (!prevDir.exists()) {	
directory does not exist 

static void doFinalize(StorageDirectory sd) throws IOException {	File prevDir = sd.getPreviousDir();	if (!prevDir.exists()) {	
finalize upgrade for is not required 

static void doFinalize(StorageDirectory sd) throws IOException {	File prevDir = sd.getPreviousDir();	if (!prevDir.exists()) {	return;	}	
finalizing upgrade of storage directory 

static void doFinalize(StorageDirectory sd) throws IOException {	File prevDir = sd.getPreviousDir();	if (!prevDir.exists()) {	return;	}	Preconditions.checkState(sd.getCurrentDir().exists(), "Current directory must exist.");	final File tmpDir = sd.getFinalizedTmp();	NNStorage.rename(prevDir, tmpDir);	NNStorage.deleteDir(tmpDir);	
finalize upgrade for is complete 

static void doPreUpgrade(Configuration conf, StorageDirectory sd) throws IOException {	
starting upgrade of storage directory 

public static void doUpgrade(StorageDirectory sd, Storage storage) throws IOException {	
performing upgrade of storage directory 

public static void doUpgrade(StorageDirectory sd, Storage storage) throws IOException {	try {	storage.writeProperties(sd);	File prevDir = sd.getPreviousDir();	File tmpDir = sd.getPreviousTmp();	Preconditions.checkState(!prevDir.exists(), "previous directory must not exist for upgrade.");	Preconditions.checkState(tmpDir.exists(), "previous.tmp directory must exist for upgrade.");	NNStorage.rename(tmpDir, prevDir);	} catch (IOException ioe) {	
unable to rename temp to previous for 

if (!prevDir.exists()) {	return;	}	File tmpDir = sd.getRemovedTmp();	Preconditions.checkState(!tmpDir.exists(), "removed.tmp directory must not exist for rollback." + "Consider restarting for recovery.");	File curDir = sd.getCurrentDir();	Preconditions.checkState(curDir.exists(), "Current directory must exist for rollback.");	NNStorage.rename(curDir, tmpDir);	NNStorage.rename(prevDir, curDir);	NNStorage.deleteDir(tmpDir);	
rollback of is complete 

========================= hadoop sample_8103 =========================

private void checkAcls(String method) throws YarnException {	UserGroupInformation user;	try {	user = UserGroupInformation.getCurrentUser();	} catch (IOException ioe) {	
couldn t get current user 

private void checkAcls(String method) throws YarnException {	UserGroupInformation user;	try {	user = UserGroupInformation.getCurrentUser();	} catch (IOException ioe) {	throw RPCUtil.getRemoteException(ioe);	}	if (!authorizer.isAdmin(user)) {	
user doesn t have permission to call 

private void checkAcls(String method) throws YarnException {	UserGroupInformation user;	try {	user = UserGroupInformation.getCurrentUser();	} catch (IOException ioe) {	throw RPCUtil.getRemoteException(ioe);	}	if (!authorizer.isAdmin(user)) {	throw RPCUtil.getRemoteException( new AccessControlException("User " + user.getShortUserName() + " doesn't have permission" + " to call '" + method + "'"));	}	
scm admin invoked by user 

========================= hadoop sample_402 =========================

List<String> expectedCommands =  new ArrayList<String>(Arrays.asList( DOCKER_LAUNCH_COMMAND, "run", "--rm", "--net=host",  "--name", containerId));	expectedCommands.addAll(localDirs);	expectedCommands.addAll(logDirs);	expectedCommands.addAll(workDirMount);	String shellScript =  workDir + "/launch_container.sh";	expectedCommands.addAll(Arrays.asList(testImage.replaceAll("['\"]", ""), "bash","\"" + shellScript + "\""));	String expectedPidString = "echo `/bin/true inspect --format {{.State.Pid}} " + containerId+"` > "+ pidFile.toString() + ".tmp";	boolean pidSetterFound = false;	while(lnr.ready()){	String line = lnr.readLine();	
line 

========================= hadoop sample_1565 =========================

if (ctx != null) {	ctx.checkException();	}	try {	Stat stat = new Stat();	byte[] data = zks.getZKDatabase().getData( parentDir + "/" + ActiveStandbyElector.LOCK_FILENAME, stat, null);	if (activeData != null && Arrays.equals(activeData, data)) {	return;	}	if (Time.now() > lastPrint + LOG_INTERVAL_MS) {	
cur data 

return;	}	if (Time.now() > lastPrint + LOG_INTERVAL_MS) {	lastPrint = Time.now();	}	} catch (NoNodeException nne) {	if (activeData == null) {	return;	}	if (Time.now() > lastPrint + LOG_INTERVAL_MS) {	
cur data no node 

========================= hadoop sample_3108 =========================

NodeResourceUpdateSchedulerEvent nodeResourceUpdatedEvent = (NodeResourceUpdateSchedulerEvent) event;	nodeMonitor.updateNodeResource(nodeResourceUpdatedEvent.getRMNode(), nodeResourceUpdatedEvent.getResourceOption());	break;	case APP_ADDED: break;	case APP_REMOVED: break;	case APP_ATTEMPT_ADDED: break;	case APP_ATTEMPT_REMOVED: break;	case CONTAINER_EXPIRED: break;	case NODE_LABELS_UPDATE: break;	case RELEASE_CONTAINER: break;	
unknown event arrived at opportunisticcontainerallocatoramservice 

========================= hadoop sample_1135 =========================

if (localFs != null) {	localFs.close();	}	if (remoteFs != null) {	remoteFs.close();	}	if (dfs != null) {	dfs.shutdown();	}	} catch (IOException ioe) {	
io exception in closing file system 

========================= hadoop sample_4622 =========================

public void cleanUpPartialOutputForTask(TaskAttemptContext context) throws IOException {	if (!this.getClass().isAnnotationPresent(Checkpointable.class)) {	throw new IllegalStateException("Invoking cleanUpPartialOutputForTask() " + "from non @Preemptable class");	}	FileSystem fs = fsFor(getTaskAttemptPath(context), context.getConfiguration());	
cleanuppartialoutputfortask removing everything belonging to in 

========================= hadoop sample_5033 =========================

if (bookkeeper.isActiveAndEnabled(tid)) {	bookkeeper.addRackRR(tid, rr);	hasActive = true;	}	}	if (hasActive) {	continue;	}	}	if (LOG.isDebugEnabled()) {	
error resolving sub cluster for resourcename we are falling back to homesubcluster 

if (LOG.isDebugEnabled()) {	}	if (bookkeeper.isActiveAndEnabled(homeSubcluster)) {	if (targetIds != null && targetIds.size() > 0) {	bookkeeper.addRackRR(homeSubcluster, rr);	} else {	bookkeeper.addLocalizedNodeRR(homeSubcluster, rr);	}	} else {	if (LOG.isDebugEnabled()) {	
the homesubcluster we are defaulting to is not active the resourcerequest will be ignored 

========================= hadoop sample_1359 =========================

ugi = UserGroupInformation.createRemoteUser(remoteUser);	checkUsername(ugi.getShortUserName(), usernameFromQuery);	if (UserGroupInformation.isSecurityEnabled()) {	ugi.setAuthenticationMethod(secureAuthMethod);	}	if (doAsUserFromQuery != null) {	ugi = UserGroupInformation.createProxyUser(doAsUserFromQuery, ugi);	ProxyUsers.authorize(ugi, getRemoteAddr(request));	}	}	
getugi is returning 

========================= hadoop sample_8178 =========================

protected void serviceStart() throws Exception {	if (!writeGlobalCleanerPidFile()) {	throw new YarnException("The global cleaner pid file already exists! " + "It appears there is another CleanerService running in the cluster");	}	this.metrics = CleanerMetrics.getInstance();	super.serviceStart();	Runnable task = CleanerTask.create(conf, store, metrics, cleanerTaskLock);	long periodInMinutes = getPeriod(conf);	scheduledExecutor.scheduleAtFixedRate(task, getInitialDelay(conf), periodInMinutes, TimeUnit.MINUTES);	
scheduled the shared cache cleaner task to run every minutes 

protected void serviceStop() throws Exception {	
shutting down the background thread 

protected void serviceStop() throws Exception {	scheduledExecutor.shutdownNow();	try {	if (scheduledExecutor.awaitTermination(10, TimeUnit.SECONDS)) {	
the background thread stopped 

protected void serviceStop() throws Exception {	scheduledExecutor.shutdownNow();	try {	if (scheduledExecutor.awaitTermination(10, TimeUnit.SECONDS)) {	} else {	
gave up waiting for the cleaner task to shutdown 

protected void serviceStop() throws Exception {	scheduledExecutor.shutdownNow();	try {	if (scheduledExecutor.awaitTermination(10, TimeUnit.SECONDS)) {	} else {	}	} catch (InterruptedException e) {	
the cleaner service was interrupted while shutting down the task 

return false;	}	FSDataOutputStream os = fs.create(pidPath, false);	final String ID = ManagementFactory.getRuntimeMXBean().getName();	os.writeUTF(ID);	os.close();	fs.deleteOnExit(pidPath);	} catch (IOException e) {	throw new YarnException(e);	}	
created the global cleaner pid file at 

private void removeGlobalCleanerPidFile() {	try {	FileSystem fs = FileSystem.get(this.conf);	String root = conf.get(YarnConfiguration.SHARED_CACHE_ROOT, YarnConfiguration.DEFAULT_SHARED_CACHE_ROOT);	Path pidPath = new Path(root, GLOBAL_CLEANER_PID);	fs.delete(pidPath, false);	
removed the global cleaner pid file at 

private void removeGlobalCleanerPidFile() {	try {	FileSystem fs = FileSystem.get(this.conf);	String root = conf.get(YarnConfiguration.SHARED_CACHE_ROOT, YarnConfiguration.DEFAULT_SHARED_CACHE_ROOT);	Path pidPath = new Path(root, GLOBAL_CLEANER_PID);	fs.delete(pidPath, false);	} catch (IOException e) {	
unable to remove the global cleaner pid file the file may need to be removed manually 

========================= hadoop sample_394 =========================

public void init(Configuration configuration, RouterRpcServer rpcServer, StateStoreService stateStore) {	this.conf = configuration;	this.server = rpcServer;	this.store = stateStore;	this.metrics = FederationRPCMetrics.create(conf, server);	ThreadFactory threadFactory = new ThreadFactoryBuilder() .setNameFormat("Federation RPC Performance Monitor-%d").build();	this.executor = Executors.newFixedThreadPool(1, threadFactory);	try {	StandardMBean bean = new StandardMBean(this.metrics, FederationRPCMBean.class);	registeredBean = MBeans.register("Router", "FederationRPC", bean);	
registered federationrpcmbean 

========================= hadoop sample_8197 =========================

protected AbstractFSContract(Configuration conf) {	super(conf);	if (maybeAddConfResource(ContractOptions.CONTRACT_OPTIONS_RESOURCE)) {	
loaded authentication keys from 

protected AbstractFSContract(Configuration conf) {	super(conf);	if (maybeAddConfResource(ContractOptions.CONTRACT_OPTIONS_RESOURCE)) {	} else {	
not loaded 

========================= hadoop sample_3221 =========================

public synchronized int read(byte[] b, int off, int len) throws IOException {	
read buffer d d 

public synchronized int read(byte[] b, int off, int len) throws IOException {	SwiftUtils.validateReadArgs(b, off, len);	if (len == 0) {	return 0;	}	int result = -1;	try {	verifyOpen();	result = httpStream.read(b, off, len);	} catch (IOException e) {	
received ioexception while reading attempting to reopen 

public synchronized int read(byte[] b, int off, int len) throws IOException {	SwiftUtils.validateReadArgs(b, off, len);	if (len == 0) {	return 0;	}	int result = -1;	try {	verifyOpen();	result = httpStream.read(b, off, len);	} catch (IOException e) {	
ioe on read 

private void innerClose(String reason) throws IOException {	try {	if (httpStream != null) {	reasonClosed = reason;	if (LOG.isDebugEnabled()) {	
closing http input stream 

protected void finalize() throws Throwable {	if (httpStream != null) {	
input stream is leaking handles by not being closed properly 

public synchronized void seek(long targetPos) throws IOException {	if (targetPos < 0) {	throw new EOFException( FSExceptionMessages.NEGATIVE_SEEK);	}	long offset = targetPos - pos;	if (LOG.isDebugEnabled()) {	LOG.debug("Seek to " + targetPos + "; current pos =" + pos + "; offset="+offset);	}	if (offset == 0) {	
seek is no op 

throw new EOFException( FSExceptionMessages.NEGATIVE_SEEK);	}	long offset = targetPos - pos;	if (LOG.isDebugEnabled()) {	LOG.debug("Seek to " + targetPos + "; current pos =" + pos + "; offset="+offset);	}	if (offset == 0) {	return;	}	if (offset < 0) {	
seek is backwards 

if (LOG.isDebugEnabled()) {	LOG.debug("Seek to " + targetPos + "; current pos =" + pos + "; offset="+offset);	}	if (offset == 0) {	return;	}	if (offset < 0) {	} else if ((rangeOffset + offset < bufferSize)) {	SwiftUtils.debug(LOG, "seek is within current stream" + "; pos= %d ; targetPos=%d; " + "offset= %d ; bufferOffset=%d", pos, targetPos, offset, rangeOffset);	try {	
chomping 

}	if (offset == 0) {	return;	}	if (offset < 0) {	} else if ((rangeOffset + offset < bufferSize)) {	SwiftUtils.debug(LOG, "seek is within current stream" + "; pos= %d ; targetPos=%d; " + "offset= %d ; bufferOffset=%d", pos, targetPos, offset, rangeOffset);	try {	chompBytes(offset);	} catch (IOException e) {	
while chomping 

return;	}	if (offset < 0) {	} else if ((rangeOffset + offset < bufferSize)) {	SwiftUtils.debug(LOG, "seek is within current stream" + "; pos= %d ; targetPos=%d; " + "offset= %d ; bufferOffset=%d", pos, targetPos, offset, rangeOffset);	try {	chompBytes(offset);	} catch (IOException e) {	}	if (targetPos - pos == 0) {	
chomping successful 

if (offset < 0) {	} else if ((rangeOffset + offset < bufferSize)) {	SwiftUtils.debug(LOG, "seek is within current stream" + "; pos= %d ; targetPos=%d; " + "offset= %d ; bufferOffset=%d", pos, targetPos, offset, rangeOffset);	try {	chompBytes(offset);	} catch (IOException e) {	}	if (targetPos - pos == 0) {	return;	}	
chomping failed 

SwiftUtils.debug(LOG, "seek is within current stream" + "; pos= %d ; targetPos=%d; " + "offset= %d ; bufferOffset=%d", pos, targetPos, offset, rangeOffset);	try {	chompBytes(offset);	} catch (IOException e) {	}	if (targetPos - pos == 0) {	return;	}	} else {	if (LOG.isDebugEnabled()) {	
seek is beyond buffer size of 

private void fillBuffer(long targetPos) throws IOException {	long length = targetPos + bufferSize;	
fetching d bytes starting at d 

========================= hadoop sample_6189 =========================

public static synchronized NfsExports getInstance(Configuration conf) {	if (exports == null) {	String matchHosts = conf.get( CommonConfigurationKeys.NFS_EXPORTS_ALLOWED_HOSTS_KEY, CommonConfigurationKeys.NFS_EXPORTS_ALLOWED_HOSTS_KEY_DEFAULT);	int cacheSize = conf.getInt(Nfs3Constant.NFS_EXPORTS_CACHE_SIZE_KEY, Nfs3Constant.NFS_EXPORTS_CACHE_SIZE_DEFAULT);	long expirationPeriodNano = conf.getLong( Nfs3Constant.NFS_EXPORTS_CACHE_EXPIRYTIME_MILLIS_KEY, Nfs3Constant.NFS_EXPORTS_CACHE_EXPIRYTIME_MILLIS_DEFAULT) * 1000 * 1000;	try {	exports = new NfsExports(cacheSize, expirationPeriodNano, matchHosts);	} catch (IllegalArgumentException e) {	
invalid nfs exports provided 

public boolean isIncluded(String address, String hostname) {	if(ipOrHost.equalsIgnoreCase(address) || ipOrHost.equalsIgnoreCase(hostname)) {	if(LOG.isDebugEnabled()) {	
exactmatcher allowing client 

public boolean isIncluded(String address, String hostname) {	if(ipOrHost.equalsIgnoreCase(address) || ipOrHost.equalsIgnoreCase(hostname)) {	if(LOG.isDebugEnabled()) {	}	return true;	}	if(LOG.isDebugEnabled()) {	
exactmatcher denying client 

public boolean isIncluded(String address, String hostname) {	if (pattern.matcher(address).matches() || pattern.matcher(hostname).matches()) {	if (LOG.isDebugEnabled()) {	
regexmatcher allowing client 

public boolean isIncluded(String address, String hostname) {	if (pattern.matcher(address).matches() || pattern.matcher(hostname).matches()) {	if (LOG.isDebugEnabled()) {	}	return true;	}	if (LOG.isDebugEnabled()) {	
regexmatcher denying client 

case 2: host = StringUtils.toLowerCase(parts[0]).trim();	String option = parts[1].trim();	if ("rw".equalsIgnoreCase(option)) {	privilege = AccessPrivilege.READ_WRITE;	}	break;	default: throw new IllegalArgumentException("Incorrectly formatted line '" + line + "'");	}	if (host.equals("*")) {	if (LOG.isDebugEnabled()) {	
using match all for and 

}	break;	default: throw new IllegalArgumentException("Incorrectly formatted line '" + line + "'");	}	if (host.equals("*")) {	if (LOG.isDebugEnabled()) {	}	return new AnonymousMatch(privilege);	} else if (CIDR_FORMAT_SHORT.matcher(host).matches()) {	if (LOG.isDebugEnabled()) {	
using cidr match for and 

if (host.equals("*")) {	if (LOG.isDebugEnabled()) {	}	return new AnonymousMatch(privilege);	} else if (CIDR_FORMAT_SHORT.matcher(host).matches()) {	if (LOG.isDebugEnabled()) {	}	return new CIDRMatch(privilege, new SubnetUtils(host).getInfo());	} else if (CIDR_FORMAT_LONG.matcher(host).matches()) {	if (LOG.isDebugEnabled()) {	
using cidr match for and 

if (LOG.isDebugEnabled()) {	}	return new CIDRMatch(privilege, new SubnetUtils(host).getInfo());	} else if (CIDR_FORMAT_LONG.matcher(host).matches()) {	if (LOG.isDebugEnabled()) {	}	String[] pair = host.split("/");	return new CIDRMatch(privilege, new SubnetUtils(pair[0], pair[1]).getInfo());	} else if (host.contains("*") || host.contains("?") || host.contains("[") || host.contains("]") || host.contains("(") || host.contains(")")) {	if (LOG.isDebugEnabled()) {	
using regex match for and 

if (LOG.isDebugEnabled()) {	}	String[] pair = host.split("/");	return new CIDRMatch(privilege, new SubnetUtils(pair[0], pair[1]).getInfo());	} else if (host.contains("*") || host.contains("?") || host.contains("[") || host.contains("]") || host.contains("(") || host.contains(")")) {	if (LOG.isDebugEnabled()) {	}	return new RegexMatch(privilege, host);	} else if (HOSTNAME_FORMAT.matcher(host).matches()) {	if (LOG.isDebugEnabled()) {	
using exact match for and 

========================= hadoop sample_4360 =========================

protected AmazonS3 newAmazonS3Client(AWSCredentialsProvider credentials, ClientConfiguration awsConf) {	
failure injection enabled do not run in production 

========================= hadoop sample_5988 =========================

public int run(String[] args) {	
starting zkrmstatestoreperf ver 

========================= hadoop sample_455 =========================

stubQueueAllocation(a, clusterResource, node_0, 0*GB);	stubQueueAllocation(b, clusterResource, node_0, 0*GB);	stubQueueAllocation(c, clusterResource, node_0, 0*GB);	stubQueueAllocation(d, clusterResource, node_0, 1*GB);	root.assignContainers(clusterResource, node_0, new ResourceLimits( clusterResource), SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);	}	verifyQueueMetrics(a, 1*GB, clusterResource);	verifyQueueMetrics(b, 2*GB, clusterResource);	verifyQueueMetrics(c, 3*GB, clusterResource);	verifyQueueMetrics(d, 4*GB, clusterResource);	
status child queues 

verifyQueueMetrics(c, 3*GB, clusterResource);	verifyQueueMetrics(d, 4*GB, clusterResource);	for(int i=0; i < 3;i++) {	d.completedContainer(clusterResource, app_0, node_0, rmContainer, null, RMContainerEventType.KILL, null, true);	}	verifyQueueMetrics(a, 1*GB, clusterResource);	verifyQueueMetrics(b, 2*GB, clusterResource);	verifyQueueMetrics(c, 3*GB, clusterResource);	verifyQueueMetrics(d, 1*GB, clusterResource);	node_0 = TestUtils.getMockNode("host_0", DEFAULT_RACK, 0, (memoryPerNode-1-2-3-1)*GB);	
status child queues 

stubQueueAllocation(a, clusterResource, node_0, 1*GB);	stubQueueAllocation(b, clusterResource, node_0, 0*GB);	stubQueueAllocation(c, clusterResource, node_0, 0*GB);	stubQueueAllocation(d, clusterResource, node_0, 0*GB);	root.assignContainers(clusterResource, node_0, new ResourceLimits( clusterResource), SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);	}	verifyQueueMetrics(a, 3*GB, clusterResource);	verifyQueueMetrics(b, 2*GB, clusterResource);	verifyQueueMetrics(c, 3*GB, clusterResource);	verifyQueueMetrics(d, 1*GB, clusterResource);	
status child queues 

verifyQueueMetrics(a, 3*GB, clusterResource);	verifyQueueMetrics(b, 2*GB, clusterResource);	verifyQueueMetrics(c, 3*GB, clusterResource);	verifyQueueMetrics(d, 1*GB, clusterResource);	a.completedContainer(clusterResource, app_0, node_0, rmContainer, null, RMContainerEventType.KILL, null, true);	verifyQueueMetrics(a, 2*GB, clusterResource);	verifyQueueMetrics(b, 2*GB, clusterResource);	verifyQueueMetrics(c, 3*GB, clusterResource);	verifyQueueMetrics(d, 1*GB, clusterResource);	node_0 = TestUtils.getMockNode("host_0", DEFAULT_RACK, 0, (memoryPerNode-2-2-3-1)*GB);	
status child queues 

node_0 = TestUtils.getMockNode("host_0", DEFAULT_RACK, 0, (memoryPerNode-2-2-3-1)*GB);	stubQueueAllocation(a, clusterResource, node_0, 0*GB);	stubQueueAllocation(b, clusterResource, node_0, 1*GB);	stubQueueAllocation(c, clusterResource, node_0, 0*GB);	stubQueueAllocation(d, clusterResource, node_0, 0*GB);	root.assignContainers(clusterResource, node_0, new ResourceLimits( clusterResource), SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);	verifyQueueMetrics(a, 2*GB, clusterResource);	verifyQueueMetrics(b, 3*GB, clusterResource);	verifyQueueMetrics(c, 3*GB, clusterResource);	verifyQueueMetrics(d, 1*GB, clusterResource);	
status child queues 

verifyQueueMetrics(a, 2*GB, clusterResource);	verifyQueueMetrics(b, 3*GB, clusterResource);	verifyQueueMetrics(c, 3*GB, clusterResource);	verifyQueueMetrics(d, 1*GB, clusterResource);	b.completedContainer(clusterResource, app_0, node_0, rmContainer, null, RMContainerEventType.KILL, null, true);	verifyQueueMetrics(a, 2*GB, clusterResource);	verifyQueueMetrics(b, 2*GB, clusterResource);	verifyQueueMetrics(c, 3*GB, clusterResource);	verifyQueueMetrics(d, 1*GB, clusterResource);	node_0 = TestUtils.getMockNode("host_0", DEFAULT_RACK, 0, (memoryPerNode-2-2-3-1)*GB);	
status child queues 

node_0 = TestUtils.getMockNode("host_0", DEFAULT_RACK, 0, (memoryPerNode-2-2-3-1)*GB);	stubQueueAllocation(a, clusterResource, node_0, 1*GB);	stubQueueAllocation(b, clusterResource, node_0, 0*GB);	stubQueueAllocation(c, clusterResource, node_0, 0*GB);	stubQueueAllocation(d, clusterResource, node_0, 0*GB);	root.assignContainers(clusterResource, node_0, new ResourceLimits( clusterResource), SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);	verifyQueueMetrics(a, 3*GB, clusterResource);	verifyQueueMetrics(b, 2*GB, clusterResource);	verifyQueueMetrics(c, 3*GB, clusterResource);	verifyQueueMetrics(d, 1*GB, clusterResource);	
status child queues 

stubQueueAllocation(c, clusterResource, node_0, 0*GB);	stubQueueAllocation(d, clusterResource, node_0, 1*GB);	root.assignContainers(clusterResource, node_0, new ResourceLimits( clusterResource), SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);	InOrder allocationOrder = inOrder(d,b);	allocationOrder.verify(d).assignContainers(eq(clusterResource), any(PlacementSet.class), any(ResourceLimits.class), any(SchedulingMode.class));	allocationOrder.verify(b).assignContainers(eq(clusterResource), any(PlacementSet.class), any(ResourceLimits.class), any(SchedulingMode.class));	verifyQueueMetrics(a, 3*GB, clusterResource);	verifyQueueMetrics(b, 2*GB, clusterResource);	verifyQueueMetrics(c, 3*GB, clusterResource);	verifyQueueMetrics(d, 2*GB, clusterResource);	
status child queues 

========================= hadoop sample_510 =========================

public XDR nullOp(XDR out, int xid, InetAddress client) {	if (LOG.isDebugEnabled()) {	
mount nullop client 

public XDR mnt(XDR xdr, XDR out, int xid, InetAddress client) {	if (hostsMatcher == null) {	return MountResponse.writeMNTResponse(Nfs3Status.NFS3ERR_ACCES, out, xid, null);	}	AccessPrivilege accessPrivilege = hostsMatcher.getAccessPrivilege(client);	if (accessPrivilege == AccessPrivilege.NONE) {	return MountResponse.writeMNTResponse(Nfs3Status.NFS3ERR_ACCES, out, xid, null);	}	String path = xdr.readString();	if (LOG.isDebugEnabled()) {	
mount mnt path client 

}	AccessPrivilege accessPrivilege = hostsMatcher.getAccessPrivilege(client);	if (accessPrivilege == AccessPrivilege.NONE) {	return MountResponse.writeMNTResponse(Nfs3Status.NFS3ERR_ACCES, out, xid, null);	}	String path = xdr.readString();	if (LOG.isDebugEnabled()) {	}	String host = client.getHostName();	if (LOG.isDebugEnabled()) {	
got host path 

if (accessPrivilege == AccessPrivilege.NONE) {	return MountResponse.writeMNTResponse(Nfs3Status.NFS3ERR_ACCES, out, xid, null);	}	String path = xdr.readString();	if (LOG.isDebugEnabled()) {	}	String host = client.getHostName();	if (LOG.isDebugEnabled()) {	}	if (!exports.contains(path)) {	
path is not shared 

}	if (!exports.contains(path)) {	MountResponse.writeMNTResponse(Nfs3Status.NFS3ERR_NOENT, out, xid, null);	return out;	}	FileHandle handle = null;	try {	HdfsFileStatus exFileStatus = dfsClient.getFileInfo(path);	handle = new FileHandle(exFileStatus.getFileId());	} catch (IOException e) {	
can t get handle for export 

}	FileHandle handle = null;	try {	HdfsFileStatus exFileStatus = dfsClient.getFileInfo(path);	handle = new FileHandle(exFileStatus.getFileId());	} catch (IOException e) {	MountResponse.writeMNTResponse(Nfs3Status.NFS3ERR_NOENT, out, xid, null);	return out;	}	assert (handle != null);	
giving handle fileid to client for export 

public XDR dump(XDR out, int xid, InetAddress client) {	if (LOG.isDebugEnabled()) {	
mount nullop client 

public XDR umnt(XDR xdr, XDR out, int xid, InetAddress client) {	String path = xdr.readString();	if (LOG.isDebugEnabled()) {	
mount umnt path client 

public XDR umntall(XDR out, int xid, InetAddress client) {	if (LOG.isDebugEnabled()) {	
mount umntall client 

========================= hadoop sample_7059 =========================

public long getMaxVirtualMemoryForTask() {	
getmaxvirtualmemoryfortask is deprecated instead use getmemoryformaptask and getmemoryforreducetask 

public void setMaxVirtualMemoryForTask(long vmem) {	
setmaxvirtualmemoryfortask is deprecated instead use setmemoryformaptask and setmemoryforreducetask 

public long getMaxPhysicalMemoryForTask() {	
the api getmaxphysicalmemoryfortask is deprecated refer to the apis getmemoryformaptask and getmemoryforreducetask for details 

public void setMaxPhysicalMemoryForTask(long mem) {	
the api setmaxphysicalmemoryfortask is deprecated the value set is ignored refer to setmemoryformaptask and setmemoryforreducetask for details 

private void checkAndWarnDeprecation() {	if(get(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) != null) {	
instead use and 

========================= hadoop sample_4675 =========================

public KeyProviderCache(long expiryMs) {	cache = CacheBuilder.newBuilder() .expireAfterAccess(expiryMs, TimeUnit.MILLISECONDS) .removalListener(new RemovalListener<URI, KeyProvider>() {	public void onRemoval( try {	assert notification.getValue() != null;	notification.getValue().close();	} catch (Throwable e) {	
error closing keyprovider with uri 

if (serverProviderUri == null) {	return null;	}	try {	return cache.get(serverProviderUri, new Callable<KeyProvider>() {	public KeyProvider call() throws Exception {	return KMSUtil.createKeyProviderFromUri(conf, serverProviderUri);	}	});	} catch (Exception e) {	
could not create keyprovider for dfsclient 

private URI createKeyProviderURI(Configuration conf) {	final String providerUriStr = conf.getTrimmed( CommonConfigurationKeysPublic.HADOOP_SECURITY_KEY_PROVIDER_PATH);	if (providerUriStr == null || providerUriStr.isEmpty()) {	
could not find uri with key to create a keyprovider 

private URI createKeyProviderURI(Configuration conf) {	final String providerUriStr = conf.getTrimmed( CommonConfigurationKeysPublic.HADOOP_SECURITY_KEY_PROVIDER_PATH);	if (providerUriStr == null || providerUriStr.isEmpty()) {	return null;	}	final URI providerUri;	try {	providerUri = new URI(providerUriStr);	} catch (URISyntaxException e) {	
keyprovider uri string is invalid 

========================= hadoop sample_6998 =========================

while(live) {	try {	int blockcount = getBlockCount();	if (blockcount < TOTAL_BLOCKS && blockcount > 0) {	String file = "/tmp" + createOps;	createFile(file, 1);	mc.getFileSystem().delete(new Path(file), true);	createOps++;	}	} catch (IOException ex) {	
createfile exception 

if (blockcount < TOTAL_BLOCKS && blockcount > 0) {	mc.getNamesystem().writeLock();	try {	lockOps++;	} finally {	mc.getNamesystem().writeUnlock();	}	Thread.sleep(1);	}	} catch (InterruptedException ex) {	
lockoperation exception 

}	}	};	threads[0].start();	threads[1].start();	final long start = Time.now();	FSNamesystem.BLOCK_DELETION_INCREMENT = 1;	mc.getFileSystem().delete(new Path(" final long end = Time.now();	threads[0].endThread();	threads[1].endThread();	
deletion took msecs 

}	}	};	threads[0].start();	threads[1].start();	final long start = Time.now();	FSNamesystem.BLOCK_DELETION_INCREMENT = 1;	mc.getFileSystem().delete(new Path(" final long end = Time.now();	threads[0].endThread();	threads[1].endThread();	
createoperations 

}	}	};	threads[0].start();	threads[1].start();	final long start = Time.now();	FSNamesystem.BLOCK_DELETION_INCREMENT = 1;	mc.getFileSystem().delete(new Path(" final long end = Time.now();	threads[0].endThread();	threads[1].endThread();	
lockoperations 

public synchronized void endThread() {	live = false;	interrupt();	try {	wait();	} catch (InterruptedException e) {	if(LOG.isDebugEnabled()) {	
ignoring 

========================= hadoop sample_7370 =========================

public void init(Configuration conf) {	isEnabled = conf.getBoolean(KEY_AUTH_SERVICE_CACHING_ENABLE, KEY_AUTH_SERVICE_CACHING_ENABLE_DEFAULT);	if (isEnabled) {	
initializing cachingauthorizer instance 

public V get(K key) {	if (!isEnabled) {	return null;	}	V result = cache.getIfPresent(key);	if (result == null) {	
cache miss 

public V get(K key) {	if (!isEnabled) {	return null;	}	V result = cache.getIfPresent(key);	if (result == null) {	}	else {	
cache hit 

public void put(K key, V value) {	if (isEnabled) {	
cache put 

========================= hadoop sample_6424 =========================

while (true) {	int numRunningNodes = 0;	for (RMNode node : rm.getRMContext().getRMNodes().values()) {	if (node.getState() == NodeState.RUNNING) {	numRunningNodes++;	}	}	if (numRunningNodes == numNMs) {	break;	}	
slsrunner is waiting for all nodes running of nms initialized 

for (RMNode node : rm.getRMContext().getRMNodes().values()) {	if (node.getState() == NodeState.RUNNING) {	numRunningNodes++;	}	}	if (numRunningNodes == numNMs) {	break;	}	Thread.sleep(1000);	}	
slsrunner takes ms to launch all nodes 

private void startAMFromSLSTrace(String inputTrace) throws IOException {	JsonFactory jsonF = new JsonFactory();	ObjectMapper mapper = new ObjectMapper();	try (Reader input = new InputStreamReader( new FileInputStream(inputTrace), "UTF-8")) {	Iterator<Map> jobIter = mapper.readValues( jsonF.createParser(input), Map.class);	while (jobIter.hasNext()) {	try {	createAMForJob(jobIter.next());	} catch (Exception e) {	
failed to create an am 

private void startAMFromRumenTrace(String inputTrace, long baselineTimeMS) throws IOException {	Configuration conf = new Configuration();	conf.set("fs.defaultFS", "file: File fin = new File(inputTrace);	try (JobTraceReader reader = new JobTraceReader( new Path(fin.getAbsolutePath()), conf)) {	LoggedJob job = reader.getNext();	while (job != null) {	try {	createAMForJob(job, baselineTimeMS);	} catch (Exception e) {	
failed to create an am 

String jobQueue = job.getQueueName();	String oldJobId = job.getJobID().toString();	long jobStartTimeMS = job.getSubmissionTime();	long jobFinishTimeMS = -1L;	if (baselineTimeMS == 0) {	baselineTimeMS = jobStartTimeMS;	}	jobStartTimeMS -= baselineTimeMS;	jobFinishTimeMS -= baselineTimeMS;	if (jobStartTimeMS < 0) {	
warning reset job start time to 

private void printSimulationInfo() {	if (printSimulation) {	LOG.info("------------------------------------");	LOG.info("# nodes = {}, # racks = {}, capacity " + "of each node {} MB memory and {} vcores.", numNMs, numRacks, nmMemoryMB, nmVCores);	LOG.info("------------------------------------");	LOG.info("# applications = {}, # total " + "tasks = {}, average # tasks per application = {}", numAMs, numTasks, (int)(Math.ceil((numTasks + 0.0) / numAMs)));	
jobid tqueue tamtype tduration t tasks 

private void printSimulationInfo() {	if (printSimulation) {	LOG.info("------------------------------------");	LOG.info("# nodes = {}, # racks = {}, capacity " + "of each node {} MB memory and {} vcores.", numNMs, numRacks, nmMemoryMB, nmVCores);	LOG.info("------------------------------------");	LOG.info("# applications = {}, # total " + "tasks = {}, average # tasks per application = {}", numAMs, numTasks, (int)(Math.ceil((numTasks + 0.0) / numAMs)));	for (Map.Entry<String, AMSimulator> entry : amMap.entrySet()) {	AMSimulator am = entry.getValue();	
t t t t 

LOG.info("------------------------------------");	LOG.info("# nodes = {}, # racks = {}, capacity " + "of each node {} MB memory and {} vcores.", numNMs, numRacks, nmMemoryMB, nmVCores);	LOG.info("------------------------------------");	LOG.info("# applications = {}, # total " + "tasks = {}, average # tasks per application = {}", numAMs, numTasks, (int)(Math.ceil((numTasks + 0.0) / numAMs)));	for (Map.Entry<String, AMSimulator> entry : amMap.entrySet()) {	AMSimulator am = entry.getValue();	}	LOG.info("------------------------------------");	LOG.info("number of queues = {}  average number of apps = {}", queueAppNumMap.size(), (int)(Math.ceil((numAMs + 0.0) / queueAppNumMap.size())));	LOG.info("------------------------------------");	
estimated simulation time is seconds 

public static void decreaseRemainingApps() {	remainingApps--;	if (remainingApps == 0) {	
slsrunner tears down 

========================= hadoop sample_6074 =========================

public String getUpgradeDomainWithDefaultValue(DatanodeInfo datanodeInfo) {	String upgradeDomain = datanodeInfo.getUpgradeDomain();	if (upgradeDomain == null) {	
upgrade domain isn t defined for 

========================= hadoop sample_8355 =========================

protected void serviceStop() throws Exception {	if (conn != null) {	
closing the hbase connection 

========================= hadoop sample_1174 =========================

while ((numRead = in.read(buf)) >= 0) {	out.write(buf, 0, numRead);	}	return fileBlock;	} catch (IOException e) {	closeQuietly(out);	out = null;	if (fileBlock != null) {	boolean b = fileBlock.delete();	if (!b) {	
ignoring failed delete 

private void handleServiceException(ServiceException e) throws IOException {	if (e.getCause() instanceof IOException) {	throw (IOException) e.getCause();	}	else {	if(LOG.isDebugEnabled()) {	
got serviceexception with error code and error message 

========================= hadoop sample_5971 =========================

public void setConf(Configuration conf) {	this.conf = conf;	keyFieldHelper = new KeyFieldHelper();	String keyFieldSeparator = conf.get(MRJobConfig.MAP_OUTPUT_KEY_FIELD_SEPERATOR, "\t");	keyFieldHelper.setKeyFieldSeparator(keyFieldSeparator);	if (conf.get("num.key.fields.for.partition") != null) {	
using deprecated num key fields for partition use mapreduce partition keypartitioner options instead 

========================= hadoop sample_5072 =========================

private void setupSingleLevelQueues(CapacitySchedulerConfiguration conf) {	conf.setQueues(CapacitySchedulerConfiguration.ROOT, new String[] {A, B});	final String Q_A = CapacitySchedulerConfiguration.ROOT + "." + A;	conf.setCapacity(Q_A, 30);	final String Q_B = CapacitySchedulerConfiguration.ROOT + "." + B;	conf.setCapacity(Q_B, 70);	
setup top level queues a and b 

root.assignContainers(clusterResource, node_0, new ResourceLimits(clusterResource), SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);	allocationOrder.verify(c).assignContainers(eq(clusterResource), any(PlacementSet.class), anyResourceLimits(), any(SchedulingMode.class));	applyAllocationToQueue(clusterResource, 2*GB, root);	root.assignContainers(clusterResource, node_0, new ResourceLimits(clusterResource), SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);	allocationOrder.verify(b).assignContainers(eq(clusterResource), any(PlacementSet.class), anyResourceLimits(), any(SchedulingMode.class));	applyAllocationToQueue(clusterResource, 2*GB, b);	verifyQueueMetrics(a, 1*GB, clusterResource);	verifyQueueMetrics(b, 6*GB, clusterResource);	verifyQueueMetrics(c, 3*GB, clusterResource);	reset(a); reset(b); reset(c);	
here 

========================= hadoop sample_506 =========================

public TestContainerManagerSecurity(String name, Configuration conf) {	
running test 

private void waitForContainerToFinishOnNM(ContainerId containerId) {	Context nmContext = yarnCluster.getNodeManager(0).getNMContext();	int interval = 4 * 60;	Assert.assertNotNull(nmContext.getContainers().containsKey(containerId));	Container waitContainer = nmContext.getContainers().get(containerId);	while ((interval-- > 0) && !waitContainer.cloneAndGetContainerStatus() .getState().equals(ContainerState.COMPLETE)) {	try {	
waiting for to complete 

private void testContainerToken(Configuration conf) throws IOException, InterruptedException, YarnException {	
running test for malice user 

private void testContainerTokenWithEpoch(Configuration conf) throws IOException, InterruptedException, YarnException {	
running test for serializing deserializing containerids 

========================= hadoop sample_1932 =========================

}	int maxSize = (int)Math.min( Runtime.getRuntime().maxMemory() * bufferPercent, Integer.MAX_VALUE);	int tmp = conf.getInt(JobContext.REDUCE_MARKRESET_BUFFER_SIZE, 0);	if (tmp >  0) {	maxSize = tmp;	}	memCache = new MemoryCache(maxSize);	fileCache = new FileCache(conf);	tid = taskid;	this.conf = conf;	
created a new backupstore with a memory of 

int i = 0;	Iterator<Segment<K,V>> itr = segmentList.iterator();	while (itr.hasNext()) {	Segment<K,V> s = itr.next();	if (i == readSegmentIndex) {	break;	}	s.close();	itr.remove();	i++;	
dropping a segment 

Segment<K,V> s = itr.next();	if (i == readSegmentIndex) {	break;	}	s.close();	itr.remove();	i++;	}	firstSegmentOffset = currentKVOffset;	readSegmentIndex = 0;	
setting the firssegmentoffset to 

s.reinitReader(firstSegmentOffset);	s.getReader().disableChecksumValidation();	}	}	}	currentKVOffset = firstSegmentOffset;	nextKVOffset = -1;	readSegmentIndex = 0;	hasMore = false;	lastSegmentEOF = false;	
reset first segment offset is segment list size is 

public void write(DataInputBuffer key, DataInputBuffer value) throws IOException {	int keyLength = key.getLength() - key.getPosition();	int valueLength = value.getLength() - value.getPosition();	WritableUtils.writeVInt(dataOut, keyLength);	WritableUtils.writeVInt(dataOut, valueLength);	dataOut.write(key.getData(), key.getPosition(), keyLength);	dataOut.write(value.getData(), value.getPosition(), valueLength);	usedSize += keyLength + valueLength + WritableUtils.getVIntSize(keyLength) + WritableUtils.getVIntSize(valueLength);	
id write to mem 

private Writer<K,V> createSpillFile() throws IOException {	Path tmp = new Path(MRJobConfig.OUTPUT + "/backup_" + tid.getId() + "_" + (spillNumber++) + ".out");	
created file 

public boolean reserve(int requestedSize, InputStream in) {	
reserve int inputstream not supported by backuprammanager 

public void unreserve(int requestedSize) {	availableSize += requestedSize;	
unreserving available 

========================= hadoop sample_4819 =========================

op.setTransactionId(txid++);	out.write(op);	}	out.close();	bkjm.finalizeLogSegment(start, (txid-1));	txid = 1;	try {	out = bkjm.startLogSegment(txid, NameNodeLayoutVersion.CURRENT_LAYOUT_VERSION);	fail("Shouldn't be able to start another journal from " + txid + " when one already exists");	} catch (Exception ioe) {	
caught exception as expected 

try {	out = bkjm.startLogSegment(txid, NameNodeLayoutVersion.CURRENT_LAYOUT_VERSION);	fail("Shouldn't be able to start another journal from " + txid + " when one already exists");	} catch (Exception ioe) {	}	txid = DEFAULT_SEGMENT_SIZE;	try {	out = bkjm.startLogSegment(txid, NameNodeLayoutVersion.CURRENT_LAYOUT_VERSION);	fail("Shouldn't be able to start another journal from " + txid + " when one already exists");	} catch (IOException ioe) {	
caught exception as expected 

long start = 1;	NamespaceInfo nsi = newNSInfo();	BookKeeperJournalManager bkjm1 = new BookKeeperJournalManager(conf, BKJMUtil.createJournalURI("/hdfsjournal-dualWriter"), nsi);	bkjm1.format(nsi);	BookKeeperJournalManager bkjm2 = new BookKeeperJournalManager(conf, BKJMUtil.createJournalURI("/hdfsjournal-dualWriter"), nsi);	EditLogOutputStream out1 = bkjm1.startLogSegment(start, NameNodeLayoutVersion.CURRENT_LAYOUT_VERSION);	try {	bkjm2.startLogSegment(start, NameNodeLayoutVersion.CURRENT_LAYOUT_VERSION);	fail("Shouldn't have been able to open the second writer");	} catch (IOException ioe) {	
caught exception as expected 

try {	for (long i = 1 ; i <= 3; i++) {	FSEditLogOp op = FSEditLogTestUtil.getNoOpInstance();	op.setTransactionId(txid++);	out.write(op);	}	out.setReadyToFlush();	out.flush();	fail("should not get to this stage");	} catch (IOException ioe) {	
error writing to bookkeeper 

bkjm.recoverUnfinalizedSegments();	out = bkjm.startLogSegment(txid, NameNodeLayoutVersion.CURRENT_LAYOUT_VERSION);	for (long i = 1 ; i <= 3; i++) {	FSEditLogOp op = FSEditLogTestUtil.getNoOpInstance();	op.setTransactionId(txid++);	out.write(op);	}	out.setReadyToFlush();	out.flush();	} catch (Exception e) {	
exception in test 

out.setReadyToFlush();	out.flush();	} catch (Exception e) {	throw e;	} finally {	if (replacementBookie != null) {	replacementBookie.shutdown();	}	newBookie.shutdown();	if (bkutil.checkBookiesUp(numBookies, 30) != numBookies) {	
not all bookies from this test shut down expect errors 

newBookie.shutdown();	assertEquals("New bookie didn't die", ensembleSize, bkutil.checkBookiesUp(ensembleSize, 10));	for (long i = 1 ; i <= 3; i++) {	FSEditLogOp op = FSEditLogTestUtil.getNoOpInstance();	op.setTransactionId(txid++);	out.write(op);	}	out.setReadyToFlush();	out.flush();	} catch (Exception e) {	
exception in test 

out.setReadyToFlush();	out.flush();	} catch (Exception e) {	throw e;	} finally {	if (replacementBookie != null) {	replacementBookie.shutdown();	}	newBookie.shutdown();	if (bkutil.checkBookiesUp(numBookies, 30) != numBookies) {	
not all bookies from this test shut down expect errors 

for (int i = 0; i < numThreads; i++) {	threads.add(new Callable<ThreadStatus>() {	public ThreadStatus call() {	BookKeeperJournalManager bkjm = null;	try {	bkjm = new BookKeeperJournalManager(conf, uri, nsi);	barrier.await();	bkjm.format(nsi);	return ThreadStatus.COMPLETED;	} catch (IOException ioe) {	
exception formatting 

public ThreadStatus call() {	BookKeeperJournalManager bkjm = null;	try {	bkjm = new BookKeeperJournalManager(conf, uri, nsi);	barrier.await();	bkjm.format(nsi);	return ThreadStatus.COMPLETED;	} catch (IOException ioe) {	return ThreadStatus.GOODEXCEPTION;	} catch (InterruptedException ie) {	
interrupted something is broken 

bkjm = new BookKeeperJournalManager(conf, uri, nsi);	barrier.await();	bkjm.format(nsi);	return ThreadStatus.COMPLETED;	} catch (IOException ioe) {	return ThreadStatus.GOODEXCEPTION;	} catch (InterruptedException ie) {	Thread.currentThread().interrupt();	return ThreadStatus.BADEXCEPTION;	} catch (Exception e) {	
some other bad exception 

} catch (InterruptedException ie) {	Thread.currentThread().interrupt();	return ThreadStatus.BADEXCEPTION;	} catch (Exception e) {	return ThreadStatus.BADEXCEPTION;	} finally {	if (bkjm != null) {	try {	bkjm.close();	} catch (IOException ioe) {	
error closing journal manager 

ExecutorService service = Executors.newFixedThreadPool(numThreads);	List<Future<ThreadStatus>> statuses = service.invokeAll(threads, 60, TimeUnit.SECONDS);	int numCompleted = 0;	for (Future<ThreadStatus> s : statuses) {	assertTrue(s.isDone());	assertTrue("Thread threw invalid exception", s.get() == ThreadStatus.COMPLETED || s.get() == ThreadStatus.GOODEXCEPTION);	if (s.get() == ThreadStatus.COMPLETED) {	numCompleted++;	}	}	
completed formats 

private void sleepBookie(final CountDownLatch l, final BookieServer bookie) throws Exception {	Thread sleeper = new Thread() {	public void run() {	try {	bookie.suspendProcessing();	l.await(60, TimeUnit.SECONDS);	bookie.resumeProcessing();	} catch (Exception e) {	
error suspending bookie 

========================= hadoop sample_7736 =========================

public void run() {	T event;	while (!stopped && !Thread.currentThread().isInterrupted()) {	try {	event = eventQueue.take();	} catch (InterruptedException e) {	
returning interrupted 

while (!stopped && !Thread.currentThread().isInterrupted()) {	try {	event = eventQueue.take();	} catch (InterruptedException e) {	return;	}	try {	handler.handle(event);	} catch (Throwable t) {	if (stopped) {	
exception during shutdown 

event = eventQueue.take();	} catch (InterruptedException e) {	return;	}	try {	handler.handle(event);	} catch (Throwable t) {	if (stopped) {	break;	}	
error in handling event type to the event dispatcher 

} catch (InterruptedException e) {	return;	}	try {	handler.handle(event);	} catch (Throwable t) {	if (stopped) {	break;	}	if (shouldExitOnError && !ShutdownHookManager.get().isShutdownInProgress()) {	
exiting bbye 

public void handle(T event) {	try {	int qSize = eventQueue.size();	if (qSize !=0 && qSize %1000 == 0) {	
size of event queue is 

public void handle(T event) {	try {	int qSize = eventQueue.size();	if (qSize !=0 && qSize %1000 == 0) {	}	int remCapacity = eventQueue.remainingCapacity();	if (remCapacity < 1000) {	
very low remaining capacity on event queue 

public void handle(T event) {	try {	int qSize = eventQueue.size();	if (qSize !=0 && qSize %1000 == 0) {	}	int remCapacity = eventQueue.remainingCapacity();	if (remCapacity < 1000) {	}	this.eventQueue.put(event);	} catch (InterruptedException e) {	
interrupted trying to exit gracefully 

========================= hadoop sample_2534 =========================

if(blockId == null) {	out.println("Please provide valid blockId!");	return;	}	BlockManager bm = namenode.getNamesystem().getBlockManager();	try {	Block block = new Block(Block.getBlockId(blockId));	BlockInfo blockInfo = bm.getStoredBlock(block);	if(blockInfo == null) {	out.println("Block "+ blockId +" " + NONEXISTENT_STATUS);	
block 

} else {	out.print(HEALTHY_STATUS);	}	out.print("\n");	}	} catch (Exception e){	String errMsg = "Fsck on blockId '" + blockId;	LOG.warn(errMsg, e);	out.println(e.getMessage());	out.print("\n\n" + errMsg);	
error in looking up block 

}	report.append('\n');	blockNumber++;	}	if ((missing > 0) || (corrupt > 0)) {	if (!showFiles && (missing > 0)) {	out.print("\n" + path + ": MISSING " + missing + " blocks of total size " + missize + " B.");	}	res.corruptFiles++;	if (isOpen) {	
fsck ignoring open file 

private void deleteCorruptedFile(String path) {	try {	namenode.getRpcServer().delete(path, true);	
fsck deleted corrupt file 

private void deleteCorruptedFile(String path) {	try {	namenode.getRpcServer().delete(path, true);	} catch (Exception e) {	
fsck error deleting corrupted file 

OutputStream fos = null;	try {	if (!lfInited) {	lostFoundInit(dfs);	}	if (!lfInitedOk) {	throw new IOException("failed to initialize lost+found");	}	String target = lostFound + fullName;	if (hdfsPathExists(target)) {	
fsck can t copy the remains of to lost found because already exists 

}	continue;	}	if (fos == null) {	fos = dfs.create(target + "/" + chain, true);	chain++;	}	try {	copyBlock(dfs, lblock, fos);	} catch (Exception e) {	
fsck could not copy block to 

copyBlock(dfs, lblock, fos);	} catch (Exception e) {	fos.flush();	fos.close();	fos = null;	internalError = true;	copyError = true;	}	}	if (copyError) {	
fsck there were errors copying the remains of the corrupted file to lost found 

} catch (Exception e) {	fos.flush();	fos.close();	fos = null;	internalError = true;	copyError = true;	}	}	if (copyError) {	} else {	
fsck copied the remains of the corrupted file to lost found 

fos.close();	fos = null;	internalError = true;	copyError = true;	}	}	if (copyError) {	} else {	}	} catch (Exception e) {	
copyblockstolostfound error processing 

ExtendedBlock block = lblock.getBlock();	while (blockReader == null) {	DatanodeInfo chosenNode;	try {	chosenNode = bestNode(dfs, lblock.getLocations(), deadNodes);	targetAddr = NetUtils.createSocketAddr(chosenNode.getXferAddr());	}  catch (IOException ie) {	if (failures >= HdfsClientConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {	throw new IOException("Could not obtain block " + lblock, ie);	}	
could not obtain block from any node 

peer = DFSUtilClient.peerFromSocketAndKey( dfs.getSaslDataTransferClient(), s, NamenodeFsck.this, blockToken, datanodeId, HdfsConstants.READ_TIMEOUT);	} finally {	if (peer == null) {	IOUtils.closeQuietly(s);	}	}	return peer;	}	}). build();	}  catch (IOException ex) {	
failed to connect to 

long bytesRead = 0;	try {	while ((cnt = blockReader.read(buf, 0, buf.length)) > 0) {	fos.write(buf, 0, cnt);	bytesRead += cnt;	}	if ( bytesRead != block.getNumBytes() ) {	throw new IOException("Recorded block size is " + block.getNumBytes() + ", but datanode returned " +bytesRead+" bytes");	}	} catch (Exception e) {	
error reading block 

private void lostFoundInit(DFSClient dfs) {	lfInited = true;	try {	String lfName = "/lost+found";	final HdfsFileStatus lfStatus = dfs.getFileInfo(lfName);	if (lfStatus == null) {	lfInitedOk = dfs.mkdirs(lfName, null, true);	lostFound = lfName;	} else if (!lfStatus.isDir()) {	
cannot use lost found a regular file with this name exists 

lfInitedOk = false;	}  else {	lostFound = lfName;	lfInitedOk = true;	}	}  catch (Exception e) {	e.printStackTrace();	lfInitedOk = false;	}	if (lostFound == null) {	
cannot initialize lost found 

========================= hadoop sample_8093 =========================

public void testMaintenanceNodes() throws Exception {	
starting testmaintenancenodes 

FSNamesystem fsn = cluster.getNameNode().namesystem;	MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();	ObjectName mxbeanName = new ObjectName( "Hadoop:service=NameNode,name=NameNodeInfo");	List<String> hosts = new ArrayList<>();	for(DataNode dn : cluster.getDataNodes()) {	hosts.add(dn.getDisplayName());	}	hostsFileWriter.initIncludeHosts(hosts.toArray( new String[hosts.size()]));	fsn.getBlockManager().getDatanodeManager().refreshNodes(conf);	String liveNodesInfo = (String) (mbs.getAttribute(mxbeanName, "LiveNodes"));	
live nodes 

}	Map<String, Long> maintenanceNodes = new HashMap<>();	maintenanceNodes.put(cluster.getDataNodes().get(0).getDisplayName(), Time.now() + expirationInMs);	hostsFileWriter.initOutOfServiceHosts(null, maintenanceNodes);	fsn.getBlockManager().getDatanodeManager().refreshNodes(conf);	boolean recheck = true;	while (recheck) {	String enteringMaintenanceNodesInfo = (String) (mbs.getAttribute(mxbeanName, "EnteringMaintenanceNodes"));	Map<String, Map<String, Object>> enteringMaintenanceNodes = (Map<String, Map<String, Object>>) JSON.parse( enteringMaintenanceNodesInfo);	if (enteringMaintenanceNodes.size() <= 0) {	
waiting for a node to enter maintenance state 

hostsFileWriter.initOutOfServiceHosts(null, maintenanceNodes);	fsn.getBlockManager().getDatanodeManager().refreshNodes(conf);	boolean recheck = true;	while (recheck) {	String enteringMaintenanceNodesInfo = (String) (mbs.getAttribute(mxbeanName, "EnteringMaintenanceNodes"));	Map<String, Map<String, Object>> enteringMaintenanceNodes = (Map<String, Map<String, Object>>) JSON.parse( enteringMaintenanceNodesInfo);	if (enteringMaintenanceNodes.size() <= 0) {	Uninterruptibles.sleepUninterruptibly(1, TimeUnit.SECONDS);	continue;	}	
nodes entering maintenance 

========================= hadoop sample_7366 =========================

File nameAndEditsDir2 = new File(base_dir, "name_and_edits2");	File nameDir = new File(base_dir, "name");	try {	Configuration conf = new HdfsConfiguration();	conf.set(DFSConfigKeys.DFS_NAMENODE_NAME_DIR_KEY, nameDir.getAbsolutePath());	conf.set( DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_REQUIRED_KEY, nameAndEditsDir2.toURI().toString());	conf.set( DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_KEY, nameAndEditsDir.toURI().toString());	cluster = new MiniDFSCluster.Builder(conf) .numDataNodes(NUM_DATA_NODES) .manageNameDfsDirs(false) .build();	fail("Successfully started cluster but should not have been able to.");	} catch (IllegalArgumentException iae) {	
expected cluster start failed due to bad configuration 

cluster.shutdown();	}	conf = new HdfsConfiguration();	conf.set(DFSConfigKeys.DFS_NAMENODE_NAME_DIR_KEY, nameOnlyDir.getPath() + "," + nameAndEditsDir.getPath());	conf.set(DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_KEY, nameAndEditsDir.getPath());	replication = (short)conf.getInt(DFSConfigKeys.DFS_REPLICATION_KEY, 3);	try {	cluster = new MiniDFSCluster.Builder(conf) .numDataNodes(NUM_DATA_NODES) .format(false) .manageNameDfsDirs(false) .build();	fail("Successfully started cluster but should not have been able to.");	} catch (IOException e) {	
expected cluster start failed due to missing latest edits dir 

========================= hadoop sample_7389 =========================

public void readAndCheckEOS(BlockReader reader, int length, boolean expectEof) throws IOException {	byte buf[] = new byte[1024];	int nRead = 0;	while (nRead < length) {	
so far read going to read more 

public void readAndCheckEOS(BlockReader reader, int length, boolean expectEof) throws IOException {	byte buf[] = new byte[1024];	int nRead = 0;	while (nRead < length) {	int n = reader.read(buf, 0, buf.length);	assertTrue(n > 0);	nRead += n;	}	if (expectEof) {	
done reading expect eof for next read 

========================= hadoop sample_7673 =========================

StringBuilder dnNewDataDirs = new StringBuilder();	for (File newVol: newVols) {	if (dnNewDataDirs.length() > 0) {	dnNewDataDirs.append(',');	}	dnNewDataDirs.append(newVol.getAbsolutePath());	}	try {	assertThat( dn.reconfigurePropertyImpl( DFSConfigKeys.DFS_DATANODE_DATA_DIR_KEY, dnNewDataDirs.toString()), is(dn.getConf().get(DFSConfigKeys.DFS_DATANODE_DATA_DIR_KEY)));	} catch (ReconfigurationException e) {	
could not reconfigure datanode 

public static void waitForDiskError(final DataNode dn, FsVolumeSpi volume) throws Exception {	
starting to wait for datanode to detect disk failure 

========================= hadoop sample_7223 =========================

public void testNativeCodeLoaded() {	if (requireTestJni() == false) {	
testnativecodeloader libhadoop so testing is not required 

}	assertFalse(NativeCodeLoader.getLibraryName().isEmpty());	assertFalse(ZlibFactory.getLibraryName().isEmpty());	if (NativeCodeLoader.buildSupportsSnappy()) {	assertFalse(SnappyCodec.getLibraryName().isEmpty());	}	if (NativeCodeLoader.buildSupportsOpenssl()) {	assertFalse(OpensslCipher.getLibraryName().isEmpty());	}	assertFalse(Lz4Codec.getLibraryName().isEmpty());	
testnativecodeloader libhadoop so is loaded 

========================= hadoop sample_2922 =========================

protected ResourceScheduler createScheduler() {	String schedulerClassName = conf.get(YarnConfiguration.RM_SCHEDULER, YarnConfiguration.DEFAULT_RM_SCHEDULER);	
using scheduler 

protected ReservationSystem createReservationSystem() {	String reservationClassName = conf.get(YarnConfiguration.RM_RESERVATION_SYSTEM_CLASS, AbstractReservationSystem.getDefaultReservationSystem(scheduler));	if (reservationClassName == null) {	return null;	}	
using reservationsystem 

protected SystemMetricsPublisher createSystemMetricsPublisher() {	SystemMetricsPublisher publisher;	if (YarnConfiguration.timelineServiceEnabled(conf) && YarnConfiguration.systemMetricsPublisherEnabled(conf)) {	if (YarnConfiguration.timelineServiceV2Enabled(conf)) {	
system metrics publisher with the timeline service is configured 

protected SystemMetricsPublisher createSystemMetricsPublisher() {	SystemMetricsPublisher publisher;	if (YarnConfiguration.timelineServiceEnabled(conf) && YarnConfiguration.systemMetricsPublisherEnabled(conf)) {	if (YarnConfiguration.timelineServiceV2Enabled(conf)) {	publisher = new TimelineServiceV2Publisher( rmContext.getRMTimelineCollectorManager());	} else {	
system metrics publisher with the timeline service is configured 

protected SystemMetricsPublisher createSystemMetricsPublisher() {	SystemMetricsPublisher publisher;	if (YarnConfiguration.timelineServiceEnabled(conf) && YarnConfiguration.systemMetricsPublisherEnabled(conf)) {	if (YarnConfiguration.timelineServiceV2Enabled(conf)) {	publisher = new TimelineServiceV2Publisher( rmContext.getRMTimelineCollectorManager());	} else {	publisher = new TimelineServiceV1Publisher();	}	} else {	
timelineservicepublisher is not configured 

boolean isWorkPreservingRecoveryEnabled = conf.getBoolean( YarnConfiguration.RM_WORK_PRESERVING_RECOVERY_ENABLED, YarnConfiguration.DEFAULT_RM_WORK_PRESERVING_RECOVERY_ENABLED);	rmContext .setWorkPreservingRecoveryEnabled(isWorkPreservingRecoveryEnabled);	} else {	rmStore = new NullRMStateStore();	}	try {	rmStore.setResourceManager(rm);	rmStore.init(conf);	rmStore.setRMDispatcher(rmDispatcher);	} catch (Exception e) {	
failed to init state store 

}	JvmPauseMonitor pauseMonitor = new JvmPauseMonitor();	addService(pauseMonitor);	jvmMetrics.setPauseMonitor(pauseMonitor);	if (conf.getBoolean(YarnConfiguration.RM_RESERVATION_SYSTEM_ENABLE, YarnConfiguration.DEFAULT_RM_RESERVATION_SYSTEM_ENABLE)) {	reservationSystem = createReservationSystem();	if (reservationSystem != null) {	reservationSystem.setRMContext(rmContext);	addIfService(reservationSystem);	rmContext.setReservationSystem(reservationSystem);	
initialized reservation system 

}	if(HAUtil.isFederationEnabled(conf)) {	String cId = YarnConfiguration.getClusterId(conf);	if (cId.isEmpty()) {	String errMsg = "Cannot initialize RM as Federation is enabled" + " but cluster id is not configured.";	LOG.error(errMsg);	throw new YarnRuntimeException(errMsg);	}	federationStateStoreService = createFederationStateStoreService();	addIfService(federationStateStoreService);	
initialized federation membership 

protected void serviceStart() throws Exception {	RMStateStore rmStore = rmContext.getStateStore();	rmStore.start();	if(recoveryEnabled) {	try {	
recovery started 

RMStateStore rmStore = rmContext.getStateStore();	rmStore.start();	if(recoveryEnabled) {	try {	rmStore.checkVersion();	if (rmContext.isWorkPreservingRecoveryEnabled()) {	rmContext.setEpoch(rmStore.getAndIncrementEpoch());	}	RMState state = rmStore.loadState();	recover(state);	
recovery ended 

rmStore.start();	if(recoveryEnabled) {	try {	rmStore.checkVersion();	if (rmContext.isWorkPreservingRecoveryEnabled()) {	rmContext.setEpoch(rmStore.getAndIncrementEpoch());	}	RMState state = rmStore.loadState();	recover(state);	} catch (Exception e) {	
failed to load recover state 

}	RMState state = rmStore.loadState();	recover(state);	} catch (Exception e) {	throw e;	}	} else {	if (HAUtil.isFederationEnabled(conf)) {	long epoch = conf.getLong(YarnConfiguration.RM_EPOCH, YarnConfiguration.DEFAULT_RM_EPOCH);	rmContext.setEpoch(epoch);	
epoch set for federation 

protected void serviceStop() throws Exception {	super.serviceStop();	DefaultMetricsSystem.shutdown();	if (rmContext != null) {	RMStateStore store = rmContext.getStateStore();	try {	if (null != store) {	store.close();	}	} catch (Exception e) {	
error closing store 

protected void createSchedulerMonitors() {	if (conf.getBoolean(YarnConfiguration.RM_SCHEDULER_ENABLE_MONITORS, YarnConfiguration.DEFAULT_RM_SCHEDULER_ENABLE_MONITORS)) {	
loading policy monitors 

protected void createSchedulerMonitors() {	if (conf.getBoolean(YarnConfiguration.RM_SCHEDULER_ENABLE_MONITORS, YarnConfiguration.DEFAULT_RM_SCHEDULER_ENABLE_MONITORS)) {	List<SchedulingEditPolicy> policies = conf.getInstances( YarnConfiguration.RM_SCHEDULER_MONITOR_POLICIES, SchedulingEditPolicy.class);	if (policies.size() > 0) {	for (SchedulingEditPolicy policy : policies) {	
loading schedulingeditpolicy 

protected void createSchedulerMonitors() {	if (conf.getBoolean(YarnConfiguration.RM_SCHEDULER_ENABLE_MONITORS, YarnConfiguration.DEFAULT_RM_SCHEDULER_ENABLE_MONITORS)) {	List<SchedulingEditPolicy> policies = conf.getInstances( YarnConfiguration.RM_SCHEDULER_MONITOR_POLICIES, SchedulingEditPolicy.class);	if (policies.size() > 0) {	for (SchedulingEditPolicy policy : policies) {	SchedulingMonitor mon = new SchedulingMonitor(rmContext, policy);	addService(mon);	}	} else {	
policy monitors configured but none specified 

public void handle(RMFatalEvent event) {	
received 

public void handle(RMFatalEvent event) {	if (HAUtil.isHAEnabled(getConfig())) {	
transitioning the resource manager to standby 

public void handle(RMFatalEvent event) {	if (HAUtil.isHAEnabled(getConfig())) {	handleTransitionToStandByInNewThread();	} else {	switch(event.getType()) {	
state store fenced even though the resource manager is not configured for high availability shutting down this resource manager to protect the integrity of the state store 

public void handle(RMFatalEvent event) {	if (HAUtil.isHAEnabled(getConfig())) {	handleTransitionToStandByInNewThread();	} else {	switch(event.getType()) {	ExitUtil.terminate(1, event.getExplanation());	break;	case STATE_STORE_OP_FAILED: if (YarnConfiguration.shouldRMFailFast(getConfig())) {	
shutting down the resource manager because a state store operation failed and the resource manager is configured to fail fast see the yarn fail fast and yarn resourcemanager fail fast properties 

public void handle(RMFatalEvent event) {	if (HAUtil.isHAEnabled(getConfig())) {	handleTransitionToStandByInNewThread();	} else {	switch(event.getType()) {	ExitUtil.terminate(1, event.getExplanation());	break;	case STATE_STORE_OP_FAILED: if (YarnConfiguration.shouldRMFailFast(getConfig())) {	ExitUtil.terminate(1, event.getExplanation());	} else {	
ignoring state store operation failure because the resource manager is not configured to fail fast see the yarn fail fast and yarn resourcemanager fail fast properties 

handleTransitionToStandByInNewThread();	} else {	switch(event.getType()) {	ExitUtil.terminate(1, event.getExplanation());	break;	case STATE_STORE_OP_FAILED: if (YarnConfiguration.shouldRMFailFast(getConfig())) {	ExitUtil.terminate(1, event.getExplanation());	} else {	}	break;	
shutting down the resource manager 

public void run() {	if (hasAlreadyRun.getAndSet(true)) {	return;	}	if (rmContext.isHAEnabled()) {	try {	
transitioning rm to standby mode 

return;	}	if (rmContext.isHAEnabled()) {	try {	transitionToStandby(true);	EmbeddedElector elector = rmContext.getLeaderElectorService();	if (elector != null) {	elector.rejoinElection();	}	} catch (Exception e) {	
failed to transition rm to standby mode 

public void handle(RMAppEvent event) {	ApplicationId appID = event.getApplicationId();	RMApp rmApp = this.rmContext.getRMApps().get(appID);	if (rmApp != null) {	try {	rmApp.handle(event);	} catch (Throwable t) {	
error in handling event type for application 

public void handle(RMAppAttemptEvent event) {	ApplicationAttemptId appAttemptID = event.getApplicationAttemptId();	ApplicationId appAttemptId = appAttemptID.getApplicationId();	RMApp rmApp = this.rmContext.getRMApps().get(appAttemptId);	if (rmApp != null) {	RMAppAttempt rmAppAttempt = rmApp.getRMAppAttempt(appAttemptID);	if (rmAppAttempt != null) {	try {	rmAppAttempt.handle(event);	} catch (Throwable t) {	
error in handling event type for applicationattempt 

RMAppAttempt rmAppAttempt = rmApp.getRMAppAttempt(appAttemptID);	if (rmAppAttempt != null) {	try {	rmAppAttempt.handle(event);	} catch (Throwable t) {	}	} else if (rmApp.getApplicationSubmissionContext() != null && rmApp.getApplicationSubmissionContext() .getKeepContainersAcrossApplicationAttempts() && event.getType() == RMAppAttemptEventType.CONTAINER_FINISHED) {	RMAppAttempt previousFailedAttempt = rmApp.getAppAttempts().values().iterator().next();	if (previousFailedAttempt != null) {	try {	
event handled by 

try {	rmAppAttempt.handle(event);	} catch (Throwable t) {	}	} else if (rmApp.getApplicationSubmissionContext() != null && rmApp.getApplicationSubmissionContext() .getKeepContainersAcrossApplicationAttempts() && event.getType() == RMAppAttemptEventType.CONTAINER_FINISHED) {	RMAppAttempt previousFailedAttempt = rmApp.getAppAttempts().values().iterator().next();	if (previousFailedAttempt != null) {	try {	previousFailedAttempt.handle(event);	} catch (Throwable t) {	
error in handling event type for applicationattempt with 

} catch (Throwable t) {	}	} else if (rmApp.getApplicationSubmissionContext() != null && rmApp.getApplicationSubmissionContext() .getKeepContainersAcrossApplicationAttempts() && event.getType() == RMAppAttemptEventType.CONTAINER_FINISHED) {	RMAppAttempt previousFailedAttempt = rmApp.getAppAttempts().values().iterator().next();	if (previousFailedAttempt != null) {	try {	previousFailedAttempt.handle(event);	} catch (Throwable t) {	}	} else {	
event not handled because previousfailedattempt is null 

public void handle(RMNodeEvent event) {	NodeId nodeId = event.getNodeId();	RMNode node = this.rmContext.getRMNodes().get(nodeId);	if (node != null) {	try {	((EventHandler<RMNodeEvent>) node).handle(event);	} catch (Throwable t) {	
error in handling event type for node 

public static HttpServer2.Builder httpServerTemplateForRM(Configuration conf, final InetSocketAddress httpAddr, final InetSocketAddress httpsAddr, String name) throws IOException {	HttpServer2.Builder builder = new HttpServer2.Builder().setName(name) .setConf(conf).setSecurityEnabled(false);	if (httpAddr.getPort() == 0) {	builder.setFindPort(true);	}	URI uri = URI.create("http: builder.addEndpoint(uri);	
starting web server for at 

String war = "hadoop-yarn-ui-" + VersionInfo.getVersion() + ".war";	URLClassLoader cl = (URLClassLoader) ClassLoader.getSystemClassLoader();	URL url = cl.findResource(war);	if (null == url) {	onDiskPath = getWebAppsPath("ui2");	} else {	onDiskPath = url.getFile();	}	}	if (onDiskPath == null || onDiskPath.isEmpty()) {	
no war file or webapps found for 

if (null == url) {	onDiskPath = getWebAppsPath("ui2");	} else {	onDiskPath = url.getFile();	}	}	if (onDiskPath == null || onDiskPath.isEmpty()) {	} else {	if (onDiskPath.endsWith(".war")) {	uiWebAppContext.setWar(onDiskPath);	
using war file at 

} else {	onDiskPath = url.getFile();	}	}	if (onDiskPath == null || onDiskPath.isEmpty()) {	} else {	if (onDiskPath.endsWith(".war")) {	uiWebAppContext.setWar(onDiskPath);	} else {	uiWebAppContext.setResourceBase(onDiskPath);	
using webapps at 

} else {	printUsage(System.err);	}	} else {	ResourceManager resourceManager = new ResourceManager();	ShutdownHookManager.get().addShutdownHook( new CompositeServiceShutdownHook(resourceManager), SHUTDOWN_HOOK_PRIORITY);	resourceManager.init(conf);	resourceManager.start();	}	} catch (Throwable t) {	
error starting resourcemanager 

static void deleteRMStateStore(Configuration conf) throws Exception {	RMStateStore rmStore = RMStateStoreFactory.getStore(conf);	rmStore.setResourceManager(new ResourceManager());	rmStore.init(conf);	rmStore.start();	try {	
deleting resourcemanager state store 

static void deleteRMStateStore(Configuration conf) throws Exception {	RMStateStore rmStore = RMStateStoreFactory.getStore(conf);	rmStore.setResourceManager(new ResourceManager());	rmStore.init(conf);	rmStore.start();	try {	rmStore.deleteStore();	
state store deleted 

static void removeApplication(Configuration conf, String applicationId) throws Exception {	RMStateStore rmStore = RMStateStoreFactory.getStore(conf);	rmStore.setResourceManager(new ResourceManager());	rmStore.init(conf);	rmStore.start();	try {	ApplicationId removeAppId = ApplicationId.fromString(applicationId);	
deleting application from state store 

static void removeApplication(Configuration conf, String applicationId) throws Exception {	RMStateStore rmStore = RMStateStoreFactory.getStore(conf);	rmStore.setResourceManager(new ResourceManager());	rmStore.init(conf);	rmStore.start();	try {	ApplicationId removeAppId = ApplicationId.fromString(applicationId);	rmStore.removeApplication(removeAppId);	
application is deleted from state store 

========================= hadoop sample_1123 =========================

private void handleAdministerException(Exception e, String user, String queue, String operation) {	
got exception while killing app as the enemy 

private void registerNode(String host, int memory, int vCores) throws Exception {	try {	resourceManager.registerNode(host, memory, vCores);	int attempts = 10;	Collection<Plan> plans;	do {	resourceManager.drainEvents();	
waiting for node capacity to be added to plan 

========================= hadoop sample_414 =========================

setTitle("Applications");	TBODY<TABLE<Hamlet>> tbody = html.table("#apps").thead() .tr() .th(".id", "ID") .th(".user", "User") .th(".name", "Name") .th(".type", "Application Type") .th(".queue", "Queue") .th(".priority", "Application Priority") .th(".starttime", "StartTime") .th(".finishtime", "FinishTime") .th(".state", "State") .th(".finalstatus", "FinalStatus") .th(".progress", "Progress") .th(".ui", "Tracking UI") ._()._().tbody();	StringBuilder appsTableData = new StringBuilder("[\n");	for (AppInfo app : apps.getApps()) {	try {	String percent = String.format("%.1f", app.getProgress() * 100.0F);	String trackingURL = app.getTrackingUrl() == null ? "#" : app.getTrackingUrl();	appsTableData.append("[\"") .append("<a href='").append(trackingURL).append("'>") .append(app.getAppId()).append("</a>\",\"") .append(escape(app.getUser())).append("\",\"") .append(escape(app.getName())).append("\",\"") .append(escape(app.getApplicationType())).append("\",\"") .append(escape(app.getQueue())).append("\",\"") .append(String.valueOf(app.getPriority())).append("\",\"") .append(app.getStartTime()).append("\",\"") .append(app.getFinishTime()).append("\",\"") .append(app.getState()).append("\",\"") .append(app.getFinalStatus()).append("\",\"") .append("<br title='").append(percent).append("'> <div class='") .append(C_PROGRESSBAR).append("' title='") .append(join(percent, '%')).append("'> ").append("<div class='") .append(C_PROGRESSBAR_VALUE).append("' style='") .append(join("width:", percent, '%')).append("'> </div> </div>") .append("\",\"<a href='").append(trackingURL).append("'>") .append("History").append("</a>");	appsTableData.append("\"],\n");	} catch (Exception e) {	
cannot add application 

========================= hadoop sample_1965 =========================

public void testNNClearsCommandsOnFailoverWithReplChanges() throws Exception {	DFSTestUtil.createFile(fs, TEST_FILE_PATH, 30*SMALL_BLOCK, (short)1, 1L);	banner("rolling NN1's edit log, forcing catch-up");	HATestUtil.waitForStandbyToCatchUp(nn1, nn2);	nn1.getRpcServer().setReplication(TEST_FILE, (short) 2);	while (BlockManagerTestUtil.getComputedDatanodeWork( nn1.getNamesystem().getBlockManager()) > 0) {	
getting more replication work computed 

assertEquals(numQueued, cluster.getNameNode(1).getNamesystem(). getPendingDataNodeMessageCount());	try {	out = fs.append(TEST_FILE_PATH);	AppendTestUtil.write(out, 20, 10);	} finally {	IOUtils.closeStream(out);	numQueued += numDN;	}	cluster.triggerBlockReports();	numQueued += numDN;	
expect and got 

========================= hadoop sample_7448 =========================

try {	Job job = Job.getInstance(new Configuration());	Assert.assertEquals(null, CopyOutputFormat.getCommitDirectory(job));	job.getConfiguration().set(DistCpConstants.CONF_LABEL_TARGET_FINAL_PATH, "");	Assert.assertEquals(null, CopyOutputFormat.getCommitDirectory(job));	Path directory = new Path("/tmp/test");	CopyOutputFormat.setCommitDirectory(job, directory);	Assert.assertEquals(directory, CopyOutputFormat.getCommitDirectory(job));	Assert.assertEquals(directory.toString(), job.getConfiguration(). get(DistCpConstants.CONF_LABEL_TARGET_FINAL_PATH));	} catch (IOException e) {	
exception encountered while running test 

try {	Job job = Job.getInstance(new Configuration());	Assert.assertEquals(null, CopyOutputFormat.getWorkingDirectory(job));	job.getConfiguration().set(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH, "");	Assert.assertEquals(null, CopyOutputFormat.getWorkingDirectory(job));	Path directory = new Path("/tmp/test");	CopyOutputFormat.setWorkingDirectory(job, directory);	Assert.assertEquals(directory, CopyOutputFormat.getWorkingDirectory(job));	Assert.assertEquals(directory.toString(), job.getConfiguration(). get(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH));	} catch (IOException e) {	
exception encountered while running test 

public void testGetOutputCommitter() {	try {	TaskAttemptContext context = new TaskAttemptContextImpl(new Configuration(), new TaskAttemptID("200707121733", 1, TaskType.MAP, 1, 1));	context.getConfiguration().set("mapred.output.dir", "/out");	Assert.assertTrue(new CopyOutputFormat().getOutputCommitter(context) instanceof CopyCommitter);	} catch (IOException e) {	
exception encountered 

} catch (IllegalStateException ignore) { }	CopyOutputFormat.setWorkingDirectory(job, new Path("/tmp/work"));	CopyOutputFormat.setCommitDirectory(job, new Path("/tmp/commit"));	try {	JobContext context = new JobContextImpl(job.getConfiguration(), jobID);	outputFormat.checkOutputSpecs(context);	} catch (IllegalStateException ignore) {	Assert.fail("Output spec check failed.");	}	} catch (IOException e) {	
exception encountered while testing checkoutput specs 

CopyOutputFormat.setCommitDirectory(job, new Path("/tmp/commit"));	try {	JobContext context = new JobContextImpl(job.getConfiguration(), jobID);	outputFormat.checkOutputSpecs(context);	} catch (IllegalStateException ignore) {	Assert.fail("Output spec check failed.");	}	} catch (IOException e) {	Assert.fail("Checkoutput Spec failure");	} catch (InterruptedException e) {	
exception encountered while testing checkoutput specs 

========================= hadoop sample_6259 =========================

filesAndEmptyDirectories++;	}	}	final RemoteIterator<LocatedFileStatus> statusIterator = fs.listFiles(baseTestDir, recursive);	final Collection<Path> listedFiles = new HashSet<>();	for (; statusIterator.hasNext();) {	final FileStatus status = statusIterator.next();	assertTrue("FileStatus " + status + " is not a file!", status.isFile());	listedFiles.add(status.getPath());	}	
listfiles 

========================= hadoop sample_5926 =========================

ugi = UserGroupInformation.createRemoteUser(realUser, handlerAuthMethod);	String doAsUser = getDoAs(request);	if (doAsUser != null) {	ugi = UserGroupInformation.createProxyUser(doAsUser, ugi);	try {	ProxyUsers.authorize(ugi, request.getRemoteAddr());	} catch (AuthorizationException ex) {	HttpExceptionUtils.createServletExceptionResponse(response, HttpServletResponse.SC_FORBIDDEN, ex);	requestCompleted = true;	if (LOG.isDebugEnabled()) {	
authentication exception 

String doAsUser = getDoAs(request);	if (doAsUser != null) {	ugi = UserGroupInformation.createProxyUser(doAsUser, ugi);	try {	ProxyUsers.authorize(ugi, request.getRemoteAddr());	} catch (AuthorizationException ex) {	HttpExceptionUtils.createServletExceptionResponse(response, HttpServletResponse.SC_FORBIDDEN, ex);	requestCompleted = true;	if (LOG.isDebugEnabled()) {	} else {	
authentication exception 

========================= hadoop sample_3722 =========================

Duration d = new Duration();	String result = SwiftTestUtils.readBytesToString(fs, p, name.length());	assertEquals(name, result);	d.finished();	readStats.add(d);	}	SwiftTestUtils.noteAction("Beginning delete");	Duration rm2 = new Duration();	fs.delete(dir, true);	rm2.finished();	
filesystem s 

assertEquals(name, result);	d.finished();	readStats.add(d);	}	SwiftTestUtils.noteAction("Beginning delete");	Duration rm2 = new Duration();	fs.delete(dir, true);	rm2.finished();	LOG.info(writeStats.toString());	LOG.info(readStats.toString());	
d d 

assertEquals(name, result);	d.finished();	readStats.add(d);	}	SwiftTestUtils.noteAction("Beginning delete");	Duration rm2 = new Duration();	fs.delete(dir, true);	rm2.finished();	LOG.info(writeStats.toString());	LOG.info(readStats.toString());	
d d 

========================= hadoop sample_6170 =========================

public void run() {	StopWatch sw = new StopWatch();	Map<String, GcTimes> gcTimesBeforeSleep = getGcTimes();	
starting jvm pause monitor 

========================= hadoop sample_3688 =========================

String fileName = jhFileName.substring(0, jhFileName.indexOf(JobHistoryUtils.JOB_HISTORY_FILE_EXTENSION));	JobIndexInfo indexInfo = new JobIndexInfo();	String[] jobDetails = fileName.split(DELIMITER);	JobID oldJobId = JobID.forName(decodeJobHistoryFileName(jobDetails[JOB_ID_INDEX]));	JobId jobId = TypeConverter.toYarn(oldJobId);	indexInfo.setJobId(jobId);	try {	try {	indexInfo.setSubmitTime(Long.parseLong( decodeJobHistoryFileName(jobDetails[SUBMIT_TIME_INDEX])));	} catch (NumberFormatException e) {	
unable to parse submit time from job history file 

try {	try {	indexInfo.setSubmitTime(Long.parseLong( decodeJobHistoryFileName(jobDetails[SUBMIT_TIME_INDEX])));	} catch (NumberFormatException e) {	}	indexInfo.setUser( decodeJobHistoryFileName(jobDetails[USER_INDEX]));	indexInfo.setJobName( decodeJobHistoryFileName(jobDetails[JOB_NAME_INDEX]));	try {	indexInfo.setFinishTime(Long.parseLong( decodeJobHistoryFileName(jobDetails[FINISH_TIME_INDEX])));	} catch (NumberFormatException e) {	
unable to parse finish time from job history file 

}	indexInfo.setUser( decodeJobHistoryFileName(jobDetails[USER_INDEX]));	indexInfo.setJobName( decodeJobHistoryFileName(jobDetails[JOB_NAME_INDEX]));	try {	indexInfo.setFinishTime(Long.parseLong( decodeJobHistoryFileName(jobDetails[FINISH_TIME_INDEX])));	} catch (NumberFormatException e) {	}	try {	indexInfo.setNumMaps(Integer.parseInt( decodeJobHistoryFileName(jobDetails[NUM_MAPS_INDEX])));	} catch (NumberFormatException e) {	
unable to parse num maps from job history file 

indexInfo.setFinishTime(Long.parseLong( decodeJobHistoryFileName(jobDetails[FINISH_TIME_INDEX])));	} catch (NumberFormatException e) {	}	try {	indexInfo.setNumMaps(Integer.parseInt( decodeJobHistoryFileName(jobDetails[NUM_MAPS_INDEX])));	} catch (NumberFormatException e) {	}	try {	indexInfo.setNumReduces(Integer.parseInt( decodeJobHistoryFileName(jobDetails[NUM_REDUCES_INDEX])));	} catch (NumberFormatException e) {	
unable to parse num reduces from job history file 

}	indexInfo.setJobStatus( decodeJobHistoryFileName(jobDetails[JOB_STATUS_INDEX]));	indexInfo.setQueueName( decodeJobHistoryFileName(jobDetails[QUEUE_NAME_INDEX]));	try{	if (jobDetails.length <= JOB_START_TIME_INDEX) {	indexInfo.setJobStartTime(indexInfo.getSubmitTime());	} else {	indexInfo.setJobStartTime(Long.parseLong( decodeJobHistoryFileName(jobDetails[JOB_START_TIME_INDEX])));	}	} catch (NumberFormatException e){	
unable to parse start time from job history file 

indexInfo.setQueueName( decodeJobHistoryFileName(jobDetails[QUEUE_NAME_INDEX]));	try{	if (jobDetails.length <= JOB_START_TIME_INDEX) {	indexInfo.setJobStartTime(indexInfo.getSubmitTime());	} else {	indexInfo.setJobStartTime(Long.parseLong( decodeJobHistoryFileName(jobDetails[JOB_START_TIME_INDEX])));	}	} catch (NumberFormatException e){	}	} catch (IndexOutOfBoundsException e) {	
parsing job history file with partial data encoded into name 

========================= hadoop sample_4458 =========================

public synchronized void remove(NodeId nodeId) {	DecommissioningNodeContext context = decomNodes.get(nodeId);	if (context != null) {	
remove in 

logDecommissioningNodesStatus();	long now = mclock.getTime();	Set<NodeId> staleNodes = new HashSet<NodeId>();	for (Iterator<Map.Entry<NodeId, DecommissioningNodeContext>> it = decomNodes.entrySet().iterator(); it.hasNext();) {	Map.Entry<NodeId, DecommissioningNodeContext> e = it.next();	DecommissioningNodeContext d = e.getValue();	if (now - d.lastUpdateTime < 5000L) {	continue;	}	if (d.nodeState != NodeState.DECOMMISSIONING) {	
remove 

DecommissioningNodeContext d = e.getValue();	if (now - d.lastUpdateTime < 5000L) {	continue;	}	if (d.nodeState != NodeState.DECOMMISSIONING) {	it.remove();	continue;	} else if (now - d.lastUpdateTime > 60000L) {	RMNode rmNode = getRmNode(d.nodeId);	if (rmNode != null && rmNode.getState() == NodeState.DECOMMISSIONED) {	
remove 

continue;	} else if (now - d.lastUpdateTime > 60000L) {	RMNode rmNode = getRmNode(d.nodeId);	if (rmNode != null && rmNode.getState() == NodeState.DECOMMISSIONED) {	it.remove();	continue;	}	}	if (d.timeoutMs >= 0 && d.decommissioningStartTime + d.timeoutMs < now) {	staleNodes.add(d.nodeId);	
identified stale and timeout node 

staleNodes.add(d.nodeId);	}	}	for (NodeId nodeId : staleNodes) {	RMNode rmNode = this.rmContext.getRMNodes().get(nodeId);	if (rmNode == null || rmNode.getState() != NodeState.DECOMMISSIONING) {	remove(nodeId);	continue;	}	if (rmNode.getState() == NodeState.DECOMMISSIONING && checkReadyToBeDecommissioned(rmNode.getNodeID())) {	
decommissioning timeout 

private void removeCompletedApps(DecommissioningNodeContext context) {	Iterator<ApplicationId> it = context.appIds.iterator();	while (it.hasNext()) {	ApplicationId appId = it.next();	RMApp rmApp = rmContext.getRMApps().get(appId);	if (rmApp == null) {	
consider non existing app as completed 

private void removeCompletedApps(DecommissioningNodeContext context) {	Iterator<ApplicationId> it = context.appIds.iterator();	while (it.hasNext()) {	ApplicationId appId = it.next();	RMApp rmApp = rmContext.getRMApps().get(appId);	if (rmApp == null) {	it.remove();	continue;	}	if (rmApp.getState() == RMAppState.FINISHED || rmApp.getState() == RMAppState.FAILED || rmApp.getState() == RMAppState.KILLED) {	
remove app 

private void readDecommissioningTimeout(Configuration conf) {	try {	if (conf == null) {	conf = new YarnConfiguration();	}	int v = conf.getInt( YarnConfiguration.RM_NODE_GRACEFUL_DECOMMISSION_TIMEOUT, YarnConfiguration.DEFAULT_RM_NODE_GRACEFUL_DECOMMISSION_TIMEOUT);	if (defaultTimeoutMs != 1000L * v) {	defaultTimeoutMs = 1000L * v;	
use new decommissioningtimeoutms 

private void readDecommissioningTimeout(Configuration conf) {	try {	if (conf == null) {	conf = new YarnConfiguration();	}	int v = conf.getInt( YarnConfiguration.RM_NODE_GRACEFUL_DECOMMISSION_TIMEOUT, YarnConfiguration.DEFAULT_RM_NODE_GRACEFUL_DECOMMISSION_TIMEOUT);	if (defaultTimeoutMs != 1000L * v) {	defaultTimeoutMs = 1000L * v;	}	} catch (Exception e) {	
error readdecommissioningtimeout 

========================= hadoop sample_697 =========================

public static SaslPropertiesResolver getSaslPropertiesResolver( Configuration conf) {	String qops = conf.get(DFS_DATA_TRANSFER_PROTECTION_KEY);	if (qops == null || qops.isEmpty()) {	
datatransferprotocol not using saslpropertiesresolver no qop found in configuration for 

public static IOStreamPair createStreamPair(Configuration conf, CipherOption cipherOption, OutputStream out, InputStream in, boolean isServer) throws IOException {	
creating iostreampair of cryptoinputstream and cryptooutputstream 

========================= hadoop sample_6941 =========================

private void waitForBlockReplication(String filename, ClientProtocol namenode, int expected, long maxWaitSec) throws IOException {	long start = Time.monotonicNow();	
checking for block replication for 

private void waitForBlockReplication(String filename, ClientProtocol namenode, int expected, long maxWaitSec) throws IOException {	long start = Time.monotonicNow();	LocatedBlocks blocks = namenode.getBlockLocations(filename, 0, Long.MAX_VALUE);	assertEquals(numBlocks, blocks.locatedBlockCount());	for (int i = 0; i < numBlocks; ++i) {	
checking for block 

long start = Time.monotonicNow();	LocatedBlocks blocks = namenode.getBlockLocations(filename, 0, Long.MAX_VALUE);	assertEquals(numBlocks, blocks.locatedBlockCount());	for (int i = 0; i < numBlocks; ++i) {	while (true) {	blocks = namenode.getBlockLocations(filename, 0, Long.MAX_VALUE);	assertEquals(numBlocks, blocks.locatedBlockCount());	LocatedBlock block = blocks.get(i);	int actual = block.getLocations().length;	if ( actual == expected ) {	
got enough replicas for th block got 

assertEquals(numBlocks, blocks.locatedBlockCount());	for (int i = 0; i < numBlocks; ++i) {	while (true) {	blocks = namenode.getBlockLocations(filename, 0, Long.MAX_VALUE);	assertEquals(numBlocks, blocks.locatedBlockCount());	LocatedBlock block = blocks.get(i);	int actual = block.getLocations().length;	if ( actual == expected ) {	break;	}	
not enough replicas for th block yet expecting got 

SimulatedFSDataset.setFactory(conf);	cluster = new MiniDFSCluster.Builder(conf).numDataNodes(numDataNodes).build();	cluster.waitActive();	String bpid = cluster.getNamesystem().getBlockPoolId();	DFSClient dfsClient = new DFSClient(new InetSocketAddress("localhost", cluster.getNameNodePort()), conf);	writeFile(cluster.getFileSystem(), testPath, numDataNodes);	waitForBlockReplication(testFile, dfsClient.getNamenode(), numDataNodes, 20);	List<Map<DatanodeStorage, BlockListAsLongs>> blocksList = cluster.getAllBlockReports(bpid);	cluster.shutdown();	cluster = null;	
restarting minicluster 

cluster = new MiniDFSCluster.Builder(conf) .numDataNodes(numDataNodes * 2) .format(false) .build();	cluster.waitActive();	Set<Block> uniqueBlocks = new HashSet<Block>();	for(Map<DatanodeStorage, BlockListAsLongs> map : blocksList) {	for(BlockListAsLongs blockList : map.values()) {	for(Block b : blockList) {	uniqueBlocks.add(new Block(b));	}	}	}	
inserting blocks 

========================= hadoop sample_7653 =========================

cluster.getDfsCluster().waitActive();	cluster.getDfsCluster().transitionToActive(0);	final DFSClient client = new DFSClient(cluster.getDfsCluster() .getNameNode(0).getNameNodeAddress(), conf);	DFSInotifyEventInputStream eis = client.getInotifyEventStream();	ScheduledExecutorService ex = Executors .newSingleThreadScheduledExecutor();	ex.schedule(new Runnable() {	public void run() {	try {	client.mkdirs("/dir", null, false);	} catch (IOException e) {	
unable to create dir 

========================= hadoop sample_7203 =========================

Set<String> ips = new HashSet<String>();	List<SubnetUtils.SubnetInfo> cidrs = new LinkedList<SubnetUtils.SubnetInfo>();	Set<String> hosts = new HashSet<String>();	for (String hostEntry : hostEntries) {	if (hostEntry.indexOf("/") > -1) {	try {	SubnetUtils subnet = new SubnetUtils(hostEntry);	subnet.setInclusiveHostCount(true);	cidrs.add(subnet.getInfo());	} catch (IllegalArgumentException e) {	
invalid cidr syntax 

========================= hadoop sample_3610 =========================

synchronized (this) {	remainingLeases = leasesById.tailMap(prevId, false);	}	Collection<Long> inodeIds = remainingLeases.keySet();	final int numResponses = Math.min( this.fsnamesystem.getMaxListOpenFilesResponses(), inodeIds.size());	final List<OpenFileEntry> openFileEntries = Lists.newArrayListWithExpectedSize(numResponses);	int count = 0;	for (Long inodeId: inodeIds) {	final INodeFile inodeFile = fsnamesystem.getFSDirectory().getInode(inodeId).asFile();	if (!inodeFile.isUnderConstruction()) {	
the file is not under construction but has lease 

private synchronized void removeLease(Lease lease, long inodeId) {	leasesById.remove(inodeId);	if (!lease.removeFile(inodeId)) {	if (LOG.isDebugEnabled()) {	LOG.debug("inode " + inodeId + " not found in lease.files (=" + lease + ")");	}	}	if (!lease.hasFiles()) {	leases.remove(lease.holder);	if (!sortedLeases.remove(lease)) {	
not found in sortedleases 

}	} finally {	fsnamesystem.writeUnlock("leaseManager");	if (needSync) {	fsnamesystem.getEditLog().logSync();	}	}	Thread.sleep(fsnamesystem.getLeaseRecheckIntervalMs());	} catch(InterruptedException ie) {	if (LOG.isDebugEnabled()) {	
is interrupted 

fsnamesystem.writeUnlock("leaseManager");	if (needSync) {	fsnamesystem.getEditLog().logSync();	}	}	Thread.sleep(fsnamesystem.getLeaseRecheckIntervalMs());	} catch(InterruptedException ie) {	if (LOG.isDebugEnabled()) {	}	} catch(Throwable e) {	
unexpected throwable 

========================= hadoop sample_8119 =========================

deleteLatch.countDown();	} else if (i == 150000) {	hbaseOutputStream.hsync();	fs.delete(hbaseFile, true);	try {	hbaseOutputStream.close();	} catch (Exception e) {	}	hbaseOutputStream = null;	} else if (i % 5000 == 0) {	
write pos size loop 

fs.delete(hbaseFile, true);	try {	hbaseOutputStream.close();	} catch (Exception e) {	}	hbaseOutputStream = null;	} else if (i % 5000 == 0) {	}	}	} catch (Exception e) {	
writer error 

}	} catch (Exception e) {	writerError.set(true);	}	}	});	t.start();	startLatch.await();	final Path snap1Dir = SnapshotTestHelper.createSnapshot( fs, snapRootDir, snap1Name);	final Path flumeS1Path = new Path(snap1Dir, flumeFileName);	
file status 

}	} catch (Exception e) {	writerError.set(true);	}	}	});	t.start();	startLatch.await();	final Path snap1Dir = SnapshotTestHelper.createSnapshot( fs, snapRootDir, snap1Name);	final Path flumeS1Path = new Path(snap1Dir, flumeFileName);	
current file status 

} catch (Exception e) {	writerError.set(true);	}	}	});	t.start();	startLatch.await();	final Path snap1Dir = SnapshotTestHelper.createSnapshot( fs, snapRootDir, snap1Name);	final Path flumeS1Path = new Path(snap1Dir, flumeFileName);	deleteLatch.await();	
file status 

} catch (Exception e) {	writerError.set(true);	}	}	});	t.start();	startLatch.await();	final Path snap1Dir = SnapshotTestHelper.createSnapshot( fs, snapRootDir, snap1Name);	final Path flumeS1Path = new Path(snap1Dir, flumeFileName);	deleteLatch.await();	
current file status 

} catch (Exception e) {	writerError.set(true);	}	}	});	t.start();	startLatch.await();	final Path snap1Dir = SnapshotTestHelper.createSnapshot( fs, snapRootDir, snap1Name);	final Path flumeS1Path = new Path(snap1Dir, flumeFileName);	deleteLatch.await();	
deleting 

========================= hadoop sample_7346 =========================

try {	LocatedFileStatusFetcher locatedFileStatusFetcher = new LocatedFileStatusFetcher( job.getConfiguration(), dirs, recursive, inputFilter, true);	locatedFiles = locatedFileStatusFetcher.getFileStatuses();	} catch (InterruptedException e) {	throw new IOException("Interrupted while getting file statuses");	}	result = Lists.newArrayList(locatedFiles);	}	sw.stop();	if (LOG.isDebugEnabled()) {	
time taken to get filestatuses 

LocatedFileStatusFetcher locatedFileStatusFetcher = new LocatedFileStatusFetcher( job.getConfiguration(), dirs, recursive, inputFilter, true);	locatedFiles = locatedFileStatusFetcher.getFileStatuses();	} catch (InterruptedException e) {	throw new IOException("Interrupted while getting file statuses");	}	result = Lists.newArrayList(locatedFiles);	}	sw.stop();	if (LOG.isDebugEnabled()) {	}	
total input files to process 

splits.add(makeSplit(path, length-bytesRemaining, splitSize, blkLocations[blkIndex].getHosts(), blkLocations[blkIndex].getCachedHosts()));	bytesRemaining -= splitSize;	}	if (bytesRemaining != 0) {	int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);	splits.add(makeSplit(path, length-bytesRemaining, bytesRemaining, blkLocations[blkIndex].getHosts(), blkLocations[blkIndex].getCachedHosts()));	}	} else {	if (LOG.isDebugEnabled()) {	if (length > Math.min(file.getBlockSize(), minSize)) {	
file is not splittable so no parallelization is possible 

}	splits.add(makeSplit(path, 0, length, blkLocations[0].getHosts(), blkLocations[0].getCachedHosts()));	}	} else {	splits.add(makeSplit(path, 0, length, new String[0]));	}	}	job.getConfiguration().setLong(NUM_INPUT_FILES, files.size());	sw.stop();	if (LOG.isDebugEnabled()) {	
total of splits generated by getsplits timetaken 

========================= hadoop sample_4993 =========================

}	entity.addRelatedEntity("test ref type 1", "test ref id 1");	entity.addRelatedEntity("test ref type 2", "test ref id 2");	entity.addPrimaryFilter("pkey1", "pval1");	entity.addPrimaryFilter("pkey2", "pval2");	entity.addOtherInfo("okey1", "oval1");	entity.addOtherInfo("okey2", "oval2");	entity.setDomainId("domain id " + j);	entities.addEntity(entity);	}	
entities in json 

for (int i = 0; i < 2; ++i) {	TimelineEvent event = new TimelineEvent();	event.setTimestamp(System.currentTimeMillis());	event.setEventType("event type " + i);	event.addEventInfo("key1", "val1");	event.addEventInfo("key2", "val2");	partEvents.addEvent(event);	}	events.addEvent(partEvents);	}	
events in json 

error1.setErrorCode(TimelinePutError.NO_START_TIME);	TimelinePutErrors.addError(error1);	List<TimelinePutError> response = new ArrayList<TimelinePutError>();	response.add(error1);	TimelinePutError error2 = new TimelinePutError();	error2.setEntityId("entity id 2");	error2.setEntityId("entity type 2");	error2.setErrorCode(TimelinePutError.IO_EXCEPTION);	response.add(error2);	TimelinePutErrors.addErrors(response);	
errors in json 

domain = new TimelineDomain();	domain.setId("test id " + (i + 1));	domain.setDescription("test description " + (i + 1));	domain.setOwner("test owner " + (i + 1));	domain.setReaders("test_reader_user_" + (i + 1) + " test_reader_group+" + (i + 1));	domain.setWriters("test_writer_user_" + (i + 1) + " test_writer_group+" + (i + 1));	domain.setCreatedTime(0L);	domain.setModifiedTime(1L);	domains.addDomain(domain);	}	
domain in json 

========================= hadoop sample_2144 =========================

private void validate(String text, Path path) throws IOException {	try {	fs.canonicalizeUri(path.toUri());	fs.checkPath(path);	assertTrue(text + " Not a directory", fs.getFileStatus(new Path("/")).isDirectory());	fs.globStatus(path);	} catch (AssertionError e) {	throw e;	} catch (Exception e) {	
failure 

========================= hadoop sample_5951 =========================

public void waitForPoolToIdle() throws InterruptedException {	Thread.sleep(100l);	
pool size pool size active count 

public void waitForPoolToIdle() throws InterruptedException {	Thread.sleep(100l);	while(!this.eventQueue.isEmpty() || !this.launcherPool.getQueue().isEmpty() || this.launcherPool.getActiveCount() > 0) {	Thread.sleep(100l);	
pool size pool size active count 

public void waitForPoolToIdle() throws InterruptedException {	Thread.sleep(100l);	while(!this.eventQueue.isEmpty() || !this.launcherPool.getQueue().isEmpty() || this.launcherPool.getActiveCount() > 0) {	Thread.sleep(100l);	}	
pool size pool size active count 

public void testHandle() throws Exception {	
starting testhandle 

ContainerManagementProtocolClient mockCM = mock(ContainerManagementProtocolClient.class);	ContainerLauncherImplUnderTest ut = new ContainerLauncherImplUnderTest(mockContext, mockCM);	Configuration conf = new Configuration();	ut.init(conf);	ut.start();	try {	ContainerId contId = makeContainerId(0l, 0, 0, 1);	TaskAttemptId taskAttemptId = makeTaskAttemptId(0l, 0, 0, TaskType.MAP, 0);	StartContainersResponse startResp = recordFactory.newRecordInstance(StartContainersResponse.class);	startResp.setAllServicesMetaData(serviceResponse);	
inserting launch event 

ContainerRemoteLaunchEvent mockLaunchEvent = mock(ContainerRemoteLaunchEvent.class);	when(mockLaunchEvent.getType()) .thenReturn(EventType.CONTAINER_REMOTE_LAUNCH);	when(mockLaunchEvent.getContainerID()) .thenReturn(contId);	when(mockLaunchEvent.getTaskAttemptID()).thenReturn(taskAttemptId);	when(mockLaunchEvent.getContainerMgrAddress()).thenReturn(cmAddress);	when(mockCM.startContainers(any(StartContainersRequest.class))).thenReturn(startResp);	when(mockLaunchEvent.getContainerToken()).thenReturn( createNewContainerToken(contId, cmAddress));	ut.handle(mockLaunchEvent);	ut.waitForPoolToIdle();	verify(mockCM).startContainers(any(StartContainersRequest.class));	
inserting cleanup event 

public void testOutOfOrder() throws Exception {	
starting testoutoforder 

ContainerLauncherImplUnderTest ut = new ContainerLauncherImplUnderTest(mockContext, mockCM);	Configuration conf = new Configuration();	ut.init(conf);	ut.start();	try {	ContainerId contId = makeContainerId(0l, 0, 0, 1);	TaskAttemptId taskAttemptId = makeTaskAttemptId(0l, 0, 0, TaskType.MAP, 0);	String cmAddress = "127.0.0.1:8000";	StartContainersResponse startResp = recordFactory.newRecordInstance(StartContainersResponse.class);	startResp.setAllServicesMetaData(serviceResponse);	
inserting cleanup event 

StartContainersResponse startResp = recordFactory.newRecordInstance(StartContainersResponse.class);	startResp.setAllServicesMetaData(serviceResponse);	ContainerLauncherEvent mockCleanupEvent = mock(ContainerLauncherEvent.class);	when(mockCleanupEvent.getType()) .thenReturn(EventType.CONTAINER_REMOTE_CLEANUP);	when(mockCleanupEvent.getContainerID()) .thenReturn(contId);	when(mockCleanupEvent.getTaskAttemptID()).thenReturn(taskAttemptId);	when(mockCleanupEvent.getContainerMgrAddress()).thenReturn(cmAddress);	ut.handle(mockCleanupEvent);	ut.waitForPoolToIdle();	verify(mockCM, never()).stopContainers(any(StopContainersRequest.class));	
inserting launch event 

public void testMyShutdown() throws Exception {	
in test shutdown 

ContainerLauncherImplUnderTest ut = new ContainerLauncherImplUnderTest(mockContext, mockCM);	Configuration conf = new Configuration();	ut.init(conf);	ut.start();	try {	ContainerId contId = makeContainerId(0l, 0, 0, 1);	TaskAttemptId taskAttemptId = makeTaskAttemptId(0l, 0, 0, TaskType.MAP, 0);	String cmAddress = "127.0.0.1:8000";	StartContainersResponse startResp = recordFactory.newRecordInstance(StartContainersResponse.class);	startResp.setAllServicesMetaData(serviceResponse);	
inserting launch event 

public void testContainerCleaned() throws Exception {	
starting testcontainercleaned 

ContainerLauncherImplUnderTest ut = new ContainerLauncherImplUnderTest(mockContext, mockCM);	Configuration conf = new Configuration();	ut.init(conf);	ut.start();	try {	ContainerId contId = makeContainerId(0l, 0, 0, 1);	TaskAttemptId taskAttemptId = makeTaskAttemptId(0l, 0, 0, TaskType.MAP, 0);	String cmAddress = "127.0.0.1:8000";	StartContainersResponse startResp = recordFactory.newRecordInstance(StartContainersResponse.class);	startResp.setAllServicesMetaData(serviceResponse);	
inserting launch event 

StartContainersResponse startResp = recordFactory.newRecordInstance(StartContainersResponse.class);	startResp.setAllServicesMetaData(serviceResponse);	ContainerRemoteLaunchEvent mockLaunchEvent = mock(ContainerRemoteLaunchEvent.class);	when(mockLaunchEvent.getType()) .thenReturn(EventType.CONTAINER_REMOTE_LAUNCH);	when(mockLaunchEvent.getContainerID()) .thenReturn(contId);	when(mockLaunchEvent.getTaskAttemptID()).thenReturn(taskAttemptId);	when(mockLaunchEvent.getContainerMgrAddress()).thenReturn(cmAddress);	when(mockLaunchEvent.getContainerToken()).thenReturn( createNewContainerToken(contId, cmAddress));	ut.handle(mockLaunchEvent);	startLaunchBarrier.await();	
inserting cleanup event 

========================= hadoop sample_5117 =========================

public void run() {	while (true) {	try {	if (Thread.currentThread().isInterrupted()) {	throw new InterruptedException();	}	int cmd = WritableUtils.readVInt(inStream);	
handling uplink command 

while (true) {	try {	if (Thread.currentThread().isInterrupted()) {	throw new InterruptedException();	}	int cmd = WritableUtils.readVInt(inStream);	if (cmd == MessageType.AUTHENTICATION_RESP.code) {	String digest = Text.readString(inStream);	authPending = !handler.authenticate(digest);	} else if (authPending) {	
message received before authentication is complete ignoring 

} else if (cmd == MessageType.REGISTER_COUNTER.code) {	int id = WritableUtils.readVInt(inStream);	String group = Text.readString(inStream);	String name = Text.readString(inStream);	handler.registerCounter(id, group, name);	} else if (cmd == MessageType.INCREMENT_COUNTER.code) {	int id = WritableUtils.readVInt(inStream);	long amount = WritableUtils.readVLong(inStream);	handler.incrementCounter(id, amount);	} else if (cmd == MessageType.DONE.code) {	
pipe child done 

public void close() throws IOException, InterruptedException {	
closing connection 

public void start() throws IOException {	
starting downlink 

public void endOfInput() throws IOException {	WritableUtils.writeVInt(stream, MessageType.CLOSE.code);	
sent close command 

public void abort() throws IOException {	WritableUtils.writeVInt(stream, MessageType.ABORT.code);	
sent abort command 

========================= hadoop sample_4706 =========================

public void testInitNullConf() throws Throwable {	BreakableService svc = new BreakableService(false, false, false);	try {	svc.init(null);	
null configurations are permitted 

========================= hadoop sample_3169 =========================

public SnapshotManager(final Configuration conf, final FSDirectory fsdir) {	this.fsdir = fsdir;	this.captureOpenFiles = conf.getBoolean( DFS_NAMENODE_SNAPSHOT_CAPTURE_OPENFILES, DFS_NAMENODE_SNAPSHOT_CAPTURE_OPENFILES_DEFAULT);	this.skipCaptureAccessTimeOnlyChange = conf.getBoolean( DFS_NAMENODE_SNAPSHOT_SKIP_CAPTURE_ACCESSTIME_ONLY_CHANGE, DFS_NAMENODE_SNAPSHOT_SKIP_CAPTURE_ACCESSTIME_ONLY_CHANGE_DEFAULT);	
loaded config captureopenfiles skipcaptureaccesstimeonlychange 

========================= hadoop sample_7983 =========================

public void initialize(Configuration conf) throws IOException {	
initializing remotesaskeygeneratorimpl instance 

if (isKerberosSupportEnabled && UserGroupInformation.isSecurityEnabled()) {	this.remoteCallHelper = new SecureWasbRemoteCallHelper(retryPolicy, false, isSpnegoTokenCacheEnabled);	} else {	this.remoteCallHelper = new WasbRemoteCallHelper(retryPolicy);	}	long sasKeyExpiryPeriodInMinutes = getSasKeyExpiryPeriod() * HOURS_IN_DAY * MINUTES_IN_HOUR;	long cacheEntryDurationInMinutes = conf.getTimeDuration(SASKEY_CACHEENTRY_EXPIRY_PERIOD, sasKeyExpiryPeriodInMinutes, TimeUnit.MINUTES);	cacheEntryDurationInMinutes = (cacheEntryDurationInMinutes > (sasKeyExpiryPeriodInMinutes - 5)) ? (sasKeyExpiryPeriodInMinutes - 5) : cacheEntryDurationInMinutes;	this.cache = new CachingAuthorizer<>(cacheEntryDurationInMinutes, "SASKEY");	this.cache.init(conf);	
initialization of remotesaskeygenerator instance successful 

public URI getContainerSASUri(String storageAccount, String container) throws SASKeyGenerationException {	RemoteSASKeyGenerationResponse sasKeyResponse = null;	try {	CachedSASKeyEntry cacheKey = new CachedSASKeyEntry(storageAccount, container, "/");	URI cacheResult = cache.get(cacheKey);	if (cacheResult != null) {	return cacheResult;	}	
generating container sas key storage account container 

public URI getRelativeBlobSASUri(String storageAccount, String container, String relativePath) throws SASKeyGenerationException {	try {	CachedSASKeyEntry cacheKey = new CachedSASKeyEntry(storageAccount, container, relativePath);	URI cacheResult = cache.get(cacheKey);	if (cacheResult != null) {	return cacheResult;	}	
generating relativepath sas key for relativepath inside container inside storage account 

========================= hadoop sample_6445 =========================

ipList.add(addr.getHostAddress());	}	}	StringBuilder builder = new StringBuilder();	for (String ip : ipList) {	builder.append(ip);	builder.append(',');	}	builder.append("127.0.1.1,");	builder.append(InetAddress.getLocalHost().getCanonicalHostName());	
local ip addresses 

========================= hadoop sample_2955 =========================

protected void sendBlockReports(DatanodeRegistration dnR, String poolId, StorageBlockReport[] reports) throws IOException {	
sending combined block reports for 

========================= hadoop sample_7254 =========================

private boolean doUserOp(UserGroupInformation ugi, final UserOp op) {	UserGroupInformation.setLoginUser(ugi);	return ugi.doAs(new PrivilegedAction<Boolean>() {	public Boolean run() {	try {	op.execute();	return true;	} catch (IOException ex) {	
ioexception thrown during doas operation 

========================= hadoop sample_7604 =========================

public void handle(RMNodeEvent event) {	
processing of type 

public void handle(RMNodeEvent event) {	try {	writeLock.lock();	NodeState oldState = getState();	try {	stateMachine.doTransition(event.getType(), event);	} catch (InvalidStateTransitionException e) {	
can t handle this event at current state 

public void handle(RMNodeEvent event) {	try {	writeLock.lock();	NodeState oldState = getState();	try {	stateMachine.doTransition(event.getType(), event);	} catch (InvalidStateTransitionException e) {	
invalid event on node oldstate 

public void handle(RMNodeEvent event) {	try {	writeLock.lock();	NodeState oldState = getState();	try {	stateMachine.doTransition(event.getType(), event);	} catch (InvalidStateTransitionException e) {	}	if (oldState != getState()) {	
node transitioned from to 

case REBOOTED: metrics.decrNumRebootedNMs();	break;	case DECOMMISSIONED: metrics.decrDecommisionedNMs();	break;	case UNHEALTHY: metrics.decrNumUnhealthyNMs();	break;	case SHUTDOWN: metrics.decrNumShutdownNMs();	break;	case DECOMMISSIONING: metrics.decrDecommissioningNMs();	break;	
unexpected previous node state 

private void updateMetricsForGracefulDecommission(NodeState initialState, NodeState finalState) {	ClusterMetrics metrics = ClusterMetrics.getMetrics();	switch (initialState) {	case UNHEALTHY : metrics.decrNumUnhealthyNMs();	break;	case RUNNING : metrics.decrNumActiveNodes();	break;	case DECOMMISSIONING : metrics.decrDecommissioningNMs();	break;	
unexpected initial state 

case RUNNING : metrics.decrNumActiveNodes();	break;	case DECOMMISSIONING : metrics.decrDecommissioningNMs();	break;	}	switch (finalState) {	case DECOMMISSIONING : metrics.incrDecommissioningNMs();	break;	case RUNNING : metrics.incrNumActiveNodes();	break;	
unexpected final state 

switch (initialState) {	case RUNNING: metrics.decrNumActiveNodes();	break;	case DECOMMISSIONING: metrics.decrDecommissioningNMs();	break;	case DECOMMISSIONED: metrics.decrDecommisionedNMs();	break;	case UNHEALTHY: metrics.decrNumUnhealthyNMs();	break;	case NEW: break;	
unexpected initial state 

case DECOMMISSIONED: metrics.incrDecommisionedNMs();	break;	case LOST: metrics.incrNumLostNMs();	break;	case REBOOTED: metrics.incrNumRebootedNMs();	break;	case UNHEALTHY: metrics.incrNumUnhealthyNMs();	break;	case SHUTDOWN: metrics.incrNumShutdownNMs();	break;	
unexpected final state 

}	if (rmNode.getState().equals(NodeState.RUNNING)) {	rmNode.context.getDispatcher().getEventHandler().handle( new NodeAddedSchedulerEvent(rmNode));	}	} else {	switch (rmNode.getState()) {	case RUNNING: ClusterMetrics.getMetrics().decrNumActiveNodes();	break;	case UNHEALTHY: ClusterMetrics.getMetrics().decrNumUnhealthyNMs();	break;	
unexpected rmnode state 

public void transition(RMNodeImpl rmNode, RMNodeEvent event) {	
try to update resource on a node 

public static void deactivateNode(RMNodeImpl rmNode, NodeState finalState) {	if (rmNode.getNodeID().getPort() == -1) {	rmNode.updateMetricsForDeactivatedNode(rmNode.getState(), finalState);	return;	}	reportNodeUnusable(rmNode, finalState);	rmNode.context.getRMNodes().remove(rmNode.nodeId);	
deactivating node as it is now 

public void transition(RMNodeImpl rmNode, RMNodeEvent event) {	Integer timeout = null;	if (RMNodeDecommissioningEvent.class.isInstance(event)) {	RMNodeDecommissioningEvent e = ((RMNodeDecommissioningEvent) event);	timeout = e.getDecommissioningTimeout();	}	if (rmNode.getState() == NodeState.DECOMMISSIONING) {	if (!Objects.equals(rmNode.getDecommissioningTimeout(), timeout)) {	
update decommissioningtimeout to be 

public void transition(RMNodeImpl rmNode, RMNodeEvent event) {	Integer timeout = null;	if (RMNodeDecommissioningEvent.class.isInstance(event)) {	RMNodeDecommissioningEvent e = ((RMNodeDecommissioningEvent) event);	timeout = e.getDecommissioningTimeout();	}	if (rmNode.getState() == NodeState.DECOMMISSIONING) {	if (!Objects.equals(rmNode.getDecommissioningTimeout(), timeout)) {	rmNode.decommissioningTimeout = timeout;	} else {	
is already decommissioning 

RMNodeDecommissioningEvent e = ((RMNodeDecommissioningEvent) event);	timeout = e.getDecommissioningTimeout();	}	if (rmNode.getState() == NodeState.DECOMMISSIONING) {	if (!Objects.equals(rmNode.getDecommissioningTimeout(), timeout)) {	rmNode.decommissioningTimeout = timeout;	} else {	}	return;	}	
put node in decommissioning 

if (!Objects.equals(rmNode.getDecommissioningTimeout(), timeout)) {	rmNode.decommissioningTimeout = timeout;	} else {	}	return;	}	rmNode.updateMetricsForGracefulDecommission(initState, finalState);	rmNode.decommissioningTimeout = timeout;	if (rmNode.originalTotalCapability == null){	rmNode.originalTotalCapability = Resources.clone(rmNode.totalCapability);	
preserve original total capability 

public void transition(RMNodeImpl rmNode, RMNodeEvent event) {	if (rmNode.originalTotalCapability != null) {	rmNode.totalCapability = rmNode.originalTotalCapability;	rmNode.originalTotalCapability = null;	}	
node in decommissioning is recommissioned back to running 

NodeState initialState = rmNode.getState();	boolean isNodeDecommissioning = initialState.equals(NodeState.DECOMMISSIONING);	if (isNodeDecommissioning) {	List<ApplicationId> keepAliveApps = statusEvent.getKeepAliveAppIds();	if (rmNode.runningApplications.isEmpty() && (keepAliveApps == null || keepAliveApps.isEmpty())) {	RMNodeImpl.deactivateNode(rmNode, NodeState.DECOMMISSIONED);	return NodeState.DECOMMISSIONED;	}	}	if (!remoteNodeHealthStatus.getIsNodeHealthy()) {	
node reported unhealthy with details 

private void handleReportedIncreasedContainers( List<Container> reportedIncreasedContainers) {	for (Container container : reportedIncreasedContainers) {	ContainerId containerId = container.getId();	if (containersToClean.contains(containerId)) {	
container already scheduled for cleanup no further processing 

private void handleReportedIncreasedContainers( List<Container> reportedIncreasedContainers) {	for (Container container : reportedIncreasedContainers) {	ContainerId containerId = container.getId();	if (containersToClean.contains(containerId)) {	continue;	}	ApplicationId containerAppId = containerId.getApplicationAttemptId().getApplicationId();	if (finishedApplications.contains(containerAppId)) {	
container belongs to an application that is already killed no further processing 

private void handleContainerStatus(List<ContainerStatus> containerStatuses) {	List<ContainerStatus> newlyLaunchedContainers = new ArrayList<ContainerStatus>();	List<ContainerStatus> newlyCompletedContainers = new ArrayList<ContainerStatus>();	int numRemoteRunningContainers = 0;	for (ContainerStatus remoteContainer : containerStatuses) {	ContainerId containerId = remoteContainer.getContainerId();	if (containersToClean.contains(containerId)) {	
container already scheduled for cleanup no further processing 

List<ContainerStatus> newlyLaunchedContainers = new ArrayList<ContainerStatus>();	List<ContainerStatus> newlyCompletedContainers = new ArrayList<ContainerStatus>();	int numRemoteRunningContainers = 0;	for (ContainerStatus remoteContainer : containerStatuses) {	ContainerId containerId = remoteContainer.getContainerId();	if (containersToClean.contains(containerId)) {	continue;	}	ApplicationId containerAppId = containerId.getApplicationAttemptId().getApplicationId();	if (finishedApplications.contains(containerAppId)) {	
container belongs to an application that is already killed no further processing 

for (ContainerStatus remoteContainer : containerStatuses) {	ContainerId containerId = remoteContainer.getContainerId();	if (containersToClean.contains(containerId)) {	continue;	}	ApplicationId containerAppId = containerId.getApplicationAttemptId().getApplicationId();	if (finishedApplications.contains(containerAppId)) {	continue;	} else if (!runningApplications.contains(containerAppId)) {	if (LOG.isDebugEnabled()) {	
container is the first container get launched for application 

========================= hadoop sample_658 =========================

public void testHotSwapOutFailedVolumeAndReporting() throws Exception {	
starting testhotswapoutfailedvolumeandreporting 

========================= hadoop sample_7311 =========================

private void assertCanNotStartNamenode(MiniDFSCluster cluster, int nnIndex) {	try {	cluster.restartNameNode(nnIndex, false);	fail("Should not have been able to start NN" + (nnIndex) + " without shared dir");	} catch (IOException ioe) {	
got expected exception 

========================= hadoop sample_7741 =========================

private Path initFiles(FileSystem fs, int numFiles, int numBytes) throws IOException{	Path dir = new Path(System.getProperty("test.build.data",".") + "/mapred");	Path multiFileDir = new Path(dir, "test.multifile");	fs.delete(multiFileDir, true);	fs.mkdirs(multiFileDir);	
creating file s in 

Path path = new Path(multiFileDir, "file_" + i);	FSDataOutputStream out = fs.create(path);	if (numBytes == -1) {	numBytes = rand.nextInt(MAX_BYTES);	}	for(int j=0; j< numBytes; j++) {	out.write(rand.nextInt());	}	out.close();	if(LOG.isDebugEnabled()) {	
created file with length 

public void testFormat() throws IOException {	
test started 

int index = Integer.parseInt( name.substring(name.lastIndexOf("file_") + 5));	assertFalse(bits.get(index));	bits.set(index);	}	assertEquals(splitLength, split.getLength());	}	}	assertEquals(bits.cardinality(), numFiles);	fs.delete(dir, true);	}	
test finished 

========================= hadoop sample_5455 =========================

public boolean addInterval(ReservationInterval interval, Resource resource) {	long startTime = interval.getStartTime();	long endTime = interval.getEndTime();	if (startTime >= 0 && endTime > startTime && endTime <= timePeriod) {	return super.addInterval(interval, resource);	} else {	
cannot set capacity beyond end time was 

public boolean removeInterval(ReservationInterval interval, Resource resource) {	long startTime = interval.getStartTime();	long endTime = interval.getEndTime();	if (!Resources.fitsIn(resource, getMinimumCapacityInInterval( new ReservationInterval(startTime, endTime - 1)))) {	
request to remove more resources than what is available 

public boolean removeInterval(ReservationInterval interval, Resource resource) {	long startTime = interval.getStartTime();	long endTime = interval.getEndTime();	if (!Resources.fitsIn(resource, getMinimumCapacityInInterval( new ReservationInterval(startTime, endTime - 1)))) {	return false;	}	if (startTime >= 0 && endTime > startTime && endTime <= timePeriod) {	return super.removeInterval(interval, resource);	} else {	
interval extends beyond the end time 

========================= hadoop sample_1046 =========================

protected void serviceStop() throws Exception {	launcherHandlingThread.interrupt();	try {	launcherHandlingThread.join();	} catch (InterruptedException ie) {	
interrupted during join 

public void run() {	while (!this.isInterrupted()) {	Runnable toLaunch;	try {	toLaunch = masterEvents.take();	launcherPool.execute(toLaunch);	} catch (InterruptedException e) {	
interrupted returning 

========================= hadoop sample_1103 =========================

sslConfDir = KeyStoreTestUtil.getClasspathDir(TestSSLHttpServer.class);	KeyStoreTestUtil.setupSSLConfig(keystoresDir, sslConfDir, conf, false, true, excludeCiphers);	Configuration sslConf = KeyStoreTestUtil.getSslConfig();	clientSslFactory = new SSLFactory(SSLFactory.Mode.CLIENT, sslConf);	clientSslFactory.init();	server = new HttpServer2.Builder() .setName("test") .addEndpoint(new URI("https: .setConf(conf) .keyPassword(sslConf.get("ssl.server.keystore.keypassword")) .keyStore(sslConf.get("ssl.server.keystore.location"), sslConf.get("ssl.server.keystore.password"), sslConf.get("ssl.server.keystore.type", "jks")) .trustStore(sslConf.get("ssl.server.truststore.location"), sslConf.get("ssl.server.truststore.password"), sslConf.get("ssl.server.truststore.type", "jks")) .excludeCiphers( sslConf.get("ssl.server.exclude.cipher.list")).build();	server.addServlet("echo", "/echo", TestHttpServer.EchoServlet.class);	server.addServlet("longheader", "/longheader", LongHeaderServlet.class);	server.start();	baseUrl = new URL("https: + NetUtils.getHostPortString(server.getConnectorAddress(0)));	
http server started 

SSLSocketFactory sslSocketF = clientSslFactory.createSSLSocketFactory();	PrefferedCipherSSLSocketFactory testPreferredCipherSSLSocketF = new PrefferedCipherSSLSocketFactory(sslSocketF, StringUtils.getTrimmedStrings(excludeCiphers));	conn.setSSLSocketFactory(testPreferredCipherSSLSocketF);	assertFalse("excludedCipher list is empty", excludeCiphers.isEmpty());	try {	InputStream in = conn.getInputStream();	ByteArrayOutputStream out = new ByteArrayOutputStream();	IOUtils.copyBytes(in, out, 1024);	fail("No Ciphers in common, SSLHandshake must fail.");	} catch (SSLHandshakeException ex) {	
no ciphers in common expected succesful test result 

HttpsURLConnection conn = (HttpsURLConnection) url.openConnection();	SSLSocketFactory sslSocketF = clientSslFactory.createSSLSocketFactory();	PrefferedCipherSSLSocketFactory testPreferredCipherSSLSocketF = new PrefferedCipherSSLSocketFactory(sslSocketF, StringUtils.getTrimmedStrings(oneEnabledCiphers));	conn.setSSLSocketFactory(testPreferredCipherSSLSocketF);	assertFalse("excludedCipher list is empty", oneEnabledCiphers.isEmpty());	try {	InputStream in = conn.getInputStream();	ByteArrayOutputStream out = new ByteArrayOutputStream();	IOUtils.copyBytes(in, out, 1024);	assertEquals(out.toString(), "a:b\nc:d\n");	
atleast one additional enabled cipher than excluded ciphers expected successful test result 

HttpsURLConnection conn = (HttpsURLConnection) url.openConnection();	SSLSocketFactory sslSocketF = clientSslFactory.createSSLSocketFactory();	PrefferedCipherSSLSocketFactory testPreferredCipherSSLSocketF = new PrefferedCipherSSLSocketFactory(sslSocketF, StringUtils.getTrimmedStrings(exclusiveEnabledCiphers));	conn.setSSLSocketFactory(testPreferredCipherSSLSocketF);	assertFalse("excludedCipher list is empty", exclusiveEnabledCiphers.isEmpty());	try {	InputStream in = conn.getInputStream();	ByteArrayOutputStream out = new ByteArrayOutputStream();	IOUtils.copyBytes(in, out, 1024);	assertEquals(out.toString(), "a:b\nc:d\n");	
atleast one additional enabled cipher than excluded ciphers expected successful test result 

========================= hadoop sample_3094 =========================

protected void serviceInit(Configuration conf) throws Exception {	String auth =  conf.get(CommonConfigurationKeys.HADOOP_SECURITY_AUTHENTICATION);	if (auth == null || "simple".equals(auth)) {	isSecurityEnabled = false;	} else if ("kerberos".equals(auth)) {	isSecurityEnabled = true;	} else {	
unrecognized attribute value for of 

} else {	}	String proxy = WebAppUtils.getProxyHostAndPort(conf);	String[] proxyParts = proxy.split(":");	proxyHost = proxyParts[0];	fetcher = new AppReportFetcher(conf);	bindAddress = conf.get(YarnConfiguration.PROXY_ADDRESS);	if(bindAddress == null || bindAddress.isEmpty()) {	throw new YarnRuntimeException(YarnConfiguration.PROXY_ADDRESS + " is not set so the proxy will not run.");	}	
instantiating proxy at 

if (YarnConfiguration.useHttps(conf)) {	WebAppUtils.loadSslConfiguration(b);	}	proxyServer = b.build();	proxyServer.addServlet(ProxyUriUtils.PROXY_SERVLET_NAME, ProxyUriUtils.PROXY_PATH_SPEC, WebAppProxyServlet.class);	proxyServer.setAttribute(FETCHER_ATTRIBUTE, fetcher);	proxyServer .setAttribute(IS_SECURITY_ENABLED_ATTRIBUTE, isSecurityEnabled);	proxyServer.setAttribute(PROXY_HOST_ATTRIBUTE, proxyHost);	proxyServer.start();	} catch (IOException e) {	
could not start proxy web server 

protected void serviceStop() throws Exception {	if(proxyServer != null) {	try {	proxyServer.stop();	} catch (Exception e) {	
error stopping proxy web server 

========================= hadoop sample_1553 =========================

List<FileStatusInfo> fileStatuses = Lists.newArrayList();	for (DiffInfo diff : diffList) {	diff.setTarget( new Path(options.getSourcePaths().get(0), diff.getTarget()));	if (diff.getType() == SnapshotDiffReport.DiffType.MODIFY) {	addToFileListing(fileListWriter, sourceRoot, diff.getTarget(), options);	} else if (diff.getType() == SnapshotDiffReport.DiffType.CREATE) {	addToFileListing(fileListWriter, sourceRoot, diff.getTarget(), options);	FileStatus sourceStatus = sourceFS.getFileStatus(diff.getTarget());	if (sourceStatus.isDirectory()) {	if (LOG.isDebugEnabled()) {	
adding source dir for traverse 

FileStatus[] sourceFiles = sourceFS.listStatus(path);	boolean explore = (sourceFiles != null && sourceFiles.length > 0);	if (!explore || rootStatus.isDirectory()) {	LinkedList<CopyListingFileStatus> rootCopyListingStatus = DistCpUtils.toCopyListingFileStatus(sourceFS, rootStatus, preserveAcls, preserveXAttrs, preserveRawXAttrs, options.getBlocksPerChunk());	writeToFileListingRoot(fileListWriter, rootCopyListingStatus, sourcePathRoot, options);	}	if (explore) {	ArrayList<FileStatus> sourceDirs = new ArrayList<FileStatus>();	for (FileStatus sourceStatus: sourceFiles) {	if (LOG.isDebugEnabled()) {	
recording source path for copy 

LinkedList<CopyListingFileStatus> sourceCopyListingStatus = DistCpUtils.toCopyListingFileStatus(sourceFS, sourceStatus, preserveAcls && sourceStatus.isDirectory(), preserveXAttrs && sourceStatus.isDirectory(), preserveRawXAttrs && sourceStatus.isDirectory(), options.getBlocksPerChunk());	for (CopyListingFileStatus fs : sourceCopyListingStatus) {	if (randomizeFileListing) {	addToFileListing(statusList, new FileStatusInfo(fs, sourcePathRoot), fileListWriter);	} else {	writeToFileListing(fileListWriter, fs, sourcePathRoot);	}	}	if (sourceStatus.isDirectory()) {	if (LOG.isDebugEnabled()) {	
adding source dir for traverse 

}	}	traverseDirectory(fileListWriter, sourceFS, sourceDirs, sourcePathRoot, options, null, statusList);	}	}	if (randomizeFileListing) {	writeToFileListing(statusList, fileListWriter);	}	fileListWriter.close();	printStats();	
build file listing completed 

private void writeToFileListing(List<FileStatusInfo> fileStatusInfoList, SequenceFile.Writer fileListWriter) throws IOException {	Collections.shuffle(fileStatusInfoList, rnd);	for (FileStatusInfo fileStatusInfo : fileStatusInfoList) {	if (LOG.isDebugEnabled()) {	
adding 

WorkReport<FileStatus[]> result = null;	try {	if (retry > 0) {	int sleepSeconds = 2;	for (int i = 1; i < retry; i++) {	sleepSeconds *= 2;	}	try {	Thread.sleep(1000 * sleepSeconds);	} catch (InterruptedException ie) {	
interrupted while sleeping in exponential backoff 

for (int i = 1; i < retry; i++) {	sleepSeconds *= 2;	}	try {	Thread.sleep(1000 * sleepSeconds);	} catch (InterruptedException ie) {	}	}	result = new WorkReport<FileStatus[]>(getFileStatus(parent.getPath()), retry, true);	} catch (FileNotFoundException fnf) {	
filenotfoundexception exception in liststatus 

}	try {	Thread.sleep(1000 * sleepSeconds);	} catch (InterruptedException ie) {	}	}	result = new WorkReport<FileStatus[]>(getFileStatus(parent.getPath()), retry, true);	} catch (FileNotFoundException fnf) {	result = new WorkReport<FileStatus[]>(new FileStatus[0], retry, true, fnf);	} catch (Exception e) {	
exception in liststatus will send for retry 

private void traverseDirectory(SequenceFile.Writer fileListWriter, FileSystem sourceFS, ArrayList<FileStatus> sourceDirs, Path sourcePathRoot, DistCpOptions options, HashSet<String> excludeList, List<FileStatusInfo> fileStatuses) throws IOException {	final boolean preserveAcls = options.shouldPreserve(FileAttribute.ACL);	final boolean preserveXAttrs = options.shouldPreserve(FileAttribute.XATTR);	final boolean preserveRawXattrs = options.shouldPreserveRawXattrs();	assert numListstatusThreads > 0;	if (LOG.isDebugEnabled()) {	
starting thread pool of liststatus workers 

}	for (FileStatus status : sourceDirs) {	workers.put(new WorkRequest<FileStatus>(status, 0));	}	while (workers.hasWork()) {	try {	WorkReport<FileStatus[]> workResult = workers.take();	int retry = workResult.getRetry();	for (FileStatus child: workResult.getItem()) {	if (LOG.isDebugEnabled()) {	
recording source path for copy 

if (randomizeFileListing) {	addToFileListing(fileStatuses, new FileStatusInfo(fs, sourcePathRoot), fileListWriter);	} else {	writeToFileListing(fileListWriter, fs, sourcePathRoot);	}	}	}	if (retry < maxRetries) {	if (child.isDirectory()) {	if (LOG.isDebugEnabled()) {	
traversing into source dir 

}	}	}	if (retry < maxRetries) {	if (child.isDirectory()) {	if (LOG.isDebugEnabled()) {	}	workers.put(new WorkRequest<FileStatus>(child, retry));	}	} else {	
giving up on after retries 

if (retry < maxRetries) {	if (child.isDirectory()) {	if (LOG.isDebugEnabled()) {	}	workers.put(new WorkRequest<FileStatus>(child, retry));	}	} else {	}	}	} catch (InterruptedException ie) {	
could not get item from childqueue retrying 

private void writeToFileListingRoot(SequenceFile.Writer fileListWriter, LinkedList<CopyListingFileStatus> fileStatus, Path sourcePathRoot, DistCpOptions options) throws IOException {	boolean syncOrOverwrite = options.shouldSyncFolder() || options.shouldOverwrite();	for (CopyListingFileStatus fs : fileStatus) {	if (fs.getPath().equals(sourcePathRoot) && fs.isDirectory() && syncOrOverwrite) {	if (LOG.isDebugEnabled()) {	
skip 

private void writeToFileListing(SequenceFile.Writer fileListWriter, CopyListingFileStatus fileStatus, Path sourcePathRoot) throws IOException {	if (LOG.isDebugEnabled()) {	
rel path full path 

========================= hadoop sample_6296 =========================

public void testRMRestartWaitForPreviousSucceededAttempt() throws Exception {	conf.setInt(YarnConfiguration.RM_AM_MAX_ATTEMPTS, 2);	MemoryRMStateStore memStore = new MockMemoryRMStateStore() {	int count = 0;	public void updateApplicationStateInternal(ApplicationId appId, ApplicationStateData appStateData) throws Exception {	if (count == 0) {	
final state is not saved 

am0 = MockRM.launchAM(app0, rm1, nm1);	am0.registerAppAttempt();	rm1.killApp(app0.getApplicationId());	rm1.waitForState(app0.getCurrentAppAttempt().getAppAttemptId(), RMAppAttemptState.KILLED);	MockRM rm2 = null;	try {	rm2 = new MockRM(conf, memStore);	rm2.start();	Assert.assertTrue("RM start successfully", true);	} catch (Exception e) {	
exception on start 

RMNodeLabelsManager mgr = new RMNodeLabelsManager();	mgr.init(getConfig());	return mgr;	}	};	try {	rm2.start();	Assert.assertTrue("RM start successfully", true);	Assert.assertEquals(1, rm2.getRMContext().getRMApps().size());	} catch (Exception e) {	
exception on start 

========================= hadoop sample_434 =========================

public Method getMethod() throws IOException {	try {	if (types != null) {	return ClientProtocol.class.getDeclaredMethod(methodName, types);	} else {	return ClientProtocol.class.getDeclaredMethod(methodName);	}	} catch (NoSuchMethodException e) {	
cannot get method with types 

public Method getMethod() throws IOException {	try {	if (types != null) {	return ClientProtocol.class.getDeclaredMethod(methodName, types);	} else {	return ClientProtocol.class.getDeclaredMethod(methodName);	}	} catch (NoSuchMethodException e) {	throw new IOException(e);	} catch (SecurityException e) {	
cannot access method with types 

========================= hadoop sample_8288 =========================

String racks[] = {"/rack1", "/rack1", "/rack2", "/rack2"};	MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf) .numDataNodes(racks.length).racks(racks).build();	final FSNamesystem ns = cluster.getNameNode().getNamesystem();	try {	final FileSystem fs = cluster.getFileSystem();	DFSTestUtil.createFile(fs, filePath, 1L, REPLICATION_FACTOR, 1L);	ExtendedBlock b = DFSTestUtil.getFirstBlock(fs, filePath);	DFSTestUtil.waitForReplication(cluster, b, 2, REPLICATION_FACTOR, 0);	BlockLocation locs[] = fs.getFileBlockLocations( fs.getFileStatus(filePath), 0, Long.MAX_VALUE);	String name = locs[0].getNames()[0];	
adding to decommission 

========================= hadoop sample_7487 =========================

cluster.waitActive();	final FileSystem fs = cluster.getFileSystem();	final Path p = new Path(DIR, "file1");	final int half = BLOCK_SIZE/2;	{	final FSDataOutputStream out = fs.create(p, true, fs.getConf().getInt(CommonConfigurationKeys.IO_FILE_BUFFER_SIZE_KEY, 4096), (short)3, BLOCK_SIZE);	write(out, 0, half);	((DFSOutputStream)out.getWrappedStream()).hflush();	}	checkFile(p, half, conf);	
leasechecker interruptandjoin 

========================= hadoop sample_7612 =========================

throw new IOException("From option " + AWS_CREDENTIALS_PROVIDER + ' ' + c, c);	}	if (awsClasses.length == 0) {	S3xLoginHelper.Login creds = getAWSAccessKeys(binding, conf);	credentials.add(new BasicAWSCredentialsProvider( creds.getUser(), creds.getPassword()));	credentials.add(new EnvironmentVariableCredentialsProvider());	credentials.add(InstanceProfileCredentialsProvider.getInstance());	} else {	for (Class<?> aClass : awsClasses) {	if (aClass == SharedInstanceProfileCredentialsProvider.class) {	
is deprecated and will be removed in future fall back to automatically 

credentials.add(new EnvironmentVariableCredentialsProvider());	credentials.add(InstanceProfileCredentialsProvider.getInstance());	} else {	for (Class<?> aClass : awsClasses) {	if (aClass == SharedInstanceProfileCredentialsProvider.class) {	aClass = InstanceProfileCredentialsProvider.class;	}	credentials.add(createAWSCredentialProvider(conf, aClass));	}	}	
for uri using credentials 

static AWSCredentialsProvider createAWSCredentialProvider( Configuration conf, Class<?> credClass) throws IOException {	AWSCredentialsProvider credentials = null;	String className = credClass.getName();	if (!AWSCredentialsProvider.class.isAssignableFrom(credClass)) {	throw new IOException("Class " + credClass + " " + NOT_AWS_PROVIDER);	}	if (Modifier.isAbstract(credClass.getModifiers())) {	throw new IOException("Class " + credClass + " " + ABSTRACT_PROVIDER);	}	
credential provider class is 

static int intOption(Configuration conf, String key, int defVal, int min) {	int v = conf.getInt(key, defVal);	Preconditions.checkArgument(v >= min, String.format("Value of %s: %d is below the minimum value %d", key, v, min));	
value of is 

static long longOption(Configuration conf, String key, long defVal, long min) {	long v = conf.getLong(key, defVal);	Preconditions.checkArgument(v >= min, String.format("Value of %s: %d is below the minimum value %d", key, v, min));	
value of is 

static long longBytesOption(Configuration conf, String key, long defVal, long min) {	long v = conf.getLongBytes(key, defVal);	Preconditions.checkArgument(v >= min, String.format("Value of %s: %d is below the minimum value %d", key, v, min));	
value of is 

public static long getMultipartSizeProperty(Configuration conf, String property, long defVal) {	long partSize = conf.getLongBytes(property, defVal);	if (partSize < MULTIPART_MIN_SIZE) {	
must be at least mb configured value is 

public static int ensureOutputParameterInRange(String name, long size) {	if (size > Integer.MAX_VALUE) {	
capped to maximum allowed size with current output mechanism 

public static Configuration propagateBucketOptions(Configuration source, String bucket) {	Preconditions.checkArgument(StringUtils.isNotEmpty(bucket), "bucket");	final String bucketPrefix = FS_S3A_BUCKET_PREFIX + bucket +'.';	
propagating entries under 

final String bucketPrefix = FS_S3A_BUCKET_PREFIX + bucket +'.';	final Configuration dest = new Configuration(source);	for (Map.Entry<String, String> entry : source) {	final String key = entry.getKey();	final String value = entry.getValue();	if (!key.startsWith(bucketPrefix) || bucketPrefix.equals(key)) {	continue;	}	final String stripped = key.substring(bucketPrefix.length());	if (stripped.startsWith("bucket.") || "impl".equals(stripped)) {	
ignoring bucket option 

final String key = entry.getKey();	final String value = entry.getValue();	if (!key.startsWith(bucketPrefix) || bucketPrefix.equals(key)) {	continue;	}	final String stripped = key.substring(bucketPrefix.length());	if (stripped.startsWith("bucket.") || "impl".equals(stripped)) {	}  else {	String origin = "[" + StringUtils.join( source.getPropertySources(key), ", ") +"]";	final String generic = FS_S3A_PREFIX + stripped;	
updating from 

static void patchSecurityCredentialProviders(Configuration conf) {	Collection<String> customCredentials = conf.getStringCollection( S3A_SECURITY_CREDENTIAL_PROVIDER_PATH);	Collection<String> hadoopCredentials = conf.getStringCollection( CREDENTIAL_PROVIDER_PATH);	if (!customCredentials.isEmpty()) {	List<String> all = Lists.newArrayList(customCredentials);	all.addAll(hadoopCredentials);	String joined = StringUtils.join(all, ',');	
setting to 

static String getServerSideEncryptionKey(Configuration conf) {	try {	return lookupPassword(conf, SERVER_SIDE_ENCRYPTION_KEY, getPassword(conf, OLD_S3A_SERVER_SIDE_ENCRYPTION_KEY, null, null));	} catch (IOException e) {	
cannot retrieve server side encryption key 

String diagnostics = passwordDiagnostics(sseKey, "key");	switch (sse) {	case SSE_C: if (sseKeyLen == 0) {	throw new IOException(SSE_C_NO_KEY_ERROR);	}	break;	case SSE_S3: if (sseKeyLen != 0) {	throw new IOException(SSE_S3_WITH_KEY_ERROR + " (" + diagnostics + ")");	}	break;	
using sse kms with 

switch (sse) {	case SSE_C: if (sseKeyLen == 0) {	throw new IOException(SSE_C_NO_KEY_ERROR);	}	break;	case SSE_S3: if (sseKeyLen != 0) {	throw new IOException(SSE_S3_WITH_KEY_ERROR + " (" + diagnostics + ")");	}	break;	break;	
data is unencrypted 

throw new IOException(SSE_C_NO_KEY_ERROR);	}	break;	case SSE_S3: if (sseKeyLen != 0) {	throw new IOException(SSE_S3_WITH_KEY_ERROR + " (" + diagnostics + ")");	}	break;	break;	break;	}	
using sse c with 

public static void clearBucketOption(Configuration conf, String bucket, String genericKey) {	final String baseKey = genericKey.startsWith(FS_S3A_PREFIX) ? genericKey.substring(FS_S3A_PREFIX.length()) : genericKey;	String k = FS_S3A_BUCKET_PREFIX + bucket + '.' + baseKey;	
unset 

========================= hadoop sample_6003 =========================

static void setRandomTextDataGeneratorListSize(Configuration conf, int listSize) {	if (LOG.isDebugEnabled()) {	
random text data generator is configured to use a dictionary with words 

static void setRandomTextDataGeneratorWordSize(Configuration conf, int wordSize) {	if (LOG.isDebugEnabled()) {	
random text data generator is configured to use a dictionary with words of length 

========================= hadoop sample_6146 =========================

int memoryMb = conf.getInt(YarnConfiguration.NM_PMEM_MB, -1);	if (memoryMb == -1) {	int physicalMemoryMB = (int) (plugin.getPhysicalMemorySize() / (1024 * 1024));	int hadoopHeapSizeMB = (int) (Runtime.getRuntime().maxMemory() / (1024 * 1024));	int containerPhysicalMemoryMB = (int) (0.8f * (physicalMemoryMB - (2 * hadoopHeapSizeMB)));	int reservedMemoryMB = conf.getInt(YarnConfiguration.NM_SYSTEM_RESERVED_PMEM_MB, -1);	if (reservedMemoryMB != -1) {	containerPhysicalMemoryMB = physicalMemoryMB - reservedMemoryMB;	}	if(containerPhysicalMemoryMB <= 0) {	
calculated memory for yarn containers is too low node memory is mb system reserved memory is mb 

========================= hadoop sample_1683 =========================

public void load() {	String fileName = this.conf.get(YarnConfiguration.FEDERATION_MACHINE_LIST, "");	try {	if (fileName == null || fileName.trim().length() == 0) {	
the machine list file path is not specified in the configuration 

String fileName = this.conf.get(YarnConfiguration.FEDERATION_MACHINE_LIST, "");	try {	if (fileName == null || fileName.trim().length() == 0) {	return;	}	Path file = null;	BufferedReader reader = null;	try {	file = Paths.get(fileName);	} catch (InvalidPathException e) {	
the configured machine list file path does not exist 

try {	reader = Files.newBufferedReader(file, Charset.defaultCharset());	String line = null;	while ((line = reader.readLine()) != null) {	String[] tokens = line.split(",");	if (tokens.length == 3) {	String nodeName = tokens[NODE_NAME_INDEX].trim().toUpperCase();	SubClusterId subClusterId = SubClusterId.newInstance(tokens[SUBCLUSTER_ID_INDEX].trim());	String rackName = tokens[RACK_NAME_INDEX].trim().toUpperCase();	if (LOG.isDebugEnabled()) {	
loading node into resolver 

try {	reader = Files.newBufferedReader(file, Charset.defaultCharset());	String line = null;	while ((line = reader.readLine()) != null) {	String[] tokens = line.split(",");	if (tokens.length == 3) {	String nodeName = tokens[NODE_NAME_INDEX].trim().toUpperCase();	SubClusterId subClusterId = SubClusterId.newInstance(tokens[SUBCLUSTER_ID_INDEX].trim());	String rackName = tokens[RACK_NAME_INDEX].trim().toUpperCase();	if (LOG.isDebugEnabled()) {	
loading rack into resolver 

String[] tokens = line.split(",");	if (tokens.length == 3) {	String nodeName = tokens[NODE_NAME_INDEX].trim().toUpperCase();	SubClusterId subClusterId = SubClusterId.newInstance(tokens[SUBCLUSTER_ID_INDEX].trim());	String rackName = tokens[RACK_NAME_INDEX].trim().toUpperCase();	if (LOG.isDebugEnabled()) {	}	this.getNodeToSubCluster().put(nodeName, subClusterId);	loadRackToSubCluster(rackName, subClusterId);	} else {	
skipping malformed line in machine list 

this.getNodeToSubCluster().put(nodeName, subClusterId);	loadRackToSubCluster(rackName, subClusterId);	} else {	}	}	} finally {	if (reader != null) {	reader.close();	}	}	
successfully loaded file 

loadRackToSubCluster(rackName, subClusterId);	} else {	}	}	} finally {	if (reader != null) {	reader.close();	}	}	} catch (Exception e) {	
failed to parse file 

========================= hadoop sample_1379 =========================

} else {	String connString = conf.get(ZK_DTSM_ZK_CONNECTION_STRING);	Preconditions.checkNotNull(connString, "Zookeeper connection string cannot be null");	String authType = conf.get(ZK_DTSM_ZK_AUTH_TYPE);	Preconditions.checkNotNull(authType, "Zookeeper authType cannot be null !!");	Preconditions.checkArgument( authType.equals("sasl") || authType.equals("none"), "Zookeeper authType must be one of [none, sasl]");	Builder builder = null;	try {	ACLProvider aclProvider = null;	if (authType.equals("sasl")) {	
connecting to zookeeper with sasl kerberos and using sasl acls 

Preconditions.checkArgument( authType.equals("sasl") || authType.equals("none"), "Zookeeper authType must be one of [none, sasl]");	Builder builder = null;	try {	ACLProvider aclProvider = null;	if (authType.equals("sasl")) {	String principal = setJaasConfiguration(conf);	System.setProperty(ZooKeeperSaslClient.LOGIN_CONTEXT_NAME_KEY, JAAS_LOGIN_ENTRY_NAME);	System.setProperty("zookeeper.authProvider.1", "org.apache.zookeeper.server.auth.SASLAuthenticationProvider");	aclProvider = new SASLOwnerACLProvider(principal);	} else {	
connecting to zookeeper without authentication 

private void loadFromZKCache(final boolean isTokenCache) {	final String cacheName = isTokenCache ? "token" : "key";	
starting to load cache 

}	int count = 0;	for (ChildData child : children) {	try {	if (isTokenCache) {	processTokenAddOrUpdate(child);	} else {	processKeyAddOrUpdate(child.getData());	}	} catch (Exception e) {	
ignoring node because it failed to load 

}	int count = 0;	for (ChildData child : children) {	try {	if (isTokenCache) {	processTokenAddOrUpdate(child);	} else {	processKeyAddOrUpdate(child.getData());	}	} catch (Exception e) {	
failure exception 

if (isTokenCache) {	processTokenAddOrUpdate(child);	} else {	processKeyAddOrUpdate(child.getData());	}	} catch (Exception e) {	++count;	}	}	if (count > 0) {	
ignored nodes while loading cache 

processTokenAddOrUpdate(child);	} else {	processKeyAddOrUpdate(child.getData());	}	} catch (Exception e) {	++count;	}	}	if (count > 0) {	}	
loaded cache 

public void stopThreads() {	super.stopThreads();	try {	if (tokenCache != null) {	tokenCache.close();	}	} catch (Exception e) {	
could not stop delegation token cache 

if (tokenCache != null) {	tokenCache.close();	}	} catch (Exception e) {	}	try {	if (delTokSeqCounter != null) {	delTokSeqCounter.close();	}	} catch (Exception e) {	
could not stop delegation token counter 

if (delTokSeqCounter != null) {	delTokSeqCounter.close();	}	} catch (Exception e) {	}	try {	if (keyIdSeqCounter != null) {	keyIdSeqCounter.close();	}	} catch (Exception e) {	
could not stop key id counter 

if (keyIdSeqCounter != null) {	keyIdSeqCounter.close();	}	} catch (Exception e) {	}	try {	if (keyCache != null) {	keyCache.close();	}	} catch (Exception e) {	
could not stop keycache 

if (keyCache != null) {	keyCache.close();	}	} catch (Exception e) {	}	try {	if (!isExternalClient && (zkClient != null)) {	zkClient.close();	}	} catch (Exception e) {	
could not stop curator framework 

try {	if (!isExternalClient && (zkClient != null)) {	zkClient.close();	}	} catch (Exception e) {	}	if (listenerThreadPool != null) {	listenerThreadPool.shutdown();	try {	if (!listenerThreadPool.awaitTermination(shutdownTimeout, TimeUnit.MILLISECONDS)) {	
forcing listener threadpool to shutdown 

private void createPersistentNode(String nodePath) throws Exception {	try {	zkClient.create().withMode(CreateMode.PERSISTENT).forPath(nodePath);	} catch (KeeperException.NodeExistsException ne) {	
znode already exists 

protected int incrementDelegationTokenSeqNum() {	try {	incrSharedCount(delTokSeqCounter);	} catch (InterruptedException e) {	
thread interrupted while performing token counter increment 

protected int incrementCurrentKeyId() {	try {	incrSharedCount(keyIdSeqCounter);	} catch (InterruptedException e) {	
thread interrupted while performing keyid increment 

protected DelegationKey getDelegationKey(int keyId) {	DelegationKey key = allKeys.get(keyId);	if (key == null) {	try {	key = getKeyFromZK(keyId);	if (key != null) {	allKeys.put(keyId, key);	}	} catch (IOException e) {	
error retrieving key from zk 

byte[] data = zkClient.getData().forPath(nodePath);	if ((data == null) || (data.length == 0)) {	return null;	}	ByteArrayInputStream bin = new ByteArrayInputStream(data);	DataInputStream din = new DataInputStream(bin);	DelegationKey key = new DelegationKey();	key.readFields(din);	return key;	} catch (KeeperException.NoNodeException e) {	
no node in path 

protected DelegationTokenInformation getTokenInfo(TokenIdent ident) {	DelegationTokenInformation tokenInfo = currentTokens.get(ident);	if (tokenInfo == null) {	try {	tokenInfo = getTokenInfoFromZK(ident);	if (tokenInfo != null) {	currentTokens.put(ident, tokenInfo);	}	} catch (IOException e) {	
error retrieving tokeninfo from zk 

private synchronized void syncLocalCacheWithZk(TokenIdent ident) {	try {	DelegationTokenInformation tokenInfo = getTokenInfoFromZK(ident);	if (tokenInfo != null && !currentTokens.containsKey(ident)) {	currentTokens.put(ident, tokenInfo);	} else if (tokenInfo == null && currentTokens.containsKey(ident)) {	currentTokens.remove(ident);	}	} catch (IOException e) {	
error retrieving tokeninfo from zk 

long renewDate = din.readLong();	int pwdLen = din.readInt();	byte[] password = new byte[pwdLen];	int numRead = din.read(password, 0, pwdLen);	if (numRead > -1) {	DelegationTokenInformation tokenInfo = new DelegationTokenInformation(renewDate, password);	return tokenInfo;	}	} catch (KeeperException.NoNodeException e) {	if (!quiet) {	
no node in path 

private void addOrUpdateDelegationKey(DelegationKey key, boolean isUpdate) throws IOException {	String nodeCreatePath = getNodePath(ZK_DTSM_MASTER_KEY_ROOT, DELEGATION_KEY_PREFIX + key.getKeyId());	ByteArrayOutputStream os = new ByteArrayOutputStream();	DataOutputStream fsOut = new DataOutputStream(os);	if (LOG.isDebugEnabled()) {	
storing zkdtsmdelegationkey 

String nodeCreatePath = getNodePath(ZK_DTSM_MASTER_KEY_ROOT, DELEGATION_KEY_PREFIX + key.getKeyId());	ByteArrayOutputStream os = new ByteArrayOutputStream();	DataOutputStream fsOut = new DataOutputStream(os);	if (LOG.isDebugEnabled()) {	}	key.write(fsOut);	try {	if (zkClient.checkExists().forPath(nodeCreatePath) != null) {	zkClient.setData().forPath(nodeCreatePath, os.toByteArray()) .setVersion(-1);	if (!isUpdate) {	
key with path already exists updating 

}	key.write(fsOut);	try {	if (zkClient.checkExists().forPath(nodeCreatePath) != null) {	zkClient.setData().forPath(nodeCreatePath, os.toByteArray()) .setVersion(-1);	if (!isUpdate) {	}	} else {	zkClient.create().withMode(CreateMode.PERSISTENT) .forPath(nodeCreatePath, os.toByteArray());	if (isUpdate) {	
updating non existent key path adding new 

if (zkClient.checkExists().forPath(nodeCreatePath) != null) {	zkClient.setData().forPath(nodeCreatePath, os.toByteArray()) .setVersion(-1);	if (!isUpdate) {	}	} else {	zkClient.create().withMode(CreateMode.PERSISTENT) .forPath(nodeCreatePath, os.toByteArray());	if (isUpdate) {	}	}	} catch (KeeperException.NodeExistsException ne) {	
znode already exists 

protected void removeStoredMasterKey(DelegationKey key) {	String nodeRemovePath = getNodePath(ZK_DTSM_MASTER_KEY_ROOT, DELEGATION_KEY_PREFIX + key.getKeyId());	if (LOG.isDebugEnabled()) {	
removing zkdtsmdelegationkey 

protected void removeStoredMasterKey(DelegationKey key) {	String nodeRemovePath = getNodePath(ZK_DTSM_MASTER_KEY_ROOT, DELEGATION_KEY_PREFIX + key.getKeyId());	if (LOG.isDebugEnabled()) {	}	try {	if (zkClient.checkExists().forPath(nodeRemovePath) != null) {	while(zkClient.checkExists().forPath(nodeRemovePath) != null){	try {	zkClient.delete().guaranteed().forPath(nodeRemovePath);	} catch (NoNodeException nne) {	
node already deleted by peer 

}	try {	if (zkClient.checkExists().forPath(nodeRemovePath) != null) {	while(zkClient.checkExists().forPath(nodeRemovePath) != null){	try {	zkClient.delete().guaranteed().forPath(nodeRemovePath);	} catch (NoNodeException nne) {	}	}	} else {	
attempted to delete a non existing znode 

if (zkClient.checkExists().forPath(nodeRemovePath) != null) {	while(zkClient.checkExists().forPath(nodeRemovePath) != null){	try {	zkClient.delete().guaranteed().forPath(nodeRemovePath);	} catch (NoNodeException nne) {	}	}	} else {	}	} catch (Exception e) {	
znode could not be removed 

protected void updateToken(TokenIdent ident, DelegationTokenInformation tokenInfo) throws IOException {	String nodeRemovePath = getNodePath(ZK_DTSM_TOKENS_ROOT, DELEGATION_TOKEN_PREFIX + ident.getSequenceNumber());	try {	if (zkClient.checkExists().forPath(nodeRemovePath) != null) {	addOrUpdateToken(ident, tokenInfo, true);	} else {	addOrUpdateToken(ident, tokenInfo, false);	
attempted to update a non existing znode 

protected void removeStoredToken(TokenIdent ident) throws IOException {	String nodeRemovePath = getNodePath(ZK_DTSM_TOKENS_ROOT, DELEGATION_TOKEN_PREFIX + ident.getSequenceNumber());	if (LOG.isDebugEnabled()) {	
removing zkdtsmdelegationtoken 

protected void removeStoredToken(TokenIdent ident) throws IOException {	String nodeRemovePath = getNodePath(ZK_DTSM_TOKENS_ROOT, DELEGATION_TOKEN_PREFIX + ident.getSequenceNumber());	if (LOG.isDebugEnabled()) {	}	try {	if (zkClient.checkExists().forPath(nodeRemovePath) != null) {	while(zkClient.checkExists().forPath(nodeRemovePath) != null){	try {	zkClient.delete().guaranteed().forPath(nodeRemovePath);	} catch (NoNodeException nne) {	
node already deleted by peer 

}	try {	if (zkClient.checkExists().forPath(nodeRemovePath) != null) {	while(zkClient.checkExists().forPath(nodeRemovePath) != null){	try {	zkClient.delete().guaranteed().forPath(nodeRemovePath);	} catch (NoNodeException nne) {	}	}	} else {	
attempted to remove a non existing znode 

String nodeCreatePath = getNodePath(ZK_DTSM_TOKENS_ROOT, DELEGATION_TOKEN_PREFIX + ident.getSequenceNumber());	ByteArrayOutputStream tokenOs = new ByteArrayOutputStream();	DataOutputStream tokenOut = new DataOutputStream(tokenOs);	ByteArrayOutputStream seqOs = new ByteArrayOutputStream();	try {	ident.write(tokenOut);	tokenOut.writeLong(info.getRenewDate());	tokenOut.writeInt(info.getPassword().length);	tokenOut.write(info.getPassword());	if (LOG.isDebugEnabled()) {	
updating storing zkdtsmdelegationtoken 

========================= hadoop sample_3714 =========================

public void init(AMRMProxyApplicationContext appContext) {	super.init(appContext);	
initializing federation interceptor 

public AllocateResponse allocate(AllocateRequest request) throws YarnException {	Preconditions.checkArgument(this.policyInterpreter != null, "Allocate should be called after registerApplicationMaster");	try {	Map<SubClusterId, AllocateRequest> requests = splitAllocateRequest(request);	Registrations newRegistrations = sendRequestsToSecondaryResourceManagers(requests);	AllocateResponse homeResponse = AMRMClientUtils.allocateWithReRegister( requests.get(this.homeSubClusterId), this.homeRM, this.amRegistrationRequest, getApplicationContext().getApplicationAttemptId());	try {	this.policyInterpreter.notifyOfResponse(this.homeSubClusterId, homeResponse);	} catch (YarnException e) {	
notifyofresponse for policy failed for home sub cluster 

Preconditions.checkArgument(this.policyInterpreter != null, "Allocate should be called after registerApplicationMaster");	try {	Map<SubClusterId, AllocateRequest> requests = splitAllocateRequest(request);	Registrations newRegistrations = sendRequestsToSecondaryResourceManagers(requests);	AllocateResponse homeResponse = AMRMClientUtils.allocateWithReRegister( requests.get(this.homeSubClusterId), this.homeRM, this.amRegistrationRequest, getApplicationContext().getApplicationAttemptId());	try {	this.policyInterpreter.notifyOfResponse(this.homeSubClusterId, homeResponse);	} catch (YarnException e) {	}	if (homeResponse.getAMRMToken() != null) {	
received new amrmtoken 

}	if (homeResponse.getAMRMToken() != null) {	YarnServerSecurityUtils.updateAMRMToken(homeResponse.getAMRMToken(), this.appOwner, getConf());	}	homeResponse = mergeAllocateResponses(homeResponse);	if (!isNullOrEmpty(newRegistrations.getSuccessfulRegistrations())) {	homeResponse = mergeRegistrationResponses(homeResponse, newRegistrations.getSuccessfulRegistrations());	}	return homeResponse;	} catch (IOException ex) {	
exception encountered while processing heart beat 

public FinishApplicationMasterResponse finishApplicationMaster( FinishApplicationMasterRequest request) throws YarnException, IOException {	boolean failedToUnRegister = false;	ExecutorCompletionService<FinishApplicationMasterResponseInfo> compSvc = null;	Set<String> subClusterIds = this.uamPool.getAllUAMIds();	if (subClusterIds.size() > 0) {	final FinishApplicationMasterRequest finishRequest = request;	compSvc = new ExecutorCompletionService<FinishApplicationMasterResponseInfo>( this.threadpool);	
sending finish application request to sub cluster rms 

public FinishApplicationMasterResponse finishApplicationMaster( FinishApplicationMasterRequest request) throws YarnException, IOException {	boolean failedToUnRegister = false;	ExecutorCompletionService<FinishApplicationMasterResponseInfo> compSvc = null;	Set<String> subClusterIds = this.uamPool.getAllUAMIds();	if (subClusterIds.size() > 0) {	final FinishApplicationMasterRequest finishRequest = request;	compSvc = new ExecutorCompletionService<FinishApplicationMasterResponseInfo>( this.threadpool);	for (final String subClusterId : subClusterIds) {	compSvc.submit(new Callable<FinishApplicationMasterResponseInfo>() {	public FinishApplicationMasterResponseInfo call() throws Exception {	
sending finish application request to rm 

if (subClusterIds.size() > 0) {	final FinishApplicationMasterRequest finishRequest = request;	compSvc = new ExecutorCompletionService<FinishApplicationMasterResponseInfo>( this.threadpool);	for (final String subClusterId : subClusterIds) {	compSvc.submit(new Callable<FinishApplicationMasterResponseInfo>() {	public FinishApplicationMasterResponseInfo call() throws Exception {	FinishApplicationMasterResponse uamResponse = null;	try {	uamResponse = uamPool.finishApplicationMaster(subClusterId, finishRequest);	} catch (Throwable e) {	
failed to finish unmanaged application master rm address applicationid 

uamResponse = uamPool.finishApplicationMaster(subClusterId, finishRequest);	} catch (Throwable e) {	}	return new FinishApplicationMasterResponseInfo(uamResponse, subClusterId);	}	});	}	}	FinishApplicationMasterResponse homeResponse = AMRMClientUtils.finishAMWithReRegister(request, this.homeRM, this.amRegistrationRequest, getApplicationContext().getApplicationAttemptId());	if (subClusterIds.size() > 0) {	
waiting for finish application response from sub cluster rms 

});	}	}	FinishApplicationMasterResponse homeResponse = AMRMClientUtils.finishAMWithReRegister(request, this.homeRM, this.amRegistrationRequest, getApplicationContext().getApplicationAttemptId());	if (subClusterIds.size() > 0) {	for (int i = 0; i < subClusterIds.size(); ++i) {	try {	Future<FinishApplicationMasterResponseInfo> future = compSvc.take();	FinishApplicationMasterResponseInfo uamResponse = future.get();	if (LOG.isDebugEnabled()) {	
received finish application response from rm 

try {	Future<FinishApplicationMasterResponseInfo> future = compSvc.take();	FinishApplicationMasterResponseInfo uamResponse = future.get();	if (LOG.isDebugEnabled()) {	}	if (uamResponse.getResponse() == null || !uamResponse.getResponse().getIsUnregistered()) {	failedToUnRegister = true;	}	} catch (Throwable e) {	failedToUnRegister = true;	
failed to finish unmanaged application master applicationid 

private SubClusterId getSubClusterForNode(String nodeName) {	SubClusterId subClusterId = null;	try {	subClusterId = this.subClusterResolver.getSubClusterForNode(nodeName);	} catch (YarnException e) {	
failed to resolve sub cluster for node skipping this node 

private SubClusterId getSubClusterForNode(String nodeName) {	SubClusterId subClusterId = null;	try {	subClusterId = this.subClusterResolver.getSubClusterForNode(nodeName);	} catch (YarnException e) {	return null;	}	if (subClusterId == null) {	
failed to resolve sub cluster for node skipping this node 

private Registrations sendRequestsToSecondaryResourceManagers( Map<SubClusterId, AllocateRequest> requests) throws YarnException, IOException {	Registrations registrations = registerWithNewSubClusters(requests.keySet());	for (Entry<SubClusterId, AllocateRequest> entry : requests.entrySet()) {	final SubClusterId subClusterId = entry.getKey();	if (subClusterId.equals(this.homeSubClusterId)) {	continue;	}	if (!this.uamPool.hasUAMId(subClusterId.getId())) {	
unmanaged am registration not found for sub cluster 

responses = asyncResponseSink.get(subClusterId);	} else {	responses = new ArrayList<>();	asyncResponseSink.put(subClusterId, responses);	}	responses.add(response);	}	try {	policyInterpreter.notifyOfResponse(subClusterId, response);	} catch (YarnException e) {	
notifyofresponse for policy failed for home sub cluster 

ExecutorCompletionService<RegisterApplicationMasterResponseInfo> completionService = new ExecutorCompletionService<>(threadpool);	for (final String subClusterId : newSubClusters) {	completionService .submit(new Callable<RegisterApplicationMasterResponseInfo>() {	public RegisterApplicationMasterResponseInfo call() throws Exception {	YarnConfiguration config = new YarnConfiguration(getConf());	FederationProxyProviderUtil.updateConfForFederation(config, subClusterId);	RegisterApplicationMasterResponse uamResponse = null;	try {	uamResponse = uamPool.createAndRegisterNewUAM(subClusterId, registerRequest, config, appContext.getApplicationAttemptId().getApplicationId(), amRegistrationResponse.getQueue(), appContext.getUser(), homeSubClusterId.toString());	} catch (Throwable e) {	
failed to register application master application 

}	return new RegisterApplicationMasterResponseInfo(uamResponse, SubClusterId.newInstance(subClusterId));	}	});	}	for (int i = 0; i < newSubClusters.size(); ++i) {	try {	Future<RegisterApplicationMasterResponseInfo> future = completionService.take();	RegisterApplicationMasterResponseInfo uamResponse = future.get();	if (LOG.isDebugEnabled()) {	
received register application response from rm 

}	for (int i = 0; i < newSubClusters.size(); ++i) {	try {	Future<RegisterApplicationMasterResponseInfo> future = completionService.take();	RegisterApplicationMasterResponseInfo uamResponse = future.get();	if (LOG.isDebugEnabled()) {	}	if (uamResponse.getResponse() == null) {	failedRegistrations.add(uamResponse.getSubClusterId());	} else {	
successfully registered unmanaged application master applicationid 

Future<RegisterApplicationMasterResponseInfo> future = completionService.take();	RegisterApplicationMasterResponseInfo uamResponse = future.get();	if (LOG.isDebugEnabled()) {	}	if (uamResponse.getResponse() == null) {	failedRegistrations.add(uamResponse.getSubClusterId());	} else {	successfulRegistrations.put(uamResponse.getSubClusterId(), uamResponse.getResponse());	}	} catch (Exception e) {	
failed to register unmanaged application master applicationid 

private AllocateResponse mergeAllocateResponses( AllocateResponse homeResponse) {	if (LOG.isDebugEnabled()) {	
remove containers 

private AllocateResponse mergeAllocateResponses( AllocateResponse homeResponse) {	if (LOG.isDebugEnabled()) {	
adding containers 

private void cacheAllocatedContainers(List<Container> containers, SubClusterId subClusterId) {	for (Container container : containers) {	if (containerIdToSubClusterIdMap.containsKey(container.getId())) {	SubClusterId existingSubClusterId = containerIdToSubClusterIdMap.get(container.getId());	if (existingSubClusterId.equals(subClusterId)) {	
duplicate containerid found in the allocated containers from same sub cluster so ignoring 

private boolean warnIfNotExists(ContainerId containerId, String actionName) {	if (!this.containerIdToSubClusterIdMap.containsKey(containerId)) {	
am is trying to a container that does not exist 

========================= hadoop sample_1729 =========================

} catch(IllegalArgumentException iae) {	String type = iae instanceof NumberFormatException ? "decimal" : "octal or symbolic";	String error = "Unable to parse configuration " + UMASK_LABEL + " with value " + confUmask + " as " + type + " umask.";	LOG.warn(error);	if (oldUmask == Integer.MIN_VALUE) {	throw new IllegalArgumentException(error);	}	}	if(oldUmask != Integer.MIN_VALUE) {	if (umask != oldUmask) {	
configuration key is deprecated convert to using octal or symbolic umask specifications 

========================= hadoop sample_4252 =========================

FileInputFormat.setInputPaths(job, workDir);	FixedLengthInputFormat format = new FixedLengthInputFormat();	format.configure(job);	InputSplit splits[] = format.getSplits(job, 1);	boolean exceptionThrown = false;	for (InputSplit split : splits) {	try {	RecordReader<LongWritable, BytesWritable> reader = format.getRecordReader(split, job, voidReporter);	} catch(IOException ioe) {	exceptionThrown = true;	
exception message 

FixedLengthInputFormat format = new FixedLengthInputFormat();	format.setRecordLength(job, 0);	format.configure(job);	InputSplit splits[] = format.getSplits(job, 1);	boolean exceptionThrown = false;	for (InputSplit split : splits) {	try {	RecordReader<LongWritable, BytesWritable> reader = format.getRecordReader(split, job, voidReporter);	} catch(IOException ioe) {	exceptionThrown = true;	
exception message 

FixedLengthInputFormat format = new FixedLengthInputFormat();	format.setRecordLength(job, -10);	format.configure(job);	InputSplit splits[] = format.getSplits(job, 1);	boolean exceptionThrown = false;	for (InputSplit split : splits) {	try {	RecordReader<LongWritable, BytesWritable> reader = format.getRecordReader(split, job, voidReporter);	} catch(IOException ioe) {	exceptionThrown = true;	
exception message 

if (i > 0) {	if (i == (MAX_TESTS-1)) {	numSplits = (int)(fileSize/Math.floor(recordLength/2));	} else {	if (MAX_TESTS % i == 0) {	numSplits = fileSize/(fileSize - random.nextInt(fileSize));	} else {	numSplits = Math.max(1, fileSize/random.nextInt(Integer.MAX_VALUE));	}	}	
number of splits set to 

InputSplit[] splits = format.getSplits(job, 100);	if (codec != null) {	assertEquals("compressed splits == 1", 1, splits.length);	}	boolean exceptionThrown = false;	for (InputSplit split : splits) {	try {	List<String> results = readSplit(format, split, job);	} catch(IOException ioe) {	exceptionThrown = true;	
exception message 

========================= hadoop sample_5443 =========================

try (OutputStream os = fs.create(PATH)) {	os.write(CONTENTS);	}	BlockLocation[] locations = fs.getFileBlockLocations(PATH, OFFSET, LENGTH);	for (BlockLocation location: locations) {	StorageType[] storageTypes = location.getStorageTypes();	Assert.assertTrue(storageTypes != null && storageTypes.length > 0 && storageTypes[0] == StorageType.DISK);	}	InetSocketAddress addr = cluster.getNameNode().getHttpAddress();	URL url1 = new URL("http", addr.getHostString(), addr.getPort(), WebHdfsFileSystem.PATH_PREFIX + "/foo?op=GETFILEBLOCKLOCATIONS");	
sending getfileblocklocations request 

os.write(CONTENTS);	}	BlockLocation[] locations = fs.getFileBlockLocations(PATH, OFFSET, LENGTH);	for (BlockLocation location: locations) {	StorageType[] storageTypes = location.getStorageTypes();	Assert.assertTrue(storageTypes != null && storageTypes.length > 0 && storageTypes[0] == StorageType.DISK);	}	InetSocketAddress addr = cluster.getNameNode().getHttpAddress();	URL url1 = new URL("http", addr.getHostString(), addr.getPort(), WebHdfsFileSystem.PATH_PREFIX + "/foo?op=GETFILEBLOCKLOCATIONS");	String response1 = getResponse(url1, "GET");	
the output of getfileblocklocations request 

for (BlockLocation location: locations) {	StorageType[] storageTypes = location.getStorageTypes();	Assert.assertTrue(storageTypes != null && storageTypes.length > 0 && storageTypes[0] == StorageType.DISK);	}	InetSocketAddress addr = cluster.getNameNode().getHttpAddress();	URL url1 = new URL("http", addr.getHostString(), addr.getPort(), WebHdfsFileSystem.PATH_PREFIX + "/foo?op=GETFILEBLOCKLOCATIONS");	String response1 = getResponse(url1, "GET");	BlockLocation[] locationArray1 = toBlockLocationArray(response1);	verifyEquals(locations, locationArray1);	URL url2 = new URL("http", addr.getHostString(), addr.getPort(), WebHdfsFileSystem.PATH_PREFIX + "/foo?op=GETFILEBLOCKLOCATIONS" + "&length=" + LENGTH + "&offset=" + OFFSET);	
sending getfileblocklocations request 

StorageType[] storageTypes = location.getStorageTypes();	Assert.assertTrue(storageTypes != null && storageTypes.length > 0 && storageTypes[0] == StorageType.DISK);	}	InetSocketAddress addr = cluster.getNameNode().getHttpAddress();	URL url1 = new URL("http", addr.getHostString(), addr.getPort(), WebHdfsFileSystem.PATH_PREFIX + "/foo?op=GETFILEBLOCKLOCATIONS");	String response1 = getResponse(url1, "GET");	BlockLocation[] locationArray1 = toBlockLocationArray(response1);	verifyEquals(locations, locationArray1);	URL url2 = new URL("http", addr.getHostString(), addr.getPort(), WebHdfsFileSystem.PATH_PREFIX + "/foo?op=GETFILEBLOCKLOCATIONS" + "&length=" + LENGTH + "&offset=" + OFFSET);	String response2 = getResponse(url2, "GET");	
the output of getfileblocklocations request 

InetSocketAddress addr = cluster.getNameNode().getHttpAddress();	URL url1 = new URL("http", addr.getHostString(), addr.getPort(), WebHdfsFileSystem.PATH_PREFIX + "/foo?op=GETFILEBLOCKLOCATIONS");	String response1 = getResponse(url1, "GET");	BlockLocation[] locationArray1 = toBlockLocationArray(response1);	verifyEquals(locations, locationArray1);	URL url2 = new URL("http", addr.getHostString(), addr.getPort(), WebHdfsFileSystem.PATH_PREFIX + "/foo?op=GETFILEBLOCKLOCATIONS" + "&length=" + LENGTH + "&offset=" + OFFSET);	String response2 = getResponse(url2, "GET");	BlockLocation[] locationArray2 = toBlockLocationArray(response2);	verifyEquals(locations, locationArray2);	URL url3 = new URL("http", addr.getHostString(), addr.getPort(), WebHdfsFileSystem.PATH_PREFIX + "/foo?op=GETFILEBLOCKLOCATIONS" + "&length=" + LENGTH);	
sending getfileblocklocations request 

URL url1 = new URL("http", addr.getHostString(), addr.getPort(), WebHdfsFileSystem.PATH_PREFIX + "/foo?op=GETFILEBLOCKLOCATIONS");	String response1 = getResponse(url1, "GET");	BlockLocation[] locationArray1 = toBlockLocationArray(response1);	verifyEquals(locations, locationArray1);	URL url2 = new URL("http", addr.getHostString(), addr.getPort(), WebHdfsFileSystem.PATH_PREFIX + "/foo?op=GETFILEBLOCKLOCATIONS" + "&length=" + LENGTH + "&offset=" + OFFSET);	String response2 = getResponse(url2, "GET");	BlockLocation[] locationArray2 = toBlockLocationArray(response2);	verifyEquals(locations, locationArray2);	URL url3 = new URL("http", addr.getHostString(), addr.getPort(), WebHdfsFileSystem.PATH_PREFIX + "/foo?op=GETFILEBLOCKLOCATIONS" + "&length=" + LENGTH);	String response3 = getResponse(url3, "GET");	
the output of getfileblocklocations request 

verifyEquals(locations, locationArray1);	URL url2 = new URL("http", addr.getHostString(), addr.getPort(), WebHdfsFileSystem.PATH_PREFIX + "/foo?op=GETFILEBLOCKLOCATIONS" + "&length=" + LENGTH + "&offset=" + OFFSET);	String response2 = getResponse(url2, "GET");	BlockLocation[] locationArray2 = toBlockLocationArray(response2);	verifyEquals(locations, locationArray2);	URL url3 = new URL("http", addr.getHostString(), addr.getPort(), WebHdfsFileSystem.PATH_PREFIX + "/foo?op=GETFILEBLOCKLOCATIONS" + "&length=" + LENGTH);	String response3 = getResponse(url3, "GET");	BlockLocation[] locationArray3 = toBlockLocationArray(response3);	verifyEquals(locations, locationArray3);	URL url4 = new URL("http", addr.getHostString(), addr.getPort(), WebHdfsFileSystem.PATH_PREFIX + "/foo?op=GETFILEBLOCKLOCATIONS" + "&offset=" + OFFSET);	
sending getfileblocklocations request 

URL url2 = new URL("http", addr.getHostString(), addr.getPort(), WebHdfsFileSystem.PATH_PREFIX + "/foo?op=GETFILEBLOCKLOCATIONS" + "&length=" + LENGTH + "&offset=" + OFFSET);	String response2 = getResponse(url2, "GET");	BlockLocation[] locationArray2 = toBlockLocationArray(response2);	verifyEquals(locations, locationArray2);	URL url3 = new URL("http", addr.getHostString(), addr.getPort(), WebHdfsFileSystem.PATH_PREFIX + "/foo?op=GETFILEBLOCKLOCATIONS" + "&length=" + LENGTH);	String response3 = getResponse(url3, "GET");	BlockLocation[] locationArray3 = toBlockLocationArray(response3);	verifyEquals(locations, locationArray3);	URL url4 = new URL("http", addr.getHostString(), addr.getPort(), WebHdfsFileSystem.PATH_PREFIX + "/foo?op=GETFILEBLOCKLOCATIONS" + "&offset=" + OFFSET);	String response4 = getResponse(url4, "GET");	
the output of getfileblocklocations request 

verifyEquals(locations, locationArray2);	URL url3 = new URL("http", addr.getHostString(), addr.getPort(), WebHdfsFileSystem.PATH_PREFIX + "/foo?op=GETFILEBLOCKLOCATIONS" + "&length=" + LENGTH);	String response3 = getResponse(url3, "GET");	BlockLocation[] locationArray3 = toBlockLocationArray(response3);	verifyEquals(locations, locationArray3);	URL url4 = new URL("http", addr.getHostString(), addr.getPort(), WebHdfsFileSystem.PATH_PREFIX + "/foo?op=GETFILEBLOCKLOCATIONS" + "&offset=" + OFFSET);	String response4 = getResponse(url4, "GET");	BlockLocation[] locationArray4 = toBlockLocationArray(response4);	verifyEquals(locations, locationArray4);	URL url5 = new URL("http", addr.getHostString(), addr.getPort(), WebHdfsFileSystem.PATH_PREFIX + "/foo?op=GETFILEBLOCKLOCATIONS" + "&offset=1200");	
sending getfileblocklocations request 

URL url3 = new URL("http", addr.getHostString(), addr.getPort(), WebHdfsFileSystem.PATH_PREFIX + "/foo?op=GETFILEBLOCKLOCATIONS" + "&length=" + LENGTH);	String response3 = getResponse(url3, "GET");	BlockLocation[] locationArray3 = toBlockLocationArray(response3);	verifyEquals(locations, locationArray3);	URL url4 = new URL("http", addr.getHostString(), addr.getPort(), WebHdfsFileSystem.PATH_PREFIX + "/foo?op=GETFILEBLOCKLOCATIONS" + "&offset=" + OFFSET);	String response4 = getResponse(url4, "GET");	BlockLocation[] locationArray4 = toBlockLocationArray(response4);	verifyEquals(locations, locationArray4);	URL url5 = new URL("http", addr.getHostString(), addr.getPort(), WebHdfsFileSystem.PATH_PREFIX + "/foo?op=GETFILEBLOCKLOCATIONS" + "&offset=1200");	String response5 = getResponse(url5, "GET");	
the output of getfileblocklocations request 

private void checkResponseContainsLocation(URL url, String TYPE) throws JSONException, IOException {	HttpURLConnection conn = (HttpURLConnection) url.openConnection();	conn.setRequestMethod(TYPE);	conn.setInstanceFollowRedirects(false);	String response = IOUtils.toString(conn.getInputStream());	
response was 

public void testWebHdfsNoRedirect() throws Exception {	MiniDFSCluster cluster = null;	final Configuration conf = WebHdfsTestUtil.createConf();	try {	cluster = new MiniDFSCluster.Builder(conf).numDataNodes(3).build();	
started cluster 

public void testWebHdfsNoRedirect() throws Exception {	MiniDFSCluster cluster = null;	final Configuration conf = WebHdfsTestUtil.createConf();	try {	cluster = new MiniDFSCluster.Builder(conf).numDataNodes(3).build();	InetSocketAddress addr = cluster.getNameNode().getHttpAddress();	URL url = new URL("http", addr.getHostString(), addr.getPort(), WebHdfsFileSystem.PATH_PREFIX + "/testWebHdfsNoRedirectCreate" + "?op=CREATE" + Param.toSortedString("&", new NoRedirectParam(true)));	
sending create request 

URL url = new URL("http", addr.getHostString(), addr.getPort(), WebHdfsFileSystem.PATH_PREFIX + "/testWebHdfsNoRedirectCreate" + "?op=CREATE" + Param.toSortedString("&", new NoRedirectParam(true)));	checkResponseContainsLocation(url, "PUT");	final WebHdfsFileSystem fs = WebHdfsTestUtil.getWebHdfsFileSystem( conf, WebHdfsConstants.WEBHDFS_SCHEME);	final String PATH = "/testWebHdfsNoRedirect";	byte[] CONTENTS = new byte[1024];	RANDOM.nextBytes(CONTENTS);	try (OutputStream os = fs.create(new Path(PATH))) {	os.write(CONTENTS);	}	url = new URL("http", addr.getHostString(), addr.getPort(), WebHdfsFileSystem.PATH_PREFIX + "/testWebHdfsNoRedirect" + "?op=OPEN" + Param.toSortedString("&", new NoRedirectParam(true)));	
sending open request 

final WebHdfsFileSystem fs = WebHdfsTestUtil.getWebHdfsFileSystem( conf, WebHdfsConstants.WEBHDFS_SCHEME);	final String PATH = "/testWebHdfsNoRedirect";	byte[] CONTENTS = new byte[1024];	RANDOM.nextBytes(CONTENTS);	try (OutputStream os = fs.create(new Path(PATH))) {	os.write(CONTENTS);	}	url = new URL("http", addr.getHostString(), addr.getPort(), WebHdfsFileSystem.PATH_PREFIX + "/testWebHdfsNoRedirect" + "?op=OPEN" + Param.toSortedString("&", new NoRedirectParam(true)));	checkResponseContainsLocation(url, "GET");	url = new URL("http", addr.getHostString(), addr.getPort(), WebHdfsFileSystem.PATH_PREFIX + "/testWebHdfsNoRedirect" + "?op=GETFILECHECKSUM" + Param.toSortedString( "&", new NoRedirectParam(true)));	
sending getfilechecksum request 

byte[] CONTENTS = new byte[1024];	RANDOM.nextBytes(CONTENTS);	try (OutputStream os = fs.create(new Path(PATH))) {	os.write(CONTENTS);	}	url = new URL("http", addr.getHostString(), addr.getPort(), WebHdfsFileSystem.PATH_PREFIX + "/testWebHdfsNoRedirect" + "?op=OPEN" + Param.toSortedString("&", new NoRedirectParam(true)));	checkResponseContainsLocation(url, "GET");	url = new URL("http", addr.getHostString(), addr.getPort(), WebHdfsFileSystem.PATH_PREFIX + "/testWebHdfsNoRedirect" + "?op=GETFILECHECKSUM" + Param.toSortedString( "&", new NoRedirectParam(true)));	checkResponseContainsLocation(url, "GET");	url = new URL("http", addr.getHostString(), addr.getPort(), WebHdfsFileSystem.PATH_PREFIX + "/testWebHdfsNoRedirect" + "?op=APPEND" + Param.toSortedString("&", new NoRedirectParam(true)));	
sending append request 

========================= hadoop sample_7184 =========================

public void setup() throws Exception {	long seed = rand.nextLong();	rand.setSeed(seed);	
running with seed 

========================= hadoop sample_588 =========================

private void ignoreDeleteMissingIfUseSnapshotDiff() {	if (deleteMissing && (useDiff || useRdiff)) {	
delete and diff rdiff are mutually exclusive the delete option will be ignored 

========================= hadoop sample_6280 =========================

public void run() {	try {	if (!shouldRun) {	
this cycle terminating immediately because shouldrun has been deactivated 

public void run() {	try {	if (!shouldRun) {	return;	}	reconcile();	} catch (Exception e) {	
exception during directoryscanner execution will continue next cycle 

public void run() {	try {	if (!shouldRun) {	return;	}	reconcile();	} catch (Exception e) {	} catch (Error er) {	
system error during directoryscanner execution permanently terminating periodic scanner 

for (Entry<Integer, Future<ScanInfoPerBlockPool>> report : compilersInProgress.entrySet()) {	Integer index = report.getKey();	try {	dirReports[index] = report.getValue().get();	if (dirReports[index] == null) {	dirReports = null;	break;	}	} catch (Exception ex) {	FsVolumeSpi fsVolumeSpi = volumes.get(index);	
error compiling report for the volume storageid 

dirReports[index] = report.getValue().get();	if (dirReports[index] == null) {	dirReports = null;	break;	}	} catch (Exception ex) {	FsVolumeSpi fsVolumeSpi = volumes.get(index);	}	}	} catch (IOException e) {	
unexpected ioexception by closing fsvolumereference 

private LinkedList<ScanInfo> compileReport(FsVolumeSpi vol, File bpFinalizedDir, File dir, LinkedList<ScanInfo> report) throws InterruptedException {	throttle();	final FileIoProvider fileIoProvider = datanode.getFileIoProvider();	List <String> fileNames;	try {	fileNames = fileIoProvider.listDirectory( volume, dir, BlockDirFilter.INSTANCE);	} catch (IOException ioe) {	
exception occured while compiling report 

private void verifyFileLocation(File actualBlockFile, File bpFinalizedDir, long blockId) {	File expectedBlockDir = DatanodeUtil.idToBlockDir(bpFinalizedDir, blockId);	File actualBlockDir = actualBlockFile.getParentFile();	if (actualBlockDir.compareTo(expectedBlockDir) != 0) {	
block found in invalid directory expected directory actual directory 

========================= hadoop sample_7868 =========================

protected void readTestConfigFile() {	String testConfigFile = getTestFile();	if (testsFromConfigFile == null) {	boolean success = false;	testConfigFile = TEST_CACHE_DATA_DIR + File.separator + testConfigFile;	try {	SAXParser p = (SAXParserFactory.newInstance()).newSAXParser();	p.parse(testConfigFile, getConfigParser());	success = true;	} catch (Exception e) {	
file not found 

private void displayResults() {	
detailed results 

private void displayResults() {	LOG.info("----------------------------------\n");	for (int i = 0; i < testsFromConfigFile.size(); i++) {	CLITestData td = testsFromConfigFile.get(i);	boolean testResult = td.getTestResult();	if (!testResult) {	LOG.info("-------------------------------------------");	
test id 

private void displayResults() {	LOG.info("----------------------------------\n");	for (int i = 0; i < testsFromConfigFile.size(); i++) {	CLITestData td = testsFromConfigFile.get(i);	boolean testResult = td.getTestResult();	if (!testResult) {	LOG.info("-------------------------------------------");	
test description 

private void displayResults() {	LOG.info("----------------------------------\n");	for (int i = 0; i < testsFromConfigFile.size(); i++) {	CLITestData td = testsFromConfigFile.get(i);	boolean testResult = td.getTestResult();	if (!testResult) {	LOG.info("-------------------------------------------");	LOG.info("");	ArrayList<CLICommand> testCommands = td.getTestCommands();	for (CLICommand cmd : testCommands) {	
test commands 

boolean testResult = td.getTestResult();	if (!testResult) {	LOG.info("-------------------------------------------");	LOG.info("");	ArrayList<CLICommand> testCommands = td.getTestCommands();	for (CLICommand cmd : testCommands) {	}	LOG.info("");	ArrayList<CLICommand> cleanupCommands = td.getCleanupCommands();	for (CLICommand cmd : cleanupCommands) {	
cleanup commands 

for (CLICommand cmd : testCommands) {	}	LOG.info("");	ArrayList<CLICommand> cleanupCommands = td.getCleanupCommands();	for (CLICommand cmd : cleanupCommands) {	}	LOG.info("");	ArrayList<ComparatorData> compdata = td.getComparatorData();	for (ComparatorData cd : compdata) {	boolean resultBoolean = cd.getTestResult();	
comparator 

for (CLICommand cmd : testCommands) {	}	LOG.info("");	ArrayList<CLICommand> cleanupCommands = td.getCleanupCommands();	for (CLICommand cmd : cleanupCommands) {	}	LOG.info("");	ArrayList<ComparatorData> compdata = td.getComparatorData();	for (ComparatorData cd : compdata) {	boolean resultBoolean = cd.getTestResult();	
comparision result pass fail 

for (CLICommand cmd : testCommands) {	}	LOG.info("");	ArrayList<CLICommand> cleanupCommands = td.getCleanupCommands();	for (CLICommand cmd : cleanupCommands) {	}	LOG.info("");	ArrayList<ComparatorData> compdata = td.getComparatorData();	for (ComparatorData cd : compdata) {	boolean resultBoolean = cd.getTestResult();	
expected output 

for (CLICommand cmd : testCommands) {	}	LOG.info("");	ArrayList<CLICommand> cleanupCommands = td.getCleanupCommands();	for (CLICommand cmd : cleanupCommands) {	}	LOG.info("");	ArrayList<ComparatorData> compdata = td.getComparatorData();	for (ComparatorData cd : compdata) {	boolean resultBoolean = cd.getTestResult();	
actual output 

for (CLICommand cmd : cleanupCommands) {	}	LOG.info("");	ArrayList<ComparatorData> compdata = td.getComparatorData();	for (ComparatorData cd : compdata) {	boolean resultBoolean = cd.getTestResult();	}	LOG.info("");	}	}	
summary results 

CLITestData td = testsFromConfigFile.get(i);	totalComparators += testsFromConfigFile.get(i).getComparatorData().size();	boolean resultBoolean = td.getTestResult();	if (resultBoolean) {	totalPass ++;	} else {	totalFail ++;	}	overallResults &= resultBoolean;	}	
testing mode 

totalComparators += testsFromConfigFile.get(i).getComparatorData().size();	boolean resultBoolean = td.getTestResult();	if (resultBoolean) {	totalPass ++;	} else {	totalFail ++;	}	overallResults &= resultBoolean;	}	LOG.info("");	
overall result pass fail 

boolean resultBoolean = td.getTestResult();	if (resultBoolean) {	totalPass ++;	} else {	totalFail ++;	}	overallResults &= resultBoolean;	}	LOG.info("");	if ((totalPass + totalFail) == 0) {	
tests pass 

boolean resultBoolean = td.getTestResult();	if (resultBoolean) {	totalPass ++;	} else {	totalFail ++;	}	overallResults &= resultBoolean;	}	LOG.info("");	if ((totalPass + totalFail) == 0) {	
tests fail 

totalPass ++;	} else {	totalFail ++;	}	overallResults &= resultBoolean;	}	LOG.info("");	if ((totalPass + totalFail) == 0) {	}	else {	
tests pass 

totalPass ++;	} else {	totalFail ++;	}	overallResults &= resultBoolean;	}	LOG.info("");	if ((totalPass + totalFail) == 0) {	}	else {	
tests fail 

} else {	totalFail ++;	}	overallResults &= resultBoolean;	}	LOG.info("");	if ((totalPass + totalFail) == 0) {	}	else {	}	
validations done each test may do multiple validations 

totalFail ++;	}	overallResults &= resultBoolean;	}	LOG.info("");	if ((totalPass + totalFail) == 0) {	}	else {	}	LOG.info("");	
failing tests 

int i = 0;	boolean foundTests = false;	for (i = 0; i < testsFromConfigFile.size(); i++) {	boolean resultBoolean = testsFromConfigFile.get(i).getTestResult();	if (!resultBoolean) {	LOG.info((i + 1) + ": " + testsFromConfigFile.get(i).getTestDesc());	foundTests = true;	}	}	if (!foundTests) {	
NONE 

boolean resultBoolean = testsFromConfigFile.get(i).getTestResult();	if (!resultBoolean) {	LOG.info((i + 1) + ": " + testsFromConfigFile.get(i).getTestDesc());	foundTests = true;	}	}	if (!foundTests) {	}	foundTests = false;	LOG.info("");	
passing tests 

LOG.info("");	LOG.info("--------------");	for (i = 0; i < testsFromConfigFile.size(); i++) {	boolean resultBoolean = testsFromConfigFile.get(i).getTestResult();	if (resultBoolean) {	LOG.info((i + 1) + ": " + testsFromConfigFile.get(i).getTestDesc());	foundTests = true;	}	}	if (!foundTests) {	
NONE 

private boolean compareTestOutput(ComparatorData compdata, Result cmdResult) {	String comparatorType = compdata.getComparatorType();	Class<?> comparatorClass = null;	boolean compareOutput = false;	if (testMode.equals(TESTMODE_TEST)) {	try {	comparatorClass = Class.forName("org.apache.hadoop.cli.util." + comparatorType);	ComparatorBase comp = (ComparatorBase) comparatorClass.newInstance();	compareOutput = comp.compare(cmdResult.getCommandOutput(), expandCommand(compdata.getExpectedOutput()));	} catch (Exception e) {	
error in instantiating the comparator 

public void testAll() {	assertTrue("Number of tests has to be greater then zero", testsFromConfigFile.size() > 0);	
TestAll 

========================= hadoop sample_3082 =========================

}	HttpUriRequest httpUriRequest = null;	switch (httpMethod) {	case HttpPut.METHOD_NAME: httpUriRequest = new HttpPut(uriBuilder.build());	break;	case HttpPost.METHOD_NAME: httpUriRequest = new HttpPost(uriBuilder.build());	break;	default: httpUriRequest = new HttpGet(uriBuilder.build());	break;	}	
securewasbremotecallhelper gethttprequest 

private Token<?> getDelegationToken( UserGroupInformation userGroupInformation) throws IOException {	if (this.delegationToken == null) {	Token<?> token = null;	for (Token iterToken : userGroupInformation.getTokens()) {	if (iterToken.getKind() .equals(WasbDelegationTokenIdentifier.TOKEN_KIND)) {	token = iterToken;	
token found in cache 

private Token<?> getDelegationToken( UserGroupInformation userGroupInformation) throws IOException {	if (this.delegationToken == null) {	Token<?> token = null;	for (Token iterToken : userGroupInformation.getTokens()) {	if (iterToken.getKind() .equals(WasbDelegationTokenIdentifier.TOKEN_KIND)) {	token = iterToken;	break;	}	}	
ugi information 

private Token<?> getDelegationToken( UserGroupInformation userGroupInformation) throws IOException {	if (this.delegationToken == null) {	Token<?> token = null;	for (Token iterToken : userGroupInformation.getTokens()) {	if (iterToken.getKind() .equals(WasbDelegationTokenIdentifier.TOKEN_KIND)) {	token = iterToken;	break;	}	}	if (token != null) {	
using ugi token 

========================= hadoop sample_6404 =========================

public void testMigrateFileToArchival() throws Exception {	
testMigrateFileToArchival 

public void testMoveSpecificPaths() throws Exception {	
testMoveSpecificPaths 

public void testMigrateOpenFileToArchival() throws Exception {	
testMigrateOpenFileToArchival 

final Path barFile = new Path(fooDir, "bar");	DFSTestUtil.createFile(test.dfs, barFile, BLOCK_SIZE, (short) 1, 0L);	FSDataOutputStream out = test.dfs.append(barFile);	out.writeBytes("hello, ");	((DFSOutputStream) out.getWrappedStream()).hsync();	try {	banner("start data migration");	test.setStoragePolicy();	test.migrate(ExitStatus.SUCCESS);	LocatedBlocks lbs = test.dfs.getClient().getLocatedBlocks( barFile.toString(), BLOCK_SIZE);	
locations 

test.migrate(ExitStatus.SUCCESS);	LocatedBlocks lbs = test.dfs.getClient().getLocatedBlocks( barFile.toString(), BLOCK_SIZE);	List<LocatedBlock> blks = lbs.getLocatedBlocks();	Assert.assertEquals(1, blks.size());	Assert.assertEquals(1, blks.get(0).getLocations().length);	banner("finish the migration, continue writing");	out.writeBytes("world!");	((DFSOutputStream) out.getWrappedStream()).hsync();	IOUtils.cleanup(LOG, out);	lbs = test.dfs.getClient().getLocatedBlocks( barFile.toString(), BLOCK_SIZE);	
locations 

public void testHotWarmColdDirs() throws Exception {	
testHotWarmColdDirs 

private void waitForAllReplicas(int expectedReplicaNum, Path file, DistributedFileSystem dfs, int retryCount) throws Exception {	
waiting for replicas count file name 

private void setVolumeFull(DataNode dn, StorageType type) {	try (FsDatasetSpi.FsVolumeReferences refs = dn.getFSDataset() .getFsVolumeReferences()) {	for (FsVolumeSpi fvs : refs) {	FsVolumeImpl volume = (FsVolumeImpl) fvs;	if (volume.getStorageType() == type) {	
setcapacity to for 

private void setVolumeFull(DataNode dn, StorageType type) {	try (FsDatasetSpi.FsVolumeReferences refs = dn.getFSDataset() .getFsVolumeReferences()) {	for (FsVolumeSpi fvs : refs) {	FsVolumeImpl volume = (FsVolumeImpl) fvs;	if (volume.getStorageType() == type) {	volume.setCapacityForTesting(0);	}	}	} catch (IOException e) {	
unexpected exception by closing fsvolumereference 

public void testNoSpaceDisk() throws Exception {	
testNoSpaceDisk 

public void testNoSpaceArchive() throws Exception {	
testNoSpaceArchive 

========================= hadoop sample_7215 =========================

entityToStorePrep.setEntityType("TEST_ENTITY_TYPE_PREP");	entityToStorePrep.setEntityId("TEST_ENTITY_ID_PREP");	entityToStorePrep.setDomainId("TEST_DOMAIN");	entityToStorePrep.addRelatedEntity("TEST_ENTITY_TYPE_2", "TEST_ENTITY_ID_2");	entityToStorePrep.setStartTime(0L);	TimelineEntities entitiesPrep = new TimelineEntities();	entitiesPrep.addEntity(entityToStorePrep);	store.put(entitiesPrep);	long start = System.currentTimeMillis();	int num = 1000000;	
start test for 

entityToStore.addOtherInfo("containerId", "container_1429158534256_0001_01_000002");	entityToStore.addOtherInfo("status", "RUNNING");	entityToStore.addRelatedEntity(tezTaskId, "TEZ_TASK_ID_1");	TimelineEntities entities = new TimelineEntities();	entities.addEntity(entityToStore);	for (int i = 0; i < num; ++i) {	entityToStore.setEntityId(tezEntityId + i);	store.put(entities);	}	long duration = System.currentTimeMillis() - start;	
duration for 

========================= hadoop sample_2017 =========================

locations = fs.getFileBlockLocations(stat, 0, stat.getLen());	}	if (locations == null) {	blocks = new OneBlockInfo[0];	} else {	if(locations.length == 0 && !stat.isDirectory()) {	locations = new BlockLocation[] { new BlockLocation() };	}	if (!isSplitable) {	if (LOG.isDebugEnabled()) {	
file is not splittable so no parallelization is possible 

========================= hadoop sample_4985 =========================

this.priority = priority;	taskId = application.getNextTaskId();	state = State.PENDING;	if (!(hosts.length == 1 && hosts[0].equals(ResourceRequest.ANY))) {	for (String host : hosts) {	this.hosts.add(host);	this.racks.add(Application.resolve(host));	}	}	this.schedulerKey = TestUtils.toSchedulerKey(priority.getPriority());	
task added to application with hosts racks 

========================= hadoop sample_619 =========================

fsTarget.delete(targetOfTests, true);	fsTarget.mkdir(targetOfTests, FileContext.DEFAULT_PERM, true);	Configuration conf = new Configuration();	String testDir = helper.getTestRootPath(fsTarget).toUri() .getPath();	linkUpFirstComponents(conf, testDir, fsTarget, "test dir");	setUpHomeDir(conf, fsTarget);	String wdDir = fsTarget.getWorkingDirectory().toUri().getPath();	linkUpFirstComponents(conf, wdDir, fsTarget, "working dir");	FileContext fc = FileContext.getFileContext(FsConstants.VIEWFS_URI, conf);	fc.setWorkingDirectory(new Path(wdDir));	
working dir is 

static void setUpHomeDir(Configuration conf, FileContext fsTarget) {	String homeDir = fsTarget.getHomeDirectory().toUri().getPath();	int indexOf2ndSlash = homeDir.indexOf('/', 1);	if (indexOf2ndSlash >0) {	linkUpFirstComponents(conf, homeDir, fsTarget, "home dir");	} else {	URI linkTarget = fsTarget.makeQualified(new Path(homeDir)).toUri();	ConfigUtil.addLink(conf, homeDir, linkTarget);	
added link for home dir 

String homeDir = fsTarget.getHomeDirectory().toUri().getPath();	int indexOf2ndSlash = homeDir.indexOf('/', 1);	if (indexOf2ndSlash >0) {	linkUpFirstComponents(conf, homeDir, fsTarget, "home dir");	} else {	URI linkTarget = fsTarget.makeQualified(new Path(homeDir)).toUri();	ConfigUtil.addLink(conf, homeDir, linkTarget);	}	String homeDirRoot = fsTarget.getHomeDirectory().getParent().toUri().getPath();	ConfigUtil.setHomeDirConf(conf, homeDirRoot);	
home dir base for viewfs 

static void linkUpFirstComponents(Configuration conf, String path, FileContext fsTarget, String info) {	int indexOfEnd = path.indexOf('/', 1);	if (Shell.WINDOWS) {	indexOfEnd = path.indexOf('/', indexOfEnd + 1);	}	String firstComponent = path.substring(0, indexOfEnd);	URI linkTarget = fsTarget.makeQualified(new Path(firstComponent)).toUri();	ConfigUtil.addLink(conf, firstComponent, linkTarget);	
added link for 

========================= hadoop sample_3313 =========================

protected int writeInputData(long genbytes, Path inputDir) throws IOException, InterruptedException {	if (genbytes > 0) {	final Configuration conf = getConf();	if (inputDir.getFileSystem(conf).exists(inputDir)) {	
gridmix input data directory already exists when generate option is used 

protected int writeInputData(long genbytes, Path inputDir) throws IOException, InterruptedException {	if (genbytes > 0) {	final Configuration conf = getConf();	if (inputDir.getFileSystem(conf).exists(inputDir)) {	return STARTUP_FAILED_ERROR;	}	CompressionEmulationUtil.setupDataGeneratorConfig(conf);	final GenerateData genData = new GenerateData(conf, inputDir, genbytes);	
generating of test data 

if (genbytes > 0) {	final Configuration conf = getConf();	if (inputDir.getFileSystem(conf).exists(inputDir)) {	return STARTUP_FAILED_ERROR;	}	CompressionEmulationUtil.setupDataGeneratorConfig(conf);	final GenerateData genData = new GenerateData(conf, inputDir, genbytes);	launchGridmixJob(genData);	FsShell shell = new FsShell(conf);	try {	
changing the permissions for inputpath 

if (inputDir.getFileSystem(conf).exists(inputDir)) {	return STARTUP_FAILED_ERROR;	}	CompressionEmulationUtil.setupDataGeneratorConfig(conf);	final GenerateData genData = new GenerateData(conf, inputDir, genbytes);	launchGridmixJob(genData);	FsShell shell = new FsShell(conf);	try {	shell.run(new String[] {"-chmod","-R","777", inputDir.toString()});	} catch (Exception e) {	
couldnt change the file permissions 

}	CompressionEmulationUtil.setupDataGeneratorConfig(conf);	final GenerateData genData = new GenerateData(conf, inputDir, genbytes);	launchGridmixJob(genData);	FsShell shell = new FsShell(conf);	try {	shell.run(new String[] {"-chmod","-R","777", inputDir.toString()});	} catch (Exception e) {	throw new IOException(e);	}	
input data generation successful 

protected void writeDistCacheData(Configuration conf) throws IOException, InterruptedException {	int fileCount = conf.getInt(GenerateDistCacheData.GRIDMIX_DISTCACHE_FILE_COUNT, -1);	if (fileCount > 0) {	final GridmixJob genDistCacheData = new GenerateDistCacheData(conf);	
generating distributed cache data of size 

private void startThreads(Configuration conf, String traceIn, Path ioPath, Path scratchDir, CountDownLatch startFlag, UserResolver userResolver) throws IOException {	try {	Path inputDir = getGridmixInputDataPath(ioPath);	GridmixJobSubmissionPolicy policy = getJobSubmissionPolicy(conf);	
submission policy is 

if (policy == GridmixJobSubmissionPolicy.SERIAL) {	statistics.addJobStatsListeners(factory);	} else {	statistics.addClusterStatsObservers(factory);	}	statistics.addJobStatsListeners(summarizer.getExecutionSummarizer());	statistics.addClusterStatsObservers(summarizer.getClusterSummarizer());	monitor.start();	submitter.start();	}catch(Exception e) {	
exception at start 

private int runJob(Configuration conf, String[] argv) throws IOException, InterruptedException {	if (argv.length < 2) {	
too few arguments to gridmix 

long genbytes = -1L;	String traceIn = null;	Path ioPath = null;	URI userRsrc = null;	try {	userResolver = ReflectionUtils.newInstance(conf.getClass(GRIDMIX_USR_RSV, SubmitterUserResolver.class, UserResolver.class), conf);	for (int i = 0; i < argv.length - 2; ++i) {	if ("-generate".equals(argv[i])) {	genbytes = StringUtils.TraditionalBinaryPrefix.string2long(argv[++i]);	if (genbytes <= 0) {	
size of input data to be generated specified using generate option should be nonnegative 

userResolver = ReflectionUtils.newInstance(conf.getClass(GRIDMIX_USR_RSV, SubmitterUserResolver.class, UserResolver.class), conf);	for (int i = 0; i < argv.length - 2; ++i) {	if ("-generate".equals(argv[i])) {	genbytes = StringUtils.TraditionalBinaryPrefix.string2long(argv[++i]);	if (genbytes <= 0) {	return ARGS_ERROR;	}	} else if ("-users".equals(argv[i])) {	userRsrc = new URI(argv[++i]);	} else {	
unknown option specified 

} else if ("-users".equals(argv[i])) {	userRsrc = new URI(argv[++i]);	} else {	printUsage(System.err);	return ARGS_ERROR;	}	}	if (userResolver.needsTargetUsersList()) {	if (userRsrc != null) {	if (!userResolver.setTargetUsers(userRsrc, conf)) {	
ignoring the user resource 

} else {	printUsage(System.err);	return ARGS_ERROR;	}	}	if (userResolver.needsTargetUsersList()) {	if (userRsrc != null) {	if (!userResolver.setTargetUsers(userRsrc, conf)) {	}	} else {	
needs target user list use users option 

}	if (userResolver.needsTargetUsersList()) {	if (userRsrc != null) {	if (!userResolver.setTargetUsers(userRsrc, conf)) {	}	} else {	printUsage(System.err);	return ARGS_ERROR;	}	} else if (userRsrc != null) {	
ignoring the user resource 

return ARGS_ERROR;	}	final FileSystem inputFs = ioPath.getFileSystem(conf);	ioPath = ioPath.makeQualified(inputFs);	boolean succeeded = false;	try {	succeeded = FileSystem.mkdirs(inputFs, ioPath, new FsPermission((short)0777));	} catch(IOException e) {	} finally {	if (!succeeded) {	
failed creation of iopath directory 

private void killComponent(Component<?> component, long maxwait) {	if (component == null) {	return;	}	component.abort();	try {	component.join(maxwait);	} catch (InterruptedException e) {	
interrupted waiting for 

public void run() {	
exiting 

killComponent(monitor, MON_SLEEP);	killComponent(statistics,MON_SLEEP);	} finally {	if (monitor == null) {	return;	}	List<JobStats> remainingJobs = monitor.getRemainingJobs();	if (remainingJobs.isEmpty()) {	return;	}	
killing running jobs 

}	List<JobStats> remainingJobs = monitor.getRemainingJobs();	if (remainingJobs.isEmpty()) {	return;	}	for (JobStats stats : remainingJobs) {	Job job = stats.getJob();	try {	if (!job.isComplete()) {	job.killJob();	
killed 

if (!job.isComplete()) {	job.killJob();	} else {	if (job.isSuccessful()) {	monitor.onSuccess(job);	} else {	monitor.onFailure(job);	}	}	} catch (IOException e) {	
failure killing 

job.killJob();	} else {	if (job.isSuccessful()) {	monitor.onSuccess(job);	} else {	monitor.onFailure(job);	}	}	} catch (IOException e) {	} catch (Exception e) {	
unexpected exception 

if (job.isSuccessful()) {	monitor.onSuccess(job);	} else {	monitor.onFailure(job);	}	}	} catch (IOException e) {	} catch (Exception e) {	}	}	
done 

========================= hadoop sample_6111 =========================

public boolean validate() {	boolean ret = super.validate();	if (this.getSourcePath() == null || this.getSourcePath().length() == 0) {	
invalid entry no source path specified 

public boolean validate() {	boolean ret = super.validate();	if (this.getSourcePath() == null || this.getSourcePath().length() == 0) {	ret = false;	}	if (!this.getSourcePath().startsWith("/")) {	
invalid entry all mount points must start with 

public boolean validate() {	boolean ret = super.validate();	if (this.getSourcePath() == null || this.getSourcePath().length() == 0) {	ret = false;	}	if (!this.getSourcePath().startsWith("/")) {	ret = false;	}	if (this.getDestinations() == null || this.getDestinations().size() == 0) {	
invalid entry no destination paths specified 

}	if (!this.getSourcePath().startsWith("/")) {	ret = false;	}	if (this.getDestinations() == null || this.getDestinations().size() == 0) {	ret = false;	}	for (RemoteLocation loc : getDestinations()) {	String nsId = loc.getNameserviceId();	if (nsId == null || nsId.length() == 0) {	
invalid entry invalid destination nameservice 

}	if (this.getDestinations() == null || this.getDestinations().size() == 0) {	ret = false;	}	for (RemoteLocation loc : getDestinations()) {	String nsId = loc.getNameserviceId();	if (nsId == null || nsId.length() == 0) {	ret = false;	}	if (loc.getDest() == null || loc.getDest().length() == 0) {	
invalid entry invalid destination path 

}	for (RemoteLocation loc : getDestinations()) {	String nsId = loc.getNameserviceId();	if (nsId == null || nsId.length() == 0) {	ret = false;	}	if (loc.getDest() == null || loc.getDest().length() == 0) {	ret = false;	}	if (!loc.getDest().startsWith("/")) {	
invalid entry all destination must start with 

========================= hadoop sample_8226 =========================

public static void setup() throws IOException {	dfs = new MiniDFSCluster.Builder(conf).build();	fileSys = dfs.getFileSystem();	if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {	
mrappjar not found not running test 

fileSys.close();	}	if (dfs != null) {	dfs.shutdown();	}	if (mr != null) {	mr.stop();	mr = null;	}	} catch (IOException ioe) {	
io exception in closing file system 

========================= hadoop sample_5414 =========================

public void testMaintenanceMinReplConfigRange() {	
setting testmaintenanceminreplconfigrange 

public void testMaintenanceMinReplConfigRange() {	setMinMaintenanceR(-1);	try {	startCluster(1, 1);	fail("Cluster start should fail when 'dfs.namenode.maintenance" + ".replication.min=-1'");	} catch (IOException e) {	
expected exception 

startCluster(1, 1);	fail("Cluster start should fail when 'dfs.namenode.maintenance" + ".replication.min=-1'");	} catch (IOException e) {	}	int defaultRepl = getConf().getInt( DFSConfigKeys.DFS_REPLICATION_KEY, DFSConfigKeys.DFS_REPLICATION_DEFAULT);	setMinMaintenanceR(defaultRepl + 1);	try {	startCluster(1, 1);	fail("Cluster start should fail when 'dfs.namenode.maintenance" + ".replication.min > " + defaultRepl + "'");	} catch (IOException e) {	
expected exception 

public void testTakeNodeOutOfEnteringMaintenance() throws Exception {	
starting testtakenodeoutofenteringmaintenance 

public void testEnteringMaintenanceExpiration() throws Exception {	
starting testenteringmaintenanceexpiration 

public void testInvalidExpiration() throws Exception {	
starting testinvalidexpiration 

public void testPutDeadNodeToMaintenance() throws Exception {	
starting testputdeadnodetomaintenance 

public void testPutDeadNodeToMaintenanceWithExpiration() throws Exception {	
starting testputdeadnodetomaintenancewithexpiration 

public void testTransitionFromDecommissioned() throws IOException {	
starting testtransitionfromdecommissioned 

public void testTransitionFromDecommissionedAndExpired() throws IOException {	
starting testtransitionfromdecommissionedandexpired 

public void testNodeDeadWhenInEnteringMaintenance() throws Exception {	
starting testnodedeadwheninenteringmaintenance 

public void testExpectedReplications() throws IOException {	
starting testexpectedreplications 

public void testZeroMinMaintenanceReplication() throws Exception {	
starting testzerominmaintenancereplication 

public void testZeroMinMaintenanceReplicationWithExpiration() throws Exception {	
starting testzerominmaintenancereplicationwithexpiration 

private void testFileBlockReplicationImpl( int maintenanceMinRepl, int numDataNodes, int numNewDataNodes, int fileBlockRepl) throws Exception {	setup();	
starting testlargerminmaintenancereplication maintminrepl numdns numnewdns filerepl 

private void testFileBlockReplicationImpl( int maintenanceMinRepl, int numDataNodes, int numNewDataNodes, int fileBlockRepl) throws Exception {	setup();	
setting maintenance minimum replication 

public void testTransitionToDecommission() throws IOException {	
starting testtransitiontodecommission 

public void testTransitionFromDecommissioning() throws IOException {	
starting testtransitionfromdecommissioning 

private void testChangeReplicationFactor(int oldFactor, int newFactor, int expectedLiveReplicas) throws IOException {	setup();	
starting testchangereplicationfactor 

public void testTakeDeadNodeOutOfMaintenance() throws Exception {	
starting testtakedeadnodeoutofmaintenance 

public void testWithNNAndDNRestart() throws Exception {	
starting testwithnnanddnrestart 

public void testWriteAfterMaintenance() throws IOException {	
starting testwriteaftermaintenance 

public void testEnterMaintenanceWhenFileOpen() throws Exception {	
starting testentermaintenancewhenfileopen 

public void testInvalidation() throws IOException {	
starting testinvalidation 

public void testFileCloseAfterEnteringMaintenance() throws Exception {	
starting testfilecloseafterenteringmaintenance 

public void testReportMaintenanceNodes() throws Exception {	ByteArrayOutputStream out = new ByteArrayOutputStream();	ByteArrayOutputStream err = new ByteArrayOutputStream();	System.setOut(new PrintStream(out));	System.setErr(new PrintStream(err));	
starting testreportmaintenancenodes 

========================= hadoop sample_7583 =========================

public static MultipleLinearRandomRetry parseCommaSeparatedString(String s) {	final String[] elements = s.split(",");	if (elements.length == 0) {	
illegal value there is no element in s 

public static MultipleLinearRandomRetry parseCommaSeparatedString(String s) {	final String[] elements = s.split(",");	if (elements.length == 0) {	return null;	}	if (elements.length % 2 != 0) {	
illegal value the number of elements in s is but an even number of elements is expected 

private static int parsePositiveInt(final String[] elements, final int i, final String originalString) {	final String s = elements[i].trim();	final int n;	try {	n = Integer.parseInt(s);	} catch(NumberFormatException nfe) {	
failed to parse s which is the index element in originalstring 

========================= hadoop sample_3893 =========================

StringBuilder configStringBuilder = new StringBuilder();	String prefix = "";	for (ConfigurationPair pair : info.configPairs) {	configStringBuilder.append(prefix).append(pair.getKey()). append(" = ").append(pair.getValue());	prefix = ", ";	}	SpanReceiver rcvr = null;	try {	rcvr = new SpanReceiver.Builder(TraceUtils.wrapHadoopConf( confPrefix, conf, info.configPairs)). className(info.getClassName().trim()). build();	} catch (RuntimeException e) {	
failed to add spanreceiver with configuration 

configStringBuilder.append(prefix).append(pair.getKey()). append(" = ").append(pair.getValue());	prefix = ", ";	}	SpanReceiver rcvr = null;	try {	rcvr = new SpanReceiver.Builder(TraceUtils.wrapHadoopConf( confPrefix, conf, info.configPairs)). className(info.getClassName().trim()). build();	} catch (RuntimeException e) {	throw e;	}	TracerPool.getGlobalTracerPool().addReceiver(rcvr);	
successfully added spanreceiver with configuration 

public synchronized void removeSpanReceiver(long spanReceiverId) throws IOException {	SpanReceiver[] receivers = TracerPool.getGlobalTracerPool().getReceivers();	for (SpanReceiver receiver : receivers) {	if (receiver.getId() == spanReceiverId) {	TracerPool.getGlobalTracerPool().removeAndCloseReceiver(receiver);	
successfully removed spanreceiver with class 

========================= hadoop sample_3485 =========================

public String getQueueForApp(ApplicationSubmissionContext asc, String user) throws YarnException {	String queueName = asc.getQueue();	ApplicationId applicationId = asc.getApplicationId();	if (mappings != null && mappings.size() > 0) {	try {	String mappedQueue = getMappedQueue(user);	if (mappedQueue != null) {	if (queueName.equals(YarnConfiguration.DEFAULT_QUEUE_NAME) || overrideWithQueueMappings) {	
application user mapping to override 

========================= hadoop sample_1142 =========================

Map<CGroupController, String> cPaths;	try {	if (this.cGroupMountPath != null && !this.enableCGroupMount) {	newMtab = ResourceHandlerModule. parseConfiguredCGroupPath(this.cGroupMountPath);	}	if (newMtab == null) {	newMtab = parseMtab(mtabFile);	}	cPaths = initializeControllerPathsFromMtab(newMtab);	} catch (IOException e) {	
failed to initialize controller paths exception 

Set<String> cgroupList = new HashSet<>(Arrays.asList(options.split(",")));	cgroupList.retainAll(validCgroups);	ret.put(path, cgroupList);	}	}	}	} catch (IOException e) {	if (Shell.LINUX) {	throw new IOException("Error while reading " + mtab, e);	} else {	
error while reading 

static String findControllerInMtab(String controller, Map<String, Set<String>> entries) {	for (Map.Entry<String, Set<String>> e : entries.entrySet()) {	if (e.getValue().contains(controller)) {	if (new File(e.getKey()).canRead()) {	return e.getKey();	} else {	
skipping inaccessible cgroup mount point s 

String mountOptions;	if (existingMountPath != null) {	mountOptions = Joiner.on(',') .join(parsedMtab.get(existingMountPath));	} else {	mountOptions = controller.getName();	}	String cGroupKV = mountOptions + "=" + requestedMountPath;	PrivilegedOperation.OperationType opType = PrivilegedOperation .OperationType.MOUNT_CGROUPS;	PrivilegedOperation op = new PrivilegedOperation(opType);	op.appendArgs(cGroupPrefix, cGroupKV);	
mounting controller at 

} else {	mountOptions = controller.getName();	}	String cGroupKV = mountOptions + "=" + requestedMountPath;	PrivilegedOperation.OperationType opType = PrivilegedOperation .OperationType.MOUNT_CGROUPS;	PrivilegedOperation op = new PrivilegedOperation(opType);	op.appendArgs(cGroupPrefix, cGroupKV);	privilegedOperationExecutor.executePrivilegedOperation(op, false);	controllerPaths.put(controller, requestedMountPath);	} catch (PrivilegedOperationException e) {	
failed to mount controller 

PrivilegedOperation op = new PrivilegedOperation(opType);	op.appendArgs(cGroupPrefix, cGroupKV);	privilegedOperationExecutor.executePrivilegedOperation(op, false);	controllerPaths.put(controller, requestedMountPath);	} catch (PrivilegedOperationException e) {	throw new ResourceHandlerException("Failed to mount controller: " + controller.getName());	} finally {	rwLock.writeLock().unlock();	}	} else {	
cgroup controller already mounted at 

private void initializePreMountedCGroupController(CGroupController controller) throws ResourceHandlerException {	String controllerPath = getControllerPath(controller);	if (controllerPath == null) {	throw new ResourceHandlerException( String.format("Controller %s not mounted." + " You either need to mount it with %s" + " or mount cgroups before launching Yarn", controller.getName(), YarnConfiguration. NM_LINUX_CONTAINER_CGROUPS_MOUNT));	}	File rootHierarchy = new File(controllerPath);	File yarnHierarchy = new File(rootHierarchy, cGroupPrefix);	String subsystemName = controller.getName();	
initializing mounted controller at 

String controllerPath = getControllerPath(controller);	if (controllerPath == null) {	throw new ResourceHandlerException( String.format("Controller %s not mounted." + " You either need to mount it with %s" + " or mount cgroups before launching Yarn", controller.getName(), YarnConfiguration. NM_LINUX_CONTAINER_CGROUPS_MOUNT));	}	File rootHierarchy = new File(controllerPath);	File yarnHierarchy = new File(rootHierarchy, cGroupPrefix);	String subsystemName = controller.getName();	if (!rootHierarchy.exists()) {	throw new ResourceHandlerException(getErrorWithDetails( "Cgroups mount point does not exist or not accessible", subsystemName, rootHierarchy.getAbsolutePath() ));	} else if (!yarnHierarchy.exists()) {	
yarn control group does not exist creating 

public String createCGroup(CGroupController controller, String cGroupId) throws ResourceHandlerException {	String path = getPathForCGroup(controller, cGroupId);	if (LOG.isDebugEnabled()) {	
createcgroup 

private boolean checkAndDeleteCgroup(File cgf) throws InterruptedException {	boolean deleted = false;	try (FileInputStream in = new FileInputStream(cgf + "/tasks")) {	if (in.read() == -1) {	Thread.sleep(deleteCGroupDelay);	deleted = cgf.delete();	if (!deleted) {	
failed attempt to delete cgroup 

try (FileInputStream in = new FileInputStream(cgf + "/tasks")) {	if (in.read() == -1) {	Thread.sleep(deleteCGroupDelay);	deleted = cgf.delete();	if (!deleted) {	}	} else {	logLineFromTasksFile(cgf);	}	} catch (IOException e) {	
failed to read cgroup tasks file 

public void deleteCGroup(CGroupController controller, String cGroupId) throws ResourceHandlerException {	boolean deleted = false;	String cGroupPath = getPathForCGroup(controller, cGroupId);	if (LOG.isDebugEnabled()) {	
deletecgroup 

do {	try {	deleted = checkAndDeleteCgroup(new File(cGroupPath));	if (!deleted) {	Thread.sleep(deleteCGroupDelay);	}	} catch (InterruptedException ex) {	}	} while (!deleted && (clock.getTime() - start) < deleteCGroupTimeout);	if (!deleted) {	
unable to delete s tried to delete for d ms 

public void updateCGroupParam(CGroupController controller, String cGroupId, String param, String value) throws ResourceHandlerException {	String cGroupParamPath = getPathForCGroupParam(controller, cGroupId, param);	PrintWriter pw = null;	if (LOG.isDebugEnabled()) {	
updatecgroupparam for path s with value s 

========================= hadoop sample_1841 =========================

private Path createFile(final String filestr, final int size, final boolean triggerLeaseRenewerInterrupt) throws IOException, InterruptedException {	AppendTestUtil.LOG.info("filestr=" + filestr);	Path filepath = new Path(filestr);	FSDataOutputStream stm = dfs.create(filepath, true, BUF_SIZE, REPLICATION_NUM, BLOCK_SIZE);	assertTrue(dfs.dfs.exists(filestr));	AppendTestUtil.LOG.info("size=" + size);	stm.write(buffer, 0, size);	
hflush 

private Path createFile(final String filestr, final int size, final boolean triggerLeaseRenewerInterrupt) throws IOException, InterruptedException {	AppendTestUtil.LOG.info("filestr=" + filestr);	Path filepath = new Path(filestr);	FSDataOutputStream stm = dfs.create(filepath, true, BUF_SIZE, REPLICATION_NUM, BLOCK_SIZE);	assertTrue(dfs.dfs.exists(filestr));	AppendTestUtil.LOG.info("size=" + size);	stm.write(buffer, 0, size);	stm.hflush();	if (triggerLeaseRenewerInterrupt) {	
leasechecker interruptandjoin 

private void recoverLease(Path filepath, DistributedFileSystem dfs) throws Exception {	if (dfs == null) {	dfs = (DistributedFileSystem)getFSAsAnotherUser(conf);	}	while (!dfs.recoverLease(filepath)) {	
sleep ms 

for(int i = 0; i < 10; i++) {	AppendTestUtil.LOG.info("i=" + i);	try {	dfs2.create(filepath, false, BUF_SIZE, (short)1, BLOCK_SIZE);	fail("Creation of an existing file should never succeed.");	} catch(FileAlreadyExistsException e) {	return;	} catch(AlreadyBeingCreatedException e) {	return;	} catch(IOException ioe) {	
unexpected 

for(int i = 0; i < 10; i++) {	AppendTestUtil.LOG.info("i=" + i);	try {	dfs2.create(filepath, false, BUF_SIZE, (short)1, BLOCK_SIZE);	fail("Creation of an existing file should never succeed.");	} catch(FileAlreadyExistsException e) {	return;	} catch(AlreadyBeingCreatedException e) {	return;	} catch(IOException ioe) {	
sleep ms 

private void verifyFile(FileSystem dfs, Path filepath, byte[] actual, int size) throws IOException {	
lease for file is recovered validating its contents now 

public void testHardLeaseRecovery() throws Exception {	String filestr = "/hardLeaseRecovery";	AppendTestUtil.LOG.info("filestr=" + filestr);	Path filepath = new Path(filestr);	FSDataOutputStream stm = dfs.create(filepath, true, BUF_SIZE, REPLICATION_NUM, BLOCK_SIZE);	assertTrue(dfs.dfs.exists(filestr));	int size = AppendTestUtil.nextInt(FILE_SIZE);	AppendTestUtil.LOG.info("size=" + size);	stm.write(buffer, 0, size);	
hflush 

public void testHardLeaseRecovery() throws Exception {	String filestr = "/hardLeaseRecovery";	AppendTestUtil.LOG.info("filestr=" + filestr);	Path filepath = new Path(filestr);	FSDataOutputStream stm = dfs.create(filepath, true, BUF_SIZE, REPLICATION_NUM, BLOCK_SIZE);	assertTrue(dfs.dfs.exists(filestr));	int size = AppendTestUtil.nextInt(FILE_SIZE);	AppendTestUtil.LOG.info("size=" + size);	stm.write(buffer, 0, size);	stm.hflush();	
leasechecker interruptandjoin 

locatedBlocks = dfs.dfs.getLocatedBlocks(filestr, 0L, size);	} while (locatedBlocks.isUnderConstruction());	assertEquals(size, locatedBlocks.getFileLength());	try {	stm.write('b');	stm.close();	fail("Writer thread should have been killed");	} catch (IOException e) {	e.printStackTrace();	}	
file size is good now validating sizes from datanodes 

DFSTestUtil.updateConfWithFakeGroupMapping(conf, u2g_map);	cluster.setLeasePeriod(HdfsConstants.LEASE_SOFTLIMIT_PERIOD, HdfsConstants.LEASE_HARDLIMIT_PERIOD);	String filestr = "/foo" + AppendTestUtil.nextInt();	AppendTestUtil.LOG.info("filestr=" + filestr);	Path filepath = new Path(filestr);	FSDataOutputStream stm = dfs.create(filepath, true, BUF_SIZE, REPLICATION_NUM, BLOCK_SIZE);	assertTrue(dfs.dfs.exists(filestr));	int size = AppendTestUtil.nextInt(FILE_SIZE);	AppendTestUtil.LOG.info("size=" + size);	stm.write(buffer, 0, size);	
hflush 

cluster.setLeasePeriod(HdfsConstants.LEASE_SOFTLIMIT_PERIOD, HdfsConstants.LEASE_HARDLIMIT_PERIOD);	String filestr = "/foo" + AppendTestUtil.nextInt();	AppendTestUtil.LOG.info("filestr=" + filestr);	Path filepath = new Path(filestr);	FSDataOutputStream stm = dfs.create(filepath, true, BUF_SIZE, REPLICATION_NUM, BLOCK_SIZE);	assertTrue(dfs.dfs.exists(filestr));	int size = AppendTestUtil.nextInt(FILE_SIZE);	AppendTestUtil.LOG.info("size=" + size);	stm.write(buffer, 0, size);	stm.hflush();	
leasechecker interruptandjoin 

FileSystem dfs2 = DFSTestUtil.getFileSystemAs(ugi, conf);	boolean done = false;	for(int i = 0; i < 10 && !done; i++) {	AppendTestUtil.LOG.info("i=" + i);	try {	dfs2.create(filepath, false, BUF_SIZE, REPLICATION_NUM, BLOCK_SIZE);	fail("Creation of an existing file should never succeed.");	} catch (FileAlreadyExistsException ex) {	done = true;	} catch (AlreadyBeingCreatedException ex) {	
good got 

boolean done = false;	for(int i = 0; i < 10 && !done; i++) {	AppendTestUtil.LOG.info("i=" + i);	try {	dfs2.create(filepath, false, BUF_SIZE, REPLICATION_NUM, BLOCK_SIZE);	fail("Creation of an existing file should never succeed.");	} catch (FileAlreadyExistsException ex) {	done = true;	} catch (AlreadyBeingCreatedException ex) {	} catch (IOException ioe) {	
unexpected ioexception 

AppendTestUtil.LOG.info("i=" + i);	try {	dfs2.create(filepath, false, BUF_SIZE, REPLICATION_NUM, BLOCK_SIZE);	fail("Creation of an existing file should never succeed.");	} catch (FileAlreadyExistsException ex) {	done = true;	} catch (AlreadyBeingCreatedException ex) {	} catch (IOException ioe) {	}	if (!done) {	
sleep ms 

done = true;	} catch (AlreadyBeingCreatedException ex) {	} catch (IOException ioe) {	}	if (!done) {	try {Thread.sleep(5000);} catch (InterruptedException e) {}	}	}	assertTrue(done);	}	
lease for file is recovered validating its contents now 

} catch (IOException ioe) {	}	if (!done) {	try {Thread.sleep(5000);} catch (InterruptedException e) {}	}	}	assertTrue(done);	}	long fileSize = dfs.getFileStatus(filepath).getLen();	assertTrue("File should be " + size + " bytes, but is actually " + " found to be " + fileSize + " bytes", fileSize == size);	
file size is good now validating data and sizes from datanodes 

}	String fileStr = "/hardLeaseRecovery";	AppendTestUtil.LOG.info("filestr=" + fileStr);	Path filePath = new Path(fileStr);	FSDataOutputStream stm = dfs.create(filePath, true, BUF_SIZE, REPLICATION_NUM, BLOCK_SIZE);	assertTrue(dfs.dfs.exists(fileStr));	AppendTestUtil.LOG.info("size=" + size);	stm.write(buffer, 0, size);	String originalLeaseHolder = NameNodeAdapter.getLeaseHolderForPath( cluster.getNameNode(), fileStr);	assertFalse("original lease holder should not be the NN", originalLeaseHolder.startsWith( HdfsServerConstants.NAMENODE_LEASE_HOLDER));	
hflush 

stm.hflush();	final HdfsDataInputStream in = (HdfsDataInputStream)dfs.open(filePath);	Assert.assertEquals(size, in.getVisibleLength());	in.close();	if (doRename) {	fileStr += ".renamed";	Path renamedPath = new Path(fileStr);	assertTrue(dfs.rename(filePath, renamedPath));	filePath = renamedPath;	}	
leasechecker interruptandjoin 

do {	Thread.sleep(SHORT_LEASE_PERIOD);	locatedBlocks = dfs.dfs.getLocatedBlocks(fileStr, 0L, size);	} while (locatedBlocks.isUnderConstruction());	assertEquals(size, locatedBlocks.getFileLength());	try {	stm.write('b');	stm.hflush();	fail("Should not be able to flush after we've lost the lease");	} catch (IOException e) {	
expceted exception on write hflush 

try {	stm.write('b');	stm.hflush();	fail("Should not be able to flush after we've lost the lease");	} catch (IOException e) {	}	try {	stm.close();	fail("Should not be able to close after we've lost the lease");	} catch (IOException e) {	
expected exception on close 

stm.write('b');	stm.hflush();	fail("Should not be able to flush after we've lost the lease");	} catch (IOException e) {	}	try {	stm.close();	fail("Should not be able to close after we've lost the lease");	} catch (IOException e) {	}	
file size is good now validating sizes from datanodes 

========================= hadoop sample_7096 =========================

rc = conns[1].getResponseCode();	Assert.assertEquals(HttpURLConnection.HTTP_OK, rc);	try {	rc = conns[2].getResponseCode();	Assert.assertEquals("Expected a too-many-requests response code", ShuffleHandler.TOO_MANY_REQ_STATUS.getCode(), rc);	long backoff = Long.valueOf( conns[2].getHeaderField(ShuffleHandler.RETRY_AFTER_HEADER));	Assert.assertTrue("The backoff value cannot be negative.", backoff > 0);	conns[2].getInputStream();	Assert.fail("Expected an IOException");	} catch (IOException ioe) {	
expected connection should not be open 

========================= hadoop sample_5316 =========================

public void teardown() throws Exception {	keepRunning = false;	if (failoverThread != null) {	failoverThread.interrupt();	try {	failoverThread.join();	} catch (InterruptedException ex) {	
error joining with failover thread 

========================= hadoop sample_2579 =========================

public List<InputSplit> getSplits(JobContext job) {	long totalRows = getNumberOfRows(job);	int numSplits = job.getConfiguration().getInt(MRJobConfig.NUM_MAPS, 1);	
generating using 

========================= hadoop sample_5746 =========================

public void testBootstrap() {	TrafficControlBandwidthHandlerImpl handlerImpl = new TrafficControlBandwidthHandlerImpl(privilegedOperationExecutorMock, cGroupsHandlerMock, trafficControllerMock);	try {	handlerImpl.bootstrap(conf);	verify(cGroupsHandlerMock).initializeCGroupController( eq(CGroupsHandler.CGroupController.NET_CLS));	verifyNoMoreInteractions(cGroupsHandlerMock);	verify(trafficControllerMock).bootstrap(eq(device), eq(ROOT_BANDWIDTH_MBIT), eq(YARN_BANDWIDTH_MBIT));	verifyNoMoreInteractions(trafficControllerMock);	} catch (ResourceHandlerException e) {	
unexpected exception 

public void testLifeCycle() {	TrafficController trafficControllerSpy = spy(new TrafficController(conf, privilegedOperationExecutorMock));	TrafficControlBandwidthHandlerImpl handlerImpl = new TrafficControlBandwidthHandlerImpl(privilegedOperationExecutorMock, cGroupsHandlerMock, trafficControllerSpy);	try {	handlerImpl.bootstrap(conf);	testPreStart(trafficControllerSpy, handlerImpl);	testPostComplete(trafficControllerSpy, handlerImpl);	} catch (ResourceHandlerException e) {	
unexpected exception 

verify(cGroupsHandlerMock).deleteCGroup( eq(CGroupsHandler.CGroupController.NET_CLS), eq(TEST_CONTAINER_ID_STR));	try {	ArgumentCaptor<PrivilegedOperation> opCaptor = ArgumentCaptor.forClass (PrivilegedOperation.class);	verify(privilegedOperationExecutorMock) .executePrivilegedOperation(opCaptor.capture(), eq(false));	List<String> args = opCaptor.getValue().getArguments();	Assert.assertEquals(PrivilegedOperation.OperationType.TC_MODIFY_STATE, opCaptor.getValue().getOperationType());	Assert.assertEquals(1, args.size());	Assert.assertTrue(new File(args.get(0)).exists());	verify(trafficControllerSpy).releaseClassId(TEST_CLASSID);	} catch (PrivilegedOperationException e) {	
caught exception 

========================= hadoop sample_1640 =========================

SwiftRestClient client = createClient();	client.authenticate();	Path path = new Path("restTestPutAndDelete");	SwiftObjectPath sobject = SwiftObjectPath.fromPath(serviceURI, path);	byte[] stuff = new byte[1];	stuff[0] = 'a';	client.upload(sobject, new ByteArrayInputStream(stuff), stuff.length);	Duration head = new Duration();	Header[] responseHeaders = client.headRequest("expect success", sobject, SwiftRestClient.NEWEST);	head.finished();	
head request duration 

========================= hadoop sample_6185 =========================

public void handle(JobEvent event) {	if (LOG.isDebugEnabled()) {	
processing of type 

public void handle(JobEvent event) {	if (LOG.isDebugEnabled()) {	}	try {	writeLock.lock();	JobStateInternal oldState = getInternalState();	try {	getStateMachine().doTransition(event.getType(), event);	} catch (InvalidStateTransitionException e) {	
can t handle this event at current state 

try {	writeLock.lock();	JobStateInternal oldState = getInternalState();	try {	getStateMachine().doTransition(event.getType(), event);	} catch (InvalidStateTransitionException e) {	addDiagnostic("Invalid event " + event.getType() + " on Job " + this.jobId);	eventHandler.handle(new JobEvent(this.jobId, JobEventType.INTERNAL_ERROR));	}	if (oldState != getInternalState()) {	
job transitioned from to 

int requiredCores = Math.max(requiredMapCores, requiredReduceCores);	if (numReduceTasks == 0) {	requiredMB = requiredMapMB;	requiredCores = requiredMapCores;	}	boolean smallMemory = (requiredMB <= sysMemSizeForUberSlot) || (sysMemSizeForUberSlot == JobConf.DISABLED_MEMORY_LIMIT);	boolean smallCpu = requiredCores <= sysCPUSizeForUberSlot;	boolean notChainJob = !isChainJob(conf);	isUber = uberEnabled && smallNumMapTasks && smallNumReduceTasks && smallInput && smallMemory && smallCpu && notChainJob;	if (isUber) {	
uberizing job m r tasks input bytes will run sequentially on single node 

private void actOnUnusableNode(NodeId nodeId, NodeState nodeState) {	if (getInternalState() == JobStateInternal.RUNNING && !allReducersComplete()) {	List<TaskAttemptId> taskAttemptIdList = nodesToSucceededTaskAttempts.get(nodeId);	if (taskAttemptIdList != null) {	String mesg = "TaskAttempt killed because it ran on unusable node " + nodeId;	for (TaskAttemptId id : taskAttemptIdList) {	if (TaskType.MAP == id.getTaskId().getTaskType()) {	
attemptid 

job.mapAttemptCompletionEvents = new ArrayList<TaskCompletionEvent>(job.numMapTasks + 10);	job.taskCompletionIdxToMapCompletionIdx = new ArrayList<Integer>( job.numMapTasks + job.numReduceTasks + 10);	job.allowedMapFailuresPercent = job.conf.getInt(MRJobConfig.MAP_FAILURES_MAX_PERCENT, 0);	job.allowedReduceFailuresPercent = job.conf.getInt(MRJobConfig.REDUCE_FAILURES_MAXPERCENT, 0);	cleanupSharedCacheUploadPolicies(job.conf);	createMapTasks(job, inputLength, taskSplitMetaInfo);	createReduceTasks(job);	job.metrics.endPreparingJob(job);	return JobStateInternal.INITED;	} catch (Exception e) {	
job init failed 

Path path = MRApps.getStagingAreaDir(job.conf, user);	if(LOG.isDebugEnabled()) {	LOG.debug("startJobs: parent=" + path + " child=" + oldJobIDString);	}	job.remoteJobSubmitDir = FileSystem.get(job.conf).makeQualified( new Path(path, oldJobIDString));	job.remoteJobConfFile = new Path(job.remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);	JobTokenIdentifier identifier = new JobTokenIdentifier(new Text(oldJobIDString));	job.jobToken = new Token<JobTokenIdentifier>(identifier, job.jobTokenSecretManager);	job.jobToken.setService(identifier.getJobId());	job.jobTokenSecretManager.addTokenForJob(oldJobIDString, job.jobToken);	
adding job token for to jobtokensecretmanager 

if(LOG.isDebugEnabled()) {	LOG.debug("startJobs: parent=" + path + " child=" + oldJobIDString);	}	job.remoteJobSubmitDir = FileSystem.get(job.conf).makeQualified( new Path(path, oldJobIDString));	job.remoteJobConfFile = new Path(job.remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);	JobTokenIdentifier identifier = new JobTokenIdentifier(new Text(oldJobIDString));	job.jobToken = new Token<JobTokenIdentifier>(identifier, job.jobTokenSecretManager);	job.jobToken.setService(identifier.getJobId());	job.jobTokenSecretManager.addTokenForJob(oldJobIDString, job.jobToken);	if (TokenCache.getShuffleSecretKey(job.jobCredentials) == null) {	
shuffle secret key missing from job credentials using job token secret as shuffle secret 

public void transition(JobImpl job, JobEvent event) {	
timeout expired in fail wait waiting for tasks to get killed going to fail job anyway 

}	}	}	JobTaskAttemptFetchFailureEvent fetchfailureEvent = (JobTaskAttemptFetchFailureEvent) event;	for (org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId mapId : fetchfailureEvent.getMaps()) {	Integer fetchFailures = job.fetchFailuresMapping.get(mapId);	fetchFailures = (fetchFailures == null) ? 1 : (fetchFailures+1);	job.fetchFailuresMapping.put(mapId, fetchFailures);	float failureRate = shufflingReduceTasks == 0 ? 1.0f : (float) fetchFailures / shufflingReduceTasks;	if (fetchFailures >= job.getMaxFetchFailuresNotifications() && failureRate >= job.getMaxAllowedFetchFailuresFraction()) {	
too many fetch failures for output of task attempt raising fetch failure to map 

public JobStateInternal transition(JobImpl job, JobEvent event) {	job.completedTaskCount++;	
num completed tasks 

public void run() {	
sending event to 

private void checkReadyForCompletionWhenAllReducersDone(JobImpl job) {	if (job.finishJobWhenReducersDone) {	int totalReduces = job.getTotalReduces();	int completedReduces = job.getCompletedReduces();	if (totalReduces > 0 && totalReduces == completedReduces && !job.completingJob) {	for (TaskId mapTaskId : job.mapTasks) {	MapTaskImpl task = (MapTaskImpl) job.tasks.get(mapTaskId);	if (!task.isFinished()) {	
killing map task 

========================= hadoop sample_5253 =========================

private void uploadResourcesInternal(Job job, Path submitJobDir) throws IOException {	Configuration conf = job.getConfiguration();	short replication = (short) conf.getInt(Job.SUBMIT_REPLICATION, Job.DEFAULT_SUBMIT_REPLICATION);	if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {	
hadoop command line option parsing not performed implement the tool interface and execute your application with toolrunner to remedy this 

private void uploadResourcesInternal(Job job, Path submitJobDir) throws IOException {	Configuration conf = job.getConfiguration();	short replication = (short) conf.getInt(Job.SUBMIT_REPLICATION, Job.DEFAULT_SUBMIT_REPLICATION);	if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {	}	
default filesystem 

private URI useSharedCache(URI sourceFile, String resourceName, Map<URI, FileStatus> statCache, Configuration conf, boolean honorFragment) throws IOException {	if (scClient == null) {	return null;	}	Path filePath = new Path(sourceFile);	if (getFileStatus(statCache, conf, filePath).isDirectory()) {	
shared cache does not support directories see yarn will not upload to the shared cache 

if (honorFragment) {	if (sourceFile.getFragment() != null) {	rn = sourceFile.getFragment();	}	}	String checksum = scClient.getFileChecksum(filePath);	URL url = null;	try {	url = scClient.use(this.appId, checksum);	} catch (YarnException e) {	
error trying to contact the shared cache manager disabling the scmclient for the rest of this job submission 

URI uri = null;	try {	String name = new Path(url.getFile()).getName();	if (rn != null && !name.equals(rn)) {	uri = new URI(url.getScheme(), url.getUserInfo(), url.getHost(), url.getPort(), url.getFile(), null, rn);	} else {	uri = new URI(url.getScheme(), url.getUserInfo(), url.getHost(), url.getPort(), url.getFile(), null, null);	}	return uri;	} catch (URISyntaxException e) {	
error trying to convert url received from shared cache to a uri 

========================= hadoop sample_4911 =========================

private void initUsersToBypassExtProvider(Configuration conf) {	String[] bypassUsers = conf.getTrimmedStrings( DFSConfigKeys.DFS_NAMENODE_INODE_ATTRIBUTES_PROVIDER_BYPASS_USERS_KEY, DFSConfigKeys.DFS_NAMENODE_INODE_ATTRIBUTES_PROVIDER_BYPASS_USERS_DEFAULT);	for(int i = 0; i < bypassUsers.length; i++) {	String tmp = bypassUsers[i].trim();	if (!tmp.isEmpty()) {	if (usersToBypassExtAttrProvider == null) {	usersToBypassExtAttrProvider = new HashSet<String>();	}	
add user to the list that will bypass external attribute provider 

}	final EnumCounters<StorageType> tsConsumed = myCounts.getTypeSpaces();	for (StorageType t : StorageType.getTypesSupportingQuota()) {	final long typeSpace = tsConsumed.get(t);	final long typeQuota = q.getTypeSpaces().get(t);	if (Quota.isViolated(typeQuota, typeSpace)) {	LOG.warn("Storage type quota violation in image for " + dir.getFullPathName() + " type = " + t.toString() + " quota = " + typeQuota + " < consumed " + typeSpace);	}	}	if (LOG.isDebugEnabled()) {	
setting quota for 

public void updateSpaceForCompleteBlock(BlockInfo completeBlk, INodesInPath inodes) throws IOException {	assert namesystem.hasWriteLock();	INodesInPath iip = inodes != null ? inodes : INodesInPath.fromINode(namesystem.getBlockCollection(completeBlk));	INodeFile fileINode = iip.getLastINode().asFile();	final long diff = fileINode.getPreferredBlockSize() - completeBlk.getNumBytes();	if (diff > 0) {	try {	updateSpaceConsumed(iip, 0, -diff, fileINode.getFileReplication());	} catch (IOException e) {	
unexpected exception while updating disk space 

static Collection<String> normalizePaths(Collection<String> paths, String errorString) {	if (paths.isEmpty()) {	return paths;	}	final Collection<String> normalized = new ArrayList<>(paths.size());	for (String dir : paths) {	if (isReservedName(dir)) {	
ignoring reserved path 

static Collection<String> normalizePaths(Collection<String> paths, String errorString) {	if (paths.isEmpty()) {	return paths;	}	final Collection<String> normalized = new ArrayList<>(paths.size());	for (String dir : paths) {	if (isReservedName(dir)) {	} else {	final Path path = new Path(dir);	if (!path.isAbsolute()) {	
ignoring relative path 

if (paths.isEmpty()) {	return paths;	}	final Collection<String> normalized = new ArrayList<>(paths.size());	for (String dir : paths) {	if (isReservedName(dir)) {	} else {	final Path path = new Path(dir);	if (!path.isAbsolute()) {	} else if (path.toUri().getScheme() != null) {	
ignoring path with scheme 

return;	}	XAttr xattr = xaf.getXAttr(CRYPTO_XATTR_ENCRYPTION_ZONE);	if (xattr == null) {	return;	}	try {	final HdfsProtos.ZoneEncryptionInfoProto ezProto = HdfsProtos.ZoneEncryptionInfoProto.parseFrom( xattr.getValue());	ezManager.unprotectedAddEncryptionZone(inode.getId(), PBHelperClient.convert(ezProto.getSuite()), PBHelperClient.convert(ezProto.getCryptoProtocolVersion()), ezProto.getKeyName());	} catch (InvalidProtocolBufferException e) {	
error parsing protocol buffer of ez xattr dir 

private static byte[][] constructRemainingPath(byte[][] components, byte[][] extraComponents, int startAt) {	int remainder = extraComponents.length - startAt;	if (remainder > 0) {	int pos = components.length;	components = Arrays.copyOf(components, pos + remainder);	System.arraycopy(extraComponents, startAt, components, pos, remainder);	}	if (NameNode.LOG.isDebugEnabled()) {	
resolved path is 

========================= hadoop sample_8031 =========================

if (nodesToUpdateLabels != null && !nodesToUpdateLabels.isEmpty()) {	updateNodeLabelsInternal(nodesToUpdateLabels);	if (isUpdatingAllNodes) {	lastAllNodesLabelUpdateMills = System.currentTimeMillis();	}	synchronized (lock) {	newlyRegisteredNodes.removeAll(nodesToUpdateLabels);	}	}	} catch (IOException e) {	
failed to update node labels 

private RMNodeLabelsMappingProvider createRMNodeLabelsMappingProvider( Configuration conf) throws IOException {	RMNodeLabelsMappingProvider nodeLabelsMappingProvider = null;	try {	Class<? extends RMNodeLabelsMappingProvider> labelsProviderClass = conf.getClass(YarnConfiguration.RM_NODE_LABELS_PROVIDER_CONFIG, null, RMNodeLabelsMappingProvider.class);	if (labelsProviderClass != null) {	nodeLabelsMappingProvider = labelsProviderClass.newInstance();	}	} catch (InstantiationException | IllegalAccessException | RuntimeException e) {	
failed to create rmnodelabelsmappingprovider based on configuration 

nodeLabelsMappingProvider = labelsProviderClass.newInstance();	}	} catch (InstantiationException | IllegalAccessException | RuntimeException e) {	throw new IOException("Failed to create RMNodeLabelsMappingProvider : " + e.getMessage(), e);	}	if (nodeLabelsMappingProvider == null) {	String msg = "RMNodeLabelsMappingProvider should be configured when " + "delegated-centralized node label configuration is enabled";	LOG.error(msg);	throw new IOException(msg);	} else if (LOG.isDebugEnabled()) {	
rm node labels mapping provider class is 

========================= hadoop sample_1139 =========================

public void testExpireBackAndForth() throws Exception {	cluster.start();	long st = Time.now();	long runFor = STRESS_RUNTIME_SECS * 1000;	int i = 0;	while (Time.now() - st < runFor) {	int from = i % 2;	int to = (i + 1) % 2;	
failing over via expiration from to 

cluster.start();	long st = Time.now();	long runFor = STRESS_RUNTIME_SECS * 1000;	Random r = new Random();	while (Time.now() - st < runFor) {	cluster.getTestContext().checkException();	int targetIdx = r.nextInt(2);	ActiveStandbyElector target = cluster.getElector(targetIdx);	long sessId = target.getZKSessionIdForTests();	if (sessId != -1) {	
expiring session x for svc d 

public Object answer(InvocationOnMock invocation) throws Throwable {	if (r.nextBoolean()) {	
throwing an exception for svc 

========================= hadoop sample_3104 =========================

long newBlockSize = defaultBlockSize * 2;	fs.getConf().setLong(Constants.FS_S3A_BLOCK_SIZE, newBlockSize);	Path dir = path("testBlockSize");	Path file = new Path(dir, "file");	createFile(fs, file, true, dataset(1024, 'a', 'z' - 'a'));	FileStatus fileStatus = fs.getFileStatus(file);	assertEquals("Double default block size in stat(): " + fileStatus, newBlockSize, fileStatus.getBlockSize());	boolean found = false;	FileStatus[] listing = fs.listStatus(dir);	for (FileStatus stat : listing) {	
entry 

========================= hadoop sample_5879 =========================

protected void init() throws ServiceException {	
using filesystemaccess jars version 

throw new ServiceException(FileSystemAccessException.ERROR.H01, KERBEROS_PRINCIPAL);	}	Configuration conf = new Configuration();	conf.set(HADOOP_SECURITY_AUTHENTICATION, "kerberos");	UserGroupInformation.setConfiguration(conf);	try {	UserGroupInformation.loginUserFromKeytab(principal, keytab);	} catch (IOException ex) {	throw new ServiceException(FileSystemAccessException.ERROR.H02, ex.getMessage(), ex);	}	
using filesystemaccess kerberos authentication principal keytab 

UserGroupInformation.setConfiguration(conf);	try {	UserGroupInformation.loginUserFromKeytab(principal, keytab);	} catch (IOException ex) {	throw new ServiceException(FileSystemAccessException.ERROR.H02, ex.getMessage(), ex);	}	} else if (security.equals("simple")) {	Configuration conf = new Configuration();	conf.set(HADOOP_SECURITY_AUTHENTICATION, "simple");	UserGroupInformation.setConfiguration(conf);	
using filesystemaccess simple pseudo authentication principal user name 

hadoopConfDir = new File(getServer().getConfigDir()).getAbsoluteFile();	}	if (!hadoopConfDir.exists()) {	throw new ServiceException(FileSystemAccessException.ERROR.H10, hadoopConfDir);	}	try {	serviceHadoopConf = loadHadoopConf(hadoopConfDir);	} catch (IOException ex) {	throw new ServiceException(FileSystemAccessException.ERROR.H11, ex.toString(), ex);	}	
filesystemaccess filesystem configuration 

public void run() {	int count = 0;	for (CachedFileSystem cacheFs : fsCache.values()) {	try {	count += cacheFs.purgeIfIdle() ? 1 : 0;	} catch (Throwable ex) {	
error while purging filesystem 

public void run() {	int count = 0;	for (CachedFileSystem cacheFs : fsCache.values()) {	try {	count += cacheFs.purgeIfIdle() ? 1 : 0;	} catch (Throwable ex) {	}	}	
purged filesystem instances 

========================= hadoop sample_6756 =========================

protected ResourceLocalizationService createResourceLocalizationService( ContainerExecutor exec, DeletionService deletionContext, Context context, NodeManagerMetrics metrics) {	return new ResourceLocalizationService(super.dispatcher, exec, deletionContext, super.dirsHandler, context, metrics) {	public void handle(LocalizationEvent event) {	switch (event.getType()) {	case INIT_APPLICATION_RESOURCES: Application app = ((ApplicationLocalizationEvent) event).getApplication();	dispatcher.getEventHandler().handle(new ApplicationInitedEvent( app.getAppId()));	break;	case LOCALIZE_CONTAINER_RESOURCES: ContainerLocalizationRequestEvent rsrcReqs = (ContainerLocalizationRequestEvent) event;	for (Collection<LocalResourceRequest> rc : rsrcReqs .getRequestedResources().values()) {	for (LocalResourceRequest req : rc) {	
debug 

========================= hadoop sample_1569 =========================

}	} finally {	namesystem.writeUnlock();	}	try {	Thread.sleep(RECHECK_INTERVAL);	} catch (InterruptedException ignored) {	}	}	if (!namesystem.isRunning()) {	
namenode is being shutdown exit safemodemonitor thread 

========================= hadoop sample_8318 =========================

public JniBasedUnixGroupsNetgroupMappingWithFallback() {	if (NativeCodeLoader.isNativeCodeLoaded()) {	this.impl = new JniBasedUnixGroupsNetgroupMapping();	} else {	
falling back to shell based 

========================= hadoop sample_3807 =========================

public void run() throws YarnException, IOException, InterruptedException {	super.run();	if (appAttemptID.getAttemptId() == 2) {	if (numAllocatedContainers.get() != 1 || numRequestedContainers.get() != numTotalContainers) {	
numallocatedcontainers is and numrequestedcontainers is application master failed exiting 

try {	Thread.sleep(3000);	} catch (InterruptedException e) {}	System.exit(100);	}	result = appMaster.finish();	} catch (Throwable t) {	System.exit(1);	}	if (result) {	
application master completed successfully exiting 

} catch (InterruptedException e) {}	System.exit(100);	}	result = appMaster.finish();	} catch (Throwable t) {	System.exit(1);	}	if (result) {	System.exit(0);	} else {	
application master failed exiting 

========================= hadoop sample_4 =========================

public void testDefaultRealm() throws Throwable {	String realm = RegistrySecurity.getDefaultRealmInJVM();	
realm 

public void testUGIProperties() throws Throwable {	UserGroupInformation user = UserGroupInformation.getCurrentUser();	ACL acl = registrySecurity.createACLForUser(user, ZooDefs.Perms.ALL);	assertFalse(RegistrySecurity.ALL_READWRITE_ACCESS.equals(acl));	
user has acl 

========================= hadoop sample_2650 =========================

public synchronized int read(ByteBuffer buf) throws IOException {	boolean canSkipChecksum = createNoChecksumContext();	try {	String traceFormatStr = "read(buf.remaining={}, block={}, filename={}, " + "canSkipChecksum={})";	
starting 

try {	String traceFormatStr = "read(buf.remaining={}, block={}, filename={}, " + "canSkipChecksum={})";	int nRead;	try {	if (canSkipChecksum && zeroReadaheadRequested) {	nRead = readWithoutBounceBuffer(buf);	} else {	nRead = readWithBounceBuffer(buf, canSkipChecksum);	}	} catch (IOException e) {	
i o error 

int nRead;	try {	if (canSkipChecksum && zeroReadaheadRequested) {	nRead = readWithoutBounceBuffer(buf);	} else {	nRead = readWithBounceBuffer(buf, canSkipChecksum);	}	} catch (IOException e) {	throw e;	}	
returning 

if (canSkipChecksum) {	dataBuf.position(slop);	fillBuffer(dataBuf, true);	} else {	dataPos -= slop;	dataBuf.position(0);	fillBuffer(dataBuf, false);	}	dataBuf.limit(dataBuf.position());	dataBuf.position(Math.min(dataBuf.position(), slop));	
loaded bytes into bounce buffer from offset of 

public synchronized int read(byte[] arr, int off, int len) throws IOException {	boolean canSkipChecksum = createNoChecksumContext();	int nRead;	try {	final String traceFormatStr = "read(arr.length={}, off={}, len={}, " + "filename={}, block={}, canSkipChecksum={})";	
starting 

int nRead;	try {	final String traceFormatStr = "read(arr.length={}, off={}, len={}, " + "filename={}, block={}, canSkipChecksum={})";	try {	if (canSkipChecksum && zeroReadaheadRequested) {	nRead = readWithoutBounceBuffer(arr, off, len);	} else {	nRead = readWithBounceBuffer(arr, off, len, canSkipChecksum);	}	} catch (IOException e) {	
i o error 

final String traceFormatStr = "read(arr.length={}, off={}, len={}, " + "filename={}, block={}, canSkipChecksum={})";	try {	if (canSkipChecksum && zeroReadaheadRequested) {	nRead = readWithoutBounceBuffer(arr, off, len);	} else {	nRead = readWithBounceBuffer(arr, off, len, canSkipChecksum);	}	} catch (IOException e) {	throw e;	}	
returning 

public ClientMmap getClientMmap(EnumSet<ReadOption> opts) {	boolean anchor = verifyChecksum && !opts.contains(ReadOption.SKIP_CHECKSUMS);	if (anchor) {	if (!createNoChecksumContext()) {	
can t get an mmap for of since skip checksums was not given we aren t skipping checksums and the block is not mlocked 

========================= hadoop sample_7002 =========================

try {	f.syncUninterruptibly();	} catch (Throwable e) {	if (e instanceof BindException) {	throw NetUtils.wrapException(null, 0, infoAddr.getHostName(), infoAddr.getPort(), (SocketException) e);	} else {	throw e;	}	}	httpAddress = (InetSocketAddress) f.channel().localAddress();	
listening http traffic on 

try {	f.syncUninterruptibly();	} catch (Throwable e) {	if (e instanceof BindException) {	throw NetUtils.wrapException(null, 0, secInfoSocAddr.getHostName(), secInfoSocAddr.getPort(), (SocketException) e);	} else {	throw e;	}	}	httpsAddress = (InetSocketAddress) f.channel().localAddress();	
listening https traffic on 

========================= hadoop sample_7901 =========================

public static void testFactory(long targetBytes, long targetRecs) throws Exception {	final Configuration conf = new Configuration();	final GridmixKey key = new GridmixKey();	final GridmixRecord val = new GridmixRecord();	
target bytes records 

========================= hadoop sample_6097 =========================

public void purgeImage(FSImageFile image) {	
purging old image 

private static void deleteOrWarn(File file) {	if (!file.delete()) {	
could not delete 

========================= hadoop sample_8089 =========================

public void tearDown() throws Exception {	
shutting down minidfscluster 

========================= hadoop sample_7686 =========================

private int doRun() throws IOException {	NamenodeProtocol proxy = createNNProtocolProxy();	NamespaceInfo nsInfo;	boolean isUpgradeFinalized;	try {	nsInfo = proxy.versionRequest();	isUpgradeFinalized = proxy.isUpgradeFinalized();	} catch (IOException ioe) {	
unable to fetch namespace information from active nn at 

private int doRun() throws IOException {	NamenodeProtocol proxy = createNNProtocolProxy();	NamespaceInfo nsInfo;	boolean isUpgradeFinalized;	try {	nsInfo = proxy.versionRequest();	isUpgradeFinalized = proxy.isUpgradeFinalized();	} catch (IOException ioe) {	if (LOG.isDebugEnabled()) {	
full exception trace 

boolean isUpgradeFinalized;	try {	nsInfo = proxy.versionRequest();	isUpgradeFinalized = proxy.isUpgradeFinalized();	} catch (IOException ioe) {	if (LOG.isDebugEnabled()) {	}	return ERR_CODE_FAILED_CONNECT;	}	if (!checkLayoutVersion(nsInfo)) {	
layout version on remote node does not match this node s layout version 

if (LOG.isDebugEnabled()) {	}	return ERR_CODE_FAILED_CONNECT;	}	if (!checkLayoutVersion(nsInfo)) {	return ERR_CODE_INVALID_VERSION;	}	System.out.println( "=====================================================\n" + "About to bootstrap Standby ID " + nnId + " from:\n" + "           Nameservice ID: " + nsId + "\n" + "        Other Namenode ID: " + otherNNId + "\n" + "  Other NN's HTTP address: " + otherHttpAddr + "\n" + "  Other NN's IPC  address: " + otherIpcAddr + "\n" + "             Namespace ID: " + nsInfo.getNamespaceID() + "\n" + "            Block pool ID: " + nsInfo.getBlockPoolID() + "\n" + "               Cluster ID: " + nsInfo.getClusterID() + "\n" + "           Layout version: " + nsInfo.getLayoutVersion() + "\n" + "       isUpgradeFinalized: " + isUpgradeFinalized + "\n" + "=====================================================");	NNStorage storage = new NNStorage(conf, dirsToFormat, editUrisToFormat);	if (!isUpgradeFinalized) {	
the active namenode is in upgrade prepare the upgrade for the standby namenode as well 

private boolean doPreUpgrade(NNStorage storage, NamespaceInfo nsInfo) throws IOException {	boolean isFormatted = false;	Map<StorageDirectory, StorageState> dataDirStates = new HashMap<>();	try {	isFormatted = FSImage.recoverStorageDirs(StartupOption.UPGRADE, storage, dataDirStates);	if (dataDirStates.values().contains(StorageState.NOT_FORMATTED)) {	isFormatted = false;	System.err.println("The original storage directory is not formatted.");	}	} catch (InconsistentFSStateException e) {	
the storage directory is in an inconsistent state 

if (!isFormatted && !format(storage, nsInfo)) {	return false;	}	FSImage.checkUpgrade(storage);	for (Iterator<StorageDirectory> it = storage.dirIterator(false);	it.hasNext();) {	StorageDirectory sd = it.next();	try {	NNUpgradeUtil.renameCurToTmp(sd);	} catch (IOException e) {	
failed to move aside pre upgrade storage in image directory 

========================= hadoop sample_8066 =========================

private void shouldThrow(PrivilegedExceptionAction<Object> action, Class<? extends Throwable> except) {	try {	action.run();	Assert.fail("action did not throw " + except);	} catch (Throwable th) {	
caught an exception 

return null;	}	}, AccessControlException.class);	long time = dtSecretManager.renewToken(token, "JobTracker");	Assert.assertTrue(dtSecretManager.isUpdateStoredTokenCalled);	assertTrue("renew time is in future", time > Time.now());	TestDelegationTokenIdentifier identifier = new TestDelegationTokenIdentifier();	byte[] tokenId = token.getIdentifier();	identifier.readFields(new DataInputStream( new ByteArrayInputStream(tokenId)));	Assert.assertTrue(null != dtSecretManager.retrievePassword(identifier));	
sleep to expire the token 

byte[] tokenId = token.getIdentifier();	identifier.readFields(new DataInputStream( new ByteArrayInputStream(tokenId)));	Assert.assertTrue(null != dtSecretManager.retrievePassword(identifier));	Thread.sleep(2000);	try {	dtSecretManager.retrievePassword(identifier);	Assert.fail("Token should have expired");	} catch (InvalidToken e) {	}	dtSecretManager.renewToken(token, "JobTracker");	
sleep beyond the max lifetime 

========================= hadoop sample_2938 =========================

public void shutdown() {	executor.shutdownNow();	for (KMSAuditLogger logger : auditLoggers) {	try {	logger.cleanup();	} catch (Exception ex) {	
failed to cleanup logger 

========================= hadoop sample_4415 =========================

case DatanodeProtocol.DNA_TRANSFER: dn.transferBlocks(bcmd.getBlockPoolId(), bcmd.getBlocks(), bcmd.getTargets(), bcmd.getTargetStorageTypes());	break;	case DatanodeProtocol.DNA_INVALIDATE: Block toDelete[] = bcmd.getBlocks();	try {	dn.getFSDataset().invalidate(bcmd.getBlockPoolId(), toDelete);	} catch(IOException e) {	throw e;	}	dn.metrics.incrBlocksRemoved(toDelete.length);	break;	
datanodecommand action dna cache for of 

case DatanodeProtocol.DNA_INVALIDATE: Block toDelete[] = bcmd.getBlocks();	try {	dn.getFSDataset().invalidate(bcmd.getBlockPoolId(), toDelete);	} catch(IOException e) {	throw e;	}	dn.metrics.incrBlocksRemoved(toDelete.length);	break;	dn.getFSDataset().cache(blockIdCmd.getBlockPoolId(), blockIdCmd.getBlockIds());	break;	
datanodecommand action dna uncache for of 

throw e;	}	dn.metrics.incrBlocksRemoved(toDelete.length);	break;	dn.getFSDataset().cache(blockIdCmd.getBlockPoolId(), blockIdCmd.getBlockIds());	break;	dn.getFSDataset().uncache(blockIdCmd.getBlockPoolId(), blockIdCmd.getBlockIds());	break;	case DatanodeProtocol.DNA_SHUTDOWN: throw new UnsupportedOperationException("Received unimplemented DNA_SHUTDOWN");	case DatanodeProtocol.DNA_FINALIZE: String bp = ((FinalizeCommand) cmd).getBlockPoolId();	
got finalize command for block pool 

dn.getFSDataset().uncache(blockIdCmd.getBlockPoolId(), blockIdCmd.getBlockIds());	break;	case DatanodeProtocol.DNA_SHUTDOWN: throw new UnsupportedOperationException("Received unimplemented DNA_SHUTDOWN");	case DatanodeProtocol.DNA_FINALIZE: String bp = ((FinalizeCommand) cmd).getBlockPoolId();	assert getBlockPoolId().equals(bp) : "BP " + getBlockPoolId() + " received DNA_FINALIZE " + "for other block pool " + bp;	dn.finalizeUpgradeForPool(bp);	break;	case DatanodeProtocol.DNA_RECOVERBLOCK: String who = "NameNode at " + actor.getNNSocketAddress();	dn.getBlockRecoveryWorker().recoverBlocks(who, ((BlockRecoveryCommand)cmd).getRecoveringBlocks());	break;	
datanodecommand action dna accesskeyupdate 

assert getBlockPoolId().equals(bp) : "BP " + getBlockPoolId() + " received DNA_FINALIZE " + "for other block pool " + bp;	dn.finalizeUpgradeForPool(bp);	break;	case DatanodeProtocol.DNA_RECOVERBLOCK: String who = "NameNode at " + actor.getNNSocketAddress();	dn.getBlockRecoveryWorker().recoverBlocks(who, ((BlockRecoveryCommand)cmd).getRecoveringBlocks());	break;	if (dn.isBlockTokenEnabled) {	dn.blockPoolTokenSecretManager.addKeys( getBlockPoolId(), ((KeyUpdateCommand) cmd).getExportedKeys());	}	break;	
datanodecommand action dna balancerbandwidthupdate 

case DatanodeProtocol.DNA_RECOVERBLOCK: String who = "NameNode at " + actor.getNNSocketAddress();	dn.getBlockRecoveryWorker().recoverBlocks(who, ((BlockRecoveryCommand)cmd).getRecoveringBlocks());	break;	if (dn.isBlockTokenEnabled) {	dn.blockPoolTokenSecretManager.addKeys( getBlockPoolId(), ((KeyUpdateCommand) cmd).getExportedKeys());	}	break;	long bandwidth = ((BalancerBandwidthCommand) cmd).getBalancerBandwidthValue();	if (bandwidth > 0) {	DataXceiverServer dxcs = (DataXceiverServer) dn.dataXceiverServer.getRunnable();	
updating balance throttler bandwidth from bytes s to bytes s 

if (dn.isBlockTokenEnabled) {	dn.blockPoolTokenSecretManager.addKeys( getBlockPoolId(), ((KeyUpdateCommand) cmd).getExportedKeys());	}	break;	long bandwidth = ((BalancerBandwidthCommand) cmd).getBalancerBandwidthValue();	if (bandwidth > 0) {	DataXceiverServer dxcs = (DataXceiverServer) dn.dataXceiverServer.getRunnable();	dxcs.balanceThrottler.setBandwidth(bandwidth);	}	break;	
unknown datanodecommand action 

private boolean processCommandFromStandby(DatanodeCommand cmd, BPServiceActor actor) throws IOException {	switch(cmd.getAction()) {	
datanodecommand action from standby dna accesskeyupdate 

private boolean processCommandFromStandby(DatanodeCommand cmd, BPServiceActor actor) throws IOException {	switch(cmd.getAction()) {	if (dn.isBlockTokenEnabled) {	dn.blockPoolTokenSecretManager.addKeys( getBlockPoolId(), ((KeyUpdateCommand) cmd).getExportedKeys());	}	break;	
got a command from standby nn ignoring command 

private boolean processCommandFromStandby(DatanodeCommand cmd, BPServiceActor actor) throws IOException {	switch(cmd.getAction()) {	if (dn.isBlockTokenEnabled) {	dn.blockPoolTokenSecretManager.addKeys( getBlockPoolId(), ((KeyUpdateCommand) cmd).getExportedKeys());	}	break;	break;	
unknown datanodecommand action 

========================= hadoop sample_7956 =========================

public TokenInfo getTokenInfo(Class<?> protocol, Configuration conf) {	if (!protocol .equals(LocalizationProtocolPB.class)) {	return null;	}	return new TokenInfo() {	public Class<? extends Annotation> annotationType() {	return null;	}	public Class<? extends TokenSelector<? extends TokenIdentifier>> value() {	
using localizertokensecurityinfo 

========================= hadoop sample_1882 =========================

private long updateNonSequentialWriteInMemory(long count) {	long newValue = nonSequentialWriteInMemory.addAndGet(count);	if (LOG.isDebugEnabled()) {	
update nonsequentialwriteinmemory by new value 

private void waitForDump() {	if (!enabledDump) {	if (LOG.isDebugEnabled()) {	
do nothing dump is disabled 

if (LOG.isDebugEnabled()) {	}	return;	}	if (nonSequentialWriteInMemory.get() < DUMP_WRITE_WATER_MARK) {	return;	}	synchronized (this) {	if (nonSequentialWriteInMemory.get() >= DUMP_WRITE_WATER_MARK) {	if (LOG.isDebugEnabled()) {	
asking dumper to dump 

private void dump() {	if (dumpOut == null) {	
create dump file 

private void dump() {	if (dumpOut == null) {	File dumpFile = new File(dumpFilePath);	try {	synchronized (this) {	Preconditions.checkState(dumpFile.createNewFile(), "The dump file should not exist: %s", dumpFilePath);	dumpOut = new FileOutputStream(dumpFile);	}	} catch (IOException e) {	
got failure when creating dump stream 

synchronized (this) {	Preconditions.checkState(dumpFile.createNewFile(), "The dump file should not exist: %s", dumpFilePath);	dumpOut = new FileOutputStream(dumpFile);	}	} catch (IOException e) {	enabledDump = false;	if (dumpOut != null) {	try {	dumpOut.close();	} catch (IOException e1) {	
can t close dump stream 

} catch (IOException e1) {	}	}	return;	}	}	if (raf == null) {	try {	raf = new RandomAccessFile(dumpFilePath, "r");	} catch (FileNotFoundException e) {	
can t get random access to file 

WriteCtx writeCtx = pendingWrites.get(key);	if (writeCtx == null) {	continue;	}	try {	long dumpedDataSize = writeCtx.dumpData(dumpOut, raf);	if (dumpedDataSize > 0) {	updateNonSequentialWriteInMemory(-dumpedDataSize);	}	} catch (IOException e) {	
dump data failed with error openfilectx state 

try {	if (nonSequentialWriteInMemory.get() >= DUMP_WRITE_WATER_MARK) {	dump();	}	synchronized (OpenFileCtx.this) {	if (nonSequentialWriteInMemory.get() < DUMP_WRITE_WATER_MARK) {	OpenFileCtx.this.notifyAll();	try {	OpenFileCtx.this.wait();	if (LOG.isDebugEnabled()) {	
dumper woke up 

try {	OpenFileCtx.this.wait();	if (LOG.isDebugEnabled()) {	}	} catch (InterruptedException e) {	LOG.info("Dumper is interrupted, dumpFilePath= " + OpenFileCtx.this.dumpFilePath);	}	}	}	if (LOG.isDebugEnabled()) {	
dumper checking openfilectx activestate enableddump 

LOG.info("Dumper is interrupted, dumpFilePath= " + OpenFileCtx.this.dumpFilePath);	}	}	}	if (LOG.isDebugEnabled()) {	}	} catch (Throwable t) {	synchronized (OpenFileCtx.this) {	OpenFileCtx.this.notifyAll();	}	
dumper get throwable dumpfilepath 

private WriteCtx checkRepeatedWriteRequest(WRITE3Request request, Channel channel, int xid) {	OffsetRange range = new OffsetRange(request.getOffset(), request.getOffset() + request.getCount());	WriteCtx writeCtx = pendingWrites.get(range);	if (writeCtx== null) {	return null;	} else {	if (xid != writeCtx.getXid()) {	
got a repeated request same range with a different xid xid in old request 

public void receivedNewWrite(DFSClient dfsClient, WRITE3Request request, Channel channel, int xid, AsyncDataService asyncDataService, IdMappingServiceProvider iug) {	if (!activeState) {	
openfilectx is inactive fileid 

public static void alterWriteRequest(WRITE3Request request, long cachedOffset) {	long offset = request.getOffset();	int count = request.getCount();	long smallerCount = offset + count - cachedOffset;	if (LOG.isDebugEnabled()) {	
got overwrite with appended data d d current offset d drop the overlapped section d d and append new data d d 

private static void trimWriteRequest(WriteCtx writeCtx, long currentOffset) {	long offset = writeCtx.getOffset();	if (LOG.isDebugEnabled()) {	int count = writeCtx.getCount();	
trim request d d current offset d drop the overlapped section d d and write new data d d 

private synchronized WriteCtx addWritesToCache(WRITE3Request request, Channel channel, int xid) {	long offset = request.getOffset();	int count = request.getCount();	long cachedOffset = nextOffset.get();	int originalCount = WriteCtx.INVALID_ORIGINAL_COUNT;	if (LOG.isDebugEnabled()) {	LOG.debug("requested offset=" + offset + " and current offset=" + cachedOffset);	}	if (offset + count <= cachedOffset) {	
got overwrite d d smaller than current offset d drop the request 

int count = request.getCount();	long cachedOffset = nextOffset.get();	int originalCount = WriteCtx.INVALID_ORIGINAL_COUNT;	if (LOG.isDebugEnabled()) {	LOG.debug("requested offset=" + offset + " and current offset=" + cachedOffset);	}	if (offset + count <= cachedOffset) {	return null;	}	if ((offset < cachedOffset) && (offset + count > cachedOffset)) {	
got overwrite with appended data d d current offset d drop the overlapped section d d and append new data d d 

int count = request.getCount();	long cachedOffset = nextOffset.get();	int originalCount = WriteCtx.INVALID_ORIGINAL_COUNT;	if (LOG.isDebugEnabled()) {	LOG.debug("requested offset=" + offset + " and current offset=" + cachedOffset);	}	if (offset + count <= cachedOffset) {	return null;	}	if ((offset < cachedOffset) && (offset + count > cachedOffset)) {	
modify this write to write only the appended data 

if (offset + count <= cachedOffset) {	return null;	}	if ((offset < cachedOffset) && (offset + count > cachedOffset)) {	alterWriteRequest(request, cachedOffset);	originalCount = count;	offset = request.getOffset();	count = request.getCount();	}	if (offset < cachedOffset) {	
offset count nextoffset 

private void processOverWrite(DFSClient dfsClient, WRITE3Request request, Channel channel, int xid, IdMappingServiceProvider iug) {	WccData wccData = new WccData(latestAttr.getWccAttr(), null);	long offset = request.getOffset();	int count = request.getCount();	WriteStableHow stableHow = request.getStableHow();	WRITE3Response response;	long cachedOffset = nextOffset.get();	if (offset + count > cachedOffset) {	
treat this jumbo write as a real random write no support 

WccData wccData = new WccData(latestAttr.getWccAttr(), null);	long offset = request.getOffset();	int count = request.getCount();	WriteStableHow stableHow = request.getStableHow();	WRITE3Response response;	long cachedOffset = nextOffset.get();	if (offset + count > cachedOffset) {	response = new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0, WriteStableHow.UNSTABLE, Nfs3Constant.WRITE_COMMIT_VERF);	} else {	if (LOG.isDebugEnabled()) {	
process perfectoverwrite 

private synchronized boolean checkAndStartWrite( AsyncDataService asyncDataService, WriteCtx writeCtx) {	if (writeCtx.getOffset() == nextOffset.get()) {	if (!asyncStatus) {	if (LOG.isDebugEnabled()) {	
trigger the write back task current nextoffset 

private synchronized boolean checkAndStartWrite( AsyncDataService asyncDataService, WriteCtx writeCtx) {	if (writeCtx.getOffset() == nextOffset.get()) {	if (!asyncStatus) {	if (LOG.isDebugEnabled()) {	}	asyncStatus = true;	asyncWriteBackStartOffset = writeCtx.getOffset();	asyncDataService.execute(new AsyncDataService.WriteBackTask(this));	} else {	if (LOG.isDebugEnabled()) {	
the write back thread is working 

WccAttr preOpAttr = latestAttr.getWccAttr();	int count = request.getCount();	WriteCtx writeCtx = addWritesToCache(request, channel, xid);	if (writeCtx == null) {	processOverWrite(dfsClient, request, channel, xid, iug);	} else {	boolean startWriting = checkAndStartWrite(asyncDataService, writeCtx);	if (!startWriting) {	waitForDump();	if (stableHow != WriteStableHow.UNSTABLE) {	
have to change stable write to unstable write 

if (writeCtx == null) {	processOverWrite(dfsClient, request, channel, xid, iug);	} else {	boolean startWriting = checkAndStartWrite(asyncDataService, writeCtx);	if (!startWriting) {	waitForDump();	if (stableHow != WriteStableHow.UNSTABLE) {	stableHow = WriteStableHow.UNSTABLE;	}	if (LOG.isDebugEnabled()) {	
unstable write request send response for offset 

private WRITE3Response processPerfectOverWrite(DFSClient dfsClient, long offset, int count, WriteStableHow stableHow, byte[] data, String path, WccData wccData, IdMappingServiceProvider iug) {	WRITE3Response response;	byte[] readbuffer = new byte[count];	int readCount = 0;	FSDataInputStream fis = null;	try {	fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));	} catch (ClosedChannelException closedException) {	
the fsdataoutputstream has been closed continue processing the perfect overwrite 

fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));	} catch (ClosedChannelException closedException) {	} catch (IOException e) {	LOG.info("hsync failed when processing possible perfect overwrite, path=" + path + " error: " + e);	return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow, Nfs3Constant.WRITE_COMMIT_VERF);	}	try {	fis = dfsClient.createWrappedInputStream(dfsClient.open(path));	readCount = fis.read(offset, readbuffer, 0, count);	if (readCount < count) {	
can t read back bytes partial read size 

return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow, Nfs3Constant.WRITE_COMMIT_VERF);	}	} catch (IOException e) {	LOG.info("Read failed when processing possible perfect overwrite, path=" + path, e);	return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow, Nfs3Constant.WRITE_COMMIT_VERF);	} finally {	IOUtils.cleanup(LOG, fis);	}	Comparator comparator = new Comparator();	if (comparator.compare(readbuffer, 0, readCount, data, 0, count) != 0) {	
perfect overwrite has different content 

} catch (IOException e) {	LOG.info("Read failed when processing possible perfect overwrite, path=" + path, e);	return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow, Nfs3Constant.WRITE_COMMIT_VERF);	} finally {	IOUtils.cleanup(LOG, fis);	}	Comparator comparator = new Comparator();	if (comparator.compare(readbuffer, 0, readCount, data, 0, count) != 0) {	response = new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0, stableHow, Nfs3Constant.WRITE_COMMIT_VERF);	} else {	
perfect overwrite has same content updating the mtime then return success 

public COMMIT_STATUS checkCommit(DFSClient dfsClient, long commitOffset, Channel channel, int xid, Nfs3FileAttributes preOpAttr, boolean fromRead) {	if (!fromRead) {	Preconditions.checkState(channel != null && preOpAttr != null);	updateLastAccessTime();	}	Preconditions.checkState(commitOffset >= 0);	COMMIT_STATUS ret = checkCommitInternal(commitOffset, channel, xid, preOpAttr, fromRead);	if (LOG.isDebugEnabled()) {	
got commit status 

try {	fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));	ret = COMMIT_STATUS.COMMIT_FINISHED;	} catch (ClosedChannelException cce) {	if (pendingWrites.isEmpty()) {	ret = COMMIT_STATUS.COMMIT_FINISHED;	} else {	ret = COMMIT_STATUS.COMMIT_ERROR;	}	} catch (IOException e) {	
got stream error during data sync 

private COMMIT_STATUS handleSpecialWait(boolean fromRead, long commitOffset, Channel channel, int xid, Nfs3FileAttributes preOpAttr) {	if (!fromRead) {	CommitCtx commitCtx = new CommitCtx(commitOffset, channel, xid, preOpAttr);	pendingCommits.put(commitOffset, commitCtx);	}	if (LOG.isDebugEnabled()) {	
return commit special wait 

public synchronized boolean streamCleanup(long fileId, long streamTimeout) {	Preconditions .checkState(streamTimeout >= NfsConfigKeys.DFS_NFS_STREAM_TIMEOUT_MIN_DEFAULT);	if (!activeState) {	return true;	}	boolean flag = false;	if (checkStreamTimeout(streamTimeout)) {	if (LOG.isDebugEnabled()) {	
stream can be closed for fileid 

private synchronized WriteCtx offerNextToWrite() {	if (pendingWrites.isEmpty()) {	if (LOG.isDebugEnabled()) {	
the async write task has no pending writes fileid 

}	Entry<OffsetRange, WriteCtx> lastEntry = pendingWrites.lastEntry();	OffsetRange range = lastEntry.getKey();	WriteCtx toWrite = lastEntry.getValue();	if (LOG.isTraceEnabled()) {	LOG.trace("range.getMin()=" + range.getMin() + " nextOffset=" + nextOffset);	}	long offset = nextOffset.get();	if (range.getMin() > offset) {	if (LOG.isDebugEnabled()) {	
the next sequential write has not arrived yet 

LOG.trace("range.getMin()=" + range.getMin() + " nextOffset=" + nextOffset);	}	long offset = nextOffset.get();	if (range.getMin() > offset) {	if (LOG.isDebugEnabled()) {	}	processCommits(nextOffset.get());	this.asyncStatus = false;	} else if (range.getMax() <= offset) {	if (LOG.isDebugEnabled()) {	
remove write which is already written from the list 

} else if (range.getMax() <= offset) {	if (LOG.isDebugEnabled()) {	}	pendingWrites.remove(range);	} else if (range.getMin() < offset && range.getMax() > offset) {	LOG.warn("Got an overlapping write " + range.toString() + ", nextOffset=" + offset + ". Remove and trim it");	pendingWrites.remove(range);	trimWriteRequest(toWrite, offset);	nextOffset.addAndGet(toWrite.getCount());	if (LOG.isDebugEnabled()) {	
change nextoffset after trim to 

} else if (range.getMin() < offset && range.getMax() > offset) {	LOG.warn("Got an overlapping write " + range.toString() + ", nextOffset=" + offset + ". Remove and trim it");	pendingWrites.remove(range);	trimWriteRequest(toWrite, offset);	nextOffset.addAndGet(toWrite.getCount());	if (LOG.isDebugEnabled()) {	}	return toWrite;	} else {	if (LOG.isDebugEnabled()) {	
remove write from the list 

nextOffset.addAndGet(toWrite.getCount());	if (LOG.isDebugEnabled()) {	}	return toWrite;	} else {	if (LOG.isDebugEnabled()) {	}	pendingWrites.remove(range);	nextOffset.addAndGet(toWrite.getCount());	if (LOG.isDebugEnabled()) {	
change nextoffset to 

try {	flushedOffset = getFlushedOffset();	entry = pendingCommits.firstEntry();	if (entry == null || entry.getValue().offset > flushedOffset) {	return;	}	fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));	status = Nfs3Status.NFS3_OK;	} catch (ClosedChannelException cce) {	if (!pendingWrites.isEmpty()) {	
can t sync for fileid channel closed with writes pending 

if (entry == null || entry.getValue().offset > flushedOffset) {	return;	}	fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));	status = Nfs3Status.NFS3_OK;	} catch (ClosedChannelException cce) {	if (!pendingWrites.isEmpty()) {	}	status = Nfs3Status.NFS3ERR_IO;	} catch (IOException e) {	
got stream error during data sync 

} catch (ClosedChannelException cce) {	if (!pendingWrites.isEmpty()) {	}	status = Nfs3Status.NFS3ERR_IO;	} catch (IOException e) {	status = Nfs3Status.NFS3ERR_IO;	}	try {	latestAttr = Nfs3Utils.getFileAttr(client, Nfs3Utils.getFileIdPath(latestAttr.getFileId()), iug);	} catch (IOException e) {	
can t get new file attr fileid 

status = Nfs3Status.NFS3ERR_IO;	} catch (IOException e) {	status = Nfs3Status.NFS3ERR_IO;	}	try {	latestAttr = Nfs3Utils.getFileAttr(client, Nfs3Utils.getFileIdPath(latestAttr.getFileId()), iug);	} catch (IOException e) {	status = Nfs3Status.NFS3ERR_IO;	}	if (latestAttr.getSize() != offset) {	
after sync the expect file size however actual file size is 

status = Nfs3Status.NFS3ERR_IO;	}	WccData wccData = new WccData(Nfs3Utils.getWccAttr(latestAttr), latestAttr);	while (entry != null && entry.getValue().offset <= flushedOffset) {	pendingCommits.remove(entry.getKey());	CommitCtx commit = entry.getValue();	COMMIT3Response response = new COMMIT3Response(status, wccData, Nfs3Constant.WRITE_COMMIT_VERF);	RpcProgramNfs3.metrics.addCommit(Nfs3Utils .getElapsedTime(commit.startTime));	Nfs3Utils.writeChannelCommit(commit.getChannel(), response .serialize(new XDR(), commit.getXid(), new VerifierNone()), commit.getXid());	if (LOG.isDebugEnabled()) {	
fileid service time ns sent response for commit 

private void doSingleWrite(final WriteCtx writeCtx) {	Channel channel = writeCtx.getChannel();	int xid = writeCtx.getXid();	long offset = writeCtx.getOffset();	int count = writeCtx.getCount();	WriteStableHow stableHow = writeCtx.getStableHow();	FileHandle handle = writeCtx.getHandle();	if (LOG.isDebugEnabled()) {	
do write fileid offset length stablehow 

long flushedOffset = getFlushedOffset();	if (flushedOffset != (offset + count)) {	throw new IOException("output stream is out of sync, pos=" + flushedOffset + " and nextOffset should be" + (offset + count));	}	if (writeCtx.getDataState() == WriteCtx.DataState.ALLOW_DUMP) {	synchronized (writeCtx) {	if (writeCtx.getDataState() == WriteCtx.DataState.ALLOW_DUMP) {	writeCtx.setDataState(WriteCtx.DataState.NO_DUMP);	updateNonSequentialWriteInMemory(-count);	if (LOG.isDebugEnabled()) {	
after writing at offset updated the memory count new value 

if (writeCtx.getDataState() == WriteCtx.DataState.ALLOW_DUMP) {	writeCtx.setDataState(WriteCtx.DataState.NO_DUMP);	updateNonSequentialWriteInMemory(-count);	if (LOG.isDebugEnabled()) {	}	}	}	}	if (!writeCtx.getReplied()) {	if (stableHow != WriteStableHow.UNSTABLE) {	
do sync for stable write 

if (!writeCtx.getReplied()) {	if (stableHow != WriteStableHow.UNSTABLE) {	try {	if (stableHow == WriteStableHow.DATA_SYNC) {	fos.hsync();	} else {	Preconditions.checkState(stableHow == WriteStableHow.FILE_SYNC, "Unknown WriteStableHow: " + stableHow);	fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));	}	} catch (IOException e) {	
hsync failed with writectx 

Preconditions.checkState(stableHow == WriteStableHow.FILE_SYNC, "Unknown WriteStableHow: " + stableHow);	fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));	}	} catch (IOException e) {	throw e;	}	}	WccAttr preOpAttr = latestAttr.getWccAttr();	WccData fileWcc = new WccData(preOpAttr, latestAttr);	if (writeCtx.getOriginalCount() != WriteCtx.INVALID_ORIGINAL_COUNT) {	
return original count instead of real data count 

WccData fileWcc = new WccData(preOpAttr, latestAttr);	if (writeCtx.getOriginalCount() != WriteCtx.INVALID_ORIGINAL_COUNT) {	count = writeCtx.getOriginalCount();	}	WRITE3Response response = new WRITE3Response(Nfs3Status.NFS3_OK, fileWcc, count, stableHow, Nfs3Constant.WRITE_COMMIT_VERF);	RpcProgramNfs3.metrics.addWrite(Nfs3Utils.getElapsedTime(writeCtx.startTime));	Nfs3Utils.writeChannel(channel, response.serialize( new XDR(), xid, new VerifierNone()), xid);	}	processCommits(writeCtx.getOffset() + writeCtx.getCount());	} catch (IOException e) {	
error writing to fileid at offset and length 

WRITE3Response response = new WRITE3Response(Nfs3Status.NFS3_OK, fileWcc, count, stableHow, Nfs3Constant.WRITE_COMMIT_VERF);	RpcProgramNfs3.metrics.addWrite(Nfs3Utils.getElapsedTime(writeCtx.startTime));	Nfs3Utils.writeChannel(channel, response.serialize( new XDR(), xid, new VerifierNone()), xid);	}	processCommits(writeCtx.getOffset() + writeCtx.getCount());	} catch (IOException e) {	if (!writeCtx.getReplied()) {	WRITE3Response response = new WRITE3Response(Nfs3Status.NFS3ERR_IO);	Nfs3Utils.writeChannel(channel, response.serialize( new XDR(), xid, new VerifierNone()), xid);	}	
clean up open file context for fileid 

========================= hadoop sample_7055 =========================

public K[] getSample(InputFormat<K,V> inf, Job job) throws IOException, InterruptedException {	List<InputSplit> splits = inf.getSplits(job);	ArrayList<K> samples = new ArrayList<K>(numSamples);	int splitsToSample = Math.min(maxSplitsSampled, splits.size());	Random r = new Random();	long seed = r.nextLong();	r.setSeed(seed);	
seed 

public static <K,V> void writePartitionFile(Job job, Sampler<K,V> sampler) throws IOException, ClassNotFoundException, InterruptedException {	Configuration conf = job.getConfiguration();	final InputFormat inf = ReflectionUtils.newInstance(job.getInputFormatClass(), conf);	int numPartitions = job.getNumReduceTasks();	K[] samples = (K[])sampler.getSample(inf, job);	
using samples 

========================= hadoop sample_5068 =========================

private void waitForDiskHealthCheck() {	long lastDisksCheckTime = dirsHandler.getLastDisksCheckTime();	long time = lastDisksCheckTime;	for (int i = 0; i < 10 && (time <= lastDisksCheckTime); i++) {	try {	Thread.sleep(1000);	} catch(InterruptedException e) {	
interrupted while waiting for nodemanager s disk health check 

private void prepareDirToFail(String dir) throws IOException {	File file = new File(dir);	FileUtil.fullyDelete(file);	file.createNewFile();	
prepared to fail 

========================= hadoop sample_1928 =========================

public void run() {	ShortCircuitCache.this.lock.lock();	try {	if (ShortCircuitCache.this.closed) return;	long curMs = Time.monotonicNow();	
cache cleaner running at 

try {	eldestKey = evictable.firstKey();	} catch (NoSuchElementException e) {	break;	}	evictionTimeNs = (Long)eldestKey;	long evictionTimeMs = TimeUnit.MILLISECONDS.convert(evictionTimeNs, TimeUnit.NANOSECONDS);	if (evictionTimeMs + maxNonMmappedEvictableLifespanMs >= curMs) break;	ShortCircuitReplica replica = (ShortCircuitReplica)evictable.get( eldestKey);	if (LOG.isTraceEnabled()) {	
cachecleaner purging 

}	evictionTimeNs = (Long)eldestKey;	long evictionTimeMs = TimeUnit.MILLISECONDS.convert(evictionTimeNs, TimeUnit.NANOSECONDS);	if (evictionTimeMs + maxNonMmappedEvictableLifespanMs >= curMs) break;	ShortCircuitReplica replica = (ShortCircuitReplica)evictable.get( eldestKey);	if (LOG.isTraceEnabled()) {	}	purge(replica);	numPurged++;	}	
finishing cache cleaner run started at demoted mmapped replicas purged replicas 

public void run() {	
about to release 

boolean success = false;	try (DomainSocket sock = DomainSocket.connect(path);	DataOutputStream out = new DataOutputStream( new BufferedOutputStream(sock.getOutputStream()))) {	new Sender(out).releaseShortCircuitFds(slot.getSlotId());	DataInputStream in = new DataInputStream(sock.getInputStream());	ReleaseShortCircuitAccessResponseProto resp = ReleaseShortCircuitAccessResponseProto.parseFrom( PBHelperClient.vintPrefixed(in));	if (resp.getStatus() != Status.SUCCESS) {	String error = resp.hasError() ? resp.getError() : "(unknown)";	throw new IOException(resp.getStatus().toString() + ": " + error);	}	
released 

DataOutputStream out = new DataOutputStream( new BufferedOutputStream(sock.getOutputStream()))) {	new Sender(out).releaseShortCircuitFds(slot.getSlotId());	DataInputStream in = new DataInputStream(sock.getInputStream());	ReleaseShortCircuitAccessResponseProto resp = ReleaseShortCircuitAccessResponseProto.parseFrom( PBHelperClient.vintPrefixed(in));	if (resp.getStatus() != Status.SUCCESS) {	String error = resp.hasError() ? resp.getError() : "(unknown)";	throw new IOException(resp.getStatus().toString() + ": " + error);	}	success = true;	} catch (IOException e) {	
failed to release short circuit shared memory slot by sending releaseshortcircuitaccessrequestproto to closing shared memory segment 

this.maxEvictableMmapedSize = maxEvictableMmapedSize;	Preconditions.checkArgument(maxEvictableMmapedLifespanMs >= 0);	this.maxEvictableMmapedLifespanMs = maxEvictableMmapedLifespanMs;	this.mmapRetryTimeoutMs = mmapRetryTimeoutMs;	this.staleThresholdMs = staleThresholdMs;	DfsClientShmManager shmManager = null;	if ((shmInterruptCheckMs > 0) && (DomainSocketWatcher.getLoadingFailureReason() == null)) {	try {	shmManager = new DfsClientShmManager(shmInterruptCheckMs);	} catch (IOException e) {	
failed to create shortcircuitshmmanager 

private void ref(ShortCircuitReplica replica) {	lock.lock();	try {	Preconditions.checkArgument(replica.refCount > 0, "can't ref %s because its refCount reached %d", replica, replica.refCount);	Long evictableTimeNs = replica.getEvictableTimeNs();	replica.refCount++;	if (evictableTimeNs != null) {	String removedFrom = removeEvictable(replica);	if (LOG.isTraceEnabled()) {	
no longer contains refcount 

lock.lock();	try {	Preconditions.checkArgument(replica.refCount > 0, "can't ref %s because its refCount reached %d", replica, replica.refCount);	Long evictableTimeNs = replica.getEvictableTimeNs();	replica.refCount++;	if (evictableTimeNs != null) {	String removedFrom = removeEvictable(replica);	if (LOG.isTraceEnabled()) {	}	} else if (LOG.isTraceEnabled()) {	
replica refcount 

long evictionTimeMs = TimeUnit.MILLISECONDS.convert(evictionTimeNs, TimeUnit.NANOSECONDS);	if (evictionTimeMs + maxEvictableMmapedLifespanMs >= now) {	if (evictableMmapped.size() < maxEvictableMmapedSize) {	break;	}	needMoreSpace = true;	}	ShortCircuitReplica replica = (ShortCircuitReplica)evictableMmapped.get( eldestKey);	if (LOG.isTraceEnabled()) {	String rationale = needMoreSpace ? "because we need more space" : "because it's too old";	
demoteoldevictable demoting 

try {	if (evictableSize == 0) {	replica = (ShortCircuitReplica)evictableMmapped.get(evictableMmapped .firstKey());	} else {	replica = (ShortCircuitReplica)evictable.get(evictable.firstKey());	}	} catch (NoSuchElementException e) {	break;	}	if (LOG.isTraceEnabled()) {	
trimevictionmaps is purging 

public ShortCircuitReplicaInfo fetchOrCreate(ExtendedBlockId key, ShortCircuitReplicaCreator creator) {	Waitable<ShortCircuitReplicaInfo> newWaitable = null;	lock.lock();	try {	ShortCircuitReplicaInfo info = null;	do {	if (closed) {	
can t fethchorcreate because the cache is closed 

ShortCircuitReplicaInfo info = null;	do {	if (closed) {	return null;	}	Waitable<ShortCircuitReplicaInfo> waitable = replicaInfoMap.get(key);	if (waitable != null) {	try {	info = fetch(key, waitable);	} catch (RetriableException e) {	
retrying 

private ShortCircuitReplicaInfo fetch(ExtendedBlockId key, Waitable<ShortCircuitReplicaInfo> waitable) throws RetriableException {	ShortCircuitReplicaInfo info;	try {	
found waitable for 

private ShortCircuitReplicaInfo fetch(ExtendedBlockId key, Waitable<ShortCircuitReplicaInfo> waitable) throws RetriableException {	ShortCircuitReplicaInfo info;	try {	info = waitable.await();	} catch (InterruptedException e) {	
interrupted while waiting for 

private ShortCircuitReplicaInfo fetch(ExtendedBlockId key, Waitable<ShortCircuitReplicaInfo> waitable) throws RetriableException {	ShortCircuitReplicaInfo info;	try {	info = waitable.await();	} catch (InterruptedException e) {	Thread.currentThread().interrupt();	throw new RetriableException("interrupted");	}	if (info.getInvalidTokenException() != null) {	
could not get due to invalidtoken exception 

info = waitable.await();	} catch (InterruptedException e) {	Thread.currentThread().interrupt();	throw new RetriableException("interrupted");	}	if (info.getInvalidTokenException() != null) {	return info;	}	ShortCircuitReplica replica = info.getReplica();	if (replica == null) {	
failed to get 

return info;	}	ShortCircuitReplica replica = info.getReplica();	if (replica == null) {	return info;	}	if (replica.purged) {	throw new RetriableException("Ignoring purged replica " + replica + ".  Retrying.");	}	if (replica.isStale()) {	
got stale replica removing this replica from the replicainfomap and retrying 

private ShortCircuitReplicaInfo create(ExtendedBlockId key, ShortCircuitReplicaCreator creator, Waitable<ShortCircuitReplicaInfo> newWaitable) {	ShortCircuitReplicaInfo info = null;	try {	
loading 

private ShortCircuitReplicaInfo create(ExtendedBlockId key, ShortCircuitReplicaCreator creator, Waitable<ShortCircuitReplicaInfo> newWaitable) {	ShortCircuitReplicaInfo info = null;	try {	info = creator.createShortCircuitReplicaInfo();	} catch (RuntimeException e) {	
failed to load 

private ShortCircuitReplicaInfo create(ExtendedBlockId key, ShortCircuitReplicaCreator creator, Waitable<ShortCircuitReplicaInfo> newWaitable) {	ShortCircuitReplicaInfo info = null;	try {	info = creator.createShortCircuitReplicaInfo();	} catch (RuntimeException e) {	}	if (info == null) info = new ShortCircuitReplicaInfo();	lock.lock();	try {	if (info.getReplica() != null) {	
successfully loaded 

}	if (info == null) info = new ShortCircuitReplicaInfo();	lock.lock();	try {	if (info.getReplica() != null) {	startCacheCleanerThreadIfNeeded();	} else {	Waitable<ShortCircuitReplicaInfo> waitableInMap = replicaInfoMap.get(key);	if (waitableInMap == newWaitable) replicaInfoMap.remove(key);	if (info.getInvalidTokenException() != null) {	
could not load due to invalidtoken exception 

if (info == null) info = new ShortCircuitReplicaInfo();	lock.lock();	try {	if (info.getReplica() != null) {	startCacheCleanerThreadIfNeeded();	} else {	Waitable<ShortCircuitReplicaInfo> waitableInMap = replicaInfoMap.get(key);	if (waitableInMap == newWaitable) replicaInfoMap.remove(key);	if (info.getInvalidTokenException() != null) {	} else {	
failed to load 

private void startCacheCleanerThreadIfNeeded() {	if (cacheCleaner == null) {	cacheCleaner = new CacheCleaner();	long rateMs = cacheCleaner.getRateInMs();	ScheduledFuture<?> future = cleanerExecutor.scheduleAtFixedRate(cacheCleaner, rateMs, rateMs, TimeUnit.MILLISECONDS);	cacheCleaner.setFuture(future);	
starting cache cleaner thread which will run every ms 

public void close() {	try {	lock.lock();	if (closed) return;	closed = true;	
closing 

}	purge((ShortCircuitReplica)evictableMmapped.get(eldestKey));	}	} finally {	lock.unlock();	}	releaserExecutor.shutdown();	cleanerExecutor.shutdown();	try {	if (!releaserExecutor.awaitTermination(30, TimeUnit.SECONDS)) {	
forcing slotreleaserthreadpool to shutdown 

}	releaserExecutor.shutdown();	cleanerExecutor.shutdown();	try {	if (!releaserExecutor.awaitTermination(30, TimeUnit.SECONDS)) {	releaserExecutor.shutdownNow();	}	} catch (InterruptedException e) {	releaserExecutor.shutdownNow();	Thread.currentThread().interrupt();	
interrupted while waiting for slotreleaserthreadpool to terminate 

try {	if (!releaserExecutor.awaitTermination(30, TimeUnit.SECONDS)) {	releaserExecutor.shutdownNow();	}	} catch (InterruptedException e) {	releaserExecutor.shutdownNow();	Thread.currentThread().interrupt();	}	try {	if (!cleanerExecutor.awaitTermination(30, TimeUnit.SECONDS)) {	
forcing cleanerthreadpool to shutdown 

releaserExecutor.shutdownNow();	Thread.currentThread().interrupt();	}	try {	if (!cleanerExecutor.awaitTermination(30, TimeUnit.SECONDS)) {	cleanerExecutor.shutdownNow();	}	} catch (InterruptedException e) {	cleanerExecutor.shutdownNow();	Thread.currentThread().interrupt();	
interrupted while waiting for cleanerthreadpool to terminate 

========================= hadoop sample_6982 =========================

public static boolean isAvailable() {	try {	if (!Shell.LINUX) {	
procfsbasedprocesstree currently is supported only on linux 

public static boolean isAvailable() {	try {	if (!Shell.LINUX) {	return false;	}	} catch (SecurityException se) {	
failed to get operating system name 

}	if (p.getAge() > olderThanAge) {	ProcessTreeSmapMemInfo procMemInfo = processSMAPTree.get(p.getPid());	if (procMemInfo != null) {	for (ProcessSmapMemoryInfo info : procMemInfo.getMemoryInfoList()) {	if (info.getPermission().trim() .equalsIgnoreCase(READ_ONLY_WITH_SHARED_PERMISSION) || info.getPermission().trim() .equalsIgnoreCase(READ_EXECUTE_WITH_SHARED_PERMISSION)) {	continue;	}	total += info.anonymous;	if (LOG.isDebugEnabled()) {	
total pid info total 

}	if (LOG.isDebugEnabled()) {	LOG.debug(procMemInfo.toString());	}	}	}	}	if (total > 0) {	total *= KB_TO_BYTES;	}	
smapbasedcumulativerssmem bytes 

}	ret = pinfo;	try {	String str = in.readLine();	Matcher m = PROCFS_STAT_FILE_FORMAT.matcher(str);	boolean mat = m.find();	if (mat) {	String processName = "(" + m.group(2) + ")";	pinfo.updateProcessInfo(processName, m.group(3), Integer.parseInt(m.group(4)), Integer.parseInt(m.group(5)), Long.parseLong(m.group(7)), new BigInteger(m.group(8)), Long.parseLong(m.group(10)), Long.parseLong(m.group(11)));	} else {	
unexpected procfs stat file is not in the expected format for process with pid 

String str = in.readLine();	Matcher m = PROCFS_STAT_FILE_FORMAT.matcher(str);	boolean mat = m.find();	if (mat) {	String processName = "(" + m.group(2) + ")";	pinfo.updateProcessInfo(processName, m.group(3), Integer.parseInt(m.group(4)), Integer.parseInt(m.group(5)), Long.parseLong(m.group(7)), new BigInteger(m.group(8)), Long.parseLong(m.group(10)), Long.parseLong(m.group(11)));	} else {	ret = null;	}	} catch (IOException io) {	
error reading the stream 

ret = null;	}	} catch (IOException io) {	ret = null;	} finally {	try {	fReader.close();	try {	in.close();	} catch (IOException i) {	
error closing the stream 

} catch (IOException io) {	ret = null;	} finally {	try {	fReader.close();	try {	in.close();	} catch (IOException i) {	}	} catch (IOException i) {	
error closing the stream 

public void updateJiffy(ProcessInfo oldInfo) {	if (oldInfo == null) {	BigInteger sum = this.stime.add(BigInteger.valueOf(this.utime));	if (sum.compareTo(MAX_LONG) > 0) {	this.dtime = 0L;	
sum of stime and utime is greater than 

ret = in.readLine();	if (ret == null) {	ret = "N/A";	} else {	ret = ret.replace('\0', ' ');	if (ret.equals("")) {	ret = "N/A";	}	}	} catch (IOException io) {	
error reading the stream 

}	}	} catch (IOException io) {	ret = "N/A";	} finally {	try {	fReader.close();	try {	in.close();	} catch (IOException i) {	
error closing the stream 

} catch (IOException io) {	ret = "N/A";	} finally {	try {	fReader.close();	try {	in.close();	} catch (IOException i) {	}	} catch (IOException i) {	
error closing the stream 

memoryMappingInfo = new ProcessSmapMemoryInfo(line);	memoryMappingInfo.setPermission(address.group(4));	pInfo.getMemoryInfoList().add(memoryMappingInfo);	continue;	}	Matcher memInfo = MEM_INFO_PATTERN.matcher(line);	if (memInfo.find()) {	String key = memInfo.group(1).trim();	String value = memInfo.group(2).replace(KB, "").trim();	if (LOG.isDebugEnabled()) {	
meminfo value 

if (memInfo.find()) {	String key = memInfo.group(1).trim();	String value = memInfo.group(2).replace(KB, "").trim();	if (LOG.isDebugEnabled()) {	}	if (memoryMappingInfo != null) {	memoryMappingInfo.setMemInfo(key, value);	}	}	} catch (Throwable t) {	
error parsing smaps line 

public void setMemInfo(String key, String value) {	MemInfo info = MemInfo.getMemInfoByName(key);	int val = 0;	try {	val = Integer.parseInt(value.trim());	} catch (NumberFormatException ne) {	
error in parsing value 

int val = 0;	try {	val = Integer.parseInt(value.trim());	} catch (NumberFormatException ne) {	return;	}	if (info == null) {	return;	}	if (LOG.isDebugEnabled()) {	
setmeminfo meminfo 

========================= hadoop sample_2168 =========================

private void verifyKillAppFailure(String submitter, String killer, String queueName, boolean setupACLs) throws Exception {	ApplicationId applicationId = submitAppAndGetAppId(submitter, queueName, setupACLs);	final KillApplicationRequest finishAppRequest = KillApplicationRequest.newInstance(applicationId);	ApplicationClientProtocol killerClient = getRMClientForUser(killer);	try {	killerClient.forceKillApplication(finishAppRequest);	Assert.fail("App killing by the enemy should fail!!");	} catch (YarnException e) {	
got exception while killing app as the enemy 

========================= hadoop sample_623 =========================

public static RMStateStore getStore(Configuration conf) {	Class<? extends RMStateStore> storeClass = conf.getClass(YarnConfiguration.RM_STORE, MemoryRMStateStore.class, RMStateStore.class);	
using rmstatestore implementation 

========================= hadoop sample_766 =========================

logger.removeAppender(AD_HOC_DUMPER_APPENDER);	logger.setLevel(currentLogLevel);	for (Enumeration appenders = Logger.getRootLogger().getAllAppenders(); appenders .hasMoreElements();) {	Object obj = appenders.nextElement();	if (obj instanceof AppenderSkeleton) {	AppenderSkeleton appender = (AppenderSkeleton) obj;	appender.setThreshold(appenderLevels.get(appender.getName()));	}	}	logFlag = false;	
done dumping adhoc logs for 

========================= hadoop sample_2187 =========================

for (long block = 1; block <= blocks; block++) {	out.write(data);	long written = block * UPLOAD_BLOCKSIZE;	if (block % blocksPer10MB == 0 || written == filesize) {	long percentage = written * 100 / filesize;	double elapsedTime = timer.elapsedTime() / NANOSEC;	double writtenMB = 1.0 * written / S_1M;	LOG.info(String.format("[%02d%%] Buffered %.2f MB out of %d MB;" + " elapsedTime=%.2fs; write to buffer bandwidth=%.2f MB/s", percentage, writtenMB, filesizeMB, elapsedTime, writtenMB / elapsedTime));	}	}	
closing stream 

readAtByte0.end("time to read data at start of file");	ops++;	readAtEOF = new ContractTestUtils.NanoTimer();	in.readFully(eof - bufferSize, buffer);	readAtEOF.end("time to read data at end of file");	ops++;	readAtByte0Again = new ContractTestUtils.NanoTimer();	in.readFully(0, buffer);	readAtByte0Again.end("time to read data at start of file again");	ops++;	
final stream state 

NativeAzureFileSystem fs = getFileSystem();	FileStatus status = fs.getFileStatus(hugefile);	long filesize = status.getLen();	long blocks = filesize / UPLOAD_BLOCKSIZE;	byte[] data = new byte[UPLOAD_BLOCKSIZE];	ContractTestUtils.NanoTimer timer = new ContractTestUtils.NanoTimer();	try (FSDataInputStream in = openDataFile()) {	for (long block = 0; block < blocks; block++) {	in.readFully(data);	}	
final stream state 

if (bytesRead == 1) {	break;	}	remaining -= bytesRead;	offset += bytesRead;	count += bytesRead;	readTimer.end();	if (bytesRead != 0) {	LOG.debug("Bytes in read #{}: {} , block bytes: {}," + " remaining in block: {}" + " duration={} nS; ns/byte: {}, bandwidth={} MB/s", reads, bytesRead, blockSize - remaining, remaining, readTimer.duration(), readTimer.nanosPerOperation(bytesRead), readTimer.bandwidthDescription(bytesRead));	} else {	
bytes returned by read operation 

offset += bytesRead;	count += bytesRead;	readTimer.end();	if (bytesRead != 0) {	LOG.debug("Bytes in read #{}: {} , block bytes: {}," + " remaining in block: {}" + " duration={} nS; ns/byte: {}, bandwidth={} MB/s", reads, bytesRead, blockSize - remaining, remaining, readTimer.duration(), readTimer.nanosPerOperation(bytesRead), readTimer.bandwidthDescription(bytesRead));	} else {	}	}	blockTimer.end("Reading block %d in %d reads", blockId, reads);	String bw = blockTimer.bandwidthDescription(blockSize);	
bandwidth of block mb s 

count += bytesRead;	readTimer.end();	if (bytesRead != 0) {	LOG.debug("Bytes in read #{}: {} , block bytes: {}," + " remaining in block: {}" + " duration={} nS; ns/byte: {}, bandwidth={} MB/s", reads, bytesRead, blockSize - remaining, remaining, readTimer.duration(), readTimer.nanosPerOperation(bytesRead), readTimer.bandwidthDescription(bytesRead));	} else {	}	}	blockTimer.end("Reading block %d in %d reads", blockId, reads);	String bw = blockTimer.bandwidthDescription(blockSize);	if (bandwidthInBytes(blockTimer, blockSize) < minimumBandwidth) {	
bandwidth too low on block resetting connection 

String bw = blockTimer.bandwidthDescription(blockSize);	if (bandwidthInBytes(blockTimer, blockSize) < minimumBandwidth) {	Assert.assertTrue("Bandwidth of " + bw + " too low after " + resetCount + " attempts", resetCount <= maxResetCount);	resetCount++;	}	}	} finally {	IOUtils.closeStream(in);	}	timer2.end("Time to read %d bytes in %d blocks", totalToRead, blockCount);	
overall bandwidth mb s reset connections 

========================= hadoop sample_6347 =========================

public synchronized boolean isSupported() {	if (!checked) {	checked = true;	String extClazzConf = conf.get(CONF_LZO_CLASS);	String extClazz = (extClazzConf != null) ? extClazzConf : System.getProperty(CONF_LZO_CLASS);	String clazz = (extClazz != null) ? extClazz : defaultClazz;	try {	
trying to load lzo codec class 

public abstract InputStream createDecompressionStream( InputStream downStream, Decompressor decompressor, int downStreamBufferSize) throws IOException;	public abstract OutputStream createCompressionStream( OutputStream downStream, Compressor compressor, int downStreamBufferSize) throws IOException;	public abstract boolean isSupported();	public Compressor getCompressor() throws IOException {	CompressionCodec codec = getCodec();	if (codec != null) {	Compressor compressor = CodecPool.getCompressor(codec);	if (compressor != null) {	if (compressor.finished()) {	
compressor obtained from codecpool already finished 

public abstract OutputStream createCompressionStream( OutputStream downStream, Compressor compressor, int downStreamBufferSize) throws IOException;	public abstract boolean isSupported();	public Compressor getCompressor() throws IOException {	CompressionCodec codec = getCodec();	if (codec != null) {	Compressor compressor = CodecPool.getCompressor(codec);	if (compressor != null) {	if (compressor.finished()) {	} else {	if(LOG.isDebugEnabled()) {	
got a compressor 

public void returnCompressor(Compressor compressor) {	if (compressor != null) {	if(LOG.isDebugEnabled()) {	
return a compressor 

public Decompressor getDecompressor() throws IOException {	CompressionCodec codec = getCodec();	if (codec != null) {	Decompressor decompressor = CodecPool.getDecompressor(codec);	if (decompressor != null) {	if (decompressor.finished()) {	
deompressor obtained from codecpool already finished 

public Decompressor getDecompressor() throws IOException {	CompressionCodec codec = getCodec();	if (codec != null) {	Decompressor decompressor = CodecPool.getDecompressor(codec);	if (decompressor != null) {	if (decompressor.finished()) {	} else {	if(LOG.isDebugEnabled()) {	
got a decompressor 

public void returnDecompressor(Decompressor decompressor) {	if (decompressor != null) {	if(LOG.isDebugEnabled()) {	
returned a decompressor 

========================= hadoop sample_3903 =========================

private void createInputFile(String rootName) throws IOException {	cleanup();	Path inputFile = new Path(MAP_INPUT_DIR, "in_file");	SequenceFile.Writer writer = SequenceFile.createWriter(fs, fsConfig, inputFile, Text.class, LongWritable.class, CompressionType.NONE);	try {	nrFiles = 0;	listSubtree(new Path(rootName), writer);	} finally {	writer.close();	}	
created map input files 

long actualSize = 0;	try {	long blockSize = fs.getDefaultBlockSize(p);	reporter.setStatus("reading " + name + "@" + offset + "/" + blockSize);	for( int curSize = bufferSize;	curSize == bufferSize && actualSize < blockSize;	actualSize += curSize) {	curSize = in.read(buffer, 0, bufferSize);	}	} catch(IOException e) {	
corrupted block detected in name at 

private void cleanup() throws IOException {	
cleaning up test files 

========================= hadoop sample_5584 =========================

private void release(ContainerId container) {	if (ref.remove(container)) {	timestamp.set(currentTime());	} else {	
container doesn t exist in the container list of the resource to which it sent release event 

public void handle(ResourceEvent event) {	try {	this.writeLock.lock();	Path resourcePath = event.getLocalResourceRequest().getPath();	if (LOG.isDebugEnabled()) {	
processing of type 

try {	this.writeLock.lock();	Path resourcePath = event.getLocalResourceRequest().getPath();	if (LOG.isDebugEnabled()) {	}	ResourceState oldState = this.stateMachine.getCurrentState();	ResourceState newState = null;	try {	newState = this.stateMachine.doTransition(event.getType(), event);	} catch (InvalidStateTransitionException e) {	
can t handle this event at current state 

========================= hadoop sample_1894 =========================

protected boolean removeEldestEntry(Map.Entry<K1, Map<K2, V>> e) {	boolean overflow = expireKey1At(size());	if (overflow && !gotOverflow) {	
metrics intern cache overflow at for 

========================= hadoop sample_3450 =========================

public List<String> resolve(List<String> hostList) {	Assert.assertTrue("hostList size is " + hostList.size(), hostList.size() <= 1);	List<String> returnList = new ArrayList<String>();	if (hostList.isEmpty()) {	return returnList;	}	if (hostList.get(0).equals(invalidHost)) {	return null;	}	
received resolve request for 

========================= hadoop sample_2095 =========================

protected void doGet(final HttpServletRequest req, final HttpServletResponse resp) throws ServletException, IOException {	final UserGroupInformation ugi;	final ServletContext context = getServletContext();	final Configuration conf = NameNodeHttpServer.getConfFromContext(context);	try {	ugi = getUGI(req, conf);	} catch(IOException ioe) {	
request for token received with no authentication from 

final Token<DelegationTokenIdentifier> token = new Token<DelegationTokenIdentifier>();	token.decodeFromUrlString(tokenString);	try {	ugi.doAs(new PrivilegedExceptionAction<Void>() {	public Void run() throws Exception {	nn.getRpcServer().cancelDelegationToken(token);	return null;	}	});	} catch(Exception e) {	
exception while cancelling token re throwing 

========================= hadoop sample_8027 =========================

private MiniQJMHACluster(Builder builder) throws IOException {	this.conf = builder.conf;	int retryCount = 0;	int basePort = 10000;	while (true) {	try {	basePort = 10000 + RANDOM.nextInt(1000) * 4;	
set miniqjmhacluster baseport to 

cluster.getNameNodeInfos()[0].setStartOpt(builder.startOpt);	cluster.getNameNodeInfos()[1].setStartOpt(builder.startOpt);	cluster.restartNameNodes();	break;	} catch (BindException e) {	if (cluster != null) {	cluster.shutdown(true);	cluster = null;	}	++retryCount;	
miniqjmhacluster port conflicts retried times 

========================= hadoop sample_7628 =========================

public FillWritable(byte fillChar) {	this.fillChar = fillChar;	r = new Random();	final long seed = r.nextLong();	
seed 

job.getConfiguration().set(MRConfig.FRAMEWORK_NAME, MRConfig.LOCAL_FRAMEWORK_NAME);	job.getConfiguration().setInt(MRJobConfig.IO_SORT_FACTOR, 1000);	job.getConfiguration().set("fs.defaultFS", "file: job.getConfiguration().setInt("test.mapcollection.num.maps", 1);	job.setInputFormatClass(FakeIF.class);	job.setOutputFormatClass(NullOutputFormat.class);	job.setMapperClass(Mapper.class);	job.setReducerClass(SpillReducer.class);	job.setMapOutputKeyClass(KeyWritable.class);	job.setMapOutputValueClass(ValWritable.class);	job.setSortComparatorClass(VariableComparator.class);	
running 

public void testRandom() throws Exception {	Configuration conf = new Configuration();	conf.setInt(Job.COMPLETION_POLL_INTERVAL_KEY, 100);	Job job = Job.getInstance(conf);	conf = job.getConfiguration();	conf.setInt(MRJobConfig.IO_SORT_MB, 1);	conf.setClass("test.mapcollection.class", RandomFactory.class, RecordFactory.class);	final Random r = new Random();	final long seed = r.nextLong();	
seed 

public void testRandomCompress() throws Exception {	Configuration conf = new Configuration();	conf.setInt(Job.COMPLETION_POLL_INTERVAL_KEY, 100);	Job job = Job.getInstance(conf);	conf = job.getConfiguration();	conf.setInt(MRJobConfig.IO_SORT_MB, 1);	conf.setBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, true);	conf.setClass("test.mapcollection.class", RandomFactory.class, RecordFactory.class);	final Random r = new Random();	final long seed = r.nextLong();	
seed 

========================= hadoop sample_5615 =========================

public long renewTokenForAppCollector( AppLevelTimelineCollector appCollector) throws IOException {	if (appCollector.getDelegationTokenForApp() != null) {	return tokenMgrService.renewToken(appCollector.getDelegationTokenForApp(), appCollector.getAppDelegationTokenRenewer());	} else {	
delegation token not available for renewal for app 

private org.apache.hadoop.yarn.api.records.Token generateTokenAndSetTimer( ApplicationId appId, AppLevelTimelineCollector appCollector) throws IOException {	Token<TimelineDelegationTokenIdentifier> timelineToken = generateTokenForAppCollector(appCollector.getAppUser());	TimelineDelegationTokenIdentifier tokenId = timelineToken.decodeIdentifier();	long renewalDelay = getRenewalDelay(tokenRenewInterval);	long regenerationDelay = getRegenerationDelay(tokenId.getMaxDate());	if (renewalDelay > 0 || regenerationDelay > 0) {	boolean isTimerForRenewal = renewalDelay < regenerationDelay;	Future<?> renewalOrRegenerationFuture = tokenRenewalExecutor.schedule( new CollectorTokenRenewer(appId, isTimerForRenewal), isTimerForRenewal? renewalDelay : regenerationDelay, TimeUnit.MILLISECONDS);	appCollector.setDelegationTokenAndFutureForApp(timelineToken, renewalOrRegenerationFuture, tokenId.getMaxDate(), tokenId.getRenewer().toString());	}	
generated a new token for app 

protected void doPostPut(ApplicationId appId, TimelineCollector collector) {	try {	updateTimelineCollectorContext(appId, collector);	org.apache.hadoop.yarn.api.records.Token token = null;	if (UserGroupInformation.isSecurityEnabled() && collector instanceof AppLevelTimelineCollector) {	AppLevelTimelineCollector appCollector = (AppLevelTimelineCollector) collector;	token = generateTokenAndSetTimer(appId, appCollector);	}	reportNewCollectorInfoToNM(appId, token);	} catch (YarnException | IOException e) {	
failed to communicate with nm collector service for 

protected void postRemove(ApplicationId appId, TimelineCollector collector) {	if (collector instanceof AppLevelTimelineCollector) {	try {	cancelTokenForAppCollector((AppLevelTimelineCollector) collector);	} catch (IOException e) {	
failed to cancel token for app collector with appid 

timelineRestServer = builder.build();	timelineRestServer.addJerseyResourcePackage( TimelineCollectorWebService.class.getPackage().getName() + ";" + GenericExceptionHandler.class.getPackage().getName() + ";" + YarnJacksonJaxbJsonProvider.class.getPackage().getName(), "/*");	timelineRestServer.setAttribute(COLLECTOR_MANAGER_ATTR_KEY, this);	timelineRestServer.start();	} catch (Exception e) {	String msg = "The per-node collector webapp failed to start.";	LOG.error(msg, e);	throw new YarnRuntimeException(msg, e);	}	this.timelineRestServerBindAddress = WebAppUtils.getResolvedAddress( timelineRestServer.getConnectorAddress(0));	
instantiated the per node collector webapp at 

private void reportNewCollectorInfoToNM(ApplicationId appId, org.apache.hadoop.yarn.api.records.Token token) throws YarnException, IOException {	ReportNewCollectorInfoRequest request = ReportNewCollectorInfoRequest.newInstance(appId, this.timelineRestServerBindAddress, token);	
report a new collector for application to the nm collector service 

private void updateTimelineCollectorContext( ApplicationId appId, TimelineCollector collector) throws YarnException, IOException {	GetTimelineCollectorContextRequest request = GetTimelineCollectorContextRequest.newInstance(appId);	
get timeline collector context for 

private void updateTimelineCollectorContext( ApplicationId appId, TimelineCollector collector) throws YarnException, IOException {	GetTimelineCollectorContextRequest request = GetTimelineCollectorContextRequest.newInstance(appId);	GetTimelineCollectorContextResponse response = getNMCollectorService().getTimelineCollectorContext(request);	String userId = response.getUserId();	if (userId != null && !userId.isEmpty()) {	if (LOG.isDebugEnabled()) {	
setting the user in the context 

GetTimelineCollectorContextResponse response = getNMCollectorService().getTimelineCollectorContext(request);	String userId = response.getUserId();	if (userId != null && !userId.isEmpty()) {	if (LOG.isDebugEnabled()) {	}	collector.getTimelineEntityContext().setUserId(userId);	}	String flowName = response.getFlowName();	if (flowName != null && !flowName.isEmpty()) {	if (LOG.isDebugEnabled()) {	
setting the flow name 

}	String flowName = response.getFlowName();	if (flowName != null && !flowName.isEmpty()) {	if (LOG.isDebugEnabled()) {	}	collector.getTimelineEntityContext().setFlowName(flowName);	}	String flowVersion = response.getFlowVersion();	if (flowVersion != null && !flowVersion.isEmpty()) {	if (LOG.isDebugEnabled()) {	
setting the flow version 

}	String flowVersion = response.getFlowVersion();	if (flowVersion != null && !flowVersion.isEmpty()) {	if (LOG.isDebugEnabled()) {	}	collector.getTimelineEntityContext().setFlowVersion(flowVersion);	}	long flowRunId = response.getFlowRunId();	if (flowRunId != 0L) {	if (LOG.isDebugEnabled()) {	
setting the flow run id 

protected CollectorNodemanagerProtocol getNMCollectorService() {	if (nmCollectorService == null) {	synchronized (this) {	if (nmCollectorService == null) {	Configuration conf = getConfig();	InetSocketAddress nmCollectorServiceAddress = conf.getSocketAddr( YarnConfiguration.NM_BIND_HOST, YarnConfiguration.NM_COLLECTOR_SERVICE_ADDRESS, YarnConfiguration.DEFAULT_NM_COLLECTOR_SERVICE_ADDRESS, YarnConfiguration.DEFAULT_NM_COLLECTOR_SERVICE_PORT);	
nmcollectorserviceaddress 

private void regenerateToken(AppLevelTimelineCollector appCollector) throws IOException {	org.apache.hadoop.yarn.api.records.Token token = generateTokenAndSetTimer(appId, appCollector);	try {	reportNewCollectorInfoToNM(appId, token);	} catch (YarnException e) {	
unable to report regenerated token to nm for 

public void run() {	TimelineCollector collector = get(appId);	if (collector == null) {	
cannot find active collector while renewing regenerating token for 

AppLevelTimelineCollector appCollector = (AppLevelTimelineCollector) collector;	synchronized (collector) {	if (!collector.isStopped()) {	try {	if (timerForRenewal) {	renewToken(appCollector);	} else {	regenerateToken(appCollector);	}	} catch (Exception e) {	
unable to renew regenerate token for 

========================= hadoop sample_361 =========================

public void testLazyPersistBlocksAreSaved() throws IOException, InterruptedException, TimeoutException {	getClusterBuilder().build();	final int NUM_BLOCKS = 10;	final String METHOD_NAME = GenericTestUtils.getMethodName();	Path path = new Path("/" + METHOD_NAME + ".dat");	makeTestFile(path, BLOCK_SIZE * NUM_BLOCKS, true);	LocatedBlocks locatedBlocks = ensureFileReplicasOnStorageType(path, RAM_DISK);	waitForMetric("RamDiskBlocksLazyPersisted", NUM_BLOCKS);	
verifying copy was saved to lazypersist 

waitForMetric("RamDiskBlocksLazyPersisted", NUM_PATHS);	for (int i = 0; i < NUM_PATHS; ++i) {	ensureFileReplicasOnStorageType(paths[i], RAM_DISK);	}	ArrayList<Integer> indexes = new ArrayList<Integer>(NUM_PATHS);	for (int i = 0; i < NUM_PATHS; ++i) {	indexes.add(i);	}	Collections.shuffle(indexes);	for (int i = 0; i < NUM_PATHS; ++i) {	
touching file 

========================= hadoop sample_7278 =========================

public void test() throws IllegalArgumentException, IOException {	Configuration conf = new Configuration();	conf.set(SynthTraceJobProducer.SLS_SYNTHETIC_TRACE_FILE, "src/test/resources/syn.json");	SynthTraceJobProducer stjp = new SynthTraceJobProducer(conf);	SynthJob js = (SynthJob) stjp.getNextJob();	int jobCount = 0;	while (js != null) {	
conf submission duration nummaps numreduces 

========================= hadoop sample_6039 =========================

private List<FileStatus> listDirectory(SwiftObjectPath path, boolean listDeep, boolean newest) throws IOException {	final byte[] bytes;	final ArrayList<FileStatus> files = new ArrayList<FileStatus>();	final Path correctSwiftPath = getCorrectSwiftPath(path);	try {	bytes = swiftRestClient.listDeepObjectsInDirectory(path, listDeep);	} catch (FileNotFoundException e) {	if (LOG.isDebugEnabled()) {	
file directory not found 

public boolean deleteObject(Path path) throws IOException {	SwiftObjectPath swiftObjectPath = toObjectPath(path);	if (!SwiftUtils.isRootDir(swiftObjectPath)) {	return swiftRestClient.delete(swiftObjectPath);	} else {	if (LOG.isDebugEnabled()) {	
not deleting root directory entry 

public void rename(Path src, Path dst) throws FileNotFoundException, SwiftOperationFailedException, IOException {	if (LOG.isDebugEnabled()) {	
mv 

SwiftObjectPath destObject = toObjectPath(dst);	if (SwiftUtils.isRootDir(srcObject)) {	throw new SwiftOperationFailedException("cannot rename root dir");	}	final SwiftFileStatus srcMetadata;	srcMetadata = getObjectMetadata(src);	SwiftFileStatus dstMetadata;	try {	dstMetadata = getObjectMetadata(dst);	} catch (FileNotFoundException e) {	
destination does not exist 

dstMetadata = getObjectMetadata(dst);	} catch (FileNotFoundException e) {	dstMetadata = null;	}	Path srcParent = src.getParent();	Path dstParent = dst.getParent();	if (dstParent != null && !dstParent.equals(srcParent)) {	try {	getObjectMetadata(dstParent);	} catch (FileNotFoundException e) {	
destination parent directory doesn t exist 

return;	}	}	} else {	destPath = toObjectPath(dst);	}	int childCount = childStats.size();	if (childCount == 0) {	copyThenDeleteObject(srcObject, destPath);	} else {	
source file appears to be partitioned copying file and deleting children 

}	} else {	destPath = toObjectPath(dst);	}	int childCount = childStats.size();	if (childCount == 0) {	copyThenDeleteObject(srcObject, destPath);	} else {	copyObject(srcObject, destPath);	for (FileStatus stat : childStats) {	
deleting partitioned file s 

Path targetPath;	if (destExists) {	targetPath = new Path(dst, src.getName());	} else {	targetPath = dst;	}	SwiftObjectPath targetObjectPath = toObjectPath(targetPath);	if (srcObject.isEqualToOrParentOf(targetObjectPath)) {	throw new SwiftOperationFailedException( "cannot move a directory under itself");	}	
mv 

String copySourceURI = copySourcePath.toUri().toString();	String copyDestSubPath = copySourceURI.substring(prefixStripCount);	Path copyDestPath = new Path(targetPath, copyDestSubPath);	if (LOG.isTraceEnabled()) {	LOG.trace("srcURI=" + srcURI + "; copySourceURI=" + copySourceURI + "; copyDestSubPath=" + copyDestSubPath + "; copyDestPath=" + copyDestPath);	}	SwiftObjectPath copyDestination = toObjectPath(copyDestPath);	try {	copyThenDeleteObject(toObjectPath(copySourcePath), copyDestination);	} catch (FileNotFoundException e) {	
skipping rename of 

try {	copyThenDeleteObject(toObjectPath(copySourcePath), copyDestination);	} catch (FileNotFoundException e) {	}	throttle();	}	if (!SwiftUtils.isRootDir(srcObject)) {	try {	copyThenDeleteObject(srcObject, targetObjectPath);	} catch (FileNotFoundException e) {	
source directory deleted during rename 

public boolean delete(Path absolutePath, boolean recursive) throws IOException {	Path swiftPath = getCorrectSwiftPath(absolutePath);	SwiftUtils.debug(LOG, "Deleting path '%s' recursive=%b", absolutePath, recursive);	boolean askForNewest = true;	SwiftFileStatus fileStatus = getObjectMetadata(swiftPath, askForNewest);	FileStatus[] statuses = listSubPaths(absolutePath, true, askForNewest);	if (statuses == null) {	
path s has no status it has gone away 

public boolean delete(Path absolutePath, boolean recursive) throws IOException {	Path swiftPath = getCorrectSwiftPath(absolutePath);	SwiftUtils.debug(LOG, "Deleting path '%s' recursive=%b", absolutePath, recursive);	boolean askForNewest = true;	SwiftFileStatus fileStatus = getObjectMetadata(swiftPath, askForNewest);	FileStatus[] statuses = listSubPaths(absolutePath, true, askForNewest);	if (statuses == null) {	return false;	}	int filecount = statuses.length;	
path s d status entries 

FileStatus[] statuses = listSubPaths(absolutePath, true, askForNewest);	if (statuses == null) {	return false;	}	int filecount = statuses.length;	if (filecount == 0) {	rmdir(absolutePath);	return true;	}	if (LOG.isDebugEnabled()) {	
s 

return false;	}	int filecount = statuses.length;	if (filecount == 0) {	rmdir(absolutePath);	return true;	}	if (LOG.isDebugEnabled()) {	}	if (filecount == 1 && swiftPath.equals(statuses[0].getPath())) {	
deleting simple file s 

rmdir(absolutePath);	return true;	}	if (LOG.isDebugEnabled()) {	}	if (filecount == 1 && swiftPath.equals(statuses[0].getPath())) {	deleteObject(absolutePath);	return true;	}	if (!fileStatus.isDir()) {	
multiple child entries but entry has data assume partitioned 

}	if (!fileStatus.isDir()) {	} else if (!recursive) {	throw new SwiftOperationFailedException("Directory " + fileStatus + " is not empty: " + SwiftUtils.fileStatsToString( statuses, "; "));	}	for (FileStatus entryStatus : statuses) {	Path entryPath = entryStatus.getPath();	try {	boolean deleted = deleteObject(entryPath);	if (!deleted) {	
failed to delete entry s continuing 

} else if (!recursive) {	throw new SwiftOperationFailedException("Directory " + fileStatus + " is not empty: " + SwiftUtils.fileStatsToString( statuses, "; "));	}	for (FileStatus entryStatus : statuses) {	Path entryPath = entryStatus.getPath();	try {	boolean deleted = deleteObject(entryPath);	if (!deleted) {	}	} catch (FileNotFoundException e) {	
path s is no longer present continuing 

for (FileStatus entryStatus : statuses) {	Path entryPath = entryStatus.getPath();	try {	boolean deleted = deleteObject(entryPath);	if (!deleted) {	}	} catch (FileNotFoundException e) {	}	throttle();	}	
deleting base entry s 

========================= hadoop sample_6193 =========================

if ((wildcard != null) && (u.getFragment() != null)) {	throw new IOException("Invalid path URI: " + p + " - cannot " + "contain both a URI fragment and a wildcard");	} else if (wildcard != null) {	name = p.getName() + Path.SEPARATOR + wildcard;	} else if (u.getFragment() != null) {	name = u.getFragment();	}	if (!StringUtils.toLowerCase(name).endsWith(".jar")) {	String old = linkLookup.put(p, name);	if ((old != null) && !name.equals(old)) {	
the same path is included more than once with different links or wildcards 

public static ClassLoader createJobClassLoader(Configuration conf) throws IOException {	ClassLoader jobClassLoader = null;	if (conf.getBoolean(MRJobConfig.MAPREDUCE_JOB_CLASSLOADER, false)) {	String appClasspath = System.getenv(Environment.APP_CLASSPATH.key());	if (appClasspath == null) {	
not creating job classloader since app classpath is not set 

public static ClassLoader createJobClassLoader(Configuration conf) throws IOException {	ClassLoader jobClassLoader = null;	if (conf.getBoolean(MRJobConfig.MAPREDUCE_JOB_CLASSLOADER, false)) {	String appClasspath = System.getenv(Environment.APP_CLASSPATH.key());	if (appClasspath == null) {	} else {	
creating job classloader 

public static void setClassLoader(ClassLoader classLoader, Configuration conf) {	if (classLoader != null) {	
setting classloader on the configuration and as the thread context classloader 

========================= hadoop sample_4454 =========================

public GetNewApplicationResponse getNewApplication( GetNewApplicationRequest request) throws YarnException, IOException {	long startTime = clock.getTime();	Map<SubClusterId, SubClusterInfo> subClustersActive = federationFacade.getSubClusters(true);	for (int i = 0; i < numSubmitRetries; ++i) {	SubClusterId subClusterId = getRandomActiveSubCluster(subClustersActive);	
getnewapplication try on subcluster 

public GetNewApplicationResponse getNewApplication( GetNewApplicationRequest request) throws YarnException, IOException {	long startTime = clock.getTime();	Map<SubClusterId, SubClusterInfo> subClustersActive = federationFacade.getSubClusters(true);	for (int i = 0; i < numSubmitRetries; ++i) {	SubClusterId subClusterId = getRandomActiveSubCluster(subClustersActive);	ApplicationClientProtocol clientRMProxy = getClientRMProxyForSubCluster(subClusterId);	GetNewApplicationResponse response = null;	try {	response = clientRMProxy.getNewApplication(request);	} catch (Exception e) {	
unable to create a new applicationid in subcluster 

public SubmitApplicationResponse submitApplication( SubmitApplicationRequest request) throws YarnException, IOException {	long startTime = clock.getTime();	if (request == null || request.getApplicationSubmissionContext() == null || request.getApplicationSubmissionContext() .getApplicationId() == null) {	routerMetrics.incrAppsFailedSubmitted();	RouterServerUtil .logAndThrowException("Missing submitApplication request or " + "applicationSubmissionContex information.", null);	}	ApplicationId applicationId = request.getApplicationSubmissionContext().getApplicationId();	List<SubClusterId> blacklist = new ArrayList<SubClusterId>();	for (int i = 0; i < numSubmitRetries; ++i) {	SubClusterId subClusterId = policyFacade.getHomeSubcluster( request.getApplicationSubmissionContext(), blacklist);	
submitapplication appid try on subcluster 

String message = "Unable to insert the ApplicationId " + applicationId + " into the FederationStateStore";	RouterServerUtil.logAndThrowException(message, e);	}	} else {	try {	federationFacade.updateApplicationHomeSubCluster(appHomeSubCluster);	} catch (YarnException e) {	String message = "Unable to update the ApplicationId " + applicationId + " into the FederationStateStore";	SubClusterId subClusterIdInStateStore = federationFacade.getApplicationHomeSubCluster(applicationId);	if (subClusterId == subClusterIdInStateStore) {	
application already submitted on subcluster 

routerMetrics.incrAppsFailedSubmitted();	RouterServerUtil.logAndThrowException(message, e);	}	}	}	ApplicationClientProtocol clientRMProxy = getClientRMProxyForSubCluster(subClusterId);	SubmitApplicationResponse response = null;	try {	response = clientRMProxy.submitApplication(request);	} catch (Exception e) {	
unable to submit the application to subcluster 

}	}	}	ApplicationClientProtocol clientRMProxy = getClientRMProxyForSubCluster(subClusterId);	SubmitApplicationResponse response = null;	try {	response = clientRMProxy.submitApplication(request);	} catch (Exception e) {	}	if (response != null) {	
application with appid submitted on 

SubClusterId subClusterId = null;	try {	subClusterId = federationFacade .getApplicationHomeSubCluster(request.getApplicationId());	} catch (YarnException e) {	routerMetrics.incrAppsFailedKilled();	RouterServerUtil.logAndThrowException("Application " + applicationId + " does not exist in FederationStateStore", e);	}	ApplicationClientProtocol clientRMProxy = getClientRMProxyForSubCluster(subClusterId);	KillApplicationResponse response = null;	try {	
forcekillapplication on subcluster 

} catch (YarnException e) {	routerMetrics.incrAppsFailedKilled();	RouterServerUtil.logAndThrowException("Application " + applicationId + " does not exist in FederationStateStore", e);	}	ApplicationClientProtocol clientRMProxy = getClientRMProxyForSubCluster(subClusterId);	KillApplicationResponse response = null;	try {	response = clientRMProxy.forceKillApplication(request);	} catch (Exception e) {	routerMetrics.incrAppsFailedKilled();	
unable to kill the application report for to subcluster 

}	ApplicationClientProtocol clientRMProxy = getClientRMProxyForSubCluster(subClusterId);	KillApplicationResponse response = null;	try {	response = clientRMProxy.forceKillApplication(request);	} catch (Exception e) {	routerMetrics.incrAppsFailedKilled();	throw e;	}	if (response == null) {	
no response when attempting to kill the application to subcluster 

} catch (YarnException e) {	routerMetrics.incrAppsFailedRetrieved();	RouterServerUtil .logAndThrowException("Application " + request.getApplicationId() + " does not exist in FederationStateStore", e);	}	ApplicationClientProtocol clientRMProxy = getClientRMProxyForSubCluster(subClusterId);	GetApplicationReportResponse response = null;	try {	response = clientRMProxy.getApplicationReport(request);	} catch (Exception e) {	routerMetrics.incrAppsFailedRetrieved();	
unable to get the application report for to subcluster 

}	ApplicationClientProtocol clientRMProxy = getClientRMProxyForSubCluster(subClusterId);	GetApplicationReportResponse response = null;	try {	response = clientRMProxy.getApplicationReport(request);	} catch (Exception e) {	routerMetrics.incrAppsFailedRetrieved();	throw e;	}	if (response == null) {	
no response when attempting to retrieve the report of the application to subcluster 

========================= hadoop sample_1986 =========================

context.setFlowName(TimelineUtils.generateDefaultFlowName( app.getName(), appId));	context.setFlowVersion(TimelineUtils.DEFAULT_FLOW_VERSION);	context.setFlowRunId(app.getStartTime());	for (String tag : app.getApplicationTags()) {	String[] parts = tag.split(":", 2);	if (parts.length != 2 || parts[1].isEmpty()) {	continue;	}	switch (parts[0].toUpperCase()) {	case TimelineUtils.FLOW_NAME_TAG_PREFIX: if (LOG.isDebugEnabled()) {	
setting the flow name 

String[] parts = tag.split(":", 2);	if (parts.length != 2 || parts[1].isEmpty()) {	continue;	}	switch (parts[0].toUpperCase()) {	case TimelineUtils.FLOW_NAME_TAG_PREFIX: if (LOG.isDebugEnabled()) {	}	context.setFlowName(parts[1]);	break;	case TimelineUtils.FLOW_VERSION_TAG_PREFIX: if (LOG.isDebugEnabled()) {	
setting the flow version 

switch (parts[0].toUpperCase()) {	case TimelineUtils.FLOW_NAME_TAG_PREFIX: if (LOG.isDebugEnabled()) {	}	context.setFlowName(parts[1]);	break;	case TimelineUtils.FLOW_VERSION_TAG_PREFIX: if (LOG.isDebugEnabled()) {	}	context.setFlowVersion(parts[1]);	break;	case TimelineUtils.FLOW_RUN_ID_TAG_PREFIX: if (LOG.isDebugEnabled()) {	
setting the flow run id 

========================= hadoop sample_1085 =========================

public void start() throws Exception {	thrs = new DummyZKFCThread[2];	thrs[0] = new DummyZKFCThread(ctx, svcs[0]);	assertEquals(0, thrs[0].zkfc.run(new String[]{"-formatZK"}));	ctx.addThread(thrs[0]);	thrs[0].start();	
waiting for to enter active state 

public void start() throws Exception {	thrs = new DummyZKFCThread[2];	thrs[0] = new DummyZKFCThread(ctx, svcs[0]);	assertEquals(0, thrs[0].zkfc.run(new String[]{"-formatZK"}));	ctx.addThread(thrs[0]);	thrs[0].start();	waitForHAState(0, HAServiceState.ACTIVE);	
adding 

public void expireActiveLockHolder(int idx) throws NoNodeException {	Stat stat = new Stat();	byte[] data = zks.getZKDatabase().getData( DummyZKFC.LOCK_ZNODE, stat, null);	assertArrayEquals(Ints.toByteArray(svcs[idx].index), data);	long session = stat.getEphemeralOwner();	
expiring svc s zookeeper session 

========================= hadoop sample_3112 =========================

}	finally {	final String jobdir = jobconf.get(JOB_DIR_LABEL);	if (jobdir != null) {	final Path jobpath = new Path(jobdir);	jobpath.getFileSystem(jobconf).delete(jobpath, true);	}	}	}	} catch(DuplicationException e) {	
input error 

final String jobdir = jobconf.get(JOB_DIR_LABEL);	if (jobdir != null) {	final Path jobpath = new Path(jobdir);	jobpath.getFileSystem(jobconf).delete(jobpath, true);	}	}	}	} catch(DuplicationException e) {	return DuplicationException.ERROR_CODE;	} catch(Exception e) {	
failed 

========================= hadoop sample_6310 =========================

try (FileInputStream in = fileIoProvider.getFileInputStream( getVolume(), file)) {	try (FileOutputStream out = fileIoProvider.getFileOutputStream( getVolume(), tmpFile)) {	IOUtils.copyBytes(in, out, 16 * 1024);	}	if (file.length() != tmpFile.length()) {	throw new IOException("Copy of file " + file + " size " + file.length()+ " into file " + tmpFile + " resulted in a size of " + tmpFile.length());	}	fileIoProvider.replaceFile(getVolume(), tmpFile, file);	} catch (IOException e) {	if (!fileIoProvider.delete(getVolume(), tmpFile)) {	
detachfile failed to delete temporary file 

public boolean breakHardLinksIfNeeded() throws IOException {	File file = getBlockFile();	if (file == null || getVolume() == null) {	throw new IOException("detachBlock:Block not found. " + this);	}	File meta = getMetaFile();	int linkCount = getHardLinkCount(file);	if (linkCount > 1) {	
breaking hardlink for x linked block 

========================= hadoop sample_7917 =========================

public void createMetaFolder() {	config.set(DistCpConstants.CONF_LABEL_META_FOLDER, "/meta");	config.set(DistCpConstants.CONF_LABEL_LISTING_FILE_PATH, "");	Path meta = new Path("/meta");	try {	cluster.getFileSystem().mkdirs(meta);	} catch (IOException e) {	
exception encountered while creating meta folder 

public void cleanupMetaFolder() {	Path meta = new Path("/meta");	try {	if (cluster.getFileSystem().exists(meta)) {	cluster.getFileSystem().delete(meta, true);	Assert.fail("Expected meta folder to be deleted");	}	} catch (IOException e) {	
exception encountered while cleaning up folder 

public void testNoCommitAction() {	TaskAttemptContext taskAttemptContext = getTaskAttemptContext(config);	JobContext jobContext = new JobContextImpl(taskAttemptContext.getConfiguration(), taskAttemptContext.getTaskAttemptID().getJobID());	try {	OutputCommitter committer = new CopyCommitter(null, taskAttemptContext);	committer.commitJob(jobContext);	Assert.assertEquals(taskAttemptContext.getStatus(), "Commit Successful");	committer.commitJob(jobContext);	Assert.assertEquals(taskAttemptContext.getStatus(), "Commit Successful");	} catch (IOException e) {	
exception encountered 

conf.set(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH, targetBase);	committer.commitJob(jobContext);	if (!checkDirectoryPermissions(fs, targetBase, sourcePerm)) {	Assert.fail("Permission don't match");	}	committer.commitJob(jobContext);	if (!checkDirectoryPermissions(fs, targetBase, sourcePerm)) {	Assert.fail("Permission don't match");	}	} catch (IOException e) {	
exception encountered while testing for preserve status 

Assert.fail("Source and target folders are not in sync");	}	committer.commitJob(jobContext);	if (!TestDistCpUtils.checkIfFoldersAreInSync(fs, targetBase, sourceBase)) {	Assert.fail("Source and target folders are not in sync");	}	if (!TestDistCpUtils.checkIfFoldersAreInSync(fs, sourceBase, targetBase)) {	Assert.fail("Source and target folders are not in sync");	}	} catch (Throwable e) {	
exception encountered while testing for delete missing 

if (!TestDistCpUtils.checkIfFoldersAreInSync(fs, targetBase, sourceBase)) {	Assert.fail("Source and target folders are not in sync");	}	Assert.assertEquals(fs.listStatus(new Path(targetBase)).length, 4);	committer.commitJob(jobContext);	if (!TestDistCpUtils.checkIfFoldersAreInSync(fs, targetBase, sourceBase)) {	Assert.fail("Source and target folders are not in sync");	}	Assert.assertEquals(fs.listStatus(new Path(targetBase)).length, 4);	} catch (IOException e) {	
exception encountered while testing for delete missing 

conf.setBoolean(DistCpConstants.CONF_LABEL_ATOMIC_COPY, true);	Assert.assertTrue(fs.exists(new Path(workPath)));	Assert.assertFalse(fs.exists(new Path(finalPath)));	committer.commitJob(jobContext);	Assert.assertFalse(fs.exists(new Path(workPath)));	Assert.assertTrue(fs.exists(new Path(finalPath)));	committer.commitJob(jobContext);	Assert.assertFalse(fs.exists(new Path(workPath)));	Assert.assertTrue(fs.exists(new Path(finalPath)));	} catch (IOException e) {	
exception encountered while testing for preserve status 

conf.set(DistCpConstants.CONF_LABEL_TARGET_FINAL_PATH, finalPath);	conf.setBoolean(DistCpConstants.CONF_LABEL_ATOMIC_COPY, true);	Assert.assertTrue(fs.exists(new Path(workPath)));	Assert.assertTrue(fs.exists(new Path(finalPath)));	try {	committer.commitJob(jobContext);	Assert.fail("Should not be able to atomic-commit to pre-existing path.");	} catch(Exception exception) {	Assert.assertTrue(fs.exists(new Path(workPath)));	Assert.assertTrue(fs.exists(new Path(finalPath)));	
atomic commit test pass 

Assert.assertTrue(fs.exists(new Path(workPath)));	Assert.assertTrue(fs.exists(new Path(finalPath)));	try {	committer.commitJob(jobContext);	Assert.fail("Should not be able to atomic-commit to pre-existing path.");	} catch(Exception exception) {	Assert.assertTrue(fs.exists(new Path(workPath)));	Assert.assertTrue(fs.exists(new Path(finalPath)));	}	} catch (IOException e) {	
exception encountered while testing for atomic commit 

========================= hadoop sample_6255 =========================

static synchronized void reset() {	if (INSTANCE != null) {	INSTANCE.queue.clear();	INSTANCE.interrupt();	try {	INSTANCE.join();	} catch (InterruptedException e) {	
failed to reset renewer 

public <T extends FileSystem & Renewable> RenewAction<T> addRenewAction(final T fs) {	synchronized (this) {	if (!isAlive()) {	start();	}	}	RenewAction<T> action = new RenewAction<T>(fs);	if (action.token != null) {	queue.add(action);	} else {	
does not have a token for renewal 

public <T extends FileSystem & Renewable> void removeRenewAction( final T fs) throws IOException {	RenewAction<T> action = new RenewAction<T>(fs);	if (queue.remove(action)) {	try {	action.cancel();	} catch (InterruptedException ie) {	
interrupted while canceling token for filesystem 

public <T extends FileSystem & Renewable> void removeRenewAction( final T fs) throws IOException {	RenewAction<T> action = new RenewAction<T>(fs);	if (queue.remove(action)) {	try {	action.cancel();	} catch (InterruptedException ie) {	if (LOG.isDebugEnabled()) {	
exception in removerenewaction 

========================= hadoop sample_4268 =========================

public FileOutputCommitter(Path outputPath, JobContext context) throws IOException {	Configuration conf = context.getConfiguration();	algorithmVersion = conf.getInt(FILEOUTPUTCOMMITTER_ALGORITHM_VERSION, FILEOUTPUTCOMMITTER_ALGORITHM_VERSION_DEFAULT);	
file output committer algorithm version is 

public FileOutputCommitter(Path outputPath, JobContext context) throws IOException {	Configuration conf = context.getConfiguration();	algorithmVersion = conf.getInt(FILEOUTPUTCOMMITTER_ALGORITHM_VERSION, FILEOUTPUTCOMMITTER_ALGORITHM_VERSION_DEFAULT);	if (algorithmVersion != 1 && algorithmVersion != 2) {	throw new IOException("Only 1 or 2 algorithm version is supported");	}	skipCleanup = conf.getBoolean( FILEOUTPUTCOMMITTER_CLEANUP_SKIPPED, FILEOUTPUTCOMMITTER_CLEANUP_SKIPPED_DEFAULT);	ignoreCleanupFailures = conf.getBoolean( FILEOUTPUTCOMMITTER_CLEANUP_FAILURES_IGNORED, FILEOUTPUTCOMMITTER_CLEANUP_FAILURES_IGNORED_DEFAULT);	
fileoutputcommitter skip cleanup temporary folders under output directory ignore cleanup failures 

public void setupJob(JobContext context) throws IOException {	if (hasOutputPath()) {	Path jobAttemptPath = getJobAttemptPath(context);	FileSystem fs = jobAttemptPath.getFileSystem( context.getConfiguration());	if (!fs.mkdirs(jobAttemptPath)) {	
mkdirs failed to create 

public void setupJob(JobContext context) throws IOException {	if (hasOutputPath()) {	Path jobAttemptPath = getJobAttemptPath(context);	FileSystem fs = jobAttemptPath.getFileSystem( context.getConfiguration());	if (!fs.mkdirs(jobAttemptPath)) {	}	} else {	
output path is null in setupjob 

int attempt = 0;	boolean jobCommitNotFinished = true;	while (jobCommitNotFinished) {	try {	commitJobInternal(context);	jobCommitNotFinished = false;	} catch (Exception e) {	if (++attempt >= maxAttemptsOnFailure) {	throw e;	} else {	
exception get thrown in job commit retry time 

protected void commitJobInternal(JobContext context) throws IOException {	if (hasOutputPath()) {	Path finalOutput = getOutputPath();	FileSystem fs = finalOutput.getFileSystem(context.getConfiguration());	if (algorithmVersion == 1) {	for (FileStatus stat: getAllCommittedTaskPaths(context)) {	mergePaths(fs, stat, finalOutput);	}	}	if (skipCleanup) {	
skip cleanup the temporary folders under job s output directory in commitjob 

for (FileStatus stat: getAllCommittedTaskPaths(context)) {	mergePaths(fs, stat, finalOutput);	}	}	if (skipCleanup) {	} else {	try {	cleanupJob(context);	} catch (IOException e) {	if (ignoreCleanupFailures) {	
error in cleanup job manually cleanup is needed 

}	if (context.getConfiguration().getBoolean( SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, true)) {	Path markerPath = new Path(outputPath, SUCCEEDED_FILE_NAME);	if (isCommitJobRepeatable(context)) {	fs.create(markerPath, true).close();	} else {	fs.create(markerPath).close();	}	}	} else {	
output path is null in commitjob 

private void mergePaths(FileSystem fs, final FileStatus from, final Path to) throws IOException {	if (LOG.isDebugEnabled()) {	
merging data from to 

Path pendingJobAttemptsPath = getPendingJobAttemptsPath();	FileSystem fs = pendingJobAttemptsPath .getFileSystem(context.getConfiguration());	try {	fs.delete(pendingJobAttemptsPath, true);	} catch (FileNotFoundException e) {	if (!isCommitJobRepeatable(context)) {	throw e;	}	}	} else {	
output path is null in cleanupjob 

if (algorithmVersion == 1) {	Path committedTaskPath = getCommittedTaskPath(context);	if (fs.exists(committedTaskPath)) {	if (!fs.delete(committedTaskPath, true)) {	throw new IOException("Could not delete " + committedTaskPath);	}	}	if (!fs.rename(taskAttemptPath, committedTaskPath)) {	throw new IOException("Could not rename " + taskAttemptPath + " to " + committedTaskPath);	}	
saved output of task to 

if (fs.exists(committedTaskPath)) {	if (!fs.delete(committedTaskPath, true)) {	throw new IOException("Could not delete " + committedTaskPath);	}	}	if (!fs.rename(taskAttemptPath, committedTaskPath)) {	throw new IOException("Could not rename " + taskAttemptPath + " to " + committedTaskPath);	}	} else {	mergePaths(fs, taskAttemptDirStatus, outputPath);	
saved output of task to 

throw new IOException("Could not delete " + committedTaskPath);	}	}	if (!fs.rename(taskAttemptPath, committedTaskPath)) {	throw new IOException("Could not rename " + taskAttemptPath + " to " + committedTaskPath);	}	} else {	mergePaths(fs, taskAttemptDirStatus, outputPath);	}	} else {	
no output found for 

}	if (!fs.rename(taskAttemptPath, committedTaskPath)) {	throw new IOException("Could not rename " + taskAttemptPath + " to " + committedTaskPath);	}	} else {	mergePaths(fs, taskAttemptDirStatus, outputPath);	}	} else {	}	} else {	
output path is null in committask 

public void abortTask(TaskAttemptContext context, Path taskAttemptPath) throws IOException {	if (hasOutputPath()) {	context.progress();	if(taskAttemptPath == null) {	taskAttemptPath = getTaskAttemptPath(context);	}	FileSystem fs = taskAttemptPath.getFileSystem(context.getConfiguration());	if(!fs.delete(taskAttemptPath, true)) {	
could not delete 

public void abortTask(TaskAttemptContext context, Path taskAttemptPath) throws IOException {	if (hasOutputPath()) {	context.progress();	if(taskAttemptPath == null) {	taskAttemptPath = getTaskAttemptPath(context);	}	FileSystem fs = taskAttemptPath.getFileSystem(context.getConfiguration());	if(!fs.delete(taskAttemptPath, true)) {	}	} else {	
output path is null in aborttask 

if(hasOutputPath()) {	context.progress();	TaskAttemptID attemptId = context.getTaskAttemptID();	int previousAttempt = getAppAttemptId(context) - 1;	if (previousAttempt < 0) {	throw new IOException ("Cannot recover task output for first attempt...");	}	Path previousCommittedTaskPath = getCommittedTaskPath( previousAttempt, context);	FileSystem fs = previousCommittedTaskPath.getFileSystem(context.getConfiguration());	if (LOG.isDebugEnabled()) {	
trying to recover task from 

Path committedTaskPath = getCommittedTaskPath(context);	if (!fs.delete(committedTaskPath, true) && fs.exists(committedTaskPath)) {	throw new IOException("Could not delete " + committedTaskPath);	}	Path committedParent = committedTaskPath.getParent();	fs.mkdirs(committedParent);	if (!fs.rename(previousCommittedTaskPath, committedTaskPath)) {	throw new IOException("Could not rename " + previousCommittedTaskPath + " to " + committedTaskPath);	}	} else {	
had no output to recover 

Path committedParent = committedTaskPath.getParent();	fs.mkdirs(committedParent);	if (!fs.rename(previousCommittedTaskPath, committedTaskPath)) {	throw new IOException("Could not rename " + previousCommittedTaskPath + " to " + committedTaskPath);	}	} else {	}	} else {	try {	FileStatus from = fs.getFileStatus(previousCommittedTaskPath);	
recovering task for upgrading scenario moving files from to 

throw new IOException("Could not rename " + previousCommittedTaskPath + " to " + committedTaskPath);	}	} else {	}	} else {	try {	FileStatus from = fs.getFileStatus(previousCommittedTaskPath);	mergePaths(fs, from, outputPath);	} catch (FileNotFoundException ignored) {	}	
done recovering task 

} else {	}	} else {	try {	FileStatus from = fs.getFileStatus(previousCommittedTaskPath);	mergePaths(fs, from, outputPath);	} catch (FileNotFoundException ignored) {	}	}	} else {	
output path is null in recovertask 

========================= hadoop sample_5029 =========================

public synchronized void launch(ContainerRemoteLaunchEvent event) {	
launching 

StartContainersRequest requestList = StartContainersRequest.newInstance(list);	StartContainersResponse response = proxy.getContainerManagementProtocol().startContainers(requestList);	if (response.getFailedRequests() != null && response.getFailedRequests().containsKey(containerID)) {	throw response.getFailedRequests().get(containerID).deSerialize();	}	ByteBuffer portInfo = response.getAllServicesMetaData().get( ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID);	int port = -1;	if(portInfo != null) {	port = ShuffleHandler.deserializeMetaData(portInfo);	}	
shuffle port returned by containermanager for 

public synchronized void kill(boolean dumpThreads) {	if(this.state == ContainerState.PREP) {	this.state = ContainerState.KILLED_BEFORE_LAUNCH;	} else if (!isCompletelyDone()) {	
killing 

protected void serviceInit(Configuration conf) throws Exception {	this.limitOnPoolSize = conf.getInt( MRJobConfig.MR_AM_CONTAINERLAUNCHER_THREAD_COUNT_LIMIT, MRJobConfig.DEFAULT_MR_AM_CONTAINERLAUNCHER_THREAD_COUNT_LIMIT);	
upper limit on the thread pool size is 

protected void serviceInit(Configuration conf) throws Exception {	this.limitOnPoolSize = conf.getInt( MRJobConfig.MR_AM_CONTAINERLAUNCHER_THREAD_COUNT_LIMIT, MRJobConfig.DEFAULT_MR_AM_CONTAINERLAUNCHER_THREAD_COUNT_LIMIT);	this.initialPoolSize = conf.getInt( MRJobConfig.MR_AM_CONTAINERLAUNCHER_THREADPOOL_INITIAL_SIZE, MRJobConfig.DEFAULT_MR_AM_CONTAINERLAUNCHER_THREADPOOL_INITIAL_SIZE);	
the thread pool initial size is 

launcherPool = new HadoopThreadPoolExecutor(initialPoolSize, Integer.MAX_VALUE, 1, TimeUnit.HOURS, new LinkedBlockingQueue<Runnable>(), tf);	eventHandlingThread = new Thread() {	public void run() {	ContainerLauncherEvent event = null;	Set<String> allNodes = new HashSet<String>();	while (!stopped.get() && !Thread.currentThread().isInterrupted()) {	try {	event = eventQueue.take();	} catch (InterruptedException e) {	if (!stopped.get()) {	
returning interrupted 

}	return;	}	allNodes.add(event.getContainerMgrAddress());	int poolSize = launcherPool.getCorePoolSize();	if (poolSize != limitOnPoolSize) {	int numNodes = allNodes.size();	int idealPoolSize = Math.min(limitOnPoolSize, numNodes);	if (poolSize < idealPoolSize) {	int newPoolSize = Math.min(limitOnPoolSize, idealPoolSize + initialPoolSize);	
setting containerlauncher pool size to as number of nodes to talk to is 

public void run() {	
processing the event 

========================= hadoop sample_5183 =========================

public void write(DataOutputStream out, Set<File> pendingUploadFiles) throws IOException {	List<File> fileList = new ArrayList<File>(pendingUploadFiles);	Collections.sort(fileList);	for (File logFile : fileList) {	if (logFile.isDirectory()) {	
is a directory ignore it 

out.write(buf, 0, len);	bytesLeft-=len;	}	else {	out.write(buf, 0, (int)bytesLeft);	break;	}	}	long newLength = logFile.length();	if(fileLength < newLength) {	
aggregated logs truncated by approximately bytes 

public void close() {	try {	if (writer != null) {	writer.close();	}	} catch (Exception e) {	
exception closing writer 

========================= hadoop sample_2352 =========================

public static Map<String, RegistryPathStatus> statChildren( RegistryOperations registryOperations, String path) throws PathNotFoundException, InvalidPathnameException, IOException {	List<String> childNames = registryOperations.list(path);	Map<String, RegistryPathStatus> results = new HashMap<String, RegistryPathStatus>();	for (String childName : childNames) {	String child = join(path, childName);	try {	RegistryPathStatus stat = registryOperations.stat(child);	results.put(childName, stat);	} catch (PathNotFoundException pnfe) {	if (LOG.isDebugEnabled()) {	
stat failed on moved 

public static Map<String, ServiceRecord> extractServiceRecords( RegistryOperations operations, String parentpath, Collection<RegistryPathStatus> stats) throws IOException {	Map<String, ServiceRecord> results = new HashMap<String, ServiceRecord>(stats.size());	for (RegistryPathStatus stat : stats) {	if (stat.size > ServiceRecord.RECORD_TYPE.length()) {	String path = join(parentpath, stat.path);	try {	ServiceRecord serviceRecord = operations.resolve(path);	results.put(path, serviceRecord);	} catch (EOFException ignored) {	if (LOG.isDebugEnabled()) {	
data too short for 

if (stat.size > ServiceRecord.RECORD_TYPE.length()) {	String path = join(parentpath, stat.path);	try {	ServiceRecord serviceRecord = operations.resolve(path);	results.put(path, serviceRecord);	} catch (EOFException ignored) {	if (LOG.isDebugEnabled()) {	}	} catch (InvalidRecordException record) {	if (LOG.isDebugEnabled()) {	
invalid record at 

ServiceRecord serviceRecord = operations.resolve(path);	results.put(path, serviceRecord);	} catch (EOFException ignored) {	if (LOG.isDebugEnabled()) {	}	} catch (InvalidRecordException record) {	if (LOG.isDebugEnabled()) {	}	} catch (NoRecordException record) {	if (LOG.isDebugEnabled()) {	
no record at 

========================= hadoop sample_2702 =========================

public void init(Clock clock, ResourceScheduler sched, Collection<Plan> plans) {	super.init(clock, sched, plans);	fs = (FairScheduler)sched;	
initializing plan follower policy 

protected Queue getPlanQueue(String planQueueName) {	Queue planQueue = fs.getQueueManager().getParentQueue(planQueueName, false);	if (planQueue == null) {	
the queue cannot be found or is not a ParentQueue 

========================= hadoop sample_1084 =========================

try {	long refreshInterval = spaceUsed.refreshInterval;	if (spaceUsed.jitter > 0) {	long jitter = spaceUsed.jitter;	refreshInterval += ThreadLocalRandom.current() .nextLong(-jitter, jitter);	}	refreshInterval = Math.max(refreshInterval, 1);	Thread.sleep(refreshInterval);	spaceUsed.refresh();	} catch (InterruptedException e) {	
thread interrupted waiting to refresh disk information 

========================= hadoop sample_4269 =========================

public static void setup() {	try {	fs = FileSystem.get(getConf());	listFile = new Path("target/tmp/listing").makeQualified(fs.getUri(), fs.getWorkingDirectory());	target = new Path("target/tmp/target").makeQualified(fs.getUri(), fs.getWorkingDirectory());	root = new Path("target/tmp").makeQualified(fs.getUri(), fs.getWorkingDirectory()).toString();	TestDistCpUtils.delete(fs, root);	} catch (IOException e) {	
exception encountered 

private void caseSingleFileMissingTarget(boolean sync) {	try {	addEntries(listFile, "singlefile1/file1");	createFiles("singlefile1/file1");	runTest(listFile, target, false, sync);	checkResult(target, 1);	} catch (IOException e) {	
exception encountered while testing distcp 

private void caseSingleFileTargetFile(boolean sync) {	try {	addEntries(listFile, "singlefile1/file1");	createFiles("singlefile1/file1", "target");	runTest(listFile, target, false, sync);	checkResult(target, 1);	} catch (IOException e) {	
exception encountered while testing distcp 

private void caseSingleFileTargetDir(boolean sync) {	try {	addEntries(listFile, "singlefile2/file2");	createFiles("singlefile2/file2");	mkdirs(target.toString());	runTest(listFile, target, true, sync);	checkResult(target, 1, "file2");	} catch (IOException e) {	
exception encountered while testing distcp 

private void caseSingleDirTargetMissing(boolean sync) {	try {	addEntries(listFile, "singledir");	mkdirs(root + "/singledir/dir1");	runTest(listFile, target, false, sync);	checkResult(target, 1, "dir1");	} catch (IOException e) {	
exception encountered while testing distcp 

public void testSingleDirTargetPresent() {	try {	addEntries(listFile, "singledir");	mkdirs(root + "/singledir/dir1");	mkdirs(target.toString());	runTest(listFile, target, true, false);	checkResult(target, 1, "singledir/dir1");	} catch (IOException e) {	
exception encountered while testing distcp 

public void testUpdateSingleDirTargetPresent() {	try {	addEntries(listFile, "Usingledir");	mkdirs(root + "/Usingledir/Udir1");	mkdirs(target.toString());	runTest(listFile, target, true, true);	checkResult(target, 1, "Udir1");	} catch (IOException e) {	
exception encountered while testing distcp 

private void caseMultiFileTargetPresent(boolean sync) {	try {	addEntries(listFile, "multifile/file3", "multifile/file4", "multifile/file5");	createFiles("multifile/file3", "multifile/file4", "multifile/file5");	mkdirs(target.toString());	runTest(listFile, target, true, sync);	checkResult(target, 3, "file3", "file4", "file5");	} catch (IOException e) {	
exception encountered while testing distcp 

private void caseMultiFileTargetMissing(boolean sync) {	try {	addEntries(listFile, "multifile/file3", "multifile/file4", "multifile/file5");	createFiles("multifile/file3", "multifile/file4", "multifile/file5");	runTest(listFile, target, false, sync);	checkResult(target, 3, "file3", "file4", "file5");	} catch (IOException e) {	
exception encountered while testing distcp 

public void testMultiDirTargetPresent() {	try {	addEntries(listFile, "multifile", "singledir");	createFiles("multifile/file3", "multifile/file4", "multifile/file5");	mkdirs(target.toString(), root + "/singledir/dir1");	runTest(listFile, target, true, false);	checkResult(target, 2, "multifile/file3", "multifile/file4", "multifile/file5", "singledir/dir1");	} catch (IOException e) {	
exception encountered while testing distcp 

public void testUpdateMultiDirTargetPresent() {	try {	addEntries(listFile, "Umultifile", "Usingledir");	createFiles("Umultifile/Ufile3", "Umultifile/Ufile4", "Umultifile/Ufile5");	mkdirs(target.toString(), root + "/Usingledir/Udir1");	runTest(listFile, target, true, true);	checkResult(target, 4, "Ufile3", "Ufile4", "Ufile5", "Udir1");	} catch (IOException e) {	
exception encountered while testing distcp 

public void testMultiDirTargetMissing() {	try {	addEntries(listFile, "multifile", "singledir");	createFiles("multifile/file3", "multifile/file4", "multifile/file5");	mkdirs(root + "/singledir/dir1");	runTest(listFile, target, false, false);	checkResult(target, 2, "multifile/file3", "multifile/file4", "multifile/file5", "singledir/dir1");	} catch (IOException e) {	
exception encountered while testing distcp 

public void testUpdateMultiDirTargetMissing() {	try {	addEntries(listFile, "multifile", "singledir");	createFiles("multifile/file3", "multifile/file4", "multifile/file5");	mkdirs(root + "/singledir/dir1");	runTest(listFile, target, false, true);	checkResult(target, 4, "file3", "file4", "file5", "dir1");	} catch (IOException e) {	
exception encountered while testing distcp 

public void testDeleteMissingInDestination() {	try {	addEntries(listFile, "srcdir");	createFiles("srcdir/file1", "dstdir/file1", "dstdir/file2");	Path target = new Path(root + "/dstdir");	runTest(listFile, target, false, true, true, false);	checkResult(target, 1, "file1");	} catch (IOException e) {	
exception encountered while running distcp 

createWithContents("dstdir/file1", contents2);	Path target = new Path(root + "/dstdir");	runTest(listFile, target, false, false, false, true);	checkResult(target, 1, "file1");	FSDataInputStream is = fs.open(new Path(root + "/dstdir/file1"));	byte[] dstContents = new byte[contents1.length];	is.readFully(dstContents);	is.close();	Assert.assertArrayEquals(contents1, dstContents);	} catch (IOException e) {	
exception encountered while running distcp 

public void testGlobTargetMissingSingleLevel() {	try {	Path listFile = new Path("target/tmp1/listing").makeQualified(fs.getUri(), fs.getWorkingDirectory());	addEntries(listFile, "*");	createFiles("multifile/file3", "multifile/file4", "multifile/file5");	createFiles("singledir/dir2/file6");	runTest(listFile, target, false, false);	checkResult(target, 2, "multifile/file3", "multifile/file4", "multifile/file5", "singledir/dir2/file6");	} catch (IOException e) {	
exception encountered while testing distcp 

public void testUpdateGlobTargetMissingSingleLevel() {	try {	Path listFile = new Path("target/tmp1/listing").makeQualified(fs.getUri(), fs.getWorkingDirectory());	addEntries(listFile, "*");	createFiles("multifile/file3", "multifile/file4", "multifile/file5");	createFiles("singledir/dir2/file6");	runTest(listFile, target, false, true);	checkResult(target, 4, "file3", "file4", "file5", "dir2/file6");	} catch (IOException e) {	
exception encountered while running distcp 

public void testGlobTargetMissingMultiLevel() {	try {	Path listFile = new Path("target/tmp1/listing").makeQualified(fs.getUri(), fs.getWorkingDirectory());	addEntries(listFile, "*/*");	createFiles("multifile/file3", "multifile/file4", "multifile/file5");	createFiles("singledir1/dir3/file7", "singledir1/dir3/file8", "singledir1/dir3/file9");	runTest(listFile, target, false, false);	checkResult(target, 4, "file3", "file4", "file5", "dir3/file7", "dir3/file8", "dir3/file9");	} catch (IOException e) {	
exception encountered while running distcp 

public void testUpdateGlobTargetMissingMultiLevel() {	try {	Path listFile = new Path("target/tmp1/listing").makeQualified(fs.getUri(), fs.getWorkingDirectory());	addEntries(listFile, "*/*");	createFiles("multifile/file3", "multifile/file4", "multifile/file5");	createFiles("singledir1/dir3/file7", "singledir1/dir3/file8", "singledir1/dir3/file9");	runTest(listFile, target, false, true);	checkResult(target, 6, "file3", "file4", "file5", "file7", "file8", "file9");	} catch (IOException e) {	
exception encountered while running distcp 

DistCpOptions options = new DistCpOptions(sources, target);	Configuration conf = getConf();	Path stagingDir = JobSubmissionFiles.getStagingDir( new Cluster(conf), conf);	stagingDir.getFileSystem(conf).mkdirs(stagingDir);	try {	new DistCp(conf, options).execute();	} catch (Throwable t) {	Assert.assertEquals(stagingDir.getFileSystem(conf). listStatus(stagingDir).length, 0);	}	} catch (Exception e) {	
exception encountered 

private void runTest(Path listFile, Path target, boolean targetExists, boolean sync, boolean delete, boolean overwrite) throws IOException {	DistCpOptions options = new DistCpOptions(listFile, target);	options.setSyncFolder(sync);	options.setDeleteMissing(delete);	options.setOverwrite(overwrite);	options.setTargetPathExists(targetExists);	options.setNumListstatusThreads(numListstatusThreads);	try {	new DistCp(getConf(), options).execute();	} catch (Exception e) {	
exception encountered 

========================= hadoop sample_6254 =========================

protected DefaultRequestInterceptorREST getInterceptorForSubCluster( SubClusterId subClusterId) {	if (interceptors.containsKey(subClusterId)) {	return interceptors.get(subClusterId);	} else {	
the interceptor for subcluster does not exist in the cache 

}	List<SubClusterId> blacklist = new ArrayList<SubClusterId>();	for (int i = 0; i < numSubmitRetries; ++i) {	SubClusterId subClusterId;	try {	subClusterId = getRandomActiveSubCluster(subClustersActive, blacklist);	} catch (YarnException e) {	routerMetrics.incrAppsFailedCreated();	return Response.status(Status.SERVICE_UNAVAILABLE) .entity(e.getLocalizedMessage()).build();	}	
getnewapplication try on subcluster 

subClusterId = getRandomActiveSubCluster(subClustersActive, blacklist);	} catch (YarnException e) {	routerMetrics.incrAppsFailedCreated();	return Response.status(Status.SERVICE_UNAVAILABLE) .entity(e.getLocalizedMessage()).build();	}	DefaultRequestInterceptorREST interceptor = getOrCreateInterceptorForSubCluster(subClusterId, subClustersActive.get(subClusterId).getRMWebServiceAddress());	Response response = null;	try {	response = interceptor.createNewApplication(hsr);	} catch (Exception e) {	
unable to create a new applicationid in subcluster 

List<SubClusterId> blacklist = new ArrayList<SubClusterId>();	for (int i = 0; i < numSubmitRetries; ++i) {	ApplicationSubmissionContext context = RMWebAppUtil.createAppSubmissionContext(newApp, this.getConf());	SubClusterId subClusterId = null;	try {	subClusterId = policyFacade.getHomeSubcluster(context, blacklist);	} catch (YarnException e) {	routerMetrics.incrAppsFailedSubmitted();	return Response .status(Status.SERVICE_UNAVAILABLE) .entity(e.getLocalizedMessage()) .build();	}	
submitapplication appid try on subcluster 

} catch (YarnException e) {	String errMsg = "Unable to update the ApplicationId " + applicationId + " into the FederationStateStore";	SubClusterId subClusterIdInStateStore;	try {	subClusterIdInStateStore = federationFacade.getApplicationHomeSubCluster(applicationId);	} catch (YarnException e1) {	routerMetrics.incrAppsFailedSubmitted();	return Response .status(Status.SERVICE_UNAVAILABLE) .entity(e1.getLocalizedMessage()) .build();	}	if (subClusterId == subClusterIdInStateStore) {	
application already submitted on subcluster 

try {	subClusterInfo = federationFacade.getSubCluster(subClusterId);	} catch (YarnException e) {	routerMetrics.incrAppsFailedSubmitted();	return Response .status(Status.SERVICE_UNAVAILABLE) .entity(e.getLocalizedMessage()) .build();	}	Response response = null;	try {	response = getOrCreateInterceptorForSubCluster(subClusterId, subClusterInfo.getRMWebServiceAddress()).submitApplication(newApp, hsr);	} catch (Exception e) {	
unable to submit the application to subcluster 

} catch (YarnException e) {	routerMetrics.incrAppsFailedSubmitted();	return Response .status(Status.SERVICE_UNAVAILABLE) .entity(e.getLocalizedMessage()) .build();	}	Response response = null;	try {	response = getOrCreateInterceptorForSubCluster(subClusterId, subClusterInfo.getRMWebServiceAddress()).submitApplication(newApp, hsr);	} catch (Exception e) {	}	if (response != null && response.getStatus() == HttpServletResponse.SC_ACCEPTED) {	
application with appid submitted on 

}	CompletionService<AppsInfo> compSvc = new ExecutorCompletionService<>(this.threadpool);	final HttpServletRequest hsrCopy = clone(hsr);	for (final SubClusterInfo info : subClustersActive.values()) {	compSvc.submit(new Callable<AppsInfo>() {	public AppsInfo call() {	DefaultRequestInterceptorREST interceptor = getOrCreateInterceptorForSubCluster( info.getSubClusterId(), info.getRMWebServiceAddress());	AppsInfo rmApps = interceptor.getApps(hsrCopy, stateQuery, statesQuery, finalStatusQuery, userQuery, queueQuery, count, startedBegin, startedEnd, finishBegin, finishEnd, applicationTypes, applicationTags, unselectedFields);	if (rmApps == null) {	routerMetrics.incrMultipleAppsFailedRetrieved();	
subcluster failed to return appreport 

try {	Future<AppsInfo> future = compSvc.take();	AppsInfo appsResponse = future.get();	long stopTime = clock.getTime();	routerMetrics.succeededMultipleAppsRetrieved(stopTime - startTime);	if (appsResponse != null) {	apps.addAll(appsResponse.getApps());	}	} catch (Throwable e) {	routerMetrics.incrMultipleAppsFailedRetrieved();	
failed to get application report 

}	CompletionService<NodeInfo> compSvc = new ExecutorCompletionService<NodeInfo>(this.threadpool);	for (final SubClusterInfo info : subClustersActive.values()) {	compSvc.submit(new Callable<NodeInfo>() {	public NodeInfo call() {	DefaultRequestInterceptorREST interceptor = getOrCreateInterceptorForSubCluster( info.getSubClusterId(), info.getRMWebServiceAddress());	try {	NodeInfo nodeInfo = interceptor.getNode(nodeId);	return nodeInfo;	} catch (Exception e) {	
subcluster failed to return nodeinfo 

for (int i = 0; i < subClustersActive.size(); i++) {	try {	Future<NodeInfo> future = compSvc.take();	NodeInfo nodeResponse = future.get();	if (nodeResponse != null) {	if (nodeInfo == null || nodeInfo.getLastHealthUpdate() < nodeResponse.getLastHealthUpdate()) {	nodeInfo = nodeResponse;	}	}	} catch (Throwable e) {	
failed to get node report 

public NodesInfo getNodes(final String states) {	NodesInfo nodes = new NodesInfo();	Map<SubClusterId, SubClusterInfo> subClustersActive = null;	try {	subClustersActive = federationFacade.getSubClusters(true);	} catch (YarnException e) {	
cannot get nodes 

}	CompletionService<NodesInfo> compSvc = new ExecutorCompletionService<NodesInfo>(this.threadpool);	for (final SubClusterInfo info : subClustersActive.values()) {	compSvc.submit(new Callable<NodesInfo>() {	public NodesInfo call() {	DefaultRequestInterceptorREST interceptor = getOrCreateInterceptorForSubCluster( info.getSubClusterId(), info.getRMWebServiceAddress());	try {	NodesInfo nodesInfo = interceptor.getNodes(states);	return nodesInfo;	} catch (Exception e) {	
subcluster failed to return nodesinfo 

});	}	for (int i = 0; i < subClustersActive.size(); i++) {	try {	Future<NodesInfo> future = compSvc.take();	NodesInfo nodesResponse = future.get();	if (nodesResponse != null) {	nodes.addAll(nodesResponse.getNodes());	}	} catch (Throwable e) {	
failed to get nodes report 

}	CompletionService<ClusterMetricsInfo> compSvc = new ExecutorCompletionService<ClusterMetricsInfo>(this.threadpool);	for (final SubClusterInfo info : subClustersActive.values()) {	compSvc.submit(new Callable<ClusterMetricsInfo>() {	public ClusterMetricsInfo call() {	DefaultRequestInterceptorREST interceptor = getOrCreateInterceptorForSubCluster( info.getSubClusterId(), info.getRMWebServiceAddress());	try {	ClusterMetricsInfo metrics = interceptor.getClusterMetricsInfo();	return metrics;	} catch (Exception e) {	
subcluster failed to return cluster metrics 

});	}	for (int i = 0; i < subClustersActive.size(); i++) {	try {	Future<ClusterMetricsInfo> future = compSvc.take();	ClusterMetricsInfo metricsResponse = future.get();	if (metricsResponse != null) {	RouterWebServiceUtil.mergeMetrics(metrics, metricsResponse);	}	} catch (Throwable e) {	
failed to get nodes report 

========================= hadoop sample_1968 =========================

public static SubClusterPolicyConfiguration loadPolicyConfiguration( String queue, Configuration conf, FederationStateStoreFacade federationFacade) {	SubClusterPolicyConfiguration configuration = null;	if (queue != null) {	try {	configuration = federationFacade.getPolicyConfiguration(queue);	} catch (YarnException e) {	
failed to get policy from federationfacade with queue 

public static SubClusterPolicyConfiguration loadPolicyConfiguration( String queue, Configuration conf, FederationStateStoreFacade federationFacade) {	SubClusterPolicyConfiguration configuration = null;	if (queue != null) {	try {	configuration = federationFacade.getPolicyConfiguration(queue);	} catch (YarnException e) {	}	}	if (configuration == null) {	
no policy configured for queue in statestore fallback to default queue 

try {	configuration = federationFacade.getPolicyConfiguration(queue);	} catch (YarnException e) {	}	}	if (configuration == null) {	queue = YarnConfiguration.DEFAULT_FEDERATION_POLICY_KEY;	try {	configuration = federationFacade.getPolicyConfiguration(queue);	} catch (YarnException e) {	
no fallback behavior defined in store defaulting to xml configuration fallback behavior 

}	}	if (configuration == null) {	queue = YarnConfiguration.DEFAULT_FEDERATION_POLICY_KEY;	try {	configuration = federationFacade.getPolicyConfiguration(queue);	} catch (YarnException e) {	}	}	if (configuration == null) {	
no policy configured for default queue in statestore fallback to local config 

public static FederationAMRMProxyPolicy loadAMRMPolicy(String queue, FederationAMRMProxyPolicy oldPolicy, Configuration conf, FederationStateStoreFacade federationFacade, SubClusterId homeSubClusterId) throws FederationPolicyInitializationException {	SubClusterPolicyConfiguration configuration = loadPolicyConfiguration(queue, conf, federationFacade);	FederationPolicyInitializationContext context = new FederationPolicyInitializationContext(configuration, federationFacade.getSubClusterResolver(), federationFacade, homeSubClusterId);	
creating policy manager of type 

========================= hadoop sample_1342 =========================

throw new ForbiddenException(msg);	}	try {	ApplicationId appID = parseApplicationId(appId);	if (appID == null) {	return Response.status(Response.Status.BAD_REQUEST).build();	}	NodeTimelineCollectorManager collectorManager = (NodeTimelineCollectorManager) context.getAttribute( NodeTimelineCollectorManager.COLLECTOR_MANAGER_ATTR_KEY);	TimelineCollector collector = collectorManager.get(appID);	if (collector == null) {	
application is not found 

throw new NotFoundException();	}	boolean isAsync = async != null && async.trim().equalsIgnoreCase("true");	if (isAsync) {	collector.putEntitiesAsync( processTimelineEntities(entities), callerUgi);	} else {	collector.putEntities(processTimelineEntities(entities), callerUgi);	}	return Response.ok().build();	} catch (Exception e) {	
error putting entities 

private static ApplicationId parseApplicationId(String appId) {	try {	if (appId != null) {	return ApplicationId.fromString(appId.trim());	} else {	return null;	}	} catch (Exception e) {	
invalid application id 

========================= hadoop sample_360 =========================

private Job runJobInBackGround(Configuration conf) throws Exception {	String input = "hello1\nhello2\nhello3\n";	Job job = MapReduceTestUtil.createJob(conf, getInputDir(), getOutputDir(), 1, 1, input);	job.setJobName("mr");	job.setPriority(JobPriority.NORMAL);	job.submit();	int i = 0;	while (i++ < 200 && job.getJobID() == null) {	
waiting for jobid 

========================= hadoop sample_5614 =========================

public void testAppendIsDenied() throws IOException {	getClusterBuilder().build();	final String METHOD_NAME = GenericTestUtils.getMethodName();	Path path = new Path("/" + METHOD_NAME + ".dat");	makeTestFile(path, BLOCK_SIZE, true);	try {	client.append(path.toString(), BUFFER_LENGTH, EnumSet.of(CreateFlag.APPEND), null, null).close();	fail("Append to LazyPersist file did not fail as expected");	} catch (Throwable t) {	
got expected exception 

public void testTruncateIsDenied() throws IOException {	getClusterBuilder().build();	final String METHOD_NAME = GenericTestUtils.getMethodName();	Path path = new Path("/" + METHOD_NAME + ".dat");	makeTestFile(path, BLOCK_SIZE, true);	try {	client.truncate(path.toString(), BLOCK_SIZE/2);	fail("Truncate to LazyPersist file did not fail as expected");	} catch (Throwable t) {	
got expected exception 

final int NUM_TASKS = 5;	makeRandomTestFile(path1, BLOCK_SIZE, true, SEED);	ensureFileReplicasOnStorageType(path1, RAM_DISK);	final CountDownLatch latch = new CountDownLatch(NUM_TASKS);	final AtomicBoolean testFailed = new AtomicBoolean(false);	Runnable readerRunnable = new Runnable() {	public void run() {	try {	Assert.assertTrue(verifyReadRandomFile(path1, BLOCK_SIZE, SEED));	} catch (Throwable e) {	
readerrunnable error 

public void run() {	System.out.println("Writer " + id + " starting... ");	int i = 0;	try {	for (i = 0; i < paths.length; i++) {	makeRandomTestFile(paths[i], BLOCK_SIZE, true, seed);	}	} catch (IOException e) {	bFail.set(true);	
writer exception writer id testfile 

========================= hadoop sample_7279 =========================

public List<String> getGroups(String user) throws IOException {	
getting groups for 

public void cacheGroupsRefresh() throws IOException {	
cache is being refreshed 

public static void clearBlackList() throws IOException {	
clearing the blacklist 

public static void clearAll() throws IOException {	
resetting fakegroupmapping 

public void cacheGroupsAdd(List<String> groups) throws IOException {	
adding to groups 

public static void addToBlackList(String user) throws IOException {	
adding to the blacklist 

conf.setLong( CommonConfigurationKeys.HADOOP_SECURITY_GROUPS_NEGATIVE_CACHE_SECS, 0);	Groups groups = new Groups(conf);	groups.cacheGroupsAdd(Arrays.asList(myGroups));	groups.refresh();	FakeGroupMapping.clearBlackList();	FakeGroupMapping.addToBlackList("user1");	assertTrue(groups.getGroups("me").size() == 2);	FakeGroupMapping.addToBlackList("me");	assertTrue(groups.getGroups("me").size() == 2);	try {	
we are not supposed to get here user1 

groups.refresh();	FakeGroupMapping.clearBlackList();	FakeGroupMapping.addToBlackList("user1");	assertTrue(groups.getGroups("me").size() == 2);	FakeGroupMapping.addToBlackList("me");	assertTrue(groups.getGroups("me").size() == 2);	try {	fail();	} catch (IOException ioe) {	if(!ioe.getMessage().startsWith("No groups found")) {	
got unexpected exception 

========================= hadoop sample_2978 =========================

private void uploadOnClose(Path keypath) throws IOException {	boolean uploadSuccess = false;	int attempt = 0;	while (!uploadSuccess) {	try {	++attempt;	bytesUploaded += uploadFileAttempt(keypath, attempt);	uploadSuccess = true;	} catch (IOException e) {	
upload failed 

protected void finalize() throws Throwable {	if(!closed) {	
stream not closed 

protected void finalize() throws Throwable {	if(!closed) {	}	if (backupFile != null) {	
leaking backing file 

private void delete(File file) {	if (file != null) {	
deleting s 

private void delete(File file) {	if (file != null) {	if (!file.delete()) {	
could not delete 

private void partUpload(boolean closingUpload) throws IOException {	if (backupStream != null) {	backupStream.close();	}	if (closingUpload && partUpload && backupFile.length() == 0) {	
skipping upload of byte final partition 

} else {	partUpload = true;	boolean uploadSuccess = false;	int attempt = 0;	while(!uploadSuccess) {	try {	++attempt;	bytesUploaded += uploadFilePartAttempt(attempt);	uploadSuccess = true;	} catch (IOException e) {	
upload failed 

========================= hadoop sample_6192 =========================

public void setup() {	LOG.info("---------------------------------");	
testing qop 

public void testErrorMessage() throws Exception {	BadTokenSecretManager sm = new BadTokenSecretManager();	final Server server = setupTestServer(conf, 5, sm);	boolean succeeded = false;	try {	doDigestRpc(server, sm);	} catch (ServiceException e) {	assertTrue(e.getCause() instanceof RemoteException);	RemoteException re = (RemoteException) e.getCause();	
logging message 

});	try {	futures[i].get(100, TimeUnit.MILLISECONDS);	} catch (TimeoutException te) {	continue;	}	Assert.fail("future"+i+" did not block");	}	proxy.sendPostponed(null, newEmptyRequest());	for (int i=0; i < futures.length; i++) {	
waiting for future 

private String getAuthMethod( final AuthMethod clientAuth, final AuthMethod serverAuth) throws Exception {	try {	return internalGetAuthMethod(clientAuth, serverAuth, UseToken.NONE);	} catch (Exception e) {	
auth method failure 

private String getAuthMethod( final AuthMethod clientAuth, final AuthMethod serverAuth, final UseToken tokenType) throws Exception {	try {	return internalGetAuthMethod(clientAuth, serverAuth, tokenType);	} catch (Exception e) {	
auth method failure 

break;	case INVALID: token = new Token<>( tokenId.getBytes(), "bad-password!".getBytes(), tokenId.getKind(), null);	SecurityUtil.setTokenService(token, addr);	break;	case OTHER: token = new Token<>();	break;	case NONE: }	clientUgi.addToken(token);	}	try {	
trying ugi tokens 

========================= hadoop sample_3144 =========================

if (scope.startsWith("~")) {	searchScope = NodeBase.ROOT;	excludedScope = scope.substring(1);	} else {	searchScope = scope;	excludedScope = null;	}	Node n = chooseRandom(searchScope, excludedScope, excludedNodes);	if (n == null) {	if (LOG.isDebugEnabled()) {	
no node to choose 

if (n == null) {	if (LOG.isDebugEnabled()) {	}	return null;	}	Preconditions.checkArgument(n instanceof DatanodeDescriptor);	DatanodeDescriptor dnDescriptor = (DatanodeDescriptor)n;	if (dnDescriptor.hasStorageType(type)) {	return dnDescriptor;	} else {	
first trial failed node has no type making second trial carrying this type 

========================= hadoop sample_7804 =========================

public AuthenticationToken authenticate(HttpServletRequest request, HttpServletResponse response) throws IOException, AuthenticationException {	AuthenticationToken token = null;	String authorization = request.getHeader(HttpConstants.AUTHORIZATION_HEADER);	if (authorization == null || !AuthenticationHandlerUtil.matchAuthScheme(HttpConstants.BASIC, authorization)) {	response.setHeader(WWW_AUTHENTICATE, HttpConstants.BASIC);	response.setStatus(HttpServletResponse.SC_UNAUTHORIZED);	if (authorization == null) {	
basic auth starting 

public AuthenticationToken authenticate(HttpServletRequest request, HttpServletResponse response) throws IOException, AuthenticationException {	AuthenticationToken token = null;	String authorization = request.getHeader(HttpConstants.AUTHORIZATION_HEADER);	if (authorization == null || !AuthenticationHandlerUtil.matchAuthScheme(HttpConstants.BASIC, authorization)) {	response.setHeader(WWW_AUTHENTICATE, HttpConstants.BASIC);	response.setStatus(HttpServletResponse.SC_UNAUTHORIZED);	if (authorization == null) {	} else {	
does not start with 

public boolean verify(String hostname, SSLSession session) {	return true;	}	});	}	tls.negotiate();	ctx.addToEnvironment(Context.SECURITY_AUTHENTICATION, SECURITY_AUTHENTICATION);	ctx.addToEnvironment(Context.SECURITY_PRINCIPAL, userDN);	ctx.addToEnvironment(Context.SECURITY_CREDENTIALS, password);	ctx.lookup(userDN);	
authentication successful for 

private void authenticateWithoutTlsExtension(String userDN, String password) throws AuthenticationException {	Hashtable<String, Object> env = new Hashtable<String, Object>();	env.put(Context.INITIAL_CONTEXT_FACTORY, "com.sun.jndi.ldap.LdapCtxFactory");	env.put(Context.PROVIDER_URL, providerUrl);	env.put(Context.SECURITY_AUTHENTICATION, SECURITY_AUTHENTICATION);	env.put(Context.SECURITY_PRINCIPAL, userDN);	env.put(Context.SECURITY_CREDENTIALS, password);	try {	Context ctx = new InitialDirContext(env);	ctx.close();	
authentication successful for 

========================= hadoop sample_2755 =========================

public static Compressor getCompressor(CompressionCodec codec, Configuration conf) {	Compressor compressor = borrow(compressorPool, codec.getCompressorType());	if (compressor == null) {	compressor = codec.createCompressor();	
got brand new compressor 

public static Compressor getCompressor(CompressionCodec codec, Configuration conf) {	Compressor compressor = borrow(compressorPool, codec.getCompressorType());	if (compressor == null) {	compressor = codec.createCompressor();	} else {	compressor.reinit(conf);	if(LOG.isDebugEnabled()) {	
got recycled compressor 

public static Decompressor getDecompressor(CompressionCodec codec) {	Decompressor decompressor = borrow(decompressorPool, codec.getDecompressorType());	if (decompressor == null) {	decompressor = codec.createDecompressor();	
got brand new decompressor 

public static Decompressor getDecompressor(CompressionCodec codec) {	Decompressor decompressor = borrow(decompressorPool, codec.getDecompressorType());	if (decompressor == null) {	decompressor = codec.createDecompressor();	} else {	if(LOG.isDebugEnabled()) {	
got recycled decompressor 

========================= hadoop sample_3847 =========================

private void createControlFile(FileSystem fs, long nrBytes, int nrFiles ) throws IOException {	
creating control file bytes files 

writer.append(new Text(name), new LongWritable(nrBytes));	} catch(Exception e) {	throw new IOException(e.getLocalizedMessage());	} finally {	if (writer != null) {	writer.close();	}	writer = null;	}	}	
created control files for files 

private void cleanup(FileSystem fs) throws IOException {	
cleaning up test files 

========================= hadoop sample_5585 =========================

private void validateConf(Configuration conf) {	int perDirFileLimit = conf.getInt(YarnConfiguration.NM_LOCAL_CACHE_MAX_FILES_PER_DIRECTORY, YarnConfiguration.DEFAULT_NM_LOCAL_CACHE_MAX_FILES_PER_DIRECTORY);	if (perDirFileLimit <= 36) {	
parameter is configured with very low value 

if (!stateStore.canRecover()|| stateStore.isNewlyCreated()) {	cleanUpLocalDirs(lfs, delService);	cleanupLogDirs(lfs, delService);	initializeLocalDirs(lfs);	initializeLogDirs(lfs);	}	} catch (Exception e) {	throw new YarnRuntimeException( "Failed to initialize LocalizationService", e);	}	diskValidator = DiskValidatorFactory.getInstance( conf.get(YarnConfiguration.DISK_VALIDATOR, YarnConfiguration.DEFAULT_DISK_VALIDATOR));	
disk validator is loaded 

private void recoverTrackerResources(LocalResourcesTracker tracker, LocalResourceTrackerState state) throws URISyntaxException {	for (LocalizedResourceProto proto : state.getLocalizedResources()) {	LocalResource rsrc = new LocalResourcePBImpl(proto.getResource());	LocalResourceRequest req = new LocalResourceRequest(rsrc);	if (LOG.isDebugEnabled()) {	
recovering localized resource at 

LocalResourceRequest req = new LocalResourceRequest(rsrc);	if (LOG.isDebugEnabled()) {	}	tracker.handle(new ResourceRecoveredEvent(req, new Path(proto.getLocalPath()), proto.getSize()));	}	for (Map.Entry<LocalResourceProto, Path> entry : state.getInProgressResources().entrySet()) {	LocalResource rsrc = new LocalResourcePBImpl(entry.getKey());	LocalResourceRequest req = new LocalResourceRequest(rsrc);	Path localPath = entry.getValue();	tracker.handle(new ResourceRecoveredEvent(req, localPath, 0));	
deleting in progress localization for at 

public void serviceStart() throws Exception {	cacheCleanup.scheduleWithFixedDelay(new CacheCleanup(dispatcher), cacheCleanupPeriod, cacheCleanupPeriod, TimeUnit.MILLISECONDS);	server = createServer();	server.start();	localizationServerAddress = getConfig().updateConnectAddr(YarnConfiguration.NM_BIND_HOST, YarnConfiguration.NM_LOCALIZER_ADDRESS, YarnConfiguration.DEFAULT_NM_LOCALIZER_ADDRESS, server.getListenerAddress());	
localizer started on port 

private void handleInitContainerResources( ContainerLocalizationRequestEvent rsrcReqs) {	Container c = rsrcReqs.getContainer();	EnumSet<ContainerState> set = EnumSet.of(ContainerState.LOCALIZING, ContainerState.RUNNING, ContainerState.REINITIALIZING);	if (!set.contains(c.getContainerState())) {	
is at state do not localize resources 

return;	}	LoadingCache<Path,Future<FileStatus>> statCache = CacheBuilder.newBuilder().build(FSDownload.createStatusCacheLoader(getConfig()));	LocalizerContext ctxt = new LocalizerContext( c.getUser(), c.getContainerId(), c.getCredentials(), statCache);	Map<LocalResourceVisibility, Collection<LocalResourceRequest>> rsrcs = rsrcReqs.getRequestedResources();	for (Map.Entry<LocalResourceVisibility, Collection<LocalResourceRequest>> e : rsrcs.entrySet()) {	LocalResourcesTracker tracker = getLocalResourcesTracker(e.getKey(), c.getUser(), c.getContainerId().getApplicationAttemptId() .getApplicationId());	for (LocalResourceRequest req : e.getValue()) {	tracker.handle(new ResourceRequestEvent(req, e.getKey(), ctxt));	if (LOG.isDebugEnabled()) {	
localizing for container 

private void submitDirForDeletion(String userName, Path dir) {	try {	lfs.getFileStatus(dir);	FileDeletionTask deletionTask = new FileDeletionTask(delService, userName, dir, null);	delService.delete(deletionTask);	} catch (UnsupportedFileSystemException ue) {	
local dir is an unsupported filesystem 

ApplicationId appId = application.getAppId();	String appIDStr = application.toString();	LocalResourcesTracker appLocalRsrcsTracker = appRsrc.remove(appId.toString());	if (appLocalRsrcsTracker != null) {	for (LocalizedResource rsrc : appLocalRsrcsTracker ) {	Path localPath = rsrc.getLocalPath();	if (localPath != null) {	try {	stateStore.removeLocalizedResource(userName, appId, localPath);	} catch (IOException e) {	
unable to remove resource for from state store 

for (LocalizedResource rsrc : appLocalRsrcsTracker ) {	Path localPath = rsrc.getLocalPath();	if (localPath != null) {	try {	stateStore.removeLocalizedResource(userName, appId, localPath);	} catch (IOException e) {	}	}	}	} else {	
removing uninitialized application 

public LocalizerHeartbeatResponse processHeartbeat(LocalizerStatus status) {	String locId = status.getLocalizerId();	synchronized (privLocalizers) {	LocalizerRunner localizer = privLocalizers.get(locId);	if (null == localizer) {	
unknown localizer with localizerid is sending heartbeat ordering it to die 

public void handle(LocalizerEvent event) {	String locId = event.getLocalizerId();	switch (event.getType()) {	case REQUEST_RESOURCE_LOCALIZATION: LocalizerResourceRequestEvent req = (LocalizerResourceRequestEvent)event;	switch (req.getVisibility()) {	case PUBLIC: publicLocalizer.addResource(req);	break;	case PRIVATE: case APPLICATION: synchronized (privLocalizers) {	LocalizerRunner localizer = privLocalizers.get(locId);	if (localizer != null && localizer.killContainerLocalizer.get()) {	
new localize request for remove old private localizer 

switch (req.getVisibility()) {	case PUBLIC: publicLocalizer.addResource(req);	break;	case PRIVATE: case APPLICATION: synchronized (privLocalizers) {	LocalizerRunner localizer = privLocalizers.get(locId);	if (localizer != null && localizer.killContainerLocalizer.get()) {	cleanupPrivLocalizers(locId);	localizer = null;	}	if (null == localizer) {	
created localizer for 

public void addResource(LocalizerResourceRequestEvent request) {	LocalizedResource rsrc = request.getResource();	LocalResourceRequest key = rsrc.getRequest();	
downloading public resource 

} else {	throw new DiskChecker.DiskErrorException( "Disk Validator is null!");	}	}	synchronized (pending) {	pending.put(queue.submit(new FSDownload(lfs, null, conf, publicDirDestPath, resource, request.getContext().getStatCache())), request);	}	} catch (IOException e) {	rsrc.unlock();	publicRsrc.handle(new ResourceFailedLocalizationEvent(request .getResource().getRequest(), e.getMessage()));	
local path for public localization is not found may be disks failed 

}	synchronized (pending) {	pending.put(queue.submit(new FSDownload(lfs, null, conf, publicDirDestPath, resource, request.getContext().getStatCache())), request);	}	} catch (IOException e) {	rsrc.unlock();	publicRsrc.handle(new ResourceFailedLocalizationEvent(request .getResource().getRequest(), e.getMessage()));	} catch (IllegalArgumentException ie) {	rsrc.unlock();	publicRsrc.handle(new ResourceFailedLocalizationEvent(request .getResource().getRequest(), ie.getMessage()));	
local path for public localization is not found incorrect path 

}	} catch (IOException e) {	rsrc.unlock();	publicRsrc.handle(new ResourceFailedLocalizationEvent(request .getResource().getRequest(), e.getMessage()));	} catch (IllegalArgumentException ie) {	rsrc.unlock();	publicRsrc.handle(new ResourceFailedLocalizationEvent(request .getResource().getRequest(), ie.getMessage()));	} catch (RejectedExecutionException re) {	rsrc.unlock();	publicRsrc.handle(new ResourceFailedLocalizationEvent(request .getResource().getRequest(), re.getMessage()));	
failed to submit rsrc for download either queue is full or threadpool is shutdown 

publicRsrc.handle(new ResourceFailedLocalizationEvent(request .getResource().getRequest(), e.getMessage()));	} catch (IllegalArgumentException ie) {	rsrc.unlock();	publicRsrc.handle(new ResourceFailedLocalizationEvent(request .getResource().getRequest(), ie.getMessage()));	} catch (RejectedExecutionException re) {	rsrc.unlock();	publicRsrc.handle(new ResourceFailedLocalizationEvent(request .getResource().getRequest(), re.getMessage()));	}	} else {	if (LOG.isDebugEnabled()) {	
skip downloading resource since it s in state 

rsrc.unlock();	publicRsrc.handle(new ResourceFailedLocalizationEvent(request .getResource().getRequest(), re.getMessage()));	}	} else {	if (LOG.isDebugEnabled()) {	}	rsrc.unlock();	}	} else {	if (LOG.isDebugEnabled()) {	
skip downloading resource since it is locked by other threads 

public void run() {	try {	while (!Thread.currentThread().isInterrupted()) {	try {	Future<Path> completed = queue.take();	LocalizerResourceRequestEvent assoc = pending.remove(completed);	try {	if (null == assoc) {	
localized unknown resource to 

LocalizerResourceRequestEvent assoc = pending.remove(completed);	try {	if (null == assoc) {	return;	}	Path local = completed.get();	LocalResourceRequest key = assoc.getResource().getRequest();	publicRsrc.handle(new ResourceLocalizedEvent(key, local, FileUtil .getDU(new File(local.toUri()))));	assoc.getResource().unlock();	} catch (ExecutionException e) {	
failed to download resource 

LocalResourceRequest req = assoc.getResource().getRequest();	publicRsrc.handle(new ResourceFailedLocalizationEvent(req, e.getMessage()));	assoc.getResource().unlock();	} catch (CancellationException e) {	}	} catch (InterruptedException e) {	return;	}	}	} catch(Throwable t) {	
error shutting down 

publicRsrc.handle(new ResourceFailedLocalizationEvent(req, e.getMessage()));	assoc.getResource().unlock();	} catch (CancellationException e) {	}	} catch (InterruptedException e) {	return;	}	}	} catch(Throwable t) {	} finally {	
public cache exiting 

exec.startLocalizer(new LocalizerStartContext.Builder() .setNmPrivateContainerTokens(nmPrivateCTokensPath) .setNmAddr(localizationServerAddress) .setUser(context.getUser()) .setAppId(context.getContainerId() .getApplicationAttemptId().getApplicationId().toString()) .setLocId(localizerId) .setDirsHandler(dirsHandler) .build());	} else {	throw new IOException("All disks failed. " + dirsHandler.getDisksHealthReport(false));	}	} catch (FSError fe) {	exception = fe;	} catch (Exception e) {	exception = e;	} finally {	if (exception != null) {	
localizer failed 

private Credentials getSystemCredentialsSentFromRM( LocalizerContext localizerContext) throws IOException {	ApplicationId appId = localizerContext.getContainerId().getApplicationAttemptId() .getApplicationId();	Credentials systemCredentials = nmContext.getSystemCredentialsForApps().get(appId);	if (systemCredentials == null) {	return null;	}	if (LOG.isDebugEnabled()) {	
adding new framework token for for localization 

try {	Credentials credentials = context.getCredentials();	if (UserGroupInformation.isSecurityEnabled()) {	Credentials systemCredentials = getSystemCredentialsSentFromRM(context);	if (systemCredentials != null) {	credentials = systemCredentials;	}	}	FileContext lfs = getLocalFileContext(getConfig());	tokenOut = lfs.create(nmPrivateCTokensPath, EnumSet.of(CREATE, OVERWRITE));	
writing credentials to the nmprivate file 

Credentials credentials = context.getCredentials();	if (UserGroupInformation.isSecurityEnabled()) {	Credentials systemCredentials = getSystemCredentialsSentFromRM(context);	if (systemCredentials != null) {	credentials = systemCredentials;	}	}	FileContext lfs = getLocalFileContext(getConfig());	tokenOut = lfs.create(nmPrivateCTokensPath, EnumSet.of(CREATE, OVERWRITE));	if (LOG.isDebugEnabled()) {	
credentials list in 

private void cleanUpLocalDir(FileContext lfs, DeletionService del, String localDir) {	long currentTimeStamp = System.currentTimeMillis();	renameLocalDir(lfs, localDir, ContainerLocalizer.USERCACHE, currentTimeStamp);	renameLocalDir(lfs, localDir, ContainerLocalizer.FILECACHE, currentTimeStamp);	renameLocalDir(lfs, localDir, ResourceLocalizationService.NM_PRIVATE_DIR, currentTimeStamp);	try {	deleteLocalDir(lfs, del, localDir);	} catch (IOException e) {	
failed to delete localdir 

private void renameLocalDir(FileContext lfs, String localDir, String localSubDir, long currentTimeStamp) {	try {	lfs.rename(new Path(localDir, localSubDir), new Path( localDir, localSubDir + "_DEL_" + currentTimeStamp));	} catch (FileNotFoundException ex) {	} catch (Exception ex) {	
failed to rename the local file under 

private void deleteLocalDir(FileContext lfs, DeletionService del, String localDir) throws IOException {	RemoteIterator<FileStatus> fileStatus = lfs.listStatus(new Path(localDir));	if (fileStatus != null) {	while (fileStatus.hasNext()) {	FileStatus status = fileStatus.next();	try {	if (status.getPath().getName().matches(".*" + ContainerLocalizer.USERCACHE + "_DEL_.*")) {	
usercache path 

while (fileStatus.hasNext()) {	FileStatus status = fileStatus.next();	try {	if (status.getPath().getName().matches(".*" + ContainerLocalizer.USERCACHE + "_DEL_.*")) {	cleanUpFilesPerUserDir(lfs, del, status.getPath());	} else if (status.getPath().getName() .matches(".*" + NM_PRIVATE_DIR + "_DEL_.*") || status.getPath().getName() .matches(".*" + ContainerLocalizer.FILECACHE + "_DEL_.*")) {	FileDeletionTask deletionTask = new FileDeletionTask(del, null, status.getPath(), null);	del.delete(deletionTask);	}	} catch (IOException ex) {	
failed to delete this local directory 

========================= hadoop sample_1895 =========================

public static void main(String[] args) {	StringUtils.startupShutdownMessage(Portmap.class, args, LOG);	final int port = RpcProgram.RPCB_PORT;	Portmap pm = new Portmap();	try {	pm.start(DEFAULT_IDLE_TIME_MILLISECONDS, new InetSocketAddress(port), new InetSocketAddress(port));	} catch (Throwable e) {	
failed to start the server cause 

========================= hadoop sample_4394 =========================

public boolean addNoChecksumAnchor() {	if (slot == null) {	return false;	}	boolean result = slot.addAnchor();	
no checksum anchor to slot added could not add 

========================= hadoop sample_6983 =========================

assertTrue("Active RM services aren't started", rm.areActiveServicesRunning());	assertTrue("RM is not ready to become active", rm.adminService.getServiceStatus().isReadyToBecomeActive());	try {	rm.getNewAppId();	nm = rm.registerNode("127.0.0.1:1", 2048);	app = rm.submitApp(1024);	attempt = app.getCurrentAppAttempt();	rm.waitForState(attempt.getAppAttemptId(), RMAppAttemptState.SCHEDULED);	} catch (Exception e) {	fail("Unable to perform Active RM functions");	
activerm check failed 

========================= hadoop sample_622 =========================

public synchronized boolean next(K key, V value) throws IOException {	if(!skipIt.hasNext()) {	
further records got skipped 

private void updateJobWithSplit(final JobConf job, InputSplit inputSplit) {	if (inputSplit instanceof FileSplit) {	FileSplit fileSplit = (FileSplit) inputSplit;	job.set(JobContext.MAP_INPUT_FILE, fileSplit.getPath().toString());	job.setLong(JobContext.MAP_INPUT_START, fileSplit.getStart());	job.setLong(JobContext.MAP_INPUT_PATH, fileSplit.getLength());	}	
processing split 

kvbuffer = new byte[maxMemUsage];	bufvoid = kvbuffer.length;	kvmeta = ByteBuffer.wrap(kvbuffer) .order(ByteOrder.nativeOrder()) .asIntBuffer();	setEquator(0);	bufstart = bufend = bufindex = equator;	kvstart = kvend = kvindex;	maxRec = kvmeta.capacity() / NMETA;	softLimit = (int)(kvbuffer.length * spillper);	bufferRemaining = softLimit;	LOG.info(JobContext.IO_SORT_MB + ": " + sortmb);	
soft limit at 

bb.write(b0, 0, 0);	int valend = bb.markRecord();	mapOutputRecordCounter.increment(1);	mapOutputByteCounter.increment( distanceTo(keystart, valend, bufvoid));	kvmeta.put(kvindex + PARTITION, partition);	kvmeta.put(kvindex + KEYSTART, keystart);	kvmeta.put(kvindex + VALSTART, valstart);	kvmeta.put(kvindex + VALLEN, distanceTo(valstart, valend));	kvindex = (kvindex - NMETA + kvmeta.capacity()) % kvmeta.capacity();	} catch (MapBufferTooSmallException e) {	
record too large for in memory buffer 

private void setEquator(int pos) {	equator = pos;	final int aligned = pos - (pos % METASIZE);	kvindex = (int) (((long)aligned - METASIZE + kvbuffer.length) % kvbuffer.length) / 4;	
equator kvi 

private void resetSpill() {	final int e = equator;	bufstart = bufend = e;	final int aligned = e - (e % METASIZE);	kvstart = kvend = (int) (((long)aligned - METASIZE + kvbuffer.length) % kvbuffer.length) / 4;	
reset equator kv kvi 

public void flush() throws IOException, ClassNotFoundException, InterruptedException {	
starting flush of map output 

public void flush() throws IOException, ClassNotFoundException, InterruptedException {	if (kvbuffer == null) {	
kvbuffer is null skipping flush 

spillDone.await();	}	checkSpillException();	final int kvbend = 4 * kvend;	if ((kvbend + METASIZE) % kvbuffer.length != equator - (equator % METASIZE)) {	resetSpill();	}	if (kvindex != kvend) {	kvend = (kvindex + NMETA) % kvmeta.capacity();	bufend = bufmark;	
spilling map output 

private void startSpill() {	assert !spillInProgress;	kvend = (kvindex + NMETA) % kvmeta.capacity();	bufend = bufmark;	spillInProgress = true;	
spilling map output 

} finally {	if (null != writer) writer.close();	}	}	if (totalIndexCacheMemory >= indexCacheMemoryLimit) {	Path indexFilename = mapOutputFile.getSpillIndexFileForWrite(numSpills, partitions spillRec.writeToFile(indexFilename, job);	} else {	indexCacheList.add(spillRec);	totalIndexCacheMemory += spillRec.size() * MAP_OUTPUT_INDEX_RECORD_LENGTH;	}	
finished spill 

========================= hadoop sample_4814 =========================

public Void run() throws Exception {	if (!connection.channel.isOpen()) {	
skipped 

try {	value = call( rpcKind, connection.protocolName, rpcRequest, timestamp);	} catch (Throwable e) {	populateResponseParamsOnError(e, responseParams);	}	if (!isResponseDeferred()) {	setupResponse(this, responseParams.returnStatus, responseParams.detailedErr, value, responseParams.errorClass, responseParams.error);	sendResponse();	} else {	if (LOG.isDebugEnabled()) {	
deferring response for callid 

public void run() {	
starting 

public void run() {	try {	doRunLoop();	} finally {	try {	readSelector.close();	} catch (IOException ioe) {	
error closing read selector in 

readSelector.select();	Iterator<SelectionKey> iter = readSelector.selectedKeys().iterator();	while (iter.hasNext()) {	key = iter.next();	iter.remove();	try {	if (key.isReadable()) {	doRead(key);	}	} catch (CancelledKeyException cke) {	
connection aborted from 

try {	if (key.isReadable()) {	doRead(key);	}	} catch (CancelledKeyException cke) {	}	key = null;	}	} catch (InterruptedException e) {	if (running) {	
unexpectedly interrupted 

doRead(key);	}	} catch (CancelledKeyException cke) {	}	key = null;	}	} catch (InterruptedException e) {	if (running) {	}	} catch (IOException ex) {	
error in reader 

}	} catch (CancelledKeyException cke) {	}	key = null;	}	} catch (InterruptedException e) {	if (running) {	}	} catch (IOException ex) {	} catch (Throwable re) {	
bug in read selector 

public void run() {	
starting 

iter.remove();	try {	if (key.isValid()) {	if (key.isAcceptable()) doAccept(key);	}	} catch (IOException e) {	}	key = null;	}	} catch (OutOfMemoryError e) {	
out of memory in server select 

key = null;	}	} catch (OutOfMemoryError e) {	closeCurrentConnection(key, e);	connectionManager.closeIdle(true);	try { Thread.sleep(60000); } catch (Exception ie) {}	} catch (Exception e) {	closeCurrentConnection(key, e);	}	}	
stopping 

public void run() {	
starting 

public void run() {	SERVER.set(Server.this);	try {	doRunLoop();	} finally {	
stopping 

public void run() {	SERVER.set(Server.this);	try {	doRunLoop();	} finally {	try {	writeSelector.close();	} catch (IOException ioe) {	
couldn t close write selector in 

while (iter.hasNext()) {	SelectionKey key = iter.next();	iter.remove();	try {	if (key.isWritable()) {	doAsyncWrite(key);	}	} catch (CancelledKeyException cke) {	RpcCall call = (RpcCall)key.attachment();	if (call != null) {	
connection aborted from 

iter.remove();	try {	if (key.isWritable()) {	doAsyncWrite(key);	}	} catch (CancelledKeyException cke) {	RpcCall call = (RpcCall)key.attachment();	if (call != null) {	}	} catch (IOException e) {	
doasyncwrite threw exception 

}	} catch (IOException e) {	}	}	long now = Time.now();	if (now < lastPurgeTime + PURGE_INTERVAL) {	continue;	}	lastPurgeTime = now;	if(LOG.isDebugEnabled()) {	
checking for old call responses 

RpcCall call = (RpcCall)key.attachment();	if (call != null && key.channel() == call.connection.channel) {	calls.add(call);	}	}	}	for (RpcCall call : calls) {	doPurge(call, now);	}	} catch (OutOfMemoryError e) {	
out of memory in server select 

calls.add(call);	}	}	}	for (RpcCall call : calls) {	doPurge(call, now);	}	} catch (OutOfMemoryError e) {	try { Thread.sleep(60000); } catch (Exception ie) {}	} catch (Exception e) {	
exception in responder 

return;	}	if (key.channel() != call.connection.channel) {	throw new IOException("doAsyncWrite: bad channel");	}	synchronized(call.connection.responseQueue) {	if (processResponse(call.connection.responseQueue, false)) {	try {	key.interestOps(0);	} catch (CancelledKeyException e) {	
exception while changing ops 

try {	synchronized (responseQueue) {	numElements = responseQueue.size();	if (numElements == 0) {	error = false;	return true;	}	call = responseQueue.removeFirst();	SocketChannel channel = call.connection.channel;	if (LOG.isDebugEnabled()) {	
responding to 

}	if (!call.rpcResponse.hasRemaining()) {	call.rpcResponse = null;	call.connection.decRpcCount();	if (numElements == 1) {	done = true;	} else {	done = false;	}	if (LOG.isDebugEnabled()) {	
responding to wrote bytes 

try {	writeSelector.wakeup();	channel.register(writeSelector, SelectionKey.OP_WRITE, call);	} catch (ClosedChannelException e) {	done = true;	} finally {	decPending();	}	}	if (LOG.isDebugEnabled()) {	
responding to wrote partial bytes 

decPending();	}	}	if (LOG.isDebugEnabled()) {	}	}	error = false;	}	} finally {	if (error && call != null) {	
call output error 

this.hostAddress = "*Unknown*";	} else {	this.hostAddress = addr.getHostAddress();	}	this.remotePort = socket.getPort();	this.responseQueue = new LinkedList<RpcCall>();	if (socketSendBufferSize != 0) {	try {	socket.setSendBufferSize(socketSendBufferSize);	} catch (IOException e) {	
connection unable to set socket send buffer size to 

RpcSaslProto saslResponse = null;	try {	try {	saslResponse = processSaslMessage(saslMessage);	} catch (IOException e) {	rpcMetrics.incrAuthenticationFailures();	if (LOG.isDebugEnabled()) {	LOG.debug(StringUtils.stringifyException(e));	}	IOException tce = (IOException) getCauseForInvalidToken(e);	
with true cause 

} catch (IOException e) {	rpcMetrics.incrAuthenticationFailures();	if (LOG.isDebugEnabled()) {	LOG.debug(StringUtils.stringifyException(e));	}	IOException tce = (IOException) getCauseForInvalidToken(e);	throw tce;	}	if (saslServer != null && saslServer.isComplete()) {	if (LOG.isDebugEnabled()) {	
sasl server context established negotiated qop is 

LOG.debug(StringUtils.stringifyException(e));	}	IOException tce = (IOException) getCauseForInvalidToken(e);	throw tce;	}	if (saslServer != null && saslServer.isComplete()) {	if (LOG.isDebugEnabled()) {	}	user = getAuthorizedUgi(saslServer.getAuthorizationID());	if (LOG.isDebugEnabled()) {	
sasl server successfully authenticated client 

private RpcSaslProto processSaslToken(RpcSaslProto saslMessage) throws SaslException {	if (!saslMessage.hasToken()) {	throw new SaslException("Client did not send a token");	}	byte[] saslToken = saslMessage.getToken().toByteArray();	if (LOG.isDebugEnabled()) {	
have read input token of size for processing by saslserver evaluateresponse 

return count;	}	int version = connectionHeaderBuf.get(0);	this.setServiceClass(connectionHeaderBuf.get(1));	dataLengthBuffer.flip();	if (HTTP_GET_BYTES.equals(dataLengthBuffer)) {	setupHttpRequestOnIpcPortResponse();	return -1;	}	if (!RpcConstants.HEADER.equals(dataLengthBuffer) || version != CURRENT_VERSION) {	
incorrect header or version mismatch from got version expected version 

private void unwrapPacketAndProcessRpcs(byte[] inBuf) throws IOException, InterruptedException {	if (LOG.isDebugEnabled()) {	
have read input token of size for processing by saslserver unwrap 

private void processOneRpc(ByteBuffer bb) throws IOException, InterruptedException {	int callId = -1;	int retry = RpcConstants.INVALID_RETRY_COUNT;	try {	final RpcWritable.Buffer buffer = RpcWritable.Buffer.wrap(bb);	final RpcRequestHeaderProto header = getMessage(RpcRequestHeaderProto.getDefaultInstance(), buffer);	callId = header.getCallId();	retry = header.getRetryCount();	if (LOG.isDebugEnabled()) {	
got 

checkRpcHeaders(header);	if (callId < 0) {	processRpcOutOfBandRequest(header, buffer);	} else if (!connectionContextRead) {	throw new FatalRpcServerException( RpcErrorCodeProto.FATAL_INVALID_RPC_HEADER, "Connection context not established");	} else {	processRpcRequest(header, buffer);	}	} catch (RpcServerException rse) {	if (LOG.isDebugEnabled()) {	
processonerpc from client threw exception 

private void processRpcRequest(RpcRequestHeaderProto header, RpcWritable.Buffer buffer) throws RpcServerException, InterruptedException {	Class<? extends Writable> rpcRequestClass = getRpcRequestWrapper(header.getRpcKind());	if (rpcRequestClass == null) {	
unknown rpc kind from client 

if (rpcRequestClass == null) {	final String err = "Unknown rpc kind in rpc header"  + header.getRpcKind();	throw new FatalRpcServerException( RpcErrorCodeProto.FATAL_INVALID_RPC_HEADER, err);	}	Writable rpcRequest;	try {	rpcRequest = buffer.newInstance(rpcRequestClass, conf);	} catch (RpcServerException rse) {	throw rse;	} catch (Throwable t) {	
unable to read call parameters for client on connection protocol for rpckind 

if (authProtocol == AuthProtocol.SASL && !saslContextEstablished) {	throw new FatalRpcServerException( RpcErrorCodeProto.FATAL_INVALID_RPC_HEADER, "Connection header sent during SASL negotiation");	}	processConnectionContext(buffer);	} else if (callId == AuthProtocol.SASL.callId) {	if (authProtocol != AuthProtocol.SASL) {	throw new FatalRpcServerException( RpcErrorCodeProto.FATAL_INVALID_RPC_HEADER, "SASL protocol not requested by client");	}	saslReadAndProcess(buffer);	} else if (callId == PING_CALL_ID) {	
received ping message 

private void authorizeConnection() throws RpcServerException {	try {	if (user != null && user.getRealUser() != null && (authMethod != AuthMethod.TOKEN)) {	ProxyUsers.authorize(user, this.getHostAddress());	}	authorize(user, protocolName, getHostInetAddress());	if (LOG.isDebugEnabled()) {	
successfully authorized 

private void authorizeConnection() throws RpcServerException {	try {	if (user != null && user.getRealUser() != null && (authMethod != AuthMethod.TOKEN)) {	ProxyUsers.authorize(user, this.getHostAddress());	}	authorize(user, protocolName, getHostInetAddress());	if (LOG.isDebugEnabled()) {	}	rpcMetrics.incrAuthorizationSuccesses();	} catch (AuthorizationException ae) {	
connection from for protocol is unauthorized for user 

private synchronized void close() {	disposeSasl();	data = null;	dataLengthBuffer = null;	if (!channel.isOpen()) return;	try {socket.shutdownOutput();} catch(Exception e) {	
ignoring socket shutdown exception 

public void run() {	
starting 

public void run() {	SERVER.set(Server.this);	while (running) {	TraceScope traceScope = null;	try {	final Call call = callQueue.take();	if (LOG.isDebugEnabled()) {	
for rpckind 

}	CallerContext.setCurrent(call.callerContext);	UserGroupInformation remoteUser = call.getRemoteUser();	if (remoteUser != null) {	remoteUser.doAs(call);	} else {	call.run();	}	} catch (InterruptedException e) {	if (running) {	
unexpectedly interrupted 

} else {	call.run();	}	} catch (InterruptedException e) {	if (running) {	if (traceScope != null) {	traceScope.getSpan().addTimelineAnnotation("unexpectedly interrupted: " + StringUtils.stringifyException(e));	}	}	} catch (Exception e) {	
caught an exception 

}	} catch (Exception e) {	if (traceScope != null) {	traceScope.getSpan().addTimelineAnnotation("Exception: " + StringUtils.stringifyException(e));	}	} finally {	CurCall.set(null);	IOUtils.cleanupWithLogger(LOG, traceScope);	}	}	
exiting 

private List<AuthMethod> getAuthMethods(SecretManager<?> secretManager, Configuration conf) {	AuthenticationMethod confAuthenticationMethod = SecurityUtil.getAuthenticationMethod(conf);	List<AuthMethod> authMethods = new ArrayList<AuthMethod>();	if (confAuthenticationMethod == AuthenticationMethod.TOKEN) {	if (secretManager == null) {	throw new IllegalArgumentException(AuthenticationMethod.TOKEN + " authentication requires a secret manager");	}	} else if (secretManager != null) {	
authentication enabled for secret manager 

AuthenticationMethod confAuthenticationMethod = SecurityUtil.getAuthenticationMethod(conf);	List<AuthMethod> authMethods = new ArrayList<AuthMethod>();	if (confAuthenticationMethod == AuthenticationMethod.TOKEN) {	if (secretManager == null) {	throw new IllegalArgumentException(AuthenticationMethod.TOKEN + " authentication requires a secret manager");	}	} else if (secretManager != null) {	authMethods.add(AuthenticationMethod.TOKEN.getAuthMethod());	}	authMethods.add(confAuthenticationMethod.getAuthMethod());	
server accepts auth methods 

headerBuilder.setClientId(ByteString.copyFrom(call.clientId));	headerBuilder.setCallId(call.callId);	headerBuilder.setRetryCount(call.retryCount);	headerBuilder.setStatus(status);	headerBuilder.setServerIpcVersionNum(CURRENT_VERSION);	if (status == RpcStatusProto.SUCCESS) {	RpcResponseHeaderProto header = headerBuilder.build();	try {	setupResponse(call, header, rv);	} catch (Throwable t) {	
error serializing call response for call 

private void setupResponse(RpcCall call, RpcResponseHeaderProto header, Writable rv) throws IOException {	final byte[] response;	if (rv == null || (rv instanceof RpcWritable.ProtobufWrapper)) {	response = setupResponseForProtobuf(header, rv);	} else {	response = setupResponseForWritable(header, rv);	}	if (response.length > maxRespSize) {	
large response size for call 

private void wrapWithSasl(RpcCall call) throws IOException {	if (call.connection.saslServer != null) {	byte[] token = call.rpcResponse.array();	synchronized (call.connection.saslServer) {	token = call.connection.saslServer.wrap(token, 0, token.length);	}	
adding saslserver wrapped token of size as call response 

public synchronized void stop() {	
stopping server on 

private void scheduleIdleScanTask() {	if (!running) {	return;	}	TimerTask idleScanTask = new TimerTask(){	public void run() {	if (!running) {	return;	}	if (LOG.isDebugEnabled()) {	
task running 

========================= hadoop sample_4052 =========================

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreAppEvent)) {	
illegal event type 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreAppEvent)) {	return RMStateStoreState.ACTIVE;	}	boolean isFenced = false;	ApplicationStateData appState = ((RMStateStoreAppEvent) event).getAppState();	ApplicationId appId = appState.getApplicationSubmissionContext().getApplicationId();	
storing info for app 

if (!(event instanceof RMStateStoreAppEvent)) {	return RMStateStoreState.ACTIVE;	}	boolean isFenced = false;	ApplicationStateData appState = ((RMStateStoreAppEvent) event).getAppState();	ApplicationId appId = appState.getApplicationSubmissionContext().getApplicationId();	try {	store.storeApplicationStateInternal(appId, appState);	store.notifyApplication( new RMAppEvent(appId, RMAppEventType.APP_NEW_SAVED));	} catch (Exception e) {	
error storing app 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateUpdateAppEvent)) {	
illegal event type 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateUpdateAppEvent)) {	return RMStateStoreState.ACTIVE;	}	boolean isFenced = false;	ApplicationStateData appState = ((RMStateUpdateAppEvent) event).getAppState();	SettableFuture<Object> result = ((RMStateUpdateAppEvent) event).getResult();	ApplicationId appId = appState.getApplicationSubmissionContext().getApplicationId();	
updating info for app 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreRemoveAppEvent)) {	
illegal event type 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreRemoveAppEvent)) {	return RMStateStoreState.ACTIVE;	}	boolean isFenced = false;	ApplicationStateData appState = ((RMStateStoreRemoveAppEvent) event).getAppState();	ApplicationId appId = appState.getApplicationSubmissionContext().getApplicationId();	
removing info for app 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreRemoveAppEvent)) {	return RMStateStoreState.ACTIVE;	}	boolean isFenced = false;	ApplicationStateData appState = ((RMStateStoreRemoveAppEvent) event).getAppState();	ApplicationId appId = appState.getApplicationSubmissionContext().getApplicationId();	try {	store.removeApplicationStateInternal(appState);	} catch (Exception e) {	
error removing app 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreAppAttemptEvent)) {	
illegal event type 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreAppAttemptEvent)) {	return RMStateStoreState.ACTIVE;	}	boolean isFenced = false;	ApplicationAttemptStateData attemptState = ((RMStateStoreAppAttemptEvent) event).getAppAttemptState();	try {	if (LOG.isDebugEnabled()) {	
storing info for attempt 

return RMStateStoreState.ACTIVE;	}	boolean isFenced = false;	ApplicationAttemptStateData attemptState = ((RMStateStoreAppAttemptEvent) event).getAppAttemptState();	try {	if (LOG.isDebugEnabled()) {	}	store.storeApplicationAttemptStateInternal(attemptState.getAttemptId(), attemptState);	store.notifyApplicationAttempt(new RMAppAttemptEvent (attemptState.getAttemptId(), RMAppAttemptEventType.ATTEMPT_NEW_SAVED));	} catch (Exception e) {	
error storing appattempt 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateUpdateAppAttemptEvent)) {	
illegal event type 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateUpdateAppAttemptEvent)) {	return RMStateStoreState.ACTIVE;	}	boolean isFenced = false;	ApplicationAttemptStateData attemptState = ((RMStateUpdateAppAttemptEvent) event).getAppAttemptState();	try {	if (LOG.isDebugEnabled()) {	
updating info for attempt 

return RMStateStoreState.ACTIVE;	}	boolean isFenced = false;	ApplicationAttemptStateData attemptState = ((RMStateUpdateAppAttemptEvent) event).getAppAttemptState();	try {	if (LOG.isDebugEnabled()) {	}	store.updateApplicationAttemptStateInternal(attemptState.getAttemptId(), attemptState);	store.notifyApplicationAttempt(new RMAppAttemptEvent (attemptState.getAttemptId(), RMAppAttemptEventType.ATTEMPT_UPDATE_SAVED));	} catch (Exception e) {	
error updating appattempt 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreRMDTEvent)) {	
illegal event type 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreRMDTEvent)) {	return RMStateStoreState.ACTIVE;	}	boolean isFenced = false;	RMStateStoreRMDTEvent dtEvent = (RMStateStoreRMDTEvent) event;	try {	
storing rmdelegationtoken and sequencenumber 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreRMDTEvent)) {	return RMStateStoreState.ACTIVE;	}	boolean isFenced = false;	RMStateStoreRMDTEvent dtEvent = (RMStateStoreRMDTEvent) event;	try {	store.storeRMDelegationTokenState( dtEvent.getRmDTIdentifier(), dtEvent.getRenewDate());	} catch (Exception e) {	
error while storing rmdelegationtoken and sequencenumber 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreRMDTEvent)) {	
illegal event type 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreRMDTEvent)) {	return RMStateStoreState.ACTIVE;	}	boolean isFenced = false;	RMStateStoreRMDTEvent dtEvent = (RMStateStoreRMDTEvent) event;	try {	
removing rmdelegationtoken and sequencenumber 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreRMDTEvent)) {	return RMStateStoreState.ACTIVE;	}	boolean isFenced = false;	RMStateStoreRMDTEvent dtEvent = (RMStateStoreRMDTEvent) event;	try {	store.removeRMDelegationTokenState(dtEvent.getRmDTIdentifier());	} catch (Exception e) {	
error while removing rmdelegationtoken and sequencenumber 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreRMDTEvent)) {	
illegal event type 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreRMDTEvent)) {	return RMStateStoreState.ACTIVE;	}	boolean isFenced = false;	RMStateStoreRMDTEvent dtEvent = (RMStateStoreRMDTEvent) event;	try {	
updating rmdelegationtoken and sequencenumber 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreRMDTEvent)) {	return RMStateStoreState.ACTIVE;	}	boolean isFenced = false;	RMStateStoreRMDTEvent dtEvent = (RMStateStoreRMDTEvent) event;	try {	store.updateRMDelegationTokenState( dtEvent.getRmDTIdentifier(), dtEvent.getRenewDate());	} catch (Exception e) {	
error while updating rmdelegationtoken and sequencenumber 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreRMDTMasterKeyEvent)) {	
illegal event type 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreRMDTMasterKeyEvent)) {	return RMStateStoreState.ACTIVE;	}	boolean isFenced = false;	RMStateStoreRMDTMasterKeyEvent dtEvent = (RMStateStoreRMDTMasterKeyEvent) event;	try {	
storing rmdtmasterkey 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreRMDTMasterKeyEvent)) {	return RMStateStoreState.ACTIVE;	}	boolean isFenced = false;	RMStateStoreRMDTMasterKeyEvent dtEvent = (RMStateStoreRMDTMasterKeyEvent) event;	try {	store.storeRMDTMasterKeyState(dtEvent.getDelegationKey());	} catch (Exception e) {	
error while storing rmdtmasterkey 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreRMDTMasterKeyEvent)) {	
illegal event type 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreRMDTMasterKeyEvent)) {	return RMStateStoreState.ACTIVE;	}	boolean isFenced = false;	RMStateStoreRMDTMasterKeyEvent dtEvent = (RMStateStoreRMDTMasterKeyEvent) event;	try {	
removing rmdtmasterkey 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreRMDTMasterKeyEvent)) {	return RMStateStoreState.ACTIVE;	}	boolean isFenced = false;	RMStateStoreRMDTMasterKeyEvent dtEvent = (RMStateStoreRMDTMasterKeyEvent) event;	try {	store.removeRMDTMasterKeyState(dtEvent.getDelegationKey());	} catch (Exception e) {	
error while removing rmdtmasterkey 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreAMRMTokenEvent)) {	
illegal event type 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreAMRMTokenEvent)) {	return RMStateStoreState.ACTIVE;	}	RMStateStoreAMRMTokenEvent amrmEvent = (RMStateStoreAMRMTokenEvent) event;	boolean isFenced = false;	try {	
updating amrmtoken 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreAMRMTokenEvent)) {	return RMStateStoreState.ACTIVE;	}	RMStateStoreAMRMTokenEvent amrmEvent = (RMStateStoreAMRMTokenEvent) event;	boolean isFenced = false;	try {	store.storeOrUpdateAMRMTokenSecretManagerState( amrmEvent.getAmrmTokenSecretManagerState(), amrmEvent.isUpdate());	} catch (Exception e) {	
error storing info for amrmtokensecretmanager 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreStoreReservationEvent)) {	
illegal event type 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreStoreReservationEvent)) {	return RMStateStoreState.ACTIVE;	}	boolean isFenced = false;	RMStateStoreStoreReservationEvent reservationEvent = (RMStateStoreStoreReservationEvent) event;	try {	
storing reservation allocation 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreStoreReservationEvent)) {	return RMStateStoreState.ACTIVE;	}	boolean isFenced = false;	RMStateStoreStoreReservationEvent reservationEvent = (RMStateStoreStoreReservationEvent) event;	try {	store.storeReservationState( reservationEvent.getReservationAllocation(), reservationEvent.getPlanName(), reservationEvent.getReservationIdName());	} catch (Exception e) {	
error while storing reservation allocation 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreStoreReservationEvent)) {	
illegal event type 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreStoreReservationEvent)) {	return RMStateStoreState.ACTIVE;	}	boolean isFenced = false;	RMStateStoreStoreReservationEvent reservationEvent = (RMStateStoreStoreReservationEvent) event;	try {	
removing reservation allocation 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreStoreReservationEvent)) {	return RMStateStoreState.ACTIVE;	}	boolean isFenced = false;	RMStateStoreStoreReservationEvent reservationEvent = (RMStateStoreStoreReservationEvent) event;	try {	store.removeReservationState( reservationEvent.getPlanName(), reservationEvent.getReservationIdName());	} catch (Exception e) {	
error while removing reservation allocation 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreRemoveAppAttemptEvent)) {	
illegal event type 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreRemoveAppAttemptEvent)) {	return RMStateStoreState.ACTIVE;	}	boolean isFenced = false;	ApplicationAttemptId attemptId = ((RMStateStoreRemoveAppAttemptEvent) event).getApplicationAttemptId();	ApplicationId appId = attemptId.getApplicationId();	
removing attempt from app 

public RMStateStoreState transition(RMStateStore store, RMStateStoreEvent event) {	if (!(event instanceof RMStateStoreRemoveAppAttemptEvent)) {	return RMStateStoreState.ACTIVE;	}	boolean isFenced = false;	ApplicationAttemptId attemptId = ((RMStateStoreRemoveAppAttemptEvent) event).getApplicationAttemptId();	ApplicationId appId = attemptId.getApplicationId();	try {	store.removeApplicationAttemptInternal(attemptId);	} catch (Exception e) {	
error removing attempt 

protected abstract void closeInternal() throws Exception;	public void checkVersion() throws Exception {	Version loadedVersion = loadVersion();	
loaded rm state version info 

protected abstract void closeInternal() throws Exception;	public void checkVersion() throws Exception {	Version loadedVersion = loadVersion();	if (loadedVersion != null && loadedVersion.equals(getCurrentVersion())) {	return;	}	if (loadedVersion == null) {	loadedVersion = getCurrentVersion();	}	if (loadedVersion.isCompatibleTo(getCurrentVersion())) {	
storing rm state version info 

protected void handleStoreEvent(RMStateStoreEvent event) {	this.writeLock.lock();	try {	if (LOG.isDebugEnabled()) {	
processing event of type 

protected void handleStoreEvent(RMStateStoreEvent event) {	this.writeLock.lock();	try {	if (LOG.isDebugEnabled()) {	}	final RMStateStoreState oldState = getRMStateStoreState();	this.stateMachine.doTransition(event.getType(), event);	if (oldState != getRMStateStoreState()) {	
rmstatestore state change from to 

protected void handleStoreEvent(RMStateStoreEvent event) {	this.writeLock.lock();	try {	if (LOG.isDebugEnabled()) {	}	final RMStateStoreState oldState = getRMStateStoreState();	this.stateMachine.doTransition(event.getType(), event);	if (oldState != getRMStateStoreState()) {	}	} catch (InvalidStateTransitionException e) {	
can t handle this event at current state 

private boolean notifyStoreOperationFailedInternal( Exception failureCause) {	boolean isFenced = false;	
state store operation failed 

========================= hadoop sample_767 =========================

public static long elapsed(long started, long finished, boolean isRunning) {	if (finished > 0 && started > 0) {	long elapsed = finished - started;	if (elapsed >= 0) {	return elapsed;	} else {	
finished time is ahead of started time 

} else {	return -1;	}	}	if (isRunning) {	long current = System.currentTimeMillis();	long elapsed = started > 0 ? current - started : 0;	if (elapsed >= 0) {	return elapsed;	} else {	
current time is ahead of started time 

========================= hadoop sample_2186 =========================

public synchronized void initSharedJournalsForRead() {	if (state == State.OPEN_FOR_READING) {	
initializing shared journals for read already open for read 

StorageDirectory sd = storage.getStorageDirectory(u);	if (sd != null) {	journalSet.add(new FileJournalManager(conf, sd, storage), required, sharedEditsDirs.contains(u));	}	} else {	journalSet.add(createJournal(u), required, sharedEditsDirs.contains(u));	}	}	}	if (journalSet.isEmpty()) {	
no edits directories configured 

static Class<? extends JournalManager> getJournalClass(Configuration conf, String uriScheme) {	String key = DFSConfigKeys.DFS_NAMENODE_EDITS_PLUGIN_PREFIX + "." + uriScheme;	Class <? extends JournalManager> clazz = null;	try {	clazz = conf.getClass(key, null, JournalManager.class);	} catch (RuntimeException re) {	throw new IllegalArgumentException( "Invalid class specified for " + uriScheme, re);	}	if (clazz == null) {	
no class configured for is empty 

========================= hadoop sample_8071 =========================

public void testDelegationToken() throws Exception {	final String renewer = "JobTracker";	UserGroupInformation.createRemoteUser(renewer);	Credentials creds = new Credentials();	Token<?> tokens[] = fs.addDelegationTokens(renewer, creds);	
delegation tokens 

========================= hadoop sample_7111 =========================

this.tokenValidity = tokenValidity;	shouldDisconnect = Boolean.parseBoolean( config.getProperty(DISCONNECT_FROM_ZOOKEEPER_ON_SHUTDOWN, "true"));	path = config.getProperty(ZOOKEEPER_PATH);	if (path == null) {	throw new IllegalArgumentException(ZOOKEEPER_PATH + " must be specified");	}	try {	nextRolloverDate = System.currentTimeMillis() + tokenValidity;	client.create().creatingParentsIfNeeded() .forPath(path, generateZKData(generateRandomSecret(), generateRandomSecret(), null));	zkVersion = 0;	
creating secret znode 

shouldDisconnect = Boolean.parseBoolean( config.getProperty(DISCONNECT_FROM_ZOOKEEPER_ON_SHUTDOWN, "true"));	path = config.getProperty(ZOOKEEPER_PATH);	if (path == null) {	throw new IllegalArgumentException(ZOOKEEPER_PATH + " must be specified");	}	try {	nextRolloverDate = System.currentTimeMillis() + tokenValidity;	client.create().creatingParentsIfNeeded() .forPath(path, generateZKData(generateRandomSecret(), generateRandomSecret(), null));	zkVersion = 0;	} catch (KeeperException.NodeExistsException nee) {	
the secret znode already exists retrieving data 

private synchronized void pushToZK(byte[] newSecret, byte[] currentSecret, byte[] previousSecret) {	byte[] bytes = generateZKData(newSecret, currentSecret, previousSecret);	try {	client.setData().withVersion(zkVersion).forPath(path, bytes);	} catch (KeeperException.BadVersionException bve) {	
unable to push to znode another server already did it 

private synchronized void pushToZK(byte[] newSecret, byte[] currentSecret, byte[] previousSecret) {	byte[] bytes = generateZKData(newSecret, currentSecret, previousSecret);	try {	client.setData().withVersion(zkVersion).forPath(path, bytes);	} catch (KeeperException.BadVersionException bve) {	} catch (Exception ex) {	
an unexpected exception occurred pushing data to zookeeper 

int previousSecretLength = bb.getInt();	byte[] previousSecret = null;	if (previousSecretLength > 0) {	previousSecret = new byte[previousSecretLength];	bb.get(previousSecret);	}	super.initSecrets(currentSecret, previousSecret);	nextRolloverDate = bb.getLong();	}	} catch (Exception ex) {	
an unexpected exception occurred while pulling data from ZooKeeper 

protected CuratorFramework createCuratorClient(Properties config) throws Exception {	String connectionString = config.getProperty( ZOOKEEPER_CONNECTION_STRING, "localhost:2181");	RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3);	ACLProvider aclProvider;	String authType = config.getProperty(ZOOKEEPER_AUTH_TYPE, "none");	if (authType.equals("sasl")) {	
connecting to zookeeper with sasl kerberos and using sasl acls 

String connectionString = config.getProperty( ZOOKEEPER_CONNECTION_STRING, "localhost:2181");	RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3);	ACLProvider aclProvider;	String authType = config.getProperty(ZOOKEEPER_AUTH_TYPE, "none");	if (authType.equals("sasl")) {	String principal = setJaasConfiguration(config);	System.setProperty(ZooKeeperSaslClient.LOGIN_CONTEXT_NAME_KEY, JAAS_LOGIN_ENTRY_NAME);	System.setProperty("zookeeper.authProvider.1", "org.apache.zookeeper.server.auth.SASLAuthenticationProvider");	aclProvider = new SASLOwnerACLProvider(principal);	} else {	
connecting to zookeeper without authentication 

========================= hadoop sample_2743 =========================

public C findCounter(String counterName, boolean create) {	try {	String[] pair = parseCounterName(counterName);	return findCounter(pair[0], FileSystemCounter.valueOf(pair[1]));	}	catch (Exception e) {	if (create) throw new IllegalArgumentException(e);	
is not a recognized counter 

========================= hadoop sample_5086 =========================

protected Queue loadResource(InputStream resourceInput) throws ParserConfigurationException, SAXException, IOException {	DocumentBuilderFactory docBuilderFactory = DocumentBuilderFactory.newInstance();	docBuilderFactory.setIgnoringComments(true);	docBuilderFactory.setNamespaceAware(true);	try {	docBuilderFactory.setXIncludeAware(true);	} catch (UnsupportedOperationException e) {	
failed to set setxincludeaware true for parser 

private Queue parseResource(Element queuesNode) {	Queue rootNode = null;	try {	if (!QUEUES_TAG.equals(queuesNode.getTagName())) {	
bad conf file top level element not queues 

private Queue parseResource(Element queuesNode) {	Queue rootNode = null;	try {	if (!QUEUES_TAG.equals(queuesNode.getTagName())) {	throw new RuntimeException("No queues defined ");	}	NamedNodeMap nmp = queuesNode.getAttributes();	Node acls = nmp.getNamedItem(ACLS_ENABLED_TAG);	if (acls != null) {	
configuring flag in is not valid this tag is ignored configure in mapred site xml see the documentation of which is used for enabling job level authorization and queue level authorization 

try {	if (!QUEUES_TAG.equals(queuesNode.getTagName())) {	throw new RuntimeException("No queues defined ");	}	NamedNodeMap nmp = queuesNode.getAttributes();	Node acls = nmp.getNamedItem(ACLS_ENABLED_TAG);	if (acls != null) {	}	NodeList props = queuesNode.getChildNodes();	if (props == null || props.getLength() <= 0) {	
bad configuration no queues defined 

NodeList props = queuesNode.getChildNodes();	if (props == null || props.getLength() <= 0) {	throw new RuntimeException(" No queues defined ");	}	for (int i = 0; i < props.getLength(); i++) {	Node propNode = props.item(i);	if (!(propNode instanceof Element)) {	continue;	}	if (!propNode.getNodeName().equals(QUEUE_TAG)) {	
at root level only tags are allowed 

Element prop = (Element) propNode;	Queue q = createHierarchy("", prop);	if(rootNode == null) {	rootNode = new Queue();	rootNode.setName("");	}	rootNode.addChild(q);	}	return rootNode;	} catch (DOMException e) {	
error parsing conf file 

========================= hadoop sample_4678 =========================

DatagramChannelFactory f = new NioDatagramChannelFactory( Executors.newCachedThreadPool(), workerCount);	server = new ConnectionlessBootstrap(f);	server.setPipeline(Channels.pipeline(RpcUtil.STAGE_RPC_MESSAGE_PARSER, rpcProgram, RpcUtil.STAGE_RPC_UDP_RESPONSE));	server.setOption("broadcast", "false");	server.setOption("sendBufferSize", SEND_BUFFER_SIZE);	server.setOption("receiveBufferSize", RECEIVE_BUFFER_SIZE);	server.setOption("reuseAddress", true);	ch = server.bind(new InetSocketAddress(port));	InetSocketAddress socketAddr = (InetSocketAddress) ch.getLocalAddress();	boundPort = socketAddr.getPort();	
started listening to udp requests at port for with workercount 

========================= hadoop sample_4382 =========================

private String relativeToWorking(String pathname) {	String cwd = System.getProperty("user.dir", "/");	pathname = (new Path(pathname)).toUri().getPath();	cwd = (new Path(cwd)).toUri().getPath();	String [] cwdParts = cwd.split(Path.SEPARATOR);	String [] pathParts = pathname.split(Path.SEPARATOR);	if (cwd.equals(pathname)) {	
relative to working 

sb.append(Path.SEPARATOR);	}	for (int i = common; i < pathParts.length; i++) {	sb.append(pathParts[i]);	sb.append(Path.SEPARATOR);	}	String s = sb.toString();	if (s.endsWith(Path.SEPARATOR)) {	s = s.substring(0, s.length() - 1);	}	
relative to working 

public void testVolumeNormalization() throws Throwable {	
test root dir is 

========================= hadoop sample_5593 =========================

static FileStatus mkdirs(FSNamesystem fsn, String src, PermissionStatus permissions, boolean createParent) throws IOException {	FSDirectory fsd = fsn.getFSDirectory();	if(NameNode.stateChangeLog.isDebugEnabled()) {	
dir namesystem mkdirs 

assert fsd.hasWriteLock();	existing = unprotectedMkdir(fsd, fsd.allocateNewInodeId(), existing, localName, perm, null, now());	if (existing == null) {	return null;	}	final INode newNode = existing.getLastINode();	NameNode.getNameNodeMetrics().incrFilesCreated();	String cur = existing.getPath();	fsd.getEditLog().logMkDir(cur, newNode);	if (NameNode.stateChangeLog.isDebugEnabled()) {	
mkdirs created directory 

========================= hadoop sample_8122 =========================

public void setUp() throws IOException {	ExitUtil.disableSystemExit();	String baseDir = PathUtils.getTestDirName(getClass());	hdfsDir = new File(baseDir, "dfs/name");	if (hdfsDir.exists() && !FileUtil.fullyDelete(hdfsDir)) {	throw new IOException("Could not delete test directory '" + hdfsDir + "'");	}	
hdfsdir is 

========================= hadoop sample_7392 =========================

synchronized (AbstractLivelinessMonitor.this) {	Iterator<Map.Entry<O, Long>> iterator = running.entrySet().iterator();	long currentTime = clock.getTime();	while (iterator.hasNext()) {	Map.Entry<O, Long> entry = iterator.next();	O key = entry.getKey();	long interval = getExpireInterval(key);	if (currentTime > entry.getValue() + interval) {	iterator.remove();	expire(key);	
expired timed out after secs 

long interval = getExpireInterval(key);	if (currentTime > entry.getValue() + interval) {	iterator.remove();	expire(key);	}	}	}	try {	Thread.sleep(monitorInterval);	} catch (InterruptedException e) {	
thread interrupted 

========================= hadoop sample_2190 =========================

Text tag = aRecord.getTag();	ResetableIterator data = retv.get(tag);	if (data == null) {	data = createResetableIterator();	retv.put(tag, data);	}	data.add(aRecord);	}	if (this.numOfValues > this.largestNumOfValues) {	this.largestNumOfValues = numOfValues;	
key this largestnumofvalues 

========================= hadoop sample_6079 =========================

public void initFilter(FilterContainer container, Configuration conf) {	String key = getEnabledConfigKey();	boolean enabled = conf.getBoolean(key, false);	if (enabled) {	container.addGlobalFilter("Cross Origin Filter", CrossOriginFilter.class.getName(), getFilterParameters(conf, getPrefix()));	} else {	
cors filter not enabled please set to true to enable it 

========================= hadoop sample_3781 =========================

}	boolean containerExitEventOccured = false;	public boolean isContainerExitEventOccured() {	return containerExitEventOccured;	}	public void handle(ContainerEvent event) {	if (event instanceof ContainerExitEvent) {	containerExitEventOccured = true;	ContainerExitEvent exitEvent = (ContainerExitEvent) event;	Assert.assertEquals(ContainerEventType.CONTAINER_EXITED_WITH_FAILURE, exitEvent.getType());	
diagnostic info 

long createTime = 1234;	Token containerToken = createContainerToken(cId, priority, createTime);	StartContainerRequest scRequest = StartContainerRequest.newInstance(containerLaunchContext, containerToken);	List<StartContainerRequest> list = new ArrayList<StartContainerRequest>();	list.add(scRequest);	StartContainersRequest allRequests = StartContainersRequest.newInstance(list);	containerManager.startContainers(allRequests);	int timeoutSecs = 0;	while (!processStartFile.exists() && timeoutSecs++ < 20) {	Thread.sleep(1000);	
waiting for process start file to be created 

long createTime = 1234;	Token containerToken = createContainerToken(cId, priority, createTime);	StartContainerRequest scRequest = StartContainerRequest.newInstance(containerLaunchContext, containerToken);	List<StartContainerRequest> list = new ArrayList<StartContainerRequest>();	list.add(scRequest);	StartContainersRequest allRequests = StartContainersRequest.newInstance(list);	containerManager.startContainers(allRequests);	int timeoutSecs = 0;	while (!processStartFile.exists() && timeoutSecs++ < 20) {	Thread.sleep(1000);	
waiting for process start file to be created 

}	Assert.assertTrue("ProcessStartFile doesn't exist!", processStartFile.exists());	BufferedReader reader = new BufferedReader(new FileReader(processStartFile));	String pid = reader.readLine().trim();	Assert.assertEquals(null, reader.readLine());	reader.close();	reader = new BufferedReader(new FileReader(childProcessStartFile));	String child = reader.readLine().trim();	Assert.assertEquals(null, reader.readLine());	reader.close();	
manually killing pid but not child pid 

========================= hadoop sample_1616 =========================

Token<DelegationTokenIdentifier> token = (Token<DelegationTokenIdentifier>) tm .createToken(UserGroupInformation.getCurrentUser(), "good");	Assert.assertNotNull(token);	Token<DelegationTokenIdentifier> cancelled = (Token<DelegationTokenIdentifier>) tm .createToken(UserGroupInformation.getCurrentUser(), "cancelled");	Assert.assertNotNull(cancelled);	tm.verifyToken(token);	tm.verifyToken(cancelled);	tm.cancelToken(cancelled, "cancelled");	final AbstractDelegationTokenSecretManager sm = tm.getDelegationTokenSecretManager();	final ZKDelegationTokenSecretManager zksm = (ZKDelegationTokenSecretManager) sm;	final AbstractDelegationTokenIdentifier idCancelled = sm.decodeTokenIdentifier(cancelled);	
waiting for the cancelled token to be removed 

AbstractDelegationTokenSecretManager.DelegationTokenInformation dtinfo = zksmNew.getTokenInfo(id);	Assert.assertNull("canceled dt should be gone!", dtinfo);	id = smNew.decodeTokenIdentifier(token);	dtinfo = zksmNew.getTokenInfoFromMemory(id);	Assert.assertNotNull("good dt should be in memory!", dtinfo);	Thread.sleep(5000);	final ZKDelegationTokenSecretManager zksm1 = zksmNew;	final AbstractDelegationTokenIdentifier id1 = id;	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	
waiting for the expired token to be removed 

========================= hadoop sample_2939 =========================

public void run() {	long start = Time.monotonicNow();	int currentIndex;	int processedFilesCount = 0;	while ((currentIndex = fileIndex.getAndIncrement()) < files.length) {	processedFilesCount++;	FileMetadata file = files[currentIndex];	try {	if (!task.execute(file)) {	
operation failed for file 

while ((currentIndex = fileIndex.getAndIncrement()) < files.length) {	processedFilesCount++;	FileMetadata file = files[currentIndex];	try {	if (!task.execute(file)) {	operationStatus = false;	} else {	filesProcessed.getAndIncrement();	}	} catch (Exception e) {	
encountered exception for operation for file 

try {	if (!task.execute(file)) {	operationStatus = false;	} else {	filesProcessed.getAndIncrement();	}	} catch (Exception e) {	lastException = new IOException("Encountered Exception for " + this.operation + " operation for file " + file.getKey(), e);	}	if (lastException != null || !operationStatus) {	
terminating execution of operation now as some other thread already got exception or operation failed 

filesProcessed.getAndIncrement();	}	} catch (Exception e) {	lastException = new IOException("Encountered Exception for " + this.operation + " operation for file " + file.getKey(), e);	}	if (lastException != null || !operationStatus) {	break;	}	}	long end = Time.monotonicNow();	
time taken to process files count for operation ms 

========================= hadoop sample_6429 =========================

private long doCopyFile(FileStatus srcstat, Path tmpfile, Path absdst, Reporter reporter) throws IOException {	long bytesCopied = 0L;	Path srcPath = srcstat.getPath();	try (FSDataInputStream in = srcPath.getFileSystem(job).open(srcPath)) {	reporter.incrCounter(Counter.BYTESEXPECTED, srcstat.getLen());	try (FSDataOutputStream out = create(tmpfile, reporter, srcstat)) {	
copying file of size bytes 

final String s = "Possible Cause for failure: Either the filesystem " + srcstat.getPath().getFileSystem(job) + " is not accessible or the file is deleted";	LOG.error(s);	out.collect(null, new Text(s));	}	try {	for (int i = 0; i < 3; ++i) {	try {	final Path tmp = new Path(job.get(TMP_DIR_LABEL), relativedst);	if (destFileSys.delete(tmp, true)) break;	} catch (Throwable ex) {	
ignoring cleanup exception 

static void fullyDelete(String dir, Configuration conf) throws IOException {	if (dir != null) {	Path tmp = new Path(dir);	boolean success = tmp.getFileSystem(conf).delete(tmp, true);	if (!success) {	
could not fully delete 

if (!(destFileSys.exists(dstparent) && destFileSys.getFileStatus(dstparent).isDirectory())) {	destPath = dstparent;	}	}	skipPath = update && sameFile(srcfs, child, dstfs, destPath, skipCRCCheck);	skipPath |= fileCount == args.filelimit || byteCount + child.getLen() > args.sizelimit;	if (!skipPath) {	++fileCount;	byteCount += child.getLen();	if (LOG.isTraceEnabled()) {	
adding file 

src_writer.sync();	dst_writer.sync();	cnsyncf = 0;	cbsyncs = 0L;	}	}	else {	++skipFileCount;	skipByteCount += child.getLen();	if (LOG.isTraceEnabled()) {	
skipping file 

}	if (args.dryrun) {	return false;	}	int mapCount = setMapCount(byteCount, jobConf);	setReplication(conf, jobConf, srcfilelist, mapCount);	FileStatus dststatus = null;	try {	dststatus = dstfs.getFileStatus(args.dst);	} catch(FileNotFoundException fnfe) {	
does not exist 

FileStatus dststatus;	try {	dststatus = dstfs.getFileStatus(dstpath);	} catch(FileNotFoundException fnfe) {	return false;	}	if (srcstatus.getLen() != dststatus.getLen()) {	return false;	}	if (skipCRCCheck) {	
skipping the crc check 

========================= hadoop sample_6307 =========================

public void testListEmptyRootDirectory() throws IOException {	for (int attempt = 1, maxAttempts = 10; attempt <= maxAttempts; ++attempt) {	try {	super.testListEmptyRootDirectory();	break;	} catch (AssertionError | FileNotFoundException e) {	if (attempt < maxAttempts) {	
attempt of for empty root directory test failed this is likely caused by eventual consistency of listings attempting retry 

} catch (AssertionError | FileNotFoundException e) {	if (attempt < maxAttempts) {	try {	Thread.sleep(1000);	} catch (InterruptedException e2) {	Thread.currentThread().interrupt();	fail("Test interrupted.");	break;	}	} else {	
empty root directory test failed attempts failing test 

========================= hadoop sample_5878 =========================

NativeIO.renameTo(tmpFile, origFile);	} catch (NativeIOException e) {	throw new IOException("Could not rename temporary file " + tmpFile + " to " + origFile + " due to failure in native rename. " + e.toString());	}	}	} else {	if (!triedToClose) {	IOUtils.closeStream(out);	}	if (!tmpFile.delete()) {	
unable to delete tmp file 

public void abort() {	try {	super.close();	} catch (IOException ioe) {	
unable to abort file 

public void abort() {	try {	super.close();	} catch (IOException ioe) {	}	if (!tmpFile.delete()) {	
unable to delete tmp file during abort 

========================= hadoop sample_7845 =========================

public void init() throws ServerException {	if (SERVER != null) {	throw new RuntimeException("HttpFSServer server already initialized");	}	SERVER = this;	super.init();	adminGroup = getConfig().get(getPrefixedName(CONF_ADMIN_GROUP), "admin");	
connects to namenode 

========================= hadoop sample_6757 =========================

static void runBalancer(Suite s, final long totalUsed, final long totalCapacity) throws Exception {	final double avg = totalUsed*100.0/totalCapacity;	LOG.info("BALANCER 0: totalUsed=" + totalUsed + ", totalCapacity=" + totalCapacity + ", avg=" + avg);	wait(s.clients, totalUsed, totalCapacity);	
balancer 

static void runBalancer(Suite s, final long totalUsed, final long totalCapacity) throws Exception {	final double avg = totalUsed*100.0/totalCapacity;	LOG.info("BALANCER 0: totalUsed=" + totalUsed + ", totalCapacity=" + totalCapacity + ", avg=" + avg);	wait(s.clients, totalUsed, totalCapacity);	Map<Integer, DatanodeStorageReport[]> preBalancerPoolUsages = getStorageReports(s);	final Collection<URI> namenodes = DFSUtil.getInternalNsRpcUris(s.conf);	final int r = Balancer.run(namenodes, s.parameters, s.conf);	Assert.assertEquals(ExitStatus.SUCCESS.getExitCode(), r);	
balancer 

static void runBalancer(Suite s, final long totalUsed, final long totalCapacity) throws Exception {	final double avg = totalUsed*100.0/totalCapacity;	LOG.info("BALANCER 0: totalUsed=" + totalUsed + ", totalCapacity=" + totalCapacity + ", avg=" + avg);	wait(s.clients, totalUsed, totalCapacity);	Map<Integer, DatanodeStorageReport[]> preBalancerPoolUsages = getStorageReports(s);	final Collection<URI> namenodes = DFSUtil.getInternalNsRpcUris(s.conf);	final int r = Balancer.run(namenodes, s.parameters, s.conf);	Assert.assertEquals(ExitStatus.SUCCESS.getExitCode(), r);	wait(s.clients, totalUsed, totalCapacity);	
balancer 

if (!balanced) {	if (i % 100 == 0) {	LOG.warn("datanodes " + d + " is not yet balanced: " + "used=" + used[d] + ", cap=" + cap[d] + ", avg=" + avg);	LOG.warn("TestBalancer.sum(used)=" + TestBalancer.sum(used) + ", TestBalancer.sum(cap)=" + TestBalancer.sum(cap));	}	sleep(100);	break;	}	}	}	
balancer 

private static void compareTotalPoolUsage(DatanodeStorageReport[] preReports, DatanodeStorageReport[] postReports) {	Assert.assertNotNull(preReports);	Assert.assertNotNull(postReports);	Assert.assertEquals(preReports.length, postReports.length);	for (DatanodeStorageReport preReport : preReports) {	String dnUuid = preReport.getDatanodeInfo().getDatanodeUuid();	for(DatanodeStorageReport postReport : postReports) {	if(postReport.getDatanodeInfo().getDatanodeUuid().equals(dnUuid)) {	Assert.assertEquals(getTotalPoolUsage(preReport), getTotalPoolUsage(postReport));	
comparision of datanode pool usage pre post balancer run prepoolusage postpoolusage 

private void unevenDistribution(final int nNameNodes, final int nNameNodesToBalance, long distributionPerNN[], long capacities[], String[] racks, Configuration conf) throws Exception {	
uneven 

final int nDataNodes = distributionPerNN.length;	if (capacities.length != nDataNodes || racks.length != nDataNodes) {	throw new IllegalArgumentException("Array length is not the same");	}	if (nNameNodesToBalance > nNameNodes) {	throw new IllegalArgumentException("Number of namenodes to balance is " + "greater than the number of namenodes.");	}	final long usedSpacePerNN = TestBalancer.sum(distributionPerNN);	final ExtendedBlock[][] blocks;	{	
uneven 

if (capacities.length != nDataNodes || racks.length != nDataNodes) {	throw new IllegalArgumentException("Array length is not the same");	}	if (nNameNodesToBalance > nNameNodes) {	throw new IllegalArgumentException("Number of namenodes to balance is " + "greater than the number of namenodes.");	}	final long usedSpacePerNN = TestBalancer.sum(distributionPerNN);	final ExtendedBlock[][] blocks;	{	final MiniDFSCluster cluster = new MiniDFSCluster .Builder(new Configuration(conf)) .nnTopology(MiniDFSNNTopology.simpleFederatedTopology(nNameNodes)) .numDataNodes(nDataNodes) .racks(racks) .simulatedCapacities(capacities) .build();	
uneven 

if (nNameNodesToBalance > nNameNodes) {	throw new IllegalArgumentException("Number of namenodes to balance is " + "greater than the number of namenodes.");	}	final long usedSpacePerNN = TestBalancer.sum(distributionPerNN);	final ExtendedBlock[][] blocks;	{	final MiniDFSCluster cluster = new MiniDFSCluster .Builder(new Configuration(conf)) .nnTopology(MiniDFSNNTopology.simpleFederatedTopology(nNameNodes)) .numDataNodes(nDataNodes) .racks(racks) .simulatedCapacities(capacities) .build();	try {	cluster.waitActive();	DFSTestUtil.setFederatedConfiguration(cluster, conf);	
uneven 

}	final long usedSpacePerNN = TestBalancer.sum(distributionPerNN);	final ExtendedBlock[][] blocks;	{	final MiniDFSCluster cluster = new MiniDFSCluster .Builder(new Configuration(conf)) .nnTopology(MiniDFSNNTopology.simpleFederatedTopology(nNameNodes)) .numDataNodes(nDataNodes) .racks(racks) .simulatedCapacities(capacities) .build();	try {	cluster.waitActive();	DFSTestUtil.setFederatedConfiguration(cluster, conf);	final Suite s = new Suite(cluster, nNameNodes, nDataNodes, null, conf);	blocks = generateBlocks(s, usedSpacePerNN);	
uneven 

cluster.waitActive();	DFSTestUtil.setFederatedConfiguration(cluster, conf);	final Suite s = new Suite(cluster, nNameNodes, nDataNodes, null, conf);	blocks = generateBlocks(s, usedSpacePerNN);	} finally {	cluster.shutdown();	}	}	conf.set(DFSConfigKeys.DFS_NAMENODE_SAFEMODE_THRESHOLD_PCT_KEY, "0.0f");	{	
uneven 

DFSTestUtil.setFederatedConfiguration(cluster, conf);	final Suite s = new Suite(cluster, nNameNodes, nDataNodes, null, conf);	blocks = generateBlocks(s, usedSpacePerNN);	} finally {	cluster.shutdown();	}	}	conf.set(DFSConfigKeys.DFS_NAMENODE_SAFEMODE_THRESHOLD_PCT_KEY, "0.0f");	{	final MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf) .nnTopology(MiniDFSNNTopology.simpleFederatedTopology(nNameNodes)) .numDataNodes(nDataNodes) .racks(racks) .simulatedCapacities(capacities) .format(false) .build();	
uneven 

blocks = generateBlocks(s, usedSpacePerNN);	} finally {	cluster.shutdown();	}	}	conf.set(DFSConfigKeys.DFS_NAMENODE_SAFEMODE_THRESHOLD_PCT_KEY, "0.0f");	{	final MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf) .nnTopology(MiniDFSNNTopology.simpleFederatedTopology(nNameNodes)) .numDataNodes(nDataNodes) .racks(racks) .simulatedCapacities(capacities) .format(false) .build();	try {	cluster.waitActive();	
uneven 

b.setBlockpools(blockpools);	BalancerParameters params = b.build();	final Suite s = new Suite(cluster, nNameNodes, nDataNodes, params, conf);	for(int n = 0; n < nNameNodes; n++) {	final Block[][] blocksDN = TestBalancer.distributeBlocks( blocks[n], s.replication, distributionPerNN);	for(int d = 0; d < blocksDN.length; d++) cluster.injectBlocks(n, d, Arrays.asList(blocksDN[d]));	LOG.info("UNEVEN 13: n=" + n);	}	final long totalCapacity = TestBalancer.sum(capacities);	final long totalUsed = nNameNodes*usedSpacePerNN;	
uneven 

BalancerParameters params = b.build();	final Suite s = new Suite(cluster, nNameNodes, nDataNodes, params, conf);	for(int n = 0; n < nNameNodes; n++) {	final Block[][] blocksDN = TestBalancer.distributeBlocks( blocks[n], s.replication, distributionPerNN);	for(int d = 0; d < blocksDN.length; d++) cluster.injectBlocks(n, d, Arrays.asList(blocksDN[d]));	LOG.info("UNEVEN 13: n=" + n);	}	final long totalCapacity = TestBalancer.sum(capacities);	final long totalUsed = nNameNodes*usedSpacePerNN;	runBalancer(s, totalUsed, totalCapacity);	
uneven 

final Block[][] blocksDN = TestBalancer.distributeBlocks( blocks[n], s.replication, distributionPerNN);	for(int d = 0; d < blocksDN.length; d++) cluster.injectBlocks(n, d, Arrays.asList(blocksDN[d]));	LOG.info("UNEVEN 13: n=" + n);	}	final long totalCapacity = TestBalancer.sum(capacities);	final long totalUsed = nNameNodes*usedSpacePerNN;	runBalancer(s, totalUsed, totalCapacity);	} finally {	cluster.shutdown();	}	
uneven 

private void runTest(final int nNameNodes, long[] capacities, String[] racks, long newCapacity, String newRack, Configuration conf) throws Exception {	final int nDataNodes = capacities.length;	LOG.info("nNameNodes=" + nNameNodes + ", nDataNodes=" + nDataNodes);	Assert.assertEquals(nDataNodes, racks.length);	
run test 

private void runTest(final int nNameNodes, long[] capacities, String[] racks, long newCapacity, String newRack, Configuration conf) throws Exception {	final int nDataNodes = capacities.length;	LOG.info("nNameNodes=" + nNameNodes + ", nDataNodes=" + nDataNodes);	Assert.assertEquals(nDataNodes, racks.length);	final MiniDFSCluster cluster = new MiniDFSCluster .Builder(new Configuration(conf)) .nnTopology(MiniDFSNNTopology.simpleFederatedTopology(nNameNodes)) .numDataNodes(nDataNodes) .racks(racks) .simulatedCapacities(capacities) .build();	
run test 

private void runTest(final int nNameNodes, long[] capacities, String[] racks, long newCapacity, String newRack, Configuration conf) throws Exception {	final int nDataNodes = capacities.length;	LOG.info("nNameNodes=" + nNameNodes + ", nDataNodes=" + nDataNodes);	Assert.assertEquals(nDataNodes, racks.length);	final MiniDFSCluster cluster = new MiniDFSCluster .Builder(new Configuration(conf)) .nnTopology(MiniDFSNNTopology.simpleFederatedTopology(nNameNodes)) .numDataNodes(nDataNodes) .racks(racks) .simulatedCapacities(capacities) .build();	DFSTestUtil.setFederatedConfiguration(cluster, conf);	try {	cluster.waitActive();	
run test 

private void runTest(final int nNameNodes, long[] capacities, String[] racks, long newCapacity, String newRack, Configuration conf) throws Exception {	final int nDataNodes = capacities.length;	LOG.info("nNameNodes=" + nNameNodes + ", nDataNodes=" + nDataNodes);	Assert.assertEquals(nDataNodes, racks.length);	final MiniDFSCluster cluster = new MiniDFSCluster .Builder(new Configuration(conf)) .nnTopology(MiniDFSNNTopology.simpleFederatedTopology(nNameNodes)) .numDataNodes(nDataNodes) .racks(racks) .simulatedCapacities(capacities) .build();	DFSTestUtil.setFederatedConfiguration(cluster, conf);	try {	cluster.waitActive();	final Suite s = new Suite(cluster, nNameNodes, nDataNodes, BalancerParameters.DEFAULT, conf);	long totalCapacity = TestBalancer.sum(capacities);	
run test 

DFSTestUtil.setFederatedConfiguration(cluster, conf);	try {	cluster.waitActive();	final Suite s = new Suite(cluster, nNameNodes, nDataNodes, BalancerParameters.DEFAULT, conf);	long totalCapacity = TestBalancer.sum(capacities);	final long totalUsed = totalCapacity*3/10;	final long size = (totalUsed/nNameNodes)/s.replication;	for(int n = 0; n < nNameNodes; n++) {	createFile(s, n, size);	}	
run test 

cluster.waitActive();	final Suite s = new Suite(cluster, nNameNodes, nDataNodes, BalancerParameters.DEFAULT, conf);	long totalCapacity = TestBalancer.sum(capacities);	final long totalUsed = totalCapacity*3/10;	final long size = (totalUsed/nNameNodes)/s.replication;	for(int n = 0; n < nNameNodes; n++) {	createFile(s, n, size);	}	cluster.startDataNodes(conf, 1, true, null, new String[]{newRack}, new long[]{newCapacity});	totalCapacity += newCapacity;	
run test 

final Suite s = new Suite(cluster, nNameNodes, nDataNodes, BalancerParameters.DEFAULT, conf);	long totalCapacity = TestBalancer.sum(capacities);	final long totalUsed = totalCapacity*3/10;	final long size = (totalUsed/nNameNodes)/s.replication;	for(int n = 0; n < nNameNodes; n++) {	createFile(s, n, size);	}	cluster.startDataNodes(conf, 1, true, null, new String[]{newRack}, new long[]{newCapacity});	totalCapacity += newCapacity;	runBalancer(s, totalUsed, totalCapacity);	
run test 

final long size = (totalUsed/nNameNodes)/s.replication;	for(int n = 0; n < nNameNodes; n++) {	createFile(s, n, size);	}	cluster.startDataNodes(conf, 1, true, null, new String[]{newRack}, new long[]{newCapacity});	totalCapacity += newCapacity;	runBalancer(s, totalUsed, totalCapacity);	} finally {	cluster.shutdown();	}	
run test 

========================= hadoop sample_7516 =========================

public void initialize(Configuration conf, FSClusterStats stats, NetworkTopology clusterMap, Host2NodesMap host2datanodeMap) {	super.initialize(conf, stats, clusterMap, host2datanodeMap);	float balancedPreferencePercent = conf.getFloat( DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY, DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_DEFAULT);	LOG.info("Available space block placement policy initialized: " + DFSConfigKeys.DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY + " = " + balancedPreferencePercent);	if (balancedPreferencePercent > 1.0) {	
the value of is greater than but should be in the range 

public void initialize(Configuration conf, FSClusterStats stats, NetworkTopology clusterMap, Host2NodesMap host2datanodeMap) {	super.initialize(conf, stats, clusterMap, host2datanodeMap);	float balancedPreferencePercent = conf.getFloat( DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY, DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_DEFAULT);	LOG.info("Available space block placement policy initialized: " + DFSConfigKeys.DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY + " = " + balancedPreferencePercent);	if (balancedPreferencePercent > 1.0) {	}	if (balancedPreferencePercent < 0.5) {	
the value of is less than so datanodes with more used percent will receive more block allocations 

========================= hadoop sample_8319 =========================

public static void main(String[] args) {	boolean result = false;	try {	ContainerLaunchFailAppMaster appMaster = new ContainerLaunchFailAppMaster();	
initializing applicationmaster 

boolean result = false;	try {	ContainerLaunchFailAppMaster appMaster = new ContainerLaunchFailAppMaster();	boolean doRun = appMaster.init(args);	if (!doRun) {	System.exit(0);	}	appMaster.run();	result = appMaster.finish();	} catch (Throwable t) {	
error running applicationmaster 

boolean doRun = appMaster.init(args);	if (!doRun) {	System.exit(0);	}	appMaster.run();	result = appMaster.finish();	} catch (Throwable t) {	System.exit(1);	}	if (result) {	
application master completed successfully exiting 

System.exit(0);	}	appMaster.run();	result = appMaster.finish();	} catch (Throwable t) {	System.exit(1);	}	if (result) {	System.exit(0);	} else {	
application master failed exiting 

========================= hadoop sample_5 =========================

public void testEmptyBasic() {	
test empty basic 

public void testEmptyBasic() {	Iterator<Integer> iter = set.iterator();	assertFalse(iter.hasNext());	assertEquals(0, set.size());	assertTrue(set.isEmpty());	
test empty done 

public void testOneElementBasic() {	
test one element basic 

public void testOneElementBasic() {	set.add(list.get(0));	assertEquals(1, set.size());	assertFalse(set.isEmpty());	Iterator<Integer> iter = set.iterator();	assertTrue(iter.hasNext());	assertEquals(list.get(0), iter.next());	assertFalse(iter.hasNext());	
test one element basic done 

public void testMultiBasic() {	
test multi element basic 

}	Iterator<Integer> iter = set.iterator();	int num = 0;	while (iter.hasNext()) {	Integer next = iter.next();	assertNotNull(next);	assertTrue(list.contains(next));	num++;	}	assertEquals(list.size(), num);	
test multi element basic done 

public void testRemoveOne() {	
test remove one 

assertTrue(set.add(list.get(0)));	assertEquals(1, set.size());	assertTrue(set.remove(list.get(0)));	assertEquals(0, set.size());	Iterator<Integer> iter = set.iterator();	assertFalse(iter.hasNext());	assertTrue(set.add(list.get(0)));	assertEquals(1, set.size());	iter = set.iterator();	assertTrue(iter.hasNext());	
test remove one done 

public void testRemoveMulti() {	
test remove multi 

}	for (int i = 0; i < NUM / 2; i++) {	assertTrue(set.remove(list.get(i)));	}	for (int i = 0; i < NUM / 2; i++) {	assertFalse(set.contains(list.get(i)));	}	for (int i = NUM / 2; i < NUM; i++) {	assertTrue(set.contains(list.get(i)));	}	
test remove multi done 

public void testRemoveAll() {	
test remove all 

}	for (int i = 0; i < NUM; i++) {	assertTrue(set.remove(list.get(i)));	}	for (int i = 0; i < NUM; i++) {	assertFalse(set.contains(list.get(i)));	}	Iterator<Integer> iter = set.iterator();	assertFalse(iter.hasNext());	assertTrue(set.isEmpty());	
test remove all done 

public void testRemoveAllViaIterator() {	
test remove all via iterator 

assertTrue(set.contains(e));	iter.remove();	assertFalse(set.contains(e));	}	for (int i = 0; i < NUM; i++) {	assertFalse(set.contains(list.get(i)));	}	Iterator<Integer> iter = set.iterator();	assertFalse(iter.hasNext());	assertTrue(set.isEmpty());	
test remove all via iterator done 

public void testPollAll() {	
test poll all 

assertEquals(0, set.size());	assertTrue(set.isEmpty());	for (int i = 0; i < NUM; i++) {	assertFalse(set.contains(list.get(i)));	}	for (Integer i : poll) {	assertTrue(list.contains(i));	}	Iterator<Integer> iter = set.iterator();	assertFalse(iter.hasNext());	
test poll all done 

public void testPollNMulti() {	
test polln multi 

assertTrue(list.contains(i));	assertFalse(set.contains(i));	}	poll = set.pollN(1000);	assertEquals(NUM - 10, poll.size());	for (Integer i : poll) {	assertTrue(list.contains(i));	}	assertTrue(set.isEmpty());	assertEquals(0, set.size());	
test polln multi done 

public void testPollNMultiArray() {	
test polln multi array 

for (int i = 0; i < NUM; i++) {	assertTrue(list.contains(poll[i]));	}	set.addAll(list);	poll = new Integer[0];	poll = set.pollToArray(poll);	for (int i = 0; i < NUM; i++) {	assertTrue(set.contains(list.get(i)));	}	assertEquals(0, poll.length);	
test polln multi array done 

public void testClear() {	
test clear 

public void testClear() {	set.addAll(list);	assertEquals(NUM, set.size());	assertFalse(set.isEmpty());	set.clear();	assertEquals(0, set.size());	assertTrue(set.isEmpty());	Iterator<Integer> iter = set.iterator();	assertFalse(iter.hasNext());	
test clear done 

public void testCapacity() {	
test capacity 

int expCap = LightWeightHashSet.MINIMUM_CAPACITY;	while (expCap < NUM && maxF * expCap < NUM) expCap <<= 1;	assertEquals(expCap, set.getCapacity());	set.clear();	set.addAll(list);	int toRemove = set.size() - (int) (set.getCapacity() * minF) + 1;	for (int i = 0; i < toRemove; i++) {	set.remove(list.get(i));	}	assertEquals(Math.max(LightWeightHashSet.MINIMUM_CAPACITY, expCap / 2), set.getCapacity());	
test capacity done 

public void testOther() {	
test other 

assertEquals(NUM - 10, array.length);	for (int i = 0; i < array.length; i++) {	assertTrue(sub2.contains(array[i]));	}	assertEquals(NUM - 10, set.size());	Object[] array2 = set.toArray();	assertEquals(NUM - 10, array2.length);	for (int i = 0; i < array2.length; i++) {	assertTrue(sub2.contains(array2[i]));	}	
test other done 

========================= hadoop sample_7146 =========================

nodeFiles = LogAggregationUtils .getRemoteNodeFileList(conf, appId, appOwner, this.fileController.getRemoteRootLogDir(), this.fileController.getRemoteRootLogDirSuffix());	} catch(Exception ex) {	html.h1("Unable to locate any logs for container " + containerId.toString());	LOG.error(ex.getMessage());	return;	}	Map<String, Long> checkSumFiles;	try {	checkSumFiles = fileController.parseCheckSumFiles(nodeFiles);	} catch (IOException ex) {	
error getting logs for 

try {	checkSumFiles = fileController.parseCheckSumFiles(nodeFiles);	} catch (IOException ex) {	html.h1("Error getting logs for " + logEntity);	return;	}	List<FileStatus> fileToRead;	try {	fileToRead = fileController.getNodeLogFileToRead(nodeFiles, nodeId.toString(), appId);	} catch (IOException ex) {	
error getting logs for 

for (FileStatus thisNodeFile : fileToRead) {	Long checkSumIndex = checkSumFiles.get( thisNodeFile.getPath().getName());	long endIndex = -1;	if (checkSumIndex != null) {	endIndex = checkSumIndex.longValue();	}	IndexedLogsMeta indexedLogsMeta = null;	try {	indexedLogsMeta = fileController.loadIndexedLogsMeta( thisNodeFile.getPath(), endIndex);	} catch (Exception ex) {	
can not load log meta from the log file 

continue;	}	if (indexedLogsMeta == null) {	continue;	}	Map<ApplicationAccessType, String> appAcls = indexedLogsMeta.getAcls();	String user = indexedLogsMeta.getUser();	String remoteUser = request().getRemoteUser();	if (!checkAcls(conf, appId, user, appAcls, remoteUser)) {	html.h1()._("User [" + remoteUser + "] is not authorized to view the logs for " + logEntity + " in log file [" + thisNodeFile.getPath().getName() + "]") ._();	
user is not authorized to view the logs for 

int currentToRead = toRead > bufferSize ? bufferSize : (int) toRead;	PRE<Hamlet> pre = html.pre();	while (toRead > 0 && (len = in.read(cbuf, 0, currentToRead)) > 0) {	pre._(new String(cbuf, 0, len, Charset.forName("UTF-8")));	toRead = toRead - len;	currentToRead = toRead > bufferSize ? bufferSize : (int) toRead;	}	pre._();	foundLog = true;	} catch (Exception ex) {	
error getting logs for 

if (desiredLogType.isEmpty()) {	html.h1("No logs available for container " + containerId.toString());	} else {	html.h1("Unable to locate '" + desiredLogType + "' log for container " + containerId.toString());	}	}	} catch (RuntimeException e) {	throw e;	} catch (Exception ex) {	html.h1()._("Error getting logs for " + logEntity)._();	
error getting logs for 

========================= hadoop sample_2344 =========================

public AdminACLsManager(Configuration conf) {	this.adminAcl = new AccessControlList(conf.get( YarnConfiguration.YARN_ADMIN_ACL, YarnConfiguration.DEFAULT_YARN_ADMIN_ACL));	try {	owner = UserGroupInformation.getCurrentUser();	adminAcl.addUser(owner.getShortUserName());	} catch (IOException e){	
could not add current user to admin 

========================= hadoop sample_2201 =========================

private static DataChecksum.Type getChecksumType(Configuration conf) {	final String checksum = conf.get( DFS_CHECKSUM_TYPE_KEY, DFS_CHECKSUM_TYPE_DEFAULT);	try {	return DataChecksum.Type.valueOf(checksum);	} catch(IllegalArgumentException iae) {	
bad checksum type using default 

========================= hadoop sample_7003 =========================

long numOperations = getOperationCount();	S3AFileSystem fs = getFileSystem();	NanoTimer timer = new NanoTimer();	for (int i = 0; i < numOperations; i++) {	Path p = getPathIteration(i, PATH_DEPTH);	OutputStream out = fs.create(p);	out.write(40);	out.close();	}	timer.end("Time to create %d files of depth %d", getOperationCount(), PATH_DEPTH);	
time per create msec 

========================= hadoop sample_5897 =========================

public void testFstat() throws Exception {	FileOutputStream fos = new FileOutputStream( new File(TEST_DIR, "testfstat"));	NativeIO.POSIX.Stat stat = NativeIO.POSIX.getFstat(fos.getFD());	fos.close();	
stat 

public void testFstatClosedFd() throws Exception {	FileOutputStream fos = new FileOutputStream( new File(TEST_DIR, "testfstat2"));	fos.close();	try {	NativeIO.POSIX.Stat stat = NativeIO.POSIX.getFstat(fos.getFD());	} catch (NativeIOException nioe) {	
got expected exception 

public void testSetFilePointer() throws Exception {	if (!Path.WINDOWS) {	return;	}	
set a file pointer on windows 

public void testCreateFile() throws Exception {	if (!Path.WINDOWS) {	return;	}	
open a file on windows with share delete shared mode 

public void testOpenMissingWithoutCreate() throws Exception {	if (Path.WINDOWS) {	return;	}	
open a missing file without o creat and it should fail 

public void testOpenMissingWithoutCreate() throws Exception {	if (Path.WINDOWS) {	return;	}	try {	FileDescriptor fd = NativeIO.POSIX.open( new File(TEST_DIR, "doesntexist").getAbsolutePath(), O_WRONLY, 0700);	fail("Able to open a new file without O_CREAT");	} catch (NativeIOException nioe) {	
got expected exception 

public void testOpenWithCreate() throws Exception {	if (Path.WINDOWS) {	return;	}	
test creating a file with o creat 

if (Path.WINDOWS) {	return;	}	FileDescriptor fd = NativeIO.POSIX.open( new File(TEST_DIR, "testWorkingOpen").getAbsolutePath(), O_WRONLY | O_CREAT, 0700);	assertNotNull(true);	assertTrue(fd.valid());	FileOutputStream fos = new FileOutputStream(fd);	fos.write("foo".getBytes());	fos.close();	assertFalse(fd.valid());	
test exclusive create 

assertNotNull(true);	assertTrue(fd.valid());	FileOutputStream fos = new FileOutputStream(fd);	fos.write("foo".getBytes());	fos.close();	assertFalse(fd.valid());	try {	fd = NativeIO.POSIX.open( new File(TEST_DIR, "testWorkingOpen").getAbsolutePath(), O_WRONLY | O_CREAT | O_EXCL, 0700);	fail("Was able to create existing file with O_EXCL");	} catch (NativeIOException nioe) {	
got expected exception for failed exclusive create 

========================= hadoop sample_3040 =========================

public static CleanerTask create(Configuration conf, SCMStore store, CleanerMetrics metrics, Lock cleanerTaskLock) {	try {	String location = conf.get(YarnConfiguration.SHARED_CACHE_ROOT, YarnConfiguration.DEFAULT_SHARED_CACHE_ROOT);	long sleepTime = conf.getLong(YarnConfiguration.SCM_CLEANER_RESOURCE_SLEEP_MS, YarnConfiguration.DEFAULT_SCM_CLEANER_RESOURCE_SLEEP_MS);	int nestedLevel = SharedCacheUtil.getCacheDepth(conf);	FileSystem fs = FileSystem.get(conf);	return new CleanerTask(location, sleepTime, nestedLevel, fs, store, metrics, cleanerTaskLock);	} catch (IOException e) {	
unable to obtain the filesystem for the cleaner service 

public void run() {	if (!this.cleanerTaskLock.tryLock()) {	
a cleaner task is already running this scheduled cleaner task will do nothing 

public void run() {	if (!this.cleanerTaskLock.tryLock()) {	return;	}	try {	if (!fs.exists(root)) {	
the shared cache root was not found the cleaner task will do nothing 

public void run() {	if (!this.cleanerTaskLock.tryLock()) {	return;	}	try {	if (!fs.exists(root)) {	return;	}	process();	} catch (Throwable e) {	
unexpected exception while initializing the cleaner task this task will do nothing 

private boolean removeResourceFromCacheFileSystem(Path path) throws IOException {	Path renamedPath = new Path(path.toString() + RENAMED_SUFFIX);	if (fs.rename(path, renamedPath)) {	
deleting 

private boolean removeResourceFromCacheFileSystem(Path path) throws IOException {	Path renamedPath = new Path(path.toString() + RENAMED_SUFFIX);	if (fs.rename(path, renamedPath)) {	return fs.delete(renamedPath, true);	} else {	
we were not able to rename the directory to we will leave it intact 

========================= hadoop sample_399 =========================

public static void setup() throws InterruptedException, IOException {	
starting up yarn cluster 

public static void setup() throws InterruptedException, IOException {	conf.setInt(YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_MB, 128);	if (yarnCluster == null) {	yarnCluster = new MiniYARNCluster( TestUnmanagedAMLauncher.class.getSimpleName(), 1, 1, 1);	yarnCluster.init(conf);	yarnCluster.start();	Configuration yarnClusterConfig = yarnCluster.getConfig();	
miniyarn resourcemanager published address 

public static void setup() throws InterruptedException, IOException {	conf.setInt(YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_MB, 128);	if (yarnCluster == null) {	yarnCluster = new MiniYARNCluster( TestUnmanagedAMLauncher.class.getSimpleName(), 1, 1, 1);	yarnCluster.init(conf);	yarnCluster.start();	Configuration yarnClusterConfig = yarnCluster.getConfig();	
miniyarn resourcemanager published web address 

public static void setup() throws InterruptedException, IOException {	conf.setInt(YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_MB, 128);	if (yarnCluster == null) {	yarnCluster = new MiniYARNCluster( TestUnmanagedAMLauncher.class.getSimpleName(), 1, 1, 1);	yarnCluster.init(conf);	yarnCluster.start();	Configuration yarnClusterConfig = yarnCluster.getConfig();	String webapp = yarnClusterConfig.get(YarnConfiguration.RM_WEBAPP_ADDRESS);	assertTrue("Web app address still unbound to a host at " + webapp, !webapp.startsWith("0.0.0.0"));	
yarn webapp is at 

private static String getTestRuntimeClasspath() {	
trying to generate classpath for app master from current thread s classpath 

public void testUMALauncher() throws Exception {	String classpath = getTestRuntimeClasspath();	String javaHome = System.getenv("JAVA_HOME");	if (javaHome == null) {	
java home not defined test not running 

public void testUMALauncher() throws Exception {	String classpath = getTestRuntimeClasspath();	String javaHome = System.getenv("JAVA_HOME");	if (javaHome == null) {	return;	}	String[] args = {	"--classpath", classpath, "--queue", "default", "--cmd", javaHome + "/bin/java -Xmx512m " + TestUnmanagedAMLauncher.class.getCanonicalName() + " success" };	
initializing launcher 

"--classpath", classpath, "--queue", "default", "--cmd", javaHome + "/bin/java -Xmx512m " + TestUnmanagedAMLauncher.class.getCanonicalName() + " success" };	UnmanagedAMLauncher launcher = new UnmanagedAMLauncher(new Configuration(yarnCluster.getConfig())) {	public void launchAM(ApplicationAttemptId attemptId) throws IOException, YarnException {	YarnApplicationAttemptState attemptState = rmClient.getApplicationAttemptReport(attemptId) .getYarnApplicationAttemptState();	Assert.assertTrue(attemptState .equals(YarnApplicationAttemptState.LAUNCHED));	super.launchAM(attemptId);	}	};	boolean initSuccess = launcher.init(args);	Assert.assertTrue(initSuccess);	
running launcher 

public void testUMALauncherError() throws Exception {	String classpath = getTestRuntimeClasspath();	String javaHome = System.getenv("JAVA_HOME");	if (javaHome == null) {	
java home not defined test not running 

public void testUMALauncherError() throws Exception {	String classpath = getTestRuntimeClasspath();	String javaHome = System.getenv("JAVA_HOME");	if (javaHome == null) {	return;	}	String[] args = {	"--classpath", classpath, "--queue", "default", "--cmd", javaHome + "/bin/java -Xmx512m " + TestUnmanagedAMLauncher.class.getCanonicalName() + " failure" };	
initializing launcher 

String classpath = getTestRuntimeClasspath();	String javaHome = System.getenv("JAVA_HOME");	if (javaHome == null) {	return;	}	String[] args = {	"--classpath", classpath, "--queue", "default", "--cmd", javaHome + "/bin/java -Xmx512m " + TestUnmanagedAMLauncher.class.getCanonicalName() + " failure" };	UnmanagedAMLauncher launcher = new UnmanagedAMLauncher(new Configuration( yarnCluster.getConfig()));	boolean initSuccess = launcher.init(args);	Assert.assertTrue(initSuccess);	
running launcher 

========================= hadoop sample_13 =========================

public void initializeApplication(ApplicationInitializationContext context) {	String user = context.getUser();	ApplicationId appId = context.getApplicationId();	ByteBuffer secret = context.getApplicationDataForService();	try {	Token<JobTokenIdentifier> jt = deserializeServiceData(secret);	JobID jobId = new JobID(Long.toString(appId.getClusterTimestamp()), appId.getId());	recordJobShuffleInfo(jobId, user, jt);	} catch (IOException e) {	
error during initapp 

public void stopApplication(ApplicationTerminationContext context) {	ApplicationId appId = context.getApplicationId();	JobID jobId = new JobID(Long.toString(appId.getClusterTimestamp()), appId.getId());	try {	removeJobShuffleInfo(jobId);	} catch (IOException e) {	
error during stopapp 

}	bootstrap.setOption("backlog", conf.getInt(SHUFFLE_LISTEN_QUEUE_SIZE, DEFAULT_SHUFFLE_LISTEN_QUEUE_SIZE));	bootstrap.setOption("child.keepAlive", true);	bootstrap.setPipelineFactory(pipelineFact);	port = conf.getInt(SHUFFLE_PORT_CONFIG_KEY, DEFAULT_SHUFFLE_PORT);	Channel ch = bootstrap.bind(new InetSocketAddress(port));	accepted.add(ch);	port = ((InetSocketAddress)ch.getLocalAddress()).getPort();	conf.set(SHUFFLE_PORT_CONFIG_KEY, Integer.toString(port));	pipelineFact.SHUFFLE.setPort(port);	
listening on port 

public synchronized ByteBuffer getMetaData() {	try {	return serializeMetaData(port);	} catch (IOException e) {	
error during getmeta 

private void startStore(Path recoveryRoot) throws IOException {	Options options = new Options();	options.createIfMissing(false);	options.logger(new LevelDBLogger());	Path dbPath = new Path(recoveryRoot, STATE_DB_NAME);	
using state database at for recovery 

private void startStore(Path recoveryRoot) throws IOException {	Options options = new Options();	options.createIfMissing(false);	options.logger(new LevelDBLogger());	Path dbPath = new Path(recoveryRoot, STATE_DB_NAME);	File dbfile = new File(dbPath.toString());	try {	stateDb = JniDBFactory.factory.open(dbfile, options);	} catch (NativeDB.DBException e) {	if (e.isNotFound() || e.getMessage().contains(" does not exist ")) {	
creating state database at 

private void checkVersion() throws IOException {	Version loadedVersion = loadVersion();	
loaded state db schema version info 

private void checkVersion() throws IOException {	Version loadedVersion = loadVersion();	if (loadedVersion.equals(getCurrentVersion())) {	return;	}	if (loadedVersion.isCompatibleTo(getCurrentVersion())) {	
storing state db schema version info 

private void addJobToken(JobID jobId, String user, Token<JobTokenIdentifier> jobToken) {	userRsrc.put(jobId.toString(), user);	secretManager.addTokenForJob(jobId.toString(), jobToken);	
added token for 

public HttpPipelineFactory(Configuration conf, Timer timer) throws Exception {	SHUFFLE = getShuffle(conf);	if (conf.getBoolean(MRConfig.SHUFFLE_SSL_ENABLED_KEY, MRConfig.SHUFFLE_SSL_ENABLED_DEFAULT)) {	
encrypted shuffle is enabled 

public AttemptPathInfo load(AttemptPathIdentifier key) throws Exception {	String base = getBaseLocation(key.jobId, key.user);	String attemptBase = base + key.attemptId;	Path indexFileName = getAuxiliaryLocalPathHandler() .getLocalPathForRead(attemptBase + "/" + INDEX_FILE_NAME);	Path mapOutputFileName = getAuxiliaryLocalPathHandler() .getLocalPathForRead(attemptBase + "/" + DATA_FILE_NAME);	if (LOG.isDebugEnabled()) {	
loaded via loader 

public void channelOpen(ChannelHandlerContext ctx, ChannelStateEvent evt) throws Exception {	if ((maxShuffleConnections > 0) && (accepted.size() >= maxShuffleConnections)) {	
current number of shuffle connections d is greater than or equal to the max allowed shuffle connections d 

}	if (!ShuffleHeader.DEFAULT_HTTP_HEADER_NAME.equals( request.getHeader(ShuffleHeader.HTTP_HEADER_NAME)) || !ShuffleHeader.DEFAULT_HTTP_HEADER_VERSION.equals( request.getHeader(ShuffleHeader.HTTP_HEADER_VERSION))) {	sendError(ctx, "Incompatible shuffle request version", BAD_REQUEST);	}	final Map<String,List<String>> q = new QueryStringDecoder(request.getUri()).getParameters();	final List<String> keepAliveList = q.get("keepAlive");	boolean keepAliveParam = false;	if (keepAliveList != null && keepAliveList.size() == 1) {	keepAliveParam = Boolean.valueOf(keepAliveList.get(0));	if (LOG.isDebugEnabled()) {	
keepaliveparam 

boolean keepAliveParam = false;	if (keepAliveList != null && keepAliveList.size() == 1) {	keepAliveParam = Boolean.valueOf(keepAliveList.get(0));	if (LOG.isDebugEnabled()) {	}	}	final List<String> mapIds = splitMaps(q.get("map"));	final List<String> reduceQ = q.get("reduce");	final List<String> jobQ = q.get("job");	if (LOG.isDebugEnabled()) {	
recv mapid reduceid jobid keepalive 

}	final String reqUri = request.getUri();	if (null == reqUri) {	sendError(ctx, FORBIDDEN);	return;	}	HttpResponse response = new DefaultHttpResponse(HTTP_1_1, OK);	try {	verifyRequest(jobId, ctx, request, response, new URL("http", "", this.port, reqUri));	} catch (IOException e) {	
shuffle failure 

Map<String, MapOutputInfo> mapOutputInfoMap = new HashMap<String, MapOutputInfo>();	Channel ch = evt.getChannel();	ChannelPipeline pipeline = ch.getPipeline();	TimeoutHandler timeoutHandler = (TimeoutHandler)pipeline.get(TIMEOUT_HANDLER);	timeoutHandler.setEnabledTimeout(false);	String user = userRsrc.get(jobId);	try {	populateHeaders(mapIds, jobId, user, reduceId, request, response, keepAliveParam, mapOutputInfoMap);	} catch(IOException e) {	ch.write(response);	
shuffle error in populating headers 

info = getMapOutputInfo(mapId, reduceContext.getReduceId(), reduceContext.getJobId(), reduceContext.getUser());	}	nextMap = sendMapOutput( reduceContext.getCtx(), reduceContext.getCtx().getChannel(), reduceContext.getUser(), mapId, reduceContext.getReduceId(), info);	if (null == nextMap) {	sendError(reduceContext.getCtx(), NOT_FOUND);	return null;	}	nextMap.addListener(new ReduceMapFileCount(reduceContext));	} catch (IOException e) {	if (e instanceof DiskChecker.DiskErrorException) {	
shuffle error 

}	nextMap = sendMapOutput( reduceContext.getCtx(), reduceContext.getCtx().getChannel(), reduceContext.getUser(), mapId, reduceContext.getReduceId(), info);	if (null == nextMap) {	sendError(reduceContext.getCtx(), NOT_FOUND);	return null;	}	nextMap.addListener(new ReduceMapFileCount(reduceContext));	} catch (IOException e) {	if (e instanceof DiskChecker.DiskErrorException) {	} else {	
shuffle error 

protected MapOutputInfo getMapOutputInfo(String mapId, int reduce, String jobId, String user) throws IOException {	AttemptPathInfo pathInfo;	try {	AttemptPathIdentifier identifier = new AttemptPathIdentifier( jobId, user, mapId);	pathInfo = pathCache.get(identifier);	if (LOG.isDebugEnabled()) {	
retrieved pathinfo for check for corresponding loaded messages to determine whether it was loaded or cached 

protected void setResponseHeaders(HttpResponse response, boolean keepAliveParam, long contentLength) {	if (!connectionKeepAliveEnabled && !keepAliveParam) {	if (LOG.isDebugEnabled()) {	
setting connection close header 

protected void setResponseHeaders(HttpResponse response, boolean keepAliveParam, long contentLength) {	if (!connectionKeepAliveEnabled && !keepAliveParam) {	if (LOG.isDebugEnabled()) {	}	response.setHeader(HttpHeaders.CONNECTION, CONNECTION_CLOSE);	} else {	response.setHeader(HttpHeaders.CONTENT_LENGTH, String.valueOf(contentLength));	response.setHeader(HttpHeaders.CONNECTION, HttpHeaders.KEEP_ALIVE);	response.setHeader(HttpHeaders.KEEP_ALIVE, "timeout=" + connectionKeepAliveTimeOut);	
content length in shuffle 

protected void verifyRequest(String appid, ChannelHandlerContext ctx, HttpRequest request, HttpResponse response, URL requestUri) throws IOException {	SecretKey tokenSecret = secretManager.retrieveTokenSecret(appid);	if (null == tokenSecret) {	
request for unknown token 

protected void verifyRequest(String appid, ChannelHandlerContext ctx, HttpRequest request, HttpResponse response, URL requestUri) throws IOException {	SecretKey tokenSecret = secretManager.retrieveTokenSecret(appid);	if (null == tokenSecret) {	throw new IOException("could not find jobid");	}	String enc_str = SecureShuffleUtils.buildMsgFrom(requestUri);	String urlHashStr = request.getHeader(SecureShuffleUtils.HTTP_HEADER_URL_HASH);	if (urlHashStr == null) {	
missing header hash for 

final IndexRecord info = mapOutputInfo.indexRecord;	final ShuffleHeader header = new ShuffleHeader(mapId, info.partLength, info.rawLength, reduce);	final DataOutputBuffer dob = new DataOutputBuffer();	header.write(dob);	ch.write(wrappedBuffer(dob.getData(), 0, dob.getLength()));	final File spillfile = new File(mapOutputInfo.mapOutputFileName.toString());	RandomAccessFile spill;	try {	spill = SecureIOUtils.openForRandomRead(spillfile, "r", user, null);	} catch (FileNotFoundException e) {	
not found 

public void exceptionCaught(ChannelHandlerContext ctx, ExceptionEvent e) throws Exception {	Channel ch = e.getChannel();	Throwable cause = e.getCause();	if (cause instanceof TooLongFrameException) {	sendError(ctx, BAD_REQUEST);	return;	} else if (cause instanceof IOException) {	if (cause instanceof ClosedChannelException) {	
ignoring closed channel error 

Throwable cause = e.getCause();	if (cause instanceof TooLongFrameException) {	sendError(ctx, BAD_REQUEST);	return;	} else if (cause instanceof IOException) {	if (cause instanceof ClosedChannelException) {	return;	}	String message = String.valueOf(cause.getMessage());	if (IGNORABLE_ERROR_MESSAGE.matcher(message).matches()) {	
ignoring client socket close 

return;	} else if (cause instanceof IOException) {	if (cause instanceof ClosedChannelException) {	return;	}	String message = String.valueOf(cause.getMessage());	if (IGNORABLE_ERROR_MESSAGE.matcher(message).matches()) {	return;	}	}	
shuffle error 

} else if (cause instanceof IOException) {	if (cause instanceof ClosedChannelException) {	return;	}	String message = String.valueOf(cause.getMessage());	if (IGNORABLE_ERROR_MESSAGE.matcher(message).matches()) {	return;	}	}	if (ch.isConnected()) {	
shuffle error 

========================= hadoop sample_5317 =========================

final Path hexfile = new Path(conf.get(HEX_FILE_PROPERTY));	final OutputStream out = new BufferedOutputStream(fs.create(hexfile));	try {	for (byte b : hex) out.write(b);	} finally {	out.close();	}	}	if (conf.getInt(DIGIT_START_PROPERTY, 1) == 1) {	final Path outfile = new Path(dir, "pi.txt");	
writing text output to 

========================= hadoop sample_5747 =========================

protected OutputStream getOutputStreamForKeystore() throws IOException {	if (LOG.isDebugEnabled()) {	
using for output stream 

protected void initFileSystem(URI uri) throws IOException {	super.initFileSystem(uri);	try {	file = new File(new URI(getPath().toString()));	if (LOG.isDebugEnabled()) {	
initialized local file as 

protected void initFileSystem(URI uri) throws IOException {	super.initFileSystem(uri);	try {	file = new File(new URI(getPath().toString()));	if (LOG.isDebugEnabled()) {	if (file.exists()) {	
the local file exists and is size 

protected void initFileSystem(URI uri) throws IOException {	super.initFileSystem(uri);	try {	file = new File(new URI(getPath().toString()));	if (LOG.isDebugEnabled()) {	if (file.exists()) {	if (LOG.isTraceEnabled()) {	if (file.canRead()) {	
we can read the local file 

protected void initFileSystem(URI uri) throws IOException {	super.initFileSystem(uri);	try {	file = new File(new URI(getPath().toString()));	if (LOG.isDebugEnabled()) {	if (file.exists()) {	if (LOG.isTraceEnabled()) {	if (file.canRead()) {	}	if (file.canWrite()) {	
we can write the local file 

file = new File(new URI(getPath().toString()));	if (LOG.isDebugEnabled()) {	if (file.exists()) {	if (LOG.isTraceEnabled()) {	if (file.canRead()) {	}	if (file.canWrite()) {	}	}	} else {	
the local file does not exist 

public void flush() throws IOException {	super.flush();	if (LOG.isDebugEnabled()) {	
resetting permissions to 

========================= hadoop sample_3739 =========================

public static MetricsContext getContext(String refName, String contextName) {	MetricsContext metricsContext;	try {	metricsContext = ContextFactory.getFactory().getContext(refName, contextName);	if (!metricsContext.isMonitoring()) {	metricsContext.startMonitoring();	}	} catch (Exception ex) {	
unable to create metrics context 

private static String getHostName() {	String hostName = null;	try {	hostName = InetAddress.getLocalHost().getHostName();	}	catch (UnknownHostException ex) {	
unable to obtain hostname 

========================= hadoop sample_3369 =========================

public static Configuration getTimelineServiceHBaseConf(Configuration conf) throws MalformedURLException {	if (conf == null) {	throw new NullPointerException();	}	Configuration hbaseConf;	String timelineServiceHBaseConfFileURL = conf.get(YarnConfiguration.TIMELINE_SERVICE_HBASE_CONFIGURATION_FILE);	if (timelineServiceHBaseConfFileURL != null && timelineServiceHBaseConfFileURL.length() > 0) {	
using hbase configuration at 

========================= hadoop sample_1211 =========================

}	};	jobHistoryServer.init(conf);	jobHistoryServer.start();	final MRClientProtocol hsService = jobHistoryServer.getClientService() .getClientHandler();	UserGroupInformation loggedInUser = UserGroupInformation .createRemoteUser("testrenewer@APACHE.ORG");	Assert.assertEquals("testrenewer", loggedInUser.getShortUserName());	loggedInUser.setAuthenticationMethod(AuthenticationMethod.KERBEROS);	Token token = getDelegationToken(loggedInUser, hsService, loggedInUser.getShortUserName());	tokenFetchTime = System.currentTimeMillis();	
got delegation token at 

try {	clientUsingDT.getJobReport(jobReportRequest);	} catch (IOException e) {	Assert.assertEquals("Unknown job job_123456_0001", e.getMessage());	}	while(System.currentTimeMillis() < tokenFetchTime + initialInterval / 2) {	Thread.sleep(500l);	}	long nextExpTime = renewDelegationToken(loggedInUser, hsService, token);	long renewalTime = System.currentTimeMillis();	
renewed token at nextexpirytime 

Thread.sleep(50l);	try {	clientUsingDT.getJobReport(jobReportRequest);	} catch (IOException e) {	Assert.assertEquals("Unknown job job_123456_0001", e.getMessage());	}	while(System.currentTimeMillis() < renewalTime + renewInterval) {	Thread.sleep(500l);	}	Thread.sleep(50l);	
at time token should be invalid 

clientUsingDT.getJobReport(jobReportRequest);	fail("Should not have succeeded with an expired token");	} catch (IOException e) {	assertTrue(e.getCause().getMessage().contains("is expired"));	}	if (clientUsingDT != null) {	clientUsingDT = null;	}	token = getDelegationToken(loggedInUser, hsService, loggedInUser.getShortUserName());	tokenFetchTime = System.currentTimeMillis();	
got delegation token at 

} catch (IOException e) {	fail("Unexpected exception" + e);	}	cancelDelegationToken(loggedInUser, hsService, token);	Token tokenWithDifferentRenewer = getDelegationToken(loggedInUser, hsService, "yarn");	cancelDelegationToken(loggedInUser, hsService, tokenWithDifferentRenewer);	if (clientUsingDT != null) {	clientUsingDT = null;	}	clientUsingDT = getMRClientProtocol(token, jobHistoryServer .getClientService().getBindAddress(), "loginuser2", conf);	
cancelled delegation token at 

========================= hadoop sample_5602 =========================

try {	uri = Util.fileAsURI(new File(uri));	Iterator<StorageDirectory> it = dirIterator();	for (; it.hasNext(); ) {	StorageDirectory sd = it.next();	if (Util.fileAsURI(sd.getRoot()).equals(uri)) {	return sd;	}	}	} catch (IOException ioe) {	
error converting file to uri 

public void writeTransactionIdFileToStorage(long txid, NameNodeDirType type) {	for (Iterator<StorageDirectory> it = dirIterator(type); it.hasNext();) {	StorageDirectory sd = it.next();	try {	writeTransactionIdFile(sd, txid);	} catch(IOException e) {	
writetransactionidtostorage failed on 

private void format(StorageDirectory sd) throws IOException {	sd.clearDirectory();	writeProperties(sd);	writeTransactionIdFile(sd, 0);	
storage directory has been successfully formatted 

private void reportErrorsOnDirectory(StorageDirectory sd) {	
error reported on storage directory 

private void reportErrorsOnDirectory(StorageDirectory sd) {	String lsd = listStorageDirectories();	
current list of storage dirs 

private void reportErrorsOnDirectory(StorageDirectory sd) {	String lsd = listStorageDirectories();	
about to remove corresponding storage 

private void reportErrorsOnDirectory(StorageDirectory sd) {	String lsd = listStorageDirectories();	try {	sd.unlock();	} catch (Exception e) {	
unable to unlock bad storage directory 

private void reportErrorsOnDirectory(StorageDirectory sd) {	String lsd = listStorageDirectories();	try {	sd.unlock();	} catch (Exception e) {	}	if (getStorageDirs().remove(sd)) {	this.removedStorageDirs.add(sd);	}	lsd = listStorageDirectories();	
at the end current list of storage dirs 

String cid = null;	Iterator<StorageDirectory> sdit = dirIterator(NameNodeDirType.IMAGE);	while(sdit.hasNext()) {	StorageDirectory sd = sdit.next();	try {	Properties props = readPropertiesFile(sd.getVersionFile());	cid = props.getProperty("clusterID");	LOG.info("current cluster id for sd="+sd.getCurrentDir() + ";lv=" + layoutVersion + ";cid=" + cid);	if(cid != null && !cid.equals("")) return cid;	} catch (Exception e) {	
this sd not available 

while(sdit.hasNext()) {	StorageDirectory sd = sdit.next();	try {	Properties props = readPropertiesFile(sd.getVersionFile());	cid = props.getProperty("clusterID");	LOG.info("current cluster id for sd="+sd.getCurrentDir() + ";lv=" + layoutVersion + ";cid=" + cid);	if(cid != null && !cid.equals("")) return cid;	} catch (Exception e) {	}	}	
couldn t find any version file containing valid clusterid 

static String newBlockPoolID() throws UnknownHostException{	String ip = "unknownIP";	try {	ip = DNS.getDefaultIP("default");	} catch (UnknownHostException e) {	
could not find ip address of inteface 

public void writeAll() throws IOException {	this.layoutVersion = getServiceLayoutVersion();	for (StorageDirectory sd : getStorageDirs()) {	try {	writeProperties(sd);	} catch (Exception e) {	
error during write properties to the version file to 

========================= hadoop sample_8030 =========================

public void setUp() throws Exception {	
setting up the directory structures 

public void tearDown() throws Exception {	
shutting down minidfscluster 

========================= hadoop sample_7582 =========================

private void setupQueueConfiguration(CapacitySchedulerConfiguration conf) {	conf.setQueues(CapacitySchedulerConfiguration.ROOT, new String[] {"a", "b", "c"});	final String A = CapacitySchedulerConfiguration.ROOT + ".a";	conf.setCapacity(A, 10);	conf.setMaximumCapacity(A, 15);	final String B = CapacitySchedulerConfiguration.ROOT + ".b";	conf.setCapacity(B, 20);	final String C = CapacitySchedulerConfiguration.ROOT + ".c";	conf.setCapacity(C, 70);	conf.setMaximumCapacity(C, 70);	
setup top level queues 

final String C4 = C + ".c4";	conf.setQueues(C, new String[] {"c1", "c2", "c3", "c4"});	conf.setCapacity(C1, 50);	conf.setMaximumCapacity(C1, 55);	conf.setCapacity(C2, 10);	conf.setMaximumCapacity(C2, 25);	conf.setCapacity(C3, 35);	conf.setMaximumCapacity(C3, 38);	conf.setCapacity(C4, 5);	conf.setMaximumCapacity(C4, 5);	
setup level queues 

final String C11 = C1 + ".c11";	final String C12 = C1 + ".c12";	final String C13 = C1 + ".c13";	conf.setQueues(C1, new String[] {"c11", "c12", "c13"});	conf.setCapacity(C11, 15);	conf.setMaximumCapacity(C11, 30);	conf.setCapacity(C12, 45);	conf.setMaximumCapacity(C12, 70);	conf.setCapacity(C13, 40);	conf.setMaximumCapacity(C13, 40);	
setup level queues 

private void setupQueueConfigurationWithoutLabels(CapacitySchedulerConfiguration conf) {	conf.setQueues(CapacitySchedulerConfiguration.ROOT, new String[] {"a", "b"});	final String A = CapacitySchedulerConfiguration.ROOT + ".a";	conf.setCapacity(A, 10);	conf.setMaximumCapacity(A, 15);	final String B = CapacitySchedulerConfiguration.ROOT + ".b";	conf.setCapacity(B, 90);	
setup top level queues 

private void setupQueueConfigurationWithLabels(CapacitySchedulerConfiguration conf) {	conf.setQueues(CapacitySchedulerConfiguration.ROOT, new String[] {"a", "b"});	conf.setCapacityByLabel(CapacitySchedulerConfiguration.ROOT, "red", 100);	conf.setCapacityByLabel(CapacitySchedulerConfiguration.ROOT, "blue", 100);	final String A = CapacitySchedulerConfiguration.ROOT + ".a";	conf.setCapacity(A, 10);	conf.setMaximumCapacity(A, 15);	final String B = CapacitySchedulerConfiguration.ROOT + ".b";	conf.setCapacity(B, 90);	
setup top level queues 

========================= hadoop sample_531 =========================

private void compareFiles(FileSystem fs, FileStatus srcStat, FileStatus dstStat) throws Exception {	
comparing and 

while (true) {	if (srcBytesRead == 0) {	srcBytesRead = srcIn.read(readSrc);	srcIdx = 0;	}	if (tgtBytesRead == 0) {	tgtBytesRead = dstIn.read(readDst);	tgtIdx = 0;	}	if (srcBytesRead == 0 || tgtBytesRead == 0) {	
compared src and dst files for bytes content match 

break;	}	}	for (; srcIdx < srcBytesRead && tgtIdx < tgtBytesRead;	++srcIdx, ++tgtIdx) {	if (readSrc[srcIdx] != readDst[tgtIdx]) {	Assert.fail("src and dst file does not match at " + totalComparedBytes + " between " + srcStat + " and " + dstStat);	}	++totalComparedBytes;	}	
compared src and dst files for bytes content match filelength 

}	}	for (; srcIdx < srcBytesRead && tgtIdx < tgtBytesRead;	++srcIdx, ++tgtIdx) {	if (readSrc[srcIdx] != readDst[tgtIdx]) {	Assert.fail("src and dst file does not match at " + totalComparedBytes + " between " + srcStat + " and " + dstStat);	}	++totalComparedBytes;	}	if (totalComparedBytes == srcStat.getLen()) {	
final 

private void copyAndVerify(final DistributedFileSystem fs, final FileEntry[] srcFiles, final FileStatus[] srcStats, final String testDst, final String[] args) throws Exception {	final String testRoot = "/testdir";	FsShell shell = new FsShell(fs.getConf());	
ls before distcp 

private void copyAndVerify(final DistributedFileSystem fs, final FileEntry[] srcFiles, final FileStatus[] srcStats, final String testDst, final String[] args) throws Exception {	final String testRoot = "/testdir";	FsShell shell = new FsShell(fs.getConf());	
lsr 

private void copyAndVerify(final DistributedFileSystem fs, final FileEntry[] srcFiles, final FileStatus[] srcStats, final String testDst, final String[] args) throws Exception {	final String testRoot = "/testdir";	FsShell shell = new FsShell(fs.getConf());	
running distcp 

private void copyAndVerify(final DistributedFileSystem fs, final FileEntry[] srcFiles, final FileStatus[] srcStats, final String testDst, final String[] args) throws Exception {	final String testRoot = "/testdir";	FsShell shell = new FsShell(fs.getConf());	ToolRunner.run(conf, new DistCp(), args);	
ls after distcp 

private void copyAndVerify(final DistributedFileSystem fs, final FileEntry[] srcFiles, final FileStatus[] srcStats, final String testDst, final String[] args) throws Exception {	final String testRoot = "/testdir";	FsShell shell = new FsShell(fs.getConf());	ToolRunner.run(conf, new DistCp(), args);	
lsr 

createFiles(fs, testRoot, srcFiles, chunkSize);	FileStatus[] srcStats = getFileStatus(fs, testRoot, srcFiles);	for (int i = 0; i < srcFiles.length; i++) {	fs.setOwner(srcStats[i].getPath(), "u" + i,  "g" + i);	}	srcStats = getFileStatus(fs, testRoot, srcFiles);	createDestDir(fs, testDst, srcStats, srcFiles);	String[] args = new String[] {"-pugp", "-blocksperchunk", String.valueOf(chunkSize), nnUri + testSrc, nnUri + testDst};	copyAndVerify(fs, srcFiles, srcStats, testDst, args);	copyAndVerify(fs, srcFiles, srcStats, testDst, args);	
modify a file and copy again 

for (int i = 0; i < srcFiles.length; i++) {	fs.setOwner(srcStats[i].getPath(), "u" + i,  "g" + i);	}	srcStats = getFileStatus(fs, testRoot, srcFiles);	createDestDir(fs, testDst, srcStats, srcFiles);	String[] args = new String[] {"-pugp", "-blocksperchunk", String.valueOf(chunkSize), nnUri + testSrc, nnUri + testDst};	copyAndVerify(fs, srcFiles, srcStats, testDst, args);	copyAndVerify(fs, srcFiles, srcStats, testDst, args);	for(int i=srcFiles.length-1; i >=0; --i) {	if (!srcFiles[i].isDirectory()) {	
modifying 

if(!cluster.getFileSystem().exists(srcfile)){	throw new Exception("src not exist");	}	final long srcLen = fs.getFileStatus(srcfile).getLen();	FileStatus[] srcstats = getFileStatus(fs, testRoot, srcfiles);	for (int i = 0; i < srcfiles.length; i++) {	fs.setOwner(srcstats[i].getPath(), "u" + i, null);	}	String[] args = new String[] {	"-blocksperchunk", String.valueOf(chunkSize), nnUri + testSrc, nnUri + testDst };	
running distcp 

========================= hadoop sample_6239 =========================

try {	callerUGI .doAs(new PrivilegedExceptionAction<SubmitApplicationResponse>() {	public SubmitApplicationResponse run() throws IOException, YarnException {	return rm.getClientRMService().submitApplication(req);	}	});	} catch (UndeclaredThrowableException ue) {	if (ue.getCause() instanceof YarnException) {	throw new BadRequestException(ue.getCause().getMessage());	}	
submit app request failed 

final String renewer = tokenData.getRenewer();	GetDelegationTokenResponse resp;	try {	resp = callerUGI .doAs(new PrivilegedExceptionAction<GetDelegationTokenResponse>() {	public GetDelegationTokenResponse run() throws IOException, YarnException {	GetDelegationTokenRequest createReq = GetDelegationTokenRequest.newInstance(renewer);	return rm.getClientRMService().getDelegationToken(createReq);	}	});	} catch (Exception e) {	
create delegation token request failed 

return rm.getClientRMService().renewDelegationToken(req);	}	});	} catch (UndeclaredThrowableException ue) {	if (ue.getCause() instanceof YarnException) {	if (ue.getCause().getCause() instanceof InvalidToken) {	throw new BadRequestException(ue.getCause().getCause().getMessage());	} else if (ue.getCause() .getCause() instanceof org.apache.hadoop.security.AccessControlException) {	return Response.status(Status.FORBIDDEN) .entity(ue.getCause().getCause().getMessage()).build();	}	
renew delegation token request failed 

});	} catch (UndeclaredThrowableException ue) {	if (ue.getCause() instanceof YarnException) {	if (ue.getCause().getCause() instanceof InvalidToken) {	throw new BadRequestException(ue.getCause().getCause().getMessage());	} else if (ue.getCause() .getCause() instanceof org.apache.hadoop.security.AccessControlException) {	return Response.status(Status.FORBIDDEN) .entity(ue.getCause().getCause().getMessage()).build();	}	throw ue;	}	
renew delegation token request failed 

if (ue.getCause() instanceof YarnException) {	if (ue.getCause().getCause() instanceof InvalidToken) {	throw new BadRequestException(ue.getCause().getCause().getMessage());	} else if (ue.getCause() .getCause() instanceof org.apache.hadoop.security.AccessControlException) {	return Response.status(Status.FORBIDDEN) .entity(ue.getCause().getCause().getMessage()).build();	}	throw ue;	}	throw ue;	} catch (Exception e) {	
renew delegation token request failed 

return rm.getClientRMService().cancelDelegationToken(req);	}	});	} catch (UndeclaredThrowableException ue) {	if (ue.getCause() instanceof YarnException) {	if (ue.getCause().getCause() instanceof InvalidToken) {	throw new BadRequestException(ue.getCause().getCause().getMessage());	} else if (ue.getCause() .getCause() instanceof org.apache.hadoop.security.AccessControlException) {	return Response.status(Status.FORBIDDEN) .entity(ue.getCause().getCause().getMessage()).build();	}	
renew delegation token request failed 

});	} catch (UndeclaredThrowableException ue) {	if (ue.getCause() instanceof YarnException) {	if (ue.getCause().getCause() instanceof InvalidToken) {	throw new BadRequestException(ue.getCause().getCause().getMessage());	} else if (ue.getCause() .getCause() instanceof org.apache.hadoop.security.AccessControlException) {	return Response.status(Status.FORBIDDEN) .entity(ue.getCause().getCause().getMessage()).build();	}	throw ue;	}	
renew delegation token request failed 

if (ue.getCause() instanceof YarnException) {	if (ue.getCause().getCause() instanceof InvalidToken) {	throw new BadRequestException(ue.getCause().getCause().getMessage());	} else if (ue.getCause() .getCause() instanceof org.apache.hadoop.security.AccessControlException) {	return Response.status(Status.FORBIDDEN) .entity(ue.getCause().getCause().getMessage()).build();	}	throw ue;	}	throw ue;	} catch (Exception e) {	
renew delegation token request failed 

try {	callerUGI .doAs(new PrivilegedExceptionAction<ReservationSubmissionResponse>() {	public ReservationSubmissionResponse run() throws IOException, YarnException {	return rm.getClientRMService().submitReservation(reservation);	}	});	} catch (UndeclaredThrowableException ue) {	if (ue.getCause() instanceof YarnException) {	throw new BadRequestException(ue.getCause().getMessage());	}	
submit reservation request failed 

resRespInfo = callerUGI .doAs(new PrivilegedExceptionAction<ReservationUpdateResponseInfo>() {	public ReservationUpdateResponseInfo run() throws IOException, YarnException {	rm.getClientRMService().updateReservation(reservation);	return new ReservationUpdateResponseInfo();	}	});	} catch (UndeclaredThrowableException ue) {	if (ue.getCause() instanceof YarnException) {	throw new BadRequestException(ue.getCause().getMessage());	}	
update reservation request failed 

resRespInfo = callerUGI .doAs(new PrivilegedExceptionAction<ReservationDeleteResponseInfo>() {	public ReservationDeleteResponseInfo run() throws IOException, YarnException {	rm.getClientRMService().deleteReservation(reservation);	return new ReservationDeleteResponseInfo();	}	});	} catch (UndeclaredThrowableException ue) {	if (ue.getCause() instanceof YarnException) {	throw new BadRequestException(ue.getCause().getMessage());	}	
update reservation request failed 

rm.getRMContext().getRMAdminService().refreshQueues();	} catch (IOException | YarnException e) {	provider.confirmPendingMutation(false);	throw e;	}	provider.confirmPendingMutation(true);	return null;	}	});	} catch (IOException e) {	
exception thrown when modifying configuration 

========================= hadoop sample_862 =========================

this.blobWrapper = blobWrapper;	CloudBlob blob = blobWrapper.getBlob();	while(leaseID == null) {	try {	leaseID = blob.acquireLease(LEASE_TIMEOUT, null);	} catch (StorageException e) {	if (throwIfPresent && e.getErrorCode().equals(LEASE_ALREADY_PRESENT)) {	throw e;	}	if (!LEASE_ALREADY_PRESENT.equals(e.getErrorCode())) {	
caught exception when trying to get lease on blob 

Thread.sleep(LEASE_ACQUIRE_RETRY_INTERVAL);	} catch (InterruptedException e) {	Thread.currentThread().interrupt();	}	}	}	renewer = new Thread(new Renewer());	renewer.setDaemon(true);	renewer.setName("AzureLeaseRenewer-" + threadNumber.getAndIncrement());	renewer.start();	
acquired lease on managed by thread 

public void free() throws StorageException {	AccessCondition accessCondition = AccessCondition.generateEmptyCondition();	accessCondition.setLeaseID(leaseID);	try {	blobWrapper.getBlob().releaseLease(accessCondition);	} catch (StorageException e) {	if ("BlobNotFound".equals(e.getErrorCode())) {	} else {	
unanticipated exception when trying to free lease on 

accessCondition.setLeaseID(leaseID);	try {	blobWrapper.getBlob().releaseLease(accessCondition);	} catch (StorageException e) {	if ("BlobNotFound".equals(e.getErrorCode())) {	} else {	throw(e);	}	} finally {	leaseFreed = true;	
freed lease on managed by thread 

public void run() {	
starting lease keep alive thread 

public void run() {	AccessCondition accessCondition = AccessCondition.generateEmptyCondition();	accessCondition.setLeaseID(leaseID);	while(!leaseFreed) {	try {	Thread.sleep(LEASE_RENEWAL_PERIOD);	} catch (InterruptedException e) {	
keep alive thread for lease interrupted 

accessCondition.setLeaseID(leaseID);	while(!leaseFreed) {	try {	Thread.sleep(LEASE_RENEWAL_PERIOD);	} catch (InterruptedException e) {	Thread.currentThread().interrupt();	}	try {	if (!leaseFreed) {	blobWrapper.getBlob().renewLease(accessCondition);	
renewed lease on 

} catch (InterruptedException e) {	Thread.currentThread().interrupt();	}	try {	if (!leaseFreed) {	blobWrapper.getBlob().renewLease(accessCondition);	}	} catch (StorageException e) {	if (!leaseFreed) {	leaseFreed = true;	
attempt to renew lease on failed but lease not yet freed reason 

========================= hadoop sample_6393 =========================

reader = new DeskewedJobTraceReader(new JobTraceReader(inPath, conf), skewBufferLength, !allowMissorting);	Path outPath = new Path(outputPathName);	outGen = new DefaultOutputter<LoggedJob>();	outGen.init(outPath, conf);	tempDir = tempDirName == null ? outPath.getParent() : new Path(tempDirName);	FileSystem fs = tempDir.getFileSystem(getConf());	if (!fs.getFileStatus(tempDir).isDirectory()) {	throw new IOException("Your temp directory is not a directory");	}	if (inputCycle <= 0) {	
you must have an input cycle length 

if (outputDuration <= 0) {	outputDuration = 60L * 60L * TICKS_PER_SECOND;	}	if (inputCycle <= 0) {	inputCycle = outputDuration;	}	timeDilation = (double) outputDuration / (double) inputCycle;	random = seeded ? new Random(randomSeed) : new Random();	if (debug) {	randomSeed = random.nextLong();	
this run effectively has a seed of 

public int compare(Pair<LoggedJob, JobTraceReader> p1, Pair<LoggedJob, JobTraceReader> p2) {	LoggedJob j1 = p1.first();	LoggedJob j2 = p2.first();	return (j1.getSubmitTime() < j2.getSubmitTime()) ? -1 : (j1 .getSubmitTime() == j2.getSubmitTime()) ? 0 : 1;	}	}	Queue<Pair<LoggedJob, JobTraceReader>> heap = new PriorityQueue<Pair<LoggedJob, JobTraceReader>>();	try {	LoggedJob job = reader.nextJob();	if (job == null) {	
the job trace is empty 

return (j1.getSubmitTime() < j2.getSubmitTime()) ? -1 : (j1 .getSubmitTime() == j2.getSubmitTime()) ? 0 : 1;	}	}	Queue<Pair<LoggedJob, JobTraceReader>> heap = new PriorityQueue<Pair<LoggedJob, JobTraceReader>>();	try {	LoggedJob job = reader.nextJob();	if (job == null) {	return EMPTY_JOB_TRACE;	}	if (startsAfter > 0) {	
starts after time is specified initial job submit time 

return EMPTY_JOB_TRACE;	}	if (startsAfter > 0) {	long approximateTime = job.getSubmitTime() + startsAfter;	job = reader.nextJob();	long skippedCount = 0;	while (job != null && job.getSubmitTime() < approximateTime) {	job = reader.nextJob();	skippedCount++;	}	
considering jobs with submit time greater than ms skipped jobs 

}	if (startsAfter > 0) {	long approximateTime = job.getSubmitTime() + startsAfter;	job = reader.nextJob();	long skippedCount = 0;	while (job != null && job.getSubmitTime() < approximateTime) {	job = reader.nextJob();	skippedCount++;	}	if (job == null) {	
no more jobs to process in the trace with starts after set to ms 

long approximateTime = job.getSubmitTime() + startsAfter;	job = reader.nextJob();	long skippedCount = 0;	while (job != null && job.getSubmitTime() < approximateTime) {	job = reader.nextJob();	skippedCount++;	}	if (job == null) {	return EMPTY_JOB_TRACE;	}	
the first job has a submit time of 

return EMPTY_JOB_TRACE;	}	}	firstJobSubmitTime = job.getSubmitTime();	long lastJobSubmitTime = firstJobSubmitTime;	int numberJobs = 0;	long currentIntervalEnd = Long.MIN_VALUE;	Path nextSegment = null;	Outputter<LoggedJob> tempGen = null;	if (debug) {	
the first job has a submit time of 

++numberJobs;	if (job.getSubmitTime() >= currentIntervalEnd) {	if (tempGen != null) {	tempGen.close();	}	nextSegment = null;	for (int i = 0; i < 3 && nextSegment == null; ++i) {	try {	nextSegment = new Path(tempDir, "segment-" + tempNameGenerator.nextLong() + ".json.gz");	if (debug) {	
the next segment name is 

continue;	} catch (IOException e) {	}	} catch (IOException e) {	}	}	if (nextSegment == null) {	throw new RuntimeException("Failed to create a new file!");	}	if (debug) {	
creating for a job with a submit time of 

job = reader.nextJob();	}	} catch (DeskewedJobTraceReader.OutOfOrderException e) {	return OUT_OF_ORDER_JOBS;	} finally {	if (tempGen != null) {	tempGen.close();	}	}	if (lastJobSubmitTime <= firstJobSubmitTime) {	
all of your job s have the same submit time please just use your input file 

return OUT_OF_ORDER_JOBS;	} finally {	if (tempGen != null) {	tempGen.close();	}	}	if (lastJobSubmitTime <= firstJobSubmitTime) {	return ALL_JOBS_SIMULTANEOUS;	}	double submitTimeSpan = lastJobSubmitTime - firstJobSubmitTime;	
your input trace spans ticks 

}	if (lastJobSubmitTime <= firstJobSubmitTime) {	return ALL_JOBS_SIMULTANEOUS;	}	double submitTimeSpan = lastJobSubmitTime - firstJobSubmitTime;	double foldingRatio = submitTimeSpan * (numberJobs + 1) / numberJobs / inputCycle;	if (debug) {	LOG.warn("run: submitTimeSpan = " + submitTimeSpan + ", numberJobs = " + numberJobs + ", inputCycle = " + inputCycle);	}	if (reader.neededSkewBufferSize() > 0) {	
you needed a skew buffer length of but no more for this input 

double submitTimeSpan = lastJobSubmitTime - firstJobSubmitTime;	double foldingRatio = submitTimeSpan * (numberJobs + 1) / numberJobs / inputCycle;	if (debug) {	LOG.warn("run: submitTimeSpan = " + submitTimeSpan + ", numberJobs = " + numberJobs + ", inputCycle = " + inputCycle);	}	if (reader.neededSkewBufferSize() > 0) {	}	double tProbability = timeDilation * concentration / foldingRatio;	if (debug) {	LOG.warn("run: timeDilation = " + timeDilation + ", concentration = " + concentration + ", foldingRatio = " + foldingRatio);	
the transcription probability is 

}	transcriptionRateInteger = (int) Math.floor(tProbability);	transcriptionRateFraction = tProbability - Math.floor(tProbability);	heap = new PriorityQueue<Pair<LoggedJob, JobTraceReader>>(tempPaths.size(), new JobEntryComparator());	for (Path tempPath : tempPaths) {	JobTraceReader thisReader = new JobTraceReader(tempPath, conf);	closees.add(thisReader);	LoggedJob streamFirstJob = thisReader.getNext();	long thisIndex = (streamFirstJob.getSubmitTime() - firstJobSubmitTime) / inputCycle;	if (debug) {	
a job with submit time of is in interval 

heap = new PriorityQueue<Pair<LoggedJob, JobTraceReader>>(tempPaths.size(), new JobEntryComparator());	for (Path tempPath : tempPaths) {	JobTraceReader thisReader = new JobTraceReader(tempPath, conf);	closees.add(thisReader);	LoggedJob streamFirstJob = thisReader.getNext();	long thisIndex = (streamFirstJob.getSubmitTime() - firstJobSubmitTime) / inputCycle;	if (debug) {	}	adjustJobTimes(streamFirstJob);	if (debug) {	
that job s submit time is adjusted to 

}	adjustJobTimes(streamFirstJob);	if (debug) {	}	heap .add(new Pair<LoggedJob, JobTraceReader>(streamFirstJob, thisReader));	}	Pair<LoggedJob, JobTraceReader> next = heap.poll();	while (next != null) {	maybeOutput(next.first());	if (debug) {	
the most recent job has an adjusted submit time of 

}	adjustJobTimes(streamFirstJob);	if (debug) {	}	heap .add(new Pair<LoggedJob, JobTraceReader>(streamFirstJob, thisReader));	}	Pair<LoggedJob, JobTraceReader> next = heap.poll();	while (next != null) {	maybeOutput(next.first());	if (debug) {	
its replacement in the heap will come from input engine 

}	Pair<LoggedJob, JobTraceReader> next = heap.poll();	while (next != null) {	maybeOutput(next.first());	if (debug) {	}	LoggedJob replacement = next.second().getNext();	if (replacement == null) {	next.second().close();	if (debug) {	
that input engine is depleted 

if (debug) {	}	LoggedJob replacement = next.second().getNext();	if (replacement == null) {	next.second().close();	if (debug) {	}	} else {	adjustJobTimes(replacement);	if (debug) {	
the replacement has an adjusted submit time of 

========================= hadoop sample_6669 =========================

if (hostPort == null) {	System.err.println("You must specify a host with -host.");	return 1;	}	if (args.isEmpty()) {	System.err.println("You must specify an operation.");	return 1;	}	String servicePrincipal = StringUtils.popOptionWithArgument("-principal", args);	if (servicePrincipal != null) {	
set service principal 

========================= hadoop sample_3484 =========================

public String name() {	return "keyComparator";	}	public byte[] findShortestSeparator(byte[] start, byte[] limit) {	return start;	}	public byte[] findShortSuccessor(byte[] key) {	return key;	}	});	
using conf database at 

}	public byte[] findShortSuccessor(byte[] key) {	return key;	}	});	File dbfile = new File(storeRoot.toString());	try {	db = JniDBFactory.factory.open(dbfile, options);	} catch (NativeDB.DBException e) {	if (e.isNotFound() || e.getMessage().contains(" does not exist ")) {	
creating conf database at 

public void run() {	long start = Time.monotonicNow();	
starting full compaction cycle 

public void run() {	long start = Time.monotonicNow();	try {	db.compactRange(null, null);	} catch (DBException e) {	
error compacting database 

public void run() {	long start = Time.monotonicNow();	try {	db.compactRange(null, null);	} catch (DBException e) {	}	long duration = Time.monotonicNow() - start;	
full compaction cycle completed in msec 

========================= hadoop sample_927 =========================

public static PlacementRule getPlacementRule(String ruleStr, Configuration conf) throws ClassNotFoundException {	Class<? extends PlacementRule> ruleClass = Class.forName(ruleStr) .asSubclass(PlacementRule.class);	
using placementrule implementation 

========================= hadoop sample_1143 =========================

private int skipUtfByteOrderMark(Text value) throws IOException {	int newMaxLineLength = (int) Math.min(3L + (long) maxLineLength, Integer.MAX_VALUE);	int newSize = in.readLine(value, newMaxLineLength, maxBytesToConsume(pos));	pos += newSize;	int textLength = value.getLength();	byte[] textBytes = value.getBytes();	if ((textLength >= 3) && (textBytes[0] == (byte)0xEF) && (textBytes[1] == (byte)0xBB) && (textBytes[2] == (byte)0xBF)) {	
found utf bom and skipped it 

} else {	newSize = in.readLine(value, maxLineLength, maxBytesToConsume(pos));	pos += newSize;	}	if (newSize == 0) {	return false;	}	if (newSize < maxLineLength) {	return true;	}	
skipped line of size at pos 

========================= hadoop sample_4711 =========================

try {	FileStatus stat = dfs.getFileStatus(file);	BlockLocation[] locs = dfs.getFileBlockLocations(stat, 0, stat .getLen());	for (BlockLocation loc : locs) {	if (replication != loc.getHosts().length) {	return false;	}	}	return true;	} catch (IOException e) {	
getfilestatus on path failed 

public static boolean verifyFileReplicasOnStorageType(FileSystem fs, DFSClient client, Path path, StorageType storageType) throws IOException {	if (!fs.exists(path)) {	
verifyfilereplicasonstoragetype file does not exist 

public static boolean verifyFileReplicasOnStorageType(FileSystem fs, DFSClient client, Path path, StorageType storageType) throws IOException {	if (!fs.exists(path)) {	return false;	}	long fileLength = client.getFileInfo(path.toString()).getLen();	LocatedBlocks locatedBlocks = client.getLocatedBlocks(path.toString(), 0, fileLength);	for (LocatedBlock locatedBlock : locatedBlocks.getLocatedBlocks()) {	if (locatedBlock.getStorageTypes()[0] != storageType) {	
verifyfilereplicasonstoragetype for file expect blk on type actual type 

public static boolean changeReplicaLength(MiniDFSCluster cluster, ExtendedBlock blk, int dnIndex, int lenDelta) throws IOException {	File blockFile = cluster.getBlockFile(dnIndex, blk);	if (blockFile != null && blockFile.exists()) {	try (RandomAccessFile raFile = new RandomAccessFile(blockFile, "rw")) {	raFile.setLength(raFile.length() + lenDelta);	}	return true;	}	
failed to change length of block 

public static HashSet<Path> closeOpenFiles( HashMap<Path, FSDataOutputStream> openFilesMap, int numFilesToClose) throws IOException {	HashSet<Path> closedFiles = new HashSet<>();	for (Iterator<Entry<Path, FSDataOutputStream>> it = openFilesMap.entrySet().iterator(); it.hasNext();) {	Entry<Path, FSDataOutputStream> entry = it.next();	
closing file 

========================= hadoop sample_7097 =========================

public void run() {	if (LOG.isDebugEnabled()) {	String msg = String.format("Running DeletionTask : %s", toString());	LOG.debug(msg);	}	boolean error = false;	if (null == getUser()) {	if (baseDirs == null || baseDirs.size() == 0) {	if (LOG.isDebugEnabled()) {	
nm deleting absolute path 

}	boolean error = false;	if (null == getUser()) {	if (baseDirs == null || baseDirs.size() == 0) {	if (LOG.isDebugEnabled()) {	}	try {	lfs.delete(subDir, true);	} catch (IOException e) {	error = true;	
failed to delete 

}	try {	lfs.delete(subDir, true);	} catch (IOException e) {	error = true;	}	} else {	for (Path baseDir : baseDirs) {	Path del = subDir == null? baseDir : new Path(baseDir, subDir);	if (LOG.isDebugEnabled()) {	
nm deleting path 

}	} else {	for (Path baseDir : baseDirs) {	Path del = subDir == null? baseDir : new Path(baseDir, subDir);	if (LOG.isDebugEnabled()) {	}	try {	lfs.delete(del, true);	} catch (IOException e) {	error = true;	
failed to delete 

try {	lfs.delete(del, true);	} catch (IOException e) {	error = true;	}	}	}	} else {	try {	if (LOG.isDebugEnabled()) {	
deleting path as user 

try {	if (LOG.isDebugEnabled()) {	}	if (baseDirs == null || baseDirs.size() == 0) {	getDeletionService().getContainerExecutor().deleteAsUser( new DeletionAsUserContext.Builder() .setUser(getUser()) .setSubDir(subDir) .build());	} else {	getDeletionService().getContainerExecutor().deleteAsUser( new DeletionAsUserContext.Builder() .setUser(getUser()) .setSubDir(subDir) .setBasedirs(baseDirs.toArray(new Path[0])) .build());	}	} catch (IOException|InterruptedException e) {	error = true;	
failed to delete as user 

========================= hadoop sample_1790 =========================

private void initMetricsCSVOutput() {	int timeIntervalMS = conf.getInt( SLSConfiguration.METRICS_RECORD_INTERVAL_MS, SLSConfiguration.METRICS_RECORD_INTERVAL_MS_DEFAULT);	File dir = new File(metricsOutputDir + "/metrics");	if(!dir.exists() && !dir.mkdirs()) {	
cannot create directory 

========================= hadoop sample_6055 =========================

private void waitForLocatedBlockWithArchiveStorageType( final DistributedFileSystem dfs, final String file, final int expectedArchiveCount) throws Exception {	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	LocatedBlock lb = null;	try {	lb = dfs.getClient().getLocatedBlocks(file, 0).get(0);	} catch (IOException e) {	
exception while getting located blocks 

========================= hadoop sample_7214 =========================

DefaultMetricsSystem.setMiniClusterMode(true);	}	Configuration config;	File hdfsDir;	public static String getFullHostName() {	try {	return DNS.getDefaultHost("default");	} catch (UnknownHostException e) {	
unable to determine hostname may interfere with obtaining valid test results 

========================= hadoop sample_7644 =========================

TimelineEvent event = new TimelineEvent();	event.setTimestamp(System.currentTimeMillis());	event.setEventType("foo_event");	entity.addEvent(event);	UserGroupInformation ugi = UserGroupInformation.getCurrentUser();	long startWrite = System.nanoTime();	try {	tlc.putEntities(entity);	} catch (Exception e) {	context.getCounter(PerfCounters.TIMELINE_SERVICE_WRITE_FAILURES). increment(1);	
writing to the timeline service failed 

UserGroupInformation ugi = UserGroupInformation.getCurrentUser();	long startWrite = System.nanoTime();	try {	tlc.putEntities(entity);	} catch (Exception e) {	context.getCounter(PerfCounters.TIMELINE_SERVICE_WRITE_FAILURES). increment(1);	}	long endWrite = System.nanoTime();	totalTime += TimeUnit.NANOSECONDS.toMillis(endWrite-startWrite);	}	
wrote entities kb in ms 

========================= hadoop sample_5679 =========================

claAppender.setLayout(new PatternLayout("%-5p [%t]: %m%n"));	claAppender.setContainerLogDir("target/testAppendInClose/logDir");	claAppender.setContainerLogFile("syslog");	claAppender.setTotalLogFileSize(1000);	claAppender.activateOptions();	final Logger claLog = Logger.getLogger("testAppendInClose-catergory");	claLog.setAdditivity(false);	claLog.addAppender(claAppender);	claLog.info(new Object() {	public String toString() {	
message1 

========================= hadoop sample_2153 =========================

public Path getTrashRoot(Path path) {	try {	if ((path == null) || !dfs.isHDFSEncryptionEnabled()) {	return super.getTrashRoot(path);	}	} catch (IOException ioe) {	
exception while checking whether encryption zone is supported 

}	} catch (IOException ioe) {	}	String parentSrc = path.isRoot()? path.toUri().getPath():path.getParent().toUri().getPath();	try {	EncryptionZone ez = dfs.getEZForPath(parentSrc);	if ((ez != null)) {	return this.makeQualified( new Path(new Path(ez.getPath(), FileSystem.TRASH_PREFIX), dfs.ugi.getShortUserName()));	}	} catch (IOException e) {	
exception in checking the encryption zone for the path 

}	} else {	Path userTrash = new Path(ezTrashRoot, dfs.ugi.getShortUserName());	try {	ret.add(getFileStatus(userTrash));	} catch (FileNotFoundException ignored) {	}	}	}	} catch (IOException e){	
cannot get all encrypted trash roots 

========================= hadoop sample_6813 =========================

public RegisterApplicationMasterResponse registerApplicationMaster( RegisterApplicationMasterRequest request) throws YarnException, IOException {	validateRunning();	String amrmToken = getAppIdentifier();	
registering application attempt 

shouldReRegisterNext = false;	synchronized (applicationContainerIdMap) {	if (applicationContainerIdMap.containsKey(amrmToken)) {	throw new InvalidApplicationMasterRequestException( AMRMClientUtils.APP_ALREADY_REGISTERED_MESSAGE);	}	applicationContainerIdMap.put(amrmToken, new ArrayList<ContainerId>());	}	synchronized (syncObj) {	syncObj.notifyAll();	if (request.getRpcPort() > 1000) {	
register call in rm start waiting 

if (applicationContainerIdMap.containsKey(amrmToken)) {	throw new InvalidApplicationMasterRequestException( AMRMClientUtils.APP_ALREADY_REGISTERED_MESSAGE);	}	applicationContainerIdMap.put(amrmToken, new ArrayList<ContainerId>());	}	synchronized (syncObj) {	syncObj.notifyAll();	if (request.getRpcPort() > 1000) {	try {	syncObj.wait();	
register call in rm wait finished 

throw new InvalidApplicationMasterRequestException( AMRMClientUtils.APP_ALREADY_REGISTERED_MESSAGE);	}	applicationContainerIdMap.put(amrmToken, new ArrayList<ContainerId>());	}	synchronized (syncObj) {	syncObj.notifyAll();	if (request.getRpcPort() > 1000) {	try {	syncObj.wait();	} catch (InterruptedException e) {	
register call in rm wait interrupted 

public FinishApplicationMasterResponse finishApplicationMaster( FinishApplicationMasterRequest request) throws YarnException, IOException {	validateRunning();	String amrmToken = getAppIdentifier();	
finishing application attempt 

public AllocateResponse allocate(AllocateRequest request) throws YarnException, IOException {	validateRunning();	if (request.getAskList() != null && request.getAskList().size() > 0 && request.getReleaseList() != null && request.getReleaseList().size() > 0) {	Assert.fail("The mock RM implementation does not support receiving " + "askList and releaseList in the same heartbeat");	}	String amrmToken = getAppIdentifier();	
allocate from application attempt 

synchronized (applicationContainerIdMap) {	Assert.assertTrue( "The application id is Not registered before allocate(): " + amrmToken, applicationContainerIdMap.containsKey(amrmToken));	List<ContainerId> ids = applicationContainerIdMap.get(amrmToken);	ids.add(containerId);	this.allocatedContainerMap.put(containerId, container);	}	}	}	}	if (request.getReleaseList() != null && request.getReleaseList().size() > 0) {	
releasing containers 

}	Assert.assertTrue("ContainerId " + id + " being released is not valid for application: " + conf.get("AMRMTOKEN"), found);	ids.remove(id);	ContainerId fakeContainerId = ContainerId.newInstance( getApplicationAttemptId(1), containerIndex.incrementAndGet());	Container fakeContainer = allocatedContainerMap.get(id);	fakeContainer.setId(fakeContainerId);	containerList.add(fakeContainer);	}	}	}	
allocating containers for application attempt AMRMTOKEN 

public SubmitApplicationResponse submitApplication( SubmitApplicationRequest request) throws YarnException, IOException {	validateRunning();	ApplicationId appId = null;	if (request.getApplicationSubmissionContext() != null) {	appId = request.getApplicationSubmissionContext().getApplicationId();	}	
application submitted 

public KillApplicationResponse forceKillApplication( KillApplicationRequest request) throws YarnException, IOException {	validateRunning();	ApplicationId appId = null;	if (request.getApplicationId() != null) {	appId = request.getApplicationId();	if (!applicationMap.remove(appId)) {	throw new ApplicationNotFoundException( "Trying to kill an absent application: " + appId);	}	}	
force killing application 

========================= hadoop sample_1285 =========================

public void run() {	JobStats stats = Statistics.generateJobStats(job.getJob(), job.getJobDesc());	try {	try {	long start = System.currentTimeMillis();	job.buildSplits(inputDir);	long end = System.currentTimeMillis();	
jobsubmitter time taken to build splits for job ms 

public void run() {	JobStats stats = Statistics.generateJobStats(job.getJob(), job.getJobDesc());	try {	try {	long start = System.currentTimeMillis();	job.buildSplits(inputDir);	long end = System.currentTimeMillis();	} catch (IOException e) {	
failed to submit as 

JobStats stats = Statistics.generateJobStats(job.getJob(), job.getJobDesc());	try {	try {	long start = System.currentTimeMillis();	job.buildSplits(inputDir);	long end = System.currentTimeMillis();	} catch (IOException e) {	monitor.submissionFailed(stats);	return;	} catch (Exception e) {	
failed to submit as 

}	long nsDelay = job.getDelay(TimeUnit.NANOSECONDS);	while (nsDelay > 0) {	TimeUnit.NANOSECONDS.sleep(nsDelay);	nsDelay = job.getDelay(TimeUnit.NANOSECONDS);	}	try {	long start = System.currentTimeMillis();	job.call();	long end = System.currentTimeMillis();	
jobsubmitter time taken to submit the job ms 

}	try {	long start = System.currentTimeMillis();	job.call();	long end = System.currentTimeMillis();	job.setSubmitted();	monitor.add(stats);	statistics.addJobStats(stats);	if (LOG.isDebugEnabled()) {	String jobID = job.getJob().getConfiguration().get(Gridmix.ORIGINAL_JOB_ID);	
original job is being simulated as 

}	try {	long start = System.currentTimeMillis();	job.call();	long end = System.currentTimeMillis();	job.setSubmitted();	monitor.add(stats);	statistics.addJobStats(stats);	if (LOG.isDebugEnabled()) {	String jobID = job.getJob().getConfiguration().get(Gridmix.ORIGINAL_JOB_ID);	
submit 

long start = System.currentTimeMillis();	job.call();	long end = System.currentTimeMillis();	job.setSubmitted();	monitor.add(stats);	statistics.addJobStats(stats);	if (LOG.isDebugEnabled()) {	String jobID = job.getJob().getConfiguration().get(Gridmix.ORIGINAL_JOB_ID);	}	} catch (IOException e) {	
failed to submit as 

statistics.addJobStats(stats);	if (LOG.isDebugEnabled()) {	String jobID = job.getJob().getConfiguration().get(Gridmix.ORIGINAL_JOB_ID);	}	} catch (IOException e) {	if (e.getCause() instanceof ClosedByInterruptException) {	throw new InterruptedException("Failed to submit " + job.getJob().getJobName());	}	monitor.submissionFailed(stats);	} catch (ClassNotFoundException e) {	
failed to submit 

}	monitor.submissionFailed(stats);	} catch (ClassNotFoundException e) {	monitor.submissionFailed(stats);	}	} catch (InterruptedException e) {	GridmixJob.pullDescription(job.id());	Thread.currentThread().interrupt();	monitor.submissionFailed(stats);	} catch(Exception e) {	
job submission failed 

public void add(final GridmixJob job) throws InterruptedException {	final boolean addToQueue = !shutdown;	if (addToQueue) {	final SubmitTask task = new SubmitTask(job);	
total number of queued jobs 

========================= hadoop sample_6110 =========================

public void testEndpoint() throws Exception {	conf = new Configuration();	String endpoint = conf.getTrimmed( S3ATestConstants.CONFIGURATION_TEST_ENDPOINT, "");	if (endpoint.isEmpty()) {	
custom endpoint test skipped as config setting was not detected 

try {	fs = S3ATestUtils.createTestFileSystem(conf);	assertNotNull(fs);	AmazonS3 s3 = fs.getAmazonS3Client();	assertNotNull(s3);	S3ClientOptions clientOptions = getField(s3, S3ClientOptions.class, "clientOptions");	assertTrue("Expected to find path style access to be switched on!", clientOptions.isPathStyleAccess());	byte[] file = ContractTestUtils.toAsciiByteArray("test file");	ContractTestUtils.writeAndRead(fs, new Path("/path/style/access/testFile"), file, file.length, (int) conf.getLongBytes(Constants.FS_S3A_BLOCK_SIZE, file.length), false, true);	} catch (final AWSS3IOException e) {	
caught exception 

========================= hadoop sample_5911 =========================

public void tearDown() throws IOException {	if (fs.delete(TEST_ROOT_DIR, true)) {	
failed to delete test root dir and its content under 

========================= hadoop sample_4603 =========================

public void testFifoScheduler() throws Exception {	
start testfifoscheduler 

Application application_1 = new Application("user_1", resourceManager);	application_1.submit();	application_1.addNodeManager(host_0, 1234, nm_0);	application_1.addNodeManager(host_1, 1234, nm_1);	Resource capability_1_0 = Resources.createResource(3 * GB);	application_1.addResourceRequestSpec(priority_1, capability_1_0);	Resource capability_1_1 = Resources.createResource(4 * GB);	application_1.addResourceRequestSpec(priority_0, capability_1_1);	Task task_1_0 = new Task(application_1, priority_1, new String[] {host_0, host_1});	application_1.addTask(task_1_0);	
send resource requests to the scheduler 

nm_0.heartbeat();	nm_1.heartbeat();	application_0.schedule();	checkApplicationResourceUsage(GB, application_0);	application_1.schedule();	checkApplicationResourceUsage(3 * GB, application_1);	nm_0.heartbeat();	nm_1.heartbeat();	checkNodeResourceUsage(4*GB, nm_0);	checkNodeResourceUsage(0*GB, nm_1);	
adding new tasks 

Task task_1_3 = new Task(application_1, priority_0, new String[] {ResourceRequest.ANY});	application_1.addTask(task_1_3);	application_1.schedule();	Task task_0_1 = new Task(application_0, priority_1, new String[] {host_0, host_1});	application_0.addTask(task_0_1);	Task task_0_2 = new Task(application_0, priority_1, new String[] {host_0, host_1});	application_0.addTask(task_0_2);	Task task_0_3 = new Task(application_0, priority_0, new String[] {ResourceRequest.ANY});	application_0.addTask(task_0_3);	application_0.schedule();	
sending hb from 

application_1.addTask(task_1_3);	application_1.schedule();	Task task_0_1 = new Task(application_0, priority_1, new String[] {host_0, host_1});	application_0.addTask(task_0_1);	Task task_0_2 = new Task(application_0, priority_1, new String[] {host_0, host_1});	application_0.addTask(task_0_2);	Task task_0_3 = new Task(application_0, priority_0, new String[] {ResourceRequest.ANY});	application_0.addTask(task_0_3);	application_0.schedule();	nm_0.heartbeat();	
sending hb from 

application_1.schedule();	Task task_0_1 = new Task(application_0, priority_1, new String[] {host_0, host_1});	application_0.addTask(task_0_1);	Task task_0_2 = new Task(application_0, priority_1, new String[] {host_0, host_1});	application_0.addTask(task_0_2);	Task task_0_3 = new Task(application_0, priority_0, new String[] {ResourceRequest.ANY});	application_0.addTask(task_0_3);	application_0.schedule();	nm_0.heartbeat();	nm_1.heartbeat();	
trying to allocate 

nm_0.heartbeat();	nm_1.heartbeat();	application_0.schedule();	checkApplicationResourceUsage(3 * GB, application_0);	application_1.schedule();	checkApplicationResourceUsage(3 * GB, application_1);	nm_0.heartbeat();	nm_1.heartbeat();	checkNodeResourceUsage(4*GB, nm_0);	checkNodeResourceUsage(2*GB, nm_1);	
finishing up task 

checkNodeResourceUsage(2*GB, nm_1);	application_0.finishTask(task_0_0);	application_0.schedule();	application_1.schedule();	nm_0.heartbeat();	nm_1.heartbeat();	checkApplicationResourceUsage(3 * GB, application_0);	checkApplicationResourceUsage(3 * GB, application_1);	checkNodeResourceUsage(4*GB, nm_0);	checkNodeResourceUsage(2*GB, nm_1);	
finishing up task 

checkNodeResourceUsage(4*GB, nm_0);	checkNodeResourceUsage(2*GB, nm_1);	application_1.finishTask(task_1_0);	application_0.schedule();	application_1.schedule();	nm_0.heartbeat();	nm_1.heartbeat();	checkApplicationResourceUsage(4 * GB, application_0);	checkApplicationResourceUsage(0 * GB, application_1);	checkNodeResourceUsage(2*GB, nm_1);	
finishing up task 

checkApplicationResourceUsage(0 * GB, application_1);	checkNodeResourceUsage(2*GB, nm_1);	application_0.finishTask(task_0_3);	application_0.schedule();	application_1.schedule();	nm_0.heartbeat();	nm_1.heartbeat();	checkApplicationResourceUsage(2 * GB, application_0);	checkApplicationResourceUsage(0 * GB, application_1);	checkNodeResourceUsage(0*GB, nm_1);	
finishing up task 

checkApplicationResourceUsage(2 * GB, application_0);	checkApplicationResourceUsage(0 * GB, application_1);	checkNodeResourceUsage(0*GB, nm_1);	application_0.finishTask(task_0_1);	application_0.schedule();	application_1.schedule();	nm_0.heartbeat();	nm_1.heartbeat();	checkApplicationResourceUsage(1 * GB, application_0);	checkApplicationResourceUsage(0 * GB, application_1);	
finishing up task 

nm_1.heartbeat();	checkApplicationResourceUsage(1 * GB, application_0);	checkApplicationResourceUsage(0 * GB, application_1);	application_0.finishTask(task_0_2);	application_0.schedule();	application_1.schedule();	nm_0.heartbeat();	nm_1.heartbeat();	checkApplicationResourceUsage(0 * GB, application_0);	checkApplicationResourceUsage(4 * GB, application_1);	
finishing up task 

nm_1.heartbeat();	checkApplicationResourceUsage(0 * GB, application_0);	checkApplicationResourceUsage(4 * GB, application_1);	application_1.finishTask(task_1_3);	application_0.schedule();	application_1.schedule();	nm_0.heartbeat();	nm_1.heartbeat();	checkApplicationResourceUsage(0 * GB, application_0);	checkApplicationResourceUsage(3 * GB, application_1);	
finishing up task 

nm_1.heartbeat();	checkApplicationResourceUsage(0 * GB, application_0);	checkApplicationResourceUsage(3 * GB, application_1);	application_1.finishTask(task_1_1);	application_0.schedule();	application_1.schedule();	nm_0.heartbeat();	nm_1.heartbeat();	checkApplicationResourceUsage(0 * GB, application_0);	checkApplicationResourceUsage(3 * GB, application_1);	
end testfifoscheduler 

MockAM am2 = rm.sendAMLaunched(attempt2.getAppAttemptId());	am2.registerAppAttempt();	SchedulerNodeReport report_nm2 = rm.getResourceScheduler().getNodeReport(nm2.getNodeId());	Assert.assertEquals(2 * GB, report_nm2.getUsedResource().getMemorySize());	am1.addRequests(new String[] { "127.0.0.1", "127.0.0.2" }, GB, 1, 1);	AllocateResponse alloc1Response = am1.schedule();	am2.addRequests(new String[] { "127.0.0.1", "127.0.0.2" }, 3 * GB, 0, 1);	AllocateResponse alloc2Response = am2.schedule();	nm1.nodeHeartbeat(true);	while (alloc1Response.getAllocatedContainers().size() < 1) {	
waiting for containers to be created for app 

am1.addRequests(new String[] { "127.0.0.1", "127.0.0.2" }, GB, 1, 1);	AllocateResponse alloc1Response = am1.schedule();	am2.addRequests(new String[] { "127.0.0.1", "127.0.0.2" }, 3 * GB, 0, 1);	AllocateResponse alloc2Response = am2.schedule();	nm1.nodeHeartbeat(true);	while (alloc1Response.getAllocatedContainers().size() < 1) {	Thread.sleep(1000);	alloc1Response = am1.schedule();	}	while (alloc2Response.getAllocatedContainers().size() < 1) {	
waiting for containers to be created for app 

Assert.assertEquals(0, report_nm1.getAvailableResource().getMemorySize());	Assert.assertEquals(2 * GB, report_nm2.getAvailableResource().getMemorySize());	Assert.assertEquals(6 * GB, report_nm1.getUsedResource().getMemorySize());	Assert.assertEquals(2 * GB, report_nm2.getUsedResource().getMemorySize());	Container c1 = allocated1.get(0);	Assert.assertEquals(GB, c1.getResource().getMemorySize());	ContainerStatus containerStatus = BuilderUtils.newContainerStatus(c1.getId(), ContainerState.COMPLETE, "", 0, c1.getResource());	nm1.containerStatus(containerStatus);	int waitCount = 0;	while (attempt1.getJustFinishedContainers().size() < 1 && waitCount++ != 20) {	
waiting for containers to be finished for app tried times already 

RMAppAttempt attempt1 = app1.getCurrentAppAttempt();	MockAM am1 = rm.sendAMLaunched(attempt1.getAppAttemptId());	am1.registerAppAttempt();	SchedulerNodeReport report_nm1 = rm.getResourceScheduler().getNodeReport(nm1.getNodeId());	Assert.assertEquals(2 * GB, report_nm1.getUsedResource().getMemorySize());	Assert.assertEquals(2 * GB, report_nm1.getAvailableResource().getMemorySize());	am1.addRequests(new String[] { "127.0.0.1", "127.0.0.2" }, 2 * GB, 1, 1);	AllocateResponse alloc1Response = am1.schedule();	nm1.nodeHeartbeat(true);	while (alloc1Response.getAllocatedContainers().size() < 1) {	
waiting for containers to be created for app 

Map<NodeId, ResourceOption> nodeResourceMap = new HashMap<NodeId, ResourceOption>();	nodeResourceMap.put(nm1.getNodeId(), ResourceOption.newInstance(Resource.newInstance(2 * GB, 1), -1));	UpdateNodeResourceRequest request = UpdateNodeResourceRequest.newInstance(nodeResourceMap);	rm.getAdminService().updateNodeResource(request);	waitCount = 0;	while (waitCount++ != 20) {	report_nm1 = rm.getResourceScheduler().getNodeReport(nm1.getNodeId());	if (null != report_nm1 && report_nm1.getAvailableResource().getMemorySize() != 0) {	break;	}	
waiting for rmnoderesourceupdateevent to be handled tried times already 

}	Thread.sleep(1000);	}	report_nm1 = rm.getResourceScheduler().getNodeReport(nm1.getNodeId());	Assert.assertEquals(4 * GB, report_nm1.getUsedResource().getMemorySize());	Assert.assertEquals(-2 * GB, report_nm1.getAvailableResource().getMemorySize());	ContainerStatus containerStatus = BuilderUtils.newContainerStatus(c1.getId(), ContainerState.COMPLETE, "", 0, c1.getResource());	nm1.containerStatus(containerStatus);	waitCount = 0;	while (attempt1.getJustFinishedContainers().size() < 1 && waitCount++ != 20) {	
waiting for containers to be finished for app tried times already 

========================= hadoop sample_566 =========================

public void testRouterClientRMServiceE2E() throws Exception {	String user = "test1";	
get new application 

public void testRouterClientRMServiceE2E() throws Exception {	String user = "test1";	GetNewApplicationResponse responseGetNewApp = getNewApplication(user);	Assert.assertNotNull(responseGetNewApp);	
submit application 

public void testRouterClientRMServiceE2E() throws Exception {	String user = "test1";	GetNewApplicationResponse responseGetNewApp = getNewApplication(user);	Assert.assertNotNull(responseGetNewApp);	SubmitApplicationResponse responseSubmitApp = submitApplication(responseGetNewApp.getApplicationId(), user);	Assert.assertNotNull(responseSubmitApp);	
kill application 

public void testRouterClientRMServiceE2E() throws Exception {	String user = "test1";	GetNewApplicationResponse responseGetNewApp = getNewApplication(user);	Assert.assertNotNull(responseGetNewApp);	SubmitApplicationResponse responseSubmitApp = submitApplication(responseGetNewApp.getApplicationId(), user);	Assert.assertNotNull(responseSubmitApp);	KillApplicationResponse responseKillApp = forceKillApplication(responseGetNewApp.getApplicationId(), user);	Assert.assertNotNull(responseKillApp);	
get cluster metrics 

public void testRouterClientRMServiceE2E() throws Exception {	String user = "test1";	GetNewApplicationResponse responseGetNewApp = getNewApplication(user);	Assert.assertNotNull(responseGetNewApp);	SubmitApplicationResponse responseSubmitApp = submitApplication(responseGetNewApp.getApplicationId(), user);	Assert.assertNotNull(responseSubmitApp);	KillApplicationResponse responseKillApp = forceKillApplication(responseGetNewApp.getApplicationId(), user);	Assert.assertNotNull(responseKillApp);	GetClusterMetricsResponse responseGetClusterMetrics = getClusterMetrics(user);	Assert.assertNotNull(responseGetClusterMetrics);	
get cluster nodes 

GetNewApplicationResponse responseGetNewApp = getNewApplication(user);	Assert.assertNotNull(responseGetNewApp);	SubmitApplicationResponse responseSubmitApp = submitApplication(responseGetNewApp.getApplicationId(), user);	Assert.assertNotNull(responseSubmitApp);	KillApplicationResponse responseKillApp = forceKillApplication(responseGetNewApp.getApplicationId(), user);	Assert.assertNotNull(responseKillApp);	GetClusterMetricsResponse responseGetClusterMetrics = getClusterMetrics(user);	Assert.assertNotNull(responseGetClusterMetrics);	GetClusterNodesResponse responseGetClusterNodes = getClusterNodes(user);	Assert.assertNotNull(responseGetClusterNodes);	
get queue info 

SubmitApplicationResponse responseSubmitApp = submitApplication(responseGetNewApp.getApplicationId(), user);	Assert.assertNotNull(responseSubmitApp);	KillApplicationResponse responseKillApp = forceKillApplication(responseGetNewApp.getApplicationId(), user);	Assert.assertNotNull(responseKillApp);	GetClusterMetricsResponse responseGetClusterMetrics = getClusterMetrics(user);	Assert.assertNotNull(responseGetClusterMetrics);	GetClusterNodesResponse responseGetClusterNodes = getClusterNodes(user);	Assert.assertNotNull(responseGetClusterNodes);	GetQueueInfoResponse responseGetQueueInfo = getQueueInfo(user);	Assert.assertNotNull(responseGetQueueInfo);	
get queue user 

KillApplicationResponse responseKillApp = forceKillApplication(responseGetNewApp.getApplicationId(), user);	Assert.assertNotNull(responseKillApp);	GetClusterMetricsResponse responseGetClusterMetrics = getClusterMetrics(user);	Assert.assertNotNull(responseGetClusterMetrics);	GetClusterNodesResponse responseGetClusterNodes = getClusterNodes(user);	Assert.assertNotNull(responseGetClusterNodes);	GetQueueInfoResponse responseGetQueueInfo = getQueueInfo(user);	Assert.assertNotNull(responseGetQueueInfo);	GetQueueUserAclsInfoResponse responseGetQueueUser = getQueueUserAcls(user);	Assert.assertNotNull(responseGetQueueUser);	
get cluster node 

GetClusterMetricsResponse responseGetClusterMetrics = getClusterMetrics(user);	Assert.assertNotNull(responseGetClusterMetrics);	GetClusterNodesResponse responseGetClusterNodes = getClusterNodes(user);	Assert.assertNotNull(responseGetClusterNodes);	GetQueueInfoResponse responseGetQueueInfo = getQueueInfo(user);	Assert.assertNotNull(responseGetQueueInfo);	GetQueueUserAclsInfoResponse responseGetQueueUser = getQueueUserAcls(user);	Assert.assertNotNull(responseGetQueueUser);	GetClusterNodeLabelsResponse responseGetClusterNode = getClusterNodeLabels(user);	Assert.assertNotNull(responseGetClusterNode);	
move application across queues 

GetClusterNodesResponse responseGetClusterNodes = getClusterNodes(user);	Assert.assertNotNull(responseGetClusterNodes);	GetQueueInfoResponse responseGetQueueInfo = getQueueInfo(user);	Assert.assertNotNull(responseGetQueueInfo);	GetQueueUserAclsInfoResponse responseGetQueueUser = getQueueUserAcls(user);	Assert.assertNotNull(responseGetQueueUser);	GetClusterNodeLabelsResponse responseGetClusterNode = getClusterNodeLabels(user);	Assert.assertNotNull(responseGetClusterNode);	MoveApplicationAcrossQueuesResponse responseMoveApp = moveApplicationAcrossQueues(user, responseGetNewApp.getApplicationId());	Assert.assertNotNull(responseMoveApp);	
get new reservation 

Assert.assertNotNull(responseGetClusterNodes);	GetQueueInfoResponse responseGetQueueInfo = getQueueInfo(user);	Assert.assertNotNull(responseGetQueueInfo);	GetQueueUserAclsInfoResponse responseGetQueueUser = getQueueUserAcls(user);	Assert.assertNotNull(responseGetQueueUser);	GetClusterNodeLabelsResponse responseGetClusterNode = getClusterNodeLabels(user);	Assert.assertNotNull(responseGetClusterNode);	MoveApplicationAcrossQueuesResponse responseMoveApp = moveApplicationAcrossQueues(user, responseGetNewApp.getApplicationId());	Assert.assertNotNull(responseMoveApp);	GetNewReservationResponse getNewReservationResponse = getNewReservation(user);	
submit reservation 

Assert.assertNotNull(responseGetQueueInfo);	GetQueueUserAclsInfoResponse responseGetQueueUser = getQueueUserAcls(user);	Assert.assertNotNull(responseGetQueueUser);	GetClusterNodeLabelsResponse responseGetClusterNode = getClusterNodeLabels(user);	Assert.assertNotNull(responseGetClusterNode);	MoveApplicationAcrossQueuesResponse responseMoveApp = moveApplicationAcrossQueues(user, responseGetNewApp.getApplicationId());	Assert.assertNotNull(responseMoveApp);	GetNewReservationResponse getNewReservationResponse = getNewReservation(user);	ReservationSubmissionResponse responseSubmitReser = submitReservation(user, getNewReservationResponse.getReservationId());	Assert.assertNotNull(responseSubmitReser);	
update reservation 

Assert.assertNotNull(responseGetQueueUser);	GetClusterNodeLabelsResponse responseGetClusterNode = getClusterNodeLabels(user);	Assert.assertNotNull(responseGetClusterNode);	MoveApplicationAcrossQueuesResponse responseMoveApp = moveApplicationAcrossQueues(user, responseGetNewApp.getApplicationId());	Assert.assertNotNull(responseMoveApp);	GetNewReservationResponse getNewReservationResponse = getNewReservation(user);	ReservationSubmissionResponse responseSubmitReser = submitReservation(user, getNewReservationResponse.getReservationId());	Assert.assertNotNull(responseSubmitReser);	ReservationUpdateResponse responseUpdateReser = updateReservation(user, getNewReservationResponse.getReservationId());	Assert.assertNotNull(responseUpdateReser);	
delete reservation 

========================= hadoop sample_1952 =========================

public void testOpenNonExistFile() throws IOException {	final Path p = new Path("/test/testOpenNonExistFile");	try {	fs.open(p).read();	fail("Expected FileNotFoundException was not thrown");	} catch(FileNotFoundException fnfe) {	
this is expected 

final FileStatus status = fs.getFileStatus(root);	assertTrue(status != null);	assertEquals(0777, status.getPermission().toShort());	assertFalse(fs.delete(root, true));	try {	final FSDataOutputStream out = fs.create(root);	out.write(1);	out.close();	fail();	} catch(IOException e) {	
this is expected 

out.write(1);	out.close();	fail();	} catch(IOException e) {	}	try {	final FSDataInputStream in = fs.open(root);	in.read();	fail();	} catch(IOException e) {	
this is expected 

{	final HttpOpParam.Op op = GetOpParam.Op.OPEN;	final URL url = webhdfs.toUrl(op, file);	final HttpURLConnection conn = (HttpURLConnection) url.openConnection();	conn.setRequestMethod(op.getType().toString());	conn.connect();	try {	WebHdfsFileSystem.jsonParse(conn, false);	fail();	} catch(IOException ioe) {	
GOOD 

========================= hadoop sample_7185 =========================

private StateChangeRequestInfo convert(HAStateChangeRequestInfoProto proto) {	RequestSource src;	switch (proto.getReqSource()) {	case REQUEST_BY_USER: src = RequestSource.REQUEST_BY_USER;	break;	case REQUEST_BY_USER_FORCED: src = RequestSource.REQUEST_BY_USER_FORCED;	break;	case REQUEST_BY_ZKFC: src = RequestSource.REQUEST_BY_ZKFC;	break;	
unknown request source 

========================= hadoop sample_3980 =========================

private void startServer(Configuration conf) throws Exception {	server = createTestServer(conf);	server.addJerseyResourcePackage( JerseyResource.class.getPackage().getName(), "/jersey/*");	server.start();	baseUrl = getServerURL(server);	
http server started 

========================= hadoop sample_3083 =========================

public boolean isChildPolicyAllowed(SchedulingPolicy childPolicy) {	
policy is only for leaf queues please choose or for parent queues 

========================= hadoop sample_970 =========================

try {	cluster.waitActive();	dfs = cluster.getFileSystem();	DFSClient client = dfs.dfs;	final Path f = new Path("/foo.txt");	createFile(dfs, f, 3);	try {	cluster.getNameNodeRpc().addBlock(f.toString(), client.clientName, null, null, HdfsConstants.GRANDFATHER_INODE_ID, null, null);	fail();	} catch(IOException ioe) {	
good 

cluster.waitActive();	dfs = cluster.getFileSystem();	DFSClient client = dfs.dfs;	final Path f = new Path("/testFileIdMismatch.txt");	createFile(dfs, f, 3);	long someOtherFileId = -1;	try {	cluster.getNameNodeRpc() .complete(f.toString(), client.clientName, null, someOtherFileId);	fail();	} catch(FileNotFoundException e) {	
caught expected filenotfoundexception 

========================= hadoop sample_7613 =========================

public Object call() throws Exception {	
invoking method on proxy 

========================= hadoop sample_6901 =========================

});	ldapServer.start();	final LdapGroupsMapping mapping = new LdapGroupsMapping();	final Configuration conf = new Configuration();	conf.set(LdapGroupsMapping.LDAP_URL_KEY, "ldap: conf.setInt(CONNECTION_TIMEOUT, connectionTimeoutMs);	mapping.setConf(conf);	try {	mapping.doGetGroups("hadoop", 1);	fail("The LDAP query should have timed out!");	} catch (NamingException ne) {	
got the exception while ldap querying 

});	ldapServer.start();	final LdapGroupsMapping mapping = new LdapGroupsMapping();	final Configuration conf = new Configuration();	conf.set(LdapGroupsMapping.LDAP_URL_KEY, "ldap: conf.setInt(READ_TIMEOUT, readTimeoutMs);	mapping.setConf(conf);	try {	mapping.doGetGroups("hadoop", 1);	fail("The LDAP query should have timed out!");	} catch (NamingException ne) {	
got the exception while ldap querying 

========================= hadoop sample_2942 =========================

private void createCgroup(String controller, String groupName) throws IOException {	String path = pathForCgroup(controller, groupName);	if (LOG.isDebugEnabled()) {	
createcgroup 

========================= hadoop sample_1682 =========================

throw new IOException("Missing username: " + rawUgi);	}	if (e == -1) {	e = rawUgi.getLength();	}	final String username = Text.decode(rawUgi.getBytes(), 0, e).trim();	UserGroupInformation ugi = null;	try {	ugi = UserGroupInformation.createProxyUser(username, UserGroupInformation.getLoginUser());	} catch (IOException ioe) {	
error while creating a proxy user 

========================= hadoop sample_6147 =========================

if (nextTxId <= 0) {	nextTxId = 1;	}	}	op.setTransactionId(nextTxId);	nextTxId++;	}	visitor.visitOp(op);	} catch (IOException e) {	if (!recoveryMode) {	
got ioexception at position 

}	op.setTransactionId(nextTxId);	nextTxId++;	}	visitor.visitOp(op);	} catch (IOException e) {	if (!recoveryMode) {	visitor.close(e);	throw e;	}	
got ioexception while reading stream resyncing 

}	visitor.visitOp(op);	} catch (IOException e) {	if (!recoveryMode) {	visitor.close(e);	throw e;	}	inputStream.resync();	} catch (RuntimeException e) {	if (!recoveryMode) {	
got runtimeexception at position 

if (!recoveryMode) {	visitor.close(e);	throw e;	}	inputStream.resync();	} catch (RuntimeException e) {	if (!recoveryMode) {	visitor.close(e);	throw e;	}	
got runtimeexception while reading stream resyncing 

========================= hadoop sample_7756 =========================

static void runWordCount(MiniMRCluster mr, JobConf jobConf, String sysDir) throws IOException {	
runWordCount 

========================= hadoop sample_5454 =========================

public synchronized void reserveResource( SchedulerApplicationAttempt application, SchedulerRequestKey schedulerKey, RMContainer container) {	RMContainer reservedContainer = getReservedContainer();	if (reservedContainer != null) {	if (!container.getContainer().getNodeId().equals(getNodeID())) {	throw new IllegalStateException("Trying to reserve" + " container " + container + " on node " + container.getReservedNode() + " when currently" + " reserved resource " + reservedContainer + " on node " + reservedContainer.getReservedNode());	}	if (!reservedContainer.getContainer().getId().getApplicationAttemptId() .equals(container.getContainer().getId().getApplicationAttemptId())) {	throw new IllegalStateException("Trying to reserve" + " container " + container + " for application " + application.getApplicationId() + " when currently" + " reserved container " + reservedContainer + " on node " + this);	}	
updated reserved container on node for application 

public synchronized void reserveResource( SchedulerApplicationAttempt application, SchedulerRequestKey schedulerKey, RMContainer container) {	RMContainer reservedContainer = getReservedContainer();	if (reservedContainer != null) {	if (!container.getContainer().getNodeId().equals(getNodeID())) {	throw new IllegalStateException("Trying to reserve" + " container " + container + " on node " + container.getReservedNode() + " when currently" + " reserved resource " + reservedContainer + " on node " + reservedContainer.getReservedNode());	}	if (!reservedContainer.getContainer().getId().getApplicationAttemptId() .equals(container.getContainer().getId().getApplicationAttemptId())) {	throw new IllegalStateException("Trying to reserve" + " container " + container + " for application " + application.getApplicationId() + " when currently" + " reserved container " + reservedContainer + " on node " + this);	}	} else {	
reserved container on node for application 

Resource reserved = resourcesPreemptedForApp.get(app);	Resource fulfilled = Resources.componentwiseMin(reserved, allocated);	Resources.subtractFrom(reserved, fulfilled);	Resources.subtractFrom(totalResourcesPreempted, fulfilled);	if (Resources.isNone(reserved)) {	resourcesPreemptedForApp.remove(app);	appIdToAppMap.remove(rmContainer.getApplicationAttemptId());	}	}	} else {	
allocated empty container 

========================= hadoop sample_966 =========================

private void launch() throws IOException, YarnException {	connect();	ContainerId masterContainerID = masterContainer.getId();	ApplicationSubmissionContext applicationContext = application.getSubmissionContext();	
setting up container for am 

ContainerLaunchContext launchContext = createAMContainerLaunchContext(applicationContext, masterContainerID);	StartContainerRequest scRequest = StartContainerRequest.newInstance(launchContext, masterContainer.getContainerToken());	List<StartContainerRequest> list = new ArrayList<StartContainerRequest>();	list.add(scRequest);	StartContainersRequest allRequests = StartContainersRequest.newInstance(list);	StartContainersResponse response = containerMgrProxy.startContainers(allRequests);	if (response.getFailedRequests() != null && response.getFailedRequests().containsKey(masterContainerID)) {	Throwable t = response.getFailedRequests().get(masterContainerID).deSerialize();	parseAndThrowException(t);	} else {	
done launching container for am 

public void run() {	switch (eventType) {	case LAUNCH: try {	
launching master 

case LAUNCH: try {	launch();	handler.handle(new RMAppAttemptEvent(application.getAppAttemptId(), RMAppAttemptEventType.LAUNCHED));	} catch(Exception ie) {	String message = "Error launching " + application.getAppAttemptId() + ". Got exception: " + StringUtils.stringifyException(ie);	LOG.info(message);	handler.handle(new RMAppAttemptEvent(application .getAppAttemptId(), RMAppAttemptEventType.LAUNCH_FAILED, message));	}	break;	case CLEANUP: try {	
cleaning master 

handler.handle(new RMAppAttemptEvent(application.getAppAttemptId(), RMAppAttemptEventType.LAUNCHED));	} catch(Exception ie) {	String message = "Error launching " + application.getAppAttemptId() + ". Got exception: " + StringUtils.stringifyException(ie);	LOG.info(message);	handler.handle(new RMAppAttemptEvent(application .getAppAttemptId(), RMAppAttemptEventType.LAUNCH_FAILED, message));	}	break;	case CLEANUP: try {	cleanup();	} catch(IOException ie) {	
error cleaning master 

}	break;	case CLEANUP: try {	cleanup();	} catch(IOException ie) {	} catch (YarnException e) {	StringBuilder sb = new StringBuilder("Container ");	sb.append(masterContainer.getId().toString());	sb.append(" is not handled by this NodeManager");	if (!e.getMessage().contains(sb.toString())) {	
error cleaning master 

cleanup();	} catch(IOException ie) {	} catch (YarnException e) {	StringBuilder sb = new StringBuilder("Container ");	sb.append(masterContainer.getId().toString());	sb.append(" is not handled by this NodeManager");	if (!e.getMessage().contains(sb.toString())) {	}	}	break;	
received unknown event type ignoring 

========================= hadoop sample_1102 =========================

static InetSocketAddress parseEntry(final String fn, final String hostName, final int port) {	InetSocketAddress addr = new InetSocketAddress(hostName, port);	if (addr.isUnresolved()) {	
failed to resolve in 

========================= hadoop sample_8323 =========================

LOG.debug("Added priority=" + schedulerKey.getPriority() + " application="+ applicationId);	}	}	final Resource capability = requestSpec.get(schedulerKey);	Set<Task> tasks = this.tasks.get(schedulerKey);	if (tasks == null) {	tasks = new HashSet<Task>();	this.tasks.put(schedulerKey, tasks);	}	tasks.add(task);	
added task to application at priority 

throw new IllegalStateException( "Finishing unknown task " + task.getTaskId() + " from application " + applicationId);	}	NodeManager nodeManager = task.getNodeManager();	ContainerId containerId = task.getContainerId();	task.stop();	List<ContainerId> containerIds = new ArrayList<ContainerId>();	containerIds.add(containerId);	StopContainersRequest stopRequest = StopContainersRequest.newInstance(containerIds);	nodeManager.stopContainers(stopRequest);	Resources.subtractFrom(used, requestSpec.get(task.getSchedulerKey()));	
finished task of application on node currently using resources 

int numContainers = containers.size();	for (SchedulerRequestKey schedulerKey: requests.keySet()) {	assign(schedulerKey, NodeType.NODE_LOCAL, containers);	assign(schedulerKey, NodeType.RACK_LOCAL, containers);	assign(schedulerKey, NodeType.OFF_SWITCH, containers);	if (containers.isEmpty()) {	break;	}	}	int assignedContainers = numContainers - containers.size();	
application assigned 

String host = container.getNodeId().toString();	if (Resources.equals(requestSpec.get(schedulerKey), container.getResource())) {	for (Iterator<Task> t=tasks.get(schedulerKey).iterator();	t.hasNext();) {	Task task = t.next();	if (task.getState() == State.PENDING && task.canSchedule(type, host)) {	NodeManager nodeManager = getNodeManager(host);	task.start(nodeManager, container.getId());	i.remove();	Resources.addTo(used, container.getResource());	
assigned container of type to task at priority on node currently using resources 

========================= hadoop sample_435 =========================

protected void serviceStart() throws Exception {	super.serviceStart();	
starting periodic service 

protected void serviceStop() throws Exception {	stopPeriodic();	
stopping periodic service 

protected synchronized void stopPeriodic() {	if (this.isRunning) {	
is shutting down 

protected synchronized void startPeriodic() {	stopPeriodic();	Runnable updateRunnable = new Runnable() {	public void run() {	
running update task 

public void run() {	try {	if (!isRunning) {	return;	}	periodicInvoke();	runCount++;	lastRun = Time.now();	} catch (Exception ex) {	errorCount++;	
service threw an exception 

========================= hadoop sample_8289 =========================

protected ContainerLauncher createContainerLauncher(AppContext context) {	return new MockContainerLauncher() {	public void handle(ContainerLauncherEvent event) {	if (event.getType() == EventType.CONTAINER_REMOTE_LAUNCH) {	ContainerRemoteLaunchEvent launchEvent = (ContainerRemoteLaunchEvent) event;	ContainerLaunchContext launchContext = launchEvent.getContainerLaunchContext();	String cmdString = launchContext.getCommands().toString();	
launchcontext 

========================= hadoop sample_5141 =========================

protected void serviceInit(Configuration conf) {	try {	this.fs = FileSystem.get(conf);	
initialized yarn registry with filesystem 

protected void serviceInit(Configuration conf) {	try {	this.fs = FileSystem.get(conf);	} catch (IOException e) {	
failed to get filesystem for registry 

RegistryTypeUtils.validateServiceRecord(path, record);	Path dataPath = formatDataPath(path);	Boolean overwrite = ((flags & BindFlags.OVERWRITE) != 0);	if (fs.exists(dataPath) && !overwrite) {	throw new FileAlreadyExistsException();	} else {	FSDataOutputStream stream = fs.create(dataPath);	byte[] bytes = serviceRecordMarshal.toBytes(record);	stream.write(bytes);	stream.close();	
bound record to path 

========================= hadoop sample_2684 =========================

public GetSubClusterInfoResponse getSubCluster( GetSubClusterInfoRequest request) throws YarnException {	FederationMembershipStateStoreInputValidator.validate(request);	SubClusterId subClusterId = request.getSubClusterId();	if (!membership.containsKey(subClusterId)) {	
the queried subcluster does not exist 

public GetSubClusterPolicyConfigurationResponse getPolicyConfiguration( GetSubClusterPolicyConfigurationRequest request) throws YarnException {	FederationPolicyStoreInputValidator.validate(request);	String queue = request.getQueue();	if (!policies.containsKey(queue)) {	
policy for queue does not exist 

========================= hadoop sample_1381 =========================

List<String> children = client.getChildren().forPath(path);	for ( String name : children ) {	String thisPath = ZKPaths.makePath(path, name);	Stat stat = client.checkExists().forPath(thisPath);	if ( (stat != null) && (stat.getNumChildren() == 0) ) {	reaper.addPath(thisPath, mode);	}	}	}	catch ( Exception e ) {	
could not get children for path 

========================= hadoop sample_3646 =========================

try {	parseDynamoDBRegion(paths);	} catch (ExitUtil.ExitException e) {	errorln(USAGE);	throw e;	}	try {	initMetadataStore(false);	} catch (FileNotFoundException e) {	println(out, "Metadata Store does not exist.");	
failed to bind to store to be destroyed 

public static int run(Configuration conf, String...args) throws Exception {	subCommand or instantiating the cmd object below */ String[] otherArgs = new GenericOptionsParser(conf, args) .getRemainingArgs();	if (otherArgs.length == 0) {	printHelp();	throw new ExitUtil.ExitException(E_USAGE, "No arguments provided");	}	final String subCommand = otherArgs[0];	
executing command 

========================= hadoop sample_6014 =========================

private CacheEntry waitForCompletion(CacheEntry newEntry) {	CacheEntry mapEntry = null;	lock.lock();	try {	mapEntry = set.get(newEntry);	if (mapEntry == null) {	if (LOG.isTraceEnabled()) {	
adding rpc request clientid callid to retrycache 

========================= hadoop sample_4029 =========================

try(MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).build()) {	DataNode dn = cluster.getDataNodes().get(0);	for (int i = 0; i < 100; i++) {	DFSTestUtil.writeFile( cluster.getFileSystem(), new Path("/foo" + String.valueOf(i) + ".txt"), "test content");	}	DataNodeTestUtils.triggerBlockReport(dn);	MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();	ObjectName mxbeanName = new ObjectName( "Hadoop:service=DataNode,name=DataNodeInfo");	String bpActorInfo = (String)mbs.getAttribute(mxbeanName, "BPServiceActorInfo");	Assert.assertEquals(dn.getBPServiceActorInfo(), bpActorInfo);	
bpactorinfo is 

DataNodeTestUtils.triggerBlockReport(dn);	MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();	ObjectName mxbeanName = new ObjectName( "Hadoop:service=DataNode,name=DataNodeInfo");	String bpActorInfo = (String)mbs.getAttribute(mxbeanName, "BPServiceActorInfo");	Assert.assertEquals(dn.getBPServiceActorInfo(), bpActorInfo);	TypeReference<ArrayList<Map<String, String>>> typeRef = new TypeReference<ArrayList<Map<String, String>>>() {};	ArrayList<Map<String, String>> bpActorInfoList = new ObjectMapper().readValue(bpActorInfo, typeRef);	int maxDataLength = Integer.valueOf(bpActorInfoList.get(0).get("maxDataLength"));	int confMaxDataLength = dn.getConf().getInt( CommonConfigurationKeys.IPC_MAXIMUM_DATA_LENGTH, CommonConfigurationKeys.IPC_MAXIMUM_DATA_LENGTH_DEFAULT);	int maxBlockReportSize = Integer.valueOf(bpActorInfoList.get(0).get("maxBlockReportSize"));	
maxdatalength is 

DataNodeTestUtils.triggerBlockReport(dn);	MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();	ObjectName mxbeanName = new ObjectName( "Hadoop:service=DataNode,name=DataNodeInfo");	String bpActorInfo = (String)mbs.getAttribute(mxbeanName, "BPServiceActorInfo");	Assert.assertEquals(dn.getBPServiceActorInfo(), bpActorInfo);	TypeReference<ArrayList<Map<String, String>>> typeRef = new TypeReference<ArrayList<Map<String, String>>>() {};	ArrayList<Map<String, String>> bpActorInfoList = new ObjectMapper().readValue(bpActorInfo, typeRef);	int maxDataLength = Integer.valueOf(bpActorInfoList.get(0).get("maxDataLength"));	int confMaxDataLength = dn.getConf().getInt( CommonConfigurationKeys.IPC_MAXIMUM_DATA_LENGTH, CommonConfigurationKeys.IPC_MAXIMUM_DATA_LENGTH_DEFAULT);	int maxBlockReportSize = Integer.valueOf(bpActorInfoList.get(0).get("maxBlockReportSize"));	
maxblockreportsize is 

========================= hadoop sample_7255 =========================

Configuration conf = getConfig();	YarnRPC rpc = YarnRPC.create(conf);	InetSocketAddress address = conf.getSocketAddr( YarnConfiguration.TIMELINE_SERVICE_BIND_HOST, YarnConfiguration.TIMELINE_SERVICE_ADDRESS, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_ADDRESS, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_PORT);	Preconditions.checkArgument(conf.getInt( YarnConfiguration.TIMELINE_SERVICE_HANDLER_THREAD_COUNT, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_CLIENT_THREAD_COUNT) > 0, "%s property value should be greater than zero", YarnConfiguration.TIMELINE_SERVICE_HANDLER_THREAD_COUNT);	server = rpc.getServer(ApplicationHistoryProtocol.class, this, address, conf, null, conf.getInt( YarnConfiguration.TIMELINE_SERVICE_HANDLER_THREAD_COUNT, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_CLIENT_THREAD_COUNT));	if (conf.getBoolean( CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION, false)) {	refreshServiceAcls(conf, new TimelinePolicyProvider());	}	server.start();	this.bindAddress = conf.updateConnectAddr(YarnConfiguration.TIMELINE_SERVICE_BIND_HOST, YarnConfiguration.TIMELINE_SERVICE_ADDRESS, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_ADDRESS, server.getListenerAddress());	
instantiated applicationhistoryclientservice at 

========================= hadoop sample_2057 =========================

cluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).format(false) .build();	cluster.waitActive();	fs = cluster.getFileSystem();	fsIn = fs.open(TEST_PATH);	try {	byte buf[] = new byte[100];	fsIn.seek(2000);	fsIn.readFully(buf, 0, buf.length);	Assert.fail("shouldn't be able to read from corrupt 0-length " + "block file.");	} catch (IOException e) {	
caught exception 

========================= hadoop sample_7605 =========================

public static void setupZooKeeper() throws Exception {	
starting zk server 

public static void setupZooKeeper() throws Exception {	zkTmpDir = File.createTempFile("zookeeper", "test");	zkTmpDir.delete();	zkTmpDir.mkdir();	try {	zks = new ZooKeeperServer(zkTmpDir, zkTmpDir, ZooKeeperDefaultPort);	serverFactory = new NIOServerCnxnFactory();	serverFactory.configure(new InetSocketAddress(ZooKeeperDefaultPort), 10);	serverFactory.startup(zks);	} catch (Exception e) {	
exception while instantiating zookeeper 

zkTmpDir.delete();	zkTmpDir.mkdir();	try {	zks = new ZooKeeperServer(zkTmpDir, zkTmpDir, ZooKeeperDefaultPort);	serverFactory = new NIOServerCnxnFactory();	serverFactory.configure(new InetSocketAddress(ZooKeeperDefaultPort), 10);	serverFactory.startup(zks);	} catch (Exception e) {	}	boolean b = LocalBookKeeper.waitForServerUp(HOSTPORT, CONNECTION_TIMEOUT);	
zookeeper server up 

========================= hadoop sample_7740 =========================

public void testBadConfiguration() throws IOException {	Configuration conf = new Configuration();	conf.set(AWS_CREDENTIALS_PROVIDER, "no.such.class");	try {	createFailingFS(conf);	} catch (IOException e) {	if (!(e.getCause() instanceof ClassNotFoundException)) {	
unexpected nested cause in 

========================= hadoop sample_5950 =========================

if (sub != null && sub.length() > 0) {	path = Path.mergePaths(path, new Path("/" + sub));	}	path = Path.mergePaths(path, new Path("/" + getDataFileName()));	try {	FSDataInputStream fdis = fs.open(path);	InputStreamReader isr = new InputStreamReader(fdis, StandardCharsets.UTF_8);	BufferedReader reader = new BufferedReader(isr);	return reader;	} catch (IOException ex) {	
cannot open write stream for to 

if (sub != null && sub.length() > 0) {	path = Path.mergePaths(path, new Path("/" + sub));	}	path = Path.mergePaths(path, new Path("/" + getDataFileName()));	try {	FSDataOutputStream fdos = fs.create(path, true);	OutputStreamWriter osw = new OutputStreamWriter(fdos, StandardCharsets.UTF_8);	BufferedWriter writer = new BufferedWriter(osw);	return writer;	} catch (IOException ex) {	
cannot open write stream for to 

========================= hadoop sample_8231 =========================

ContainerId containerId = ContainerId.newContainerId(appAttemptId, i);	TaskAttemptId taskAttemptId = MRBuilderUtils.newTaskAttemptId(taskId, i);	containerLauncher.handle(new ContainerLauncherEvent(taskAttemptId, containerId, "host" + i + ":1234", null, ContainerLauncher.EventType.CONTAINER_REMOTE_LAUNCH));	}	waitForEvents(containerLauncher, 10);	Assert.assertEquals(10, threadPool.getPoolSize());	Assert.assertNull(containerLauncher.foundErrors);	containerLauncher.finishEventHandling = true;	int timeOut = 0;	while (containerLauncher.numEventsProcessed.get() < 10 && timeOut++ < 200) {	
waiting for number of events processed to become it is now timeout is 

private void waitForEvents(CustomContainerLauncher containerLauncher, int expectedNumEvents) throws InterruptedException {	int timeOut = 0;	while (containerLauncher.numEventsProcessing.get() < expectedNumEvents && timeOut++ < 20) {	
waiting for number of events to become it is now 

Map<TaskId, Task> tasks = job.getTasks();	Assert.assertEquals("Num tasks is not correct", 1, tasks.size());	Task task = tasks.values().iterator().next();	app.waitForState(task, TaskState.SCHEDULED);	Map<TaskAttemptId, TaskAttempt> attempts = tasks.values().iterator() .next().getAttempts();	Assert.assertEquals("Num attempts is not correct", maxAttempts, attempts.size());	TaskAttempt attempt = attempts.values().iterator().next();	app.waitForInternalState((TaskAttemptImpl) attempt, TaskAttemptStateInternal.ASSIGNED);	app.waitForState(job, JobState.FAILED);	String diagnostics = attempt.getDiagnostics().toString();	
attempt getdiagnostics 

public void run() {	
processing the event 

========================= hadoop sample_5116 =========================

MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(3).build();	cluster.waitActive();	try {	FileSystem nnfs = FileSystem.get(conf);	assertFalse(nnfs.exists(CHILD_FILE1));	try {	nnfs.setOwner(CHILD_FILE1, "foo", "bar");	assertTrue(false);	}	catch(java.io.FileNotFoundException e) {	
good got 

nnfs.setOwner(CHILD_FILE1, "foo", "bar");	assertTrue(false);	}	catch(java.io.FileNotFoundException e) {	}	try {	nnfs.setPermission(CHILD_FILE1, new FsPermission((short)0777));	assertTrue(false);	}	catch(java.io.FileNotFoundException e) {	
good got 

========================= hadoop sample_7081 =========================

public static String executeDockerCommand(DockerCommand dockerCommand, String containerId, Map<String, String> env, Configuration conf, PrivilegedOperationExecutor privilegedOperationExecutor, boolean disableFailureLogging) throws ContainerExecutionException {	DockerClient dockerClient = new DockerClient(conf);	String commandFile = dockerClient.writeCommandToTempFile(dockerCommand, containerId);	PrivilegedOperation dockerOp = new PrivilegedOperation( PrivilegedOperation.OperationType.RUN_DOCKER_CMD);	dockerOp.appendArgs(commandFile);	if (disableFailureLogging) {	dockerOp.disableFailureLogging();	}	if (LOG.isDebugEnabled()) {	
running docker command 

} else if (currentContainerStatus .equals(DockerContainerStatus.DEAD.getName())) {	dockerContainerStatus = DockerContainerStatus.DEAD;	} else if (currentContainerStatus .equals(DockerContainerStatus.EXITED.getName())) {	dockerContainerStatus = DockerContainerStatus.EXITED;	} else if (currentContainerStatus .equals(DockerContainerStatus.NONEXISTENT.getName())) {	dockerContainerStatus = DockerContainerStatus.NONEXISTENT;	} else {	dockerContainerStatus = DockerContainerStatus.UNKNOWN;	}	if (LOG.isDebugEnabled()) {	
container status containerid 

} else if (currentContainerStatus .equals(DockerContainerStatus.NONEXISTENT.getName())) {	dockerContainerStatus = DockerContainerStatus.NONEXISTENT;	} else {	dockerContainerStatus = DockerContainerStatus.UNKNOWN;	}	if (LOG.isDebugEnabled()) {	}	return dockerContainerStatus;	} catch (ContainerExecutionException e) {	if (LOG.isDebugEnabled()) {	
container status containerid 

========================= hadoop sample_1856 =========================

logAndSetStatus(reporter, "Iterating over reduction values for key " + key);	while (values.hasNext()) {	Text value = values.next();	try {	OperationOutput val = new OperationOutput(key, value);	if (collector == null) {	collector = val;	} else {	collector = OperationOutput.merge(collector, val);	}	
combined into with 

========================= hadoop sample_5546 =========================

if ("default".equals(strInterface)) {	return new String[] { cachedHostAddress };	}	NetworkInterface netIf;	try {	netIf = NetworkInterface.getByName(strInterface);	if (netIf == null) {	netIf = getSubinterface(strInterface);	}	} catch (SocketException e) {	
i o error finding interface 

}	if (hosts.isEmpty() && tryfallbackResolution) {	for (InetAddress address : addresses) {	final String canonicalHostName = address.getCanonicalHostName();	if (!InetAddresses.isInetAddress(canonicalHostName)) {	hosts.add(canonicalHostName);	}	}	}	if (hosts.isEmpty()) {	
unable to determine hostname for interface 

private static String resolveLocalHostname() {	String localhost;	try {	localhost = InetAddress.getLocalHost().getCanonicalHostName();	} catch (UnknownHostException e) {	
unable to determine local hostname falling back to localhost 

private static String resolveLocalHostIPAddress() {	String address;	try {	address = InetAddress.getLocalHost().getHostAddress();	} catch (UnknownHostException e) {	
unable to determine address of the host falling back to localhost address 

private static String resolveLocalHostIPAddress() {	String address;	try {	address = InetAddress.getLocalHost().getHostAddress();	} catch (UnknownHostException e) {	try {	address = InetAddress.getByName(LOCALHOST).getHostAddress();	} catch (UnknownHostException noLocalHostAddressException) {	
unable to determine local loopback address of localhost this system s network configuration is unsupported 

if ("default".equals(strInterface)) {	return Arrays.asList(InetAddress.getByName(cachedHostAddress));	}	NetworkInterface netIf;	try {	netIf = NetworkInterface.getByName(strInterface);	if (netIf == null) {	netIf = getSubinterface(strInterface);	}	} catch (SocketException e) {	
i o error finding interface 

========================= hadoop sample_3538 =========================

Path corruptFile = new Path("/testMissingBlocks/corruptFile");	DFSTestUtil.createFile(dfs, corruptFile, fileLen, (short)3, 0);	ExtendedBlock block = DFSTestUtil.getFirstBlock(dfs, corruptFile);	cluster.corruptReplica(0, block);	FSDataInputStream in = dfs.open(corruptFile);	try {	in.readFully(new byte[fileLen]);	} catch (ChecksumException ignored) {	}	in.close();	
waiting for missing blocks count to increase 

while (dfs.getMissingBlocksCount() <= 0) {	Thread.sleep(100);	}	assertTrue(dfs.getMissingBlocksCount() == 1);	assertEquals(4, dfs.getUnderReplicatedBlocksCount());	assertEquals(3, bm.getUnderReplicatedNotMissingBlocks());	MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();	ObjectName mxbeanName = new ObjectName( "Hadoop:service=NameNode,name=NameNodeInfo");	Assert.assertEquals(1, (long)(Long) mbs.getAttribute(mxbeanName, "NumberOfMissingBlocks"));	dfs.delete(corruptFile, true);	
waiting for missing blocks count to be zero 

========================= hadoop sample_7652 =========================

NamenodeProtocols spyNN = spy(preSpyNN);	DFSClient client = new DFSClient(null, spyNN, conf, null);	int maxBlockAcquires = client.getConf().getMaxBlockAcquireFailures();	assertTrue(maxBlockAcquires > 0);	DFSTestUtil.createFile(fs, file, fileSize, (short)1, 12345L /*seed*/);	doAnswer(new FailNTimesAnswer(preSpyNN, maxBlockAcquires + 1)) .when(spyNN).getBlockLocations(anyString(), anyLong(), anyLong());	try {	IOUtils.copyBytes(client.open(file.toString()), new IOUtils.NullOutputStream(), conf, true);	fail("Didn't get exception");	} catch (IOException ioe) {	
got expected exception 

assertTrue(maxBlockAcquires > 0);	DFSTestUtil.createFile(fs, file, fileSize, (short)1, 12345L /*seed*/);	doAnswer(new FailNTimesAnswer(preSpyNN, maxBlockAcquires + 1)) .when(spyNN).getBlockLocations(anyString(), anyLong(), anyLong());	try {	IOUtils.copyBytes(client.open(file.toString()), new IOUtils.NullOutputStream(), conf, true);	fail("Didn't get exception");	} catch (IOException ioe) {	}	doAnswer(new FailNTimesAnswer(preSpyNN, maxBlockAcquires)) .when(spyNN).getBlockLocations(anyString(), anyLong(), anyLong());	IOUtils.copyBytes(client.open(file.toString()), new IOUtils.NullOutputStream(), conf, true);	
starting test case for failure reset 

IOUtils.copyBytes(client.open(file.toString()), new IOUtils.NullOutputStream(), conf, true);	fail("Didn't get exception");	} catch (IOException ioe) {	}	doAnswer(new FailNTimesAnswer(preSpyNN, maxBlockAcquires)) .when(spyNN).getBlockLocations(anyString(), anyLong(), anyLong());	IOUtils.copyBytes(client.open(file.toString()), new IOUtils.NullOutputStream(), conf, true);	doAnswer(new FailNTimesAnswer(preSpyNN, maxBlockAcquires)) .when(spyNN).getBlockLocations(anyString(), anyLong(), anyLong());	DFSInputStream is = client.open(file.toString());	byte buf[] = new byte[10];	IOUtils.readFully(is, buf, 0, buf.length);	
first read successful after some failures 

NamenodeProtocols preSpyNN = cluster.getNameNodeRpc();	NamenodeProtocols spyNN = spy(preSpyNN);	DFSClient client = new DFSClient(null, spyNN, conf, null);	doAnswer(new Answer<LocatedBlock>() {	private int getBlockCount(LocatedBlock ret) throws IOException {	LocatedBlocks lb = cluster.getNameNodeRpc().getBlockLocations(src, 0, Long.MAX_VALUE);	assertEquals(lb.getLastLocatedBlock().getBlock(), ret.getBlock());	return lb.getLocatedBlocks().size();	}	public LocatedBlock answer(InvocationOnMock invocation) throws Throwable {	
called addblock 

} catch(NotReplicatedYetException e) {	throw new AssertionError("Unexpected exception", e);	}	final int blockCount2 = getBlockCount(ret2);	assertEquals(blockCount, blockCount2);	return ret2;	}	}).when(spyNN).addBlock(Mockito.anyString(), Mockito.anyString(), Mockito.<ExtendedBlock> any(), Mockito.<DatanodeInfo[]> any(), Mockito.anyLong(), Mockito.<String[]> any(), Mockito.<EnumSet<AddBlockFlag>> any());	doAnswer(new Answer<Boolean>() {	public Boolean answer(InvocationOnMock invocation) throws Throwable {	
called complete 

throw new AssertionError("Unexpected exception", e);	}	final int blockCount2 = getBlockCount(ret2);	assertEquals(blockCount, blockCount2);	return ret2;	}	}).when(spyNN).addBlock(Mockito.anyString(), Mockito.anyString(), Mockito.<ExtendedBlock> any(), Mockito.<DatanodeInfo[]> any(), Mockito.anyLong(), Mockito.<String[]> any(), Mockito.<EnumSet<AddBlockFlag>> any());	doAnswer(new Answer<Boolean>() {	public Boolean answer(InvocationOnMock invocation) throws Throwable {	if (!(Boolean)invocation.callRealMethod()) {	
complete call returned false not faking a retry rpc 

return ret2;	}	}).when(spyNN).addBlock(Mockito.anyString(), Mockito.anyString(), Mockito.<ExtendedBlock> any(), Mockito.<DatanodeInfo[]> any(), Mockito.anyLong(), Mockito.<String[]> any(), Mockito.<EnumSet<AddBlockFlag>> any());	doAnswer(new Answer<Boolean>() {	public Boolean answer(InvocationOnMock invocation) throws Throwable {	if (!(Boolean)invocation.callRealMethod()) {	return false;	}	try {	boolean ret = (Boolean) invocation.callRealMethod();	
complete call returned true faked second rpc returned 

}).when(spyNN).addBlock(Mockito.anyString(), Mockito.anyString(), Mockito.<ExtendedBlock> any(), Mockito.<DatanodeInfo[]> any(), Mockito.anyLong(), Mockito.<String[]> any(), Mockito.<EnumSet<AddBlockFlag>> any());	doAnswer(new Answer<Boolean>() {	public Boolean answer(InvocationOnMock invocation) throws Throwable {	if (!(Boolean)invocation.callRealMethod()) {	return false;	}	try {	boolean ret = (Boolean) invocation.callRealMethod();	return ret;	} catch (Throwable t) {	
idempotent retry threw exception 

public LocatedBlocks answer(InvocationOnMock invocation) throws IOException {	Object args[] = invocation.getArguments();	LocatedBlocks realAnswer = realNN.getBlockLocations( (String)args[0], (Long)args[1], (Long)args[2]);	if (failuresLeft-- > 0) {	
failntimesanswer injecting failure 

public LocatedBlocks answer(InvocationOnMock invocation) throws IOException {	Object args[] = invocation.getArguments();	LocatedBlocks realAnswer = realNN.getBlockLocations( (String)args[0], (Long)args[1], (Long)args[2]);	if (failuresLeft-- > 0) {	return makeBadBlockList(realAnswer);	}	
failntimesanswer no longer failing 

System.out.println("Testing DFSClient random waiting on busy blocks.");	int xcievers  = 2;	int fileLen   = 6*1024*1024;	int threads   = 50;	int retries   = 3;	int timeWin   = 300;	long timestamp = Time.now();	boolean pass = busyTest(xcievers, threads, fileLen, timeWin, retries);	long timestamp2 = Time.now();	if ( pass ) {	
test succeeded time spent sec 

int xcievers  = 2;	int fileLen   = 6*1024*1024;	int threads   = 50;	int retries   = 3;	int timeWin   = 300;	long timestamp = Time.now();	boolean pass = busyTest(xcievers, threads, fileLen, timeWin, retries);	long timestamp2 = Time.now();	if ( pass ) {	} else {	
test failed but relax time spent sec 

boolean pass = busyTest(xcievers, threads, fileLen, timeWin, retries);	long timestamp2 = Time.now();	if ( pass ) {	} else {	}	retries = 50;	timestamp = Time.now();	pass = busyTest(xcievers, threads, fileLen, timeWin, retries);	timestamp2 = Time.now();	assertTrue("Something wrong! Test 2 got Exception with maxmum retries!", pass);	
test succeeded time spent sec 

timestamp = Time.now();	pass = busyTest(xcievers, threads, fileLen, timeWin, retries);	timestamp2 = Time.now();	assertTrue("Something wrong! Test 2 got Exception with maxmum retries!", pass);	retries = 3;	timeWin = 1000;	timestamp = Time.now();	pass = busyTest(xcievers, threads, fileLen, timeWin, retries);	timestamp2 = Time.now();	if ( pass ) {	
test succeeded time spent sec 

pass = busyTest(xcievers, threads, fileLen, timeWin, retries);	timestamp2 = Time.now();	assertTrue("Something wrong! Test 2 got Exception with maxmum retries!", pass);	retries = 3;	timeWin = 1000;	timestamp = Time.now();	pass = busyTest(xcievers, threads, fileLen, timeWin, retries);	timestamp2 = Time.now();	if ( pass ) {	} else {	
test failed but relax time spent sec 

timestamp2 = Time.now();	if ( pass ) {	} else {	}	retries = 50;	timeWin = 1000;	timestamp = Time.now();	pass = busyTest(xcievers, threads, fileLen, timeWin, retries);	timestamp2 = Time.now();	assertTrue("Something wrong! Test 4 got Exception with maxmum retries!", pass);	
test succeeded time spent sec 

conf.setInt(DFSConfigKeys.DFS_DATANODE_SOCKET_REUSE_KEEPALIVE_KEY, 0);	MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(replicationFactor).build();	cluster.waitActive();	FileSystem fs = cluster.getFileSystem();	Path file1 = new Path("test_data.dat");	file1 = file1.makeQualified(fs.getUri(), fs.getWorkingDirectory());	try {	FSDataOutputStream stm = fs.create(file1, true, bufferSize, replicationFactor, blockSize);	assertTrue(file1 + " should be a file", fs.getFileStatus(file1).isFile());	System.out.println("Path : \"" + file1 + "\"");	
path 

IOUtils.readFully(in, buf, 0, bufferSize);	MessageDigest m = MessageDigest.getInstance("SHA");	m.update(buf, 0, bufferSize);	byte[] hash_sha = m.digest();	buf = null;	in.close();	fs.close();	assertTrue("hashed keys are not the same size", hash_sha.length == expected_sha.length);	assertTrue("hashed keys are not equal", Arrays.equals(hash_sha, expected_sha));	counter.inc();	
thread correctly read the block 

MessageDigest m = MessageDigest.getInstance("SHA");	m.update(buf, 0, bufferSize);	byte[] hash_sha = m.digest();	buf = null;	in.close();	fs.close();	assertTrue("hashed keys are not the same size", hash_sha.length == expected_sha.length);	assertTrue("hashed keys are not equal", Arrays.equals(hash_sha, expected_sha));	counter.inc();	} catch (BlockMissingException e) {	
bad blockmissingexception is caught 

final InetSocketAddress addr = NetUtils.getConnectAddress(server);	DatanodeID fakeDnId = DFSTestUtil.getLocalDatanodeID(addr.getPort());	ExtendedBlock b = new ExtendedBlock("fake-pool", new Block(12345L));	LocatedBlock fakeBlock = new LocatedBlock(b, new DatanodeInfo[0]);	ClientDatanodeProtocol proxy = null;	try {	proxy = DFSUtilClient.createClientDatanodeProtocolProxy( fakeDnId, conf, 500, false, fakeBlock);	proxy.getReplicaVisibleLength(new ExtendedBlock("bpid", 1));	fail ("Did not get expected exception: SocketTimeoutException");	} catch (SocketTimeoutException e) {	
got the expected exception sockettimeoutexception 

} catch (Exception e) {	exceptions.add(e);	}	}	}).start();	final Path file2 = new Path(dir, "bar");	DFSTestUtil.createFile(fs, file2, length, numDatanodes, 20120406L);	assertEquals(fs.getFileChecksum(file1), fs.getFileChecksum(file2));	assertTrue(HdfsUtils.isHealthy(uri));	final Path nonExisting = new Path(dir, "nonExisting");	
setpermission 

}).start();	final Path file2 = new Path(dir, "bar");	DFSTestUtil.createFile(fs, file2, length, numDatanodes, 20120406L);	assertEquals(fs.getFileChecksum(file1), fs.getFileChecksum(file2));	assertTrue(HdfsUtils.isHealthy(uri));	final Path nonExisting = new Path(dir, "nonExisting");	try {	fs.setPermission(nonExisting, new FsPermission((short)0));	fail();	} catch(FileNotFoundException fnfe) {	
good 

========================= hadoop sample_7202 =========================

private void cleanupDir(Path p) {	try {	
deleting 

private void cleanupDir(Path p) {	try {	fs.delete(p, true);	} catch (IOException e) {	
error deleting test dir 

protected final Path path(String pathString) {	Path p = new Path(pathString).makeQualified(fs.getUri(), getTestBaseDir());	
resolving 

========================= hadoop sample_3241 =========================

public void run() {	HealthCheckerExitStatus status = HealthCheckerExitStatus.SUCCESS;	try {	shexec.execute();	} catch (ExitCodeException e) {	status = HealthCheckerExitStatus.FAILED_WITH_EXIT_CODE;	if (Shell.WINDOWS && shexec.isTimedOut()) {	status = HealthCheckerExitStatus.TIMED_OUT;	}	} catch (Exception e) {	
caught exception 

private synchronized void setHealthStatus(boolean isHealthy, String output) {	
health status being set as 

private synchronized void setHealthStatus(boolean isHealthy, String output, long time) {	
health status being set as 

========================= hadoop sample_3611 =========================

if (null == this.storageInteractionLayer) {	if (!useSecureMode) {	this.storageInteractionLayer = new StorageInterfaceImpl();	} else {	this.storageInteractionLayer = new SecureStorageInterfaceImpl( useLocalSasKeyMode, conf);	}	}	configureAzureStorageSession();	createAzureStorageSession();	pageBlobDirs = getDirectorySet(KEY_PAGE_BLOB_DIRECTORIES);	
page blob directories 

this.storageInteractionLayer = new StorageInterfaceImpl();	} else {	this.storageInteractionLayer = new SecureStorageInterfaceImpl( useLocalSasKeyMode, conf);	}	}	configureAzureStorageSession();	createAzureStorageSession();	pageBlobDirs = getDirectorySet(KEY_PAGE_BLOB_DIRECTORIES);	userAgentId = conf.get(USER_AGENT_ID_KEY, USER_AGENT_ID_DEFAULT);	blockBlobWithCompationDirs = getDirectorySet( KEY_BLOCK_BLOB_WITH_COMPACTION_DIRECTORIES);	
block blobs with compaction directories 

userAgentId = conf.get(USER_AGENT_ID_KEY, USER_AGENT_ID_DEFAULT);	blockBlobWithCompationDirs = getDirectorySet( KEY_BLOCK_BLOB_WITH_COMPACTION_DIRECTORIES);	atomicRenameDirs = getDirectorySet(KEY_ATOMIC_RENAME_DIRECTORIES);	String hbaseRoot;	try {	hbaseRoot = verifyAndConvertToStandardFormat( sessionConfiguration.get("hbase.rootdir", "hbase"));	if (hbaseRoot != null) {	atomicRenameDirs.add(hbaseRoot);	}	} catch (URISyntaxException e) {	
unable to initialize hbase root as an atomic rename directory 

blockBlobWithCompationDirs = getDirectorySet( KEY_BLOCK_BLOB_WITH_COMPACTION_DIRECTORIES);	atomicRenameDirs = getDirectorySet(KEY_ATOMIC_RENAME_DIRECTORIES);	String hbaseRoot;	try {	hbaseRoot = verifyAndConvertToStandardFormat( sessionConfiguration.get("hbase.rootdir", "hbase"));	if (hbaseRoot != null) {	atomicRenameDirs.add(hbaseRoot);	}	} catch (URISyntaxException e) {	}	
atomic rename directories 

URI storageUri = new URI(getHTTPScheme() + ":" + PATH_DELIMITER + PATH_DELIMITER + accountName);	String containerName = getContainerFromAuthority(uri);	storageInteractionLayer.createBlobClient(storageUri);	suppressRetryPolicyInClientIfNeeded();	container = storageInteractionLayer.getContainerReference(containerName);	rootDirectory = container.getDirectoryReference("");	boolean canAccess;	try {	canAccess = container.exists(getInstrumentedContext());	} catch (StorageException ex) {	
service returned storageexception when checking existence of container in account 

}	String propertyValue = sessionConfiguration.get(KEY_ACCOUNT_SAS_PREFIX + containerName + "." + accountName);	if (propertyValue != null) {	connectUsingSASCredentials(accountName, containerName, propertyValue);	return;	}	propertyValue = getAccountKeyFromConfiguration(accountName, sessionConfiguration);	if (StringUtils.isNotEmpty(propertyValue)) {	connectUsingConnectionStringCredentials( getAccountFromAuthority(sessionUri), getContainerFromAuthority(sessionUri), propertyValue);	} else {	
the account access key is not configured for now try anonymous access 

return true;	}	try {	URI uriPageBlobDir = new URI(dir);	if (null == uriPageBlobDir.getAuthority()) {	if (key.startsWith(trim(defaultFS, "/") + "/" + dir + "/")){	return true;	}	}	} catch (URISyntaxException e) {	
uri syntax error creating uri for 

private InputStream openInputStream(CloudBlobWrapper blob) throws StorageException, IOException {	if (blob instanceof CloudBlockBlobWrapper) {	
using stream seek algorithm 

public FileMetadata retrieveMetadata(String key) throws IOException {	if (null == storageInteractionLayer) {	final String errMsg = String.format( "Storage session expected for URI '%s' but does not exist.", sessionUri);	throw new AssertionError(errMsg);	}	
retrieving metadata for 

}	try {	if (checkContainer(ContainerAccessType.PureRead) == ContainerState.DoesntExist) {	return null;	}	if (key.equals("/")) {	return new FileMetadata(key, 0, defaultPermissionNoBlobMetadata(), BlobMaterialization.Implicit);	}	CloudBlobWrapper blob = getBlobReference(key);	if (null != blob && blob.exists(getInstrumentedContext())) {	
found as an explicit blob checking if it s a file or folder 

}	if (key.equals("/")) {	return new FileMetadata(key, 0, defaultPermissionNoBlobMetadata(), BlobMaterialization.Implicit);	}	CloudBlobWrapper blob = getBlobReference(key);	if (null != blob && blob.exists(getInstrumentedContext())) {	try {	blob.downloadAttributes(getInstrumentedContext());	BlobProperties properties = blob.getProperties();	if (retrieveFolderAttribute(blob)) {	
is a folder blob 

return new FileMetadata(key, 0, defaultPermissionNoBlobMetadata(), BlobMaterialization.Implicit);	}	CloudBlobWrapper blob = getBlobReference(key);	if (null != blob && blob.exists(getInstrumentedContext())) {	try {	blob.downloadAttributes(getInstrumentedContext());	BlobProperties properties = blob.getProperties();	if (retrieveFolderAttribute(blob)) {	return new FileMetadata(key, properties.getLastModified().getTime(), getPermissionStatus(blob), BlobMaterialization.Explicit);	} else {	
is a normal blob 

}	} catch(StorageException e){	if (!NativeAzureFileSystemHelper.isFileNotFoundException(e)) {	throw e;	}	}	}	Iterable<ListBlobItem> objects = listRootBlobs( key, true, EnumSet.of(BlobListingDetails.METADATA), null, getInstrumentedContext());	for (ListBlobItem blobItem : objects) {	if (blobItem instanceof CloudBlockBlobWrapper || blobItem instanceof CloudPageBlobWrapper) {	
found blob as a directory using this file under it to infer its properties 

private void safeDelete(CloudBlobWrapper blob, SelfRenewingLease lease) throws StorageException {	OperationContext operationContext = getInstrumentedContext();	try {	blob.delete(operationContext, lease);	} catch (StorageException e) {	if (!NativeAzureFileSystemHelper.isFileNotFoundException(e)) {	
encountered storage exception for delete on blob exception details error code 

private void safeDelete(CloudBlobWrapper blob, SelfRenewingLease lease) throws StorageException {	OperationContext operationContext = getInstrumentedContext();	try {	blob.delete(operationContext, lease);	} catch (StorageException e) {	if (!NativeAzureFileSystemHelper.isFileNotFoundException(e)) {	}	if (e.getErrorCode() != null && "BlobNotFound".equals(e.getErrorCode()) && operationContext.getRequestResults().size() > 1 && operationContext.getRequestResults().get(0).getException() != null) {	
swallowing delete exception on retry 

} catch (IOException e) {	Throwable t = e.getCause();	if (t != null && t instanceof StorageException) {	StorageException se = (StorageException) t;	if ("LeaseIdMissing".equals(se.getErrorCode())){	SelfRenewingLease lease = null;	try {	lease = acquireLease(key);	return delete(key, lease);	} catch (AzureException e3) {	
got unexpected exception trying to acquire lease on 

lease = acquireLease(key);	return delete(key, lease);	} catch (AzureException e3) {	throw e3;	} finally {	try {	if (lease != null){	lease.free();	}	} catch (Exception e4){	
unable to free lease on 

public void rename(String srcKey, String dstKey, boolean acquireLease, SelfRenewingLease existingLease) throws IOException {	
moving to 

options.setRetryPolicyFactory(new RetryExponentialRetry( copyBlobMinBackoff, copyBlobDeltaBackoff, copyBlobMaxBackoff, copyBlobMaxRetries));	dstBlob.startCopyFromBlob(srcBlob, options, getInstrumentedContext());	} else {	throw se;	}	}	waitForCopyToComplete(dstBlob, getInstrumentedContext());	safeDelete(srcBlob, lease);	} catch (StorageException e) {	if (e.getHttpStatusCode() == HttpURLConnection.HTTP_UNAVAILABLE) {	
rename copyblob storageexception serverbusy retry complete will attempt client side copy for page blob 

opStream.write(buffer, 0, len);	}	opStream.flush();	opStream.close();	ipStream.close();	} else {	throw new AzureException(e);	}	safeDelete(srcBlob, lease);	} catch(StorageException se) {	
rename copyblob storageexception failed 

public SelfRenewingLease acquireLease(String key) throws AzureException {	
acquiring lease on 

protected void finalize() throws Throwable {	
finalize called 

========================= hadoop sample_6413 =========================

public RouterPolicyFacade(Configuration conf, FederationStateStoreFacade facade, SubClusterResolver resolver, SubClusterId homeSubcluster) throws FederationPolicyInitializationException {	this.federationFacade = facade;	this.subClusterResolver = resolver;	this.globalConfMap = new ConcurrentHashMap<>();	this.globalPolicyMap = new ConcurrentHashMap<>();	String defaultKey = YarnConfiguration.DEFAULT_FEDERATION_POLICY_KEY;	SubClusterPolicyConfiguration configuration = null;	try {	configuration = federationFacade.getPolicyConfiguration(defaultKey);	} catch (YarnException e) {	
no fallback behavior defined in store defaulting to xml configuration fallback behavior 

queue = YarnConfiguration.DEFAULT_QUEUE_NAME;	}	SubClusterPolicyConfiguration configuration = null;	try {	configuration = federationFacade.getPolicyConfiguration(queue);	} catch (YarnException e) {	String errMsg = "There is no policy configured for the queue: " + queue + ", falling back to defaults.";	LOG.warn(errMsg, e);	}	if (configuration == null) {	
there is no policies configured for queue we fallback to default policy for 

========================= hadoop sample_1339 =========================

public JniBasedUnixGroupsMappingWithFallback() {	if (NativeCodeLoader.isNativeCodeLoaded()) {	this.impl = new JniBasedUnixGroupsMapping();	} else {	
falling back to shell based 

========================= hadoop sample_3754 =========================

}	if (trashRoot.getParent().toString().startsWith(qpath)) {	throw new IOException("Cannot move \"" + path + "\" to the trash, as it contains the trash");	}	Path trashPath = makeTrashRelativePath(trashCurrent, path);	Path baseTrashPath = makeTrashRelativePath(trashCurrent, path.getParent());	IOException cause = null;	for (int i = 0; i < 2; i++) {	try {	if (!fs.mkdirs(baseTrashPath, PERMISSION)) {	
can t create mkdir trash directory 

}	Path trashPath = makeTrashRelativePath(trashCurrent, path);	Path baseTrashPath = makeTrashRelativePath(trashCurrent, path.getParent());	IOException cause = null;	for (int i = 0; i < 2; i++) {	try {	if (!fs.mkdirs(baseTrashPath, PERMISSION)) {	return false;	}	} catch (IOException e) {	
can t create trash directory 

} catch (IOException e) {	cause = e;	break;	}	try {	String orig = trashPath.toString();	while(fs.exists(trashPath)) {	trashPath = new Path(orig + Time.now());	}	fs.rename(path, trashPath, Rename.TO_TRASH);	
moved to trash at 

public void createCheckpoint(Date date) throws IOException {	Collection<FileStatus> trashRoots = fs.getTrashRoots(false);	for (FileStatus trashRoot: trashRoots) {	
trashpolicydefault createcheckpoint for trashroot 

public void deleteCheckpoint() throws IOException {	Collection<FileStatus> trashRoots = fs.getTrashRoots(false);	for (FileStatus trashRoot : trashRoots) {	
trashpolicydefault deletecheckpoint for trashroot 

if (now >= end) {	Collection<FileStatus> trashRoots;	trashRoots = fs.getTrashRoots(true);	for (FileStatus trashRoot : trashRoots) {	if (!trashRoot.isDirectory()) continue;	try {	TrashPolicyDefault trash = new TrashPolicyDefault(fs, conf);	trash.deleteCheckpoint(trashRoot.getPath());	trash.createCheckpoint(trashRoot.getPath(), new Date(now));	} catch (IOException e) {	
trash caught skipping 

if (!trashRoot.isDirectory()) continue;	try {	TrashPolicyDefault trash = new TrashPolicyDefault(fs, conf);	trash.deleteCheckpoint(trashRoot.getPath());	trash.createCheckpoint(trashRoot.getPath(), new Date(now));	} catch (IOException e) {	}	}	}	} catch (Exception e) {	
runtimeexception during trash emptier run 

} catch (IOException e) {	}	}	}	} catch (Exception e) {	}	}	try {	fs.close();	} catch(IOException e) {	
trash cannot close filesystem 

Path checkpointBase;	synchronized (CHECKPOINT) {	checkpointBase = new Path(trashRoot, CHECKPOINT.format(date));	}	Path checkpoint = checkpointBase;	Path current = new Path(trashRoot, CURRENT);	int attempt = 0;	while (true) {	try {	fs.rename(current, checkpoint, Rename.NONE);	
created trash checkpoint 

private void deleteCheckpoint(Path trashRoot) throws IOException {	
trashpolicydefault deletecheckpoint for trashroot 

Path path = dirs[i].getPath();	String dir = path.toUri().getPath();	String name = path.getName();	if (name.equals(CURRENT.getName())) {	continue;	}	long time;	try {	time = getTimeFromCheckpoint(name);	} catch (ParseException e) {	
unexpected item in trash ignoring 

continue;	}	long time;	try {	time = getTimeFromCheckpoint(name);	} catch (ParseException e) {	continue;	}	if ((now - deletionInterval) > time) {	if (fs.delete(path, true)) {	
deleted trash checkpoint 

}	long time;	try {	time = getTimeFromCheckpoint(name);	} catch (ParseException e) {	continue;	}	if ((now - deletionInterval) > time) {	if (fs.delete(path, true)) {	} else {	
couldn t delete checkpoint ignoring 

========================= hadoop sample_4091 =========================

public EventBatch poll() throws IOException, MissingEventsException {	try (TraceScope ignored = tracer.newScope("inotifyPoll")) {	if (lastReadTxid == -1) {	
poll lastreadtxid is reading current txid from nn 

EventBatchList el = namenode.getEditsFromTxid(lastReadTxid + 1);	if (el.getLastTxid() != -1) {	syncTxid = el.getSyncTxid();	it = el.getBatches().iterator();	long formerLastReadTxid = lastReadTxid;	lastReadTxid = el.getLastTxid();	if (el.getFirstTxid() != formerLastReadTxid + 1) {	throw new MissingEventsException(formerLastReadTxid + 1, el.getFirstTxid());	}	} else {	
poll read no edits from the nn when requesting edits after txid 

public EventBatch poll(long time, TimeUnit tu) throws IOException, InterruptedException, MissingEventsException {	EventBatch next;	try (TraceScope ignored = tracer.newScope("inotifyPollWithTimeout")) {	long initialTime = Time.monotonicNow();	long totalWait = TimeUnit.MILLISECONDS.convert(time, tu);	long nextWait = INITIAL_WAIT_MS;	while ((next = poll()) == null) {	long timeLeft = totalWait - (Time.monotonicNow() - initialTime);	if (timeLeft <= 0) {	
timed poll timed out 

long nextWait = INITIAL_WAIT_MS;	while ((next = poll()) == null) {	long timeLeft = totalWait - (Time.monotonicNow() - initialTime);	if (timeLeft <= 0) {	break;	} else if (timeLeft < nextWait * 2) {	nextWait = timeLeft;	} else {	nextWait *= 2;	}	
timed poll poll returned null sleeping for ms 

public EventBatch take() throws IOException, InterruptedException, MissingEventsException {	EventBatch next;	try (TraceScope ignored = tracer.newScope("inotifyTake")) {	int nextWaitMin = INITIAL_WAIT_MS;	while ((next = poll()) == null) {	int sleepTime = nextWaitMin + rng.nextInt(nextWaitMin);	
take poll returned null sleeping for ms 

========================= hadoop sample_6805 =========================

public static FSDataOutputStream wrapIfNecessary(Configuration conf, FSDataOutputStream out, boolean closeOutputStream) throws IOException {	if (isEncryptedSpillEnabled(conf)) {	out.write(ByteBuffer.allocate(8).putLong(out.getPos()).array());	byte[] iv = createIV(conf);	out.write(iv);	if (LOG.isDebugEnabled()) {	
iv written to stream 

if (length > -1) {	in = new LimitInputStream(in, length);	}	byte[] offsetArray = new byte[8];	IOUtils.readFully(in, offsetArray, 0, 8);	long offset = ByteBuffer.wrap(offsetArray).getLong();	CryptoCodec cryptoCodec = CryptoCodec.getInstance(conf);	byte[] iv = new byte[cryptoCodec.getCipherSuite().getAlgorithmBlockSize()];	IOUtils.readFully(in, iv, 0, cryptoCodec.getCipherSuite().getAlgorithmBlockSize());	if (LOG.isDebugEnabled()) {	
iv read from 

public static FSDataInputStream wrapIfNecessary(Configuration conf, FSDataInputStream in) throws IOException {	if (isEncryptedSpillEnabled(conf)) {	CryptoCodec cryptoCodec = CryptoCodec.getInstance(conf);	int bufferSize = getBufferSize(conf);	IOUtils.readFully(in, new byte[8], 0, 8);	byte[] iv = new byte[cryptoCodec.getCipherSuite().getAlgorithmBlockSize()];	IOUtils.readFully(in, iv, 0, cryptoCodec.getCipherSuite().getAlgorithmBlockSize());	if (LOG.isDebugEnabled()) {	
iv read from stream 

========================= hadoop sample_4838 =========================

BlockingService reconfigurationPbService = ReconfigurationProtocolService .newReflectiveBlockingService(reconfigurationProtocolXlator);	TraceAdminProtocolServerSideTranslatorPB traceAdminXlator = new TraceAdminProtocolServerSideTranslatorPB(this);	BlockingService traceAdminService = TraceAdminService .newReflectiveBlockingService(traceAdminXlator);	WritableRpcEngine.ensureInitialized();	InetSocketAddress serviceRpcAddr = nn.getServiceRpcServerAddress(conf);	if (serviceRpcAddr != null) {	String bindHost = nn.getServiceRpcServerBindHost(conf);	if (bindHost == null) {	bindHost = serviceRpcAddr.getHostName();	}	
service rpc server is binding to 

serviceRpcServer = null;	serviceRPCAddress = null;	}	InetSocketAddress lifelineRpcAddr = nn.getLifelineRpcServerAddress(conf);	if (lifelineRpcAddr != null) {	RPC.setProtocolEngine(conf, HAServiceProtocolPB.class, ProtobufRpcEngine.class);	String bindHost = nn.getLifelineRpcServerBindHost(conf);	if (bindHost == null) {	bindHost = lifelineRpcAddr.getHostName();	}	
lifeline rpc server is binding to 

nn.setRpcLifelineServerAddress(conf, lifelineRPCAddress);	} else {	lifelineRpcServer = null;	lifelineRPCAddress = null;	}	InetSocketAddress rpcAddr = nn.getRpcServerAddress(conf);	String bindHost = nn.getRpcServerBindHost(conf);	if (bindHost == null) {	bindHost = rpcAddr.getHostName();	}	
rpc server is binding to 

public void errorReport(NamenodeRegistration registration, int errorCode, String msg) throws IOException {	checkNNStartup();	namesystem.checkOperation(OperationCategory.UNCHECKED);	namesystem.checkSuperuserPrivilege();	verifyRequest(registration);	
error report from 

public HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable<CreateFlag> flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions) throws IOException {	checkNNStartup();	String clientMachine = getClientMachine();	if (stateChangeLog.isDebugEnabled()) {	
dir namenode create file for at 

public LastBlockWithStatus append(String src, String clientName, EnumSetWritable<CreateFlag> flag) throws IOException {	checkNNStartup();	String clientMachine = getClientMachine();	if (stateChangeLog.isDebugEnabled()) {	
dir namenode append file for at 

public boolean rename(String src, String dst) throws IOException {	checkNNStartup();	if(stateChangeLog.isDebugEnabled()) {	
dir namenode rename to 

public void rename2(String src, String dst, Options.Rename... options) throws IOException {	checkNNStartup();	if(stateChangeLog.isDebugEnabled()) {	
dir namenode rename to 

public boolean truncate(String src, long newLength, String clientName) throws IOException {	if(stateChangeLog.isDebugEnabled()) {	
dir namenode truncate to 

public boolean mkdirs(String src, FsPermission masked, boolean createParent) throws IOException {	checkNNStartup();	if(stateChangeLog.isDebugEnabled()) {	
dir namenode mkdirs 

public RollingUpgradeInfo rollingUpgrade(RollingUpgradeAction action) throws IOException {	checkNNStartup();	
rollingupgrade 

public DatanodeCommand cacheReport(DatanodeRegistration nodeReg, String poolId, List<Long> blockIds) throws IOException {	checkNNStartup();	verifyRequest(nodeReg);	if (blockStateChangeLog.isDebugEnabled()) {	
block namenode cachereport from blocks 

public void blockReceivedAndDeleted(final DatanodeRegistration nodeReg, String poolId, StorageReceivedDeletedBlocks[] receivedAndDeletedBlocks) throws IOException {	checkNNStartup();	verifyRequest(nodeReg);	metrics.incrBlockReceivedAndDeletedOps();	if(blockStateChangeLog.isDebugEnabled()) {	
block namenode blockreceivedanddeleted from blocks 

metrics.incrBlockReceivedAndDeletedOps();	if(blockStateChangeLog.isDebugEnabled()) {	}	final BlockManager bm = namesystem.getBlockManager();	for (final StorageReceivedDeletedBlocks r : receivedAndDeletedBlocks) {	bm.enqueueBlockOp(new Runnable() {	public void run() {	try {	namesystem.processIncrementalBlockReport(nodeReg, r);	} catch (Exception ex) {	
block namenode blockreceivedanddeleted failed from 

public void errorReport(DatanodeRegistration nodeReg, int errorCode, String msg) throws IOException {	checkNNStartup();	String dnName = (nodeReg == null) ? "Unknown DataNode" : nodeReg.toString();	if (errorCode == DatanodeProtocol.NOTIFY) {	
error report from 

public void errorReport(DatanodeRegistration nodeReg, int errorCode, String msg) throws IOException {	checkNNStartup();	String dnName = (nodeReg == null) ? "Unknown DataNode" : nodeReg.toString();	if (errorCode == DatanodeProtocol.NOTIFY) {	return;	}	verifyRequest(nodeReg);	if (errorCode == DatanodeProtocol.DISK_ERROR) {	
disk error on 

public void errorReport(DatanodeRegistration nodeReg, int errorCode, String msg) throws IOException {	checkNNStartup();	String dnName = (nodeReg == null) ? "Unknown DataNode" : nodeReg.toString();	if (errorCode == DatanodeProtocol.NOTIFY) {	return;	}	verifyRequest(nodeReg);	if (errorCode == DatanodeProtocol.DISK_ERROR) {	} else if (errorCode == DatanodeProtocol.FATAL_DISK_ERROR) {	
fatal disk error on 

checkNNStartup();	String dnName = (nodeReg == null) ? "Unknown DataNode" : nodeReg.toString();	if (errorCode == DatanodeProtocol.NOTIFY) {	return;	}	verifyRequest(nodeReg);	if (errorCode == DatanodeProtocol.DISK_ERROR) {	} else if (errorCode == DatanodeProtocol.FATAL_DISK_ERROR) {	namesystem.getBlockManager().getDatanodeManager().removeDatanode(nodeReg);	} else {	
error report from 

private void verifyRequest(NodeRegistration nodeReg) throws IOException {	final String id = nodeReg.getRegistrationID();	final String expectedID = namesystem.getRegistrationID();	if (!expectedID.equals(id)) {	
registration ids mismatched the id is but the expected id is 

public void refreshUserToGroupsMappings() throws IOException {	
refreshing all user to groups mappings requested by user 

public void refreshSuperUserGroupsConfiguration() {	
refreshing superuser proxy group mapping list 

public void refreshCallQueue() {	
refreshing call queue 

public String[] getGroupsForUser(String user) throws IOException {	if (LOG.isDebugEnabled()) {	
getting groups for user 

private void verifySoftwareVersion(DatanodeRegistration dnReg) throws IncorrectVersionException {	String dnVersion = dnReg.getSoftwareVersion();	if (VersionUtil.compareVersions(dnVersion, minimumDataNodeVersion) < 0) {	IncorrectVersionException ive = new IncorrectVersionException( minimumDataNodeVersion, dnVersion, "DataNode", "NameNode");	
dn 

String nnVersion = VersionInfo.getVersion();	if (!dnVersion.equals(nnVersion)) {	String messagePrefix = "Reported DataNode version '" + dnVersion + "' of DN " + dnReg + " does not match NameNode version '" + nnVersion + "'";	long nnCTime = nn.getFSImage().getStorage().getCTime();	long dnCTime = dnReg.getStorageInfo().getCTime();	if (nnCTime != dnCTime) {	IncorrectVersionException ive = new IncorrectVersionException( messagePrefix + " and CTime of DN ('" + dnCTime + "') does not match CTime of NN ('" + nnCTime + "')");	LOG.warn(ive.toString(), ive);	throw ive;	} else {	
note this is normal during a rolling upgrade 

int totalEvents = 0;	long maxSeenTxid = -1;	long firstSeenTxid = -1;	if (syncTxid > 0 && txid > syncTxid) {	return new EventBatchList(batches, firstSeenTxid, maxSeenTxid, syncTxid);	}	Collection<EditLogInputStream> streams = null;	try {	streams = log.selectInputStreams(txid, 0, null, readInProgress);	} catch (IllegalStateException e) {	
nn is transitioning from active to standby and fseditlog is closed could not read edits 

========================= hadoop sample_8118 =========================

public void testRenameTwiceInSnapshot() throws Exception {	hdfs.mkdirs(sub1);	hdfs.allowSnapshot(sub1);	DFSTestUtil.createFile(hdfs, file1, BLOCKSIZE, REPL, SEED);	hdfs.createSnapshot(sub1, snap1);	hdfs.rename(file1, file2);	hdfs.createSnapshot(sub1, snap2);	hdfs.rename(file2, file3);	SnapshotDiffReport diffReport;	diffReport = hdfs.getSnapshotDiffReport(sub1, snap1, snap2);	
difflist is 

hdfs.rename(file1, file2);	hdfs.createSnapshot(sub1, snap2);	hdfs.rename(file2, file3);	SnapshotDiffReport diffReport;	diffReport = hdfs.getSnapshotDiffReport(sub1, snap1, snap2);	List<DiffReportEntry> entries = diffReport.getDiffList();	assertTrue(entries.size() == 2);	assertTrue(existsInDiffReport(entries, DiffType.MODIFY, "", null));	assertTrue(existsInDiffReport(entries, DiffType.RENAME, file1.getName(), file2.getName()));	diffReport = hdfs.getSnapshotDiffReport(sub1, snap2, "");	
difflist is 

List<DiffReportEntry> entries = diffReport.getDiffList();	assertTrue(entries.size() == 2);	assertTrue(existsInDiffReport(entries, DiffType.MODIFY, "", null));	assertTrue(existsInDiffReport(entries, DiffType.RENAME, file1.getName(), file2.getName()));	diffReport = hdfs.getSnapshotDiffReport(sub1, snap2, "");	entries = diffReport.getDiffList();	assertTrue(entries.size() == 2);	assertTrue(existsInDiffReport(entries, DiffType.MODIFY, "", null));	assertTrue(existsInDiffReport(entries, DiffType.RENAME, file2.getName(), file3.getName()));	diffReport = hdfs.getSnapshotDiffReport(sub1, snap1, "");	
difflist is 

final Path sub2 = new Path(sub1, "sub2");	final Path sub2file1 = new Path(sub2, "sub2file1");	final Path sub2file2 = new Path(sub2, "sub2file2");	final String sub1snap1 = "sub1snap1";	hdfs.mkdirs(sub1);	hdfs.mkdirs(sub2);	DFSTestUtil.createFile(hdfs, sub2file1, BLOCKSIZE, REPL, SEED);	SnapshotTestHelper.createSnapshot(hdfs, sub1, sub1snap1);	hdfs.rename(sub2file1, sub2file2);	SnapshotDiffReport diffReport = hdfs.getSnapshotDiffReport(sub1, sub1snap1, "");	
difflist is diffreport tostring 

final Path sub2 = new Path(sub1, "sub2");	final Path sub3 = new Path(sub1, "sub3");	final Path sub2file1 = new Path(sub2, "sub2file1");	final String sub1snap1 = "sub1snap1";	hdfs.mkdirs(sub1);	hdfs.mkdirs(sub2);	DFSTestUtil.createFile(hdfs, sub2file1, BLOCKSIZE, REPL, SEED);	SnapshotTestHelper.createSnapshot(hdfs, sub1, sub1snap1);	hdfs.rename(sub2, sub3);	SnapshotDiffReport diffReport = hdfs.getSnapshotDiffReport(sub1, sub1snap1, "");	
difflist is diffreport tostring 

final Path fileInBar = new Path(bar, "file1");	hdfs.rename(file1InFoo, fileInBar);	final Path newDir = new Path(root, "newDir");	hdfs.rename(bar, newDir);	final Path file2InNewDir = new Path(newDir, "file2");	hdfs.rename(file2InFoo, file2InNewDir);	final Path file1InNewDir = new Path(newDir, "file1");	hdfs.rename(file3InFoo, file1InNewDir, Rename.OVERWRITE);	SnapshotTestHelper.createSnapshot(hdfs, root, "s1");	SnapshotDiffReport report = hdfs.getSnapshotDiffReport(root, "s0", "s1");	
difflist is report tostring 

========================= hadoop sample_7347 =========================

p.setMethod = m;	}	}	}	}	Iterator<Map.Entry<String, GetSetPair>> itr = ret.entrySet().iterator();	while (itr.hasNext()) {	Map.Entry<String, GetSetPair> cur = itr.next();	GetSetPair gsp = cur.getValue();	if ((gsp.getMethod == null) || (gsp.setMethod == null)) {	
exclude potential property s 

}	}	}	Iterator<Map.Entry<String, GetSetPair>> itr = ret.entrySet().iterator();	while (itr.hasNext()) {	Map.Entry<String, GetSetPair> cur = itr.next();	GetSetPair gsp = cur.getValue();	if ((gsp.getMethod == null) || (gsp.setMethod == null)) {	itr.remove();	} else {	
new property s type s 

}	}	Iterator<Map.Entry<String, GetSetPair>> itr = ret.entrySet().iterator();	while (itr.hasNext()) {	Map.Entry<String, GetSetPair> cur = itr.next();	GetSetPair gsp = cur.getValue();	if ((gsp.getMethod == null) || (gsp.setMethod == null)) {	itr.remove();	} else {	gsp.testValue = genTypeValue(gsp.type);	
testvalue s 

protected  <R, P> void validatePBImplRecord(Class<R> recordClass, Class<P> protoClass) throws Exception {	
validate s s 

========================= hadoop sample_2145 =========================

private void initializeUsageAndUserLimitForCompute(Resource clusterResource, String partition, LeafQueue leafQueue, Map<String, Resource> rollingResourceUsagePerUser) {	for (String user : leafQueue.getAllUsers()) {	rollingResourceUsagePerUser.put(user, Resources.clone( leafQueue.getUser(user).getResourceUsage().getUsed(partition)));	if (LOG.isDebugEnabled()) {	
rolling resource usage for user is 

private void preemptFromLeastStarvedApp(LeafQueue leafQueue, FiCaSchedulerApp app, Map<ApplicationAttemptId, Set<RMContainer>> selectedCandidates, Resource clusterResource, Resource totalPreemptedResourceAllowed, Map<String, Resource> resToObtainByPartition, Map<String, Resource> rollingResourceUsagePerUser) {	List<RMContainer> liveContainers = new ArrayList<>(app.getLiveContainers());	sortContainers(liveContainers);	if (LOG.isDebugEnabled()) {	
totalpreemptedresourceallowed for preemption at this round is 

continue;	}	if (null != preemptionContext.getKillableContainers() && preemptionContext .getKillableContainers().contains(c.getContainerId())) {	continue;	}	if (c.isAMContainer()) {	continue;	}	if (fifoPreemptionComputePlugin.skipContainerBasedOnIntraQueuePolicy(app, clusterResource, rollingUsedResourcePerUser, c)) {	if (LOG.isDebugEnabled()) {	
skipping container with resource as userlimit for user with resource usage is going under ul 

========================= hadoop sample_679 =========================

public synchronized void initialize(URI uri, Configuration conf ) throws IOException {	super.initialize(uri, conf);	setConf(conf);	UserParam.setUserPattern(conf.get( HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY, HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));	AclPermissionParam.setAclPermissionPattern(conf.get( HdfsClientConfigKeys.DFS_WEBHDFS_ACL_PERMISSION_PATTERN_KEY, HdfsClientConfigKeys.DFS_WEBHDFS_ACL_PERMISSION_PATTERN_DEFAULT));	int connectTimeout = (int) conf.getTimeDuration( HdfsClientConfigKeys.DFS_WEBHDFS_SOCKET_CONNECT_TIMEOUT_KEY, URLConnectionFactory.DEFAULT_SOCKET_TIMEOUT, TimeUnit.MILLISECONDS);	int readTimeout = (int) conf.getTimeDuration( HdfsClientConfigKeys.DFS_WEBHDFS_SOCKET_READ_TIMEOUT_KEY, URLConnectionFactory.DEFAULT_SOCKET_TIMEOUT, TimeUnit.MILLISECONDS);	boolean isOAuth = conf.getBoolean( HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_KEY, HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_DEFAULT);	if(isOAuth) {	
enabling in webhdfs 

super.initialize(uri, conf);	setConf(conf);	UserParam.setUserPattern(conf.get( HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY, HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));	AclPermissionParam.setAclPermissionPattern(conf.get( HdfsClientConfigKeys.DFS_WEBHDFS_ACL_PERMISSION_PATTERN_KEY, HdfsClientConfigKeys.DFS_WEBHDFS_ACL_PERMISSION_PATTERN_DEFAULT));	int connectTimeout = (int) conf.getTimeDuration( HdfsClientConfigKeys.DFS_WEBHDFS_SOCKET_CONNECT_TIMEOUT_KEY, URLConnectionFactory.DEFAULT_SOCKET_TIMEOUT, TimeUnit.MILLISECONDS);	int readTimeout = (int) conf.getTimeDuration( HdfsClientConfigKeys.DFS_WEBHDFS_SOCKET_READ_TIMEOUT_KEY, URLConnectionFactory.DEFAULT_SOCKET_TIMEOUT, TimeUnit.MILLISECONDS);	boolean isOAuth = conf.getBoolean( HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_KEY, HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_DEFAULT);	if(isOAuth) {	connectionFactory = URLConnectionFactory .newOAuth2URLConnectionFactory(connectTimeout, readTimeout, conf);	} else {	
not enabling in webhdfs 

protected synchronized Token<?> getDelegationToken() throws IOException {	if (delegationToken == null) {	Token<?> token = tokenSelector.selectToken( new Text(getCanonicalServiceName()), ugi.getTokens());	if (token != null) {	
using ugi token 

protected synchronized Token<?> getDelegationToken() throws IOException {	if (delegationToken == null) {	Token<?> token = tokenSelector.selectToken( new Text(getCanonicalServiceName()), ugi.getTokens());	if (token != null) {	canRefreshDelegationToken = false;	} else {	if (canRefreshDelegationToken) {	token = getDelegationToken(null);	if (token != null) {	
fetched new token 

if (cachedHomeDirectory == null) {	final HttpOpParam.Op op = GetOpParam.Op.GETHOMEDIRECTORY;	try {	String pathFromDelegatedFS = new FsPathResponseRunner<String>(op, null, new UserParam(ugi)) {	String decodeResponse(Map<?, ?> json) throws IOException {	return JsonUtilClient.getPath(json);	}	}   .run();	cachedHomeDirectory = new Path(pathFromDelegatedFS).makeQualified( this.getUri(), null);	} catch (IOException e) {	
unable to get homedirectory from original file system 

} catch(Exception e) {	throw new IOException("Unexpected HTTP response: code=" + code + " != " + op.getExpectedHttpResponseCode() + ", " + op.toQueryString() + ", message=" + conn.getResponseMessage(), e);	}	if (m == null) {	throw new IOException("Unexpected HTTP response: code=" + code + " != " + op.getExpectedHttpResponseCode() + ", " + op.toQueryString() + ", message=" + conn.getResponseMessage());	} else if (m.get(RemoteException.class.getSimpleName()) == null) {	return m;	}	IOException re = JsonUtilClient.toRemoteException(m);	if (re.getMessage() != null && re.getMessage().endsWith( StandbyException.class.getSimpleName())) {	
detected standbyexception 

private void shouldRetry(final IOException ioe, final int retry ) throws IOException {	InetSocketAddress nnAddr = getCurrentNNAddr();	if (checkRetry) {	try {	final RetryPolicy.RetryAction a = retryPolicy.shouldRetry( ioe, retry, 0, true);	boolean isRetry = a.action == RetryPolicy.RetryAction.RetryDecision.RETRY;	boolean isFailoverAndRetry = a.action == RetryPolicy.RetryAction.RetryDecision.FAILOVER_AND_RETRY;	if (isRetry || isFailoverAndRetry) {	
retrying connect to namenode already retried time s retry policy is delay ms 

boolean isRetry = a.action == RetryPolicy.RetryAction.RetryDecision.RETRY;	boolean isFailoverAndRetry = a.action == RetryPolicy.RetryAction.RetryDecision.FAILOVER_AND_RETRY;	if (isRetry || isFailoverAndRetry) {	if (isFailoverAndRetry) {	resetStateToFailOver();	}	Thread.sleep(a.delayMillis);	return;	}	} catch(Exception e) {	
original exception is 

public synchronized void close() throws IOException {	try {	if (canRefreshDelegationToken && delegationToken != null) {	cancelDelegationToken(delegationToken);	}	} catch (IOException ioe) {	
token cancel failed 

public BlockLocation[] getFileBlockLocations(final Path p, final long offset, final long length) throws IOException {	statistics.incrementReadOps(1);	storageStatistics.incrementOpCounter(OpType.GET_FILE_BLOCK_LOCATIONS);	BlockLocation[] locations = null;	try {	locations = getFileBlockLocations( GetOpParam.Op.GETFILEBLOCKLOCATIONS, p, offset, length);	} catch (RemoteException e) {	if(e.getMessage() != null && e.getMessage().contains( "Invalid value for webhdfs parameter") && e.getMessage().contains( GetOpParam.Op.GETFILEBLOCKLOCATIONS.toString())) {	
invalid webhdfs operation parameter fallback to use instead 

storageStatistics.incrementOpCounter(OpType.GET_TRASH_ROOT);	final HttpOpParam.Op op = GetOpParam.Op.GETTRASHROOT;	try {	String strTrashPath = new FsPathResponseRunner<String>(op, path) {	String decodeResponse(Map<?, ?> json) throws IOException {	return JsonUtilClient.getPath(json);	}	}.run();	return new Path(strTrashPath).makeQualified(getUri(), null);	} catch(IOException e) {	
cannot find trash root of 

========================= hadoop sample_6872 =========================

info = new Properties();	String versionInfoFile = component + "-version-info.properties";	InputStream is = null;	try {	is = Thread.currentThread().getContextClassLoader() .getResourceAsStream(versionInfoFile);	if (is == null) {	throw new IOException("Resource not found");	}	info.load(is);	} catch (IOException ex) {	
could not read 

public static void main(String[] args) {	
version 

========================= hadoop sample_3689 =========================

public FileMetadata retrieveMetadata(String key) throws IOException {	StorageObject object = null;	try {	
getting metadata for key from bucket 

public InputStream retrieve(String key) throws IOException {	try {	
getting key from bucket 

public InputStream retrieve(String key, long byteRangeStart) throws IOException {	try {	
getting key from bucket with byterangestart 

public void delete(String key) throws IOException {	try {	
deleting key from bucket 

public void copy(String srcKey, String dstKey) throws IOException {	try {	if(LOG.isDebugEnabled()) {	
copying srckey to dstkey in bucket 

========================= hadoop sample_6034 =========================

protected void serviceStart() throws Exception {	Runnable speculationBackgroundCore = new Runnable() {	public void run() {	while (!stopped && !Thread.currentThread().isInterrupted()) {	long backgroundRunStartTime = clock.getTime();	try {	int speculations = computeSpeculations();	long mininumRecomp = speculations > 0 ? soonestRetryAfterSpeculate : soonestRetryAfterNoSpeculate;	long wait = Math.max(mininumRecomp, clock.getTime() - backgroundRunStartTime);	if (speculations > 0) {	
we launched speculations sleeping milliseconds 

long backgroundRunStartTime = clock.getTime();	try {	int speculations = computeSpeculations();	long mininumRecomp = speculations > 0 ? soonestRetryAfterSpeculate : soonestRetryAfterNoSpeculate;	long wait = Math.max(mininumRecomp, clock.getTime() - backgroundRunStartTime);	if (speculations > 0) {	}	Object pollResult = scanControl.poll(wait, TimeUnit.MILLISECONDS);	} catch (InterruptedException e) {	if (!stopped) {	
background thread returning interrupted 

public void scanForSpeculations() {	
we got asked to run a debug speculation scan 

private synchronized void processSpeculatorEvent(SpeculatorEvent event) {	switch (event.getType()) {	case ATTEMPT_STATUS_UPDATE: statusUpdate(event.getReportedStatus(), event.getTimestamp());	break;	case TASK_CONTAINER_NEED_UPDATE: {	AtomicInteger need = containerNeed(event.getTaskID());	need.addAndGet(event.containersNeededChange());	break;	}	case ATTEMPT_START: {	
attempt start 

case TASK_CONTAINER_NEED_UPDATE: {	AtomicInteger need = containerNeed(event.getTaskID());	need.addAndGet(event.containersNeededChange());	break;	}	case ATTEMPT_START: {	estimator.enrollAttempt (event.getReportedStatus(), event.getTimestamp());	break;	}	case JOB_CREATE: {	
job create 

protected void addSpeculativeAttempt(TaskId taskID) {	
defaultspeculator addspeculativeattempt we are speculating 

========================= hadoop sample_5248 =========================

public void run() {	if (!metricsLog.isInfoEnabled() || !hasAppenders(metricsLog) || objectName == null) {	return;	}	
begin metrics dump 

final String mBeanNameName = MBeans.getMbeanNameName(mbeanName);	final Set<String> attributeNames = getFilteredAttributes(mBeanInfo);	final AttributeList attributes = server.getAttributes(mbeanName, attributeNames.toArray(new String[attributeNames.size()]));	for (Object o : attributes) {	final Attribute attribute = (Attribute) o;	final Object value = attribute.getValue();	final String valueStr = (value != null) ? value.toString() : "null";	metricsLog.info(mBeanNameName + ":" + attribute.getName() + "=" + trimLine(valueStr));	}	} catch (Exception e) {	
failed to get metrics for mbean 

final AttributeList attributes = server.getAttributes(mbeanName, attributeNames.toArray(new String[attributeNames.size()]));	for (Object o : attributes) {	final Attribute attribute = (Attribute) o;	final Object value = attribute.getValue();	final String valueStr = (value != null) ? value.toString() : "null";	metricsLog.info(mBeanNameName + ":" + attribute.getName() + "=" + trimLine(valueStr));	}	} catch (Exception e) {	}	}	
end metrics dump 

========================= hadoop sample_8180 =========================

public synchronized TimelineEntities getEntities(String entityType, Long limit, Long windowStart, Long windowEnd, String fromId, Long fromTs, NameValuePair primaryFilter, Collection<NameValuePair> secondaryFilters, EnumSet<Field> fields, CheckAcl checkAcl) throws IOException {	if (getServiceStopped()) {	
service stopped return null for the storage 

public synchronized TimelineEntity getEntity(String entityId, String entityType, EnumSet<Field> fieldsToRetrieve) {	if (getServiceStopped()) {	
service stopped return null for the storage 

public synchronized TimelineEvents getEntityTimelines(String entityType, SortedSet<String> entityIds, Long limit, Long windowStart, Long windowEnd, Set<String> eventTypes) {	if (getServiceStopped()) {	
service stopped return null for the storage 

public TimelineDomain getDomain(String domainId) throws IOException {	if (getServiceStopped()) {	
service stopped return null for the storage 

public TimelineDomains getDomains(String owner) throws IOException {	if (getServiceStopped()) {	
service stopped return null for the storage 

public synchronized TimelinePutResponse put(TimelineEntities data) {	TimelinePutResponse response = new TimelinePutResponse();	if (getServiceStopped()) {	
service stopped return null for the storage 

public void put(TimelineDomain domain) throws IOException {	if (getServiceStopped()) {	
service stopped return null for the storage 

========================= hadoop sample_2077 =========================

public static void setup() throws IOException {	if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {	
mrappjar not found not running test 

========================= hadoop sample_5618 =========================

public static UserGroupInformation verifyAdminAccess( YarnAuthorizationProvider authorizer, String method, String module, final Log LOG) throws IOException {	UserGroupInformation user;	try {	user = UserGroupInformation.getCurrentUser();	} catch (IOException ioe) {	
couldn t get current user 

public static UserGroupInformation verifyAdminAccess( YarnAuthorizationProvider authorizer, String method, String module, final Log LOG) throws IOException {	UserGroupInformation user;	try {	user = UserGroupInformation.getCurrentUser();	} catch (IOException ioe) {	RMAuditLogger.logFailure("UNKNOWN", method, "", "AdminService", "Couldn't get current user");	throw ioe;	}	if (!authorizer.isAdmin(user)) {	
user doesn t have permission to call 

user = UserGroupInformation.getCurrentUser();	} catch (IOException ioe) {	RMAuditLogger.logFailure("UNKNOWN", method, "", "AdminService", "Couldn't get current user");	throw ioe;	}	if (!authorizer.isAdmin(user)) {	RMAuditLogger.logFailure(user.getShortUserName(), method, "", module, RMAuditLogger.AuditConstants.UNAUTHORIZED_USER);	throw new AccessControlException("User " + user.getShortUserName() + " doesn't have permission" + " to call '" + method + "'");	}	if (LOG.isTraceEnabled()) {	
invoked by user 

========================= hadoop sample_696 =========================

public void testStartingWithUpgradeInProgressSucceeds() throws Exception {	MiniDFSCluster cluster = null;	try {	cluster = new MiniDFSCluster.Builder(conf) .nnTopology(MiniDFSNNTopology.simpleHATopology()) .numDataNodes(0) .build();	for (int i = 0; i < 2; i++) {	for (URI uri : cluster.getNameDirs(i)) {	File prevTmp = new File(new File(uri), Storage.STORAGE_TMP_PREVIOUS);	
creating previous tmp dir 

========================= hadoop sample_7449 =========================

case SIMPLE: {	return;	}	case TOKEN: {	protocol = "";	serverId = SaslRpcServer.SASL_DEFAULT_REALM;	break;	}	case KERBEROS: {	String fullName = UserGroupInformation.getCurrentUser().getUserName();	
kerberos principal name is 

throw new UnsupportedCallbackException(callback, "Unrecognized SASL DIGEST-MD5 Callback");	}	}	if (pc != null) {	TokenIdentifier tokenIdentifier = getIdentifier(nc.getDefaultName(), secretManager);	char[] password = getPassword(tokenIdentifier);	UserGroupInformation user = null;	user = tokenIdentifier.getUser();	connection.attemptingUser = user;	if (LOG.isDebugEnabled()) {	
sasl server digest callback setting password for client 

String authzid = ac.getAuthorizationID();	if (authid.equals(authzid)) {	ac.setAuthorized(true);	} else {	ac.setAuthorized(false);	}	if (ac.isAuthorized()) {	if (LOG.isDebugEnabled()) {	UserGroupInformation logUser = getIdentifier(authzid, secretManager).getUser();	String username = logUser == null ? null : logUser.getUserName();	
sasl server digest callback setting canonicalized client id 

}	if (ac != null) {	String authid = ac.getAuthenticationID();	String authzid = ac.getAuthorizationID();	if (authid.equals(authzid)) {	ac.setAuthorized(true);	} else {	ac.setAuthorized(false);	}	if (ac.isAuthorized()) {	
sasl server gssapi callback setting canonicalized client id 

========================= hadoop sample_3774 =========================

private void sleepBookie(final CountDownLatch latch, final BookieServer bookie) throws Exception {	Thread sleeper = new Thread() {	public void run() {	try {	bookie.suspendProcessing();	latch.await(2, TimeUnit.MINUTES);	bookie.resumeProcessing();	} catch (Exception e) {	
error suspending bookie 

========================= hadoop sample_7737 =========================

for(String rmId : getRMHAIds(conf)) {	String key = addSuffix(YarnConfiguration.RM_ADDRESS, rmId);	String addr = conf.get(key);	if (addr == null) {	continue;	}	InetSocketAddress s;	try {	s = NetUtils.createSocketAddr(addr);	} catch (Exception e) {	
exception in creating socket address 

========================= hadoop sample_291 =========================

final CountDownLatch addVolumeCompletionLatch = new CountDownLatch(newVolumeCount);	final Thread listStorageThread = new Thread(new Runnable() {	public void run() {	while (addVolumeCompletionLatch.getCount() != newVolumeCount) {	int i = 0;	while(i++ < 1000) {	try {	dn.getStorage().listStorageDirectories();	} catch (Exception e) {	listStorageError.set(true);	
error listing storage 

public void run() {	try {	r.setSeed(Time.now());	if (r.nextInt(10) > 4) {	int s = r.nextInt(10) + 1;	Thread.sleep(s * 100);	}	invocationOnMock.callRealMethod();	} catch (Throwable throwable) {	addVolumeError.set(true);	
error adding volume 

========================= hadoop sample_7222 =========================

}	if (!new File(argvSplit[0]).isAbsolute()) {	PathFinder finder = new PathFinder("PATH");	finder.prependPathComponent(currentDir.toString());	File f = finder.getAbsolutePath(argvSplit[0]);	if (f != null) {	argvSplit[0] = f.getAbsolutePath();	}	f = null;	}	
pipemapred exec 

addEnvironment(childEnv, job_.get("stream.addenvironment"));	envPut(childEnv, "TMPDIR", System.getProperty("java.io.tmpdir"));	ProcessBuilder builder = new ProcessBuilder(argvSplit);	builder.environment().putAll(childEnv.toMap());	sim = builder.start();	clientOut_ = new DataOutputStream(new BufferedOutputStream( sim.getOutputStream(), BUFFER_SIZE));	clientIn_ = new DataInputStream(new BufferedInputStream( sim.getInputStream(), BUFFER_SIZE));	clientErr_ = new DataInputStream(new BufferedInputStream(sim.getErrorStream()));	startTime_ = System.currentTimeMillis();	} catch (IOException e) {	
configuration exception 

ProcessBuilder builder = new ProcessBuilder(argvSplit);	builder.environment().putAll(childEnv.toMap());	sim = builder.start();	clientOut_ = new DataOutputStream(new BufferedOutputStream( sim.getOutputStream(), BUFFER_SIZE));	clientIn_ = new DataInputStream(new BufferedInputStream( sim.getInputStream(), BUFFER_SIZE));	clientErr_ = new DataInputStream(new BufferedInputStream(sim.getErrorStream()));	startTime_ = System.currentTimeMillis();	} catch (IOException e) {	throw new RuntimeException("configuration exception", e);	} catch (InterruptedException e)  {	
configuration exception 

lineReader = new LineReader((InputStream)clientErr_, job_);	while (lineReader.readLine(line) > 0) {	String lineStr = line.toString();	if (matchesReporter(lineStr)) {	if (matchesCounter(lineStr)) {	incrCounter(lineStr);	} else if (matchesStatus(lineStr)) {	processProvidedStatus_ = true;	setStatus(lineStr);	} else {	
cannot parse reporter line 

reporter.progress();	}	line.clear();	}	if (lineReader != null) {	lineReader.close();	}	if (clientErr_ != null) {	clientErr_.close();	clientErr_ = null;	
mrerrorthread done 

private void incrCounter(String line) {	String trimmedLine = line.substring(counterPrefix.length()).trim();	String[] columns = trimmedLine.split(",");	if (columns.length == 3) {	try {	reporter.incrCounter(columns[0], columns[1], Long.parseLong(columns[2]));	} catch (NumberFormatException e) {	
cannot parse counter increment from line 

private void incrCounter(String line) {	String trimmedLine = line.substring(counterPrefix.length()).trim();	String[] columns = trimmedLine.split(",");	if (columns.length == 3) {	try {	reporter.incrCounter(columns[0], columns[1], Long.parseLong(columns[2]));	} catch (NumberFormatException e) {	}	} else {	
cannot parse counter line 

public void mapRedFinished() {	try {	if (!doPipe_) {	
mapRedFinished 

} catch (IOException io) {	LOG.warn(io);	}	}	try {	waitOutputThreads();	} catch (IOException io) {	LOG.warn(io);	}	if (sim != null) sim.destroy();	
mapRedFinished 

LOG.warn(io);	}	}	try {	waitOutputThreads();	} catch (IOException io) {	LOG.warn(io);	}	if (sim != null) sim.destroy();	} catch (RuntimeException e) {	
pipemapred failed 

========================= hadoop sample_5835 =========================

public void close() throws IOException {	try {	serverSocket.close();	} catch(IOException e) {	
error closing tcppeerserver 

========================= hadoop sample_7808 =========================

public static void main(String[] args) throws Exception {	
starting the schema creation 

o.setRequired(false);	options.addOption(o);	o = new Option(SKIP_EXISTING_TABLE_OPTION_SHORT, "skipExistingTable", false, "skip existing Hbase tables and continue to create new tables");	o.setRequired(false);	options.addOption(o);	CommandLineParser parser = new PosixParser();	CommandLine commandLine = null;	try {	commandLine = parser.parse(options, args);	} catch (Exception e) {	
error 

private static void createAllSchemas(Configuration hbaseConf, boolean skipExisting) {	List<Exception> exceptions = new ArrayList<>();	try {	if (skipExisting) {	
will skip existing tables and continue on htable creation exceptions 

private static void createAllSchemas(Configuration hbaseConf, boolean skipExisting) {	List<Exception> exceptions = new ArrayList<>();	try {	if (skipExisting) {	}	createAllTables(hbaseConf, skipExisting);	
successfully created hbase schema 

private static void createAllSchemas(Configuration hbaseConf, boolean skipExisting) {	List<Exception> exceptions = new ArrayList<>();	try {	if (skipExisting) {	}	createAllTables(hbaseConf, skipExisting);	} catch (IOException e) {	
error in creating hbase tables 

private static void createAllSchemas(Configuration hbaseConf, boolean skipExisting) {	List<Exception> exceptions = new ArrayList<>();	try {	if (skipExisting) {	}	createAllTables(hbaseConf, skipExisting);	} catch (IOException e) {	exceptions.add(e);	}	if (exceptions.size() > 0) {	
schema creation finished with the following exceptions 

createAllTables(hbaseConf, skipExisting);	} catch (IOException e) {	exceptions.add(e);	}	if (exceptions.size() > 0) {	for (Exception e : exceptions) {	LOG.warn(e.getMessage());	}	System.exit(-1);	} else {	
schema creation finished successfully 

try {	conn = ConnectionFactory.createConnection(hbaseConf);	Admin admin = conn.getAdmin();	if (admin == null) {	throw new IOException("Cannot create table since admin is null");	}	try {	new EntityTable().createTable(admin, hbaseConf);	} catch (IOException e) {	if (skipExisting) {	
skip and continue on 

} catch (IOException e) {	if (skipExisting) {	} else {	throw e;	}	}	try {	new AppToFlowTable().createTable(admin, hbaseConf);	} catch (IOException e) {	if (skipExisting) {	
skip and continue on 

} catch (IOException e) {	if (skipExisting) {	} else {	throw e;	}	}	try {	new ApplicationTable().createTable(admin, hbaseConf);	} catch (IOException e) {	if (skipExisting) {	
skip and continue on 

} catch (IOException e) {	if (skipExisting) {	} else {	throw e;	}	}	try {	new FlowRunTable().createTable(admin, hbaseConf);	} catch (IOException e) {	if (skipExisting) {	
skip and continue on 

} catch (IOException e) {	if (skipExisting) {	} else {	throw e;	}	}	try {	new FlowActivityTable().createTable(admin, hbaseConf);	} catch (IOException e) {	if (skipExisting) {	
skip and continue on 

} catch (IOException e) {	if (skipExisting) {	} else {	throw e;	}	}	try {	new SubApplicationTable().createTable(admin, hbaseConf);	} catch (IOException e) {	if (skipExisting) {	
skip and continue on 

========================= hadoop sample_1175 =========================

PortmapMapping mapping = PortmapRequest.mapping(in);	String key = PortmapMapping.key(mapping);	if (LOG.isDebugEnabled()) {	LOG.debug("Portmap GETPORT key=" + key + " " + mapping);	}	PortmapMapping value = map.get(key);	int res = 0;	if (value != null) {	res = value.getPort();	if (LOG.isDebugEnabled()) {	
found mapping for key port 

if (LOG.isDebugEnabled()) {	LOG.debug("Portmap GETPORT key=" + key + " " + mapping);	}	PortmapMapping value = map.get(key);	int res = 0;	if (value != null) {	res = value.getPort();	if (LOG.isDebugEnabled()) {	}	} else {	
warning no mapping for key 

public void exceptionCaught(ChannelHandlerContext ctx, ExceptionEvent e) {	
encountered 

========================= hadoop sample_4398 =========================

public void write(DataOutput out) throws IOException {	
writing containertokenidentifierfortest to rpc layer 

========================= hadoop sample_1933 =========================

private static Node coreResolve(String hostName) {	List <String> tmpList = new ArrayList<String>(1);	tmpList.add(hostName);	List <String> rNameList = dnsToSwitchMapping.resolve(tmpList);	String rName = null;	if (rNameList == null || rNameList.get(0) == null) {	rName = NetworkTopology.DEFAULT_RACK;	if (LOG.isDebugEnabled()) {	
couldn t resolve falling back to 

tmpList.add(hostName);	List <String> rNameList = dnsToSwitchMapping.resolve(tmpList);	String rName = null;	if (rNameList == null || rNameList.get(0) == null) {	rName = NetworkTopology.DEFAULT_RACK;	if (LOG.isDebugEnabled()) {	}	} else {	rName = rNameList.get(0);	if (LOG.isDebugEnabled()) {	
resolved to 

========================= hadoop sample_2173 =========================

static Map<String, InetSocketAddress> getAddressesForNameserviceId( Configuration conf, String nsId, String defaultValue, String... keys) {	Collection<String> nnIds = getNameNodeIds(conf, nsId);	Map<String, InetSocketAddress> ret = Maps.newLinkedHashMap();	for (String nnId : emptyAsSingletonNull(nnIds)) {	String suffix = concatSuffixes(nsId, nnId);	String address = getConfValue(defaultValue, suffix, conf, keys);	if (address != null) {	InetSocketAddress isa = NetUtils.createSocketAddr(address);	if (isa.isUnresolved()) {	
namenode for remains unresolved for id check your hdfs site xml file to ensure namenodes are configured properly 

public static boolean isLocalAddress(InetSocketAddress targetAddr) {	InetAddress addr = targetAddr.getAddress();	Boolean cached = localAddrMap.get(addr.getHostAddress());	if (cached != null) {	
address is local not 

public static boolean isLocalAddress(InetSocketAddress targetAddr) {	InetAddress addr = targetAddr.getAddress();	Boolean cached = localAddrMap.get(addr.getHostAddress());	if (cached != null) {	return cached;	}	boolean local = NetUtils.isLocalAddress(addr);	
address is local not 

========================= hadoop sample_6999 =========================

public boolean isResourceAvailable() {	long availableSpace = df.getAvailable();	if (LOG.isDebugEnabled()) {	
space available on volume is 

public boolean isResourceAvailable() {	long availableSpace = df.getAvailable();	if (LOG.isDebugEnabled()) {	}	if (availableSpace < duReserved) {	
space available on volume is which is below the configured reserved amount 

========================= hadoop sample_8084 =========================

final YarnConfiguration conf = new YarnConfiguration();	conf.set(YarnConfiguration.RM_PRINCIPAL, "testuser/localhost@apache.org");	conf.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION, "kerberos");	UserGroupInformation.setConfiguration(conf);	ResourceScheduler scheduler = createMockScheduler(conf);	long initialInterval = 10000l;	long maxLifetime= 20000l;	long renewInterval = 10000l;	RMDelegationTokenSecretManager rmDtSecretManager = createRMDelegationTokenSecretManager( initialInterval, maxLifetime, renewInterval);	rmDtSecretManager.startThreads();	
creating delegationtokensecretmanager with initialinterval maxlifetime renewinterval 

final ClientRMService clientRMService = new ClientRMServiceForTest(conf, scheduler, rmDtSecretManager);	clientRMService.init(conf);	clientRMService.start();	ApplicationClientProtocol clientRMWithDT = null;	try {	UserGroupInformation loggedInUser = UserGroupInformation .createRemoteUser("testrenewer@APACHE.ORG");	Assert.assertEquals("testrenewer", loggedInUser.getShortUserName());	loggedInUser.setAuthenticationMethod(AuthenticationMethod.KERBEROS);	org.apache.hadoop.yarn.api.records.Token token = getDelegationToken(loggedInUser, clientRMService, loggedInUser.getShortUserName());	long tokenFetchTime = System.currentTimeMillis();	
got delegation token at 

} catch (IOException e) {	fail("Unexpected exception" + e);	}  catch (YarnException e) {	fail("Unexpected exception" + e);	}	while(System.currentTimeMillis() < tokenFetchTime + initialInterval / 2) {	Thread.sleep(500l);	}	long nextExpTime = renewDelegationToken(loggedInUser, clientRMService, token);	long renewalTime = System.currentTimeMillis();	
renewed token at nextexpirytime 

clientRMWithDT.getNewApplication(request);	} catch (IOException e) {	fail("Unexpected exception" + e);	} catch (YarnException e) {	fail("Unexpected exception" + e);	}	while(System.currentTimeMillis() < renewalTime + renewInterval) {	Thread.sleep(500l);	}	Thread.sleep(50l);	
at time token should be invalid 

} catch (Exception e) {	assertEquals(InvalidToken.class.getName(), e.getClass().getName());	assertTrue(e.getMessage().contains("is expired"));	}	if (clientRMWithDT != null) {	RPC.stopProxy(clientRMWithDT);	clientRMWithDT = null;	}	token = getDelegationToken(loggedInUser, clientRMService, loggedInUser.getShortUserName());	tokenFetchTime = System.currentTimeMillis();	
got delegation token at 

fail("Unexpected exception" + e);	} catch (YarnException e) {	fail("Unexpected exception" + e);	}	cancelDelegationToken(loggedInUser, clientRMService, token);	if (clientRMWithDT != null) {	RPC.stopProxy(clientRMWithDT);	clientRMWithDT = null;	}	clientRMWithDT = getClientRMProtocolWithDT(token, clientRMService.getBindAddress(), "loginuser2", conf);	
cancelled delegation token at 

========================= hadoop sample_638 =========================

public PageBlobOutputStream(final CloudPageBlobWrapper blob, final OperationContext opContext, final Configuration conf) throws StorageException {	this.blob = blob;	this.outBuffer = new ByteArrayOutputStream();	this.opContext = opContext;	this.lastQueuedTask = null;	this.ioQueue = new LinkedBlockingQueue<Runnable>();	this.ioThreadPool = new ThreadPoolExecutor(1, 1, 2, TimeUnit.SECONDS, ioQueue);	long pageBlobConfigSize = conf.getLong("fs.azure.page.blob.size", 0);	
read value of fs azure page blob size as from configuration if not present 

public synchronized void close() throws IOException {	if (closed) {	return;	}	
closing page blob output stream 

public synchronized void close() throws IOException {	if (closed) {	return;	}	flush();	checkStreamState();	ioThreadPool.shutdown();	try {	LOG.debug(ioThreadPool.toString());	if (!ioThreadPool.awaitTermination(10, TimeUnit.MINUTES)) {	
timed out after minutes waiting for io requests to finish 

checkStreamState();	ioThreadPool.shutdown();	try {	LOG.debug(ioThreadPool.toString());	if (!ioThreadPool.awaitTermination(10, TimeUnit.MINUTES)) {	NativeAzureFileSystemHelper.logAllLiveStackTraces();	LOG.debug(ioThreadPool.toString());	throw new IOException("Timed out waiting for IO requests to finish");	}	} catch (InterruptedException e) {	
caught interruptedexception 

public void run() {	try {	
before runinternal 

public void run() {	try {	runInternal();	
after runinternal 

private void writePayloadToServer(byte[] rawPayload) {	final ByteArrayInputStream wrapperStream = new ByteArrayInputStream(rawPayload);	
writing payload of bytes to azure page blob 

long end = System.currentTimeMillis();	LOG.trace("Azure uploadPages time for " + rawPayload.length + " bytes = " + (end - start));	} catch (IOException ex) {	LOG.debug(ExceptionUtils.getStackTrace(ex));	lastError = ex;	} catch (StorageException ex) {	LOG.debug(ExceptionUtils.getStackTrace(ex));	lastError = new IOException(ex);	}	if (lastError != null) {	
caught error in pagebloboutputstream writepayloadtoserver 

}	final int MAX_RETRIES = 3;	int retries = 1;	boolean resizeDone = false;	while(!resizeDone && retries <= MAX_RETRIES) {	try {	cloudPageBlob.resize(newSize);	resizeDone = true;	currentBlobSize = newSize;	} catch (StorageException e) {	
failed to extend size of 

public synchronized void hsync() throws IOException {	
entering pagebloboutputstream hsync 

========================= hadoop sample_6425 =========================

conf.setInt(DFSConfigKeys.DFS_NAMENODE_MAINTENANCE_REPLICATION_MIN_KEY, Math.min(maintenanceMinReplication, numDataNodes));	}	int safemodeExtension = conf.getInt( DFS_NAMENODE_SAFEMODE_EXTENSION_TESTING_KEY, 0);	conf.setInt(DFS_NAMENODE_SAFEMODE_EXTENSION_KEY, safemodeExtension);	int decommissionInterval = conf.getInt( DFS_NAMENODE_DECOMMISSION_INTERVAL_TESTING_KEY, 3);	conf.setInt(DFS_NAMENODE_DECOMMISSION_INTERVAL_KEY, decommissionInterval);	if (!useConfiguredTopologyMappingClass) {	conf.setClass(NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY, StaticMapping.class, DNSToSwitchMapping.class);	}	if (!nnTopology.allHttpPortsSpecified() && nnTopology.isHA()) {	
minidfscluster disabling checkpointing in the standby node since no http ports have been specified 

conf.setInt(DFS_NAMENODE_SAFEMODE_EXTENSION_KEY, safemodeExtension);	int decommissionInterval = conf.getInt( DFS_NAMENODE_DECOMMISSION_INTERVAL_TESTING_KEY, 3);	conf.setInt(DFS_NAMENODE_DECOMMISSION_INTERVAL_KEY, decommissionInterval);	if (!useConfiguredTopologyMappingClass) {	conf.setClass(NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY, StaticMapping.class, DNSToSwitchMapping.class);	}	if (!nnTopology.allHttpPortsSpecified() && nnTopology.isHA()) {	conf.setBoolean(DFS_HA_STANDBY_CHECKPOINTS_KEY, false);	}	if (!nnTopology.allIpcPortsSpecified() && nnTopology.isHA()) {	
minidfscluster disabling log roll triggering in the standby node since no ipc ports have been specified 

conf.setBoolean(DFS_HA_STANDBY_CHECKPOINTS_KEY, false);	}	if (!nnTopology.allIpcPortsSpecified() && nnTopology.isHA()) {	conf.setInt(DFS_HA_LOGROLL_PERIOD_KEY, -1);	}	EditLogFileOutputStream.setShouldSkipFsyncForTesting(skipFsyncForTesting);	federation = nnTopology.isFederated();	try {	createNameNodesAndSetConf( nnTopology, manageNameDfsDirs, manageNameDfsSharedDirs, enableManagedDfsDirsRedundancy, format, startOpt, clusterId, conf);	} catch (IOException ioe) {	
ioe creating namenodes permissions dump 

URI srcDir = Lists.newArrayList(srcDirs).get(0);	FileSystem dstFS = FileSystem.getLocal(dstConf).getRaw();	for (URI dstDir : dstDirs) {	Preconditions.checkArgument(!dstDir.equals(srcDir), "src and dst are the same: " + dstDir);	File dstDirF = new File(dstDir);	if (dstDirF.exists()) {	if (!FileUtil.fullyDelete(dstDirF)) {	throw new IOException("Unable to delete: " + dstDirF);	}	}	
copying namedir from primary node dir to 

public URI getURI(int nnIndex) {	String hostPort = nameNodes[nnIndex].nameNode.getNameNodeAddressHostPortString();	URI uri = null;	try {	uri = new URI("hdfs: } catch (URISyntaxException e) {	
unexpected urisyntaxexception 

public void waitNameNodeUp(int nnIndex) {	while (!isNameNodeUp(nnIndex)) {	try {	
waiting for namenode at to start 

public void waitClusterUp() throws IOException {	int i = 0;	if (numDataNodes > 0) {	while (!isClusterUp()) {	try {	
waiting for the mini hdfs cluster to start 

setupDatanodeAddress(dnConf, setupHostsFile, checkDataNodeAddrConfig);	if (manageDfsDirs) {	String dirs = makeDataNodeDirs(i, storageTypes == null ? null : storageTypes[i - curDatanodesNum]);	dnConf.set(DFS_DATANODE_DATA_DIR_KEY, dirs);	conf.set(DFS_DATANODE_DATA_DIR_KEY, dirs);	}	if (simulatedCapacities != null) {	SimulatedFSDataset.setFactory(dnConf);	dnConf.setLong(SimulatedFSDataset.CONFIG_PROPERTY_CAPACITY, simulatedCapacities[i-curDatanodesNum]);	}	
starting datanode with 

String dirs = makeDataNodeDirs(i, storageTypes == null ? null : storageTypes[i - curDatanodesNum]);	dnConf.set(DFS_DATANODE_DATA_DIR_KEY, dirs);	conf.set(DFS_DATANODE_DATA_DIR_KEY, dirs);	}	if (simulatedCapacities != null) {	SimulatedFSDataset.setFactory(dnConf);	dnConf.setLong(SimulatedFSDataset.CONFIG_PROPERTY_CAPACITY, simulatedCapacities[i-curDatanodesNum]);	}	if (hosts != null) {	dnConf.set(DFSConfigKeys.DFS_DATANODE_HOST_NAME_KEY, hosts[i - curDatanodesNum]);	
starting datanode with hostname set to 

}	if (simulatedCapacities != null) {	SimulatedFSDataset.setFactory(dnConf);	dnConf.setLong(SimulatedFSDataset.CONFIG_PROPERTY_CAPACITY, simulatedCapacities[i-curDatanodesNum]);	}	if (hosts != null) {	dnConf.set(DFSConfigKeys.DFS_DATANODE_HOST_NAME_KEY, hosts[i - curDatanodesNum]);	}	if (racks != null) {	String name = hosts[i - curDatanodesNum];	
adding node with hostname to rack 

}	++numRetries;	continue;	}	throw e;	}	}	if(dn == null) throw new IOException("Cannot start DataNode in " + dnConf.get(DFS_DATANODE_DATA_DIR_KEY));	String service = SecurityUtil.buildTokenService(dn.getXferAddress()).toString();	if (racks != null) {	
adding node with service to rack 

public void shutdown(boolean deleteDfsDir, boolean closeFileSystem) {	
shutting down the mini hdfs cluster 

public void shutdown(boolean deleteDfsDir, boolean closeFileSystem) {	if (checkExitOnShutdown)  {	if (ExitUtil.terminateCalled()) {	
test resulted in an unexpected exit 

if (ExitUtil.terminateCalled()) {	ExitUtil.resetFirstExitException();	throw new AssertionError("Test resulted in an unexpected exit");	}	}	if (closeFileSystem) {	for (FileSystem fs : fileSystems) {	try {	fs.close();	} catch (IOException ioe) {	
exception while closing file system 

public void shutdownDataNode(int dnIndex) {	
shutting down datanode 

private void stopAndJoinNameNode(NameNode nn) {	if (nn == null) {	return;	}	
shutting down the namenode 

shutdownNameNode(nnIndex);	if (args.length != 0) {	startOpt = null;	} else {	args = createArgs(startOpt);	}	NameNode nn = NameNode.createNameNode(args, conf);	nameNodes[nnIndex] = new NameNodeInfo(nn, nameserviceId, nnId, startOpt, conf);	if (waitActive) {	waitClusterUp();	
restarted the namenode 

public synchronized DataNodeProperties stopDataNode(int i) {	if (i < 0 || i >= dataNodes.size()) {	return null;	}	DataNodeProperties dnprop = dataNodes.remove(i);	DataNode dn = dnprop.datanode;	
minidfscluster stopping datanode from a total of datanodes 

public synchronized DataNodeProperties stopDataNodeForUpgrade(int i) throws IOException {	if (i < 0 || i >= dataNodes.size()) {	return null;	}	DataNodeProperties dnprop = dataNodes.remove(i);	DataNode dn = dnprop.datanode;	
minidfscluster stopping datanode from a total of datanodes 

waitDataNodeFullyStarted(curDn);	} catch (TimeoutException | InterruptedException e) {	throw new IOException(e);	}	try (FsDatasetSpi.FsVolumeReferences volumes = curDn.getFSDataset() .getFsVolumeReferences()) {	assert storageCapacities[curDnIdx].length == storagesPerDatanode;	assert volumes.size() == storagesPerDatanode;	int j = 0;	for (FsVolumeSpi fvs : volumes) {	FsVolumeImpl volume = (FsVolumeImpl) fvs;	
setcapacityfortesting for 

public synchronized boolean restartDataNodes(boolean keepPort) throws IOException {	for (int i = dataNodes.size() - 1; i >= 0; i--) {	if (!restartDataNode(i, keepPort)) return false;	
restarted datanode 

public void waitActive(int nnIndex) throws IOException {	if (nameNodes.length == 0 || nameNodes[nnIndex] == null || nameNodes[nnIndex].nameNode == null) {	return;	}	InetSocketAddress addr = nameNodes[nnIndex].nameNode.getServiceRpcAddress();	assert addr.getPort() != 0;	DFSClient client = new DFSClient(addr, conf);	while (shouldWait(client.datanodeReport(DatanodeReportType.LIVE), addr)) {	try {	
waiting for cluster to become active 

public void waitActive() throws IOException {	for (int index = 0; index < nameNodes.length; index++) {	int failedCount = 0;	while (true) {	try {	waitActive(index);	break;	} catch (IOException e) {	failedCount++;	if (failedCount > 1) {	
tried waitactive time s and failed giving up 

waitActive(index);	break;	} catch (IOException e) {	failedCount++;	if (failedCount > 1) {	throw e;	}	}	}	}	
cluster is active 

private synchronized boolean shouldWait(DatanodeInfo[] dnInfo, InetSocketAddress addr) {	for (DataNodeProperties dn : dataNodes) {	if (!dn.datanode.isConnectedToNN(addr)) {	
bpofferservice in datanode failed to connect to namenode at 

if (!dn.datanode.isConnectedToNN(addr)) {	return false;	}	}	if (dnInfo.length != numDataNodes) {	LOG.info("dnInfo.length != numDataNodes");	return true;	}	for (DataNodeProperties dn : dataNodes) {	if (!dn.datanode.isDatanodeFullyStarted()) {	
dn datanode isdatanodefullystarted 

LOG.info("dnInfo.length != numDataNodes");	return true;	}	for (DataNodeProperties dn : dataNodes) {	if (!dn.datanode.isDatanodeFullyStarted()) {	return true;	}	}	for (DatanodeInfo dn : dnInfo) {	if (dn.getCapacity() == 0 || dn.getLastUpdate() <= 0) {	
no heartbeat from datanode 

if (hostsFile.length() == 0) {	throw new IOException("Parameter dfs.hosts is not setup in conf");	}	String address = "127.0.0.1:" + NetUtils.getFreeSocketPort();	if (checkDataNodeAddrConfig) {	conf.setIfUnset(DFS_DATANODE_ADDRESS_KEY, address);	} else {	conf.set(DFS_DATANODE_ADDRESS_KEY, address);	}	addToFile(hostsFile, address);	
adding datanode to hosts file 

========================= hadoop sample_7664 =========================

public long uploadCompleted() {	long delta = upload.getProgress().getBytesTransferred() - lastBytesTransferred;	if (delta > 0) {	
write delta changed after finished bytes 

========================= hadoop sample_5989 =========================

try {	LocatedFileStatusFetcher locatedFileStatusFetcher = new LocatedFileStatusFetcher( job, dirs, recursive, inputFilter, false);	locatedFiles = locatedFileStatusFetcher.getFileStatuses();	} catch (InterruptedException e) {	throw new IOException("Interrupted while getting file statuses");	}	result = Iterables.toArray(locatedFiles, FileStatus.class);	}	sw.stop();	if (LOG.isDebugEnabled()) {	
time taken to get filestatuses 

LocatedFileStatusFetcher locatedFileStatusFetcher = new LocatedFileStatusFetcher( job, dirs, recursive, inputFilter, false);	locatedFiles = locatedFileStatusFetcher.getFileStatuses();	} catch (InterruptedException e) {	throw new IOException("Interrupted while getting file statuses");	}	result = Iterables.toArray(locatedFiles, FileStatus.class);	}	sw.stop();	if (LOG.isDebugEnabled()) {	}	
total input files to process 

splits.add(makeSplit(path, length-bytesRemaining, splitSize, splitHosts[0], splitHosts[1]));	bytesRemaining -= splitSize;	}	if (bytesRemaining != 0) {	String[][] splitHosts = getSplitHostsAndCachedHosts(blkLocations, length - bytesRemaining, bytesRemaining, clusterMap);	splits.add(makeSplit(path, length - bytesRemaining, bytesRemaining, splitHosts[0], splitHosts[1]));	}	} else {	if (LOG.isDebugEnabled()) {	if (length > Math.min(file.getBlockSize(), minSize)) {	
file is not splittable so no parallelization is possible 

}	String[][] splitHosts = getSplitHostsAndCachedHosts(blkLocations,0,length,clusterMap);	splits.add(makeSplit(path, 0, length, splitHosts[0], splitHosts[1]));	}	} else {	splits.add(makeSplit(path, 0, length, new String[0]));	}	}	sw.stop();	if (LOG.isDebugEnabled()) {	
total of splits generated by getsplits timetaken 

========================= hadoop sample_4731 =========================

public boolean loadCache(boolean force) {	try {	MembershipStore membership = getMembershipStore();	membership.loadCache(force);	} catch (IOException e) {	
cannot update membership from the state store 

GetNamenodeRegistrationsRequest request = GetNamenodeRegistrationsRequest.newInstance(partial);	MembershipStore membership = getMembershipStore();	GetNamenodeRegistrationsResponse response = membership.getNamenodeRegistrations(request);	List<MembershipState> records = response.getNamenodeMemberships();	if (records != null && records.size() == 1) {	MembershipState record = records.get(0);	UpdateNamenodeRegistrationRequest updateRequest = UpdateNamenodeRegistrationRequest.newInstance( record.getNameserviceId(), record.getNamenodeId(), ACTIVE);	membership.updateNamenodeRegistration(updateRequest);	}	} catch (StateStoreUnavailableException e) {	
cannot update as active state store unavailable 

public List<? extends FederationNamenodeContext> getNamenodesForNameserviceId( final String nsId) throws IOException {	List<? extends FederationNamenodeContext> ret = cacheNS.get(nsId);	if (ret == null) {	try {	MembershipState partial = MembershipState.newInstance();	partial.setNameserviceId(nsId);	GetNamenodeRegistrationsRequest request = GetNamenodeRegistrationsRequest.newInstance(partial);	final List<MembershipState> result = getRecentRegistrationForQuery(request, true, false);	if (result == null || result.isEmpty()) {	
cannot locate eligible nns for 

partial.setNameserviceId(nsId);	GetNamenodeRegistrationsRequest request = GetNamenodeRegistrationsRequest.newInstance(partial);	final List<MembershipState> result = getRecentRegistrationForQuery(request, true, false);	if (result == null || result.isEmpty()) {	return null;	} else {	cacheNS.put(nsId, result);	ret = result;	}	} catch (StateStoreUnavailableException e) {	
cannot get active nn for state store unavailable 

public List<? extends FederationNamenodeContext> getNamenodesForBlockPoolId( final String bpId) throws IOException {	List<? extends FederationNamenodeContext> ret = cacheBP.get(bpId);	if (ret == null) {	try {	MembershipState partial = MembershipState.newInstance();	partial.setBlockPoolId(bpId);	GetNamenodeRegistrationsRequest request = GetNamenodeRegistrationsRequest.newInstance(partial);	final List<MembershipState> result = getRecentRegistrationForQuery(request, true, false);	if (result == null || result.isEmpty()) {	
cannot locate eligible nns for 

MembershipState partial = MembershipState.newInstance();	partial.setBlockPoolId(bpId);	GetNamenodeRegistrationsRequest request = GetNamenodeRegistrationsRequest.newInstance(partial);	final List<MembershipState> result = getRecentRegistrationForQuery(request, true, false);	if (result == null || result.isEmpty()) {	} else {	cacheBP.put(bpId, result);	ret = result;	}	} catch (StateStoreUnavailableException e) {	
cannot get active nn for state store unavailable 

public boolean registerNamenode(NamenodeStatusReport report) throws IOException {	if (this.routerId == null) {	
cannot register namenode router id is not known 

if (membership.getState() == EXPIRED && !addExpired) {	iterator.remove();	} else if (membership.getState() == UNAVAILABLE && !addUnavailable) {	iterator.remove();	}	}	}	List<MembershipState> priorityList = new ArrayList<>();	priorityList.addAll(memberships);	Collections.sort(priorityList, new NamenodePriorityComparator());	
selected most recent nn for query 

========================= hadoop sample_8207 =========================

public void run() {	try {	updateNodeLabelsFromConfig(new YarnConfiguration());	} catch (Exception e) {	
failed to update node labels from configuration xml 

========================= hadoop sample_1925 =========================

fs.delete(dirnamePath, true);	} else {	nn.mkdirs(dirname, p, true);	nn.delete(dirname, true);	}	} catch (SafeModeException sme) {	} catch (Throwable e) {	if (e instanceof RemoteException && ((RemoteException)e).getClassName() .contains("SafeModeException")) {	return;	}	
got error in transaction thread 

cluster.waitActive();	fileSys = cluster.getFileSystem();	final FSNamesystem namesystem = cluster.getNamesystem();	FSImage fsimage = namesystem.getFSImage();	FSEditLog editLog = fsimage.getEditLog();	startTransactionWorkers(cluster, caughtErr);	for (int i = 0; i < NUM_SAVE_IMAGE && caughtErr.get() == null; i++) {	try {	Thread.sleep(20);	} catch (InterruptedException ignored) {}	
save entering safe mode 

FSImage fsimage = namesystem.getFSImage();	FSEditLog editLog = fsimage.getEditLog();	startTransactionWorkers(cluster, caughtErr);	for (int i = 0; i < NUM_SAVE_IMAGE && caughtErr.get() == null; i++) {	try {	Thread.sleep(20);	} catch (InterruptedException ignored) {}	namesystem.enterSafeMode(false);	long logStartTxId = fsimage.getStorage().getMostRecentCheckpointTxId() + 1;	verifyEditLogs(namesystem, fsimage, NNStorage.getInProgressEditsFileName(logStartTxId), logStartTxId);	
save saving namespace 

FSEditLog editLog = fsimage.getEditLog();	startTransactionWorkers(cluster, caughtErr);	for (int i = 0; i < NUM_SAVE_IMAGE && caughtErr.get() == null; i++) {	try {	Thread.sleep(20);	} catch (InterruptedException ignored) {}	namesystem.enterSafeMode(false);	long logStartTxId = fsimage.getStorage().getMostRecentCheckpointTxId() + 1;	verifyEditLogs(namesystem, fsimage, NNStorage.getInProgressEditsFileName(logStartTxId), logStartTxId);	namesystem.saveNamespace();	
save leaving safemode 

Thread.sleep(20);	} catch (InterruptedException ignored) {}	namesystem.enterSafeMode(false);	long logStartTxId = fsimage.getStorage().getMostRecentCheckpointTxId() + 1;	verifyEditLogs(namesystem, fsimage, NNStorage.getInProgressEditsFileName(logStartTxId), logStartTxId);	namesystem.saveNamespace();	long savedImageTxId = fsimage.getStorage().getMostRecentCheckpointTxId();	verifyEditLogs(namesystem, fsimage, NNStorage.getFinalizedEditsFileName(logStartTxId, savedImageTxId), logStartTxId);	assertEquals(fsimage.getStorage().getMostRecentCheckpointTxId(), editLog.getLastWrittenTxId() - 1);	namesystem.leaveSafeMode(false);	
save complete 

FSImage fsimage = namesystem.getFSImage();	FSEditLog editLog = fsimage.getEditLog();	JournalAndStream jas = editLog.getJournals().get(0);	EditLogFileOutputStream spyElos = spy((EditLogFileOutputStream)jas.getCurrentStream());	jas.setCurrentStreamForTests(spyElos);	final AtomicReference<Throwable> deferredException = new AtomicReference<Throwable>();	final CountDownLatch waitToEnterFlush = new CountDownLatch(1);	final Thread doAnEditThread = new Thread() {	public void run() {	try {	
starting mkdirs 

FSEditLog editLog = fsimage.getEditLog();	JournalAndStream jas = editLog.getJournals().get(0);	EditLogFileOutputStream spyElos = spy((EditLogFileOutputStream)jas.getCurrentStream());	jas.setCurrentStreamForTests(spyElos);	final AtomicReference<Throwable> deferredException = new AtomicReference<Throwable>();	final CountDownLatch waitToEnterFlush = new CountDownLatch(1);	final Thread doAnEditThread = new Thread() {	public void run() {	try {	namesystem.mkdirs("/test", new PermissionStatus("test","test", new FsPermission((short)00755)), true);	
mkdirs complete 

JournalAndStream jas = editLog.getJournals().get(0);	EditLogFileOutputStream spyElos = spy((EditLogFileOutputStream)jas.getCurrentStream());	jas.setCurrentStreamForTests(spyElos);	final AtomicReference<Throwable> deferredException = new AtomicReference<Throwable>();	final CountDownLatch waitToEnterFlush = new CountDownLatch(1);	final Thread doAnEditThread = new Thread() {	public void run() {	try {	namesystem.mkdirs("/test", new PermissionStatus("test","test", new FsPermission((short)00755)), true);	} catch (Throwable ioe) {	
got exception 

try {	namesystem.mkdirs("/test", new PermissionStatus("test","test", new FsPermission((short)00755)), true);	} catch (Throwable ioe) {	deferredException.set(ioe);	waitToEnterFlush.countDown();	}	}	};	Answer<Void> blockingFlush = new Answer<Void>() {	public Void answer(InvocationOnMock invocation) throws Throwable {	
flush called 

namesystem.mkdirs("/test", new PermissionStatus("test","test", new FsPermission((short)00755)), true);	} catch (Throwable ioe) {	deferredException.set(ioe);	waitToEnterFlush.countDown();	}	}	};	Answer<Void> blockingFlush = new Answer<Void>() {	public Void answer(InvocationOnMock invocation) throws Throwable {	if (useAsyncEditLog || Thread.currentThread() == doAnEditThread) {	
edit thread telling main thread we made it to flush section 

} catch (Throwable ioe) {	deferredException.set(ioe);	waitToEnterFlush.countDown();	}	}	};	Answer<Void> blockingFlush = new Answer<Void>() {	public Void answer(InvocationOnMock invocation) throws Throwable {	if (useAsyncEditLog || Thread.currentThread() == doAnEditThread) {	waitToEnterFlush.countDown();	
edit thread sleeping for secs 

deferredException.set(ioe);	waitToEnterFlush.countDown();	}	}	};	Answer<Void> blockingFlush = new Answer<Void>() {	public Void answer(InvocationOnMock invocation) throws Throwable {	if (useAsyncEditLog || Thread.currentThread() == doAnEditThread) {	waitToEnterFlush.countDown();	Thread.sleep(BLOCK_TIME*1000);	
going through to flush this will allow the main thread to continue 

}	}	};	Answer<Void> blockingFlush = new Answer<Void>() {	public Void answer(InvocationOnMock invocation) throws Throwable {	if (useAsyncEditLog || Thread.currentThread() == doAnEditThread) {	waitToEnterFlush.countDown();	Thread.sleep(BLOCK_TIME*1000);	}	invocation.callRealMethod();	
flush complete 

if (useAsyncEditLog || Thread.currentThread() == doAnEditThread) {	waitToEnterFlush.countDown();	Thread.sleep(BLOCK_TIME*1000);	}	invocation.callRealMethod();	return null;	}	};	doAnswer(blockingFlush).when(spyElos).flush();	doAnEditThread.start();	
main thread waiting to enter flush 

Thread.sleep(BLOCK_TIME*1000);	}	invocation.callRealMethod();	return null;	}	};	doAnswer(blockingFlush).when(spyElos).flush();	doAnEditThread.start();	waitToEnterFlush.await();	assertNull(deferredException.get());	
main thread detected that logsync is in unsynchronized section 

Thread.sleep(BLOCK_TIME*1000);	}	invocation.callRealMethod();	return null;	}	};	doAnswer(blockingFlush).when(spyElos).flush();	doAnEditThread.start();	waitToEnterFlush.await();	assertNull(deferredException.get());	
trying to enter safe mode 

Thread.sleep(BLOCK_TIME*1000);	}	invocation.callRealMethod();	return null;	}	};	doAnswer(blockingFlush).when(spyElos).flush();	doAnEditThread.start();	waitToEnterFlush.await();	assertNull(deferredException.get());	
this should block for sec since flush will sleep that long 

return null;	}	};	doAnswer(blockingFlush).when(spyElos).flush();	doAnEditThread.start();	waitToEnterFlush.await();	assertNull(deferredException.get());	long st = Time.now();	namesystem.setSafeMode(SafeModeAction.SAFEMODE_ENTER);	long et = Time.now();	
entered safe mode 

};	doAnswer(blockingFlush).when(spyElos).flush();	doAnEditThread.start();	waitToEnterFlush.await();	assertNull(deferredException.get());	long st = Time.now();	namesystem.setSafeMode(SafeModeAction.SAFEMODE_ENTER);	long et = Time.now();	assertTrue(et - st > (BLOCK_TIME - 1)*1000);	namesystem.saveNamespace();	
joining on edit thread 

long st = Time.now();	namesystem.setSafeMode(SafeModeAction.SAFEMODE_ENTER);	long et = Time.now();	assertTrue(et - st > (BLOCK_TIME - 1)*1000);	namesystem.saveNamespace();	doAnEditThread.join();	assertNull(deferredException.get());	assertEquals(3, verifyEditLogs(namesystem, fsimage, NNStorage.getFinalizedEditsFileName(1, 3), 1));	assertEquals(1, verifyEditLogs(namesystem, fsimage, NNStorage.getInProgressEditsFileName(4), 4));	} finally {	
closing nn 

DFSTestUtil.formatNameNode(conf);	final FSNamesystem namesystem = FSNamesystem.loadFromDisk(conf);	try {	FSImage fsimage = namesystem.getFSImage();	final FSEditLog editLog = fsimage.getEditLog();	final AtomicReference<Throwable> deferredException = new AtomicReference<Throwable>();	final CountDownLatch sleepingBeforeSync = new CountDownLatch(1);	final Thread doAnEditThread = new Thread() {	public void run() {	try {	
starting setowner 

final Thread doAnEditThread = new Thread() {	public void run() {	try {	namesystem.writeLock();	try {	editLog.logSetOwner("/","test","test");	} finally {	namesystem.writeUnlock();	}	sleepingBeforeSync.countDown();	
edit thread sleeping for secs 

try {	namesystem.writeLock();	try {	editLog.logSetOwner("/","test","test");	} finally {	namesystem.writeUnlock();	}	sleepingBeforeSync.countDown();	Thread.sleep(BLOCK_TIME*1000);	editLog.logSync();	
edit thread logsync complete 

namesystem.writeLock();	try {	editLog.logSetOwner("/","test","test");	} finally {	namesystem.writeUnlock();	}	sleepingBeforeSync.countDown();	Thread.sleep(BLOCK_TIME*1000);	editLog.logSync();	} catch (Throwable ioe) {	
got exception 

Thread.sleep(BLOCK_TIME*1000);	editLog.logSync();	} catch (Throwable ioe) {	deferredException.set(ioe);	sleepingBeforeSync.countDown();	}	}	};	doAnEditThread.setDaemon(true);	doAnEditThread.start();	
main thread waiting to just before logsync 

} catch (Throwable ioe) {	deferredException.set(ioe);	sleepingBeforeSync.countDown();	}	}	};	doAnEditThread.setDaemon(true);	doAnEditThread.start();	sleepingBeforeSync.await(200, TimeUnit.MILLISECONDS);	assertNull(deferredException.get());	
main thread detected that logsync about to be called 

} catch (Throwable ioe) {	deferredException.set(ioe);	sleepingBeforeSync.countDown();	}	}	};	doAnEditThread.setDaemon(true);	doAnEditThread.start();	sleepingBeforeSync.await(200, TimeUnit.MILLISECONDS);	assertNull(deferredException.get());	
trying to enter safe mode 

}	}	};	doAnEditThread.setDaemon(true);	doAnEditThread.start();	sleepingBeforeSync.await(200, TimeUnit.MILLISECONDS);	assertNull(deferredException.get());	long st = Time.now();	namesystem.setSafeMode(SafeModeAction.SAFEMODE_ENTER);	long et = Time.now();	
entered safe mode after ms 

};	doAnEditThread.setDaemon(true);	doAnEditThread.start();	sleepingBeforeSync.await(200, TimeUnit.MILLISECONDS);	assertNull(deferredException.get());	long st = Time.now();	namesystem.setSafeMode(SafeModeAction.SAFEMODE_ENTER);	long et = Time.now();	assertTrue(et - st < (BLOCK_TIME/2)*1000);	namesystem.saveNamespace();	
joining on edit thread 

long st = Time.now();	namesystem.setSafeMode(SafeModeAction.SAFEMODE_ENTER);	long et = Time.now();	assertTrue(et - st < (BLOCK_TIME/2)*1000);	namesystem.saveNamespace();	doAnEditThread.join();	assertNull(deferredException.get());	assertEquals(3, verifyEditLogs(namesystem, fsimage, NNStorage.getFinalizedEditsFileName(1, 3), 1));	assertEquals(1, verifyEditLogs(namesystem, fsimage, NNStorage.getInProgressEditsFileName(4), 4));	} finally {	
closing nn 

========================= hadoop sample_7408 =========================

public void trimWrite(int delta) {	Preconditions.checkState(delta < count);	if (LOG.isDebugEnabled()) {	
trim write request by delta 

public void writeData(HdfsDataOutputStream fos) throws IOException {	Preconditions.checkState(fos != null);	ByteBuffer dataBuffer;	try {	dataBuffer = getData();	} catch (Exception e1) {	
failed to get request data offset count error 

========================= hadoop sample_7058 =========================

try {	int code = httpNotification(notification.getUri(), notification.getTimeout());	if (code != 200) {	throw new IOException("Invalid response status code: " + code);	}	else {	break;	}	}	catch (IOException ioex) {	
notification error 

if (code != 200) {	throw new IOException("Invalid response status code: " + code);	}	else {	break;	}	}	catch (IOException ioex) {	}	catch (Exception ex) {	
notification error 

}	}	catch (IOException ioex) {	}	catch (Exception ex) {	}	try {	Thread.sleep(notification.getRetryInterval());	}	catch (InterruptedException iex) {	
notification retry error 

========================= hadoop sample_4674 =========================

final Path path = new Path("/test/testFilePartUpload");	int len = 8192;	final byte[] src = SwiftTestUtils.dataset(len, 32, 144);	FSDataOutputStream out = fs.create(path, false, getBufferSize(), (short) 1, BLOCK_SIZE);	try {	int totalPartitionsToWrite = len / PART_SIZE_BYTES;	assertPartitionsWritten("Startup", out, 0);	int firstWriteLen = 2048;	out.write(src, 0, firstWriteLen);	long expected = getExpectedPartitionsWritten(firstWriteLen, PART_SIZE_BYTES, false);	
first write predict d partitions written 

final byte[] src = SwiftTestUtils.dataset(len, 32, 144);	FSDataOutputStream out = fs.create(path, false, getBufferSize(), (short) 1, BLOCK_SIZE);	try {	int totalPartitionsToWrite = len / PART_SIZE_BYTES;	assertPartitionsWritten("Startup", out, 0);	int firstWriteLen = 2048;	out.write(src, 0, firstWriteLen);	long expected = getExpectedPartitionsWritten(firstWriteLen, PART_SIZE_BYTES, false);	assertPartitionsWritten("First write completed", out, expected);	int remainder = len - firstWriteLen;	
remainder writing d bytes 

final Path path = new Path("/test/testFilePartUploadLengthCheck");	int len = 8192;	final byte[] src = SwiftTestUtils.dataset(len, 32, 144);	FSDataOutputStream out = fs.create(path, false, getBufferSize(), (short) 1, BLOCK_SIZE);	try {	int totalPartitionsToWrite = len / PART_SIZE_BYTES;	assertPartitionsWritten("Startup", out, 0);	int firstWriteLen = 2048;	out.write(src, 0, firstWriteLen);	long expected = getExpectedPartitionsWritten(firstWriteLen, PART_SIZE_BYTES, false);	
first write predict d partitions written 

final byte[] src = SwiftTestUtils.dataset(len, 32, 144);	FSDataOutputStream out = fs.create(path, false, getBufferSize(), (short) 1, BLOCK_SIZE);	try {	int totalPartitionsToWrite = len / PART_SIZE_BYTES;	assertPartitionsWritten("Startup", out, 0);	int firstWriteLen = 2048;	out.write(src, 0, firstWriteLen);	long expected = getExpectedPartitionsWritten(firstWriteLen, PART_SIZE_BYTES, false);	assertPartitionsWritten("First write completed", out, expected);	int remainder = len - firstWriteLen;	
remainder writing d bytes 

========================= hadoop sample_6167 =========================

fileSys.mkdirs(new Path(TEST_PATH));	fileSys.mkdirs(new Path(TEST_PATH2));	sd = fsimage.getStorage().dirIterator(NameNodeDirType.EDITS).next();	} finally {	if (cluster != null) {	cluster.shutdown();	}	}	File editFile = FSImageTestUtil.findLatestEditsLog(sd).getFile();	assertTrue("Should exist: " + editFile, editFile.exists());	
corrupting edit log file 

} finally {	if (cluster != null) {	cluster.shutdown();	}	}	File editFile = FSImageTestUtil.findLatestEditsLog(sd).getFile();	assertTrue("Should exist: " + editFile, editFile.exists());	corruptor.corrupt(editFile);	cluster = null;	try {	
trying to start normally this should fail 

cluster = null;	try {	cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0) .enableManagedDfsDirsRedundancy(false).format(false).build();	cluster.waitActive();	cluster.shutdown();	if (needRecovery) {	fail("expected the corrupted edit log to prevent normal startup");	}	} catch (IOException e) {	if (!needRecovery) {	
got unexpected failure with 

if (!needRecovery) {	fail("got unexpected exception " + e.getMessage());	}	} finally {	if (cluster != null) {	cluster.shutdown();	}	}	cluster = null;	try {	
running recovery 

cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0) .enableManagedDfsDirsRedundancy(false).format(false) .startupOption(recoverStartOpt).build();	} catch (IOException e) {	fail("caught IOException while trying to recover. " + "message was " + e.getMessage() + "\nstack trace\n" + StringUtils.stringifyException(e));	} finally {	if (cluster != null) {	cluster.shutdown();	}	}	cluster = null;	try {	
starting cluster normally after recovery 

} catch (IOException e) {	fail("caught IOException while trying to recover. " + "message was " + e.getMessage() + "\nstack trace\n" + StringUtils.stringifyException(e));	} finally {	if (cluster != null) {	cluster.shutdown();	}	}	cluster = null;	try {	cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0) .enableManagedDfsDirsRedundancy(false).format(false).build();	
successfully recovered the corrupted edit log 

========================= hadoop sample_7326 =========================

protected void serviceStart() throws Exception {	SCMWebApp scmWebApp = new SCMWebApp(scm);	this.webApp = WebApps.$for("sharedcache").at(bindAddress).start(scmWebApp);	
instantiated at 

========================= hadoop sample_395 =========================

public void init(Configuration config, RMContext context, ResourceScheduler sched) {	
preemption monitor 

private void preemptOrkillSelectedContainerAfterWait( Map<ApplicationAttemptId, Set<RMContainer>> selectedCandidates, long currentTime) {	if (LOG.isDebugEnabled()) {	
starting to preempt containers for selectedcandidates and size 

for (String partitionToLookAt : allPartitions) {	cloneQueues(root, Resources .clone(nlm.getResourceByLabel(partitionToLookAt, clusterResources)), partitionToLookAt);	}	}	this.leafQueueNames = ImmutableSet.copyOf(getLeafQueueNames( getQueueByPartition(CapacitySchedulerConfiguration.ROOT, RMNodeLabelsManager.NO_LABEL)));	Resource totalPreemptionAllowed = Resources.multiply(clusterResources, percentageClusterPreemptionAllowed);	Map<ApplicationAttemptId, Set<RMContainer>> toPreempt = new HashMap<>();	for (PreemptionCandidatesSelector selector : candidatesSelectionPolicies) {	long startTime = 0;	if (LOG.isDebugEnabled()) {	
trying to use to select preemption candidates 

this.leafQueueNames = ImmutableSet.copyOf(getLeafQueueNames( getQueueByPartition(CapacitySchedulerConfiguration.ROOT, RMNodeLabelsManager.NO_LABEL)));	Resource totalPreemptionAllowed = Resources.multiply(clusterResources, percentageClusterPreemptionAllowed);	Map<ApplicationAttemptId, Set<RMContainer>> toPreempt = new HashMap<>();	for (PreemptionCandidatesSelector selector : candidatesSelectionPolicies) {	long startTime = 0;	if (LOG.isDebugEnabled()) {	startTime = clock.getTime();	}	toPreempt = selector.selectCandidates(toPreempt, clusterResources, totalPreemptionAllowed);	if (LOG.isDebugEnabled()) {	
uses millisecond to run 

long startTime = 0;	if (LOG.isDebugEnabled()) {	startTime = clock.getTime();	}	toPreempt = selector.selectCandidates(toPreempt, clusterResources, totalPreemptionAllowed);	if (LOG.isDebugEnabled()) {	int totalSelected = 0;	for (Set<RMContainer> set : toPreempt.values()) {	totalSelected += set.size();	}	
so far total containers selected to be preempted 

========================= hadoop sample_680 =========================

public void testRecoverAfterDoubleFailures() throws Exception {	final long MAX_IPC_NUMBER = determineMaxIpcNumber();	for (int failA = 1; failA <= MAX_IPC_NUMBER; failA++) {	for (int failB = 1; failB <= MAX_IPC_NUMBER; failB++) {	String injectionStr = "(" + failA + ", " + failB + ")";	
beginning test failing at 

cluster.waitActive();	QuorumJournalManager qjm = null;	try {	qjm = createInjectableQJM(cluster);	qjm.format(FAKE_NSINFO);	List<AsyncLogger> loggers = qjm.getLoggerSetForTests().getLoggersForTests();	failIpcNumber(loggers.get(0), failA);	failIpcNumber(loggers.get(1), failB);	int lastAckedTxn = doWorkload(cluster, qjm);	if (lastAckedTxn < 6) {	
failed after injecting failures at this is expected since we injected a failure in the majority 

public void testRandomized() throws Exception {	long seed;	Long userSpecifiedSeed = Long.getLong(RAND_SEED_PROPERTY);	if (userSpecifiedSeed != null) {	
using seed specified in system property 

public void testRandomized() throws Exception {	long seed;	Long userSpecifiedSeed = Long.getLong(RAND_SEED_PROPERTY);	if (userSpecifiedSeed != null) {	seed = userSpecifiedSeed;	GenericTestUtils.setLogLevel(ProtobufRpcEngine.LOG, Level.ALL);	} else {	seed = new Random().nextLong();	}	
random seed 

Random r = new Random(seed);	MiniJournalCluster cluster = new MiniJournalCluster.Builder(conf) .build();	cluster.waitActive();	QuorumJournalManager qjmForInitialFormat = createInjectableQJM(cluster);	qjmForInitialFormat.format(FAKE_NSINFO);	qjmForInitialFormat.close();	try {	long txid = 0;	long lastAcked = 0;	for (int i = 0; i < NUM_WRITER_ITERS; i++) {	
starting writer 

try {	long txid = 0;	long lastAcked = 0;	for (int i = 0; i < NUM_WRITER_ITERS; i++) {	QuorumJournalManager qjm = createRandomFaultyQJM(cluster, r);	try {	long recovered;	try {	recovered = QJMTestUtil.recoverAndReturnLastTxn(qjm);	} catch (Throwable t) {	
failed recovery 

}	assertTrue("Recovered only up to txnid " + recovered + " but had gotten an ack for " + lastAcked, recovered >= lastAcked);	txid = recovered + 1;	if (txid > 100 && i % 10 == 1) {	qjm.purgeLogsOlderThan(txid - 100);	}	Holder<Throwable> thrown = new Holder<Throwable>(null);	for (int j = 0; j < SEGMENTS_PER_WRITER; j++) {	lastAcked = writeSegmentUntilCrash(cluster, qjm, txid, 4, thrown);	if (thrown.held != null) {	
failed write 

private static int doWorkload(MiniJournalCluster cluster, QuorumJournalManager qjm) throws IOException {	int lastAcked = 0;	try {	qjm.recoverUnfinalizedSegments();	writeSegment(cluster, qjm, 1, 3, true);	lastAcked = 3;	writeSegment(cluster, qjm, 4, 3, true);	lastAcked = 6;	} catch (QuorumException qe) {	
failed to write at txid 

protected QJournalProtocol createProxy() throws IOException {	QJournalProtocol realProxy = super.createProxy();	return mockProxy( new WrapEveryCall<Object>(realProxy) {	void beforeCall(InvocationOnMock invocation) throws Exception {	if (random.nextFloat() < injectionProbability) {	isUp = !isUp;	
transitioned to up down 

protected QJournalProtocol createProxy() throws IOException {	final QJournalProtocol realProxy = super.createProxy();	QJournalProtocol mock = mockProxy( new WrapEveryCall<Object>(realProxy) {	void beforeCall(InvocationOnMock invocation) throws Exception {	rpcCount++;	String callStr = "[" + addr + "] " + invocation.getMethod().getName() + "(" + Joiner.on(", ").join(invocation.getArguments()) + ")";	Callable<Void> inject = injections.get(rpcCount);	if (inject != null) {	
injecting code before ipc 

protected QJournalProtocol createProxy() throws IOException {	final QJournalProtocol realProxy = super.createProxy();	QJournalProtocol mock = mockProxy( new WrapEveryCall<Object>(realProxy) {	void beforeCall(InvocationOnMock invocation) throws Exception {	rpcCount++;	String callStr = "[" + addr + "] " + invocation.getMethod().getName() + "(" + Joiner.on(", ").join(invocation.getArguments()) + ")";	Callable<Void> inject = injections.get(rpcCount);	if (inject != null) {	inject.call();	} else {	
ipc call 

========================= hadoop sample_7633 =========================

public boolean check(TimelineEntity entity) throws IOException {	try{	return timelineACLsManager.checkAccess( ugi, ApplicationAccessType.VIEW_APP, entity);	} catch (YarnException e) {	
error when verifying access for user on the events of the timeline entity 

Iterator<TimelineEvents.EventsOfOneEntity> eventsItr = events.getAllEvents().iterator();	while (eventsItr.hasNext()) {	TimelineEvents.EventsOfOneEntity eventsOfOneEntity = eventsItr.next();	try {	TimelineEntity entity = store.getEntity( eventsOfOneEntity.getEntityId(), eventsOfOneEntity.getEntityType(), EnumSet.of(Field.PRIMARY_FILTERS));	addDefaultDomainIdIfAbsent(entity);	if (!timelineACLsManager.checkAccess( callerUGI, ApplicationAccessType.VIEW_APP, entity)) {	eventsItr.remove();	}	} catch (Exception e) {	
error when verifying access for user on the events of the timeline entity 

if (existingEntity != null) {	addDefaultDomainIdIfAbsent(existingEntity);	if (!existingEntity.getDomainId().equals(entity.getDomainId())) {	throw new YarnException("The domain of the timeline entity " + "{ id: " + entity.getEntityId() + ", type: " + entity.getEntityType() + " } is not allowed to be changed from " + existingEntity.getDomainId() + " to " + entity.getDomainId());	}	}	if (!timelineACLsManager.checkAccess( callerUGI, ApplicationAccessType.MODIFY_APP, entity)) {	throw new YarnException(callerUGI + " is not allowed to put the timeline entity " + "{ id: " + entity.getEntityId() + ", type: " + entity.getEntityType() + " } into the domain " + entity.getDomainId() + ".");	}	} catch (Exception e) {	
skip the timeline entity id type 

========================= hadoop sample_2061 =========================

} catch (Exception ex) {	System.err.println(cmd.substring(1) + ": " + ex.getLocalizedMessage());	debugException = ex;	}	} catch (Exception e) {	exitCode = -1;	debugException = e;	System.err.println(cmd.substring(1) + ": " + e.getLocalizedMessage());	}	if (LOG.isDebugEnabled()) {	
exception encountered 

========================= hadoop sample_7799 =========================

int ns1 = FSImageTestUtil.getFSImage(nn1).getNamespaceID();	int ns2 = FSImageTestUtil.getFSImage(nn2).getNamespaceID();	assertNotSame("namespace ids should be different", ns1, ns2);	LOG.info("nn1: lv=" + lv1 + ";cid=" + cid1 + ";bpid=" + bpid1 + ";uri=" + nn1.getNameNodeAddress());	LOG.info("nn2: lv=" + lv2 + ";cid=" + cid2 + ";bpid=" + bpid2 + ";uri=" + nn2.getNameNodeAddress());	DataNode dn = cluster.getDataNodes().get(0);	final Map<String, Object> volInfos = dn.data.getVolumeInfoMap();	Assert.assertTrue("No volumes in the fsdataset", volInfos.size() > 0);	int i = 0;	for (Map.Entry<String, Object> e : volInfos.entrySet()) {	
vol 

LOG.info("nn1: lv=" + lv1 + ";cid=" + cid1 + ";bpid=" + bpid1 + ";uri=" + nn1.getNameNodeAddress());	LOG.info("nn2: lv=" + lv2 + ";cid=" + cid2 + ";bpid=" + bpid2 + ";uri=" + nn2.getNameNodeAddress());	DataNode dn = cluster.getDataNodes().get(0);	final Map<String, Object> volInfos = dn.data.getVolumeInfoMap();	Assert.assertTrue("No volumes in the fsdataset", volInfos.size() > 0);	int i = 0;	for (Map.Entry<String, Object> e : volInfos.entrySet()) {	}	assertEquals("number of volumes is wrong", cluster.getFsDatasetTestUtils(0).getDefaultNumOfDataDirs(), volInfos.size());	for (BPOfferService bpos : dn.getAllBpOs()) {	
bp 

assertNotNull("cannot create nn1", nn1);	String bpid1 = FSImageTestUtil.getFSImage(nn1).getBlockPoolID();	String cid1 = FSImageTestUtil.getFSImage(nn1).getClusterID();	int lv1 = FSImageTestUtil.getFSImage(nn1).getLayoutVersion();	LOG.info("nn1: lv=" + lv1 + ";cid=" + cid1 + ";bpid=" + bpid1 + ";uri=" + nn1.getNameNodeAddress());	DataNode dn = cluster.getDataNodes().get(0);	final Map<String, Object> volInfos = dn.data.getVolumeInfoMap();	Assert.assertTrue("No volumes in the fsdataset", volInfos.size() > 0);	int i = 0;	for (Map.Entry<String, Object> e : volInfos.entrySet()) {	
vol 

public void testClusterIdMismatch() throws Exception {	MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf) .nnTopology(MiniDFSNNTopology.simpleFederatedTopology(2)) .build();	try {	cluster.waitActive();	DataNode dn = cluster.getDataNodes().get(0);	List<BPOfferService> bposs = dn.getAllBpOs();	
dn bpos len should be 

public void testClusterIdMismatch() throws Exception {	MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf) .nnTopology(MiniDFSNNTopology.simpleFederatedTopology(2)) .build();	try {	cluster.waitActive();	DataNode dn = cluster.getDataNodes().get(0);	List<BPOfferService> bposs = dn.getAllBpOs();	Assert.assertEquals("should've registered with two namenodes", bposs.size(),2);	cluster.addNameNode(conf, 9938);	Thread.sleep(500);	bposs = dn.getAllBpOs();	
dn bpos len should be 

cluster.addNameNode(conf, 9938);	Thread.sleep(500);	bposs = dn.getAllBpOs();	Assert.assertEquals("should've registered with three namenodes", bposs.size(),3);	StartupOption.FORMAT.setClusterId("DifferentCID");	cluster.addNameNode(conf, 9948);	NameNode nn4 = cluster.getNameNode(3);	assertNotNull("cannot create nn4", nn4);	Thread.sleep(500);	bposs = dn.getAllBpOs();	
dn bpos len still should be 

========================= hadoop sample_7263 =========================

if (isEnabled()) {	try {	fsURI = new URI(fsName);	filesystem = FileSystem.get(fsURI, getConf());	} catch (URISyntaxException e) {	throw new IOException("Invalid URI " + fsName);	} catch (IllegalArgumentException e) {	throw new IOException("Unable to initialize filesystem " + fsName + ": " + e, e);	}	} else {	
skipping tests as fs name is not defined in 

========================= hadoop sample_3220 =========================

checkNotEmpty();	if (reuseLastProvider && lastProvider != null) {	return lastProvider.getCredentials();	}	AmazonClientException lastException = null;	for (AWSCredentialsProvider provider : providers) {	try {	AWSCredentials credentials = provider.getCredentials();	if ((credentials.getAWSAccessKeyId() != null && credentials.getAWSSecretKey() != null) || (credentials instanceof AnonymousAWSCredentials)) {	lastProvider = provider;	
using credentials from 

AmazonClientException lastException = null;	for (AWSCredentialsProvider provider : providers) {	try {	AWSCredentials credentials = provider.getCredentials();	if ((credentials.getAWSAccessKeyId() != null && credentials.getAWSSecretKey() != null) || (credentials instanceof AnonymousAWSCredentials)) {	lastProvider = provider;	return credentials;	}	} catch (AmazonClientException e) {	lastException = e;	
no credentials provided by 

========================= hadoop sample_6022 =========================

}	if (bzip2Loaded) {	bzip2LibraryName = Bzip2Factory.getLibraryName(conf);	}	}	if (Shell.WINDOWS) {	try {	winutilsPath = Shell.getWinUtilsFile().getCanonicalPath();	winutilsExists = true;	} catch (IOException e) {	
no winutils 

========================= hadoop sample_3670 =========================

public void testLocateDirectory() throws Throwable {	describe("verify that locating a directory is an error");	createFile(path("/test/filename"));	FileStatus status = fs.getFileStatus(path("/test"));	
filesystem is target is 

========================= hadoop sample_6188 =========================

continue;	}	long logUploadedTime = thisNodeFile.getModificationTime();	reader = new AggregatedLogFormat.LogReader( conf, thisNodeFile.getPath());	String owner = null;	Map<ApplicationAccessType, String> appAcls = null;	try {	owner = reader.getApplicationOwner();	appAcls = reader.getApplicationAcls();	} catch (IOException e) {	
error getting logs for 

Map<ApplicationAccessType, String> appAcls = null;	try {	owner = reader.getApplicationOwner();	appAcls = reader.getApplicationAcls();	} catch (IOException e) {	continue;	}	String remoteUser = request().getRemoteUser();	if (!checkAcls(conf, appId, owner, appAcls, remoteUser)) {	html.h1()._("User [" + remoteUser + "] is not authorized to view the logs for " + logEntity + " in log file [" + thisNodeFile.getPath().getName() + "]") ._();	
user is not authorized to view the logs for 

if (!checkAcls(conf, appId, owner, appAcls, remoteUser)) {	html.h1()._("User [" + remoteUser + "] is not authorized to view the logs for " + logEntity + " in log file [" + thisNodeFile.getPath().getName() + "]") ._();	continue;	}	AggregatedLogFormat.ContainerLogsReader logReader = reader .getContainerLogsReader(containerId);	if (logReader == null) {	continue;	}	foundLog = readContainerLogs(html, logReader, start, end, desiredLogType, logUploadedTime);	} catch (IOException ex) {	
error getting logs for 

}	if (!foundLog) {	if (desiredLogType.isEmpty()) {	html.h1("No logs available for container " + containerId.toString());	} else {	html.h1("Unable to locate '" + desiredLogType + "' log for container " + containerId.toString());	}	}	} catch (IOException e) {	html.h1()._("Error getting logs for " + logEntity)._();	
error getting logs for 

========================= hadoop sample_2348 =========================

public static void main(String[] args) {	boolean result = false;	try {	ApplicationMaster appMaster = new ApplicationMaster();	
initializing applicationmaster 

boolean result = false;	try {	ApplicationMaster appMaster = new ApplicationMaster();	boolean doRun = appMaster.init(args);	if (!doRun) {	System.exit(0);	}	appMaster.run();	result = appMaster.finish();	} catch (Throwable t) {	
error running applicationmaster 

if (!doRun) {	System.exit(0);	}	appMaster.run();	result = appMaster.finish();	} catch (Throwable t) {	LogManager.shutdown();	ExitUtil.terminate(1, t);	}	if (result) {	
application master completed successfully exiting 

}	appMaster.run();	result = appMaster.finish();	} catch (Throwable t) {	LogManager.shutdown();	ExitUtil.terminate(1, t);	}	if (result) {	System.exit(0);	} else {	
application master failed exiting 

private void dumpOutDebugInfo() {	
dump debug output 

for (Map.Entry<String, String> env : envs.entrySet()) {	LOG.info("System env: key=" + env.getKey() + ", val=" + env.getValue());	System.out.println("System env: key=" + env.getKey() + ", val=" + env.getValue());	}	BufferedReader buf = null;	try {	String lines = Shell.WINDOWS ? Shell.execCommand("cmd", "/c", "dir") : Shell.execCommand("ls", "-al");	buf = new BufferedReader(new StringReader(lines));	String line = "";	while ((line = buf.readLine()) != null) {	
system cwd content 

opts.addOption("help", false, "Print usage");	CommandLine cliParser = new GnuParser().parse(opts, args);	if (args.length == 0) {	printUsage(opts);	throw new IllegalArgumentException( "No args specified for application master to initialize");	}	if (fileExist(log4jPath)) {	try {	Log4jPropertyHelper.updateLog4jConfiguration(ApplicationMaster.class, log4jPath);	} catch (Exception e) {	
can not set up custom properties 

}	}	containerMaxRetries = Integer.parseInt( cliParser.getOptionValue("container_max_retries", "0"));	containrRetryInterval = Integer.parseInt(cliParser.getOptionValue( "container_retry_interval", "0"));	if (YarnConfiguration.timelineServiceEnabled(conf)) {	timelineServiceV2Enabled = ((int) YarnConfiguration.getTimelineServiceVersion(conf) == 2);	timelineServiceV1Enabled = !timelineServiceV2Enabled;	} else {	timelineClient = null;	timelineV2Client = null;	
timeline service is not enabled 

public void run() throws YarnException, IOException, InterruptedException {	
starting applicationmaster 

public void run() throws YarnException, IOException, InterruptedException {	Credentials credentials = UserGroupInformation.getCurrentUser().getCredentials();	DataOutputBuffer dob = new DataOutputBuffer();	credentials.writeTokenStorageToStream(dob);	Iterator<Token<?>> iter = credentials.getAllTokens().iterator();	
executing with tokens 

amRMClient.registerTimelineV2Client(timelineV2Client);	}	if (timelineServiceV2Enabled) {	publishApplicationAttemptEventOnTimelineServiceV2( DSEvent.DS_APP_ATTEMPT_START);	} else if (timelineServiceV1Enabled) {	publishApplicationAttemptEvent(timelineClient, appAttemptID.toString(), DSEvent.DS_APP_ATTEMPT_START, domainId, appSubmitterUgi);	}	appMasterHostname = NetUtils.getHostname();	RegisterApplicationMasterResponse response = amRMClient .registerApplicationMaster(appMasterHostname, appMasterRpcPort, appMasterTrackingUrl);	long maxMem = response.getMaximumResourceCapability().getMemorySize();	
max mem capability of resources in this cluster 

}	if (timelineServiceV2Enabled) {	publishApplicationAttemptEventOnTimelineServiceV2( DSEvent.DS_APP_ATTEMPT_START);	} else if (timelineServiceV1Enabled) {	publishApplicationAttemptEvent(timelineClient, appAttemptID.toString(), DSEvent.DS_APP_ATTEMPT_START, domainId, appSubmitterUgi);	}	appMasterHostname = NetUtils.getHostname();	RegisterApplicationMasterResponse response = amRMClient .registerApplicationMaster(appMasterHostname, appMasterRpcPort, appMasterTrackingUrl);	long maxMem = response.getMaximumResourceCapability().getMemorySize();	int maxVCores = response.getMaximumResourceCapability().getVirtualCores();	
max vcores capability of resources in this cluster 

int maxVCores = response.getMaximumResourceCapability().getVirtualCores();	if (containerMemory > maxMem) {	LOG.info("Container memory specified above max threshold of cluster." + " Using max value." + ", specified=" + containerMemory + ", max=" + maxMem);	containerMemory = maxMem;	}	if (containerVirtualCores > maxVCores) {	LOG.info("Container virtual cores specified above max threshold of cluster." + " Using max value." + ", specified=" + containerVirtualCores + ", max=" + maxVCores);	containerVirtualCores = maxVCores;	}	List<Container> previousAMRunningContainers = response.getContainersFromPreviousAttempts();	
received previous attempts running containers on am registration 

public Void run() throws Exception {	if (YarnConfiguration.timelineServiceEnabled(conf)) {	if (timelineServiceV2Enabled) {	timelineV2Client = TimelineV2Client.createTimelineClient( appAttemptID.getApplicationId());	timelineV2Client.init(conf);	timelineV2Client.start();	
timeline service client is enabled 

public Void run() throws Exception {	if (YarnConfiguration.timelineServiceEnabled(conf)) {	if (timelineServiceV2Enabled) {	timelineV2Client = TimelineV2Client.createTimelineClient( appAttemptID.getApplicationId());	timelineV2Client.init(conf);	timelineV2Client.start();	} else {	timelineClient = TimelineClient.createTimelineClient();	timelineClient.init(conf);	timelineClient.start();	
timeline service client is enabled 

timelineV2Client.init(conf);	timelineV2Client.start();	} else {	timelineClient = TimelineClient.createTimelineClient();	timelineClient.init(conf);	timelineClient.start();	}	} else {	timelineClient = null;	timelineV2Client = null;	
timeline service is not enabled 

}	if (timelineServiceV2Enabled) {	publishApplicationAttemptEventOnTimelineServiceV2( DSEvent.DS_APP_ATTEMPT_END);	} else if (timelineServiceV1Enabled) {	publishApplicationAttemptEvent(timelineClient, appAttemptID.toString(), DSEvent.DS_APP_ATTEMPT_END, domainId, appSubmitterUgi);	}	for (Thread launchThread : launchThreads) {	try {	launchThread.join(10000);	} catch (InterruptedException e) {	
exception thrown in thread join 

} else if (timelineServiceV1Enabled) {	publishApplicationAttemptEvent(timelineClient, appAttemptID.toString(), DSEvent.DS_APP_ATTEMPT_END, domainId, appSubmitterUgi);	}	for (Thread launchThread : launchThreads) {	try {	launchThread.join(10000);	} catch (InterruptedException e) {	e.printStackTrace();	}	}	
application completed stopping running containers 

publishApplicationAttemptEvent(timelineClient, appAttemptID.toString(), DSEvent.DS_APP_ATTEMPT_END, domainId, appSubmitterUgi);	}	for (Thread launchThread : launchThreads) {	try {	launchThread.join(10000);	} catch (InterruptedException e) {	e.printStackTrace();	}	}	nmClientAsync.stop();	
application completed signalling finish to rm 

appStatus = FinalApplicationStatus.SUCCEEDED;	} else {	appStatus = FinalApplicationStatus.FAILED;	appMessage = "Diagnostics." + ", total=" + numTotalContainers + ", completed=" + numCompletedContainers.get() + ", allocated=" + numAllocatedContainers.get() + ", failed=" + numFailedContainers.get();	LOG.info(appMessage);	success = false;	}	try {	amRMClient.unregisterApplicationMaster(appStatus, appMessage, null);	} catch (YarnException ex) {	
failed to unregister application 

} else {	appStatus = FinalApplicationStatus.FAILED;	appMessage = "Diagnostics." + ", total=" + numTotalContainers + ", completed=" + numCompletedContainers.get() + ", allocated=" + numAllocatedContainers.get() + ", failed=" + numFailedContainers.get();	LOG.info(appMessage);	success = false;	}	try {	amRMClient.unregisterApplicationMaster(appStatus, appMessage, null);	} catch (YarnException ex) {	} catch (IOException e) {	
failed to unregister application 

public void onContainersCompleted(List<ContainerStatus> completedContainers) {	LOG.info("Got response from RM for container ask, completedCnt=" + completedContainers.size());	for (ContainerStatus containerStatus : completedContainers) {	LOG.info(appAttemptID + " got container status for containerID=" + containerStatus.getContainerId() + ", state=" + containerStatus.getState() + ", exitStatus=" + containerStatus.getExitStatus() + ", diagnostics=" + containerStatus.getDiagnostics());	assert (containerStatus.getState() == ContainerState.COMPLETE);	if (!launchedContainers.contains(containerStatus.getContainerId())) {	
ignoring completed status of unknown container probably launched by previous attempt 

public void onError(Throwable e) {	
error in rmcallbackhandler 

public void onContainerStopped(ContainerId containerId) {	if (LOG.isDebugEnabled()) {	
succeeded to stop container 

public void onContainerStarted(ContainerId containerId, Map<String, ByteBuffer> allServiceResponse) {	if (LOG.isDebugEnabled()) {	
succeeded to start container 

public void onStartContainerError(ContainerId containerId, Throwable t) {	
failed to start container 

public void onGetContainerStatusError( ContainerId containerId, Throwable t) {	
failed to query the status of container 

public void onStopContainerError(ContainerId containerId, Throwable t) {	
failed to stop container 

if (!scriptPath.isEmpty()) {	Path renamedScriptPath = null;	if (Shell.WINDOWS) {	renamedScriptPath = new Path(scriptPath + ".bat");	} else {	renamedScriptPath = new Path(scriptPath + ".sh");	}	try {	renameScriptFile(renamedScriptPath);	} catch (Exception e) {	
not able to add suffix bat sh to the shell script filename 

private void renameScriptFile(final Path renamedScriptPath) throws IOException, InterruptedException {	appSubmitterUgi.doAs(new PrivilegedExceptionAction<Void>() {	public Void run() throws IOException {	FileSystem fs = renamedScriptPath.getFileSystem(conf);	fs.rename(new Path(scriptPath), renamedScriptPath);	return null;	}	});	
user added suffix sh bat to script file as 

private ContainerRequest setupContainerAskForRM() {	Priority pri = Priority.newInstance(requestPriority);	Resource capability = Resource.newInstance(containerMemory, containerVirtualCores);	ContainerRequest request = new ContainerRequest(capability, null, null, pri);	
requested container ask 

entity.addPrimaryFilter(APPID_TIMELINE_FILTER_NAME, container.getId() .getApplicationAttemptId().getApplicationId().toString());	TimelineEvent event = new TimelineEvent();	event.setTimestamp(System.currentTimeMillis());	event.setEventType(DSEvent.DS_CONTAINER_START.toString());	event.addEventInfo("Node", container.getNodeId().toString());	event.addEventInfo("Resources", container.getResource().toString());	entity.addEvent(event);	try {	processTimelineResponseErrors( putContainerEntity(timelineClient, container.getId().getApplicationAttemptId(), entity));	} catch (YarnException | IOException | ClientHandlerException e) {	
container start event could not be published for 

entity.setDomainId(domainId);	entity.addPrimaryFilter(USER_TIMELINE_FILTER_NAME, ugi.getShortUserName());	TimelineEvent event = new TimelineEvent();	event.setEventType(appEvent.toString());	event.setTimestamp(System.currentTimeMillis());	entity.addEvent(event);	try {	TimelinePutResponse response = timelineClient.putEntities(entity);	processTimelineResponseErrors(response);	} catch (YarnException | IOException | ClientHandlerException e) {	
app attempt start end event could not be published for 

private TimelinePutResponse processTimelineResponseErrors( TimelinePutResponse response) {	List<TimelinePutResponse.TimelinePutError> errors = response.getErrors();	if (errors.size() == 0) {	
timeline entities are successfully put 

private TimelinePutResponse processTimelineResponseErrors( TimelinePutResponse response) {	List<TimelinePutResponse.TimelinePutError> errors = response.getErrors();	if (errors.size() == 0) {	} else {	for (TimelinePutResponse.TimelinePutError error : errors) {	
error when publishing entity server side error code 

entity.addEvent(event);	entity.setIdPrefix(TimelineServiceHelper.invertLong(startTime));	try {	appSubmitterUgi.doAs(new PrivilegedExceptionAction<Object>() {	public TimelinePutResponse run() throws Exception {	timelineV2Client.putEntities(entity);	return null;	}	});	} catch (Exception e) {	
container start event could not be published for 

entity.addEvent(event);	entity.setIdPrefix(TimelineServiceHelper.invertLong(containerStartTime));	try {	appSubmitterUgi.doAs(new PrivilegedExceptionAction<Object>() {	public TimelinePutResponse run() throws Exception {	timelineV2Client.putEntities(entity);	return null;	}	});	} catch (Exception e) {	
container end event could not be published for 

entity.addEvent(event);	entity.setIdPrefix( TimelineServiceHelper.invertLong(appAttemptID.getAttemptId()));	try {	appSubmitterUgi.doAs(new PrivilegedExceptionAction<Object>() {	public TimelinePutResponse run() throws Exception {	timelineV2Client.putEntitiesAsync(entity);	return null;	}	});	} catch (Exception e) {	
app attempt start end event could not be published for 

========================= hadoop sample_9 =========================

protected void sleepFor(int ms) {	
would have slept for ms 

========================= hadoop sample_3109 =========================

} catch (InvalidStateTransitionException e) {	return false;	}	Map<NodeId, RMContainer> reservedContainers = this.reservedContainers.get(schedulerKey);	if (reservedContainers == null) {	reservedContainers = new HashMap<NodeId, RMContainer>();	this.reservedContainers.put(schedulerKey, reservedContainers);	}	reservedContainers.put(node.getNodeID(), rmContainer);	if (LOG.isDebugEnabled()) {	
application attempt reserved container on node this attempt currently has reserved containers at priority currentreservation 

if (updateType != null) {	container.setVersion(container.getVersion() + 1);	}	if (isWaitingForAMContainer()) {	containerType = ContainerType.APPLICATION_MASTER;	}	try {	container.setContainerToken(rmContext.getContainerTokenSecretManager() .createContainerToken(container.getId(), container.getVersion(), container.getNodeId(), getUser(), container.getResource(), container.getPriority(), rmContainer.getCreationTime(), this.logAggregationContext, rmContainer.getNodeLabelExpression(), containerType, container.getExecutionType()));	updateNMToken(container);	} catch (IllegalArgumentException e) {	
error trying to assign container token and nm token to an updated container 

public void recoverContainer(SchedulerNode node, RMContainer rmContainer) {	try {	writeLock.lock();	appSchedulingInfo.recoverContainer(rmContainer, node.getPartition());	if (rmContainer.getState().equals(RMContainerState.COMPLETED)) {	return;	}	
schedulerattempt is recovering container 

========================= hadoop sample_950 =========================

public void init(Clock clock, ResourceScheduler sched, Collection<Plan> plans) {	super.init(clock, sched, plans);	
initializing plan follower policy 

protected Queue getPlanQueue(String planQueueName) {	CSQueue queue = cs.getQueue(planQueueName);	if (!(queue instanceof PlanQueue)) {	
the plan is not an planqueue 

protected void addReservationQueue( String planQueueName, Queue queue, String currResId) {	PlanQueue planQueue = (PlanQueue)queue;	try {	ReservationQueue resQueue = new ReservationQueue(cs, currResId, planQueue);	cs.addQueue(resQueue);	} catch (SchedulerDynamicEditException e) {	
exception while trying to activate reservation for plan 

protected void addReservationQueue( String planQueueName, Queue queue, String currResId) {	PlanQueue planQueue = (PlanQueue)queue;	try {	ReservationQueue resQueue = new ReservationQueue(cs, currResId, planQueue);	cs.addQueue(resQueue);	} catch (SchedulerDynamicEditException e) {	} catch (IOException e) {	
exception while trying to activate reservation for plan 

protected void createDefaultReservationQueue( String planQueueName, Queue queue, String defReservationId) {	PlanQueue planQueue = (PlanQueue)queue;	if (cs.getQueue(defReservationId) == null) {	try {	ReservationQueue defQueue = new ReservationQueue(cs, defReservationId, planQueue);	cs.addQueue(defQueue);	} catch (SchedulerDynamicEditException e) {	
exception while trying to create default reservation queue for plan 

protected void createDefaultReservationQueue( String planQueueName, Queue queue, String defReservationId) {	PlanQueue planQueue = (PlanQueue)queue;	if (cs.getQueue(defReservationId) == null) {	try {	ReservationQueue defQueue = new ReservationQueue(cs, defReservationId, planQueue);	cs.addQueue(defQueue);	} catch (SchedulerDynamicEditException e) {	} catch (IOException e) {	
exception while trying to create default reservation queue for plan 

========================= hadoop sample_1071 =========================

private void initNewDirs() {	if (newDirs == null) {	return;	}	for (StorageDirectory sd : newDirs) {	try {	storage.writeProperties(sd);	
wrote version in the new storage 

}	editStreams = editLog.selectInputStreams( imageFiles.get(0).getCheckpointTxId() + 1, toAtLeastTxId, recovery, false);	} else {	editStreams = FSImagePreTransactionalStorageInspector .getEditLogStreams(storage);	}	int maxOpSize = conf.getInt(DFSConfigKeys.DFS_NAMENODE_MAX_OP_SIZE_KEY, DFSConfigKeys.DFS_NAMENODE_MAX_OP_SIZE_DEFAULT);	for (EditLogInputStream elis : editStreams) {	elis.setMaxOpSize(maxOpSize);	}	for (EditLogInputStream l : editStreams) {	
planning to load edit log stream 

} else {	editStreams = FSImagePreTransactionalStorageInspector .getEditLogStreams(storage);	}	int maxOpSize = conf.getInt(DFSConfigKeys.DFS_NAMENODE_MAX_OP_SIZE_KEY, DFSConfigKeys.DFS_NAMENODE_MAX_OP_SIZE_DEFAULT);	for (EditLogInputStream elis : editStreams) {	elis.setMaxOpSize(maxOpSize);	}	for (EditLogInputStream l : editStreams) {	}	if (!editStreams.iterator().hasNext()) {	
no edit log streams selected 

}	FSImageFile imageFile = null;	for (int i = 0; i < imageFiles.size(); i++) {	try {	imageFile = imageFiles.get(i);	loadFSImageFile(target, recovery, imageFile, startOpt);	break;	} catch (IllegalReservedPathException ie) {	throw new IOException("Failed to load image from " + imageFile, ie);	} catch (Exception e) {	
failed to load image from 

private void loadFSImage(File curFile, MD5Hash expectedMd5, FSNamesystem target, MetaRecoveryContext recovery, boolean requireSameLayoutVersion) throws IOException {	target.setBlockPoolId(this.getBlockPoolID());	FSImageFormat.LoaderDelegator loader = FSImageFormat.newLoader(conf, target);	loader.load(curFile, requireSameLayoutVersion);	MD5Hash readImageMd5 = loader.getLoadedImageMd5();	if (expectedMd5 != null && !expectedMd5.equals(readImageMd5)) {	throw new IOException("Image file " + curFile + " is corrupt with MD5 checksum of " + readImageMd5 + " but expecting " + expectedMd5);	}	long txId = loader.getLoadedImageTxId();	
loaded image for txid from 

public void run() {	try {	saveFSImage(context, sd, nnf);	} catch (SaveNamespaceCancelledException snce) {	
cancelled image saving for 

public void run() {	try {	saveFSImage(context, sd, nnf);	} catch (SaveNamespaceCancelledException snce) {	} catch (Throwable t) {	
unable to save image for 

private void waitForThreads(List<Thread> threads) {	for (Thread thread : threads) {	while (thread.isAlive()) {	try {	thread.join();	} catch (InterruptedException iex) {	
caught interrupted exception while waiting for thread to finish retrying join 

public synchronized void saveNamespace(FSNamesystem source, NameNodeFile nnf, Canceler canceler) throws IOException {	assert editLog != null : "editLog must be initialized";	
save namespace 

private void renameCheckpoint(long txid, NameNodeFile fromNnf, NameNodeFile toNnf, boolean renameMD5) throws IOException {	ArrayList<StorageDirectory> al = null;	for (StorageDirectory sd : storage.dirIterable(NameNodeDirType.IMAGE)) {	try {	renameImageFileInDir(sd, fromNnf, toNnf, txid, renameMD5);	} catch (IOException ioe) {	
unable to rename checkpoint in 

private void deleteCancelledCheckpoint(long txid) throws IOException {	ArrayList<StorageDirectory> al = Lists.newArrayList();	for (StorageDirectory sd : storage.dirIterable(NameNodeDirType.IMAGE)) {	File ckpt = NNStorage.getStorageFile(sd, NameNodeFile.IMAGE_NEW, txid);	if (ckpt.exists() && !ckpt.delete()) {	
unable to delete cancelled checkpoint in 

private void renameImageFileInDir(StorageDirectory sd, NameNodeFile fromNnf, NameNodeFile toNnf, long txid, boolean renameMD5) throws IOException {	final File fromFile = NNStorage.getStorageFile(sd, fromNnf, txid);	final File toFile = NNStorage.getStorageFile(sd, toNnf, txid);	if(LOG.isDebugEnabled()) {	
renaming to 

========================= hadoop sample_8047 =========================

final AtomicReference<Throwable> childError = new AtomicReference<>();	for (int i = 0; i < numThreads; i++) {	threadPool.submit(new Runnable() {	public void run() {	allExecutorThreadsReady.countDown();	try {	startBlocker.await();	final FileSystem fs = cluster.getFileSystem();	fs.mkdirs(new Path("/testStatisticsParallelChild"));	} catch (Throwable t) {	
child failed when calling mkdir 

allDone.await();	assertNull("Child failed with exception " + childError.get(), childError.get());	checkStatistics(fs, 0, numThreads, 0);	checkOpStatistics(OpType.MKDIRS, numThreads + oldMkdirOpCount);	for (Iterator<LongStatistic> opCountIter = FileSystem.getGlobalStorageStatistics() .get(DFSOpsCountStatistics.NAME).getLongStatistics();	opCountIter.hasNext();) {	final LongStatistic opCount = opCountIter.next();	if (OpType.MKDIRS.getSymbol().equals(opCount.getName())) {	assertEquals("Unexpected op count from iterator!", numThreads + oldMkdirOpCount, opCount.getValue());	}	
t 

{	final FileChecksum zeroChecksum = hdfs.getFileChecksum(zeroByteFile);	final String magicValue = "MD5-of-0MD5-of-0CRC32:70bc8f4b72a86921468bf8e8441dce51";	assertEquals(magicValue, zeroChecksum.toString());	final FileChecksum checksumWith0 = hdfs.getFileChecksum(bar, 0);	assertEquals(zeroChecksum, checksumWith0);	try {	hdfs.getFileChecksum(new Path(dir, "none-existent"), 8);	fail();	} catch (Exception ioe) {	
good getting an exception 

assertEquals(zeroChecksum, checksumWith0);	try {	hdfs.getFileChecksum(new Path(dir, "none-existent"), 8);	fail();	} catch (Exception ioe) {	}	try {	final FileChecksum noneExistentChecksumWith0 = hdfs.getFileChecksum(new Path(dir, "none-existent"), 0);	fail();	} catch (Exception ioe) {	
good getting an exception 

assertEquals(webhdfsfoocs, barcs);	assertEquals(webhdfs_qfoocs.hashCode(), barhashcode);	assertEquals(webhdfs_qfoocs, barcs);	}	hdfs.setPermission(dir, new FsPermission((short)0));	{	try {	hftp.getFileChecksum(qualified);	fail();	} catch(IOException ioe) {	
good getting an exception 

hftp.getFileChecksum(qualified);	fail();	} catch(IOException ioe) {	}	}	{	try {	webhdfs.getFileChecksum(webhdfsqualified);	fail();	} catch(IOException ioe) {	
good getting an exception 

public void testDFSDataOutputStreamBuilderForCreation() throws Exception {	Configuration conf = getTestConfiguration();	String testFile = "/testDFSDataOutputStreamBuilder";	Path testFilePath = new Path(testFile);	try (MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf) .numDataNodes(1).build()) {	DistributedFileSystem fs = cluster.getFileSystem();	HdfsDataOutputStreamBuilder builder = fs.createFile(testFilePath) .blockSize(4096).replication((short)1);	assertFalse(fs.exists(testFilePath));	try (FSDataOutputStream out = fs.createFile(testFilePath).build()) {	
test create an empty file 

========================= hadoop sample_7648 =========================

if (args[1] == null) {	throw new ServiceException("null param while calling Method: [" + method.getName() + "]");	}	Tracer tracer = Tracer.curThreadTracer();	TraceScope traceScope = null;	if (tracer != null) {	traceScope = tracer.newScope(RpcClientUtil.methodToTraceString(method));	}	RequestHeaderProto rpcRequestHeader = constructRpcRequestHeader(method);	if (LOG.isTraceEnabled()) {	
call 

}	RequestHeaderProto rpcRequestHeader = constructRpcRequestHeader(method);	if (LOG.isTraceEnabled()) {	}	final Message theRequest = (Message) args[1];	final RpcWritable.Buffer val;	try {	val = (RpcWritable.Buffer) client.call(RPC.RpcKind.RPC_PROTOCOL_BUFFER, new RpcProtobufRequest(rpcRequestHeader, theRequest), remoteId, fallbackToSimpleAuth);	} catch (Throwable e) {	if (LOG.isTraceEnabled()) {	
exception 

}	if (traceScope != null) {	traceScope.addTimelineAnnotation("Call got exception: " + e.toString());	}	throw new ServiceException(e);	} finally {	if (traceScope != null) traceScope.close();	}	if (LOG.isDebugEnabled()) {	long callTime = Time.now() - startTime;	
call took ms 

Message prototype = null;	try {	prototype = getReturnProtoType(method);	} catch (Exception e) {	throw new ServiceException(e);	}	Message returnMessage;	try {	returnMessage = buf.getValue(prototype.getDefaultInstanceForType());	if (LOG.isTraceEnabled()) {	
response 

========================= hadoop sample_4064 =========================

public void inspectDirectory(StorageDirectory sd) throws IOException {	if (!sd.getVersionFile().exists()) {	
no version file in 

public void inspectDirectory(StorageDirectory sd) throws IOException {	if (!sd.getVersionFile().exists()) {	needToSave |= true;	return;	}	try {	maxSeenTxId = Math.max(maxSeenTxId, NNStorage.readTransactionIdFile(sd));	} catch (IOException ioe) {	
unable to determine the max transaction id seen by 

try {	maxSeenTxId = Math.max(maxSeenTxId, NNStorage.readTransactionIdFile(sd));	} catch (IOException ioe) {	return;	}	File currentDir = sd.getCurrentDir();	File filesInStorage[];	try {	filesInStorage = FileUtil.listFiles(currentDir);	} catch (IOException ioe) {	
unable to inspect storage directory 

return;	}	File currentDir = sd.getCurrentDir();	File filesInStorage[];	try {	filesInStorage = FileUtil.listFiles(currentDir);	} catch (IOException ioe) {	return;	}	for (File f : filesInStorage) {	
checking file 

}	for (File f : filesInStorage) {	String name = f.getName();	Matcher imageMatch = this.matchPattern(name);	if (imageMatch != null) {	if (sd.getStorageDirType().isOfType(NameNodeDirType.IMAGE)) {	try {	long txid = Long.parseLong(imageMatch.group(1));	foundImages.add(new FSImageFile(sd, f, txid));	} catch (NumberFormatException nfe) {	
image file has improperly formatted transaction id 

String name = f.getName();	Matcher imageMatch = this.matchPattern(name);	if (imageMatch != null) {	if (sd.getStorageDirType().isOfType(NameNodeDirType.IMAGE)) {	try {	long txid = Long.parseLong(imageMatch.group(1));	foundImages.add(new FSImageFile(sd, f, txid));	} catch (NumberFormatException nfe) {	}	} else {	
found image file at but storage directory is not configured to contain images 

========================= hadoop sample_8102 =========================

private int skipUtfByteOrderMark() throws IOException {	int newMaxLineLength = (int) Math.min(3L + (long) maxLineLength, Integer.MAX_VALUE);	int newSize = in.readLine(value, newMaxLineLength, maxBytesToConsume(pos));	pos += newSize;	int textLength = value.getLength();	byte[] textBytes = value.getBytes();	if ((textLength >= 3) && (textBytes[0] == (byte)0xEF) && (textBytes[1] == (byte)0xBB) && (textBytes[2] == (byte)0xBF)) {	
found utf bom and skipped it 

while (getFilePosition() <= end || in.needAdditionalRecordAfterSplit()) {	if (pos == 0) {	newSize = skipUtfByteOrderMark();	} else {	newSize = in.readLine(value, maxLineLength, maxBytesToConsume(pos));	pos += newSize;	}	if ((newSize == 0) || (newSize < maxLineLength)) {	break;	}	
skipped line of size at pos 

========================= hadoop sample_4988 =========================

private static <T extends IOException> T wrapWithMessage( T exception, String msg) {	Class<? extends Throwable> clazz = exception.getClass();	try {	Constructor<? extends Throwable> ctor = clazz.getConstructor(String.class);	Throwable t = ctor.newInstance(msg);	return (T) (t.initCause(exception));	} catch (Throwable e) {	
unable to wrap exception of type it has no string constructor 

========================= hadoop sample_6235 =========================

private void updateLastPromisedEpoch(long newEpoch) throws IOException {	
updating lastpromisedepoch from to for client 

} else {	if (segInfo != null) {	builder.setSegmentState(segInfo);	}	}	builder.setLastWriterEpoch(lastWriterEpoch.get());	if (committedTxnId.get() != HdfsServerConstants.INVALID_TXID) {	builder.setLastCommittedTxId(committedTxnId.get());	}	PrepareRecoveryResponseProto resp = builder.build();	
prepared recovery for segment 

Preconditions.checkArgument(segment.getEndTxId() > 0 && segment.getEndTxId() >= segmentTxId, "bad recovery state for segment %s: %s", segmentTxId, TextFormat.shortDebugString(segment));	PersistedRecoveryPaxosData oldData = getPersistedPaxosData(segmentTxId);	PersistedRecoveryPaxosData newData = PersistedRecoveryPaxosData.newBuilder() .setAcceptedInEpoch(reqInfo.getEpoch()) .setSegmentState(segment) .build();	if (oldData != null) {	alwaysAssert(oldData.getAcceptedInEpoch() <= reqInfo.getEpoch(), "Bad paxos transition, out-of-order epochs.\nOld: %s\nNew: %s\n", oldData, newData);	}	File syncedFile = null;	SegmentStateProto currentSegment = getSegmentInfo(segmentTxId);	if (currentSegment == null || currentSegment.getEndTxId() != segment.getEndTxId()) {	if (currentSegment == null) {	
synchronizing log no current segment in place 

PersistedRecoveryPaxosData newData = PersistedRecoveryPaxosData.newBuilder() .setAcceptedInEpoch(reqInfo.getEpoch()) .setSegmentState(segment) .build();	if (oldData != null) {	alwaysAssert(oldData.getAcceptedInEpoch() <= reqInfo.getEpoch(), "Bad paxos transition, out-of-order epochs.\nOld: %s\nNew: %s\n", oldData, newData);	}	File syncedFile = null;	SegmentStateProto currentSegment = getSegmentInfo(segmentTxId);	if (currentSegment == null || currentSegment.getEndTxId() != segment.getEndTxId()) {	if (currentSegment == null) {	updateHighestWrittenTxId(Math.max(segment.getEndTxId(), highestWrittenTxId));	} else {	
synchronizing log old segment is not the right length 

if (txnRange(currentSegment).containsLong(committedTxnId.get()) && !txnRange(segment).containsLong(committedTxnId.get())) {	throw new AssertionError( "Cannot replace segment " + TextFormat.shortDebugString(currentSegment) + " with new segment " + TextFormat.shortDebugString(segment) + ": would discard already-committed txn " + committedTxnId.get());	}	alwaysAssert(currentSegment.getIsInProgress(), "Should never be asked to synchronize a different log on top of an " + "already-finalized segment");	if (txnRange(currentSegment).containsLong(highestWrittenTxId)) {	updateHighestWrittenTxId(segment.getEndTxId());	}	}	syncedFile = syncLog(reqInfo, segment, fromUrl);	} else {	
skipping download of log already have up to date logs 

}	syncedFile = syncLog(reqInfo, segment, fromUrl);	} else {	}	JournalFaultInjector.get().beforePersistPaxosData();	persistPaxosData(segmentTxId, newData);	JournalFaultInjector.get().afterPersistPaxosData();	if (syncedFile != null) {	FileUtil.replaceFile(syncedFile, storage.getInProgressEditLog(segmentTxId));	}	
accepted recovery for segment 

UserGroupInformation.getCurrentUser().checkTGTAndReloginFromKeytab();	}	boolean success = false;	try {	TransferFsImage.doGetUrl(url, localPaths, storage, true);	assert tmpFile.exists();	success = true;	} finally {	if (!success) {	if (!tmpFile.delete()) {	
failed to delete temporary file 

private void completeHalfDoneAcceptRecovery( PersistedRecoveryPaxosData paxosData) throws IOException {	if (paxosData == null) {	return;	}	long segmentId = paxosData.getSegmentState().getStartTxId();	long epoch = paxosData.getAcceptedInEpoch();	File tmp = storage.getSyncLogTemporaryFile(segmentId, epoch);	if (tmp.exists()) {	File dst = storage.getInProgressEditLog(segmentId);	
rolling forward previously half completed synchronization 

========================= hadoop sample_8381 =========================

private long resultSetColToLong(ResultSet rs, int colNum, int sqlDataType) throws SQLException {	try {	switch (sqlDataType) {	case Types.DATE: return rs.getDate(colNum).getTime();	case Types.TIME: return rs.getTime(colNum).getTime();	case Types.TIMESTAMP: return rs.getTimestamp(colNum).getTime();	default: throw new SQLException("Not a date-type field");	}	} catch (NullPointerException npe) {	
encountered a null date in the split column splits may be poorly balanced 

========================= hadoop sample_5049 =========================

public void service(HttpServletRequest req, HttpServletResponse res) throws ServletException, IOException {	res.setCharacterEncoding("UTF-8");	String uri = HtmlQuoting.quoteHtmlChars(req.getRequestURI());	if (uri == null) {	uri = "/";	}	if (devMode && uri.equals("/__stop")) {	res.setStatus(res.SC_NO_CONTENT);	
dev mode restart requested 

doGet(req, res);	return;	}	String pathInfo = req.getPathInfo();	if (pathInfo == null) {	pathInfo = "/";	}	try {	pathInfo = new URI(pathInfo).getPath();	}  catch (URISyntaxException ex) {	
failed to decode path 

try {	dest.action.invoke(controller, (Object[]) null);	if (!rc.rendered) {	if (dest.defaultViewClass != null) {	render(dest.defaultViewClass);	} else if (rc.status == 200) {	throw new IllegalStateException("No view rendered for 200");	}	}	} catch (Exception e) {	
error handling uri 

public static void removeCookie(HttpServletResponse res, String name, String path) {	
removing cookie on 

private void prepareToExit() {	checkState(devMode, "only in dev mode");	new Timer("webapp exit", true).schedule(new TimerTask() {	
webappp exiting 

========================= hadoop sample_2276 =========================

DFSInputStream c_in = dfs.open(dirString + "c");	FSDataOutputStream d_out = createFsOut(dfs, dirString + "d");	doThrow(new RemoteException(InvalidToken.class.getName(), "Your token is worthless")).when(spyNN).renewLease(anyString());	LeaseRenewer originalRenewer = dfs.getLeaseRenewer();	dfs.lastLeaseRenewal = Time.monotonicNow() - HdfsConstants.LEASE_SOFTLIMIT_PERIOD - 1000;	try {	dfs.renewLease();	} catch (IOException e) {}	try {	d_out.write(buf, 0, 1024);	
write worked beyond the soft limit as expected 

} catch (IOException e) {	Assert.fail("Write failed.");	}	dfs.lastLeaseRenewal = Time.monotonicNow() - HdfsConstants.LEASE_HARDLIMIT_PERIOD - 1000;	dfs.renewLease();	try {	d_out.write(buf, 0, 1024);	d_out.close();	Assert.fail("Write did not fail even after the fatal lease renewal failure");	} catch (IOException e) {	
write failed as expected 

Thread.sleep(1000);	Assert.assertTrue(originalRenewer.isEmpty());	doNothing().when(spyNN).renewLease(anyString());	try {	int num = c_in.read(buf, 0, 1);	if (num != 1) {	Assert.fail("Failed to read 1 byte");	}	c_in.close();	} catch (IOException e) {	
read failed with 

}	c_in.close();	} catch (IOException e) {	Assert.fail("Read after lease renewal failure failed");	}	try {	c_out = createFsOut(dfs, dirString + "c");	c_out.write(buf, 0, 1024);	c_out.close();	} catch (IOException e) {	
write failed with 

try {	Path p = new Path("/test-file");	Path d = new Path("/test-d");	Path d2 = new Path("/test-d-other");	FileSystem fs = cluster.getFileSystem();	FSDataOutputStream out = fs.create(p);	out.writeBytes("something");	Assert.assertTrue(hasLease(cluster, p));	Assert.assertEquals(1, leaseCount(cluster));	DistributedFileSystem fs2 = (DistributedFileSystem) FileSystem.newInstance(fs.getUri(), fs.getConf());	
dms rename file into dir 

Assert.assertEquals(1, leaseCount(cluster));	DistributedFileSystem fs2 = (DistributedFileSystem) FileSystem.newInstance(fs.getUri(), fs.getConf());	Path pRenamed = new Path(d, p.getName());	fs2.mkdirs(d);	fs2.rename(p, pRenamed);	Assert.assertFalse(p+" exists", fs2.exists(p));	Assert.assertTrue(pRenamed+" not found", fs2.exists(pRenamed));	Assert.assertFalse("has lease for "+p, hasLease(cluster, p));	Assert.assertTrue("no lease for "+pRenamed, hasLease(cluster, pRenamed));	Assert.assertEquals(1, leaseCount(cluster));	
dms rename parent dir 

Assert.assertTrue("no lease for "+pRenamed, hasLease(cluster, pRenamed));	Assert.assertEquals(1, leaseCount(cluster));	Path pRenamedAgain = new Path(d2, pRenamed.getName());	fs2.rename(d, d2);	Assert.assertFalse(d+" exists", fs2.exists(d));	Assert.assertFalse("has lease for "+pRenamed, hasLease(cluster, pRenamed));	Assert.assertTrue(d2+" not found", fs2.exists(d2));	Assert.assertTrue(pRenamedAgain+" not found", fs2.exists(pRenamedAgain));	Assert.assertTrue("no lease for "+pRenamedAgain, hasLease(cluster, pRenamedAgain));	Assert.assertEquals(1, leaseCount(cluster));	
dms rename parent again 

========================= hadoop sample_7131 =========================

if (!disallowFallbackToRandomSecretProvider && "file".equals(name) && config.getProperty(SIGNATURE_SECRET_FILE) == null) {	name = "random";	}	SignerSecretProvider provider;	if ("file".equals(name)) {	provider = new FileSignerSecretProvider();	try {	provider.init(config, ctx, validity);	} catch (Exception e) {	if (!disallowFallbackToRandomSecretProvider) {	
unable to initialize filesignersecretprovider falling back to use random secrets 

AuthenticationException authenticationEx = null;	HttpServletRequest httpRequest = (HttpServletRequest) request;	HttpServletResponse httpResponse = (HttpServletResponse) response;	boolean isHttps = "https".equals(httpRequest.getScheme());	try {	boolean newToken = false;	AuthenticationToken token;	try {	token = getToken(httpRequest);	if (LOG.isDebugEnabled()) {	
got token from httprequest 

boolean isHttps = "https".equals(httpRequest.getScheme());	try {	boolean newToken = false;	AuthenticationToken token;	try {	token = getToken(httpRequest);	if (LOG.isDebugEnabled()) {	}	}	catch (AuthenticationException ex) {	
authenticationtoken ignored 

if (LOG.isDebugEnabled()) {	}	}	catch (AuthenticationException ex) {	authenticationEx = ex;	token = null;	}	if (authHandler.managementOperation(token, httpRequest, httpResponse)) {	if (token == null) {	if (LOG.isDebugEnabled()) {	
request triggering authentication handler 

}	if (token.getExpires() != 0) {	token.setExpires(System.currentTimeMillis() + getValidity() * 1000);	}	}	newToken = true;	}	if (token != null) {	unauthorizedResponse = false;	if (LOG.isDebugEnabled()) {	
request user authenticated 

newToken = true;	}	if (newToken && !token.isExpired() && token != AuthenticationToken.ANONYMOUS) {	String signedToken = signer.sign(token.toString());	createAuthCookie(httpResponse, signedToken, getCookieDomain(), getCookiePath(), token.getExpires(), isCookiePersistent(), isHttps);	}	doFilter(filterChain, httpRequest, httpResponse);	}	} else {	if (LOG.isDebugEnabled()) {	
managementoperation returned false for request token 

}	} else {	if (LOG.isDebugEnabled()) {	}	unauthorizedResponse = false;	}	} catch (AuthenticationException ex) {	errCode = HttpServletResponse.SC_FORBIDDEN;	authenticationEx = ex;	if (LOG.isDebugEnabled()) {	
authentication exception 

} else {	if (LOG.isDebugEnabled()) {	}	unauthorizedResponse = false;	}	} catch (AuthenticationException ex) {	errCode = HttpServletResponse.SC_FORBIDDEN;	authenticationEx = ex;	if (LOG.isDebugEnabled()) {	} else {	
authentication exception 

========================= hadoop sample_2763 =========================

for (int i = 0; !done; i++) {	outputStream = writerStorageAccount.getStore().storefile( key, new PermissionStatus("", "", FsPermission.getDefault()), key);	Arrays.fill(dataBlockWrite, (byte) (i % 256));	for (int j = 0; j < NUMBER_OF_BLOCKS; j++) {	outputStream.write(dataBlockWrite);	}	outputStream.flush();	outputStream.close();	}	} catch (AzureException e) {	
datablockwriter thread encountered a storage exception 

outputStream = writerStorageAccount.getStore().storefile( key, new PermissionStatus("", "", FsPermission.getDefault()), key);	Arrays.fill(dataBlockWrite, (byte) (i % 256));	for (int j = 0; j < NUMBER_OF_BLOCKS; j++) {	outputStream.write(dataBlockWrite);	}	outputStream.flush();	outputStream.close();	}	} catch (AzureException e) {	} catch (IOException e) {	
datablockwriter thread encountered an i o exception 

========================= hadoop sample_6370 =========================

protected MRClientProtocol instantiateHistoryProxy(final Configuration conf, final InetSocketAddress hsAddress) throws IOException {	if (LOG.isDebugEnabled()) {	
connecting to mrhistoryserver at 

========================= hadoop sample_4463 =========================

public Writable call(Server server, String protocol, Writable rpcRequest, long receiveTime) throws Exception ;	}	static final Logger LOG = LoggerFactory.getLogger(RPC.class);	static Class<?>[] getSuperInterfaces(Class<?>[] childInterfaces) {	List<Class<?>> allInterfaces = new ArrayList<Class<?>>();	for (Class<?> childInterface : childInterfaces) {	if (VersionedProtocol.class.isAssignableFrom(childInterface)) {	allInterfaces.add(childInterface);	allInterfaces.addAll( Arrays.asList( getSuperInterfaces(childInterface.getInterfaces())));	} else {	
interface ignored because it does not extend versionedprotocol 

public static <T> ProtocolProxy<T> waitForProtocolProxy(Class<T> protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout) throws IOException {	long startTime = Time.now();	IOException ioe;	while (true) {	try {	return getProtocolProxy(protocol, clientVersion, addr, UserGroupInformation.getCurrentUser(), conf, NetUtils .getDefaultSocketFactory(conf), rpcTimeout, connectionRetryPolicy);	} catch(ConnectException se) {	
server at not available yet zzzzz 

public static <T> ProtocolProxy<T> waitForProtocolProxy(Class<T> protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout) throws IOException {	long startTime = Time.now();	IOException ioe;	while (true) {	try {	return getProtocolProxy(protocol, clientVersion, addr, UserGroupInformation.getCurrentUser(), conf, NetUtils .getDefaultSocketFactory(conf), rpcTimeout, connectionRetryPolicy);	} catch(ConnectException se) {	ioe = se;	} catch(SocketTimeoutException te) {	
problem connecting to server 

long startTime = Time.now();	IOException ioe;	while (true) {	try {	return getProtocolProxy(protocol, clientVersion, addr, UserGroupInformation.getCurrentUser(), conf, NetUtils .getDefaultSocketFactory(conf), rpcTimeout, connectionRetryPolicy);	} catch(ConnectException se) {	ioe = se;	} catch(SocketTimeoutException te) {	ioe = te;	} catch(NoRouteToHostException nrthe) {	
no route to host for server 

((Closeable) proxy).close();	return;	} else {	InvocationHandler handler = Proxy.getInvocationHandler(proxy);	if (handler instanceof Closeable) {	((Closeable) handler).close();	return;	}	}	} catch (IOException e) {	
closing proxy or invocation handler caused exception 

========================= hadoop sample_4033 =========================

public NMTokenSecretManagerInRM(Configuration conf) {	this.conf = conf;	timer = new Timer();	rollingInterval = this.conf.getLong( YarnConfiguration.RM_NMTOKEN_MASTER_KEY_ROLLING_INTERVAL_SECS, YarnConfiguration.DEFAULT_RM_NMTOKEN_MASTER_KEY_ROLLING_INTERVAL_SECS) this.activationDelay = (long) (conf.getLong(YarnConfiguration.RM_NM_EXPIRY_INTERVAL_MS, YarnConfiguration.DEFAULT_RM_NM_EXPIRY_INTERVAL_MS) * 1.5);	
nmtokenkeyrollinginterval ms and nmtokenkeyactivationdelay ms 

public void rollMasterKey() {	super.writeLock.lock();	try {	
rolling master key for nm tokens 

public void rollMasterKey() {	super.writeLock.lock();	try {	if (this.currentMasterKey == null) {	this.currentMasterKey = createNewMasterKey();	} else {	this.nextMasterKey = createNewMasterKey();	
going to activate master key with key id in ms 

public void activateNextMasterKey() {	super.writeLock.lock();	try {	
activating next master key with id 

public void clearNodeSetForAttempt(ApplicationAttemptId attemptId) {	super.writeLock.lock();	try {	HashSet<NodeId> nodeSet = this.appAttemptToNodeKeyMap.get(attemptId);	if (nodeSet != null) {	
clear node set for 

public NMToken createAndGetNMToken(String applicationSubmitter, ApplicationAttemptId appAttemptId, Container container) {	try {	this.writeLock.lock();	HashSet<NodeId> nodeSet = this.appAttemptToNodeKeyMap.get(appAttemptId);	NMToken nmToken = null;	if (nodeSet != null) {	if (!nodeSet.contains(container.getNodeId())) {	
sending nmtoken for nodeid for container 

========================= hadoop sample_706 =========================

public static void setup() throws IOException {	try {	dfsCluster = new MiniDFSCluster.Builder(conf).numDataNodes(2) .format(true).racks(null).build();	remoteFs = dfsCluster.getFileSystem();	} catch (IOException io) {	throw new RuntimeException("problem starting mini dfs cluster", io);	}	if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {	
mrappjar not found not running test 

private void testSleepJobInternal(Configuration sleepConf, boolean useRemoteJar, boolean jobSubmissionShouldSucceed, ResourceViolation violation) throws Exception {	LOG.info("\n\n\nStarting testSleepJob: useRemoteJar=" + useRemoteJar);	if (!jobSubmissionShouldSucceed && violation == null) {	Assert.fail("Test is misconfigured. jobSubmissionShouldSucceed is set" + " to false and a ResourceViolation is not specified.");	}	if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {	
mrappjar not found not running test 

public void testJobWithChangePriority() throws Exception {	Configuration sleepConf = new Configuration(mrCluster.getConfig());	Assume.assumeFalse(sleepConf.get(YarnConfiguration.RM_SCHEDULER) .equals(FairScheduler.class.getCanonicalName()));	if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {	
mrappjar not found not running test 

private void testJobClassloader(boolean useCustomClasses) throws IOException, InterruptedException, ClassNotFoundException {	LOG.info("\n\n\nStarting testJobClassloader()" + " useCustomClasses=" + useCustomClasses);	if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {	
mrappjar not found not running test 

public void testRandomWriter() throws IOException, InterruptedException, ClassNotFoundException {	
starting testrandomwriter 

public void testRandomWriter() throws IOException, InterruptedException, ClassNotFoundException {	if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {	
mrappjar not found not running test 

public void testFailingMapper() throws IOException, InterruptedException, ClassNotFoundException {	
starting testfailingmapper 

public void testFailingMapper() throws IOException, InterruptedException, ClassNotFoundException {	if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {	
mrappjar not found not running test 

public void testSleepJobWithSecurityOn() throws IOException, InterruptedException, ClassNotFoundException {	
starting testsleepjobwithsecurityon 

if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {	return;	}	mrCluster.getConfig().set( CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION, "kerberos");	mrCluster.getConfig().set(YarnConfiguration.RM_KEYTAB, "/etc/krb5.keytab");	mrCluster.getConfig().set(YarnConfiguration.NM_KEYTAB, "/etc/krb5.keytab");	mrCluster.getConfig().set(YarnConfiguration.RM_PRINCIPAL, "rm/sightbusy-lx@LOCALHOST");	mrCluster.getConfig().set(YarnConfiguration.NM_PRINCIPAL, "nm/sightbusy-lx@LOCALHOST");	UserGroupInformation.setConfiguration(mrCluster.getConfig());	UserGroupInformation user = UserGroupInformation.getCurrentUser();	
user name is 

return;	}	mrCluster.getConfig().set( CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION, "kerberos");	mrCluster.getConfig().set(YarnConfiguration.RM_KEYTAB, "/etc/krb5.keytab");	mrCluster.getConfig().set(YarnConfiguration.NM_KEYTAB, "/etc/krb5.keytab");	mrCluster.getConfig().set(YarnConfiguration.RM_PRINCIPAL, "rm/sightbusy-lx@LOCALHOST");	mrCluster.getConfig().set(YarnConfiguration.NM_PRINCIPAL, "nm/sightbusy-lx@LOCALHOST");	UserGroupInformation.setConfiguration(mrCluster.getConfig());	UserGroupInformation user = UserGroupInformation.getCurrentUser();	for (Token<? extends TokenIdentifier> str : user.getTokens()) {	
token is 

Map<String, Path> filesMap = pathsToMap(localFiles);	Assert.assertTrue(filesMap.containsKey("distributed.first.symlink"));	Assert.assertEquals(1, localFs.getFileStatus( filesMap.get("distributed.first.symlink")).getLen());	Assert.assertTrue(filesMap.containsKey("distributed.second.jar"));	Assert.assertTrue(localFs.getFileStatus( filesMap.get("distributed.second.jar")).getLen() > 1);	Map<String, Path> archivesMap = pathsToMap(localArchives);	Assert.assertTrue(archivesMap.containsKey("distributed.third.jar"));	Assert.assertTrue(localFs.exists(new Path( archivesMap.get("distributed.third.jar"), "distributed.jar.inside3")));	Assert.assertTrue(archivesMap.containsKey("distributed.fourth.jar"));	Assert.assertTrue(localFs.exists(new Path( archivesMap.get("distributed.fourth.jar"), "distributed.jar.inside4")));	
java classpath java class path 

private void testDistributedCache(String jobJarPath, boolean withWildcard) throws Exception {	if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {	
mrappjar not found not running test 

public void testThreadDumpOnTaskTimeout() throws IOException, InterruptedException, ClassNotFoundException {	if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {	
mrappjar not found not running test 

final JobId jobId = TypeConverter.toYarn(job.getJobID());	final ApplicationId appID = jobId.getAppId();	int pollElapsed = 0;	while (true) {	Thread.sleep(1000);	pollElapsed += 1000;	if (TERMINAL_RM_APP_STATES.contains(mrCluster.getResourceManager() .getRMContext().getRMApps().get(appID).getState())) {	break;	}	if (pollElapsed >= 60000) {	
application did not reach terminal state within seconds 

final String appIdStr = appID.toString();	final String appIdSuffix = appIdStr.substring("application_".length(), appIdStr.length());	final String containerGlob = "container_" + appIdSuffix + "_*_*";	final String syslogGlob = appIdStr + Path.SEPARATOR + containerGlob + Path.SEPARATOR + TaskLog.LogName.SYSLOG;	int numAppMasters = 0;	int numMapTasks = 0;	for (int i = 0; i < NUM_NODE_MGRS; i++) {	final Configuration nmConf = mrCluster.getNodeManager(i).getConfig();	for (String logDir : nmConf.getTrimmedStrings(YarnConfiguration.NM_LOG_DIRS)) {	final Path absSyslogGlob = new Path(logDir + Path.SEPARATOR + syslogGlob);	
checking for glob 

public void testSharedCache() throws Exception {	Path localJobJarPath = makeJobJarWithLib(TEST_ROOT_DIR.toUri().toString());	if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {	
mrappjar not found not running test 

========================= hadoop sample_5622 =========================

public void run() {	int failures = 0;	
thread started 

public void run() {	int failures = 0;	try {	while (!stopped && !Thread.currentThread().isInterrupted()) {	try {	int numNewMaps = getMapCompletionEvents();	failures = 0;	if (numNewMaps > 0) {	
got new map outputs 

public void run() {	int failures = 0;	try {	while (!stopped && !Thread.currentThread().isInterrupted()) {	try {	int numNewMaps = getMapCompletionEvents();	failures = 0;	if (numNewMaps > 0) {	}	
getmapeventsthread about to sleep for 

while (!stopped && !Thread.currentThread().isInterrupted()) {	try {	int numNewMaps = getMapCompletionEvents();	failures = 0;	if (numNewMaps > 0) {	}	if (!Thread.currentThread().isInterrupted()) {	Thread.sleep(SLEEP_TIME);	}	} catch (InterruptedException e) {	
eventfetcher is interrupted returning 

int numNewMaps = getMapCompletionEvents();	failures = 0;	if (numNewMaps > 0) {	}	if (!Thread.currentThread().isInterrupted()) {	Thread.sleep(SLEEP_TIME);	}	} catch (InterruptedException e) {	return;	} catch (IOException ie) {	
exception in getting events 

public void shutDown() {	this.stopped = true;	interrupt();	try {	join(5000);	} catch(InterruptedException ie) {	
got interrupted while joining 

protected int getMapCompletionEvents() throws IOException, InterruptedException {	int numNewMaps = 0;	TaskCompletionEvent events[] = null;	do {	MapTaskCompletionEventsUpdate update = umbilical.getMapCompletionEvents( (org.apache.hadoop.mapred.JobID)reduce.getJobID(), fromEventIdx, maxEventsToFetch, (org.apache.hadoop.mapred.TaskAttemptID)reduce);	events = update.getMapTaskCompletionEvents();	
got map completion events from 

========================= hadoop sample_4930 =========================

tickTime = conf.getInt(KEY_ZKSERVICE_TICK_TIME, ZooKeeperServer.DEFAULT_TICK_TIME);	String instancedirname = conf.getTrimmed( KEY_ZKSERVICE_DIR, "");	host = conf.getTrimmed(KEY_ZKSERVICE_HOST, DEFAULT_ZKSERVICE_HOST);	if (instancedirname.isEmpty()) {	File testdir = new File(System.getProperty("test.dir", "target"));	instanceDir = new File(testdir, "zookeeper" + getName());	} else {	instanceDir = new File(instancedirname);	FileUtil.fullyDelete(instanceDir);	}	
instance directory is 

protected void serviceStart() throws Exception {	setupSecurity();	ZooKeeperServer zkServer = new ZooKeeperServer();	FileTxnSnapLog ftxn = new FileTxnSnapLog(dataDir, dataDir);	zkServer.setTxnLogFactory(ftxn);	zkServer.setTickTime(tickTime);	
starting local zookeeper service 

protected void serviceStart() throws Exception {	setupSecurity();	ZooKeeperServer zkServer = new ZooKeeperServer();	FileTxnSnapLog ftxn = new FileTxnSnapLog(dataDir, dataDir);	zkServer.setTxnLogFactory(ftxn);	zkServer.setTickTime(tickTime);	factory = ServerCnxnFactory.createFactory();	factory.configure(getAddress(port), -1);	factory.startup(zkServer);	String connectString = getConnectionString();	
in memory zk started at 

========================= hadoop sample_2671 =========================

private void printConfWarningIfNeeded(DfsClientConf conf) {	String existing = this.getConfString();	String requested = conf.getShortCircuitConf().confAsString();	if (!existing.equals(requested)) {	if (!printedConfWarning) {	printedConfWarning = true;	
existing client context does not match requested configuration existing requested 

========================= hadoop sample_7023 =========================

protected void emitMetric(String groupName, String name, String type, String value, GangliaConf gConf, GangliaSlope gSlope) throws IOException {	if (name == null) {	
metric was emitted with no name 

protected void emitMetric(String groupName, String name, String type, String value, GangliaConf gConf, GangliaSlope gSlope) throws IOException {	if (name == null) {	return;	} else if (value == null) {	
metric name was emitted with a null value 

protected void emitMetric(String groupName, String name, String type, String value, GangliaConf gConf, GangliaSlope gSlope) throws IOException {	if (name == null) {	return;	} else if (value == null) {	return;	} else if (type == null) {	
metric name value has no type 

protected void emitMetric(String groupName, String name, String type, String value, GangliaConf gConf, GangliaSlope gSlope) throws IOException {	if (name == null) {	return;	} else if (value == null) {	return;	} else if (type == null) {	return;	}	if (LOG.isDebugEnabled()) {	
emitting metric type value slope from hostname 

========================= hadoop sample_3430 =========================

private FsVolumeReference chooseVolume(List<FsVolumeImpl> list, long blockSize) throws IOException {	while (true) {	FsVolumeImpl volume = blockChooser.chooseVolume(list, blockSize);	try {	return volume.obtainReference();	} catch (ClosedChannelException e) {	
chosen a closed volume 

public void run() {	try (FsVolumeReference ref = v.obtainReference()) {	
adding replicas to map for block pool on volume 

public void run() {	try (FsVolumeReference ref = v.obtainReference()) {	long startTime = Time.monotonicNow();	v.getVolumeMap(bpid, volumeMap, ramDiskReplicaMap);	long timeTaken = Time.monotonicNow() - startTime;	
time to add replicas to map for block pool on volume ms 

public void run() {	try (FsVolumeReference ref = v.obtainReference()) {	long startTime = Time.monotonicNow();	v.getVolumeMap(bpid, volumeMap, ramDiskReplicaMap);	long timeTaken = Time.monotonicNow() - startTime;	} catch (ClosedChannelException e) {	
the volume is closed while adding replicas ignored 

public void run() {	try (FsVolumeReference ref = v.obtainReference()) {	long startTime = Time.monotonicNow();	v.getVolumeMap(bpid, volumeMap, ramDiskReplicaMap);	long timeTaken = Time.monotonicNow() - startTime;	} catch (ClosedChannelException e) {	} catch (IOException ioe) {	
caught exception while adding replicas from will throw later 

private void removeVolume(FsVolumeImpl target) {	if (volumes.remove(target)) {	if (blockScanner != null) {	blockScanner.removeVolumeScanner(target);	}	try {	target.setClosed();	} catch (IOException e) {	
error occurs when waiting volume to close 

if (volumes.remove(target)) {	if (blockScanner != null) {	blockScanner.removeVolumeScanner(target);	}	try {	target.setClosed();	} catch (IOException e) {	}	target.shutdown();	volumesBeingRemoved.add(target);	
removed volume 

blockScanner.removeVolumeScanner(target);	}	try {	target.setClosed();	} catch (IOException e) {	}	target.shutdown();	volumesBeingRemoved.add(target);	} else {	if (FsDatasetImpl.LOG.isDebugEnabled()) {	
volume does not exist or is removed by others 

public void run() {	try (FsVolumeReference ref = v.obtainReference()) {	
scanning block pool on volume 

public void run() {	try (FsVolumeReference ref = v.obtainReference()) {	long startTime = Time.monotonicNow();	v.addBlockPool(bpid, conf);	long timeTaken = Time.monotonicNow() - startTime;	
time taken to scan block pool on ms 

public void run() {	try (FsVolumeReference ref = v.obtainReference()) {	long startTime = Time.monotonicNow();	v.addBlockPool(bpid, conf);	long timeTaken = Time.monotonicNow() - startTime;	} catch (ClosedChannelException e) {	} catch (IOException ioe) {	
caught exception while scanning will throw later 

========================= hadoop sample_7920 =========================

public void setup() throws Exception {	long seed = rand.nextLong();	rand.setSeed(seed);	
running with seed 

========================= hadoop sample_585 =========================

System.exit(0);	}	try {	StringUtils.startupShutdownMessage(Router.class, argv, LOG);	Router router = new Router();	ShutdownHookManager.get().addShutdownHook( new CompositeServiceShutdownHook(router), SHUTDOWN_HOOK_PRIORITY);	Configuration conf = new HdfsConfiguration();	router.init(conf);	router.start();	} catch (Throwable e) {	
failed to start router 

========================= hadoop sample_8293 =========================

try {	dtSecretManager.renewToken(token, "FakeRenewer");	Assert.fail("should have failed");	} catch (AccessControlException ace) {	}	dtSecretManager.renewToken(token, "JobTracker");	DelegationTokenIdentifier identifier = new DelegationTokenIdentifier();	byte[] tokenId = token.getIdentifier();	identifier.readFields(new DataInputStream( new ByteArrayInputStream(tokenId)));	Assert.assertTrue(null != dtSecretManager.retrievePassword(identifier));	
sleep to expire the token 

byte[] tokenId = token.getIdentifier();	identifier.readFields(new DataInputStream( new ByteArrayInputStream(tokenId)));	Assert.assertTrue(null != dtSecretManager.retrievePassword(identifier));	Thread.sleep(6000);	try {	dtSecretManager.retrievePassword(identifier);	Assert.fail("Token should have expired");	} catch (InvalidToken e) {	}	dtSecretManager.renewToken(token, "JobTracker");	
sleep beyond the max lifetime 

Assert.assertNotNull(token);	DelegationTokenIdentifier identifier = new DelegationTokenIdentifier();	byte[] tokenId = token.getIdentifier();	DataInputStream in = new DataInputStream(new ByteArrayInputStream(tokenId));	try {	identifier.readFields(in);	} finally {	in.close();	}	Assert.assertNotNull(identifier);	
a valid token should have non null password and should be renewed successfully 

========================= hadoop sample_7166 =========================

static protected FSDataOutputStream writeFile(FileSystem fileSys, Path name, int repl, int numOfBlocks, boolean completeFile) throws IOException {	FSDataOutputStream stm = fileSys.create(name, true, fileSys.getConf() .getInt(CommonConfigurationKeys.IO_FILE_BUFFER_SIZE_KEY, 4096), (short) repl, blockSize);	byte[] buffer = new byte[blockSize*numOfBlocks];	Random rand = new Random(seed);	rand.nextBytes(buffer);	stm.write(buffer);	
created file with replicas 

datanodeInfos.add(NameNodeAdapter.getDatanode( cluster.getNamesystem(nnIndex), info[index]));	found = true;	break;	}	}	if (!found) {	throw new IOException("invalid datanodeUuid " + datanodeUuid);	}	}	}	
taking node out of service 

protected void putNodeInService(int nnIndex, DatanodeInfo outOfServiceNode) throws IOException {	
putting node in service 

protected void waitNodeState(List<DatanodeInfo> nodes, AdminStates state) {	for (DatanodeInfo node : nodes) {	boolean done = (state == node.getAdminState());	while (!done) {	
waiting for node to change state to current state 

protected void waitNodeState(List<DatanodeInfo> nodes, AdminStates state) {	for (DatanodeInfo node : nodes) {	boolean done = (state == node.getAdminState());	while (!done) {	try {	Thread.sleep(HEARTBEAT_INTERVAL * 500);	} catch (InterruptedException e) {	}	done = (state == node.getAdminState());	}	
node reached the state 

========================= hadoop sample_7609 =========================

private void shouldThrow(Class<? extends Exception> ecls, Runnable runnable) {	try {	runnable.run();	} catch (Exception e) {	assertSame(ecls, e.getClass());	
got expected 

========================= hadoop sample_5634 =========================

public SimulatedFsDatasetVerifier(DataStorage storage, Configuration conf) {	super(storage, conf);	
assigned datanodeuuid is 

========================= hadoop sample_7259 =========================

public void testStreamXmlMultiInnerFast() throws Exception {	if (hasPerl) {	blockSize = 60;	isSlowMatch = "false";	super.testCommandLine();	}	else {	
no perl skipping test 

public void testStreamXmlMultiOuterFast() throws Exception {	if (hasPerl) {	blockSize = 80;	isSlowMatch = "false";	super.testCommandLine();	}	else {	
no perl skipping test 

public void testStreamXmlMultiInnerSlow() throws Exception {	if (hasPerl) {	blockSize = 60;	isSlowMatch = "true";	super.testCommandLine();	}	else {	
no perl skipping test 

public void testStreamXmlMultiOuterSlow() throws Exception {	if (hasPerl) {	blockSize = 80;	isSlowMatch = "true";	super.testCommandLine();	}	else {	
no perl skipping test 

========================= hadoop sample_5771 =========================

private Dest lookupRoute(WebApp.HTTP method, String path) {	String key = path;	do {	Dest dest = routes.get(key);	if (dest != null && methodAllowed(method, dest)) {	if ((Object)key == path) {	
exact match for 

private Dest lookupRoute(WebApp.HTTP method, String path) {	String key = path;	do {	Dest dest = routes.get(key);	if (dest != null && methodAllowed(method, dest)) {	if ((Object)key == path) {	return dest;	} else if (isGoodMatch(dest, path)) {	
prefix for 

return resolveAction(method, dest, path);	}	Map.Entry<String, Dest> lower = routes.lowerEntry(key);	if (lower == null) {	return null;	}	dest = lower.getValue();	if (prefixMatches(dest, path)) {	if (methodAllowed(method, dest)) {	if (isGoodMatch(dest, path)) {	
prefix match for 

static boolean prefixMatches(Dest dest, String path) {	
checking prefix for path 

private <T> Class<? extends T> load(Class<T> cls, String className) {	
trying 

private <T> Class<? extends T> load(Class<T> cls, String className) {	try {	Class<?> found = Class.forName(className);	if (cls.isAssignableFrom(found)) {	
found 

private <T> Class<? extends T> load(Class<T> cls, String className) {	try {	Class<?> found = Class.forName(className);	if (cls.isAssignableFrom(found)) {	return (Class<? extends T>) found;	}	
found a but it s not a 

========================= hadoop sample_2237 =========================

public <V> Future<V> submit(Callable<V> callable) {	if (LOG.isDebugEnabled()) {	
submitting 

protected void serviceInit(Configuration conf) throws Exception {	super.serviceInit(conf);	RegistrySecurity registrySecurity = getRegistrySecurity();	if (registrySecurity.isSecureRegistry()) {	ACL sasl = registrySecurity.createSaslACLFromCurrentUser(ZooDefs.Perms.ALL);	registrySecurity.addSystemACL(sasl);	
registry system acls 

protected void serviceStart() throws Exception {	super.serviceStart();	try {	createRootRegistryPaths();	} catch (NoPathPermissionsException e) {	String message = String.format(Locale.ENGLISH, "Failed to create root paths {%s};" + "%ndiagnostics={%s}" + "%ncurrent registry is:" + "%n{%s}", e, bindingDiagnosticDetails(), dumpRegistryRobustly(true));	
failure 

public void createRootRegistryPaths() throws IOException {	List<ACL> systemACLs = getRegistrySecurity().getSystemACLs();	
system acls 

protected void verifyRealmValidity() throws ServiceStateException {	if (isSecure()) {	String realm = getRegistrySecurity().getKerberosRealm();	if (StringUtils.isEmpty(realm)) {	throw new ServiceStateException("Cannot determine service realm");	}	if (LOG.isDebugEnabled()) {	
started registry operations in realm 

ServiceRecord serviceRecord = resolve(path);	toDelete = selector.shouldSelect(path, registryPathStatus, serviceRecord);	} catch (EOFException ignored) {	} catch (InvalidRecordException ignored) {	} catch (NoRecordException ignored) {	} catch (PathNotFoundException e) {	return 0;	}	if (toDelete && !entries.isEmpty()) {	if (LOG.isDebugEnabled()) {	
match on record with children 

} catch (InvalidRecordException ignored) {	} catch (NoRecordException ignored) {	} catch (PathNotFoundException e) {	return 0;	}	if (toDelete && !entries.isEmpty()) {	if (LOG.isDebugEnabled()) {	}	switch (purgePolicy) {	case SkipOnChildren: if (LOG.isDebugEnabled()) {	
skipping deletion 

}	if (toDelete && !entries.isEmpty()) {	if (LOG.isDebugEnabled()) {	}	switch (purgePolicy) {	case SkipOnChildren: if (LOG.isDebugEnabled()) {	}	toDelete = false;	break;	case PurgeAll: if (LOG.isDebugEnabled()) {	
scheduling for deletion with children 

case SkipOnChildren: if (LOG.isDebugEnabled()) {	}	toDelete = false;	break;	case PurgeAll: if (LOG.isDebugEnabled()) {	}	toDelete = true;	entries = new ArrayList<RegistryPathStatus>(0);	break;	case FailOnChildren: if (LOG.isDebugEnabled()) {	
failing deletion operation 

public Integer call() throws Exception {	if (LOG.isDebugEnabled()) {	
executing 

========================= hadoop sample_2667 =========================

final Long fileLength = dfs.getFileStatus(file).getLen();	final Long file2Length = dfs.getFileStatus(file2).getLen();	int ret = -1;	try {	ret = shell.run(new String[] {"-du", dir.toString()});	} catch (Exception e) {	System.err.println("Exception raised from DFSShell.run " + e.getLocalizedMessage());	}	assertEquals(0, ret);	String returnString = out.toString();	
du return is 

writeFile(dfs, newFile);	final Long newFileLength = dfs.getFileStatus(newFile).getLen();	ret = -1;	try {	ret = shell.run(new String[] {"-du", "-s", parent.toString()});	} catch (Exception e) {	System.err.println("Exception raised from DFSShell.run " + e.getLocalizedMessage());	}	assertEquals(0, ret);	returnString = out.toString();	
du s return is 

assertTrue(returnString.contains(combinedLength.toString()));	out.reset();	ret = -1;	try {	ret = shell.run(new String[] {"-du", parent.toString()});	} catch (Exception e) {	System.err.println("Exception raised from DFSShell.run " + e.getLocalizedMessage());	}	assertEquals(0, ret);	returnString = out.toString();	
du return is 

assertTrue(returnString.contains(combinedLength.toString()));	out.reset();	ret = -1;	try {	ret = shell.run(new String[] {"-du", "-s", "-x", parent.toString()});	} catch (Exception e) {	System.err.println("Exception raised from DFSShell.run " + e.getLocalizedMessage());	}	assertEquals(0, ret);	returnString = out.toString();	
du s x return is 

assertTrue(returnString.contains(exludeSnapshotLength.toString()));	out.reset();	ret = -1;	try {	ret = shell.run(new String[] {"-du", "-x", parent.toString()});	} catch (Exception e) {	System.err.println("Exception raised from DFSShell.run " + e.getLocalizedMessage());	}	assertEquals(0, ret);	returnString = out.toString();	
du x return is 

writeFile(dfs, newFile);	final Long newFileLength = dfs.getFileStatus(newFile).getLen();	int val = -1;	try {	val = shell.run(new String[] {"-count", "-v", parent.toString() });	} catch (Exception e) {	System.err.println("Exception raised from DFSShell.run " + e.getLocalizedMessage());	}	assertEquals(0, val);	String returnString = out.toString();	
count return is 

assertEquals(fileLength + file2Length + newFileLength, in.nextLong());	out.reset();	val = -1;	try {	val = shell.run(new String[] {"-count", "-x", "-v", parent.toString()});	} catch (Exception e) {	System.err.println("Exception raised from DFSShell.run " + e.getLocalizedMessage());	}	assertEquals(0, val);	returnString = out.toString();	
count x return is 

final Path testFile = new Path("testTailWithFresh", "file1");	dfs.create(testFile);	final ByteArrayOutputStream out = new ByteArrayOutputStream();	System.setOut(new PrintStream(out));	final Thread tailer = new Thread() {	public void run() {	final String[] argv = new String[]{"-tail", "-f", testFile.toString()};	try {	ToolRunner.run(new FsShell(dfs.getConf()), argv);	} catch (Exception e) {	
client that tails the test file fails 

private void confirmPermissionChange(String toApply, String expected, FileSystem fs, FsShell shell, Path dir2) throws IOException {	
confirming permission change of to 

private void confirmPermissionChange(String toApply, String expected, FileSystem fs, FsShell shell, Path dir2) throws IOException {	runCmd(shell, "-chmod", toApply, dir2.toString());	String result = fs.getFileStatus(dir2).getPermission().toString();	
permission change result 

========================= hadoop sample_7170 =========================

new Thread() {	public void run() {	UserGroupInformation.createUserForTesting(ENEMY, new String[] {});	UserGroupInformation.createUserForTesting(FRIEND, new String[] { FRIENDLY_GROUP });	UserGroupInformation.createUserForTesting(SUPER_USER, new String[] { SUPER_GROUP });	resourceManager.start();	};	}.start();	int waitCount = 0;	while (resourceManager.getServiceState() == STATE.INITED && waitCount++ < 60) {	
waiting for rm to start 

verifyEnemyAppReport(appReport);	List<ApplicationReport> appReports = enemyRmClient .getApplications(recordFactory .newRecordInstance(GetApplicationsRequest.class)) .getApplicationList();	Assert.assertEquals("App view by enemy should list the apps!!", 4, appReports.size());	for (ApplicationReport report : appReports) {	verifyEnemyAppReport(report);	}	try {	enemyRmClient.forceKillApplication(finishAppRequest);	Assert.fail("App killing by the enemy should fail!!");	} catch (YarnException e) {	
got exception while killing app as the enemy 

========================= hadoop sample_439 =========================

public Map<String, Double> getOutliers(Map<String, Double> stats) {	if (stats.size() < minNumResources) {	
skipping statistical outlier detection as we don t have latency data for enough resources have need at least 

========================= hadoop sample_7873 =========================

int loaded = 0;	for (String classname : toCreate) {	try {	Class<?> loadClass = getClassLoader().loadClass(classname);	Object instance = loadClass.getConstructor().newInstance();	if (!(instance instanceof Configuration)) {	throw new ExitUtil.ExitException(EXIT_SERVICE_CREATION_FAILURE, "Could not create " + classname + " because it is not a Configuration class/subclass");	}	loaded++;	} catch (ClassNotFoundException e) {	
failed to load because it is not on the classpath 

Class<?> loadClass = getClassLoader().loadClass(classname);	Object instance = loadClass.getConstructor().newInstance();	if (!(instance instanceof Configuration)) {	throw new ExitUtil.ExitException(EXIT_SERVICE_CREATION_FAILURE, "Could not create " + classname + " because it is not a Configuration class/subclass");	}	loaded++;	} catch (ClassNotFoundException e) {	} catch (ExitUtil.ExitException e) {	throw e;	} catch (Exception e) {	
failed to create 

public ExitUtil.ExitException launchService(Configuration conf, List<String> processedArgs, boolean addShutdownHook, boolean execute) {	ExitUtil.ExitException exitException;	try {	int exitCode = coreServiceLaunch(conf, processedArgs, addShutdownHook, execute);	if (service != null) {	Throwable failure = service.getFailureCause();	if (failure != null) {	Service.STATE failureState = service.getFailureState();	if (failureState == Service.STATE.STOPPED) {	
failure during shutdown 

protected int coreServiceLaunch(Configuration conf, List<String> processedArgs, boolean addShutdownHook, boolean execute) throws Exception {	instantiateService(conf);	ServiceShutdownHook shutdownHook = null;	if (addShutdownHook) {	shutdownHook = new ServiceShutdownHook(service);	shutdownHook.register(SHUTDOWN_PRIORITY);	}	String name = getServiceName();	
launched service 

protected int coreServiceLaunch(Configuration conf, List<String> processedArgs, boolean addShutdownHook, boolean execute) throws Exception {	instantiateService(conf);	ServiceShutdownHook shutdownHook = null;	if (addShutdownHook) {	shutdownHook = new ServiceShutdownHook(service);	shutdownHook.register(SHUTDOWN_PRIORITY);	}	String name = getServiceName();	LaunchableService launchableService = null;	if (service instanceof LaunchableService) {	
service implements launchableservice 

ServiceShutdownHook shutdownHook = null;	if (addShutdownHook) {	shutdownHook = new ServiceShutdownHook(service);	shutdownHook.register(SHUTDOWN_PRIORITY);	}	String name = getServiceName();	LaunchableService launchableService = null;	if (service instanceof LaunchableService) {	launchableService = (LaunchableService) service;	if (launchableService.isInState(Service.STATE.INITED)) {	
launchableservice initialized in constructor before cli arguments passed in 

service.init(configuration);	}	int exitCode;	try {	service.start();	exitCode = EXIT_SUCCESS;	if (execute && service.isInState(Service.STATE.STARTED)) {	if (launchableService != null) {	try {	exitCode = launchableService.execute();	
service execution returned exit code 

service.start();	exitCode = EXIT_SUCCESS;	if (execute && service.isInState(Service.STATE.STARTED)) {	if (launchableService != null) {	try {	exitCode = launchableService.execute();	} finally {	service.stop();	}	} else {	
waiting for service threads to terminate 

Preconditions.checkArgument(conf != null, "null conf");	Preconditions.checkArgument(serviceClassName != null, "null service classname");	Preconditions.checkArgument(!serviceClassName.isEmpty(), "undefined service classname");	configuration = conf;	Object instance;	try {	Class<?> serviceClass = getClassLoader().loadClass(serviceClassName);	try {	instance = serviceClass.getConstructor().newInstance();	} catch (NoSuchMethodException noEmptyConstructor) {	
no empty constructor 

public void uncaughtException(Thread thread, Throwable exception) {	
uncaught exception in thread exiting 

protected List<String> parseCommandArgs(Configuration conf, List<String> args) {	Preconditions.checkNotNull(commandOptions, "Command options have not been created");	StringBuilder argString = new StringBuilder(args.size() * 32);	for (String arg : args) {	argString.append("\"").append(arg).append("\" ");	}	
command line 

argString.append("\"").append(arg).append("\" ");	}	try {	String[] argArray = args.toArray(new String[args.size()]);	GenericOptionsParser parser = createGenericOptionsParser(conf, argArray);	if (!parser.isParseSuccessful()) {	throw new ServiceLaunchException(EXIT_COMMAND_ARGUMENT_ERROR, E_PARSE_FAILED + " %s", argString);	}	CommandLine line = parser.getCommandLine();	List<String> remainingArgs = Arrays.asList(parser.getRemainingArgs());	
remaining arguments 

if (!parser.isParseSuccessful()) {	throw new ServiceLaunchException(EXIT_COMMAND_ARGUMENT_ERROR, E_PARSE_FAILED + " %s", argString);	}	CommandLine line = parser.getCommandLine();	List<String> remainingArgs = Arrays.asList(parser.getRemainingArgs());	if (line.hasOption(ARG_CONF)) {	String[] filenames = line.getOptionValues(ARG_CONF);	verifyConfigurationFilesExist(filenames);	for (String filename : filenames) {	File file = new File(filename);	
configuration files 

if (line.hasOption(ARG_CONF)) {	String[] filenames = line.getOptionValues(ARG_CONF);	verifyConfigurationFilesExist(filenames);	for (String filename : filenames) {	File file = new File(filename);	confResourceUrls.add(file.toURI().toURL());	}	}	if (line.hasOption(ARG_CONFCLASS)) {	List<String> classnameList = Arrays.asList( line.getOptionValues(ARG_CONFCLASS));	
configuration classes 

protected void verifyConfigurationFilesExist(String[] filenames) {	if (filenames == null) {	return;	}	for (String filename : filenames) {	File file = new File(filename);	
conf file 

========================= hadoop sample_4072 =========================

private static ByteBuffer convertCredentialsToByteBuffer( Credentials credentials) {	ByteBuffer appAttemptTokens = null;	DataOutputBuffer dob = new DataOutputBuffer();	try {	if (credentials != null) {	credentials.writeTokenStorageToStream(dob);	appAttemptTokens = ByteBuffer.wrap(dob.getData(), 0, dob.getLength());	}	return appAttemptTokens;	} catch (IOException e) {	
failed to convert credentials to bytebuffer 

try {	Credentials credentials = null;	if (appAttemptTokens != null) {	credentials = new Credentials();	appAttemptTokens.rewind();	dibb.reset(appAttemptTokens);	credentials.readTokenStorageStream(dibb);	}	return credentials;	} catch (IOException e) {	
failed to convert credentials from bytebuffer 

========================= hadoop sample_747 =========================

private void initializeFacadeInternal(Configuration config) {	this.conf = config;	try {	this.stateStore = (FederationStateStore) createRetryInstance(this.conf, YarnConfiguration.FEDERATION_STATESTORE_CLIENT_CLASS, YarnConfiguration.DEFAULT_FEDERATION_STATESTORE_CLIENT_CLASS, FederationStateStore.class, createRetryPolicy(conf));	this.stateStore.init(conf);	this.subclusterResolver = createInstance(conf, YarnConfiguration.FEDERATION_CLUSTER_RESOLVER_CLASS, YarnConfiguration.DEFAULT_FEDERATION_CLUSTER_RESOLVER_CLASS, SubClusterResolver.class);	this.subclusterResolver.load();	initCache();	} catch (YarnException ex) {	
failed to initialize the federationstatestorefacade object 

private void initCache() {	cacheTimeToLive = conf.getInt(YarnConfiguration.FEDERATION_CACHE_TIME_TO_LIVE_SECS, YarnConfiguration.DEFAULT_FEDERATION_CACHE_TIME_TO_LIVE_SECS);	if (isCachingEnabled()) {	CachingProvider jcacheProvider = Caching.getCachingProvider();	CacheManager jcacheManager = jcacheProvider.getCacheManager();	this.cache = jcacheManager.getCache(this.getClass().getSimpleName());	if (this.cache == null) {	
creating a jcache manager with name 

public SubClusterInfo getSubCluster(final SubClusterId subClusterId, final boolean flushCache) throws YarnException {	if (flushCache && isCachingEnabled()) {	
flushing subclusters from cache and rehydrating from store most likely on account of rm failover 

========================= hadoop sample_1335 =========================

MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_OF_DATANODES).build();	try {	cluster.waitActive();	final String bpid = cluster.getNamesystem().getBlockPoolId();	final List<DataNode> datanodes = cluster.getDataNodes();	final DFSClient client = cluster.getFileSystem().dfs;	assertReports(NUM_OF_DATANODES, DatanodeReportType.ALL, client, datanodes, bpid);	assertReports(NUM_OF_DATANODES, DatanodeReportType.LIVE, client, datanodes, bpid);	assertReports(0, DatanodeReportType.DEAD, client, datanodes, bpid);	final DataNode last = datanodes.get(datanodes.size() - 1);	
xxx shutdown datanode 

========================= hadoop sample_7598 =========================

public void abort() throws IOException {	
aborting 

========================= hadoop sample_8397 =========================

public long getSum(long time) {	long sum = 0;	for (Bucket bucket : buckets) {	boolean stale = bucket.isStaleNow(time);	if (!stale) {	sum += bucket.value.get();	}	if (LOG.isDebugEnabled()) {	long bucketTime = bucket.updateTime.get();	String timeStr = new Date(bucketTime).toString();	
sum bucket updatetime isstale at 

========================= hadoop sample_7961 =========================

protected void assertLaunchOutcome(int expected, String text, String... args) {	try {	if (LOG.isDebugEnabled()) {	
launching service with expected outcome 

========================= hadoop sample_3164 =========================

try {	if (ch == null) {	socket.connect(endpoint, timeout);	} else {	SocketIOWithTimeout.connect(ch, endpoint, timeout);	}	} catch (SocketTimeoutException ste) {	throw new ConnectTimeoutException(ste.getMessage());	}	if (socket.getLocalPort() == socket.getPort() && socket.getLocalAddress().equals(socket.getInetAddress())) {	
detected a loopback tcp socket disconnecting it 

private static <T extends IOException> T wrapWithMessage( T exception, String msg) {	Class<? extends Throwable> clazz = exception.getClass();	try {	Constructor<? extends Throwable> ctor = clazz.getConstructor(String.class);	Throwable t = ctor.newInstance(msg);	return (T)(t.initCause(exception));	} catch (Throwable e) {	
unable to wrap exception of type it has no string constructor 

public static List<InetAddress> getIPs(String subnet, boolean returnSubinterfaces) {	List<InetAddress> addrs = new ArrayList<InetAddress>();	SubnetInfo subnetInfo = new SubnetUtils(subnet).getInfo();	Enumeration<NetworkInterface> nifs;	try {	nifs = NetworkInterface.getNetworkInterfaces();	} catch (SocketException e) {	
unable to get host interfaces 

========================= hadoop sample_3534 =========================

hdfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_ENTER, false);	hdfs.saveNamespace();	List<XAttr> attributes = new ArrayList<XAttr>();	attributes.add(XAttrHelper.buildXAttr("user.attr1", "value1".getBytes()));	attr1JSon = JsonUtil.toJsonString(attributes, null);	attributes.add(XAttrHelper.buildXAttr("user.attr2", "value2".getBytes()));	originalFsimage = FSImageTestUtil.findLatestImageFile(FSImageTestUtil .getFSImage(cluster.getNameNode()).getStorage().getStorageDir(0));	if (originalFsimage == null) {	throw new RuntimeException("Didn't generate or can't find fsimage");	}	
original fs image file is 

========================= hadoop sample_7127 =========================

Thread thread = new Thread(shutdown);	thread.setDaemon(true);	thread.setName("Service Forced Shutdown");	thread.start();	try {	thread.join(shutdownTimeMillis);	} catch (InterruptedException ignored) {	}	forcedShutdownTimedOut = !shutdown.getServiceWasShutdown();	if (forcedShutdownTimedOut) {	
service did not shut down in time 

========================= hadoop sample_4079 =========================

for (File f : listing) {	if (f.isDirectory()) {	if (hasDirectoryChanged(f)) {	changed = true;	}	} else if (f.isFile()) {	if (hasFileChanged(f)) {	changed = true;	}	} else {	
skipping entry that is not a file or directory 

public void writeChecksums() throws IOException {	ObjectMapper mapper = new ObjectMapper();	try (BufferedOutputStream out = new BufferedOutputStream( new FileOutputStream(checksumFile))) {	mapper.writeValue(out, computedChecksums);	
wrote protoc checksums to file 

public void execute() throws MojoExecutionException {	try {	List<String> command = new ArrayList<String>();	command.add(protocCommand);	command.add("--version");	Exec exec = new Exec(mojo);	List<String> out = new ArrayList<String>();	if (exec.run(command, out) == 127) {	
protoc not found at 

try {	List<String> command = new ArrayList<String>();	command.add(protocCommand);	command.add("--version");	Exec exec = new Exec(mojo);	List<String> out = new ArrayList<String>();	if (exec.run(command, out) == 127) {	throw new MojoExecutionException("protoc failure");	} else {	if (out.isEmpty()) {	
stdout 

List<File> changedSources = new ArrayList<>();	boolean sourcesChanged = false;	for (File f : FileSetUtils.convertFileSetToFiles(source)) {	if (comparator.hasChanged(f) || importsChanged) {	sourcesChanged = true;	changedSources.add(f);	command.add(f.getCanonicalPath());	}	}	if (!sourcesChanged && !importsChanged) {	
no changes detected in protoc files skipping generation 

b.append(f.toString());	prefix = ", ";	}	b.append("]");	mojo.getLog().debug(b.toString());	}	exec = new Exec(mojo);	out = new ArrayList<String>();	List<String> err = new ArrayList<>();	if (exec.run(command, out, err) != 0) {	
protoc compiler error 

========================= hadoop sample_4429 =========================

out = fs.create(file, true, 8192, (short)3, (long)(128*1024*1024));	Mockito.when(faultInjector.corruptPacket()).thenReturn(true, false);	Mockito.when(faultInjector.uncorruptPacket()).thenReturn(false);	try {	for (int i = 0; i < 5; i++) {	out.write(data, 0, 65535);	}	out.close();	fail("Write did not fail");	} catch (IOException ioe) {	
got expected exception 

util.waitReplication(fs, "/srcdat", (short)2);	final int dnIdx = 0;	final DataNode dn = cluster.getDataNodes().get(dnIdx);	final String bpid = cluster.getNamesystem().getBlockPoolId();	List<FinalizedReplica> replicas = dn.getFSDataset().getFinalizedBlocks(bpid);	assertTrue("Replicas do not exist", !replicas.isEmpty());	for (int idx = 0; idx < replicas.size(); idx++) {	FinalizedReplica replica = replicas.get(idx);	ExtendedBlock eb = new ExtendedBlock(bpid, replica);	if (idx % 3 == 0) {	
deliberately removing meta for block 

final String bpid = cluster.getNamesystem().getBlockPoolId();	List<FinalizedReplica> replicas = dn.getFSDataset().getFinalizedBlocks(bpid);	assertTrue("Replicas do not exist", !replicas.isEmpty());	for (int idx = 0; idx < replicas.size(); idx++) {	FinalizedReplica replica = replicas.get(idx);	ExtendedBlock eb = new ExtendedBlock(bpid, replica);	if (idx % 3 == 0) {	cluster.deleteMeta(dnIdx, eb);	} else if (idx % 3 == 1) {	final int newSize = 2;	
deliberately truncating meta file for block to size bytes 

if (idx % 3 == 0) {	cluster.deleteMeta(dnIdx, eb);	} else if (idx % 3 == 1) {	final int newSize = 2;	cluster.truncateMeta(dnIdx, eb, newSize);	} else {	cluster.corruptMeta(dnIdx, eb);	}	}	assertTrue("Corrupted replicas not handled properly.", util.checkFiles(fs, "/srcdat"));	
all file still have a valid replica 

cluster.deleteMeta(dnIdx, eb);	} else if (idx % 3 == 1) {	final int newSize = 2;	cluster.truncateMeta(dnIdx, eb, newSize);	} else {	cluster.corruptMeta(dnIdx, eb);	}	}	assertTrue("Corrupted replicas not handled properly.", util.checkFiles(fs, "/srcdat"));	util.setReplication(fs, "/srcdat", (short)1);	
the excess corrupted replica test is disabled pending hadoop 

FileSystem fs = cluster.getFileSystem();	DFSTestUtil.createFile(fs, file, fileSize, replFactor, 12345L /*seed*/);	DFSTestUtil.waitReplication(fs, file, replFactor);	ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, file);	int blockFilesCorrupted = cluster.corruptBlockOnDataNodes(block);	assertEquals("All replicas not corrupted", replFactor, blockFilesCorrupted);	try {	IOUtils.copyBytes(fs.open(file), new IOUtils.NullOutputStream(), conf, true);	fail("Didn't get exception");	} catch (IOException ioe) {	
got expected exception 

========================= hadoop sample_7655 =========================

Configuration conf = getConfig();	YarnRPC rpc = YarnRPC.create(conf);	initializeWebApp(conf);	InetSocketAddress address = conf.getSocketAddr( JHAdminConfig.MR_HISTORY_BIND_HOST, JHAdminConfig.MR_HISTORY_ADDRESS, JHAdminConfig.DEFAULT_MR_HISTORY_ADDRESS, JHAdminConfig.DEFAULT_MR_HISTORY_PORT);	server = rpc.getServer(HSClientProtocol.class, protocolHandler, address, conf, jhsDTSecretManager, conf.getInt(JHAdminConfig.MR_HISTORY_CLIENT_THREAD_COUNT, JHAdminConfig.DEFAULT_MR_HISTORY_CLIENT_THREAD_COUNT));	if (conf.getBoolean( CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION, false)) {	server.refreshServiceAcl(conf, new ClientHSPolicyProvider());	}	server.start();	this.bindAddress = conf.updateConnectAddr(JHAdminConfig.MR_HISTORY_BIND_HOST, JHAdminConfig.MR_HISTORY_ADDRESS, JHAdminConfig.DEFAULT_MR_HISTORY_ADDRESS, server.getListenerAddress());	
instantiated historyclientservice at 

========================= hadoop sample_5392 =========================

protected synchronized void heartbeat() throws Exception {	AllocateRequest allocateRequest = AllocateRequest.newInstance(this.lastResponseID, super.getApplicationProgress(), new ArrayList<ResourceRequest>(), new ArrayList<ContainerId>(), null);	AllocateResponse allocateResponse = null;	try {	allocateResponse = scheduler.allocate(allocateRequest);	retrystartTime = System.currentTimeMillis();	} catch (ApplicationAttemptNotFoundException e) {	
event from rm shutting down application master 

protected synchronized void heartbeat() throws Exception {	AllocateRequest allocateRequest = AllocateRequest.newInstance(this.lastResponseID, super.getApplicationProgress(), new ArrayList<ResourceRequest>(), new ArrayList<ContainerId>(), null);	AllocateResponse allocateResponse = null;	try {	allocateResponse = scheduler.allocate(allocateRequest);	retrystartTime = System.currentTimeMillis();	} catch (ApplicationAttemptNotFoundException e) {	eventHandler.handle(new JobEvent(this.getJob().getID(), JobEventType.JOB_AM_REBOOT));	throw new YarnRuntimeException( "Resource Manager doesn't recognize AttemptId: " + this.getContext().getApplicationID(), e);	} catch (ApplicationMasterNotRegisteredException e) {	
applicationmaster is out of sync with resourcemanager hence resync and send outstanding requests 

allocateResponse = scheduler.allocate(allocateRequest);	retrystartTime = System.currentTimeMillis();	} catch (ApplicationAttemptNotFoundException e) {	eventHandler.handle(new JobEvent(this.getJob().getID(), JobEventType.JOB_AM_REBOOT));	throw new YarnRuntimeException( "Resource Manager doesn't recognize AttemptId: " + this.getContext().getApplicationID(), e);	} catch (ApplicationMasterNotRegisteredException e) {	this.lastResponseID = 0;	register();	} catch (Exception e) {	if (System.currentTimeMillis() - retrystartTime >= retryInterval) {	
could not contact rm after milliseconds 

public void handle(ContainerAllocatorEvent event) {	if (event.getType() == ContainerAllocator.EventType.CONTAINER_REQ) {	
processing the event 

========================= hadoop sample_5238 =========================

public void map(IntWritable key, IntWritable val, Context context) throws IOException {	TimelineClient tlc = TimelineClient.createTimelineClient();	TimelineEntityConverterV1 converter = new TimelineEntityConverterV1();	JobHistoryFileReplayHelper helper = new JobHistoryFileReplayHelper(context);	int replayMode = helper.getReplayMode();	Collection<JobFiles> jobs = helper.getJobFiles();	JobHistoryFileParser parser = helper.getParser();	if (jobs.isEmpty()) {	
will process no jobs 

public void map(IntWritable key, IntWritable val, Context context) throws IOException {	TimelineClient tlc = TimelineClient.createTimelineClient();	TimelineEntityConverterV1 converter = new TimelineEntityConverterV1();	JobHistoryFileReplayHelper helper = new JobHistoryFileReplayHelper(context);	int replayMode = helper.getReplayMode();	Collection<JobFiles> jobs = helper.getJobFiles();	JobHistoryFileParser parser = helper.getParser();	if (jobs.isEmpty()) {	} else {	
will process jobs 

TimelineEntityConverterV1 converter = new TimelineEntityConverterV1();	JobHistoryFileReplayHelper helper = new JobHistoryFileReplayHelper(context);	int replayMode = helper.getReplayMode();	Collection<JobFiles> jobs = helper.getJobFiles();	JobHistoryFileParser parser = helper.getParser();	if (jobs.isEmpty()) {	} else {	}	for (JobFiles job: jobs) {	String jobIdStr = job.getJobId();	
processing 

JobId jobId = TypeConverter.toYarn(JobID.forName(jobIdStr));	ApplicationId appId = jobId.getAppId();	try {	Path historyFilePath = job.getJobHistoryFilePath();	Path confFilePath = job.getJobConfFilePath();	if ((historyFilePath == null) || (confFilePath == null)) {	continue;	}	JobInfo jobInfo = parser.parseHistoryFile(historyFilePath);	Configuration jobConf = parser.parseConfiguration(confFilePath);	
parsed the job history file and the configuration file for job 

try {	Path historyFilePath = job.getJobHistoryFilePath();	Path confFilePath = job.getJobConfFilePath();	if ((historyFilePath == null) || (confFilePath == null)) {	continue;	}	JobInfo jobInfo = parser.parseHistoryFile(historyFilePath);	Configuration jobConf = parser.parseConfiguration(confFilePath);	long totalTime = 0;	Set<TimelineEntity> entitySet = converter.createTimelineEntities(jobInfo, jobConf);	
converted them into timeline entities for job 

try {	switch (replayMode) {	case JobHistoryFileReplayHelper.WRITE_ALL_AT_ONCE: writeAllEntities(tlc, entitySet, ugi);	break;	case JobHistoryFileReplayHelper.WRITE_PER_ENTITY: writePerEntity(tlc, entitySet, ugi);	break;	default: break;	}	} catch (Exception e) {	context.getCounter(PerfCounters.TIMELINE_SERVICE_WRITE_FAILURES). increment(1);	
writing to the timeline service failed 

case JobHistoryFileReplayHelper.WRITE_PER_ENTITY: writePerEntity(tlc, entitySet, ugi);	break;	default: break;	}	} catch (Exception e) {	context.getCounter(PerfCounters.TIMELINE_SERVICE_WRITE_FAILURES). increment(1);	}	long endWrite = System.nanoTime();	totalTime += TimeUnit.NANOSECONDS.toMillis(endWrite-startWrite);	int numEntities = entitySet.size();	
wrote entities in ms 

private void writePerEntity(TimelineClient tlc, Set<TimelineEntity> entitySet, UserGroupInformation ugi) throws IOException, YarnException {	for (TimelineEntity entity : entitySet) {	tlc.putEntities(entity);	
wrote entity 

========================= hadoop sample_5687 =========================

Path targetOfTests = fileSystemTestHelper.getTestRootPath(fsTarget);	fsTarget.delete(targetOfTests, true);	fsTarget.mkdirs(targetOfTests);	String testDir = fileSystemTestHelper.getTestRootPath(fsTarget).toUri() .getPath();	linkUpFirstComponents(conf, testDir, fsTarget, "test dir");	setUpHomeDir(conf, fsTarget);	String wdDir = fsTarget.getWorkingDirectory().toUri().getPath();	linkUpFirstComponents(conf, wdDir, fsTarget, "working dir");	FileSystem fsView = FileSystem.get(FsConstants.VIEWFS_URI, conf);	fsView.setWorkingDirectory(new Path(wdDir));	
working dir is 

static void setUpHomeDir(Configuration conf, FileSystem fsTarget) {	String homeDir = fsTarget.getHomeDirectory().toUri().getPath();	int indexOf2ndSlash = homeDir.indexOf('/', 1);	if (indexOf2ndSlash >0) {	linkUpFirstComponents(conf, homeDir, fsTarget, "home dir");	} else {	URI linkTarget = fsTarget.makeQualified(new Path(homeDir)).toUri();	ConfigUtil.addLink(conf, homeDir, linkTarget);	
added link for home dir 

String homeDir = fsTarget.getHomeDirectory().toUri().getPath();	int indexOf2ndSlash = homeDir.indexOf('/', 1);	if (indexOf2ndSlash >0) {	linkUpFirstComponents(conf, homeDir, fsTarget, "home dir");	} else {	URI linkTarget = fsTarget.makeQualified(new Path(homeDir)).toUri();	ConfigUtil.addLink(conf, homeDir, linkTarget);	}	String homeDirRoot = fsTarget.getHomeDirectory().getParent().toUri().getPath();	ConfigUtil.setHomeDirConf(conf, homeDirRoot);	
home dir base for viewfs 

static void linkUpFirstComponents(Configuration conf, String path, FileSystem fsTarget, String info) {	int indexOfEnd = path.indexOf('/', 1);	if (Shell.WINDOWS) {	indexOfEnd = path.indexOf('/', indexOfEnd + 1);	}	String firstComponent = path.substring(0, indexOfEnd);	URI linkTarget = fsTarget.makeQualified(new Path(firstComponent)).toUri();	ConfigUtil.addLink(conf, firstComponent, linkTarget);	
added link for 

========================= hadoop sample_3303 =========================

throw new IOException( "not able to find the highest writable parent dir");	}	File badDir = new File(dir, "bad_files");	if (!badDir.mkdirs()) {	if (!badDir.isDirectory()) {	throw new IOException("Mkdirs failed to create " + badDir.toString());	}	}	String suffix = "." + rand.nextInt();	File badFile = new File(badDir, f.getName()+suffix);	
moving bad file to 

if (!badDir.mkdirs()) {	if (!badDir.isDirectory()) {	throw new IOException("Mkdirs failed to create " + badDir.toString());	}	}	String suffix = "." + rand.nextInt();	File badFile = new File(badDir, f.getName()+suffix);	in.close();	boolean b = f.renameTo(badFile);	if (!b) {	
ignoring failure of renameto 

String suffix = "." + rand.nextInt();	File badFile = new File(badDir, f.getName()+suffix);	in.close();	boolean b = f.renameTo(badFile);	if (!b) {	}	File checkFile = ((RawLocalFileSystem)fs).pathToFile(getChecksumFile(p));	sums.close();	b = checkFile.renameTo(new File(badDir, checkFile.getName()+suffix));	if (!b) {	
ignoring failure of renameto 

in.close();	boolean b = f.renameTo(badFile);	if (!b) {	}	File checkFile = ((RawLocalFileSystem)fs).pathToFile(getChecksumFile(p));	sums.close();	b = checkFile.renameTo(new File(badDir, checkFile.getName()+suffix));	if (!b) {	}	} catch (IOException e) {	
error moving bad file 

========================= hadoop sample_4096 =========================

public synchronized List<String> getGroups(String user) {	for(int retry = 0; retry < RECONNECT_RETRY_COUNT; retry++) {	try {	return doGetGroups(user, groupHierarchyLevels);	} catch (NamingException e) {	LOG.warn("Failed to get groups for user " + user + " (retry=" + retry + ") by " + e);	
TRACE 

keystorePass = extractPassword(conf.get(LDAP_KEYSTORE_PASSWORD_FILE_KEY, LDAP_KEYSTORE_PASSWORD_FILE_DEFAULT));	}	bindUser = conf.get(BIND_USER_KEY, BIND_USER_DEFAULT);	bindPassword = getPassword(conf, BIND_PASSWORD_KEY, BIND_PASSWORD_DEFAULT);	if (bindPassword.isEmpty()) {	bindPassword = extractPassword( conf.get(BIND_PASSWORD_FILE_KEY, BIND_PASSWORD_FILE_DEFAULT));	}	String baseDN = conf.getTrimmed(BASE_DN_KEY, BASE_DN_DEFAULT);	userbaseDN = conf.getTrimmed(USER_BASE_DN_KEY, baseDN);	if (LOG.isDebugEnabled()) {	
usersearch basedn 

bindPassword = getPassword(conf, BIND_PASSWORD_KEY, BIND_PASSWORD_DEFAULT);	if (bindPassword.isEmpty()) {	bindPassword = extractPassword( conf.get(BIND_PASSWORD_FILE_KEY, BIND_PASSWORD_FILE_DEFAULT));	}	String baseDN = conf.getTrimmed(BASE_DN_KEY, BASE_DN_DEFAULT);	userbaseDN = conf.getTrimmed(USER_BASE_DN_KEY, baseDN);	if (LOG.isDebugEnabled()) {	}	groupbaseDN = conf.getTrimmed(GROUP_BASE_DN_KEY, baseDN);	if (LOG.isDebugEnabled()) {	
groupsearch basedn 

========================= hadoop sample_3753 =========================

protected <T extends BaseRecord> BufferedReader getReader( Class<T> clazz, String sub) {	String filename = StateStoreUtils.getRecordName(clazz);	if (sub != null && sub.length() > 0) {	filename += "/" + sub;	}	filename += "/" + getDataFileName();	try {	
loading file 

filename += "/" + sub;	}	filename += "/" + getDataFileName();	try {	File file = new File(getRootDir(), filename);	FileInputStream fis = new FileInputStream(file);	InputStreamReader isr = new InputStreamReader(fis, StandardCharsets.UTF_8);	BufferedReader reader = new BufferedReader(isr);	return reader;	} catch (Exception ex) {	
cannot open read stream for record 

filename += "/" + sub;	}	filename += "/" + getDataFileName();	try {	File file = new File(getRootDir(), filename);	FileOutputStream fos = new FileOutputStream(file, false);	OutputStreamWriter osw = new OutputStreamWriter(fos, StandardCharsets.UTF_8);	BufferedWriter writer = new BufferedWriter(osw);	return writer;	} catch (IOException ex) {	
cannot open read stream for record 

========================= hadoop sample_8236 =========================

protected final synchronized void addService(String name, AuxiliaryService service) {	
adding auxiliary service name 

for (final String sName : auxNames) {	try {	Preconditions .checkArgument( validateAuxServiceName(sName), "The ServiceName: " + sName + " set in " + YarnConfiguration.NM_AUX_SERVICES +" is invalid." + "The valid service name should only contain a-zA-Z0-9_ " + "and can not start with numbers");	String classKey = String.format( YarnConfiguration.NM_AUX_SERVICE_FMT, sName);	String className = conf.get(classKey);	final String appClassPath = conf.get(String.format( YarnConfiguration.NM_AUX_SERVICES_CLASSPATH, sName));	AuxiliaryService s = null;	boolean useCustomerClassLoader = appClassPath != null && !appClassPath.isEmpty() && className != null && !className.isEmpty();	if (useCustomerClassLoader) {	s = AuxiliaryServiceWithCustomClassLoader.getInstance( conf, className, appClassPath);	
the aux service are using the custom classloader 

Class<? extends AuxiliaryService> sClass = conf.getClass( classKey, null, AuxiliaryService.class);	if (sClass == null) {	throw new RuntimeException("No class defined for " + sName);	}	s = ReflectionUtils.newInstance(sClass, conf);	}	if (s == null) {	throw new RuntimeException("No object created for " + sName);	}	if(!sName.equals(s.getName())) {	
the auxiliary service named in the configuration is for which has a name of because these are not the same tools trying to send servicedata and read service meta data may have issues unless the refer to the name in the config 

}	s.setAuxiliaryLocalPathHandler(auxiliaryLocalPathHandler);	addService(sName, s);	if (recoveryEnabled) {	Path storePath = new Path(stateStoreRoot, sName);	stateStoreFs.mkdirs(storePath, storeDirPerms);	s.setRecoveryPath(storePath);	}	s.init(conf);	} catch (RuntimeException e) {	
failed to initialize 

public void stateChanged(Service service) {	
service changed state 

public void handle(AuxServicesEvent event) {	
got event for appid 

public void handle(AuxServicesEvent event) {	switch (event.getType()) {	
got application init for service 

========================= hadoop sample_1914 =========================

public void activeQueue() throws YarnException {	try {	this.writeLock.lock();	if (getState() == QueueState.RUNNING) {	
the specified queue is already in the running state 

========================= hadoop sample_936 =========================

if (columns.length > 4) {	String partition = columns[4];	if (partition.startsWith("sd") || partition.startsWith("hd") || partition.startsWith("vd") || partition.startsWith("xvd")) {	String schedulerPath = "/sys/block/" + partition + "/queue/scheduler";	File schedulerFile = new File(schedulerPath);	if (schedulerFile.exists()) {	try {	byte[] contents = Files.readAllBytes(Paths.get(schedulerPath));	String schedulerString = new String(contents, "UTF-8").trim();	if (!schedulerString.contains("[cfq]")) {	
device does not use the cfq scheduler disk isolation using cgroups will not work on this partition 

if (partition.startsWith("sd") || partition.startsWith("hd") || partition.startsWith("vd") || partition.startsWith("xvd")) {	String schedulerPath = "/sys/block/" + partition + "/queue/scheduler";	File schedulerFile = new File(schedulerPath);	if (schedulerFile.exists()) {	try {	byte[] contents = Files.readAllBytes(Paths.get(schedulerPath));	String schedulerString = new String(contents, "UTF-8").trim();	if (!schedulerString.contains("[cfq]")) {	}	} catch (IOException ie) {	
unable to determine disk scheduler type for partition 

public List<PrivilegedOperation> preStart(Container container) throws ResourceHandlerException {	String cgroupId = container.getContainerId().toString();	cGroupsHandler .createCGroup(CGroupsHandler.CGroupController.BLKIO, cgroupId);	try {	cGroupsHandler.updateCGroupParam(CGroupsHandler.CGroupController.BLKIO, cgroupId, CGroupsHandler.CGROUP_PARAM_BLKIO_WEIGHT, DEFAULT_WEIGHT);	} catch (ResourceHandlerException re) {	cGroupsHandler.deleteCGroup(CGroupsHandler.CGroupController.BLKIO, cgroupId);	
could not update cgroup for container 

========================= hadoop sample_1851 =========================

public Map<ApplicationId, ApplicationReport> getApplications(long appsNum, long appStartedTimeBegin, long appStartedTimeEnd) throws YarnException, IOException {	TimelineEntities entities = timelineDataManager.getEntities( ApplicationMetricsConstants.ENTITY_TYPE, null, null, appStartedTimeBegin, appStartedTimeEnd, null, null, appsNum == Long.MAX_VALUE ? this.maxLoadedApplications : appsNum, EnumSet.allOf(Field.class), UserGroupInformation.getLoginUser());	Map<ApplicationId, ApplicationReport> apps = new LinkedHashMap<ApplicationId, ApplicationReport>();	if (entities != null && entities.getEntities() != null) {	for (TimelineEntity entity : entities.getEntities()) {	try {	ApplicationReportExt app = generateApplicationReport(entity, ApplicationReportField.ALL);	apps.put(app.appReport.getApplicationId(), app.appReport);	} catch (Exception e) {	
error on generating application report for 

checkAccess(app);	if (app.appReport.getCurrentApplicationAttemptId() != null) {	ApplicationAttemptReport appAttempt = getApplicationAttempt( app.appReport.getCurrentApplicationAttemptId(), false);	app.appReport.setHost(appAttempt.getHost());	app.appReport.setRpcPort(appAttempt.getRpcPort());	app.appReport.setTrackingUrl(appAttempt.getTrackingUrl());	app.appReport.setOriginalTrackingUrl(appAttempt.getOriginalTrackingUrl());	}	} catch (AuthorizationException | ApplicationAttemptNotFoundException e) {	if (e instanceof AuthorizationException) {	
failed to authorize when generating application report for use a placeholder for its latest attempt id 

if (app.appReport.getCurrentApplicationAttemptId() != null) {	ApplicationAttemptReport appAttempt = getApplicationAttempt( app.appReport.getCurrentApplicationAttemptId(), false);	app.appReport.setHost(appAttempt.getHost());	app.appReport.setRpcPort(appAttempt.getRpcPort());	app.appReport.setTrackingUrl(appAttempt.getTrackingUrl());	app.appReport.setOriginalTrackingUrl(appAttempt.getOriginalTrackingUrl());	}	} catch (AuthorizationException | ApplicationAttemptNotFoundException e) {	if (e instanceof AuthorizationException) {	} else {	
no application attempt found for use a placeholder for its latest attempt id 

========================= hadoop sample_2050 =========================

DataNodeTestUtils.injectDataDirFailure(dn1Vol1, dn2Vol1);	Path file2 = new Path("/test2");	DFSTestUtil.createFile(fs, file2, 1024, (short)3, 1L);	DFSTestUtil.waitReplication(fs, file2, (short)3);	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	int underReplicatedBlocks = BlockManagerTestUtil .checkHeartbeatAndGetUnderReplicatedBlocksCount( cluster.getNamesystem(), bm);	if (underReplicatedBlocks > 0) {	return true;	}	
there is no under replicated block after volume failure 

private void startNewDataNodeWithDiskFailure(File badDataDir, boolean tolerated) throws Exception {	final File data5 = new File(dataDir, "data5");	final String newDirs = badDataDir.toString() + "," + data5.toString();	final Configuration newConf = new Configuration(conf);	newConf.set(DFSConfigKeys.DFS_DATANODE_DATA_DIR_KEY, newDirs);	
setting dfs datanode data dir for new datanode as 

========================= hadoop sample_7252 =========================

final int maxOldFileLen = 2*BLOCK_SIZE+1;	final int maxFlushedBytes = BLOCK_SIZE;	byte[] contents = AppendTestUtil.initBuffer( maxOldFileLen+2*maxFlushedBytes);	for (int oldFileLen =0; oldFileLen <=maxOldFileLen; oldFileLen++) {	for (int flushedBytes1=0; flushedBytes1<=maxFlushedBytes;	flushedBytes1++) {	for (int flushedBytes2=0; flushedBytes2 <=maxFlushedBytes;	flushedBytes2++) {	final int fileLen = oldFileLen + flushedBytes1 + flushedBytes2;	final Path p = new Path("foo"+ oldFileLen +"_"+ flushedBytes1 +"_"+ flushedBytes2);	
creating file 

========================= hadoop sample_7602 =========================

String line;	while ((line = reader.readLine()) != null) {	String[] nodes = line.split("[ \t\n\f\r]+");	if (nodes != null) {	for (int i = 0; i < nodes.length; i++) {	nodes[i] = nodes[i].trim();	if (nodes[i].startsWith("#")) {	break;	}	if (!nodes[i].isEmpty()) {	
adding a node nodes i to the list of hosts from 

public void refresh(String includesFile, String excludesFile) throws IOException {	
refreshing hosts include exclude list 

public void refresh(InputStream inFileInputStream, InputStream exFileInputStream) throws IOException {	
refreshing hosts include exclude list 

public void setIncludesFile(String includesFile) {	
setting the includes file to 

public void setExcludesFile(String excludesFile) {	
setting the excludes file to 

public void updateFileNames(String includesFile, String excludesFile) {	
setting the includes file to 

public void updateFileNames(String includesFile, String excludesFile) {	
setting the excludes file to 

========================= hadoop sample_3641 =========================

private ClientThrottlingIntercept() {	readThrottler = new ClientThrottlingAnalyzer("read");	writeThrottler = new ClientThrottlingAnalyzer("write");	
client side throttling is enabled for the wasb file system 

========================= hadoop sample_6443 =========================

if (line.hasOption("archives")) {	conf.set("tmparchives", validateFiles(line.getOptionValue("archives")), "from -archives command line option");	}	conf.setBoolean("mapreduce.client.genericoptionsparser.used", true);	if(line.hasOption("tokenCacheFile")) {	String fileName = line.getOptionValue("tokenCacheFile");	FileSystem localFs = FileSystem.getLocal(conf);	Path p = localFs.makeQualified(new Path(fileName));	localFs.getFileStatus(p);	if(LOG.isDebugEnabled()) {	
setting conf tokensfile 

if (jars == null || jars.trim().isEmpty()) {	return null;	}	String[] files = jars.split(",");	List<URL> cp = new ArrayList<URL>();	for (String file : files) {	Path tmp = new Path(file);	if (tmp.getFileSystem(conf).equals(FileSystem.getLocal(conf))) {	cp.add(FileSystem.getLocal(conf).pathToFile(tmp).toURI().toURL());	} else {	
the libjars file is not on the local filesystem it will not be added to the local classpath 

private void expandWildcard(List<String> finalPaths, Path path, FileSystem fs) throws IOException {	FileStatus status = fs.getFileStatus(path);	if (!status.isDirectory()) {	throw new FileNotFoundException(path + " is not a directory.");	}	List<Path> jars = FileUtil.getJarsInDirectory(path.toString(), fs.equals(FileSystem.getLocal(conf)));	if (jars.isEmpty()) {	
does not have jars in it it will be ignored 

private boolean parseGeneralOptions(Options opts, String[] args) throws IOException {	opts = buildGeneralOptions(opts);	CommandLineParser parser = new GnuParser();	boolean parsed = false;	try {	commandLine = parser.parse(opts, preProcessForWindows(args), true);	processGeneralOptions(commandLine);	parsed = true;	} catch(ParseException e) {	
options parsing failed 

========================= hadoop sample_3704 =========================

public void testNegativeSeek() throws Throwable {	instream = fs.open(smallSeekFile);	assertEquals(0, instream.getPos());	try {	instream.seek(-1);	long p = instream.getPos();	
seek to returned a position of 

========================= hadoop sample_6156 =========================

protected void serviceInit(Configuration conf) throws Exception {	this.monitoringInterval = conf.getLong(YarnConfiguration.NM_RESOURCE_MON_INTERVAL_MS, YarnConfiguration.DEFAULT_NM_RESOURCE_MON_INTERVAL_MS);	this.resourceCalculatorPlugin = ResourceCalculatorPlugin.getNodeResourceMonitorPlugin(conf);	
using resourcecalculatorplugin 

private boolean isEnabled() {	if (this.monitoringInterval <= 0) {	LOG.info("Node Resource monitoring interval is <=0. " + this.getClass().getName() + " is disabled.");	return false;	}	if (resourceCalculatorPlugin == null) {	
resourcecalculatorplugin is unavailable on this system is disabled 

protected void serviceStop() throws Exception {	if (this.isEnabled()) {	this.monitoringThread.interrupt();	try {	this.monitoringThread.join(10 * 1000);	} catch (InterruptedException e) {	
could not wait for the thread to join 

public void run() {	while (true) {	long pmem = resourceCalculatorPlugin.getPhysicalMemorySize() - resourceCalculatorPlugin.getAvailablePhysicalMemorySize();	long vmem = resourceCalculatorPlugin.getVirtualMemorySize() - resourceCalculatorPlugin.getAvailableVirtualMemorySize();	float vcores = resourceCalculatorPlugin.getNumVCoresUsed();	nodeUtilization = ResourceUtilization.newInstance( (int) (pmem >> 20), (int) (vmem >> 20), vcores);	try {	Thread.sleep(monitoringInterval);	} catch (InterruptedException e) {	
is interrupted exiting 

========================= hadoop sample_1693 =========================

buffer.writeOp(op2);	FSEditLogOp.AllocateBlockIdOp op3 = FSEditLogOp.AllocateBlockIdOp.getInstance(cache.get()) .setBlockId(0);	op3.setTransactionId(3);	buffer.writeOp(op3);	GenericTestUtils.LogCapturer logs = GenericTestUtils.LogCapturer.captureLogs(EditsDoubleBuffer.LOG);	try {	buffer.close();	fail();	} catch (IOException ioe) {	GenericTestUtils.assertExceptionContains( "bytes still to be flushed and cannot be closed.", ioe);	
exception expected 

========================= hadoop sample_7317 =========================

public static <T> T createAHSProxy(final Configuration conf, final Class<T> protocol, InetSocketAddress ahsAddress) throws IOException {	
connecting to application history server at 

========================= hadoop sample_2545 =========================

assertSame(renewer, LeaseRenewer.getInstance( FAKE_AUTHORITY, FAKE_UGI_A, mockClient2));	DFSOutputStream mockStream2 = Mockito.mock(DFSOutputStream.class);	renewer.put(fileId, mockStream2, mockClient2);	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	try {	Mockito.verify(mockClient1, Mockito.atLeastOnce()).renewLease();	Mockito.verify(mockClient2, Mockito.atLeastOnce()).renewLease();	return true;	} catch (AssertionError err) {	
not yet satisfied 

========================= hadoop sample_6785 =========================

public RMNMInfo(RMContext rmc, ResourceScheduler sched) {	this.rmContext = rmc;	this.scheduler = sched;	StandardMBean bean;	try {	bean = new StandardMBean(this,RMNMInfoBeans.class);	MBeans.register("ResourceManager", "RMNMInfo", bean);	} catch (NotCompliantMBeanException e) {	
error registering rmnminfo mbean 

public RMNMInfo(RMContext rmc, ResourceScheduler sched) {	this.rmContext = rmc;	this.scheduler = sched;	StandardMBean bean;	try {	bean = new StandardMBean(this,RMNMInfoBeans.class);	MBeans.register("ResourceManager", "RMNMInfo", bean);	} catch (NotCompliantMBeanException e) {	}	
registered rmnminfo mbean 

========================= hadoop sample_1040 =========================

public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) {	releaseDfsResources();	DefaultHttpResponse resp = ExceptionHandler.exceptionCaught(cause);	resp.headers().set(CONNECTION, CLOSE);	ctx.writeAndFlush(resp).addListener(ChannelFutureListener.CLOSE);	if (LOG != null && LOG.isDebugEnabled()) {	
exception in channel handler 

========================= hadoop sample_7897 =========================

final AtomicBoolean running = new AtomicBoolean(true);	final AtomicBoolean failed = new AtomicBoolean(false);	Thread t = new Thread() {	public void run() {	while (running.get()) {	try {	out.write("test".getBytes());	out.hflush();	Thread.sleep(1000);	} catch (IOException | InterruptedException e) {	
exception during write 

DataNodeTestUtils.triggerBlockReport(d);	}	List<DatanodeInfo> pipelineList = Arrays.asList(pipeline);	DatanodeInfo newNode = null;	for (DatanodeInfo node : newNodes) {	if (!pipelineList.contains(node)) {	newNode = node;	break;	}	}	
number of nodes in pipeline newnode 

for (DatanodeInfo node : newNodes) {	if (!pipelineList.contains(node)) {	newNode = node;	break;	}	}	for (int i = 0; i < newNodes.length; i++) {	if (newNodes[i].getName().equals(newNode.getName())) {	continue;	}	
shutdown 

========================= hadoop sample_7194 =========================

protected void doSecureLogin() throws IOException {	}	};	new Thread() {	public void run() {	resourceManager.start();	};	}.start();	int waitCount = 0;	while (resourceManager.getServiceState() == STATE.INITED && waitCount++ < 60) {	
waiting for rm to start 

========================= hadoop sample_598 =========================

addService(containersLauncher);	this.nodeStatusUpdater = nodeStatusUpdater;	this.containerScheduler = createContainerScheduler(context);	addService(containerScheduler);	AuxiliaryLocalPathHandler auxiliaryLocalPathHandler = new AuxiliaryLocalPathHandlerImpl(dirsHandler);	auxiliaryServices = new AuxServices(auxiliaryLocalPathHandler);	auxiliaryServices.registerServiceListener(this);	addService(auxiliaryServices);	Configuration conf = context.getConf();	if (YarnConfiguration.timelineServiceV2Enabled(conf) && YarnConfiguration.systemMetricsPublisherEnabled(conf)) {	
yarn system metrics publishing service is enabled 

protected void createAMRMProxyService(Configuration conf) {	this.amrmProxyEnabled = conf.getBoolean(YarnConfiguration.AMRM_PROXY_ENABLED, YarnConfiguration.DEFAULT_AMRM_PROXY_ENABLED) || conf.getBoolean(YarnConfiguration.DIST_SCHEDULING_ENABLED, YarnConfiguration.DEFAULT_DIST_SCHEDULING_ENABLED);	if (amrmProxyEnabled) {	
amrmproxyservice is enabled all the am rm requests will be intercepted by the proxy 

protected void createAMRMProxyService(Configuration conf) {	this.amrmProxyEnabled = conf.getBoolean(YarnConfiguration.AMRM_PROXY_ENABLED, YarnConfiguration.DEFAULT_AMRM_PROXY_ENABLED) || conf.getBoolean(YarnConfiguration.DIST_SCHEDULING_ENABLED, YarnConfiguration.DEFAULT_DIST_SCHEDULING_ENABLED);	if (amrmProxyEnabled) {	this.setAMRMProxyService( new AMRMProxyService(this.context, this.dispatcher));	addService(this.getAMRMProxyService());	} else {	
amrmproxyservice is disabled 

private void recover() throws IOException, URISyntaxException {	NMStateStoreService stateStore = context.getNMStateStore();	if (stateStore.canRecover()) {	rsrcLocalizationSrvc.recoverLocalizedResources( stateStore.loadLocalizationState());	if (this.amrmProxyEnabled) {	this.getAMRMProxyService().recover();	}	RecoveredApplicationsState appsState = stateStore.loadApplicationsState();	for (ContainerManagerApplicationProto proto : appsState.getApplications()) {	if (LOG.isDebugEnabled()) {	
recovering application with state 

this.getAMRMProxyService().recover();	}	RecoveredApplicationsState appsState = stateStore.loadApplicationsState();	for (ContainerManagerApplicationProto proto : appsState.getApplications()) {	if (LOG.isDebugEnabled()) {	}	recoverApplication(proto);	}	for (RecoveredContainerState rcs : stateStore.loadContainersState()) {	if (LOG.isDebugEnabled()) {	
recovering container with state 

}	recoverApplication(proto);	}	for (RecoveredContainerState rcs : stateStore.loadContainersState()) {	if (LOG.isDebugEnabled()) {	}	recoverContainer(rcs);	}	dispatcher.getEventHandler().handle( new ContainerSchedulerEvent(null, ContainerSchedulerEventType.RECOVERY_COMPLETED));	} else {	
not a recoverable state store nothing to recover 

}	LogAggregationContext logAggregationContext = null;	if (p.getLogAggregationContext() != null) {	logAggregationContext = new LogAggregationContextPBImpl(p.getLogAggregationContext());	}	FlowContext fc = null;	if (p.getFlowContext() != null) {	FlowContextProto fcp = p.getFlowContext();	fc = new FlowContext(fcp.getFlowName(), fcp.getFlowVersion(), fcp.getFlowRunId());	if (LOG.isDebugEnabled()) {	
recovering flow context for an application 

}	FlowContext fc = null;	if (p.getFlowContext() != null) {	FlowContextProto fcp = p.getFlowContext();	fc = new FlowContext(fcp.getFlowName(), fcp.getFlowVersion(), fcp.getFlowRunId());	if (LOG.isDebugEnabled()) {	}	} else {	fc = new FlowContext(TimelineUtils.generateDefaultFlowName(null, appId), YarnConfiguration.DEFAULT_FLOW_VERSION, appId.getClusterTimestamp());	if (LOG.isDebugEnabled()) {	
no prior existing flow context found using default flow context for an application 

if (p.getFlowContext() != null) {	FlowContextProto fcp = p.getFlowContext();	fc = new FlowContext(fcp.getFlowName(), fcp.getFlowVersion(), fcp.getFlowRunId());	if (LOG.isDebugEnabled()) {	}	} else {	fc = new FlowContext(TimelineUtils.generateDefaultFlowName(null, appId), YarnConfiguration.DEFAULT_FLOW_VERSION, appId.getClusterTimestamp());	if (LOG.isDebugEnabled()) {	}	}	
recovering application 

ContainerLaunchContext launchContext = req.getContainerLaunchContext();	ContainerTokenIdentifier token = null;	if(rcs.getCapability() != null) {	ContainerTokenIdentifier originalToken = BuilderUtils.newContainerTokenIdentifier(req.getContainerToken());	token = new ContainerTokenIdentifier(originalToken.getContainerID(), originalToken.getVersion(), originalToken.getNmHostAddress(), originalToken.getApplicationSubmitter(), rcs.getCapability(), originalToken.getExpiryTimeStamp(), originalToken.getMasterKeyId(), originalToken.getRMIdentifier(), originalToken.getPriority(), originalToken.getCreationTime(), originalToken.getLogAggregationContext(), originalToken.getNodeLabelExpression(), originalToken.getContainerType(), originalToken.getExecutionType());	} else {	token = BuilderUtils.newContainerTokenIdentifier(req.getContainerToken());	}	ContainerId containerId = token.getContainerID();	ApplicationId appId = containerId.getApplicationAttemptId().getApplicationId();	
recovering in state with exit code 

ContainerId containerId = token.getContainerID();	ApplicationId appId = containerId.getApplicationAttemptId().getApplicationId();	Application app = context.getApplications().get(appId);	if (app != null) {	recoverActiveContainer(launchContext, token, rcs);	if (rcs.getRecoveryType() == RecoveredContainerType.KILL) {	dispatcher.getEventHandler().handle( new ContainerKillEvent(containerId, ContainerExitStatus.ABORTED, "Due to invalid StateStore info container was killed" + " during recovery"));	}	} else {	if (rcs.getStatus() != RecoveredContainerStatus.COMPLETED) {	
has no corresponding application 

ApplicationId appId = containerId.getApplicationAttemptId().getApplicationId();	Application app = context.getApplications().get(appId);	if (app != null) {	recoverActiveContainer(launchContext, token, rcs);	if (rcs.getRecoveryType() == RecoveredContainerType.KILL) {	dispatcher.getEventHandler().handle( new ContainerKillEvent(containerId, ContainerExitStatus.ABORTED, "Due to invalid StateStore info container was killed" + " during recovery"));	}	} else {	if (rcs.getStatus() != RecoveredContainerStatus.COMPLETED) {	}	
adding to recently stopped containers 

while (--waitIterations >= 0) {	newContainers.clear();	for (Container container : context.getContainers().values()) {	if (container.getContainerState() == org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerState.NEW) {	newContainers.add(container.getContainerId());	}	}	if (newContainers.isEmpty()) {	break;	}	
waiting for containers 

if (container.getContainerState() == org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerState.NEW) {	newContainers.add(container.getContainerId());	}	}	if (newContainers.isEmpty()) {	break;	}	Thread.sleep(sleepMsec);	}	if (waitIterations < 0) {	
timeout waiting for recovered containers 

super.serviceStart();	if (delayedRpcServerStart) {	waitForRecoveredContainers();	server.start();	connectAddress = NetUtils.getConnectAddress(server);	NodeId serverNode = buildNodeId(connectAddress, hostOverride);	if (!serverNode.equals(nodeId)) {	throw new IOException("Node mismatch after server started, expected '" + nodeId + "' but found '" + serverNode + "'");	}	}	
containermanager started at 

super.serviceStart();	if (delayedRpcServerStart) {	waitForRecoveredContainers();	server.start();	connectAddress = NetUtils.getConnectAddress(server);	NodeId serverNode = buildNodeId(connectAddress, hostOverride);	if (!serverNode.equals(nodeId)) {	throw new IOException("Node mismatch after server started, expected '" + nodeId + "' but found '" + serverNode + "'");	}	}	
containermanager bound to 

public void cleanUpApplicationsOnNMShutDown() {	Map<ApplicationId, Application> applications = this.context.getApplications();	if (applications.isEmpty()) {	return;	}	
applications still running 

if (applications.isEmpty()) {	return;	}	if (this.context.getNMStateStore().canRecover() && !this.context.getDecommissioned()) {	if (getConfig().getBoolean(YarnConfiguration.NM_RECOVERY_SUPERVISED, YarnConfiguration.DEFAULT_NM_RECOVERY_SUPERVISED)) {	return;	}	}	List<ApplicationId> appIds = new ArrayList<ApplicationId>(applications.keySet());	this.handle(new CMgrCompletedAppsEvent(appIds, CMgrCompletedAppsEvent.Reason.ON_SHUTDOWN));	
waiting for applications to be finished 

return;	}	}	List<ApplicationId> appIds = new ArrayList<ApplicationId>(applications.keySet());	this.handle(new CMgrCompletedAppsEvent(appIds, CMgrCompletedAppsEvent.Reason.ON_SHUTDOWN));	long waitStartTime = System.currentTimeMillis();	while (!applications.isEmpty() && System.currentTimeMillis() - waitStartTime < waitForContainersOnShutdownMillis) {	try {	Thread.sleep(1000);	} catch (InterruptedException ex) {	
interrupted while sleeping on applications finish on shutdown 

List<ApplicationId> appIds = new ArrayList<ApplicationId>(applications.keySet());	this.handle(new CMgrCompletedAppsEvent(appIds, CMgrCompletedAppsEvent.Reason.ON_SHUTDOWN));	long waitStartTime = System.currentTimeMillis();	while (!applications.isEmpty() && System.currentTimeMillis() - waitStartTime < waitForContainersOnShutdownMillis) {	try {	Thread.sleep(1000);	} catch (InterruptedException ex) {	}	}	if (applications.isEmpty()) {	
all applications in finished state 

this.handle(new CMgrCompletedAppsEvent(appIds, CMgrCompletedAppsEvent.Reason.ON_SHUTDOWN));	long waitStartTime = System.currentTimeMillis();	while (!applications.isEmpty() && System.currentTimeMillis() - waitStartTime < waitForContainersOnShutdownMillis) {	try {	Thread.sleep(1000);	} catch (InterruptedException ex) {	}	}	if (applications.isEmpty()) {	} else {	
done waiting for applications to be finished still alive 

public void cleanupContainersOnNMResync() {	Map<ContainerId, Container> containers = context.getContainers();	if (containers.isEmpty()) {	return;	}	
containers still running on 

public void cleanupContainersOnNMResync() {	Map<ContainerId, Container> containers = context.getContainers();	if (containers.isEmpty()) {	return;	}	List<ContainerId> containerIds = new ArrayList<ContainerId>(containers.keySet());	
waiting for containers to be killed 

this.handle(new CMgrCompletedContainersEvent(containerIds, CMgrCompletedContainersEvent.Reason.ON_NODEMANAGER_RESYNC));	boolean allContainersCompleted = false;	while (!containers.isEmpty() && !allContainersCompleted) {	allContainersCompleted = true;	for (Entry<ContainerId, Container> container : containers.entrySet()) {	if (((ContainerImpl) container.getValue()).getCurrentState() != ContainerState.COMPLETE) {	allContainersCompleted = false;	try {	Thread.sleep(1000);	} catch (InterruptedException ex) {	
interrupted while sleeping on container kill on resync 

allContainersCompleted = false;	try {	Thread.sleep(1000);	} catch (InterruptedException ex) {	}	break;	}	}	}	if (allContainersCompleted) {	
all containers in done state 

try {	Thread.sleep(1000);	} catch (InterruptedException ex) {	}	break;	}	}	}	if (allContainersCompleted) {	} else {	
done waiting for containers to be killed still alive 

if (logAggregationContext != null) {	builder.setLogAggregationContext(( (LogAggregationContextPBImpl)logAggregationContext).getProto());	}	builder.clearCredentials();	if (credentials != null) {	DataOutputBuffer dob = new DataOutputBuffer();	try {	credentials.writeTokenStorageToStream(dob);	builder.setCredentials(ByteString.copyFrom(dob.getData()));	} catch (IOException e) {	
cannot serialize credentials 

protected void startContainerInternal( ContainerTokenIdentifier containerTokenIdentifier, StartContainerRequest request) throws YarnException, IOException {	ContainerId containerId = containerTokenIdentifier.getContainerID();	String containerIdStr = containerId.toString();	String user = containerTokenIdentifier.getApplicationSubmitter();	
start request for by user 

if (YarnConfiguration.timelineServiceV2Enabled(getConfig())) {	String flowName = launchContext.getEnvironment() .get(TimelineUtils.FLOW_NAME_TAG_PREFIX);	String flowVersion = launchContext.getEnvironment() .get(TimelineUtils.FLOW_VERSION_TAG_PREFIX);	String flowRunIdStr = launchContext.getEnvironment() .get(TimelineUtils.FLOW_RUN_ID_TAG_PREFIX);	long flowRunId = 0L;	if (flowRunIdStr != null && !flowRunIdStr.isEmpty()) {	flowRunId = Long.parseLong(flowRunIdStr);	}	flowContext = new FlowContext(flowName, flowVersion, flowRunId);	if (LOG.isDebugEnabled()) {	
flow context created for an application 

long flowRunId = 0L;	if (flowRunIdStr != null && !flowRunIdStr.isEmpty()) {	flowRunId = Long.parseLong(flowRunIdStr);	}	flowContext = new FlowContext(flowName, flowVersion, flowRunId);	if (LOG.isDebugEnabled()) {	}	}	Application application = new ApplicationImpl(dispatcher, user, flowContext, applicationID, credentials, context);	if (context.getApplications().putIfAbsent(applicationID, application) == null) {	
creating a new application reference for app 

protected void stopContainerInternal(ContainerId containerID) throws YarnException, IOException {	String containerIDStr = containerID.toString();	Container container = this.context.getContainers().get(containerID);	
stopping container with container id 

protected ContainerStatus getContainerStatusInternal(ContainerId containerID, NMTokenIdentifier nmTokenIdentifier) throws YarnException {	String containerIDStr = containerID.toString();	Container container = this.context.getContainers().get(containerID);	
getting container status for 

Container container = this.context.getContainers().get(containerID);	authorizeGetAndStopContainerRequest(containerID, container, false, nmTokenIdentifier);	if (container == null) {	if (nodeStatusUpdater.isContainerRecentlyStopped(containerID)) {	throw RPCUtil.getRemoteException("Container " + containerIDStr + " was recently stopped on node manager.");	} else {	throw RPCUtil.getRemoteException("Container " + containerIDStr + " is not handled by this NodeManager");	}	}	ContainerStatus containerStatus = container.cloneAndGetContainerStatus();	
returning 

public void handle(ContainerEvent event) {	Map<ContainerId,Container> containers = ContainerManagerImpl.this.context.getContainers();	Container c = containers.get(event.getContainerID());	if (c != null) {	c.handle(event);	if (nmMetricsPublisher != null) {	nmMetricsPublisher.publishContainerEvent(event);	}	} else {	
event sent to absent container 

public void handle(ApplicationEvent event) {	Application app = ContainerManagerImpl.this.context.getApplications().get( event.getApplicationID());	if (app != null) {	app.handle(event);	if (nmMetricsPublisher != null) {	nmMetricsPublisher.publishApplicationEvent(event);	}	} else {	
event sent to absent application 

public void handle(ContainerManagerEvent event) {	switch (event.getType()) {	case FINISH_APPS: CMgrCompletedAppsEvent appsFinishedEvent = (CMgrCompletedAppsEvent) event;	for (ApplicationId appID : appsFinishedEvent.getAppsToCleanup()) {	Application app = this.context.getApplications().get(appID);	if (app == null) {	
couldn t find application while processing finish apps event the resourcemanager allocated resources for this application to the nodemanager but no active containers were found to process 

switch (event.getType()) {	case FINISH_APPS: CMgrCompletedAppsEvent appsFinishedEvent = (CMgrCompletedAppsEvent) event;	for (ApplicationId appID : appsFinishedEvent.getAppsToCleanup()) {	Application app = this.context.getApplications().get(appID);	if (app == null) {	continue;	}	boolean shouldDropEvent = false;	for (Container container : app.getContainers().values()) {	if (container.isRecovering()) {	
drop finish apps event to because container is recovering 

diagnostic = "Application killed by ResourceManager";	}	this.dispatcher.getEventHandler().handle( new ApplicationFinishEvent(appID, diagnostic));	}	break;	case FINISH_CONTAINERS: CMgrCompletedContainersEvent containersFinishedEvent = (CMgrCompletedContainersEvent) event;	for (ContainerId containerId : containersFinishedEvent .getContainersToCleanup()) {	ApplicationId appId = containerId.getApplicationAttemptId().getApplicationId();	Application app = this.context.getApplications().get(appId);	if (app == null) {	
couldn t find app while processing finish containers event 

break;	case FINISH_CONTAINERS: CMgrCompletedContainersEvent containersFinishedEvent = (CMgrCompletedContainersEvent) event;	for (ContainerId containerId : containersFinishedEvent .getContainersToCleanup()) {	ApplicationId appId = containerId.getApplicationAttemptId().getApplicationId();	Application app = this.context.getApplications().get(appId);	if (app == null) {	continue;	}	Container container = app.getContainers().get(containerId);	if (container == null) {	
couldn t find container while processing finish containers event 

ApplicationId appId = containerId.getApplicationAttemptId().getApplicationId();	Application app = this.context.getApplications().get(appId);	if (app == null) {	continue;	}	Container container = app.getContainers().get(containerId);	if (container == null) {	continue;	}	if (container.isRecovering()) {	
drop finish containers event to because container is recovering 

}	this.dispatcher.getEventHandler().handle( new ContainerKillEvent(containerId, ContainerExitStatus.KILLED_BY_RESOURCEMANAGER, "Container Killed by ResourceManager"));	}	break;	case UPDATE_CONTAINERS: CMgrUpdateContainersEvent containersDecreasedEvent = (CMgrUpdateContainersEvent) event;	for (org.apache.hadoop.yarn.api.records.Container container : containersDecreasedEvent.getContainersToUpdate()) {	try {	ContainerTokenIdentifier containerTokenIdentifier = BuilderUtils.newContainerTokenIdentifier( container.getContainerToken());	updateContainerInternal(container.getId(), containerTokenIdentifier);	} catch (YarnException e) {	
unable to decrease container resource 

this.dispatcher.getEventHandler().handle( new ContainerKillEvent(containerId, ContainerExitStatus.KILLED_BY_RESOURCEMANAGER, "Container Killed by ResourceManager"));	}	break;	case UPDATE_CONTAINERS: CMgrUpdateContainersEvent containersDecreasedEvent = (CMgrUpdateContainersEvent) event;	for (org.apache.hadoop.yarn.api.records.Container container : containersDecreasedEvent.getContainersToUpdate()) {	try {	ContainerTokenIdentifier containerTokenIdentifier = BuilderUtils.newContainerTokenIdentifier( container.getContainerToken());	updateContainerInternal(container.getId(), containerTokenIdentifier);	} catch (YarnException e) {	} catch (IOException e) {	
unable to update container resource in store 

public ResourceLocalizationResponse localize( ResourceLocalizationRequest request) throws YarnException, IOException {	ContainerId containerId = request.getContainerId();	Container container = preReInitializeOrLocalizeCheck(containerId, ReInitOp.LOCALIZE);	try {	Map<LocalResourceVisibility, Collection<LocalResourceRequest>> req = container.getResourceSet().addResources(request.getLocalResources());	if (req != null && !req.isEmpty()) {	dispatcher.getEventHandler() .handle(new ContainerLocalizationRequestEvent(container, req));	}	} catch (URISyntaxException e) {	
error when parsing local resource uri for 

public void reInitializeContainer(ContainerId containerId, ContainerLaunchContext reInitLaunchContext, boolean autoCommit) throws YarnException {	Container container = preReInitializeOrLocalizeCheck(containerId, ReInitOp.RE_INIT);	ResourceSet resourceSet = new ResourceSet();	try {	if (reInitLaunchContext != null) {	resourceSet.addResources(reInitLaunchContext.getLocalResources());	}	dispatcher.getEventHandler().handle( new ContainerReInitEvent(containerId, reInitLaunchContext, resourceSet, autoCommit));	container.setIsReInitializing(true);	} catch (URISyntaxException e) {	
error when parsing local resource uri for upgrade of container 

private void internalSignalToContainer(SignalContainerRequest request, String sentBy) {	ContainerId containerId = request.getContainerId();	Container container = this.context.getContainers().get(containerId);	if (container != null) {	
signal request by 

private void internalSignalToContainer(SignalContainerRequest request, String sentBy) {	ContainerId containerId = request.getContainerId();	Container container = this.context.getContainers().get(containerId);	if (container != null) {	this.dispatcher.getEventHandler().handle( new SignalContainersLauncherEvent(container, request.getCommand()));	} else {	
container no longer exists 

========================= hadoop sample_1806 =========================

throw new FailoverFailedException(msg, e);	}	if (!toSvcStatus.getState().equals(HAServiceState.STANDBY)) {	throw new FailoverFailedException( "Can't failover to an active service");	}	if (!toSvcStatus.isReadyToBecomeActive()) {	String notReadyReason = toSvcStatus.getNotReadyReason();	if (!forceActive) {	throw new FailoverFailedException( target + " is not ready to become active: " + notReadyReason);	} else {	
service is not ready to become active but forcing 

if (tryFence) {	if (!fromSvc.getFencer().fence(fromSvc)) {	throw new FailoverFailedException("Unable to fence " + fromSvc + ". Fencing failed.");	}	}	boolean failed = false;	Throwable cause = null;	try {	HAServiceProtocolHelper.transitionToActive( toSvc.getProxy(conf, rpcTimeoutToNewActive), createReqInfo());	} catch (ServiceFailedException sfe) {	
unable to make active failing back 

}	}	boolean failed = false;	Throwable cause = null;	try {	HAServiceProtocolHelper.transitionToActive( toSvc.getProxy(conf, rpcTimeoutToNewActive), createReqInfo());	} catch (ServiceFailedException sfe) {	failed = true;	cause = sfe;	} catch (IOException ioe) {	
unable to make active unable to connect failing back 

========================= hadoop sample_3991 =========================

try {	writeLock.lock();	Map<NodeId, RMContainer> reservedContainers = this.reservedContainers.get( schedulerKey);	RMContainer reservedContainer = reservedContainers.remove( node.getNodeID());	if (reservedContainers.isEmpty()) {	this.reservedContainers.remove(schedulerKey);	}	resetReReservations(schedulerKey);	Resource resource = reservedContainer.getContainer().getResource();	this.attemptResourceUsage.decReserved(resource);	
application unreserved on node currently has at priority currentreservation 

}	node.allocateContainer(allocatedContainer);	if (!isAmRunning() && !getUnmanagedAM()) {	setAMResource(capability);	getQueue().addAMResourceUsage(capability);	setAmRunning(true);	}	return capability;	}	if (LOG.isDebugEnabled()) {	
resource request exceeds the available resources of the node 

getQueue().addAMResourceUsage(capability);	setAmRunning(true);	}	return capability;	}	if (LOG.isDebugEnabled()) {	}	if (isReservable(capability) && !node.isPreemptedForApp(this) && reserve(pendingAsk.getPerAllocationResource(), node, reservedContainer, type, schedulerKey)) {	updateAMDiagnosticMsg(capability, " exceeds the available resources of " + "the node and the request is reserved)");	if (LOG.isDebugEnabled()) {	
s resource request is reserved 

if (LOG.isDebugEnabled()) {	}	if (isReservable(capability) && !node.isPreemptedForApp(this) && reserve(pendingAsk.getPerAllocationResource(), node, reservedContainer, type, schedulerKey)) {	updateAMDiagnosticMsg(capability, " exceeds the available resources of " + "the node and the request is reserved)");	if (LOG.isDebugEnabled()) {	}	return FairScheduler.CONTAINER_RESERVED;	} else {	updateAMDiagnosticMsg(capability, " exceeds the available resources of " + "the node and the request cannot be reserved)");	if (LOG.isDebugEnabled()) {	
couldn t create reservation for app at priority 

private Resource assignContainer(FSSchedulerNode node, boolean reserved) {	if (LOG.isDebugEnabled()) {	
node offered to app reserved 

try {	writeLock.lock();	for (SchedulerRequestKey schedulerKey : keysToTry) {	if (!reserved && !hasContainerForNode(schedulerKey, node)) {	continue;	}	addSchedulingOpportunity(schedulerKey);	PendingAsk rackLocalPendingAsk = getPendingAsk(schedulerKey, node.getRackName());	PendingAsk nodeLocalPendingAsk = getPendingAsk(schedulerKey, node.getNodeName());	if (nodeLocalPendingAsk.getCount() > 0 && !appSchedulingInfo.canDelayTo(schedulerKey, node.getNodeName())) {	
relax locality off is not supported on local request 

if (nodeLocalPendingAsk.getCount() > 0 && !appSchedulingInfo.canDelayTo(schedulerKey, node.getNodeName())) {	}	NodeType allowedLocality;	if (scheduler.isContinuousSchedulingEnabled()) {	allowedLocality = getAllowedLocalityLevelByTime(schedulerKey, scheduler.getNodeLocalityDelayMs(), scheduler.getRackLocalityDelayMs(), scheduler.getClock().getTime());	} else {	allowedLocality = getAllowedLocalityLevel(schedulerKey, scheduler.getNumClusterNodes(), scheduler.getNodeLocalityThreshold(), scheduler.getRackLocalityThreshold());	}	if (rackLocalPendingAsk.getCount() > 0 && nodeLocalPendingAsk.getCount() > 0) {	if (LOG.isTraceEnabled()) {	
assign container on node assigntype node local allowedlocality priority app attempt id 

if (rackLocalPendingAsk.getCount() > 0 && nodeLocalPendingAsk.getCount() > 0) {	if (LOG.isTraceEnabled()) {	}	return assignContainer(node, nodeLocalPendingAsk, NodeType.NODE_LOCAL, reserved, schedulerKey);	}	if (!appSchedulingInfo.canDelayTo(schedulerKey, node.getRackName())) {	continue;	}	if (rackLocalPendingAsk.getCount() > 0 && (allowedLocality.equals(NodeType.RACK_LOCAL) || allowedLocality .equals(NodeType.OFF_SWITCH))) {	if (LOG.isTraceEnabled()) {	
assign container on node assigntype rack local allowedlocality priority app attempt id 

}	return assignContainer(node, rackLocalPendingAsk, NodeType.RACK_LOCAL, reserved, schedulerKey);	}	PendingAsk offswitchAsk = getPendingAsk(schedulerKey, ResourceRequest.ANY);	if (!appSchedulingInfo.canDelayTo(schedulerKey, ResourceRequest.ANY)) {	continue;	}	if (offswitchAsk.getCount() > 0) {	if (getSchedulingPlacementSet(schedulerKey).getUniqueLocationAsks() <= 1 || allowedLocality.equals(NodeType.OFF_SWITCH)) {	if (LOG.isTraceEnabled()) {	
assign container on node assigntype off switch allowedlocality priority app attempt id 

continue;	}	if (offswitchAsk.getCount() > 0) {	if (getSchedulingPlacementSet(schedulerKey).getUniqueLocationAsks() <= 1 || allowedLocality.equals(NodeType.OFF_SWITCH)) {	if (LOG.isTraceEnabled()) {	}	return assignContainer(node, offswitchAsk, NodeType.OFF_SWITCH, reserved, schedulerKey);	}	}	if (LOG.isTraceEnabled()) {	
can t assign container on node allowedlocality priority app attempt id 

public Resource assignContainer(FSSchedulerNode node) {	if (isOverAMShareLimit()) {	PendingAsk amAsk = appSchedulingInfo.getNextPendingAsk();	updateAMDiagnosticMsg(amAsk.getPerAllocationResource(), " exceeds maximum AM resource allowed).");	if (LOG.isDebugEnabled()) {	
am resource request exceeds maximum am resource allowed 

========================= hadoop sample_961 =========================

try {	int sumBufferSize = fs.getSumBufferSize(fs.getBytesPerSum(), bufferSize);	sums = fs.getRawFileSystem().open(sumFile, sumBufferSize);	byte[] version = new byte[CHECKSUM_VERSION.length];	sums.readFully(version);	if (!Arrays.equals(version, CHECKSUM_VERSION)) throw new IOException("Not a checksum file: "+sumFile);	this.bytesPerSum = sums.readInt();	set(fs.verifyChecksum, DataChecksum.newCrc32(), bytesPerSum, 4);	} catch (IOException e) {	if (!(e instanceof FileNotFoundException) || e.getMessage().endsWith(" (Permission denied)")) {	
problem opening checksum file ignoring exception 

========================= hadoop sample_4192 =========================

public void testFailingMapper() throws IOException, InterruptedException, ClassNotFoundException {	
starting uberized testfailingmapper 

public void testFailingMapper() throws IOException, InterruptedException, ClassNotFoundException {	if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {	
mrappjar not found not running test 

========================= hadoop sample_5629 =========================

public void testDiskCheckTimeout() throws Exception {	
executing 

public void testDiskCheckTimeoutInvokesOneCallbackOnly() throws Exception {	
executing 

public void testTimeoutExceptionIsNotThrownForGoodDisk() throws Exception {	
executing 

========================= hadoop sample_7244 =========================

private void prepareBookKeeperEnv() throws IOException {	final String zkAvailablePath = conf.get(BKJM_ZK_LEDGERS_AVAILABLE_PATH, BKJM_ZK_LEDGERS_AVAILABLE_PATH_DEFAULT);	final CountDownLatch zkPathLatch = new CountDownLatch(1);	final AtomicBoolean success = new AtomicBoolean(false);	StringCallback callback = new StringCallback() {	public void processResult(int rc, String path, Object ctx, String name) {	if (KeeperException.Code.OK.intValue() == rc || KeeperException.Code.NODEEXISTS.intValue() == rc) {	
successfully created bookie available path 

private void prepareBookKeeperEnv() throws IOException {	final String zkAvailablePath = conf.get(BKJM_ZK_LEDGERS_AVAILABLE_PATH, BKJM_ZK_LEDGERS_AVAILABLE_PATH_DEFAULT);	final CountDownLatch zkPathLatch = new CountDownLatch(1);	final AtomicBoolean success = new AtomicBoolean(false);	StringCallback callback = new StringCallback() {	public void processResult(int rc, String path, Object ctx, String name) {	if (KeeperException.Code.OK.intValue() == rc || KeeperException.Code.NODEEXISTS.intValue() == rc) {	success.set(true);	} else {	KeeperException.Code code = KeeperException.Code.get(rc);	
error failed to create bookie available path 

public void format(NamespaceInfo ns) throws IOException {	try {	Stat baseStat = null;	Stat ledgerStat = null;	if ((baseStat = zkc.exists(basePath, false)) != null) {	if ((ledgerStat = zkc.exists(ledgerPath, false)) != null) {	for (EditLogLedgerMetadata l : getLedgerList(true)) {	try {	bkc.deleteLedger(l.getLedgerId());	} catch (BKException.BKNoSuchLedgerExistsException bke) {	
ledger does not exist cannot delete 

}	ZKUtil.deleteRecursive(zkc, basePath);	}	zkc.create(basePath, new byte[] {'0'}, Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);	VersionProto.Builder builder = VersionProto.newBuilder();	builder.setNamespaceInfo(PBHelper.convert(ns)) .setLayoutVersion(BKJM_LAYOUT_VERSION);	byte[] data = TextFormat.printToString(builder.build()).getBytes(UTF_8);	zkc.create(versionPath, data, Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);	zkc.create(ledgerPath, new byte[] {'0'}, Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);	} catch (KeeperException ke) {	
error accessing zookeeper to format 

private void cleanupLedger(LedgerHandle lh) {	try {	long id = currentLedger.getId();	currentLedger.close();	bkc.deleteLedger(id);	} catch (BKException bke) {	
error closing ledger 

private void cleanupLedger(LedgerHandle lh) {	try {	long id = currentLedger.getId();	currentLedger.close();	bkc.deleteLedger(id);	} catch (BKException bke) {	} catch (InterruptedException ie) {	Thread.currentThread().interrupt();	
interrupted while closing ledger 

List<String> children = zkc.getChildren(ledgerPath, false);	for (String child : children) {	if (!child.startsWith(BKJM_EDIT_INPROGRESS)) {	continue;	}	String znode = ledgerPath + "/" + child;	EditLogLedgerMetadata l = EditLogLedgerMetadata.read(zkc, znode);	try {	long endTxId = recoverLastTxId(l, true);	if (endTxId == HdfsServerConstants.INVALID_TXID) {	
unrecoverable corruption has occurred in segment at path unable to continue recovery 

}	String znode = ledgerPath + "/" + child;	EditLogLedgerMetadata l = EditLogLedgerMetadata.read(zkc, znode);	try {	long endTxId = recoverLastTxId(l, true);	if (endTxId == HdfsServerConstants.INVALID_TXID) {	throw new IOException("Unrecoverable corruption," + " please check logs.");	}	finalizeLogSegment(l.getFirstTxId(), endTxId);	} catch (SegmentEmptyException see) {	
inprogress znode refers to a ledger which is empty this occurs when the nn crashes after opening a segment but before writing the op start log segment op it is safe to delete metadata 

========================= hadoop sample_7751 =========================

public void testDataNodeDoesNotSplitReports() throws IOException, InterruptedException {	LocatedBlocks blocks = createFileGetBlocks(GenericTestUtils.getMethodName());	assertThat(cluster.getDataNodes().size(), is(1));	for (LocatedBlock block : blocks.getLocatedBlocks()) {	dn0.notifyNamenodeDeletedBlock( block.getBlock(), block.getStorageIDs()[0]);	}	
triggering report after deleting blocks 

========================= hadoop sample_7301 =========================

private void putEntity(TimelineEntity entity) {	try {	if (LOG.isDebugEnabled()) {	
publishing the entity json style content 

private void putEntity(TimelineEntity entity) {	try {	if (LOG.isDebugEnabled()) {	}	client.putEntities(entity);	} catch (Exception e) {	
error when publishing entity 

========================= hadoop sample_648 =========================

public void run() {	try {	startFlag.await();	if (Thread.currentThread().isInterrupted()) {	
stress interrupted before start exiting 

public void run() {	try {	startFlag.await();	if (Thread.currentThread().isInterrupted()) {	return;	}	
start stress 

public void run() {	try {	startFlag.await();	if (Thread.currentThread().isInterrupted()) {	return;	}	while (!Thread.currentThread().isInterrupted()) {	try {	while (loadStatus.overloaded()) {	if (LOG.isDebugEnabled()) {	
updating the overload status 

return;	}	while (!Thread.currentThread().isInterrupted()) {	try {	while (loadStatus.overloaded()) {	if (LOG.isDebugEnabled()) {	}	try {	checkLoadAndGetSlotsToBackfill();	} catch (IOException ioe) {	
stress check failed 

while (loadStatus.overloaded()) {	if (LOG.isDebugEnabled()) {	}	try {	checkLoadAndGetSlotsToBackfill();	} catch (IOException ioe) {	return;	}	if (loadStatus.overloaded()) {	if (LOG.isDebugEnabled()) {	
stress cluster overloaded in run sleeping 

checkLoadAndGetSlotsToBackfill();	} catch (IOException ioe) {	return;	}	if (loadStatus.overloaded()) {	if (LOG.isDebugEnabled()) {	}	try {	Thread.sleep(1000);	} catch (InterruptedException ie) {	
stress interrupted while sleeping exiting 

}	try {	Thread.sleep(1000);	} catch (InterruptedException ie) {	return;	}	}	}	while (!loadStatus.overloaded()) {	if (LOG.isDebugEnabled()) {	
stress cluster underloaded in run stressing 

return;	}	}	}	while (!loadStatus.overloaded()) {	if (LOG.isDebugEnabled()) {	}	try {	final JobStory job = getNextJobFiltered();	if (null == job) {	
stress finished consuming the input trace exiting 

}	while (!loadStatus.overloaded()) {	if (LOG.isDebugEnabled()) {	}	try {	final JobStory job = getNextJobFiltered();	if (null == job) {	return;	}	if (LOG.isDebugEnabled()) {	
job selected 

UserGroupInformation ugi = UserGroupInformation.createRemoteUser(job.getUser());	UserGroupInformation tgtUgi = userResolver.getTargetUgi(ugi);	GridmixJob tJob = jobCreator.createGridmixJob(conf, 0L, job, scratch, tgtUgi, sequence.getAndIncrement());	submitter.add(tJob);	int incompleteMapTasks = (int) calcEffectiveIncompleteMapTasks( loadStatus.getMapCapacity(), job.getNumberMaps(), 0.0f);	loadStatus.decrementMapLoad(incompleteMapTasks);	int incompleteReduceTasks = (int) calcEffectiveIncompleteReduceTasks( loadStatus.getReduceCapacity(), job.getNumberReduces(), 0.0f);	loadStatus.decrementReduceLoad(incompleteReduceTasks);	loadStatus.decrementJobLoad(1);	} catch (IOException e) {	
stress error while submitting the job 

loadStatus.decrementJobLoad(1);	} catch (IOException e) {	error = e;	return;	}	}	} finally {	}	}	} catch (InterruptedException e) {	
stress interrupted in the main block 

public void update(Statistics.ClusterStats item) {	ClusterStatus clusterStatus = item.getStatus();	try {	loadStatus.updateMapCapacity(clusterStatus.getMaxMapTasks());	loadStatus.updateReduceCapacity(clusterStatus.getMaxReduceTasks());	int numTrackers = clusterStatus.getTaskTrackers();	int jobLoad = (int) (maxJobTrackerRatio * numTrackers) - item.getNumRunningJob();	loadStatus.updateJobLoad(jobLoad);	} catch (Exception e) {	
couldn t get the new status 

protected void checkLoadAndGetSlotsToBackfill() throws IOException, InterruptedException {	if (loadStatus.getJobLoad() <= 0) {	if (LOG.isDebugEnabled()) {	
jobload overloaded is numjobsbackfill is 

int mapCapacity = loadStatus.getMapCapacity();	int reduceCapacity = loadStatus.getReduceCapacity();	if (mapCapacity < 0 || reduceCapacity < 0) {	return;	}	int maxMapLoad = (int) (overloadMapTaskMapSlotRatio * mapCapacity);	int maxReduceLoad = (int) (overloadReduceTaskReduceSlotRatio * reduceCapacity);	int totalMapTasks = ClusterStats.getSubmittedMapTasks();	int totalReduceTasks = ClusterStats.getSubmittedReduceTasks();	if (LOG.isDebugEnabled()) {	
total submitted map tasks 

int mapCapacity = loadStatus.getMapCapacity();	int reduceCapacity = loadStatus.getReduceCapacity();	if (mapCapacity < 0 || reduceCapacity < 0) {	return;	}	int maxMapLoad = (int) (overloadMapTaskMapSlotRatio * mapCapacity);	int maxReduceLoad = (int) (overloadReduceTaskReduceSlotRatio * reduceCapacity);	int totalMapTasks = ClusterStats.getSubmittedMapTasks();	int totalReduceTasks = ClusterStats.getSubmittedReduceTasks();	if (LOG.isDebugEnabled()) {	
total submitted reduce tasks 

int mapCapacity = loadStatus.getMapCapacity();	int reduceCapacity = loadStatus.getReduceCapacity();	if (mapCapacity < 0 || reduceCapacity < 0) {	return;	}	int maxMapLoad = (int) (overloadMapTaskMapSlotRatio * mapCapacity);	int maxReduceLoad = (int) (overloadReduceTaskReduceSlotRatio * reduceCapacity);	int totalMapTasks = ClusterStats.getSubmittedMapTasks();	int totalReduceTasks = ClusterStats.getSubmittedReduceTasks();	if (LOG.isDebugEnabled()) {	
max map load 

int mapCapacity = loadStatus.getMapCapacity();	int reduceCapacity = loadStatus.getReduceCapacity();	if (mapCapacity < 0 || reduceCapacity < 0) {	return;	}	int maxMapLoad = (int) (overloadMapTaskMapSlotRatio * mapCapacity);	int maxReduceLoad = (int) (overloadReduceTaskReduceSlotRatio * reduceCapacity);	int totalMapTasks = ClusterStats.getSubmittedMapTasks();	int totalReduceTasks = ClusterStats.getSubmittedReduceTasks();	if (LOG.isDebugEnabled()) {	
max reduce load 

int mapSlotsBackFill = (int) (maxMapLoad - totalMapTasks);	int reduceSlotsBackFill = (int) (maxReduceLoad - totalReduceTasks);	Set<JobID> seenJobIDs = new HashSet<JobID>();	if (totalMapTasks > maxMapLoad || totalReduceTasks > maxReduceLoad) {	float incompleteMapTasks = 0;	float incompleteReduceTasks = 0;	for (JobStats job : ClusterStats.getRunningJobStats()) {	JobID id = job.getJob().getJobID();	seenJobIDs.add(id);	if (blacklistedJobs.contains(id)) {	
ignoring blacklisted job 

JobID id = job.getJob().getJobID();	seenJobIDs.add(id);	if (blacklistedJobs.contains(id)) {	continue;	}	int noOfMaps = job.getNoOfMaps();	int noOfReduces = job.getNoOfReds();	if (noOfMaps > 0 || noOfReduces > 0) {	JobStatus status = job.getJobStatus();	if (status != null && status.isJobComplete()) {	
blacklisting completed job 

float reduceProgress = 0f;	if (status != null) {	mapProgress = status.getMapProgress();	reduceProgress = status.getReduceProgress();	}	incompleteMapTasks += calcEffectiveIncompleteMapTasks(mapCapacity, noOfMaps, mapProgress);	int currentMapSlotsBackFill = (int) (maxMapLoad - incompleteMapTasks);	if (currentMapSlotsBackFill <= 0) {	incompleteReduceTasks = totalReduceTasks;	if (LOG.isDebugEnabled()) {	
terminating overload check due to high map load 

}	break;	}	if (noOfReduces > 0) {	incompleteReduceTasks += calcEffectiveIncompleteReduceTasks(reduceCapacity, noOfReduces, reduceProgress);	}	int currentReduceSlotsBackFill = (int) (maxReduceLoad - incompleteReduceTasks);	if (currentReduceSlotsBackFill <= 0) {	incompleteMapTasks = totalMapTasks;	if (LOG.isDebugEnabled()) {	
terminating overload check due to high reduce load 

incompleteReduceTasks += calcEffectiveIncompleteReduceTasks(reduceCapacity, noOfReduces, reduceProgress);	}	int currentReduceSlotsBackFill = (int) (maxReduceLoad - incompleteReduceTasks);	if (currentReduceSlotsBackFill <= 0) {	incompleteMapTasks = totalMapTasks;	if (LOG.isDebugEnabled()) {	}	break;	}	} else {	
blacklisting empty job 

break;	}	} else {	blacklistedJobs.add(id);	}	}	mapSlotsBackFill = (int) (maxMapLoad - incompleteMapTasks);	reduceSlotsBackFill = (int)(maxReduceLoad - incompleteReduceTasks);	blacklistedJobs.retainAll(seenJobIDs);	if (LOG.isDebugEnabled() && blacklistedJobs.size() > 0) {	
blacklisted jobs count 

mapSlotsBackFill = (int) (maxMapLoad - incompleteMapTasks);	reduceSlotsBackFill = (int)(maxReduceLoad - incompleteReduceTasks);	blacklistedJobs.retainAll(seenJobIDs);	if (LOG.isDebugEnabled() && blacklistedJobs.size() > 0) {	}	}	loadStatus.updateMapLoad(mapSlotsBackFill);	loadStatus.updateReduceLoad(reduceSlotsBackFill);	if (loadStatus.getMapLoad() <= 0) {	if (LOG.isDebugEnabled()) {	
map load overloaded is mapslotsbackfill is 

}	loadStatus.updateMapLoad(mapSlotsBackFill);	loadStatus.updateReduceLoad(reduceSlotsBackFill);	if (loadStatus.getMapLoad() <= 0) {	if (LOG.isDebugEnabled()) {	}	return;	}	if (loadStatus.getReduceLoad() <= 0) {	if (LOG.isDebugEnabled()) {	
reduce load overloaded is reduceslotsbackfill is 

if (LOG.isDebugEnabled()) {	}	return;	}	if (loadStatus.getReduceLoad() <= 0) {	if (LOG.isDebugEnabled()) {	}	return;	}	if (LOG.isDebugEnabled()) {	
overall overloaded is current load status is 

public void start() {	
starting stress submission 

========================= hadoop sample_6140 =========================

private void updateCurrentMasterKey(MasterKeyData key) {	super.currentMasterKey = key;	try {	stateStore.storeContainerTokenCurrentMasterKey(key.getMasterKey());	} catch (IOException e) {	
unable to update current master key in state store 

private void updatePreviousMasterKey(MasterKeyData key) {	previousMasterKey = key;	try {	stateStore.storeContainerTokenPreviousMasterKey(key.getMasterKey());	} catch (IOException e) {	
unable to update previous master key in state store 

public synchronized void setMasterKey(MasterKey masterKeyRecord) {	if (super.currentMasterKey == null || super.currentMasterKey.getMasterKey() .getKeyId() != masterKeyRecord.getKeyId()) {	
rolling master key for container tokens got key with id 

removeAnyContainerTokenIfExpired();	ContainerId containerId = tokenId.getContainerID();	Long expTime = tokenId.getExpiryTimeStamp();	if (!recentlyStartedContainerTracker.containsKey(expTime)) {	recentlyStartedContainerTracker .put(expTime, new ArrayList<ContainerId>());	}	recentlyStartedContainerTracker.get(expTime).add(containerId);	try {	stateStore.storeContainerToken(containerId, expTime);	} catch (IOException e) {	
unable to store token for container 

protected synchronized void removeAnyContainerTokenIfExpired() {	Iterator<Entry<Long, List<ContainerId>>> containersI = this.recentlyStartedContainerTracker.entrySet().iterator();	Long currTime = System.currentTimeMillis();	while (containersI.hasNext()) {	Entry<Long, List<ContainerId>> containerEntry = containersI.next();	if (containerEntry.getKey() < currTime) {	for (ContainerId container : containerEntry.getValue()) {	try {	stateStore.removeContainerToken(container);	} catch (IOException e) {	
unable to remove token for container 

public synchronized void setNodeId(NodeId nodeId) {	nodeHostAddr = nodeId.toString();	
updating node address 

========================= hadoop sample_1685 =========================

if (initializers != null) {	conf = new Configuration(conf);	conf.set(BIND_ADDRESS, bindAddress);	for(FilterInitializer c : initializers) {	c.initFilter(this, conf);	}	}	addDefaultServlets();	if (pathSpecs != null) {	for (String path : pathSpecs) {	
adding path spec 

public boolean isRunning() {	if (super.isRunning()) {	return true;	}	
httpserver acceptor isrunning is false rechecking 

public boolean isRunning() {	if (super.isRunning()) {	return true;	}	try {	Thread.sleep(10);	} catch (InterruptedException ie) {	Thread.currentThread().interrupt();	}	boolean runState = super.isRunning();	
httpserver acceptor isrunning is 

public void addInternalServlet(String name, String pathSpec, Class<? extends HttpServlet> clazz, boolean requireAuth) {	ServletHolder holder = new ServletHolder(clazz);	if (name != null) {	holder.setName(name);	}	webAppContext.addServlet(holder, pathSpec);	if(requireAuth && UserGroupInformation.isSecurityEnabled()) {	
adding kerberos spnego filter to 

public void start() throws IOException {	try {	try {	openListener();	
jetty bound to port 

public void start() throws IOException {	try {	try {	openListener();	webServer.start();	} catch (IOException ex) {	
httpserver start threw a non bind ioexception 

public void start() throws IOException {	try {	try {	openListener();	webServer.start();	} catch (IOException ex) {	throw ex;	} catch (MultiException ex) {	
httpserver start threw a multiexception 

public void stop() throws Exception {	MultiException exception = null;	try {	listener.close();	} catch (Exception e) {	
error while stopping listener for webapp 

try {	listener.close();	} catch (Exception e) {	exception = addMultiException(exception, e);	}	try {	if (sslFactory != null) {	sslFactory.destroy();	}	} catch (Exception e) {	
error while destroying the sslfactory 

if (sslFactory != null) {	sslFactory.destroy();	}	} catch (Exception e) {	exception = addMultiException(exception, e);	}	try {	webAppContext.clearAttributes();	webAppContext.stop();	} catch (Exception e) {	
error while stopping web app context for webapp 

}	try {	webAppContext.clearAttributes();	webAppContext.stop();	} catch (Exception e) {	exception = addMultiException(exception, e);	}	try {	webServer.stop();	} catch (Exception e) {	
error while stopping web server for webapp 

========================= hadoop sample_3968 =========================

public synchronized void removeShm(ShortCircuitShm shm) {	if (LOG.isTraceEnabled()) {	
removing shm 

if (dswLoadingFailure != null) {	throw new IOException(dswLoadingFailure);	}	watcher = new DomainSocketWatcher(interruptCheck, "datanode");	enabled = true;	if (LOG.isDebugEnabled()) {	LOG.debug("created new ShortCircuitRegistry with interruptCheck=" + interruptCheck + ", shmPath=" + shmFactory.getPath());	}	} catch (IOException e) {	if (LOG.isDebugEnabled()) {	
disabling shortcircuitregistry 

public NewShmInfo createNewMemorySegment(String clientName, DomainSocket sock) throws IOException {	NewShmInfo info = null;	RegisteredShm shm = null;	ShmId shmId = null;	synchronized (this) {	if (!enabled) {	if (LOG.isTraceEnabled()) {	
createnewmemorysegment shortcircuitregistry is not enabled 

} finally {	if (shm == null) {	IOUtils.closeQuietly(fis);	}	}	info = new NewShmInfo(shmId, fis);	segments.put(shmId, shm);	}	watcher.add(sock, shm);	if (LOG.isTraceEnabled()) {	
createnewmemorysegment created 

public synchronized void registerSlot(ExtendedBlockId blockId, SlotId slotId, boolean isCached) throws InvalidRequestException {	if (!enabled) {	if (LOG.isTraceEnabled()) {	
can t register a slot because the shortcircuitregistry is not enabled 

public synchronized void unregisterSlot(SlotId slotId) throws InvalidRequestException {	if (!enabled) {	if (LOG.isTraceEnabled()) {	
unregisterslot shortcircuitregistry is not enabled 

========================= hadoop sample_7881 =========================

public boolean createReservation(ReservationId reservationId, String user, Plan plan, ReservationDefinition contract) throws PlanningException {	
placing the following reservationrequest 

public boolean createReservation(ReservationId reservationId, String user, Plan plan, ReservationDefinition contract) throws PlanningException {	try {	boolean res = planner.createReservation(reservationId, user, plan, contract);	if (res) {	
outcome success reservation id contract 

public boolean createReservation(ReservationId reservationId, String user, Plan plan, ReservationDefinition contract) throws PlanningException {	try {	boolean res = planner.createReservation(reservationId, user, plan, contract);	if (res) {	} else {	
outcome failure reservation id contract 

public boolean createReservation(ReservationId reservationId, String user, Plan plan, ReservationDefinition contract) throws PlanningException {	try {	boolean res = planner.createReservation(reservationId, user, plan, contract);	if (res) {	} else {	}	return res;	} catch (PlanningException e) {	
outcome failure reservation id contract 

public boolean updateReservation(ReservationId reservationId, String user, Plan plan, ReservationDefinition contract) throws PlanningException {	
updating the following reservationrequest 

public boolean deleteReservation(ReservationId reservationId, String user, Plan plan) throws PlanningException {	
removing the following reservationid 

========================= hadoop sample_1056 =========================

public void testMissingServerResource() throws Throwable {	try {	HttpServer2 server = createServer("NoSuchWebapp");	String serverDescription = server.toString();	stop(server);	fail("Expected an exception, got " + serverDescription);	} catch (FileNotFoundException expected) {	
expected exception 

========================= hadoop sample_3084 =========================

public List<InputSplit> getSplits(JobContext jobContext) throws IOException, InterruptedException {	
dynamicinputformat getting splits for job 

private static int getMaxChunksTolerable(Configuration conf) {	int maxChunksTolerable = conf.getInt( DistCpConstants.CONF_LABEL_MAX_CHUNKS_TOLERABLE, DistCpConstants.MAX_CHUNKS_TOLERABLE_DEFAULT);	if (maxChunksTolerable <= 0) {	
should be positive fall back to default value 

private static int getMaxChunksIdeal(Configuration conf) {	int maxChunksIdeal = conf.getInt( DistCpConstants.CONF_LABEL_MAX_CHUNKS_IDEAL, DistCpConstants.MAX_CHUNKS_IDEAL_DEFAULT);	if (maxChunksIdeal <= 0) {	
should be positive fall back to default value 

private static int getMinRecordsPerChunk(Configuration conf) {	int minRecordsPerChunk = conf.getInt( DistCpConstants.CONF_LABEL_MIN_RECORDS_PER_CHUNK, DistCpConstants.MIN_RECORDS_PER_CHUNK_DEFAULT);	if (minRecordsPerChunk <= 0) {	
should be positive fall back to default value 

private static int getSplitRatio(Configuration conf) {	int splitRatio = conf.getInt( DistCpConstants.CONF_LABEL_SPLIT_RATIO, DistCpConstants.SPLIT_RATIO_DEFAULT);	if (splitRatio <= 0) {	
should be positive fall back to default value 

========================= hadoop sample_6286 =========================

public ConnectionManager(Configuration config) {	this.conf = config;	this.maxSize = this.conf.getInt( DFSConfigKeys.DFS_ROUTER_NAMENODE_CONNECTION_POOL_SIZE, DFSConfigKeys.DFS_ROUTER_NAMENODE_CONNECTION_POOL_SIZE_DEFAULT);	this.pools = new HashMap<>();	this.creator = new ConnectionCreator(creatorQueue);	this.creator.setDaemon(true);	this.poolCleanupPeriodMs = this.conf.getLong( DFSConfigKeys.DFS_ROUTER_NAMENODE_CONNECTION_POOL_CLEAN, DFSConfigKeys.DFS_ROUTER_NAMENODE_CONNECTION_POOL_CLEAN_DEFAULT);	
cleaning connection pools every seconds 

public ConnectionManager(Configuration config) {	this.conf = config;	this.maxSize = this.conf.getInt( DFSConfigKeys.DFS_ROUTER_NAMENODE_CONNECTION_POOL_SIZE, DFSConfigKeys.DFS_ROUTER_NAMENODE_CONNECTION_POOL_SIZE_DEFAULT);	this.pools = new HashMap<>();	this.creator = new ConnectionCreator(creatorQueue);	this.creator.setDaemon(true);	this.poolCleanupPeriodMs = this.conf.getLong( DFSConfigKeys.DFS_ROUTER_NAMENODE_CONNECTION_POOL_CLEAN, DFSConfigKeys.DFS_ROUTER_NAMENODE_CONNECTION_POOL_CLEAN_DEFAULT);	this.connectionCleanupPeriodMs = this.conf.getLong( DFSConfigKeys.DFS_ROUTER_NAMENODE_CONNECTION_CLEAN_MS, DFSConfigKeys.DFS_ROUTER_NAMENODE_CONNECTION_CLEAN_MS_DEFAULT);	
cleaning connections every seconds 

public void start() {	this.creator.start();	long recyleTimeMs = Math.min( poolCleanupPeriodMs, connectionCleanupPeriodMs);	
cleaning every seconds 

public ConnectionContext getConnection( UserGroupInformation ugi, String nnAddress) throws IOException {	if (!this.running) {	
cannot get a connection to because the manager isn t running 

pool = new ConnectionPool( this.conf, nnAddress, ugi, this.minSize, this.maxSize);	this.pools.put(connectionId, pool);	}	} finally {	writeLock.unlock();	}	}	ConnectionContext conn = pool.getConnection();	if (conn == null || !conn.isUsable()) {	if (!this.creatorQueue.offer(pool)) {	
cannot add more than connections at the same time 

} finally {	writeLock.unlock();	}	}	ConnectionContext conn = pool.getConnection();	if (conn == null || !conn.isUsable()) {	if (!this.creatorQueue.offer(pool)) {	}	}	if (conn != null && conn.isClosed()) {	
we got a closed connection from 

public void run() {	long currentTime = Time.now();	List<ConnectionPoolId> toRemove = new LinkedList<>();	readLock.lock();	try {	for (Entry<ConnectionPoolId, ConnectionPool> entry : pools.entrySet()) {	ConnectionPool pool = entry.getValue();	long lastTimeActive = pool.getLastActiveTime();	boolean isStale = currentTime > (lastTimeActive + poolCleanupPeriodMs);	if (lastTimeActive > 0 && isStale) {	
closing and removing stale pool 

try {	for (Entry<ConnectionPoolId, ConnectionPool> entry : pools.entrySet()) {	ConnectionPool pool = entry.getValue();	long lastTimeActive = pool.getLastActiveTime();	boolean isStale = currentTime > (lastTimeActive + poolCleanupPeriodMs);	if (lastTimeActive > 0 && isStale) {	pool.close();	ConnectionPoolId poolId = entry.getKey();	toRemove.add(poolId);	} else {	
cleaning up 

private void cleanup(ConnectionPool pool) {	if (pool.getNumConnections() > pool.getMinSize()) {	long timeSinceLastActive = Time.now() - pool.getLastActiveTime();	int total = pool.getNumConnections();	int active = getNumActiveConnections();	if (timeSinceLastActive > connectionCleanupPeriodMs || active < MIN_ACTIVE_RATIO * total) {	List<ConnectionContext> conns = pool.removeConnections(1);	for (ConnectionContext conn : conns) {	conn.close();	}	
removed connection used seconds ago pool has connections 

while (this.running) {	try {	ConnectionPool pool = this.queue.take();	try {	int total = pool.getNumConnections();	int active = pool.getNumActiveConnections();	if (pool.getNumConnections() < pool.getMaxSize() && active >= MIN_ACTIVE_RATIO * total) {	ConnectionContext conn = pool.newConnection();	pool.addConnection(conn);	} else {	
cannot add more than connections to 

ConnectionPool pool = this.queue.take();	try {	int total = pool.getNumConnections();	int active = pool.getNumActiveConnections();	if (pool.getNumConnections() < pool.getMaxSize() && active >= MIN_ACTIVE_RATIO * total) {	ConnectionContext conn = pool.newConnection();	pool.addConnection(conn);	} else {	}	} catch (IOException e) {	
cannot create a new connection 

int total = pool.getNumConnections();	int active = pool.getNumActiveConnections();	if (pool.getNumConnections() < pool.getMaxSize() && active >= MIN_ACTIVE_RATIO * total) {	ConnectionContext conn = pool.newConnection();	pool.addConnection(conn);	} else {	}	} catch (IOException e) {	}	} catch (InterruptedException e) {	
the connection creator was interrupted 

========================= hadoop sample_8298 =========================

public void run() {	while (fsRunning) {	long period = Math.min(DEFAULT_RECHECK_INTERVAL, timeout);	try {	pendingReplicationCheck();	Thread.sleep(period);	} catch (InterruptedException ie) {	if(LOG.isDebugEnabled()) {	
pendingreplicationmonitor thread is interrupted 

========================= hadoop sample_8332 =========================

public void initJunitModeTest() throws Exception {	
initJunitModeTest 

private void initClusterModeTest() throws IOException {	LOG = LogFactory.getLog(TestWriteRead.class);	
initClusterModeTest 

byte[] inBuffer = new byte[BUFFER_SIZE];	for (int i = 0; i < BUFFER_SIZE; i++) {	outBuffer[i] = (byte) (i & 0x00ff);	}	try {	Path path = getFullyQualifiedPath(fname);	long fileLengthBeforeOpen = 0;	if (ifExists(path)) {	if (truncateOption) {	out = useFCOption ? mfc.create(path,EnumSet.of(CreateFlag.OVERWRITE)): mfs.create(path, truncateOption);	
file already exists file open with truncate mode 

}	try {	Path path = getFullyQualifiedPath(fname);	long fileLengthBeforeOpen = 0;	if (ifExists(path)) {	if (truncateOption) {	out = useFCOption ? mfc.create(path,EnumSet.of(CreateFlag.OVERWRITE)): mfs.create(path, truncateOption);	} else {	out = useFCOption ? mfc.create(path, EnumSet.of(CreateFlag.APPEND)) : mfs.append(path);	fileLengthBeforeOpen = getFileLengthFromNN(path);	
file already exists of size file open for append mode 

private void usage() {	
usage useseqread useposread append truncate chunksize nn loop ntimes f filename 

trw.initClusterModeTest();	trw.getCmdLineOption(args);	int stat = trw.clusterTestWriteRead1();	if (stat == 0) {	System.out.println("Status: clusterTestWriteRead1 test PASS");	} else {	System.out.println("Status: clusterTestWriteRead1 test FAIL with " + stat + " failures");	}	System.exit(stat);	} catch (IOException e) {	
exception in main 

========================= hadoop sample_7213 =========================

public JobState getState() {	JobState js = null;	try {	js = JobState.valueOf(jobIndexInfo.getJobStatus());	} catch (Exception e) {	
exception while parsing job state defaulting to killed 

========================= hadoop sample_5384 =========================

launch = new RecoverPausedContainerLaunch(context, getConfig(), dispatcher, exec, app, event.getContainer(), dirsHandler, containerManager);	containerLauncher.submit(launch);	break;	case CLEANUP_CONTAINER: case CLEANUP_CONTAINER_FOR_REINIT: ContainerLaunch launcher = running.remove(containerId);	if (launcher == null) {	return;	}	try {	launcher.cleanupContainer();	} catch (IOException e) {	
got exception while cleaning container ignoring 

return;	}	try {	launcher.cleanupContainer();	} catch (IOException e) {	}	break;	case SIGNAL_CONTAINER: SignalContainersLauncherEvent signalEvent = (SignalContainersLauncherEvent) event;	ContainerLaunch runningContainer = running.get(containerId);	if (runningContainer == null) {	
container not running nothing to signal 

}	break;	case SIGNAL_CONTAINER: SignalContainersLauncherEvent signalEvent = (SignalContainersLauncherEvent) event;	ContainerLaunch runningContainer = running.get(containerId);	if (runningContainer == null) {	return;	}	try {	runningContainer.signalContainer(signalEvent.getCommand());	} catch (IOException e) {	
got exception while signaling container with command 

} catch (IOException e) {	}	break;	case PAUSE_CONTAINER: ContainerLaunch launchedContainer = running.get(containerId);	if (launchedContainer == null) {	return;	}	try {	launchedContainer.pauseContainer();	} catch (Exception e) {	
got exception while pausing container 

} catch (Exception e) {	}	break;	case RESUME_CONTAINER: ContainerLaunch launchCont = running.get(containerId);	if (launchCont == null) {	return;	}	try {	launchCont.resumeContainer();	} catch (Exception e) {	
got exception while resuming container 

========================= hadoop sample_1778 =========================

es.awaitTermination(1, TimeUnit.MINUTES);	es = null;	System.gc();	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	int size = stats.getAllThreadLocalDataSize();	allDataSize.set(size);	if (size == 0) {	return true;	}	
not all references have been cleaned up still references left 

es.awaitTermination(1, TimeUnit.MINUTES);	es = null;	System.gc();	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	int size = stats.getAllThreadLocalDataSize();	allDataSize.set(size);	if (size == 0) {	return true;	}	
triggering another gc 

========================= hadoop sample_3315 =========================

public void testAddCachedLocWhenEmpty() {	DatanodeInfo[] ds = new DatanodeInfo[0];	ExtendedBlock b1 = new ExtendedBlock("bpid", 1, 1, 1);	LocatedBlock l1 = new LocatedBlock(b1, ds);	DatanodeDescriptor dn = new DatanodeDescriptor( new DatanodeID("127.0.0.1", "localhost", "abcd", 5000, 5001, 5002, 5003));	try {	l1.addCachedLoc(dn);	fail("Adding dn when block is empty should throw");	} catch (IllegalArgumentException e) {	
expected exception 

========================= hadoop sample_7585 =========================

String cgroupId = container.getContainerId().toString();	long containerSoftLimit = (long) (container.getResource().getMemorySize() * this.softLimit);	long containerHardLimit = container.getResource().getMemorySize();	cGroupsHandler.createCGroup(MEMORY, cgroupId);	try {	cGroupsHandler.updateCGroupParam(MEMORY, cgroupId, CGroupsHandler.CGROUP_PARAM_MEMORY_HARD_LIMIT_BYTES, String.valueOf(containerHardLimit) + "M");	cGroupsHandler.updateCGroupParam(MEMORY, cgroupId, CGroupsHandler.CGROUP_PARAM_MEMORY_SOFT_LIMIT_BYTES, String.valueOf(containerSoftLimit) + "M");	cGroupsHandler.updateCGroupParam(MEMORY, cgroupId, CGroupsHandler.CGROUP_PARAM_MEMORY_SWAPPINESS, String.valueOf(swappiness));	} catch (ResourceHandlerException re) {	cGroupsHandler.deleteCGroup(MEMORY, cgroupId);	
could not update cgroup for container 

========================= hadoop sample_1847 =========================

nlsifo = response.getEntity(NodeLabelsInfo.class);	assertEquals(2, nlsifo.getNodeLabels().size());	for (NodeLabelInfo nl : nlsifo.getNodeLabelsInfo()) {	if (nl.getName().equals("b")) {	assertFalse(nl.getExclusivity());	}	}	MultivaluedMapImpl params = new MultivaluedMapImpl();	params.add("labels", "a");	response = r.path("ws").path("v1").path("cluster") .path("nodes").path("nid:0") .path("replace-labels") .queryParam("user.name", userName) .queryParams(params) .accept(MediaType.APPLICATION_JSON) .post(ClientResponse.class);	
posted node nodelabel 

if (nl.getName().equals("b")) {	assertFalse(nl.getExclusivity());	}	}	MultivaluedMapImpl params = new MultivaluedMapImpl();	params.add("labels", "a");	response = r.path("ws").path("v1").path("cluster") .path("nodes").path("nid:0") .path("replace-labels") .queryParam("user.name", userName) .queryParams(params) .accept(MediaType.APPLICATION_JSON) .post(ClientResponse.class);	params = new MultivaluedMapImpl();	params.add("labels", "b");	response = r.path("ws").path("v1").path("cluster") .path("nodes").path("nid1:0") .path("replace-labels") .queryParam("user.name", userName) .queryParams(params) .accept(MediaType.APPLICATION_JSON) .post(ClientResponse.class);	
posted node nodelabel 

}	MultivaluedMapImpl params = new MultivaluedMapImpl();	params.add("labels", "a");	response = r.path("ws").path("v1").path("cluster") .path("nodes").path("nid:0") .path("replace-labels") .queryParam("user.name", userName) .queryParams(params) .accept(MediaType.APPLICATION_JSON) .post(ClientResponse.class);	params = new MultivaluedMapImpl();	params.add("labels", "b");	response = r.path("ws").path("v1").path("cluster") .path("nodes").path("nid1:0") .path("replace-labels") .queryParam("user.name", userName) .queryParams(params) .accept(MediaType.APPLICATION_JSON) .post(ClientResponse.class);	params = new MultivaluedMapImpl();	params.add("labels", "b");	response = r.path("ws").path("v1").path("cluster") .path("nodes").path("nid2:0") .path("replace-labels") .queryParam("user.name", userName) .queryParams(params) .accept(MediaType.APPLICATION_JSON) .post(ClientResponse.class);	
posted node nodelabel 

assertEquals(1, ltni.getLabelsToNodes().size());	nodes = ltni.getLabelsToNodes().get(new NodeLabelInfo("a"));	assertTrue(nodes.getNodeIDs().contains("nid:0"));	response = r.path("ws").path("v1").path("cluster") .path("nodes").path("nid:0") .path("get-labels").queryParam("user.name", userName) .accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);	assertEquals(MediaType.APPLICATION_JSON_TYPE, response.getType());	nlsifo = response.getEntity(NodeLabelsInfo.class);	assertTrue(nlsifo.getNodeLabelsInfo().contains(new NodeLabelInfo("a")));	params = new MultivaluedMapImpl();	params.add("labels", "b");	response = r.path("ws").path("v1").path("cluster") .path("nodes").path("nid:0") .path("replace-labels") .queryParam("user.name", userName) .queryParams(params) .accept(MediaType.APPLICATION_JSON) .post(ClientResponse.class);	
posted node nodelabel 

response = r.path("ws").path("v1").path("cluster") .path("replace-node-to-labels") .queryParam("user.name", userName) .accept(MediaType.APPLICATION_JSON) .entity(toJson(ntli, NodeToLabelsEntryList.class), MediaType.APPLICATION_JSON) .post(ClientResponse.class);	response = r.path("ws").path("v1").path("cluster") .path("get-node-to-labels").queryParam("user.name", userName) .accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);	assertEquals(MediaType.APPLICATION_JSON_TYPE, response.getType());	NodeToLabelsInfo ntlinfo = response.getEntity(NodeToLabelsInfo.class);	NodeLabelsInfo nlinfo = ntlinfo.getNodeToLabels().get("nid:0");	assertEquals(1, nlinfo.getNodeLabels().size());	assertTrue(nlinfo.getNodeLabelsInfo().contains(new NodeLabelInfo("a")));	params = new MultivaluedMapImpl();	params.add("labels", "");	response = r.path("ws").path("v1").path("cluster") .path("nodes").path("nid:0") .path("replace-labels") .queryParam("user.name", userName) .queryParams(params) .accept(MediaType.APPLICATION_JSON) .post(ClientResponse.class);	
posted node nodelabel 

params = new MultivaluedMapImpl();	params.add("labels", "");	response = r.path("ws").path("v1").path("cluster") .path("nodes").path("nid:0") .path("replace-labels") .queryParam("user.name", userName) .queryParams(params) .accept(MediaType.APPLICATION_JSON) .post(ClientResponse.class);	response = r.path("ws").path("v1").path("cluster") .path("nodes").path("nid:0") .path("get-labels").queryParam("user.name", userName) .accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);	assertEquals(MediaType.APPLICATION_JSON_TYPE, response.getType());	nlsifo = response.getEntity(NodeLabelsInfo.class);	assertTrue(nlsifo.getNodeLabelsInfo().isEmpty());	params = new MultivaluedMapImpl();	params.add("labels", "a");	response = r.path("ws").path("v1").path("cluster") .path("nodes").path("nid:0") .path("replace-labels") .queryParam("user.name", userName) .queryParams(params) .accept(MediaType.APPLICATION_JSON) .post(ClientResponse.class);	
posted node nodelabel 

assertEquals(MediaType.APPLICATION_JSON_TYPE, response.getType());	nlsifo = response.getEntity(NodeLabelsInfo.class);	assertEquals(0, nlsifo.getNodeLabels().size());	nlsifo = new NodeLabelsInfo();	nlsifo.getNodeLabelsInfo().add(new NodeLabelInfo("x", false));	nlsifo.getNodeLabelsInfo().add(new NodeLabelInfo("y", false));	response = r.path("ws") .path("v1") .path("cluster") .path("add-node-labels") .queryParam("user.name", userName) .accept(MediaType.APPLICATION_JSON) .entity(toJson(nlsifo, NodeLabelsInfo.class), MediaType.APPLICATION_JSON).post(ClientResponse.class);	params = new MultivaluedMapImpl();	params.add("labels", "y");	response = r.path("ws").path("v1").path("cluster").path("nodes").path("nid:0") .path("replace-labels").queryParam("user.name", userName) .queryParams(params) .accept(MediaType.APPLICATION_JSON) .post(ClientResponse.class);	
posted node nodelabel 

nli = new NodeToLabelsEntry("nid:0", labels);	ntli.getNodeToLabels().add(nli);	response = r.path("ws") .path("v1") .path("cluster") .path("replace-node-to-labels") .queryParam("user.name", userName) .accept(MediaType.APPLICATION_JSON) .entity(toJson(ntli, NodeToLabelsEntryList.class), MediaType.APPLICATION_JSON).post(ClientResponse.class);	response = r.path("ws").path("v1").path("cluster").path("get-node-to-labels") .queryParam("user.name", userName) .accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);	assertEquals(MediaType.APPLICATION_JSON_TYPE, response.getType());	ntlinfo = response.getEntity(NodeToLabelsInfo.class);	nlinfo = ntlinfo.getNodeToLabels().get("nid:0");	assertEquals(1, nlinfo.getNodeLabels().size());	assertFalse(nlinfo.getNodeLabelsInfo().contains( new NodeLabelInfo("x", false)));	response = r.path("ws").path("v1").path("cluster").path("nodes").path("nid:0") .path("replace-labels").queryParam("user.name", userName) .accept(MediaType.APPLICATION_JSON) .entity("{\"nodeLabelName\": [\"x\"]}", MediaType.APPLICATION_JSON) .post(ClientResponse.class);	
posted node nodelabel 

========================= hadoop sample_473 =========================

public void testEscapeJobSummary() {	summary.setJobName("aa\rbb\ncc\r\ndd");	String out = summary.getJobSummaryString();	
summary 

========================= hadoop sample_5107 =========================

protected void doGet(final HttpServletRequest req, final HttpServletResponse resp) throws ServletException, IOException {	final UserGroupInformation ugi;	final ServletContext context = getServletContext();	final Configuration conf = NameNodeHttpServer.getConfFromContext(context);	try {	ugi = getUGI(req, conf);	} catch(IOException ioe) {	
request for token received with no authentication from 

protected void doGet(final HttpServletRequest req, final HttpServletResponse resp) throws ServletException, IOException {	final UserGroupInformation ugi;	final ServletContext context = getServletContext();	final Configuration conf = NameNodeHttpServer.getConfFromContext(context);	try {	ugi = getUGI(req, conf);	} catch(IOException ioe) {	resp.sendError(HttpServletResponse.SC_FORBIDDEN, "Unable to identify or authenticate user");	return;	}	
sending token 

dos = new DataOutputStream(resp.getOutputStream());	final DataOutputStream dosFinal = dos;	ugi.doAs(new PrivilegedExceptionAction<Void>() {	public Void run() throws IOException {	final Credentials ts = DelegationTokenSecretManager.createCredentials( nn, ugi, renewerFinal);	ts.write(dosFinal);	return null;	}	});	} catch(Exception e) {	
exception while sending token re throwing 

========================= hadoop sample_7993 =========================

public void initializeQueues(CapacitySchedulerConfiguration conf) throws IOException {	root = parseQueue(this.csContext, conf, null, CapacitySchedulerConfiguration.ROOT, queues, queues, NOOP);	setQueueAcls(authorizer, appPriorityACLManager, queues);	labelManager.reinitializeQueueLabels(getQueueToLabels());	this.queueStateManager.initialize(this);	
initialized root queue 

for (String childQueueName : childQueueNames) {	CSQueue childQueue = parseQueue(csContext, conf, queue, childQueueName, queues, oldQueues, hook);	childQueues.add(childQueue);	}	parentQueue.setChildQueues(childQueues);	}	if (queue instanceof LeafQueue && queues.containsKey(queueName) && queues.get(queueName) instanceof LeafQueue) {	throw new IOException("Two leaf queues were named " + queueName + ". Leaf queue names must be distinct");	}	queues.put(queueName, queue);	
initialized queue 

private void validateQueueHierarchy(Map<String, CSQueue> queues, Map<String, CSQueue> newQueues) throws IOException {	for (Map.Entry<String, CSQueue> e : queues.entrySet()) {	if (!(e.getValue() instanceof ReservationQueue)) {	String queueName = e.getKey();	CSQueue oldQueue = e.getValue();	CSQueue newQueue = newQueues.get(queueName);	if (null == newQueue) {	if (oldQueue.getState() == QueueState.STOPPED) {	
deleting queue as it is not present in the modified capacity configuration xml 

CSQueue newQueue = newQueues.get(queueName);	if (null == newQueue) {	if (oldQueue.getState() == QueueState.STOPPED) {	} else {	throw new IOException(oldQueue.getQueuePath() + " is deleted from" + " the new capacity scheduler configuration, but the" + " queue is not yet in stopped state. " + "Current State : " + oldQueue.getState());	}	} else if (!oldQueue.getQueuePath().equals(newQueue.getQueuePath())) {	throw new IOException(queueName + " is moved from:" + oldQueue.getQueuePath() + " to:" + newQueue.getQueuePath() + " after refresh, which is not allowed.");	} else  if (oldQueue instanceof LeafQueue && newQueue instanceof ParentQueue) {	if (oldQueue.getState() == QueueState.STOPPED) {	
converting the leaf queue to parent queue 

throw new IOException(oldQueue.getQueuePath() + " is deleted from" + " the new capacity scheduler configuration, but the" + " queue is not yet in stopped state. " + "Current State : " + oldQueue.getState());	}	} else if (!oldQueue.getQueuePath().equals(newQueue.getQueuePath())) {	throw new IOException(queueName + " is moved from:" + oldQueue.getQueuePath() + " to:" + newQueue.getQueuePath() + " after refresh, which is not allowed.");	} else  if (oldQueue instanceof LeafQueue && newQueue instanceof ParentQueue) {	if (oldQueue.getState() == QueueState.STOPPED) {	} else {	throw new IOException("Can not convert the leaf queue: " + oldQueue.getQueuePath() + " to parent queue since " + "it is not yet in stopped state. Current State : " + oldQueue.getState());	}	} else if (oldQueue instanceof ParentQueue && newQueue instanceof LeafQueue) {	
converting the parent queue to leaf queue 

========================= hadoop sample_920 =========================

public boolean checkAccess(UserGroupInformation callerUGI, JobACL jobOperation, String jobOwner, AccessControlList jobACL) {	if (LOG.isDebugEnabled()) {	
checkaccess job acls jobowner jobacl user 

========================= hadoop sample_4716 =========================

public static void main(String[] args) {	try {	UnmanagedAMLauncher client = new UnmanagedAMLauncher();	
initializing client 

public static void main(String[] args) {	try {	UnmanagedAMLauncher client = new UnmanagedAMLauncher();	boolean doRun = client.init(args);	if (!doRun) {	System.exit(0);	}	client.run();	} catch (Throwable t) {	
error running client 

final BufferedReader inReader = new BufferedReader(new InputStreamReader( amProc.getInputStream(), Charset.forName("UTF-8")));	Thread errThread = new Thread() {	public void run() {	try {	String line = errReader.readLine();	while((line != null) && !isInterrupted()) {	System.err.println(line);	line = errReader.readLine();	}	} catch(IOException ioe) {	
error reading the error stream 

};	Thread outThread = new Thread() {	public void run() {	try {	String line = inReader.readLine();	while((line != null) && !isInterrupted()) {	System.out.println(line);	line = inReader.readLine();	}	} catch(IOException ioe) {	
error reading the out stream 

} catch(IOException ioe) {	}	}	};	try {	errThread.start();	outThread.start();	} catch (IllegalStateException ise) { }	try {	int exitCode = amProc.waitFor();	
am process exited with value 

e.printStackTrace();	} finally {	amCompleted = true;	}	try {	errThread.join();	outThread.join();	errReader.close();	inReader.close();	} catch (InterruptedException ie) {	
shellexecutor interrupted while reading the error out stream 

} finally {	amCompleted = true;	}	try {	errThread.join();	outThread.join();	errReader.close();	inReader.close();	} catch (InterruptedException ie) {	} catch (IOException ioe) {	
error while closing the error out stream 

public boolean run() throws IOException, YarnException {	
starting client 

public boolean run() throws IOException, YarnException {	rmClient.start();	try {	
setting up application submission context for asm 

ApplicationSubmissionContext appContext = rmClient.createApplication() .getApplicationSubmissionContext();	ApplicationId appId = appContext.getApplicationId();	appContext.setApplicationName(appName);	Priority pri = Records.newRecord(Priority.class);	pri.setPriority(amPriority);	appContext.setPriority(pri);	appContext.setQueue(amQueue);	ContainerLaunchContext amContainer = Records .newRecord(ContainerLaunchContext.class);	appContext.setAMContainerSpec(amContainer);	appContext.setUnmanagedAM(true);	
setting unmanaged am 

ApplicationSubmissionContext appContext = rmClient.createApplication() .getApplicationSubmissionContext();	ApplicationId appId = appContext.getApplicationId();	appContext.setApplicationName(appName);	Priority pri = Records.newRecord(Priority.class);	pri.setPriority(amPriority);	appContext.setPriority(pri);	appContext.setQueue(amQueue);	ContainerLaunchContext amContainer = Records .newRecord(ContainerLaunchContext.class);	appContext.setAMContainerSpec(amContainer);	appContext.setUnmanagedAM(true);	
submitting application to asm 

appContext.setPriority(pri);	appContext.setQueue(amQueue);	ContainerLaunchContext amContainer = Records .newRecord(ContainerLaunchContext.class);	appContext.setAMContainerSpec(amContainer);	appContext.setUnmanagedAM(true);	rmClient.submitApplication(appContext);	ApplicationReport appReport = monitorApplication(appId, EnumSet.of(YarnApplicationState.ACCEPTED, YarnApplicationState.KILLED, YarnApplicationState.FAILED, YarnApplicationState.FINISHED));	if (appReport.getYarnApplicationState() == YarnApplicationState.ACCEPTED) {	ApplicationAttemptReport attemptReport = monitorCurrentAppAttempt(appId, YarnApplicationAttemptState.LAUNCHED);	ApplicationAttemptId attemptId = attemptReport.getApplicationAttemptId();	
launching am with application attempt id 

rmClient.submitApplication(appContext);	ApplicationReport appReport = monitorApplication(appId, EnumSet.of(YarnApplicationState.ACCEPTED, YarnApplicationState.KILLED, YarnApplicationState.FAILED, YarnApplicationState.FINISHED));	if (appReport.getYarnApplicationState() == YarnApplicationState.ACCEPTED) {	ApplicationAttemptReport attemptReport = monitorCurrentAppAttempt(appId, YarnApplicationAttemptState.LAUNCHED);	ApplicationAttemptId attemptId = attemptReport.getApplicationAttemptId();	launchAM(attemptId);	appReport = monitorApplication(appId, EnumSet.of(YarnApplicationState.KILLED, YarnApplicationState.FAILED, YarnApplicationState.FINISHED));	}	YarnApplicationState appState = appReport.getYarnApplicationState();	FinalApplicationStatus appStatus = appReport.getFinalApplicationStatus();	
app ended with state and status 

if (appReport.getYarnApplicationState() == YarnApplicationState.ACCEPTED) {	ApplicationAttemptReport attemptReport = monitorCurrentAppAttempt(appId, YarnApplicationAttemptState.LAUNCHED);	ApplicationAttemptId attemptId = attemptReport.getApplicationAttemptId();	launchAM(attemptId);	appReport = monitorApplication(appId, EnumSet.of(YarnApplicationState.KILLED, YarnApplicationState.FAILED, YarnApplicationState.FINISHED));	}	YarnApplicationState appState = appReport.getYarnApplicationState();	FinalApplicationStatus appStatus = appReport.getFinalApplicationStatus();	boolean success;	if (YarnApplicationState.FINISHED == appState && FinalApplicationStatus.SUCCEEDED == appStatus) {	
application has completed successfully 

if (attemptId != null) {	attemptReport = rmClient.getApplicationAttemptReport(attemptId);	if (attemptState.equals(attemptReport.getYarnApplicationAttemptState())) {	return attemptReport;	}	}	LOG.info("Current attempt state of " + appId + " is " + (attemptReport == null ? " N/A " : attemptReport.getYarnApplicationAttemptState()) + ", waiting for current attempt to reach " + attemptState);	try {	Thread.sleep(1000);	} catch (InterruptedException e) {	
interrupted while waiting for current attempt of to reach 

first = false;	expectedFinalState.append(state.name());	} else {	expectedFinalState.append("," + state.name());	}	}	while (true) {	try {	Thread.sleep(1000);	} catch (InterruptedException e) {	
thread sleep in monitoring loop interrupted 

========================= hadoop sample_14 =========================

public int run(String[] args) throws Exception {	if (args.length != 2) {	usage();	return 2;	}	
starting 

LOG.error(e.getMessage());	return -1;	}	job.addCacheFile(partitionUri);	long end = System.currentTimeMillis();	System.out.println("Spent " + (end - start) + "ms computing partitions.");	job.setPartitionerClass(TotalOrderPartitioner.class);	}	job.getConfiguration().setInt("dfs.replication", getOutputReplication(job));	int ret = job.waitForCompletion(true) ? 0 : 1;	
done 

========================= hadoop sample_5740 =========================

private void verifyDataNodeVolumeMetrics(final FileSystem fs, final MiniDFSCluster cluster, final Path fileName) throws IOException {	List<DataNode> datanodes = cluster.getDataNodes();	DataNode datanode = datanodes.get(0);	final ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, fileName);	final FsVolumeSpi volume = datanode.getFSDataset().getVolume(block);	DataNodeVolumeMetrics metrics = volume.getMetrics();	MetricsRecordBuilder rb = getMetrics(volume.getMetrics().name());	assertCounter("TotalDataFileIos", metrics.getTotalDataFileIos(), rb);	
totalmetadataoperations 

private void verifyDataNodeVolumeMetrics(final FileSystem fs, final MiniDFSCluster cluster, final Path fileName) throws IOException {	List<DataNode> datanodes = cluster.getDataNodes();	DataNode datanode = datanodes.get(0);	final ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, fileName);	final FsVolumeSpi volume = datanode.getFSDataset().getVolume(block);	DataNodeVolumeMetrics metrics = volume.getMetrics();	MetricsRecordBuilder rb = getMetrics(volume.getMetrics().name());	assertCounter("TotalDataFileIos", metrics.getTotalDataFileIos(), rb);	
totaldatafileios 

private void verifyDataNodeVolumeMetrics(final FileSystem fs, final MiniDFSCluster cluster, final Path fileName) throws IOException {	List<DataNode> datanodes = cluster.getDataNodes();	DataNode datanode = datanodes.get(0);	final ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, fileName);	final FsVolumeSpi volume = datanode.getFSDataset().getVolume(block);	DataNodeVolumeMetrics metrics = volume.getMetrics();	MetricsRecordBuilder rb = getMetrics(volume.getMetrics().name());	assertCounter("TotalDataFileIos", metrics.getTotalDataFileIos(), rb);	
totalfileioerrors 

private void verifyDataNodeVolumeMetrics(final FileSystem fs, final MiniDFSCluster cluster, final Path fileName) throws IOException {	List<DataNode> datanodes = cluster.getDataNodes();	DataNode datanode = datanodes.get(0);	final ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, fileName);	final FsVolumeSpi volume = datanode.getFSDataset().getVolume(block);	DataNodeVolumeMetrics metrics = volume.getMetrics();	MetricsRecordBuilder rb = getMetrics(volume.getMetrics().name());	assertCounter("TotalDataFileIos", metrics.getTotalDataFileIos(), rb);	
metadataoperationsamplecount 

private void verifyDataNodeVolumeMetrics(final FileSystem fs, final MiniDFSCluster cluster, final Path fileName) throws IOException {	List<DataNode> datanodes = cluster.getDataNodes();	DataNode datanode = datanodes.get(0);	final ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, fileName);	final FsVolumeSpi volume = datanode.getFSDataset().getVolume(block);	DataNodeVolumeMetrics metrics = volume.getMetrics();	MetricsRecordBuilder rb = getMetrics(volume.getMetrics().name());	assertCounter("TotalDataFileIos", metrics.getTotalDataFileIos(), rb);	
metadataoperationmean 

private void verifyDataNodeVolumeMetrics(final FileSystem fs, final MiniDFSCluster cluster, final Path fileName) throws IOException {	List<DataNode> datanodes = cluster.getDataNodes();	DataNode datanode = datanodes.get(0);	final ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, fileName);	final FsVolumeSpi volume = datanode.getFSDataset().getVolume(block);	DataNodeVolumeMetrics metrics = volume.getMetrics();	MetricsRecordBuilder rb = getMetrics(volume.getMetrics().name());	assertCounter("TotalDataFileIos", metrics.getTotalDataFileIos(), rb);	
metadatafileiostddev 

private void verifyDataNodeVolumeMetrics(final FileSystem fs, final MiniDFSCluster cluster, final Path fileName) throws IOException {	List<DataNode> datanodes = cluster.getDataNodes();	DataNode datanode = datanodes.get(0);	final ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, fileName);	final FsVolumeSpi volume = datanode.getFSDataset().getVolume(block);	DataNodeVolumeMetrics metrics = volume.getMetrics();	MetricsRecordBuilder rb = getMetrics(volume.getMetrics().name());	assertCounter("TotalDataFileIos", metrics.getTotalDataFileIos(), rb);	
datafileiosamplecount 

private void verifyDataNodeVolumeMetrics(final FileSystem fs, final MiniDFSCluster cluster, final Path fileName) throws IOException {	List<DataNode> datanodes = cluster.getDataNodes();	DataNode datanode = datanodes.get(0);	final ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, fileName);	final FsVolumeSpi volume = datanode.getFSDataset().getVolume(block);	DataNodeVolumeMetrics metrics = volume.getMetrics();	MetricsRecordBuilder rb = getMetrics(volume.getMetrics().name());	assertCounter("TotalDataFileIos", metrics.getTotalDataFileIos(), rb);	
datafileiomean 

private void verifyDataNodeVolumeMetrics(final FileSystem fs, final MiniDFSCluster cluster, final Path fileName) throws IOException {	List<DataNode> datanodes = cluster.getDataNodes();	DataNode datanode = datanodes.get(0);	final ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, fileName);	final FsVolumeSpi volume = datanode.getFSDataset().getVolume(block);	DataNodeVolumeMetrics metrics = volume.getMetrics();	MetricsRecordBuilder rb = getMetrics(volume.getMetrics().name());	assertCounter("TotalDataFileIos", metrics.getTotalDataFileIos(), rb);	
datafileiostddev 

private void verifyDataNodeVolumeMetrics(final FileSystem fs, final MiniDFSCluster cluster, final Path fileName) throws IOException {	List<DataNode> datanodes = cluster.getDataNodes();	DataNode datanode = datanodes.get(0);	final ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, fileName);	final FsVolumeSpi volume = datanode.getFSDataset().getVolume(block);	DataNodeVolumeMetrics metrics = volume.getMetrics();	MetricsRecordBuilder rb = getMetrics(volume.getMetrics().name());	assertCounter("TotalDataFileIos", metrics.getTotalDataFileIos(), rb);	
flushiosamplecount 

private void verifyDataNodeVolumeMetrics(final FileSystem fs, final MiniDFSCluster cluster, final Path fileName) throws IOException {	List<DataNode> datanodes = cluster.getDataNodes();	DataNode datanode = datanodes.get(0);	final ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, fileName);	final FsVolumeSpi volume = datanode.getFSDataset().getVolume(block);	DataNodeVolumeMetrics metrics = volume.getMetrics();	MetricsRecordBuilder rb = getMetrics(volume.getMetrics().name());	assertCounter("TotalDataFileIos", metrics.getTotalDataFileIos(), rb);	
flushiomean 

private void verifyDataNodeVolumeMetrics(final FileSystem fs, final MiniDFSCluster cluster, final Path fileName) throws IOException {	List<DataNode> datanodes = cluster.getDataNodes();	DataNode datanode = datanodes.get(0);	final ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, fileName);	final FsVolumeSpi volume = datanode.getFSDataset().getVolume(block);	DataNodeVolumeMetrics metrics = volume.getMetrics();	MetricsRecordBuilder rb = getMetrics(volume.getMetrics().name());	assertCounter("TotalDataFileIos", metrics.getTotalDataFileIos(), rb);	
flushiostddev 

private void verifyDataNodeVolumeMetrics(final FileSystem fs, final MiniDFSCluster cluster, final Path fileName) throws IOException {	List<DataNode> datanodes = cluster.getDataNodes();	DataNode datanode = datanodes.get(0);	final ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, fileName);	final FsVolumeSpi volume = datanode.getFSDataset().getVolume(block);	DataNodeVolumeMetrics metrics = volume.getMetrics();	MetricsRecordBuilder rb = getMetrics(volume.getMetrics().name());	assertCounter("TotalDataFileIos", metrics.getTotalDataFileIos(), rb);	
synciosamplecount 

private void verifyDataNodeVolumeMetrics(final FileSystem fs, final MiniDFSCluster cluster, final Path fileName) throws IOException {	List<DataNode> datanodes = cluster.getDataNodes();	DataNode datanode = datanodes.get(0);	final ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, fileName);	final FsVolumeSpi volume = datanode.getFSDataset().getVolume(block);	DataNodeVolumeMetrics metrics = volume.getMetrics();	MetricsRecordBuilder rb = getMetrics(volume.getMetrics().name());	assertCounter("TotalDataFileIos", metrics.getTotalDataFileIos(), rb);	
synciomean 

private void verifyDataNodeVolumeMetrics(final FileSystem fs, final MiniDFSCluster cluster, final Path fileName) throws IOException {	List<DataNode> datanodes = cluster.getDataNodes();	DataNode datanode = datanodes.get(0);	final ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, fileName);	final FsVolumeSpi volume = datanode.getFSDataset().getVolume(block);	DataNodeVolumeMetrics metrics = volume.getMetrics();	MetricsRecordBuilder rb = getMetrics(volume.getMetrics().name());	assertCounter("TotalDataFileIos", metrics.getTotalDataFileIos(), rb);	
synciostddev 

private void verifyDataNodeVolumeMetrics(final FileSystem fs, final MiniDFSCluster cluster, final Path fileName) throws IOException {	List<DataNode> datanodes = cluster.getDataNodes();	DataNode datanode = datanodes.get(0);	final ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, fileName);	final FsVolumeSpi volume = datanode.getFSDataset().getVolume(block);	DataNodeVolumeMetrics metrics = volume.getMetrics();	MetricsRecordBuilder rb = getMetrics(volume.getMetrics().name());	assertCounter("TotalDataFileIos", metrics.getTotalDataFileIos(), rb);	
readiosamplecount 

private void verifyDataNodeVolumeMetrics(final FileSystem fs, final MiniDFSCluster cluster, final Path fileName) throws IOException {	List<DataNode> datanodes = cluster.getDataNodes();	DataNode datanode = datanodes.get(0);	final ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, fileName);	final FsVolumeSpi volume = datanode.getFSDataset().getVolume(block);	DataNodeVolumeMetrics metrics = volume.getMetrics();	MetricsRecordBuilder rb = getMetrics(volume.getMetrics().name());	assertCounter("TotalDataFileIos", metrics.getTotalDataFileIos(), rb);	
readiomean 

private void verifyDataNodeVolumeMetrics(final FileSystem fs, final MiniDFSCluster cluster, final Path fileName) throws IOException {	List<DataNode> datanodes = cluster.getDataNodes();	DataNode datanode = datanodes.get(0);	final ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, fileName);	final FsVolumeSpi volume = datanode.getFSDataset().getVolume(block);	DataNodeVolumeMetrics metrics = volume.getMetrics();	MetricsRecordBuilder rb = getMetrics(volume.getMetrics().name());	assertCounter("TotalDataFileIos", metrics.getTotalDataFileIos(), rb);	
readiostddev 

private void verifyDataNodeVolumeMetrics(final FileSystem fs, final MiniDFSCluster cluster, final Path fileName) throws IOException {	List<DataNode> datanodes = cluster.getDataNodes();	DataNode datanode = datanodes.get(0);	final ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, fileName);	final FsVolumeSpi volume = datanode.getFSDataset().getVolume(block);	DataNodeVolumeMetrics metrics = volume.getMetrics();	MetricsRecordBuilder rb = getMetrics(volume.getMetrics().name());	assertCounter("TotalDataFileIos", metrics.getTotalDataFileIos(), rb);	
writeiosamplecount 

private void verifyDataNodeVolumeMetrics(final FileSystem fs, final MiniDFSCluster cluster, final Path fileName) throws IOException {	List<DataNode> datanodes = cluster.getDataNodes();	DataNode datanode = datanodes.get(0);	final ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, fileName);	final FsVolumeSpi volume = datanode.getFSDataset().getVolume(block);	DataNodeVolumeMetrics metrics = volume.getMetrics();	MetricsRecordBuilder rb = getMetrics(volume.getMetrics().name());	assertCounter("TotalDataFileIos", metrics.getTotalDataFileIos(), rb);	
writeiomean 

private void verifyDataNodeVolumeMetrics(final FileSystem fs, final MiniDFSCluster cluster, final Path fileName) throws IOException {	List<DataNode> datanodes = cluster.getDataNodes();	DataNode datanode = datanodes.get(0);	final ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, fileName);	final FsVolumeSpi volume = datanode.getFSDataset().getVolume(block);	DataNodeVolumeMetrics metrics = volume.getMetrics();	MetricsRecordBuilder rb = getMetrics(volume.getMetrics().name());	assertCounter("TotalDataFileIos", metrics.getTotalDataFileIos(), rb);	
writeiostddev 

private void verifyDataNodeVolumeMetrics(final FileSystem fs, final MiniDFSCluster cluster, final Path fileName) throws IOException {	List<DataNode> datanodes = cluster.getDataNodes();	DataNode datanode = datanodes.get(0);	final ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, fileName);	final FsVolumeSpi volume = datanode.getFSDataset().getVolume(block);	DataNodeVolumeMetrics metrics = volume.getMetrics();	MetricsRecordBuilder rb = getMetrics(volume.getMetrics().name());	assertCounter("TotalDataFileIos", metrics.getTotalDataFileIos(), rb);	
fileioerrorsamplecount 

private void verifyDataNodeVolumeMetrics(final FileSystem fs, final MiniDFSCluster cluster, final Path fileName) throws IOException {	List<DataNode> datanodes = cluster.getDataNodes();	DataNode datanode = datanodes.get(0);	final ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, fileName);	final FsVolumeSpi volume = datanode.getFSDataset().getVolume(block);	DataNodeVolumeMetrics metrics = volume.getMetrics();	MetricsRecordBuilder rb = getMetrics(volume.getMetrics().name());	assertCounter("TotalDataFileIos", metrics.getTotalDataFileIos(), rb);	
fileioerrormean 

private void verifyDataNodeVolumeMetrics(final FileSystem fs, final MiniDFSCluster cluster, final Path fileName) throws IOException {	List<DataNode> datanodes = cluster.getDataNodes();	DataNode datanode = datanodes.get(0);	final ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, fileName);	final FsVolumeSpi volume = datanode.getFSDataset().getVolume(block);	DataNodeVolumeMetrics metrics = volume.getMetrics();	MetricsRecordBuilder rb = getMetrics(volume.getMetrics().name());	assertCounter("TotalDataFileIos", metrics.getTotalDataFileIos(), rb);	
fileioerrorstddev 

========================= hadoop sample_7287 =========================

public long getDirecorySize() {	try {	if (!isShared() && root != null && root.exists()) {	return FileUtils.sizeOfDirectory(root);	}	} catch (Exception e) {	
failed to get directory size 

public void clearDirectory() throws IOException {	File curDir = this.getCurrentDir();	if (curDir.exists()) {	File[] files = FileUtil.listFiles(curDir);	
will remove files 

public StorageState analyzeStorage(StartupOption startOpt, Storage storage, boolean checkCurrentIsEmpty) throws IOException {	assert root != null : "root is null";	boolean hadMkdirs = false;	String rootPath = root.getCanonicalPath();	try {	if (!root.exists()) {	if (startOpt != StartupOption.FORMAT && startOpt != StartupOption.HOTSWAP) {	
storage directory does not exist 

public StorageState analyzeStorage(StartupOption startOpt, Storage storage, boolean checkCurrentIsEmpty) throws IOException {	assert root != null : "root is null";	boolean hadMkdirs = false;	String rootPath = root.getCanonicalPath();	try {	if (!root.exists()) {	if (startOpt != StartupOption.FORMAT && startOpt != StartupOption.HOTSWAP) {	return StorageState.NON_EXISTENT;	}	
does not exist creating 

String rootPath = root.getCanonicalPath();	try {	if (!root.exists()) {	if (startOpt != StartupOption.FORMAT && startOpt != StartupOption.HOTSWAP) {	return StorageState.NON_EXISTENT;	}	if (!root.mkdirs()) throw new IOException("Cannot create directory " + rootPath);	hadMkdirs = true;	}	if (!root.isDirectory()) {	
is not a directory 

if (startOpt != StartupOption.FORMAT && startOpt != StartupOption.HOTSWAP) {	return StorageState.NON_EXISTENT;	}	if (!root.mkdirs()) throw new IOException("Cannot create directory " + rootPath);	hadMkdirs = true;	}	if (!root.isDirectory()) {	return StorageState.NON_EXISTENT;	}	if (!FileUtil.canWrite(root)) {	
cannot access storage directory 

if (!root.mkdirs()) throw new IOException("Cannot create directory " + rootPath);	hadMkdirs = true;	}	if (!root.isDirectory()) {	return StorageState.NON_EXISTENT;	}	if (!FileUtil.canWrite(root)) {	return StorageState.NON_EXISTENT;	}	} catch(SecurityException ex) {	
cannot access storage directory 

public void doRecover(StorageState curState) throws IOException {	File curDir = getCurrentDir();	String rootPath = root.getCanonicalPath();	switch(curState) {	
completing previous upgrade for storage directory 

public void doRecover(StorageState curState) throws IOException {	File curDir = getCurrentDir();	String rootPath = root.getCanonicalPath();	switch(curState) {	rename(getPreviousTmp(), getPreviousDir());	return;	
recovering storage directory from previous upgrade 

public void doRecover(StorageState curState) throws IOException {	File curDir = getCurrentDir();	String rootPath = root.getCanonicalPath();	switch(curState) {	rename(getPreviousTmp(), getPreviousDir());	return;	if (curDir.exists()) deleteDir(curDir);	rename(getPreviousTmp(), curDir);	return;	
completing previous rollback for storage directory 

File curDir = getCurrentDir();	String rootPath = root.getCanonicalPath();	switch(curState) {	rename(getPreviousTmp(), getPreviousDir());	return;	if (curDir.exists()) deleteDir(curDir);	rename(getPreviousTmp(), curDir);	return;	deleteDir(getRemovedTmp());	return;	
recovering storage directory from previous rollback 

switch(curState) {	rename(getPreviousTmp(), getPreviousDir());	return;	if (curDir.exists()) deleteDir(curDir);	rename(getPreviousTmp(), curDir);	return;	deleteDir(getRemovedTmp());	return;	rename(getRemovedTmp(), curDir);	return;	
completing previous finalize for storage directory 

return;	if (curDir.exists()) deleteDir(curDir);	rename(getPreviousTmp(), curDir);	return;	deleteDir(getRemovedTmp());	return;	rename(getRemovedTmp(), curDir);	return;	deleteDir(getFinalizedTmp());	return;	
completing previous checkpoint for storage directory 

deleteDir(getRemovedTmp());	return;	rename(getRemovedTmp(), curDir);	return;	deleteDir(getFinalizedTmp());	return;	File prevCkptDir = getPreviousCheckpoint();	if (prevCkptDir.exists()) deleteDir(prevCkptDir);	rename(getLastCheckpointTmp(), prevCkptDir);	return;	
recovering storage directory from failed checkpoint 

public void lock() throws IOException {	if (isShared()) {	
locking is disabled for 

NativeIO.copyFileUnbuffered(srcFile, destFile);	} catch (NativeIOException e) {	throw new IOException("Failed to copy " + srcFile.getCanonicalPath() + " to " + destFile.getCanonicalPath() + " due to failure in NativeIO#copyFileUnbuffered(). " + e.toString());	}	if (srcFile.length() != destFile.length()) {	throw new IOException("Failed to copy full contents from '" + srcFile + "' to '" + destFile + "'");	}	if (preserveFileDate) {	if (destFile.setLastModified(srcFile.lastModified()) == false) {	if (LOG.isDebugEnabled()) {	
failed to preserve last modified date from to 

========================= hadoop sample_8172 =========================

Matcher m = JOB_ID_PARSER.matcher(fileName);	if (!m.matches()) {	continue;	}	String jobId = m.group(1);	int lastId = Integer.parseInt(m.group(2));	int mod = lastId % size;	if (mod != i) {	continue;	}	
this mapper will process file 

private void setFilePath(String fileName, Path path, JobFiles jobFiles) {	FileType type = getFileType(fileName);	switch (type) {	case JOB_HISTORY_FILE: if (jobFiles.getJobHistoryFilePath() == null) {	jobFiles.setJobHistoryFilePath(path);	} else {	
we already have the job history file skipping 

FileType type = getFileType(fileName);	switch (type) {	case JOB_HISTORY_FILE: if (jobFiles.getJobHistoryFilePath() == null) {	jobFiles.setJobHistoryFilePath(path);	} else {	}	break;	case JOB_CONF_FILE: if (jobFiles.getJobConfFilePath() == null) {	jobFiles.setJobConfFilePath(path);	} else {	
we already have the job conf file skipping 

case JOB_HISTORY_FILE: if (jobFiles.getJobHistoryFilePath() == null) {	jobFiles.setJobHistoryFilePath(path);	} else {	}	break;	case JOB_CONF_FILE: if (jobFiles.getJobConfFilePath() == null) {	jobFiles.setJobConfFilePath(path);	} else {	}	break;	
unknown type 

========================= hadoop sample_5605 =========================

public void init(String contextName, ContextFactory factory) {	super.init(contextName, factory);	
initializing the for ganglia metrics 

protected void emitMetric(String name, String type,  String value) throws IOException {	if (name == null) {	
metric was emitted with no name 

protected void emitMetric(String name, String type,  String value) throws IOException {	if (name == null) {	return;	} else if (value == null) {	
metric name was emitted with a null value 

protected void emitMetric(String name, String type,  String value) throws IOException {	if (name == null) {	return;	} else if (value == null) {	return;	} else if (type == null) {	
metric name value has no type 

protected void emitMetric(String name, String type,  String value) throws IOException {	if (name == null) {	return;	} else if (value == null) {	return;	} else if (type == null) {	return;	}	if (LOG.isDebugEnabled()) {	
emitting metric type value from hostname 

========================= hadoop sample_3381 =========================

pingRequest = buf.toByteArray();	}	this.pingInterval = remoteId.getPingInterval();	if (rpcTimeout > 0) {	this.soTimeout = (doPing && pingInterval < rpcTimeout) ? pingInterval : rpcTimeout;	} else {	this.soTimeout = pingInterval;	}	this.serviceClass = serviceClass;	if (LOG.isDebugEnabled()) {	
the ping interval is ms 

private synchronized boolean updateAddress() throws IOException {	InetSocketAddress currentAddr = NetUtils.createSocketAddrForHost( server.getHostName(), server.getPort());	if (!server.equals(currentAddr)) {	
address change detected old new 

InetSocketAddress bindAddr = null;	if (ticket != null && ticket.hasKerberosCredentials()) {	KerberosInfo krbInfo = remoteId.getProtocol().getAnnotation(KerberosInfo.class);	if (krbInfo != null) {	String principal = ticket.getUserName();	String host = SecurityUtil.getHostFromPrincipal(principal);	InetAddress localAddr = NetUtils.getLocalInetAddress(host);	if (localAddr != null) {	this.socket.setReuseAddress(true);	if (LOG.isDebugEnabled()) {	
binding to 

private synchronized void handleSaslConnectionFailure( final int currRetries, final int maxRetries, final Exception ex, final Random rand, final UserGroupInformation ugi) throws IOException, InterruptedException {	ugi.doAs(new PrivilegedExceptionAction<Object>() {	public Object run() throws IOException, InterruptedException {	final short MAX_BACKOFF = 5000;	closeConnection();	disposeSasl();	if (shouldAuthenticateOverKrb()) {	if (currRetries < maxRetries) {	if(LOG.isDebugEnabled()) {	
exception encountered while connecting to the server 

UserGroupInformation.getLoginUser().reloginFromTicketCache();	}	Thread.sleep((rand.nextInt(MAX_BACKOFF) + 1));	return null;	} else {	String msg = "Couldn't setup connection for " + UserGroupInformation.getLoginUser().getUserName() + " to " + remoteId;	LOG.warn(msg, ex);	throw (IOException) new IOException(msg).initCause(ex);	}	} else {	
exception encountered while connecting to the server 

}	UserGroupInformation ticket = remoteId.getTicket();	if (ticket != null) {	final UserGroupInformation realUser = ticket.getRealUser();	if (realUser != null) {	ticket = realUser;	}	}	try {	if (LOG.isDebugEnabled()) {	
connecting to 

authMethod = saslRpcClient.getAuthMethod();	if (rand == null) {	rand = new Random();	}	handleSaslConnectionFailure(numRetries++, maxRetriesOnSasl, ex, rand, ticket);	continue;	}	if (authMethod != AuthMethod.SIMPLE) {	ipcStreams.setSaslClient(saslRpcClient);	remoteId.saslQop = (String)saslRpcClient.getNegotiatedProperty(Sasl.QOP);	
negotiated qop is 

private void closeConnection() {	if (socket == null) {	return;	}	try {	socket.close();	} catch (IOException e) {	
not able to close a socket 

closeConnection();	final RetryAction action;	try {	action = connectionRetryPolicy.shouldRetry(ioe, curRetries, 0, true);	} catch(Exception e) {	throw e instanceof IOException? (IOException)e: new IOException(e);	}	if (action.action == RetryAction.RetryDecision.FAIL) {	if (action.reason != null) {	if (LOG.isDebugEnabled()) {	
failed to connect to server 

throw e instanceof IOException? (IOException)e: new IOException(e);	}	if (action.action == RetryAction.RetryDecision.FAIL) {	if (action.reason != null) {	if (LOG.isDebugEnabled()) {	}	}	throw ioe;	}	if (Thread.currentThread().isInterrupted()) {	
interrupted while trying for connection 

throw ioe;	}	if (Thread.currentThread().isInterrupted()) {	throw ioe;	}	try {	Thread.sleep(action.delayMillis);	} catch (InterruptedException e) {	throw (IOException)new InterruptedIOException("Interrupted: action=" + action + ", retry policy=" + connectionRetryPolicy).initCause(e);	}	
retrying connect to server already tried time s retry policy is 

public void run() {	
starting having connections 

public void run() {	try {	while (waitForWork()) {	receiveRpcResponse();	}	} catch (Throwable t) {	
unexpected error reading responses on connection 

public void run() {	try {	while (waitForWork()) {	receiveRpcResponse();	}	} catch (Throwable t) {	markClosed(new IOException("Error reading responses", t));	}	close();	
stopped remaining connections 

RpcWritable.wrap(call.rpcRequest).writeTo(buf);	synchronized (sendRpcRequestLock) {	Future<?> senderFuture = sendParamsExecutor.submit(new Runnable() {	public void run() {	try {	synchronized (ipcStreams.out) {	if (shouldCloseConnection.get()) {	return;	}	if (LOG.isDebugEnabled()) {	
sending 

if (shouldCloseConnection.get()) {	return;	}	touch();	try {	ByteBuffer bb = ipcStreams.readResponse();	RpcWritable.Buffer packet = RpcWritable.Buffer.wrap(bb);	RpcResponseHeaderProto header = packet.getValue(RpcResponseHeaderProto.getDefaultInstance());	checkResponse(header);	int callId = header.getCallId();	
got value 

call.setRpcResponse(value);	}	if (packet.remaining() > 0) {	throw new RpcClientException("RPC response length mismatch");	}	if (status != RpcStatusProto.SUCCESS) {	final String exceptionClassName = header.hasExceptionClassName() ? header.getExceptionClassName() : "ServerDidNotSetExceptionClassName";	final String errorMsg = header.hasErrorMsg() ? header.getErrorMsg() : "ServerDidNotSetErrorMsg" ;	final RpcErrorCodeProto erCode = (header.hasErrorDetail() ? header.getErrorDetail() : null);	if (erCode == null) {	
detailed error code not set by server on rpc error 

private synchronized void close() {	if (!shouldCloseConnection.get()) {	
the connection is not in the closed state 

private synchronized void close() {	if (!shouldCloseConnection.get()) {	return;	}	connections.remove(remoteId, this);	IOUtils.closeStream(ipcStreams);	disposeSasl();	if (closeException == null) {	if (!calls.isEmpty()) {	
a connection is closed for no cause and calls are not empty 

connections.remove(remoteId, this);	IOUtils.closeStream(ipcStreams);	disposeSasl();	if (closeException == null) {	if (!calls.isEmpty()) {	closeException = new IOException("Unexpected closed connection");	cleanupCalls();	}	} else {	if (LOG.isDebugEnabled()) {	
closing ipc connection to 

if (!calls.isEmpty()) {	closeException = new IOException("Unexpected closed connection");	cleanupCalls();	}	} else {	if (LOG.isDebugEnabled()) {	}	cleanupCalls();	}	closeConnection();	
closed 

public void stop() {	if (LOG.isDebugEnabled()) {	
stopping client 

========================= hadoop sample_4014 =========================

public synchronized List<String> getGroups(String user) throws IOException {	Set<String> groupSet = new TreeSet<String>();	List<String> groups = null;	for (GroupMappingServiceProvider provider : providersList) {	try {	groups = provider.getGroups(user);	} catch (Exception e) {	
unable to get groups for user via because 

public synchronized List<String> getGroups(String user) throws IOException {	Set<String> groupSet = new TreeSet<String>();	List<String> groups = null;	for (GroupMappingServiceProvider provider : providersList) {	try {	groups = provider.getGroups(user);	} catch (Exception e) {	
stacktrace 

private void loadMappingProviders() {	String[] providerNames = conf.getStrings(MAPPING_PROVIDERS_CONFIG_KEY, new String[]{});	String providerKey;	for (String name : providerNames) {	providerKey = MAPPING_PROVIDER_CONFIG_PREFIX + "." + name;	Class<?> providerClass = conf.getClass(providerKey, null);	if (providerClass == null) {	
the mapping provider does not have a valid class 

========================= hadoop sample_3801 =========================

public ReplicaAccessor build() {	if (visibleLength < 1024) {	
syntheticreplicaaccessorfactory returning null for a smaller replica with size 

========================= hadoop sample_7141 =========================

public void testDSShellWithNodeLabelExpression() throws Exception {	initializeNodeLabels();	NMContainerMonitor mon = new NMContainerMonitor();	Thread t = new Thread(mon);	t.start();	String[] args = {	"--jar", TestDistributedShell.APPMASTER_JAR, "--num_containers", "4", "--shell_command", "sleep", "--shell_args", "15", "--master_memory", "512", "--master_vcores", "2", "--container_memory", "128", "--container_vcores", "1", "--node_label_expression", "x" };	
initializing ds client 

public void testDSShellWithNodeLabelExpression() throws Exception {	initializeNodeLabels();	NMContainerMonitor mon = new NMContainerMonitor();	Thread t = new Thread(mon);	t.start();	String[] args = {	"--jar", TestDistributedShell.APPMASTER_JAR, "--num_containers", "4", "--shell_command", "sleep", "--shell_args", "15", "--master_memory", "512", "--master_vcores", "2", "--container_memory", "128", "--container_vcores", "1", "--node_label_expression", "x" };	final Client client = new Client(new Configuration(distShellTest.yarnCluster.getConfig()));	boolean initSuccess = client.init(args);	Assert.assertTrue(initSuccess);	
running ds client 

========================= hadoop sample_2 =========================

public void teardown() {	if (cluster != null) {	try {	cluster.stop();	} catch (Exception e) {	
when stopping the cluster 

public void testAutoFailoverOnBadHealth() throws Exception {	cluster.start();	DummyHAService svc1 = cluster.getService(1);	
faking unhealthy should failover to 

public void testAutoFailoverOnBadHealth() throws Exception {	cluster.start();	DummyHAService svc1 = cluster.getService(1);	cluster.setHealthy(0, false);	
waiting for to enter initializing state 

public void testAutoFailoverOnBadHealth() throws Exception {	cluster.start();	DummyHAService svc1 = cluster.getService(1);	cluster.setHealthy(0, false);	cluster.waitForHAState(0, HAServiceState.INITIALIZING);	cluster.waitForHAState(1, HAServiceState.ACTIVE);	
allowing to be healthy again making unreachable and fail to gracefully go to standby 

public void testAutoFailoverOnBadState() throws Exception {	cluster.start();	DummyHAService svc0 = cluster.getService(0);	
faking to change the state should failover to 

public void testDontFailoverToUnhealthyNode() throws Exception {	cluster.start();	cluster.setHealthy(1, false);	cluster.waitForHealthState(1, HealthMonitor.State.SERVICE_UNHEALTHY);	cluster.getElector(0).preventSessionReestablishmentForTests();	try {	cluster.expireActiveLockHolder(0);	
expired s zk session waiting a second to give a chance to take the lock if it is ever going to 

public void testDontFailoverToUnhealthyNode() throws Exception {	cluster.start();	cluster.setHealthy(1, false);	cluster.waitForHealthState(1, HealthMonitor.State.SERVICE_UNHEALTHY);	cluster.getElector(0).preventSessionReestablishmentForTests();	try {	cluster.expireActiveLockHolder(0);	Thread.sleep(1000);	cluster.waitForActiveLockHolder(null);	} finally {	
allowing s elector to re establish its connection 

public void testBecomingActiveFails() throws Exception {	cluster.start();	DummyHAService svc1 = cluster.getService(1);	
making fail to become active 

public void testBecomingActiveFails() throws Exception {	cluster.start();	DummyHAService svc1 = cluster.getService(1);	cluster.setFailToBecomeActive(1, true);	
faking unhealthy should not successfully failover to 

public void testBecomingActiveFails() throws Exception {	cluster.start();	DummyHAService svc1 = cluster.getService(1);	cluster.setFailToBecomeActive(1, true);	cluster.setHealthy(0, false);	cluster.waitForHealthState(0, State.SERVICE_UNHEALTHY);	cluster.waitForActiveLockHolder(null);	Mockito.verify(svc1.proxy, Mockito.timeout(2000).atLeastOnce()) .transitionToActive(Mockito.<StateChangeRequestInfo>any());	cluster.waitForHAState(0, HAServiceState.INITIALIZING);	cluster.waitForHAState(1, HAServiceState.STANDBY);	
faking healthy again should go back to 

cluster.setHealthy(0, false);	cluster.waitForHealthState(0, State.SERVICE_UNHEALTHY);	cluster.waitForActiveLockHolder(null);	Mockito.verify(svc1.proxy, Mockito.timeout(2000).atLeastOnce()) .transitionToActive(Mockito.<StateChangeRequestInfo>any());	cluster.waitForHAState(0, HAServiceState.INITIALIZING);	cluster.waitForHAState(1, HAServiceState.STANDBY);	cluster.setHealthy(0, true);	cluster.waitForHAState(0, HAServiceState.ACTIVE);	cluster.waitForHAState(1, HAServiceState.STANDBY);	cluster.waitForActiveLockHolder(0);	
allowing to become active expiring 

========================= hadoop sample_3102 =========================

public void testBlocksClosed() throws Throwable {	Path dest = path("testBlocksClosed");	describe(" testBlocksClosed");	FSDataOutputStream stream = getFileSystem().create(dest, true);	S3AInstrumentation.OutputStreamStatistics statistics = S3ATestUtils.getOutputStreamStatistics(stream);	byte[] data = ContractTestUtils.dataset(16, 'a', 26);	stream.write(data);	
closing output stream 

public void testBlocksClosed() throws Throwable {	Path dest = path("testBlocksClosed");	describe(" testBlocksClosed");	FSDataOutputStream stream = getFileSystem().create(dest, true);	S3AInstrumentation.OutputStreamStatistics statistics = S3ATestUtils.getOutputStreamStatistics(stream);	byte[] data = ContractTestUtils.dataset(16, 'a', 26);	stream.write(data);	stream.close();	assertEquals("total allocated blocks in " + statistics, 1, statistics.blocksAllocated());	assertEquals("actively allocated blocks in " + statistics, 0, statistics.blocksActivelyAllocated());	
end of test case 

========================= hadoop sample_5890 =========================

try {	DatanodeID bpReg = new DatanodeID( datanode.getBPOfferService(bpid).bpRegistration);	InterDatanodeProtocol proxyDN = bpReg.equals(id)? datanode: DataNode.createInterDataNodeProtocolProxy(id, conf, dnConf.socketTimeout, dnConf.connectToDnViaHostname);	ReplicaRecoveryInfo info = callInitReplicaRecovery(proxyDN, rBlock);	if (info != null && info.getGenerationStamp() >= block.getGenerationStamp() && info.getNumBytes() > 0) {	++candidateReplicaCnt;	if (info.getOriginalReplicaState().getValue() <= ReplicaState.RWR.getValue()) {	syncList.add(new BlockRecord(id, proxyDN, info));	} else {	if (LOG.isDebugEnabled()) {	
block recovery ignored replica with invalid original state from datanode 

++candidateReplicaCnt;	if (info.getOriginalReplicaState().getValue() <= ReplicaState.RWR.getValue()) {	syncList.add(new BlockRecord(id, proxyDN, info));	} else {	if (LOG.isDebugEnabled()) {	}	}	} else {	if (LOG.isDebugEnabled()) {	if (info == null) {	
block recovery datanode does not have replica for block 

if (info.getOriginalReplicaState().getValue() <= ReplicaState.RWR.getValue()) {	syncList.add(new BlockRecord(id, proxyDN, info));	} else {	if (LOG.isDebugEnabled()) {	}	}	} else {	if (LOG.isDebugEnabled()) {	if (info == null) {	} else {	
block recovery ignored replica with invalid generation stamp or length from datanode 

public Daemon recoverBlocks(final String who, final Collection<RecoveringBlock> blocks) {	Daemon d = new Daemon(datanode.threadGroup, new Runnable() {	public void run() {	for(RecoveringBlock b : blocks) {	try {	logRecoverBlock(who, b);	RecoveryTaskContiguous task = new RecoveryTaskContiguous(b);	task.recover();	} catch (IOException e) {	
recoverblocks failed 

========================= hadoop sample_7907 =========================

public MergeQueue(Configuration conf, FileSystem fs, Path[] inputs, boolean deleteInputs, CompressionCodec codec, RawComparator<K> comparator, Progressable reporter, Counters.Counter mergedMapOutputsCounter, TaskType taskType) throws IOException {	this.conf = conf;	this.fs = fs;	this.codec = codec;	this.comparator = comparator;	this.reporter = reporter;	if (taskType == TaskType.MAP) {	considerFinalMergeForProgress();	}	for (Path file : inputs) {	
mergeq adding 

========================= hadoop sample_4628 =========================

MiniDFSNNTopology topology = new MiniDFSNNTopology() .addNameservice(new MiniDFSNNTopology.NSConf("ns1") .addNN(new MiniDFSNNTopology.NNConf("nn1").setHttpPort(basePort)) .addNN(new MiniDFSNNTopology.NNConf("nn2").setHttpPort(basePort + 1)));	cluster = new MiniDFSCluster.Builder(conf) .nnTopology(topology) .numDataNodes(1) .build();	cluster.waitActive();	nn0 = cluster.getNameNode(0);	nn1 = cluster.getNameNode(1);	fs = HATestUtil.configureFailoverFs(cluster, conf);	cluster.transitionToActive(0);	++retryCount;	break;	} catch (BindException e) {	
set up minidfscluster failed due to port conflicts retry times 

========================= hadoop sample_7454 =========================

protected static <T> T genericForward( final String webApp, final HttpServletRequest hsr, final Class<T> returnType, final HTTPMethods method, final String targetPath, final Object formParam, final Map<String, String[]> additionalParam) {	UserGroupInformation callerUGI = null;	if (hsr != null) {	callerUGI = RMWebAppUtil.getCallerUserGroupInformation(hsr, true);	} else {	callerUGI = UserGroupInformation.createRemoteUser(user);	}	if (callerUGI == null) {	
unable to obtain user name user not authenticated 

if (Response.class.equals(returnType)) {	return (T) RouterWebServiceUtil.clientResponseToResponse(response);	}	if (response.getStatus() == SC_OK) {	return response.getEntity(returnType);	}	if (response.getStatus() == SC_NO_CONTENT) {	try {	return returnType.getConstructor().newInstance();	} catch (RuntimeException | ReflectiveOperationException e) {	
cannot create empty entity for 

========================= hadoop sample_1979 =========================

int typeIndex = proxyConf.indexOf("@");	Proxy.Type proxyType = Proxy.Type.HTTP;	if(typeIndex != -1 && proxyConf.substring(0, typeIndex).compareToIgnoreCase("socks") == 0) {	proxyType = Proxy.Type.SOCKS;	}	String hostname = proxyConf.substring(typeIndex + 1, proxyConf.lastIndexOf(":"));	String portConf = proxyConf.substring(proxyConf.lastIndexOf(":") + 1);	try {	int port = Integer.parseInt(portConf);	proxyToUse = new Proxy(proxyType, new InetSocketAddress(hostname, port));	
job end notification using proxy type proxytype hostname hostname and port port 

Proxy.Type proxyType = Proxy.Type.HTTP;	if(typeIndex != -1 && proxyConf.substring(0, typeIndex).compareToIgnoreCase("socks") == 0) {	proxyType = Proxy.Type.SOCKS;	}	String hostname = proxyConf.substring(typeIndex + 1, proxyConf.lastIndexOf(":"));	String portConf = proxyConf.substring(proxyConf.lastIndexOf(":") + 1);	try {	int port = Integer.parseInt(portConf);	proxyToUse = new Proxy(proxyType, new InetSocketAddress(hostname, port));	} catch(NumberFormatException nfe) {	
job end notification couldn t parse configured proxy s port not going to use a proxy 

protected boolean notifyURLOnce() {	boolean success = false;	try {	
job end notification trying 

protected boolean notifyURLOnce() {	boolean success = false;	try {	HttpURLConnection conn = (HttpURLConnection) urlToNotify.openConnection(proxyToUse);	conn.setConnectTimeout(timeout);	conn.setReadTimeout(timeout);	conn.setAllowUserInteraction(false);	if(conn.getResponseCode() != HttpURLConnection.HTTP_OK) {	
job end notification to failed with code and message conn getresponsemessage 

boolean success = false;	try {	HttpURLConnection conn = (HttpURLConnection) urlToNotify.openConnection(proxyToUse);	conn.setConnectTimeout(timeout);	conn.setReadTimeout(timeout);	conn.setAllowUserInteraction(false);	if(conn.getResponseCode() != HttpURLConnection.HTTP_OK) {	}	else {	success = true;	
job end notification to succeeded 

HttpURLConnection conn = (HttpURLConnection) urlToNotify.openConnection(proxyToUse);	conn.setConnectTimeout(timeout);	conn.setReadTimeout(timeout);	conn.setAllowUserInteraction(false);	if(conn.getResponseCode() != HttpURLConnection.HTTP_OK) {	}	else {	success = true;	}	} catch(IOException ioe) {	
job end notification to failed 

public void notify(JobReport jobReport) throws InterruptedException {	if (userUrl.contains(JOB_ID)) {	userUrl = userUrl.replace(JOB_ID, jobReport.getJobId().toString());	}	if (userUrl.contains(JOB_STATUS)) {	userUrl = userUrl.replace(JOB_STATUS, jobReport.getJobState().toString());	}	try {	urlToNotify = new URL(userUrl);	} catch (MalformedURLException mue) {	
job end notification couldn t parse 

if (userUrl.contains(JOB_STATUS)) {	userUrl = userUrl.replace(JOB_STATUS, jobReport.getJobState().toString());	}	try {	urlToNotify = new URL(userUrl);	} catch (MalformedURLException mue) {	return;	}	boolean success = false;	while (numTries-- > 0 && !success) {	
job end notification attempts left 

return;	}	boolean success = false;	while (numTries-- > 0 && !success) {	success = notifyURLOnce();	if (!success) {	Thread.sleep(waitInterval);	}	}	if (!success) {	
job end notification failed to notify 

}	boolean success = false;	while (numTries-- > 0 && !success) {	success = notifyURLOnce();	if (!success) {	Thread.sleep(waitInterval);	}	}	if (!success) {	} else {	
job end notification succeeded for 

========================= hadoop sample_5193 =========================

try {	Thread.sleep(5000);	}	catch (Exception e) {	}	}	synchronized(this) {	Iterator<ControlledJob> it = jobsInProgress.iterator();	while(it.hasNext()) {	ControlledJob j = it.next();	
checking state of job 

try {	Thread.sleep(5000);	}	catch (Exception e) {	}	if (this.runnerState != ThreadState.RUNNING && this.runnerState != ThreadState.SUSPENDED) {	break;	}	}	}catch(Throwable t) {	
error while trying to run jobs 

========================= hadoop sample_5039 =========================

switch (resource.getType()) {	case ARCHIVE: {	String lowerDst = StringUtils.toLowerCase(dst.getName());	if (lowerDst.endsWith(".jar")) {	RunJar.unJar(localrsrc, dst);	} else if (lowerDst.endsWith(".zip")) {	FileUtil.unZip(localrsrc, dst);	} else if (lowerDst.endsWith(".tar.gz") || lowerDst.endsWith(".tgz") || lowerDst.endsWith(".tar")) {	FileUtil.unTar(localrsrc, dst);	} else {	
cannot unpack 

String p = resource.getPattern();	RunJar.unJar(localrsrc, dst, p == null ? RunJar.MATCH_ANY : Pattern.compile(p));	File newDst = new File(dst, dst.getName());	if (!dst.exists() && !dst.mkdir()) {	throw new IOException("Unable to create directory: [" + dst + "]");	}	if (!localrsrc.renameTo(newDst)) {	throw new IOException("Unable to rename file: [" + localrsrc + "] to [" + newDst + "]");	}	} else if (lowerDst.endsWith(".zip")) {	
treating as an archive even though it was specified as pattern 

File newDst = new File(dst, dst.getName());	if (!dst.exists() && !dst.mkdir()) {	throw new IOException("Unable to create directory: [" + dst + "]");	}	if (!localrsrc.renameTo(newDst)) {	throw new IOException("Unable to rename file: [" + localrsrc + "] to [" + newDst + "]");	}	} else if (lowerDst.endsWith(".zip")) {	FileUtil.unZip(localrsrc, dst);	} else if (lowerDst.endsWith(".tar.gz") || lowerDst.endsWith(".tgz") || lowerDst.endsWith(".tar")) {	
treating as an archive even though it was specified as pattern 

throw new IOException("Unable to create directory: [" + dst + "]");	}	if (!localrsrc.renameTo(newDst)) {	throw new IOException("Unable to rename file: [" + localrsrc + "] to [" + newDst + "]");	}	} else if (lowerDst.endsWith(".zip")) {	FileUtil.unZip(localrsrc, dst);	} else if (lowerDst.endsWith(".tar.gz") || lowerDst.endsWith(".tgz") || lowerDst.endsWith(".tar")) {	FileUtil.unTar(localrsrc, dst);	} else {	
cannot unpack 

public Path call() throws Exception {	final Path sCopy;	try {	sCopy = resource.getResource().toPath();	} catch (URISyntaxException e) {	throw new IOException("Invalid resource", e);	}	if (LOG.isDebugEnabled()) {	
starting to download 

try {	Path dTmp = null == userUgi ? files.makeQualified(copy(sCopy, dst_work)) : userUgi.doAs(new PrivilegedExceptionAction<Path>() {	public Path run() throws Exception {	return files.makeQualified(copy(sCopy, dst_work));	};	});	unpack(new File(dTmp.toUri()), new File(dFinal.toUri()));	changePermissions(dFinal.getFileSystem(conf), dFinal);	files.rename(dst_work, destDirPath, Rename.OVERWRITE);	if (LOG.isDebugEnabled()) {	
file has been downloaded to 

}	boolean isDir = f.isDirectory();	FsPermission perm = cachePerms;	if (resource.getVisibility() == LocalResourceVisibility.PUBLIC) {	perm = isDir ? PUBLIC_DIR_PERMS : PUBLIC_FILE_PERMS;	}	else {	perm = isDir ? PRIVATE_DIR_PERMS : PRIVATE_FILE_PERMS;	}	if (LOG.isDebugEnabled()) {	
changing permissions for path to perm 

========================= hadoop sample_2178 =========================

final int NUM_THREADS = 10;	DFSTestUtil.createFile(dfs, new Path(TEST_FILE), TEST_FILE_LEN, (short)1, SEED);	Runnable readerRunnable = new Runnable() {	public void run() {	try {	byte contents[] = DFSTestUtil.readFileBuffer(dfs, new Path(TEST_FILE));	Assert.assertFalse(creationIsBlocked.get());	byte expected[] = DFSTestUtil. calculateFileContentsFromSeed(SEED, TEST_FILE_LEN);	Assert.assertTrue(Arrays.equals(contents, expected));	} catch (Throwable e) {	
readerrunnable error 

} catch (Throwable t) {	Assert.assertTrue("expected to see 'TCP reads were disabled " + "for testing' in exception " + t, t.getMessage().contains( "TCP reads were disabled for testing"));	} finally {	if (blockReader != null) blockReader.close();	}	gotFailureLatch.countDown();	shouldRetryLatch.await();	try {	blockReader = BlockReaderTestUtil. getBlockReader(cluster, lblock, 0, TEST_FILE_LEN);	} catch (Throwable t) {	
error trying to retrieve a block reader the second time 

gotFailureLatch.countDown();	shouldRetryLatch.await();	try {	blockReader = BlockReaderTestUtil. getBlockReader(cluster, lblock, 0, TEST_FILE_LEN);	} catch (Throwable t) {	throw t;	} finally {	if (blockReader != null) blockReader.close();	}	} catch (Throwable t) {	
getblockreader failure 

BlockReader blockReader = null;	try {	blockReader = BlockReaderTestUtil. getBlockReader(cluster, lblock, 0, TEST_FILE_LEN);	sem.release();	try {	blockReader.readAll(buf, 0, TEST_FILE_LEN);	} finally {	sem.acquireUninterruptibly();	}	} catch (ClosedByInterruptException e) {	
got the expected closedbyinterruptexception 

blockReader.readAll(buf, 0, TEST_FILE_LEN);	} finally {	sem.acquireUninterruptibly();	}	} catch (ClosedByInterruptException e) {	sem.release();	break;	} finally {	if (blockReader != null) blockReader.close();	}	
read another bytes 

sem.acquireUninterruptibly();	}	} catch (ClosedByInterruptException e) {	sem.release();	break;	} finally {	if (blockReader != null) blockReader.close();	}	}	} catch (Throwable t) {	
getblockreader failure 

========================= hadoop sample_7680 =========================

org.apache.hadoop.yarn.server.api.records.NodeStatus nodeStatus = recordFactory .newRecordInstance(org.apache.hadoop.yarn.server.api.records.NodeStatus.class);	nodeStatus.setNodeId(request3.getNodeId());	nodeStatus.setResponseId(lastResponseID);	nodeStatus.setNodeHealthStatus(recordFactory.newRecordInstance(NodeHealthStatus.class));	nodeStatus.getNodeHealthStatus().setIsNodeHealthy(true);	NodeHeartbeatRequest request = recordFactory .newRecordInstance(NodeHeartbeatRequest.class);	request.setNodeStatus(nodeStatus);	lastResponseID = resourceTrackerService.nodeHeartbeat(request) .getResponseId();	Thread.sleep(1000);	} catch(Exception e) {	
failed to heartbeat 

========================= hadoop sample_633 =========================

public byte[] createPassword(ContainerTokenIdentifier identifier) {	if (LOG.isDebugEnabled()) {	
creating password for for user to be run on nm 

protected byte[] retrievePasswordInternal(ContainerTokenIdentifier identifier, MasterKeyData masterKey) throws org.apache.hadoop.security.token.SecretManager.InvalidToken {	if (LOG.isDebugEnabled()) {	
retrieving password for for user to be run on nm 

========================= hadoop sample_1304 =========================

public void refresh() {	
clearing usertogroupsmap cache 

public void refresh() {	try {	impl.cacheGroupsRefresh();	} catch (IOException e) {	
error refreshing groups cache 

public void cacheGroupsAdd(List<String> groups) {	try {	impl.cacheGroupsAdd(groups);	} catch (IOException e) {	
error caching groups 

public static synchronized Groups getUserToGroupsMappingService( Configuration conf) {	if(GROUPS == null) {	if(LOG.isDebugEnabled()) {	
creating new groups object 

========================= hadoop sample_3769 =========================

traceWriter.init(options.traceOutput, getConf());	topologyWriter = new DefaultOutputter<LoggedNetworkTopology>();	topologyWriter.init(options.topologyOutput, getConf());	try {	JobBuilder jobBuilder = null;	for (Path p : options.inputs) {	InputDemuxer inputDemuxer = options.inputDemuxerClass.newInstance();	try {	inputDemuxer.bindTo(p, getConf());	} catch (IOException e) {	
unable to bind path skipping 

continue;	}	Pair<String, InputStream> filePair = null;	try {	while ((filePair = inputDemuxer.getNext()) != null) {	RewindableInputStream ris = new RewindableInputStream(filePair.second());	JobHistoryParser parser = null;	try {	String jobID = JobHistoryUtils.extractJobID(filePair.first());	if (jobID == null) {	
file skipped invalid file name 

if (jobBuilder != null) {	traceWriter.output(jobBuilder.build());	}	jobBuilder = new JobBuilder(jobID);	}	if (JobHistoryUtils.isJobConfXml(filePair.first())) {	processJobConf(JobConfigurationParser.parse(ris.rewind()), jobBuilder);	} else {	parser = JobHistoryParserFactory.getParser(ris);	if (parser == null) {	
file skipped cannot find suitable parser 

if (parser == null) {	ris.close();	} else {	parser.close();	parser = null;	}	}	}	} catch (Throwable t) {	if (filePair != null) {	
tracebuilder got an error while processing the possibly virtual file within path 

if (filePair != null) {	}	} finally {	inputDemuxer.close();	}	}	if (jobBuilder != null) {	traceWriter.output(jobBuilder.build());	jobBuilder = null;	} else {	
no job found in traces 

========================= hadoop sample_6584 =========================

public static boolean isACLsFileNewer(long time) {	boolean newer = false;	String confDir = System.getProperty(KMS_CONFIG_DIR);	if (confDir != null) {	Path confPath = new Path(confDir);	if (!confPath.isUriPathAbsolute()) {	throw new RuntimeException("System property '" + KMS_CONFIG_DIR + "' must be an absolute path: " + confDir);	}	File f = new File(confDir, KMS_ACLS_XML);	
checking file modification time is last reload time is 

========================= hadoop sample_4413 =========================

assertTrue(fnode.isQuotaSet());	long file1Len = BLOCKSIZE * 3;	int bufLen = BLOCKSIZE / 16;	DFSTestUtil.createFile(dfs, createdFile1foo, bufLen, file1Len, BLOCKSIZE, REPLICATION, seed);	long ssdConsumed = fnode.asDirectory().getDirectoryWithQuotaFeature() .getSpaceConsumed().getTypeSpaces().get(StorageType.SSD);	assertEquals(file1Len, ssdConsumed);	try {	dfs.rename(createdFile1foo, createdFile1bar);	fail("Should have failed with QuotaByStorageTypeExceededException ");	} catch (Throwable t) {	
got expected exception 

long file2Len = BLOCKSIZE + BLOCKSIZE / 2;	DFSTestUtil.createFile(dfs, createdFile2, bufLen, file2Len, BLOCKSIZE, REPLICATION, seed);	currentSSDConsumed = fnode.asDirectory().getDirectoryWithQuotaFeature() .getSpaceConsumed().getTypeSpaces().get(StorageType.SSD);	assertEquals(file1Len + file2Len, currentSSDConsumed);	Path createdFile3 = new Path(foo, "created_file3.data");	long file3Len = BLOCKSIZE;	try {	DFSTestUtil.createFile(dfs, createdFile3, bufLen, file3Len, BLOCKSIZE, REPLICATION, seed);	fail("Should have failed with QuotaByStorageTypeExceededException ");	} catch (Throwable t) {	
got expected exception 

dfs.mkdirs(child);	dfs.setStoragePolicy(parent, HdfsConstants.ONESSD_STORAGE_POLICY_NAME);	dfs.setQuotaByStorageType(child, StorageType.SSD, 2 * BLOCKSIZE);	Path createdFile1 = new Path(child, "created_file1.data");	long file1Len = BLOCKSIZE * 2 + BLOCKSIZE / 2;	int bufLen = BLOCKSIZE / 16;	try {	DFSTestUtil.createFile(dfs, createdFile1, bufLen, file1Len, BLOCKSIZE, REPLICATION, seed);	fail("Should have failed with QuotaByStorageTypeExceededException ");	} catch (Throwable t) {	
got expected exception 

assertTrue(fnode.isDirectory());	assertTrue(fnode.isQuotaSet());	long currentSSDConsumed = fnode.asDirectory().getDirectoryWithQuotaFeature() .getSpaceConsumed().getTypeSpaces().get(StorageType.SSD);	assertEquals(file1Len, currentSSDConsumed);	Path createdFile2 = new Path(child, "created_file2.data");	long file2Len = BLOCKSIZE;	try {	DFSTestUtil.createFile(dfs, createdFile2, bufLen, file2Len, BLOCKSIZE, replication, seed);	fail("Should have failed with QuotaByStorageTypeExceededException ");	} catch (Throwable t) {	
got expected exception 

dfs.setStoragePolicy(parent, HdfsConstants.ONESSD_STORAGE_POLICY_NAME);	dfs.setQuotaByStorageType(parent, StorageType.SSD, 2 * BLOCKSIZE);	dfs.setQuotaByStorageType(child, StorageType.SSD, 3 * BLOCKSIZE);	Path createdFile1 = new Path(child, "created_file1.data");	long file1Len = BLOCKSIZE * 2 + BLOCKSIZE / 2;	int bufLen = BLOCKSIZE / 16;	try {	DFSTestUtil.createFile(dfs, createdFile1, bufLen, file1Len, BLOCKSIZE, REPLICATION, seed);	fail("Should have failed with QuotaByStorageTypeExceededException ");	} catch (Throwable t) {	
got expected exception 

dfs.setQuotaByStorageType(testDir, StorageType.SSD, ssdQuota);	INode testDirNode = fsdir.getINode4Write(testDir.toString());	assertTrue(testDirNode.isDirectory());	assertTrue(testDirNode.isQuotaSet());	Path createdFile = new Path(testDir, "created_file.data");	long fileLen = testFileLenInBlocks * BLOCKSIZE;	try {	DFSTestUtil.createFile(dfs, createdFile, BLOCKSIZE / 16, fileLen, BLOCKSIZE, replication, seed);	fail("Should have failed with DSQuotaExceededException or " + "QuotaByStorageTypeExceededException ");	} catch (Throwable t) {	
got expected exception 

DFSTestUtil.createFile(dfs, createdFile, BLOCKSIZE / 16, fileLen, BLOCKSIZE, REPLICATION, seed);	assertTrue(dfs.exists(createdFile));	assertTrue(dfs.isFile(createdFile));	dfs.setQuotaByStorageType(testDir, StorageType.DISK, storageTypeSpaceQuota);	dfs.setStoragePolicy(testDir, HdfsConstants.WARM_STORAGE_POLICY_NAME);	try {	createdFile = new Path(testDir, "file2.data");	DFSTestUtil.createFile(dfs, createdFile, BLOCKSIZE / 16, fileLen, BLOCKSIZE, REPLICATION, seed);	fail("should fail on QuotaByStorageTypeExceededException");	} catch (QuotaByStorageTypeExceededException e) {	
got expected exception 

final long storageSpaceQuota = BLOCKSIZE * 2;	dfs.setStoragePolicy(testDir, HdfsConstants.HOT_STORAGE_POLICY_NAME);	dfs.setQuota(testDir, HdfsConstants.QUOTA_DONT_SET, storageSpaceQuota);	Path createdFile = null;	final long fileLen = BLOCKSIZE;	try {	createdFile = new Path(testDir, "file1.data");	DFSTestUtil.createFile(dfs, createdFile, BLOCKSIZE / 16, fileLen, BLOCKSIZE, REPLICATION, seed);	fail("should fail on DSQuotaExceededException");	} catch (DSQuotaExceededException e) {	
got expected exception 

========================= hadoop sample_7395 =========================

}	NetworkTopologyWithNodeGroup clusterMapNodeGroup = (NetworkTopologyWithNodeGroup) clusterMap;	DatanodeStorageInfo target = null;	String scope = clusterMapNodeGroup.getNodeGroup(favoredNode.getNetworkLocation());	try {	target = chooseRandom(scope, favoriteAndExcludedNodes, blocksize, maxNodesPerRack, results, avoidStaleNodes, storageTypes);	} catch (NotEnoughReplicasException e) {	continue;	}	if (target == null) {	
could not find a target for file within nodegroup of favored node 

return 0;	}	int countOfExcludedNodes = 0;	for(String hostname : chosenNode.getDependentHostNames()) {	DatanodeDescriptor node = this.host2datanodeMap.getDataNodeByHostName(hostname);	if(node!=null) {	if (excludedNodes.add(node)) {	countOfExcludedNodes++;	}	} else {	
not able to find datanode which has dependency with datanode 

========================= hadoop sample_8349 =========================

public void addCounter(C counter) {	C ours = findCounter(counter.getName());	if (ours != null) {	ours.setValue(counter.getValue());	} else {	
is not a known counter 

public C addCounter(String name, String displayName, long value) {	C counter = findCounter(name);	if (counter != null) {	counter.setValue(value);	} else {	
is not a known counter 

public C findCounter(String counterName) {	try {	T enumValue = valueOf(counterName);	return findCounter(enumValue);	} catch (IllegalArgumentException e) {	
is not a recognized counter 

========================= hadoop sample_5081 =========================

public MRAsyncDiskService(FileSystem localFileSystem, String... nonCanonicalVols) throws IOException {	this.localFileSystem = localFileSystem;	this.volumes = new String[nonCanonicalVols.length];	for (int v = 0; v < nonCanonicalVols.length; v++) {	this.volumes[v] = normalizePath(nonCanonicalVols[v]);	
normalized volume 

public MRAsyncDiskService(FileSystem localFileSystem, String... nonCanonicalVols) throws IOException {	this.localFileSystem = localFileSystem;	this.volumes = new String[nonCanonicalVols.length];	for (int v = 0; v < nonCanonicalVols.length; v++) {	this.volumes[v] = normalizePath(nonCanonicalVols[v]);	}	asyncDiskService = new AsyncDiskService(this.volumes);	for (int v = 0 ; v < volumes.length; v++) {	Path absoluteSubdir = new Path(volumes[v], TOBEDELETED);	if (!localFileSystem.mkdirs(absoluteSubdir)) {	
cannot create in ignored 

boolean success = false;	Exception e = null;	try {	Path absolutePathToBeDeleted = new Path(volume, pathToBeDeleted);	success = localFileSystem.delete(absolutePathToBeDeleted, true);	} catch (Exception ex) {	e = ex;	}	if (!success) {	if (e != null) {	
failure in with exception 

Exception e = null;	try {	Path absolutePathToBeDeleted = new Path(volume, pathToBeDeleted);	success = localFileSystem.delete(absolutePathToBeDeleted, true);	} catch (Exception ex) {	e = ex;	}	if (!success) {	if (e != null) {	} else {	
failure in 

Path absolutePathToBeDeleted = new Path(volume, pathToBeDeleted);	success = localFileSystem.delete(absolutePathToBeDeleted, true);	} catch (Exception ex) {	e = ex;	}	if (!success) {	if (e != null) {	} else {	}	} else {	
successfully did 

========================= hadoop sample_5594 =========================

public void channelConnected(ChannelHandlerContext ctx, ChannelStateEvent e) {	if (LOG.isDebugEnabled()) {	
sending prc request 

public void exceptionCaught(ChannelHandlerContext ctx, ExceptionEvent e) {	
unexpected exception from downstream 

========================= hadoop sample_4385 =========================

private Slot allocSlotFromExistingShm(ExtendedBlockId blockId) {	if (notFull.isEmpty()) {	return null;	}	Entry<ShmId, DfsClientShm> entry = notFull.firstEntry();	DfsClientShm shm = entry.getValue();	ShmId shmId = shm.getShmId();	Slot slot = shm.allocAndRegisterSlot(blockId);	if (shm.isFull()) {	
pulled the last slot out of 

}	Entry<ShmId, DfsClientShm> entry = notFull.firstEntry();	DfsClientShm shm = entry.getValue();	ShmId shmId = shm.getShmId();	Slot slot = shm.allocAndRegisterSlot(blockId);	if (shm.isFull()) {	DfsClientShm removedShm = notFull.remove(shmId);	Preconditions.checkState(removedShm == shm);	full.put(shmId, shm);	} else {	
pulled slot out of 

byte buf[] = new byte[1];	FileInputStream[] fis = new FileInputStream[1];	if (sock.recvFileInputStreams(fis, buf, 0, buf.length) < 0) {	throw new EOFException("got EOF while trying to transfer the " + "file descriptor for the shared memory segment.");	}	if (fis[0] == null) {	throw new IOException("the datanode " + datanode + " failed to " + "pass a file descriptor for the shared memory segment.");	}	try {	DfsClientShm shm = new DfsClientShm(PBHelperClient.convert(resp.getId()), fis[0], this, peer);	
createnewshm created 

if (fis[0] == null) {	throw new IOException("the datanode " + datanode + " failed to " + "pass a file descriptor for the shared memory segment.");	}	try {	DfsClientShm shm = new DfsClientShm(PBHelperClient.convert(resp.getId()), fis[0], this, peer);	return shm;	} finally {	try {	fis[0].close();	} catch (Throwable e) {	
exception in closing 

}	try {	DfsClientShm shm = new DfsClientShm(PBHelperClient.convert(resp.getId()), fis[0], this, peer);	return shm;	} finally {	try {	fis[0].close();	} catch (Throwable e) {	}	}	
datanode does not support short circuit shared memory access 

DfsClientShm shm = new DfsClientShm(PBHelperClient.convert(resp.getId()), fis[0], this, peer);	return shm;	} finally {	try {	fis[0].close();	} catch (Throwable e) {	}	}	disabled = true;	return null;	
error requesting short circuit shared memory access 

public Slot allocSlot(DatanodeInfo datanode, DomainPeer peer, MutableBoolean usedPeer, ExtendedBlockId blockId, String clientName) throws IOException {	lock.lock();	try {	if (closed) {	
the dfsclientshmmanager isclosed 

lock.lock();	try {	if (closed) return;	closed = true;	} finally {	lock.unlock();	}	try {	domainSocketWatcher.close();	} catch (Throwable e) {	
exception in closing 

========================= hadoop sample_6984 =========================

public TaskTrackerInfo[] getBlacklistedTrackers() throws IOException, InterruptedException {	
getblacklistedtrackers not implemented yet 

========================= hadoop sample_5701 =========================

private void runTest(final int nNameNodes, final int nDataNodes, Configuration conf) throws Exception {	LOG.info("nNameNodes=" + nNameNodes + ", nDataNodes=" + nDataNodes);	
run test 

private void runTest(final int nNameNodes, final int nDataNodes, Configuration conf) throws Exception {	LOG.info("nNameNodes=" + nNameNodes + ", nDataNodes=" + nDataNodes);	final MiniDFSCluster cluster = new MiniDFSCluster .Builder(conf) .nnTopology(MiniDFSNNTopology.simpleFederatedTopology(nNameNodes)) .numDataNodes(nDataNodes) .build();	
run test 

private void runTest(final int nNameNodes, final int nDataNodes, Configuration conf) throws Exception {	LOG.info("nNameNodes=" + nNameNodes + ", nDataNodes=" + nDataNodes);	final MiniDFSCluster cluster = new MiniDFSCluster .Builder(conf) .nnTopology(MiniDFSNNTopology.simpleFederatedTopology(nNameNodes)) .numDataNodes(nDataNodes) .build();	DFSTestUtil.setFederatedConfiguration(cluster, conf);	try {	cluster.waitActive();	
run test 

private void runTest(final int nNameNodes, final int nDataNodes, Configuration conf) throws Exception {	LOG.info("nNameNodes=" + nNameNodes + ", nDataNodes=" + nDataNodes);	final MiniDFSCluster cluster = new MiniDFSCluster .Builder(conf) .nnTopology(MiniDFSNNTopology.simpleFederatedTopology(nNameNodes)) .numDataNodes(nDataNodes) .build();	DFSTestUtil.setFederatedConfiguration(cluster, conf);	try {	cluster.waitActive();	final Suite s = new Suite(cluster, nNameNodes, nDataNodes);	for(int i = 0; i < nNameNodes; i++) {	s.createFile(i, 1024);	}	
run test 

s.createFile(i, 1024);	}	final String[] urls = new String[nNameNodes];	for(int i = 0; i < urls.length; i++) {	urls[i] = cluster.getFileSystem(i).getUri() + FILE_NAME;	LOG.info("urls[" + i + "]=" + urls[i]);	final String result = TestFsck.runFsck(conf, 0, false, urls[i]);	LOG.info("result=" + result);	Assert.assertTrue(result.contains("Status: HEALTHY"));	}	
run test 

}	for(int i = 0; i < vurls.length; i++) {	LOG.info("vurls[" + i + "]=" + vurls[i]);	final String result = TestFsck.runFsck(conf, 0, false, vurls[i]);	LOG.info("result=" + result);	Assert.assertTrue(result.contains("Status: HEALTHY"));	}	} finally {	cluster.shutdown();	}	
run test 

========================= hadoop sample_7383 =========================

private static void handleException(Exception e, String url, long startTime, String invalidNumMsg) throws BadRequestException, WebApplicationException {	long endTime = Time.monotonicNow();	
processed url but encountered exception took ms 

throw new BadRequestException(invalidNumMsg + " is not a numeric value.");	} else if (e instanceof IllegalArgumentException) {	throw new BadRequestException(e.getMessage() == null ? "Requested Invalid Field." : e.getMessage());	} else if (e instanceof NotFoundException) {	throw (NotFoundException)e;	} else if (e instanceof TimelineParseException) {	throw new BadRequestException(e.getMessage() == null ? "Filter Parsing failed." : e.getMessage());	} else if (e instanceof BadRequestException) {	throw (BadRequestException)e;	} else {	
error while processing rest request 

public TimelineAbout about( init(res);	return TimelineUtils.createTimelineAbout("Timeline Reader API");	}	public Set<TimelineEntity> getEntities( String url = req.getRequestURI() + (req.getQueryString() == null ? "" : QUERY_STRING_SEP + req.getQueryString());	UserGroupInformation callerUGI = TimelineReaderWebServicesUtils.getUser(req);	
received url from user 

public Set<TimelineEntity> getEntities( return getEntities(req, res, null, appId, entityType, userId, flowName, flowRunId, limit, createdTimeStart, createdTimeEnd, relatesTo, isRelatedTo, infofilters, conffilters, metricfilters, eventfilters, confsToRetrieve, metricsToRetrieve, fields, metricsLimit, metricsTimeStart, metricsTimeEnd, fromId);	}	public Set<TimelineEntity> getEntities( String url = req.getRequestURI() + (req.getQueryString() == null ? "" : QUERY_STRING_SEP + req.getQueryString());	UserGroupInformation callerUGI = TimelineReaderWebServicesUtils.getUser(req);	
received url from user 

public TimelineEntity getEntity( String url = req.getRequestURI() + (req.getQueryString() == null ? "" : QUERY_STRING_SEP + req.getQueryString());	UserGroupInformation callerUGI = TimelineReaderWebServicesUtils.getUser(req);	
received url from user 

public TimelineEntity getEntity( return getEntity(req, res, null, appId, entityType, entityId, userId, flowName, flowRunId, confsToRetrieve, metricsToRetrieve, fields, metricsLimit, metricsTimeStart, metricsTimeEnd, entityIdPrefix);	}	public TimelineEntity getEntity( String url = req.getRequestURI() + (req.getQueryString() == null ? "" : QUERY_STRING_SEP + req.getQueryString());	UserGroupInformation callerUGI = TimelineReaderWebServicesUtils.getUser(req);	
received url from user 

public TimelineEntity getFlowRun( String url = req.getRequestURI() + (req.getQueryString() == null ? "" : QUERY_STRING_SEP + req.getQueryString());	UserGroupInformation callerUGI = TimelineReaderWebServicesUtils.getUser(req);	
received url from user 

public Set<TimelineEntity> getFlowRuns( String url = req.getRequestURI() + (req.getQueryString() == null ? "" : QUERY_STRING_SEP + req.getQueryString());	UserGroupInformation callerUGI = TimelineReaderWebServicesUtils.getUser(req);	
received url from user 

public Set<TimelineEntity> getFlowRuns( return getFlowRuns(req, res, null, userId, flowName, limit, createdTimeStart, createdTimeEnd, metricsToRetrieve, fields, fromId);	}	public Set<TimelineEntity> getFlowRuns( String url = req.getRequestURI() + (req.getQueryString() == null ? "" : QUERY_STRING_SEP + req.getQueryString());	UserGroupInformation callerUGI = TimelineReaderWebServicesUtils.getUser(req);	
received url from user 

public Set<TimelineEntity> getFlows( return getFlows(req, res, null, limit, dateRange, fromId);	}	public Set<TimelineEntity> getFlows( String url = req.getRequestURI() + (req.getQueryString() == null ? "" : QUERY_STRING_SEP + req.getQueryString());	UserGroupInformation callerUGI = TimelineReaderWebServicesUtils.getUser(req);	
received url from user 

public TimelineEntity getApp( String url = req.getRequestURI() + (req.getQueryString() == null ? "" : QUERY_STRING_SEP + req.getQueryString());	UserGroupInformation callerUGI = TimelineReaderWebServicesUtils.getUser(req);	
received url from user 

public TimelineEntity getApp( return getApp(req, res, null, appId, flowName, flowRunId, userId, confsToRetrieve, metricsToRetrieve, fields, metricsLimit, metricsTimeStart, metricsTimeEnd);	}	public TimelineEntity getApp( String url = req.getRequestURI() + (req.getQueryString() == null ? "" : QUERY_STRING_SEP + req.getQueryString());	UserGroupInformation callerUGI = TimelineReaderWebServicesUtils.getUser(req);	
received url from user 

public Set<TimelineEntity> getFlowRunApps( String url = req.getRequestURI() + (req.getQueryString() == null ? "" : QUERY_STRING_SEP + req.getQueryString());	UserGroupInformation callerUGI = TimelineReaderWebServicesUtils.getUser(req);	
received url from user 

public TimelineEntity getContainer(@Context HttpServletRequest req, return getContainer(req, res, null, appId, containerId, userId, flowName, flowRunId, confsToRetrieve, metricsToRetrieve, fields, metricsLimit, entityIdPrefix, metricsTimeStart, metricsTimeEnd);	}	public TimelineEntity getContainer(@Context HttpServletRequest req, return getEntity(req, res, clusterId, appId, TimelineEntityType.YARN_CONTAINER.toString(), containerId, userId, flowName, flowRunId, confsToRetrieve, metricsToRetrieve, fields, metricsLimit, metricsTimeStart, metricsTimeEnd, entityIdPrefix);	}	public Set<String> getEntityTypes( return getEntityTypes(req, res, null, appId, flowName, flowRunId, userId);	}	public Set<String> getEntityTypes( String url = req.getRequestURI() + (req.getQueryString() == null ? "" : QUERY_STRING_SEP + req.getQueryString());	UserGroupInformation callerUGI = TimelineReaderWebServicesUtils.getUser(req);	
received url from user 

public Set<TimelineEntity> getSubAppEntities( return getSubAppEntities(req, res, null, userId, entityType, limit, createdTimeStart, createdTimeEnd, relatesTo, isRelatedTo, infofilters, conffilters, metricfilters, eventfilters, confsToRetrieve, metricsToRetrieve, fields, metricsLimit, metricsTimeStart, metricsTimeEnd, fromId);	}	public Set<TimelineEntity> getSubAppEntities( String url = req.getRequestURI() + (req.getQueryString() == null ? "" : QUERY_STRING_SEP + req.getQueryString());	UserGroupInformation callerUGI = TimelineReaderWebServicesUtils.getUser(req);	
received url from user 

public Set<TimelineEntity> getSubAppEntities(@Context HttpServletRequest req, return getSubAppEntities(req, res, null, userId, entityType, entityId, confsToRetrieve, metricsToRetrieve, fields, metricsLimit, metricsTimeStart, metricsTimeEnd, entityIdPrefix);	}	public Set<TimelineEntity> getSubAppEntities(@Context HttpServletRequest req, String url = req.getRequestURI() + (req.getQueryString() == null ? "" : QUERY_STRING_SEP + req.getQueryString());	UserGroupInformation callerUGI = TimelineReaderWebServicesUtils.getUser(req);	
received url from user 

========================= hadoop sample_330 =========================

if (spnegoPrincipals.length == 0) {	throw new ServletException("Principals do not exist in the keytab");	}	} else {	spnegoPrincipals = new String[]{principal};	}	KeyTab keytabInstance = KeyTab.getInstance(keytabFile);	serverSubject.getPrivateCredentials().add(keytabInstance);	for (String spnegoPrincipal : spnegoPrincipals) {	Principal krbPrincipal = new KerberosPrincipal(spnegoPrincipal);	
using keytab for principal 

public AuthenticationToken authenticate(HttpServletRequest request, final HttpServletResponse response) throws IOException, AuthenticationException {	AuthenticationToken token = null;	String authorization = request.getHeader( KerberosAuthenticator.AUTHORIZATION);	if (authorization == null || !authorization.startsWith(KerberosAuthenticator.NEGOTIATE)) {	response.setHeader(WWW_AUTHENTICATE, KerberosAuthenticator.NEGOTIATE);	response.setStatus(HttpServletResponse.SC_UNAUTHORIZED);	if (authorization == null) {	
spnego starting for url 

public AuthenticationToken authenticate(HttpServletRequest request, final HttpServletResponse response) throws IOException, AuthenticationException {	AuthenticationToken token = null;	String authorization = request.getHeader( KerberosAuthenticator.AUTHORIZATION);	if (authorization == null || !authorization.startsWith(KerberosAuthenticator.NEGOTIATE)) {	response.setHeader(WWW_AUTHENTICATE, KerberosAuthenticator.NEGOTIATE);	response.setStatus(HttpServletResponse.SC_UNAUTHORIZED);	if (authorization == null) {	} else {	
does not start with 

private AuthenticationToken runWithPrincipal(String serverPrincipal, byte[] clientToken, Base64 base64, HttpServletResponse response) throws IOException, GSSException {	GSSContext gssContext = null;	GSSCredential gssCreds = null;	AuthenticationToken token = null;	try {	
spnego initiated with server principal 

gssCreds = this.gssManager.createCredential( this.gssManager.createName(serverPrincipal, KerberosUtil.NT_GSS_KRB5_PRINCIPAL_OID), GSSCredential.INDEFINITE_LIFETIME, new Oid[]{	KerberosUtil.GSS_SPNEGO_MECH_OID, KerberosUtil.GSS_KRB5_MECH_OID }, GSSCredential.ACCEPT_ONLY);	gssContext = this.gssManager.createContext(gssCreds);	byte[] serverToken = gssContext.acceptSecContext(clientToken, 0, clientToken.length);	if (serverToken != null && serverToken.length > 0) {	String authenticate = base64.encodeToString(serverToken);	response.setHeader(KerberosAuthenticator.WWW_AUTHENTICATE, KerberosAuthenticator.NEGOTIATE + " " + authenticate);	}	if (!gssContext.isEstablished()) {	response.setStatus(HttpServletResponse.SC_UNAUTHORIZED);	
spnego in progress 

response.setHeader(KerberosAuthenticator.WWW_AUTHENTICATE, KerberosAuthenticator.NEGOTIATE + " " + authenticate);	}	if (!gssContext.isEstablished()) {	response.setStatus(HttpServletResponse.SC_UNAUTHORIZED);	} else {	String clientPrincipal = gssContext.getSrcName().toString();	KerberosName kerberosName = new KerberosName(clientPrincipal);	String userName = kerberosName.getShortName();	token = new AuthenticationToken(userName, clientPrincipal, getType());	response.setStatus(HttpServletResponse.SC_OK);	
spnego completed for client principal 

========================= hadoop sample_2752 =========================

out.hsync();	cluster.startDataNodes(conf, 1, true, null, null, null);	ExtendedBlock blk = DFSTestUtil.getFirstBlock(fs, testPath);	MaterializedReplica replica = cluster.getMaterializedReplica(0, blk);	replica.deleteData();	replica.deleteMeta();	out.close();	int liveReplicas = 0;	while (true) {	if ((liveReplicas = countReplicas(namesystem, blk).liveReplicas()) < 2) {	
live replicas after corruption 

int liveReplicas = 0;	while (true) {	if ((liveReplicas = countReplicas(namesystem, blk).liveReplicas()) < 2) {	break;	}	Thread.sleep(100);	}	assertEquals("There should be less than 2 replicas in the " + "liveReplicasMap", 1, liveReplicas);	while (true) {	if ((liveReplicas = countReplicas(namesystem, blk).liveReplicas()) > 1) {	
live replicas after rereplication 

while (true) {	if ((liveReplicas = countReplicas(namesystem, blk).liveReplicas()) > 1) {	break;	}	Thread.sleep(100);	}	assertEquals("There should be two live replicas", 2, liveReplicas);	while (true) {	Thread.sleep(100);	if (countReplicas(namesystem, blk).corruptReplicas() == 0) {	
corrupt replicas becomes 

========================= hadoop sample_7550 =========================

public static void setUp() throws java.lang.Exception {	
setup 

public static void tearDown() throws Exception {	
teardown 

{	FSDataOutputStream out = fs.create(p, false, buffersize, REPLICATION, BLOCK_SIZE);	AppendTestUtil.write(out, 0, (int)(BLOCK_SIZE/2));	out.close();	}	FSDataOutputStream out = fs.append(p);	try {	AppendTestUtil.createHdfsWithDifferentUsername(conf).append(p);	fail("This should fail.");	} catch(IOException ioe) {	
good got an exception 

FSDataOutputStream out = fs.append(p);	try {	AppendTestUtil.createHdfsWithDifferentUsername(conf).append(p);	fail("This should fail.");	} catch(IOException ioe) {	}	try {	((DistributedFileSystem) AppendTestUtil .createHdfsWithDifferentUsername(conf)).append(p, EnumSet.of(CreateFlag.APPEND, CreateFlag.NEW_BLOCK), 4096, null);	fail("This should fail.");	} catch(IOException ioe) {	
good got an exception 

{	FSDataOutputStream out = fs.create(p, false, buffersize, REPLICATION, BLOCK_SIZE);	AppendTestUtil.write(out, 0, (int)(BLOCK_SIZE/2));	out.close();	}	FSDataOutputStream out = fs.append(p, EnumSet.of(CreateFlag.APPEND, CreateFlag.NEW_BLOCK), 4096, null);	try {	((DistributedFileSystem) AppendTestUtil .createHdfsWithDifferentUsername(conf)).append(p, EnumSet.of(CreateFlag.APPEND, CreateFlag.NEW_BLOCK), 4096, null);	fail("This should fail.");	} catch(IOException ioe) {	
good got an exception 

FSDataOutputStream out = fs.append(p, EnumSet.of(CreateFlag.APPEND, CreateFlag.NEW_BLOCK), 4096, null);	try {	((DistributedFileSystem) AppendTestUtil .createHdfsWithDifferentUsername(conf)).append(p, EnumSet.of(CreateFlag.APPEND, CreateFlag.NEW_BLOCK), 4096, null);	fail("This should fail.");	} catch(IOException ioe) {	}	try {	AppendTestUtil.createHdfsWithDifferentUsername(conf).append(p);	fail("This should fail.");	} catch(IOException ioe) {	
good got an exception 

========================= hadoop sample_7100 =========================

public void run() {	
starting thread 

public void run() {	SelfRenewingLease lease = null;	NativeAzureFileSystem nfs = (NativeAzureFileSystem) fs;	if (name.equals("first-thread")) {	try {	lease = nfs.getStore().acquireLease(key);	
acquired lease 

}	assertTrue(lease != null);	try {	Thread.sleep(SelfRenewingLease.LEASE_RENEWAL_PERIOD + 2000);	} catch (InterruptedException e) {	Thread.currentThread().interrupt();	}	try {	firstEndTime = System.currentTimeMillis();	lease.free();	
freed lease 

} catch (StorageException e) {	fail("Unanticipated exception");	}	} else if (name.equals("second-thread")) {	try {	Thread.sleep(2000);	} catch (InterruptedException e) {	Thread.currentThread().interrupt();	}	try {	
before getting lease 

}	} else if (name.equals("second-thread")) {	try {	Thread.sleep(2000);	} catch (InterruptedException e) {	Thread.currentThread().interrupt();	}	try {	lease = nfs.getStore().acquireLease(key);	secondStartTime = System.currentTimeMillis();	
acquired lease 

}	try {	lease = nfs.getStore().acquireLease(key);	secondStartTime = System.currentTimeMillis();	} catch (AzureException e) {	assertTrue("Unanticipated exception", false);	}	assertTrue(lease != null);	try {	lease.free();	
freed lease 

}	assertTrue(lease != null);	try {	lease.free();	} catch (StorageException e) {	assertTrue("Unanticipated exception", false);	}	} else {	fail("Unknown thread name");	}	
is exiting 

========================= hadoop sample_6341 =========================

static Collection<URI> getNameServiceUris(Configuration conf, Collection<String> nameServices, String... keys) {	Set<URI> ret = new HashSet<URI>();	Set<URI> nonPreferredUris = new HashSet<URI>();	for (String nsId : nameServices) {	URI nsUri = createUri(HdfsConstants.HDFS_URI_SCHEME, nsId, -1);	boolean useLogicalUri = false;	try {	useLogicalUri = HAUtil.useLogicalUri(conf, nsUri);	} catch (IOException e){	
getting exception while trying to determine if nameservice can use logical uri 

String namenodeId = null;	int found = 0;	Collection<String> nsIds = DFSUtilClient.getNameServiceIds(conf);	for (String nsId : DFSUtilClient.emptyAsSingletonNull(nsIds)) {	if (knownNsId != null && !knownNsId.equals(nsId)) {	continue;	}	Collection<String> nnIds = DFSUtilClient.getNameNodeIds(conf, nsId);	for (String nnId : DFSUtilClient.emptyAsSingletonNull(nnIds)) {	if (LOG.isTraceEnabled()) {	
addresskey s nsid s nnid s 

}	String key = addKeySuffixes(addressKey, nsId, nnId);	String addr = conf.get(key);	if (addr == null) {	continue;	}	InetSocketAddress s = null;	try {	s = NetUtils.createSocketAddr(addr);	} catch (Exception e) {	
exception in creating socket address 

public static HttpConfig.Policy getHttpPolicy(Configuration conf) {	String policyStr = conf.get(DFSConfigKeys.DFS_HTTP_POLICY_KEY);	if (policyStr == null) {	boolean https = conf.getBoolean(DFSConfigKeys.DFS_HTTPS_ENABLE_KEY, DFSConfigKeys.DFS_HTTPS_ENABLE_DEFAULT);	boolean hadoopSsl = conf.getBoolean( CommonConfigurationKeys.HADOOP_SSL_ENABLED_KEY, CommonConfigurationKeys.HADOOP_SSL_ENABLED_DEFAULT);	if (hadoopSsl) {	
is deprecated please use 

public static HttpConfig.Policy getHttpPolicy(Configuration conf) {	String policyStr = conf.get(DFSConfigKeys.DFS_HTTP_POLICY_KEY);	if (policyStr == null) {	boolean https = conf.getBoolean(DFSConfigKeys.DFS_HTTPS_ENABLE_KEY, DFSConfigKeys.DFS_HTTPS_ENABLE_DEFAULT);	boolean hadoopSsl = conf.getBoolean( CommonConfigurationKeys.HADOOP_SSL_ENABLED_KEY, CommonConfigurationKeys.HADOOP_SSL_ENABLED_DEFAULT);	if (hadoopSsl) {	}	if (https) {	
is deprecated please use 

public static Configuration loadSslConfiguration(Configuration conf) {	Configuration sslConf = new Configuration(false);	sslConf.addResource(conf.get( DFSConfigKeys.DFS_SERVER_HTTPS_KEYSTORE_RESOURCE_KEY, DFSConfigKeys.DFS_SERVER_HTTPS_KEYSTORE_RESOURCE_DEFAULT));	final String[] reqSslProps = {	DFSConfigKeys.DFS_SERVER_HTTPS_TRUSTSTORE_LOCATION_KEY, DFSConfigKeys.DFS_SERVER_HTTPS_KEYSTORE_LOCATION_KEY, DFSConfigKeys.DFS_SERVER_HTTPS_KEYSTORE_PASSWORD_KEY, DFSConfigKeys.DFS_SERVER_HTTPS_KEYPASSWORD_KEY };	for (String sslProp : reqSslProps) {	if (sslConf.get(sslProp) == null) {	
ssl config is missing if is specified make sure it is a relative path 

public static HttpServer2.Builder httpServerTemplateForNNAndJN( Configuration conf, final InetSocketAddress httpAddr, final InetSocketAddress httpsAddr, String name, String spnegoUserNameKey, String spnegoKeytabFileKey) throws IOException {	HttpConfig.Policy policy = getHttpPolicy(conf);	HttpServer2.Builder builder = new HttpServer2.Builder().setName(name) .setConf(conf).setACL(new AccessControlList(conf.get(DFS_ADMIN, " "))) .setSecurityEnabled(UserGroupInformation.isSecurityEnabled()) .setUsernameConfKey(spnegoUserNameKey) .setKeytabConfKey(getSpnegoKeytabKey(conf, spnegoKeytabFileKey));	if (UserGroupInformation.isSecurityEnabled()) {	
starting web server as 

public static HttpServer2.Builder httpServerTemplateForNNAndJN( Configuration conf, final InetSocketAddress httpAddr, final InetSocketAddress httpsAddr, String name, String spnegoUserNameKey, String spnegoKeytabFileKey) throws IOException {	HttpConfig.Policy policy = getHttpPolicy(conf);	HttpServer2.Builder builder = new HttpServer2.Builder().setName(name) .setConf(conf).setACL(new AccessControlList(conf.get(DFS_ADMIN, " "))) .setSecurityEnabled(UserGroupInformation.isSecurityEnabled()) .setUsernameConfKey(spnegoUserNameKey) .setKeytabConfKey(getSpnegoKeytabKey(conf, spnegoKeytabFileKey));	if (UserGroupInformation.isSecurityEnabled()) {	}	if (policy.isHttpEnabled()) {	if (httpAddr.getPort() == 0) {	builder.setFindPort(true);	}	URI uri = URI.create("http: builder.addEndpoint(uri);	
starting web server for at 

}	URI uri = URI.create("http: builder.addEndpoint(uri);	}	if (policy.isHttpsEnabled() && httpsAddr != null) {	Configuration sslConf = loadSslConfiguration(conf);	loadSslConfToHttpServerBuilder(builder, sslConf);	if (httpsAddr.getPort() == 0) {	builder.setFindPort(true);	}	URI uri = URI.create("https: builder.addEndpoint(uri);	
starting web server for at 

static String getPassword(Configuration conf, String alias) {	String password = null;	try {	char[] passchars = conf.getPassword(alias);	if (passchars != null) {	password = new String(passchars);	}	}	catch (IOException ioe) {	
setting password to null since ioexception is caught when getting password 

========================= hadoop sample_7803 =========================

public void connect() throws IOException {	Preconditions.checkState(is == null, "Already connected");	try {	
connecting to 

========================= hadoop sample_4239 =========================

public void restart() {	if (!miniMRYarnCluster.getServiceState().equals(STATE.STARTED)){	
cannot restart the mini cluster start it first 

========================= hadoop sample_5428 =========================

for(DatanodeInfo node:datanodes) {	if(node != newNode) {	if( node.getNetworkLocation().equals(newNode.getNetworkLocation())) {	source = node;	} else {	proxies.add( node );	}	}	}	assertTrue(source!=null && proxies.size()==2);	
testcase proxy does not contain the block 

if(node != newNode) {	if( node.getNetworkLocation().equals(newNode.getNetworkLocation())) {	source = node;	} else {	proxies.add( node );	}	}	}	assertTrue(source!=null && proxies.size()==2);	assertFalse(replaceBlock(b, source, newNode, proxies.get(0)));	
testcase destination contains the block 

proxies.add( node );	}	}	}	assertTrue(source!=null && proxies.size()==2);	assertFalse(replaceBlock(b, source, newNode, proxies.get(0)));	assertFalse(replaceBlock(b, source, proxies.get(0), proxies.get(1)));	LOG.info("Testcase 3: Source=" + source + " Proxy=" + proxies.get(0) + " Destination=" + newNode );	assertTrue(replaceBlock(b, source, proxies.get(0), newNode));	checkBlocks(new DatanodeInfo[]{newNode, proxies.get(0), proxies.get(1)}, fileName.toString(), DEFAULT_BLOCK_SIZE, REPLICATION_FACTOR, client);	
testcase invalid del hint 

do {	try {	Thread.sleep(100);	} catch(InterruptedException e) {	}	List<LocatedBlock> blocks = client.getNamenode(). getBlockLocations(fileName, 0, fileLen).getLocatedBlocks();	assertEquals(1, blocks.size());	DatanodeInfo[] nodes = blocks.get(0).getLocations();	notDone = (nodes.length != replFactor);	if (notDone) {	
expected replication factor is but the real replication factor is 

List<LocatedBlock> blocks = client.getNamenode(). getBlockLocations(fileName, 0, fileLen).getLocatedBlocks();	assertEquals(1, blocks.size());	DatanodeInfo[] nodes = blocks.get(0).getLocations();	notDone = (nodes.length != replFactor);	if (notDone) {	} else {	List<DatanodeInfo> nodeLocations = Arrays.asList(nodes);	for (DatanodeInfo node : includeNodes) {	if (!nodeLocations.contains(node) ) {	notDone=true;	
block is not located at 

notDone=true;	break;	}	}	}	if (Time.monotonicNow() > failtime) {	String expectedNodesList = "";	String currentNodesList = "";	for (DatanodeInfo dn : includeNodes) expectedNodesList += dn + ", ";	for (DatanodeInfo dn : nodes) currentNodesList += dn + ", ";	
expected replica nodes are 

notDone=true;	break;	}	}	}	if (Time.monotonicNow() > failtime) {	String expectedNodesList = "";	String currentNodesList = "";	for (DatanodeInfo dn : includeNodes) expectedNodesList += dn + ", ";	for (DatanodeInfo dn : nodes) currentNodesList += dn + ", ";	
current actual replica nodes are 

}	}	if (Time.monotonicNow() > failtime) {	String expectedNodesList = "";	String currentNodesList = "";	for (DatanodeInfo dn : includeNodes) expectedNodesList += dn + ", ";	for (DatanodeInfo dn : nodes) currentNodesList += dn + ", ";	throw new TimeoutException( "Did not achieve expected replication to expected nodes " + "after more than " + TIMEOUT + " msec.  See logs for details.");	}	} while(notDone);	
achieved expected replication values in msec 

assertTrue(locatedBlocks.size() == 1);	assertTrue(locatedBlocks.get(0).getLocations().length == 1);	cluster.startDataNodes(conf, 1, true, null, null, null, null);	assertEquals("Number of datanodes should be 2", 2, cluster.getDataNodes().size());	DataNode dn0 = cluster.getDataNodes().get(0);	DataNode dn1 = cluster.getDataNodes().get(1);	String activeNNBPId = cluster.getNamesystem(0).getBlockPoolId();	DatanodeDescriptor sourceDnDesc = NameNodeAdapter.getDatanode( cluster.getNamesystem(0), dn0.getDNRegistrationForBP(activeNNBPId));	DatanodeDescriptor destDnDesc = NameNodeAdapter.getDatanode( cluster.getNamesystem(0), dn1.getDNRegistrationForBP(activeNNBPId));	ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, fileName);	
replaceblock 

========================= hadoop sample_7291 =========================

public final JobMetaData setContainerStart(final String containerId, final long time) {	if (rawStart.put(containerId, time) != null) {	
find duplicate container launch time for so we replace it with 

public final JobMetaData setContainerEnd(final String containerId, final long time) {	if (rawEnd.put(containerId, time) != null) {	
find duplicate container release time for so we replace it with 

final RLESparseResourceAllocation skylineList = new RLESparseResourceAllocation(resourceOverTime, new DefaultResourceCalculator());	resourceSkyline.setSkylineList(skylineList);	if (containerSpec == null) {	containerSpec = Resource.newInstance(1024, 1);	}	resourceSkyline.setContainerSpec(containerSpec);	for (final Map.Entry<String, Long> entry : rawStart.entrySet()) {	final long timeStart = entry.getValue();	final Long timeEnd = rawEnd.get(entry.getKey());	if (timeEnd == null) {	
container release time not found for 

========================= hadoop sample_6480 =========================

private void initProviderList() {	if (providerList == null) {	synchronized (frameworkLoader) {	if (providerList == null) {	List<ClientProtocolProvider> localProviderList = new ArrayList<ClientProtocolProvider>();	try {	for (ClientProtocolProvider provider : frameworkLoader) {	localProviderList.add(provider);	}	} catch(ServiceConfigurationError e) {	
failed to instantiate clientprotocolprovider please check the meta inf services org apache hadoop mapreduce protocol clientprotocolprovider files on the classpath 

private void initialize(InetSocketAddress jobTrackAddr, Configuration conf) throws IOException {	initProviderList();	final IOException initEx = new IOException( "Cannot initialize Cluster. Please check your configuration for " + MRConfig.FRAMEWORK_NAME + " and the correspond server addresses.");	if (jobTrackAddr != null) {	LOG.info( "Initializing cluster for Job Tracker=" + jobTrackAddr.toString());	}	for (ClientProtocolProvider provider : providerList) {	
trying clientprotocolprovider 

ClientProtocol clientProtocol = null;	try {	if (jobTrackAddr == null) {	clientProtocol = provider.create(conf);	} else {	clientProtocol = provider.create(jobTrackAddr, conf);	}	if (clientProtocol != null) {	clientProtocolProvider = provider;	client = clientProtocol;	
picked as the clientprotocolprovider 

if (jobTrackAddr == null) {	clientProtocol = provider.create(conf);	} else {	clientProtocol = provider.create(jobTrackAddr, conf);	}	if (clientProtocol != null) {	clientProtocolProvider = provider;	client = clientProtocol;	break;	} else {	
cannot pick as the clientprotocolprovider returned null protocol 

========================= hadoop sample_4901 =========================

public FinishApplicationMasterResponse finishApplicationMaster( FinishApplicationMasterRequest request) throws YarnException, IOException {	ApplicationAttemptId applicationAttemptId = YarnServerSecurityUtils.authorizeRequest().getApplicationAttemptId();	ApplicationId appId = applicationAttemptId.getApplicationId();	RMApp rmApp = rmContext.getRMApps().get(applicationAttemptId.getApplicationId());	if (YarnConfiguration.timelineServiceV2Enabled(getConfig())) {	((RMAppImpl) rmApp).removeCollectorData();	}	if (rmApp.isAppFinalStateStored()) {	
unregistered successfully 

}	AllocateResponse response = recordFactory.newRecordInstance(AllocateResponse.class);	this.amsProcessingChain.allocate( amrmTokenIdentifier.getApplicationAttemptId(), request, response);	MasterKeyData nextMasterKey = this.rmContext.getAMRMTokenSecretManager().getNextMasterKeyData();	if (nextMasterKey != null && nextMasterKey.getMasterKey().getKeyId() != amrmTokenIdentifier .getKeyId()) {	RMApp app = this.rmContext.getRMApps().get(appAttemptId.getApplicationId());	RMAppAttempt appAttempt = app.getRMAppAttempt(appAttemptId);	RMAppAttemptImpl appAttemptImpl = (RMAppAttemptImpl)appAttempt;	Token<AMRMTokenIdentifier> amrmToken = appAttempt.getAMRMToken();	if (nextMasterKey.getMasterKey().getKeyId() != appAttemptImpl.getAMRMTokenKeyId()) {	
the amrmtoken has been rolled over send new amrmtoken back to application 

public void registerAppAttempt(ApplicationAttemptId attemptId) {	AllocateResponse response = recordFactory.newRecordInstance(AllocateResponse.class);	response.setResponseId(PRE_REGISTER_RESPONSE_ID);	
registering app attempt 

public void unregisterAttempt(ApplicationAttemptId attemptId) {	
unregistering app attempt 

========================= hadoop sample_737 =========================

}	Node nodeGroup = getNode(node.getNetworkLocation());	if (nodeGroup == null) {	nodeGroup = new InnerNodeWithNodeGroup(node.getNetworkLocation());	}	rack = getNode(nodeGroup.getNetworkLocation());	if (rack != null && (!(rack instanceof InnerNode) || rack.getParent() == null)) {	throw new IllegalArgumentException("Unexpected data node " + node.toString() + " at an illegal network location");	}	if (clusterMap.add(node)) {	
adding a new node 

rack = getNode(nodeGroup.getNetworkLocation());	if (rack != null && (!(rack instanceof InnerNode) || rack.getParent() == null)) {	throw new IllegalArgumentException("Unexpected data node " + node.toString() + " at an illegal network location");	}	if (clusterMap.add(node)) {	if (rack == null) {	incrementRacks();	}	}	if(LOG.isDebugEnabled()) {	
networktopology became 

public void remove(Node node) {	if (node==null) return;	if( node instanceof InnerNode ) {	throw new IllegalArgumentException( "Not allow to remove an inner node: "+NodeBase.getPath(node));	}	
removing a node 

Node nodeGroup = getNode(node.getNetworkLocation());	if (nodeGroup == null) {	nodeGroup = factory.newInnerNode(node.getNetworkLocation());	}	InnerNode rack = (InnerNode)getNode(nodeGroup.getNetworkLocation());	if (rack == null) {	numOfRacks--;	}	}	if(LOG.isDebugEnabled()) {	
networktopology became 

========================= hadoop sample_3544 =========================

};	Collections.sort(subclusters, cmp);	for (SubClusterInfo subcluster : subclusters) {	SubClusterId subClusterId = subcluster.getSubClusterId();	String webAppAddress = subcluster.getRMWebServiceAddress();	String capability = subcluster.getCapability();	ClusterMetricsInfo subClusterInfo = getClusterMetricsInfo(capability);	tbody.tr().td().a(" .td(Integer.toString(subClusterInfo.getAppsSubmitted())) .td(Integer.toString(subClusterInfo.getAppsPending())) .td(Integer.toString(subClusterInfo.getAppsRunning())) .td(Integer.toString(subClusterInfo.getAppsFailed())) .td(Integer.toString(subClusterInfo.getAppsKilled())) .td(Integer.toString(subClusterInfo.getAppsCompleted())) .td(Integer.toString(subClusterInfo.getContainersAllocated())) .td(Integer.toString(subClusterInfo.getReservedContainers())) .td(Integer.toString(subClusterInfo.getPendingContainers())) .td(StringUtils.byteDesc( subClusterInfo.getAvailableMB() * BYTES_IN_MB)) .td(StringUtils.byteDesc( subClusterInfo.getAllocatedMB() * BYTES_IN_MB)) .td(StringUtils.byteDesc( subClusterInfo.getReservedMB() * BYTES_IN_MB)) .td(StringUtils.byteDesc( subClusterInfo.getTotalMB() * BYTES_IN_MB)) .td(Long.toString(subClusterInfo.getAvailableVirtualCores())) .td(Long.toString(subClusterInfo.getAllocatedVirtualCores())) .td(Long.toString(subClusterInfo.getReservedVirtualCores())) .td(Long.toString(subClusterInfo.getTotalVirtualCores())) .td(Integer.toString(subClusterInfo.getActiveNodes())) .td(Integer.toString(subClusterInfo.getLostNodes())) .td(Integer.toString(subClusterInfo.getDecommissionedNodes())) .td(Integer.toString(subClusterInfo.getUnhealthyNodes())) .td(Integer.toString(subClusterInfo.getRebootedNodes())) .td(Integer.toString(subClusterInfo.getTotalNodes()))._();	}	} catch (YarnException e) {	
cannot render resourcemanager 

private static ClusterMetricsInfo getClusterMetricsInfo(String capability) {	ClusterMetricsInfo clusterMetrics = null;	try {	JSONJAXBContext jc = new JSONJAXBContext( JSONConfiguration.mapped().rootUnwrapping(false).build(), ClusterMetricsInfo.class);	JSONUnmarshaller unmarshaller = jc.createJSONUnmarshaller();	clusterMetrics = unmarshaller.unmarshalFromJSON( new StringReader(capability), ClusterMetricsInfo.class);	} catch (Exception e) {	
cannot parse subcluster info 

========================= hadoop sample_1980 =========================

LinkedList<StorageDirectory> sds = new LinkedList<StorageDirectory>();	while (iter.hasNext()) {	sds.add(iter.next());	}	editLog.close();	cluster.shutdown();	for (StorageDirectory sd : sds) {	File editFile = NNStorage.getFinalizedEditsFile(sd, 1, 3);	assertTrue(editFile.exists());	long fileLen = editFile.length();	
corrupting log file len 

LOG.info("\n===========================================\n" + "Starting empty cluster");	cluster = new MiniDFSCluster.Builder(conf) .numDataNodes(NUM_DATA_NODES) .format(true) .build();	cluster.waitActive();	FileSystem fs = cluster.getFileSystem();	for (int i = 0; i < numTransactions; i++) {	fs.mkdirs(new Path("/test" + i));	}	File nameDir = new File(cluster.getNameDirs(0).iterator().next().getPath());	File dfsDir = nameDir.getParentFile();	assertEquals(dfsDir.getName(), "dfs");	
copying data directory aside to a hot backup 

cluster.waitActive();	FileSystem fs = cluster.getFileSystem();	for (int i = 0; i < numTransactions; i++) {	fs.mkdirs(new Path("/test" + i));	}	File nameDir = new File(cluster.getNameDirs(0).iterator().next().getPath());	File dfsDir = nameDir.getParentFile();	assertEquals(dfsDir.getName(), "dfs");	File backupDir = new File(dfsDir.getParentFile(), "dfs.backup-while-running");	FileUtils.copyDirectory(dfsDir, backupDir);	
shutting down cluster 

List<URI> editUris = ImmutableList.of(f1.toURI(), f2.toURI());	NNStorage storage = setupEdits(editUris, 10, new AbortSpec(1, 0), new AbortSpec(2, 1), new AbortSpec(3, 0), new AbortSpec(4, 1), new AbortSpec(5, 0), new AbortSpec(6, 1), new AbortSpec(7, 0), new AbortSpec(8, 1), new AbortSpec(9, 0), new AbortSpec(10, 1));	long totaltxnread = 0;	FSEditLog editlog = getFSEditLog(storage);	editlog.initJournalsForWrite();	long startTxId = 1;	Iterable<EditLogInputStream> editStreams = editlog.selectInputStreams(startTxId, TXNS_PER_ROLL*11);	for (EditLogInputStream edits : editStreams) {	FSEditLogLoader.EditLogValidation val = FSEditLogLoader.scanEditLog(edits, Long.MAX_VALUE);	long read = (val.getEndTxId() - edits.getFirstTxId()) + 1;	
loading edits read 

========================= hadoop sample_7507 =========================

public static void waitForNNToIssueDeletions(final NameNode nn) throws Exception {	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	
waiting for nn to issue block deletions to dns 

========================= hadoop sample_7442 =========================

public String reconfigurePropertyImpl(String property, String newVal) throws ReconfigurationException {	switch (property) {	case DFS_DATANODE_DATA_DIR_KEY: {	IOException rootException = null;	try {	
reconfiguring to 

IOException rootException = null;	try {	this.refreshVolumes(newVal);	return getConf().get(DFS_DATANODE_DATA_DIR_KEY);	} catch (IOException e) {	rootException = e;	} finally {	try {	triggerBlockReport( new BlockReportOptions.Factory().setIncremental(false).build());	} catch (IOException e) {	
exception while sending the block report after refreshing volumes to 

if (rootException != null) {	throw new ReconfigurationException(property, newVal, getConf().get(property), rootException);	}	}	}	break;	}	case DFS_DATANODE_BALANCE_MAX_NUM_CONCURRENT_MOVES_KEY: {	ReconfigurationException rootException = null;	try {	
reconfiguring to 

if (movers <= 0) {	rootException = new ReconfigurationException( property, newVal, getConf().get(property), new IllegalArgumentException( "balancer max concurrent movers must be larger than 0"));	}	}	xserver.updateBalancerMaxConcurrentMovers(movers);	return Integer.toString(movers);	} catch (NumberFormatException nfe) {	rootException = new ReconfigurationException( property, newVal, getConf().get(property), nfe);	} finally {	if (rootException != null) {	
exception in updating balancer max concurrent movers s to s 

StringBuilder errorMessageBuilder = new StringBuilder();	List<String> effectiveVolumes = Lists.newArrayList();	for (StorageLocation sl : changedVolumes.unchangedLocations) {	effectiveVolumes.add(sl.toString());	}	try {	if (numOldDataDirs + getFSDataset().getNumFailedVolumes() + changedVolumes.newLocations.size() - changedVolumes.deactivateLocations.size() <= 0) {	throw new IOException("Attempt to remove all volumes.");	}	if (!changedVolumes.newLocations.isEmpty()) {	
adding new volumes 

}	}));	}	for (int i = 0; i < changedVolumes.newLocations.size(); i++) {	StorageLocation volume = changedVolumes.newLocations.get(i);	Future<IOException> ioExceptionFuture = exceptions.get(i);	try {	IOException ioe = ioExceptionFuture.get();	if (ioe != null) {	errorMessageBuilder.append( String.format("FAILED TO ADD: %s: %s%n", volume, ioe.getMessage()));	
failed to add volume 

}	for (int i = 0; i < changedVolumes.newLocations.size(); i++) {	StorageLocation volume = changedVolumes.newLocations.get(i);	Future<IOException> ioExceptionFuture = exceptions.get(i);	try {	IOException ioe = ioExceptionFuture.get();	if (ioe != null) {	errorMessageBuilder.append( String.format("FAILED TO ADD: %s: %s%n", volume, ioe.getMessage()));	} else {	effectiveVolumes.add(volume.toString());	
successfully added volume 

Future<IOException> ioExceptionFuture = exceptions.get(i);	try {	IOException ioe = ioExceptionFuture.get();	if (ioe != null) {	errorMessageBuilder.append( String.format("FAILED TO ADD: %s: %s%n", volume, ioe.getMessage()));	} else {	effectiveVolumes.add(volume.toString());	}	} catch (Exception e) {	errorMessageBuilder.append( String.format("FAILED to ADD: %s: %s%n", volume, e.toString()));	
failed to add volume 

}	} catch (Exception e) {	errorMessageBuilder.append( String.format("FAILED to ADD: %s: %s%n", volume, e.toString()));	}	}	}	try {	removeVolumes(changedVolumes.deactivateLocations);	} catch (IOException e) {	errorMessageBuilder.append(e.getMessage());	
failed to remove volume 

private void startPlugins(Configuration conf) {	try {	plugins = conf.getInstances(DFS_DATANODE_PLUGINS_KEY, ServicePlugin.class);	} catch (RuntimeException e) {	String pluginsValue = conf.get(DFS_DATANODE_PLUGINS_KEY);	
unable to load datanode plugins specified list of plugins 

private void startPlugins(Configuration conf) {	try {	plugins = conf.getInstances(DFS_DATANODE_PLUGINS_KEY, ServicePlugin.class);	} catch (RuntimeException e) {	String pluginsValue = conf.get(DFS_DATANODE_PLUGINS_KEY);	throw e;	}	for (ServicePlugin p: plugins) {	try {	p.start(this);	
started plug in 

try {	plugins = conf.getInstances(DFS_DATANODE_PLUGINS_KEY, ServicePlugin.class);	} catch (RuntimeException e) {	String pluginsValue = conf.get(DFS_DATANODE_PLUGINS_KEY);	throw e;	}	for (ServicePlugin p: plugins) {	try {	p.start(this);	} catch (Throwable t) {	
serviceplugin could not be started 

ipcServer = new RPC.Builder(getConf()) .setProtocol(ClientDatanodeProtocolPB.class) .setInstance(service) .setBindAddress(ipcAddr.getHostName()) .setPort(ipcAddr.getPort()) .setNumHandlers( getConf().getInt(DFS_DATANODE_HANDLER_COUNT_KEY, DFS_DATANODE_HANDLER_COUNT_DEFAULT)).setVerbose(false) .setSecretManager(blockPoolTokenSecretManager).build();	ReconfigurationProtocolServerSideTranslatorPB reconfigurationProtocolXlator = new ReconfigurationProtocolServerSideTranslatorPB(this);	service = ReconfigurationProtocolService .newReflectiveBlockingService(reconfigurationProtocolXlator);	DFSUtil.addPBProtocol(getConf(), ReconfigurationProtocolPB.class, service, ipcServer);	InterDatanodeProtocolServerSideTranslatorPB interDatanodeProtocolXlator = new InterDatanodeProtocolServerSideTranslatorPB(this);	service = InterDatanodeProtocolService .newReflectiveBlockingService(interDatanodeProtocolXlator);	DFSUtil.addPBProtocol(getConf(), InterDatanodeProtocolPB.class, service, ipcServer);	TraceAdminProtocolServerSideTranslatorPB traceAdminXlator = new TraceAdminProtocolServerSideTranslatorPB(this);	BlockingService traceAdminService = TraceAdminService .newReflectiveBlockingService(traceAdminXlator);	DFSUtil.addPBProtocol( getConf(), TraceAdminProtocolPB.class, traceAdminService, ipcServer);	
opened ipc server at 

String reason = null;	if (conf.getInt(DFS_DATANODE_DIRECTORYSCAN_INTERVAL_KEY, DFS_DATANODE_DIRECTORYSCAN_INTERVAL_DEFAULT) < 0) {	reason = "verification is turned off by configuration";	} else if ("SimulatedFSDataset".equals(data.getClass().getSimpleName())) {	reason = "verifcation is not supported by SimulatedFSDataset";	}	if (reason == null) {	directoryScanner = new DirectoryScanner(this, data, conf);	directoryScanner.start();	} else {	
periodic directory tree verification scan is disabled because 

if (secureResources != null) {	tcpPeerServer = new TcpPeerServer(secureResources);	} else {	int backlogLength = getConf().getInt( CommonConfigurationKeysPublic.IPC_SERVER_LISTEN_QUEUE_SIZE_KEY, CommonConfigurationKeysPublic.IPC_SERVER_LISTEN_QUEUE_SIZE_DEFAULT);	tcpPeerServer = new TcpPeerServer(dnConf.socketWriteTimeout, DataNode.getStreamingAddr(getConf()), backlogLength);	}	if (dnConf.getTransferSocketRecvBufferSize() > 0) {	tcpPeerServer.setReceiveBufferSize( dnConf.getTransferSocketRecvBufferSize());	}	streamingAddr = tcpPeerServer.getStreamingAddr();	
opened streaming server at 

}	streamingAddr = tcpPeerServer.getStreamingAddr();	this.threadGroup = new ThreadGroup("dataXceiverServer");	xserver = new DataXceiverServer(tcpPeerServer, getConf(), this);	this.dataXceiverServer = new Daemon(threadGroup, xserver);	this.threadGroup.setDaemon(true);	if (getConf().getBoolean( HdfsClientConfigKeys.Read.ShortCircuit.KEY, HdfsClientConfigKeys.Read.ShortCircuit.DEFAULT) || getConf().getBoolean( HdfsClientConfigKeys.DFS_CLIENT_DOMAIN_SOCKET_DATA_TRAFFIC, HdfsClientConfigKeys .DFS_CLIENT_DOMAIN_SOCKET_DATA_TRAFFIC_DEFAULT)) {	DomainPeerServer domainPeerServer = getDomainPeerServer(getConf(), streamingAddr.getPort());	if (domainPeerServer != null) {	this.localDataXceiverServer = new Daemon(threadGroup, new DataXceiverServer(domainPeerServer, getConf(), this));	
listening on unix domain socket 

private static DomainPeerServer getDomainPeerServer(Configuration conf, int port) throws IOException {	String domainSocketPath = conf.getTrimmed(DFSConfigKeys.DFS_DOMAIN_SOCKET_PATH_KEY, DFSConfigKeys.DFS_DOMAIN_SOCKET_PATH_DEFAULT);	if (domainSocketPath.isEmpty()) {	if (conf.getBoolean(HdfsClientConfigKeys.Read.ShortCircuit.KEY, HdfsClientConfigKeys.Read.ShortCircuit.DEFAULT) && (!conf.getBoolean(HdfsClientConfigKeys.DFS_CLIENT_USE_LEGACY_BLOCKREADERLOCAL, HdfsClientConfigKeys.DFS_CLIENT_USE_LEGACY_BLOCKREADERLOCAL_DEFAULT))) {	
although short circuit local reads are configured they are disabled because you didn t configure 

public void reportBadBlocks(ExtendedBlock block) throws IOException{	FsVolumeSpi volume = getFSDataset().getVolume(block);	if (volume == null) {	
cannot find fsvolumespi to report bad block 

private void checkBlockToken(ExtendedBlock block, Token<BlockTokenIdentifier> token, AccessMode accessMode) throws IOException {	if (isBlockTokenEnabled) {	BlockTokenIdentifier id = new BlockTokenIdentifier();	ByteArrayInputStream buf = new ByteArrayInputStream(token.getIdentifier());	DataInputStream in = new DataInputStream(buf);	id.readFields(in);	if (LOG.isDebugEnabled()) {	
got 

public void shutdown() {	stopMetricsLogger();	if (plugins != null) {	for (ServicePlugin p : plugins) {	try {	p.stop();	
stopped plug in 

public void shutdown() {	stopMetricsLogger();	if (plugins != null) {	for (ServicePlugin p : plugins) {	try {	p.stop();	} catch (Throwable t) {	
serviceplugin could not be stopped 

List<BPOfferService> bposArray = (this.blockPoolManager == null) ? new ArrayList<BPOfferService>() : this.blockPoolManager.getAllNamenodeThreads();	if (!shutdownForUpgrade) {	shouldRun = false;	}	if (dataXceiverServer != null) {	try {	xserver.sendOOBToPeers();	((DataXceiverServer) this.dataXceiverServer.getRunnable()).kill();	this.dataXceiverServer.interrupt();	} catch (Exception e) {	
exception interrupting dataxceiverserver 

long timeNotified = Time.monotonicNow();	if (localDataXceiverServer != null) {	((DataXceiverServer) this.localDataXceiverServer.getRunnable()).kill();	this.localDataXceiverServer.interrupt();	}	shutdownPeriodicScanners();	if (httpServer != null) {	try {	httpServer.close();	} catch (Exception e) {	
exception shutting down datanode httpserver 

}	this.shouldRun = false;	shutdownReconfigurationTask();	if (this.threadGroup != null) {	int sleepMs = 2;	while (true) {	if (!this.shutdownForUpgrade || (this.shutdownForUpgrade && (Time.monotonicNow() - timeNotified > 1000))) {	this.threadGroup.interrupt();	break;	}	
waiting for threadgroup to exit active threads is 

} catch (InterruptedException ie) {	}	}	if (ipcServer != null) {	ipcServer.stop();	}	if(blockPoolManager != null) {	try {	this.blockPoolManager.shutDownAll(bposArray);	} catch (InterruptedException ie) {	
received exception in blockpoolmanager shutdownall 

if(blockPoolManager != null) {	try {	this.blockPoolManager.shutDownAll(bposArray);	} catch (InterruptedException ie) {	}	}	if (storage != null) {	try {	this.storage.unlockAll();	} catch (IOException ie) {	
exception when unlocking storage 

metrics.shutdown();	}	if (diskMetrics != null) {	diskMetrics.shutdownAndWait();	}	if (dataNodeInfoBeanName != null) {	MBeans.unregister(dataNodeInfoBeanName);	dataNodeInfoBeanName = null;	}	if (shortCircuitRegistry != null) shortCircuitRegistry.shutdown();	
shutdown complete 

public void checkDiskErrorAsync(FsVolumeSpi volume) {	volumeChecker.checkVolume( volume, new DatasetVolumeChecker.Callback() {	public void call(Set<FsVolumeSpi> healthyVolumes, Set<FsVolumeSpi> failedVolumes) {	if (failedVolumes.size() > 0) {	
checkdiskerrorasync callback got failed volumes 

public void checkDiskErrorAsync(FsVolumeSpi volume) {	volumeChecker.checkVolume( volume, new DatasetVolumeChecker.Callback() {	public void call(Set<FsVolumeSpi> healthyVolumes, Set<FsVolumeSpi> failedVolumes) {	if (failedVolumes.size() > 0) {	} else {	
checkdiskerrorasync no volume failures detected 

private void handleDiskError(String failedVolumes) {	final boolean hasEnoughResources = data.hasEnoughResource();	
datanode handlediskerror on keep running 

final boolean hasEnoughResources = data.hasEnoughResource();	int dpError = hasEnoughResources ? DatanodeProtocol.DISK_ERROR : DatanodeProtocol.FATAL_DISK_ERROR;	metrics.incrVolumeFailures();	for(BPOfferService bpos: blockPoolManager.getAllNamenodeThreads()) {	bpos.trySendErrorReport(dpError, failedVolumes);	}	if(hasEnoughResources) {	scheduleAllBlockReport(0);	return;	}	
datanode is shutting down due to failed volumes 

private void reportBadBlock(final BPOfferService bpos, final ExtendedBlock block, final String msg) {	FsVolumeSpi volume = getFSDataset().getVolume(block);	if (volume == null) {	
cannot find fsvolumespi to report bad block 

xmitsInProgress.getAndIncrement();	Socket sock = null;	DataOutputStream out = null;	DataInputStream in = null;	BlockSender blockSender = null;	final boolean isClient = clientname.length() > 0;	try {	final String dnAddr = targets[0].getXferAddr(connectToDnViaHostname);	InetSocketAddress curTarget = NetUtils.createSocketAddr(dnAddr);	if (LOG.isDebugEnabled()) {	
connecting to datanode 

throw new InvalidBlockTokenException( "Got access token error for connect ack, targets=" + Arrays.asList(targets));	} else {	throw new IOException("Bad connect ack, targets=" + Arrays.asList(targets) + " status=" + closeAck.getStatus());	}	}	} else {	metrics.incrBlocksReplicated();	}	} catch (IOException ie) {	if (ie instanceof InvalidChecksumSizeException) {	
adding block for scanning 

throw new IOException("Bad connect ack, targets=" + Arrays.asList(targets) + " status=" + closeAck.getStatus());	}	}	} else {	metrics.incrBlocksReplicated();	}	} catch (IOException ie) {	if (ie instanceof InvalidChecksumSizeException) {	blockScanner.markSuspectBlock(data.getVolume(b).getStorageID(), b);	}	
failed to transfer to got 

}	} else {	metrics.incrBlocksReplicated();	}	} catch (IOException ie) {	if (ie instanceof InvalidChecksumSizeException) {	blockScanner.markSuspectBlock(data.getVolume(b).getStorageID(), b);	}	IOException cause = DatanodeUtil.getCauseIfDiskError(ie);	if (cause != null) {	
ioexception in datatransfer run cause is 

public static List<StorageLocation> getStorageLocations(Configuration conf) {	Collection<String> rawLocations = conf.getTrimmedStringCollection(DFS_DATANODE_DATA_DIR_KEY);	List<StorageLocation> locations = new ArrayList<StorageLocation>(rawLocations.size());	for(String locationString : rawLocations) {	final StorageLocation location;	try {	location = StorageLocation.parse(locationString);	} catch (IOException ioe) {	
failed to initialize storage directory exception details 

public static List<StorageLocation> getStorageLocations(Configuration conf) {	Collection<String> rawLocations = conf.getTrimmedStringCollection(DFS_DATANODE_DATA_DIR_KEY);	List<StorageLocation> locations = new ArrayList<StorageLocation>(rawLocations.size());	for(String locationString : rawLocations) {	final StorageLocation location;	try {	location = StorageLocation.parse(locationString);	} catch (IOException ioe) {	continue;	} catch (SecurityException se) {	
failed to initialize storage directory exception details 

static boolean parseArguments(String args[], Configuration conf) {	StartupOption startOpt = StartupOption.REGULAR;	int i = 0;	if (args != null && args.length != 0) {	String cmd = args[i++];	if ("-r".equalsIgnoreCase(cmd) || "--rack".equalsIgnoreCase(cmd)) {	
r rack arguments are not supported anymore rackid resolution is handled by the namenode 

int errorCode = 0;	try {	StringUtils.startupShutdownMessage(DataNode.class, args, LOG);	DataNode datanode = createDataNode(args, null, resources);	if (datanode != null) {	datanode.join();	} else {	errorCode = 1;	}	} catch (Throwable e) {	
exception in securemain 

StringUtils.startupShutdownMessage(DataNode.class, args, LOG);	DataNode datanode = createDataNode(args, null, resources);	if (datanode != null) {	datanode.join();	} else {	errorCode = 1;	}	} catch (Throwable e) {	terminate(1, e);	} finally {	
exiting datanode 

throw new org.apache.hadoop.ipc.RetriableException( "Datanode not registered. Try again later.");	}	if (isBlockTokenEnabled) {	Set<TokenIdentifier> tokenIds = UserGroupInformation.getCurrentUser() .getTokenIdentifiers();	if (tokenIds.size() != 1) {	throw new IOException("Can't continue since none or more than one " + "BlockTokenIdentifier is found.");	}	for (TokenIdentifier tokenId : tokenIds) {	BlockTokenIdentifier id = (BlockTokenIdentifier) tokenId;	if (LOG.isDebugEnabled()) {	
got 

public void deleteBlockPool(String blockPoolId, boolean force) throws IOException {	checkSuperuserPrivilege();	LOG.info("deleteBlockPool command received for block pool " + blockPoolId + ", force=" + force);	if (blockPoolManager.get(blockPoolId) != null) {	
the block pool is still running cannot be deleted 

public void evictWriters() throws IOException {	checkSuperuserPrivilege();	
evicting all writers 

public void checkDiskError() throws IOException {	Set<FsVolumeSpi> unhealthyVolumes;	try {	unhealthyVolumes = volumeChecker.checkAllVolumes(data);	lastDiskErrorCheck = Time.monotonicNow();	} catch (InterruptedException e) {	
interruped while running disk check 

public void checkDiskError() throws IOException {	Set<FsVolumeSpi> unhealthyVolumes;	try {	unhealthyVolumes = volumeChecker.checkAllVolumes(data);	lastDiskErrorCheck = Time.monotonicNow();	} catch (InterruptedException e) {	throw new IOException("Interrupted while running disk check", e);	}	if (unhealthyVolumes.size() > 0) {	
checkdiskerror got failed volumes 

Set<FsVolumeSpi> unhealthyVolumes;	try {	unhealthyVolumes = volumeChecker.checkAllVolumes(data);	lastDiskErrorCheck = Time.monotonicNow();	} catch (InterruptedException e) {	throw new IOException("Interrupted while running disk check", e);	}	if (unhealthyVolumes.size() > 0) {	handleVolumeFailures(unhealthyVolumes);	} else {	
checkdiskerror encountered no failures 

private void handleVolumeFailures(Set<FsVolumeSpi> unhealthyVolumes) {	if (unhealthyVolumes.isEmpty()) {	
handlevolumefailures done with empty unhealthyVolumes 

data.handleVolumeFailures(unhealthyVolumes);	final Set<File> unhealthyDirs = new HashSet<>(unhealthyVolumes.size());	StringBuilder sb = new StringBuilder("DataNode failed volumes:");	for (FsVolumeSpi vol : unhealthyVolumes) {	unhealthyDirs.add(new File(vol.getBasePath()).getAbsoluteFile());	sb.append(vol).append(";");	}	try {	removeVolumes(unhealthyDirs, false);	} catch (IOException e) {	
error occurred when removing unhealthy storage dirs 

public List<DatanodeVolumeInfo> getVolumeReport() throws IOException {	checkSuperuserPrivilege();	Map<String, Object> volumeInfoMap = data.getVolumeInfoMap();	if (volumeInfoMap == null) {	
datanode volume info not available 

========================= hadoop sample_7911 =========================

lfs.mkdir(workDir, FsPermission.getDirDefault(), true);	} catch (IOException e) {	throw new RuntimeException(e);	}	Configuration conf = new Configuration();	yarnImage = "yarnImage";	long time = System.currentTimeMillis();	conf.set(YarnConfiguration.NM_LOCAL_DIRS, "/tmp/nm-local-dir" + time);	conf.set(YarnConfiguration.NM_LOG_DIRS, "/tmp/userlogs" + time);	dockerUrl = System.getProperty("docker-service-url");	
dockerurl 

private boolean isDockerDaemonRunningLocally() {	boolean dockerDaemonRunningLocally = true;	try {	shellExec("docker info");	} catch (Exception e) {	
docker daemon is not running on local machine 

public void testLaunchContainer() throws IOException {	if (!shouldRun()) {	
docker not installed aborting test 

========================= hadoop sample_1596 =========================

private static void execWaitRet(String cmd) throws IOException {	
exec 

private static void execIgnoreRet(String cmd) throws IOException {	
exec 

private static void execAssertSucceeds(String cmd) throws IOException {	
exec 

private static void execAssertFails(String cmd) throws IOException {	
exec 

public void run() {	try {	InputStreamReader isr = new InputStreamReader(is);	BufferedReader br = new BufferedReader(isr);	String line=null;	while ( (line = br.readLine()) != null) {	
fuse line 

fuseCmd, nameNode, mountPoint, "-obig_writes", "-oentry_timeout=0.1", "-oattribute_timeout=0.1", "-ononempty", "-f", "-ordbuffer=32768", "rw" };	String [] env = {	"CLASSPATH="+cp, "LD_LIBRARY_PATH="+lp, "PATH=/usr/bin:/bin" };	execWaitRet("fusermount -u " + mountPoint);	execAssertSucceeds("rm -rf " + mountPoint);	execAssertSucceeds("mkdir -p " + mountPoint);	String cmdStr = "";	for (String c : mountCmd) {	cmdStr += (" " + c);	}	
now mounting with 

========================= hadoop sample_6674 =========================

private void initBloomFilter(Path dirName, Configuration conf) {	DataInputStream in = null;	try {	FileSystem fs = dirName.getFileSystem(conf);	in = fs.open(new Path(dirName, BLOOM_FILE_NAME));	bloomFilter = new DynamicBloomFilter();	bloomFilter.readFields(in);	in.close();	in = null;	} catch (IOException ioe) {	
can t open bloomfilter fallback to mapfile 

========================= hadoop sample_3925 =========================

public MonitoringTimerTask(Configuration conf) throws YarnRuntimeException {	float highUsableSpacePercentagePerDisk = conf.getFloat( YarnConfiguration.NM_MAX_PER_DISK_UTILIZATION_PERCENTAGE, YarnConfiguration.DEFAULT_NM_MAX_PER_DISK_UTILIZATION_PERCENTAGE);	float lowUsableSpacePercentagePerDisk = conf.getFloat( YarnConfiguration.NM_WM_LOW_PER_DISK_UTILIZATION_PERCENTAGE, highUsableSpacePercentagePerDisk);	if (lowUsableSpacePercentagePerDisk > highUsableSpacePercentagePerDisk) {	
using as because is not configured properly 

private void updateDirsAfterTest() {	Configuration conf = getConfig();	List<String> localDirs = getLocalDirs();	conf.setStrings(NM_GOOD_LOCAL_DIRS, localDirs.toArray(new String[localDirs.size()]));	List<String> logDirs = getLogDirs();	conf.setStrings(NM_GOOD_LOG_DIRS, logDirs.toArray(new String[logDirs.size()]));	if (!areDisksHealthy()) {	
most of the disks failed 

FileSystem localFS = FileSystem.getLocal(getConfig());	for (String dir : dirs) {	try {	Path tmpDir = new Path(dir);	File tmpFile = tmpDir.isAbsolute() ? new File(localFS.makeQualified(tmpDir).toUri()) : new File(dir);	Path file = new Path(tmpFile.getPath(), pathStr);	if (localFS.exists(file)) {	return file;	}	} catch (IOException ie) {	
failed to find at 

public static String[] validatePaths(String[] paths) {	ArrayList<String> validPaths = new ArrayList<String>();	for (int i = 0; i < paths.length; ++i) {	try {	URI uriPath = (new Path(paths[i])).toUri();	if (uriPath.getScheme() == null || uriPath.getScheme().equals(FILE_SCHEME)) {	validPaths.add(new Path(uriPath.getPath()).toString());	} else {	
is not a valid path path should be with scheme or without scheme 

========================= hadoop sample_1918 =========================

query.append(" AS ").append(tableName);	}	query.append(" WHERE ");	if (conditions != null && conditions.length() > 0) {	query.append("( ").append(conditions).append(" ) AND ");	}	query.append(conditionClauses.toString());	} else {	String inputQuery = dbConf.getInputQuery();	if (inputQuery.indexOf(DataDrivenDBInputFormat.SUBSTITUTE_TOKEN) == -1) {	
could not find the clause substitution token in the query parallel splits may not work correctly 

if (conditions != null && conditions.length() > 0) {	query.append("( ").append(conditions).append(" ) AND ");	}	query.append(conditionClauses.toString());	} else {	String inputQuery = dbConf.getInputQuery();	if (inputQuery.indexOf(DataDrivenDBInputFormat.SUBSTITUTE_TOKEN) == -1) {	}	query.append(inputQuery.replace(DataDrivenDBInputFormat.SUBSTITUTE_TOKEN, conditionClauses.toString()));	}	
using query 

========================= hadoop sample_5058 =========================

reset();	end(stream);	if (conf == null) {	stream = init(blockSize, workFactor);	return;	}	blockSize = Bzip2Factory.getBlockSize(conf);	workFactor = Bzip2Factory.getWorkFactor(conf);	stream = init(blockSize, workFactor);	if(LOG.isDebugEnabled()) {	
reinit compressor with new compression configuration 

========================= hadoop sample_3860 =========================

public boolean checkAccess(UserGroupInformation callerUGI, ApplicationAccessType applicationAccessType, String applicationOwner, ApplicationId applicationId) {	if (LOG.isDebugEnabled()) {	
verifying access type for on application owned by 

if (LOG.isDebugEnabled()) {	}	String user = callerUGI.getShortUserName();	if (!areACLsEnabled()) {	return true;	}	AccessControlList applicationACL = DEFAULT_YARN_APP_ACL;	Map<ApplicationAccessType, AccessControlList> acls = this.applicationACLS .get(applicationId);	if (acls == null) {	if (LOG.isDebugEnabled()) {	
acl not found for application owned by using default 

AccessControlList applicationACL = DEFAULT_YARN_APP_ACL;	Map<ApplicationAccessType, AccessControlList> acls = this.applicationACLS .get(applicationId);	if (acls == null) {	if (LOG.isDebugEnabled()) {	}	} else {	AccessControlList applicationACLInMap = acls.get(applicationAccessType);	if (applicationACLInMap != null) {	applicationACL = applicationACLInMap;	} else if (LOG.isDebugEnabled()) {	
acl not found for access type for application owned by using default 

========================= hadoop sample_2288 =========================

public void interrupted(IrqHandler.InterruptData data) {	
interrupt caught 

========================= hadoop sample_3163 =========================

public void add(Node node) {	if (node==null) return;	int newDepth = NodeBase.locationToDepth(node.getNetworkLocation()) + 1;	netlock.writeLock().lock();	try {	if( node instanceof InnerNode ) {	throw new IllegalArgumentException( "Not allow to add an inner node: "+NodeBase.getPath(node));	}	if ((depthOfAllLeaves != -1) && (depthOfAllLeaves != newDepth)) {	
error can t add leaf node at depth to topology 

throw new IllegalArgumentException( "Not allow to add an inner node: "+NodeBase.getPath(node));	}	if ((depthOfAllLeaves != -1) && (depthOfAllLeaves != newDepth)) {	throw new InvalidTopologyException("Failed to add " + NodeBase.getPath(node) + ": You cannot have a rack and a non-rack node at the same " + "level of the network topology.");	}	Node rack = getNodeForNetworkLocation(node);	if (rack != null && !(rack instanceof InnerNode)) {	throw new IllegalArgumentException("Unexpected data node " + node.toString() + " at an illegal network location");	}	if (clusterMap.add(node)) {	
adding a new node 

if (clusterMap.add(node)) {	if (rack == null) {	incrementRacks();	}	if (!(node instanceof InnerNode)) {	if (depthOfAllLeaves == -1) {	depthOfAllLeaves = node.getLevel();	}	}	}	
networktopology became 

public void remove(Node node) {	if (node==null) return;	if( node instanceof InnerNode ) {	throw new IllegalArgumentException( "Not allow to remove an inner node: "+NodeBase.getPath(node));	}	
removing a node 

throw new IllegalArgumentException( "Not allow to remove an inner node: "+NodeBase.getPath(node));	}	netlock.writeLock().lock();	try {	if (clusterMap.remove(node)) {	InnerNode rack = (InnerNode)getNode(node.getNetworkLocation());	if (rack == null) {	numOfRacks--;	}	}	
networktopology became 

public int getDistance(Node node1, Node node2) {	if ((node1 != null && node1.equals(node2)) || (node1 == null && node2 == null))  {	return 0;	}	if (node1 == null || node2 == null) {	
one of the nodes is a null pointer 

}	while(n1!=null && n2!=null && n1.getParent()!=n2.getParent()) {	n1=n1.getParent();	n2=n2.getParent();	dis+=2;	}	} finally {	netlock.readLock().unlock();	}	if (n1==null) {	
the cluster does not contain node 

n2=n2.getParent();	dis+=2;	}	} finally {	netlock.readLock().unlock();	}	if (n1==null) {	return Integer.MAX_VALUE;	}	if (n2==null) {	
the cluster does not contain node 

static public int getDistanceByPath(Node node1, Node node2) {	if (node1 == null && node2 == null) {	return 0;	}	if (node1 == null || node2 == null) {	
one of the nodes is a null pointer 

availableNodes = countNumOfAvailableNodes("~" + excludedScope, excludedNodes);	}	LOG.debug("Choosing random from {} available nodes on node {}," + " scope={}, excludedScope={}, excludeNodes={}", availableNodes, innerNode, scope, excludedScope, excludedNodes);	if (availableNodes > 0) {	do {	int leaveIndex = r.nextInt(numOfDatanodes);	ret = innerNode.getLeaf(leaveIndex, node);	if (excludedNodes == null || !excludedNodes.contains(ret)) {	break;	} else {	
node is excluded continuing 

if (availableNodes > 0) {	do {	int leaveIndex = r.nextInt(numOfDatanodes);	ret = innerNode.getLeaf(leaveIndex, node);	if (excludedNodes == null || !excludedNodes.contains(ret)) {	break;	} else {	}	} while (true);	}	
chooserandom returning 

========================= hadoop sample_3533 =========================

int retCode = 1;	ProcessBuilder pb = new ProcessBuilder(command);	try {	Process p = pb.start();	OutputBufferThread stdOut = new OutputBufferThread(p.getInputStream());	OutputBufferThread stdErr = new OutputBufferThread(p.getErrorStream());	stdOut.start();	stdErr.start();	retCode = p.waitFor();	if (retCode != 0) {	
failed with error code 

mojo.getLog().debug(s);	}	}	stdOut.join();	stdErr.join();	output.addAll(stdOut.getOutput());	if (errors != null) {	errors.addAll(stdErr.getOutput());	}	} catch (IOException ioe) {	
failed 

}	}	stdOut.join();	stdErr.join();	output.addAll(stdOut.getOutput());	if (errors != null) {	errors.addAll(stdErr.getOutput());	}	} catch (IOException ioe) {	} catch (InterruptedException ie) {	
failed 

========================= hadoop sample_4425 =========================

public void handle(Signal s) {	signalCount.incrementAndGet();	InterruptData data = new InterruptData(s.getName(), s.getNumber());	
interrupted 

========================= hadoop sample_4075 =========================

public void middleStep() throws Exception {	ContainerSimulator cs = null;	synchronized(completedContainerList) {	while ((cs = containerQueue.poll()) != null) {	runningContainers.remove(cs.getId());	completedContainerList.add(cs.getId());	
container has completed 

ns.setNodeHealthStatus(NodeHealthStatus.newInstance(true, "", 0));	beatRequest.setNodeStatus(ns);	NodeHeartbeatResponse beatResponse = rm.getResourceTrackerService().nodeHeartbeat(beatRequest);	if (! beatResponse.getContainersToCleanup().isEmpty()) {	synchronized(releasedContainerList) {	for (ContainerId containerId : beatResponse.getContainersToCleanup()){	if (amContainerList.contains(containerId)) {	synchronized(amContainerList) {	amContainerList.remove(containerId);	}	
nodemanager releases an am 

synchronized(releasedContainerList) {	for (ContainerId containerId : beatResponse.getContainersToCleanup()){	if (amContainerList.contains(containerId)) {	synchronized(amContainerList) {	amContainerList.remove(containerId);	}	} else {	cs = runningContainers.remove(containerId);	containerQueue.remove(cs);	releasedContainerList.add(containerId);	
nodemanager releases a container 

for (ContainerSimulator container : runningContainers.values()) {	csList.add(newContainerStatus(container.getId(), ContainerState.RUNNING, ContainerExitStatus.SUCCESS));	}	synchronized(amContainerList) {	for (ContainerId cId : amContainerList) {	csList.add(newContainerStatus(cId, ContainerState.RUNNING, ContainerExitStatus.SUCCESS));	}	}	synchronized(completedContainerList) {	for (ContainerId cId : completedContainerList) {	
nodemanager completed container 

}	}	synchronized(completedContainerList) {	for (ContainerId cId : completedContainerList) {	csList.add(newContainerStatus( cId, ContainerState.COMPLETE, ContainerExitStatus.SUCCESS));	}	completedContainerList.clear();	}	synchronized(releasedContainerList) {	for (ContainerId cId : releasedContainerList) {	
nodemanager released container 

public void addNewContainer(Container container, long lifeTimeMS) {	
nodemanager launches a new container 

========================= hadoop sample_6048 =========================

public RegisterApplicationMasterResponse createAndRegisterApplicationMaster( RegisterApplicationMasterRequest request) throws YarnException, IOException {	this.registerRequest = request;	UnmanagedAMIdentifier identifier = initializeUnmanagedAM(this.applicationId);	try {	this.userUgi = UserGroupInformation.createProxyUser( identifier.getAttemptId().toString(), UserGroupInformation.getCurrentUser());	} catch (IOException e) {	
exception while trying to get current user 

public RegisterApplicationMasterResponse createAndRegisterApplicationMaster( RegisterApplicationMasterRequest request) throws YarnException, IOException {	this.registerRequest = request;	UnmanagedAMIdentifier identifier = initializeUnmanagedAM(this.applicationId);	try {	this.userUgi = UserGroupInformation.createProxyUser( identifier.getAttemptId().toString(), UserGroupInformation.getCurrentUser());	} catch (IOException e) {	throw new YarnRuntimeException(e);	}	this.rmProxy = createRMProxy(ApplicationMasterProtocol.class, this.conf, this.userUgi, identifier.getToken());	
registering the unmanaged application master 

public FinishApplicationMasterResponse finishApplicationMaster( FinishApplicationMasterRequest request) throws YarnException, IOException {	this.handlerThread.shutdown();	if (this.rmProxy == null) {	if (this.registerRequest != null) {	
unmanaged am still not successfully launched registered yet stopping the uam client thread anyways 

public void allocateAsync(AllocateRequest request, AsyncCallback<AllocateResponse> callback) throws YarnException {	try {	this.requestQueue.put(new AsyncAllocateRequestInfo(request, callback));	} catch (InterruptedException ex) {	
interrupted while waiting to put on response queue 

public void allocateAsync(AllocateRequest request, AsyncCallback<AllocateResponse> callback) throws YarnException {	try {	this.requestQueue.put(new AsyncAllocateRequestInfo(request, callback));	} catch (InterruptedException ex) {	}	if (this.rmProxy == null) {	if (this.registerRequest != null) {	
unmanaged am still not successfully launched registered yet saving the allocate request and send later 

context.setQueue(this.conf.get(DEFAULT_QUEUE_CONFIG, YarnConfiguration.DEFAULT_QUEUE_NAME));	} else {	context.setQueue(this.queueName);	}	ContainerLaunchContext amContainer = this.recordFactory.newRecordInstance(ContainerLaunchContext.class);	Resource resource = BuilderUtils.newResource(1024, 1);	context.setResource(resource);	context.setAMContainerSpec(amContainer);	submitRequest.setApplicationSubmissionContext(context);	context.setUnmanagedAM(true);	
submitting unmanaged application 

while (true) {	if (appAttemptId == null) {	ApplicationReport report = getApplicationReport(appId);	YarnApplicationState state = report.getYarnApplicationState();	if (appStates.contains(state)) {	if (state != YarnApplicationState.ACCEPTED) {	throw new YarnRuntimeException( "Received non-accepted application state: " + state + ". Application " + appId + " not the first attempt?");	}	appAttemptId = getApplicationReport(appId).getCurrentApplicationAttemptId();	} else {	
current application state of is will retry later 

} else {	}	}	if (appAttemptId != null) {	GetApplicationAttemptReportRequest req = this.recordFactory .newRecordInstance(GetApplicationAttemptReportRequest.class);	req.setApplicationAttemptId(appAttemptId);	ApplicationAttemptReport attemptReport = this.rmClient .getApplicationAttemptReport(req).getApplicationAttemptReport();	if (attemptState .equals(attemptReport.getYarnApplicationAttemptState())) {	return attemptReport;	}	
current attempt state of is waiting for current attempt to reach 

GetApplicationAttemptReportRequest req = this.recordFactory .newRecordInstance(GetApplicationAttemptReportRequest.class);	req.setApplicationAttemptId(appAttemptId);	ApplicationAttemptReport attemptReport = this.rmClient .getApplicationAttemptReport(req).getApplicationAttemptReport();	if (attemptState .equals(attemptReport.getYarnApplicationAttemptState())) {	return attemptReport;	}	}	try {	Thread.sleep(this.asyncApiPollIntervalMillis);	} catch (InterruptedException e) {	
interrupted while waiting for current attempt of to reach 

protected UnmanagedAMIdentifier getUAMIdentifier() throws IOException, YarnException {	Token<AMRMTokenIdentifier> token = null;	org.apache.hadoop.yarn.api.records.Token amrmToken = getApplicationReport(this.attemptId.getApplicationId()).getAMRMToken();	if (amrmToken != null) {	token = ConverterUtils.convertFromYarn(amrmToken, (Text) null);	} else {	
amrmtoken not found in the application report for application 

if (LOG.isDebugEnabled()) {	LOG.debug("Sending Heartbeat to Unmanaged AM. AskList:" + ((request.getAskList() == null) ? " empty" : request.getAskList().size()));	}	request.setResponseId(lastResponseId);	AllocateResponse response = AMRMClientUtils.allocateWithReRegister( request, rmProxy, registerRequest, attemptId);	if (response == null) {	throw new YarnException("Null allocateResponse from allocate");	}	lastResponseId = response.getResponseId();	if (response.getAMRMToken() != null) {	
received new amrmtoken 

}	if (LOG.isDebugEnabled()) {	LOG.debug("Received Heartbeat reply from RM. Allocated Containers:" + ((response.getAllocatedContainers() == null) ? " empty" : response.getAllocatedContainers().size()));	}	if (requestInfo.getCallback() == null) {	throw new YarnException("Null callback from requestInfo");	}	requestInfo.getCallback().callback(response);	} catch (InterruptedException ex) {	if (LOG.isDebugEnabled()) {	
interrupted while waiting for queue 

LOG.debug("Received Heartbeat reply from RM. Allocated Containers:" + ((response.getAllocatedContainers() == null) ? " empty" : response.getAllocatedContainers().size()));	}	if (requestInfo.getCallback() == null) {	throw new YarnException("Null callback from requestInfo");	}	requestInfo.getCallback().callback(response);	} catch (InterruptedException ex) {	if (LOG.isDebugEnabled()) {	}	} catch (IOException ex) {	
io error occurred while processing heart beat for 

}	if (requestInfo.getCallback() == null) {	throw new YarnException("Null callback from requestInfo");	}	requestInfo.getCallback().callback(response);	} catch (InterruptedException ex) {	if (LOG.isDebugEnabled()) {	}	} catch (IOException ex) {	} catch (Throwable ex) {	
error occurred while processing heart beat for 

throw new YarnException("Null callback from requestInfo");	}	requestInfo.getCallback().callback(response);	} catch (InterruptedException ex) {	if (LOG.isDebugEnabled()) {	}	} catch (IOException ex) {	} catch (Throwable ex) {	}	}	
unmanagedapplicationmanager has been stopped for amrequesthandlerthread thread is exiting 

public void uncaughtException(Thread t, Throwable e) {	
heartbeat thread for application attempt crashed 

========================= hadoop sample_1332 =========================

private void enqueueEdit(Edit edit) {	if (LOG.isDebugEnabled()) {	
logedit 

logSync(getLastWrittenTxId());	} catch (RuntimeException ex) {	syncEx = ex;	}	while ((edit = syncWaitQ.poll()) != null) {	edit.logSyncNotify(syncEx);	}	}	}	} catch (InterruptedException ie) {	
was interrupted exiting 

========================= hadoop sample_8113 =========================

public Resource getMaxShare() {	Resource maxResource = maxShare.getResource(scheduler.getClusterResource());	Resource result = Resources.componentwiseMax(maxResource, minShare);	if (!Resources.equals(maxResource, result)) {	
queue s has max resources s less than min resources s 

public void setFairShare(Resource fairShare) {	this.fairShare = fairShare;	metrics.setFairShare(fairShare);	if (LOG.isDebugEnabled()) {	
the updated fairshare for is 

public abstract List<FSQueue> getChildQueues();	public abstract void collectSchedulerApplications( Collection<ApplicationAttemptId> apps);	public abstract int getNumRunnableApps();	boolean assignContainerPreCheck(FSSchedulerNode node) {	if (node.getReservedContainer() != null) {	if (LOG.isDebugEnabled()) {	
assigning container failed on node because it has reserved containers 

public abstract List<FSQueue> getChildQueues();	public abstract void collectSchedulerApplications( Collection<ApplicationAttemptId> apps);	public abstract int getNumRunnableApps();	boolean assignContainerPreCheck(FSSchedulerNode node) {	if (node.getReservedContainer() != null) {	if (LOG.isDebugEnabled()) {	}	return false;	} else if (!Resources.fitsIn(getResourceUsage(), getMaxShare())) {	if (LOG.isDebugEnabled()) {	
assigning container failed on node because queue resource usage is larger than maxshare 

========================= hadoop sample_982 =========================

executor.shutdown();	Map<DatanodeInfo, HdfsBlocksMetadata> metadatas = Maps.newHashMapWithExpectedSize(datanodeBlocks.size());	for (int i = 0; i < futures.size(); i++) {	VolumeBlockLocationCallable callable = callables.get(i);	DatanodeInfo datanode = callable.getDatanodeInfo();	Future<HdfsBlocksMetadata> future = futures.get(i);	try {	HdfsBlocksMetadata metadata = future.get();	metadatas.put(callable.getDatanodeInfo(), metadata);	} catch (CancellationException e) {	
cancelled while waiting for datanode 

VolumeBlockLocationCallable callable = callables.get(i);	DatanodeInfo datanode = callable.getDatanodeInfo();	Future<HdfsBlocksMetadata> future = futures.get(i);	try {	HdfsBlocksMetadata metadata = future.get();	metadatas.put(callable.getDatanodeInfo(), metadata);	} catch (CancellationException e) {	} catch (ExecutionException e) {	Throwable t = e.getCause();	if (t instanceof InvalidBlockTokenException) {	
invalid access token when trying to retrieve information from datanode 

try {	HdfsBlocksMetadata metadata = future.get();	metadatas.put(callable.getDatanodeInfo(), metadata);	} catch (CancellationException e) {	} catch (ExecutionException e) {	Throwable t = e.getCause();	if (t instanceof InvalidBlockTokenException) {	throw (InvalidBlockTokenException) t;	}	else if (t instanceof UnsupportedOperationException) {	
datanode does not support required gethdfsblocksmetadata api 

metadatas.put(callable.getDatanodeInfo(), metadata);	} catch (CancellationException e) {	} catch (ExecutionException e) {	Throwable t = e.getCause();	if (t instanceof InvalidBlockTokenException) {	throw (InvalidBlockTokenException) t;	}	else if (t instanceof UnsupportedOperationException) {	throw (UnsupportedOperationException) t;	} else {	
failed to query block locations on datanode 

} catch (ExecutionException e) {	Throwable t = e.getCause();	if (t instanceof InvalidBlockTokenException) {	throw (InvalidBlockTokenException) t;	}	else if (t instanceof UnsupportedOperationException) {	throw (UnsupportedOperationException) t;	} else {	}	if (LOG.isDebugEnabled()) {	
could not fetch information from datanode 

if (t instanceof InvalidBlockTokenException) {	throw (InvalidBlockTokenException) t;	}	else if (t instanceof UnsupportedOperationException) {	throw (UnsupportedOperationException) t;	} else {	}	if (LOG.isDebugEnabled()) {	}	} catch (InterruptedException e) {	
interrupted while fetching hdfsblocksmetadata 

continue;	}	long[] metaBlockIds = metadata.getBlockIds();	List<byte[]> metaVolumeIds = metadata.getVolumeIds();	List<Integer> metaVolumeIndexes = metadata.getVolumeIndexes();	for (int j = 0; j < metaBlockIds.length; j++) {	int volumeIndex = metaVolumeIndexes.get(j);	long blockId = metaBlockIds[j];	if (volumeIndex == Integer.MAX_VALUE || volumeIndex >= metaVolumeIds.size() || !blockIdToLocBlock.containsKey(blockId)) {	if (LOG.isDebugEnabled()) {	
no data for block 

DatanodeInfo[] dnInfos = locBlock.getLocations();	int index = -1;	for (int k = 0; k < dnInfos.length; k++) {	if (dnInfos[k].equals(datanode)) {	index = k;	break;	}	}	if (index < 0) {	if (LOG.isDebugEnabled()) {	
datanode responded with a block volume id we did not request omitting 

========================= hadoop sample_6789 =========================

protected abstract Object doExecute(Object... arguments) throws Exception;	public Object execute(Object... arguments) throws Exception {	Exception latestException;	int counter = 0;	while (true) {	try {	return doExecute(arguments);	} catch(Exception exception) {	
failure in retriable command 

========================= hadoop sample_6273 =========================

String volumeName = volume.getBasePath();	metadataOpStats.put(volumeName, metrics.getMetadataOperationMean());	readIoStats.put(volumeName, metrics.getReadIoMean());	writeIoStats.put(volumeName, metrics.getWriteIoMean());	}	} finally {	if (fsVolumeReferences != null) {	try {	fsVolumeReferences.close();	} catch (IOException e) {	
error in releasing fs volume references 

}	} finally {	if (fsVolumeReferences != null) {	try {	fsVolumeReferences.close();	} catch (IOException e) {	}	}	}	if (metadataOpStats.isEmpty() && readIoStats.isEmpty() && writeIoStats.isEmpty()) {	
no disk stats available for detecting outliers 

}	}	}	if (metadataOpStats.isEmpty() && readIoStats.isEmpty() && writeIoStats.isEmpty()) {	return;	}	detectAndUpdateDiskOutliers(metadataOpStats, readIoStats, writeIoStats);	try {	Thread.sleep(detectionInterval);	} catch (InterruptedException e) {	
disk outlier detection thread interrupted 

}	Map<String, Double> readIoOutliers = slowDiskDetector .getOutliers(readIoStats);	for (Map.Entry<String, Double> entry : readIoOutliers.entrySet()) {	addDiskStat(diskStats, entry.getKey(), DiskOp.READ, entry.getValue());	}	Map<String, Double> writeIoOutliers = slowDiskDetector .getOutliers(writeIoStats);	for (Map.Entry<String, Double> entry : writeIoOutliers.entrySet()) {	addDiskStat(diskStats, entry.getKey(), DiskOp.WRITE, entry.getValue());	}	diskOutliersStats = diskStats;	
updated disk outliers 

public void shutdownAndWait() {	shouldRun = false;	slowDiskDetectionDaemon.interrupt();	try {	slowDiskDetectionDaemon.join();	} catch (InterruptedException e) {	
disk outlier detection daemon did not shutdown 

========================= hadoop sample_7874 =========================

assertEquals(addResponse.getResult(), 3);	Integer[] integers = new Integer[] {1, 2};	TestProtos.AddRequestProto2 addRequest2 = TestProtos.AddRequestProto2.newBuilder().addAllParams( Arrays.asList(integers)).build();	addResponse = proxy.add2(null, addRequest2);	assertEquals(addResponse.getResult(), 3);	boolean caught = false;	try {	proxy.error(null, newEmptyRequest());	} catch (ServiceException e) {	if(LOG.isDebugEnabled()) {	
caught 

UserGroupInformation.setConfiguration(serverConf);	server = setupTestServer(serverConf, 5);	boolean succeeded = false;	try {	UserGroupInformation.setConfiguration(conf);	proxy = getClient(addr, conf);	proxy.echo(null, newEchoRequest(""));	} catch (ServiceException e) {	assertTrue(e.getCause() instanceof RemoteException);	RemoteException re = (RemoteException) e.getCause();	
logging message 

UserGroupInformation.setConfiguration(serverConf);	server = setupTestServer(serverConf, 5);	succeeded = false;	proxy = null;	try {	UserGroupInformation.setConfiguration(conf);	proxy = getClient(addr, conf);	proxy.echo(null, newEchoRequest(""));	} catch (ServiceException e) {	RemoteException re = (RemoteException) e.getCause();	
logging message 

while (num == 0 || leaderRunning.get()) {	proxy.slowPing(null, newSlowPingRequest(false));	}	proxy.slowPing(null, newSlowPingRequest(false));	} catch (Exception e) {	if (num == 0) {	leaderRunning.set(false);	} else {	error.set(e);	}	
thread 

succeeded = true;	} else {	lastException = unwrapExeption;	}	}	} finally {	executorService.shutdown();	stop(server, proxy);	}	if (lastException != null) {	
last received non retriableexception 

GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	MetricsRecordBuilder rb2 = getMetrics("DecayRpcSchedulerMetrics2." + ns);	long decayedCallVolume1 = MetricsAsserts.getLongCounter( "DecayedCallVolume", rb2);	long rawCallVolume1 = MetricsAsserts.getLongCounter( "CallVolume", rb2);	int uniqueCaller1 = MetricsAsserts.getIntCounter( "UniqueCallers", rb2);	long callVolumePriority0 = MetricsAsserts.getLongGauge( "Priority.0.CompletedCallVolume", rb2);	long callVolumePriority1 = MetricsAsserts.getLongGauge( "Priority.1.CompletedCallVolume", rb2);	double avgRespTimePriority0 = MetricsAsserts.getDoubleGauge( "Priority.0.AvgResponseTime", rb2);	double avgRespTimePriority1 = MetricsAsserts.getDoubleGauge( "Priority.1.AvgResponseTime", rb2);	
decayedcallvolume 

GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	MetricsRecordBuilder rb2 = getMetrics("DecayRpcSchedulerMetrics2." + ns);	long decayedCallVolume1 = MetricsAsserts.getLongCounter( "DecayedCallVolume", rb2);	long rawCallVolume1 = MetricsAsserts.getLongCounter( "CallVolume", rb2);	int uniqueCaller1 = MetricsAsserts.getIntCounter( "UniqueCallers", rb2);	long callVolumePriority0 = MetricsAsserts.getLongGauge( "Priority.0.CompletedCallVolume", rb2);	long callVolumePriority1 = MetricsAsserts.getLongGauge( "Priority.1.CompletedCallVolume", rb2);	double avgRespTimePriority0 = MetricsAsserts.getDoubleGauge( "Priority.0.AvgResponseTime", rb2);	double avgRespTimePriority1 = MetricsAsserts.getDoubleGauge( "Priority.1.AvgResponseTime", rb2);	
callvolume 

GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	MetricsRecordBuilder rb2 = getMetrics("DecayRpcSchedulerMetrics2." + ns);	long decayedCallVolume1 = MetricsAsserts.getLongCounter( "DecayedCallVolume", rb2);	long rawCallVolume1 = MetricsAsserts.getLongCounter( "CallVolume", rb2);	int uniqueCaller1 = MetricsAsserts.getIntCounter( "UniqueCallers", rb2);	long callVolumePriority0 = MetricsAsserts.getLongGauge( "Priority.0.CompletedCallVolume", rb2);	long callVolumePriority1 = MetricsAsserts.getLongGauge( "Priority.1.CompletedCallVolume", rb2);	double avgRespTimePriority0 = MetricsAsserts.getDoubleGauge( "Priority.0.AvgResponseTime", rb2);	double avgRespTimePriority1 = MetricsAsserts.getDoubleGauge( "Priority.1.AvgResponseTime", rb2);	
uniquecaller 

GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	MetricsRecordBuilder rb2 = getMetrics("DecayRpcSchedulerMetrics2." + ns);	long decayedCallVolume1 = MetricsAsserts.getLongCounter( "DecayedCallVolume", rb2);	long rawCallVolume1 = MetricsAsserts.getLongCounter( "CallVolume", rb2);	int uniqueCaller1 = MetricsAsserts.getIntCounter( "UniqueCallers", rb2);	long callVolumePriority0 = MetricsAsserts.getLongGauge( "Priority.0.CompletedCallVolume", rb2);	long callVolumePriority1 = MetricsAsserts.getLongGauge( "Priority.1.CompletedCallVolume", rb2);	double avgRespTimePriority0 = MetricsAsserts.getDoubleGauge( "Priority.0.AvgResponseTime", rb2);	double avgRespTimePriority1 = MetricsAsserts.getDoubleGauge( "Priority.1.AvgResponseTime", rb2);	
priority completedcallvolume 

GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	MetricsRecordBuilder rb2 = getMetrics("DecayRpcSchedulerMetrics2." + ns);	long decayedCallVolume1 = MetricsAsserts.getLongCounter( "DecayedCallVolume", rb2);	long rawCallVolume1 = MetricsAsserts.getLongCounter( "CallVolume", rb2);	int uniqueCaller1 = MetricsAsserts.getIntCounter( "UniqueCallers", rb2);	long callVolumePriority0 = MetricsAsserts.getLongGauge( "Priority.0.CompletedCallVolume", rb2);	long callVolumePriority1 = MetricsAsserts.getLongGauge( "Priority.1.CompletedCallVolume", rb2);	double avgRespTimePriority0 = MetricsAsserts.getDoubleGauge( "Priority.0.AvgResponseTime", rb2);	double avgRespTimePriority1 = MetricsAsserts.getDoubleGauge( "Priority.1.AvgResponseTime", rb2);	
priority completedcallvolume 

GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	MetricsRecordBuilder rb2 = getMetrics("DecayRpcSchedulerMetrics2." + ns);	long decayedCallVolume1 = MetricsAsserts.getLongCounter( "DecayedCallVolume", rb2);	long rawCallVolume1 = MetricsAsserts.getLongCounter( "CallVolume", rb2);	int uniqueCaller1 = MetricsAsserts.getIntCounter( "UniqueCallers", rb2);	long callVolumePriority0 = MetricsAsserts.getLongGauge( "Priority.0.CompletedCallVolume", rb2);	long callVolumePriority1 = MetricsAsserts.getLongGauge( "Priority.1.CompletedCallVolume", rb2);	double avgRespTimePriority0 = MetricsAsserts.getDoubleGauge( "Priority.0.AvgResponseTime", rb2);	double avgRespTimePriority1 = MetricsAsserts.getDoubleGauge( "Priority.1.AvgResponseTime", rb2);	
priority avgresponsetime 

GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	MetricsRecordBuilder rb2 = getMetrics("DecayRpcSchedulerMetrics2." + ns);	long decayedCallVolume1 = MetricsAsserts.getLongCounter( "DecayedCallVolume", rb2);	long rawCallVolume1 = MetricsAsserts.getLongCounter( "CallVolume", rb2);	int uniqueCaller1 = MetricsAsserts.getIntCounter( "UniqueCallers", rb2);	long callVolumePriority0 = MetricsAsserts.getLongGauge( "Priority.0.CompletedCallVolume", rb2);	long callVolumePriority1 = MetricsAsserts.getLongGauge( "Priority.1.CompletedCallVolume", rb2);	double avgRespTimePriority0 = MetricsAsserts.getDoubleGauge( "Priority.0.AvgResponseTime", rb2);	double avgRespTimePriority1 = MetricsAsserts.getDoubleGauge( "Priority.1.AvgResponseTime", rb2);	
priority avgresponsetime 

double avgRespTimePriority1 = MetricsAsserts.getDoubleGauge( "Priority.1.AvgResponseTime", rb2);	return decayedCallVolume1 > beginDecayedCallVolume && rawCallVolume1 > beginRawCallVolume && uniqueCaller1 > beginUniqueCaller;	}	}, 30, 60000);	}	} finally {	executorService.shutdown();	stop(server, proxy);	}	if (lastException != null) {	
last received non retriableexception 

server = setupTestServer(builder);	try {	try {	Configuration c = new Configuration(conf);	c.setInt(CommonConfigurationKeys.IPC_CLIENT_RPC_TIMEOUT_KEY, 1000);	proxy = getClient(addr, c);	proxy.sleep(null, newSleepRequest(3000));	fail("RPC should time out.");	} catch (ServiceException e) {	assertTrue(e.getCause() instanceof SocketTimeoutException);	
got expected timeout 

}	try {	Configuration c = new Configuration(conf);	c.setBoolean(CommonConfigurationKeys.IPC_CLIENT_PING_KEY, false);	c.setInt(CommonConfigurationKeys.IPC_CLIENT_RPC_TIMEOUT_KEY, 1000);	proxy = getClient(addr, c);	proxy.sleep(null, newSleepRequest(3000));	fail("RPC should time out.");	} catch (ServiceException e) {	assertTrue(e.getCause() instanceof SocketTimeoutException);	
got expected timeout 

fail("RPC should time out.");	} catch (ServiceException e) {	assertTrue(e.getCause() instanceof SocketTimeoutException);	}	try {	Configuration c = new Configuration(conf);	c.setInt(CommonConfigurationKeys.IPC_CLIENT_RPC_TIMEOUT_KEY, -1);	proxy = getClient(addr, c);	proxy.sleep(null, newSleepRequest(2000));	} catch (ServiceException e) {	
got unexpected exception 

}	try {	Configuration c = new Configuration(conf);	c.setBoolean(CommonConfigurationKeys.IPC_CLIENT_PING_KEY, true);	c.setInt(CommonConfigurationKeys.IPC_PING_INTERVAL_KEY, 800);	c.setInt(CommonConfigurationKeys.IPC_CLIENT_RPC_TIMEOUT_KEY, 1000);	proxy = getClient(addr, c);	try {	proxy.sleep(null, newSleepRequest(1300));	} catch (ServiceException e) {	
got unexpected exception 

proxy = getClient(addr, c);	try {	proxy.sleep(null, newSleepRequest(1300));	} catch (ServiceException e) {	fail("RPC should not time out.");	}	proxy.sleep(null, newSleepRequest(2000));	fail("RPC should time out.");	} catch (ServiceException e) {	assertTrue(e.getCause() instanceof SocketTimeoutException);	
got expected timeout 

========================= hadoop sample_3134 =========================

public FederationStateStoreService(RMContext rmContext) {	super(FederationStateStoreService.class.getName());	
federationstatestoreservice initialized 

protected void serviceInit(Configuration conf) throws Exception {	this.config = conf;	RetryPolicy retryPolicy = FederationStateStoreFacade.createRetryPolicy(conf);	this.stateStoreClient = (FederationStateStore) FederationStateStoreFacade.createRetryInstance( conf, YarnConfiguration.FEDERATION_STATESTORE_CLIENT_CLASS, YarnConfiguration.DEFAULT_FEDERATION_STATESTORE_CLIENT_CLASS, FederationStateStore.class, retryPolicy);	this.stateStoreClient.init(conf);	
initialized state store client class 

protected void serviceInit(Configuration conf) throws Exception {	this.config = conf;	RetryPolicy retryPolicy = FederationStateStoreFacade.createRetryPolicy(conf);	this.stateStoreClient = (FederationStateStore) FederationStateStoreFacade.createRetryInstance( conf, YarnConfiguration.FEDERATION_STATESTORE_CLIENT_CLASS, YarnConfiguration.DEFAULT_FEDERATION_STATESTORE_CLIENT_CLASS, FederationStateStore.class, retryPolicy);	this.stateStoreClient.init(conf);	this.subClusterId = SubClusterId.newInstance(YarnConfiguration.getClusterId(conf));	heartbeatInterval = conf.getLong( YarnConfiguration.FEDERATION_STATESTORE_HEARTBEAT_INTERVAL_SECS, YarnConfiguration.DEFAULT_FEDERATION_STATESTORE_HEARTBEAT_INTERVAL_SECS);	if (heartbeatInterval <= 0) {	heartbeatInterval = YarnConfiguration.DEFAULT_FEDERATION_STATESTORE_HEARTBEAT_INTERVAL_SECS;	}	
initialized federation membership service 

protected void serviceStop() throws Exception {	Exception ex = null;	try {	if (this.scheduledExecutorService != null && !this.scheduledExecutorService.isShutdown()) {	this.scheduledExecutorService.shutdown();	
stopped federation membership heartbeat 

protected void serviceStop() throws Exception {	Exception ex = null;	try {	if (this.scheduledExecutorService != null && !this.scheduledExecutorService.isShutdown()) {	this.scheduledExecutorService.shutdown();	}	} catch (Exception e) {	
failed to shutdown scheduledexecutorservice 

private void registerAndInitializeHeartbeat() {	String clientRMAddress = getServiceAddress(rmContext.getClientRMService().getBindAddress());	String amRMAddress = getServiceAddress( rmContext.getApplicationMasterService().getBindAddress());	String rmAdminAddress = getServiceAddress( config.getSocketAddr(YarnConfiguration.RM_ADMIN_ADDRESS, YarnConfiguration.DEFAULT_RM_ADMIN_ADDRESS, YarnConfiguration.DEFAULT_RM_ADMIN_PORT));	String webAppAddress = getServiceAddress(NetUtils .createSocketAddr(WebAppUtils.getRMWebAppURLWithScheme(config)));	SubClusterInfo subClusterInfo = SubClusterInfo.newInstance(subClusterId, amRMAddress, clientRMAddress, rmAdminAddress, webAppAddress, SubClusterState.SC_NEW, ResourceManager.getClusterTimeStamp(), "");	try {	registerSubCluster(SubClusterRegisterRequest.newInstance(subClusterInfo));	
successfully registered for federation subcluster 

String webAppAddress = getServiceAddress(NetUtils .createSocketAddr(WebAppUtils.getRMWebAppURLWithScheme(config)));	SubClusterInfo subClusterInfo = SubClusterInfo.newInstance(subClusterId, amRMAddress, clientRMAddress, rmAdminAddress, webAppAddress, SubClusterState.SC_NEW, ResourceManager.getClusterTimeStamp(), "");	try {	registerSubCluster(SubClusterRegisterRequest.newInstance(subClusterInfo));	} catch (Exception e) {	throw new YarnRuntimeException( "Failed to register Federation membership with the StateStore", e);	}	stateStoreHeartbeat = new FederationStateStoreHeartbeat(subClusterId, stateStoreClient, rmContext.getScheduler());	scheduledExecutorService = HadoopExecutors.newSingleThreadScheduledExecutor();	scheduledExecutorService.scheduleWithFixedDelay(stateStoreHeartbeat, heartbeatInterval, heartbeatInterval, TimeUnit.SECONDS);	
started federation membership heartbeat with interval 

========================= hadoop sample_1109 =========================

addRecordStore(MountTableStoreImpl.class);	this.monitorService = new StateStoreConnectionMonitorService(this);	this.addService(monitorService);	MembershipState.setExpirationMs(conf.getLong( DFSConfigKeys.FEDERATION_STORE_MEMBERSHIP_EXPIRATION_MS, DFSConfigKeys.FEDERATION_STORE_MEMBERSHIP_EXPIRATION_MS_DEFAULT));	this.cacheUpdater = new StateStoreCacheUpdateService(this);	addService(this.cacheUpdater);	this.metrics = StateStoreMetrics.create(conf);	try {	StandardMBean bean = new StandardMBean(metrics, StateStoreMBean.class);	ObjectName registeredObject = MBeans.register("Router", "StateStore", bean);	
registered statestorembean 

MembershipState.setExpirationMs(conf.getLong( DFSConfigKeys.FEDERATION_STORE_MEMBERSHIP_EXPIRATION_MS, DFSConfigKeys.FEDERATION_STORE_MEMBERSHIP_EXPIRATION_MS_DEFAULT));	this.cacheUpdater = new StateStoreCacheUpdateService(this);	addService(this.cacheUpdater);	this.metrics = StateStoreMetrics.create(conf);	try {	StandardMBean bean = new StandardMBean(metrics, StateStoreMBean.class);	ObjectName registeredObject = MBeans.register("Router", "StateStore", bean);	} catch (NotCompliantMBeanException e) {	throw new RuntimeException("Bad StateStoreMBean setup", e);	} catch (MetricsException e) {	
failed to register state store bean 

public void loadDriver() {	synchronized (this.driver) {	if (!isDriverReady()) {	String driverName = this.driver.getClass().getSimpleName();	if (this.driver.init( conf, getIdentifier(), getSupportedRecords(), metrics)) {	
connection to the state store driver is open and ready 

public void loadDriver() {	synchronized (this.driver) {	if (!isDriverReady()) {	String driverName = this.driver.getClass().getSimpleName();	if (this.driver.init( conf, getIdentifier(), getSupportedRecords(), metrics)) {	this.refreshCaches();	} else {	
cannot initialize state store driver 

if (isDriverReady()) {	List<StateStoreCache> cachesToUpdate = new LinkedList<>();	cachesToUpdate.addAll(cachesToUpdateInternal);	cachesToUpdate.addAll(cachesToUpdateExternal);	for (StateStoreCache cachedStore : cachesToUpdate) {	String cacheName = cachedStore.getClass().getSimpleName();	boolean result = false;	try {	result = cachedStore.loadCache(force);	} catch (IOException e) {	
error updating cache for 

for (StateStoreCache cachedStore : cachesToUpdate) {	String cacheName = cachedStore.getClass().getSimpleName();	boolean result = false;	try {	result = cachedStore.loadCache(force);	} catch (IOException e) {	result = false;	}	if (!result) {	success = false;	
cache update failed for cache 

result = cachedStore.loadCache(force);	} catch (IOException e) {	result = false;	}	if (!result) {	success = false;	}	}	} else {	success = false;	
skipping state store cache update driver is not ready 

========================= hadoop sample_8282 =========================

MyContainerManager containerManager = new MyContainerManager();	rm = new MockRMWithAMS(conf, containerManager);	rm.start();	MockNM nm1 = rm.registerNode("localhost:1234", 5120);	Map<ApplicationAccessType, String> acls = new HashMap<ApplicationAccessType, String>(2);	acls.put(ApplicationAccessType.VIEW_APP, "*");	RMApp app = rm.submitApp(1024, "appname", "appuser", acls);	nm1.nodeHeartbeat(true);	int waitCount = 0;	while (containerManager.containerTokens == null && waitCount++ < 20) {	
waiting for am launch to happen 

public void testUnauthorizedAccess() throws Exception {	MyContainerManager containerManager = new MyContainerManager();	rm = new MockRMWithAMS(conf, containerManager);	rm.start();	MockNM nm1 = rm.registerNode("localhost:1234", 5120);	RMApp app = rm.submitApp(1024);	nm1.nodeHeartbeat(true);	int waitCount = 0;	while (containerManager.containerTokens == null && waitCount++ < 40) {	
waiting for am launch to happen 

private void waitForLaunchedState(RMAppAttempt attempt) throws InterruptedException {	int waitCount = 0;	while (attempt.getAppAttemptState() != RMAppAttemptState.LAUNCHED && waitCount++ < 40) {	
waiting for appattempt to reach launched state current state is 

========================= hadoop sample_594 =========================

}	if (blockManager.shouldUpdateBlockKey(now - lastBlockKeyUpdate)) {	synchronized(HeartbeatManager.this) {	for(DatanodeDescriptor d : datanodes) {	d.setNeedKeyUpdate(true);	}	}	lastBlockKeyUpdate = now;	}	} catch (Exception e) {	
exception while checking heartbeat 

}	lastBlockKeyUpdate = now;	}	} catch (Exception e) {	}	try {	Thread.sleep(5000);	} catch (InterruptedException ignored) {	}	if (shouldAbortHeartbeatCheck(-5000)) {	
skipping next heartbeat scan due to excessive pause 

========================= hadoop sample_8328 =========================

conf.writeXml(new FileOutputStream(nodeHealthConfigFile));	conf.addResource(nodeHealthConfigFile.getName());	writeNodeHealthScriptFile("", true);	LocalDirsHandlerService dirsHandler = new LocalDirsHandlerService();	NodeHealthScriptRunner nodeHealthScriptRunner = spy(NodeManager.getNodeHealthScriptRunner(conf));	NodeHealthCheckerService nodeHealthChecker = new NodeHealthCheckerService( nodeHealthScriptRunner, dirsHandler);	nodeHealthChecker.init(conf);	doReturn(true).when(nodeHealthScriptRunner).isHealthy();	doReturn("").when(nodeHealthScriptRunner).getHealthReport();	setHealthStatus(healthStatus, nodeHealthChecker.isHealthy(), nodeHealthChecker.getHealthReport(), nodeHealthChecker.getLastHealthReportTime());	
checking initial healthy condition 

NodeHealthScriptRunner nodeHealthScriptRunner = spy(NodeManager.getNodeHealthScriptRunner(conf));	NodeHealthCheckerService nodeHealthChecker = new NodeHealthCheckerService( nodeHealthScriptRunner, dirsHandler);	nodeHealthChecker.init(conf);	doReturn(true).when(nodeHealthScriptRunner).isHealthy();	doReturn("").when(nodeHealthScriptRunner).getHealthReport();	setHealthStatus(healthStatus, nodeHealthChecker.isHealthy(), nodeHealthChecker.getHealthReport(), nodeHealthChecker.getLastHealthReportTime());	Assert.assertTrue("Node health status reported unhealthy", healthStatus .getIsNodeHealthy());	Assert.assertTrue("Node health status reported unhealthy", healthStatus .getHealthReport().equals(nodeHealthChecker.getHealthReport()));	doReturn(false).when(nodeHealthScriptRunner).isHealthy();	setHealthStatus(healthStatus, nodeHealthChecker.isHealthy(), nodeHealthChecker.getHealthReport(), nodeHealthChecker.getLastHealthReportTime());	
checking healthy unhealthy 

doReturn("").when(nodeHealthScriptRunner).getHealthReport();	setHealthStatus(healthStatus, nodeHealthChecker.isHealthy(), nodeHealthChecker.getHealthReport(), nodeHealthChecker.getLastHealthReportTime());	Assert.assertTrue("Node health status reported unhealthy", healthStatus .getIsNodeHealthy());	Assert.assertTrue("Node health status reported unhealthy", healthStatus .getHealthReport().equals(nodeHealthChecker.getHealthReport()));	doReturn(false).when(nodeHealthScriptRunner).isHealthy();	setHealthStatus(healthStatus, nodeHealthChecker.isHealthy(), nodeHealthChecker.getHealthReport(), nodeHealthChecker.getLastHealthReportTime());	Assert.assertFalse("Node health status reported healthy", healthStatus .getIsNodeHealthy());	Assert.assertTrue("Node health status reported healthy", healthStatus .getHealthReport().equals(nodeHealthChecker.getHealthReport()));	doReturn(true).when(nodeHealthScriptRunner).isHealthy();	setHealthStatus(healthStatus, nodeHealthChecker.isHealthy(), nodeHealthChecker.getHealthReport(), nodeHealthChecker.getLastHealthReportTime());	
checking unhealthy healthy 

setHealthStatus(healthStatus, nodeHealthChecker.isHealthy(), nodeHealthChecker.getHealthReport(), nodeHealthChecker.getLastHealthReportTime());	Assert.assertFalse("Node health status reported healthy", healthStatus .getIsNodeHealthy());	Assert.assertTrue("Node health status reported healthy", healthStatus .getHealthReport().equals(nodeHealthChecker.getHealthReport()));	doReturn(true).when(nodeHealthScriptRunner).isHealthy();	setHealthStatus(healthStatus, nodeHealthChecker.isHealthy(), nodeHealthChecker.getHealthReport(), nodeHealthChecker.getLastHealthReportTime());	Assert.assertTrue("Node health status reported unhealthy", healthStatus .getIsNodeHealthy());	Assert.assertTrue("Node health status reported unhealthy", healthStatus .getHealthReport().equals(nodeHealthChecker.getHealthReport()));	doReturn(false).when(nodeHealthScriptRunner).isHealthy();	doReturn(NodeHealthScriptRunner.NODE_HEALTH_SCRIPT_TIMED_OUT_MSG) .when(nodeHealthScriptRunner).getHealthReport();	setHealthStatus(healthStatus, nodeHealthChecker.isHealthy(), nodeHealthChecker.getHealthReport(), nodeHealthChecker.getLastHealthReportTime());	
checking healthy timeout 

========================= hadoop sample_1670 =========================

public static String readOutput(Path outDir, Configuration conf) throws IOException {	FileSystem fs = outDir.getFileSystem(conf);	StringBuffer result = new StringBuffer();	Path[] fileList = FileUtil.stat2Paths(fs.listStatus(outDir, new Utils.OutputFileUtils.OutputFilesFilter()));	for (Path outputFile : fileList) {	
Path 

========================= hadoop sample_5633 =========================

} catch (InterruptedException ie) {	throw (IOException)new InterruptedIOException().initCause(ie);	}	if (resp == null || resp.getClientResponseStatus() != ClientResponse.Status.OK) {	String msg = "Failed to get the response from the timeline server.";	LOG.error(msg);	if (resp != null) {	msg += " HTTP error code: " + resp.getStatus();	if (LOG.isDebugEnabled()) {	String output = resp.getEntity(String.class);	
http error code server response 

public ClientResponse doPostingObject(Object object, String path) {	WebResource webResource = client.resource(resURI);	if (path == null) {	if (LOG.isDebugEnabled()) {	
post to 

public ClientResponse doPostingObject(Object object, String path) {	WebResource webResource = client.resource(resURI);	if (path == null) {	if (LOG.isDebugEnabled()) {	}	ClientResponse r = webResource.accept(MediaType.APPLICATION_JSON) .type(MediaType.APPLICATION_JSON) .post(ClientResponse.class, object);	r.bufferEntity();	return r;	} else if (path.equals("domain")) {	if (LOG.isDebugEnabled()) {	
put to 

========================= hadoop sample_2549 =========================

public void run() {	MGR.shutdownInProgress.set(true);	for (HookEntry entry: MGR.getShutdownHooksInOrder()) {	Future<?> future = EXECUTOR.submit(entry.getHook());	try {	future.get(entry.getTimeout(), entry.getTimeUnit());	} catch (TimeoutException ex) {	future.cancel(true);	
shutdownhook timeout 

public void run() {	MGR.shutdownInProgress.set(true);	for (HookEntry entry: MGR.getShutdownHooksInOrder()) {	Future<?> future = EXECUTOR.submit(entry.getHook());	try {	future.get(entry.getTimeout(), entry.getTimeUnit());	} catch (TimeoutException ex) {	future.cancel(true);	} catch (Throwable ex) {	
shutdownhook failed 

try {	future.get(entry.getTimeout(), entry.getTimeUnit());	} catch (TimeoutException ex) {	future.cancel(true);	} catch (Throwable ex) {	}	}	try {	EXECUTOR.shutdown();	if (!EXECUTOR.awaitTermination(TIMEOUT_DEFAULT, TIME_UNIT_DEFAULT)) {	
shutdownhookmanger shutdown forcefully 

} catch (TimeoutException ex) {	future.cancel(true);	} catch (Throwable ex) {	}	}	try {	EXECUTOR.shutdown();	if (!EXECUTOR.awaitTermination(TIMEOUT_DEFAULT, TIME_UNIT_DEFAULT)) {	EXECUTOR.shutdownNow();	}	
shutdownhookmanger complete shutdown 

future.cancel(true);	} catch (Throwable ex) {	}	}	try {	EXECUTOR.shutdown();	if (!EXECUTOR.awaitTermination(TIMEOUT_DEFAULT, TIME_UNIT_DEFAULT)) {	EXECUTOR.shutdownNow();	}	} catch (InterruptedException ex) {	
shutdownhookmanger interrupted while waiting for termination 

========================= hadoop sample_3708 =========================

Path fswd = FileSystem.get(getConf()).getWorkingDirectory();	Configuration vConf = ViewFileSystemTestSetup.createConfig(false);	ConfigUtil.addLink(vConf, "/usr", new URI(fswd.toString()));	fs = FileSystem.get(FsConstants.VIEWFS_URI, vConf);	fs.setWorkingDirectory(new Path("/usr"));	listFile = new Path("target/tmp/listing").makeQualified(fs.getUri(), fs.getWorkingDirectory());	target = new Path("target/tmp/target").makeQualified(fs.getUri(), fs.getWorkingDirectory());	root = new Path("target/tmp").makeQualified(fs.getUri(), fs.getWorkingDirectory()).toString();	TestDistCpUtils.delete(fs, root);	} catch (IOException e) {	
exception encountered 

private void runTest(Path listFile, Path target, boolean targetExists, boolean sync) throws IOException {	DistCpOptions options = new DistCpOptions(listFile, target);	options.setSyncFolder(sync);	options.setTargetPathExists(targetExists);	try {	new DistCp(getConf(), options).execute();	} catch (Exception e) {	
exception encountered 

========================= hadoop sample_6265 =========================

public static boolean launchJob(URI fileSys, JobConf conf, int numMaps, int numReduces) throws IOException {	final Path inDir = new Path("/testing/input");	final Path outDir = new Path("/testing/output");	FileSystem fs = FileSystem.get(fileSys, conf);	fs.delete(outDir, true);	if (!fs.mkdirs(inDir)) {	
can t create 

conf.setNumMapTasks(numMaps);	conf.setNumReduceTasks(numReduces);	RunningJob runningJob = JobClient.runJob(conf);	try {	assertTrue(runningJob.isComplete());	assertTrue(runningJob.isSuccessful());	assertTrue("Output folder not found!", fs.exists(new Path("/testing/output/" + OUTPUT_FILENAME)));	} catch (NullPointerException npe) {	fail("A NPE should not have happened.");	}	
job is complete 

========================= hadoop sample_5449 =========================

public void testRouterRMAdminServiceE2E() throws Exception {	String user = "test1";	
refresh queues 

public void testRouterRMAdminServiceE2E() throws Exception {	String user = "test1";	RefreshQueuesResponse responseRefreshQueues = refreshQueues(user);	Assert.assertNotNull(responseRefreshQueues);	
refresh nodes 

public void testRouterRMAdminServiceE2E() throws Exception {	String user = "test1";	RefreshQueuesResponse responseRefreshQueues = refreshQueues(user);	Assert.assertNotNull(responseRefreshQueues);	RefreshNodesResponse responseRefreshNodes = refreshNodes(user);	Assert.assertNotNull(responseRefreshNodes);	
refresh super user 

public void testRouterRMAdminServiceE2E() throws Exception {	String user = "test1";	RefreshQueuesResponse responseRefreshQueues = refreshQueues(user);	Assert.assertNotNull(responseRefreshQueues);	RefreshNodesResponse responseRefreshNodes = refreshNodes(user);	Assert.assertNotNull(responseRefreshNodes);	RefreshSuperUserGroupsConfigurationResponse responseRefreshSuperUser = refreshSuperUserGroupsConfiguration(user);	Assert.assertNotNull(responseRefreshSuperUser);	
refresh user to group 

public void testRouterRMAdminServiceE2E() throws Exception {	String user = "test1";	RefreshQueuesResponse responseRefreshQueues = refreshQueues(user);	Assert.assertNotNull(responseRefreshQueues);	RefreshNodesResponse responseRefreshNodes = refreshNodes(user);	Assert.assertNotNull(responseRefreshNodes);	RefreshSuperUserGroupsConfigurationResponse responseRefreshSuperUser = refreshSuperUserGroupsConfiguration(user);	Assert.assertNotNull(responseRefreshSuperUser);	RefreshUserToGroupsMappingsResponse responseRefreshUserToGroup = refreshUserToGroupsMappings(user);	Assert.assertNotNull(responseRefreshUserToGroup);	
refresh admin acls 

RefreshQueuesResponse responseRefreshQueues = refreshQueues(user);	Assert.assertNotNull(responseRefreshQueues);	RefreshNodesResponse responseRefreshNodes = refreshNodes(user);	Assert.assertNotNull(responseRefreshNodes);	RefreshSuperUserGroupsConfigurationResponse responseRefreshSuperUser = refreshSuperUserGroupsConfiguration(user);	Assert.assertNotNull(responseRefreshSuperUser);	RefreshUserToGroupsMappingsResponse responseRefreshUserToGroup = refreshUserToGroupsMappings(user);	Assert.assertNotNull(responseRefreshUserToGroup);	RefreshAdminAclsResponse responseRefreshAdminAcls = refreshAdminAcls(user);	Assert.assertNotNull(responseRefreshAdminAcls);	
refresh service acls 

RefreshNodesResponse responseRefreshNodes = refreshNodes(user);	Assert.assertNotNull(responseRefreshNodes);	RefreshSuperUserGroupsConfigurationResponse responseRefreshSuperUser = refreshSuperUserGroupsConfiguration(user);	Assert.assertNotNull(responseRefreshSuperUser);	RefreshUserToGroupsMappingsResponse responseRefreshUserToGroup = refreshUserToGroupsMappings(user);	Assert.assertNotNull(responseRefreshUserToGroup);	RefreshAdminAclsResponse responseRefreshAdminAcls = refreshAdminAcls(user);	Assert.assertNotNull(responseRefreshAdminAcls);	RefreshServiceAclsResponse responseRefreshServiceAcls = refreshServiceAcls(user);	Assert.assertNotNull(responseRefreshServiceAcls);	
update node resource 

RefreshSuperUserGroupsConfigurationResponse responseRefreshSuperUser = refreshSuperUserGroupsConfiguration(user);	Assert.assertNotNull(responseRefreshSuperUser);	RefreshUserToGroupsMappingsResponse responseRefreshUserToGroup = refreshUserToGroupsMappings(user);	Assert.assertNotNull(responseRefreshUserToGroup);	RefreshAdminAclsResponse responseRefreshAdminAcls = refreshAdminAcls(user);	Assert.assertNotNull(responseRefreshAdminAcls);	RefreshServiceAclsResponse responseRefreshServiceAcls = refreshServiceAcls(user);	Assert.assertNotNull(responseRefreshServiceAcls);	UpdateNodeResourceResponse responseUpdateNodeResource = updateNodeResource(user);	Assert.assertNotNull(responseUpdateNodeResource);	
refresh nodes resource 

RefreshUserToGroupsMappingsResponse responseRefreshUserToGroup = refreshUserToGroupsMappings(user);	Assert.assertNotNull(responseRefreshUserToGroup);	RefreshAdminAclsResponse responseRefreshAdminAcls = refreshAdminAcls(user);	Assert.assertNotNull(responseRefreshAdminAcls);	RefreshServiceAclsResponse responseRefreshServiceAcls = refreshServiceAcls(user);	Assert.assertNotNull(responseRefreshServiceAcls);	UpdateNodeResourceResponse responseUpdateNodeResource = updateNodeResource(user);	Assert.assertNotNull(responseUpdateNodeResource);	RefreshNodesResourcesResponse responseRefreshNodesResources = refreshNodesResources(user);	Assert.assertNotNull(responseRefreshNodesResources);	
add to cluster nodelabels 

RefreshAdminAclsResponse responseRefreshAdminAcls = refreshAdminAcls(user);	Assert.assertNotNull(responseRefreshAdminAcls);	RefreshServiceAclsResponse responseRefreshServiceAcls = refreshServiceAcls(user);	Assert.assertNotNull(responseRefreshServiceAcls);	UpdateNodeResourceResponse responseUpdateNodeResource = updateNodeResource(user);	Assert.assertNotNull(responseUpdateNodeResource);	RefreshNodesResourcesResponse responseRefreshNodesResources = refreshNodesResources(user);	Assert.assertNotNull(responseRefreshNodesResources);	AddToClusterNodeLabelsResponse responseAddToClusterNodeLabels = addToClusterNodeLabels(user);	Assert.assertNotNull(responseAddToClusterNodeLabels);	
remove to cluster nodelabels 

RefreshServiceAclsResponse responseRefreshServiceAcls = refreshServiceAcls(user);	Assert.assertNotNull(responseRefreshServiceAcls);	UpdateNodeResourceResponse responseUpdateNodeResource = updateNodeResource(user);	Assert.assertNotNull(responseUpdateNodeResource);	RefreshNodesResourcesResponse responseRefreshNodesResources = refreshNodesResources(user);	Assert.assertNotNull(responseRefreshNodesResources);	AddToClusterNodeLabelsResponse responseAddToClusterNodeLabels = addToClusterNodeLabels(user);	Assert.assertNotNull(responseAddToClusterNodeLabels);	RemoveFromClusterNodeLabelsResponse responseRemoveFromClusterNodeLabels = removeFromClusterNodeLabels(user);	Assert.assertNotNull(responseRemoveFromClusterNodeLabels);	
replace labels on node 

UpdateNodeResourceResponse responseUpdateNodeResource = updateNodeResource(user);	Assert.assertNotNull(responseUpdateNodeResource);	RefreshNodesResourcesResponse responseRefreshNodesResources = refreshNodesResources(user);	Assert.assertNotNull(responseRefreshNodesResources);	AddToClusterNodeLabelsResponse responseAddToClusterNodeLabels = addToClusterNodeLabels(user);	Assert.assertNotNull(responseAddToClusterNodeLabels);	RemoveFromClusterNodeLabelsResponse responseRemoveFromClusterNodeLabels = removeFromClusterNodeLabels(user);	Assert.assertNotNull(responseRemoveFromClusterNodeLabels);	ReplaceLabelsOnNodeResponse responseReplaceLabelsOnNode = replaceLabelsOnNode(user);	Assert.assertNotNull(responseReplaceLabelsOnNode);	
check for decommissioning nodes 

RefreshNodesResourcesResponse responseRefreshNodesResources = refreshNodesResources(user);	Assert.assertNotNull(responseRefreshNodesResources);	AddToClusterNodeLabelsResponse responseAddToClusterNodeLabels = addToClusterNodeLabels(user);	Assert.assertNotNull(responseAddToClusterNodeLabels);	RemoveFromClusterNodeLabelsResponse responseRemoveFromClusterNodeLabels = removeFromClusterNodeLabels(user);	Assert.assertNotNull(responseRemoveFromClusterNodeLabels);	ReplaceLabelsOnNodeResponse responseReplaceLabelsOnNode = replaceLabelsOnNode(user);	Assert.assertNotNull(responseReplaceLabelsOnNode);	CheckForDecommissioningNodesResponse responseCheckForDecom = checkForDecommissioningNodes(user);	Assert.assertNotNull(responseCheckForDecom);	
refresh cluster max priority 

AddToClusterNodeLabelsResponse responseAddToClusterNodeLabels = addToClusterNodeLabels(user);	Assert.assertNotNull(responseAddToClusterNodeLabels);	RemoveFromClusterNodeLabelsResponse responseRemoveFromClusterNodeLabels = removeFromClusterNodeLabels(user);	Assert.assertNotNull(responseRemoveFromClusterNodeLabels);	ReplaceLabelsOnNodeResponse responseReplaceLabelsOnNode = replaceLabelsOnNode(user);	Assert.assertNotNull(responseReplaceLabelsOnNode);	CheckForDecommissioningNodesResponse responseCheckForDecom = checkForDecommissioningNodes(user);	Assert.assertNotNull(responseCheckForDecom);	RefreshClusterMaxPriorityResponse responseRefreshClusterMaxPriority = refreshClusterMaxPriority(user);	Assert.assertNotNull(responseRefreshClusterMaxPriority);	
get groups for user 

========================= hadoop sample_1959 =========================

protected void serviceInit(Configuration configuration) throws Exception {	this.conf = configuration;	String nnDesc = nameserviceId;	if (this.namenodeId != null && !this.namenodeId.isEmpty()) {	this.localTarget = new NNHAServiceTarget( conf, nameserviceId, namenodeId);	nnDesc += "-" + namenodeId;	} else {	this.localTarget = null;	}	this.rpcAddress = getRpcAddress(conf, nameserviceId, namenodeId);	
rpc address 

String nnDesc = nameserviceId;	if (this.namenodeId != null && !this.namenodeId.isEmpty()) {	this.localTarget = new NNHAServiceTarget( conf, nameserviceId, namenodeId);	nnDesc += "-" + namenodeId;	} else {	this.localTarget = null;	}	this.rpcAddress = getRpcAddress(conf, nameserviceId, namenodeId);	this.serviceAddress = DFSUtil.getNamenodeServiceAddr(conf, nameserviceId, namenodeId);	if (this.serviceAddress == null) {	
cannot locate rpc service address for nn using rpc address 

this.localTarget = new NNHAServiceTarget( conf, nameserviceId, namenodeId);	nnDesc += "-" + namenodeId;	} else {	this.localTarget = null;	}	this.rpcAddress = getRpcAddress(conf, nameserviceId, namenodeId);	this.serviceAddress = DFSUtil.getNamenodeServiceAddr(conf, nameserviceId, namenodeId);	if (this.serviceAddress == null) {	this.serviceAddress = this.rpcAddress;	}	
service rpc address 

}	this.rpcAddress = getRpcAddress(conf, nameserviceId, namenodeId);	this.serviceAddress = DFSUtil.getNamenodeServiceAddr(conf, nameserviceId, namenodeId);	if (this.serviceAddress == null) {	this.serviceAddress = this.rpcAddress;	}	this.lifelineAddress = DFSUtil.getNamenodeLifelineAddr(conf, nameserviceId, namenodeId);	if (this.lifelineAddress == null) {	this.lifelineAddress = this.serviceAddress;	}	
lifeline rpc address 

this.rpcAddress = getRpcAddress(conf, nameserviceId, namenodeId);	this.serviceAddress = DFSUtil.getNamenodeServiceAddr(conf, nameserviceId, namenodeId);	if (this.serviceAddress == null) {	this.serviceAddress = this.rpcAddress;	}	this.lifelineAddress = DFSUtil.getNamenodeLifelineAddr(conf, nameserviceId, namenodeId);	if (this.lifelineAddress == null) {	this.lifelineAddress = this.serviceAddress;	}	this.webAddress = DFSUtil.getNamenodeWebAddr(conf, nameserviceId, namenodeId);	
web address 

private void updateState() {	NamenodeStatusReport report = getNamenodeStatusReport();	if (!report.registrationValid()) {	
namenode is not operational 

private void updateState() {	NamenodeStatusReport report = getNamenodeStatusReport();	if (!report.registrationValid()) {	} else if (report.haStateValid()) {	
received service state from ha namenode 

private void updateState() {	NamenodeStatusReport report = getNamenodeStatusReport();	if (!report.registrationValid()) {	} else if (report.haStateValid()) {	} else if (localTarget == null) {	
reporting non ha namenode as operational 

private void updateState() {	NamenodeStatusReport report = getNamenodeStatusReport();	if (!report.registrationValid()) {	} else if (report.haStateValid()) {	} else if (localTarget == null) {	} else {	return;	}	try {	if (!resolver.registerNamenode(report)) {	
cannot register namenode 

if (!report.registrationValid()) {	} else if (report.haStateValid()) {	} else if (localTarget == null) {	} else {	return;	}	try {	if (!resolver.registerNamenode(report)) {	}	} catch (IOException e) {	
cannot register namenode in the state store 

} else if (report.haStateValid()) {	} else if (localTarget == null) {	} else {	return;	}	try {	if (!resolver.registerNamenode(report)) {	}	} catch (IOException e) {	} catch (Exception ex) {	
unhandled exception updating nn registration for 

protected NamenodeStatusReport getNamenodeStatusReport() {	NamenodeStatusReport report = new NamenodeStatusReport(nameserviceId, namenodeId, rpcAddress, serviceAddress, lifelineAddress, webAddress);	try {	
probing nn at service address 

if (!report.registrationValid()) {	return report;	}	try {	ClientProtocol client = NameNodeProxies .createProxy(this.conf, serviceURI, ClientProtocol.class) .getProxy();	if (client != null) {	boolean isSafeMode = client.setSafeMode( SafeModeAction.SAFEMODE_GET, false);	report.setSafeMode(isSafeMode);	}	} catch (Exception e) {	
cannot fetch safemode state for 

} catch (Exception e) {	}	updateJMXParameters(webAddress, report);	if (localTarget != null) {	try {	HAServiceProtocol haProtocol = localTarget.getProxy(conf, 30*1000);	HAServiceStatus status = haProtocol.getServiceStatus();	report.setHAServiceState(status.getState());	} catch (Throwable e) {	if (e.getMessage().startsWith("HA for namenode is not enabled")) {	
ha for is not enabled 

updateJMXParameters(webAddress, report);	if (localTarget != null) {	try {	HAServiceProtocol haProtocol = localTarget.getProxy(conf, 30*1000);	HAServiceStatus status = haProtocol.getServiceStatus();	report.setHAServiceState(status.getState());	} catch (Throwable e) {	if (e.getMessage().startsWith("HA for namenode is not enabled")) {	localTarget = null;	} else {	
cannot fetch ha status for 

HAServiceStatus status = haProtocol.getServiceStatus();	report.setHAServiceState(status.getState());	} catch (Throwable e) {	if (e.getMessage().startsWith("HA for namenode is not enabled")) {	localTarget = null;	} else {	}	}	}	} catch(IOException e) {	
cannot communicate with 

report.setHAServiceState(status.getState());	} catch (Throwable e) {	if (e.getMessage().startsWith("HA for namenode is not enabled")) {	localTarget = null;	} else {	}	}	}	} catch(IOException e) {	} catch(Throwable e) {	
unexpected exception while communicating with 

JSONObject jsonObject = aux.getJSONObject(i);	String name = jsonObject.getString("name");	if (name.equals("Hadoop:service=NameNode,name=FSNamesystemState")) {	report.setDatanodeInfo( jsonObject.getInt("NumLiveDataNodes"), jsonObject.getInt("NumDeadDataNodes"), jsonObject.getInt("NumDecommissioningDataNodes"), jsonObject.getInt("NumDecomLiveDataNodes"), jsonObject.getInt("NumDecomDeadDataNodes"));	} else if (name.equals( "Hadoop:service=NameNode,name=FSNamesystem")) {	report.setNamesystemInfo( jsonObject.getLong("CapacityRemaining"), jsonObject.getLong("CapacityTotal"), jsonObject.getLong("FilesTotal"), jsonObject.getLong("BlocksTotal"), jsonObject.getLong("MissingBlocks"), jsonObject.getLong("PendingReplicationBlocks"), jsonObject.getLong("UnderReplicatedBlocks"), jsonObject.getLong("PendingDeletionBlocks"));	}	}	}	} catch (Exception e) {	
cannot get stat from using jmx 

========================= hadoop sample_8294 =========================

public void run() {	for (int i = 0; i < count; i++) {	try {	final long param = RANDOM.nextLong();	LongWritable value = call(client, param, server, conf);	if (value.get() != param) {	
call failed 

public void run() {	for (int i = 0; i < count; i++) {	try {	final long param = RANDOM.nextLong();	LongWritable value = call(client, param, server, conf);	if (value.get() != param) {	failed = true;	break;	}	} catch (Exception e) {	
caught 

private static void assertExceptionContains( Throwable t, String substring) {	String msg = StringUtils.stringifyException(t);	assertTrue("Exception should contain substring '" + substring + "':\n" + msg, msg.contains(substring));	
got expected exception 

}).when(spyFactory).createSocket();	Server server = new TestServer(1, true);	Client client = new Client(LongWritable.class, conf, spyFactory);	server.start();	try {	InetSocketAddress address = NetUtils.getConnectAddress(server);	try {	call(client, RANDOM.nextLong(), address, conf);	fail("Expected an exception to have been thrown");	} catch (Exception e) {	
caught expected exception 

public void testIpcTimeout() throws IOException {	Server server = new TestServer(1, true);	InetSocketAddress addr = NetUtils.getConnectAddress(server);	server.start();	Client client = new Client(LongWritable.class, conf);	try {	call(client, new LongWritable(RANDOM.nextLong()), addr, MIN_SLEEP_TIME / 2, conf);	fail("Expected an exception to have been thrown");	} catch (SocketTimeoutException e) {	
get a sockettimeoutexception 

public void testIpcConnectTimeout() throws IOException {	Server server = new TestServer(1, true);	InetSocketAddress addr = NetUtils.getConnectAddress(server);	Client.setConnectTimeout(conf, 100);	Client client = new Client(LongWritable.class, conf);	try {	call(client, new LongWritable(RANDOM.nextLong()), addr, MIN_SLEEP_TIME * 2, conf);	fail("Expected an exception to have been thrown");	} catch (SocketTimeoutException e) {	
get a sockettimeoutexception 

Client client = new Client(LongWritable.class, conf);	Client.getClientExecutor().submit(new Runnable() {	public void run() {	while(true);	}	});	Thread.currentThread().interrupt();	client.stop();	try {	assertTrue(Thread.currentThread().isInterrupted());	
expected thread interrupt during client cleanup 

Client.getClientExecutor().submit(new Runnable() {	public void run() {	while(true);	}	});	Thread.currentThread().interrupt();	client.stop();	try {	assertTrue(Thread.currentThread().isInterrupted());	} catch (AssertionError e) {	
the client did not interrupt after handling an interrupted exception 

========================= hadoop sample_3122 =========================

public static void setup() throws IOException {	if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {	
mrappjar not found not running test 

public void testSpeculativeExecution() throws Exception {	if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {	
mrappjar not found not running test 

========================= hadoop sample_5625 =========================

}	collectCells(currentColumnCells, currentAggOp, cell, alreadySeenAggDim, converter, scannerContext);	nextCell(scannerContext);	}	if ((!currentColumnCells.isEmpty()) && ((limit <= 0 || addedCnt < limit))) {	addedCnt += emitCells(cells, currentColumnCells, currentAggOp, converter, currentTimestamp);	if (LOG.isDebugEnabled()) {	if (addedCnt > 0) {	LOG.debug("emitted cells. " + addedCnt + " for " + this.action + " rowKey=" + FlowRunRowKey.parseRowKey(CellUtil.cloneRow(cells.get(0))));	} else {	
emitted no cells for 

return previouslyChosenCell;	}	case GLOBAL_MAX: if (converter.compare( currentCellValue, previouslyChosenCellValue) > 0) {	return currentCell;	} else {	return previouslyChosenCell;	}	default: return currentCell;	}	} catch (IllegalArgumentException iae) {	
caught iae during conversion to long 

public void close() throws IOException {	if (flowRunScanner != null) {	flowRunScanner.close();	} else {	
scanner close called but scanner is null 

========================= hadoop sample_1236 =========================

}	}	} catch (DBException e) {	throw new IOException(e);	} finally {	if (iter != null) {	iter.close();	}	}	for (ContainerId containerId : containersToRemove) {	
remove container with incomplete records 

throw new IOException(e);	} finally {	if (iter != null) {	iter.close();	}	}	for (ContainerId containerId : containersToRemove) {	try {	removeContainer(containerId);	} catch (IOException e) {	
unable to remove container in store 

rcs.startRequest.setContainerToken(updatedToken);	rcs.capability = new ResourcePBImpl(tokenIdentifierProto.getResource());	rcs.version = tokenIdentifierProto.getVersion();	} else if (suffix.equals(CONTAINER_REMAIN_RETRIES_KEY_SUFFIX)) {	rcs.setRemainingRetryAttempts( Integer.parseInt(asString(entry.getValue())));	} else if (suffix.equals(CONTAINER_WORK_DIR_KEY_SUFFIX)) {	rcs.setWorkDir(asString(entry.getValue()));	} else if (suffix.equals(CONTAINER_LOG_DIR_KEY_SUFFIX)) {	rcs.setLogDir(asString(entry.getValue()));	} else {	
the container will be killed because of the unknown key during recovery 

private List<LocalizedResourceProto> loadCompletedResources( LeveldbIterator iter, String keyPrefix) throws IOException {	List<LocalizedResourceProto> rsrcs = new ArrayList<LocalizedResourceProto>();	while (iter.hasNext()) {	Entry<byte[],byte[]> entry = iter.peekNext();	String key = asString(entry.getKey());	if (!key.startsWith(keyPrefix)) {	break;	}	if (LOG.isDebugEnabled()) {	
loading completed resource from 

private Map<LocalResourceProto, Path> loadStartedResources( LeveldbIterator iter, String keyPrefix) throws IOException {	Map<LocalResourceProto, Path> rsrcs = new HashMap<LocalResourceProto, Path>();	while (iter.hasNext()) {	Entry<byte[],byte[]> entry = iter.peekNext();	String key = asString(entry.getKey());	if (!key.startsWith(keyPrefix)) {	break;	}	Path localPath = new Path(key.substring(keyPrefix.length()));	if (LOG.isDebugEnabled()) {	
loading in progress resource at 

public void finishResourceLocalization(String user, ApplicationId appId, LocalizedResourceProto proto) throws IOException {	String localPath = proto.getLocalPath();	String startedKey = getResourceStartedKey(user, appId, localPath);	String completedKey = getResourceCompletedKey(user, appId, localPath);	if (LOG.isDebugEnabled()) {	
storing localized resource to 

public void removeLocalizedResource(String user, ApplicationId appId, Path localPath) throws IOException {	String localPathStr = localPath.toString();	String startedKey = getResourceStartedKey(user, appId, localPathStr);	String completedKey = getResourceCompletedKey(user, appId, localPathStr);	if (LOG.isDebugEnabled()) {	
removing local resource at 

private void cleanupDeprecatedFinishedApps() {	try {	cleanupKeysWithPrefix(FINISHED_APPS_KEY_PREFIX);	} catch (Exception e) {	
cleanup keys with prefix from leveldb failed 

batch = db.createWriteBatch();	iter.seek(bytes(prefix));	while (iter.hasNext()) {	byte[] key = iter.next().getKey();	String keyStr = asString(key);	if (!keyStr.startsWith(prefix)) {	break;	}	batch.delete(key);	if (LOG.isDebugEnabled()) {	
cleanup from leveldb 

while (iter.hasNext()) {	Entry<byte[], byte[]> entry = iter.peekNext();	String key = asString(entry.getKey());	if (!key.startsWith(AMRMPROXY_KEY_PREFIX)) {	break;	}	String suffix = key.substring(AMRMPROXY_KEY_PREFIX.length());	if (suffix.equals(CURRENT_MASTER_KEY_SUFFIX)) {	iter.next();	result.setCurrentMasterKey(parseMasterKey(entry.getValue()));	
recovered for amrmproxy current master key id 

if (!key.startsWith(AMRMPROXY_KEY_PREFIX)) {	break;	}	String suffix = key.substring(AMRMPROXY_KEY_PREFIX.length());	if (suffix.equals(CURRENT_MASTER_KEY_SUFFIX)) {	iter.next();	result.setCurrentMasterKey(parseMasterKey(entry.getValue()));	} else if (suffix.equals(NEXT_MASTER_KEY_SUFFIX)) {	iter.next();	result.setNextMasterKey(parseMasterKey(entry.getValue()));	
recovered for amrmproxy next master key id 

} else {	int idEndPos;	ApplicationAttemptId attemptId;	try {	idEndPos = key.indexOf('/', AMRMPROXY_KEY_PREFIX.length());	if (idEndPos < 0) {	throw new IOException( "Unable to determine attemptId in key: " + key);	}	attemptId = ApplicationAttemptId.fromString( key.substring(AMRMPROXY_KEY_PREFIX.length(), idEndPos));	} catch (Exception e) {	
unknown key remove and move on 

if (idEndPos < 0) {	throw new IOException( "Unable to determine attemptId in key: " + key);	}	attemptId = ApplicationAttemptId.fromString( key.substring(AMRMPROXY_KEY_PREFIX.length(), idEndPos));	} catch (Exception e) {	unknownKeys.add(key);	continue;	}	Map<String, byte[]> appContext = loadAMRMProxyAppContextMap(iter, key.substring(0, idEndPos + 1));	result.getAppContexts().put(attemptId, appContext);	
recovered for amrmproxy map size 

protected DB openDatabase(Configuration conf) throws IOException {	Path storeRoot = createStorageDir(conf);	Options options = new Options();	options.createIfMissing(false);	options.logger(new LeveldbLogger());	
using state database at for recovery 

protected DB openDatabase(Configuration conf) throws IOException {	Path storeRoot = createStorageDir(conf);	Options options = new Options();	options.createIfMissing(false);	options.logger(new LeveldbLogger());	File dbfile = new File(storeRoot.toString());	try {	db = JniDBFactory.factory.open(dbfile, options);	} catch (NativeDB.DBException e) {	if (e.isNotFound() || e.getMessage().contains(" does not exist ")) {	
creating state database at 

public void run() {	long start = Time.monotonicNow();	
starting full compaction cycle 

public void run() {	long start = Time.monotonicNow();	try {	db.compactRange(null, null);	} catch (DBException e) {	
error compacting database 

public void run() {	long start = Time.monotonicNow();	try {	db.compactRange(null, null);	} catch (DBException e) {	}	long duration = Time.monotonicNow() - start;	
full compaction cycle completed in msec 

protected void checkVersion() throws IOException {	Version loadedVersion = loadVersion();	
loaded nm state version info 

protected void checkVersion() throws IOException {	Version loadedVersion = loadVersion();	if (loadedVersion.equals(getCurrentVersion())) {	return;	}	if (loadedVersion.isCompatibleTo(getCurrentVersion())) {	
storing nm state version info 

========================= hadoop sample_1689 =========================

String message = "Disallowed NodeManager from  " + host + ", Sending SHUTDOWN signal to the NodeManager.";	LOG.info(message);	response.setDiagnosticsMessage(message);	response.setNodeAction(NodeAction.SHUTDOWN);	return response;	}	String nid = nodeId.toString();	Resource dynamicLoadCapability = loadNodeResourceFromDRConfiguration(nid);	if (dynamicLoadCapability != null) {	if (LOG.isDebugEnabled()) {	
resource for node is adjusted from to due to settings in dynamic resources xml 

response.setNodeAction(NodeAction.SHUTDOWN);	return response;	}	response.setContainerTokenMasterKey(containerTokenSecretManager .getCurrentKey());	response.setNMTokenMasterKey(nmTokenSecretManager .getCurrentKey());	RMNode rmNode = new RMNodeImpl(nodeId, rmContext, host, cmPort, httpPort, resolve(host), capability, nodeManagerVersion, physicalResource);	RMNode oldNode = this.rmContext.getRMNodes().putIfAbsent(nodeId, rmNode);	if (oldNode == null) {	this.rmContext.getDispatcher().getEventHandler().handle( new RMNodeStartedEvent(nodeId, request.getNMContainerStatuses(), request.getRunningApplications()));	} else {	
reconnect from the node at 

this.rmContext.getDispatcher().getEventHandler().handle( new RMNodeStartedEvent(nodeId, request.getNMContainerStatuses(), request.getRunningApplications()));	} else {	this.nmLivelinessMonitor.unregister(nodeId);	oldNode.resetLastNodeHeartBeatResponse();	this.rmContext .getDispatcher() .getEventHandler() .handle( new RMNodeReconnectEvent(nodeId, rmNode, request .getRunningApplications(), request.getNMContainerStatuses()));	}	this.nmTokenSecretManager.removeNodeKey(nodeId);	this.nmLivelinessMonitor.register(nodeId);	if (!rmContext.isWorkPreservingRecoveryEnabled()) {	if (!request.getNMContainerStatuses().isEmpty()) {	
received container statuses on node manager register 

Map<ApplicationId, AppCollectorData> liveAppCollectorsMap = new HashMap<>();	Map<ApplicationId, RMApp> rmApps = rmContext.getRMApps();	for (ApplicationId appId : runningApps) {	RMApp app = rmApps.get(appId);	if (app != null) {	AppCollectorData appCollectorData = rmApps.get(appId) .getCollectorData();	if (appCollectorData != null) {	liveAppCollectorsMap.put(appId, appCollectorData);	} else {	if (LOG.isDebugEnabled()) {	
collector for applicaton hasn t registered yet 

for (Map.Entry<ApplicationId, AppCollectorData> entry: registeringCollectorsMap.entrySet()) {	ApplicationId appId = entry.getKey();	AppCollectorData collectorData = entry.getValue();	if (collectorData != null) {	if (!collectorData.isStamped()) {	collectorData.setRMIdentifier( ResourceManager.getClusterTimeStamp());	collectorData.setVersion( timelineCollectorVersion.getAndIncrement());	}	RMApp rmApp = rmApps.get(appId);	if (rmApp == null) {	
cannot update collector info because application id is not found in rmcontext 

if (!collectorData.isStamped()) {	collectorData.setRMIdentifier( ResourceManager.getClusterTimeStamp());	collectorData.setVersion( timelineCollectorVersion.getAndIncrement());	}	RMApp rmApp = rmApps.get(appId);	if (rmApp == null) {	} else {	synchronized (rmApp) {	AppCollectorData previousCollectorData = rmApp.getCollectorData();	if (AppCollectorData.happensBefore(previousCollectorData, collectorData)) {	
update collector information for application with new address timestamp 

public UnRegisterNodeManagerResponse unRegisterNodeManager( UnRegisterNodeManagerRequest request) throws YarnException, IOException {	UnRegisterNodeManagerResponse response = recordFactory .newRecordInstance(UnRegisterNodeManagerResponse.class);	NodeId nodeId = request.getNodeId();	RMNode rmNode = this.rmContext.getRMNodes().get(nodeId);	if (rmNode == null) {	
node not found ignoring the unregister from node id 

public UnRegisterNodeManagerResponse unRegisterNodeManager( UnRegisterNodeManagerRequest request) throws YarnException, IOException {	UnRegisterNodeManagerResponse response = recordFactory .newRecordInstance(UnRegisterNodeManagerResponse.class);	NodeId nodeId = request.getNodeId();	RMNode rmNode = this.rmContext.getRMNodes().get(nodeId);	if (rmNode == null) {	return response;	}	
node with node id has shutdown hence unregistering the node 

private void updateNodeLabelsFromNMReport(Set<String> nodeLabels, NodeId nodeId) throws IOException {	try {	Map<NodeId, Set<String>> labelsUpdate = new HashMap<NodeId, Set<String>>();	labelsUpdate.put(nodeId, nodeLabels);	this.rmContext.getNodeLabelManager().replaceLabelsOnNode(labelsUpdate);	if (LOG.isDebugEnabled()) {	
node labels from node were accepted from rm 

========================= hadoop sample_644 =========================

ReservationId reservationID = getNewReservation(clientService) .getReservationId();	ReservationSubmissionRequest request = createReservationSubmissionRequest( reservationID);	ReservationSubmissionResponse response = null;	try {	response = clientService.submitReservation(request);	} catch (Exception e) {	Assert.fail(e.getMessage());	}	Assert.assertNotNull(response);	Assert.assertNotNull(reservationID);	
submit reservation response 

ReservationId reservationID = getNewReservation(clientService) .getReservationId();	ReservationSubmissionRequest request = createReservationSubmissionRequest( reservationID);	ReservationSubmissionResponse response = null;	try {	response = clientService.submitReservation(request);	} catch (Exception e) {	Assert.fail(e.getMessage());	}	Assert.assertNotNull(response);	Assert.assertNotNull(reservationID);	
submit reservation response 

private void addNodeCapacityToPlan(MockRM rm, int memory, int vCores) {	try {	rm.registerNode("127.0.0.1:1", memory, vCores);	int attempts = 10;	do {	rm1.drainEvents();	rm.getRMContext().getReservationSystem() .synchronizePlan(ReservationSystemTestUtil.reservationQ, false);	if (rm.getRMContext().getReservationSystem() .getPlan(ReservationSystemTestUtil.reservationQ).getTotalCapacity() .getMemorySize() > 0) {	break;	}	
waiting for node capacity to be added to plan 

ReservationId reservationID = getNewReservation(clientService) .getReservationId();	ReservationSubmissionRequest request = createReservationSubmissionRequest( reservationID);	ReservationSubmissionResponse response = null;	try {	response = clientService.submitReservation(request);	} catch (Exception e) {	Assert.fail(e.getMessage());	}	Assert.assertNotNull(response);	Assert.assertNotNull(reservationID);	
submit reservation response 

ReservationId reservationID = getNewReservation(clientService) .getReservationId();	ReservationSubmissionRequest request = createReservationSubmissionRequest( reservationID);	ReservationSubmissionResponse response = null;	try {	response = clientService.submitReservation(request);	} catch (Exception e) {	Assert.fail(e.getMessage());	}	Assert.assertNotNull(response);	Assert.assertNotNull(reservationID);	
submit reservation response 

ReservationId reservationID = getNewReservation(clientService) .getReservationId();	ReservationSubmissionRequest request = createReservationSubmissionRequest( reservationID);	ReservationSubmissionResponse response = null;	try {	response = clientService.submitReservation(request);	} catch (Exception e) {	Assert.fail(e.getMessage());	}	Assert.assertNotNull(response);	Assert.assertNotNull(reservationID);	
submit reservation response 

ReservationId reservationID = getNewReservation(clientService) .getReservationId();	ReservationSubmissionRequest request = createReservationSubmissionRequest( reservationID);	ReservationSubmissionResponse response = null;	try {	response = clientService.submitReservation(request);	} catch (Exception e) {	Assert.fail(e.getMessage());	}	Assert.assertNotNull(response);	Assert.assertNotNull(reservationID);	
submit reservation response 

ReservationSubmissionRequest request = createReservationSubmissionRequest( resID1);	ReservationDefinition reservationDefinition = request.getReservationDefinition();	ReservationSubmissionResponse response = null;	try {	response = clientService.submitReservation(request);	} catch (Exception e) {	Assert.fail(e.getMessage());	}	Assert.assertNotNull(response);	Assert.assertNotNull(resID1);	
submit reservation response 

Assert.assertNotNull(resID1);	ReservationId resID2 = getNewReservation(clientService) .getReservationId();	request.setReservationId(resID2);	try {	response = clientService.submitReservation(request);	} catch (Exception e) {	Assert.fail(e.getMessage());	}	Assert.assertNotNull(response);	Assert.assertNotNull(resID2);	
submit reservation response 

Assert.assertNotNull(resID2);	ReservationId resID3 = getNewReservation(clientService) .getReservationId();	request.setReservationId(resID3);	try {	response = clientService.submitReservation(request);	} catch (Exception e) {	Assert.fail(e.getMessage());	}	Assert.assertNotNull(response);	Assert.assertNotNull(resID3);	
submit reservation response 

private void waitForReservationActivation(MockRM rm, ReservationId reservationId, String planName) {	try {	int attempts = 20;	do {	rm.getRMContext().getReservationSystem().synchronizePlan(planName, false);	if (rm.getResourceScheduler() .getQueueInfo(reservationId.toString(), false, false) .getCapacity() > 0f) {	break;	}	
waiting for reservation to be active 

========================= hadoop sample_490 =========================

public void set(String string) {	
truncating long string chars starting with 

public static int writeString(DataOutput out, String s) throws IOException {	
truncating long string chars starting with 

========================= hadoop sample_3933 =========================

public synchronized ContainerManagementProtocolProxyData getProxy( String containerManagerBindAddr, ContainerId containerId) throws InvalidToken {	ContainerManagementProtocolProxyData proxy = cmProxy.get(containerManagerBindAddr);	while (proxy != null && !proxy.token.getIdentifier().equals( nmTokenCache.getToken(containerManagerBindAddr).getIdentifier())) {	if (LOG.isDebugEnabled()) {	
refreshing proxy as nmtoken got updated for node 

private boolean tryCloseProxy( ContainerManagementProtocolProxyData proxy) {	proxy.activeCallers--;	if (proxy.scheduledForClose && proxy.activeCallers < 0) {	if (LOG.isDebugEnabled()) {	
closing proxy 

public synchronized void stopAllProxies() {	List<String> nodeIds = new ArrayList<String>();	nodeIds.addAll(this.cmProxy.keySet());	for (String nodeId : nodeIds) {	ContainerManagementProtocolProxyData proxy = cmProxy.get(nodeId);	proxy.activeCallers = 0;	try {	removeProxy(proxy);	} catch (Throwable t) {	
error closing connection 

protected ContainerManagementProtocol newProxy(final YarnRPC rpc, String containerManagerBindAddr, ContainerId containerId, Token token) throws InvalidToken {	if (token == null) {	throw new InvalidToken("No NMToken sent for " + containerManagerBindAddr);	}	final InetSocketAddress cmAddr = NetUtils.createSocketAddr(containerManagerBindAddr);	if (LOG.isDebugEnabled()) {	
opening proxy 

========================= hadoop sample_2621 =========================

public static Token<? extends TokenIdentifier> toToken(final Map<?, ?> m) throws IOException {	if (m == null) {	return null;	}	String urlString = (String) m.get(URL_STRING);	if (urlString != null) {	final Token<DelegationTokenIdentifier> token = new Token<>();	
read url string param 

========================= hadoop sample_6398 =========================

private synchronized boolean release(String reason, Exception ex) throws IOException {	if (!released) {	reasonClosed = reason;	try {	if (LOG.isDebugEnabled()) {	
releasing connection to 

private IOException releaseAndRethrow(String operation, IOException ex) {	try {	release(operation, ex);	} catch (IOException ioe) {	
exception during release 

public int read() throws IOException {	assumeNotReleased();	int read = 0;	try {	read = inStream.read();	} catch (EOFException e) {	if (LOG.isDebugEnabled()) {	
eof exception 

SwiftUtils.validateReadArgs(b, off, len);	if (len == 0) {	return 0;	}	assumeNotReleased();	int read;	try {	read = inStream.read(b, off, len);	} catch (EOFException e) {	if (LOG.isDebugEnabled()) {	
eof exception 

protected void finalize() {	try {	if (release("finalize()", constructionStack)) {	
input stream of not closed properly cleaned up in finalize 

protected void finalize() {	try {	if (release("finalize()", constructionStack)) {	}	} catch (Exception e) {	
exception while releasing in finalizer 

========================= hadoop sample_6232 =========================

public void put(URI uri, Map<String, List<String>> responseHeaders) {	List<String> headers = responseHeaders.get("Set-Cookie");	if (headers != null) {	for (String header : headers) {	List<HttpCookie> cookies;	try {	cookies = HttpCookie.parse(header);	} catch (IllegalArgumentException iae) {	
cannot parse cookie header 

String value = cookie.getValue();	if (!value.startsWith("\"")) {	value = "\"" + value + "\"";	cookie.setValue(value);	}	}	authCookie = cookie;	cookieHeaders = new HashMap<>();	cookieHeaders.put("Cookie", Arrays.asList(cookie.toString()));	}	
setting token value to 

========================= hadoop sample_2764 =========================

public void execute() throws MojoExecutionException {	long start = System.nanoTime();	validatePlatform();	runCMake();	runMake();	runMake();	long end = System.nanoTime();	
cmake compilation finished successfully in millisecond s 

public void runCMake() throws MojoExecutionException {	validatePlatform();	validateSourceParams(source, output);	if (output.mkdirs()) {	
mkdirs 

}	}	cmd.add("-G");	cmd.add("Unix Makefiles");	String prefix = "";	StringBuilder bld = new StringBuilder();	for (String c : cmd) {	bld.append(prefix).append(c);	prefix = " ";	}	
running 

}	}	cmd.add("-G");	cmd.add("Unix Makefiles");	String prefix = "";	StringBuilder bld = new StringBuilder();	for (String c : cmd) {	bld.append(prefix).append(c);	prefix = " ";	}	
with extra environment variables 

throw new MojoExecutionException("Interrupted while waiting for " + "CMake process", e);	} finally {	if (proc != null) {	proc.destroy();	}	if (outThread != null) {	try {	outThread.interrupt();	outThread.join();	} catch (InterruptedException e) {	
interrupted while joining output thread 

cmd.add("VERBOSE=1");	if (target != null) {	cmd.add(target);	}	StringBuilder bld = new StringBuilder();	String prefix = "";	for (String c : cmd) {	bld.append(prefix).append(c);	prefix = " ";	}	
running 

}	} catch (InterruptedException e) {	throw new MojoExecutionException("Interrupted during Process#waitFor", e);	} catch (IOException e) {	throw new MojoExecutionException("Error executing make", e);	} finally {	if (stdoutThread != null) {	try {	stdoutThread.join();	} catch (InterruptedException e) {	
interrupted while joining stdoutthread 

if (retCode != 0) {	for (String line: stdoutThread.getOutput()) {	getLog().warn(line);	}	}	}	if (stderrThread != null) {	try {	stderrThread.join();	} catch (InterruptedException e) {	
interrupted while joining stderrthread 

========================= hadoop sample_4433 =========================

public void setup() throws IOException {	localFS.delete(new Path(localDir.getAbsolutePath()), true);	localFS.delete(new Path(tmpDir.getAbsolutePath()), true);	localFS.delete(new Path(localLogDir.getAbsolutePath()), true);	localFS.delete(new Path(remoteLogDir.getAbsolutePath()), true);	localDir.mkdir();	tmpDir.mkdir();	localLogDir.mkdir();	remoteLogDir.mkdir();	
created localdir in 

public void setup() throws IOException {	localFS.delete(new Path(localDir.getAbsolutePath()), true);	localFS.delete(new Path(tmpDir.getAbsolutePath()), true);	localFS.delete(new Path(localLogDir.getAbsolutePath()), true);	localFS.delete(new Path(remoteLogDir.getAbsolutePath()), true);	localDir.mkdir();	tmpDir.mkdir();	localLogDir.mkdir();	remoteLogDir.mkdir();	
created tmpdir in 

protected DeletionService createDeletionService() {	return new DeletionService(exec) {	public void delete(DeletionTask deletionTask) {	
psuedo delete user type 

public static void waitForContainerState( ContainerManagementProtocol containerManager, ContainerId containerID, List<ContainerState> finalStates, int timeOutMax) throws InterruptedException, YarnException, IOException {	List<ContainerId> list = new ArrayList<ContainerId>();	list.add(containerID);	GetContainerStatusesRequest request = GetContainerStatusesRequest.newInstance(list);	ContainerStatus containerStatus = null;	HashSet<ContainerState> fStates = new HashSet<>(finalStates);	int timeoutSecs = 0;	do {	Thread.sleep(1000);	containerStatus = containerManager.getContainerStatuses(request) .getContainerStatuses().get(0);	
waiting for container to get into one of states current state is 

list.add(containerID);	GetContainerStatusesRequest request = GetContainerStatusesRequest.newInstance(list);	ContainerStatus containerStatus = null;	HashSet<ContainerState> fStates = new HashSet<>(finalStates);	int timeoutSecs = 0;	do {	Thread.sleep(1000);	containerStatus = containerManager.getContainerStatuses(request) .getContainerStatuses().get(0);	timeoutSecs += 1;	} while (!fStates.contains(containerStatus.getState()) && timeoutSecs < timeOutMax);	
container state is 

public static void waitForApplicationState( ContainerManagerImpl containerManager, ApplicationId appID, ApplicationState finalState) throws InterruptedException {	Application app = containerManager.getContext().getApplications().get(appID);	int timeout = 0;	while (!(app.getApplicationState().equals(finalState)) && timeout++ < 15) {	
waiting for app to reach current state is 

Container container = null;	org.apache.hadoop.yarn.server.nodemanager .containermanager.container.ContainerState currentState = null;	int timeoutSecs = 0;	do {	Thread.sleep(1000);	container = containerManager.getContext().getContainers().get(containerID);	if (container != null) {	currentState = container.getContainerState();	}	if (currentState != null) {	
waiting for nm container to get into one of the following states current state is 

do {	Thread.sleep(1000);	container = containerManager.getContext().getContainers().get(containerID);	if (container != null) {	currentState = container.getContainerState();	}	if (currentState != null) {	}	timeoutSecs += 1;	} while (!finalStates.contains(currentState) && timeoutSecs < timeOutMax);	
container state is 

========================= hadoop sample_1631 =========================

catch (Exception e) {	renderText(e.getMessage());	return;	}	if (app.getJob() != null) {	try {	String tt = $(TASK_TYPE);	tt = tt.isEmpty() ? "All" : StringUtils.capitalize( org.apache.hadoop.util.StringUtils.toLowerCase( MRApps.taskType(tt).toString()));	setTitle(join(tt, " Tasks for ", $(JOB_ID)));	} catch (Exception e) {	
failed to render tasks page with task type for job id 

if (taskType.isEmpty()) {	throw new RuntimeException("missing task-type.");	}	String attemptState = $(ATTEMPT_STATE);	if (attemptState.isEmpty()) {	throw new RuntimeException("missing attempt-state.");	}	setTitle(join(attemptState, " ", MRApps.taskType(taskType).toString(), " attempts in ", $(JOB_ID)));	render(attemptsPage());	} catch (Exception e) {	
failed to render attempts page with task type for job id 

assert(!jobId.isEmpty());	JobId jobID = MRApps.toJobID($(JOB_ID));	Job job = app.context.getJob(jobID);	assert(job != null);	try {	Configuration jobConf = job.loadConfFile();	response().setContentType("text/xml");	response().setHeader("Content-Disposition", "attachment; filename=" + jobId + ".xml");	jobConf.writeXml(writer());	} catch (IOException e) {	
error reading writing job conf file for job 

========================= hadoop sample_5222 =========================

static FileStatus concat(FSDirectory fsd, String target, String[] srcs, boolean logRetryCache) throws IOException {	validatePath(target, srcs);	assert srcs != null;	if (FSDirectory.LOG.isDebugEnabled()) {	
concat to 

if (FSDirectory.LOG.isDebugEnabled()) {	}	FSPermissionChecker pc = fsd.getPermissionChecker();	final INodesInPath targetIIP = fsd.resolvePath(pc, target, DirOp.WRITE);	if (fsd.isPermissionEnabled()) {	fsd.checkPathAccess(pc, targetIIP, FsAction.WRITE);	}	verifyTargetFile(fsd, target, targetIIP);	INodeFile[] srcFiles = verifySrcFiles(fsd, srcs, targetIIP, pc);	if(NameNode.stateChangeLog.isDebugEnabled()) {	
dir namesystem concat to 

static void unprotectedConcat(FSDirectory fsd, INodesInPath targetIIP, INodeFile[] srcList, long timestamp) throws IOException {	assert fsd.hasWriteLock();	if (NameNode.stateChangeLog.isDebugEnabled()) {	
dir fsnamesystem concat to 

========================= hadoop sample_8105 =========================

rm.sendNodeStarted(node);	node.nodeHeartbeat(true);	MockNM lostNode = rm.registerNode("host2:1235", 1024);	rm.sendNodeStarted(lostNode);	lostNode.nodeHeartbeat(true);	rm.waitForState(lostNode.getNodeId(), NodeState.RUNNING);	rm.sendNodeLost(lostNode);	Configuration conf = new Configuration();	YarnRPC rpc = YarnRPC.create(conf);	InetSocketAddress rmAddress = rm.getClientRMService().getBindAddress();	
connecting to resourcemanager at 

final CyclicBarrier endBarrier = new CyclicBarrier(2);	EventHandler eventHandler = new EventHandler() {	public void handle(Event rawEvent) {	if (rawEvent instanceof RMAppEvent) {	RMAppEvent event = (RMAppEvent) rawEvent;	if (event.getApplicationId().equals(appId1)) {	try {	startBarrier.await();	endBarrier.await();	} catch (BrokenBarrierException e) {	
broken barrier 

EventHandler eventHandler = new EventHandler() {	public void handle(Event rawEvent) {	if (rawEvent instanceof RMAppEvent) {	RMAppEvent event = (RMAppEvent) rawEvent;	if (event.getApplicationId().equals(appId1)) {	try {	startBarrier.await();	endBarrier.await();	} catch (BrokenBarrierException e) {	} catch (InterruptedException e) {	
interrupted while awaiting barriers 

labelsMgr.addToCluserNodeLabels(ImmutableSet.of(labelX, labelY));	NodeId node1 = NodeId.newInstance("host1", 1234);	NodeId node2 = NodeId.newInstance("host2", 1234);	Map<NodeId, Set<String>> map = new HashMap<NodeId, Set<String>>();	map.put(node1, ImmutableSet.of("x"));	map.put(node2, ImmutableSet.of("y"));	labelsMgr.replaceLabelsOnNode(map);	Configuration conf = new Configuration();	YarnRPC rpc = YarnRPC.create(conf);	InetSocketAddress rmAddress = rm.getClientRMService().getBindAddress();	
connecting to resourcemanager at 

Map<NodeId, Set<String>> map = new HashMap<NodeId, Set<String>>();	map.put(node1A, ImmutableSet.of("x"));	map.put(node1B, ImmutableSet.of("z"));	map.put(node2A, ImmutableSet.of("y"));	map.put(node3A, ImmutableSet.of("y"));	map.put(node3B, ImmutableSet.of("z"));	labelsMgr.replaceLabelsOnNode(map);	Configuration conf = new Configuration();	YarnRPC rpc = YarnRPC.create(conf);	InetSocketAddress rmAddress = rm.getClientRMService().getBindAddress();	
connecting to resourcemanager at 

YarnConfiguration conf = new YarnConfiguration();	conf.set(YarnConfiguration.RM_NODES_EXCLUDE_FILE_PATH, excludeFile);	MockRM rm = new MockRM(conf) {	protected ClientRMService createClientRMService() {	return new ClientRMService(this.rmContext, scheduler, this.rmAppManager, this.applicationACLsManager, this.queueACLsManager, this.getRMContext().getRMDelegationTokenSecretManager());	};	};	rm.start();	YarnRPC rpc = YarnRPC.create(conf);	InetSocketAddress rmAddress = rm.getClientRMService().getBindAddress();	
connecting to resourcemanager at 

========================= hadoop sample_613 =========================

throw new GeneralSecurityException("The property '" + locationProperty + "' has not been set in the ssl configuration file.");	}	String passwordProperty = resolvePropertyName(mode, SSL_KEYSTORE_PASSWORD_TPL_KEY);	String keystorePassword = getPassword(conf, passwordProperty, "");	if (keystorePassword.isEmpty()) {	throw new GeneralSecurityException("The property '" + passwordProperty + "' has not been set in the ssl configuration file.");	}	String keyPasswordProperty = resolvePropertyName(mode, SSL_KEYSTORE_KEYPASSWORD_TPL_KEY);	keystoreKeyPassword = getPassword( conf, keyPasswordProperty, keystorePassword);	if (LOG.isDebugEnabled()) {	
keystore 

keystoreKeyPassword = getPassword( conf, keyPasswordProperty, keystorePassword);	if (LOG.isDebugEnabled()) {	}	InputStream is = new FileInputStream(keystoreLocation);	try {	keystore.load(is, keystorePassword.toCharArray());	} finally {	is.close();	}	if (LOG.isDebugEnabled()) {	
loaded keystore 

String locationProperty = resolvePropertyName(mode, SSL_TRUSTSTORE_LOCATION_TPL_KEY);	String truststoreLocation = conf.get(locationProperty, "");	if (!truststoreLocation.isEmpty()) {	String passwordProperty = resolvePropertyName(mode, SSL_TRUSTSTORE_PASSWORD_TPL_KEY);	String truststorePassword = getPassword(conf, passwordProperty, "");	if (truststorePassword.isEmpty()) {	throw new GeneralSecurityException("The property '" + passwordProperty + "' has not been set in the ssl configuration file.");	}	long truststoreReloadInterval = conf.getLong( resolvePropertyName(mode, SSL_TRUSTSTORE_RELOAD_INTERVAL_TPL_KEY), DEFAULT_SSL_TRUSTSTORE_RELOAD_INTERVAL);	if (LOG.isDebugEnabled()) {	
truststore 

String truststorePassword = getPassword(conf, passwordProperty, "");	if (truststorePassword.isEmpty()) {	throw new GeneralSecurityException("The property '" + passwordProperty + "' has not been set in the ssl configuration file.");	}	long truststoreReloadInterval = conf.getLong( resolvePropertyName(mode, SSL_TRUSTSTORE_RELOAD_INTERVAL_TPL_KEY), DEFAULT_SSL_TRUSTSTORE_RELOAD_INTERVAL);	if (LOG.isDebugEnabled()) {	}	trustManager = new ReloadingX509TrustManager(truststoreType, truststoreLocation, truststorePassword, truststoreReloadInterval);	trustManager.init();	if (LOG.isDebugEnabled()) {	
loaded truststore 

long truststoreReloadInterval = conf.getLong( resolvePropertyName(mode, SSL_TRUSTSTORE_RELOAD_INTERVAL_TPL_KEY), DEFAULT_SSL_TRUSTSTORE_RELOAD_INTERVAL);	if (LOG.isDebugEnabled()) {	}	trustManager = new ReloadingX509TrustManager(truststoreType, truststoreLocation, truststorePassword, truststoreReloadInterval);	trustManager.init();	if (LOG.isDebugEnabled()) {	}	trustManagers = new TrustManager[]{trustManager};	} else {	if (LOG.isDebugEnabled()) {	
the property has not been set no truststore will be loaded 

========================= hadoop sample_3749 =========================

public String getAppProxyUrl(Configuration conf, ApplicationId applicationId) {	try {	final String scheme = WebAppUtils.getHttpSchemePrefix(conf);	URI proxyUri = ProxyUriUtils.getUriFromAMUrl(scheme, getProxyHostAndPort(conf));	URI result = ProxyUriUtils.getProxyUri(null, proxyUri, applicationId);	return result.toASCIIString();	} catch(URISyntaxException e) {	
could not generate default proxy tracking url for 

========================= hadoop sample_1133 =========================

long chunkOffset = lastChunkOffset;	if ( lastChunkLen > 0 ) {	chunkOffset += lastChunkLen;	}	if ( (pos + firstChunkOffset) != chunkOffset ) {	throw new IOException("Mismatch in pos : " + pos + " + " + firstChunkOffset + " != " + chunkOffset);	}	if (dataLeft <= 0) {	PacketHeader header = new PacketHeader();	header.readFields(in);	
dfsclient readchunk got header 

========================= hadoop sample_7008 =========================

private void assertCannotStartNameNodes() {	try {	cluster.restartNameNode(0, false);	fail("Should not have been able to start NN1 without shared dir");	} catch (IOException ioe) {	
got expected exception 

try {	cluster.restartNameNode(0, false);	fail("Should not have been able to start NN1 without shared dir");	} catch (IOException ioe) {	GenericTestUtils.assertExceptionContains( "storage directory does not exist or is not accessible", ioe);	}	try {	cluster.restartNameNode(1, false);	fail("Should not have been able to start NN2 without shared dir");	} catch (IOException ioe) {	
got expected exception 

========================= hadoop sample_7458 =========================

dependingJobs.add(job_3);	Job job_4 = new Job(jobConf_4, dependingJobs);	JobControl theControl = new JobControl("Test");	theControl.addJob(job_1);	theControl.addJob(job_2);	theControl.addJob(job_3);	theControl.addJob(job_4);	Thread theController = new Thread(theControl);	theController.start();	while (!theControl.allFinished()) {	
jobs in waiting state 

dependingJobs.add(job_3);	Job job_4 = new Job(jobConf_4, dependingJobs);	JobControl theControl = new JobControl("Test");	theControl.addJob(job_1);	theControl.addJob(job_2);	theControl.addJob(job_3);	theControl.addJob(job_4);	Thread theController = new Thread(theControl);	theController.start();	while (!theControl.allFinished()) {	
jobs in ready state 

dependingJobs.add(job_3);	Job job_4 = new Job(jobConf_4, dependingJobs);	JobControl theControl = new JobControl("Test");	theControl.addJob(job_1);	theControl.addJob(job_2);	theControl.addJob(job_3);	theControl.addJob(job_4);	Thread theController = new Thread(theControl);	theController.start();	while (!theControl.allFinished()) {	
jobs in running state 

dependingJobs.add(job_3);	Job job_4 = new Job(jobConf_4, dependingJobs);	JobControl theControl = new JobControl("Test");	theControl.addJob(job_1);	theControl.addJob(job_2);	theControl.addJob(job_3);	theControl.addJob(job_4);	Thread theController = new Thread(theControl);	theController.start();	while (!theControl.allFinished()) {	
jobs in success state 

dependingJobs.add(job_3);	Job job_4 = new Job(jobConf_4, dependingJobs);	JobControl theControl = new JobControl("Test");	theControl.addJob(job_1);	theControl.addJob(job_2);	theControl.addJob(job_3);	theControl.addJob(job_4);	Thread theController = new Thread(theControl);	theController.start();	while (!theControl.allFinished()) {	
jobs in failed state 

========================= hadoop sample_5465 =========================

copyAndAssert(tmpFile, outFile, maxBandwidth, 20, 0, CB.BUFFER);	copyAndAssert(tmpFile, outFile, maxBandwidth, 10, 0, CB.BUFFER);	copyAndAssert(tmpFile, outFile, maxBandwidth, 50, 0, CB.BUFFER);	copyAndAssert(tmpFile, outFile, maxBandwidth, 20, 0, CB.BUFF_OFFSET);	copyAndAssert(tmpFile, outFile, maxBandwidth, 10, 0, CB.BUFF_OFFSET);	copyAndAssert(tmpFile, outFile, maxBandwidth, 50, 0, CB.BUFF_OFFSET);	copyAndAssert(tmpFile, outFile, maxBandwidth, 20, 0, CB.ONE_C);	copyAndAssert(tmpFile, outFile, maxBandwidth, 10, 0, CB.ONE_C);	copyAndAssert(tmpFile, outFile, maxBandwidth, 50, 0, CB.ONE_C);	} catch (IOException e) {	
exception encountered 

========================= hadoop sample_6249 =========================

public void testPurge() throws Exception {	setupAppender(2, 1, 1);	
test message 

cutoff.clear();	cutoff.add(0L);	Assert.assertEquals(1, appender.getErrorCounts(cutoff).size());	Assert.assertEquals(1, appender.getErrorCounts(cutoff).get(0).longValue());	Assert.assertEquals(1, appender.getErrorMessagesAndCounts(cutoff).get(0) .size());	Thread.sleep(3000);	Assert.assertEquals(1, appender.getErrorCounts(cutoff).size());	Assert.assertEquals(0, appender.getErrorCounts(cutoff).get(0).longValue());	Assert.assertEquals(0, appender.getErrorMessagesAndCounts(cutoff).get(0) .size());	setupAppender(2, 1000, 2);	
test message 

cutoff.clear();	cutoff.add(0L);	Assert.assertEquals(1, appender.getErrorCounts(cutoff).size());	Assert.assertEquals(1, appender.getErrorCounts(cutoff).get(0).longValue());	Assert.assertEquals(1, appender.getErrorMessagesAndCounts(cutoff).get(0) .size());	Thread.sleep(3000);	Assert.assertEquals(1, appender.getErrorCounts(cutoff).size());	Assert.assertEquals(0, appender.getErrorCounts(cutoff).get(0).longValue());	Assert.assertEquals(0, appender.getErrorMessagesAndCounts(cutoff).get(0) .size());	setupAppender(2, 1000, 2);	
test message 

Assert.assertEquals(1, appender.getErrorCounts(cutoff).get(0).longValue());	Assert.assertEquals(1, appender.getErrorMessagesAndCounts(cutoff).get(0) .size());	Thread.sleep(3000);	Assert.assertEquals(1, appender.getErrorCounts(cutoff).size());	Assert.assertEquals(0, appender.getErrorCounts(cutoff).get(0).longValue());	Assert.assertEquals(0, appender.getErrorMessagesAndCounts(cutoff).get(0) .size());	setupAppender(2, 1000, 2);	Assert.assertEquals(1, appender.getErrorCounts(cutoff).size());	Assert.assertEquals(5, appender.getErrorCounts(cutoff).get(0).longValue());	Assert.assertEquals(2, appender.getErrorMessagesAndCounts(cutoff).get(0) .size());	
test message 

public void testErrorCounts() throws Exception {	cutoff.clear();	setupAppender(100, 100, 100);	cutoff.add(0L);	
test message 

public void testErrorCounts() throws Exception {	cutoff.clear();	setupAppender(100, 100, 100);	cutoff.add(0L);	
test message 

public void testErrorCounts() throws Exception {	cutoff.clear();	setupAppender(100, 100, 100);	cutoff.add(0L);	Assert.assertEquals(1, appender.getErrorCounts(cutoff).size());	Assert.assertEquals(1, appender.getWarningCounts(cutoff).size());	Assert.assertEquals(5, appender.getErrorCounts(cutoff).get(0).longValue());	Assert .assertEquals(0, appender.getWarningCounts(cutoff).get(0).longValue());	Thread.sleep(1000);	cutoff.add(Time.now() / 1000);	
test message 

public void testWarningCounts() throws Exception {	cutoff.clear();	setupAppender(100, 100, 100);	cutoff.add(0L);	
test message 

public void testWarningCounts() throws Exception {	cutoff.clear();	setupAppender(100, 100, 100);	cutoff.add(0L);	
test message 

public void testWarningCounts() throws Exception {	cutoff.clear();	setupAppender(100, 100, 100);	cutoff.add(0L);	Assert.assertEquals(1, appender.getErrorCounts(cutoff).size());	Assert.assertEquals(1, appender.getWarningCounts(cutoff).size());	Assert.assertEquals(0, appender.getErrorCounts(cutoff).get(0).longValue());	Assert .assertEquals(5, appender.getWarningCounts(cutoff).get(0).longValue());	Thread.sleep(1000);	cutoff.add(Time.now() / 1000);	
test message 

public void testWarningMessages() throws Exception {	cutoff.clear();	setupAppender(100, 100, 100);	cutoff.add(0L);	
test message 

public void testWarningMessages() throws Exception {	cutoff.clear();	setupAppender(100, 100, 100);	cutoff.add(0L);	
test message 

Assert.assertEquals(0, errorsMap.size());	Assert.assertEquals(2, warningsMap.size());	Assert.assertTrue(warningsMap.containsKey("test message 1"));	Assert.assertTrue(warningsMap.containsKey("test message 2"));	Log4jWarningErrorMetricsAppender.Element msg1Info = warningsMap.get("test message 1");	Log4jWarningErrorMetricsAppender.Element msg2Info = warningsMap.get("test message 2");	Assert.assertEquals(2, msg1Info.count.intValue());	Assert.assertEquals(3, msg2Info.count.intValue());	Thread.sleep(1000);	cutoff.add(Time.now() / 1000);	
test message 

public void testErrorMessages() throws Exception {	cutoff.clear();	setupAppender(100, 100, 100);	cutoff.add(0L);	
test message 

public void testErrorMessages() throws Exception {	cutoff.clear();	setupAppender(100, 100, 100);	cutoff.add(0L);	
test message 

Assert.assertEquals(2, errorsMap.size());	Assert.assertEquals(0, warningsMap.size());	Assert.assertTrue(errorsMap.containsKey("test message 1"));	Assert.assertTrue(errorsMap.containsKey("test message 2"));	Log4jWarningErrorMetricsAppender.Element msg1Info = errorsMap.get("test message 1");	Log4jWarningErrorMetricsAppender.Element msg2Info = errorsMap.get("test message 2");	Assert.assertEquals(2, msg1Info.count.intValue());	Assert.assertEquals(3, msg2Info.count.intValue());	Thread.sleep(1000);	cutoff.add(Time.now() / 1000);	
test message 

public void testInfoDebugTrace() {	cutoff.clear();	setupAppender(100, 100, 100);	cutoff.add(0L);	
test message 

public void testInfoDebugTrace() {	cutoff.clear();	setupAppender(100, 100, 100);	cutoff.add(0L);	
test message 

public void testInfoDebugTrace() {	cutoff.clear();	setupAppender(100, 100, 100);	cutoff.add(0L);	
test message 

========================= hadoop sample_2089 =========================

public static void main(String[] args) {	Thread.setDefaultUncaughtExceptionHandler(new YarnUncaughtExceptionHandler());	StringUtils.startupShutdownMessage(SharedCacheManager.class, args, LOG);	try {	Configuration conf = new YarnConfiguration();	SharedCacheManager sharedCacheManager = new SharedCacheManager();	ShutdownHookManager.get().addShutdownHook( new CompositeServiceShutdownHook(sharedCacheManager), SHUTDOWN_HOOK_PRIORITY);	sharedCacheManager.init(conf);	sharedCacheManager.start();	} catch (Throwable t) {	
error starting sharedcachemanager 

========================= hadoop sample_392 =========================

private void checkHaStateChange(StateChangeRequestInfo req) throws AccessControlException {	switch (req.getSource()) {	case REQUEST_BY_USER: if (autoFailoverEnabled) {	throw new AccessControlException( "Manual failover for this ResourceManager is disallowed, " + "because automatic failover is enabled.");	}	break;	case REQUEST_BY_USER_FORCED: if (autoFailoverEnabled) {	
allowing manual failover from even though automatic failover is enabled because the user specified the force flag 

public UpdateNodeResourceResponse updateNodeResource( UpdateNodeResourceRequest request) throws YarnException, IOException {	final String operation = "updateNodeResource";	UserGroupInformation user = checkAcls(operation);	checkRMStatus(user.getShortUserName(), operation, "update node resource.");	Map<NodeId, ResourceOption> nodeResourceMap = request.getNodeResourceMap();	Set<NodeId> nodeIds = nodeResourceMap.keySet();	for (NodeId nodeId : nodeIds) {	RMNode node = this.rm.getRMContext().getRMNodes().get(nodeId);	if (node == null) {	
resource update get failed on all nodes due to change resource on an unrecognized node 

if (node == null) {	throw RPCUtil.getRemoteException( "Resource update get failed on all nodes due to change resource " + "on an unrecognized node: " + nodeId);	}	}	boolean allSuccess = true;	for (Map.Entry<NodeId, ResourceOption> entry : nodeResourceMap.entrySet()) {	ResourceOption newResourceOption = entry.getValue();	NodeId nodeId = entry.getKey();	RMNode node = this.rm.getRMContext().getRMNodes().get(nodeId);	if (node == null) {	
resource update get failed on an unrecognized node 

}	boolean allSuccess = true;	for (Map.Entry<NodeId, ResourceOption> entry : nodeResourceMap.entrySet()) {	ResourceOption newResourceOption = entry.getValue();	NodeId nodeId = entry.getKey();	RMNode node = this.rm.getRMContext().getRMNodes().get(nodeId);	if (node == null) {	allSuccess = false;	} else {	this.rm.getRMContext().getDispatcher().getEventHandler() .handle(new RMNodeResourceUpdateEvent(nodeId, newResourceOption));	
update resource on node with resource 

========================= hadoop sample_668 =========================

public synchronized void pushMetric(final MetricsRecord mr) {	intervalHeartBeat();	try {	mr.incrMetric(getName(), getPreviousIntervalValue());	} catch (Exception e) {	
pushmetric failed for 

========================= hadoop sample_3362 =========================

PrivateToken(Token<T> publicToken, Text newService) {	super(publicToken.identifier, publicToken.password, publicToken.kind, newService);	assert !publicToken.isPrivate();	publicService = publicToken.service;	if (LOG.isDebugEnabled()) {	
cloned private token from 

}	renewer = TRIVIAL_RENEWER;	synchronized (renewers) {	for (TokenRenewer canidate : renewers) {	if (canidate.handleKind(this.kind)) {	renewer = canidate;	return renewer;	}	}	}	
no tokenrenewer defined for token kind 

========================= hadoop sample_3732 =========================

public void testRmEmptyRootDirRecursive() throws Throwable {	skipIfUnsupported(TEST_ROOT_TESTS_ENABLED);	Path root = new Path("/");	assertIsDirectory(root);	boolean deleted = getFileSystem().delete(root, true);	
rm r of empty dir result is 

public Void call() throws Exception {	FileStatus[] deleted = deleteChildren(fs, root, true);	FileStatus[] children = listChildren(fs, root);	if (children.length > 0) {	fail(String.format( "After %d attempts: listing after rm /* not empty" + "\n%s\n%s\n%s", iterations.incrementAndGet(), dumpStats("final", children), dumpStats("deleted", deleted), dumpStats("original", originalChildren)));	}	return null;	}	}, new LambdaTestUtils.ProportionalRetryInterval(50, 1000));	boolean deleted = fs.delete(root, false);	
rm of empty dir result is 

public void testRmRootRecursive() throws Throwable {	skipIfUnsupported(TEST_ROOT_TESTS_ENABLED);	Path root = new Path("/");	assertIsDirectory(root);	Path file = new Path("/testRmRootRecursive");	ContractTestUtils.touch(getFileSystem(), file);	boolean deleted = getFileSystem().delete(root, true);	assertIsDirectory(root);	
rm rf result is 

========================= hadoop sample_3227 =========================

public void testNetgroups() throws Exception {	if(!NativeCodeLoader.isNativeCodeLoaded()) {	
not testing netgroups this test only runs when native code is compiled 

public void testNetgroups() throws Exception {	if(!NativeCodeLoader.isNativeCodeLoaded()) {	return;	}	String groupMappingClassName = System.getProperty("TestAccessControlListGroupMapping");	if(groupMappingClassName == null) {	LOG.info("Not testing netgroups, no group mapping class specified, " + "use -DTestAccessControlListGroupMapping=$className to specify " + "group mapping class (must implement GroupMappingServiceProvider " + "interface and support netgroups)");	return;	}	
testing netgroups using 

========================= hadoop sample_2965 =========================

qjm = new QuorumJournalManager( conf, uri, FAKE_NSINFO, new FaultyLoggerFactory());	try {	qjm.createNewUniqueEpoch();	newEpoch = qjm.getLoggerSetForTests().getEpoch();	break;	} catch (IOException ioe) {	} finally {	qjm.close();	}	}	
created epoch 

========================= hadoop sample_7634 =========================

for (ReplicaInfo b : FsDatasetTestUtil.getReplicas(fds, bpid)) {	File f = b.getBlockFile();	File mf = b.getMetaFile();	if (f.exists() && f.length() != 0 && mf.exists()) {	FileOutputStream s = null;	FileChannel channel = null;	try {	s = new FileOutputStream(f);	channel = s.getChannel();	channel.truncate(0);	
truncated block file 

private long deleteBlockFile() {	try(AutoCloseableLock lock = fds.acquireDatasetLock()) {	for (ReplicaInfo b : FsDatasetTestUtil.getReplicas(fds, bpid)) {	File f = b.getBlockFile();	File mf = b.getMetaFile();	if (f.exists() && mf.exists() && f.delete()) {	
deleting block file 

private long deleteMetaFile() {	try(AutoCloseableLock lock = fds.acquireDatasetLock()) {	for (ReplicaInfo b : FsDatasetTestUtil.getReplicas(fds, bpid)) {	File file = b.getMetaFile();	if (file.exists() && file.delete()) {	
deleting metadata file 

private long createBlockFile() throws IOException {	long id = getFreeBlockId();	try (FsDatasetSpi.FsVolumeReferences volumes = fds.getFsVolumeReferences()) {	int numVolumes = volumes.size();	int index = rand.nextInt(numVolumes - 1);	File finalizedDir = volumes.get(index).getFinalizedDir(bpid);	File file = new File(finalizedDir, getBlockFile(id));	if (file.createNewFile()) {	
created block file 

private long createMetaFile() throws IOException {	long id = getFreeBlockId();	try (FsDatasetSpi.FsVolumeReferences refs = fds.getFsVolumeReferences()) {	int numVolumes = refs.size();	int index = rand.nextInt(numVolumes - 1);	File finalizedDir = refs.get(index).getFinalizedDir(bpid);	File file = new File(finalizedDir, getMetaFile(id));	if (file.createNewFile()) {	
created metafile 

private long createBlockMetaFile() throws IOException {	long id = getFreeBlockId();	try (FsDatasetSpi.FsVolumeReferences refs = fds.getFsVolumeReferences()) {	int numVolumes = refs.size();	int index = rand.nextInt(numVolumes - 1);	File finalizedDir = refs.get(index).getFinalizedDir(bpid);	File file = new File(finalizedDir, getBlockFile(id));	if (file.createNewFile()) {	
created block file 

try (FsDatasetSpi.FsVolumeReferences refs = fds.getFsVolumeReferences()) {	int numVolumes = refs.size();	int index = rand.nextInt(numVolumes - 1);	File finalizedDir = refs.get(index).getFinalizedDir(bpid);	File file = new File(finalizedDir, getBlockFile(id));	if (file.createNewFile()) {	String name1 = file.getAbsolutePath() + ".l";	String name2 = file.getAbsolutePath() + ".n";	file = new File(name1);	if (file.createNewFile()) {	
created extraneous file 

File finalizedDir = refs.get(index).getFinalizedDir(bpid);	File file = new File(finalizedDir, getBlockFile(id));	if (file.createNewFile()) {	String name1 = file.getAbsolutePath() + ".l";	String name2 = file.getAbsolutePath() + ".n";	file = new File(name1);	if (file.createNewFile()) {	}	file = new File(name2);	if (file.createNewFile()) {	
created extraneous file 

String name1 = file.getAbsolutePath() + ".l";	String name2 = file.getAbsolutePath() + ".n";	file = new File(name1);	if (file.createNewFile()) {	}	file = new File(name2);	if (file.createNewFile()) {	}	file = new File(finalizedDir, getMetaFile(id));	if (file.createNewFile()) {	
created metafile 

conf.setInt( DFSConfigKeys.DFS_DATANODE_DIRECTORYSCAN_THROTTLE_LIMIT_MS_PER_SEC_KEY, 100);	DataNode dataNode = cluster.getDataNodes().get(0);	createFile(GenericTestUtils.getMethodName(), BLOCK_LENGTH * blocks, false);	float ratio = 0.0f;	int retries = maxRetries;	while ((retries > 0) && ((ratio < 7f) || (ratio > 10f))) {	scanner = new DirectoryScanner(dataNode, fds, conf);	ratio = runThrottleTest(blocks);	retries -= 1;	}	
ratio 

assertTrue("Throttle is too restrictive", ratio <= 10f);	assertTrue("Throttle is too permissive", ratio >= 7f);	conf.setInt( DFSConfigKeys.DFS_DATANODE_DIRECTORYSCAN_THROTTLE_LIMIT_MS_PER_SEC_KEY, 200);	ratio = 0.0f;	retries = maxRetries;	while ((retries > 0) && ((ratio < 2.75f) || (ratio > 4.5f))) {	scanner = new DirectoryScanner(dataNode, fds, conf);	ratio = runThrottleTest(blocks);	retries -= 1;	}	
ratio 

assertTrue("Throttle is too permissive", ratio >= 2.75f);	conf.setInt(DFSConfigKeys.DFS_DATANODE_DIRECTORYSCAN_THREADS_KEY, 3);	conf.setInt( DFSConfigKeys.DFS_DATANODE_DIRECTORYSCAN_THROTTLE_LIMIT_MS_PER_SEC_KEY, 100);	ratio = 0.0f;	retries = maxRetries;	while ((retries > 0) && ((ratio < 7f) || (ratio > 10f))) {	scanner = new DirectoryScanner(dataNode, fds, conf);	ratio = runThrottleTest(blocks);	retries -= 1;	}	
ratio 

interruptor.schedule(new Runnable() {	public void run() {	nowMs.set(Time.monotonicNow());	scanner.shutdown();	}	}, 2L, TimeUnit.SECONDS);	scanner.reconcile();	assertFalse(scanner.getRunStatus());	long finalMs = nowMs.get();	if (finalMs > 0) {	
scanner took ms to shutdown 

long finalMs = nowMs.get();	if (finalMs > 0) {	assertTrue("Scanner took too long to shutdown", Time.monotonicNow() - finalMs < 1000L);	}	ratio = (float)scanner.timeWaitingMs.get() / scanner.timeRunningMs.get();	retries -= 1;	}	} finally {	interruptor.shutdown();	}	
ratio 

========================= hadoop sample_7264 =========================

public FederationMetrics(Router router) throws IOException {	this.router = router;	try {	StandardMBean bean = new StandardMBean(this, FederationMBean.class);	this.beanName = MBeans.register("Router", "FederationState", bean);	
registered router mbean 

this.router = router;	try {	StandardMBean bean = new StandardMBean(this, FederationMBean.class);	this.beanName = MBeans.register("Router", "FederationState", bean);	} catch (NotCompliantMBeanException e) {	throw new RuntimeException("Bad Router MBean setup", e);	}	this.namenodeResolver = this.router.getNamenodeResolver();	this.stateStore = this.router.getStateStore();	if (this.stateStore == null) {	
state store not available 

innerInfo.putAll(map);	long dateModified = namenode.getDateModified();	long lastHeartbeat = getSecondsSince(dateModified);	innerInfo.put("lastHeartbeat", lastHeartbeat);	MembershipStats stats = namenode.getStats();	long used = stats.getTotalSpace() - stats.getAvailableSpace();	innerInfo.put("used", used);	info.put(namenode.getNamenodeKey(), Collections.unmodifiableMap(innerInfo));	}	} catch (IOException e) {	
enable to fetch json representation of namenodes 

innerInfo.putAll(map);	long dateModified = namenode.getDateModified();	long lastHeartbeat = getSecondsSince(dateModified);	innerInfo.put("lastHeartbeat", lastHeartbeat);	MembershipStats stats = namenode.getStats();	long used = stats.getTotalSpace() - stats.getAvailableSpace();	innerInfo.put("used", used);	info.put(namenode.getNamenodeKey(), Collections.unmodifiableMap(innerInfo));	}	} catch (IOException e) {	
cannot retrieve nameservices for jmx 

innerInfo.put("path", StringUtils.join(",", paths));	if (nameservices.size() > 1) {	innerInfo.put("order", entry.getDestOrder().toString());	} else {	innerInfo.put("order", "");	}	innerInfo.put("readonly", entry.isReadOnly());	info.add(Collections.unmodifiableMap(innerInfo));	}	} catch (IOException e) {	
cannot generate json of mount table from store 

public int getNumNameservices() {	try {	Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();	return nss.size();	} catch (IOException e) {	
cannot fetch number of expired registrations from the store 

public int getNumNamenodes() {	try {	GetNamenodeRegistrationsRequest request = GetNamenodeRegistrationsRequest.newInstance();	GetNamenodeRegistrationsResponse response = membershipStore.getNamenodeRegistrations(request);	List<MembershipState> memberships = response.getNamenodeMemberships();	return memberships.size();	} catch (IOException e) {	
cannot retrieve numnamenodes for jmx 

public int getNumExpiredNamenodes() {	try {	GetNamenodeRegistrationsRequest request = GetNamenodeRegistrationsRequest.newInstance();	GetNamenodeRegistrationsResponse response = membershipStore.getExpiredNamenodeRegistrations(request);	List<MembershipState> expiredMemberships = response.getNamenodeMemberships();	return expiredMemberships.size();	} catch (IOException e) {	
cannot retrieve numexpirednamenodes for jmx 

Arrays.sort(usages);	median = usages[usages.length / 2];	max = usages[usages.length - 1];	min = usages[0];	for (i = 0; i < usages.length; i++) {	dev += (usages[i] - totalDfsUsed) * (usages[i] - totalDfsUsed);	}	dev = (float) Math.sqrt(dev / usages.length);	}	} catch (IOException e) {	
cannot get the live nodes 

public String getClusterId() {	try {	Collection<String> clusterIds = getNamespaceInfo("getClusterId");	return clusterIds.toString();	} catch (IOException e) {	
cannot fetch cluster id metrics 

public String getBlockPoolId() {	try {	Collection<String> blockpoolIds = getNamespaceInfo("getBlockPoolId");	return blockpoolIds.toString();	} catch (IOException e) {	
cannot fetch block pool id metrics 

private int getNameserviceAggregatedInt(String methodName) {	int total = 0;	try {	Collection<Object> data = getNameservicesStats(methodName);	for (Object o : data) {	Integer l = (Integer) o;	total += l;	}	} catch (IOException e) {	
cannot invoke for jmx 

private long getNameserviceAggregatedLong(String methodName) {	long total = 0;	try {	Collection<Object> data = getNameservicesStats(methodName);	for (Object o : data) {	Long l = (Long) o;	total += l;	}	} catch (IOException e) {	
cannot invoke for jmx 

Map<String, Class<?>> getters = new HashMap<>();	for (Method m : record.getClass().getDeclaredMethods()) {	if (m.getName().startsWith("get")) {	try {	Class<?> type = m.getReturnType();	char[] c = m.getName().substring(3).toCharArray();	c[0] = Character.toLowerCase(c[0]);	String key = new String(c);	getters.put(key, type);	} catch (Exception e) {	
cannot execute getter on 

private static Object getField(BaseRecord record, String fieldName) {	Object result = null;	Method m = locateGetter(record, fieldName);	if (m != null) {	try {	result = m.invoke(record);	} catch (Exception e) {	
cannot get field on 

========================= hadoop sample_8191 =========================

public static void resetDefaultRealm() {	try {	defaultRealm = KerberosUtil.getDefaultRealm();	} catch (Exception ke) {	
resetting default realm failed current default realm will still be used 

params = new String[]{realm, serviceName};	} else {	params = new String[]{realm, serviceName, hostName};	}	for(Rule r: rules) {	String result = r.apply(params);	if (result != null) {	return result;	}	}	
no auth to local rules applied to 

========================= hadoop sample_2748 =========================

for (Block block : postponedMisreplicatedBlocks) {	dumpBlockMeta(block, out);	}	pendingReplications.metaSave(out);	invalidateBlocks.dump(out);	Set<Block> corruptBlocks = corruptReplicas.getCorruptBlocks();	out.println("Corrupt Blocks:");	for(Block block : corruptBlocks) {	Collection<DatanodeDescriptor> corruptNodes = corruptReplicas.getNodes(block);	if (corruptNodes == null) {	
is corrupt but has no associated node 

private LocatedBlock createLocatedBlock(final BlockInfo blk, final long pos ) throws IOException {	if (!blk.isComplete()) {	final DatanodeStorageInfo[] storages = blk.getUnderConstructionFeature() .getExpectedStorageLocations();	final ExtendedBlock eb = new ExtendedBlock(getBlockPoolId(), blk);	return newLocatedBlock(eb, storages, pos, false);	}	NumberReplicas numberReplicas = countNodes(blk);	final int numCorruptNodes = numberReplicas.corruptReplicas();	final int numCorruptReplicas = corruptReplicas.numCorruptReplicas(blk);	if (numCorruptNodes != numCorruptReplicas) {	
inconsistent number of corrupt replicas for blockmap has but corrupt replicas map has 

public BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode, final long size) throws UnregisteredNodeException {	final DatanodeDescriptor node = getDatanodeManager().getDatanode(datanode);	if (node == null) {	
block getblocks asking for blocks from an unrecorded node 

if (storage.getState() != State.NORMAL) {	continue;	}	final DatanodeDescriptor node = storage.getDatanodeDescriptor();	invalidateBlocks.add(b, node, false);	if (datanodes != null) {	datanodes.append(node).append(" ");	}	}	if (datanodes != null && datanodes.length() != 0) {	
block addtoinvalidates 

public void findAndMarkBlockAsCorrupt(final ExtendedBlock blk, final DatanodeInfo dn, String storageID, String reason) throws IOException {	assert namesystem.hasWriteLock();	final BlockInfo storedBlock = getStoredBlock(blk.getLocalBlock());	if (storedBlock == null) {	
block findandmarkblockascorrupt not found 

throw new IOException("Cannot mark " + blk + " as corrupt because datanode " + dn + " (" + dn.getDatanodeUuid() + ") does not exist");	}	DatanodeStorageInfo storage = null;	if (storageID != null) {	storage = node.getStorageInfo(storageID);	}	if (storage == null) {	storage = storedBlock.findStorageInfo(node);	}	if (storage == null) {	
block findandmarkblockascorrupt not found on 

private void markBlockAsCorrupt(BlockToMarkCorrupt b, DatanodeStorageInfo storageInfo, DatanodeDescriptor node) throws IOException {	if (b.getCorrupted().isDeleted()) {	
block markblockascorrupt cannot be marked as corrupt as it does not belong to any file 

private boolean invalidateBlock(BlockToMarkCorrupt b, DatanodeInfo dn ) throws IOException {	
block invalidateblock on 

private boolean invalidateBlock(BlockToMarkCorrupt b, DatanodeInfo dn ) throws IOException {	DatanodeDescriptor node = getDatanodeManager().getDatanode(dn);	if (node == null) {	throw new IOException("Cannot invalidate " + b + " because datanode " + dn + " does not exist.");	}	NumberReplicas nr = countNodes(b.getStored());	if (nr.replicasOnStaleNodes() > 0) {	
block invalidateblocks postponing invalidation of on because replica s are located on nodes with potentially out of date block reports 

if (node == null) {	throw new IOException("Cannot invalidate " + b + " because datanode " + dn + " does not exist.");	}	NumberReplicas nr = countNodes(b.getStored());	if (nr.replicasOnStaleNodes() > 0) {	postponeBlock(b.getCorrupted());	return false;	} else if (nr.liveReplicas() >= 1) {	addToInvalidates(b.getCorrupted(), dn);	removeStoredBlock(b.getStored(), node);	
block invalidateblocks on listed for deletion 

}	NumberReplicas nr = countNodes(b.getStored());	if (nr.replicasOnStaleNodes() > 0) {	postponeBlock(b.getCorrupted());	return false;	} else if (nr.liveReplicas() >= 1) {	addToInvalidates(b.getCorrupted(), dn);	removeStoredBlock(b.getStored(), node);	return true;	} else {	
block invalidateblocks on is the only copy and was not deleted 

neededReplications.remove(block, priority);	rw.resetTargets();	return false;	}	NumberReplicas numReplicas = countNodes(block);	final short requiredReplication = getExpectedLiveRedundancyNum(block, numReplicas);	final int pendingNum = pendingReplications.getNumReplicas(block);	if (hasEnoughEffectiveReplicas(block, numReplicas, pendingNum)) {	neededReplications.remove(block, priority);	rw.resetTargets();	
block removing from neededreplications as it has enough replicas 

}	DatanodeStorageInfo[] targets = rw.getTargets();	if ( (numReplicas.liveReplicas() >= requiredReplication) && (!isPlacementPolicySatisfied(block)) ) {	if (rw.getSrcNode().getNetworkLocation().equals( targets[0].getDatanodeDescriptor().getNetworkLocation())) {	return false;	}	}	rw.getSrcNode().addBlockToBeReplicated(block, targets);	DatanodeStorageInfo.incrementBlocksScheduled(targets);	pendingReplications.increment(block, DatanodeStorageInfo.toDatanodeDescriptors(targets));	
block block is moved from neededreplications to pendingReplications 

public long requestBlockReportLeaseId(DatanodeRegistration nodeReg) {	assert namesystem.hasReadLock();	DatanodeDescriptor node = null;	try {	node = datanodeManager.getDatanode(nodeReg);	} catch (UnregisteredNodeException e) {	
unregistered datanode 

public long requestBlockReportLeaseId(DatanodeRegistration nodeReg) {	assert namesystem.hasReadLock();	DatanodeDescriptor node = null;	try {	node = datanodeManager.getDatanode(nodeReg);	} catch (UnregisteredNodeException e) {	return 0;	}	if (node == null) {	
failed to find datanode 

numRemovedComplete++;	if (checkMinReplication(b)) {	numRemovedSafe++;	}	}	}	removeBlock(b);	}	if (trackBlockCounts) {	if (LOG.isDebugEnabled()) {	
adjusting safe mode totals for deletion decreasing safeblocks by totalblocks by 

try {	node = datanodeManager.getDatanode(nodeID);	if (node == null || !node.isRegistered()) {	throw new IOException( "ProcessReport from dead or unregistered node: " + nodeID);	}	DatanodeStorageInfo storageInfo = node.getStorageInfo(storage.getStorageID());	if (storageInfo == null) {	storageInfo = node.updateStorage(storage);	}	if (namesystem.isInStartupSafeMode() && storageInfo.getBlockReportCount() > 0) {	
block processreport discarded non initial block report from because namenode still in startup phase 

if (namesystem.isInStartupSafeMode() && storageInfo.getBlockReportCount() > 0) {	blockReportLeaseManager.removeLease(node);	return !node.hasStaleStorages();	}	if (context != null) {	if (!blockReportLeaseManager.checkLease(node, startTime, context.getLeaseId())) {	return false;	}	}	if (storageInfo.getBlockReportCount() == 0) {	
block processreport processing first storage report for from datanode 

processFirstBlockReport(storageInfo, newReport);	} else {	invalidatedBlocks = processReport(storageInfo, newReport, context);	}	storageInfo.receivedBlockReport();	} finally {	endTime = Time.monotonicNow();	namesystem.writeUnlock();	}	for (Block b : invalidatedBlocks) {	
block processreport on node size does not belong to any file 

} finally {	endTime = Time.monotonicNow();	namesystem.writeUnlock();	}	for (Block b : invalidatedBlocks) {	}	final NameNodeMetrics metrics = NameNode.getNameNodeMetrics();	if (metrics != null) {	metrics.addBlockReport((int) (endTime - startTime));	}	
block processreport from storage node blocks hasstalestorage processing time msecs invalidatedblocks 

DatanodeDescriptor node;	try {	node = datanodeManager.getDatanode(nodeID);	if (context != null) {	if (context.getTotalRpcs() == context.getCurRpc() + 1) {	long leaseId = this.getBlockReportLeaseManager().removeLease(node);	BlockManagerFaultInjector.getInstance(). removeBlockReportLease(node, leaseId);	node.setLastBlockReportTime(now());	node.setLastBlockReportMonotonic(Time.monotonicNow());	}	
processing rpc with index out of total rpcs in processreport 

}	for (Block b : toRemove) {	removeStoredBlock(b, node);	}	int numBlocksLogged = 0;	for (BlockInfo b : toAdd) {	addStoredBlock(b, storageInfo, null, numBlocksLogged < maxNumBlocksToLog);	numBlocksLogged++;	}	if (numBlocksLogged > maxNumBlocksToLog) {	
block processreport logged info for of reported 

boolean isCorrupt = true;	if (newStorages != null) {	for (DatanodeStorageInfo newStorage : newStorages) {	if (newStorage!= null && storage.equals(newStorage)) {	isCorrupt = false;	break;	}	}	}	if (isCorrupt) {	
block markblockreplicasascorrupt mark block replica on as corrupt because the dn is not in the new committed storage list 

private void queueReportedBlock(DatanodeStorageInfo storageInfo, Block block, ReplicaState reportedState, String reason) {	assert shouldPostponeBlocksFromFuture;	if (LOG.isDebugEnabled()) {	
queueing reported block in state from datanode for later processing because 

private void processQueuedMessages(Iterable<ReportedBlockInfo> rbis) throws IOException {	for (ReportedBlockInfo rbi : rbis) {	if (LOG.isDebugEnabled()) {	
processing previouly queued message 

public void processAllPendingDNMessages() throws IOException {	assert !shouldPostponeBlocksFromFuture : "processAllPendingDNMessages() should be called after disabling " + "block postponement.";	int count = pendingDNMessages.count();	if (count > 0) {	
processing messages from datanodes that were previously queued during standby state 

return null;	default: return null;	}	case RBW: case RWR: if (!storedBlock.isComplete()) {	return null;	} else if (storedBlock.getGenerationStamp() != reported.getGenerationStamp()) {	final long reportedGS = reported.getGenerationStamp();	return new BlockToMarkCorrupt(storedBlock, reportedGS, "reported " + reportedState + " replica with genstamp " + reportedGS + " does not match COMPLETE block's genstamp in block map " + storedBlock.getGenerationStamp(), Reason.GENSTAMP_MISMATCH);	} else {	if (reportedState == ReplicaState.RBW) {	
received an rbw replica for on ignoring it since it is complete with the same genstamp 

private Block addStoredBlock(final BlockInfo block, DatanodeStorageInfo storageInfo, DatanodeDescriptor delNodeHint, boolean logEveryBlock) throws IOException {	assert block != null && namesystem.hasWriteLock();	BlockInfo storedBlock;	DatanodeDescriptor node = storageInfo.getDatanodeDescriptor();	if (!block.isComplete()) {	storedBlock = blocksMap.getStoredBlock(block);	} else {	storedBlock = block;	}	if (storedBlock == null || storedBlock.isDeleted()) {	
block addstoredblock on size but it does not belong to any file 

}	AddBlockResult result = storageInfo.addBlock(storedBlock);	int curReplicaDelta;	if (result == AddBlockResult.ADDED) {	curReplicaDelta = (node.isDecommissioned()) ? 0 : 1;	if (logEveryBlock) {	blockLog.debug("BLOCK* addStoredBlock: {} is added to {} (size={})", node, storedBlock, storedBlock.getNumBytes());	}	} else if (result == AddBlockResult.REPLACED) {	curReplicaDelta = 0;	
block addstoredblock block moved to storagetype on node 

if (result == AddBlockResult.ADDED) {	curReplicaDelta = (node.isDecommissioned()) ? 0 : 1;	if (logEveryBlock) {	blockLog.debug("BLOCK* addStoredBlock: {} is added to {} (size={})", node, storedBlock, storedBlock.getNumBytes());	}	} else if (result == AddBlockResult.REPLACED) {	curReplicaDelta = 0;	} else {	corruptReplicas.removeFromCorruptReplicasMap(block, node, Reason.GENSTAMP_MISMATCH);	curReplicaDelta = 0;	
block addstoredblock redundant addstoredblock request received for on node size 

neededReplications.remove(storedBlock, numCurrentReplica, num.readOnlyReplicas(), num.outOfServiceReplicas(), fileReplication);	} else {	updateNeededReplications(storedBlock, curReplicaDelta, 0);	}	if (numCurrentReplica > fileReplication) {	processOverReplicatedBlock(storedBlock, fileReplication, node, delNodeHint);	}	int corruptReplicasCount = corruptReplicas.numCorruptReplicas(storedBlock);	int numCorruptNodes = num.corruptReplicas();	if (numCorruptNodes != corruptReplicasCount) {	
inconsistent number of corrupt replicas for blockmap has but corrupt replicas map has 

Collection<DatanodeDescriptor> nodes = corruptReplicas.getNodes(blk);	boolean removedFromBlocksMap = true;	if (nodes == null) return;	DatanodeDescriptor[] nodesCopy = nodes.toArray(new DatanodeDescriptor[0]);	for (DatanodeDescriptor node : nodesCopy) {	try {	if (!invalidateBlock(new BlockToMarkCorrupt(blk, null, Reason.ANY), node)) {	removedFromBlocksMap = false;	}	} catch (IOException e) {	
invalidatecorruptreplicas error in deleting bad block on 

public void processMisReplicatedBlocks() {	assert namesystem.hasWriteLock();	stopReplicationInitializer();	neededReplications.clear();	replicationQueuesInitializer = new Daemon() {	public void run() {	try {	processMisReplicatesAsync();	} catch (InterruptedException ie) {	
interrupted while processing replication queues 

public void processMisReplicatedBlocks() {	assert namesystem.hasWriteLock();	stopReplicationInitializer();	neededReplications.clear();	replicationQueuesInitializer = new Daemon() {	public void run() {	try {	processMisReplicatesAsync();	} catch (InterruptedException ie) {	} catch (Exception e) {	
error while processing replication queues async 

private void stopReplicationInitializer() {	if (replicationQueuesInitializer != null) {	replicationQueuesInitializer.interrupt();	try {	replicationQueuesInitializer.join();	} catch (final InterruptedException e) {	
interrupted while waiting for replicationqueueinitializer returning 

long totalProcessed = 0;	long sleepDuration = Math.max(1, Math.min(numBlocksPerIteration/1000, 10000));	while (namesystem.isRunning() && !Thread.currentThread().isInterrupted()) {	int processed = 0;	namesystem.writeLockInterruptibly();	try {	while (processed < numBlocksPerIteration && blocksItr.hasNext()) {	BlockInfo block = blocksItr.next();	MisReplicationResult res = processMisReplicatedBlock(block);	switch (res) {	
under replicated block 

while (namesystem.isRunning() && !Thread.currentThread().isInterrupted()) {	int processed = 0;	namesystem.writeLockInterruptibly();	try {	while (processed < numBlocksPerIteration && blocksItr.hasNext()) {	BlockInfo block = blocksItr.next();	MisReplicationResult res = processMisReplicatedBlock(block);	switch (res) {	nrUnderReplicated++;	break;	
over replicated block 

namesystem.writeLockInterruptibly();	try {	while (processed < numBlocksPerIteration && blocksItr.hasNext()) {	BlockInfo block = blocksItr.next();	MisReplicationResult res = processMisReplicatedBlock(block);	switch (res) {	nrUnderReplicated++;	break;	nrOverReplicated++;	break;	
invalid block 

while (processed < numBlocksPerIteration && blocksItr.hasNext()) {	BlockInfo block = blocksItr.next();	MisReplicationResult res = processMisReplicatedBlock(block);	switch (res) {	nrUnderReplicated++;	break;	nrOverReplicated++;	break;	nrInvalid++;	break;	
postpone block 

switch (res) {	nrUnderReplicated++;	break;	nrOverReplicated++;	break;	nrInvalid++;	break;	nrPostponed++;	postponeBlock(block);	break;	
under construction block 

}	processed++;	}	totalProcessed += processed;	replicationQueuesInitProgress = Math.min((double) totalProcessed if (!blocksItr.hasNext()) {	LOG.info("Total number of blocks            = " + blocksMap.size());	LOG.info("Number of invalid blocks          = " + nrInvalid);	LOG.info("Number of under-replicated blocks = " + nrUnderReplicated);	LOG.info("Number of  over-replicated blocks = " + nrOverReplicated + ((nrPostponed > 0) ? (" (" + nrPostponed + " postponed)") : ""));	LOG.info("Number of blocks being written    = " + nrUnderConstruction);	
state replication queue initialization scan for invalid over and under replicated blocks completed in msec 

LOG.info("Number of  over-replicated blocks = " + nrOverReplicated + ((nrPostponed > 0) ? (" (" + nrPostponed + " postponed)") : ""));	LOG.info("Number of blocks being written    = " + nrUnderConstruction);	break;	}	} finally {	namesystem.writeUnlock();	Thread.sleep(sleepDuration);	}	}	if (Thread.currentThread().isInterrupted()) {	
interrupted while processing replication queues 

delNodeHint = null;	}	Collection<DatanodeStorageInfo> nonExcess = new ArrayList<DatanodeStorageInfo>();	Collection<DatanodeDescriptor> corruptNodes = corruptReplicas .getNodes(block);	for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {	if (storage.getState() != State.NORMAL) {	continue;	}	final DatanodeDescriptor cur = storage.getDatanodeDescriptor();	if (storage.areBlockContentsStale()) {	
block processoverreplicatedblock postponing since storage does not yet have up to date information 

private void processChosenExcessReplica( final Collection<DatanodeStorageInfo> nonExcess, final DatanodeStorageInfo chosen, BlockInfo storedBlock) {	nonExcess.remove(chosen);	addToExcessReplicate(chosen.getDatanodeDescriptor(), storedBlock);	addToInvalidates(storedBlock, chosen.getDatanodeDescriptor());	
block chooseexcessreplicates is added to invalidated blocks set 

private void addToExcessReplicate(DatanodeInfo dn, Block block) {	assert namesystem.hasWriteLock();	LightWeightHashSet<Block> excessBlocks = excessReplicateMap.get( dn.getDatanodeUuid());	if (excessBlocks == null) {	excessBlocks = new LightWeightHashSet<>();	excessReplicateMap.put(dn.getDatanodeUuid(), excessBlocks);	}	if (excessBlocks.add(block)) {	excessBlocksCount.incrementAndGet();	
block addtoexcessreplicate is added to excessreplicatemap 

public void removeStoredBlock(Block block, DatanodeDescriptor node) {	
block removestoredblock from 

public void removeStoredBlock(Block block, DatanodeDescriptor node) {	assert (namesystem.hasWriteLock());	{	BlockInfo storedBlock = getStoredBlock(block);	if (storedBlock == null || !blocksMap.removeNode(storedBlock, node)) {	
block removestoredblock has already been removed from node 

if (storedBlock == null || !blocksMap.removeNode(storedBlock, node)) {	return;	}	CachedBlock cblock = namesystem.getCacheManager().getCachedBlocks() .get(new CachedBlock(block.getBlockId(), (short) 0, false));	if (cblock != null) {	boolean removed = false;	removed |= node.getPendingCached().remove(cblock);	removed |= node.getCached().remove(cblock);	removed |= node.getPendingUncached().remove(cblock);	if (removed) {	
block removestoredblock removed from caching related lists on node 

}	}	if (!storedBlock.isDeleted()) {	bmSafeMode.decrementSafeBlockCount(storedBlock);	updateNeededReplications(storedBlock, -1, 0);	}	LightWeightHashSet<Block> excessBlocks = excessReplicateMap.get( node.getDatanodeUuid());	if (excessBlocks != null) {	if (excessBlocks.remove(block)) {	excessBlocksCount.decrementAndGet();	
block removestoredblock is removed from excessBlocks 

assert toUC.size() + toAdd.size() + toInvalidate.size() + toCorrupt.size() <= 1 : "The block should be only in one of the lists.";	for (StatefulBlockInfo b : toUC) {	addStoredBlockUnderConstruction(b, storageInfo);	}	long numBlocksLogged = 0;	for (BlockInfo b : toAdd) {	addStoredBlock(b, storageInfo, delHintNode, numBlocksLogged < maxNumBlocksToLog);	numBlocksLogged++;	}	if (numBlocksLogged > maxNumBlocksToLog) {	
block addblock logged info for of reported 

addStoredBlockUnderConstruction(b, storageInfo);	}	long numBlocksLogged = 0;	for (BlockInfo b : toAdd) {	addStoredBlock(b, storageInfo, delHintNode, numBlocksLogged < maxNumBlocksToLog);	numBlocksLogged++;	}	if (numBlocksLogged > maxNumBlocksToLog) {	}	for (Block b : toInvalidate) {	
block addblock block on node size does not belong to any file 

public void processIncrementalBlockReport(final DatanodeID nodeID, final StorageReceivedDeletedBlocks srdb) throws IOException {	assert namesystem.hasWriteLock();	final DatanodeDescriptor node = datanodeManager.getDatanode(nodeID);	if (node == null || !node.isRegistered()) {	
block processincrementalblockreport is received from dead or unregistered node 

received++;	break;	case RECEIVING_BLOCK: receiving++;	processAndHandleReportedBlock(storageInfo, rdbi.getBlock(), ReplicaState.RBW, null);	break;	default: String msg = "Unknown block status code reported by " + node + ": " + rdbi;	blockLog.warn(msg);	assert false : msg;	break;	}	
block block is received from 

break;	case RECEIVING_BLOCK: receiving++;	processAndHandleReportedBlock(storageInfo, rdbi.getBlock(), ReplicaState.RBW, null);	break;	default: String msg = "Unknown block status code reported by " + node + ": " + rdbi;	blockLog.warn(msg);	assert false : msg;	break;	}	}	
block namenode processincrementalblockreport from receiving received deleted 

private int invalidateWorkForOneNode(DatanodeInfo dn) {	final List<Block> toInvalidate;	namesystem.writeLock();	try {	if (namesystem.isInSafeMode()) {	
in safemode not computing replication work 

private int invalidateWorkForOneNode(DatanodeInfo dn) {	final List<Block> toInvalidate;	namesystem.writeLock();	try {	if (namesystem.isInSafeMode()) {	return 0;	}	try {	DatanodeDescriptor dnDescriptor = datanodeManager.getDatanode(dn);	if (dnDescriptor == null) {	
datanode cannot be found with uuid removing block invalidation work 

toInvalidate = invalidateBlocks.invalidateWork(dnDescriptor);	if (toInvalidate == null) {	return 0;	}	} catch(UnregisteredNodeException une) {	return 0;	}	} finally {	namesystem.writeUnlock();	}	
block ask to delete 

while (namesystem.isRunning()) {	try {	if (isPopulatingReplQueues()) {	computeDatanodeWork();	processPendingReplications();	rescanPostponedMisreplicatedBlocks();	}	Thread.sleep(replicationRecheckInterval);	} catch (Throwable t) {	if (!namesystem.isRunning()) {	
stopping replicationmonitor 

try {	if (isPopulatingReplQueues()) {	computeDatanodeWork();	processPendingReplications();	rescanPostponedMisreplicatedBlocks();	}	Thread.sleep(replicationRecheckInterval);	} catch (Throwable t) {	if (!namesystem.isRunning()) {	if (!(t instanceof InterruptedException)) {	
replicationmonitor received an exception while shutting down 

processPendingReplications();	rescanPostponedMisreplicatedBlocks();	}	Thread.sleep(replicationRecheckInterval);	} catch (Throwable t) {	if (!namesystem.isRunning()) {	if (!(t instanceof InterruptedException)) {	}	break;	} else if (!checkNSRunning && t instanceof InterruptedException) {	
stopping replicationmonitor for testing 

}	Thread.sleep(replicationRecheckInterval);	} catch (Throwable t) {	if (!namesystem.isRunning()) {	if (!(t instanceof InterruptedException)) {	}	break;	} else if (!checkNSRunning && t instanceof InterruptedException) {	break;	}	
replicationmonitor thread received runtime exception 

public void initializeReplQueues() {	
initializing replication queues 

========================= hadoop sample_8345 =========================

public Set<String> getAccessibleNodeLabels(String queue) {	String accessibleLabelStr = get(getQueuePrefix(queue) + ACCESSIBLE_NODE_LABELS);	if (accessibleLabelStr == null) {	if (!queue.equals(ROOT)) {	return null;	}	} else {	if (queue.equals(ROOT)) {	
accessible node labels for root queue will be ignored it will be automatically set to 

public Resource getMaximumAllocationPerQueue(String queue) {	String queuePrefix = getQueuePrefix(queue);	long maxAllocationMbPerQueue = getInt(queuePrefix + MAXIMUM_ALLOCATION_MB, (int)UNDEFINED);	int maxAllocationVcoresPerQueue = getInt( queuePrefix + MAXIMUM_ALLOCATION_VCORES, (int)UNDEFINED);	if (LOG.isDebugEnabled()) {	
max alloc mb per queue for is 

public Resource getMaximumAllocationPerQueue(String queue) {	String queuePrefix = getQueuePrefix(queue);	long maxAllocationMbPerQueue = getInt(queuePrefix + MAXIMUM_ALLOCATION_MB, (int)UNDEFINED);	int maxAllocationVcoresPerQueue = getInt( queuePrefix + MAXIMUM_ALLOCATION_VCORES, (int)UNDEFINED);	if (LOG.isDebugEnabled()) {	
max alloc vcores per queue for is 

public Resource getMaximumAllocationPerQueue(String queue) {	String queuePrefix = getQueuePrefix(queue);	long maxAllocationMbPerQueue = getInt(queuePrefix + MAXIMUM_ALLOCATION_MB, (int)UNDEFINED);	int maxAllocationVcoresPerQueue = getInt( queuePrefix + MAXIMUM_ALLOCATION_VCORES, (int)UNDEFINED);	if (LOG.isDebugEnabled()) {	}	Resource clusterMax = getMaximumAllocation();	if (maxAllocationMbPerQueue == (int)UNDEFINED) {	
max alloc mb per queue for is undefined 

String queuePrefix = getQueuePrefix(queue);	long maxAllocationMbPerQueue = getInt(queuePrefix + MAXIMUM_ALLOCATION_MB, (int)UNDEFINED);	int maxAllocationVcoresPerQueue = getInt( queuePrefix + MAXIMUM_ALLOCATION_VCORES, (int)UNDEFINED);	if (LOG.isDebugEnabled()) {	}	Resource clusterMax = getMaximumAllocation();	if (maxAllocationMbPerQueue == (int)UNDEFINED) {	maxAllocationMbPerQueue = clusterMax.getMemorySize();	}	if (maxAllocationVcoresPerQueue == (int)UNDEFINED) {	
max alloc vcore per queue for is undefined 

public int getOffSwitchPerHeartbeatLimit() {	int limit = getInt(OFFSWITCH_PER_HEARTBEAT_LIMIT, DEFAULT_OFFSWITCH_PER_HEARTBEAT_LIMIT);	if (limit < 1) {	
using 

========================= hadoop sample_900 =========================

public void messageReceived(ChannelHandlerContext ctx, MessageEvent e) throws Exception {	ChannelBuffer buf = (ChannelBuffer) e.getMessage();	ByteBuffer b = buf.toByteBuffer().asReadOnlyBuffer();	XDR in = new XDR(b, XDR.State.READING);	RpcInfo info = null;	try {	RpcCall callHeader = RpcCall.read(in);	ChannelBuffer dataBuffer = ChannelBuffers.wrappedBuffer(in.buffer() .slice());	info = new RpcInfo(callHeader, dataBuffer, ctx, e.getChannel(), e.getRemoteAddress());	} catch (Exception exc) {	
malformed rpc request from 

========================= hadoop sample_4366 =========================

public void setConf(Configuration conf) {	this.conf = conf;	final Class<? extends Random> klass = conf.getClass( HADOOP_SECURITY_SECURE_RANDOM_IMPL_KEY, OsSecureRandom.class, Random.class);	try {	random = ReflectionUtils.newInstance(klass, conf);	if (LOG.isDebugEnabled()) {	
using as random number generator 

public void setConf(Configuration conf) {	this.conf = conf;	final Class<? extends Random> klass = conf.getClass( HADOOP_SECURITY_SECURE_RANDOM_IMPL_KEY, OsSecureRandom.class, Random.class);	try {	random = ReflectionUtils.newInstance(klass, conf);	if (LOG.isDebugEnabled()) {	}	} catch (Exception e) {	
unable to use falling back to java securerandom 

========================= hadoop sample_3509 =========================

List<String> localDirs = dirsHandler.getLocalDirs();	List<String> logDirs = dirsHandler.getLogDirs();	createUserLocalDirs(localDirs, user);	createUserCacheDirs(localDirs, user);	createAppDirs(localDirs, user, appId);	createAppLogDirs(appId, logDirs, user);	Path appStorageDir = getWorkingDir(localDirs, user, appId);	String tokenFn = String.format(ContainerLocalizer.TOKEN_FILE_NAME_FMT, locId);	Path tokenDst = new Path(appStorageDir, tokenFn);	copyFile(nmPrivateContainerTokensPath, tokenDst, user);	
copying from to 

Path launchDst = new Path(containerWorkDir, ContainerLaunch.CONTAINER_SCRIPT);	copyFile(nmPrivateContainerScriptPath, launchDst, user);	LocalWrapperScriptBuilder sb = getLocalWrapperScriptBuilder( containerIdStr, containerWorkDir);	if (Shell.WINDOWS && sb.getWrapperScriptPath().toString().length() > WIN_MAX_PATH) {	throw new IOException(String.format( "Cannot launch container using script at path %s, because it exceeds " + "the maximum supported path length of %d characters.  Consider " + "configuring shorter directories in %s.", sb.getWrapperScriptPath(), WIN_MAX_PATH, YarnConfiguration.NM_LOCAL_DIRS));	}	Path pidFile = getPidFilePath(containerId);	if (pidFile != null) {	sb.writeLocalWrapperScript(launchDst, pidFile);	} else {	
container was marked as inactive returning terminated error 

return ExitCode.TERMINATED.getExitCode();	}	Shell.CommandExecutor shExec = null;	try {	setScriptExecutable(launchDst, user);	setScriptExecutable(sb.getWrapperScriptPath(), user);	shExec = buildCommandExecutor(sb.getWrapperScriptPath().toString(), containerIdStr, user, pidFile, container.getResource(), new File(containerWorkDir.toUri().getPath()), container.getLaunchContext().getEnvironment());	if (isContainerActive(containerId)) {	shExec.execute();	} else {	
container was marked as inactive returning terminated error 

if (isContainerActive(containerId)) {	shExec.execute();	} else {	return ExitCode.TERMINATED.getExitCode();	}	} catch (IOException e) {	if (null == shExec) {	return -1;	}	int exitCode = shExec.getExitCode();	
exit code from container is 

shExec.execute();	} else {	return ExitCode.TERMINATED.getExitCode();	}	} catch (IOException e) {	if (null == shExec) {	return -1;	}	int exitCode = shExec.getExitCode();	if (exitCode != ExitCode.FORCE_KILLED.getExitCode() && exitCode != ExitCode.TERMINATED.getExitCode()) {	
exception from container launch with container id and exit code 

protected CommandExecutor buildCommandExecutor(String wrapperScriptPath, String containerIdStr, String user, Path pidFile, Resource resource, File workDir, Map<String, String> environment) {	String[] command = getRunCommand(wrapperScriptPath, containerIdStr, user, pidFile, this.getConf(), resource);	
launchcontainer 

public boolean signalContainer(ContainerSignalContext ctx) throws IOException {	String user = ctx.getUser();	String pid = ctx.getPid();	Signal signal = ctx.getSignal();	if (LOG.isDebugEnabled()) {	
sending signal to pid as user 

public void deleteAsUser(DeletionAsUserContext ctx) throws IOException, InterruptedException {	Path subDir = ctx.getSubDir();	List<Path> baseDirs = ctx.getBasedirs();	if (baseDirs == null || baseDirs.size() == 0) {	
deleting absolute path 

public void deleteAsUser(DeletionAsUserContext ctx) throws IOException, InterruptedException {	Path subDir = ctx.getSubDir();	List<Path> baseDirs = ctx.getBasedirs();	if (baseDirs == null || baseDirs.size() == 0) {	if (!lfs.delete(subDir, true)) {	
delete returned false for path 

public void deleteAsUser(DeletionAsUserContext ctx) throws IOException, InterruptedException {	Path subDir = ctx.getSubDir();	List<Path> baseDirs = ctx.getBasedirs();	if (baseDirs == null || baseDirs.size() == 0) {	if (!lfs.delete(subDir, true)) {	}	return;	}	for (Path baseDir : baseDirs) {	Path del = subDir == null ? baseDir : new Path(baseDir, subDir);	
deleting path 

List<Path> baseDirs = ctx.getBasedirs();	if (baseDirs == null || baseDirs.size() == 0) {	if (!lfs.delete(subDir, true)) {	}	return;	}	for (Path baseDir : baseDirs) {	Path del = subDir == null ? baseDir : new Path(baseDir, subDir);	try {	if (!lfs.delete(del, true)) {	
delete returned false for path 

protected Path getWorkingDir(List<String> localDirs, String user, String appId) throws IOException {	long totalAvailable = 0L;	long[] availableOnDisk = new long[localDirs.size()];	int i = 0;	for (String localDir : localDirs) {	Path curBase = getApplicationDir(new Path(localDir), user, appId);	long space = 0L;	try {	space = getDiskFreeSpace(curBase);	} catch (IOException e) {	
unable to get free space for 

========================= hadoop sample_1922 =========================

public void testCapacityScheduler() throws Exception {	
start testcapacityscheduler 

application_1.addNodeManager(host_0, 1234, nm_0);	application_1.addNodeManager(host_1, 1234, nm_1);	Resource capability_1_0 = Resources.createResource(3 * GB, 1);	application_1.addResourceRequestSpec(priority_1, capability_1_0);	Resource capability_1_1 = Resources.createResource(2 * GB, 1);	application_1.addResourceRequestSpec(priority_0, capability_1_1);	Task task_1_0 = new Task(application_1, priority_1, new String[] {host_0, host_1});	application_1.addTask(task_1_0);	application_0.schedule();	application_1.schedule();	
kick 

application_0.schedule();	application_1.schedule();	nodeUpdate(nm_0);	nodeUpdate(nm_1);	application_0.schedule();	checkApplicationResourceUsage(1 * GB, application_0);	application_1.schedule();	checkApplicationResourceUsage(3 * GB, application_1);	checkNodeResourceUsage(4*GB, nm_0);	checkNodeResourceUsage(0*GB, nm_1);	
adding new tasks 

application_1.schedule();	checkApplicationResourceUsage(3 * GB, application_1);	checkNodeResourceUsage(4*GB, nm_0);	checkNodeResourceUsage(0*GB, nm_1);	Task task_1_1 = new Task(application_1, priority_0, new String[] {ResourceRequest.ANY});	application_1.addTask(task_1_1);	application_1.schedule();	Task task_0_1 = new Task(application_0, priority_0, new String[] {host_0, host_1});	application_0.addTask(task_0_1);	application_0.schedule();	
sending hb from 

checkApplicationResourceUsage(3 * GB, application_1);	checkNodeResourceUsage(4*GB, nm_0);	checkNodeResourceUsage(0*GB, nm_1);	Task task_1_1 = new Task(application_1, priority_0, new String[] {ResourceRequest.ANY});	application_1.addTask(task_1_1);	application_1.schedule();	Task task_0_1 = new Task(application_0, priority_0, new String[] {host_0, host_1});	application_0.addTask(task_0_1);	application_0.schedule();	nodeUpdate(nm_0);	
sending hb from 

checkNodeResourceUsage(4*GB, nm_0);	checkNodeResourceUsage(0*GB, nm_1);	Task task_1_1 = new Task(application_1, priority_0, new String[] {ResourceRequest.ANY});	application_1.addTask(task_1_1);	application_1.schedule();	Task task_0_1 = new Task(application_0, priority_0, new String[] {host_0, host_1});	application_0.addTask(task_0_1);	application_0.schedule();	nodeUpdate(nm_0);	nodeUpdate(nm_1);	
trying to allocate 

nodeUpdate(nm_0);	nodeUpdate(nm_1);	application_0.schedule();	checkApplicationResourceUsage(1 * GB, application_0);	application_1.schedule();	checkApplicationResourceUsage(5 * GB, application_1);	nodeUpdate(nm_0);	nodeUpdate(nm_1);	checkNodeResourceUsage(4*GB, nm_0);	checkNodeResourceUsage(2*GB, nm_1);	
end testcapacityscheduler 

public void testNotAssignMultiple() throws Exception {	
start testnotassignmultiple 

Resource capability10 = Resources.createResource(3 * GB, 1);	application1.addResourceRequestSpec(priority0, capability10);	Resource capability11 = Resources.createResource(4 * GB, 1);	application1.addResourceRequestSpec(priority1, capability11);	Task task10 = new Task(application1, priority0, new String[] {host0});	Task task11 = new Task(application1, priority1, new String[] {host0});	application1.addTask(task10);	application1.addTask(task11);	application0.schedule();	application1.schedule();	
kick 

application1.schedule();	checkNodeResourceUsage(3 * GB, nm0);	checkApplicationResourceUsage(0 * GB, application0);	checkApplicationResourceUsage(3 * GB, application1);	nodeUpdate(rm, nm0);	application0.schedule();	checkApplicationResourceUsage(1 * GB, application0);	application1.schedule();	checkApplicationResourceUsage(3 * GB, application1);	checkNodeResourceUsage(4 * GB, nm0);	
start testnotassignmultiple 

public void testAssignMultiple() throws Exception {	
start testassignmultiple 

Resource capability10 = Resources.createResource(3 * GB, 1);	application1.addResourceRequestSpec(priority0, capability10);	Resource capability11 = Resources.createResource(4 * GB, 1);	application1.addResourceRequestSpec(priority1, capability11);	Task task10 = new Task(application1, priority0, new String[] {host0});	Task task11 = new Task(application1, priority1, new String[] {host0});	application1.addTask(task10);	application1.addTask(task11);	application0.schedule();	application1.schedule();	
kick 

application1.schedule();	checkNodeResourceUsage(4 * GB, nm0);	checkApplicationResourceUsage(1 * GB, application0);	checkApplicationResourceUsage(3 * GB, application1);	nodeUpdate(rm, nm0);	application0.schedule();	checkApplicationResourceUsage(3 * GB, application0);	application1.schedule();	checkApplicationResourceUsage(7 * GB, application1);	checkNodeResourceUsage(10 * GB, nm0);	
start testassignmultiple 

conf.setUserLimitFactor(A1, 100.0f);	conf.setCapacity(A2, A2_CAPACITY);	conf.setUserLimitFactor(A2, 100.0f);	conf.setQueues(B, new String[] {"b1", "b2", "b3"});	conf.setCapacity(B1, B1_CAPACITY);	conf.setUserLimitFactor(B1, 100.0f);	conf.setCapacity(B2, B2_CAPACITY);	conf.setUserLimitFactor(B2, 100.0f);	conf.setCapacity(B3, B3_CAPACITY);	conf.setUserLimitFactor(B3, 100.0f);	
setup top level queues a and b 

private CapacitySchedulerConfiguration setupQueueConfWithOutChildrenOfB( CapacitySchedulerConfiguration conf) {	conf.setQueues(CapacitySchedulerConfiguration.ROOT, new String[] {"a","b"});	conf.setCapacity(A, A_CAPACITY);	conf.setCapacity(B, B_CAPACITY);	conf.setQueues(A, new String[] {"a1","a2"});	conf.setCapacity(A1, A1_CAPACITY);	conf.setUserLimitFactor(A1, 100.0f);	conf.setCapacity(A2, A2_CAPACITY);	conf.setUserLimitFactor(A2, 100.0f);	
setup top level queues a and b without children 

conf.setQueues(A, new String[] { "a1", "a2" });	conf.setCapacity(A1, A1_CAPACITY);	conf.setUserLimitFactor(A1, 100.0f);	conf.setCapacity(A2, A2_CAPACITY);	conf.setUserLimitFactor(A2, 100.0f);	conf.setQueues(B, new String[] { "b2", "b3" });	conf.setCapacity(B2, B2_CAPACITY + B1_CAPACITY);	conf.setUserLimitFactor(B2, 100.0f);	conf.setCapacity(B3, B3_CAPACITY);	conf.setUserLimitFactor(B3, 100.0f);	
setup top level queues a and b without 

private CapacitySchedulerConfiguration setupQueueConfigurationWithOutB( CapacitySchedulerConfiguration conf) {	conf.setQueues(CapacitySchedulerConfiguration.ROOT, new String[] { "a" });	conf.setCapacity(A, A_CAPACITY + B_CAPACITY);	conf.setQueues(A, new String[] { "a1", "a2" });	conf.setCapacity(A1, A1_CAPACITY);	conf.setUserLimitFactor(A1, 100.0f);	conf.setCapacity(A2, A2_CAPACITY);	conf.setUserLimitFactor(A2, 100.0f);	
setup top level queues a 

private CapacitySchedulerConfiguration setupBlockedQueueConfiguration( CapacitySchedulerConfiguration conf) {	conf.setQueues(CapacitySchedulerConfiguration.ROOT, new String[]{"a", "b"});	conf.setCapacity(A, 80f);	conf.setCapacity(B, 20f);	conf.setUserLimitFactor(A, 100);	conf.setUserLimitFactor(B, 100);	conf.setMaximumCapacity(A, 100);	conf.setMaximumCapacity(B, 100);	
setup top level queues a and b 

RMAppAttempt attempt1 = app1.getCurrentAppAttempt();	MockAM am1 = rm.sendAMLaunched(attempt1.getAppAttemptId());	am1.registerAppAttempt();	SchedulerNodeReport report_nm1 = rm.getResourceScheduler().getNodeReport( nm1.getNodeId());	Assert.assertEquals(2 * GB, report_nm1.getUsedResource().getMemorySize());	Assert.assertEquals(2 * GB, report_nm1.getAvailableResource().getMemorySize());	am1.addRequests(new String[] { "127.0.0.1", "127.0.0.2" }, 2 * GB, 1, 1);	AllocateResponse alloc1Response = am1.schedule();	nm1.nodeHeartbeat(true);	while (alloc1Response.getAllocatedContainers().size() < 1) {	
waiting for containers to be created for app 

nodeResourceMap.put(nm1.getNodeId(), ResourceOption.newInstance(Resource.newInstance(2 * GB, 1), -1));	UpdateNodeResourceRequest request = UpdateNodeResourceRequest.newInstance(nodeResourceMap);	AdminService as = ((MockRM)rm).getAdminService();	as.updateNodeResource(request);	waitCount = 0;	while (waitCount++ != 20) {	report_nm1 = rm.getResourceScheduler().getNodeReport(nm1.getNodeId());	if (report_nm1.getAvailableResource().getMemorySize() != 0) {	break;	}	
waiting for rmnoderesourceupdateevent to be handled tried times already 

}	Thread.sleep(1000);	}	report_nm1 = rm.getResourceScheduler().getNodeReport(nm1.getNodeId());	Assert.assertEquals(4 * GB, report_nm1.getUsedResource().getMemorySize());	Assert.assertEquals(-2 * GB, report_nm1.getAvailableResource().getMemorySize());	ContainerStatus containerStatus = BuilderUtils.newContainerStatus( c1.getId(), ContainerState.COMPLETE, "", 0, c1.getResource());	nm1.containerStatus(containerStatus);	waitCount = 0;	while (attempt1.getJustFinishedContainers().size() < 1 && waitCount++ != 20) {	
waiting for containers to be finished for app tried times already 

Assert.assertEquals(1, attempt1.getJustFinishedContainers().size());	Assert.assertEquals(1, am1.schedule().getCompletedContainersStatuses().size());	report_nm1 = rm.getResourceScheduler().getNodeReport(nm1.getNodeId());	Assert.assertEquals(2 * GB, report_nm1.getUsedResource().getMemorySize());	Assert.assertEquals(0 * GB, report_nm1.getAvailableResource().getMemorySize());	am1.addRequests(new String[] { "127.0.0.1", "127.0.0.2" }, 3 * GB, 1, 1);	alloc1Response = am1.schedule();	Assert.assertEquals("Shouldn't have enough resource to allocate containers", 0, alloc1Response.getAllocatedContainers().size());	int times = 0;	while (alloc1Response.getAllocatedContainers().size() < 1 && times++ < 10) {	
waiting for containers to be allocated for app tried times already 

MockNM nm1 = rm.registerNode("localhost:1234", 5120);	Map<ApplicationAccessType, String> acls = new HashMap<ApplicationAccessType, String>(2);	acls.put(ApplicationAccessType.VIEW_APP, "*");	RMApp app = rm.submitApp(1024, "appname", "appuser", acls);	nm1.nodeHeartbeat(true);	RMAppAttempt attempt = app.getCurrentAppAttempt();	ApplicationAttemptId applicationAttemptId = attempt.getAppAttemptId();	int msecToWait = 10000;	int msecToSleep = 100;	while (attempt.getAppAttemptState() != RMAppAttemptState.LAUNCHED && msecToWait > 0) {	
waiting for appattempt to reach launched state current state is 

try {	cs.reinitialize(conf, mockContext);	fail("Expected to throw exception when refresh queue tries to delete a" + " queue with running apps");	} catch (IOException e) {	}	conf = new CapacitySchedulerConfiguration();	setupQueueConfigurationWithOutB1(conf);	try {	cs.reinitialize(conf, mockContext);	} catch (IOException e) {	
expected to not throw exception when refresh queue tries to delete a queue without running apps 

========================= hadoop sample_516 =========================

public static void setSessionTimeZone(Configuration conf, Connection conn) throws SQLException {	Method method;	try {	method = conn.getClass().getMethod( "setSessionTimeZone", new Class [] {String.class});	} catch (Exception ex) {	
could not find method setsessiontimezone in 

Method method;	try {	method = conn.getClass().getMethod( "setSessionTimeZone", new Class [] {String.class});	} catch (Exception ex) {	throw new SQLException(ex);	}	String clientTimeZone = conf.get(SESSION_TIMEZONE_KEY, "GMT");	try {	method.setAccessible(true);	method.invoke(conn, clientTimeZone);	
time zone has been set to 

try {	method = conn.getClass().getMethod( "setSessionTimeZone", new Class [] {String.class});	} catch (Exception ex) {	throw new SQLException(ex);	}	String clientTimeZone = conf.get(SESSION_TIMEZONE_KEY, "GMT");	try {	method.setAccessible(true);	method.invoke(conn, clientTimeZone);	} catch (Exception ex) {	
time zone could not be set on oracle database 

try {	method = conn.getClass().getMethod( "setSessionTimeZone", new Class [] {String.class});	} catch (Exception ex) {	throw new SQLException(ex);	}	String clientTimeZone = conf.get(SESSION_TIMEZONE_KEY, "GMT");	try {	method.setAccessible(true);	method.invoke(conn, clientTimeZone);	} catch (Exception ex) {	
setting default time zone gmt 

throw new SQLException(ex);	}	String clientTimeZone = conf.get(SESSION_TIMEZONE_KEY, "GMT");	try {	method.setAccessible(true);	method.invoke(conn, clientTimeZone);	} catch (Exception ex) {	try {	method.invoke(conn, "GMT");	} catch (Exception ex2) {	
could not set time zone for oracle connection 

========================= hadoop sample_5062 =========================

public ShellBasedIdMapping(Configuration conf, boolean constructFullMapAtInit) throws IOException {	this.constructFullMapAtInit = constructFullMapAtInit;	long updateTime = conf.getLong( IdMappingConstant.USERGROUPID_UPDATE_MILLIS_KEY, IdMappingConstant.USERGROUPID_UPDATE_MILLIS_DEFAULT);	if (updateTime < IdMappingConstant.USERGROUPID_UPDATE_MILLIS_MIN) {	
user configured user account update time is less than minute use minute instead 

private void checkAndUpdateMaps() {	if (isExpired()) {	
update cache now 

private void checkAndUpdateMaps() {	if (isExpired()) {	try {	updateMaps();	} catch (IOException e) {	
can t update the maps will use the old ones which can potentially cause problem 

private static void reportDuplicateEntry(final String header, final Integer key, final String value, final Integer ekey, final String evalue) {	
new entry d s existing entry d s n s n s the new entry is to be ignored for the following reason 

BufferedReader br = null;	try {	Process process = Runtime.getRuntime().exec( new String[] { "bash", "-c", command });	br = new BufferedReader( new InputStreamReader(process.getInputStream(), Charset.defaultCharset()));	String line = null;	while ((line = br.readLine()) != null) {	String[] nameId = line.split(regex);	if ((nameId == null) || (nameId.length != 2)) {	throw new IOException("Can't parse " + mapName + " list entry:" + line);	}	
add to map id 

continue;	}	if (map.containsValue(value)) {	final Integer prevKey = map.inverse().get(value);	reportDuplicateEntry( "Got multiple ids associated with the same name: ", key, value, prevKey, value);	continue;	}	map.put(key, value);	updated = true;	}	
updated map size 

}	if (map.containsValue(value)) {	final Integer prevKey = map.inverse().get(value);	reportDuplicateEntry( "Got multiple ids associated with the same name: ", key, value, prevKey, value);	continue;	}	map.put(key, value);	updated = true;	}	} catch (IOException e) {	
can t update map 

map.put(key, value);	updated = true;	}	} catch (IOException e) {	throw e;	} finally {	if (br != null) {	try {	br.close();	} catch (IOException e1) {	
can t close bufferedreader of command result 

private boolean checkSupportedPlatform() {	if (!OS.startsWith("Linux") && !OS.startsWith("Mac")) {	
platform is not supported can t update user map and group map and nobody will be used for any user and group 

private synchronized void updateStaticMapping() throws IOException {	final boolean init = (staticMapping == null);	if (staticMappingFile.exists()) {	long lmTime = staticMappingFile.lastModified();	if (lmTime != lastModificationTimeStaticMap) {	
using reloading for static uid gid mapping 

long lmTime = staticMappingFile.lastModified();	if (lmTime != lastModificationTimeStaticMap) {	lastModificationTimeStaticMap = lmTime;	staticMapping = parseStaticMap(staticMappingFile);	}	} else {	if (init) {	staticMapping = new StaticMapping(new HashMap<Integer, Integer>(), new HashMap<Integer, Integer>());	}	if (lastModificationTimeStaticMap != 0 || init) {	
not doing static uid gid mapping because does not exist 

Map<Integer, Integer> gidMapping = new HashMap<Integer, Integer>();	BufferedReader in = new BufferedReader(new InputStreamReader( new FileInputStream(staticMapFile), StandardCharsets.UTF_8));	try {	String line = null;	while ((line = in.readLine()) != null) {	if (EMPTY_LINE.matcher(line).matches() || COMMENT_LINE.matcher(line).matches()) {	continue;	}	Matcher lineMatcher = MAPPING_LINE.matcher(line);	if (!lineMatcher.matches()) {	
could not parse line lines should be of the form uid gid remote id local id blank lines and everything following a on a line will be ignored 

public int getUidAllowingUnknown(String user) {	checkAndUpdateMaps();	int uid;	try {	uid = getUid(user);	} catch (IOException e) {	uid = user.hashCode();	
can t map user use its string hashcode 

public int getGidAllowingUnknown(String group) {	checkAndUpdateMaps();	int gid;	try {	gid = getGid(group);	} catch (IOException e) {	gid = group.hashCode();	
can t map group use its string hashcode 

========================= hadoop sample_3765 =========================

public void invalidateStorage(FSImage fi, Set<File> filesToInvalidate) throws IOException {	ArrayList<StorageDirectory> al = new ArrayList<StorageDirectory>(2);	Iterator<StorageDirectory> it = fi.getStorage().dirIterator();	while(it.hasNext()) {	StorageDirectory sd = it.next();	if(filesToInvalidate.contains(sd.getRoot())) {	
causing io error on 

public void testDfsAdminCmd() throws Exception {	cluster = new MiniDFSCluster.Builder(config). numDataNodes(2). manageNameDfsDirs(false).build();	cluster.waitActive();	try {	FSImage fsi = cluster.getNameNode().getFSImage();	boolean restore = fsi.getStorage().getRestoreFailedStorage();	
restore is 

========================= hadoop sample_7376 =========================

public void testNativeCodeLoaded() {	if (requireTestJni() == false) {	
testnativecodeloader libhadoop so testing is not required 

public void testNativeCodeLoaded() {	if (requireTestJni() == false) {	return;	}	if (!NativeCodeLoader.isNativeCodeLoaded()) {	String LD_LIBRARY_PATH = System.getenv().get("LD_LIBRARY_PATH");	if (LD_LIBRARY_PATH == null) LD_LIBRARY_PATH = "";	fail("TestNativeCodeLoader: libhadoop.so testing was required, but " + "libhadoop.so was not loaded.  LD_LIBRARY_PATH = " + LD_LIBRARY_PATH);	}	
testhdfsnativecodeloader libhadoop so is loaded 

========================= hadoop sample_7726 =========================

public void stop() {	if(checkpointManager != null) {	checkpointManager.shouldRun = false;	}	if(namenode != null && getRegistration() != null) {	try {	namenode.errorReport(getRegistration(), NamenodeProtocol.FATAL, "Shutting down.");	} catch(IOException e) {	
failed to report to name node 

public FenceResponse fence(JournalInfo journalInfo, long epoch, String fencerInfo) throws IOException {	
fenced by with epoch 

InetSocketAddress nnAddress = NameNode.getServiceAddress(conf, true);	this.namenode = NameNodeProxies.createNonHAProxy(conf, nnAddress, NamenodeProtocol.class, UserGroupInformation.getCurrentUser(), true).getProxy();	this.nnRpcAddress = NetUtils.getHostPortString(nnAddress);	this.nnHttpAddress = DFSUtil.getInfoServer(nnAddress, conf, DFSUtil.getHttpClientScheme(conf)).toURL();	NamespaceInfo nsInfo = null;	while(!isStopRequested()) {	try {	nsInfo = handshake(namenode);	break;	} catch(SocketTimeoutException e) {	
problem connecting to server 

this.nnHttpAddress = DFSUtil.getInfoServer(nnAddress, conf, DFSUtil.getHttpClientScheme(conf)).toURL();	NamespaceInfo nsInfo = null;	while(!isStopRequested()) {	try {	nsInfo = handshake(namenode);	break;	} catch(SocketTimeoutException e) {	try {	Thread.sleep(1000);	} catch (InterruptedException ie) {	
encountered exception 

nsInfo.validateStorage(storage);	}	bnImage.initEditLog(StartupOption.REGULAR);	setRegistration();	NamenodeRegistration nnReg = null;	while(!isStopRequested()) {	try {	nnReg = namenode.registerSubordinateNamenode(getRegistration());	break;	} catch(SocketTimeoutException e) {	
problem connecting to name node 

setRegistration();	NamenodeRegistration nnReg = null;	while(!isStopRequested()) {	try {	nnReg = namenode.registerSubordinateNamenode(getRegistration());	break;	} catch(SocketTimeoutException e) {	try {	Thread.sleep(1000);	} catch (InterruptedException ie) {	
encountered exception 

========================= hadoop sample_8129 =========================

public String getStorageAccountKey(String accountName, Configuration conf) throws KeyProviderException {	String key = null;	try {	Configuration c = ProviderUtils.excludeIncompatibleCredentialProviders( conf, NativeAzureFileSystem.class);	char[] keyChars = c.getPassword(getStorageAccountKeyName(accountName));	if (keyChars != null) {	key = new String(keyChars);	}	} catch(IOException ioe) {	
unable to get key from credential providers 

========================= hadoop sample_6422 =========================

public SynthTraceJobProducer(Configuration conf, Path path) throws IOException {	
SynthTraceJobProducer 

listStoryParams = new PriorityQueue<>(10, new Comparator<StoryParams>() {	public int compare(StoryParams o1, StoryParams o2) {	long value = o2.actualSubmissionTime - o1.actualSubmissionTime;	if ((int)value != value) {	throw new ArithmeticException("integer overflow");	}	return (int)value;	}	});	createStoryParams();	
generated deadlines for jobs 

========================= hadoop sample_6072 =========================

long totalNonDfsUsed = 0;	Set<DatanodeStorageInfo> failedStorageInfos = null;	final boolean checkFailedStorages;	if (volumeFailureSummary != null && this.volumeFailureSummary != null) {	checkFailedStorages = volumeFailureSummary.getLastVolumeFailureDate() > this.volumeFailureSummary.getLastVolumeFailureDate();	} else {	checkFailedStorages = (volFailures > this.volumeFailures) || !heartbeatedSinceRegistration;	}	if (checkFailedStorages) {	if (this.volumeFailures != volFailures) {	
number of failed storages changes from to 

excessStorages.remove(report.getStorage().getStorageID());	}	for (final DatanodeStorageInfo storageInfo : excessStorages.values()) {	if (storageInfo.numBlocks() == 0) {	DatanodeStorageInfo info = storageMap.remove(storageInfo.getStorageID());	if (!hasStorageType(info.getStorageType())) {	if (getParent() instanceof DFSTopologyNodeImpl) {	((DFSTopologyNodeImpl) getParent()).childRemoveStorage(getName(), info.getStorageType());	}	}	
removed storage from datanode 

}	for (final DatanodeStorageInfo storageInfo : excessStorages.values()) {	if (storageInfo.numBlocks() == 0) {	DatanodeStorageInfo info = storageMap.remove(storageInfo.getStorageID());	if (!hasStorageType(info.getStorageType())) {	if (getParent() instanceof DFSTopologyNodeImpl) {	((DFSTopologyNodeImpl) getParent()).childRemoveStorage(getName(), info.getStorageType());	}	}	} else {	
deferring removal of stale storage with blocks 

private void updateFailedStorage( Set<DatanodeStorageInfo> failedStorageInfos) {	for (DatanodeStorageInfo storageInfo : failedStorageInfos) {	if (storageInfo.getState() != DatanodeStorage.State.FAILED) {	
failed 

========================= hadoop sample_8353 =========================

public static void writeChannel(Channel channel, XDR out, int xid) {	if (channel == null) {	
null channel should only happen in tests do nothing 

public static void writeChannelCommit(Channel channel, XDR out, int xid) {	if (RpcProgramNfs3.LOG.isDebugEnabled()) {	
commit done 

========================= hadoop sample_7049 =========================

String nnId = record.getNamenodeKey();	MembershipState existingEntry = null;	cacheReadLock.lock();	try {	existingEntry = this.activeRegistrations.get(nnId);	} finally {	cacheReadLock.unlock();	}	if (existingEntry != null) {	if (existingEntry.getState() != record.getState()) {	
nn registration state has changed 

MembershipState existingEntry = null;	cacheReadLock.lock();	try {	existingEntry = this.activeRegistrations.get(nnId);	} finally {	cacheReadLock.unlock();	}	if (existingEntry != null) {	if (existingEntry.getState() != record.getState()) {	} else {	
updating nn registration 

try {	existingEntry = this.activeRegistrations.get(nnId);	} finally {	cacheReadLock.unlock();	}	if (existingEntry != null) {	if (existingEntry.getState() != record.getState()) {	} else {	}	} else {	
inserting new nn registration 

String nsId = membership.getNameserviceId();	FederationNamespaceInfo nsInfo = new FederationNamespaceInfo(bpId, cId, nsId);	this.activeNamespaces.add(nsInfo);	}	}	for (List<MembershipState> nnRegistration : nnRegistrations.values()) {	MembershipState representativeRecord = getRepresentativeQuorum(nnRegistration);	String nnKey = representativeRecord.getNamenodeKey();	this.activeRegistrations.put(nnKey, representativeRecord);	}	
refreshed nn registrations from state store 

TreeSet<MembershipState> largestSet = new TreeSet<>();	for (TreeSet<MembershipState> matchingSet : occurenceMap.values()) {	if (largestSet.size() < matchingSet.size()) {	largestSet = matchingSet;	}	}	if (largestSet.size() > records.size() / 2) {	return largestSet.first();	} else if (records.size() > 0) {	TreeSet<MembershipState> sortedList = new TreeSet<>(records);	
quorum failed using most recent 

========================= hadoop sample_8216 =========================

public void write(DataOutput out) throws IOException {	
writing nmtokenidentifiernewfortest to rpc layer 

========================= hadoop sample_1934 =========================

conf.setLong(DFSConfigKeys.DFS_NAMENODE_NUM_EXTRA_EDITS_RETAINED_KEY, 0);	File sd0 = new File(TEST_ROOT_DIR, "nn0");	File sd1 = new File(TEST_ROOT_DIR, "nn1");	File cd0 = new File(sd0, "current");	File cd1 = new File(sd1, "current");	conf.set(DFSConfigKeys.DFS_NAMENODE_NAME_DIR_KEY, Joiner.on(",").join(sd0, sd1));	try {	cluster = new MiniDFSCluster.Builder(conf) .numDataNodes(0) .manageNameDfsDirs(false) .format(true).build();	NameNode nn = cluster.getNameNode();	doSaveNamespace(nn);	
after first save images and should exist in both dirs 

conf.set(DFSConfigKeys.DFS_NAMENODE_NAME_DIR_KEY, Joiner.on(",").join(sd0, sd1));	try {	cluster = new MiniDFSCluster.Builder(conf) .numDataNodes(0) .manageNameDfsDirs(false) .format(true).build();	NameNode nn = cluster.getNameNode();	doSaveNamespace(nn);	assertGlobEquals(cd0, "fsimage_\\d*", getImageFileName(0), getImageFileName(2));	assertGlobEquals(cd1, "fsimage_\\d*", getImageFileName(0), getImageFileName(2));	assertGlobEquals(cd0, "edits_.*", getFinalizedEditsFileName(1, 2), getInProgressEditsFileName(3));	assertGlobEquals(cd1, "edits_.*", getFinalizedEditsFileName(1, 2), getInProgressEditsFileName(3));	doSaveNamespace(nn);	
after second save image should be purged and image should exist in both 

doSaveNamespace(nn);	assertGlobEquals(cd0, "fsimage_\\d*", getImageFileName(0), getImageFileName(2));	assertGlobEquals(cd1, "fsimage_\\d*", getImageFileName(0), getImageFileName(2));	assertGlobEquals(cd0, "edits_.*", getFinalizedEditsFileName(1, 2), getInProgressEditsFileName(3));	assertGlobEquals(cd1, "edits_.*", getFinalizedEditsFileName(1, 2), getInProgressEditsFileName(3));	doSaveNamespace(nn);	assertGlobEquals(cd0, "fsimage_\\d*", getImageFileName(2), getImageFileName(4));	assertGlobEquals(cd1, "fsimage_\\d*", getImageFileName(2), getImageFileName(4));	assertGlobEquals(cd0, "edits_.*", getFinalizedEditsFileName(3, 4), getInProgressEditsFileName(5));	assertGlobEquals(cd1, "edits_.*", getFinalizedEditsFileName(3, 4), getInProgressEditsFileName(5));	
failing first storage dir by chmodding it 

assertGlobEquals(cd1, "fsimage_\\d*", getImageFileName(0), getImageFileName(2));	assertGlobEquals(cd0, "edits_.*", getFinalizedEditsFileName(1, 2), getInProgressEditsFileName(3));	assertGlobEquals(cd1, "edits_.*", getFinalizedEditsFileName(1, 2), getInProgressEditsFileName(3));	doSaveNamespace(nn);	assertGlobEquals(cd0, "fsimage_\\d*", getImageFileName(2), getImageFileName(4));	assertGlobEquals(cd1, "fsimage_\\d*", getImageFileName(2), getImageFileName(4));	assertGlobEquals(cd0, "edits_.*", getFinalizedEditsFileName(3, 4), getInProgressEditsFileName(5));	assertGlobEquals(cd1, "edits_.*", getFinalizedEditsFileName(3, 4), getInProgressEditsFileName(5));	assertEquals(0, FileUtil.chmod(cd0.getAbsolutePath(), "000"));	doSaveNamespace(nn);	
restoring accessibility of first storage dir 

assertGlobEquals(cd0, "edits_.*", getFinalizedEditsFileName(1, 2), getInProgressEditsFileName(3));	assertGlobEquals(cd1, "edits_.*", getFinalizedEditsFileName(1, 2), getInProgressEditsFileName(3));	doSaveNamespace(nn);	assertGlobEquals(cd0, "fsimage_\\d*", getImageFileName(2), getImageFileName(4));	assertGlobEquals(cd1, "fsimage_\\d*", getImageFileName(2), getImageFileName(4));	assertGlobEquals(cd0, "edits_.*", getFinalizedEditsFileName(3, 4), getInProgressEditsFileName(5));	assertGlobEquals(cd1, "edits_.*", getFinalizedEditsFileName(3, 4), getInProgressEditsFileName(5));	assertEquals(0, FileUtil.chmod(cd0.getAbsolutePath(), "000"));	doSaveNamespace(nn);	assertEquals(0, FileUtil.chmod(cd0.getAbsolutePath(), "755"));	
nothing should have been purged in first storage dir 

doSaveNamespace(nn);	assertGlobEquals(cd0, "fsimage_\\d*", getImageFileName(2), getImageFileName(4));	assertGlobEquals(cd1, "fsimage_\\d*", getImageFileName(2), getImageFileName(4));	assertGlobEquals(cd0, "edits_.*", getFinalizedEditsFileName(3, 4), getInProgressEditsFileName(5));	assertGlobEquals(cd1, "edits_.*", getFinalizedEditsFileName(3, 4), getInProgressEditsFileName(5));	assertEquals(0, FileUtil.chmod(cd0.getAbsolutePath(), "000"));	doSaveNamespace(nn);	assertEquals(0, FileUtil.chmod(cd0.getAbsolutePath(), "755"));	assertGlobEquals(cd0, "fsimage_\\d*", getImageFileName(2), getImageFileName(4));	assertGlobEquals(cd0, "edits_.*", getFinalizedEditsFileName(3, 4), getInProgressEditsFileName(5));	
fsimage should be purged in second storage dir 

assertGlobEquals(cd1, "fsimage_\\d*", getImageFileName(2), getImageFileName(4));	assertGlobEquals(cd0, "edits_.*", getFinalizedEditsFileName(3, 4), getInProgressEditsFileName(5));	assertGlobEquals(cd1, "edits_.*", getFinalizedEditsFileName(3, 4), getInProgressEditsFileName(5));	assertEquals(0, FileUtil.chmod(cd0.getAbsolutePath(), "000"));	doSaveNamespace(nn);	assertEquals(0, FileUtil.chmod(cd0.getAbsolutePath(), "755"));	assertGlobEquals(cd0, "fsimage_\\d*", getImageFileName(2), getImageFileName(4));	assertGlobEquals(cd0, "edits_.*", getFinalizedEditsFileName(3, 4), getInProgressEditsFileName(5));	assertGlobEquals(cd1, "fsimage_\\d*", getImageFileName(4), getImageFileName(6));	assertGlobEquals(cd1, "edits_.*", getFinalizedEditsFileName(5, 6), getInProgressEditsFileName(7));	
on next save we should purge logs from the failed dir but not images since the image directory is in failed state 

assertGlobEquals(cd0, "edits_.*", getFinalizedEditsFileName(3, 4), getInProgressEditsFileName(5));	assertGlobEquals(cd1, "fsimage_\\d*", getImageFileName(4), getImageFileName(6));	assertGlobEquals(cd1, "edits_.*", getFinalizedEditsFileName(5, 6), getInProgressEditsFileName(7));	doSaveNamespace(nn);	assertGlobEquals(cd1, "fsimage_\\d*", getImageFileName(6), getImageFileName(8));	assertGlobEquals(cd1, "edits_.*", getFinalizedEditsFileName(7, 8), getInProgressEditsFileName(9));	assertGlobEquals(cd0, "fsimage_\\d*", getImageFileName(2), getImageFileName(4));	assertGlobEquals(cd0, "edits_.*", getInProgressEditsFileName(9));	} finally {	FileUtil.chmod(cd0.getAbsolutePath(), "755");	
shutting down 

private static void doSaveNamespace(NameNode nn) throws IOException {	
saving namespace 

========================= hadoop sample_7419 =========================

public void uncaughtException(Thread t, Throwable e) {	if(ShutdownHookManager.get().isShutdownInProgress()) {	
thread threw an throwable but we are shutting down so ignoring this 

public void uncaughtException(Thread t, Throwable e) {	if(ShutdownHookManager.get().isShutdownInProgress()) {	} else if(e instanceof Error) {	try {	
thread threw an error shutting down now 

if(e instanceof OutOfMemoryError) {	try {	System.err.println("Halting due to Out Of Memory Error...");	} catch (Throwable err) {	}	ExitUtil.halt(-1);	} else {	ExitUtil.terminate(-1);	}	} else {	
thread threw an exception 

========================= hadoop sample_2532 =========================

for (Class<?> filterInitializer : initializersClasses) {	if (filterInitializer.getName() .equals(AuthenticationFilterInitializer.class.getName())) {	if (!hasRMAuthFilterInitializer) {	target.add(RMAuthenticationFilterInitializer.class.getName());	}	continue;	}	target.add(filterInitializer.getName());	}	actualInitializers = StringUtils.join(",", target);	
using rm authentication filter kerberos delegation token for rm webapp authentication 

========================= hadoop sample_783 =========================

while (!pathstack.empty()) {	String p = pathstack.pop();	try {	byte[] lastReturnedName = HdfsFileStatus.EMPTY_NAME;	DirectoryListing thisListing;	do {	assert lastReturnedName != null;	thisListing = nn.getListing(p, lastReturnedName, false);	if (thisListing == null) {	if (lastReturnedName.length == 0) {	
listpathsservlet path does not exist 

} catch (IOException re) {	writeXml(re, p, doc);	}	}	return null;	}	});	} catch(IOException ioe) {	writeXml(ioe, path, doc);	} catch (InterruptedException e) {	
listpathservlet encountered interruptedexception 

========================= hadoop sample_8095 =========================

bytesSync = fileSize.get();	}	src_writer.append(fileSize, filePath);	}	if (src_writer != null) {	src_writer.close();	}	fs.deleteOnExit(distCacheFilesList);	conf.setInt(GenerateDistCacheData.GRIDMIX_DISTCACHE_FILE_COUNT, fileCount);	conf.setLong(GenerateDistCacheData.GRIDMIX_DISTCACHE_BYTE_COUNT, byteCount);	
number of hdfs based distributed cache files to be generated is total size of hdfs based distributed cache files to be generated is 

}	src_writer.append(fileSize, filePath);	}	if (src_writer != null) {	src_writer.close();	}	fs.deleteOnExit(distCacheFilesList);	conf.setInt(GenerateDistCacheData.GRIDMIX_DISTCACHE_FILE_COUNT, fileCount);	conf.setLong(GenerateDistCacheData.GRIDMIX_DISTCACHE_BYTE_COUNT, byteCount);	if (!shouldGenerateDistCacheData() && fileCount > 0) {	
missing distributed cache files under the directory that are needed for gridmix to emulate distributed cache load either use generate option to generate distributed cache data along with input data or disable distributed cache emulation by configuring to false 

========================= hadoop sample_6121 =========================

public void testFilterFileSystem() throws Exception {	int errors = 0;	for (Method m : FileSystem.class.getDeclaredMethods()) {	if (Modifier.isStatic(m.getModifiers()) || Modifier.isPrivate(m.getModifiers()) || Modifier.isFinal(m.getModifiers())) {	continue;	}	try {	MustNotImplement.class.getMethod(m.getName(), m.getParameterTypes());	try {	FilterFileSystem.class.getDeclaredMethod(m.getName(), m.getParameterTypes());	
filterfilesystem must not implement 

MustNotImplement.class.getMethod(m.getName(), m.getParameterTypes());	try {	FilterFileSystem.class.getDeclaredMethod(m.getName(), m.getParameterTypes());	errors++;	} catch (NoSuchMethodException ex) {	}	} catch (NoSuchMethodException exc) {	try{	FilterFileSystem.class.getDeclaredMethod(m.getName(), m.getParameterTypes());	} catch(NoSuchMethodException exc2){	
filterfilesystem must implement 

========================= hadoop sample_3289 =========================

public void contextInitialized(ServletContextEvent sce) {	try {	String confDir = System.getProperty(KMSConfiguration.KMS_CONFIG_DIR);	if (confDir == null) {	throw new RuntimeException("System property '" + KMSConfiguration.KMS_CONFIG_DIR + "' not defined");	}	kmsConf = KMSConfiguration.getKMSConf();	initLogging(confDir);	UserGroupInformation.setConfiguration(kmsConf);	LOG.info("-------------------------------------------------------------");	
java runtime version java runtime version 

public void contextInitialized(ServletContextEvent sce) {	try {	String confDir = System.getProperty(KMSConfiguration.KMS_CONFIG_DIR);	if (confDir == null) {	throw new RuntimeException("System property '" + KMSConfiguration.KMS_CONFIG_DIR + "' not defined");	}	kmsConf = KMSConfiguration.getKMSConf();	initLogging(confDir);	UserGroupInformation.setConfiguration(kmsConf);	LOG.info("-------------------------------------------------------------");	
user user name 

public void contextInitialized(ServletContextEvent sce) {	try {	String confDir = System.getProperty(KMSConfiguration.KMS_CONFIG_DIR);	if (confDir == null) {	throw new RuntimeException("System property '" + KMSConfiguration.KMS_CONFIG_DIR + "' not defined");	}	kmsConf = KMSConfiguration.getKMSConf();	initLogging(confDir);	UserGroupInformation.setConfiguration(kmsConf);	LOG.info("-------------------------------------------------------------");	
kms hadoop version 

String providerString = kmsConf.get(KMSConfiguration.KEY_PROVIDER_URI);	if (providerString == null) {	throw new IllegalStateException("No KeyProvider has been defined");	}	KeyProvider keyProvider = KeyProviderFactory.get(new URI(providerString), kmsConf);	if (kmsConf.getBoolean(KMSConfiguration.KEY_CACHE_ENABLE, KMSConfiguration.KEY_CACHE_ENABLE_DEFAULT)) {	long keyTimeOutMillis = kmsConf.getLong(KMSConfiguration.KEY_CACHE_TIMEOUT_KEY, KMSConfiguration.KEY_CACHE_TIMEOUT_DEFAULT);	long currKeyTimeOutMillis = kmsConf.getLong(KMSConfiguration.CURR_KEY_CACHE_TIMEOUT_KEY, KMSConfiguration.CURR_KEY_CACHE_TIMEOUT_DEFAULT);	keyProvider = new CachingKeyProvider(keyProvider, keyTimeOutMillis, currKeyTimeOutMillis);	}	
initialized keyprovider 

if (kmsConf.getBoolean(KMSConfiguration.KEY_CACHE_ENABLE, KMSConfiguration.KEY_CACHE_ENABLE_DEFAULT)) {	long keyTimeOutMillis = kmsConf.getLong(KMSConfiguration.KEY_CACHE_TIMEOUT_KEY, KMSConfiguration.KEY_CACHE_TIMEOUT_DEFAULT);	long currKeyTimeOutMillis = kmsConf.getLong(KMSConfiguration.CURR_KEY_CACHE_TIMEOUT_KEY, KMSConfiguration.CURR_KEY_CACHE_TIMEOUT_DEFAULT);	keyProvider = new CachingKeyProvider(keyProvider, keyTimeOutMillis, currKeyTimeOutMillis);	}	keyProviderCryptoExtension = KeyProviderCryptoExtension. createKeyProviderCryptoExtension(keyProvider);	keyProviderCryptoExtension = new EagerKeyGeneratorKeyProviderCryptoExtension(kmsConf, keyProviderCryptoExtension);	if (kmsConf.getBoolean(KMSConfiguration.KEY_AUTHORIZATION_ENABLE, KMSConfiguration.KEY_AUTHORIZATION_ENABLE_DEFAULT)) {	keyProviderCryptoExtension = new KeyAuthorizationKeyProvider( keyProviderCryptoExtension, kmsAcls);	}	
initialized keyprovidercryptoextension 

long keyTimeOutMillis = kmsConf.getLong(KMSConfiguration.KEY_CACHE_TIMEOUT_KEY, KMSConfiguration.KEY_CACHE_TIMEOUT_DEFAULT);	long currKeyTimeOutMillis = kmsConf.getLong(KMSConfiguration.CURR_KEY_CACHE_TIMEOUT_KEY, KMSConfiguration.CURR_KEY_CACHE_TIMEOUT_DEFAULT);	keyProvider = new CachingKeyProvider(keyProvider, keyTimeOutMillis, currKeyTimeOutMillis);	}	keyProviderCryptoExtension = KeyProviderCryptoExtension. createKeyProviderCryptoExtension(keyProvider);	keyProviderCryptoExtension = new EagerKeyGeneratorKeyProviderCryptoExtension(kmsConf, keyProviderCryptoExtension);	if (kmsConf.getBoolean(KMSConfiguration.KEY_AUTHORIZATION_ENABLE, KMSConfiguration.KEY_AUTHORIZATION_ENABLE_DEFAULT)) {	keyProviderCryptoExtension = new KeyAuthorizationKeyProvider( keyProviderCryptoExtension, kmsAcls);	}	final int defaultBitlength = kmsConf .getInt(KeyProvider.DEFAULT_BITLENGTH_NAME, KeyProvider.DEFAULT_BITLENGTH);	
default key bitlength is 

long keyTimeOutMillis = kmsConf.getLong(KMSConfiguration.KEY_CACHE_TIMEOUT_KEY, KMSConfiguration.KEY_CACHE_TIMEOUT_DEFAULT);	long currKeyTimeOutMillis = kmsConf.getLong(KMSConfiguration.CURR_KEY_CACHE_TIMEOUT_KEY, KMSConfiguration.CURR_KEY_CACHE_TIMEOUT_DEFAULT);	keyProvider = new CachingKeyProvider(keyProvider, keyTimeOutMillis, currKeyTimeOutMillis);	}	keyProviderCryptoExtension = KeyProviderCryptoExtension. createKeyProviderCryptoExtension(keyProvider);	keyProviderCryptoExtension = new EagerKeyGeneratorKeyProviderCryptoExtension(kmsConf, keyProviderCryptoExtension);	if (kmsConf.getBoolean(KMSConfiguration.KEY_AUTHORIZATION_ENABLE, KMSConfiguration.KEY_AUTHORIZATION_ENABLE_DEFAULT)) {	keyProviderCryptoExtension = new KeyAuthorizationKeyProvider( keyProviderCryptoExtension, kmsAcls);	}	final int defaultBitlength = kmsConf .getInt(KeyProvider.DEFAULT_BITLENGTH_NAME, KeyProvider.DEFAULT_BITLENGTH);	
kms started 

public void contextDestroyed(ServletContextEvent sce) {	try {	keyProviderCryptoExtension.close();	} catch (IOException ioe) {	
error closing keyprovidercryptoextension 

} catch (IOException ioe) {	}	kmsAudit.shutdown();	kmsAcls.stopReloader();	pauseMonitor.stop();	JvmMetrics.shutdownSingleton();	DefaultMetricsSystem.shutdown();	jmxReporter.stop();	jmxReporter.close();	metricRegistry = null;	
kms stopped 

========================= hadoop sample_4409 =========================

public static synchronized void terminate(ExitException ee) throws ExitException {	int status = ee.getExitCode();	String msg = ee.getMessage();	if (status != 0) {	
exiting with status 

public static synchronized void terminate(ExitException ee) throws ExitException {	int status = ee.getExitCode();	String msg = ee.getMessage();	if (status != 0) {	
exiting with status 

public static synchronized void terminate(ExitException ee) throws ExitException {	int status = ee.getExitCode();	String msg = ee.getMessage();	if (status != 0) {	}	if (systemExitDisabled) {	
terminate called 

public static synchronized void halt(HaltException ee) throws HaltException {	int status = ee.getExitCode();	String msg = ee.getMessage();	try {	if (status != 0) {	
halt with status 

public static synchronized void halt(HaltException ee) throws HaltException {	int status = ee.getExitCode();	String msg = ee.getMessage();	try {	if (status != 0) {	
halt with status 

public static synchronized void halt(HaltException ee) throws HaltException {	int status = ee.getExitCode();	String msg = ee.getMessage();	try {	if (status != 0) {	}	} catch (Exception ignored) {	}	if (systemHaltDisabled) {	
halt called 

========================= hadoop sample_3636 =========================

List<InputSplit> splits = format.getSplits(job);	boolean exceptionThrown = false;	for (InputSplit split : splits) {	try {	TaskAttemptContext context = MapReduceTestUtil. createDummyMapTaskAttemptContext(job.getConfiguration());	RecordReader<LongWritable, BytesWritable> reader = format.createRecordReader(split, context);	MapContext<LongWritable, BytesWritable, LongWritable, BytesWritable> mcontext = new MapContextImpl<LongWritable, BytesWritable, LongWritable, BytesWritable>(job.getConfiguration(), context.getTaskAttemptID(), reader, null, null, MapReduceTestUtil.createDummyReporter(), split);	reader.initialize(split, mcontext);	} catch(IOException ioe) {	exceptionThrown = true;	
exception message 

List<InputSplit> splits = format.getSplits(job);	boolean exceptionThrown = false;	for (InputSplit split : splits) {	try {	TaskAttemptContext context = MapReduceTestUtil.createDummyMapTaskAttemptContext( job.getConfiguration());	RecordReader<LongWritable, BytesWritable> reader = format.createRecordReader(split, context);	MapContext<LongWritable, BytesWritable, LongWritable, BytesWritable> mcontext = new MapContextImpl<LongWritable, BytesWritable, LongWritable, BytesWritable>(job.getConfiguration(), context.getTaskAttemptID(), reader, null, null, MapReduceTestUtil.createDummyReporter(), split);	reader.initialize(split, mcontext);	} catch(IOException ioe) {	exceptionThrown = true;	
exception message 

List<InputSplit> splits = format.getSplits(job);	boolean exceptionThrown = false;	for (InputSplit split : splits) {	try {	TaskAttemptContext context = MapReduceTestUtil. createDummyMapTaskAttemptContext(job.getConfiguration());	RecordReader<LongWritable, BytesWritable> reader = format.createRecordReader(split, context);	MapContext<LongWritable, BytesWritable, LongWritable, BytesWritable> mcontext = new MapContextImpl<LongWritable, BytesWritable, LongWritable, BytesWritable>(job.getConfiguration(), context.getTaskAttemptID(), reader, null, null, MapReduceTestUtil.createDummyReporter(), split);	reader.initialize(split, mcontext);	} catch(IOException ioe) {	exceptionThrown = true;	
exception message 

if (i > 0) {	if (i == (MAX_TESTS-1)) {	numSplits = (int)(fileSize/Math.floor(recordLength/2));	} else {	if (MAX_TESTS % i == 0) {	numSplits = fileSize/(fileSize - random.nextInt(fileSize));	} else {	numSplits = Math.max(1, fileSize/random.nextInt(Integer.MAX_VALUE));	}	}	
number of splits set to 

List<InputSplit> splits = format.getSplits(job);	if (codec != null) {	assertEquals("compressed splits == 1", 1, splits.size());	}	boolean exceptionThrown = false;	for (InputSplit split : splits) {	try {	List<String> results = readSplit(format, split, job);	} catch(IOException ioe) {	exceptionThrown = true;	
exception message 

========================= hadoop sample_5648 =========================

protected synchronized void finishApplication(ApplicationId applicationId) {	if (applicationId == null) {	
rmappmanager received completed appid of null skipping 

protected void submitApplication( ApplicationSubmissionContext submissionContext, long submitTime, String user) throws YarnException {	ApplicationId applicationId = submissionContext.getApplicationId();	RMAppImpl application = createAndPopulateNewRMApp( submissionContext, submitTime, user, false, -1);	try {	if (UserGroupInformation.isSecurityEnabled()) {	this.rmContext.getDelegationTokenRenewer() .addApplicationAsync(applicationId, BuilderUtils.parseCredentials(submissionContext), submissionContext.getCancelTokensWhenComplete(), application.getUser(), BuilderUtils.parseTokensConf(submissionContext));	} else {	this.rmContext.getDispatcher().getEventHandler() .handle(new RMAppEvent(applicationId, RMAppEventType.START));	}	} catch (Exception e) {	
unable to parse credentials for 

if (!amReqs.get(0).equals(anyReq)) {	amReqs.remove(anyReq);	amReqs.add(0, anyReq);	}	for (ResourceRequest amReq : amReqs) {	SchedulerUtils.normalizeAndValidateRequest(amReq, scheduler.getMaximumResourceCapability(), submissionContext.getQueue(), scheduler, isRecovery, rmContext);	amReq.setCapability( scheduler.getNormalizedResource(amReq.getCapability()));	}	return amReqs;	} catch (InvalidResourceRequestException e) {	
rm app submission failed in validating am resource request for application 

public void recover(RMState state) throws Exception {	RMStateStore store = rmContext.getStateStore();	assert store != null;	Map<ApplicationId, ApplicationStateData> appStates = state.getApplicationState();	
recovering applications 

RMStateStore store = rmContext.getStateStore();	assert store != null;	Map<ApplicationId, ApplicationStateData> appStates = state.getApplicationState();	int count = 0;	try {	for (ApplicationStateData appState : appStates.values()) {	recoverApplication(appState, state);	count += 1;	}	} finally {	
successfully recovered out of applications 

public void handle(RMAppManagerEvent event) {	ApplicationId applicationId = event.getApplicationId();	
rmappmanager processing event for of type 

public void handle(RMAppManagerEvent event) {	ApplicationId applicationId = event.getApplicationId();	switch (event.getType()) {	case APP_COMPLETED : finishApplication(applicationId);	logApplicationSummary(applicationId);	checkAppNumCompletedLimit();	break;	case APP_MOVE : try {	moveApplicationAcrossQueue(applicationId, event.getTargetQueueForMove());	} catch (YarnException e) {	
move application has failed 

switch (event.getType()) {	case APP_COMPLETED : finishApplication(applicationId);	logApplicationSummary(applicationId);	checkAppNumCompletedLimit();	break;	case APP_MOVE : try {	moveApplicationAcrossQueue(applicationId, event.getTargetQueueForMove());	} catch (YarnException e) {	}	break;	
invalid eventtype ignoring 

app.getApplicationSubmissionContext().setQueue(queue);	ApplicationStateData appState = ApplicationStateData.newInstance( app.getSubmitTime(), app.getStartTime(), app.getApplicationSubmissionContext(), app.getUser(), app.getCallerContext());	appState.setApplicationTimeouts(app.getApplicationTimeouts());	rmContext.getStateStore().updateApplicationStateSynchronously(appState, false, future);	try {	Futures.get(future, YarnException.class);	} catch (YarnException ex) {	if (!toSuppressException) {	throw ex;	}	
statestore update failed for move application to queue with below exception 

========================= hadoop sample_1125 =========================

public void init(Configuration conf) {	allocateLeft = conf.getBoolean(FAVOR_EARLY_ALLOCATION, DEFAULT_GREEDY_FAVOR_EARLY_ALLOCATION);	if (allocateLeft) {	
initializing the greedyreservationagent to favor left allocations controlled by parameter 

public void init(Configuration conf) {	allocateLeft = conf.getBoolean(FAVOR_EARLY_ALLOCATION, DEFAULT_GREEDY_FAVOR_EARLY_ALLOCATION);	if (allocateLeft) {	} else {	
initializing the greedyreservationagent to favor right allocations controlled by parameter 

public boolean createReservation(ReservationId reservationId, String user, Plan plan, ReservationDefinition contract) throws PlanningException {	
placing the following reservationrequest 

public boolean createReservation(ReservationId reservationId, String user, Plan plan, ReservationDefinition contract) throws PlanningException {	try {	boolean res = planner.createReservation(reservationId, user, plan, contract);	if (res) {	
outcome success reservation id contract 

public boolean createReservation(ReservationId reservationId, String user, Plan plan, ReservationDefinition contract) throws PlanningException {	try {	boolean res = planner.createReservation(reservationId, user, plan, contract);	if (res) {	} else {	
outcome failure reservation id contract 

public boolean createReservation(ReservationId reservationId, String user, Plan plan, ReservationDefinition contract) throws PlanningException {	try {	boolean res = planner.createReservation(reservationId, user, plan, contract);	if (res) {	} else {	}	return res;	} catch (PlanningException e) {	
outcome failure reservation id contract 

public boolean updateReservation(ReservationId reservationId, String user, Plan plan, ReservationDefinition contract) throws PlanningException {	
updating the following reservationrequest 

public boolean deleteReservation(ReservationId reservationId, String user, Plan plan) throws PlanningException {	
removing the following reservationid 

========================= hadoop sample_1060 =========================

Router router = new Router();	try {	if (null != routerShutdownHook) {	ShutdownHookManager.get().removeShutdownHook(routerShutdownHook);	}	routerShutdownHook = new CompositeServiceShutdownHook(router);	ShutdownHookManager.get().addShutdownHook(routerShutdownHook, SHUTDOWN_HOOK_PRIORITY);	router.init(conf);	router.start();	} catch (Throwable t) {	
error starting router 

========================= hadoop sample_1963 =========================

public void listStatusReturnsAsExpected() throws IOException {	getMockServer().enqueue(new MockResponse().setResponseCode(200) .setBody(TestADLResponseData.getListFileStatusJSONResponse(10)));	long startTime = Time.monotonicNow();	FileStatus[] ls = getMockAdlFileSystem() .listStatus(new Path("/test1/test2"));	long endTime = Time.monotonicNow();	
time 

public void listStatusReturnsAsExpected() throws IOException {	getMockServer().enqueue(new MockResponse().setResponseCode(200) .setBody(TestADLResponseData.getListFileStatusJSONResponse(10)));	long startTime = Time.monotonicNow();	FileStatus[] ls = getMockAdlFileSystem() .listStatus(new Path("/test1/test2"));	long endTime = Time.monotonicNow();	Assert.assertEquals(10, ls.length);	getMockServer().enqueue(new MockResponse().setResponseCode(200) .setBody(TestADLResponseData.getListFileStatusJSONResponse(200)));	startTime = Time.monotonicNow();	ls = getMockAdlFileSystem().listStatus(new Path("/test1/test2"));	endTime = Time.monotonicNow();	
time 

Assert.assertEquals(10, ls.length);	getMockServer().enqueue(new MockResponse().setResponseCode(200) .setBody(TestADLResponseData.getListFileStatusJSONResponse(200)));	startTime = Time.monotonicNow();	ls = getMockAdlFileSystem().listStatus(new Path("/test1/test2"));	endTime = Time.monotonicNow();	Assert.assertEquals(200, ls.length);	getMockServer().enqueue(new MockResponse().setResponseCode(200) .setBody(TestADLResponseData.getListFileStatusJSONResponse(2048)));	startTime = Time.monotonicNow();	ls = getMockAdlFileSystem().listStatus(new Path("/test1/test2"));	endTime = Time.monotonicNow();	
time 

public void listStatusOnFailure() throws IOException {	getMockServer().enqueue(new MockResponse().setResponseCode(403).setBody( TestADLResponseData.getErrorIllegalArgumentExceptionJSONResponse()));	FileStatus[] ls = null;	long startTime = Time.monotonicNow();	try {	ls = getMockAdlFileSystem().listStatus(new Path("/test1/test2"));	} catch (IOException e) {	Assert.assertTrue(e.getMessage().contains("Invalid"));	}	long endTime = Time.monotonicNow();	
time 

for (int i = 0; i < 10; ++i) {	getMockServer().enqueue(new MockResponse().setResponseCode(500).setBody( TestADLResponseData.getErrorInternalServerExceptionJSONResponse()));	}	startTime = Time.monotonicNow();	try {	ls = getMockAdlFileSystem().listStatus(new Path("/test1/test2"));	} catch (IOException e) {	Assert.assertTrue(e.getMessage().contains("Internal Server Error"));	}	endTime = Time.monotonicNow();	
time 

public void listStatusAclBit() throws URISyntaxException, IOException {	getMockServer().enqueue(new MockResponse().setResponseCode(200) .setBody(TestADLResponseData.getListFileStatusJSONResponse(true)));	FileStatus[] ls = null;	long startTime = Time.monotonicNow();	ls = getMockAdlFileSystem() .listStatus(new Path("/test1/test2"));	long endTime = Time.monotonicNow();	
time 

long endTime = Time.monotonicNow();	for (int i = 0; i < ls.length; i++) {	Assert.assertTrue(ls[i].isDirectory());	Assert.assertEquals(true, ls[i].getPermission().getAclBit());	}	ls = null;	getMockServer().enqueue(new MockResponse().setResponseCode(200) .setBody(TestADLResponseData.getListFileStatusJSONResponse(false)));	startTime = Time.monotonicNow();	ls = getMockAdlFileSystem() .listStatus(new Path("/test1/test2"));	endTime = Time.monotonicNow();	
time 

========================= hadoop sample_6526 =========================

protected void serviceInit(Configuration conf) throws Exception {	int memoryMb = NodeManagerHardwareUtils.getContainerMemoryMB(conf);	float vMemToPMem = conf.getFloat( YarnConfiguration.NM_VMEM_PMEM_RATIO, YarnConfiguration.DEFAULT_NM_VMEM_PMEM_RATIO);	int virtualMemoryMb = (int)Math.ceil(memoryMb * vMemToPMem);	int virtualCores = NodeManagerHardwareUtils.getVCores(conf);	
nodemanager resources memory set to mb 

protected void serviceInit(Configuration conf) throws Exception {	int memoryMb = NodeManagerHardwareUtils.getContainerMemoryMB(conf);	float vMemToPMem = conf.getFloat( YarnConfiguration.NM_VMEM_PMEM_RATIO, YarnConfiguration.DEFAULT_NM_VMEM_PMEM_RATIO);	int virtualMemoryMb = (int)Math.ceil(memoryMb * vMemToPMem);	int virtualCores = NodeManagerHardwareUtils.getVCores(conf);	
nodemanager resources vcores set to 

protected void serviceStart() throws Exception {	this.nodeId = this.context.getNodeId();	
node id assigned is 

private void unRegisterNM() {	RecordFactory recordFactory = RecordFactoryPBImpl.get();	UnRegisterNodeManagerRequest request = recordFactory .newRecordInstance(UnRegisterNodeManagerRequest.class);	request.setNodeId(this.nodeId);	try {	resourceTracker.unRegisterNodeManager(request);	
successfully unregistered the node with resourcemanager 

private void unRegisterNM() {	RecordFactory recordFactory = RecordFactoryPBImpl.get();	UnRegisterNodeManagerRequest request = recordFactory .newRecordInstance(UnRegisterNodeManagerRequest.class);	request.setNodeId(this.nodeId);	try {	resourceTracker.unRegisterNodeManager(request);	} catch (Exception e) {	
unregistration of the node failed 

protected void rebootNodeStatusUpdaterAndRegisterWithRM() {	synchronized(shutdownMonitor) {	if(this.isStopped) {	
currently being shutdown aborting reboot 

return;	}	this.isStopped = true;	sendOutofBandHeartBeat();	try {	statusUpdater.join();	registerWithRM();	statusUpdater = new Thread(statusUpdaterRunnable, "Node Status Updater");	statusUpdater.start();	this.isStopped = false;	
nodestatusupdater thread is reregistered and restarted 

protected void registerWithRM() throws YarnException, IOException {	RegisterNodeManagerResponse regNMResponse;	Set<NodeLabel> nodeLabels = nodeLabelsHandler.getNodeLabelsForRegistration();	synchronized (this.context) {	List<NMContainerStatus> containerReports = getNMContainerStatuses();	RegisterNodeManagerRequest request = RegisterNodeManagerRequest.newInstance(nodeId, httpPort, totalResource, nodeManagerVersionId, containerReports, getRunningApplications(), nodeLabels, physicalResource);	if (containerReports != null) {	
registering with rm using containers 

protected NodeStatus getNodeStatus(int responseId) throws IOException {	NodeHealthStatus nodeHealthStatus = this.context.getNodeHealthStatus();	nodeHealthStatus.setHealthReport(healthChecker.getHealthReport());	nodeHealthStatus.setIsNodeHealthy(healthChecker.isHealthy());	nodeHealthStatus.setLastHealthReportTime(healthChecker .getLastHealthReportTime());	if (LOG.isDebugEnabled()) {	
node s health status 

protected List<ContainerStatus> getContainerStatuses() throws IOException {	List<ContainerStatus> containerStatuses = new ArrayList<ContainerStatus>();	for (Container container : this.context.getContainers().values()) {	ContainerId containerId = container.getContainerId();	ApplicationId applicationId = containerId.getApplicationAttemptId() .getApplicationId();	org.apache.hadoop.yarn.api.records.ContainerStatus containerStatus = container.cloneAndGetContainerStatus();	if (containerStatus.getState() == ContainerState.COMPLETE) {	if (isApplicationStopped(applicationId)) {	if (LOG.isDebugEnabled()) {	
is completing remove from nm context 

pendingCompletedContainers.put(containerId, containerStatus);	}	}	addCompletedContainer(containerId);	} else {	containerStatuses.add(containerStatus);	}	}	containerStatuses.addAll(pendingCompletedContainers.values());	if (LOG.isDebugEnabled()) {	
sending out container statuses 

if (!this.context.getApplications().containsKey(applicationId)) {	context.getContainers().remove(containerId);	continue;	}	NMContainerStatus status = container.getNMContainerStatus();	containerStatuses.add(status);	if (status.getContainerState() == ContainerState.COMPLETE) {	addCompletedContainer(containerId);	}	}	
sending out nm container statuses 

if (nmContainer == null) {	iter.remove();	removedNullContainers.add(containerId);	} else if (nmContainer.getContainerState().equals( org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerState.DONE)) {	context.getContainers().remove(containerId);	removedContainers.add(containerId);	iter.remove();	}	}	if (!removedContainers.isEmpty()) {	
removed completed containers from nm context 

long currentTime = System.currentTimeMillis();	Iterator<ContainerId> i = recentlyStoppedContainers.keySet().iterator();	while (i.hasNext()) {	ContainerId cid = i.next();	if (recentlyStoppedContainers.get(cid) < currentTime) {	if (!context.getContainers().containsKey(cid)) {	i.remove();	try {	context.getNMStateStore().removeContainer(cid);	} catch (IOException e) {	
unable to remove container in store 

Credentials credentials = new Credentials();	DataInputByteBuffer buf = new DataInputByteBuffer();	ByteBuffer buffer = entry.getValue();	buffer.rewind();	buf.reset(buffer);	credentials.readTokenStorageStream(buf);	map.put(entry.getKey(), credentials);	}	if (LOG.isDebugEnabled()) {	for (Map.Entry<ApplicationId, Credentials> entry : map.entrySet()) {	
retrieved credentials form rm for 

}	ContainerQueuingLimit queuingLimit = response.getContainerQueuingLimit();	if (queuingLimit != null) {	context.getContainerManager().updateQueuingLimit(queuingLimit);	}	}	Resource newResource = response.getResource();	if (newResource != null) {	updateNMResource(newResource);	if (LOG.isDebugEnabled()) {	
node s resource is updated to 

}	}	if (YarnConfiguration.timelineServiceV2Enabled(context.getConf())) {	updateTimelineCollectorData(response);	}	} catch (ConnectException e) {	dispatcher.getEventHandler().handle( new NodeManagerEvent(NodeManagerEventType.SHUTDOWN));	failedToConnect = true;	throw new YarnRuntimeException(e);	} catch (Throwable e) {	
caught exception in status updater 

} catch (InterruptedException e) {	}	}	}	}	}	private void updateTimelineCollectorData( NodeHeartbeatResponse response) {	Map<ApplicationId, AppCollectorData> incomingCollectorsMap = response.getAppCollectors();	if (incomingCollectorsMap == null) {	if (LOG.isDebugEnabled()) {	
no collectors to update rm 

}	Map<ApplicationId, AppCollectorData> knownCollectors = context.getKnownCollectors();	for (Map.Entry<ApplicationId, AppCollectorData> entry : incomingCollectorsMap.entrySet()) {	ApplicationId appId = entry.getKey();	AppCollectorData collectorData = entry.getValue();	Application application = context.getApplications().get(appId);	if (application != null) {	AppCollectorData existingData = knownCollectors.get(appId);	if (AppCollectorData.happensBefore(existingData, collectorData)) {	if (LOG.isDebugEnabled()) {	
sync a new collector address for application from rm 

private boolean handleShutdownOrResyncCommand( NodeHeartbeatResponse response) {	if (response.getNodeAction() == NodeAction.SHUTDOWN) {	
recieved shutdown signal from resourcemanager as part of heartbeat hence shutting down 

private boolean handleShutdownOrResyncCommand( NodeHeartbeatResponse response) {	if (response.getNodeAction() == NodeAction.SHUTDOWN) {	
message from resourcemanager 

private boolean handleShutdownOrResyncCommand( NodeHeartbeatResponse response) {	if (response.getNodeAction() == NodeAction.SHUTDOWN) {	context.setDecommissioned(true);	dispatcher.getEventHandler().handle( new NodeManagerEvent(NodeManagerEventType.SHUTDOWN));	return true;	}	if (response.getNodeAction() == NodeAction.RESYNC) {	
node is out of sync with resourcemanager hence resyncing 

private boolean handleShutdownOrResyncCommand( NodeHeartbeatResponse response) {	if (response.getNodeAction() == NodeAction.SHUTDOWN) {	context.setDecommissioned(true);	dispatcher.getEventHandler().handle( new NodeManagerEvent(NodeManagerEventType.SHUTDOWN));	return true;	}	if (response.getNodeAction() == NodeAction.RESYNC) {	
message from resourcemanager 

public Set<NodeLabel> getNodeLabelsForHeartbeat() {	Set<NodeLabel> nodeLabelsForHeartbeat = nodeLabelsProvider.getNodeLabels();	nodeLabelsForHeartbeat = (nodeLabelsForHeartbeat == null) ? CommonNodeLabelsManager.EMPTY_NODELABEL_SET : nodeLabelsForHeartbeat;	boolean areNodeLabelsUpdated = nodeLabelsForHeartbeat.size() != previousNodeLabels.size() || !previousNodeLabels.containsAll(nodeLabelsForHeartbeat);	areLabelsSentToRM = false;	if (areNodeLabelsUpdated || isResyncIntervalElapsed()) {	previousNodeLabels = nodeLabelsForHeartbeat;	try {	if (LOG.isDebugEnabled()) {	
labels from provider 

while (iterator.hasNext()) {	try {	CommonNodeLabelsManager .checkAndThrowLabelName(iterator.next().getName());	} catch (IOException e) {	errorMsg.append(e.getMessage());	errorMsg.append(" , ");	hasInvalidLabel = true;	}	}	if (hasInvalidLabel) {	
invalid node label s from provider 

public void verifyRMHeartbeatResponseForNodeLabels( NodeHeartbeatResponse response) {	if (areLabelsSentToRM) {	if (response.getAreNodeLabelsAcceptedByRM() && LOG.isDebugEnabled()) {	
node labels were accepted by rm 

public void verifyRMHeartbeatResponseForNodeLabels( NodeHeartbeatResponse response) {	if (areLabelsSentToRM) {	if (response.getAreNodeLabelsAcceptedByRM() && LOG.isDebugEnabled()) {	} else {	
nm node labels were not accepted by rm and message from rm 

========================= hadoop sample_1719 =========================

protected void renameToFailure(Path src, Path dst) throws IOException {	try {	getStore().rename(src, dst);	fail("Expected failure renaming " + src + " to " + dst + "- but got success");	} catch (SwiftOperationFailedException e) {	
rename failed expected 

protected void renameToFailure(Path src, Path dst) throws IOException {	try {	getStore().rename(src, dst);	fail("Expected failure renaming " + src + " to " + dst + "- but got success");	} catch (SwiftOperationFailedException e) {	} catch (FileNotFoundException e) {	
rename failed expected 

========================= hadoop sample_6176 =========================

public void recover() throws YarnException, IOException {	Path newMirrorPath = new Path(fsWorkingPath, MIRROR_FILENAME + ".new");	Path oldMirrorPath = new Path(fsWorkingPath, MIRROR_FILENAME);	loadFromMirror(newMirrorPath, oldMirrorPath);	if (fs.exists(newMirrorPath)) {	try {	fs.delete(oldMirrorPath, false);	} catch (IOException e) {	if (LOG.isDebugEnabled()) {	
exception while removing old mirror 

loadFromMirror(newMirrorPath, oldMirrorPath);	if (fs.exists(newMirrorPath)) {	try {	fs.delete(oldMirrorPath, false);	} catch (IOException e) {	if (LOG.isDebugEnabled()) {	}	}	fs.rename(newMirrorPath, oldMirrorPath);	}	
node label store recover is completed 

========================= hadoop sample_2565 =========================

public void close() {	shouldRun = false;	try {	if (blockKeyUpdater != null) {	blockKeyUpdater.daemon.interrupt();	}	} catch(Exception e) {	
exception shutting down access key updater thread 

public void run() {	try {	while (shouldRun) {	try {	blockTokenSecretManager.addKeys(namenode.getBlockKeys());	} catch (IOException e) {	
failed to set keys 

public void run() {	try {	while (shouldRun) {	try {	blockTokenSecretManager.addKeys(namenode.getBlockKeys());	} catch (IOException e) {	}	Thread.sleep(sleepInterval);	}	} catch (InterruptedException e) {	
interruptedexception in block key updater thread 

try {	while (shouldRun) {	try {	blockTokenSecretManager.addKeys(namenode.getBlockKeys());	} catch (IOException e) {	}	Thread.sleep(sleepInterval);	}	} catch (InterruptedException e) {	} catch (Throwable e) {	
exception in block key updater thread 

public void close() throws IOException {	try {	daemon.interrupt();	} catch(Exception e) {	
exception shutting down key updater thread 

========================= hadoop sample_8187 =========================

public final void validate( final Map<RecurrenceId, List<ResourceSkyline>> jobHistory, final int timeInterval) throws InvalidInputException {	if ((jobHistory == null) || (jobHistory.size() == 0)) {	
job resource skyline history is invalid please try again with valid resource skyline history 

public final void validate( final Map<RecurrenceId, List<ResourceSkyline>> jobHistory, final int timeInterval) throws InvalidInputException {	if ((jobHistory == null) || (jobHistory.size() == 0)) {	throw new InvalidInputException("Job ResourceSkyline history", "invalid");	}	if (timeInterval <= 0) {	
solver timeinterval is invalid please specify a positive value 

public final List<ResourceSkyline> aggregateSkylines( final Map<RecurrenceId, List<ResourceSkyline>> jobHistory, final int minJobRuns) throws InvalidInputException {	List<ResourceSkyline> resourceSkylines = new ArrayList<ResourceSkyline>();	for (Map.Entry<RecurrenceId, List<ResourceSkyline>> entry : jobHistory .entrySet()) {	ResourceSkyline skylineAgg = null;	skylineAgg = mergeSkyline(entry.getValue());	resourceSkylines.add(skylineAgg);	}	int numJobs = resourceSkylines.size();	if (numJobs < minJobRuns) {	
solver requires job resource skyline history for at least runs but it only receives history info for runs 

========================= hadoop sample_6463 =========================

public HttpURLConnection openConnection(URL url, Token token, String doAs) throws IOException, AuthenticationException {	Preconditions.checkNotNull(url, "url");	Preconditions.checkNotNull(token, "token");	Map<String, String> extraParams = new HashMap<String, String>();	org.apache.hadoop.security.token.Token<? extends TokenIdentifier> dToken = null;	
connecting to url with token as 

public HttpURLConnection openConnection(URL url, Token token, String doAs) throws IOException, AuthenticationException {	Preconditions.checkNotNull(url, "url");	Preconditions.checkNotNull(token, "token");	Map<String, String> extraParams = new HashMap<String, String>();	org.apache.hadoop.security.token.Token<? extends TokenIdentifier> dToken = null;	if (!token.isSet()) {	Credentials creds = UserGroupInformation.getCurrentUser(). getCredentials();	if (LOG.isDebugEnabled()) {	
token not set looking for delegation token creds 

Map<String, String> extraParams = new HashMap<String, String>();	org.apache.hadoop.security.token.Token<? extends TokenIdentifier> dToken = null;	if (!token.isSet()) {	Credentials creds = UserGroupInformation.getCurrentUser(). getCredentials();	if (LOG.isDebugEnabled()) {	}	if (!creds.getAllTokens().isEmpty()) {	InetSocketAddress serviceAddr = new InetSocketAddress(url.getHost(), url.getPort());	Text service = SecurityUtil.buildTokenService(serviceAddr);	dToken = creds.getToken(service);	
using delegation token from service 

========================= hadoop sample_3725 =========================

public DistributedSchedulingAllocateResponse allocateForDistributedScheduling( DistributedSchedulingAllocateRequest request) throws YarnException, IOException {	OpportunisticContainerAllocator.PartitionedResourceRequests partitionedAsks = containerAllocator .partitionAskList(request.getAllocateRequest().getAskList());	List<Container> allocatedContainers = containerAllocator.allocateContainers( request.getAllocateRequest().getResourceBlacklistRequest(), partitionedAsks.getOpportunistic(), applicationAttemptId, oppContainerContext, rmIdentifier, appSubmitter);	request.setAllocatedContainers(allocatedContainers);	request.getAllocateRequest().setAskList(partitionedAsks.getGuaranteed());	if (LOG.isDebugEnabled()) {	
forwarding allocate request to the distributed scheduler service on yarn rm 

========================= hadoop sample_1718 =========================

public void init(String contextName, ContextFactory factory) {	super.init(contextName, factory);	int nKids;	try {	String sKids = getAttribute(ARITY_LABEL);	nKids = Integer.parseInt(sKids);	} catch (Exception e) {	
unable to initialize composite metric could not init arity 

protected void emitRecord(String contextName, String recordName, OutputRecord outRec) throws IOException {	for (MetricsContext ctxt : subctxt) {	try {	((AbstractMetricsContext)ctxt).emitRecord( contextName, recordName, outRec);	if (contextName == null || recordName == null || outRec == null) {	throw new IOException(contextName + ":" + recordName + ":" + outRec);	}	} catch (IOException e) {	
emitrecord failed 

protected void flush() throws IOException {	for (MetricsContext ctxt : subctxt) {	try {	((AbstractMetricsContext)ctxt).flush();	} catch (IOException e) {	
flush failed 

public void startMonitoring() throws IOException {	for (MetricsContext ctxt : subctxt) {	try {	ctxt.startMonitoring();	} catch (IOException e) {	
startmonitoring failed 

========================= hadoop sample_3375 =========================

os.close();	if (fs.exists(mirrorPath)) {	fs.delete(oldMirrorPath, false);	fs.rename(mirrorPath, oldMirrorPath);	}	fs.rename(writingMirrorPath, mirrorPath);	fs.delete(writingMirrorPath, false);	fs.delete(oldMirrorPath, false);	editlogOs = fs.create(editLogPath, true);	editlogOs.close();	
finished write mirror at 

os.close();	if (fs.exists(mirrorPath)) {	fs.delete(oldMirrorPath, false);	fs.rename(mirrorPath, oldMirrorPath);	}	fs.rename(writingMirrorPath, mirrorPath);	fs.delete(writingMirrorPath, false);	fs.delete(oldMirrorPath, false);	editlogOs = fs.create(editLogPath, true);	editlogOs.close();	
finished create editlog file at 

========================= hadoop sample_2564 =========================

tracker = new PositionTrackingInputStream(new BufferedInputStream( new FileInputStream(new File(inputFile))));	in = new DataInputStream(tracker);	int imageVersionFile = findImageVersion(in);	fsip = ImageLoader.LoaderFactory.getLoader(imageVersionFile);	if(fsip == null) throw new IOException("No image processor to read version " + imageVersionFile + " is available.");	fsip.loadImage(in, processor, skipBlocks);	done = true;	} finally {	if (!done) {	if (tracker != null) {	
image loading failed at offset 

in = new DataInputStream(tracker);	int imageVersionFile = findImageVersion(in);	fsip = ImageLoader.LoaderFactory.getLoader(imageVersionFile);	if(fsip == null) throw new IOException("No image processor to read version " + imageVersionFile + " is available.");	fsip.loadImage(in, processor, skipBlocks);	done = true;	} finally {	if (!done) {	if (tracker != null) {	} else {	
failed to load image file 

========================= hadoop sample_7789 =========================

public void testRamDiskNotChosenByDefault() throws IOException {	getClusterBuilder().setStorageTypes(new StorageType[] {RAM_DISK, RAM_DISK}) .build();	final String METHOD_NAME = GenericTestUtils.getMethodName();	Path path = new Path("/" + METHOD_NAME + ".dat");	try {	makeTestFile(path, BLOCK_SIZE, false);	fail("Block placement to RAM_DISK should have failed without lazyPersist flag");	} catch (Throwable t) {	
got expected exception 

========================= hadoop sample_7273 =========================

public void close() {	keyManager.close();	IOUtils.closeStream(out);	if (fs != null) {	try {	fs.delete(idPath, true);	} catch(IOException ioe) {	
failed to delete 

========================= hadoop sample_8186 =========================

public void run() {	try {	doAppLogAggregation();	} catch (Exception e) {	
error occurred while aggregating the log for the application 

public void run() {	try {	doAppLogAggregation();	} catch (Exception e) {	doAppLogAggregationPostCleanUp();	} finally {	if (!this.appAggregationFinished.get() && !this.aborted.get()) {	
log aggregation did not complete for application 

========================= hadoop sample_1827 =========================

public RouterAdminServer(Configuration conf, Router router) throws IOException {	super(RouterAdminServer.class.getName());	this.conf = conf;	this.router = router;	int handlerCount = this.conf.getInt( DFSConfigKeys.DFS_ROUTER_ADMIN_HANDLER_COUNT_KEY, DFSConfigKeys.DFS_ROUTER_ADMIN_HANDLER_COUNT_DEFAULT);	RPC.setProtocolEngine(this.conf, RouterAdminProtocolPB.class, ProtobufRpcEngine.class);	RouterAdminProtocolServerSideTranslatorPB routerAdminProtocolTranslator = new RouterAdminProtocolServerSideTranslatorPB(this);	BlockingService clientNNPbService = RouterAdminProtocolService. newReflectiveBlockingService(routerAdminProtocolTranslator);	InetSocketAddress confRpcAddress = conf.getSocketAddr( DFSConfigKeys.DFS_ROUTER_ADMIN_BIND_HOST_KEY, DFSConfigKeys.DFS_ROUTER_ADMIN_ADDRESS_KEY, DFSConfigKeys.DFS_ROUTER_ADMIN_ADDRESS_DEFAULT, DFSConfigKeys.DFS_ROUTER_ADMIN_PORT_DEFAULT);	String bindHost = conf.get( DFSConfigKeys.DFS_ROUTER_ADMIN_BIND_HOST_KEY, confRpcAddress.getHostName());	
admin server binding to 

========================= hadoop sample_8305 =========================

public boolean canCommit(TaskAttemptID taskAttemptID) throws IOException {	
commit go no go request from 

public void commitPending(TaskAttemptID taskAttemptID, TaskStatus taskStatsu) throws IOException, InterruptedException {	
commit pending state update from 

public void done(TaskAttemptID taskAttemptID) throws IOException {	
done acknowledgment from 

public void fatalError(TaskAttemptID taskAttemptID, String msg) throws IOException {	
task exited 

public void fsError(TaskAttemptID taskAttemptID, String message) throws IOException {	
task failed due to fserror 

public MapTaskCompletionEventsUpdate getMapCompletionEvents( JobID jobIdentifier, int startIndex, int maxEvents, TaskAttemptID taskAttemptID) throws IOException {	
mapcompletionevents request from startindex maxevents 

public boolean ping(TaskAttemptID taskAttemptID) throws IOException {	if (LOG.isDebugEnabled()) {	
ping from 

public void reportDiagnosticInfo(TaskAttemptID taskAttemptID, String diagnosticInfo) throws IOException {	diagnosticInfo = StringInterner.weakIntern(diagnosticInfo);	
diagnostics report from 

public boolean statusUpdate(TaskAttemptID taskAttemptID, TaskStatus taskStatus) throws IOException, InterruptedException {	org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId yarnAttemptID = TypeConverter.toYarn(taskAttemptID);	taskHeartbeatHandler.progressing(yarnAttemptID);	TaskAttemptStatus taskAttemptStatus = new TaskAttemptStatus();	taskAttemptStatus.id = yarnAttemptID;	taskAttemptStatus.progress = taskStatus.getProgress();	
progress of taskattempt is 

public JvmTask getTask(JvmContext context) throws IOException {	JVMId jvmId = context.jvmId;	
jvm with id asked for a task 

public JvmTask getTask(JvmContext context) throws IOException {	JVMId jvmId = context.jvmId;	JvmTask jvmTask = null;	WrappedJvmID wJvmID = new WrappedJvmID(jvmId.getJobId(), jvmId.isMap, jvmId.getId());	if (!jvmIDToActiveAttemptMap.containsKey(wJvmID)) {	
jvm with id is invalid and will be killed 

public JvmTask getTask(JvmContext context) throws IOException {	JVMId jvmId = context.jvmId;	JvmTask jvmTask = null;	WrappedJvmID wJvmID = new WrappedJvmID(jvmId.getJobId(), jvmId.isMap, jvmId.getId());	if (!jvmIDToActiveAttemptMap.containsKey(wJvmID)) {	jvmTask = TASK_FOR_INVALID_JVM;	} else {	if (!launchedJVMs.contains(wJvmID)) {	jvmTask = null;	
jvm with id asking for task before am launch registered given null task 

JvmTask jvmTask = null;	WrappedJvmID wJvmID = new WrappedJvmID(jvmId.getJobId(), jvmId.isMap, jvmId.getId());	if (!jvmIDToActiveAttemptMap.containsKey(wJvmID)) {	jvmTask = TASK_FOR_INVALID_JVM;	} else {	if (!launchedJVMs.contains(wJvmID)) {	jvmTask = null;	} else {	org.apache.hadoop.mapred.Task task = jvmIDToActiveAttemptMap.remove(wJvmID);	launchedJVMs.remove(wJvmID);	
jvm with id given task 

========================= hadoop sample_5167 =========================

FileStatus status = fs.getFileStatus(dir);	if (fs.hasMetadataStore()) {	assertTrue("Ancestor is dir", status.isDirectory());	} else {	fail("getFileStatus should fail due to delayed visibility.");	}	} catch (FileNotFoundException e) {	if (fs.hasMetadataStore()) {	fail("S3Guard failed to list parent of inconsistent child.");	}	
file not found as expected 

========================= hadoop sample_5921 =========================

public boolean init(final Configuration config, final String id, final Collection<Class<? extends BaseRecord>> records, final StateStoreMetrics stateStoreMetrics) {	this.conf = config;	this.identifier = id;	this.metrics = stateStoreMetrics;	if (this.identifier == null) {	
the identifier for the state store connection is not set 

public boolean init(final Configuration config, final String id, final Collection<Class<? extends BaseRecord>> records, final StateStoreMetrics stateStoreMetrics) {	this.conf = config;	this.identifier = id;	this.metrics = stateStoreMetrics;	if (this.identifier == null) {	}	boolean success = initDriver();	if (!success) {	
cannot intialize driver for 

this.metrics = stateStoreMetrics;	if (this.identifier == null) {	}	boolean success = initDriver();	if (!success) {	return false;	}	for (Class<? extends BaseRecord> cls : records) {	String recordString = StateStoreUtils.getRecordName(cls);	if (!initRecordStorage(recordString, cls)) {	
cannot initialize record store for 

private String getHostname() {	String hostname = "Unknown";	try {	hostname = InetAddress.getLocalHost().getHostName();	} catch (Exception e) {	
cannot get local address 

========================= hadoop sample_8240 =========================

public synchronized InputStream getConfigurationInputStream( Configuration bootstrapConf, String name) throws IOException, YarnException {	if (name == null || name.isEmpty()) {	throw new YarnException( "Illegal argument! The parameter should not be null or empty");	}	Path filePath;	if (YarnConfiguration.RM_CONFIGURATION_FILES.contains(name)) {	filePath = new Path(this.configDir, name);	if (!fs.exists(filePath)) {	
not found 

}	Path filePath;	if (YarnConfiguration.RM_CONFIGURATION_FILES.contains(name)) {	filePath = new Path(this.configDir, name);	if (!fs.exists(filePath)) {	return null;	}	} else {	filePath = new Path(name);	if (!fs.exists(filePath)) {	
not found 

========================= hadoop sample_2533 =========================

private void printReceivedEntities() {	for (int i = 0; i < client.getNumOfTimelineEntitiesPublished(); i++) {	TimelineEntities publishedEntities = client.getPublishedEntities(i);	StringBuilder entitiesPerPublish = new StringBuilder();	for (TimelineEntity entity : publishedEntities.getEntities()) {	entitiesPerPublish.append(entity.getId());	entitiesPerPublish.append(",");	}	
entities published index 

========================= hadoop sample_2163 =========================

public void corruptBlocks(MiniDFSCluster cluster) throws IOException {	for (int corruptIdx : blocksToCorrupt) {	ExtendedBlock block = dfsClient.getNamenode().getBlockLocations(name, blockSize * corruptIdx, Long.MAX_VALUE).get(0).getBlock();	for (int i = 0; i < numDataNodes; i++) {	File blockFile = cluster.getBlockFile(i, block);	if(blockFile != null && blockFile.exists()) {	FileOutputStream blockFileStream = new FileOutputStream(blockFile, false);	blockFileStream.write("corrupt".getBytes());	blockFileStream.close();	
corrupted block file 

datanodeInfo = info;	}	}	if (!checkDecommissionInProgress.get() && datanodeInfo != null && datanodeInfo.isDecommissionInProgress()) {	checkDecommissionInProgress.set(true);	}	if (datanodeInfo != null && datanodeInfo.isDecommissioned()) {	return true;	}	} catch (Exception e) {	
unexpected exception 

try {	for (DatanodeInfo info : dfs.getDataNodeStats()) {	if (dn1Name.equals(info.getXferAddr())) {	dnInfo = info;	}	}	if (dnInfo != null && dnInfo.isInMaintenance()) {	return true;	}	} catch (Exception e) {	
unexpected exception 

if (dnName.equals(info.getXferAddr())) {	datanodeInfo = info;	}	}	if (datanodeInfo != null && datanodeInfo.isEnteringMaintenance()) {	String fsckOut = runFsck(conf, 5, false, "/", "-maintenance", "-blockId", bIds[0]);	assertTrue(fsckOut.contains( NamenodeFsck.ENTERING_MAINTENANCE_STATUS));	return true;	}	} catch (Exception e) {	
unexpected exception 

DatanodeInfo datanodeInfo = null;	for (DatanodeInfo info : dfs.getDataNodeStats()) {	if (dnName.equals(info.getXferAddr())) {	datanodeInfo = info;	}	}	if (datanodeInfo != null && datanodeInfo.isInMaintenance()) {	return true;	}	} catch (Exception e) {	
unexpected exception 

if (dnName.equals(info.getXferAddr())) {	datanodeInfo = info;	}	}	if (datanodeInfo != null && datanodeInfo.isEnteringMaintenance()) {	String fsckOut = runFsck(conf, 0, true, testFile, "-maintenance");	assertTrue(fsckOut.contains(NamenodeFsck.HEALTHY_STATUS));	return true;	}	} catch (Exception e) {	
unexpected exception 

try {	for (DatanodeInfo info : dfs.getDataNodeStats()) {	if (dnName.equals(info.getXferAddr())) {	datanodeInfo = info;	}	}	if (datanodeInfo != null && datanodeInfo.isInMaintenance()) {	return true;	}	} catch (Exception e) {	
unexpected exception 

conf.setLong(DFSConfigKeys.DFS_BLOCKREPORT_INTERVAL_MSEC_KEY, 1000L);	conf.setInt(DFSConfigKeys.DFS_DATANODE_DIRECTORYSCAN_INTERVAL_KEY, 1);	conf.setInt(DFSConfigKeys.DFS_REPLICATION_KEY, replication);	cluster = new MiniDFSCluster.Builder(conf).build();	DistributedFileSystem dfs = cluster.getFileSystem();	cluster.waitActive();	final String srcDir = "/srcdat";	final DFSTestUtil util = new DFSTestUtil.Builder().setName("TestFsck") .setMinSize(dfsBlockSize * 2).setMaxSize(dfsBlockSize * 3) .setNumFiles(1).build();	util.createFiles(dfs, srcDir, (short) replication);	final String[] fileNames = util.getFileNames(srcDir);	
created files 

}	}	if (numCorrupt == null) {	Assert.fail("Cannot find corrupt blocks count in fsck output.");	}	if (Integer.parseInt(numCorrupt) == ctf.getTotalMissingBlocks()) {	assertTrue(str.contains(NamenodeFsck.CORRUPT_STATUS));	return true;	}	} catch (Exception e) {	
exception caught 

assertTrue(str.contains(NamenodeFsck.CORRUPT_STATUS));	return true;	}	} catch (Exception e) {	Assert.fail("Caught unexpected exception.");	}	return false;	}	}, 1000, 60000);	runFsck(conf, 1, true, "/", "-files", "-blocks", "-racks");	
moving blocks to lost found 

return false;	}	}, 1000, 60000);	runFsck(conf, 1, true, "/", "-files", "-blocks", "-racks");	runFsck(conf, 1, false, "/", "-move");	final List<LocatedFileStatus> retVal = new ArrayList<>();	final RemoteIterator<LocatedFileStatus> iter = dfs.listFiles(new Path("/lost+found"), true);	while (iter.hasNext()) {	retVal.add(iter.next());	}	
items in lost found 

========================= hadoop sample_7361 =========================

job.setMapperClass(RandomTextDataMapper.class);	job.setNumReduceTasks(0);	job.setMapOutputKeyClass(Text.class);	job.setMapOutputValueClass(Text.class);	job.setInputFormatClass(GenDataFormat.class);	job.setJarByClass(GenerateData.class);	FileOutputFormat.setCompressOutput(job, true);	try {	FileInputFormat.addInputPath(job, new Path("ignored"));	} catch (IOException e) {	
error while adding input path 

static void setupDataGeneratorConfig(Configuration conf) {	boolean compress = isCompressionEmulationEnabled(conf);	if (compress) {	float ratio = getMapInputCompressionEmulationRatio(conf);	
gridmix is configured to generate compressed input data with a compression ratio of 

FileStatus[] outFileStatuses = fs.listStatus(inputDir, new Utils.OutputFileUtils.OutputFilesFilter());	for (FileStatus status : outFileStatuses) {	if (compressionCodecs != null) {	CompressionCodec codec = compressionCodecs.getCodec(status.getPath());	if (codec != null) {	++numCompressedFiles;	compressedDataSize += status.getLen();	}	}	}	
gridmix is configured to use compressed input data 

FileStatus[] outFileStatuses = fs.listStatus(inputDir, new Utils.OutputFileUtils.OutputFilesFilter());	for (FileStatus status : outFileStatuses) {	if (compressionCodecs != null) {	CompressionCodec codec = compressionCodecs.getCodec(status.getPath());	if (codec != null) {	++numCompressedFiles;	compressedDataSize += status.getLen();	}	}	}	
total size of compressed input data 

FileStatus[] outFileStatuses = fs.listStatus(inputDir, new Utils.OutputFileUtils.OutputFilesFilter());	for (FileStatus status : outFileStatuses) {	if (compressionCodecs != null) {	CompressionCodec codec = compressionCodecs.getCodec(status.getPath());	if (codec != null) {	++numCompressedFiles;	compressedDataSize += status.getLen();	}	}	}	
total number of compressed input data files 

++numCompressedFiles;	compressedDataSize += status.getLen();	}	}	}	if (numCompressedFiles == 0) {	throw new RuntimeException("No compressed file found in the input" + " directory : " + inputDir.toString() + ". To enable compression" + " emulation, run Gridmix either with " + " an input directory containing compressed input file(s) or" + " use the -generate option to (re)generate it. If compression" + " emulation is not desired, disable it by setting '" + COMPRESSION_EMULATION_ENABLE + "' to 'false'.");	}	if (uncompressedDataSize > 0) {	double ratio = ((double)compressedDataSize) / uncompressedDataSize;	
input data compression ratio 

========================= hadoop sample_6120 =========================

public Object answer(InvocationOnMock invocation) throws Throwable {	
delayanswer firing firelatch 

public Object answer(InvocationOnMock invocation) throws Throwable {	fireCounter.getAndIncrement();	fireLatch.countDown();	try {	
delayanswer waiting on waitlatch 

public Object answer(InvocationOnMock invocation) throws Throwable {	fireCounter.getAndIncrement();	fireLatch.countDown();	try {	waitLatch.await();	
delayanswer delay complete 

public Object answer(InvocationOnMock invocation) throws Throwable {	try {	if (log != null) {	
call to on TRACE 

========================= hadoop sample_2860 =========================

traceScope = tracer.newScope(RpcClientUtil.methodToTraceString(method));	}	ObjectWritable value;	try {	value = (ObjectWritable) client.call(RPC.RpcKind.RPC_WRITABLE, new Invocation(method, args), remoteId, fallbackToSimpleAuth);	} finally {	if (traceScope != null) traceScope.close();	}	if (LOG.isDebugEnabled()) {	long callTime = Time.monotonicNow() - startTime;	
call 

exception = (IOException)target;	throw (IOException)target;	} else {	IOException ioe = new IOException(target.toString());	ioe.setStackTrace(target.getStackTrace());	exception = ioe;	throw ioe;	}	} catch (Throwable e) {	if (!(e instanceof IOException)) {	
unexpected throwable object 

========================= hadoop sample_4058 =========================

public Void call() throws Exception {	Configuration conf = new Configuration();	URI uri = createKMSUri(getKMSUrl());	KeyProvider kp = createProvider(uri, conf);	Assert.assertTrue(kp.getKeys().isEmpty());	Assert.assertEquals(0, kp.getKeysMetadata().length);	KeyProvider.Options options = new KeyProvider.Options(conf);	options.setCipher("AES/CTR/NoPadding");	options.setBitLength(128);	options.setDescription("l1");	
creating key with name 

public Void run() throws Exception {	KeyProvider kp = createProvider(uri, clientConf);	KeyProviderDelegationTokenExtension kpdte = KeyProviderDelegationTokenExtension. createKeyProviderDelegationTokenExtension(kp);	final Credentials credentials = new Credentials();	final Token<?>[] tokens = kpdte.addDelegationTokens("client1", credentials);	Assert.assertEquals(1, credentials.getAllTokens().size());	InetSocketAddress kmsAddr = new InetSocketAddress(getKMSUrl().getHost(), getKMSUrl().getPort());	Assert.assertEquals(KMSDelegationToken.TOKEN_KIND, credentials.getToken(SecurityUtil.buildTokenService(kmsAddr)). getKind());	for (Token<?> token : tokens) {	if (!(token.getKind().equals(KMSDelegationToken.TOKEN_KIND))) {	
skipping token 

KeyProviderDelegationTokenExtension kpdte = KeyProviderDelegationTokenExtension. createKeyProviderDelegationTokenExtension(kp);	final Credentials credentials = new Credentials();	final Token<?>[] tokens = kpdte.addDelegationTokens("client1", credentials);	Assert.assertEquals(1, credentials.getAllTokens().size());	InetSocketAddress kmsAddr = new InetSocketAddress(getKMSUrl().getHost(), getKMSUrl().getPort());	Assert.assertEquals(KMSDelegationToken.TOKEN_KIND, credentials.getToken(SecurityUtil.buildTokenService(kmsAddr)). getKind());	for (Token<?> token : tokens) {	if (!(token.getKind().equals(KMSDelegationToken.TOKEN_KIND))) {	continue;	}	
got dt for 

} else {	otherUgi = UserGroupInformation.createUserForTesting("client1", new String[] {"other group"});	UserGroupInformation.setLoginUser(otherUgi);	}	try {	otherUgi.doAs(new PrivilegedExceptionAction<Void>() {	public Void run() throws Exception {	boolean renewed = false;	for (Token<?> token : tokens) {	if (!(token.getKind() .equals(KMSDelegationToken.TOKEN_KIND))) {	
skipping token 

UserGroupInformation.setLoginUser(otherUgi);	}	try {	otherUgi.doAs(new PrivilegedExceptionAction<Void>() {	public Void run() throws Exception {	boolean renewed = false;	for (Token<?> token : tokens) {	if (!(token.getKind() .equals(KMSDelegationToken.TOKEN_KIND))) {	continue;	}	
got dt for 

}	try {	otherUgi.doAs(new PrivilegedExceptionAction<Void>() {	public Void run() throws Exception {	boolean renewed = false;	for (Token<?> token : tokens) {	if (!(token.getKind() .equals(KMSDelegationToken.TOKEN_KIND))) {	continue;	}	long tokenLife = token.renew(clientConf);	
renewed token of kind new lifetime 

otherUgi.doAs(new PrivilegedExceptionAction<Void>() {	public Void run() throws Exception {	boolean renewed = false;	for (Token<?> token : tokens) {	if (!(token.getKind() .equals(KMSDelegationToken.TOKEN_KIND))) {	continue;	}	long tokenLife = token.renew(clientConf);	Thread.sleep(100);	long newTokenLife = token.renew(clientConf);	
renewed token of kind new lifetime 

}	long tokenLife = token.renew(clientConf);	Thread.sleep(100);	long newTokenLife = token.renew(clientConf);	Assert.assertTrue(newTokenLife > tokenLife);	renewed = true;	}	Assert.assertTrue(renewed);	for (Token<?> token : tokens) {	if (!(token.getKind() .equals(KMSDelegationToken.TOKEN_KIND))) {	
skipping token 

Thread.sleep(100);	long newTokenLife = token.renew(clientConf);	Assert.assertTrue(newTokenLife > tokenLife);	renewed = true;	}	Assert.assertTrue(renewed);	for (Token<?> token : tokens) {	if (!(token.getKind() .equals(KMSDelegationToken.TOKEN_KIND))) {	continue;	}	
got dt for 

long newTokenLife = token.renew(clientConf);	Assert.assertTrue(newTokenLife > tokenLife);	renewed = true;	}	Assert.assertTrue(renewed);	for (Token<?> token : tokens) {	if (!(token.getKind() .equals(KMSDelegationToken.TOKEN_KIND))) {	continue;	}	token.cancel(clientConf);	
cancelled token of kind 

Assert.assertTrue(renewed);	for (Token<?> token : tokens) {	if (!(token.getKind() .equals(KMSDelegationToken.TOKEN_KIND))) {	continue;	}	token.cancel(clientConf);	try {	token.renew(clientConf);	Assert .fail("should not be able to renew a canceled token");	} catch (Exception e) {	
expected exception when renewing token 

final KeyProviderDelegationTokenExtension kpdte = KeyProviderDelegationTokenExtension. createKeyProviderDelegationTokenExtension(kp);	final InetSocketAddress kmsAddr = new InetSocketAddress(getKMSUrl().getHost(), getKMSUrl().getPort());	final Collection<Token<?>> job1Token = new HashSet<>();	doAs("client", new PrivilegedExceptionAction<Void>() {	public Void run() throws Exception {	final Credentials credentials = new Credentials();	kpdte.addDelegationTokens("client", credentials);	Assert.assertEquals(1, credentials.getAllTokens().size());	Assert.assertEquals(KMSDelegationToken.TOKEN_KIND, credentials. getToken(SecurityUtil.buildTokenService(kmsAddr)).getKind());	UserGroupInformation.getCurrentUser().addCredentials(credentials);	
added kms dt to credentials 

DelegationTokenIdentifier id = new DelegationTokenIdentifier(token.getKind());	id.readFields(dis);	dis.close();	final long maxTime = id.getMaxDate();	Thread.sleep(5100);	Assert.assertTrue("maxTime " + maxTime + " is not less than now.", maxTime > 0 && maxTime < Time.now());	try {	kp.getKeys();	Assert.fail("Operation should fail since dt is expired.");	} catch (Exception e) {	
expected error 

kpdte.addDelegationTokens("client", newCreds);	Assert.assertEquals(1, newCreds.getAllTokens().size());	Assert.assertEquals(KMSDelegationToken.TOKEN_KIND, newCreds.getToken(SecurityUtil.buildTokenService(kmsAddr)). getKind());	final Credentials oldCreds = new Credentials();	for (Token<?> token : job1Token) {	if (token.getKind().equals(KMSDelegationToken.TOKEN_KIND)) {	oldCreds .addToken(SecurityUtil.buildTokenService(kmsAddr), token);	}	}	UserGroupInformation.getCurrentUser().addCredentials(oldCreds);	
added old kms dt to credentials 

for (Token<?> token : job1Token) {	if (token.getKind().equals(KMSDelegationToken.TOKEN_KIND)) {	oldCreds .addToken(SecurityUtil.buildTokenService(kmsAddr), token);	}	}	UserGroupInformation.getCurrentUser().addCredentials(oldCreds);	try {	kp.getKeys();	Assert.fail("Operation should fail since dt is expired.");	} catch (Exception e) {	
expected error 

}	UserGroupInformation.getCurrentUser().addCredentials(oldCreds);	try {	kp.getKeys();	Assert.fail("Operation should fail since dt is expired.");	} catch (Exception e) {	}	Assert.assertEquals(1, newCreds.getAllTokens().size());	Assert.assertEquals(KMSDelegationToken.TOKEN_KIND, newCreds.getToken(SecurityUtil.buildTokenService(kmsAddr)). getKind());	UserGroupInformation.getCurrentUser().addCredentials(newCreds);	
credetials now are 

public void testKMSJMX() throws Exception {	Configuration conf = new Configuration();	final File confDir = getTestDir();	conf = createBaseKMSConf(confDir, conf);	final String processName = "testkmsjmx";	conf.set(KMSConfiguration.METRICS_PROCESS_NAME_KEY, processName);	writeConf(confDir, conf);	runServer(null, null, confDir, new KMSCallable<Void>() {	public Void call() throws Exception {	final URL jmxUrl = new URL( getKMSUrl() + "/jmx?user.name=whatever&qry=Hadoop:service=" + processName + ",name=JvmMetrics");	
requesting jmx from 

runServer(null, null, confDir, new KMSCallable<Void>() {	public Void call() throws Exception {	final URL jmxUrl = new URL( getKMSUrl() + "/jmx?user.name=whatever&qry=Hadoop:service=" + processName + ",name=JvmMetrics");	final StringBuilder sb = new StringBuilder();	final InputStream in = jmxUrl.openConnection().getInputStream();	final byte[] buffer = new byte[64 * 1024];	int len;	while ((len = in.read(buffer)) > 0) {	sb.append(new String(buffer, 0, len));	}	
jmx returned 

========================= hadoop sample_4408 =========================

}	AppPriorityACLGroup userPriorityACL = new AppPriorityACLGroup();	List<StringBuilder> userAndGroupName = new ArrayList<>();	for (String kvPair : aclSubGroup.trim().split(" +")) {	String[] splits = kvPair.split("=");	if (splits != null && splits.length > 1) {	parsePriorityACLType(userPriorityACL, splits, userAndGroupName);	}	}	if (userPriorityACL.getMaxPriority().getPriority() > clusterMaxPriority .getPriority()) {	
acl configuration for is greater that cluster max priority resetting acls to 

========================= hadoop sample_917 =========================

SubApplicationColumn.TYPE.store(rowKey, subApplicationTable, null, te.getType());	SubApplicationColumn.CREATED_TIME.store(rowKey, subApplicationTable, null, te.getCreatedTime());	SubApplicationColumn.FLOW_VERSION.store(rowKey, subApplicationTable, null, flowVersion);	storeInfo(rowKey, te.getInfo(), flowVersion, SubApplicationColumnPrefix.INFO, subApplicationTable);	storeMetrics(rowKey, te.getMetrics(), SubApplicationColumnPrefix.METRIC, subApplicationTable);	storeEvents(rowKey, te.getEvents(), SubApplicationColumnPrefix.EVENT, subApplicationTable);	storeConfig(rowKey, te.getConfigs(), SubApplicationColumnPrefix.CONFIG, subApplicationTable);	storeRelations(rowKey, te.getIsRelatedToEntities(), SubApplicationColumnPrefix.IS_RELATED_TO, subApplicationTable);	storeRelations(rowKey, te.getRelatesToEntities(), SubApplicationColumnPrefix.RELATES_TO, subApplicationTable);	break;	
invalid table name provided 

private <T> void storeEvents(byte[] rowKey, Set<TimelineEvent> events, ColumnPrefix<T> columnPrefix, TypedBufferedMutator<T> table) throws IOException {	if (events != null) {	for (TimelineEvent event : events) {	if (event != null) {	String eventId = event.getId();	if (eventId != null) {	long eventTimestamp = event.getTimestamp();	if (eventTimestamp == TimelineEvent.INVALID_TIMESTAMP) {	
timestamp is not set for event using the current timestamp 

protected void serviceStop() throws Exception {	if (entityTable != null) {	
closing the entity table 

protected void serviceStop() throws Exception {	if (entityTable != null) {	entityTable.close();	}	if (appToFlowTable != null) {	
closing the app flow table 

protected void serviceStop() throws Exception {	if (entityTable != null) {	entityTable.close();	}	if (appToFlowTable != null) {	appToFlowTable.close();	}	if (applicationTable != null) {	
closing the application table 

if (entityTable != null) {	entityTable.close();	}	if (appToFlowTable != null) {	appToFlowTable.close();	}	if (applicationTable != null) {	applicationTable.close();	}	if (flowRunTable != null) {	
closing the flow run table 

if (appToFlowTable != null) {	appToFlowTable.close();	}	if (applicationTable != null) {	applicationTable.close();	}	if (flowRunTable != null) {	flowRunTable.close();	}	if (flowActivityTable != null) {	
closing the flowactivitytable table 

if (flowRunTable != null) {	flowRunTable.close();	}	if (flowActivityTable != null) {	flowActivityTable.close();	}	if (subApplicationTable != null) {	subApplicationTable.close();	}	if (conn != null) {	
closing the hbase connection 

========================= hadoop sample_1173 =========================

return System.currentTimeMillis() + 3000;	}	MyToken token = (MyToken)t;	if(token.isCanceled()) {	throw new InvalidToken("token has been canceled");	}	lastRenewed = token;	counter ++;	LOG.info("Called MYDFS.renewdelegationtoken " + token + ";this dfs=" + this.hashCode() + ";c=" + counter);	if(tokenToRenewIn2Sec == token) {	
renew in seconds 

public void cancel(Token<?> t, Configuration conf) {	cancelled = true;	if (t instanceof MyToken) {	MyToken token = (MyToken) t;	
cancel token 

public void initialize(URI uri, Configuration conf) throws IOException {}	public MyToken getDelegationToken(String renewer) throws IOException {	MyToken result = createTokens(new Text(renewer));	
called mydfs getdelegationtoken 

========================= hadoop sample_444 =========================

public String getSlowDiskReportAsJsonString() {	ObjectMapper objectMapper = new ObjectMapper();	try {	if (slowDisksReport.isEmpty()) {	return null;	}	return objectMapper.writeValueAsString(slowDisksReport);	} catch (JsonProcessingException e) {	
failed to serialize statistics 

========================= hadoop sample_8352 =========================

public void testRetryAddBlockWhileInChooseTarget() throws Exception {	final String src = "/testRetryAddBlockWhileInChooseTarget";	final FSNamesystem ns = cluster.getNamesystem();	final NamenodeProtocols nn = cluster.getNameNodeRpc();	nn.create(src, FsPermission.getFileDefault(), "clientName", new EnumSetWritable<CreateFlag>(EnumSet.of(CreateFlag.CREATE)), true, (short)3, 1024, null);	
starting first addblock for 

ns.readLock();	FSDirWriteFileOp.ValidateAddBlockResult r;	FSPermissionChecker pc = Mockito.mock(FSPermissionChecker.class);	try {	r = FSDirWriteFileOp.validateAddBlock(ns, pc, src, HdfsConstants.GRANDFATHER_INODE_ID, "clientName", null, onRetryBlock);	} finally {	ns.readUnlock();;	}	DatanodeStorageInfo targets[] = FSDirWriteFileOp.chooseTargetForNewBlock( ns.getBlockManager(), src, null, null, null, r);	assertNotNull("Targets must be generated", targets);	
starting second addblock for 

public void testAddBlockRetryShouldReturnBlockWithLocations() throws Exception {	final String src = "/testAddBlockRetryShouldReturnBlockWithLocations";	NamenodeProtocols nameNodeRpc = cluster.getNameNodeRpc();	nameNodeRpc.create(src, FsPermission.getFileDefault(), "clientName", new EnumSetWritable<CreateFlag>(EnumSet.of(CreateFlag.CREATE)), true, (short) 3, 1024, null);	
starting first addblock for 

========================= hadoop sample_7418 =========================

public void done(TaskAttemptID taskid) throws IOException {	
task reporting done 

public void fsError(TaskAttemptID taskId, String message) throws IOException {	
task reporting file system error 

public void shuffleError(TaskAttemptID taskId, String message) throws IOException {	
task reporting shuffle error 

public void fatalError(TaskAttemptID taskId, String msg) throws IOException {	
task reporting fatal error 

public void reportDiagnosticInfo(TaskAttemptID taskid, String trace) throws IOException {	
task has problem 

public void reportNextRecordRange(TaskAttemptID taskid, SortedRanges.Range range) throws IOException {	
task reportednextrecordrange 

public void setProgress(float progress) {	super.setProgress(progress);	float mapTaskProgress = map.getProgress().getProgress();	
map task progress is 

========================= hadoop sample_5425 =========================

private synchronized void endBlock() throws IOException {	backupStream.close();	nextBlockOutputStream();	store.storeBlock(nextBlock, backupFile);	internalClose();	boolean b = backupFile.delete();	if (!b) {	
ignoring failed delete 

if (closed) {	return;	}	flush();	if (filePos == 0 || bytesWrittenToBlock != 0) {	endBlock();	}	backupStream.close();	boolean b = backupFile.delete();	if (!b) {	
ignoring failed delete 

========================= hadoop sample_5976 =========================

String scheme = path.toUri().getScheme();	clazz = FileSystem.getFileSystemClass(scheme, config);	} catch (IOException ioe) {	if (newProviderPath.length() > 0) {	newProviderPath.append(",");	}	newProviderPath.append(provider);	}	if (clazz != null) {	if (fileSystemClass.isAssignableFrom(clazz)) {	
filesystem based provider excluded from provider path due to recursive dependency 

if (clazz != null) {	if (fileSystemClass.isAssignableFrom(clazz)) {	} else {	if (newProviderPath.length() > 0) {	newProviderPath.append(",");	}	newProviderPath.append(provider);	}	}	} catch (URISyntaxException e) {	
credential provider uri is invalid 

========================= hadoop sample_3764 =========================

public void shutdownHookManager() {	ShutdownHookManager mgr = ShutdownHookManager.get();	Assert.assertNotNull(mgr);	Assert.assertEquals(0, mgr.getShutdownHooksInOrder().size());	Runnable hook1 = new Runnable() {	public void run() {	
shutdown complete 

public void shutdownHookManager() {	ShutdownHookManager mgr = ShutdownHookManager.get();	Assert.assertNotNull(mgr);	Assert.assertEquals(0, mgr.getShutdownHooksInOrder().size());	Runnable hook1 = new Runnable() {	public void run() {	}	};	Runnable hook2 = new Runnable() {	public void run() {	
shutdown complete 

}	};	Runnable hook2 = new Runnable() {	public void run() {	}	};	Runnable hook3 = new Runnable() {	public void run() {	try {	sleep(3000);	
shutdown complete 

};	Runnable hook2 = new Runnable() {	public void run() {	}	};	Runnable hook3 = new Runnable() {	public void run() {	try {	sleep(3000);	} catch (InterruptedException ex) {	
shutdown interrupted exception 

sleep(3000);	} catch (InterruptedException ex) {	Assert.fail("Hook 3 should not timeout.");	}	}	};	Runnable hook4 = new Runnable() {	public void run() {	try {	sleep(3500);	
shutdown complete 

Assert.fail("Hook 3 should not timeout.");	}	}	};	Runnable hook4 = new Runnable() {	public void run() {	try {	sleep(3500);	Assert.fail("Hook 4 should timeout");	} catch (InterruptedException ex) {	
shutdown interrupted exception 

Assert.assertEquals(hook2, mgr.getShutdownHooksInOrder().get(0).getHook());	Assert.assertEquals(hook1, mgr.getShutdownHooksInOrder().get(1).getHook());	mgr.addShutdownHook(hook3, 2, 4, TimeUnit.SECONDS);	Assert.assertTrue(mgr.hasShutdownHook(hook3));	Assert.assertEquals(hook3, mgr.getShutdownHooksInOrder().get(0).getHook());	Assert.assertEquals(4, mgr.getShutdownHooksInOrder().get(0).getTimeout());	mgr.addShutdownHook(hook4, 3, 2, TimeUnit.SECONDS);	Assert.assertTrue(mgr.hasShutdownHook(hook4));	Assert.assertEquals(hook4, mgr.getShutdownHooksInOrder().get(0).getHook());	Assert.assertEquals(2, mgr.getShutdownHooksInOrder().get(0).getTimeout());	
shutdown starts here 

========================= hadoop sample_2876 =========================

public void addJobStats(JobStats stats) {	int seq = GridmixJob.getJobSeqId(stats.getJob());	if (seq < 0) {	
not tracking job as seq id is less than zero 

public void add(Statistics.JobStats job) {	if (!statistics.isAlive()) {	return;	}	JobStats stat = submittedJobsMap.remove(GridmixJob.getJobSeqId(job.getJob()));	if (stat == null) {	
statistics missing entry for job 

}	JobStats stat = submittedJobsMap.remove(GridmixJob.getJobSeqId(job.getJob()));	if (stat == null) {	return;	}	subtractFromNumMapsSubmitted(stat.getNoOfMaps());	subtractFromNumReducesSubmitted(stat.getNoOfReds());	completedJobsInCurrentInterval++;	if (completedJobsInCurrentInterval >= maxJobCompletedInInterval) {	if (LOG.isDebugEnabled()) {	
reached maximum limit of jobs in a polling interval 

public void run() {	try {	startFlag.await();	if (Thread.currentThread().isInterrupted()) {	return;	}	} catch (InterruptedException ie) {	
statistics error while waiting for other threads to get ready 

}	} catch (InterruptedException ie) {	return;	}	while (!shutdown) {	lock.lock();	try {	jobCompleted.await(jtPollingInterval, TimeUnit.MILLISECONDS);	} catch (InterruptedException ie) {	if (!shutdown) {	
statistics interrupt while waiting for completion of a job 

}	return;	} finally {	lock.unlock();	}	if (clusterStatlisteners.size() > 0) {	try {	ClusterStatus clusterStatus = cluster.getClusterStatus();	updateAndNotifyClusterStatsListeners(clusterStatus);	} catch (IOException e) {	
statistics io exception while polling jt 

========================= hadoop sample_6136 =========================

try {	while ((elis = allStreams.poll()) != null) {	try {	elis.skipUntil(txId);	while (true) {	FSEditLogOp op = elis.readOp();	if (op == null) {	break;	}	if (abortOnGap && (op.getTransactionId() != txId)) {	
getnumberoftransactions detected gap at txid 

========================= hadoop sample_7377 =========================

public static ResourceCalculatorPlugin getResourceCalculatorPlugin( Class<? extends ResourceCalculatorPlugin> clazz, Configuration conf) {	if (clazz != null) {	return ReflectionUtils.newInstance(clazz, conf);	}	try {	return new ResourceCalculatorPlugin();	} catch (UnsupportedOperationException ue) {	
failed to instantiate default resource calculator 

public static ResourceCalculatorPlugin getResourceCalculatorPlugin( Class<? extends ResourceCalculatorPlugin> clazz, Configuration conf) {	if (clazz != null) {	return ReflectionUtils.newInstance(clazz, conf);	}	try {	return new ResourceCalculatorPlugin();	} catch (UnsupportedOperationException ue) {	} catch (Throwable t) {	
failed to instantiate default resource calculator 

========================= hadoop sample_2175 =========================

public static void setUp() throws Exception {	config = new Configuration();	if ( DFS_BASE_DIR.exists() && !FileUtil.fullyDelete(DFS_BASE_DIR) ) {	throw new IOException("Could not delete hdfs directory '" + DFS_BASE_DIR + "'");	}	
hdfsdir is 

public static void tearDown() throws Exception {	if (cluster!=null) {	cluster.shutdown();	
stopping mini cluster 

public void testAllowFormat() throws IOException {	
starting mini cluster 

public void testAllowFormat() throws IOException {	NameNode nn;	config.setBoolean(DFS_NAMENODE_SUPPORT_ALLOW_FORMAT_KEY, true);	cluster = new MiniDFSCluster.Builder(config).manageDataDfsDirs(false) .manageNameDfsDirs(false) .build();	cluster.waitActive();	assertNotNull(cluster);	nn = cluster.getNameNode();	assertNotNull(nn);	
mini cluster created ok 

public void testAllowFormat() throws IOException {	NameNode nn;	config.setBoolean(DFS_NAMENODE_SUPPORT_ALLOW_FORMAT_KEY, true);	cluster = new MiniDFSCluster.Builder(config).manageDataDfsDirs(false) .manageNameDfsDirs(false) .build();	cluster.waitActive();	assertNotNull(cluster);	nn = cluster.getNameNode();	assertNotNull(nn);	
verifying format will fail with allowformat false 

assertNotNull(cluster);	nn = cluster.getNameNode();	assertNotNull(nn);	config.setBoolean(DFS_NAMENODE_SUPPORT_ALLOW_FORMAT_KEY, false);	try {	cluster.shutdown();	NameNode.format(config);	fail("Format succeeded, when it should have failed");	} catch (IOException e) {	assertTrue("Exception was not about formatting Namenode", e.getMessage().startsWith("The option " + DFS_NAMENODE_SUPPORT_ALLOW_FORMAT_KEY));	
expected failure 

assertNotNull(cluster);	nn = cluster.getNameNode();	assertNotNull(nn);	config.setBoolean(DFS_NAMENODE_SUPPORT_ALLOW_FORMAT_KEY, false);	try {	cluster.shutdown();	NameNode.format(config);	fail("Format succeeded, when it should have failed");	} catch (IOException e) {	assertTrue("Exception was not about formatting Namenode", e.getMessage().startsWith("The option " + DFS_NAMENODE_SUPPORT_ALLOW_FORMAT_KEY));	
done verifying format will fail with allowformat false 

nn = cluster.getNameNode();	assertNotNull(nn);	config.setBoolean(DFS_NAMENODE_SUPPORT_ALLOW_FORMAT_KEY, false);	try {	cluster.shutdown();	NameNode.format(config);	fail("Format succeeded, when it should have failed");	} catch (IOException e) {	assertTrue("Exception was not about formatting Namenode", e.getMessage().startsWith("The option " + DFS_NAMENODE_SUPPORT_ALLOW_FORMAT_KEY));	}	
verifying format will succeed with allowformat true 

config.setBoolean(DFS_NAMENODE_SUPPORT_ALLOW_FORMAT_KEY, false);	try {	cluster.shutdown();	NameNode.format(config);	fail("Format succeeded, when it should have failed");	} catch (IOException e) {	assertTrue("Exception was not about formatting Namenode", e.getMessage().startsWith("The option " + DFS_NAMENODE_SUPPORT_ALLOW_FORMAT_KEY));	}	config.setBoolean(DFS_NAMENODE_SUPPORT_ALLOW_FORMAT_KEY, true);	NameNode.format(config);	
done verifying format will succeed with allowformat true 

========================= hadoop sample_7398 =========================

public void testBasicChmod() throws IOException {	requireWinutils();	File a = new File(TEST_DIR, "a");	a.createNewFile();	chmod("377", a);	try {	readFile(a);	assertFalse("readFile should have failed!", true);	} catch (IOException ex) {	
expected failed read from a file with permissions 

readFile(a);	assertFalse("readFile should have failed!", true);	} catch (IOException ex) {	}	chmod("700", a);	chmod("577", a);	try {	writeFile(a, "test");	fail("writeFile should have failed!");	} catch (IOException ex) {	
expected failed write to a file with permissions 

chmod("700", a);	assertTrue(a.delete());	File winutilsFile = Shell.getWinUtilsFile();	File aExe = new File(TEST_DIR, "a.exe");	FileUtils.copyFile(winutilsFile, aExe);	chmod("677", aExe);	try {	Shell.execCommand(aExe.getCanonicalPath(), "ls");	fail("executing " + aExe + " should have failed!");	} catch (IOException ex) {	
expected failed to execute a file with permissions 

assertNull("Listing a directory without read permission should fail", files);	chmod("700", a);	files = a.list();	assertEquals("b", files[0]);	chmod("500", a);	File c = new File(a, "c");	try {	c.createNewFile();	fail("writeFile should have failed!");	} catch (IOException ex) {	
expected failed to create a file when directory permissions are 

public void testSymlinkRejectsForwardSlashesInLink() throws IOException {	requireWinutils();	File newFile = new File(TEST_DIR, "file");	assertTrue(newFile.createNewFile());	String target = newFile.getPath();	String link = new File(TEST_DIR, "link").getPath().replaceAll("\\\\", "/");	try {	Shell.execCommand(winutils, "symlink", link, target);	fail(String.format("did not receive expected failure creating symlink " + "with forward slashes in link: link = %s, target = %s", link, target));	} catch (IOException e) {	
expected failed to create symlink with forward slashes in target 

public void testSymlinkRejectsForwardSlashesInTarget() throws IOException {	requireWinutils();	File newFile = new File(TEST_DIR, "file");	assertTrue(newFile.createNewFile());	String target = newFile.getPath().replaceAll("\\\\", "/");	String link = new File(TEST_DIR, "link").getPath();	try {	Shell.execCommand(winutils, "symlink", link, target);	fail(String.format("did not receive expected failure creating symlink " + "with forward slashes in target: link = %s, target = %s", link, target));	} catch (IOException e) {	
expected failed to create symlink with forward slashes in target 

========================= hadoop sample_2933 =========================

public void configure(JobConf conf) {	try {	config = new ConfigExtractor(conf);	ConfigExtractor.dumpOptions(config);	filesystem = config.getBaseDirectory().getFileSystem(conf);	} catch (Exception e) {	
unable to setup slive 

========================= hadoop sample_5576 =========================

Set<String> types = new TreeSet<>();	TimelineReaderContext context = getContext();	EntityRowKeyPrefix prefix = new EntityRowKeyPrefix(context.getClusterId(), context.getUserId(), context.getFlowName(), context.getFlowRunId(), context.getAppId());	byte[] currRowKey = prefix.getRowKeyPrefix();	byte[] nextRowKey = prefix.getRowKeyPrefix();	nextRowKey[nextRowKey.length - 1]++;	FilterList typeFilterList = new FilterList();	typeFilterList.addFilter(new FirstKeyOnlyFilter());	typeFilterList.addFilter(new KeyOnlyFilter());	typeFilterList.addFilter(new PageFilter(1));	
filterlist created for scan is 

typeFilterList.addFilter(new PageFilter(1));	int counter = 0;	while (true) {	try (ResultScanner results = getResult(hbaseConf, conn, typeFilterList, currRowKey, nextRowKey)) {	TimelineEntity entity = parseEntityForType(results.next());	if (entity == null) {	break;	}	++counter;	if (!types.add(entity.getType())) {	
failed to add type to the result set because there is a duplicated copy 

try (ResultScanner results = getResult(hbaseConf, conn, typeFilterList, currRowKey, nextRowKey)) {	TimelineEntity entity = parseEntityForType(results.next());	if (entity == null) {	break;	}	++counter;	if (!types.add(entity.getType())) {	}	String currType = entity.getType();	if (LOG.isDebugEnabled()) {	
current row key 

try (ResultScanner results = getResult(hbaseConf, conn, typeFilterList, currRowKey, nextRowKey)) {	TimelineEntity entity = parseEntityForType(results.next());	if (entity == null) {	break;	}	++counter;	if (!types.add(entity.getType())) {	}	String currType = entity.getType();	if (LOG.isDebugEnabled()) {	
new entity type discovered 

}	++counter;	if (!types.add(entity.getType())) {	}	String currType = entity.getType();	if (LOG.isDebugEnabled()) {	}	currRowKey = getNextRowKey(prefix.getRowKeyPrefix(), currType);	}	}	
scanned records for types 

========================= hadoop sample_1165 =========================

public void reinitialize(Configuration newConf, RMContext rmContext) throws IOException {	try {	writeLock.lock();	Configuration configuration = new Configuration(newConf);	CapacitySchedulerConfiguration oldConf = this.conf;	this.conf = csConfProvider.loadConfiguration(configuration);	validateConf(this.conf);	try {	
re initializing queues 

throw new QueueInvalidException(queueErrorMsg);	}	}	try {	queue.submitApplication(applicationId, user, queueName);	} catch (AccessControlException ace) {	}	queue.getMetrics().submitApp(user);	SchedulerApplication<FiCaSchedulerApp> application = new SchedulerApplication<FiCaSchedulerApp>(queue, user, priority);	applications.put(applicationId, application);	
accepted application from user in queue 

}	}	try {	queue.submitApplication(applicationId, user, queueName);	} catch (AccessControlException ace) {	}	queue.getMetrics().submitApp(user);	SchedulerApplication<FiCaSchedulerApp> application = new SchedulerApplication<FiCaSchedulerApp>(queue, user, priority);	applications.put(applicationId, application);	if (LOG.isDebugEnabled()) {	
is recovering skip notifying app accepted 

return;	}	if (!(queue instanceof LeafQueue)) {	String message = "Application " + applicationId + " submitted by user " + user + " to non-leaf queue: " + queueName;	this.rmContext.getDispatcher().getEventHandler().handle( new RMAppEvent(applicationId, RMAppEventType.APP_REJECTED, message));	return;	}	try {	queue.submitApplication(applicationId, user, queueName);	} catch (AccessControlException ace) {	
failed to submit application to queue from user 

}	try {	queue.submitApplication(applicationId, user, queueName);	} catch (AccessControlException ace) {	this.rmContext.getDispatcher().getEventHandler().handle( new RMAppEvent(applicationId, RMAppEventType.APP_REJECTED, ace.toString()));	return;	}	queue.getMetrics().submitApp(user);	SchedulerApplication<FiCaSchedulerApp> application = new SchedulerApplication<FiCaSchedulerApp>(queue, user, priority);	applications.put(applicationId, application);	
accepted application from user in queue 

private void addApplicationAttempt( ApplicationAttemptId applicationAttemptId, boolean transferStateFromPreviousAttempt, boolean isAttemptRecovering) {	try {	writeLock.lock();	SchedulerApplication<FiCaSchedulerApp> application = applications.get( applicationAttemptId.getApplicationId());	if (application == null) {	
application cannot be found in scheduler 

return;	}	CSQueue queue = (CSQueue) application.getQueue();	FiCaSchedulerApp attempt = new FiCaSchedulerApp(applicationAttemptId, application.getUser(), queue, queue.getAbstractUsersManager(), rmContext, application.getPriority(), isAttemptRecovering, activitiesManager);	if (transferStateFromPreviousAttempt) {	attempt.transferStateFromPreviousAttempt( application.getCurrentAppAttempt());	}	application.setCurrentAppAttempt(attempt);	attempt.setPriority(application.getPriority());	queue.submitApplicationAttempt(attempt, application.getUser());	
added application attempt to scheduler from user in queue 

CSQueue queue = (CSQueue) application.getQueue();	FiCaSchedulerApp attempt = new FiCaSchedulerApp(applicationAttemptId, application.getUser(), queue, queue.getAbstractUsersManager(), rmContext, application.getPriority(), isAttemptRecovering, activitiesManager);	if (transferStateFromPreviousAttempt) {	attempt.transferStateFromPreviousAttempt( application.getCurrentAppAttempt());	}	application.setCurrentAppAttempt(attempt);	attempt.setPriority(application.getPriority());	queue.submitApplicationAttempt(attempt, application.getUser());	if (isAttemptRecovering) {	if (LOG.isDebugEnabled()) {	
is recovering skipping notifying attempt added 

private void doneApplication(ApplicationId applicationId, RMAppState finalState) {	try {	writeLock.lock();	SchedulerApplication<FiCaSchedulerApp> application = applications.get( applicationId);	if (application == null) {	
couldn t find application 

private void doneApplication(ApplicationId applicationId, RMAppState finalState) {	try {	writeLock.lock();	SchedulerApplication<FiCaSchedulerApp> application = applications.get( applicationId);	if (application == null) {	return;	}	CSQueue queue = (CSQueue) application.getQueue();	if (!(queue instanceof LeafQueue)) {	
cannot finish application from non leaf queue 

private void doneApplicationAttempt( ApplicationAttemptId applicationAttemptId, RMAppAttemptState rmAppAttemptFinalState, boolean keepContainers) {	try {	writeLock.lock();	LOG.info("Application Attempt " + applicationAttemptId + " is done." + " finalState=" + rmAppAttemptFinalState);	FiCaSchedulerApp attempt = getApplicationAttempt(applicationAttemptId);	SchedulerApplication<FiCaSchedulerApp> application = applications.get( applicationAttemptId.getApplicationId());	if (application == null || attempt == null) {	
unknown application has completed 

try {	writeLock.lock();	LOG.info("Application Attempt " + applicationAttemptId + " is done." + " finalState=" + rmAppAttemptFinalState);	FiCaSchedulerApp attempt = getApplicationAttempt(applicationAttemptId);	SchedulerApplication<FiCaSchedulerApp> application = applications.get( applicationAttemptId.getApplicationId());	if (application == null || attempt == null) {	return;	}	for (RMContainer rmContainer : attempt.getLiveContainers()) {	if (keepContainers && rmContainer.getState().equals( RMContainerState.RUNNING)) {	
skip killing 

}	super.completedContainer(rmContainer, SchedulerUtils .createAbnormalContainerStatus(rmContainer.getContainerId(), SchedulerUtils.COMPLETED_APPLICATION), RMContainerEventType.KILL);	}	for (RMContainer rmContainer : attempt.getReservedContainers()) {	super.completedContainer(rmContainer, SchedulerUtils .createAbnormalContainerStatus(rmContainer.getContainerId(), "Application Complete"), RMContainerEventType.KILL);	}	attempt.stop(rmAppAttemptFinalState);	String queueName = attempt.getQueue().getQueueName();	CSQueue queue = this.getQueue(queueName);	if (!(queue instanceof LeafQueue)) {	
cannot finish application from non leaf queue 

public Allocation allocate(ApplicationAttemptId applicationAttemptId, List<ResourceRequest> ask, List<ContainerId> release, List<String> blacklistAdditions, List<String> blacklistRemovals, ContainerUpdates updateRequests) {	FiCaSchedulerApp application = getApplicationAttempt(applicationAttemptId);	if (application == null) {	
calling allocate on removed or non existent application 

public Allocation allocate(ApplicationAttemptId applicationAttemptId, List<ResourceRequest> ask, List<ContainerId> release, List<String> blacklistAdditions, List<String> blacklistRemovals, ContainerUpdates updateRequests) {	FiCaSchedulerApp application = getApplicationAttempt(applicationAttemptId);	if (application == null) {	return EMPTY_ALLOCATION;	}	if (!application.getApplicationAttemptId().equals(applicationAttemptId)) {	
calling allocate on previous or removed or non existent application attempt 

}	if (!ask.isEmpty()) {	if (LOG.isDebugEnabled()) {	LOG.debug( "allocate: pre-update " + applicationAttemptId + " ask size =" + ask.size());	application.showRequests();	}	if (application.updateResourceRequests(ask)) {	updateDemandForQueue = (LeafQueue) application.getQueue();	}	if (LOG.isDebugEnabled()) {	
allocate post update 

assignment = allocateContainersToNode(ps, true);	if (null != assignment && assignment.getType() == NodeType.OFF_SWITCH) {	offswitchCount++;	}	if (null != assignment && Resources.greaterThan(calculator, getClusterResource(), assignment.getResource(), Resources.none())) {	assignedContainers++;	}	}	if (offswitchCount >= offswitchPerHeartbeatLimit) {	if (LOG.isDebugEnabled()) {	
assigned maximum number of off switch containers assignments so far 

private CSAssignment allocateContainerOnSingleNode(PlacementSet<FiCaSchedulerNode> ps, FiCaSchedulerNode node, boolean withNodeHeartbeat) {	if (getNode(node.getNodeID()) != node) {	
trying to schedule on a removed node please double check 

private CSAssignment allocateContainerOnSingleNode(PlacementSet<FiCaSchedulerNode> ps, FiCaSchedulerNode node, boolean withNodeHeartbeat) {	if (getNode(node.getNodeID()) != node) {	return null;	}	CSAssignment assignment;	RMContainer reservedContainer = node.getReservedContainer();	if (reservedContainer != null) {	FiCaSchedulerApp reservedApplication = getCurrentAttemptForContainer( reservedContainer.getContainerId());	
trying to fulfill reservation for application on node 

ActivitiesLogger.NODE.finishAllocatedNodeAllocation(activitiesManager, node, reservedContainer.getContainerId(), AllocationState.ALLOCATED_FROM_RESERVED);	} else{	ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node, queue.getParent().getQueueName(), queue.getQueueName(), ActivityState.ACCEPTED, ActivityDiagnosticConstant.EMPTY);	ActivitiesLogger.NODE.finishAllocatedNodeAllocation(activitiesManager, node, reservedContainer.getContainerId(), AllocationState.SKIPPED);	}	assignment.setSchedulingMode( SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);	submitResourceCommitRequest(getClusterResource(), assignment);	}	if (node.getReservedContainer() != null) {	if (LOG.isDebugEnabled()) {	
skipping scheduling since node is reserved by application 

assignment.setSchedulingMode( SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);	submitResourceCommitRequest(getClusterResource(), assignment);	}	if (node.getReservedContainer() != null) {	if (LOG.isDebugEnabled()) {	}	return null;	}	if (calculator.computeAvailableContainers(Resources .add(node.getUnallocatedResource(), node.getTotalKillableResources()), minimumAllocation) <= 0) {	if (LOG.isDebugEnabled()) {	
this node or this node partition doesn t have available or killable resource 

if (LOG.isDebugEnabled()) {	}	return null;	}	if (calculator.computeAvailableContainers(Resources .add(node.getUnallocatedResource(), node.getTotalKillableResources()), minimumAllocation) <= 0) {	if (LOG.isDebugEnabled()) {	}	return null;	}	if (LOG.isDebugEnabled()) {	
trying to schedule on node available 

private CSAssignment allocateContainersOnMultiNodes( PlacementSet<FiCaSchedulerNode> ps) {	if (getRootQueue().getQueueCapacities().getUsedCapacity( ps.getPartition()) >= 1.0f && preemptionManager.getKillableResource( CapacitySchedulerConfiguration.ROOT, ps.getPartition()) == Resources .none()) {	if (LOG.isDebugEnabled()) {	
this node or this node partition doesn t have available or killable resource 

markContainerForKillable(killableContainer);	}	break;	case MARK_CONTAINER_FOR_NONKILLABLE: {	if (isLazyPreemptionEnabled) {	ContainerPreemptEvent cancelKillContainerEvent = (ContainerPreemptEvent) event;	markContainerForNonKillable(cancelKillContainerEvent.getContainer());	}	}	break;	
invalid eventtype ignoring 

private void addNode(RMNode nodeManager) {	try {	writeLock.lock();	FiCaSchedulerNode schedulerNode = new FiCaSchedulerNode(nodeManager, usePortForNodeName, nodeManager.getNodeLabels());	nodeTracker.addNode(schedulerNode);	if (labelManager != null) {	labelManager.activateNode(nodeManager.getNodeID(), schedulerNode.getTotalResource());	}	Resource clusterResource = getClusterResource();	getRootQueue().updateClusterResource(clusterResource, new ResourceLimits(clusterResource));	
added node clusterresource 

private void removeNode(RMNode nodeInfo) {	try {	writeLock.lock();	if (labelManager != null) {	labelManager.deactivateNode(nodeInfo.getNodeID());	}	NodeId nodeId = nodeInfo.getNodeID();	FiCaSchedulerNode node = nodeTracker.getNode(nodeId);	if (node == null) {	
attempting to remove non existent node 

}	nodeTracker.removeNode(nodeId);	Resource clusterResource = getClusterResource();	getRootQueue().updateClusterResource(clusterResource, new ResourceLimits(clusterResource));	int numNodes = nodeTracker.nodeCount();	if (scheduleAsynchronously && numNodes == 0) {	for (AsyncScheduleThread t : asyncSchedulerThreads) {	t.suspendSchedule();	}	}	
removed node clusterresource 

protected void completedContainerInternal( RMContainer rmContainer, ContainerStatus containerStatus, RMContainerEventType event) {	Container container = rmContainer.getContainer();	ContainerId containerId = container.getId();	FiCaSchedulerApp application = getCurrentAttemptForContainer( container.getId());	ApplicationId appId = containerId.getApplicationAttemptId().getApplicationId();	if (application == null) {	
container of finished application completed with event 

protected void completedContainerInternal( RMContainer rmContainer, ContainerStatus containerStatus, RMContainerEventType event) {	Container container = rmContainer.getContainer();	ContainerId containerId = container.getId();	FiCaSchedulerApp application = getCurrentAttemptForContainer( container.getId());	ApplicationId appId = containerId.getApplicationAttemptId().getApplicationId();	if (application == null) {	return;	}	FiCaSchedulerNode node = getNode(container.getNodeId());	if (null == node) {	
container of removed node completed with event 

public void markContainerForPreemption(ApplicationAttemptId aid, RMContainer cont) {	if(LOG.isDebugEnabled()){	
appattempt container 

public void markContainerForKillable( RMContainer killableContainer) {	try {	writeLock.lock();	if (LOG.isDebugEnabled()) {	
container 

private void markContainerForNonKillable( RMContainer nonKillableContainer) {	try {	writeLock.lock();	if (LOG.isDebugEnabled()) {	
container 

public boolean checkAccess(UserGroupInformation callerUGI, QueueACL acl, String queueName) {	CSQueue queue = getQueue(queueName);	if (queue == null) {	if (LOG.isDebugEnabled()) {	
acl not found for queue access type for queue 

public void removeQueue(String queueName) throws SchedulerDynamicEditException {	try {	writeLock.lock();	
removing queue 

CSQueue q = this.getQueue(queueName);	if (!(q instanceof ReservationQueue)) {	throw new SchedulerDynamicEditException( "The queue that we are asked " + "to remove (" + queueName + ") is not a ReservationQueue");	}	ReservationQueue disposableLeafQueue = (ReservationQueue) q;	if (disposableLeafQueue.getNumApplications() > 0) {	throw new SchedulerDynamicEditException( "The queue " + queueName + " is not empty " + disposableLeafQueue .getApplications().size() + " active apps " + disposableLeafQueue.getPendingApplications().size() + " pending apps");	}	((PlanQueue) disposableLeafQueue.getParent()).removeChildQueue(q);	this.queueManager.removeQueue(queueName);	
removal of reservationqueue has succeeded 

throw new SchedulerDynamicEditException( "Queue " + queue.getQueueName() + " is not a ReservationQueue");	}	ReservationQueue newQueue = (ReservationQueue) queue;	if (newQueue.getParent() == null || !(newQueue .getParent() instanceof PlanQueue)) {	throw new SchedulerDynamicEditException( "ParentQueue for " + newQueue.getQueueName() + " is not properly set (should be set and be a PlanQueue)");	}	PlanQueue parentPlan = (PlanQueue) newQueue.getParent();	String queuename = newQueue.getQueueName();	parentPlan.addChildQueue(newQueue);	this.queueManager.addQueue(queuename, newQueue);	
creation of reservationqueue succeeded 

float sumChilds = ((PlanQueue) parent).sumOfChildCapacities();	float newChildCap = sumChilds - queue.getCapacity() + entitlement.getCapacity();	if (newChildCap >= 0 && newChildCap < 1.0f + CSQueueUtils.EPSILON) {	if (Math.abs(entitlement.getCapacity() - queue.getCapacity()) == 0 && Math.abs( entitlement.getMaxCapacity() - queue.getMaximumCapacity()) == 0) {	return;	}	newQueue.setEntitlement(entitlement);	} else{	throw new SchedulerDynamicEditException( "Sum of child queues would exceed 100% for PlanQueue: " + parent .getQueueName());	}	
set entitlement for reservationqueue to request was 

}	if (!app.isStopped()) {	source.finishApplicationAttempt(app, sourceQueueName);	dest.submitApplicationAttempt(app, user);	}	app.move(dest);	}	source.appFinished();	source.getParent().finishApplication(appId, user);	application.setQueue(dest);	
app successfully moved from to 

public Resource getMaximumResourceCapability(String queueName) {	CSQueue queue = getQueue(queueName);	if (queue == null) {	
unknown queue 

public Resource getMaximumResourceCapability(String queueName) {	CSQueue queue = getQueue(queueName);	if (queue == null) {	return getMaximumResourceCapability();	}	if (!(queue instanceof LeafQueue)) {	
queue is not an leaf queue 

public Priority checkAndGetApplicationPriority( Priority priorityRequestedByApp, UserGroupInformation user, String queueName, ApplicationId applicationId) throws YarnException {	try {	readLock.lock();	Priority appPriority = priorityRequestedByApp;	if (null == appPriority) {	appPriority = this.appPriorityACLManager.getDefaultPriority(queueName, user);	if (null == appPriority) {	appPriority = this.queueManager.getDefaultPriorityForQueue(queueName);	}	
application is submitted without priority hence considering default queue cluster priority 

if (null == appPriority) {	appPriority = this.queueManager.getDefaultPriorityForQueue(queueName);	}	}	if (appPriority.getPriority() > getMaxClusterLevelAppPriority() .getPriority()) {	appPriority = Priority .newInstance(getMaxClusterLevelAppPriority().getPriority());	}	if (!appPriorityACLManager.checkAccess(user, queueName, appPriority)) {	throw new YarnException(new AccessControlException( "User " + user + " does not have permission to submit/update " + applicationId + " for " + appPriority));	}	
priority is acceptable in queue for application 

if (application.getPriority().equals(appPriority)) {	future.set(null);	return appPriority;	}	rmApp.getApplicationSubmissionContext().setPriority(appPriority);	ApplicationStateData appState = ApplicationStateData.newInstance( rmApp.getSubmitTime(), rmApp.getStartTime(), rmApp.getApplicationSubmissionContext(), rmApp.getUser(), rmApp.getCallerContext());	appState.setApplicationTimeouts(rmApp.getApplicationTimeouts());	rmContext.getStateStore().updateApplicationStateSynchronously(appState, false, future);	LeafQueue queue = (LeafQueue) getQueue(rmApp.getQueue());	queue.updateApplicationPriority(application, appPriority);	
priority is updated in queue for application for the user 

}	}	if (LOG.isDebugEnabled()) {	LOG.debug("Try to commit allocation proposal=" + request);	}	if (attemptId != null) {	FiCaSchedulerApp app = getApplicationAttempt(attemptId);	if (app != null && attemptId.equals(app.getApplicationAttemptId())) {	if (app.accept(cluster, request)) {	app.apply(cluster, request);	
allocation proposal accepted 

}	if (LOG.isDebugEnabled()) {	LOG.debug("Try to commit allocation proposal=" + request);	}	if (attemptId != null) {	FiCaSchedulerApp app = getApplicationAttempt(attemptId);	if (app != null && attemptId.equals(app.getApplicationAttemptId())) {	if (app.accept(cluster, request)) {	app.apply(cluster, request);	} else{	
failed to accept allocation proposal 

}	FiCaSchedulerNode sourceNode = getNode(toBeMovedContainer.getNodeId());	if (null == sourceNode) {	if (LOG.isDebugEnabled()) {	LOG.debug("Failed to move reservation, cannot find source node=" + toBeMovedContainer.getNodeId());	}	return false;	}	if (getNode(targetNode.getNodeID()) != targetNode) {	if (LOG.isDebugEnabled()) {	
failed to move reservation node updated or removed moving cancelled 

}	return false;	}	if (getNode(targetNode.getNodeID()) != targetNode) {	if (LOG.isDebugEnabled()) {	}	return false;	}	if (targetNode.getReservedContainer() != null) {	if (LOG.isDebugEnabled()) {	
target node s reservation status changed moving cancelled 

public long getMaximumApplicationLifetime(String queueName) {	CSQueue queue = getQueue(queueName);	if (queue == null || !(queue instanceof LeafQueue)) {	
unknown queue 

========================= hadoop sample_901 =========================

public String getDiagnosticInfo() { return diagnosticInfo; }	public void setDiagnosticInfo(String info) {	if (diagnosticInfo != null && diagnosticInfo.length() == getMaxStringSize()) {	
task diagnostic info for task 

public String getDiagnosticInfo() { return diagnosticInfo; }	public void setDiagnosticInfo(String info) {	if (diagnosticInfo != null && diagnosticInfo.length() == getMaxStringSize()) {	return;	}	diagnosticInfo = ((diagnosticInfo == null) ? info : diagnosticInfo.concat(info));	if (diagnosticInfo != null && diagnosticInfo.length() > getMaxStringSize()) {	
task diagnostic info for task 

public String getStateString() { return stateString; }	public void setStateString(String stateString) {	if (stateString != null) {	if (stateString.length() <= getMaxStringSize()) {	this.stateString = stateString;	} else {	
state string for task 

========================= hadoop sample_4672 =========================

public void run() {	try {	shexec.execute();	setNodeLabels(fetchLabelsFromScriptOutput(shexec.getOutput()));	} catch (Exception e) {	if (shexec.isTimedOut()) {	
node labels script timed out caught exception 

public void run() {	try {	shexec.execute();	setNodeLabels(fetchLabelsFromScriptOutput(shexec.getOutput()));	} catch (Exception e) {	if (shexec.isTimedOut()) {	} else {	
execution of node labels script failed caught exception 

========================= hadoop sample_1923 =========================

public static Configuration addSecurityConfiguration(Configuration conf) {	conf = new HdfsConfiguration(conf);	String nameNodePrincipal = conf.get( DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY, "");	if (LOG.isDebugEnabled()) {	
using nn principal 

========================= hadoop sample_7770 =========================

Vector<String> args = new Vector<String>();	if (isSetsidAvailable()) {	args.add("setsid");	}	args.add("bash");	args.add("-c");	args.add(" echo $$ > " + pidFile + "; sh " + shellScript + " " + N + ";");	shexec = new ShellCommandExecutor(args.toArray(new String[0]));	shexec.execute();	} catch (ExitCodeException ee) {	
shell command exit with a non zero exit code this is expected as we are killing the subprocesses of the task intentionally 

if (isSetsidAvailable()) {	args.add("setsid");	}	args.add("bash");	args.add("-c");	args.add(" echo $$ > " + pidFile + "; sh " + shellScript + " " + N + ";");	shexec = new ShellCommandExecutor(args.toArray(new String[0]));	shexec.execute();	} catch (ExitCodeException ee) {	} catch (IOException ioe) {	
error executing shell command 

args.add("setsid");	}	args.add("bash");	args.add("-c");	args.add(" echo $$ > " + pidFile + "; sh " + shellScript + " " + N + ";");	shexec = new ShellCommandExecutor(args.toArray(new String[0]));	shexec.execute();	} catch (ExitCodeException ee) {	} catch (IOException ioe) {	} finally {	
exit code 

tempFile = new File(TEST_ROOT_DIR, getClass().getName() + "_pidFile_" + rm.nextInt() + ".pid");	tempFile.deleteOnExit();	pidFile = TEST_ROOT_DIR + File.separator + tempFile.getName();	lowestDescendant = TEST_ROOT_DIR + File.separator + "lowestDescendantPidFile";	lostDescendant = TEST_ROOT_DIR + File.separator + "lostDescendantPidFile";	File file = new File(shellScript);	FileUtils.writeStringToFile(file, "# rogue task\n" + "sleep 1\n" + "echo hello\n" + "if [ $1 -ne 0 ]\n" + "then\n" + " sh " + shellScript + " $(($1-1))\n" + "else\n" + " echo $$ > " + lowestDescendant + "\n" + "(sleep 300&\n" + "echo $! > " + lostDescendant + ")\n" + " while true\n do\n" + "  sleep 5\n" + " done\n" + "fi");	Thread t = new RogueTaskThread();	t.start();	String pid = getRogueTaskPID();	
root process pid 

pidFile = TEST_ROOT_DIR + File.separator + tempFile.getName();	lowestDescendant = TEST_ROOT_DIR + File.separator + "lowestDescendantPidFile";	lostDescendant = TEST_ROOT_DIR + File.separator + "lostDescendantPidFile";	File file = new File(shellScript);	FileUtils.writeStringToFile(file, "# rogue task\n" + "sleep 1\n" + "echo hello\n" + "if [ $1 -ne 0 ]\n" + "then\n" + " sh " + shellScript + " $(($1-1))\n" + "else\n" + " echo $$ > " + lowestDescendant + "\n" + "(sleep 300&\n" + "echo $! > " + lostDescendant + ")\n" + " while true\n do\n" + "  sleep 5\n" + " done\n" + "fi");	Thread t = new RogueTaskThread();	t.start();	String pid = getRogueTaskPID();	ProcfsBasedProcessTree p = createProcessTree(pid);	p.updateProcessTree();	
processtree 

p.updateProcessTree();	File leaf = new File(lowestDescendant);	while (!leaf.exists()) {	try {	Thread.sleep(500);	} catch (InterruptedException ie) {	break;	}	}	p.updateProcessTree();	
processtree 

File leaf = new File(lowestDescendant);	while (!leaf.exists()) {	try {	Thread.sleep(500);	} catch (InterruptedException ie) {	break;	}	}	p.updateProcessTree();	String lostpid = getPidFromPidFile(lostDescendant);	
orphaned pid 

isAlive = isAlive(pid);	}	if (!isAlive) {	break;	}	Thread.sleep(100);	}	if (isAlive) {	fail("ProcessTree shouldn't be alive");	}	
process tree dump follows 

}	Assert.assertTrue("Process-tree dump doesn't start with a proper header", processTreeDump.startsWith("\t|- PID PPID PGRPID SESSID CMD_NAME " + "USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) " + "RSSMEM_USAGE(PAGES) FULL_CMD_LINE\n"));	for (int i = N; i >= 0; i--) {	String cmdLineDump = "\\|- [0-9]+ [0-9]+ [0-9]+ [0-9]+ \\(sh\\)" + " [0-9]+ [0-9]+ [0-9]+ [0-9]+ sh " + shellScript + " " + i;	Pattern pat = Pattern.compile(cmdLineDump);	Matcher mat = pat.matcher(processTreeDump);	Assert.assertTrue("Process-tree dump doesn't contain the cmdLineDump of " + i + "th process!", mat.find());	}	try {	t.join(2000);	
roguetaskthread successfully joined 

Assert.assertTrue("Process-tree dump doesn't start with a proper header", processTreeDump.startsWith("\t|- PID PPID PGRPID SESSID CMD_NAME " + "USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) " + "RSSMEM_USAGE(PAGES) FULL_CMD_LINE\n"));	for (int i = N; i >= 0; i--) {	String cmdLineDump = "\\|- [0-9]+ [0-9]+ [0-9]+ [0-9]+ \\(sh\\)" + " [0-9]+ [0-9]+ [0-9]+ [0-9]+ sh " + shellScript + " " + i;	Pattern pat = Pattern.compile(cmdLineDump);	Matcher mat = pat.matcher(processTreeDump);	Assert.assertTrue("Process-tree dump doesn't contain the cmdLineDump of " + i + "th process!", mat.find());	}	try {	t.join(2000);	} catch (InterruptedException ie) {	
interrupted while joining roguetaskthread 

public static String getPidFromPidFile(String pidFileName) {	BufferedReader pidFile = null;	FileReader fReader = null;	String pid = null;	try {	fReader = new FileReader(pidFileName);	pidFile = new BufferedReader(fReader);	} catch (FileNotFoundException f) {	
pidfile doesn t exist 

String pid = null;	try {	fReader = new FileReader(pidFileName);	pidFile = new BufferedReader(fReader);	} catch (FileNotFoundException f) {	return pid;	}	try {	pid = pidFile.readLine();	} catch (IOException i) {	
failed to read from 

} finally {	try {	if (fReader != null) {	fReader.close();	}	try {	if (pidFile != null) {	pidFile.close();	}	} catch (IOException i) {	
error closing the stream 

if (fReader != null) {	fReader.close();	}	try {	if (pidFile != null) {	pidFile.close();	}	} catch (IOException i) {	}	} catch (IOException i) {	
error closing the stream 

cmdLines[2] = "proc(3) arg5 arg6";	cmdLines[3] = "proc4 arg7 arg8";	cmdLines[4] = "proc5 arg9 arg10";	cmdLines[5] = "proc6 arg11 arg12";	createMemoryMappingInfo(memInfos);	writeStatFiles(procfsRootDir, pids, procInfos, memInfos);	writeCmdLineFiles(procfsRootDir, pids, cmdLines);	ProcfsBasedProcessTree processTree = createProcessTree("100", procfsRootDir.getAbsolutePath(), SystemClock.getInstance());	processTree.updateProcessTree();	String processTreeDump = processTree.getProcessTreeDump();	
process tree dump follows 

protected static boolean isSetsidAvailable() {	ShellCommandExecutor shexec = null;	boolean setsidSupported = true;	try {	String[] args = { "setsid", "bash", "-c", "echo $$" };	shexec = new ShellCommandExecutor(args);	shexec.execute();	} catch (IOException ioe) {	
setsid is not available on this machine so not using it 

protected static boolean isSetsidAvailable() {	ShellCommandExecutor shexec = null;	boolean setsidSupported = true;	try {	String[] args = { "setsid", "bash", "-c", "echo $$" };	shexec = new ShellCommandExecutor(args);	shexec.execute();	} catch (IOException ioe) {	setsidSupported = false;	} finally {	
setsid exited with exit code 

public static void setupPidDirs(File procfsRootDir, String[] pids) throws IOException {	for (String pid : pids) {	File pidDir = new File(procfsRootDir, pid);	pidDir.mkdir();	if (!pidDir.exists()) {	throw new IOException("couldn't make process directory under " + "fake procfs");	} else {	
created pid dir 

public static void writeStatFiles(File procfsRootDir, String[] pids, ProcessStatInfo[] procs, ProcessTreeSmapMemInfo[] smaps) throws IOException {	for (int i = 0; i < pids.length; i++) {	File statFile = new File(new File(procfsRootDir, pids[i]), ProcfsBasedProcessTree.PROCFS_STAT_FILE);	BufferedWriter bw = null;	try {	FileWriter fw = new FileWriter(statFile);	bw = new BufferedWriter(fw);	bw.write(procs[i].getStatLine());	
wrote stat file for with contents 

}	}	if (smaps != null) {	File smapFile = new File(new File(procfsRootDir, pids[i]), ProcfsBasedProcessTree.SMAPS);	bw = null;	try {	FileWriter fw = new FileWriter(smapFile);	bw = new BufferedWriter(fw);	bw.write(smaps[i].toString());	bw.flush();	
wrote smap file for with contents 

private static void writeCmdLineFiles(File procfsRootDir, String[] pids, String[] cmdLines) throws IOException {	for (int i = 0; i < pids.length; i++) {	File statFile = new File(new File(procfsRootDir, pids[i]), ProcfsBasedProcessTree.PROCFS_CMDLINE_FILE);	BufferedWriter bw = null;	try {	bw = new BufferedWriter(new FileWriter(statFile));	bw.write(cmdLines[i]);	
wrote command line file for with contents 

========================= hadoop sample_2088 =========================

HATestUtil.waitForStandbyToCatchUp(nn0, nn1);	HATestUtil.waitForCheckpoint(cluster2, 1, ImmutableList.of(100));	assertEquals("Should be zero right after the checkpoint.", 0L, cluster2.getNameNode(1).getNamesystem() .getTransactionsSinceLastCheckpoint());	fs2.mkdirs(new Path("/tmp-t3"));	fs2.mkdirs(new Path("/tmp-t4"));	HATestUtil.waitForStandbyToCatchUp(nn0, nn1);	assertEquals("SBN failed to track 2 added txns after the ckpt.", 4L, cluster2.getNameNode(1).getNamesystem() .getTransactionsSinceLastCheckpoint());	cluster2.shutdown();	break;	} catch (Exception e) {	
unable to set up ha cluster exception thrown 

========================= hadoop sample_7320 =========================

public synchronized void shutdown() {	
shutting down all asyncdiskservice threads 

public synchronized boolean awaitTermination(long milliseconds) throws InterruptedException {	long end = Time.now() + milliseconds;	for (Map.Entry<String, ThreadPoolExecutor> e: executors.entrySet()) {	ThreadPoolExecutor executor = e.getValue();	if (!executor.awaitTermination( Math.max(end - Time.now(), 0), TimeUnit.MILLISECONDS)) {	
asyncdiskservice awaittermination timeout 

public synchronized boolean awaitTermination(long milliseconds) throws InterruptedException {	long end = Time.now() + milliseconds;	for (Map.Entry<String, ThreadPoolExecutor> e: executors.entrySet()) {	ThreadPoolExecutor executor = e.getValue();	if (!executor.awaitTermination( Math.max(end - Time.now(), 0), TimeUnit.MILLISECONDS)) {	return false;	}	}	
all asyncdiskservice threads are terminated 

public synchronized List<Runnable> shutdownNow() {	
shutting down all asyncdiskservice threads immediately 

========================= hadoop sample_3699 =========================

public static void cleanup(String action, FileSystem fileSystem, Path path) {	noteAction(action);	try {	rm(fileSystem, path, true, false);	} catch (Exception e) {	
error deleting in 

public static void downgrade(String message, Throwable failure) {	
downgrading test 

public static void skip(String message) {	
skipping 

public long end(String format, Object... args) {	long d = end();	
duration of ns 

========================= hadoop sample_3226 =========================

private static int test0(IntWritable key, MarkableIterator<IntWritable> values) throws IOException {	int errors = 0;	IntWritable i;	ArrayList<IntWritable> expectedValues = new ArrayList<IntWritable>();	
executing test for key 

private static int test0(IntWritable key, MarkableIterator<IntWritable> values) throws IOException {	int errors = 0;	IntWritable i;	ArrayList<IntWritable> expectedValues = new ArrayList<IntWritable>();	values.mark();	
test marking 

int errors = 0;	IntWritable i;	ArrayList<IntWritable> expectedValues = new ArrayList<IntWritable>();	values.mark();	while (values.hasNext()) {	i = values.next();	expectedValues.add(i);	LOG.info(key + ":" + i);	}	values.reset();	
test reset 

i = values.next();	expectedValues.add(i);	LOG.info(key + ":" + i);	}	values.reset();	int count = 0;	while (values.hasNext()) {	i = values.next();	LOG.info(key + ":" + i);	if (i != expectedValues.get(count)) {	
test check expected got 

int count = 0;	while (values.hasNext()) {	i = values.next();	LOG.info(key + ":" + i);	if (i != expectedValues.get(count)) {	errors ++;	return errors;	}	count ++;	}	
test done 

private static int test1(IntWritable key, MarkableIterator<IntWritable> values) throws IOException {	IntWritable i;	int errors = 0;	int count = 0;	ArrayList<IntWritable> expectedValues = new ArrayList<IntWritable>();	ArrayList<IntWritable> expectedValues1 = new ArrayList<IntWritable>();	
executing test for key 

private static int test1(IntWritable key, MarkableIterator<IntWritable> values) throws IOException {	IntWritable i;	int errors = 0;	int count = 0;	ArrayList<IntWritable> expectedValues = new ArrayList<IntWritable>();	ArrayList<IntWritable> expectedValues1 = new ArrayList<IntWritable>();	values.mark();	
test marking 

while (values.hasNext()) {	i = values.next();	LOG.info(key + ":" + i);	expectedValues.add(i);	if (count == 2) {	break;	}	count ++;	}	values.reset();	
test reset 

count ++;	}	values.reset();	count = 0;	while (values.hasNext()) {	i = values.next();	LOG.info(key + ":" + i);	if (count < expectedValues.size()) {	if (i != expectedValues.get(count)) {	errors ++;	
test check expected got 

i = values.next();	LOG.info(key + ":" + i);	if (count < expectedValues.size()) {	if (i != expectedValues.get(count)) {	errors ++;	return errors;	}	}	if (count == 3) {	values.mark();	
test marking 

}	if (count >= 3) {	expectedValues1.add(i);	}	if (count == 5) {	break;	}	count ++;	}	if (count < expectedValues.size()) {	
test check iterator returned lesser values 

break;	}	count ++;	}	if (count < expectedValues.size()) {	errors ++;	return errors;	}	values.reset();	count = 0;	
test reset 

}	values.reset();	count = 0;	expectedValues.clear();	while (values.hasNext()) {	i = values.next();	LOG.info(key + ":" + i);	if (count < expectedValues1.size()) {	if (i != expectedValues1.get(count)) {	errors ++;	
test check expected got 

i = values.next();	LOG.info(key + ":" + i);	if (count < expectedValues1.size()) {	if (i != expectedValues1.get(count)) {	errors ++;	return errors;	}	}	if (count == 25) {	values.mark();	
test marking 

}	if (count == 25) {	values.mark();	}	if (count >= 25) {	expectedValues.add(i);	}	count ++;	}	if (count < expectedValues1.size()) {	
test check iterator returned fewer values 

if (count >= 25) {	expectedValues.add(i);	}	count ++;	}	if (count < expectedValues1.size()) {	errors ++;	return errors;	}	values.reset();	
test reset 

errors ++;	return errors;	}	values.reset();	count = 0;	while (values.hasNext()) {	i = values.next();	LOG.info(key + ":" + i);	if (i != expectedValues.get(count)) {	errors ++;	
test check expected got 

values.reset();	count = 0;	while (values.hasNext()) {	i = values.next();	LOG.info(key + ":" + i);	if (i != expectedValues.get(count)) {	errors ++;	return errors;	}	}	
test done 

private static int test2(IntWritable key, MarkableIterator<IntWritable> values) throws IOException {	IntWritable i;	int errors = 0;	int count = 0;	ArrayList<IntWritable> expectedValues = new ArrayList<IntWritable>();	ArrayList<IntWritable> expectedValues1 = new ArrayList<IntWritable>();	
executing test for key 

private static int test2(IntWritable key, MarkableIterator<IntWritable> values) throws IOException {	IntWritable i;	int errors = 0;	int count = 0;	ArrayList<IntWritable> expectedValues = new ArrayList<IntWritable>();	ArrayList<IntWritable> expectedValues1 = new ArrayList<IntWritable>();	values.mark();	
test marking 

i = values.next();	LOG.info(key + ":" + i);	expectedValues.add(i);	if (count == 8) {	break;	}	count ++;	}	values.reset();	count = 0;	
test reset 

count ++;	}	values.reset();	count = 0;	while (values.hasNext()) {	i = values.next();	LOG.info(key + ":" + i);	if (count < expectedValues.size()) {	if (i != expectedValues.get(count)) {	errors ++;	
test check expected got 

i = values.next();	LOG.info(key + ":" + i);	if (count < expectedValues.size()) {	if (i != expectedValues.get(count)) {	errors ++;	return errors;	}	}	if (count == 3) {	values.mark();	
test marking 

}	if (count == 3) {	values.mark();	}	if (count >= 3) {	expectedValues1.add(i);	}	count ++;	}	values.reset();	
test reset 

}	values.reset();	expectedValues.clear();	count = 0;	while (values.hasNext()) {	i = values.next();	LOG.info(key + ":" + i);	if (count < expectedValues1.size()) {	if (i != expectedValues1.get(count)) {	errors ++;	
test check expected got 

i = values.next();	LOG.info(key + ":" + i);	if (count < expectedValues1.size()) {	if (i != expectedValues1.get(count)) {	errors ++;	return errors;	}	}	if (count == 20) {	values.mark();	
test marking 

if (count == 20) {	values.mark();	}	if (count >= 20) {	expectedValues.add(i);	}	count ++;	}	values.reset();	count = 0;	
test reset 

}	count ++;	}	values.reset();	count = 0;	while (values.hasNext()) {	i = values.next();	LOG.info(key + ":" + i);	if (i != expectedValues.get(count)) {	errors ++;	
test check expected got 

values.reset();	count = 0;	while (values.hasNext()) {	i = values.next();	LOG.info(key + ":" + i);	if (i != expectedValues.get(count)) {	errors ++;	return errors;	}	}	
test done 

private static int test3(IntWritable key, MarkableIterator<IntWritable> values) throws IOException {	int errors = 0;	IntWritable i;	ArrayList<IntWritable> expectedValues = new ArrayList<IntWritable>();	
executing test for key 

private static int test3(IntWritable key, MarkableIterator<IntWritable> values) throws IOException {	int errors = 0;	IntWritable i;	ArrayList<IntWritable> expectedValues = new ArrayList<IntWritable>();	values.mark();	
test marking 

private static int test3(IntWritable key, MarkableIterator<IntWritable> values) throws IOException {	int errors = 0;	IntWritable i;	ArrayList<IntWritable> expectedValues = new ArrayList<IntWritable>();	values.mark();	int count = 0;	while (values.hasNext()) {	i = values.next();;	LOG.info(key + ":" + i);	if (count == 5) {	
test clearing mark 

ArrayList<IntWritable> expectedValues = new ArrayList<IntWritable>();	values.mark();	int count = 0;	while (values.hasNext()) {	i = values.next();;	LOG.info(key + ":" + i);	if (count == 5) {	values.clearMark();	}	if (count == 8) {	
test marking 

}	if (count == 8) {	values.mark();	}	if (count >= 8) {	expectedValues.add(i);	}	count ++;	}	values.reset();	
test after reset 

values.mark();	}	if (count >= 8) {	expectedValues.add(i);	}	count ++;	}	values.reset();	if (!values.hasNext()) {	errors ++;	
test check hasnext returned false 

errors ++;	return errors;	}	count = 0;	while (values.hasNext()) {	i = values.next();	LOG.info(key + ":" + i);	if (count < expectedValues.size()) {	if (i != expectedValues.get(count)) {	errors ++;	
test check expected got 

i = values.next();	LOG.info(key + ":" + i);	if (count < expectedValues.size()) {	if (i != expectedValues.get(count)) {	errors ++;	return errors;	}	}	if (count == 10) {	values.clearMark();	
test after clear mark 

return errors;	}	}	if (count == 10) {	values.clearMark();	}	count ++;	}	boolean successfulClearMark = false;	try {	
test before reset 

}	count ++;	}	boolean successfulClearMark = false;	try {	values.reset();	} catch (IOException e) {	successfulClearMark = true;	}	if (!successfulClearMark) {	
test check reset was successfule even after clearmark 

boolean successfulClearMark = false;	try {	values.reset();	} catch (IOException e) {	successfulClearMark = true;	}	if (!successfulClearMark) {	errors ++;	return errors;	}	
test done 

private void validateOutput() throws IOException {	Path[] outputFiles = FileUtil.stat2Paths( localFs.listStatus(new Path(TEST_ROOT_DIR + "/out"), new Utils.OutputFileUtils.OutputFilesFilter()));	if (outputFiles.length > 0) {	InputStream is = localFs.open(outputFiles[0]);	BufferedReader reader = new BufferedReader(new InputStreamReader(is));	String line = reader.readLine();	while (line != null) {	StringTokenizer tokeniz = new StringTokenizer(line, "\t");	String key = tokeniz.nextToken();	String value = tokeniz.nextToken();	
output key value 

========================= hadoop sample_5608 =========================

private void intializePriorityDigraph() {	
initializing priority preemption directed graph 

int j = path2.size() - 1;	while (path1.get(i).queueName.equals(path2.get(j).queueName)) {	i--;	j--;	}	int p1 = path1.get(i).relativePriority;	int p2 = path2.get(j).relativePriority;	if (p1 < p2) {	priorityDigraph.put(q2, q1, true);	if (LOG.isDebugEnabled()) {	
added priority ordering edge 

}	int p1 = path1.get(i).relativePriority;	int p2 = path2.get(j).relativePriority;	if (p1 < p2) {	priorityDigraph.put(q2, q1, true);	if (LOG.isDebugEnabled()) {	}	} else if (p2 < p1) {	priorityDigraph.put(q1, q2, true);	if (LOG.isDebugEnabled()) {	
added priority ordering edge 

========================= hadoop sample_686 =========================

public T answer(InvocationOnMock invocation) throws Throwable {	
awaiting remaining latch count is 

public T answer(InvocationOnMock invocation) throws Throwable {	T result = (T)invocation.callRealMethod();	latch.countDown();	
countdown remaining latch count is 

========================= hadoop sample_7265 =========================

public boolean canCommit(TaskAttemptId taskAttemptID) {	readLock.lock();	boolean canCommit = false;	try {	if (commitAttempt != null) {	canCommit = taskAttemptID.equals(commitAttempt);	
result of cancommit for 

private TaskAttemptImpl addAttempt(Avataar avataar) {	TaskAttemptImpl attempt = createAttempt();	attempt.setAvataar(avataar);	if (LOG.isDebugEnabled()) {	
created attempt 

public void handle(TaskEvent event) {	if (LOG.isDebugEnabled()) {	
processing of type 

public void handle(TaskEvent event) {	if (LOG.isDebugEnabled()) {	}	try {	writeLock.lock();	TaskStateInternal oldState = getInternalState();	try {	stateMachine.doTransition(event.getType(), event);	} catch (InvalidStateTransitionException e) {	
can t handle this event at current state for 

}	try {	writeLock.lock();	TaskStateInternal oldState = getInternalState();	try {	stateMachine.doTransition(event.getType(), event);	} catch (InvalidStateTransitionException e) {	internalError(event.getType());	}	if (oldState != getInternalState()) {	
task transitioned from to 

protected void internalError(TaskEventType type) {	
invalid event on task 

private void sendTaskSucceededEvents() {	eventHandler.handle(new JobTaskEvent(taskId, TaskState.SUCCEEDED));	
task succeeded with attempt 

private TaskStateInternal recover(TaskInfo taskInfo, OutputCommitter committer, boolean recoverTaskOutput) {	
recovering task from prior app attempt status was 

successfulAttempt = attempt.getID();	}	}	}	nextAttemptNumber = savedNextAttemptNumber;	TaskStateInternal taskState = TaskStateInternal.valueOf( taskInfo.getTaskStatus());	switch (taskState) {	case SUCCEEDED: if (successfulAttempt != null) {	sendTaskSucceededEvents();	} else {	
missing successful attempt for task recovering as running 

public void transition(TaskImpl task, TaskEvent event) {	
scheduling a redundant attempt for task 

public void transition(TaskImpl task, TaskEvent event) {	TaskTAttemptEvent ev = (TaskTAttemptEvent) event;	TaskAttemptId attemptID = ev.getTaskAttemptID();	if (task.commitAttempt == null) {	task.commitAttempt = attemptID;	
given a go for committing the task output 

public void transition(TaskImpl task, TaskEvent event) {	TaskTAttemptEvent ev = (TaskTAttemptEvent) event;	TaskAttemptId attemptID = ev.getTaskAttemptID();	if (task.commitAttempt == null) {	task.commitAttempt = attemptID;	} else {	
already given a go for committing the task output so killing 

public void transition(TaskImpl task, TaskEvent event) {	TaskTAttemptEvent taskTAttemptEvent = (TaskTAttemptEvent) event;	TaskAttemptId taskAttemptId = taskTAttemptEvent.getTaskAttemptID();	task.handleTaskAttemptCompletion( taskAttemptId, TaskAttemptCompletionEventStatus.SUCCEEDED);	task.finishedAttempts.add(taskAttemptId);	task.inProgressAttempts.remove(taskAttemptId);	task.successfulAttempt = taskAttemptId;	task.sendTaskSucceededEvents();	for (TaskAttempt attempt : task.attempts.values()) {	if (attempt.getID() != task.successfulAttempt && !attempt.isFinished()) {	
issuing kill to other attempt 

public TaskStateInternal transition(TaskImpl task, TaskEvent event) {	TaskAttemptId taskAttemptId = ((TaskTAttemptEvent) event).getTaskAttemptID();	task.handleTaskAttemptCompletion(taskAttemptId, taCompletionEventStatus);	task.finishedAttempts.add(taskAttemptId);	if (task.finishedAttempts.size() == task.attempts.size()) {	if (task.historyTaskStartGenerated) {	TaskFailedEvent taskFailedEvent = createTaskFailedEvent(task, null, finalState, null);	task.eventHandler.handle(new JobHistoryEvent(task.taskId.getJobId(), taskFailedEvent));	} else {	
not generating historyfinish event since start event not generated for task 

} else {	task.handleTaskAttemptCompletion( taskAttemptId, TaskAttemptCompletionEventStatus.TIPFAILED);	for (TaskAttempt taskAttempt : task.attempts.values()) {	task.killUnfinishedAttempt (taskAttempt, "Task has failed. Killing attempt!");	}	task.inProgressAttempts.clear();	if (task.historyTaskStartGenerated) {	TaskFailedEvent taskFailedEvent = createTaskFailedEvent(task, attempt.getDiagnostics(), TaskStateInternal.FAILED, taskAttemptId);	task.eventHandler.handle(new JobHistoryEvent(task.taskId.getJobId(), taskFailedEvent));	} else {	
not generating historyfinish event since start event not generated for task 

public TaskStateInternal transition(TaskImpl task, TaskEvent event) {	TaskTAttemptEvent castEvent = (TaskTAttemptEvent) event;	if (task.getInternalState() == TaskStateInternal.SUCCEEDED && !castEvent.getTaskAttemptID().equals(task.successfulAttempt)) {	task.finishedAttempts.add(castEvent.getTaskAttemptID());	task.inProgressAttempts.remove(castEvent.getTaskAttemptID());	return TaskStateInternal.SUCCEEDED;	}	if (!TaskType.MAP.equals(task.getType())) {	
unexpected event for reduce task 

if (event instanceof TaskTAttemptEvent) {	TaskTAttemptEvent castEvent = (TaskTAttemptEvent) event;	attemptId = castEvent.getTaskAttemptID();	if (task.getInternalState() == TaskStateInternal.SUCCEEDED && !attemptId.equals(task.successfulAttempt)) {	task.finishedAttempts.add(castEvent.getTaskAttemptID());	task.inProgressAttempts.remove(castEvent.getTaskAttemptID());	return TaskStateInternal.SUCCEEDED;	}	}	if (!TaskType.MAP.equals(task.getType())) {	
unexpected event for reduce task 

public void transition(TaskImpl task, TaskEvent event) {	if (task.historyTaskStartGenerated) {	TaskFailedEvent taskFailedEvent = createTaskFailedEvent(task, null, TaskStateInternal.KILLED, null);	task.eventHandler.handle(new JobHistoryEvent(task.taskId.getJobId(), taskFailedEvent));	}else {	
not generating historyfinish event since start event not generated for task 

========================= hadoop sample_5258 =========================

public void teardown() throws Exception {	
fs details 

========================= hadoop sample_5872 =========================

protected void handleWritingApplicationHistoryEvent( WritingApplicationHistoryEvent event) {	switch (event.getType()) {	case APP_START: WritingApplicationStartEvent wasEvent = (WritingApplicationStartEvent) event;	try {	writer.applicationStarted(wasEvent.getApplicationStartData());	
stored the start data of application 

protected void handleWritingApplicationHistoryEvent( WritingApplicationHistoryEvent event) {	switch (event.getType()) {	case APP_START: WritingApplicationStartEvent wasEvent = (WritingApplicationStartEvent) event;	try {	writer.applicationStarted(wasEvent.getApplicationStartData());	} catch (IOException e) {	
error when storing the start data of application 

switch (event.getType()) {	case APP_START: WritingApplicationStartEvent wasEvent = (WritingApplicationStartEvent) event;	try {	writer.applicationStarted(wasEvent.getApplicationStartData());	} catch (IOException e) {	}	break;	case APP_FINISH: WritingApplicationFinishEvent wafEvent = (WritingApplicationFinishEvent) event;	try {	writer.applicationFinished(wafEvent.getApplicationFinishData());	
stored the finish data of application 

case APP_START: WritingApplicationStartEvent wasEvent = (WritingApplicationStartEvent) event;	try {	writer.applicationStarted(wasEvent.getApplicationStartData());	} catch (IOException e) {	}	break;	case APP_FINISH: WritingApplicationFinishEvent wafEvent = (WritingApplicationFinishEvent) event;	try {	writer.applicationFinished(wafEvent.getApplicationFinishData());	} catch (IOException e) {	
error when storing the finish data of application 

break;	case APP_FINISH: WritingApplicationFinishEvent wafEvent = (WritingApplicationFinishEvent) event;	try {	writer.applicationFinished(wafEvent.getApplicationFinishData());	} catch (IOException e) {	}	break;	case APP_ATTEMPT_START: WritingApplicationAttemptStartEvent waasEvent = (WritingApplicationAttemptStartEvent) event;	try {	writer.applicationAttemptStarted(waasEvent .getApplicationAttemptStartData());	
stored the start data of application attempt 

case APP_FINISH: WritingApplicationFinishEvent wafEvent = (WritingApplicationFinishEvent) event;	try {	writer.applicationFinished(wafEvent.getApplicationFinishData());	} catch (IOException e) {	}	break;	case APP_ATTEMPT_START: WritingApplicationAttemptStartEvent waasEvent = (WritingApplicationAttemptStartEvent) event;	try {	writer.applicationAttemptStarted(waasEvent .getApplicationAttemptStartData());	} catch (IOException e) {	
error when storing the start data of application attempt 

break;	case APP_ATTEMPT_START: WritingApplicationAttemptStartEvent waasEvent = (WritingApplicationAttemptStartEvent) event;	try {	writer.applicationAttemptStarted(waasEvent .getApplicationAttemptStartData());	} catch (IOException e) {	}	break;	case APP_ATTEMPT_FINISH: WritingApplicationAttemptFinishEvent waafEvent = (WritingApplicationAttemptFinishEvent) event;	try {	writer.applicationAttemptFinished(waafEvent .getApplicationAttemptFinishData());	
stored the finish data of application attempt 

case APP_ATTEMPT_START: WritingApplicationAttemptStartEvent waasEvent = (WritingApplicationAttemptStartEvent) event;	try {	writer.applicationAttemptStarted(waasEvent .getApplicationAttemptStartData());	} catch (IOException e) {	}	break;	case APP_ATTEMPT_FINISH: WritingApplicationAttemptFinishEvent waafEvent = (WritingApplicationAttemptFinishEvent) event;	try {	writer.applicationAttemptFinished(waafEvent .getApplicationAttemptFinishData());	} catch (IOException e) {	
error when storing the finish data of application attempt 

break;	case APP_ATTEMPT_FINISH: WritingApplicationAttemptFinishEvent waafEvent = (WritingApplicationAttemptFinishEvent) event;	try {	writer.applicationAttemptFinished(waafEvent .getApplicationAttemptFinishData());	} catch (IOException e) {	}	break;	case CONTAINER_START: WritingContainerStartEvent wcsEvent = (WritingContainerStartEvent) event;	try {	writer.containerStarted(wcsEvent.getContainerStartData());	
stored the start data of container 

case APP_ATTEMPT_FINISH: WritingApplicationAttemptFinishEvent waafEvent = (WritingApplicationAttemptFinishEvent) event;	try {	writer.applicationAttemptFinished(waafEvent .getApplicationAttemptFinishData());	} catch (IOException e) {	}	break;	case CONTAINER_START: WritingContainerStartEvent wcsEvent = (WritingContainerStartEvent) event;	try {	writer.containerStarted(wcsEvent.getContainerStartData());	} catch (IOException e) {	
error when storing the start data of container 

break;	case CONTAINER_START: WritingContainerStartEvent wcsEvent = (WritingContainerStartEvent) event;	try {	writer.containerStarted(wcsEvent.getContainerStartData());	} catch (IOException e) {	}	break;	case CONTAINER_FINISH: WritingContainerFinishEvent wcfEvent = (WritingContainerFinishEvent) event;	try {	writer.containerFinished(wcfEvent.getContainerFinishData());	
stored the finish data of container 

case CONTAINER_START: WritingContainerStartEvent wcsEvent = (WritingContainerStartEvent) event;	try {	writer.containerStarted(wcsEvent.getContainerStartData());	} catch (IOException e) {	}	break;	case CONTAINER_FINISH: WritingContainerFinishEvent wcfEvent = (WritingContainerFinishEvent) event;	try {	writer.containerFinished(wcfEvent.getContainerFinishData());	} catch (IOException e) {	
error when storing the finish data of container 

writer.containerStarted(wcsEvent.getContainerStartData());	} catch (IOException e) {	}	break;	case CONTAINER_FINISH: WritingContainerFinishEvent wcfEvent = (WritingContainerFinishEvent) event;	try {	writer.containerFinished(wcfEvent.getContainerFinishData());	} catch (IOException e) {	}	break;	
unknown writingapplicationhistoryevent type 

========================= hadoop sample_1098 =========================

protected void serviceStart() throws Exception {	super.serviceStart();	
getting the active app list to initialize the in memory scm store 

protected void serviceStart() throws Exception {	super.serviceStart();	synchronized (initialAppsLock) {	initialApps = appChecker.getActiveApplications();	}	
apps recorded as active at this time 

protected void serviceStart() throws Exception {	super.serviceStart();	synchronized (initialAppsLock) {	initialApps = appChecker.getActiveApplications();	}	Runnable task = new AppCheckTask(appChecker);	scheduler.scheduleAtFixedRate(task, initialDelayMin, checkPeriodMin, TimeUnit.MINUTES);	
scheduled the in memory scm store app check task to run every minutes 

protected void serviceStop() throws Exception {	
stopping the service 

protected void serviceStop() throws Exception {	if (scheduler != null) {	
shutting down the background thread 

protected void serviceStop() throws Exception {	if (scheduler != null) {	scheduler.shutdownNow();	try {	if (!scheduler.awaitTermination(10, TimeUnit.SECONDS)) {	
gave up waiting for the app check task to shutdown 

protected void serviceStop() throws Exception {	if (scheduler != null) {	scheduler.shutdownNow();	try {	if (!scheduler.awaitTermination(10, TimeUnit.SECONDS)) {	}	} catch (InterruptedException e) {	
the inmemoryscmstore was interrupted while shutting down the app check task 

protected void serviceStop() throws Exception {	if (scheduler != null) {	scheduler.shutdownNow();	try {	if (!scheduler.awaitTermination(10, TimeUnit.SECONDS)) {	}	} catch (InterruptedException e) {	}	
the background thread stopped 

private void bootstrap(Configuration conf) throws IOException {	Map<String, String> initialCachedResources = getInitialCachedResources(FileSystem.get(conf), conf);	
bootstrapping from cache resources located in the file system 

Map<String, String> initialCachedResources = getInitialCachedResources(FileSystem.get(conf), conf);	Iterator<Map.Entry<String, String>> it = initialCachedResources.entrySet().iterator();	while (it.hasNext()) {	Map.Entry<String, String> e = it.next();	String key = intern(e.getKey());	String fileName = e.getValue();	SharedCacheResource resource = new SharedCacheResource(fileName);	cachedResources.put(key, resource);	it.remove();	}	
bootstrapping complete 

public void run() {	try {	
checking the initial app list for finished applications 

public void run() {	try {	synchronized (initialAppsLock) {	if (initialApps.isEmpty()) {	} else {	
looking into apps to see if they are still active 

if (initialApps.isEmpty()) {	} else {	Iterator<ApplicationId> it = initialApps.iterator();	while (it.hasNext()) {	ApplicationId id = it.next();	try {	if (!taskAppChecker.isApplicationActive(id)) {	it.remove();	}	} catch (YarnException e) {	
exception while checking the app status will leave the entry in the list 

while (it.hasNext()) {	ApplicationId id = it.next();	try {	if (!taskAppChecker.isApplicationActive(id)) {	it.remove();	}	} catch (YarnException e) {	}	}	}	
there are now entries in the list 

try {	if (!taskAppChecker.isApplicationActive(id)) {	it.remove();	}	} catch (YarnException e) {	}	}	}	}	} catch (Throwable e) {	
unexpected exception thrown during in memory store app check task rescheduling task 

========================= hadoop sample_405 =========================

job.setOutputKeyClass(BytesWritable.class);	job.setOutputValueClass(BytesWritable.class);	JobClient client = new JobClient(job);	ClusterStatus cluster = client.getClusterStatus();	long totalDataSize = dataSizePerMap * numMapsPerHost job.set("test.tmb.bytes_per_map", String.valueOf(dataSizePerMap * 1024 * 1024));	job.setNumReduceTasks(0);	job.setNumMapTasks(numMapsPerHost * cluster.getTaskTrackers());	FileOutputFormat.setOutputPath(job, INPUT_DIR);	FileSystem fs = FileSystem.get(job);	fs.delete(BASE_DIR, true);	
generating random input for the benchmark 

job.setOutputKeyClass(BytesWritable.class);	job.setOutputValueClass(BytesWritable.class);	JobClient client = new JobClient(job);	ClusterStatus cluster = client.getClusterStatus();	long totalDataSize = dataSizePerMap * numMapsPerHost job.set("test.tmb.bytes_per_map", String.valueOf(dataSizePerMap * 1024 * 1024));	job.setNumReduceTasks(0);	job.setNumMapTasks(numMapsPerHost * cluster.getTaskTrackers());	FileOutputFormat.setOutputPath(job, INPUT_DIR);	FileSystem fs = FileSystem.get(job);	fs.delete(BASE_DIR, true);	
total data mb 

job.setOutputKeyClass(BytesWritable.class);	job.setOutputValueClass(BytesWritable.class);	JobClient client = new JobClient(job);	ClusterStatus cluster = client.getClusterStatus();	long totalDataSize = dataSizePerMap * numMapsPerHost job.set("test.tmb.bytes_per_map", String.valueOf(dataSizePerMap * 1024 * 1024));	job.setNumReduceTasks(0);	job.setNumMapTasks(numMapsPerHost * cluster.getTaskTrackers());	FileOutputFormat.setOutputPath(job, INPUT_DIR);	FileSystem fs = FileSystem.get(job);	fs.delete(BASE_DIR, true);	
data per map mb 

job.setOutputKeyClass(BytesWritable.class);	job.setOutputValueClass(BytesWritable.class);	JobClient client = new JobClient(job);	ClusterStatus cluster = client.getClusterStatus();	long totalDataSize = dataSizePerMap * numMapsPerHost job.set("test.tmb.bytes_per_map", String.valueOf(dataSizePerMap * 1024 * 1024));	job.setNumReduceTasks(0);	job.setNumMapTasks(numMapsPerHost * cluster.getTaskTrackers());	FileOutputFormat.setOutputPath(job, INPUT_DIR);	FileSystem fs = FileSystem.get(job);	fs.delete(BASE_DIR, true);	
number of spills 

job.setOutputKeyClass(BytesWritable.class);	job.setOutputValueClass(BytesWritable.class);	JobClient client = new JobClient(job);	ClusterStatus cluster = client.getClusterStatus();	long totalDataSize = dataSizePerMap * numMapsPerHost job.set("test.tmb.bytes_per_map", String.valueOf(dataSizePerMap * 1024 * 1024));	job.setNumReduceTasks(0);	job.setNumMapTasks(numMapsPerHost * cluster.getTaskTrackers());	FileOutputFormat.setOutputPath(job, INPUT_DIR);	FileSystem fs = FileSystem.get(job);	fs.delete(BASE_DIR, true);	
number of maps per host 

job.setOutputKeyClass(BytesWritable.class);	job.setOutputValueClass(BytesWritable.class);	JobClient client = new JobClient(job);	ClusterStatus cluster = client.getClusterStatus();	long totalDataSize = dataSizePerMap * numMapsPerHost job.set("test.tmb.bytes_per_map", String.valueOf(dataSizePerMap * 1024 * 1024));	job.setNumReduceTasks(0);	job.setNumMapTasks(numMapsPerHost * cluster.getTaskTrackers());	FileOutputFormat.setOutputPath(job, INPUT_DIR);	FileSystem fs = FileSystem.get(job);	fs.delete(BASE_DIR, true);	
number of hosts 

public int run (String[] args) throws Exception {	
starting the benchmark for threaded spills 

job.setReducerClass(IdentityReducer.class);	FileInputFormat.addInputPath(job, INPUT_DIR);	FileOutputFormat.setOutputPath(job, OUTPUT_DIR);	JobClient client = new JobClient(job);	ClusterStatus cluster = client.getClusterStatus();	job.setNumMapTasks(numMapsPerHost * cluster.getTaskTrackers());	job.setNumReduceTasks(1);	int ioSortMb = (int)Math.ceil(FACTOR * dataSizePerMap);	job.set(JobContext.IO_SORT_MB, String.valueOf(ioSortMb));	fs = FileSystem.get(job);	
running sort with spill per map 

JobClient client = new JobClient(job);	ClusterStatus cluster = client.getClusterStatus();	job.setNumMapTasks(numMapsPerHost * cluster.getTaskTrackers());	job.setNumReduceTasks(1);	int ioSortMb = (int)Math.ceil(FACTOR * dataSizePerMap);	job.set(JobContext.IO_SORT_MB, String.valueOf(ioSortMb));	fs = FileSystem.get(job);	long startTime = System.currentTimeMillis();	JobClient.runJob(job);	long endTime = System.currentTimeMillis();	
total time taken millisec 

job.set(JobContext.IO_SORT_MB, String.valueOf(ioSortMb));	fs = FileSystem.get(job);	long startTime = System.currentTimeMillis();	JobClient.runJob(job);	long endTime = System.currentTimeMillis();	fs.delete(OUTPUT_DIR, true);	JobConf spilledJob = new JobConf(job, ThreadedMapBenchmark.class);	ioSortMb = (int)Math.ceil(FACTOR spilledJob.set(JobContext.IO_SORT_MB, String.valueOf(ioSortMb));	spilledJob.setJobName("threaded-map-benchmark-spilled");	spilledJob.setJarByClass(ThreadedMapBenchmark.class);	
running sort with spills per map 

JobClient.runJob(job);	long endTime = System.currentTimeMillis();	fs.delete(OUTPUT_DIR, true);	JobConf spilledJob = new JobConf(job, ThreadedMapBenchmark.class);	ioSortMb = (int)Math.ceil(FACTOR spilledJob.set(JobContext.IO_SORT_MB, String.valueOf(ioSortMb));	spilledJob.setJobName("threaded-map-benchmark-spilled");	spilledJob.setJarByClass(ThreadedMapBenchmark.class);	startTime = System.currentTimeMillis();	JobClient.runJob(spilledJob);	endTime = System.currentTimeMillis();	
total time taken millisec 

========================= hadoop sample_5521 =========================

public synchronized int read(byte[] buf, int off, int len) throws IOException {	UUID randomId = (LOG.isTraceEnabled() ? UUID.randomUUID() : null);	
starting read file from datanode 

public synchronized int read(byte[] buf, int off, int len) throws IOException {	UUID randomId = (LOG.isTraceEnabled() ? UUID.randomUUID() : null);	if (curDataSlice == null || curDataSlice.remaining() == 0 && bytesNeededToFinish > 0) {	try (TraceScope ignored = tracer.newScope( "BlockReaderRemote2#readNextPacket(" + blockId + ")")) {	readNextPacket();	}	}	
finishing read 

private void readNextPacket() throws IOException {	packetReceiver.receiveNextPacket(in);	PacketHeader curHeader = packetReceiver.getHeader();	curDataSlice = packetReceiver.getDataSlice();	assert curDataSlice.capacity() == curHeader.getDataLen();	
dfsclient readnextpacket got header 

private void readTrailingEmptyPacket() throws IOException {	
reading empty packet at end of read 

========================= hadoop sample_7009 =========================

public static void delete(FileSystem fs, String path) {	try {	if (fs != null) {	if (path != null) {	fs.delete(new Path(path), true);	}	}	} catch (IOException e) {	
exception encountered 

public static void createDirectory(FileSystem fs, Path dirPath) throws IOException {	fs.delete(dirPath, true);	boolean created = fs.mkdirs(dirPath);	if (!created) {	
could not create directory this might cause test failures 

========================= hadoop sample_6248 =========================

private boolean shouldRunTest() throws MojoExecutionException {	String skipTests = session. getSystemProperties().getProperty("skipTests");	if (isTruthy(skipTests)) {	
skiptests is in effect for test 

private boolean shouldRunTest() throws MojoExecutionException {	String skipTests = session. getSystemProperties().getProperty("skipTests");	if (isTruthy(skipTests)) {	return false;	}	if (!binary.exists()) {	if (skipIfMissing) {	
skipping missing test 

if (test.equals(ALL_NATIVE)) {	found = true;	break;	}	if (test.equals(testName)) {	found = true;	break;	}	}	if (!found) {	
did not find test in list 

if (preconditions != null) {	int idx = 1;	for (Map.Entry<String, String> entry : preconditions.entrySet()) {	String key = entry.getKey();	String val = entry.getValue();	if (key == null) {	throw new MojoExecutionException("NULL is not a valid " + "precondition type.  " + VALID_PRECONDITION_TYPES_STR);	}	if (key.equals("and")) {	if (!isTruthy(val)) {	
skipping test because precondition number was not met 

String val = entry.getValue();	if (key == null) {	throw new MojoExecutionException("NULL is not a valid " + "precondition type.  " + VALID_PRECONDITION_TYPES_STR);	}	if (key.equals("and")) {	if (!isTruthy(val)) {	return false;	}	} else if (key.equals("andNot")) {	if (isTruthy(val)) {	
skipping test because negative precondition number was met 

return;	}	if (!results.isDirectory()) {	if (!results.mkdirs()) {	throw new MojoExecutionException("Failed to create " + "output directory '" + results + "'!");	}	}	List<String> cmd = new LinkedList<String>();	cmd.add(binary.getAbsolutePath());	getLog().info("-------------------------------------------------------");	
c m a k e b u i l d e r t e s t 

bld.append(" ").append(entry);	}	getLog().info(bld.toString());	ProcessBuilder pb = new ProcessBuilder(cmd);	Exec.addEnvironment(pb, env);	if (workingDirectory != null) {	pb.directory(workingDirectory);	}	pb.redirectError(new File(results, testName + ".stderr"));	pb.redirectOutput(new File(results, testName + ".stdout"));	
with extra environment variables 

} catch (IOException e) {	throw new MojoExecutionException("IOException while executing the test " + testName, e);	} catch (InterruptedException e) {	throw new MojoExecutionException("Interrupted while executing " + "the test " + testName, e);	} finally {	if (testThread != null) {	testThread.interrupt();	try {	testThread.join();	} catch (InterruptedException e) {	
interrupted while waiting for testthread 

}	status = "TIMED OUT";	} else if (retCode == 0) {	status = "SUCCESS";	} else {	status = "ERROR CODE " + String.valueOf(retCode);	}	try {	writeStatusFile(status);	} catch (Exception e) {	
failed to write status file 

}	try {	writeStatusFile(status);	} catch (Exception e) {	}	if (proc != null) {	proc.destroy();	}	}	long end = System.nanoTime();	
status after millisecond s 

========================= hadoop sample_4432 =========================

List<String> commands = Arrays.asList(Shell.getRunScriptCommand(scriptFile));	containerLaunchContext.setCommands(commands);	StartContainerRequest scRequest = StartContainerRequest.newInstance(containerLaunchContext, createContainerToken(cId, DUMMY_RM_IDENTIFIER, context.getNodeId(), user, context.getContainerTokenSecretManager()));	List<StartContainerRequest> list = new ArrayList<>();	list.add(scRequest);	StartContainersRequest allRequests = StartContainersRequest.newInstance(list);	containerManager.startContainers(allRequests);	int timeoutSecs = 0;	while (!processStartFile.exists() && timeoutSecs++ < 20) {	Thread.sleep(1000);	
waiting for process start file to be created 

private String doRestartTests(ContainerId cId, File oldStartFile, String testString, String pid, boolean canRollback) throws YarnException, IOException, InterruptedException {	int beforeRestart = metrics.getRunningContainers();	Container container = containerManager.getContext().getContainers().get(cId);	Assert.assertFalse(container.isReInitializing());	containerManager.restartContainer(cId);	Assert.assertTrue(container.isReInitializing());	int timeoutSecs = 0;	while (DefaultContainerExecutor.containerIsAlive(pid) && (metrics.getRunningContainers() == beforeRestart) && container.isReInitializing() && timeoutSecs++ < 20) {	Thread.sleep(1000);	
waiting for original process to die and new process to start 

containerManager.restartContainer(cId);	Assert.assertTrue(container.isReInitializing());	int timeoutSecs = 0;	while (DefaultContainerExecutor.containerIsAlive(pid) && (metrics.getRunningContainers() == beforeRestart) && container.isReInitializing() && timeoutSecs++ < 20) {	Thread.sleep(1000);	}	Assert.assertFalse("Old Process Still alive!!", DefaultContainerExecutor.containerIsAlive(pid));	String newPid = null;	timeoutSecs = 0;	while (timeoutSecs++ < 20) {	
waiting for new process file to be created 

File oldStartFile = new File(tmpDir, "start_file_o.txt").getAbsoluteFile();	oldStartFile.delete();	ContainerId cId = createContainerId(0);	containerManager.rollbackLastReInitialization(cId);	Container container = containerManager.getContext().getContainers().get(cId);	Assert.assertTrue(container.isReInitializing());	Assert.assertFalse("Original Process is still alive!", DefaultContainerExecutor.containerIsAlive(pids[0]));	int timeoutSecs = 0;	while (container.isReInitializing() && timeoutSecs++ < 20) {	Thread.sleep(1000);	
waiting for reinitialization to complete 

Assert.assertTrue(container.isReInitializing());	Assert.assertFalse("Original Process is still alive!", DefaultContainerExecutor.containerIsAlive(pids[0]));	int timeoutSecs = 0;	while (container.isReInitializing() && timeoutSecs++ < 20) {	Thread.sleep(1000);	}	Assert.assertFalse(container.isReInitializing());	timeoutSecs = 0;	while (!oldStartFile.exists() && timeoutSecs++ < 20) {	Thread.sleep(1000);	
waiting for new process start file to be created 

ContainerId cId = createContainerId(0);	File oldStartFile = new File(tmpDir, "start_file_o.txt").getAbsoluteFile();	String pid = prepareInitialContainer(cId, oldStartFile);	File newStartFile = new File(tmpDir, "start_file_n.txt").getAbsoluteFile();	prepareContainerUpgrade(false, true, false, cId, newStartFile);	Assert.assertFalse("Original Process is still alive!", DefaultContainerExecutor.containerIsAlive(pid));	int timeoutSecs = 0;	while (!oldStartFile.exists() && timeoutSecs++ < 20) {	System.out.println("\nFiles: " + Arrays.toString(oldStartFile.getParentFile().list()));	Thread.sleep(1000);	
waiting for new process start file to be created 

containerManager.reInitializeContainer(cId, containerLaunchContext, autoCommit);	try {	containerManager.reInitializeContainer(cId, containerLaunchContext, autoCommit);	} catch (Exception e) {	Assert.assertTrue(e.getMessage().contains("Cannot perform RE_INIT"));	}	int timeoutSecs = 0;	int maxTimeToWait = failLoc ? 10 : 20;	while (!startFile.exists() && timeoutSecs++ < maxTimeToWait) {	Thread.sleep(1000);	
waiting for new process start file to be created 

writeScriptFile(fileWriterOld, "Hello World!", startFile, cId, false);	ContainerLaunchContext containerLaunchContext = prepareContainerLaunchContext(scriptFileOld, "dest_file", false, 4);	StartContainerRequest scRequest = StartContainerRequest.newInstance(containerLaunchContext, createContainerToken(cId, DUMMY_RM_IDENTIFIER, context.getNodeId(), user, context.getContainerTokenSecretManager()));	List<StartContainerRequest> list = new ArrayList<>();	list.add(scRequest);	StartContainersRequest allRequests = StartContainersRequest.newInstance(list);	containerManager.startContainers(allRequests);	int timeoutSecs = 0;	while (!startFile.exists() && timeoutSecs++ < 20) {	Thread.sleep(1000);	
waiting for process start file to be created 

commands.add(scriptFile.getAbsolutePath());	containerLaunchContext.setCommands(commands);	StartContainerRequest scRequest = StartContainerRequest.newInstance( containerLaunchContext, createContainerToken(cId, DUMMY_RM_IDENTIFIER, context.getNodeId(), user, context.getContainerTokenSecretManager()));	List<StartContainerRequest> list = new ArrayList<>();	list.add(scRequest);	StartContainersRequest allRequests = StartContainersRequest.newInstance(list);	containerManager.startContainers(allRequests);	int timeoutSecs = 0;	while (!processStartFile.exists() && timeoutSecs++ < 20) {	Thread.sleep(1000);	
waiting for process start file to be created 

========================= hadoop sample_1630 =========================

public long parseForStore(TimelineDataManager tdm, Path appDirPath, boolean appCompleted, JsonFactory jsonFactory, ObjectMapper objMapper, FileSystem fs) throws IOException {	
parsing for log dir on attempt 

public long parseForStore(TimelineDataManager tdm, Path appDirPath, boolean appCompleted, JsonFactory jsonFactory, ObjectMapper objMapper, FileSystem fs) throws IOException {	Path logPath = getPath(appDirPath);	FileStatus status = fs.getFileStatus(logPath);	long numParsed = 0;	if (status != null) {	long startTime = Time.monotonicNow();	try {	
parsing at offset 

public long parseForStore(TimelineDataManager tdm, Path appDirPath, boolean appCompleted, JsonFactory jsonFactory, ObjectMapper objMapper, FileSystem fs) throws IOException {	Path logPath = getPath(appDirPath);	FileStatus status = fs.getFileStatus(logPath);	long numParsed = 0;	if (status != null) {	long startTime = Time.monotonicNow();	try {	long count = parsePath(tdm, logPath, appCompleted, jsonFactory, objMapper, fs);	
parsed entities from in msec 

Path logPath = getPath(appDirPath);	FileStatus status = fs.getFileStatus(logPath);	long numParsed = 0;	if (status != null) {	long startTime = Time.monotonicNow();	try {	long count = parsePath(tdm, logPath, appCompleted, jsonFactory, objMapper, fs);	numParsed += count;	} catch (RuntimeException e) {	if (e.getCause() instanceof JsonParseException && (status.getLen() > 0 || offset > 0)) {	
log appears to be corrupted skip 

if (status != null) {	long startTime = Time.monotonicNow();	try {	long count = parsePath(tdm, logPath, appCompleted, jsonFactory, objMapper, fs);	numParsed += count;	} catch (RuntimeException e) {	if (e.getCause() instanceof JsonParseException && (status.getLen() > 0 || offset > 0)) {	}	}	} else {	
no longer exists skip for scanning 

ArrayList<TimelineEntity> entityList = new ArrayList<TimelineEntity>(1);	long bytesParsed;	long bytesParsedLastBatch = 0;	boolean postError = false;	try {	MappingIterator<TimelineEntity> iter = objMapper.readValues(parser, TimelineEntity.class);	while (iter.hasNext()) {	TimelineEntity entity = iter.next();	String etype = entity.getEntityType();	String eid = entity.getEntityId();	
read entity 

long bytesParsedLastBatch = 0;	boolean postError = false;	try {	MappingIterator<TimelineEntity> iter = objMapper.readValues(parser, TimelineEntity.class);	while (iter.hasNext()) {	TimelineEntity entity = iter.next();	String etype = entity.getEntityType();	String eid = entity.getEntityId();	++count;	bytesParsed = parser.getCurrentLocation().getCharOffset() + 1;	
parser now at offset 

boolean postError = false;	try {	MappingIterator<TimelineEntity> iter = objMapper.readValues(parser, TimelineEntity.class);	while (iter.hasNext()) {	TimelineEntity entity = iter.next();	String etype = entity.getEntityType();	String eid = entity.getEntityId();	++count;	bytesParsed = parser.getCurrentLocation().getCharOffset() + 1;	try {	
adding to store 

TimelineEntity entity = iter.next();	String etype = entity.getEntityType();	String eid = entity.getEntityId();	++count;	bytesParsed = parser.getCurrentLocation().getCharOffset() + 1;	try {	entityList.add(entity);	entities.setEntities(entityList);	TimelinePutResponse response = tdm.postEntities(entities, ugi);	for (TimelinePutResponse.TimelinePutError e : response.getErrors()) {	
error putting entity 

protected long doParse(TimelineDataManager tdm, JsonParser parser, ObjectMapper objMapper, UserGroupInformation ugi, boolean appCompleted) throws IOException {	long count = 0;	long bytesParsed;	long bytesParsedLastBatch = 0;	boolean putError = false;	try {	MappingIterator<TimelineDomain> iter = objMapper.readValues(parser, TimelineDomain.class);	while (iter.hasNext()) {	TimelineDomain domain = iter.next();	domain.setOwner(ugi.getShortUserName());	
read domain 

long bytesParsed;	long bytesParsedLastBatch = 0;	boolean putError = false;	try {	MappingIterator<TimelineDomain> iter = objMapper.readValues(parser, TimelineDomain.class);	while (iter.hasNext()) {	TimelineDomain domain = iter.next();	domain.setOwner(ugi.getShortUserName());	++count;	bytesParsed = parser.getCurrentLocation().getCharOffset() + 1;	
parser now at offset 

========================= hadoop sample_307 =========================

protected void dispatch(Event event) {	
dispatching the event 

========================= hadoop sample_2159 =========================

List<String> localDirs = dirsHandler.getLocalDirs();	List<String> logDirs = dirsHandler.getLogDirs();	List<String> containerLocalDirs = getContainerLocalDirs(localDirs);	List<String> containerLogDirs = getContainerLogDirs(logDirs);	if (!dirsHandler.areDisksHealthy()) {	ret = ContainerExitStatus.DISKS_FAILED;	throw new IOException("Most of the disks failed. " + dirsHandler.getDisksHealthReport(false));	}	ret = launchContainer(new ContainerStartContext.Builder() .setContainer(container) .setLocalizedResources(localResources) .setNmPrivateContainerScriptPath(nmPrivateContainerScriptPath) .setNmPrivateTokensPath(nmPrivateTokensPath) .setUser(container.getUser()) .setAppId(appIdStr) .setContainerWorkDir(containerWorkDir) .setLocalDirs(localDirs) .setLogDirs(logDirs) .setContainerLocalDirs(containerLocalDirs) .setContainerLogDirs(containerLogDirs) .build());	} catch (ConfigurationException e) {	
failed to launch container due to configuration error 

if (!dirsHandler.areDisksHealthy()) {	ret = ContainerExitStatus.DISKS_FAILED;	throw new IOException("Most of the disks failed. " + dirsHandler.getDisksHealthReport(false));	}	ret = launchContainer(new ContainerStartContext.Builder() .setContainer(container) .setLocalizedResources(localResources) .setNmPrivateContainerScriptPath(nmPrivateContainerScriptPath) .setNmPrivateTokensPath(nmPrivateTokensPath) .setUser(container.getUser()) .setAppId(appIdStr) .setContainerWorkDir(containerWorkDir) .setLocalDirs(localDirs) .setLogDirs(logDirs) .setContainerLocalDirs(containerLocalDirs) .setContainerLogDirs(containerLogDirs) .build());	} catch (ConfigurationException e) {	dispatcher.getEventHandler().handle(new ContainerExitEvent( containerId, ContainerEventType.CONTAINER_EXITED_WITH_FAILURE, ret, e.getMessage()));	getContext().getNodeStatusUpdater().reportException(e);	return ret;	} catch (Throwable e) {	
failed to relaunch container 

private void deleteAsUser(Path path) {	try {	exec.deleteAsUser(new DeletionAsUserContext.Builder() .setUser(container.getUser()) .setSubDir(path) .build());	} catch (Exception e) {	
failed to delete 

========================= hadoop sample_1775 =========================

protected void serviceStart() throws Exception {	this.scmClient = createClientProxy();	if (LOG.isDebugEnabled()) {	
connecting to shared cache manager at 

========================= hadoop sample_2620 =========================

public void testNameDirError() throws IOException {	
starting testnamedirerror 

public void testWriteTransactionIdHandlesIOE() throws Exception {	
check ioexception handled correctly by writetransactionidfile 

public void testSecondaryNamenodeError1() throws IOException {	
starting 

public void testSecondaryNamenodeError2() throws IOException {	
starting 

public void testSecondaryNamenodeError3() throws IOException {	
starting 

private void doSecondaryFailsToReturnImage() throws IOException {	
starting testsecondaryfailstoreturnimage 

public void testNameNodeImageSendFailWrongSize() throws IOException {	
starting testnamenodeimagesendfailwrongsize 

public void testNameNodeImageSendFailWrongDigest() throws IOException {	
starting testnamenodeimagesendfailwrongdigest 

secondary = startSecondaryNameNode(conf);	NNStorage storage = secondary.getFSImage().getStorage();	for (StorageDirectory sd : storage.dirIterable(null)) {	assertLockFails(sd);	savedSd = sd;	}	LOG.info("===> Shutting down first 2NN");	secondary.shutdown();	secondary = null;	LOG.info("===> Locking a dir, starting second 2NN");	
trying to lock 

cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0).build();	nameDirs = cluster.getNameDirs(0);	cluster.getFileSystem().mkdirs(testPath);	snn = startSecondaryNameNode(conf);	snn.doCheckpoint();	} finally {	cleanup(snn);	cleanup(cluster);	cluster = null;	}	
trying to import checkpoint when the namenode already contains an image this should fail 

}	try {	cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0).format(false) .startupOption(StartupOption.IMPORT).build();	fail("NameNode did not fail to start when it already contained " + "an image");	} catch (IOException ioe) {	GenericTestUtils.assertExceptionContains( "NameNode already contains an image", ioe);	} finally {	cleanup(cluster);	cluster = null;	}	
removing nn storage contents 

cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0).format(false) .startupOption(StartupOption.IMPORT).build();	fail("NameNode did not fail to start when it already contained " + "an image");	} catch (IOException ioe) {	GenericTestUtils.assertExceptionContains( "NameNode already contains an image", ioe);	} finally {	cleanup(cluster);	cluster = null;	}	for(URI uri : nameDirs) {	File dir = new File(uri.getPath());	
cleaning 

} catch (IOException ioe) {	GenericTestUtils.assertExceptionContains( "NameNode already contains an image", ioe);	} finally {	cleanup(cluster);	cluster = null;	}	for(URI uri : nameDirs) {	File dir = new File(uri.getPath());	removeAndRecreateDir(dir);	}	
trying to import checkpoint 

fs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);	try {	admin.run(args);	} catch(Exception e) {	throw new IOException(e);	}	final int EXPECTED_TXNS_FIRST_SEG = 13;	for(URI uri : editsDirs) {	File ed = new File(uri.getPath());	File curDir = new File(ed, "current");	
files in 

SecondaryNameNode secondary = null;	Configuration conf = new HdfsConfiguration();	try {	cluster = new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes) .format(true).build();	secondary = startSecondaryNameNode(conf);	Mockito.doThrow(new IOException( "Injecting failure after rolling edit logs")) .when(faultInjector).afterSecondaryCallsRollEditLog();	try {	secondary.doCheckpoint();	fail("Should have failed upload");	} catch (IOException ioe) {	
got expected failure 

try {	secondary.doCheckpoint();	fail("Should have failed upload");	} catch (IOException ioe) {	assertTrue(ioe.toString().contains("Injecting failure"));	}	try {	secondary.doCheckpoint();	fail("Should have failed upload");	} catch (IOException ioe) {	
got expected failure 

public void testSecondaryImageDownload() throws IOException {	
starting testsecondaryimagedownload 

cluster = null;	try {	Thread.sleep(100);	} catch (InterruptedException ie) {	}	cluster = new MiniDFSCluster.Builder(conf) .numDataNodes(0) .nameNodePort(origPort) .nameNodeHttpPort(origHttpPort) .format(true).build();	try {	secondary.doCheckpoint();	fail("Should have failed checkpoint against a different namespace");	} catch (IOException ioe) {	
got expected failure 

conf.setInt(DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_TXNS_KEY, 10);	conf.setInt(DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_CHECK_PERIOD_KEY, 1);	try {	cluster = new MiniDFSCluster.Builder(conf) .numDataNodes(0) .format(true).build();	FileSystem fs = cluster.getFileSystem();	secondary = startSecondaryNameNode(conf);	secondary.startCheckpointThread();	final NNStorage storage = secondary.getFSImage().getStorage();	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	
waiting for checkpoint txn id to go to 

GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	return storage.getMostRecentCheckpointTxId() == 2;	}	}, 200, 15000);	for (int i = 0; i < 10; i++) {	fs.mkdirs(new Path("/test" + i));	}	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	
waiting for checkpoint txn id to go 

assertEquals(SecondaryNameNode.CommandLineOpts.Command.CHECKPOINT, opts.getCommand());	assertTrue(opts.shouldForceCheckpoint());	opts.parse("-geteditsize");	assertEquals(SecondaryNameNode.CommandLineOpts.Command.GETEDITSIZE, opts.getCommand());	opts.parse("-format");	assertTrue(opts.shouldFormat());	try {	opts.parse("-geteditsize", "-checkpoint");	fail("Should have failed bad parsing for two actions");	} catch (ParseException e) {	
encountered 

assertTrue(opts.shouldFormat());	try {	opts.parse("-geteditsize", "-checkpoint");	fail("Should have failed bad parsing for two actions");	} catch (ParseException e) {	}	try {	opts.parse("-checkpoint", "xx");	fail("Should have failed for bad checkpoint arg");	} catch (ParseException e) {	
encountered 

private static void cleanup(SecondaryNameNode snn) {	if (snn != null) {	try {	snn.shutdown();	} catch (Exception e) {	
could not shut down secondary namenode 

private static void cleanup(MiniDFSCluster cluster) {	if (cluster != null) {	try {	cluster.shutdown();	} catch (Exception e) {	
could not shutdown minidfscluster 

========================= hadoop sample_7422 =========================

public static Map<?, ?> parse(final String jsonString) throws IOException {	try {	ObjectMapper mapper = new ObjectMapper();	return mapper.reader(Map.class).readValue(jsonString);	} catch (Exception e) {	
json parsing exception while parsing 

public static Map<?, ?> parse(final String jsonString) throws IOException {	try {	ObjectMapper mapper = new ObjectMapper();	return mapper.reader(Map.class).readValue(jsonString);	} catch (Exception e) {	if (jsonString.toLowerCase(Locale.ENGLISH).contains("server error")) {	
internal server error was encountered while making a request 

========================= hadoop sample_6399 =========================

private IOStreamPair getEncryptedStreams(Peer peer, OutputStream underlyingOut, InputStream underlyingIn) throws IOException {	if (peer.hasSecureChannel() || dnConf.getTrustedChannelResolver().isTrusted(getPeerAddress(peer))) {	return new IOStreamPair(underlyingIn, underlyingOut);	}	Map<String, String> saslProps = createSaslPropertiesForEncryption( dnConf.getEncryptionAlgorithm());	if (LOG.isDebugEnabled()) {	
server using encryption algorithm 

localResponse = sasl.evaluateChallengeOrResponse(remoteResponse);	checkSaslComplete(sasl, saslProps);	CipherOption cipherOption = null;	if (sasl.isNegotiatedQopPrivacy()) {	Configuration conf = dnConf.getConf();	cipherOption = negotiateCipherOption(conf, cipherOptions);	if (LOG.isDebugEnabled()) {	if (cipherOption == null) {	String cipherSuites = conf.get(DFS_ENCRYPT_DATA_TRANSFER_CIPHER_SUITES_KEY);	if (cipherSuites != null && !cipherSuites.isEmpty()) {	
server accepts cipher suites but client does not accept any of them 

CipherOption cipherOption = null;	if (sasl.isNegotiatedQopPrivacy()) {	Configuration conf = dnConf.getConf();	cipherOption = negotiateCipherOption(conf, cipherOptions);	if (LOG.isDebugEnabled()) {	if (cipherOption == null) {	String cipherSuites = conf.get(DFS_ENCRYPT_DATA_TRANSFER_CIPHER_SUITES_KEY);	if (cipherSuites != null && !cipherSuites.isEmpty()) {	}	} else {	
server using cipher suite with client 

========================= hadoop sample_8368 =========================

public RMContainerTokenSecretManager(Configuration conf) {	super(conf);	this.timer = new Timer();	this.rollingInterval = conf.getLong( YarnConfiguration.RM_CONTAINER_TOKEN_MASTER_KEY_ROLLING_INTERVAL_SECS, YarnConfiguration.DEFAULT_RM_CONTAINER_TOKEN_MASTER_KEY_ROLLING_INTERVAL_SECS) * 1000;	this.activationDelay = (long) (conf.getLong(YarnConfiguration.RM_NM_EXPIRY_INTERVAL_MS, YarnConfiguration.DEFAULT_RM_NM_EXPIRY_INTERVAL_MS) * 1.5);	
containertokenkeyrollinginterval ms and containertokenkeyactivationdelay ms 

public void rollMasterKey() {	super.writeLock.lock();	try {	
rolling master key for container tokens 

public void rollMasterKey() {	super.writeLock.lock();	try {	if (this.currentMasterKey == null) {	this.currentMasterKey = createNewMasterKey();	} else {	this.nextMasterKey = createNewMasterKey();	
going to activate master key with key id in ms 

public void activateNextMasterKey() {	super.writeLock.lock();	try {	
activating next master key with id 

========================= hadoop sample_700 =========================

static long delete(FSDirectory fsd, INodesInPath iip, BlocksMapUpdateInfo collectedBlocks, List<INode> removedINodes, List<Long> removedUCFiles, long mtime) throws IOException {	if (NameNode.stateChangeLog.isDebugEnabled()) {	
dir fsdirectory delete 

static BlocksMapUpdateInfo deleteInternal( FSNamesystem fsn, INodesInPath iip, boolean logRetryCache) throws IOException {	assert fsn.hasWriteLock();	if (NameNode.stateChangeLog.isDebugEnabled()) {	
dir namesystem delete 

List<Long> removedUCFiles = new ChunkedArrayList<>();	long mtime = now();	long filesRemoved = delete( fsd, iip, collectedBlocks, removedINodes, removedUCFiles, mtime);	if (filesRemoved < 0) {	return null;	}	fsd.getEditLog().logDelete(iip.getPath(), mtime, logRetryCache);	incrDeletedFileCount(filesRemoved);	fsn.removeLeasesAndINodes(removedUCFiles, removedINodes, true);	if (NameNode.stateChangeLog.isDebugEnabled()) {	
dir namesystem delete is removed 

private static boolean deleteAllowed(final INodesInPath iip) {	if (iip.length() < 1 || iip.getLastINode() == null) {	if (NameNode.stateChangeLog.isDebugEnabled()) {	
dir fsdirectory unprotecteddelete failed to remove because it does not exist 

private static boolean deleteAllowed(final INodesInPath iip) {	if (iip.length() < 1 || iip.getLastINode() == null) {	if (NameNode.stateChangeLog.isDebugEnabled()) {	}	return false;	} else if (iip.length() == 1) {	
dir fsdirectory unprotecteddelete failed to remove because the root is not allowed to be deleted 

return false;	}	final INodeDirectory parent = targetNode.getParent();	parent.updateModificationTime(mtime, latestSnapshot);	if (!targetNode.isInLatestSnapshot(latestSnapshot)) {	targetNode.destroyAndCollectBlocks(reclaimContext);	} else {	targetNode.cleanSubtree(reclaimContext, CURRENT_STATE_ID, latestSnapshot);	}	if (NameNode.stateChangeLog.isDebugEnabled()) {	
dir fsdirectory unprotecteddelete is removed 

========================= hadoop sample_8016 =========================

private void verifyFileContent(DistributedFileSystem fs, SlowWriter[] slowwriters) throws IOException {	
verify the file 

static void sleepSeconds(final int waittime) throws InterruptedException {	
wait seconds 

========================= hadoop sample_7619 =========================

public void setup() throws Exception {	if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {	
mrappjar not found not running test 

public void testJobWithNonNormalizedCapabilities() throws Exception {	if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {	
mrappjar not found not running test 

public void tearDown() {	if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {	
mrappjar not found not running test 

========================= hadoop sample_5624 =========================

public Token<MRDelegationTokenIdentifier> selectToken(Text service, Collection<Token<? extends TokenIdentifier>> tokens) {	if (service == null) {	return null;	}	
looking for a token with service 

public Token<MRDelegationTokenIdentifier> selectToken(Text service, Collection<Token<? extends TokenIdentifier>> tokens) {	if (service == null) {	return null;	}	for (Token<? extends TokenIdentifier> token : tokens) {	if (LOG.isDebugEnabled()) {	
token kind is and the token s service name is 

========================= hadoop sample_4465 =========================

public AmazonDynamoDB createDynamoDBClient(String defaultRegion) throws IOException {	Preconditions.checkNotNull(getConf(), "Should have been configured before usage");	final Configuration conf = getConf();	final AWSCredentialsProvider credentials = createAWSCredentialProviderSet(null, conf);	final ClientConfiguration awsConf = DefaultS3ClientFactory.createAwsConf(conf);	final String region = getRegion(conf, defaultRegion);	
creating dynamodb client in region 

========================= hadoop sample_6008 =========================

fs.mkdirs(new Path("/tmp/in/1"));	target = new Path("/tmp/out/1");	fs.create(target).close();	options = new DistCpOptions(srcPaths, target);	try {	validatePaths(options);	Assert.fail("Invalid inputs accepted");	} catch (InvalidInputException ignore) { }	TestDistCpUtils.delete(fs, "/tmp");	} catch (IOException e) {	
exception encountered 

Path target = new Path("/tmp/out");	Path listingFile = new Path("/tmp/list");	DistCpOptions options = new DistCpOptions(srcPaths, target);	CopyListing listing = CopyListing.getCopyListing(getConf(), CREDENTIALS, options);	try {	listing.buildListing(listingFile, options);	Assert.fail("Duplicates not detected");	} catch (DuplicateFileException ignore) {	}	} catch (IOException e) {	
exception encountered in test 

Assert.assertEquals(listing.getBytesToCopy(), 10);	Assert.assertEquals(listing.getNumberOfPaths(), 3);	TestDistCpUtils.delete(fs, "/tmp");	try {	listing.buildListing(listingFile, options);	Assert.fail("Invalid input not detected");	} catch (InvalidInputException ignore) {	}	TestDistCpUtils.delete(fs, "/tmp");	} catch (IOException e) {	
exception encountered 

final Path listFile = new Path(testRoot, "/tmp/fileList.seq");	listing.buildListing(listFile, options);	reader = new SequenceFile.Reader(getConf(), SequenceFile.Reader.file(listFile));	CopyListingFileStatus fileStatus = new CopyListingFileStatus();	Text relativePath = new Text();	Assert.assertTrue(reader.next(relativePath, fileStatus));	Assert.assertTrue(relativePath.toString().equals(""));	}	catch (Exception e) {	Assert.fail("Unexpected exception encountered.");	
unexpected exception 

========================= hadoop sample_6264 =========================

public void run() {	boolean succeeded = false;	final FsDatasetImpl dataset = (FsDatasetImpl)datanode.getFSDataset();	try (FsVolumeReference ref = this.targetVolume) {	int smallBufferSize = DFSUtilClient.getSmallBufferSize(EMPTY_HDFS_CONF);	File targetFiles[] = FsDatasetImpl.copyBlockFiles( blockId, genStamp, metaFile, blockFile, lazyPersistDir, true, smallBufferSize, conf);	dataset.onCompleteLazyPersist(bpId, blockId, creationTime, targetFiles, (FsVolumeImpl) targetVolume.getVolume());	succeeded = true;	} catch (Exception e){	
lazywriter failed to async persist ramdisk block pool id block id 

========================= hadoop sample_7926 =========================

protected static Counters maybeParseCounters(String counters) {	try {	return parseCounters(counters);	} catch (ParseException e) {	
the counter string counters is badly formatted 

protected static Counters parseCounters(String counters) throws ParseException {	if (counters == null) {	
historyeventemitters null counter detected 

========================= hadoop sample_6634 =========================

public static ConfigurationMutationACLPolicy getPolicy(Configuration conf) {	Class<? extends ConfigurationMutationACLPolicy> policyClass = conf.getClass(YarnConfiguration.RM_SCHEDULER_MUTATION_ACL_POLICY_CLASS, DefaultConfigurationMutationACLPolicy.class, ConfigurationMutationACLPolicy.class);	
using configurationmutationaclpolicy implementation 

========================= hadoop sample_995 =========================

private void displayUsage() {	
this must be run in only the distributed mode localjobrunner not supported tusage mrreliabilitytest libjars path to hadoop examples jar scratchdir dir scratchdir points to a scratch space on this host where temp files for this test will be created defaults to current working dir passwordless ssh must be set up between this host and the nodes which the test is going to use the test should be run on a free cluster with no parallel job submission going on as the test requires to restart tasktrackers and kill tasks any job submission while the tests are running can cause jobs tests to fail 

private void runSleepJobTest(final JobClient jc, final Configuration conf) throws Exception {	ClusterStatus c = jc.getClusterStatus();	int maxMaps = c.getMaxMapTasks() * 2;	int maxReduces = maxMaps;	int mapSleepTime = (int)c.getTTExpiryInterval();	int reduceSleepTime = mapSleepTime;	String[] sleepJobArgs = new String[] {	"-m", Integer.toString(maxMaps), "-r", Integer.toString(maxReduces), "-mt", Integer.toString(mapSleepTime), "-rt", Integer.toString(reduceSleepTime)};	runTest(jc, conf, "org.apache.hadoop.mapreduce.SleepJob", sleepJobArgs, new KillTaskThread(jc, 2, 0.2f, false, 2), new KillTrackerThread(jc, 2, 0.4f, false, 1));	
sleepjob done 

private void runRandomWriterTest(final JobClient jc, final Configuration conf, final String inputPath) throws Exception {	runTest(jc, conf, "org.apache.hadoop.examples.RandomWriter", new String[]{inputPath}, null, new KillTrackerThread(jc, 0, 0.4f, false, 1));	
randomwriter job done 

private void runSortTest(final JobClient jc, final Configuration conf, final String inputPath, final String outputPath) throws Exception {	runTest(jc, conf, "org.apache.hadoop.examples.Sort", new String[]{inputPath, outputPath}, new KillTaskThread(jc, 2, 0.2f, false, 2), new KillTrackerThread(jc, 2, 0.8f, false, 1));	
sort job done 

private void runSortValidatorTest(final JobClient jc, final Configuration conf, final String inputPath, final String outputPath) throws Exception {	runTest(jc, conf, "org.apache.hadoop.mapred.SortValidator", new String[] {	"-sortInput", inputPath, "-sortOutput", outputPath}, new KillTaskThread(jc, 2, 0.2f, false, 1), new KillTrackerThread(jc, 2, 0.8f, false, 1));	
sortvalidator job done 

private void checkJobExitStatus(int status, String jobName) {	if (status != 0) {	
job failed with status 

private void checkJobExitStatus(int status, String jobName) {	if (status != 0) {	System.exit(status);	} else {	
done 

private void runTest(final JobClient jc, final Configuration conf, final String jobClass, final String[] args, KillTaskThread killTaskThread, KillTrackerThread killTrackerThread) throws Exception {	Thread t = new Thread("Job Test") {	public void run() {	try {	Class<?> jobClassObj = conf.getClassByName(jobClass);	int status = ToolRunner.run(conf, (Tool)(jobClassObj.newInstance()), args);	checkJobExitStatus(status, jobClass);	} catch (Exception e) {	
job failed to run 

checkJobExitStatus(status, jobClass);	} catch (Exception e) {	System.exit(-1);	}	}	};	t.setDaemon(true);	t.start();	JobStatus[] jobs;	while ((jobs = jc.jobsToComplete()).length == 0) {	
waiting for the job to start 

};	t.setDaemon(true);	t.start();	JobStatus[] jobs;	while ((jobs = jc.jobsToComplete()).length == 0) {	Thread.sleep(1000);	}	JobID jobId = jobs[jobs.length - 1].getJobID();	RunningJob rJob = jc.getJob(jobId);	if(rJob.isComplete()) {	
the last job returned by the querying jobtracker is complete exiting the test 

JobStatus[] jobs;	while ((jobs = jc.jobsToComplete()).length == 0) {	Thread.sleep(1000);	}	JobID jobId = jobs[jobs.length - 1].getJobID();	RunningJob rJob = jc.getJob(jobId);	if(rJob.isComplete()) {	System.exit(-1);	}	while (rJob.getJobState() == JobStatus.PREP) {	
jobid not started running yet 

System.exit(-1);	}	while (rJob.getJobState() == JobStatus.PREP) {	Thread.sleep(1000);	rJob = jc.getJob(jobId);	}	if (killTaskThread != null) {	killTaskThread.setRunningJob(rJob);	killTaskThread.start();	killTaskThread.join();	
done with the task kill fail tests 

}	if (killTaskThread != null) {	killTaskThread.setRunningJob(rJob);	killTaskThread.start();	killTaskThread.join();	}	if (killTrackerThread != null) {	killTrackerThread.setRunningJob(rJob);	killTrackerThread.start();	killTrackerThread.join();	
done with the tests to do with lost tasktrackers 

private void stopStartTrackers(boolean considerMaps) {	if (considerMaps) {	
will stop resume tasktrackers based on maps progress 

private void stopStartTrackers(boolean considerMaps) {	if (considerMaps) {	} else {	
will stop resume tasktrackers based on reduces progress 

private void stopStartTrackers(boolean considerMaps) {	if (considerMaps) {	} else {	}	
initial progress threshold threshold multiplier number of iterations 

private void stopTaskTrackers(ClusterStatus c) throws Exception {	Collection <String> trackerNames = c.getActiveTrackerNames();	ArrayList<String> trackerNamesList = new ArrayList<String>(trackerNames);	Collections.shuffle(trackerNamesList);	int count = 0;	FileOutputStream fos = new FileOutputStream(new File(slavesFile));	
stopping a few trackers 

private void stopTaskTrackers(ClusterStatus c) throws Exception {	Collection <String> trackerNames = c.getActiveTrackerNames();	ArrayList<String> trackerNamesList = new ArrayList<String>(trackerNames);	Collections.shuffle(trackerNamesList);	int count = 0;	FileOutputStream fos = new FileOutputStream(new File(slavesFile));	for (String tracker : trackerNamesList) {	String host = convertTrackerNameToHostName(tracker);	
marking tracker on host 

private void startTaskTrackers() throws Exception {	
resuming the stopped trackers 

private void killBasedOnProgress(boolean considerMaps) {	boolean fail = false;	if (considerMaps) {	
will kill tasks based on maps progress 

private void killBasedOnProgress(boolean considerMaps) {	boolean fail = false;	if (considerMaps) {	} else {	
will kill tasks based on reduces progress 

private void killBasedOnProgress(boolean considerMaps) {	boolean fail = false;	if (considerMaps) {	} else {	}	
initial progress threshold threshold multiplier number of iterations 

progress = jc.getJob(rJob.getID()).mapProgress();	} else {	progress = jc.getJob(rJob.getID()).reduceProgress();	}	if (progress >= thresholdVal) {	numIterationsDone++;	if (numIterationsDone > 0 && numIterationsDone % 2 == 0) {	fail = true;	}	ClusterStatus c = jc.getClusterStatus();	
killing a few tasks 

Collection<TaskAttemptID> runningTasks = new ArrayList<TaskAttemptID>();	TaskReport mapReports[] = jc.getMapTaskReports(rJob.getID());	for (TaskReport mapReport : mapReports) {	if (mapReport.getCurrentStatus() == TIPStatus.RUNNING) {	runningTasks.addAll(mapReport.getRunningTaskAttempts());	}	}	if (runningTasks.size() > c.getTaskTrackers()/2) {	int count = 0;	for (TaskAttemptID t : runningTasks) {	
killed task 

runningTasks.clear();	TaskReport reduceReports[] = jc.getReduceTaskReports(rJob.getID());	for (TaskReport reduceReport : reduceReports) {	if (reduceReport.getCurrentStatus() == TIPStatus.RUNNING) {	runningTasks.addAll(reduceReport.getRunningTaskAttempts());	}	}	if (runningTasks.size() > c.getTaskTrackers()/2) {	int count = 0;	for (TaskAttemptID t : runningTasks) {	
killed task 

========================= hadoop sample_5448 =========================

((Closeable) proxy).close();	return;	} else {	InvocationHandler handler = Proxy.getInvocationHandler(proxy);	if (handler instanceof Closeable) {	((Closeable) handler).close();	return;	}	}	} catch (Exception e) {	
cannot call close method due to exception ignoring 

========================= hadoop sample_2362 =========================

if (mulp <= 0 && strs.length > 2 && strs[2] != null) {	mulp = 100 / (new Integer(strs[2]).intValue());	}	List<RMContainer> liveContainers = new ArrayList<RMContainer>();	List<RMContainer> reservedContainers = new ArrayList<RMContainer>();	ApplicationId appId = ApplicationId.newInstance(0L, id);	ApplicationAttemptId appAttemptId = ApplicationAttemptId .newInstance(appId, 1);	FiCaSchedulerApp app = mock(FiCaSchedulerApp.class);	when(app.getAMResource(anyString())) .thenReturn(Resources.createResource(0, 0));	mockContainers(strs[1], app, appAttemptId, queueName, reservedContainers, liveContainers);	
application mock queue appid 

========================= hadoop sample_429 =========================

public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException {	if (filterConfig == null) return;	String uri = ((HttpServletRequest)request).getRequestURI();	
filtering 

static void access(String urlstring) throws IOException {	
access 

========================= hadoop sample_3089 =========================

private ConnectionConfigurator getConnConfigurator(SSLFactory sslFactoryObj) {	try {	return initSslConnConfigurator(DEFAULT_SOCKET_TIMEOUT, sslFactoryObj);	} catch (Exception e) {	
cannot load customized ssl related configuration fallback to system generic settings 

throw e;	}	}	if (leftRetries > 0) {	leftRetries--;	}	retried = true;	try {	Thread.sleep(retryInterval);	} catch (InterruptedException ie) {	
client retry sleep interrupted 

========================= hadoop sample_2548 =========================

public synchronized void pushMetric(final MetricsRecord mr) {	if (changed) {	try {	mr.setMetric(getName(), value);	} catch (Exception e) {	
pushmetric failed for 

========================= hadoop sample_3359 =========================

protected void serviceStart() throws Exception {	
starting amrmproxyservice 

Configuration conf = getConfig();	YarnRPC rpc = YarnRPC.create(conf);	UserGroupInformation.setConfiguration(conf);	this.listenerEndpoint = conf.getSocketAddr(YarnConfiguration.AMRM_PROXY_ADDRESS, YarnConfiguration.DEFAULT_AMRM_PROXY_ADDRESS, YarnConfiguration.DEFAULT_AMRM_PROXY_PORT);	Configuration serverConf = new Configuration(conf);	serverConf.set( CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION, SaslRpcServer.AuthMethod.TOKEN.toString());	int numWorkerThreads = serverConf.getInt( YarnConfiguration.AMRM_PROXY_CLIENT_THREAD_COUNT, YarnConfiguration.DEFAULT_AMRM_PROXY_CLIENT_THREAD_COUNT);	this.secretManager.start();	this.server = rpc.getServer(ApplicationMasterProtocol.class, this, listenerEndpoint, serverConf, this.secretManager, numWorkerThreads);	this.server.start();	
amrmproxyservice listening on address 

protected void serviceStop() throws Exception {	
stopping amrmproxyservice 

public void recover() throws IOException {	
recovering amrmproxyservice 

public void recover() throws IOException {	RecoveredAMRMProxyState state = this.nmContext.getNMStateStore().loadAMRMProxyState();	this.secretManager.recover(state);	
recovering running applications for amrmproxy 

public void recover() throws IOException {	RecoveredAMRMProxyState state = this.nmContext.getNMStateStore().loadAMRMProxyState();	this.secretManager.recover(state);	for (Map.Entry<ApplicationAttemptId, Map<String, byte[]>> entry : state .getAppContexts().entrySet()) {	ApplicationAttemptId attemptId = entry.getKey();	
recovering app attempt 

}	if (amrmToken == null) {	throw new IOException( "No amrmToken found for app attempt " + attemptId);	}	if (user == null) {	throw new IOException("No user found for app attempt " + attemptId);	}	Token<AMRMTokenIdentifier> localToken = this.secretManager.createAndGetAMRMToken(attemptId);	initializePipeline(attemptId, user, amrmToken, localToken, entry.getValue(), true);	} catch (Exception e) {	
exception when recovering removing it from nmstatestore and move on 

public RegisterApplicationMasterResponse registerApplicationMaster( RegisterApplicationMasterRequest request) throws YarnException, IOException {	
registering application master host port tracking url 

public FinishApplicationMasterResponse finishApplicationMaster( FinishApplicationMasterRequest request) throws YarnException, IOException {	
finishing application master tracking url 

public void processApplicationStartRequest(StartContainerRequest request) throws IOException, YarnException {	
callback received for initializing request processing pipeline for an am 

protected void initializePipeline(ApplicationAttemptId applicationAttemptId, String user, Token<AMRMTokenIdentifier> amrmToken, Token<AMRMTokenIdentifier> localToken, Map<String, byte[]> recoveredDataMap, boolean isRecovery) {	RequestInterceptorChainWrapper chainWrapper = null;	synchronized (applPipelineMap) {	if (applPipelineMap .containsKey(applicationAttemptId.getApplicationId())) {	
request to start an already existing appid was received this can happen if an application failed and a new attempt was created on this machine applicationid 

protected void initializePipeline(ApplicationAttemptId applicationAttemptId, String user, Token<AMRMTokenIdentifier> amrmToken, Token<AMRMTokenIdentifier> localToken, Map<String, byte[]> recoveredDataMap, boolean isRecovery) {	RequestInterceptorChainWrapper chainWrapper = null;	synchronized (applPipelineMap) {	if (applPipelineMap .containsKey(applicationAttemptId.getApplicationId())) {	RequestInterceptorChainWrapper chainWrapperBackup = this.applPipelineMap.get(applicationAttemptId.getApplicationId());	if (chainWrapperBackup != null && chainWrapperBackup.getApplicationAttemptId() != null && !chainWrapperBackup.getApplicationAttemptId() .equals(applicationAttemptId)) {	
remove the previous pipeline for applicationid 

RequestInterceptorChainWrapper chainWrapper = null;	synchronized (applPipelineMap) {	if (applPipelineMap .containsKey(applicationAttemptId.getApplicationId())) {	RequestInterceptorChainWrapper chainWrapperBackup = this.applPipelineMap.get(applicationAttemptId.getApplicationId());	if (chainWrapperBackup != null && chainWrapperBackup.getApplicationAttemptId() != null && !chainWrapperBackup.getApplicationAttemptId() .equals(applicationAttemptId)) {	RequestInterceptorChainWrapper pipeline = applPipelineMap.remove(applicationAttemptId.getApplicationId());	if (!isRecovery && this.nmContext.getNMStateStore() != null) {	try {	this.nmContext.getNMStateStore() .removeAMRMProxyAppContext(applicationAttemptId);	} catch (IOException e) {	
error removing amrmproxy application context for 

RequestInterceptorChainWrapper pipeline = applPipelineMap.remove(applicationAttemptId.getApplicationId());	if (!isRecovery && this.nmContext.getNMStateStore() != null) {	try {	this.nmContext.getNMStateStore() .removeAMRMProxyAppContext(applicationAttemptId);	} catch (IOException e) {	}	}	try {	pipeline.getRootInterceptor().shutdown();	} catch (Throwable ex) {	
failed to shutdown the request processing pipeline for app 

pipeline.getRootInterceptor().shutdown();	} catch (Throwable ex) {	}	} else {	return;	}	}	chainWrapper = new RequestInterceptorChainWrapper();	this.applPipelineMap.put(applicationAttemptId.getApplicationId(), chainWrapper);	}	
initializing request processing pipeline for application applicationid for the user 

throw new YarnRuntimeException( "null recoveredDataMap recieved for recover");	}	interceptorChain.recover(recoveredDataMap);	}	chainWrapper.init(interceptorChain, applicationAttemptId);	if (!isRecovery && this.nmContext.getNMStateStore() != null) {	try {	this.nmContext.getNMStateStore().storeAMRMProxyAppContextEntry( applicationAttemptId, NMSS_USER_KEY, user.getBytes("UTF-8"));	this.nmContext.getNMStateStore().storeAMRMProxyAppContextEntry( applicationAttemptId, NMSS_AMRMTOKEN_KEY, amrmToken.encodeToUrlString().getBytes("UTF-8"));	} catch (IOException e) {	
error storing amrmproxy application context entry for 

protected void stopApplication(ApplicationId applicationId) {	Preconditions.checkArgument(applicationId != null, "applicationId is null");	RequestInterceptorChainWrapper pipeline = this.applPipelineMap.remove(applicationId);	if (pipeline == null) {	
no interceptor pipeline for application likely because its am is not run in this node 

protected void stopApplication(ApplicationId applicationId) {	Preconditions.checkArgument(applicationId != null, "applicationId is null");	RequestInterceptorChainWrapper pipeline = this.applPipelineMap.remove(applicationId);	if (pipeline == null) {	} else {	this.secretManager .applicationMasterFinished(pipeline.getApplicationAttemptId());	
stopping the request processing pipeline for application 

protected void stopApplication(ApplicationId applicationId) {	Preconditions.checkArgument(applicationId != null, "applicationId is null");	RequestInterceptorChainWrapper pipeline = this.applPipelineMap.remove(applicationId);	if (pipeline == null) {	} else {	this.secretManager .applicationMasterFinished(pipeline.getApplicationAttemptId());	try {	pipeline.getRootInterceptor().shutdown();	} catch (Throwable ex) {	
failed to shutdown the request processing pipeline for app 

} else {	this.secretManager .applicationMasterFinished(pipeline.getApplicationAttemptId());	try {	pipeline.getRootInterceptor().shutdown();	} catch (Throwable ex) {	}	if (this.nmContext.getNMStateStore() != null) {	try {	this.nmContext.getNMStateStore() .removeAMRMProxyAppContext(pipeline.getApplicationAttemptId());	} catch (IOException e) {	
error removing amrmproxy application context for 

private void updateAMRMTokens(AMRMTokenIdentifier amrmTokenIdentifier, RequestInterceptorChainWrapper pipeline, AllocateResponse allocateResponse) {	AMRMProxyApplicationContextImpl context = (AMRMProxyApplicationContextImpl) pipeline.getRootInterceptor() .getApplicationContext();	if (allocateResponse.getAMRMToken() != null) {	
rm rolled master key for amrm tokens 

AMRMProxyApplicationContextImpl context = (AMRMProxyApplicationContextImpl) pipeline.getRootInterceptor() .getApplicationContext();	if (allocateResponse.getAMRMToken() != null) {	org.apache.hadoop.yarn.api.records.Token token = allocateResponse.getAMRMToken();	allocateResponse.setAMRMToken(null);	org.apache.hadoop.security.token.Token<AMRMTokenIdentifier> newToken = new org.apache.hadoop.security.token.Token<AMRMTokenIdentifier>( token.getIdentifier().array(), token.getPassword().array(), new Text(token.getKind()), new Text(token.getService()));	context.setAMRMToken(newToken);	if (this.nmContext.getNMStateStore() != null) {	try {	this.nmContext.getNMStateStore().storeAMRMProxyAppContextEntry( context.getApplicationAttemptId(), NMSS_AMRMTOKEN_KEY, newToken.encodeToUrlString().getBytes("UTF-8"));	} catch (IOException e) {	
error storing amrmproxy application context entry for 

try {	this.nmContext.getNMStateStore().storeAMRMProxyAppContextEntry( context.getApplicationAttemptId(), NMSS_AMRMTOKEN_KEY, newToken.encodeToUrlString().getBytes("UTF-8"));	} catch (IOException e) {	}	}	}	MasterKeyData nextMasterKey = this.secretManager.getNextMasterKeyData();	if (nextMasterKey != null && nextMasterKey.getMasterKey().getKeyId() != amrmTokenIdentifier .getKeyId()) {	Token<AMRMTokenIdentifier> localToken = context.getLocalAMRMToken();	if (nextMasterKey.getMasterKey().getKeyId() != context .getLocalAMRMTokenKeyId()) {	
the local amrmtoken has been rolled over send new local amrmtoken back to application 

public void handle(ApplicationEvent event) {	Application app = AMRMProxyService.this.nmContext.getApplications().get( event.getApplicationID());	if (app != null) {	switch (event.getType()) {	
application stop event received for stopping appid 

public void handle(ApplicationEvent event) {	Application app = AMRMProxyService.this.nmContext.getApplications().get( event.getApplicationID());	if (app != null) {	switch (event.getType()) {	AMRMProxyService.this.stopApplication(event.getApplicationID());	break;	default: if (LOG.isDebugEnabled()) {	
amrmproxy is ignoring event 

Application app = AMRMProxyService.this.nmContext.getApplications().get( event.getApplicationID());	if (app != null) {	switch (event.getType()) {	AMRMProxyService.this.stopApplication(event.getApplicationID());	break;	default: if (LOG.isDebugEnabled()) {	}	break;	}	} else {	
event sent to absent application 

========================= hadoop sample_1734 =========================

public void testClientRetrySafeMode() throws Exception {	final Map<Path, Boolean> results = Collections .synchronizedMap(new HashMap<Path, Boolean>());	final Path test = new Path("/test");	cluster.getConfiguration(0).setInt( DFSConfigKeys.DFS_NAMENODE_SAFEMODE_MIN_DATANODES_KEY, 3);	NameNodeAdapter.enterSafeMode(nn0, false);	Whitebox.setInternalState(nn0.getNamesystem(), "manualSafeMode", false);	BlockManagerTestUtil.setStartupSafeModeForTest(nn0.getNamesystem() .getBlockManager());	assertTrue(nn0.getNamesystem().isInStartupSafeMode());	
enter safemode 

final Path test = new Path("/test");	cluster.getConfiguration(0).setInt( DFSConfigKeys.DFS_NAMENODE_SAFEMODE_MIN_DATANODES_KEY, 3);	NameNodeAdapter.enterSafeMode(nn0, false);	Whitebox.setInternalState(nn0.getNamesystem(), "manualSafeMode", false);	BlockManagerTestUtil.setStartupSafeModeForTest(nn0.getNamesystem() .getBlockManager());	assertTrue(nn0.getNamesystem().isInStartupSafeMode());	new Thread() {	public void run() {	try {	boolean mkdir = fs.mkdirs(test);	
mkdir finished result is 

assertTrue(nn0.getNamesystem().isInStartupSafeMode());	new Thread() {	public void run() {	try {	boolean mkdir = fs.mkdirs(test);	synchronized (TestHASafeMode.this) {	results.put(test, mkdir);	TestHASafeMode.this.notifyAll();	}	} catch (Exception e) {	
got exception while calling mkdir 

results.put(test, mkdir);	TestHASafeMode.this.notifyAll();	}	} catch (Exception e) {	}	}	}.start();	assertFalse("The directory should not be created while NN in safemode", fs.exists(test));	Thread.sleep(1000);	NameNodeAdapter.leaveSafeMode(nn0);	
leave safemode 

========================= hadoop sample_7463 =========================

static public ObjectName register(String serviceName, String nameName, Object theMbean) {	final MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();	ObjectName name = getMBeanName(serviceName, nameName);	try {	mbs.registerMBean(theMbean, name);	
registered 

static public ObjectName register(String serviceName, String nameName, Object theMbean) {	final MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();	ObjectName name = getMBeanName(serviceName, nameName);	try {	mbs.registerMBean(theMbean, name);	return name;	} catch (InstanceAlreadyExistsException iaee) {	if (LOG.isTraceEnabled()) {	
failed to register mbean name 

static public ObjectName register(String serviceName, String nameName, Object theMbean) {	final MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();	ObjectName name = getMBeanName(serviceName, nameName);	try {	mbs.registerMBean(theMbean, name);	return name;	} catch (InstanceAlreadyExistsException iaee) {	if (LOG.isTraceEnabled()) {	} else {	
failed to register mbean name instance already exists 

final MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();	ObjectName name = getMBeanName(serviceName, nameName);	try {	mbs.registerMBean(theMbean, name);	return name;	} catch (InstanceAlreadyExistsException iaee) {	if (LOG.isTraceEnabled()) {	} else {	}	} catch (Exception e) {	
failed to register mbean name 

static public void unregister(ObjectName mbeanName) {	
unregistering 

static public void unregister(ObjectName mbeanName) {	final MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();	if (mbeanName == null) {	
stacktrace 

static public void unregister(ObjectName mbeanName) {	final MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();	if (mbeanName == null) {	return;	}	try {	mbs.unregisterMBean(mbeanName);	} catch (Exception e) {	
error unregistering 

static private ObjectName getMBeanName(String serviceName, String nameName) {	String nameStr = DOMAIN_PREFIX + SERVICE_PREFIX + serviceName + "," + NAME_PREFIX + nameName;	try {	return DefaultMetricsSystem.newMBeanName(nameStr);	} catch (Exception e) {	
error creating mbean object name 

========================= hadoop sample_3420 =========================

final DistributedFileSystem targetFs = (DistributedFileSystem) tfs;	Path tmpDir = null;	try {	tmpDir = createTargetTmpDir(targetFs, targetDir);	DiffInfo[] renameAndDeleteDiffs = getRenameAndDeleteDiffsForSync(targetDir);	if (renameAndDeleteDiffs.length > 0) {	syncDiff(renameAndDeleteDiffs, targetFs, tmpDir);	}	return true;	} catch (Exception e) {	
failed to use snapshot diff for distcp 

final Path source = new Path(DFSUtilClient.bytes2String(entry.getSourcePath()));	list.add(new DiffInfo(source, null, dt));	} else if (dt == SnapshotDiffReport.DiffType.RENAME) {	final Path source = new Path(DFSUtilClient.bytes2String(entry.getSourcePath()));	final Path target = new Path(DFSUtilClient.bytes2String(entry.getTargetPath()));	list.add(new DiffInfo(source, target, dt));	}	}	return true;	} catch (IOException e) {	
failed to compute snapshot diff on 

private void deleteTargetTmpDir(DistributedFileSystem targetFs, Path tmpDir) {	try {	if (tmpDir != null) {	targetFs.delete(tmpDir, true);	}	} catch (IOException e) {	
unable to cleanup tmp dir 

private boolean checkNoChange(DistributedFileSystem fs, Path path) {	try {	final String from = getSnapshotName(inputOptions.getFromSnapshot());	SnapshotDiffReport targetDiff = fs.getSnapshotDiffReport(path, from, "");	if (!targetDiff.getDiffList().isEmpty()) {	
the target has been modified since snapshot 

private boolean checkNoChange(DistributedFileSystem fs, Path path) {	try {	final String from = getSnapshotName(inputOptions.getFromSnapshot());	SnapshotDiffReport targetDiff = fs.getSnapshotDiffReport(path, from, "");	if (!targetDiff.getDiffList().isEmpty()) {	return false;	} else {	return true;	}	} catch (IOException e) {	
failed to compute snapshot diff on at snapshot 

========================= hadoop sample_6272 =========================

proxyUriBases.put("dummy", conf.getInitParameter(PROXY_URI_BASE));	} else {	proxyHosts = conf.getInitParameter(PROXY_HOSTS) .split(PROXY_HOSTS_DELIMITER);	String[] proxyUriBasesArr = conf.getInitParameter(PROXY_URI_BASES) .split(PROXY_URI_BASES_DELIMITER);	proxyUriBases = new HashMap<>(proxyUriBasesArr.length);	for (String proxyUriBase : proxyUriBasesArr) {	try {	URL url = new URL(proxyUriBase);	proxyUriBases.put(url.getHost() + ":" + url.getPort(), proxyUriBase);	} catch(MalformedURLException e) {	
does not appear to be a valid url 

protected Set<String> getProxyAddresses() throws ServletException {	long now = System.currentTimeMillis();	synchronized(this) {	if (proxyAddresses == null || (lastUpdate + UPDATE_INTERVAL) >= now) {	proxyAddresses = new HashSet<>();	for (String proxyHost : proxyHosts) {	try {	for(InetAddress add : InetAddress.getAllByName(proxyHost)) {	if (LOG.isDebugEnabled()) {	
proxy address is 

proxyAddresses = new HashSet<>();	for (String proxyHost : proxyHosts) {	try {	for(InetAddress add : InetAddress.getAllByName(proxyHost)) {	if (LOG.isDebugEnabled()) {	}	proxyAddresses.add(add.getHostAddress());	}	lastUpdate = now;	} catch (UnknownHostException e) {	
could not locate skipping 

public void doFilter(ServletRequest req, ServletResponse resp, FilterChain chain) throws IOException, ServletException {	ProxyUtils.rejectNonHttpRequests(req);	HttpServletRequest httpReq = (HttpServletRequest)req;	HttpServletResponse httpResp = (HttpServletResponse)resp;	if (LOG.isDebugEnabled()) {	
remote address for request is 

if (httpReq.getCookies() != null) {	for(Cookie c: httpReq.getCookies()) {	if(WebAppProxyServlet.PROXY_USER_COOKIE_NAME.equals(c.getName())){	user = c.getValue();	break;	}	}	}	if (user == null) {	if (LOG.isDebugEnabled()) {	
could not find cookie so user will not be set 

public boolean isValidUrl(String url) {	boolean isValid = false;	try {	HttpURLConnection conn = (HttpURLConnection) new URL(url) .openConnection();	conn.connect();	isValid = conn.getResponseCode() == HttpURLConnection.HTTP_OK;	} catch (Exception e) {	
failed to connect to 

========================= hadoop sample_1558 =========================

public FederationStateStoreHeartbeat(SubClusterId subClusterId, FederationStateStore stateStoreClient, ResourceScheduler scheduler) {	this.stateStoreService = stateStoreClient;	this.subClusterId = subClusterId;	this.rs = scheduler;	this.currentClusterState = new StringWriter();	try {	this.jc = new JSONJAXBContext( JSONConfiguration.mapped().rootUnwrapping(false).build(), ClusterMetricsInfo.class);	marshaller = jc.createJSONMarshaller();	} catch (JAXBException e) {	
exception while trying to initialize jaxb context 

public FederationStateStoreHeartbeat(SubClusterId subClusterId, FederationStateStore stateStoreClient, ResourceScheduler scheduler) {	this.stateStoreService = stateStoreClient;	this.subClusterId = subClusterId;	this.rs = scheduler;	this.currentClusterState = new StringWriter();	try {	this.jc = new JSONJAXBContext( JSONConfiguration.mapped().rootUnwrapping(false).build(), ClusterMetricsInfo.class);	marshaller = jc.createJSONMarshaller();	} catch (JAXBException e) {	}	
initialized federation membership for cluster with timestamp 

private void updateClusterState() {	try {	currentClusterState.getBuffer().setLength(0);	ClusterMetricsInfo clusterMetricsInfo = new ClusterMetricsInfo(rs);	marshaller.marshallToJSON(clusterMetricsInfo, currentClusterState);	capability = currentClusterState.toString();	} catch (Exception e) {	
exception while trying to generate cluster state so reverting to last know state 

public synchronized void run() {	try {	updateClusterState();	SubClusterHeartbeatRequest request = SubClusterHeartbeatRequest .newInstance(subClusterId, SubClusterState.SC_RUNNING, capability);	stateStoreService.subClusterHeartbeat(request);	
sending the heartbeat with capability 

public synchronized void run() {	try {	updateClusterState();	SubClusterHeartbeatRequest request = SubClusterHeartbeatRequest .newInstance(subClusterId, SubClusterState.SC_RUNNING, capability);	stateStoreService.subClusterHeartbeat(request);	} catch (Exception e) {	
exception when trying to heartbeat 

========================= hadoop sample_1108 =========================

public void testMountTableScalability() throws IOException {	List<MountTable> emptyList = new ArrayList<>();	mountTable.refreshEntries(emptyList);	for (int i = 0; i < 100000; i++) {	Map<String, String> map = getMountTableEntry("1", "/" + i);	MountTable record = MountTable.newInstance("/" + i, map);	mountTable.addEntry(record);	if (i % 10000 == 0) {	
adding flat mount record 

========================= hadoop sample_7526 =========================

public String getJson() {	Collection<ReportForJson> validReports = getJsonReports( MAX_NODES_TO_REPORT);	ObjectMapper objectMapper = new ObjectMapper();	try {	return objectMapper.writeValueAsString(validReports);	} catch (JsonProcessingException e) {	
failed to serialize statistics 

========================= hadoop sample_8329 =========================

checkStatus("/nonexistent");	checkStatus("/nonexistent/a");	final String username = UserGroupInformation.getCurrentUser().getShortUserName() + "1";	final HftpFileSystem hftp2 = cluster.getHftpFileSystemAs(username, CONF, 0, "somegroup");	{	final Path nonexistent = new Path("/nonexistent");	try {	hftp2.getFileStatus(nonexistent);	Assert.fail();	} catch(IOException ioe) {	
good getting an exception 

} catch(IOException ioe) {	}	}	{	final Path dir = new Path("/dir");	fs.setPermission(dir, new FsPermission((short)0));	try {	hftp2.getFileStatus(new Path(dir, "a"));	Assert.fail();	} catch(IOException ioe) {	
good getting an exception 

========================= hadoop sample_7208 =========================

private void shouldRetry(final IOException ioe, final int retry, final String url) throws IOException {	CharSequence authenticationExceptionMessage = Constants.AUTHENTICATION_FAILED_ERROR_MESSAGE;	if (ioe instanceof WasbRemoteCallException && ioe.getMessage() .equals(authenticationExceptionMessage)) {	throw ioe;	}	try {	final RetryPolicy.RetryAction a = (retryPolicy != null) ? retryPolicy .shouldRetry(ioe, retry, 0, true) : RetryPolicy.RetryAction.FAIL;	boolean isRetry = a.action == RetryPolicy.RetryAction.RetryDecision.RETRY;	boolean isFailoverAndRetry = a.action == RetryPolicy.RetryAction.RetryDecision.FAILOVER_AND_RETRY;	if (isRetry || isFailoverAndRetry) {	
retrying connect to remote service already tried time s retry policy is delay ms 

boolean isFailoverAndRetry = a.action == RetryPolicy.RetryAction.RetryDecision.FAILOVER_AND_RETRY;	if (isRetry || isFailoverAndRetry) {	Thread.sleep(a.delayMillis);	return;	}	} catch (InterruptedIOException e) {	LOG.warn(e.getMessage(), e);	Thread.currentThread().interrupt();	return;	} catch (Exception e) {	
original exception is 

Thread.sleep(a.delayMillis);	return;	}	} catch (InterruptedIOException e) {	LOG.warn(e.getMessage(), e);	Thread.currentThread().interrupt();	return;	} catch (Exception e) {	throw new WasbRemoteCallException(e.getMessage(), e);	}	
not retrying anymore already retried the urls time s 

========================= hadoop sample_6419 =========================

private void setKMSACLs(Configuration conf) {	Map<Type, AccessControlList> tempAcls = new HashMap<Type, AccessControlList>();	Map<Type, AccessControlList> tempBlacklist = new HashMap<Type, AccessControlList>();	for (Type aclType : Type.values()) {	String aclStr = conf.get(aclType.getAclConfigKey(), ACL_DEFAULT);	tempAcls.put(aclType, new AccessControlList(aclStr));	String blacklistStr = conf.get(aclType.getBlacklistConfigKey());	if (blacklistStr != null) {	tempBlacklist.put(aclType, new AccessControlList(blacklistStr));	
blacklist 

private void setKMSACLs(Configuration conf) {	Map<Type, AccessControlList> tempAcls = new HashMap<Type, AccessControlList>();	Map<Type, AccessControlList> tempBlacklist = new HashMap<Type, AccessControlList>();	for (Type aclType : Type.values()) {	String aclStr = conf.get(aclType.getAclConfigKey(), ACL_DEFAULT);	tempAcls.put(aclType, new AccessControlList(aclStr));	String blacklistStr = conf.get(aclType.getBlacklistConfigKey());	if (blacklistStr != null) {	tempBlacklist.put(aclType, new AccessControlList(blacklistStr));	}	
acl 

private void parseAclsWithPrefix(final Configuration conf, final String prefix, final KeyOpType keyOp, Map<KeyOpType, AccessControlList> results) {	String confKey = prefix + keyOp;	String aclStr = conf.get(confKey);	if (aclStr != null) {	if (keyOp == KeyOpType.ALL) {	
invalid key op for ignoring 

private void parseAclsWithPrefix(final Configuration conf, final String prefix, final KeyOpType keyOp, Map<KeyOpType, AccessControlList> results) {	String confKey = prefix + keyOp;	String aclStr = conf.get(confKey);	if (aclStr != null) {	if (keyOp == KeyOpType.ALL) {	} else {	if (aclStr.equals("*")) {	
for key op is set to 

public void run() {	try {	if (KMSConfiguration.isACLsFileNewer(lastReload)) {	setKMSACLs(loadACLs());	setKeyACLs(loadACLs());	}	} catch (Exception ex) {	
could not reload acls file s 

private Configuration loadACLs() {	
loading acls file 

public boolean hasAccess(Type type, UserGroupInformation ugi) {	boolean access = acls.get(type).isUserAllowed(ugi);	if (LOG.isDebugEnabled()) {	
checking user for 

public boolean hasAccess(Type type, UserGroupInformation ugi) {	boolean access = acls.get(type).isUserAllowed(ugi);	if (LOG.isDebugEnabled()) {	}	if (access) {	AccessControlList blacklist = blacklistedAcls.get(type);	access = (blacklist == null) || !blacklist.isUserInList(ugi);	if (LOG.isDebugEnabled()) {	if (blacklist == null) {	
no blacklist for 

public boolean hasAccess(Type type, UserGroupInformation ugi) {	boolean access = acls.get(type).isUserAllowed(ugi);	if (LOG.isDebugEnabled()) {	}	if (access) {	AccessControlList blacklist = blacklistedAcls.get(type);	access = (blacklist == null) || !blacklist.isUserInList(ugi);	if (LOG.isDebugEnabled()) {	if (blacklist == null) {	} else if (access) {	
user is in 

boolean access = acls.get(type).isUserAllowed(ugi);	if (LOG.isDebugEnabled()) {	}	if (access) {	AccessControlList blacklist = blacklistedAcls.get(type);	access = (blacklist == null) || !blacklist.isUserInList(ugi);	if (LOG.isDebugEnabled()) {	if (blacklist == null) {	} else if (access) {	} else {	
user is not in 

AccessControlList blacklist = blacklistedAcls.get(type);	access = (blacklist == null) || !blacklist.isUserInList(ugi);	if (LOG.isDebugEnabled()) {	if (blacklist == null) {	} else if (access) {	} else {	}	}	}	if (LOG.isDebugEnabled()) {	
user type result 

private boolean checkKeyAccess(String keyName, UserGroupInformation ugi, KeyOpType opType) {	Map<KeyOpType, AccessControlList> keyAcl = keyAcls.get(keyName);	if (keyAcl == null) {	
key has no acls defined using defaults 

private boolean checkKeyAccess(String keyName, UserGroupInformation ugi, KeyOpType opType) {	Map<KeyOpType, AccessControlList> keyAcl = keyAcls.get(keyName);	if (keyAcl == null) {	keyAcl = defaultKeyAcls;	}	boolean access = checkKeyAccess(keyAcl, ugi, opType);	if (LOG.isDebugEnabled()) {	
user optype keyname result 

private boolean checkKeyAccess(Map<KeyOpType, AccessControlList> keyAcl, UserGroupInformation ugi, KeyOpType opType) {	AccessControlList acl = keyAcl.get(opType);	if (acl == null) {	
no acl available for key denying access for 

private boolean checkKeyAccess(Map<KeyOpType, AccessControlList> keyAcl, UserGroupInformation ugi, KeyOpType opType) {	AccessControlList acl = keyAcl.get(opType);	if (acl == null) {	return false;	} else {	if (LOG.isDebugEnabled()) {	
checking user for 

========================= hadoop sample_4424 =========================

Set<String> target = new LinkedHashSet<String>();	for (String filterInitializer : parts) {	filterInitializer = filterInitializer.trim();	if (filterInitializer.equals( AuthenticationFilterInitializer.class.getName()) || filterInitializer.isEmpty()) {	continue;	}	target.add(filterInitializer);	}	target.addAll(defaultInitializers);	String actualInitializers = org.apache.commons.lang.StringUtils.join(target, ",");	
filter initializers set for timeline service 

========================= hadoop sample_1299 =========================

nm1.nodeHeartbeat(true);	RMAppAttempt attempt1 = app1.getCurrentAppAttempt();	MockAM am1 = rm.sendAMLaunched(attempt1.getAppAttemptId());	am1.registerAppAttempt();	int allocCount = 0;	am1.addRequests(new String[] {"127.0.0.1"}, GB, 1, 1);	AllocateResponse alloc1Response = am1.schedule();	allocCount++;	nm1.nodeHeartbeat(true);	while (alloc1Response.getAllocatedContainers().size() < 1) {	
waiting for containers to be created for app 

MockNM nm1 = rm.registerNode("127.0.0.1:1234", 6 * GB);	RMApp app1 = rm.submitApp(2048);	nm1.nodeHeartbeat(true);	RMAppAttempt attempt1 = app1.getCurrentAppAttempt();	MockAM am1 = rm.sendAMLaunched(attempt1.getAppAttemptId());	am1.registerAppAttempt();	am1.addRequests(new String[] { "127.0.0.1" }, GB, 1, 1);	AllocateResponse alloc1Response = am1.schedule();	nm1.nodeHeartbeat(true);	while (alloc1Response.getAllocatedContainers().size() < 1) {	
waiting for containers to be created for app 

MockNM nm1 = rm.registerNode("127.0.0.1:1234", 6 * GB);	RMApp app1 = rm.submitApp(1024);	nm1.nodeHeartbeat(true);	RMAppAttempt attempt1 = app1.getCurrentAppAttempt();	MockAM am1 = rm.sendAMLaunched(attempt1.getAppAttemptId());	am1.registerAppAttempt();	am1.addRequests(new String[] { "127.0.0.1" }, GB, 1, 1);	AllocateResponse alloc1Response = am1.schedule();	nm1.nodeHeartbeat(true);	while (alloc1Response.getAllocatedContainers().size() < 1) {	
waiting for containers to be created for app 

MockAM am1 = rm.sendAMLaunched(attempt1.getAppAttemptId());	am1.registerAppAttempt();	AllocateRequestPBImpl allocateRequest = new AllocateRequestPBImpl();	List<ContainerId> release = new ArrayList<ContainerId>();	List<ResourceRequest> ask = new ArrayList<ResourceRequest>();	allocateRequest.setReleaseList(release);	allocateRequest.setAskList(ask);	allocateRequest.setProgress(Float.POSITIVE_INFINITY);	am1.allocate(allocateRequest);	while(attempt1.getProgress()!=1){	
waiting for allocate event to be handled 

allocateRequest.setReleaseList(release);	allocateRequest.setAskList(ask);	allocateRequest.setProgress(Float.POSITIVE_INFINITY);	am1.allocate(allocateRequest);	while(attempt1.getProgress()!=1){	sleep(100);	}	allocateRequest.setProgress(Float.NaN);	am1.allocate(allocateRequest);	while(attempt1.getProgress()!=0){	
waiting for allocate event to be handled 

sleep(100);	}	allocateRequest.setProgress(Float.NaN);	am1.allocate(allocateRequest);	while(attempt1.getProgress()!=0){	sleep(100);	}	allocateRequest.setProgress((float)9);	am1.allocate(allocateRequest);	while(attempt1.getProgress()!=1){	
waiting for allocate event to be handled 

sleep(100);	}	allocateRequest.setProgress((float)9);	am1.allocate(allocateRequest);	while(attempt1.getProgress()!=1){	sleep(100);	}	allocateRequest.setProgress(Float.NEGATIVE_INFINITY);	am1.allocate(allocateRequest);	while(attempt1.getProgress()!=0){	
waiting for allocate event to be handled 

sleep(100);	}	allocateRequest.setProgress(Float.NEGATIVE_INFINITY);	am1.allocate(allocateRequest);	while(attempt1.getProgress()!=0){	sleep(100);	}	allocateRequest.setProgress((float)0.5);	am1.allocate(allocateRequest);	while(attempt1.getProgress()!=0.5){	
waiting for allocate event to be handled 

sleep(100);	}	allocateRequest.setProgress((float)0.5);	am1.allocate(allocateRequest);	while(attempt1.getProgress()!=0.5){	sleep(100);	}	allocateRequest.setProgress((float)-1);	am1.allocate(allocateRequest);	while(attempt1.getProgress()!=0){	
waiting for allocate event to be handled 

========================= hadoop sample_604 =========================

private void printTokens(JobID jobId, Credentials credentials) throws IOException {	
submitting tokens for job 

private void readTokensFromFiles(Configuration conf, Credentials credentials) throws IOException {	String binaryTokenFilename = conf.get(MRJobConfig.MAPREDUCE_JOB_CREDENTIALS_BINARY);	if (binaryTokenFilename != null) {	Credentials binary = Credentials.readTokenStorageFile( FileSystem.getLocal(conf).makeQualified( new Path(binaryTokenFilename)), conf);	credentials.addAll(binary);	}	String tokensFileName = conf.get("mapreduce.job.credentials.json");	if(tokensFileName != null) {	
loading user s secret keys from 

try {	Map<String, String> nm = READER.readValue(new File(localFileName));	for(Map.Entry<String, String> ent: nm.entrySet()) {	credentials.addSecretKey(new Text(ent.getKey()), ent.getValue() .getBytes(Charsets.UTF_8));	}	} catch (JsonMappingException e) {	json_error = true;	} catch (JsonParseException e) {	json_error = true;	}	
couldn t parse token cache json file with user secret keys 

private void populateTokenCache(Configuration conf, Credentials credentials) throws IOException{	readTokensFromFiles(conf, credentials);	String [] nameNodes = conf.getStrings(MRJobConfig.JOB_NAMENODES);	
adding the following namenodes delegation tokens 

========================= hadoop sample_4832 =========================

}	return splitter.split(job.getConfiguration(), results, getDBConf().getInputOrderBy());	} catch (SQLException e) {	throw new IOException(e.getMessage());	} finally {	try {	if (null != results) {	results.close();	}	} catch (SQLException se) {	
sqlexception closing resultset 

if (null != results) {	results.close();	}	} catch (SQLException se) {	}	try {	if (null != statement) {	statement.close();	}	} catch (SQLException se) {	
sqlexception closing statement 

try {	if (null != statement) {	statement.close();	}	} catch (SQLException se) {	}	try {	connection.commit();	closeConnection();	} catch (SQLException se) {	
sqlexception committing split transaction 

public static void setBoundingQuery(Configuration conf, String query) {	if (null != query) {	if (query.indexOf(SUBSTITUTE_TOKEN) == -1) {	
could not find token in query splits may not partition data 

protected RecordReader<LongWritable, T> createDBRecordReader(DBInputSplit split, Configuration conf) throws IOException {	DBConfiguration dbConf = getDBConf();	Class<T> inputClass = (Class<T>) (dbConf.getInputClass());	String dbProductName = getDBProductName();	
creating db record reader for db product 

========================= hadoop sample_5059 =========================

public Configuration bindArgs(Configuration config, List<String> args) throws Exception {	Configuration configuration = super.bindArgs(config, args);	if (!args.isEmpty()) {	StringBuilder argsList = new StringBuilder();	for (String arg : args) {	argsList.append('"').append(arg).append("\" ");	}	
got arguments 

========================= hadoop sample_3162 =========================

private void recursiveDelete(Path path) {	try {	allowRecursiveDelete(fs, path.toString());	fs.delete(path, true);	} catch (IOException e) {	
failed to delete 

========================= hadoop sample_6360 =========================

private boolean shouldRun() {	if (exec == null) {	
not running test because container executor path is not set 

public void testContainerKill() throws Exception {	Assume.assumeTrue(shouldRun());	final ContainerId sleepId = getNextContainerId();	Thread t = new Thread() {	public void run() {	try {	runAndBlock(sleepId, "sleep", "100");	} catch (IOException|ConfigurationException e) {	
caught exception while running sleep 

} catch (IOException|ConfigurationException e) {	}	};	};	t.setDaemon(true);	t.start();	assertTrue(t.isAlive());	String pid = null;	int count = 10;	while ((pid = exec.getProcessId(sleepId)) == null && count > 0) {	
sleeping for ms before checking for pid 

t.setDaemon(true);	t.start();	assertTrue(t.isAlive());	String pid = null;	int count = 10;	while ((pid = exec.getProcessId(sleepId)) == null && count > 0) {	Thread.sleep(200);	count--;	}	assertNotNull(pid);	
going to killing the process 

t.start();	assertTrue(t.isAlive());	String pid = null;	int count = 10;	while ((pid = exec.getProcessId(sleepId)) == null && count > 0) {	Thread.sleep(200);	count--;	}	assertNotNull(pid);	exec.signalContainer(new ContainerSignalContext.Builder() .setUser(appSubmitter) .setPid(pid) .setSignal(Signal.TERM) .build());	
sleeping for to let the sleep be killed 

========================= hadoop sample_1578 =========================

public synchronized T fromJson(String json) throws IOException, JsonParseException, JsonMappingException {	try {	return mapper.readValue(json, classType);	} catch (IOException e) {	
exception while parsing json 

public synchronized T fromFile(File jsonFile) throws IOException, JsonParseException, JsonMappingException {	try {	return mapper.readValue(jsonFile, classType);	} catch (IOException e) {	
exception while parsing json file 

public synchronized T fromResource(String resource) throws IOException, JsonParseException, JsonMappingException {	InputStream resStream = null;	try {	resStream = this.getClass().getResourceAsStream(resource);	if (resStream == null) {	throw new FileNotFoundException(resource);	}	return mapper.readValue(resStream, classType);	} catch (IOException e) {	
exception while parsing json resource 

========================= hadoop sample_2704 =========================

public void getTrashDirectoryForBlockFile(String fileName, int nestingLevel) {	final String blockFileSubdir = makeRandomBlockFileSubdir(nestingLevel);	final String blockFileName = fileName;	String testFilePath = storage.getSingularStorageDir().getRoot() + File.separator + Storage.STORAGE_DIR_CURRENT + blockFileSubdir + blockFileName;	String expectedTrashPath = storage.getSingularStorageDir().getRoot() + File.separator + BlockPoolSliceStorage.TRASH_ROOT_DIR + blockFileSubdir.substring(0, blockFileSubdir.length() - 1);	
got subdir 

public void getTrashDirectoryForBlockFile(String fileName, int nestingLevel) {	final String blockFileSubdir = makeRandomBlockFileSubdir(nestingLevel);	final String blockFileName = fileName;	String testFilePath = storage.getSingularStorageDir().getRoot() + File.separator + Storage.STORAGE_DIR_CURRENT + blockFileSubdir + blockFileName;	String expectedTrashPath = storage.getSingularStorageDir().getRoot() + File.separator + BlockPoolSliceStorage.TRASH_ROOT_DIR + blockFileSubdir.substring(0, blockFileSubdir.length() - 1);	
generated file path 

public void getRestoreDirectoryForBlockFile(String fileName, int nestingLevel) {	BlockPoolSliceStorage storage = makeBlockPoolStorage();	final String blockFileSubdir = makeRandomBlockFileSubdir(nestingLevel);	final String blockFileName = fileName;	String deletedFilePath = storage.getSingularStorageDir().getRoot() + File.separator + BlockPoolSliceStorage.TRASH_ROOT_DIR + blockFileSubdir + blockFileName;	String expectedRestorePath = storage.getSingularStorageDir().getRoot() + File.separator + Storage.STORAGE_DIR_CURRENT + blockFileSubdir.substring(0, blockFileSubdir.length() - 1);	
generated deleted file path 

========================= hadoop sample_7290 =========================

private ClientSCMMetrics() {	registry = new MetricsRegistry("clientRequests");	
initialized 

========================= hadoop sample_389 =========================

reducerUnconditionalPreemptionDelayMs = 1000 * conf.getInt( MRJobConfig.MR_JOB_REDUCER_UNCONDITIONAL_PREEMPT_DELAY_SEC, MRJobConfig.DEFAULT_MR_JOB_REDUCER_UNCONDITIONAL_PREEMPT_DELAY_SEC);	reducerNoHeadroomPreemptionDelayMs = conf.getInt( MRJobConfig.MR_JOB_REDUCER_PREEMPT_DELAY_SEC, MRJobConfig.DEFAULT_MR_JOB_REDUCER_PREEMPT_DELAY_SEC) * 1000;	maxRunningMaps = conf.getInt(MRJobConfig.JOB_RUNNING_MAP_LIMIT, MRJobConfig.DEFAULT_JOB_RUNNING_MAP_LIMIT);	maxRunningReduces = conf.getInt(MRJobConfig.JOB_RUNNING_REDUCE_LIMIT, MRJobConfig.DEFAULT_JOB_RUNNING_REDUCE_LIMIT);	RackResolver.init(conf);	retryInterval = getConfig().getLong(MRJobConfig.MR_AM_TO_RM_WAIT_INTERVAL_MS, MRJobConfig.DEFAULT_MR_AM_TO_RM_WAIT_INTERVAL_MS);	mapNodeLabelExpression = conf.get(MRJobConfig.MAP_NODE_LABEL_EXP);	reduceNodeLabelExpression = conf.get(MRJobConfig.REDUCE_NODE_LABEL_EXP);	retrystartTime = System.currentTimeMillis();	this.scheduledRequests.setNumOpportunisticMapsPercent( conf.getInt(MRJobConfig.MR_NUM_OPPORTUNISTIC_MAPS_PERCENT, MRJobConfig.DEFAULT_MR_NUM_OPPORTUNISTIC_MAPS_PERCENT));	
of the mappers will be scheduled using opportunistic containers 

protected void serviceStart() throws Exception {	this.eventHandlingThread = new Thread() {	public void run() {	ContainerAllocatorEvent event;	while (!stopped.get() && !Thread.currentThread().isInterrupted()) {	try {	event = RMContainerAllocator.this.eventQueue.take();	} catch (InterruptedException e) {	if (!stopped.get()) {	
returning interrupted 

try {	event = RMContainerAllocator.this.eventQueue.take();	} catch (InterruptedException e) {	if (!stopped.get()) {	}	return;	}	try {	handleEvent(event);	} catch (Throwable t) {	
error in handling event type to the containreallocator 

public void handle(ContainerAllocatorEvent event) {	int qSize = eventQueue.size();	if (qSize != 0 && qSize % 1000 == 0) {	
size of event queue in rmcontainerallocator is 

public void handle(ContainerAllocatorEvent event) {	int qSize = eventQueue.size();	if (qSize != 0 && qSize % 1000 == 0) {	}	int remCapacity = eventQueue.remainingCapacity();	if (remCapacity < 1000) {	
very low remaining capacity in the event queue of rmcontainerallocator 

recalculateReduceSchedule = true;	if (event.getType() == ContainerAllocator.EventType.CONTAINER_REQ) {	ContainerRequestEvent reqEvent = (ContainerRequestEvent) event;	boolean isMap = reqEvent.getAttemptID().getTaskId().getTaskType(). equals(TaskType.MAP);	if (isMap) {	handleMapContainerRequest(reqEvent);	} else {	handleReduceContainerRequest(reqEvent);	}	} else if ( event.getType() == ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {	
processing the event 

ContainerId containerId = assignedRequests.get(aId);	if (containerId != null) {	removed = true;	assignedRequests.remove(aId);	containersReleased++;	pendingRelease.add(containerId);	release(containerId);	}	}	if (!removed) {	
could not deallocate container for task attemptid 

private void handleReduceContainerRequest(ContainerRequestEvent reqEvent) {	assert(reqEvent.getAttemptID().getTaskId().getTaskType().equals( TaskType.REDUCE));	Resource supportedMaxContainerCapability = getMaxContainerCapability();	JobId jobId = getJob().getID();	if (reduceResourceRequest.equals(Resources.none())) {	reduceResourceRequest = reqEvent.getCapability();	eventHandler.handle(new JobHistoryEvent(jobId, new NormalizedResourceEvent( org.apache.hadoop.mapreduce.TaskType.REDUCE, reduceResourceRequest.getMemorySize())));	
reduceresourcerequest 

private void handleMapContainerRequest(ContainerRequestEvent reqEvent) {	assert(reqEvent.getAttemptID().getTaskId().getTaskType().equals( TaskType.MAP));	Resource supportedMaxContainerCapability = getMaxContainerCapability();	JobId jobId = getJob().getID();	if (mapResourceRequest.equals(Resources.none())) {	mapResourceRequest = reqEvent.getCapability();	eventHandler.handle(new JobHistoryEvent(jobId, new NormalizedResourceEvent( org.apache.hadoop.mapreduce.TaskType.MAP, mapResourceRequest.getMemorySize())));	
mapresourcerequest 

private void preemptReducer(int hangingMapRequests) {	clearAllPendingReduceRequests();	int preemptionReduceNumForOneMap = ResourceCalculatorUtils.divideAndCeilContainers(mapResourceRequest, reduceResourceRequest, getSchedulerResourceTypes());	int preemptionReduceNumForPreemptionLimit = ResourceCalculatorUtils.divideAndCeilContainers( Resources.multiply(getResourceLimit(), maxReducePreemptionLimit), reduceResourceRequest, getSchedulerResourceTypes());	int preemptionReduceNumForAllMaps = ResourceCalculatorUtils.divideAndCeilContainers( Resources.multiply(mapResourceRequest, hangingMapRequests), reduceResourceRequest, getSchedulerResourceTypes());	int toPreempt = Math.min(Math.max(preemptionReduceNumForOneMap, preemptionReduceNumForPreemptionLimit), preemptionReduceNumForAllMaps);	
going to preempt due to lack of space for maps 

public void scheduleReduces( int totalMaps, int completedMaps, int scheduledMaps, int scheduledReduces, int assignedMaps, int assignedReduces, Resource mapResourceReqt, Resource reduceResourceReqt, int numPendingReduces, float maxReduceRampupLimit, float reduceSlowStart) {	if (numPendingReduces == 0) {	return;	}	Resource headRoom = getAvailableResources();	LOG.info("Recalculating schedule, headroom=" + headRoom);	if (!getIsReduceStarted()) {	int completedMapsForReduceSlowstart = (int)Math.ceil(reduceSlowStart * totalMaps);	if(completedMaps < completedMapsForReduceSlowstart) {	
reduce slow start threshold not met completedmapsforreduceslowstart 

if (numPendingReduces == 0) {	return;	}	Resource headRoom = getAvailableResources();	LOG.info("Recalculating schedule, headroom=" + headRoom);	if (!getIsReduceStarted()) {	int completedMapsForReduceSlowstart = (int)Math.ceil(reduceSlowStart * totalMaps);	if(completedMaps < completedMapsForReduceSlowstart) {	return;	} else {	
reduce slow start threshold reached scheduling reduces 

LOG.info("Recalculating schedule, headroom=" + headRoom);	if (!getIsReduceStarted()) {	int completedMapsForReduceSlowstart = (int)Math.ceil(reduceSlowStart * totalMaps);	if(completedMaps < completedMapsForReduceSlowstart) {	return;	} else {	setIsReduceStarted(true);	}	}	if (scheduledMaps == 0 && numPendingReduces > 0) {	
all maps assigned ramping up all remaining reduces 

Resource idealReduceResourceLimit = Resources.multiply(totalResourceLimit, Math.min(completedMapPercent, maxReduceRampupLimit));	Resource ideaMapResourceLimit = Resources.subtract(totalResourceLimit, idealReduceResourceLimit);	if (ResourceCalculatorUtils.computeAvailableContainers(ideaMapResourceLimit, mapResourceReqt, getSchedulerResourceTypes()) >= (scheduledMaps + assignedMaps)) {	Resource unusedMapResourceLimit = Resources.subtract(ideaMapResourceLimit, netScheduledMapResource);	finalReduceResourceLimit = Resources.add(idealReduceResourceLimit, unusedMapResourceLimit);	finalMapResourceLimit = Resources.subtract(totalResourceLimit, finalReduceResourceLimit);	} else {	finalMapResourceLimit = ideaMapResourceLimit;	finalReduceResourceLimit = idealReduceResourceLimit;	}	
completedmappercent totalresourcelimit finalmapresourcelimit finalreduceresourcelimit netscheduledmapresource netscheduledreduceresource 

Resource unusedMapResourceLimit = Resources.subtract(ideaMapResourceLimit, netScheduledMapResource);	finalReduceResourceLimit = Resources.add(idealReduceResourceLimit, unusedMapResourceLimit);	finalMapResourceLimit = Resources.subtract(totalResourceLimit, finalReduceResourceLimit);	} else {	finalMapResourceLimit = ideaMapResourceLimit;	finalReduceResourceLimit = idealReduceResourceLimit;	}	int rampUp = ResourceCalculatorUtils.computeAvailableContainers(Resources.subtract( finalReduceResourceLimit, netScheduledReduceResource), reduceResourceReqt, getSchedulerResourceTypes());	if (rampUp > 0) {	rampUp = Math.min(rampUp, numPendingReduces);	
ramping up 

finalMapResourceLimit = ideaMapResourceLimit;	finalReduceResourceLimit = idealReduceResourceLimit;	}	int rampUp = ResourceCalculatorUtils.computeAvailableContainers(Resources.subtract( finalReduceResourceLimit, netScheduledReduceResource), reduceResourceReqt, getSchedulerResourceTypes());	if (rampUp > 0) {	rampUp = Math.min(rampUp, numPendingReduces);	rampUpReduces(rampUp);	} else if (rampUp < 0) {	int rampDown = -1 * rampUp;	rampDown = Math.min(rampDown, scheduledReduces);	
ramping down 

applyConcurrentTaskLimits();	Resource headRoom = Resources.clone(getAvailableResources());	AllocateResponse response;	try {	response = makeRemoteRequest();	retrystartTime = System.currentTimeMillis();	} catch (ApplicationAttemptNotFoundException e ) {	eventHandler.handle(new JobEvent(this.getJob().getID(), JobEventType.JOB_AM_REBOOT));	throw new RMContainerAllocationException( "Resource Manager doesn't recognize AttemptId: " + this.getContext().getApplicationAttemptId(), e);	} catch (ApplicationMasterNotRegisteredException e) {	
applicationmaster is out of sync with resourcemanager hence resync and send outstanding requests 

return null;	} catch (InvalidLabelResourceRequestException e) {	String diagMsg = "Requested node-label-expression is invalid: " + StringUtils.stringifyException(e);	LOG.info(diagMsg);	JobId jobId = this.getJob().getID();	eventHandler.handle(new JobDiagnosticsUpdateEvent(jobId, diagMsg));	eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));	throw e;	} catch (Exception e) {	if (System.currentTimeMillis() - retrystartTime >= retryInterval) {	
could not contact rm after milliseconds 

}	List<ContainerStatus> finishedContainers = response.getCompletedContainersStatuses();	if (newContainers.size() + finishedContainers.size() > 0 || !headRoom.equals(newHeadRoom)) {	recalculateReduceSchedule = true;	if (LOG.isDebugEnabled() && !headRoom.equals(newHeadRoom)) {	LOG.debug("headroom=" + newHeadRoom);	}	}	if (LOG.isDebugEnabled()) {	for (Container cont : newContainers) {	
received new container 

if (nodeState.isUnusable()) {	unusableNodes.add(nr.getNodeId());	}	}	for (int i = 0; i < 2; ++i) {	HashMap<TaskAttemptId, Container> taskSet = i == 0 ? assignedRequests.maps : assignedRequests.reduces;	for (Map.Entry<TaskAttemptId, Container> entry : taskSet.entrySet()) {	TaskAttemptId tid = entry.getKey();	NodeId taskAttemptNodeId = entry.getValue().getNodeId();	if (unusableNodes.contains(taskAttemptNodeId)) {	
killing taskattempt because it is running on unusable node 

private void assign(List<Container> allocatedContainers) {	Iterator<Container> it = allocatedContainers.iterator();	
got allocated containers 

private void assign(List<Container> allocatedContainers) {	Iterator<Container> it = allocatedContainers.iterator();	containersAllocated += allocatedContainers.size();	int reducePending = reduces.size();	while (it.hasNext()) {	Container allocated = it.next();	if (LOG.isDebugEnabled()) {	
assigning container with priority to nm 

Priority priority = allocated.getPriority();	Resource allocatedResource = allocated.getResource();	if (PRIORITY_FAST_FAIL_MAP.equals(priority) || PRIORITY_MAP.equals(priority) || PRIORITY_OPPORTUNISTIC_MAP.equals(priority)) {	if (ResourceCalculatorUtils.computeAvailableContainers(allocatedResource, mapResourceRequest, getSchedulerResourceTypes()) <= 0 || maps.isEmpty()) {	LOG.info("Cannot assign container " + allocated + " for a map as either " + " container memory less than required " + mapResourceRequest + " or no pending map tasks - maps.isEmpty=" + maps.isEmpty());	isAssignable = false;	}	}	else if (PRIORITY_REDUCE.equals(priority)) {	if (ResourceCalculatorUtils.computeAvailableContainers(allocatedResource, reduceResourceRequest, getSchedulerResourceTypes()) <= 0 || (reducePending <= 0)) {	
cannot assign container for a reduce as either container memory less than required or no pending reduce tasks 

isAssignable = false;	}	}	else if (PRIORITY_REDUCE.equals(priority)) {	if (ResourceCalculatorUtils.computeAvailableContainers(allocatedResource, reduceResourceRequest, getSchedulerResourceTypes()) <= 0 || (reducePending <= 0)) {	isAssignable = false;	} else {	reducePending--;	}	} else {	
container allocated at unwanted priority returning to rm 

} else {	isAssignable = false;	}	if(!isAssignable) {	containerNotAssigned(allocated);	it.remove();	continue;	}	String allocatedHost = allocated.getNodeId().getHost();	if (isNodeBlacklisted(allocatedHost)) {	
got allocated container on a blacklisted host releasing container 

}	if(!isAssignable) {	containerNotAssigned(allocated);	it.remove();	continue;	}	String allocatedHost = allocated.getNodeId().getHost();	if (isNodeBlacklisted(allocatedHost)) {	ContainerRequest toBeReplacedReq = getContainerReqToReplace(allocated);	if (toBeReplacedReq != null) {	
placing a new container request for task attempt 

decContainerReq(toBeReplacedReq);	if (toBeReplacedReq.attemptID.getTaskId().getTaskType() == TaskType.MAP) {	maps.put(newReq.attemptID, newReq);	}	else {	reduces.put(newReq.attemptID, newReq);	}	addContainerReq(newReq);	}	else {	
could not map allocated container to a valid request releasing allocated container 

}	containerNotAssigned(allocated);	it.remove();	continue;	}	}	assignContainers(allocatedContainers);	it = allocatedContainers.iterator();	while (it.hasNext()) {	Container allocated = it.next();	
releasing unassigned container 

private void containerAssigned(Container allocated, ContainerRequest assigned) {	decContainerReq(assigned);	eventHandler.handle(new TaskAttemptContainerAssignedEvent( assigned.attemptID, allocated, applicationACLs));	assignedRequests.add(allocated, assigned.attemptID);	if (LOG.isDebugEnabled()) {	
assigned container to task on node 

private ContainerRequest assignWithoutLocality(Container allocated) {	ContainerRequest assigned = null;	Priority priority = allocated.getPriority();	if (PRIORITY_FAST_FAIL_MAP.equals(priority)) {	
assigning container to fast fail map 

private ContainerRequest assignWithoutLocality(Container allocated) {	ContainerRequest assigned = null;	Priority priority = allocated.getPriority();	if (PRIORITY_FAST_FAIL_MAP.equals(priority)) {	assigned = assignToFailedMap(allocated);	} else if (PRIORITY_REDUCE.equals(priority)) {	if (LOG.isDebugEnabled()) {	
assigning container to reduce 

private ContainerRequest getContainerReqToReplace(Container allocated) {	
finding containerreq for allocated container 

private ContainerRequest getContainerReqToReplace(Container allocated) {	Priority priority = allocated.getPriority();	ContainerRequest toBeReplaced = null;	if (PRIORITY_FAST_FAIL_MAP.equals(priority)) {	
replacing fast fail map container 

private ContainerRequest getContainerReqToReplace(Container allocated) {	Priority priority = allocated.getPriority();	ContainerRequest toBeReplaced = null;	if (PRIORITY_FAST_FAIL_MAP.equals(priority)) {	Iterator<TaskAttemptId> iter = earlierFailedMaps.iterator();	while (toBeReplaced == null && iter.hasNext()) {	toBeReplaced = maps.get(iter.next());	}	
found replacement 

Priority priority = allocated.getPriority();	ContainerRequest toBeReplaced = null;	if (PRIORITY_FAST_FAIL_MAP.equals(priority)) {	Iterator<TaskAttemptId> iter = earlierFailedMaps.iterator();	while (toBeReplaced == null && iter.hasNext()) {	toBeReplaced = maps.get(iter.next());	}	return toBeReplaced;	}	else if (PRIORITY_MAP.equals(priority) || PRIORITY_OPPORTUNISTIC_MAP.equals(priority)) {	
replacing map container 

}	else {	TaskAttemptId tId = maps.keySet().iterator().next();	toBeReplaced = maps.remove(tId);	}	}	else if (PRIORITY_REDUCE.equals(priority)) {	TaskAttemptId tId = reduces.keySet().iterator().next();	toBeReplaced = reduces.remove(tId);	}	
found replacement 

private ContainerRequest assignToFailedMap(Container allocated) {	ContainerRequest assigned = null;	while (assigned == null && earlierFailedMaps.size() > 0 && canAssignMaps()) {	TaskAttemptId tId = earlierFailedMaps.removeFirst();	if (maps.containsKey(tId)) {	assigned = maps.remove(tId);	JobCounterUpdateEvent jce = new JobCounterUpdateEvent(assigned.attemptID.getTaskId().getJobId());	jce.addCounterUpdate(JobCounter.OTHER_LOCAL_MAPS, 1);	eventHandler.handle(jce);	
assigned from earlierfailedmaps 

private ContainerRequest assignToReduce(Container allocated) {	ContainerRequest assigned = null;	if (assigned == null && reduces.size() > 0 && canAssignReduces()) {	TaskAttemptId tId = reduces.keySet().iterator().next();	assigned = reduces.remove(tId);	
assigned to reduce 

Iterator<Container> it = allocatedContainers.iterator();	while(it.hasNext() && maps.size() > 0 && canAssignMaps()){	Container allocated = it.next();	Priority priority = allocated.getPriority();	assert (PRIORITY_MAP.equals(priority) || PRIORITY_OPPORTUNISTIC_MAP.equals(priority));	if (!PRIORITY_OPPORTUNISTIC_MAP.equals(priority)) {	String host = allocated.getNodeId().getHost();	LinkedList<TaskAttemptId> list = mapsHostMapping.get(host);	while (list != null && list.size() > 0) {	if (LOG.isDebugEnabled()) {	
host matched to the request list 

TaskAttemptId tId = list.removeFirst();	if (maps.containsKey(tId)) {	ContainerRequest assigned = maps.remove(tId);	containerAssigned(allocated, assigned);	it.remove();	JobCounterUpdateEvent jce = new JobCounterUpdateEvent(assigned.attemptID.getTaskId() .getJobId());	jce.addCounterUpdate(JobCounter.DATA_LOCAL_MAPS, 1);	eventHandler.handle(jce);	hostLocalAssigned++;	if (LOG.isDebugEnabled()) {	
assigned based on host match 

TaskAttemptId tId = list.removeFirst();	if (maps.containsKey(tId)) {	ContainerRequest assigned = maps.remove(tId);	containerAssigned(allocated, assigned);	it.remove();	JobCounterUpdateEvent jce = new JobCounterUpdateEvent(assigned.attemptID.getTaskId() .getJobId());	jce.addCounterUpdate(JobCounter.RACK_LOCAL_MAPS, 1);	eventHandler.handle(jce);	rackLocalAssigned++;	if (LOG.isDebugEnabled()) {	
assigned based on rack match 

Priority priority = allocated.getPriority();	assert (PRIORITY_MAP.equals(priority) || PRIORITY_OPPORTUNISTIC_MAP.equals(priority));	TaskAttemptId tId = maps.keySet().iterator().next();	ContainerRequest assigned = maps.remove(tId);	containerAssigned(allocated, assigned);	it.remove();	JobCounterUpdateEvent jce = new JobCounterUpdateEvent(assigned.attemptID.getTaskId().getJobId());	jce.addCounterUpdate(JobCounter.OTHER_LOCAL_MAPS, 1);	eventHandler.handle(jce);	if (LOG.isDebugEnabled()) {	
assigned based on match 

========================= hadoop sample_5307 =========================

private static boolean isNativeSnappyLoadable() {	boolean snappyAvailable = false;	boolean loaded = false;	try {	System.loadLibrary("snappy");	
snappy native library is available 

private static boolean isNativeSnappyLoadable() {	boolean snappyAvailable = false;	boolean loaded = false;	try {	System.loadLibrary("snappy");	snappyAvailable = true;	boolean hadoopNativeAvailable = NativeCodeLoader.isNativeCodeLoaded();	loaded = snappyAvailable && hadoopNativeAvailable;	if (loaded) {	
snappy native library loaded 

private static boolean isNativeSnappyLoadable() {	boolean snappyAvailable = false;	boolean loaded = false;	try {	System.loadLibrary("snappy");	snappyAvailable = true;	boolean hadoopNativeAvailable = NativeCodeLoader.isNativeCodeLoaded();	loaded = snappyAvailable && hadoopNativeAvailable;	if (loaded) {	} else {	
snappy native library not loaded 

boolean loaded = false;	try {	System.loadLibrary("snappy");	snappyAvailable = true;	boolean hadoopNativeAvailable = NativeCodeLoader.isNativeCodeLoaded();	loaded = snappyAvailable && hadoopNativeAvailable;	if (loaded) {	} else {	}	} catch (Throwable t) {	
failed to load snappy 

private boolean checkSetInputNullPointerException(Compressor compressor) {	try {	compressor.setInput(null, 0, 1);	} catch (NullPointerException npe) {	return true;	} catch (Exception ex) {	
checksetinputnullpointerexception error 

private boolean checkCompressNullPointerException(Compressor compressor, byte[] rawData) {	try {	compressor.setInput(rawData, 0, rawData.length);	compressor.compress(null, 0, 1);	} catch (NullPointerException npe) {	return true;	} catch (Exception ex) {	
checkcompressnullpointerexception error 

private boolean checkCompressNullPointerException( Decompressor decompressor, byte[] rawData) {	try {	decompressor.setInput(rawData, 0, rawData.length);	decompressor.decompress(null, 0, 1);	} catch (NullPointerException npe) {	return true;	} catch (Exception ex) {	
checkcompressnullpointerexception error 

private boolean checkSetInputNullPointerException( Decompressor decompressor) {	try {	decompressor.setInput(null, 0, 1);	} catch (NullPointerException npe) {	return true;	} catch (Exception ex) {	
checksetinputnullpointerexception error 

private boolean checkSetInputArrayIndexOutOfBoundsException( Compressor compressor) {	try {	compressor.setInput(new byte[] { (byte) 0 }, 0, -1);	} catch (ArrayIndexOutOfBoundsException e) {	return true;	} catch (Exception e) {	
checksetinputarrayindexoutofboundsexception error 

private boolean checkCompressArrayIndexOutOfBoundsException( Compressor compressor, byte[] rawData) {	try {	compressor.setInput(rawData, 0, rawData.length);	compressor.compress(new byte[rawData.length], 0, -1);	} catch (ArrayIndexOutOfBoundsException e) {	return true;	} catch (Exception e) {	
checkcompressarrayindexoutofboundsexception error 

private boolean checkCompressArrayIndexOutOfBoundsException( Decompressor decompressor, byte[] rawData) {	try {	decompressor.setInput(rawData, 0, rawData.length);	decompressor.decompress(new byte[rawData.length], 0, -1);	} catch (ArrayIndexOutOfBoundsException e) {	return true;	} catch (Exception e) {	
checkcompressarrayindexoutofboundsexception error 

private boolean checkSetInputArrayIndexOutOfBoundsException( Decompressor decompressor) {	try {	decompressor.setInput(new byte[] { (byte) 0 }, 0, -1);	} catch (ArrayIndexOutOfBoundsException e) {	return true;	} catch (Exception e) {	
checknullpointerexception error 

========================= hadoop sample_2991 =========================

public synchronized void addPersistedDelegationToken( TokenIdent identifier, long renewDate) throws IOException {	if (running) {	throw new IOException( "Can't add persisted delegation token to a running SecretManager.");	}	int keyId = identifier.getMasterKeyId();	DelegationKey dKey = allKeys.get(keyId);	if (dKey == null) {	
no key found for persisted identifier 

private void updateCurrentKey() throws IOException {	
updating the current master key for generating delegation tokens 

protected synchronized byte[] createPassword(TokenIdent identifier) {	int sequenceNum;	long now = Time.now();	sequenceNum = incrementDelegationTokenSeqNum();	identifier.setIssueDate(now);	identifier.setMaxDate(now + tokenMaxLifetime);	identifier.setMasterKeyId(currentKey.getKeyId());	identifier.setSequenceNumber(sequenceNum);	
creating password for identifier currentkey 

sequenceNum = incrementDelegationTokenSeqNum();	identifier.setIssueDate(now);	identifier.setMaxDate(now + tokenMaxLifetime);	identifier.setMasterKeyId(currentKey.getKeyId());	identifier.setSequenceNumber(sequenceNum);	byte[] password = createPassword(identifier.getBytes(), currentKey.getKey());	DelegationTokenInformation tokenInfo = new DelegationTokenInformation(now + tokenRenewInterval, password, getTrackingIdIfEnabled(identifier));	try {	storeToken(identifier, tokenInfo);	} catch (IOException ioe) {	
could not store token 

public synchronized long renewToken(Token<TokenIdent> token, String renewer) throws InvalidToken, IOException {	ByteArrayInputStream buf = new ByteArrayInputStream(token.getIdentifier());	DataInputStream in = new DataInputStream(buf);	TokenIdent id = createIdentifier();	id.readFields(in);	
token renewal for identifier total currenttokens 

public synchronized TokenIdent cancelToken(Token<TokenIdent> token, String canceller) throws IOException {	ByteArrayInputStream buf = new ByteArrayInputStream(token.getIdentifier());	DataInputStream in = new DataInputStream(buf);	TokenIdent id = createIdentifier();	id.readFields(in);	
token cancellation requested for identifier 

Map.Entry<TokenIdent, DelegationTokenInformation> entry = i.next();	long renewDate = entry.getValue().getRenewDate();	if (renewDate < now) {	expiredTokens.add(entry.getKey());	i.remove();	}	}	}	for (TokenIdent ident : expiredTokens) {	logExpireToken(ident);	
removing expired token 

public void stopThreads() {	
stopping expired delegation token remover thread 

public void run() {	LOG.info("Starting expired delegation token remover thread, " + "tokenRemoverScanInterval=" + tokenRemoverScanInterval try {	while (running) {	long now = Time.now();	if (lastMasterKeyUpdate + keyUpdateInterval < now) {	try {	rollMasterKey();	lastMasterKeyUpdate = now;	} catch (IOException e) {	
master key updating failed 

} catch (IOException e) {	}	}	if (lastTokenCacheCleanup + tokenRemoverScanInterval < now) {	removeExpiredToken();	lastTokenCacheCleanup = now;	}	try {	Thread.sleep(Math.min(5000, keyUpdateInterval));	} catch (InterruptedException ie) {	
expiredtokenremover received 

if (lastTokenCacheCleanup + tokenRemoverScanInterval < now) {	removeExpiredToken();	lastTokenCacheCleanup = now;	}	try {	Thread.sleep(Math.min(5000, keyUpdateInterval));	} catch (InterruptedException ie) {	}	}	} catch (Throwable t) {	
expiredtokenremover thread received unexpected exception 

========================= hadoop sample_3729 =========================

if (result != null) {	Map<byte[], byte[]> columns = result.getFamilyMap(columnFamilyBytes);	for (Entry<byte[], byte[]> entry : columns.entrySet()) {	byte[] columnKey = entry.getKey();	if (columnKey != null && columnKey.length > 0) {	K converterColumnKey = null;	if (columnPrefixBytes == null) {	try {	converterColumnKey = keyConverter.decode(columnKey);	} catch (IllegalArgumentException iae) {	
illegal column found skipping this column 

continue;	}	} else {	byte[][] columnNameParts = Separator.QUALIFIERS.split(columnKey, 2);	if (columnNameParts.length > 0) {	byte[] actualColumnPrefixBytes = columnNameParts[0];	if (Bytes.equals(columnPrefixBytes, actualColumnPrefixBytes) && columnNameParts.length == 2) {	try {	converterColumnKey = keyConverter.decode(columnNameParts[1]);	} catch (IllegalArgumentException iae) {	
illegal column found skipping this column 

========================= hadoop sample_1201 =========================

public MutableGaugeLong lookupGauge(String name) {	MutableMetric metric = lookupMetric(name);	if (metric == null) {	
no gauge 

public MutableQuantiles lookupQuantiles(String name) {	MutableMetric metric = lookupMetric(name);	if (metric == null) {	
no quantiles 

public void incrementGauge(Statistic op, long count) {	MutableGaugeLong gauge = lookupGauge(op.getSymbol());	if (gauge != null) {	gauge.incr(count);	} else {	
no gauge 

public void decrementGauge(Statistic op, long count) {	MutableGaugeLong gauge = lookupGauge(op.getSymbol());	if (gauge != null) {	gauge.decr(count);	} else {	
no gauge 

public void close() {	if (bytesPendingUpload.get() > 0) {	
closing output stream statistics while data is still marked as pending upload in 

========================= hadoop sample_5995 =========================

public void setConf(Configuration conf) {	conf = new HdfsConfiguration(conf);	String nameNodePrincipal = conf.get( DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY, "");	if (LOG.isDebugEnabled()) {	
using nn principal 

========================= hadoop sample_7802 =========================

private void checkNNVersion(NamespaceInfo nsInfo) throws IncorrectVersionException {	String nnVersion = nsInfo.getSoftwareVersion();	String minimumNameNodeVersion = dnConf.getMinimumNameNodeVersion();	if (VersionUtil.compareVersions(nnVersion, minimumNameNodeVersion) < 0) {	IncorrectVersionException ive = new IncorrectVersionException( minimumNameNodeVersion, nnVersion, "NameNode", "DataNode");	LOG.warn(ive.getMessage());	throw ive;	}	String dnVersion = VersionInfo.getVersion();	if (!nnVersion.equals(dnVersion)) {	
reported namenode version does not match datanode version but is within acceptable limits note this is normal during a rolling upgrade 

private void handleRollingUpgradeStatus(HeartbeatResponse resp) throws IOException {	RollingUpgradeStatus rollingUpgradeStatus = resp.getRollingUpdateStatus();	if (rollingUpgradeStatus != null && rollingUpgradeStatus.getBlockPoolId().compareTo(bpos.getBlockPoolId()) != 0) {	
invalid blockpoolid in heartbeatresponse expected 

final long startTime = scheduler.monotonicNow();	final boolean sendHeartbeat = scheduler.isHeartbeatDue(startTime);	HeartbeatResponse resp = null;	if (sendHeartbeat) {	boolean requestBlockReportLease = (fullBlockReportLeaseId == 0) && scheduler.isBlockReportDue(startTime);	if (!dn.areHeartbeatsDisabledForTests()) {	resp = sendHeartBeat(requestBlockReportLease);	assert resp != null;	if (resp.getFullBlockReportLeaseId() != 0) {	if (fullBlockReportLeaseId != 0) {	
sent back a full block report lease id of but we already have a lease id of overwriting old lease id 

dn.getMetrics().addHeartbeat(scheduler.monotonicNow() - startTime);	bpos.updateActorStatesFromHeartbeat( this, resp.getNameNodeHaState());	state = resp.getNameNodeHaState().getState();	if (state == HAServiceState.ACTIVE) {	handleRollingUpgradeStatus(resp);	}	long startProcessCommands = monotonicNow();	if (!processCommand(resp.getCommands())) continue;	long endProcessCommands = monotonicNow();	if (endProcessCommands - startProcessCommands > 2000) {	
took ms to process commands from nn 

if (endProcessCommands - startProcessCommands > 2000) {	}	}	}	if (ibrManager.sendImmediately() || sendHeartbeat) {	ibrManager.sendIBRs(bpNamenode, bpRegistration, bpos.getBlockPoolId());	}	List<DatanodeCommand> cmds = null;	boolean forceFullBr = scheduler.forceFullBlockReport.getAndSet(false);	if (forceFullBr) {	
forcing a full block report to 

DatanodeCommand cmd = cacheReport();	processCommand(new DatanodeCommand[]{ cmd });	}	if (sendHeartbeat) {	dn.getMetrics().addHeartbeatTotal( scheduler.monotonicNow() - startTime);	}	ibrManager.waitTillNextIBR(scheduler.getHeartbeatWaitTime());	} catch(RemoteException re) {	String reClass = re.getClassName();	if (UnregisteredNodeException.class.getName().equals(reClass) || DisallowedDatanodeException.class.getName().equals(reClass) || IncorrectVersionException.class.getName().equals(reClass)) {	
is shutting down 

if (sendHeartbeat) {	dn.getMetrics().addHeartbeatTotal( scheduler.monotonicNow() - startTime);	}	ibrManager.waitTillNextIBR(scheduler.getHeartbeatWaitTime());	} catch(RemoteException re) {	String reClass = re.getClassName();	if (UnregisteredNodeException.class.getName().equals(reClass) || DisallowedDatanodeException.class.getName().equals(reClass) || IncorrectVersionException.class.getName().equals(reClass)) {	shouldServiceRun = false;	return;	}	
remoteexception in offerservice 

}	ibrManager.waitTillNextIBR(scheduler.getHeartbeatWaitTime());	} catch(RemoteException re) {	String reClass = re.getClassName();	if (UnregisteredNodeException.class.getName().equals(reClass) || DisallowedDatanodeException.class.getName().equals(reClass) || IncorrectVersionException.class.getName().equals(reClass)) {	shouldServiceRun = false;	return;	}	sleepAfterException();	} catch (IOException e) {	
ioexception in offerservice 

public void run() {	
starting to offer service 

public void run() {	try {	while (true) {	try {	connectToNNAndHandshake();	break;	} catch (IOException ioe) {	runningState = RunningState.INIT_FAILED;	if (shouldRetryInit()) {	
initialization failed for 

while (true) {	try {	connectToNNAndHandshake();	break;	} catch (IOException ioe) {	runningState = RunningState.INIT_FAILED;	if (shouldRetryInit()) {	sleepAndLogInterrupts(5000, "initializing");	} else {	runningState = RunningState.FAILED;	
initialization failed for exiting 

}	}	runningState = RunningState.RUNNING;	if (initialRegistrationComplete != null) {	initialRegistrationComplete.countDown();	}	while (shouldRun()) {	try {	offerService();	} catch (Exception ex) {	
exception in bpofferservice for 

}	while (shouldRun()) {	try {	offerService();	} catch (Exception ex) {	sleepAndLogInterrupts(5000, "offering service");	}	}	runningState = RunningState.EXITED;	} catch (Throwable ex) {	
unexpected exception in block pool 

try {	offerService();	} catch (Exception ex) {	sleepAndLogInterrupts(5000, "offering service");	}	}	runningState = RunningState.EXITED;	} catch (Throwable ex) {	runningState = RunningState.FAILED;	} finally {	
ending block pool service for 

while (shouldRun()) {	try {	if (lifelineNamenode == null) {	lifelineNamenode = dn.connectToLifelineNN(lifelineNnAddr);	}	sendLifelineIfDue();	Thread.sleep(scheduler.getLifelineWaitTime());	} catch (InterruptedException e) {	Thread.currentThread().interrupt();	} catch (IOException e) {	
ioexception in lifelinesender for 

if (lifelineNamenode == null) {	lifelineNamenode = dn.connectToLifelineNN(lifelineNnAddr);	}	sendLifelineIfDue();	Thread.sleep(scheduler.getLifelineWaitTime());	} catch (InterruptedException e) {	Thread.currentThread().interrupt();	} catch (IOException e) {	}	}	
lifelinesender for exiting 

public void start() {	lifelineThread = new Thread(this, formatThreadName("lifeline", lifelineNnAddr));	lifelineThread.setDaemon(true);	lifelineThread.setUncaughtExceptionHandler( new Thread.UncaughtExceptionHandler() {	public void uncaughtException(Thread thread, Throwable t) {	
terminating on unexpected exception 

private void sendLifelineIfDue() throws IOException {	long startTime = scheduler.monotonicNow();	if (!scheduler.isLifelineDue(startTime)) {	if (LOG.isDebugEnabled()) {	
skipping sending lifeline for because it is not due 

private void sendLifelineIfDue() throws IOException {	long startTime = scheduler.monotonicNow();	if (!scheduler.isLifelineDue(startTime)) {	if (LOG.isDebugEnabled()) {	}	return;	}	if (dn.areHeartbeatsDisabledForTests()) {	if (LOG.isDebugEnabled()) {	
skipping sending lifeline for because heartbeats are disabled for tests 

private void sendLifeline() throws IOException {	StorageReport[] reports = dn.getFSDataset().getStorageReports(bpos.getBlockPoolId());	if (LOG.isDebugEnabled()) {	
sending lifeline with storage reports from service actor 

========================= hadoop sample_7947 =========================

private void startUDPServer() {	SimpleUdpServer udpServer = new SimpleUdpServer(rpcProgram.getPort(), rpcProgram, 1);	rpcProgram.startDaemons();	try {	udpServer.run();	} catch (Throwable e) {	
failed to start the udp server 

private void startTCPServer() {	SimpleTcpServer tcpServer = new SimpleTcpServer(rpcProgram.getPort(), rpcProgram, 1);	rpcProgram.startDaemons();	try {	tcpServer.run();	} catch (Throwable e) {	
failed to start the tcp server 

public void start(boolean register) {	startUDPServer();	startTCPServer();	if (register) {	ShutdownHookManager.get().addShutdownHook(new Unregister(), SHUTDOWN_HOOK_PRIORITY);	try {	rpcProgram.register(PortmapMapping.TRANSPORT_UDP, udpBoundPort);	rpcProgram.register(PortmapMapping.TRANSPORT_TCP, tcpBoundPort);	} catch (Throwable e) {	
failed to register the mount service 

========================= hadoop sample_4392 =========================

G group = isFGroup ? fgroups.get(newGroupName) : groups.get(newGroupName);	if (group == null) {	group = groupFactory.newGroup(newGroupName, limits);	if (isFGroup) {	fgroups.put(newGroupName, group);	} else {	limits.checkGroups(groups.size() + 1);	groups.put(newGroupName, group);	}	if (groupNameInLegacyMap) {	
group is deprecated use instead 

========================= hadoop sample_5079 =========================

public void messageReceived(ChannelHandlerContext ctx, MessageEvent e) {	ChannelBuffer buf = (ChannelBuffer) e.getMessage();	XDR rsp = new XDR(buf.array());	if (rsp.getBytes().length == 0) {	
rsp length is zero why 

RpcReply reply = RpcReply.read(rsp);	int xid = reply.getXid();	if (xid != 0x8000004c) {	return;	}	int status = rsp.readInt();	if (status != Nfs3Status.NFS3_OK) {	LOG.error("Create failed, status =" + status);	return;	}	
create succeeded 

Arrays.fill(data1, (byte) 7);	Arrays.fill(data2, (byte) 8);	Arrays.fill(data3, (byte) 9);	NfsConfiguration conf = new NfsConfiguration();	WriteClient client = new WriteClient("localhost", conf.getInt( NfsConfigKeys.DFS_NFS_SERVER_PORT_KEY, NfsConfigKeys.DFS_NFS_SERVER_PORT_DEFAULT), create(), false);	client.run();	while (handle == null) {	Thread.sleep(1000);	System.out.println("handle is still null...");	}	
send request 

========================= hadoop sample_7044 =========================

rootInputDir = rootInputDir.makeQualified(lfs.getUri(), lfs.getWorkingDirectory());	Path rootTempDir = new Path(System.getProperty("test.build.data", System.getProperty("java.io.tmpdir")), "testTraceReader");	rootTempDir = rootTempDir.makeQualified(lfs.getUri(), lfs.getWorkingDirectory());	Path inputFile = new Path(rootInputDir, "wordcount.json.gz");	Path tempFile = new Path(rootTempDir, "gridmix3-wc.json");	InputStream origStdIn = System.in;	InputStream tmpIs = null;	try {	DebugGridmix dgm = new DebugGridmix();	JobStoryProducer jsp = dgm.createJobStoryProducer(inputFile.toString(), conf);	
verifying jobstory from compressed trace 

Path inputFile = new Path(rootInputDir, "wordcount.json.gz");	Path tempFile = new Path(rootTempDir, "gridmix3-wc.json");	InputStream origStdIn = System.in;	InputStream tmpIs = null;	try {	DebugGridmix dgm = new DebugGridmix();	JobStoryProducer jsp = dgm.createJobStoryProducer(inputFile.toString(), conf);	verifyWordCountJobStory(jsp.getNextJob());	expandGzippedTrace(lfs, inputFile, tempFile);	jsp = dgm.createJobStoryProducer(tempFile.toString(), conf);	
verifying jobstory from uncompressed trace 

InputStream tmpIs = null;	try {	DebugGridmix dgm = new DebugGridmix();	JobStoryProducer jsp = dgm.createJobStoryProducer(inputFile.toString(), conf);	verifyWordCountJobStory(jsp.getNextJob());	expandGzippedTrace(lfs, inputFile, tempFile);	jsp = dgm.createJobStoryProducer(tempFile.toString(), conf);	verifyWordCountJobStory(jsp.getNextJob());	tmpIs = lfs.open(tempFile);	System.setIn(tmpIs);	
verifying jobstory from trace in standard input 

public void testReplaySubmit() throws Exception {	policy = GridmixJobSubmissionPolicy.REPLAY;	
replay started at 

public void testReplaySubmit() throws Exception {	policy = GridmixJobSubmissionPolicy.REPLAY;	doSubmission(null, false);	
replay ended at 

public void testStressSubmit() throws Exception {	policy = GridmixJobSubmissionPolicy.STRESS;	
stress started at 

public void testStressSubmit() throws Exception {	policy = GridmixJobSubmissionPolicy.STRESS;	doSubmission(null, false);	
stress ended at 

========================= hadoop sample_6086 =========================

protected synchronized void refresh() {	try {	duShell.startRefresh();	} catch (IOException ioe) {	
could not get disk usage information for path 

========================= hadoop sample_4111 =========================

protected void serviceInit(Configuration conf) throws Exception {	
jobhistory init 

protected void serviceStop() throws Exception {	
stopping jobhistory 

protected void serviceStop() throws Exception {	if (scheduledExecutor != null) {	
stopping history cleaner move to done 

boolean interrupted = false;	long currentTime = System.currentTimeMillis();	while (!scheduledExecutor.isShutdown() && System.currentTimeMillis() > currentTime + 1000l && !interrupted) {	try {	Thread.sleep(20);	} catch (InterruptedException e) {	interrupted = true;	}	}	if (!scheduledExecutor.isShutdown()) {	
historycleanerservice move to done shutdown may not have succeeded forcing a shutdown 

public void run() {	try {	
starting scan to move intermediate done files 

public void run() {	try {	hsManager.scanIntermediateDirectory();	} catch (IOException e) {	
error while scanning intermediate done dir 

public void run() {	
history cleaner started 

public void run() {	try {	hsManager.clean();	} catch (IOException e) {	
error trying to clean up 

public void run() {	try {	hsManager.clean();	} catch (IOException e) {	}	
history cleaner complete 

public Map<JobId, Job> getAllJobs(ApplicationId appID) {	if (LOG.isDebugEnabled()) {	
called getalljobs appid 

public void refreshLoadedJobCache() {	if (getServiceState() == STATE.STARTED) {	if (storage instanceof CachedHistoryStorage) {	((CachedHistoryStorage) storage).refreshLoadedJobCache();	} else {	throw new UnsupportedOperationException(storage.getClass().getName() + " is expected to be an instance of " + CachedHistoryStorage.class.getName());	}	} else {	
failed to execute refreshloadedjobcache jobhistory service is not started 

if (getServiceState() == STATE.STARTED) {	conf = createConf();	long maxHistoryAge = conf.getLong(JHAdminConfig.MR_HISTORY_MAX_AGE_MS, JHAdminConfig.DEFAULT_MR_HISTORY_MAX_AGE);	hsManager.setMaxHistoryAge(maxHistoryAge);	if (futureHistoryCleaner != null) {	futureHistoryCleaner.cancel(false);	}	futureHistoryCleaner = null;	scheduleHistoryCleaner();	} else {	
failed to execute refreshjobretentionsettings job history service is not started 

========================= hadoop sample_5350 =========================

private static Unsafe safetyDance() {	try {	Field f = Unsafe.class.getDeclaredField("theUnsafe");	f.setAccessible(true);	return (Unsafe)f.get(null);	} catch (Throwable e) {	
failed to load misc unsafe 

public void free() {	try {	POSIX.munmap(baseAddress, mmappedLength);	} catch (IOException e) {	
failed to munmap 

public void free() {	try {	POSIX.munmap(baseAddress, mmappedLength);	} catch (IOException e) {	}	
freed 

========================= hadoop sample_6985 =========================

if (dns == null) {	return;	}	for (int i = 0; i < dns.length; i++) {	try {	if (dns[i] == null) {	continue;	}	dns[i].shutdown();	} catch (Exception e) {	
cannot close 

========================= hadoop sample_7228 =========================

protected <T, R> List<R> runInParallel(List<T> testContexts, final Function<T, R> func) {	ExecutorCompletionService<R> completionService = new ExecutorCompletionService<R>(this.getThreadPool());	LOG.info("Sending requests to endpoints asynchronously. Number of test contexts=" + testContexts.size());	for (int index = 0; index < testContexts.size(); index++) {	final T testContext = testContexts.get(index);	
adding request to threadpool for test context 

protected <T, R> List<R> runInParallel(List<T> testContexts, final Function<T, R> func) {	ExecutorCompletionService<R> completionService = new ExecutorCompletionService<R>(this.getThreadPool());	LOG.info("Sending requests to endpoints asynchronously. Number of test contexts=" + testContexts.size());	for (int index = 0; index < testContexts.size(); index++) {	final T testContext = testContexts.get(index);	completionService.submit(new Callable<R>() {	public R call() throws Exception {	
sending request test context 

protected <T, R> List<R> runInParallel(List<T> testContexts, final Function<T, R> func) {	ExecutorCompletionService<R> completionService = new ExecutorCompletionService<R>(this.getThreadPool());	LOG.info("Sending requests to endpoints asynchronously. Number of test contexts=" + testContexts.size());	for (int index = 0; index < testContexts.size(); index++) {	final T testContext = testContexts.get(index);	completionService.submit(new Callable<R>() {	public R call() throws Exception {	R response = null;	try {	response = func.invoke(testContext);	
successfully sent request for context 

ExecutorCompletionService<R> completionService = new ExecutorCompletionService<R>(this.getThreadPool());	LOG.info("Sending requests to endpoints asynchronously. Number of test contexts=" + testContexts.size());	for (int index = 0; index < testContexts.size(); index++) {	final T testContext = testContexts.get(index);	completionService.submit(new Callable<R>() {	public R call() throws Exception {	R response = null;	try {	response = func.invoke(testContext);	} catch (Throwable ex) {	
failed to process request for context 

});	}	ArrayList<R> responseList = new ArrayList<R>();	LOG.info("Waiting for responses from endpoints. Number of contexts=" + testContexts.size());	for (int i = 0; i < testContexts.size(); ++i) {	try {	final Future<R> future = completionService.take();	final R response = future.get(3000, TimeUnit.MILLISECONDS);	responseList.add(response);	} catch (Throwable e) {	
failed to process request 

protected <T> List<RegisterApplicationMasterResponseInfo<T>> registerApplicationMastersInParallel( final ArrayList<T> testContexts) {	List<RegisterApplicationMasterResponseInfo<T>> responses = runInParallel(testContexts, new Function<T, RegisterApplicationMasterResponseInfo<T>>() {	public RegisterApplicationMasterResponseInfo<T> invoke( T testContext) {	RegisterApplicationMasterResponseInfo<T> response = null;	try {	int index = testContexts.indexOf(testContext);	response = new RegisterApplicationMasterResponseInfo<T>( registerApplicationMaster(index), testContext);	Assert.assertNotNull(response.getResponse());	Assert.assertEquals(Integer.toString(index), response .getResponse().getQueue());	
sucessfully registered application master with test context 

List<RegisterApplicationMasterResponseInfo<T>> responses = runInParallel(testContexts, new Function<T, RegisterApplicationMasterResponseInfo<T>>() {	public RegisterApplicationMasterResponseInfo<T> invoke( T testContext) {	RegisterApplicationMasterResponseInfo<T> response = null;	try {	int index = testContexts.indexOf(testContext);	response = new RegisterApplicationMasterResponseInfo<T>( registerApplicationMaster(index), testContext);	Assert.assertNotNull(response.getResponse());	Assert.assertEquals(Integer.toString(index), response .getResponse().getQueue());	} catch (Throwable ex) {	response = null;	
failed to register application master with test context 

protected <T> List<FinishApplicationMasterResponseInfo<T>> finishApplicationMastersInParallel( final ArrayList<T> testContexts) {	List<FinishApplicationMasterResponseInfo<T>> responses = runInParallel(testContexts, new Function<T, FinishApplicationMasterResponseInfo<T>>() {	public FinishApplicationMasterResponseInfo<T> invoke( T testContext) {	FinishApplicationMasterResponseInfo<T> response = null;	try {	response = new FinishApplicationMasterResponseInfo<T>( finishApplicationMaster( testContexts.indexOf(testContext), FinalApplicationStatus.SUCCEEDED), testContext);	Assert.assertNotNull(response.getResponse());	
sucessfully finished application master with test contexts 

protected <T> List<FinishApplicationMasterResponseInfo<T>> finishApplicationMastersInParallel( final ArrayList<T> testContexts) {	List<FinishApplicationMasterResponseInfo<T>> responses = runInParallel(testContexts, new Function<T, FinishApplicationMasterResponseInfo<T>>() {	public FinishApplicationMasterResponseInfo<T> invoke( T testContext) {	FinishApplicationMasterResponseInfo<T> response = null;	try {	response = new FinishApplicationMasterResponseInfo<T>( finishApplicationMaster( testContexts.indexOf(testContext), FinalApplicationStatus.SUCCEEDED), testContext);	Assert.assertNotNull(response.getResponse());	} catch (Throwable ex) {	response = null;	
failed to finish application master with test context 

========================= hadoop sample_1606 =========================

String priorLastKey = null;	do {	PartialListing listing = fs.getStoreInterface().listAll(srcKey, AZURE_LIST_ALL, AZURE_UNBOUNDED_DEPTH, priorLastKey);	for(FileMetadata file : listing.getFiles()) {	fileMetadataList.add(file);	}	priorLastKey = listing.getPriorLastKey();	} while (priorLastKey != null);	fileMetadata = fileMetadataList.toArray(new FileMetadata[fileMetadataList.size()]);	long end = Time.monotonicNow();	
time taken to list blobs for rename operation is ms 

public FolderRenamePending(Path redoFile, NativeAzureFileSystem fs) throws IllegalArgumentException, IOException {	this.fs = fs;	Path f = redoFile;	FSDataInputStream input = fs.open(f);	byte[] bytes = new byte[MAX_RENAME_PENDING_FILE_SIZE];	int l = input.read(bytes);	if (l <= 0) {	
deleting empty rename pending file no data available 

json = objMapper.readValue(contents, JsonNode.class);	this.committed = true;	} catch (JsonMappingException e) {	this.committed = false;	} catch (JsonParseException e) {	this.committed = false;	} catch (IOException e) {	this.committed = false;	}	if (!this.committed) {	
deleting corruped rename pending file 

public void writeFile(NativeAzureFileSystem fs) throws IOException {	Path path = getRenamePendingFilePath();	
preparing to write atomic rename state to 

StringBuilder builder = new StringBuilder();	builder.append("[\n");	for (int i = 0; i != fileMetadata.length; i++) {	if (i > 0) {	builder.append(",\n");	}	builder.append("    ");	String noPrefix = StringUtils.removeStart(fileMetadata[i].getKey(), srcKey + "/");	builder.append(quote(noPrefix));	if (builder.length() >= MAX_RENAME_PENDING_FILE_SIZE - FORMATTING_BUFFER) {	
internal error exceeded maximum rename pending file size of bytes 

for(String fileName : fileStrings) {	finishSingleFileRename(fileName);	}	try {	FileMetadata srcMetaData = this.getSourceMetadata();	if (srcMetaData.getBlobMaterialization() == BlobMaterialization.Explicit) {	fs.getStoreInterface().rename(this.getSrcKey(), this.getDstKey(), false, lease);	}	fs.getStoreInterface().delete(srcKey, lease);	} catch (Exception e) {	
unable to delete source folder during folder rename redo if the source folder is already gone this is not an error condition continuing with redo 

Path dstFile = fullPath(dstKey, fileName);	String srcName = fs.pathToKey(srcFile);	String dstName = fs.pathToKey(dstFile);	boolean srcExists = fs.getStoreInterface().explicitFileExists(srcName);	boolean dstExists = fs.getStoreInterface().explicitFileExists(dstName);	if(srcExists) {	fs.getStoreInterface().rename(srcName, dstName, true, null);	} else if (!srcExists && dstExists) {	;	} else {	
attempting to complete rename of file during folder rename redo and file was not found in source or destination this must mean the rename of this file has already completed 

if (statistics != null) {	statistics.incrementBytesRead(1);	}	}	return result;	} catch(EOFException e) {	return -1;	} catch(IOException e) {	Throwable innerException = NativeAzureFileSystemHelper.checkForAzureStorageException(e);	if (innerException instanceof StorageException) {	
encountered storage exception for read on blob exception details error code 

if (result > 0) {	pos += result;	}	if (null != statistics && result > 0) {	statistics.incrementBytesRead(result);	}	return result;	} catch(IOException e) {	Throwable innerException = NativeAzureFileSystemHelper.checkForAzureStorageException(e);	if (innerException instanceof StorageException) {	
encountered storage exception for read on blob exception details error code 

((Seekable) in).seek(pos);	this.pos = pos;	} else {	IOUtils.closeStream(in);	in = store.retrieve(key);	this.pos = in.skip(pos);	}	} else {	this.pos += in.skip(pos - this.pos);	}	
seek to position bytes skipped 

public void write(int b) throws IOException {	try {	out.write(b);	} catch(IOException e) {	if (e.getCause() instanceof StorageException) {	StorageException storageExcp  = (StorageException) e.getCause();	
encountered storage exception for write on blob exception details error code 

public void write(byte[] b) throws IOException {	try {	out.write(b);	} catch(IOException e) {	if (e.getCause() instanceof StorageException) {	StorageException storageExcp  = (StorageException) e.getCause();	
encountered storage exception for write on blob exception details error code 

public void write(byte[] b, int off, int len) throws IOException {	try {	out.write(b, off, len);	} catch(IOException e) {	if (e.getCause() instanceof StorageException) {	StorageException storageExcp  = (StorageException) e.getCause();	
encountered storage exception for write on blob exception details error code 

metricsSourceName = newMetricsSourceName();	String sourceDesc = "Azure Storage Volume File System metrics";	AzureFileSystemMetricsSystem.registerSource(metricsSourceName, sourceDesc, instrumentation);	}	store.initialize(uri, conf, instrumentation);	setConf(conf);	this.ugi = UserGroupInformation.getCurrentUser();	this.uri = URI.create(uri.getScheme() + ": this.workingDir = new Path("/user", UserGroupInformation.getCurrentUser() .getShortUserName()).makeQualified(getUri(), getWorkingDirectory());	this.blockSize = conf.getLong(AZURE_BLOCK_SIZE_PROPERTY_NAME, MAX_AZURE_BLOCK_SIZE);	this.appendSupportEnabled = conf.getBoolean(APPEND_SUPPORT_ENABLE_PROPERTY_NAME, false);	
nativeazurefilesystem initializing 

public FSDataOutputStream append(Path f, int bufferSize, Progressable progress) throws IOException {	if (!appendSupportEnabled) {	throw new UnsupportedOperationException("Append Support not enabled");	}	
opening file for append 

} catch (AzureException e) {	String errorCode = "";	try {	StorageException e2 = (StorageException) e.getCause();	errorCode = e2.getErrorCode();	} catch (Exception e3) {	}	if (errorCode.equals("BlobNotFound")) {	throw new FileNotFoundException("Cannot create file " + f.getName() + " because parent folder does not exist.");	}	
got unexpected exception trying to get lease on 

if (errorCode.equals("BlobNotFound")) {	throw new FileNotFoundException("Cannot create file " + f.getName() + " because parent folder does not exist.");	}	throw e;	}	}	if (!exists(parent)) {	try {	lease.free();	} catch (Exception e) {	
unable to free lease because 

private FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, boolean createParent, int bufferSize, short replication, long blockSize, Progressable progress, SelfRenewingLease parentFolderLease) throws FileAlreadyExistsException, IOException {	
creating file 

private boolean deleteWithAuthEnabled(Path f, boolean recursive, boolean skipParentFolderLastModifiedTimeUpdate) throws IOException {	
deleting file 

if (parentMetadata == null) {	throw new IOException("File " + f + " has a parent directory " + parentPath + " whose metadata cannot be retrieved. Can't resolve");	}	if (!parentMetadata.isDir()) {	throw new AzureException("File " + f + " has a parent directory " + parentPath + " which is also a file. Can't resolve.");	}	}	if (!metaFile.isDir()) {	if (parentPath != null && parentPath.getParent() != null) {	if (parentMetadata.getBlobMaterialization() == BlobMaterialization.Implicit) {	
found an implicit parent directory while trying to delete the file creating the directory blob for it in 

return false;	}	} catch(IOException e) {	Throwable innerException = checkForAzureStorageException(e);	if (innerException instanceof StorageException && isFileNotFoundException((StorageException) innerException)) {	return false;	}	throw e;	}	} else {	
directory delete encountered 

} catch(IOException e) {	Throwable innerException = checkForAzureStorageException(e);	if (innerException instanceof StorageException && isFileNotFoundException((StorageException) innerException)) {	return false;	}	throw e;	}	} else {	if (parentPath != null && parentPath.getParent() != null) {	if (parentMetadata.getBlobMaterialization() == BlobMaterialization.Implicit) {	
found an implicit parent directory while trying to delete the directory creating the directory blob for it in 

try {	isPartialDelete = getFolderContentsToDelete(metaFile, fileMetadataList);	} catch (IOException e) {	Throwable innerException = checkForAzureStorageException(e);	if (innerException instanceof StorageException && isFileNotFoundException((StorageException) innerException)) {	return false;	}	throw e;	}	long end = Time.monotonicNow();	
time taken to list blobs for delete operation ms 

throw e;	}	long end = Time.monotonicNow();	final FileMetadata[] contents = fileMetadataList.toArray(new FileMetadata[fileMetadataList.size()]);	if (contents.length > 0 && !recursive) {	throw new IOException("Non-recursive delete of non-empty directory " + f);	}	AzureFileSystemThreadTask task = new AzureFileSystemThreadTask() {	public boolean execute(FileMetadata file) throws IOException{	if (!deleteFile(file.getKey(), file.isDir())) {	
attempt to delete non existent directory file 

}	AzureFileSystemThreadTask task = new AzureFileSystemThreadTask() {	public boolean execute(FileMetadata file) throws IOException{	if (!deleteFile(file.getKey(), file.isDir())) {	}	return true;	}	};	AzureFileSystemThreadPoolExecutor executor = getThreadPoolExecutor(this.deleteThreadCount, "AzureBlobDeleteThread", "Delete", key, AZURE_DELETE_THREADS);	if (!executor.executeParallel(contents, task)) {	
failed to delete files subfolders in blob 

if (!deleteFile(file.getKey(), file.isDir())) {	}	return true;	}	};	AzureFileSystemThreadPoolExecutor executor = getThreadPoolExecutor(this.deleteThreadCount, "AzureBlobDeleteThread", "Delete", key, AZURE_DELETE_THREADS);	if (!executor.executeParallel(contents, task)) {	return false;	}	if (metaFile.getKey().equals("/")) {	
cannot delete root directory 

}	};	AzureFileSystemThreadPoolExecutor executor = getThreadPoolExecutor(this.deleteThreadCount, "AzureBlobDeleteThread", "Delete", key, AZURE_DELETE_THREADS);	if (!executor.executeParallel(contents, task)) {	return false;	}	if (metaFile.getKey().equals("/")) {	return false;	}	if (isPartialDelete || (store.retrieveMetadata(metaFile.getKey()) != null && !deleteFile(metaFile.getKey(), metaFile.isDir()))) {	
failed delete directory 

if (isPartialDelete || (store.retrieveMetadata(metaFile.getKey()) != null && !deleteFile(metaFile.getKey(), metaFile.isDir()))) {	return false;	}	Path parent = absolutePath.getParent();	if (parent != null && parent.getParent() != null) {	if (!skipParentFolderLastModifiedTimeUpdate) {	updateParentFolderLastModifiedTime(key);	}	}	}	
delete successful for 

private boolean deleteWithoutAuth(Path f, boolean recursive, boolean skipParentFolderLastModifiedTimeUpdate) throws IOException {	
deleting file 

}	throw e;	}	if (parentMetadata == null) {	throw new IOException("File " + f + " has a parent directory " + parentPath + " whose metadata cannot be retrieved. Can't resolve");	}	if (!parentMetadata.isDir()) {	throw new AzureException("File " + f + " has a parent directory " + parentPath + " which is also a file. Can't resolve.");	}	if (parentMetadata.getBlobMaterialization() == BlobMaterialization.Implicit) {	
found an implicit parent directory while trying to delete the file creating the directory blob for it in 

return false;	}	} catch(IOException e) {	Throwable innerException = checkForAzureStorageException(e);	if (innerException instanceof StorageException && isFileNotFoundException((StorageException) innerException)) {	return false;	}	throw e;	}	} else {	
directory delete encountered 

if (isFileNotFoundException((StorageException) innerException)) {	throw new IOException("File " + f + " has a parent directory " + parentPath + " whose metadata cannot be retrieved. Can't resolve");	}	}	throw e;	}	if (parentMetadata == null) {	throw new IOException("File " + f + " has a parent directory " + parentPath + " whose metadata cannot be retrieved. Can't resolve");	}	if (parentMetadata.getBlobMaterialization() == BlobMaterialization.Implicit) {	
found an implicit parent directory while trying to delete the directory creating the directory blob for it in 

priorLastKey = listing.getPriorLastKey();	} catch (IOException e) {	Throwable innerException = checkForAzureStorageException(e);	if (innerException instanceof StorageException && isFileNotFoundException((StorageException) innerException)) {	return false;	}	throw e;	}	} while (priorLastKey != null);	long end = Time.monotonicNow();	
time taken to list blobs for delete operation ms 

long end = Time.monotonicNow();	final FileMetadata[] contents = fileMetadataList.toArray(new FileMetadata[fileMetadataList.size()]);	if (contents.length > 0) {	if (!recursive) {	throw new IOException("Non-recursive delete of non-empty directory "+ f);	}	}	AzureFileSystemThreadTask task = new AzureFileSystemThreadTask() {	public boolean execute(FileMetadata file) throws IOException{	if (!deleteFile(file.getKey(), file.isDir())) {	
attempt to delete non existent directory file 

}	AzureFileSystemThreadTask task = new AzureFileSystemThreadTask() {	public boolean execute(FileMetadata file) throws IOException{	if (!deleteFile(file.getKey(), file.isDir())) {	}	return true;	}	};	AzureFileSystemThreadPoolExecutor executor = getThreadPoolExecutor(this.deleteThreadCount, "AzureBlobDeleteThread", "Delete", key, AZURE_DELETE_THREADS);	if (!executor.executeParallel(contents, task)) {	
failed to delete files subfolders in blob 

if (!deleteFile(file.getKey(), file.isDir())) {	}	return true;	}	};	AzureFileSystemThreadPoolExecutor executor = getThreadPoolExecutor(this.deleteThreadCount, "AzureBlobDeleteThread", "Delete", key, AZURE_DELETE_THREADS);	if (!executor.executeParallel(contents, task)) {	return false;	}	if (store.retrieveMetadata(metaFile.getKey()) != null && !deleteFile(metaFile.getKey(), metaFile.isDir())) {	
failed delete directory 

if (store.retrieveMetadata(metaFile.getKey()) != null && !deleteFile(metaFile.getKey(), metaFile.isDir())) {	return false;	}	Path parent = absolutePath.getParent();	if (parent != null && parent.getParent() != null) {	if (!skipParentFolderLastModifiedTimeUpdate) {	updateParentFolderLastModifiedTime(key);	}	}	}	
delete successful for 

boolean isPartialDelete = false;	Path pathToDelete = makeAbsolute(keyToPath(folderToDelete.getKey()));	foldersToProcess.push(folderToDelete);	while (!foldersToProcess.empty()) {	FileMetadata currentFolder = foldersToProcess.pop();	Path currentPath = makeAbsolute(keyToPath(currentFolder.getKey()));	boolean canDeleteChildren = true;	try {	performAuthCheck(currentPath, WasbAuthorizationOperations.WRITE, "delete", pathToDelete);	} catch (WasbAuthorizationException we) {	
authorization check failed for 

performAuthCheck(currentPath, WasbAuthorizationOperations.WRITE, "delete", pathToDelete);	} catch (WasbAuthorizationException we) {	canDeleteChildren = false;	}	if (canDeleteChildren) {	ArrayList<FileMetadata> fileMetadataList = getChildrenMetadata(currentFolder.getKey(), maxListingDepth);	for (FileMetadata childItem : fileMetadataList) {	if (isStickyBitCheckViolated(childItem, currentFolder, false)) {	canDeleteChildren = false;	Path filePath = makeAbsolute(keyToPath(childItem.getKey()));	
user does not have permissions to delete parent directory has sticky bit set 

canDeleteChildren = false;	Path filePath = makeAbsolute(keyToPath(childItem.getKey()));	} else {	if (childItem.isDir()) {	foldersToProcess.push(childItem);	}	folderContentsMap.put(childItem.getKey(), childItem);	}	}	} else {	
authorization check failed files or folders under will not be processed for deletion 

}	folderContentsMap.put(childItem.getKey(), childItem);	}	}	} else {	}	if (!canDeleteChildren) {	String pathToRemove = currentFolder.getKey();	while (!pathToRemove.equals(folderToDelete.getKey())) {	if (folderContentsMap.containsKey(pathToRemove)) {	
cannot delete since some of its contents cannot be deleted 

private boolean isStickyBitCheckViolated(FileMetadata metaData, FileMetadata parentMetadata, boolean throwOnException) throws IOException {	try {	return isStickyBitCheckViolated(metaData, parentMetadata);	} catch (FileNotFoundException ex) {	if (throwOnException) {	throw ex;	} else {	
encountered filenotfoundexception while performing stickybit check operation for 

public FileStatus getFileStatus(Path f) throws FileNotFoundException, IOException {	
getting the file status for 

meta = store.retrieveMetadata(key);	} catch(Exception ex) {	Throwable innerException = NativeAzureFileSystemHelper.checkForAzureStorageException(ex);	if (innerException instanceof StorageException && NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) innerException)) {	throw new FileNotFoundException(String.format("%s is not found", key));	}	throw ex;	}	if (meta != null) {	if (meta.isDir()) {	
path is a folder 

}	throw ex;	}	if (meta != null) {	if (meta.isDir()) {	if (conditionalRedoFolderRename(f)) {	throw new FileNotFoundException( absolutePath + ": No such file or directory.");	}	return newDirectory(meta, absolutePath);	}	
found the path as a file 

public FileStatus[] listStatus(Path f) throws FileNotFoundException, IOException {	
listing status for 

meta = store.retrieveMetadata(key);	} catch (IOException ex) {	Throwable innerException = NativeAzureFileSystemHelper.checkForAzureStorageException(ex);	if (innerException instanceof StorageException && NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) innerException)) {	throw new FileNotFoundException(String.format("%s is not found", f));	}	throw ex;	}	if (meta != null) {	if (!meta.isDir()) {	
found path as a file 

Path subpath = keyToPath(fileMetadata.getKey());	if (fileMetadata.isDir()) {	if (fileMetadata.getKey().equals(AZURE_TEMP_FOLDER)) {	continue;	}	status.add(newDirectory(fileMetadata, subpath));	} else {	status.add(newFile(fileMetadata, subpath));	}	}	
found path as a directory with files in it 

if (fileMetadata.isDir()) {	if (fileMetadata.getKey().equals(AZURE_TEMP_FOLDER)) {	continue;	}	status.add(newDirectory(fileMetadata, subpath));	} else {	status.add(newFile(fileMetadata, subpath));	}	}	} else {	
did not find any metadata for path 

private Path getAncestor(Path f) throws IOException {	for (Path current = f, parent = current.getParent();	parent != null;	current = parent, parent = current.getParent()) {	String currentKey = pathToKey(current);	FileMetadata currentMetadata = store.retrieveMetadata(currentKey);	if (currentMetadata != null && currentMetadata.isDir()) {	Path ancestor = keyToPath(currentMetadata.getKey());	
found ancestor for path 

public boolean mkdirs(Path f, FsPermission permission, boolean noUmask) throws IOException {	
creating directory 

public FSDataInputStream open(Path f, int bufferSize) throws FileNotFoundException, IOException {	
opening file 

public boolean rename(Path src, Path dst) throws FileNotFoundException, IOException {	FolderRenamePending renamePending = null;	
moving to 

}	performAuthCheck(srcParentFolder, WasbAuthorizationOperations.WRITE, "rename", absoluteSrcPath);	if (this.azureAuthorization) {	try {	performStickyBitCheckForRenameOperation(absoluteSrcPath, srcParentFolder);	} catch (FileNotFoundException ex) {	return false;	} catch (IOException ex) {	Throwable innerException = checkForAzureStorageException(ex);	if (innerException instanceof StorageException && isFileNotFoundException((StorageException) innerException)) {	
encountered filenotfound exception when performing sticky bit check on failing rename 

Path absoluteDstPath = makeAbsolute(dst);	Path dstParentFolder = absoluteDstPath.getParent();	String dstKey = pathToKey(absoluteDstPath);	FileMetadata dstMetadata = null;	try {	dstMetadata = store.retrieveMetadata(dstKey);	} catch (IOException ex) {	Throwable innerException = NativeAzureFileSystemHelper.checkForAzureStorageException(ex);	if (innerException instanceof StorageException) {	if (NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) innerException)) {	
blobnotfound exception encountered for destination key swallowing the exception to handle race condition gracefully 

if (innerException instanceof StorageException) {	if (NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) innerException)) {	}	} else {	throw ex;	}	}	if (dstMetadata != null && dstMetadata.isDir()) {	performAuthCheck(absoluteDstPath, WasbAuthorizationOperations.WRITE, "rename", absoluteDstPath);	dstKey = pathToKey(makeAbsolute(new Path(dst, src.getName())));	
destination is a directory adjusted the destination to be 

if (NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) innerException)) {	}	} else {	throw ex;	}	}	if (dstMetadata != null && dstMetadata.isDir()) {	performAuthCheck(absoluteDstPath, WasbAuthorizationOperations.WRITE, "rename", absoluteDstPath);	dstKey = pathToKey(makeAbsolute(new Path(dst, src.getName())));	} else if (dstMetadata != null) {	
destination is an already existing file failing the rename 

dstKey = pathToKey(makeAbsolute(new Path(dst, src.getName())));	} else if (dstMetadata != null) {	return false;	} else {	FileMetadata parentOfDestMetadata = null;	try {	parentOfDestMetadata = store.retrieveMetadata(pathToKey(absoluteDstPath.getParent()));	} catch (IOException ex) {	Throwable innerException = NativeAzureFileSystemHelper.checkForAzureStorageException(ex);	if (innerException instanceof StorageException && NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) innerException)) {	
parent of destination doesn t exists failing rename 

try {	parentOfDestMetadata = store.retrieveMetadata(pathToKey(absoluteDstPath.getParent()));	} catch (IOException ex) {	Throwable innerException = NativeAzureFileSystemHelper.checkForAzureStorageException(ex);	if (innerException instanceof StorageException && NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) innerException)) {	return false;	}	throw ex;	}	if (parentOfDestMetadata == null) {	
parent of the destination doesn t exist failing the rename 

} catch (IOException ex) {	Throwable innerException = NativeAzureFileSystemHelper.checkForAzureStorageException(ex);	if (innerException instanceof StorageException && NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) innerException)) {	return false;	}	throw ex;	}	if (parentOfDestMetadata == null) {	return false;	} else if (!parentOfDestMetadata.isDir()) {	
parent of the destination is a file failing the rename 

} else {	performAuthCheck(dstParentFolder, WasbAuthorizationOperations.WRITE, "rename", absoluteDstPath);	}	}	FileMetadata srcMetadata = null;	try {	srcMetadata = store.retrieveMetadata(srcKey);	} catch (IOException ex) {	Throwable innerException = NativeAzureFileSystemHelper.checkForAzureStorageException(ex);	if (innerException instanceof StorageException && NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) innerException)) {	
source doesn t exists failing rename 

try {	srcMetadata = store.retrieveMetadata(srcKey);	} catch (IOException ex) {	Throwable innerException = NativeAzureFileSystemHelper.checkForAzureStorageException(ex);	if (innerException instanceof StorageException && NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) innerException)) {	return false;	}	throw ex;	}	if (srcMetadata == null) {	
source doesn t exist failing the rename 

} catch (IOException ex) {	Throwable innerException = NativeAzureFileSystemHelper.checkForAzureStorageException(ex);	if (innerException instanceof StorageException && NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) innerException)) {	return false;	}	throw ex;	}	if (srcMetadata == null) {	return false;	} else if (!srcMetadata.isDir()) {	
source found as a file renaming 

throw ex;	}	if (srcMetadata == null) {	return false;	} else if (!srcMetadata.isDir()) {	try {	store.rename(srcKey, dstKey);	} catch(IOException ex) {	Throwable innerException = NativeAzureFileSystemHelper.checkForAzureStorageException(ex);	if (innerException instanceof StorageException && NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) innerException)) {	
blobnotfoundexception encountered failing rename 

} catch(IOException ex) {	Throwable innerException = NativeAzureFileSystemHelper.checkForAzureStorageException(ex);	if (innerException instanceof StorageException && NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) innerException)) {	return false;	}	throw ex;	}	} else {	renamePending = prepareAtomicFolderRename(srcKey, dstKey);	renamePending.execute();	
renamed to successfully 

throw ex;	}	} else {	renamePending = prepareAtomicFolderRename(srcKey, dstKey);	renamePending.execute();	renamePending.cleanup();	return true;	}	updateParentFolderLastModifiedTime(srcKey);	updateParentFolderLastModifiedTime(dstKey);	
renamed to successfully 

} catch (AzureException e) {	String errorCode = "";	try {	StorageException e2 = (StorageException) e.getCause();	errorCode = e2.getErrorCode();	} catch (Exception e3) {	}	if (errorCode.equals("BlobNotFound")) {	throw new FileNotFoundException("Folder does not exist: " + parentKey);	}	
got unexpected exception trying to get lease on 

if (errorCode.equals("BlobNotFound")) {	throw new FileNotFoundException("Folder does not exist: " + parentKey);	}	throw e;	} finally {	try {	if (lease != null) {	lease.free();	}	} catch (Exception e) {	
unable to free lease on 

private void performStickyBitCheckForRenameOperation(Path srcPath, Path srcParentPath) throws FileNotFoundException, WasbAuthorizationException, IOException {	String srcKey = pathToKey(srcPath);	FileMetadata srcMetadata = null;	srcMetadata = store.retrieveMetadata(srcKey);	if (srcMetadata == null) {	
source doesn t exist failing rename 

private void performStickyBitCheckForRenameOperation(Path srcPath, Path srcParentPath) throws FileNotFoundException, WasbAuthorizationException, IOException {	String srcKey = pathToKey(srcPath);	FileMetadata srcMetadata = null;	srcMetadata = store.retrieveMetadata(srcKey);	if (srcMetadata == null) {	throw new FileNotFoundException( String.format("%s does not exist.", srcPath));	}	String parentkey = pathToKey(srcParentPath);	FileMetadata parentMetadata = store.retrieveMetadata(parentkey);	if (parentMetadata == null) {	
path doesn t exist failing rename 

if (isClosed) {	return;	}	super.close();	store.close();	long startTime = System.currentTimeMillis();	if(!getConf().getBoolean(SKIP_AZURE_METRICS_PROPERTY_NAME, false)) {	AzureFileSystemMetricsSystem.unregisterSource(metricsSourceName);	AzureFileSystemMetricsSystem.fileSystemClosed();	}	
submitting metrics when file system closed took ms 

public void recoverFilesWithDanglingTempData(Path root, Path destination) throws IOException {	
recovering files with dangling temp data in 

public void deleteFilesWithDanglingTempData(Path root) throws IOException {	
deleting files with dangling temp data in 

protected void finalize() throws Throwable {	
finalize called 

public String getOwnerForPath(Path absolutePath) throws IOException {	String owner = "";	FileMetadata meta = null;	String key = pathToKey(absolutePath);	try {	meta = store.retrieveMetadata(key);	if (meta != null) {	owner = meta.getPermissionStatus().getUserName();	
retrieved as owner for path 

public String getOwnerForPath(Path absolutePath) throws IOException {	String owner = "";	FileMetadata meta = null;	String key = pathToKey(absolutePath);	try {	meta = store.retrieveMetadata(key);	if (meta != null) {	owner = meta.getPermissionStatus().getUserName();	} else {	
cannot find file folder returning owner as empty string 

========================= hadoop sample_6439 =========================

private synchronized S3ADataBlocks.DataBlock createBlockIfNeeded() throws IOException {	if (activeBlock == null) {	blockCount++;	if (blockCount>= Constants.MAX_MULTIPART_COUNT) {	
number of partitions in stream exceeds limit for write may fail 

private void clearActiveBlock() {	if (activeBlock != null) {	
clearing active block 

public synchronized void write(byte[] source, int offset, int len) throws IOException {	S3ADataBlocks.validateWriteArgs(source, offset, len);	checkOpen();	if (len == 0) {	return;	}	S3ADataBlocks.DataBlock block = createBlockIfNeeded();	int written = block.write(source, offset, len);	int remainingCapacity = block.remainingCapacity();	if (written < len) {	
writing more data than block has capacity triggering upload 

private synchronized void uploadCurrentBlock() throws IOException {	Preconditions.checkState(hasActiveBlock(), "No active block");	
writing block 

private synchronized void uploadCurrentBlock() throws IOException {	Preconditions.checkState(hasActiveBlock(), "No active block");	if (multiPartUpload == null) {	
initiating multipart upload 

public void close() throws IOException {	if (closed.getAndSet(true)) {	
ignoring close as stream is already closed 

bytes = putObject();	}	} else {	if (hasBlock && block.hasData()) {	uploadCurrentBlock();	}	final List<PartETag> partETags = multiPartUpload.waitForAllPartUploads();	multiPartUpload.complete(partETags);	bytes = bytesSubmitted;	}	
upload complete for 

}	final List<PartETag> partETags = multiPartUpload.waitForAllPartUploads();	multiPartUpload.complete(partETags);	bytes = bytesSubmitted;	}	} catch (IOException ioe) {	writeOperationHelper.writeFailed(ioe);	throw ioe;	} finally {	closeAll(LOG, block, blockFactory);	
statistics 

private int putObject() throws IOException {	
executing regular upload for 

closeAll(LOG, uploadData, block);	}	return result;	}	});	clearActiveBlock();	try {	putObjectResult.get();	return size;	} catch (InterruptedException ie) {	
interrupted object upload 

private void uploadBlockAsync(final S3ADataBlocks.DataBlock block) throws IOException {	
queueing upload of 

final int size = block.dataSize();	final S3ADataBlocks.BlockUploadData uploadData = block.startUpload();	final int currentPartNumber = partETagsFutures.size() + 1;	final UploadPartRequest request = writeOperationHelper.newUploadPartRequest( uploadId, currentPartNumber, size, uploadData.getUploadStream(), uploadData.getFile());	long transferQueueTime = now();	BlockUploadProgress callback = new BlockUploadProgress( block, progressListener, transferQueueTime);	request.setGeneralProgressListener(callback);	statistics.blockUploadQueued(block.dataSize());	ListenableFuture<PartETag> partETagFuture = executorService.submit(new Callable<PartETag>() {	public PartETag call() throws Exception {	
uploading part for id 

final UploadPartRequest request = writeOperationHelper.newUploadPartRequest( uploadId, currentPartNumber, size, uploadData.getUploadStream(), uploadData.getFile());	long transferQueueTime = now();	BlockUploadProgress callback = new BlockUploadProgress( block, progressListener, transferQueueTime);	request.setGeneralProgressListener(callback);	statistics.blockUploadQueued(block.dataSize());	ListenableFuture<PartETag> partETagFuture = executorService.submit(new Callable<PartETag>() {	public PartETag call() throws Exception {	PartETag partETag;	try {	partETag = fs.uploadPart(request).getPartETag();	
completed upload of to part 

final UploadPartRequest request = writeOperationHelper.newUploadPartRequest( uploadId, currentPartNumber, size, uploadData.getUploadStream(), uploadData.getFile());	long transferQueueTime = now();	BlockUploadProgress callback = new BlockUploadProgress( block, progressListener, transferQueueTime);	request.setGeneralProgressListener(callback);	statistics.blockUploadQueued(block.dataSize());	ListenableFuture<PartETag> partETagFuture = executorService.submit(new Callable<PartETag>() {	public PartETag call() throws Exception {	PartETag partETag;	try {	partETag = fs.uploadPart(request).getPartETag();	
stream statistics of 

private List<PartETag> waitForAllPartUploads() throws IOException {	
waiting for uploads to complete 

private List<PartETag> waitForAllPartUploads() throws IOException {	try {	return Futures.allAsList(partETagsFutures).get();	} catch (InterruptedException ie) {	
interrupted partupload 

private List<PartETag> waitForAllPartUploads() throws IOException {	try {	return Futures.allAsList(partETagsFutures).get();	} catch (InterruptedException ie) {	Thread.currentThread().interrupt();	return null;	} catch (ExecutionException ee) {	
while waiting for upload completion 

private List<PartETag> waitForAllPartUploads() throws IOException {	try {	return Futures.allAsList(partETagsFutures).get();	} catch (InterruptedException ie) {	Thread.currentThread().interrupt();	return null;	} catch (ExecutionException ee) {	
cancelling futures 

do {	try {	LOG.debug(operation);	writeOperationHelper.abortMultipartUpload(uploadId);	return;	} catch (AmazonClientException e) {	lastException = e;	statistics.exceptionInMultipartAbort();	}	} while (shouldRetry(operation, lastException, retryCount++));	
unable to abort multipart upload you may need to purge uploaded parts 

private boolean shouldRetry(String operation, AmazonClientException e, int retryCount) {	try {	RetryPolicy.RetryAction retryAction = retryPolicy.shouldRetry(e, retryCount, 0, true);	boolean retry = retryAction == RetryPolicy.RetryAction.RETRY;	if (retry) {	fs.incrementStatistic(IGNORED_ERRORS);	
retrying after exception 

switch (eventType) {	case REQUEST_BYTE_TRANSFER_EVENT: statistics.bytesTransferred(bytesTransferred);	break;	case TRANSFER_PART_STARTED_EVENT: transferStartTime = now();	statistics.blockUploadStarted(transferStartTime - transferQueueTime, size);	incrementWriteOperations();	break;	case TRANSFER_PART_COMPLETED_EVENT: statistics.blockUploadCompleted(now() - transferStartTime, size);	break;	case TRANSFER_PART_FAILED_EVENT: statistics.blockUploadFailed(now() - transferStartTime, size);	
transfer failure of block 

========================= hadoop sample_5983 =========================

protected static boolean deletePath(PathDeletionContext context) throws IOException {	context.enablePathForCleanup();	if (LOG.isDebugEnabled()) {	
trying to delete 

public void run() {	if (LOG.isDebugEnabled()) {	
started 

public void run() {	if (LOG.isDebugEnabled()) {	}	PathDeletionContext context = null;	while (true) {	try {	context = queue.take();	if (!deletePath(context)) {	
cleanupthread unable to delete path 

public void run() {	if (LOG.isDebugEnabled()) {	}	PathDeletionContext context = null;	while (true) {	try {	context = queue.take();	if (!deletePath(context)) {	}	else if (LOG.isDebugEnabled()) {	
deleted 

PathDeletionContext context = null;	while (true) {	try {	context = queue.take();	if (!deletePath(context)) {	}	else if (LOG.isDebugEnabled()) {	}	} catch (InterruptedException t) {	if (context == null) {	
interrupted deletion of an invalid path path deletion context is null 

while (true) {	try {	context = queue.take();	if (!deletePath(context)) {	}	else if (LOG.isDebugEnabled()) {	}	} catch (InterruptedException t) {	if (context == null) {	} else {	
interrupted deletion of 

if (!deletePath(context)) {	}	else if (LOG.isDebugEnabled()) {	}	} catch (InterruptedException t) {	if (context == null) {	} else {	}	return;	} catch (Exception e) {	
error deleting path 

========================= hadoop sample_4691 =========================

String edits = nnHelper.generateEdits();	LOG.info("Generated edits=" + edits);	String editsParsedXml = folder.newFile("editsParsed.xml").getAbsolutePath();	String editsReparsed = folder.newFile("editsParsed").getAbsolutePath();	String editsParsedXML_caseInSensitive = folder.newFile("editsRecoveredParsed.XML").getAbsolutePath();	assertEquals(0, runOev(edits, editsParsedXml, "xml", false));	assertEquals(0, runOev(edits, editsParsedXML_caseInSensitive, "xml", false));	assertEquals(0, runOev(editsParsedXml, editsReparsed, "binary", false));	assertEquals(0, runOev(editsParsedXML_caseInSensitive, editsReparsed, "binary", false));	assertTrue("Edits " + edits + " should have all op codes", hasAllOpCodes(edits));	
comparing generated file with reference file 

private int runOev(String inFilename, String outFilename, String processor, boolean recovery) throws IOException {	
running oev 

private boolean hasAllOpCodes(String inFilename) throws IOException {	String outFilename = inFilename + ".stats";	FileOutputStream fout = new FileOutputStream(outFilename);	StatisticsEditsVisitor visitor = new StatisticsEditsVisitor(fout);	OfflineEditsViewer oev = new OfflineEditsViewer();	if (oev.go(inFilename, outFilename, "stats", new Flags(), visitor) != 0) return false;	
statistics for 

FileOutputStream fout = new FileOutputStream(outFilename);	StatisticsEditsVisitor visitor = new StatisticsEditsVisitor(fout);	OfflineEditsViewer oev = new OfflineEditsViewer();	if (oev.go(inFilename, outFilename, "stats", new Flags(), visitor) != 0) return false;	boolean hasAllOpCodes = true;	for (FSEditLogOpCodes opCode : FSEditLogOpCodes.values()) {	if (skippedOps.contains(opCode)) continue;	Long count = visitor.getStatistics().get(opCode);	if ((count == null) || (count == 0)) {	hasAllOpCodes = false;	
opcode not tested in 

========================= hadoop sample_7117 =========================

public void testNumVersionsReportedCorrect() throws IOException {	FSNamesystem fsn = Mockito.mock(FSNamesystem.class);	Mockito.when(fsn.hasWriteLock()).thenReturn(true);	DatanodeManager dm = mockDatanodeManager(fsn, new Configuration());	Random rng = new Random();	int seed = rng.nextInt();	rng = new Random(seed);	
using seed for testing 

rng = new Random(seed);	HashMap <String, DatanodeRegistration> sIdToDnReg = new HashMap<String, DatanodeRegistration>();	for(int i=0; i<NUM_ITERATIONS; ++i) {	if(rng.nextBoolean() && i%3 == 0 && sIdToDnReg.size()!=0) {	int randomIndex = rng.nextInt() % sIdToDnReg.size();	Iterator<Map.Entry<String, DatanodeRegistration>> it = sIdToDnReg.entrySet().iterator();	for(int j=0; j<randomIndex-1; ++j) {	it.next();	}	DatanodeRegistration toRemove = it.next().getValue();	
removing node ip version 

if(rng.nextBoolean()) {	dr.setIpAddr(dr.getIpAddr() + "newIP");	}	} else {	String ip = "someIP" + storageID;	Mockito.when(dr.getIpAddr()).thenReturn(ip);	Mockito.when(dr.getXferAddr()).thenReturn(ip + ":9000");	Mockito.when(dr.getXferPort()).thenReturn(9000);	}	Mockito.when(dr.getSoftwareVersion()).thenReturn( "version" + rng.nextInt(5));	
registering node storageid version ip address 

String ver = it.getValue().getSoftwareVersion();	if(!mapToCheck.containsKey(ver)) {	throw new AssertionError("The correct number of datanodes of a " + "version was not found on iteration " + i);	}	mapToCheck.put(ver, mapToCheck.get(ver) - 1);	if(mapToCheck.get(ver) == 0) {	mapToCheck.remove(ver);	}	}	for(Entry <String, Integer> entry: mapToCheck.entrySet()) {	
still in map has 

conf.setBoolean( DFSConfigKeys.DFS_REJECT_UNRESOLVED_DN_TOPOLOGY_MAPPING_KEY, true);	conf.setClass( CommonConfigurationKeysPublic.NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY, TestDatanodeManager.MyResolver.class, DNSToSwitchMapping.class);	DatanodeManager dm = mockDatanodeManager(fsn, conf);	String storageID = "someStorageID-123";	DatanodeRegistration dr = Mockito.mock(DatanodeRegistration.class);	Mockito.when(dr.getDatanodeUuid()).thenReturn(storageID);	try {	dm.registerDatanode(dr);	Assert.fail("Expected an UnresolvedTopologyException");	} catch (UnresolvedTopologyException ute) {	
expected topology is not resolved and registration is rejected 

========================= hadoop sample_7547 =========================

protected synchronized void rollSecret() {	if (!isDestroyed) {	
rolling secret 

========================= hadoop sample_2745 =========================

public Token<LocalizerTokenIdentifier> selectToken(Text service, Collection<Token<? extends TokenIdentifier>> tokens) {	
using localizertokenselector 

public Token<LocalizerTokenIdentifier> selectToken(Text service, Collection<Token<? extends TokenIdentifier>> tokens) {	for (Token<? extends TokenIdentifier> token : tokens) {	if (LOG.isDebugEnabled()) {	
token of kind is found 

public Token<LocalizerTokenIdentifier> selectToken(Text service, Collection<Token<? extends TokenIdentifier>> tokens) {	for (Token<? extends TokenIdentifier> token : tokens) {	if (LOG.isDebugEnabled()) {	}	if (LocalizerTokenIdentifier.KIND.equals(token.getKind())) {	return (Token<LocalizerTokenIdentifier>) token;	}	}	
returning null 

========================= hadoop sample_1885 =========================

public void run() {	while (shouldRun) {	scan(streamTimeout);	try {	long workedTime = Time.monotonicNow() - lastWakeupTime;	if (workedTime < rotation) {	if (LOG.isTraceEnabled()) {	
streammonitor can still have a sleep 

scan(streamTimeout);	try {	long workedTime = Time.monotonicNow() - lastWakeupTime;	if (workedTime < rotation) {	if (LOG.isTraceEnabled()) {	}	Thread.sleep(rotation - workedTime);	}	lastWakeupTime = Time.monotonicNow();	} catch (InterruptedException e) {	
streammonitor got interrupted 

========================= hadoop sample_7052 =========================

public void onContainersCompleted(List<ContainerStatus> statuses) {	completedContainers = statuses;	synchronized (completedContainers) {	while (completedContainers != null) {	try {	completedContainers.wait();	} catch (InterruptedException ex) {	
interrupted during wait 

public void onContainersUpdated( List<UpdatedContainer> changed) {	synchronized (changedContainers) {	changedContainers.clear();	changedContainers.addAll(changed);	while (!changedContainers.isEmpty()) {	try {	changedContainers.wait();	} catch (InterruptedException ex) {	
interrupted during wait 

public void onContainersAllocated(List<Container> containers) {	allocatedContainers = containers;	synchronized (allocatedContainers) {	while (allocatedContainers != null) {	try {	allocatedContainers.wait();	} catch (InterruptedException ex) {	
interrupted during wait 

========================= hadoop sample_2600 =========================

public Object getProxy(Class protocol, InetSocketAddress addr, Configuration conf) {	
creating a hadoopyarnprotorpc proxy for protocol 

public Server getServer(Class protocol, Object instance, InetSocketAddress addr, Configuration conf, SecretManager<? extends TokenIdentifier> secretManager, int numHandlers, String portRangeConfig) {	
creating a hadoopyarnprotorpc server for protocol with handlers 

========================= hadoop sample_2529 =========================

private void coverColumn(ColumnHeader<ColumnName> col) {	
cover 

private void uncoverColumn(ColumnHeader<ColumnName> col) {	
uncover 

========================= hadoop sample_5716 =========================

protected byte[] createPassword(NMTokenIdentifier identifier) {	if (LOG.isDebugEnabled()) {	
creating password for for user to run on nm 

protected byte[] retrivePasswordInternal(NMTokenIdentifier identifier, MasterKeyData masterKey) {	if (LOG.isDebugEnabled()) {	
creating password for for user to run on nm 

========================= hadoop sample_1305 =========================

public void addToken(Text alias, Token<? extends TokenIdentifier> t) {	if (t == null) {	
null token ignored for 

========================= hadoop sample_3768 =========================

public void testSerialSubmit() throws Exception {	policy = GridmixJobSubmissionPolicy.SERIAL;	
serial started at 

public void testSerialSubmit() throws Exception {	policy = GridmixJobSubmissionPolicy.SERIAL;	doSubmission(JobCreator.SLEEPJOB.name(), false);	
serial ended at 

public void testReplaySubmit() throws Exception {	policy = GridmixJobSubmissionPolicy.REPLAY;	
replay started at 

public void testReplaySubmit() throws Exception {	policy = GridmixJobSubmissionPolicy.REPLAY;	doSubmission(JobCreator.SLEEPJOB.name(), false);	
replay ended at 

public void testStressSubmit() throws Exception {	policy = GridmixJobSubmissionPolicy.STRESS;	
replay started at 

public void testStressSubmit() throws Exception {	policy = GridmixJobSubmissionPolicy.STRESS;	doSubmission(JobCreator.SLEEPJOB.name(), false);	
replay ended at 

========================= hadoop sample_6090 =========================

}	this.numQueues = aNumQueues;	this.queueWeights = conf.getInts(ns + "." + IPC_CALLQUEUE_WRRMUX_WEIGHTS_KEY);	if (this.queueWeights.length == 0) {	this.queueWeights = getDefaultQueueWeights(this.numQueues);	} else if (this.queueWeights.length != this.numQueues) {	throw new IllegalArgumentException(ns + "." + IPC_CALLQUEUE_WRRMUX_WEIGHTS_KEY + " must specify exactly " + this.numQueues + " weights: one for each priority level.");	}	this.currentQueueIndex = new AtomicInteger(0);	this.requestsLeft = new AtomicInteger(this.queueWeights[0]);	
weightedroundrobinmultiplexer is being used 

========================= hadoop sample_4042 =========================

public NodeHeartbeatResponse nodeHeartbeat(NodeHeartbeatRequest request) throws YarnException, IOException {	NodeStatus nodeStatus = request.getNodeStatus();	
got heartbeat number 

========================= hadoop sample_1610 =========================

key = reader.getCurrentKey();	clazz = key.getClass();	assertEquals("Key class is Text.", Text.class, clazz);	value = reader.getCurrentValue();	clazz = value.getClass();	assertEquals("Value class is Text.", Text.class, clazz);	final int k = Integer.parseInt(key.toString());	final int v = Integer.parseInt(value.toString());	assertEquals("Bad key", 0, k % 2);	assertEquals("Mismatched key/value", k / 2, v);	
read 

Text value = null;	try {	int count = 0;	while (reader.nextKeyValue()) {	key = reader.getCurrentKey();	value = reader.getCurrentValue();	final int k = Integer.parseInt(key.toString());	final int v = Integer.parseInt(value.toString());	assertEquals("Bad key", 0, k % 2);	assertEquals("Mismatched key/value", k / 2, v);	
read 

========================= hadoop sample_5653 =========================

protected void doShuffle(MapHost host, IFileInputStream iFin, long compressedLength, long decompressedLength, ShuffleClientMetrics metrics, Reporter reporter) throws IOException {	InputStream input = iFin;	if (codec != null) {	decompressor.reset();	input = codec.createInputStream(input, decompressor);	}	try {	IOUtils.readFully(input, memory, 0, memory.length);	metrics.inputBytes(memory.length);	reporter.progress();	
read bytes from map output for 

========================= hadoop sample_4941 =========================

private static long validateExpiryTime(CacheDirectiveInfo info, long maxRelativeExpiryTime) throws InvalidRequestException {	
validating directive pool maxrelativeexpirytime 

String path = validatePath(info);	short replication = validateReplication( info, pool.getDefaultReplication());	long expiryTime = validateExpiryTime(info, pool.getMaxRelativeExpiryMs());	if (!flags.contains(CacheFlag.FORCE)) {	checkLimit(pool, path, replication);	}	long id = getNextDirectiveId();	directive = new CacheDirective(id, path, replication, expiryTime);	addInternal(directive, pool);	} catch (IOException e) {	
adddirective of failed 

long expiryTime = validateExpiryTime(info, pool.getMaxRelativeExpiryMs());	if (!flags.contains(CacheFlag.FORCE)) {	checkLimit(pool, path, replication);	}	long id = getNextDirectiveId();	directive = new CacheDirective(id, path, replication, expiryTime);	addInternal(directive, pool);	} catch (IOException e) {	throw e;	}	
adddirective of successful 

checkWritePermission(pc, destPool);	if (!flags.contains(CacheFlag.FORCE)) {	checkLimit(destPool, infoWithDefaults.getPath().toUri().getPath(), infoWithDefaults.getReplication());	}	}	validateExpiryTime(infoWithDefaults, destPool.getMaxRelativeExpiryMs());	setNeedsRescan();	removeInternal(prevEntry);	addInternal(new CacheDirective(builder.build()), destPool);	} catch (IOException e) {	
modifydirective of failed 

checkLimit(destPool, infoWithDefaults.getPath().toUri().getPath(), infoWithDefaults.getReplication());	}	}	validateExpiryTime(infoWithDefaults, destPool.getMaxRelativeExpiryMs());	setNeedsRescan();	removeInternal(prevEntry);	addInternal(new CacheDirective(builder.build()), destPool);	} catch (IOException e) {	throw e;	}	
modifydirective of successfully applied 

public void removeDirective(long id, FSPermissionChecker pc) throws IOException {	assert namesystem.hasWriteLock();	try {	CacheDirective directive = getById(id);	checkWritePermission(pc, directive.getPool());	removeInternal(directive);	} catch (IOException e) {	
removedirective of failed 

public void removeDirective(long id, FSPermissionChecker pc) throws IOException {	assert namesystem.hasWriteLock();	try {	CacheDirective directive = getById(id);	checkWritePermission(pc, directive.getPool());	removeInternal(directive);	} catch (IOException e) {	throw e;	}	
removedirective of successful 

try {	CachePoolInfo.validate(info);	String poolName = info.getPoolName();	pool = cachePools.get(poolName);	if (pool != null) {	throw new InvalidRequestException("Cache pool " + poolName + " already exists.");	}	pool = CachePool.createFromInfoAndDefaults(info);	cachePools.put(pool.getPoolName(), pool);	} catch (IOException e) {	
addcachepool of failed 

String poolName = info.getPoolName();	pool = cachePools.get(poolName);	if (pool != null) {	throw new InvalidRequestException("Cache pool " + poolName + " already exists.");	}	pool = CachePool.createFromInfoAndDefaults(info);	cachePools.put(pool.getPoolName(), pool);	} catch (IOException e) {	throw e;	}	
addcachepool of successful 

if (info.getMaxRelativeExpiryMs() != null) {	final Long maxRelativeExpiry = info.getMaxRelativeExpiryMs();	pool.setMaxRelativeExpiryMs(maxRelativeExpiry);	bld.append(prefix).append("set maxRelativeExpiry to " + maxRelativeExpiry);	prefix = "; ";	}	if (prefix.isEmpty()) {	bld.append("no changes.");	}	} catch (IOException e) {	
modifycachepool of failed 

pool.setMaxRelativeExpiryMs(maxRelativeExpiry);	bld.append(prefix).append("set maxRelativeExpiry to " + maxRelativeExpiry);	prefix = "; ";	}	if (prefix.isEmpty()) {	bld.append("no changes.");	}	} catch (IOException e) {	throw e;	}	
modifycachepool of successful 

}	Iterator<CacheDirective> iter = pool.getDirectiveList().iterator();	while (iter.hasNext()) {	CacheDirective directive = iter.next();	directivesByPath.remove(directive.getPath());	directivesById.remove(directive.getId());	iter.remove();	}	setNeedsRescan();	} catch (IOException e) {	
removecachepool of failed 

while (iter.hasNext()) {	CacheDirective directive = iter.next();	directivesByPath.remove(directive.getPath());	directivesById.remove(directive.getId());	iter.remove();	}	setNeedsRescan();	} catch (IOException e) {	throw e;	}	
removecachepool of successful 

for (DatanodeDescriptor datanode : cachedDNs) {	boolean found = false;	for (DatanodeInfo loc : block.getLocations()) {	if (loc.equals(datanode)) {	block.addCachedLoc(loc);	found = true;	break;	}	}	if (!found) {	
datanode is not a valid cache location for block because that node does not have a backing replica 

}	processCacheReportImpl(datanode, blockIds);	} finally {	endTime = Time.monotonicNow();	namesystem.writeUnlock("processCacheReport");	}	final NameNodeMetrics metrics = NameNode.getNameNodeMetrics();	if (metrics != null) {	metrics.addCacheBlockReport((int) (endTime - startTime));	}	
processed cache report from blocks processing time msecs 

private void processCacheReportImpl(final DatanodeDescriptor datanode, final List<Long> blockIds) {	CachedBlocksList cached = datanode.getCached();	cached.clear();	CachedBlocksList cachedList = datanode.getCached();	CachedBlocksList pendingCachedList = datanode.getPendingCached();	for (Iterator<Long> iter = blockIds.iterator(); iter.hasNext(); ) {	long blockId = iter.next();	
cache report from datanode has block 

CachedBlocksList cachedList = datanode.getCached();	CachedBlocksList pendingCachedList = datanode.getPendingCached();	for (Iterator<Long> iter = blockIds.iterator(); iter.hasNext(); ) {	long blockId = iter.next();	CachedBlock cachedBlock = new CachedBlock(blockId, (short)0, false);	CachedBlock prevCachedBlock = cachedBlocks.get(cachedBlock);	if (prevCachedBlock != null) {	cachedBlock = prevCachedBlock;	} else {	cachedBlocks.put(cachedBlock);	
added block to cachedblocks 

long blockId = iter.next();	CachedBlock cachedBlock = new CachedBlock(blockId, (short)0, false);	CachedBlock prevCachedBlock = cachedBlocks.get(cachedBlock);	if (prevCachedBlock != null) {	cachedBlock = prevCachedBlock;	} else {	cachedBlocks.put(cachedBlock);	}	if (!cachedBlock.isPresent(cachedList)) {	cachedList.add(cachedBlock);	
added block to cached list 

if (prevCachedBlock != null) {	cachedBlock = prevCachedBlock;	} else {	cachedBlocks.put(cachedBlock);	}	if (!cachedBlock.isPresent(cachedList)) {	cachedList.add(cachedBlock);	}	if (cachedBlock.isPresent(pendingCachedList)) {	pendingCachedList.remove(cachedBlock);	
removed block from pending cached list 

========================= hadoop sample_8061 =========================

if(parts.length > 3) {	RMContext context = rmWebApp.getRMContext();	String type = parts[2];	ApplicationId appId = null;	ApplicationAttemptId appAttemptId = null;	ContainerId containerId = null;	switch(type){	case "app": try {	appId = Apps.toAppID(parts[3]);	} catch (YarnRuntimeException | NumberFormatException e) {	
error parsing as an applicationid 

} catch (YarnRuntimeException | NumberFormatException e) {	return redirectPath;	}	if(!context.getRMApps().containsKey(appId)) {	redirectPath = pjoin(ahsPageURLPrefix, "app", appId);	}	break;	case "appattempt": try{	appAttemptId = ApplicationAttemptId.fromString(parts[3]);	} catch (IllegalArgumentException e) {	
error parsing as an applicationattemptid 

} catch (IllegalArgumentException e) {	return redirectPath;	}	if(!context.getRMApps().containsKey( appAttemptId.getApplicationId())) {	redirectPath = pjoin(ahsPageURLPrefix, "appattempt", appAttemptId);	}	break;	case "container": try {	containerId = ContainerId.fromString(parts[3]);	} catch (IllegalArgumentException e) {	
error parsing as an containerid 

========================= hadoop sample_777 =========================

public void testRelativePathAsURI() throws IOException {	URI u = Util.stringAsURI(RELATIVE_FILE_PATH);	
uri 

public void testURI() throws IOException {	
testing correct unix uri 

public void testURI() throws IOException {	URI u = Util.stringAsURI(URI_UNIX);	
uri 

public void testURI() throws IOException {	URI u = Util.stringAsURI(URI_UNIX);	assertNotNull("Uri should not be null at this point", u);	assertEquals(URI_FILE_SCHEMA, u.getScheme());	assertEquals(URI_PATH_UNIX, u.getPath());	
testing correct windows uri 

public void testURI() throws IOException {	URI u = Util.stringAsURI(URI_UNIX);	assertNotNull("Uri should not be null at this point", u);	assertEquals(URI_FILE_SCHEMA, u.getScheme());	assertEquals(URI_PATH_UNIX, u.getPath());	u = Util.stringAsURI(URI_WINDOWS);	
uri 

========================= hadoop sample_7510 =========================

when(report.getYarnApplicationState()).thenReturn(YarnApplicationState.FAILED);	when(resourceMgrDelegate.getApplicationReport(appId)).thenReturn(report);	Credentials credentials = new Credentials();	File jobxml = new File(testWorkDir, "job.xml");	OutputStream out = new FileOutputStream(jobxml);	conf.writeXml(out);	out.close();	try {	yarnRunner.submitJob(jobId, testWorkDir.getAbsolutePath().toString(), credentials);	} catch(IOException io) {	
logging exception 

========================= hadoop sample_5487 =========================

public int run(String[] argv) {	if (argv.length < 1) {	OptionsParser.usage();	return DistCpConstants.INVALID_ARGUMENT;	}	try {	inputOptions = (OptionsParser.parse(argv));	setOptionsForSplitLargeFile();	setTargetPathExists();	
input options 

public int run(String[] argv) {	if (argv.length < 1) {	OptionsParser.usage();	return DistCpConstants.INVALID_ARGUMENT;	}	try {	inputOptions = (OptionsParser.parse(argv));	setOptionsForSplitLargeFile();	setTargetPathExists();	} catch (Throwable e) {	
invalid arguments 

setOptionsForSplitLargeFile();	setTargetPathExists();	} catch (Throwable e) {	System.err.println("Invalid arguments: " + e.getMessage());	OptionsParser.usage();	return DistCpConstants.INVALID_ARGUMENT;	}	try {	execute();	} catch (InvalidInputException e) {	
invalid input 

} catch (Throwable e) {	System.err.println("Invalid arguments: " + e.getMessage());	OptionsParser.usage();	return DistCpConstants.INVALID_ARGUMENT;	}	try {	execute();	} catch (InvalidInputException e) {	return DistCpConstants.INVALID_ARGUMENT;	} catch (DuplicateFileException e) {	
duplicate files in input path 

OptionsParser.usage();	return DistCpConstants.INVALID_ARGUMENT;	}	try {	execute();	} catch (InvalidInputException e) {	return DistCpConstants.INVALID_ARGUMENT;	} catch (DuplicateFileException e) {	return DistCpConstants.DUPLICATE_INPUT;	} catch (AclsNotSupportedException e) {	
acls not supported on at least one file system 

}	try {	execute();	} catch (InvalidInputException e) {	return DistCpConstants.INVALID_ARGUMENT;	} catch (DuplicateFileException e) {	return DistCpConstants.DUPLICATE_INPUT;	} catch (AclsNotSupportedException e) {	return DistCpConstants.ACLS_NOT_SUPPORTED;	} catch (XAttrsNotSupportedException e) {	
xattrs not supported on at least one file system 

execute();	} catch (InvalidInputException e) {	return DistCpConstants.INVALID_ARGUMENT;	} catch (DuplicateFileException e) {	return DistCpConstants.DUPLICATE_INPUT;	} catch (AclsNotSupportedException e) {	return DistCpConstants.ACLS_NOT_SUPPORTED;	} catch (XAttrsNotSupportedException e) {	return DistCpConstants.XATTRS_NOT_SUPPORTED;	} catch (Exception e) {	
exception encountered 

prepareFileListing(job);	job.submit();	submitted = true;	} finally {	if (!submitted) {	cleanup();	}	}	String jobID = job.getJobID().toString();	job.getConfiguration().set(DistCpConstants.CONF_LABEL_DISTCP_JOB_ID, jobID);	
distcp job id 

private void setOptionsForSplitLargeFile() throws IOException {	if (!inputOptions.splitLargeFile()) {	return;	}	Path target = inputOptions.getTargetPath();	FileSystem targetFS = target.getFileSystem(getConf());	checkConcatSupport(targetFS);	
enabling preserving blocksize since is passed 

private void setOptionsForSplitLargeFile() throws IOException {	if (!inputOptions.splitLargeFile()) {	return;	}	Path target = inputOptions.getTargetPath();	FileSystem targetFS = target.getFileSystem(getConf());	checkConcatSupport(targetFS);	inputOptions.preserve(FileAttribute.BLOCKSIZE);	
set to false since is passed 

private void setOptionsForSplitLargeFile() throws IOException {	if (!inputOptions.splitLargeFile()) {	return;	}	Path target = inputOptions.getTargetPath();	FileSystem targetFS = target.getFileSystem(getConf());	checkConcatSupport(targetFS);	inputOptions.preserve(FileAttribute.BLOCKSIZE);	inputOptions.setAppend(false);	
set to false since is passed 

}	CopyOutputFormat.setWorkingDirectory(job, workDir);	} else {	CopyOutputFormat.setWorkingDirectory(job, targetPath);	}	CopyOutputFormat.setCommitDirectory(job, targetPath);	Path logPath = inputOptions.getLogPath();	if (logPath == null) {	logPath = new Path(metaFolder, "_logs");	} else {	
distcp job log path 

private Path createMetaFolderPath() throws Exception {	Configuration configuration = getConf();	Path stagingDir = JobSubmissionFiles.getStagingDir( new Cluster(configuration), configuration);	Path metaFolderPath = new Path(stagingDir, PREFIX + String.valueOf(rand.nextInt()));	
meta folder location 

public static void main(String argv[]) {	int exitCode;	try {	DistCp distCp = new DistCp();	Cleanup CLEANUP = new Cleanup(distCp);	ShutdownHookManager.get().addShutdownHook(CLEANUP, SHUTDOWN_HOOK_PRIORITY);	exitCode = ToolRunner.run(getDefaultConf(), distCp, argv);	}	catch (Exception e) {	
couldn t complete distcp operation 

private synchronized void cleanup() {	try {	if (metaFolder != null) {	if (jobFS != null) {	jobFS.delete(metaFolder, true);	}	metaFolder = null;	}	} catch (IOException e) {	
unable to cleanup meta folder 

========================= hadoop sample_6301 =========================

private StorageDirectory loadStorageDirectory(NamespaceInfo nsInfo, File dataDir, StartupOption startOpt, List<Callable<StorageDirectory>> callables, Configuration conf) throws IOException {	StorageDirectory sd = new StorageDirectory(dataDir, null, true);	try {	StorageState curState = sd.analyzeStorage(startOpt, this, true);	switch (curState) {	case NORMAL: break;	
block pool storage directory does not exist 

private StorageDirectory loadStorageDirectory(NamespaceInfo nsInfo, File dataDir, StartupOption startOpt, List<Callable<StorageDirectory>> callables, Configuration conf) throws IOException {	StorageDirectory sd = new StorageDirectory(dataDir, null, true);	try {	StorageState curState = sd.analyzeStorage(startOpt, this, true);	switch (curState) {	case NORMAL: break;	throw new IOException("Storage directory " + dataDir + " does not exist");	
block pool storage directory is not formatted for formatting 

private void format(StorageDirectory bpSdir, NamespaceInfo nsInfo) throws IOException {	
formatting block pool directory 

private boolean doTransition(StorageDirectory sd, NamespaceInfo nsInfo, StartupOption startOpt, List<Callable<StorageDirectory>> callables, Configuration conf) throws IOException {	if (startOpt == StartupOption.ROLLBACK && sd.getPreviousDir().exists()) {	Preconditions.checkState(!getTrashRootDir(sd).exists(), sd.getPreviousDir() + " and " + getTrashRootDir(sd) + " should not " + " both be present.");	doRollback(sd, nsInfo);	} else if (startOpt == StartupOption.ROLLBACK && !sd.getPreviousDir().exists()) {	int restored = restoreBlockFilesFromTrash(getTrashRootDir(sd));	
restored block files from trash 

throw new IOException("Incompatible namespaceIDs in " + sd.getRoot().getCanonicalPath() + ": namenode namespaceID = " + nsInfo.getNamespaceID() + "; datanode namespaceID = " + getNamespaceID());	}	if (!blockpoolID.equals(nsInfo.getBlockPoolID())) {	throw new IOException("Incompatible blockpoolIDs in " + sd.getRoot().getCanonicalPath() + ": namenode blockpoolID = " + nsInfo.getBlockPoolID() + "; datanode blockpoolID = " + blockpoolID);	}	if (this.layoutVersion == HdfsServerConstants.DATANODE_LAYOUT_VERSION && this.cTime == nsInfo.getCTime()) {	return false;	}	if (this.layoutVersion > HdfsServerConstants.DATANODE_LAYOUT_VERSION) {	int restored = restoreBlockFilesFromTrash(getTrashRootDir(sd));	
restored block files from trash before the layout upgrade these blocks will be moved to the previous directory during the upgrade 

private void doUpgrade(String name, final StorageDirectory bpSd, NamespaceInfo nsInfo, final File bpPrevDir, final File bpTmpDir, final File bpCurDir, final int oldLV, Configuration conf) throws IOException {	linkAllBlocks(bpTmpDir, bpCurDir, oldLV, conf);	this.layoutVersion = HdfsServerConstants.DATANODE_LAYOUT_VERSION;	assert this.namespaceID == nsInfo.getNamespaceID() : "Data-node and name-node layout versions must be the same.";	this.cTime = nsInfo.getCTime();	writeProperties(bpSd);	rename(bpTmpDir, bpPrevDir);	
upgrade of is complete 

continue;	}	if (restoreDirectory == null) {	restoreDirectory = new File(getRestoreDirectory(child));	if (!restoreDirectory.exists() && !restoreDirectory.mkdirs()) {	throw new IOException("Failed to create directory " + restoreDirectory);	}	}	final File newChild = new File(restoreDirectory, child.getName());	if (newChild.exists() && newChild.length() >= child.length()) {	
not overwriting with smaller file from trash directory this message can be safely ignored 

public void run() {	try {	deleteDir(tmpDir);	} catch (IOException ex) {	
finalize upgrade for failed 

public void run() {	try {	deleteDir(tmpDir);	} catch (IOException ex) {	}	
finalize upgrade for is complete 

private static void linkAllBlocks(File fromDir, File toDir, int diskLayoutVersion, Configuration conf) throws IOException {	HardLink hardLink = new HardLink();	DataStorage.linkBlocks(fromDir, toDir, DataStorage.STORAGE_DIR_FINALIZED, diskLayoutVersion, hardLink, conf);	DataStorage.linkBlocks(fromDir, toDir, DataStorage.STORAGE_DIR_RBW, diskLayoutVersion, hardLink, conf);	
linked blocks from to 

public void clearTrash() {	final List<File> trashRoots = new ArrayList<>();	for (StorageDirectory sd : getStorageDirs()) {	File trashRoot = getTrashRootDir(sd);	if (trashRoot.exists() && sd.getPreviousDir().exists()) {	
trash and previousdir shouldn t both exist for storage directory 

assert false;	} else {	trashRoots.add(trashRoot);	}	}	stopTrashCleaner();	trashCleaner = new Daemon(new Runnable() {	public void run() {	for(File trashRoot : trashRoots){	FileUtil.fullyDelete(trashRoot);	
cleared trash for storage directory 

public void setRollingUpgradeMarkers(List<StorageDirectory> dnStorageDirs) throws IOException {	for (StorageDirectory sd : dnStorageDirs) {	File bpRoot = getBpRoot(blockpoolID, sd.getCurrentDir());	File markerFile = new File(bpRoot, ROLLING_UPGRADE_MARKER_FILE);	if (!storagesWithRollingUpgradeMarker.contains(bpRoot.toString())) {	if (!markerFile.exists() && markerFile.createNewFile()) {	
created 

public void setRollingUpgradeMarkers(List<StorageDirectory> dnStorageDirs) throws IOException {	for (StorageDirectory sd : dnStorageDirs) {	File bpRoot = getBpRoot(blockpoolID, sd.getCurrentDir());	File markerFile = new File(bpRoot, ROLLING_UPGRADE_MARKER_FILE);	if (!storagesWithRollingUpgradeMarker.contains(bpRoot.toString())) {	if (!markerFile.exists() && markerFile.createNewFile()) {	} else {	
already exists 

public void clearRollingUpgradeMarkers(List<StorageDirectory> dnStorageDirs) throws IOException {	for (StorageDirectory sd : dnStorageDirs) {	File bpRoot = getBpRoot(blockpoolID, sd.getCurrentDir());	File markerFile = new File(bpRoot, ROLLING_UPGRADE_MARKER_FILE);	if (!storagesWithoutRollingUpgradeMarker.contains(bpRoot.toString())) {	if (markerFile.exists()) {	
deleting 

public void clearRollingUpgradeMarkers(List<StorageDirectory> dnStorageDirs) throws IOException {	for (StorageDirectory sd : dnStorageDirs) {	File bpRoot = getBpRoot(blockpoolID, sd.getCurrentDir());	File markerFile = new File(bpRoot, ROLLING_UPGRADE_MARKER_FILE);	if (!storagesWithoutRollingUpgradeMarker.contains(bpRoot.toString())) {	if (markerFile.exists()) {	doFinalize(sd.getCurrentDir());	if (!markerFile.delete()) {	
failed to delete 

========================= hadoop sample_7906 =========================

public void testClientRetryWithFailover(final AtMostOnceOp op) throws Exception {	final Map<String, Object> results = new HashMap<String, Object>();	op.prepare();	DummyRetryInvocationHandler.block.set(true);	new Thread() {	public void run() {	try {	op.invoke();	Object result = op.getResult();	
operation finished 

new Thread() {	public void run() {	try {	op.invoke();	Object result = op.getResult();	synchronized (TestRetryCacheWithHA.this) {	results.put(op.name, result == null ? "SUCCESS" : result);	TestRetryCacheWithHA.this.notifyAll();	}	} catch (Exception e) {	
got exception while calling 

}	} catch (Exception e) {	} finally {	IOUtils.cleanup(null, op.client);	}	}	}.start();	assertTrue("After waiting the operation " + op.name + " still has not taken effect on NN yet", op.checkNamenodeBeforeReturn());	cluster.transitionToStandby(0);	cluster.transitionToActive(1);	
setting block to false 

}	}.start();	assertTrue("After waiting the operation " + op.name + " still has not taken effect on NN yet", op.checkNamenodeBeforeReturn());	cluster.transitionToStandby(0);	cluster.transitionToActive(1);	DummyRetryInvocationHandler.block.set(false);	synchronized (this) {	while (!results.containsKey(op.name)) {	this.wait();	}	
got the result of 

========================= hadoop sample_7455 =========================

conf.set(JHAdminConfig.MR_HISTORY_ADDRESS, HSHOSTADDRESS);	RMService rmService = new RMService("test");	rmService.init(conf);	rmService.start();	AMService amService = new AMService();	amService.init(conf);	amService.start(conf);	HistoryService historyService = new HistoryService();	historyService.init(conf);	historyService.start(conf);	
services started 

amService.init(conf);	amService.start(conf);	HistoryService historyService = new HistoryService();	historyService.init(conf);	historyService.start(conf);	Cluster cluster = new Cluster(conf);	org.apache.hadoop.mapreduce.JobID jobID = new org.apache.hadoop.mapred.JobID("201103121733", 1);	org.apache.hadoop.mapreduce.Counters counters = cluster.getJob(jobID).getCounters();	validateCounters(counters);	Assert.assertTrue(amContact);	
sleeping for seconds before stop for the client socket to not get eof immediately 

HistoryService historyService = new HistoryService();	historyService.init(conf);	historyService.start(conf);	Cluster cluster = new Cluster(conf);	org.apache.hadoop.mapreduce.JobID jobID = new org.apache.hadoop.mapred.JobID("201103121733", 1);	org.apache.hadoop.mapreduce.Counters counters = cluster.getJob(jobID).getCounters();	validateCounters(counters);	Assert.assertTrue(amContact);	Thread.sleep(5000);	amService.stop();	
sleeping for seconds after stop for the server to exit cleanly 

private void validateCounters(org.apache.hadoop.mapreduce.Counters counters) {	Iterator<org.apache.hadoop.mapreduce.CounterGroup> it = counters.iterator();	while (it.hasNext()) {	org.apache.hadoop.mapreduce.CounterGroup group = it.next();	
group 

private void validateCounters(org.apache.hadoop.mapreduce.Counters counters) {	Iterator<org.apache.hadoop.mapreduce.CounterGroup> it = counters.iterator();	while (it.hasNext()) {	org.apache.hadoop.mapreduce.CounterGroup group = it.next();	Iterator<org.apache.hadoop.mapreduce.Counter> itc = group.iterator();	while (itc.hasNext()) {	
counter is 

========================= hadoop sample_5468 =========================

public void processResult(CuratorFramework client, CuratorEvent event) throws Exception {	
received 

========================= hadoop sample_2657 =========================

public static URI stringAsURI(String s) throws IOException {	URI u = null;	try {	u = new URI(s);	} catch (URISyntaxException e){	
syntax error in uri please check hdfs configuration 

public static URI stringAsURI(String s) throws IOException {	URI u = null;	try {	u = new URI(s);	} catch (URISyntaxException e){	}	if(u == null || u.getScheme() == null){	
path should be specified as a uri in configuration files please update hdfs configuration 

public static List<URI> stringCollectionAsURIs( Collection<String> names) {	List<URI> uris = new ArrayList<URI>(names.size());	for(String name : names) {	try {	uris.add(stringAsURI(name));	} catch (IOException e) {	
error while processing uri 

public static boolean isDiskStatsEnabled(int fileIOSamplingPercentage) {	final boolean isEnabled;	if (fileIOSamplingPercentage <= 0) {	
set to disabling file io profiling 

public static boolean isDiskStatsEnabled(int fileIOSamplingPercentage) {	final boolean isEnabled;	if (fileIOSamplingPercentage <= 0) {	isEnabled = false;	} else {	
set to enabling file io profiling 

========================= hadoop sample_8173 =========================

private void testCacheAndUncacheBlock() throws Exception {	
beginning testcacheanduncacheblock 

numCacheCommands = cmds;	}	for (int i=0; i<NUM_BLOCKS; i++) {	setHeartbeatResponse(uncacheBlock(locs[i]));	current = DFSTestUtil. verifyExpectedCacheUsage(current - blockSizes[i], NUM_BLOCKS - 1 - i, fsd);	dnMetrics = getMetrics(dn.getMetrics().name());	long cmds = MetricsAsserts.getLongCounter("BlocksUncached", dnMetrics);	assertTrue("Expected more uncache requests from the NN", cmds > numUncacheCommands);	numUncacheCommands = cmds;	}	
finishing testcacheanduncacheblock 

public void testCacheAndUncacheBlockWithRetries() throws Exception {	NativeIO.POSIX.setCacheManipulator(new NoMlockCacheManipulator() {	private final Set<String> seenIdentifiers = new HashSet<String>();	public void mlock(String identifier, ByteBuffer mmap, long length) throws IOException {	if (seenIdentifiers.contains(identifier)) {	
mlocking 

public void testFilesExceedMaxLockedMemory() throws Exception {	
beginning testfilesexceedmaxlockedmemory 

}, 500, 30000);	assertTrue("Expected more than 0 failed cache attempts", fsd.getNumBlocksFailedToCache() > 0);	int curCachedBlocks = 16;	for (int i=0; i<numFiles-1; i++) {	setHeartbeatResponse(uncacheBlocks(fileLocs[i]));	long uncachedBytes = rounder.roundUp(fileSizes[i]);	total -= uncachedBytes;	curCachedBlocks -= uncachedBytes / BLOCK_SIZE;	DFSTestUtil.verifyExpectedCacheUsage(total, curCachedBlocks, fsd);	}	
finishing testfilesexceedmaxlockedmemory 

public void testUncachingBlocksBeforeCachingFinishes() throws Exception {	
beginning testuncachingblocksbeforecachingfinishes 

HdfsBlockLocation[] locs = (HdfsBlockLocation[])fs.getFileBlockLocations(testFile, 0, testFileLen);	assertEquals("Unexpected number of blocks", NUM_BLOCKS, locs.length);	final long[] blockSizes = getBlockSizes(locs);	final long cacheCapacity = fsd.getCacheCapacity();	long cacheUsed = fsd.getCacheUsed();	long current = 0;	assertEquals("Unexpected cache capacity", CACHE_CAPACITY, cacheCapacity);	assertEquals("Unexpected amount of cache used", current, cacheUsed);	NativeIO.POSIX.setCacheManipulator(new NoMlockCacheManipulator() {	public void mlock(String identifier, ByteBuffer mmap, long length) throws IOException {	
an mlock operation is starting on 

}	}	});	for (int i=0; i<NUM_BLOCKS; i++) {	setHeartbeatResponse(cacheBlock(locs[i]));	current = DFSTestUtil.verifyExpectedCacheUsage( current + blockSizes[i], i + 1, fsd);	}	setHeartbeatResponse(new DatanodeCommand[] {	getResponse(locs, DatanodeProtocol.DNA_UNCACHE) });	current = DFSTestUtil.verifyExpectedCacheUsage(0, 0, fsd);	
finishing testuncachingblocksbeforecachingfinishes 

final Path BIG_FILE = new Path("/bigFile");	DFSTestUtil.createFile(fs, BIG_FILE, TOTAL_BLOCKS_PER_CACHE * BLOCK_SIZE, (short)1, 0xbeef);	final DistributedFileSystem dfs = cluster.getFileSystem();	dfs.addCachePool(new CachePoolInfo("pool"));	final long bigCacheDirectiveId = dfs.addCacheDirective(new CacheDirectiveInfo.Builder() .setPool("pool").setPath(BIG_FILE).setReplication((short)1).build());	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	MetricsRecordBuilder dnMetrics = getMetrics(dn.getMetrics().name());	long blocksCached = MetricsAsserts.getLongCounter("BlocksCached", dnMetrics);	if (blocksCached != TOTAL_BLOCKS_PER_CACHE) {	
waiting for to be cached right now only blocks are cached 

final DistributedFileSystem dfs = cluster.getFileSystem();	dfs.addCachePool(new CachePoolInfo("pool"));	final long bigCacheDirectiveId = dfs.addCacheDirective(new CacheDirectiveInfo.Builder() .setPool("pool").setPath(BIG_FILE).setReplication((short)1).build());	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	MetricsRecordBuilder dnMetrics = getMetrics(dn.getMetrics().name());	long blocksCached = MetricsAsserts.getLongCounter("BlocksCached", dnMetrics);	if (blocksCached != TOTAL_BLOCKS_PER_CACHE) {	return false;	}	
blocks are now cached 

try {	iter = dfs.listCacheDirectives( new CacheDirectiveInfo.Builder().build());	CacheDirectiveEntry entry;	do {	entry = iter.next();	} while (entry.getInfo().getId() != shortCacheDirectiveId);	if (entry.getStats().getFilesCached() != 1) {	LOG.info("waiting for directive " + shortCacheDirectiveId + " to be cached.  stats = " + entry.getStats());	return false;	}	
directive has been cached 

========================= hadoop sample_7286 =========================

String jobJar = conf.get(MRJobConfig.JAR);	if (jobJar != null) {	final Path jobJarPath = new Path(jobJar);	final FileSystem jobJarFs = FileSystem.get(jobJarPath.toUri(), conf);	Path remoteJobJar = jobJarPath.makeQualified(jobJarFs.getUri(), jobJarFs.getWorkingDirectory());	LocalResourceVisibility jobJarViz = conf.getBoolean(MRJobConfig.JOBJAR_VISIBILITY, MRJobConfig.JOBJAR_VISIBILITY_DEFAULT) ? LocalResourceVisibility.PUBLIC : LocalResourceVisibility.APPLICATION;	LocalResource rc = createLocalResource(jobJarFs, remoteJobJar, MRJobConfig.JOB_JAR, LocalResourceType.PATTERN, jobJarViz);	String pattern = conf.getPattern(JobContext.JAR_UNPACK_PATTERN, JobConf.UNPACK_JAR_PATTERN_DEFAULT).pattern();	rc.setPattern(pattern);	localResources.put(MRJobConfig.JOB_JAR, rc);	
the job jar file on the remote fs is 

if (jobJar != null) {	final Path jobJarPath = new Path(jobJar);	final FileSystem jobJarFs = FileSystem.get(jobJarPath.toUri(), conf);	Path remoteJobJar = jobJarPath.makeQualified(jobJarFs.getUri(), jobJarFs.getWorkingDirectory());	LocalResourceVisibility jobJarViz = conf.getBoolean(MRJobConfig.JOBJAR_VISIBILITY, MRJobConfig.JOBJAR_VISIBILITY_DEFAULT) ? LocalResourceVisibility.PUBLIC : LocalResourceVisibility.APPLICATION;	LocalResource rc = createLocalResource(jobJarFs, remoteJobJar, MRJobConfig.JOB_JAR, LocalResourceType.PATTERN, jobJarViz);	String pattern = conf.getPattern(JobContext.JAR_UNPACK_PATTERN, JobConf.UNPACK_JAR_PATTERN_DEFAULT).pattern();	rc.setPattern(pattern);	localResources.put(MRJobConfig.JOB_JAR, rc);	} else {	
job jar is not present not adding any jar to the list of resources 

private static void configureJobConf(Configuration conf, Map<String, LocalResource> localResources, final org.apache.hadoop.mapred.JobID oldJobId) throws IOException {	Path path = MRApps.getStagingAreaDir(conf, UserGroupInformation.getCurrentUser().getShortUserName());	Path remoteJobSubmitDir = new Path(path, oldJobId.toString());	Path remoteJobConfPath = new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);	FileSystem remoteFS = FileSystem.get(conf);	localResources.put(MRJobConfig.JOB_CONF_FILE, createLocalResource(remoteFS, remoteJobConfPath, null, LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));	
the job conf file on the remote fs is 

private static ByteBuffer configureTokens(Token<JobTokenIdentifier> jobToken, Credentials credentials, Map<String, ByteBuffer> serviceData) throws IOException {	
adding tokens and secret keys for nm use for launching container 

private static ByteBuffer configureTokens(Token<JobTokenIdentifier> jobToken, Credentials credentials, Map<String, ByteBuffer> serviceData) throws IOException {	Credentials taskCredentials = new Credentials(credentials);	TokenCache.setJobToken(jobToken, taskCredentials);	DataOutputBuffer containerTokens_dob = new DataOutputBuffer();	
size of containertokens dob is 

private static ByteBuffer configureTokens(Token<JobTokenIdentifier> jobToken, Credentials credentials, Map<String, ByteBuffer> serviceData) throws IOException {	Credentials taskCredentials = new Credentials(credentials);	TokenCache.setJobToken(jobToken, taskCredentials);	DataOutputBuffer containerTokens_dob = new DataOutputBuffer();	taskCredentials.writeTokenStorageToStream(containerTokens_dob);	ByteBuffer taskCredentialsBuffer = ByteBuffer.wrap(containerTokens_dob.getData(), 0, containerTokens_dob.getLength());	
putting shuffle token in servicedata 

private static ByteBuffer configureTokens(Token<JobTokenIdentifier> jobToken, Credentials credentials, Map<String, ByteBuffer> serviceData) throws IOException {	Credentials taskCredentials = new Credentials(credentials);	TokenCache.setJobToken(jobToken, taskCredentials);	DataOutputBuffer containerTokens_dob = new DataOutputBuffer();	taskCredentials.writeTokenStorageToStream(containerTokens_dob);	ByteBuffer taskCredentialsBuffer = ByteBuffer.wrap(containerTokens_dob.getData(), 0, containerTokens_dob.getLength());	byte[] shuffleSecret = TokenCache.getShuffleSecretKey(credentials);	if (shuffleSecret == null) {	
cannot locate shuffle secret in credentials using job token as shuffle secret 

private static void addExternalShuffleProviders(Configuration conf, Map<String, ByteBuffer> serviceData) {	Collection<String> shuffleProviders = conf.getStringCollection( MRJobConfig.MAPREDUCE_JOB_SHUFFLE_PROVIDER_SERVICES);	if (!shuffleProviders.isEmpty()) {	Collection<String> auxNames = conf.getStringCollection(YarnConfiguration.NM_AUX_SERVICES);	for (final String shuffleProvider : shuffleProviders) {	if (shuffleProvider .equals(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID)) {	continue;	}	if (auxNames.contains(shuffleProvider)) {	
adding shuffleprovider service to servicedata 

public void handle(TaskAttemptEvent event) {	if (LOG.isDebugEnabled()) {	
processing of type 

public void handle(TaskAttemptEvent event) {	if (LOG.isDebugEnabled()) {	}	writeLock.lock();	try {	final TaskAttemptStateInternal oldState = getInternalState()  ;	try {	stateMachine.doTransition(event.getType(), event);	} catch (InvalidStateTransitionException e) {	
can t handle this event at current state for 

stateMachine.doTransition(event.getType(), event);	} catch (InvalidStateTransitionException e) {	eventHandler.handle(new JobDiagnosticsUpdateEvent( this.attemptId.getTaskId().getJobId(), "Invalid event " + event.getType() + " on TaskAttempt " + this.attemptId));	eventHandler.handle(new JobEvent(this.attemptId.getTaskId().getJobId(), JobEventType.INTERNAL_ERROR));	}	if (oldState != getInternalState()) {	if (getInternalState() == TaskAttemptStateInternal.FAILED) {	String nodeId = null == this.container ? "Not-assigned" : this.container.getNodeId().toString();	LOG.info(attemptId + " transitioned from state " + oldState + " to " + getInternalState() + ", event type is " + event.getType() + " and nodeId=" + nodeId);	} else {	
taskattempt transitioned from to 

reportedStatus.mapFinishTime = taInfo.getMapFinishTime();	reportedStatus.shuffleFinishTime = taInfo.getShuffleFinishTime();	reportedStatus.sortFinishTime = taInfo.getSortFinishTime();	addDiagnosticInfo(taInfo.getError());	boolean needToClean = false;	String recoveredState = taInfo.getTaskStatus();	if (recoverOutput && TaskAttemptState.SUCCEEDED.toString().equals(recoveredState)) {	TaskAttemptContext tac = new TaskAttemptContextImpl(conf, TypeConverter.fromYarn(attemptId));	try {	committer.recoverTask(tac);	
recovered output from task attempt 

reportedStatus.shuffleFinishTime = taInfo.getShuffleFinishTime();	reportedStatus.sortFinishTime = taInfo.getSortFinishTime();	addDiagnosticInfo(taInfo.getError());	boolean needToClean = false;	String recoveredState = taInfo.getTaskStatus();	if (recoverOutput && TaskAttemptState.SUCCEEDED.toString().equals(recoveredState)) {	TaskAttemptContext tac = new TaskAttemptContextImpl(conf, TypeConverter.fromYarn(attemptId));	try {	committer.recoverTask(tac);	} catch (Exception e) {	
unable to recover task attempt 

reportedStatus.shuffleFinishTime = taInfo.getShuffleFinishTime();	reportedStatus.sortFinishTime = taInfo.getSortFinishTime();	addDiagnosticInfo(taInfo.getError());	boolean needToClean = false;	String recoveredState = taInfo.getTaskStatus();	if (recoverOutput && TaskAttemptState.SUCCEEDED.toString().equals(recoveredState)) {	TaskAttemptContext tac = new TaskAttemptContextImpl(conf, TypeConverter.fromYarn(attemptId));	try {	committer.recoverTask(tac);	} catch (Exception e) {	
task attempt will be recovered as killed 

logAttemptFinishedEvent(attemptState);	} else if (TaskAttemptState.FAILED.toString().equals(recoveredState)) {	attemptState = TaskAttemptStateInternal.FAILED;	reportedStatus.taskState = TaskAttemptState.FAILED;	eventHandler.handle(createJobCounterUpdateEventTAFailed(this, false));	TaskAttemptUnsuccessfulCompletionEvent tauce = createTaskAttemptUnsuccessfulCompletionEvent(this, TaskAttemptStateInternal.FAILED);	eventHandler.handle( new JobHistoryEvent(attemptId.getTaskId().getJobId(), tauce));	} else {	if (!TaskAttemptState.KILLED.toString().equals(recoveredState)) {	if (String.valueOf(recoveredState).isEmpty()) {	
TaskAttempt had not completed recovering as killed 

} else if (TaskAttemptState.FAILED.toString().equals(recoveredState)) {	attemptState = TaskAttemptStateInternal.FAILED;	reportedStatus.taskState = TaskAttemptState.FAILED;	eventHandler.handle(createJobCounterUpdateEventTAFailed(this, false));	TaskAttemptUnsuccessfulCompletionEvent tauce = createTaskAttemptUnsuccessfulCompletionEvent(this, TaskAttemptStateInternal.FAILED);	eventHandler.handle( new JobHistoryEvent(attemptId.getTaskId().getJobId(), tauce));	} else {	if (!TaskAttemptState.KILLED.toString().equals(recoveredState)) {	if (String.valueOf(recoveredState).isEmpty()) {	} else {	
taskattempt found in unexpected state recovering as killed 

reportedStatus.taskState = TaskAttemptState.KILLED;	eventHandler.handle(createJobCounterUpdateEventTAKilled(this, false));	TaskAttemptUnsuccessfulCompletionEvent tauce = createTaskAttemptUnsuccessfulCompletionEvent(this, TaskAttemptStateInternal.KILLED);	eventHandler.handle( new JobHistoryEvent(attemptId.getTaskId().getJobId(), tauce));	}	if (needToClean) {	TaskAttemptContext tac = new TaskAttemptContextImpl(conf, TypeConverter.fromYarn(attemptId));	try {	committer.abortTask(tac);	} catch (Exception e) {	
task cleanup failed for attempt 

private void sendLaunchedEvents() {	JobCounterUpdateEvent jce = new JobCounterUpdateEvent(attemptId.getTaskId() .getJobId());	jce.addCounterUpdate(attemptId.getTaskId().getTaskType() == TaskType.MAP ? JobCounter.TOTAL_LAUNCHED_MAPS : JobCounter.TOTAL_LAUNCHED_REDUCES, 1);	eventHandler.handle(jce);	
taskattempt using containerid on nm 

protected String resolveHost(String src) {	String result = src;	try {	InetAddress addr = InetAddress.getByName(src);	result = addr.getHostName();	} catch (UnknownHostException e) {	
failed to resolve address continuing to use the same 

}	taskAttempt.eventHandler.handle( new ContainerAllocatorEvent(taskAttempt.attemptId, ContainerAllocator.EventType.CONTAINER_DEALLOCATE));	if (withdrawsContainerRequest) {	taskAttempt.eventHandler.handle (new SpeculatorEvent(taskAttempt.getID().getTaskId(), -1));	}	switch(finalState) {	case FAILED: taskAttempt.eventHandler.handle(new TaskTAttemptEvent( taskAttempt.attemptId, TaskEventType.T_ATTEMPT_FAILED));	break;	case KILLED: taskAttempt.eventHandler.handle(new TaskTAttemptKilledEvent( taskAttempt.attemptId, false));	break;	
task final state is not failed or killed 

public void transition(TaskAttemptImpl taskAttempt, TaskAttemptEvent event) {	TaskAttemptTooManyFetchFailureEvent fetchFailureEvent = (TaskAttemptTooManyFetchFailureEvent) event;	Preconditions .checkArgument(taskAttempt.getID().getTaskId().getTaskType() == TaskType.MAP);	taskAttempt.addDiagnosticInfo("Too many fetch failures." + " Failing the attempt. Last failure reported by " + fetchFailureEvent.getReduceId() + " from host " + fetchFailureEvent.getReduceHost());	if (taskAttempt.getLaunchTime() != 0) {	taskAttempt.eventHandler .handle(createJobCounterUpdateEventTAFailed(taskAttempt, true));	TaskAttemptUnsuccessfulCompletionEvent tauce = createTaskAttemptUnsuccessfulCompletionEvent(taskAttempt, TaskAttemptStateInternal.FAILED);	taskAttempt.eventHandler.handle(new JobHistoryEvent( taskAttempt.attemptId.getTaskId().getJobId(), tauce));	}else {	
not generating historyfinish event since start event not generated for taskattempt 

public TaskAttemptStateInternal transition(TaskAttemptImpl taskAttempt, TaskAttemptEvent event) {	if(taskAttempt.getID().getTaskId().getTaskType() == TaskType.REDUCE) {	
ignoring killed event for successful reduce task attempt 

public TaskAttemptStateInternal transition(TaskAttemptImpl taskAttempt, TaskAttemptEvent event) {	taskAttempt.appContext.getTaskAttemptFinishingMonitor().unregister( taskAttempt.attemptId);	sendContainerCleanup(taskAttempt, event);	if(taskAttempt.getID().getTaskId().getTaskType() == TaskType.REDUCE) {	
ignoring killed event for successful reduce task attempt 

public void transition(TaskAttemptImpl taskAttempt, TaskAttemptEvent event) {	TaskAttemptDiagnosticsUpdateEvent diagEvent = (TaskAttemptDiagnosticsUpdateEvent) event;	
diagnostics report from 

========================= hadoop sample_5254 =========================

conf.setBoolean(YarnConfiguration.AMRM_PROXY_ENABLED, true);	cluster.init(conf);	cluster.start();	final Configuration yarnConf = cluster.getConfig();	yarnConf.set(YarnConfiguration.RM_SCHEDULER_ADDRESS, YarnConfiguration.DEFAULT_AMRM_PROXY_ADDRESS);	rmClient.init(yarnConf);	rmClient.start();	ApplicationAttemptId appAttmptId = createApp(rmClient, cluster, conf);	ApplicationId appId = appAttmptId.getApplicationId();	client = createAMRMProtocol(rmClient, appId, cluster, yarnConf);	
register application master 

RegisterApplicationMasterResponse responseRegister = client.registerApplicationMaster(RegisterApplicationMasterRequest .newInstance(NetUtils.getHostname(), 1024, ""));	Assert.assertNotNull(responseRegister);	Assert.assertNotNull(responseRegister.getQueue());	Assert.assertNotNull(responseRegister.getApplicationACLs());	Assert.assertNotNull(responseRegister.getClientToAMTokenMasterKey());	Assert .assertNotNull(responseRegister.getContainersFromPreviousAttempts());	Assert.assertNotNull(responseRegister.getSchedulerResourceTypes());	Assert.assertNotNull(responseRegister.getMaximumResourceCapability());	RMApp rmApp = cluster.getResourceManager().getRMContext().getRMApps().get(appId);	Assert.assertEquals(RMAppState.RUNNING, rmApp.getState());	
allocate resources application master 

AllocateRequest request = createAllocateRequest(rmClient.getNodeReports(NodeState.RUNNING));	AllocateResponse allocResponse = client.allocate(request);	Assert.assertNotNull(allocResponse);	Assert.assertEquals(0, allocResponse.getAllocatedContainers().size());	request.setAskList(new ArrayList<ResourceRequest>());	request.setResponseId(request.getResponseId() + 1);	Thread.sleep(1000);	allocResponse = client.allocate(request);	Assert.assertNotNull(allocResponse);	Assert.assertEquals(2, allocResponse.getAllocatedContainers().size());	
testamrmproxy finish application master 

cluster.init(conf);	cluster.start();	final Configuration yarnConf = cluster.getConfig();	yarnConf.set(YarnConfiguration.RM_SCHEDULER_ADDRESS, YarnConfiguration.DEFAULT_AMRM_PROXY_ADDRESS);	rmClient.init(yarnConf);	rmClient.start();	ApplicationAttemptId appAttmptId = createApp(rmClient, cluster, conf);	ApplicationId appId = appAttmptId.getApplicationId();	client = createAMRMProtocol(rmClient, appId, cluster, yarnConf);	client.registerApplicationMaster(RegisterApplicationMasterRequest .newInstance(NetUtils.getHostname(), 1024, ""));	
testamrmproxytokenrenewal allocate resources application master 

for (int i = 0; i < 5; i++) {	response = client.allocate(request);	request.setResponseId(request.getResponseId() + 1);	if (response.getAMRMToken() != null && !response.getAMRMToken().equals(lastToken)) {	break;	}	lastToken = response.getAMRMToken();	Thread.sleep(4500);	}	Assert.assertFalse(response.getAMRMToken().equals(lastToken));	
testamrmproxy finish application master 

========================= hadoop sample_2591 =========================

JobTokenSecretManager sm = new JobTokenSecretManager();	final Server server = new RPC.Builder(conf) .setProtocol(TaskUmbilicalProtocol.class).setInstance(mockTT) .setBindAddress(ADDRESS).setPort(0).setNumHandlers(5).setVerbose(true) .setSecretManager(sm).build();	server.start();	final UserGroupInformation current = UserGroupInformation.getCurrentUser();	final InetSocketAddress addr = NetUtils.getConnectAddress(server);	String jobId = current.getUserName();	JobTokenIdentifier tokenId = new JobTokenIdentifier(new Text(jobId));	Token<JobTokenIdentifier> token = new Token<JobTokenIdentifier>(tokenId, sm);	sm.addTokenForJob(jobId, token);	SecurityUtil.setTokenService(token, addr);	
service address for token is 

========================= hadoop sample_5604 =========================

String multicastTtlValue = getAttribute(MULTICAST_TTL_PROPERTY);	if (multicastEnabled) {	if (multicastTtlValue == null) {	multicastTtl = DEFAULT_MULTICAST_TTL;	} else {	multicastTtl = Integer.parseInt(multicastTtlValue);	}	}	try {	if (multicastEnabled) {	
enabling multicast for ganglia with ttl 

sb.append('.');	int sbBaseLen = sb.length();	for (String metricName : outRec.getMetricNames()) {	Object metric = outRec.getMetric(metricName);	String type = typeTable.get(metric.getClass());	if (type != null) {	sb.append(metricName);	emitMetric(sb.toString(), type, metric.toString());	sb.setLength(sbBaseLen);	} else {	
unknown metrics type 

========================= hadoop sample_3380 =========================

protected synchronized final void enterState(DestState current, DestState next) throws IllegalStateException {	verifyState(current);	
entering state 

public void close() throws IOException {	if (enterClosedState()) {	
closed 

private ByteBuffer requestBuffer(int limit) {	
requesting buffer of size 

private void releaseBuffer(ByteBuffer buffer) {	
releasing buffer 

public synchronized void close() {	
bytebufferinputstream close for 

public synchronized void mark(int readlimit) {	
mark at 

public synchronized void reset() throws IOException {	
reset 

protected void innerClose() throws IOException {	final DestState state = getState();	
closing 

protected void innerClose() throws IOException {	final DestState state = getState();	switch (state) {	case Writing: if (bufferFile.exists()) {	
block deleting buffer file as upload did not start 

protected void innerClose() throws IOException {	final DestState state = getState();	switch (state) {	case Writing: if (bufferFile.exists()) {	closeBlock();	}	break;	
block buffer file exists close upload stream 

========================= hadoop sample_6028 =========================

public FairCallQueue(int priorityLevels, int capacity, String ns, Configuration conf) {	if(priorityLevels < 1) {	throw new IllegalArgumentException("Number of Priority Levels must be " + "at least 1");	}	int numQueues = priorityLevels;	
faircallqueue is in use with queues with total capacity of 

========================= hadoop sample_4015 =========================

final Capacities desired = new Capacities(rc2, wc2);	capacityOut = exec(newSetCapacity(), S3GuardTool.SetCapacity.NAME, "-" + READ_FLAG, Long.toString(rc2), "-" + WRITE_FLAG, Long.toString(wc2), fsURI);	LOG.info("Set Capacity output=\n{}", capacityOut);	final AtomicInteger c = new AtomicInteger(0);	LambdaTestUtils.eventually(60000, new LambdaTestUtils.VoidCallable() {	public void call() throws Exception {	c.incrementAndGet();	Map<String, String> diags = getMetadataStore().getDiagnostics();	Capacities updated = getCapacities(diags);	String tableInfo = String.format("[%02d] table state: %s", c.intValue(), diags.get(STATUS));	
capacities 

Capacities updated = getCapacities(diags);	String tableInfo = String.format("[%02d] table state: %s", c.intValue(), diags.get(STATUS));	desired.checkEquals(tableInfo, updated);	}	}, new LambdaTestUtils.ProportionalRetryInterval(500, 5000));	Destroy destroyCmd = new Destroy(fs.getConf());	String destroyed = exec(destroyCmd, "destroy", "-meta", "dynamodb: assertFalse(String.format("%s still exists", testTableName), exist(db, testTableName));	expectSuccess("Destroy command did not exit successfully - see output", destroyCmd, "destroy", "-meta", "dynamodb: } catch (ResourceNotFoundException e) {	throw new AssertionError( String.format("DynamoDB table %s does not exist", testTableName), e);	} finally {	
table may have not been cleaned up 

========================= hadoop sample_5940 =========================

SequenceFileAsBinaryOutputFormat.setSequenceFileOutputKeyClass(job, IntWritable.class );	SequenceFileAsBinaryOutputFormat.setSequenceFileOutputValueClass(job, DoubleWritable.class );	SequenceFileAsBinaryOutputFormat.setCompressOutput(job, true);	SequenceFileAsBinaryOutputFormat.setOutputCompressionType(job, CompressionType.BLOCK);	BytesWritable bkey = new BytesWritable();	BytesWritable bval = new BytesWritable();	RecordWriter <BytesWritable, BytesWritable> writer = new SequenceFileAsBinaryOutputFormat().getRecordWriter(fs, job, file.toString(), Reporter.NULL);	IntWritable iwritable = new IntWritable();	DoubleWritable dwritable = new DoubleWritable();	DataOutputBuffer outbuf = new DataOutputBuffer();	
creating data by sequencefileasbinaryoutputformat 

}	} finally {	writer.close(Reporter.NULL);	}	InputFormat<IntWritable,DoubleWritable> iformat = new SequenceFileInputFormat<IntWritable,DoubleWritable>();	int count = 0;	r.setSeed(seed);	DataInputBuffer buf = new DataInputBuffer();	final int NUM_SPLITS = 3;	SequenceFileInputFormat.addInputPath(job, file);	
reading data by sequencefileinputformat 

========================= hadoop sample_5413 =========================

Thread t = new Thread() {	try {	assertEquals("element", 1, (int) q.dequeue());	q.consume(new Consumer<Integer>() {	assertEquals("element", 2, (int) e);	trigger.run();	}	});	}	catch (InterruptedException e) {	
Interrupted 

private void shouldThrowCME(Fun callback) throws Exception {	try {	callback.run();	}	catch (ConcurrentModificationException e) {	LOG.info(e.toString());	return;	}	
should ve thrown cme 

final SinkQueue<Integer> q = new SinkQueue<Integer>(capacity);	for (int i : values) {	q.enqueue(i);	}	final CountDownLatch barrier = new CountDownLatch(1);	Thread t = new Thread() {	try {	Thread.sleep(10);	q.consume(new Consumer<Integer>() {	public void consume(Integer e) throws InterruptedException {	
sleeping 

try {	Thread.sleep(10);	q.consume(new Consumer<Integer>() {	public void consume(Integer e) throws InterruptedException {	barrier.countDown();	Thread.sleep(1000 * 86400);	}	});	}	catch (InterruptedException ex) {	
Interrupted 

========================= hadoop sample_2786 =========================

final LinkedList<ReferenceFileInfo> refList = new LinkedList<ReferenceFileInfo>();	Iterator<ReferenceFileInfo> refIter;	boolean printChecksum = false;	void unpackStorage(String tarFileName, String referenceName) throws IOException {	String tarFile = System.getProperty("test.cache.data", "target/test/cache") + "/" + tarFileName;	File dataDir = GenericTestUtils.getTestDir();	File dfsDir = new File(dataDir, "dfs");	if ( dfsDir.exists() && !FileUtil.fullyDelete(dfsDir) ) {	throw new IOException("Could not delete dfs directory '" + dfsDir + "'");	}	
unpacking 

private void verifyChecksum(String path, long checksum) throws IOException {	if ( refIter == null ) {	refIter = refList.iterator();	}	if ( printChecksum ) {	
crc info for reference file t 

for (int tries = 0; tries < 30; tries++) {	try {	return dfs.dfs.open(pathName);	} catch (IOException e) {	exc = e;	}	if (!exc.getMessage().contains("Cannot obtain " + "block length for LocatedBlock")) {	throw exc;	}	try {	
open failed times retrying 

========================= hadoop sample_7584 =========================

public void quit() {	
exiting on user request 

========================= hadoop sample_7992 =========================

private void recoverFile(final FileSystem fs) throws Exception {	
recovering file lease 

private void recoverFile(final FileSystem fs) throws Exception {	cluster.setLeasePeriod(1000, HdfsConstants.LEASE_HARDLIMIT_PERIOD);	int tries = 60;	boolean recovered = false;	FSDataOutputStream out = null;	while (!recovered && tries-- > 0) {	try {	out = fs.append(file1);	
successfully opened for append 

private void recoverFile(final FileSystem fs) throws Exception {	cluster.setLeasePeriod(1000, HdfsConstants.LEASE_HARDLIMIT_PERIOD);	int tries = 60;	boolean recovered = false;	FSDataOutputStream out = null;	while (!recovered && tries-- > 0) {	try {	out = fs.append(file1);	recovered = true;	} catch (IOException e) {	
failed open for append waiting on lease recovery 

} catch (InterruptedException ex) {	}	}	}	if (out != null) {	out.close();	}	if (!recovered) {	fail("Recovery should take < 1 min");	}	
past out lease recovery 

final AtomicReference<Throwable> err = new AtomicReference<Throwable>();	Thread t = new Thread() {	public void run() {	try {	stm.close();	} catch (Throwable t) {	err.set(t);	}	}};	t.start();	
waiting for close to get to latch 

Thread t = new Thread() {	public void run() {	try {	stm.close();	} catch (Throwable t) {	err.set(t);	}	}};	t.start();	delayer.waitForCall();	
killing lease checker 

stm.close();	} catch (Throwable t) {	err.set(t);	}	}};	t.start();	delayer.waitForCall();	client.getLeaseRenewer().interruptAndJoin();	FileSystem fs1 = cluster.getFileSystem();	FileSystem fs2 = AppendTestUtil.createHdfsWithDifferentUsername( fs1.getConf());	
recovering file 

} catch (Throwable t) {	err.set(t);	}	}};	t.start();	delayer.waitForCall();	client.getLeaseRenewer().interruptAndJoin();	FileSystem fs1 = cluster.getFileSystem();	FileSystem fs2 = AppendTestUtil.createHdfsWithDifferentUsername( fs1.getConf());	recoverFile(fs2);	
telling close to proceed 

err.set(t);	}	}};	t.start();	delayer.waitForCall();	client.getLeaseRenewer().interruptAndJoin();	FileSystem fs1 = cluster.getFileSystem();	FileSystem fs2 = AppendTestUtil.createHdfsWithDifferentUsername( fs1.getConf());	recoverFile(fs2);	delayer.proceed();	
waiting for close to finish 

}	}};	t.start();	delayer.waitForCall();	client.getLeaseRenewer().interruptAndJoin();	FileSystem fs1 = cluster.getFileSystem();	FileSystem fs2 = AppendTestUtil.createHdfsWithDifferentUsername( fs1.getConf());	recoverFile(fs2);	delayer.proceed();	t.join();	
close finished 

final AtomicReference<Throwable> err = new AtomicReference<Throwable>();	Thread t = new Thread() {	public void run() {	try {	stm.close();	} catch (Throwable t) {	err.set(t);	}	}};	t.start();	
waiting for close to get to latch 

Thread t = new Thread() {	public void run() {	try {	stm.close();	} catch (Throwable t) {	err.set(t);	}	}};	t.start();	delayer.waitForCall();	
killing lease checker 

stm.close();	} catch (Throwable t) {	err.set(t);	}	}};	t.start();	delayer.waitForCall();	client.getLeaseRenewer().interruptAndJoin();	FileSystem fs1 = cluster.getFileSystem();	FileSystem fs2 = AppendTestUtil.createHdfsWithDifferentUsername( fs1.getConf());	
recovering file 

} catch (Throwable t) {	err.set(t);	}	}};	t.start();	delayer.waitForCall();	client.getLeaseRenewer().interruptAndJoin();	FileSystem fs1 = cluster.getFileSystem();	FileSystem fs2 = AppendTestUtil.createHdfsWithDifferentUsername( fs1.getConf());	recoverFile(fs2);	
opening file for append from new fs 

err.set(t);	}	}};	t.start();	delayer.waitForCall();	client.getLeaseRenewer().interruptAndJoin();	FileSystem fs1 = cluster.getFileSystem();	FileSystem fs2 = AppendTestUtil.createHdfsWithDifferentUsername( fs1.getConf());	recoverFile(fs2);	FSDataOutputStream appenderStream = fs2.append(file1);	
writing some data from new appender 

}	}};	t.start();	delayer.waitForCall();	client.getLeaseRenewer().interruptAndJoin();	FileSystem fs1 = cluster.getFileSystem();	FileSystem fs2 = AppendTestUtil.createHdfsWithDifferentUsername( fs1.getConf());	recoverFile(fs2);	FSDataOutputStream appenderStream = fs2.append(file1);	AppendTestUtil.write(appenderStream, 0, 4096);	
telling old close to proceed 

}};	t.start();	delayer.waitForCall();	client.getLeaseRenewer().interruptAndJoin();	FileSystem fs1 = cluster.getFileSystem();	FileSystem fs2 = AppendTestUtil.createHdfsWithDifferentUsername( fs1.getConf());	recoverFile(fs2);	FSDataOutputStream appenderStream = fs2.append(file1);	AppendTestUtil.write(appenderStream, 0, 4096);	delayer.proceed();	
waiting for close to finish 

t.start();	delayer.waitForCall();	client.getLeaseRenewer().interruptAndJoin();	FileSystem fs1 = cluster.getFileSystem();	FileSystem fs2 = AppendTestUtil.createHdfsWithDifferentUsername( fs1.getConf());	recoverFile(fs2);	FSDataOutputStream appenderStream = fs2.append(file1);	AppendTestUtil.write(appenderStream, 0, 4096);	delayer.proceed();	t.join();	
close finished 

dn.shutdown();	DFSTestUtil.waitForDatanodeDeath(dn);	}	}	}	DFSTestUtil.waitReplication(fileSystem, f, (short) 0);	try{	fileSystem.append(f);	fail("Append should fail because insufficient locations");	} catch (IOException e){	
expected exception 

========================= hadoop sample_7212 =========================

static void startupShutdownMessage(Class<?> clazz, String[] args, final LogAdapter LOG) {	final String hostname = NetUtils.getHostname();	final String classname = clazz.getSimpleName();	LOG.info(createStartupShutdownMessage(classname, hostname, args));	if (SystemUtils.IS_OS_UNIX) {	try {	SignalLogger.INSTANCE.register(LOG);	} catch (Throwable t) {	
failed to register any unix signal loggers 

final String classname = clazz.getSimpleName();	LOG.info(createStartupShutdownMessage(classname, hostname, args));	if (SystemUtils.IS_OS_UNIX) {	try {	SignalLogger.INSTANCE.register(LOG);	} catch (Throwable t) {	}	}	ShutdownHookManager.get().addShutdownHook( new Runnable() {	public void run() {	
shutdown msg 

========================= hadoop sample_3601 =========================

protected abstract String getSchemeName();	protected abstract OutputStream getOutputStreamForKeystore() throws IOException;	protected abstract boolean keystoreExists() throws IOException;	protected abstract InputStream getInputStreamForFile() throws IOException;	protected abstract void createPermissions(String perms) throws IOException;	protected abstract void stashOriginalFilePermissions() throws IOException;	protected void initFileSystem(URI keystoreUri) throws IOException {	path = ProviderUtils.unnestUri(keystoreUri);	if (LOG.isDebugEnabled()) {	
backing jks path initialized to 

public void flush() throws IOException {	writeLock.lock();	try {	if (!changed) {	
keystore hasn t changed returning 

public void flush() throws IOException {	writeLock.lock();	try {	if (!changed) {	return;	}	
writing out keystore 

========================= hadoop sample_3744 =========================

public static void handleNotRegisteredExceptionAndReRegister( ApplicationAttemptId attemptId, ApplicationMasterProtocol rmProxy, RegisterApplicationMasterRequest registerRequest) throws YarnException {	
app attempt not registered most likely due to rm failover trying to re register 

public static void handleNotRegisteredExceptionAndReRegister( ApplicationAttemptId attemptId, ApplicationMasterProtocol rmProxy, RegisterApplicationMasterRequest registerRequest) throws YarnException {	try {	rmProxy.registerApplicationMaster(registerRequest);	} catch (Exception e) {	if (e instanceof InvalidApplicationMasterRequestException && e.getMessage().contains(APP_ALREADY_REGISTERED_MESSAGE)) {	
concurrent thread successfully registered moving on 

public static void handleNotRegisteredExceptionAndReRegister( ApplicationAttemptId attemptId, ApplicationMasterProtocol rmProxy, RegisterApplicationMasterRequest registerRequest) throws YarnException {	try {	rmProxy.registerApplicationMaster(registerRequest);	} catch (Exception e) {	if (e instanceof InvalidApplicationMasterRequestException && e.getMessage().contains(APP_ALREADY_REGISTERED_MESSAGE)) {	} else {	
error trying to re register am 

public static <T> T createRMProxy(final Configuration configuration, final Class<T> protocol, UserGroupInformation user, final Token<? extends TokenIdentifier> token) throws IOException {	try {	String rmClusterId = configuration.get(YarnConfiguration.RM_CLUSTER_ID, "yarn_cluster");	
creating rmproxy to rm for protocol for user 

========================= hadoop sample_1329 =========================

public void run() {	localNumOpsExecuted = 0;	localCumulativeTime = 0;	arg1 = statsOp.getExecutionArgument(daemonId);	try {	benchmarkOne();	} catch(IOException ex) {	
statsdaemon failed 

} catch (NotReplicatedYetException|RemoteException e) {	if (e instanceof RemoteException) {	String className = ((RemoteException) e).getClassName();	if (!className.equals(NotReplicatedYetException.class.getName())) {	throw e;	}	}	try {	Thread.sleep(100);	} catch (InterruptedException ie) {	
interrupted while retrying addblock 

String excludeFN = config.get(DFSConfigKeys.DFS_HOSTS_EXCLUDE, "exclude");	FileOutputStream excludeFile = new FileOutputStream(excludeFN);	excludeFile.getChannel().truncate(0L);	int nrDatanodes = blockReportObject.getNumDatanodes();	numDecommissionedBlocks = 0;	for(int i=0; i < nodesToDecommission; i++) {	TinyDatanode dn = blockReportObject.datanodes[nrDatanodes-1-i];	numDecommissionedBlocks += dn.nrBlocks;	excludeFile.write(dn.getXferAddr().getBytes());	excludeFile.write('\n');	
datanode is decommissioned 

if(runAll || RenameFileStats.OP_RENAME_NAME.equals(type)) {	opStat = new RenameFileStats(args);	ops.add(opStat);	}	if(runAll || BlockReportStats.OP_BLOCK_REPORT_NAME.equals(type)) {	opStat = new BlockReportStats(args);	ops.add(opStat);	}	if(runAll || ReplicationStats.OP_REPLICATION_NAME.equals(type)) {	if (nnUri.getScheme() != null && nnUri.getScheme().equals("hdfs")) {	
the replication test is ignored as it does not support standalone namenode in another process or on another host 

}	}	if(runAll || CleanAllStats.OP_CLEAN_NAME.equals(type)) {	opStat = new CleanAllStats(args);	ops.add(opStat);	}	if (ops.isEmpty()) {	printUsage();	}	if (nnUri.getScheme() == null || nnUri.getScheme().equals("file")) {	
remote namenode is not specified creating one 

bpid = nameNode.getNamesystem().getBlockPoolId();	} else {	DistributedFileSystem dfs = (DistributedFileSystem) FileSystem.get(getConf());	nameNodeProto = DFSTestUtil.getNamenodeProtocolProxy(config, nnUri, UserGroupInformation.getCurrentUser());	clientProto = dfs.getClient().getNamenode();	dataNodeProto = new DatanodeProtocolClientSideTranslatorPB( DFSUtilClient.getNNAddress(nnUri), config);	refreshUserMappingsProto = DFSTestUtil.getRefreshUserMappingsProtocolProxy(config, nnUri);	getBlockPoolId(dfs);	}	for(OperationStatsBase op : ops) {	
starting benchmark 

========================= hadoop sample_7357 =========================

separator = "&";	}	if (proxyUser != null) {	buf.append(separator).append("doas=").append(proxyUser);	}	boolean isHttps = nnUri.getScheme().equals("https");	HttpURLConnection conn = null;	DataInputStream dis = null;	InetSocketAddress serviceAddr = NetUtils.createSocketAddr(nnUri .getAuthority());	try {	
retrieving token from 

static public long renewDelegationToken(URLConnectionFactory factory, URI nnAddr, Token<DelegationTokenIdentifier> tok) throws IOException, AuthenticationException {	StringBuilder buf = new StringBuilder(nnAddr.toString()) .append(RENEW_DELEGATION_TOKEN_PATH_SPEC).append("?") .append(TOKEN).append("=") .append(tok.encodeToUrlString());	HttpURLConnection connection = null;	BufferedReader in = null;	try {	connection = run(factory, new URL(buf.toString()));	in = new BufferedReader(new InputStreamReader( connection.getInputStream(), Charsets.UTF_8));	long result = Long.parseLong(in.readLine());	return result;	} catch (IOException ie) {	
error in renew over http 

HttpURLConnection connection = null;	BufferedReader in = null;	try {	connection = run(factory, new URL(buf.toString()));	in = new BufferedReader(new InputStreamReader( connection.getInputStream(), Charsets.UTF_8));	long result = Long.parseLong(in.readLine());	return result;	} catch (IOException ie) {	IOException e = getExceptionFromResponse(connection);	if (e != null) {	
rethrowing exception from http request 

if(rs.length < 2) return null;	exceptionClass = rs[0];	exceptionMsg = rs[1];	LOG.info("Error response from HTTP request=" + resp + ";ec=" + exceptionClass + ";em="+exceptionMsg);	if(exceptionClass == null || exceptionClass.isEmpty()) return null;	try {	Class<? extends Exception> ec = Class.forName(exceptionClass).asSubclass(Exception.class);	java.lang.reflect.Constructor<? extends Exception> constructor = ec.getConstructor (new Class[] {String.class});	e =  (IOException) constructor.newInstance (exceptionMsg);	} catch (Exception ee)  {	
failed to create object of this class 

private static HttpURLConnection run(URLConnectionFactory factory, URL url) throws IOException, AuthenticationException {	HttpURLConnection conn = null;	try {	conn = (HttpURLConnection) factory.openConnection(url, true);	if (conn.getResponseCode() != HttpURLConnection.HTTP_OK) {	String msg = conn.getResponseMessage();	throw new IOException("Error when dealing remote token: " + msg);	}	} catch (IOException ie) {	
error when dealing remote token 

HttpURLConnection conn = null;	try {	conn = (HttpURLConnection) factory.openConnection(url, true);	if (conn.getResponseCode() != HttpURLConnection.HTTP_OK) {	String msg = conn.getResponseMessage();	throw new IOException("Error when dealing remote token: " + msg);	}	} catch (IOException ie) {	IOException e = getExceptionFromResponse(conn);	if (e != null) {	
rethrowing exception from http request 

========================= hadoop sample_6815 =========================

c.setKeystoreType(keyStoreType);	c.setPassword(keyStorePassword);	}	if (trustStore != null) {	c.setTruststore(trustStore);	c.setTruststoreType(trustStoreType);	c.setTrustPassword(trustStorePassword);	}	if(null != excludeCiphers && !excludeCiphers.isEmpty()) {	c.setExcludeCipherSuites( StringUtils.getTrimmedStrings(excludeCiphers));	
excluded cipher list 

if (initializers != null) {	conf = new Configuration(conf);	conf.set(BIND_ADDRESS, hostName);	for (FilterInitializer c : initializers) {	c.initFilter(this, conf);	}	}	addDefaultServlets();	if (pathSpecs != null) {	for (String path : pathSpecs) {	
adding path spec 

public boolean isRunning() {	if (super.isRunning()) {	return true;	}	
httpserver acceptor isrunning is false rechecking 

public boolean isRunning() {	if (super.isRunning()) {	return true;	}	try {	Thread.sleep(10);	} catch (InterruptedException ie) {	Thread.currentThread().interrupt();	}	boolean runState = super.isRunning();	
httpserver acceptor isrunning is 

public void addInternalServlet(String name, String pathSpec, Class<? extends HttpServlet> clazz, boolean requireAuth) {	ServletHolder holder = new ServletHolder(clazz);	if (name != null) {	holder.setName(name);	}	webAppContext.addServlet(holder, pathSpec);	if(requireAuth && UserGroupInformation.isSecurityEnabled()) {	
adding kerberos spnego filter to 

public void start() throws IOException {	try {	try {	openListeners();	webServer.start();	} catch (IOException ex) {	
httpserver start threw a non bind ioexception 

public void start() throws IOException {	try {	try {	openListeners();	webServer.start();	} catch (IOException ex) {	throw ex;	} catch (MultiException ex) {	
httpserver start threw a multiexception 

private static void bindListener(Connector listener) throws Exception {	listener.close();	listener.open();	
jetty bound to port 

public void stop() throws Exception {	MultiException exception = null;	for (Connector c : listeners) {	try {	c.close();	} catch (Exception e) {	
error while stopping listener for webapp 

c.close();	} catch (Exception e) {	exception = addMultiException(exception, e);	}	}	try {	secretProvider.destroy();	webAppContext.clearAttributes();	webAppContext.stop();	} catch (Exception e) {	
error while stopping web app context for webapp 

try {	secretProvider.destroy();	webAppContext.clearAttributes();	webAppContext.stop();	} catch (Exception e) {	exception = addMultiException(exception, e);	}	try {	webServer.stop();	} catch (Exception e) {	
error while stopping web server for webapp 

========================= hadoop sample_3969 =========================

public void testMRNewTimelineServiceEventHandling() throws Exception {	
testmrnewtimelineserviceeventhandling start 

conf.setBoolean(MRJobConfig.MAPREDUCE_JOB_EMIT_TIMELINE_DATA, true);	conf.set(FileSystemTimelineWriterImpl.TIMELINE_SERVICE_STORAGE_DIR_ROOT, storageDir);	conf.set(YarnConfiguration.NM_AUX_SERVICES, TIMELINE_AUX_SERVICE_NAME);	conf.set(YarnConfiguration.NM_AUX_SERVICES + "." + TIMELINE_AUX_SERVICE_NAME + ".class", PerNodeTimelineCollectorsAuxService.class.getName());	conf.setBoolean(YarnConfiguration.SYSTEM_METRICS_PUBLISHER_ENABLED, true);	MiniMRYarnCluster cluster = null;	try {	cluster = new MiniMRYarnCluster( TestMRTimelineEventHandling.class.getSimpleName(), 1, true);	cluster.init(conf);	cluster.start();	
a minimryarncluster get start 

conf.set(YarnConfiguration.NM_AUX_SERVICES, TIMELINE_AUX_SERVICE_NAME);	conf.set(YarnConfiguration.NM_AUX_SERVICES + "." + TIMELINE_AUX_SERVICE_NAME + ".class", PerNodeTimelineCollectorsAuxService.class.getName());	conf.setBoolean(YarnConfiguration.SYSTEM_METRICS_PUBLISHER_ENABLED, true);	MiniMRYarnCluster cluster = null;	try {	cluster = new MiniMRYarnCluster( TestMRTimelineEventHandling.class.getSimpleName(), 1, true);	cluster.init(conf);	cluster.start();	Path inDir = new Path(testDir, "input");	Path outDir = new Path(testDir, "output");	
run job which should be successful 

yarnClient.init(new Configuration(cluster.getConfig()));	yarnClient.start();	EnumSet<YarnApplicationState> appStates = EnumSet.allOf(YarnApplicationState.class);	ApplicationId firstAppId = null;	List<ApplicationReport> apps = yarnClient.getApplications(appStates);	Assert.assertEquals(apps.size(), 1);	ApplicationReport appReport = apps.get(0);	firstAppId = appReport.getApplicationId();	UtilsForTests.waitForAppFinished(job, cluster);	checkNewTimelineEvent(firstAppId, appReport, storageDir);	
run job which should be failed 

========================= hadoop sample_5444 =========================

private void caseSingleFileMissingTarget(boolean sync) {	try {	Path listFile = new Path("/tmp/listing");	Path target = new Path("/tmp/target");	addEntries(listFile, "/tmp/singlefile1/file1");	createFiles("/tmp/singlefile1/file1");	runTest(listFile, target, false, sync);	checkResult(listFile, 0);	} catch (IOException e) {	
exception encountered while testing build listing 

private void caseSingleFileTargetFile(boolean sync) {	try {	Path listFile = new Path("/tmp/listing");	Path target = new Path("/tmp/target");	addEntries(listFile, "/tmp/singlefile1/file1");	createFiles("/tmp/singlefile1/file1", target.toString());	runTest(listFile, target, false, sync);	checkResult(listFile, 0);	} catch (IOException e) {	
exception encountered while testing build listing 

private void caseSingleFileTargetDir(boolean sync) {	try {	Path listFile = new Path("/tmp/listing");	Path target = new Path("/tmp/target");	addEntries(listFile, "/tmp/singlefile2/file2");	createFiles("/tmp/singlefile2/file2");	mkdirs(target.toString());	runTest(listFile, target, true, sync);	checkResult(listFile, 1);	} catch (IOException e) {	
exception encountered while testing build listing 

private void caseSingleDirTargetMissing(boolean sync) {	try {	Path listFile = new Path("/tmp/listing");	Path target = new Path("/tmp/target");	addEntries(listFile, "/tmp/singledir");	mkdirs("/tmp/singledir/dir1");	runTest(listFile, target, false, sync);	checkResult(listFile, 1);	} catch (IOException e) {	
exception encountered while testing build listing 

public void testSingleDirTargetPresent() {	try {	Path listFile = new Path("/tmp/listing");	Path target = new Path("/tmp/target");	addEntries(listFile, "/tmp/singledir");	mkdirs("/tmp/singledir/dir1");	mkdirs(target.toString());	runTest(listFile, target, true);	checkResult(listFile, 1);	} catch (IOException e) {	
exception encountered while testing build listing 

public void testUpdateSingleDirTargetPresent() {	try {	Path listFile = new Path("/tmp/listing");	Path target = new Path("/tmp/target");	addEntries(listFile, "/tmp/Usingledir");	mkdirs("/tmp/Usingledir/Udir1");	mkdirs(target.toString());	runTest(listFile, target, true, true);	checkResult(listFile, 1);	} catch (IOException e) {	
exception encountered while testing build listing 

private void caseMultiFileTargetPresent(boolean sync) {	try {	Path listFile = new Path("/tmp/listing");	Path target = new Path("/tmp/target");	addEntries(listFile, "/tmp/multifile/file3", "/tmp/multifile/file4", "/tmp/multifile/file5");	createFiles("/tmp/multifile/file3", "/tmp/multifile/file4", "/tmp/multifile/file5");	mkdirs(target.toString());	runTest(listFile, target, true, sync);	checkResult(listFile, 3);	} catch (IOException e) {	
exception encountered while testing build listing 

private void caseMultiFileTargetMissing(boolean sync) {	try {	Path listFile = new Path("/tmp/listing");	Path target = new Path("/tmp/target");	addEntries(listFile, "/tmp/multifile/file3", "/tmp/multifile/file4", "/tmp/multifile/file5");	createFiles("/tmp/multifile/file3", "/tmp/multifile/file4", "/tmp/multifile/file5");	runTest(listFile, target, false, sync);	checkResult(listFile, 3);	} catch (IOException e) {	
exception encountered while testing build listing 

public void testMultiDirTargetPresent() {	try {	Path listFile = new Path("/tmp/listing");	Path target = new Path("/tmp/target");	addEntries(listFile, "/tmp/multifile", "/tmp/singledir");	createFiles("/tmp/multifile/file3", "/tmp/multifile/file4", "/tmp/multifile/file5");	mkdirs(target.toString(), "/tmp/singledir/dir1");	runTest(listFile, target, true);	checkResult(listFile, 4);	} catch (IOException e) {	
exception encountered while testing build listing 

public void testUpdateMultiDirTargetPresent() {	try {	Path listFile = new Path("/tmp/listing");	Path target = new Path("/tmp/target");	addEntries(listFile, "/tmp/Umultifile", "/tmp/Usingledir");	createFiles("/tmp/Umultifile/Ufile3", "/tmp/Umultifile/Ufile4", "/tmp/Umultifile/Ufile5");	mkdirs(target.toString(), "/tmp/Usingledir/Udir1");	runTest(listFile, target, true);	checkResult(listFile, 4);	} catch (IOException e) {	
exception encountered while testing build listing 

private void caseMultiDirTargetMissing(boolean sync) {	try {	Path listFile = new Path("/tmp/listing");	Path target = new Path("/tmp/target");	addEntries(listFile, "/tmp/multifile", "/tmp/singledir");	createFiles("/tmp/multifile/file3", "/tmp/multifile/file4", "/tmp/multifile/file5");	mkdirs("/tmp/singledir/dir1");	runTest(listFile, target, sync);	checkResult(listFile, 4);	} catch (IOException e) {	
exception encountered while testing build listing 

private void caseGlobTargetMissingSingleLevel(boolean sync) {	try {	Path listFile = new Path("/tmp1/listing");	Path target = new Path("/tmp/target");	addEntries(listFile, "/tmp/*");	createFiles("/tmp/multifile/file3", "/tmp/multifile/file4", "/tmp/multifile/file5");	createFiles("/tmp/singledir/dir2/file6");	runTest(listFile, target, sync);	checkResult(listFile, 5);	} catch (IOException e) {	
exception encountered while testing build listing 

private void caseGlobTargetMissingMultiLevel(boolean sync) {	try {	Path listFile = new Path("/tmp1/listing");	Path target = new Path("/tmp/target");	addEntries(listFile, "/tmp/*/*");	createFiles("/tmp/multifile/file3", "/tmp/multifile/file4", "/tmp/multifile/file5");	createFiles("/tmp/singledir1/dir3/file7", "/tmp/singledir1/dir3/file8", "/tmp/singledir1/dir3/file9");	runTest(listFile, target, sync);	checkResult(listFile, 6);	} catch (IOException e) {	
exception encountered while testing build listing 

try {	Path listFile = new Path("/tmp1/listing");	Path target = new Path("/tmp/target");	addEntries(listFile, "/tmp/*/*");	createFiles("/tmp/multifile/file3", "/tmp/multifile/file4", "/tmp/multifile/file5");	createFiles("/tmp/singledir1/dir3/file7", "/tmp/singledir1/dir3/file8", "/tmp/singledir1/dir3/file9");	mkdirs(target.toString());	runTest(listFile, target, true);	checkResult(listFile, 6);	} catch (IOException e) {	
exception encountered while testing build listing 

try {	Path listFile = new Path("/tmp1/listing");	Path target = new Path("/tmp/target");	addEntries(listFile, "/tmp/*/*");	createFiles("/tmp/Umultifile/Ufile3", "/tmp/Umultifile/Ufile4", "/tmp/Umultifile/Ufile5");	createFiles("/tmp/Usingledir1/Udir3/Ufile7", "/tmp/Usingledir1/Udir3/Ufile8", "/tmp/Usingledir1/Udir3/Ufile9");	mkdirs(target.toString());	runTest(listFile, target, true);	checkResult(listFile, 6);	} catch (IOException e) {	
exception encountered while testing build listing 

========================= hadoop sample_6268 =========================

public void announce() {	
running test 

========================= hadoop sample_5852 =========================

public boolean nextKeyValue() throws IOException, InterruptedException {	if (chunk == null) {	
recordreader is null no records to be read 

public boolean nextKeyValue() throws IOException, InterruptedException {	if (chunk == null) {	return false;	}	if (chunk.getReader().nextKeyValue()) {	++numRecordsProcessedByThisMap;	return true;	}	
current chunk exhausted attempting to pick up new one 

========================= hadoop sample_6287 =========================

reloadThread = new Thread() {	public void run() {	while (running) {	long time = clock.getTime();	long lastModified = allocFile.lastModified();	if (lastModified > lastSuccessfulReload && time > lastModified + ALLOC_RELOAD_WAIT_MS) {	try {	reloadAllocations();	} catch (Exception ex) {	if (!lastReloadAttemptFailed) {	
failed to reload fair scheduler config file will use existing allocations 

if (lastModified > lastSuccessfulReload && time > lastModified + ALLOC_RELOAD_WAIT_MS) {	try {	reloadAllocations();	} catch (Exception ex) {	if (!lastReloadAttemptFailed) {	}	lastReloadAttemptFailed = true;	}	} else if (lastModified == 0l) {	if (!lastReloadAttemptFailed) {	
failed to reload fair scheduler config file because last modified returned file exists 

lastReloadAttemptFailed = true;	}	} else if (lastModified == 0l) {	if (!lastReloadAttemptFailed) {	}	lastReloadAttemptFailed = true;	}	try {	Thread.sleep(reloadIntervalMs);	} catch (InterruptedException ex) {	
interrupted while waiting to reload alloc configuration 

public void serviceStop() throws Exception {	running = false;	if (reloadThread != null) {	reloadThread.interrupt();	try {	reloadThread.join(THREAD_JOIN_TIMEOUT_MS);	} catch (InterruptedException e) {	
reloadthread fails to join 

public File getAllocationFile(Configuration conf) {	String allocFilePath = conf.get(FairSchedulerConfiguration.ALLOCATION_FILE, FairSchedulerConfiguration.DEFAULT_ALLOCATION_FILE);	File allocFile = new File(allocFilePath);	if (!allocFile.isAbsolute()) {	URL url = Thread.currentThread().getContextClassLoader() .getResource(allocFilePath);	if (url == null) {	
not found on the classpath 

public synchronized void reloadAllocations() throws IOException, ParserConfigurationException, SAXException, AllocationConfigurationException {	if (allocFile == null) {	reloadListener.onReload(null);	return;	}	
loading allocation file 

} else if ("reservation-planner".equals(element.getTagName())) {	String text = ((Text) element.getFirstChild()).getData().trim();	planner = text;	} else if ("reservation-agent".equals(element.getTagName())) {	String text = ((Text) element.getFirstChild()).getData().trim();	reservationAgent = text;	} else if ("reservation-policy".equals(element.getTagName())) {	String text = ((Text) element.getFirstChild()).getData().trim();	reservationAdmissionPolicy = text;	} else {	
bad element in allocations file 

private void checkMinAndMaxResource(Map<String, Resource> minResources, Map<String, ConfigurableResource> maxResources, String queueName) {	ConfigurableResource maxConfigurableResource = maxResources.get(queueName);	Resource minResource = minResources.get(queueName);	if (maxConfigurableResource != null && minResource != null) {	Resource maxResource = maxConfigurableResource.getResource();	if (maxResource != null && !Resources.fitsIn(minResource, maxResource)) {	
queue s has max resources s less than min resources s 

========================= hadoop sample_976 =========================

public void testSimple() throws Exception {	
running testsimple 

public void testMapNodeLocality() throws Exception {	
running testmapnodelocality 

public void testResource() throws Exception {	
running testresource 

public void testReducerRampdownDiagnostics() throws Exception {	
running tesreducerrampdowndiagnostics 

public void testPreemptReducers() throws Exception {	
running testpreemptreducers 

public void testNonAggressivelyPreemptReducers() throws Exception {	
running testnonaggressivelypreemptreducers 

public void testUnconditionalPreemptReducers() throws Exception {	
running testforcepreemptreducers 

public void testMapReduceAllocationWithNodeLabelExpression() throws Exception {	
running testmapreduceallocationwithnodelabelexpression 

public void testUpdateCollectorInfo() throws Exception {	
running testupdatecollectorinfo 

public void testMapReduceScheduling() throws Exception {	
running testmapreducescheduling 

public void testReportedAppProgress() throws Exception {	
running testreportedappprogress 

public void testReportedAppProgressWithOnlyMaps() throws Exception {	
running testreportedappprogresswithonlymaps 

public void testBlackListedNodes() throws Exception {	
running testblacklistednodes 

public void testIgnoreBlacklisting() throws Exception {	
running testignoreblacklisting 

rm.drainEvents();	ApplicationAttemptId appAttemptId = app.getCurrentAppAttempt().getAppAttemptId();	rm.sendAMLaunched(appAttemptId);	rm.drainEvents();	JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);	Job mockJob = mock(Job.class);	when(mockJob.getReport()).thenReturn( MRBuilderUtils.newJobReport(jobId, "job", "user", JobState.RUNNING, 0, 0, 0, 0, 0, 0, 0, "jobfile", null, false, ""));	MyContainerAllocator allocator = new MyContainerAllocator(rm, conf, appAttemptId, mockJob);	assigned = getContainerOnHost(jobId, 1, 1024, new String[] { "h1" }, nodeManagers[0], allocator, 0, 0, 0, 0, rm);	Assert.assertEquals("No of assignments must be 1", 1, assigned.size());	
failing container on node should be blacklisted and ignore blacklisting enabled 

public void testBlackListedNodesWithSchedulingToThatNode() throws Exception {	
running testblacklistednodeswithschedulingtothatnode 

ApplicationAttemptId appAttemptId = app.getCurrentAppAttempt() .getAppAttemptId();	rm.sendAMLaunched(appAttemptId);	rm.drainEvents();	JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);	Job mockJob = mock(Job.class);	when(mockJob.getReport()).thenReturn( MRBuilderUtils.newJobReport(jobId, "job", "user", JobState.RUNNING, 0, 0, 0, 0, 0, 0, 0, "jobfile", null, false, ""));	MyContainerAllocator allocator = new MyContainerAllocator(rm, conf, appAttemptId, mockJob);	MockNM nodeManager1 = rm.registerNode("h1:1234", 10240);	MockNM nodeManager3 = rm.registerNode("h3:1234", 10240);	rm.drainEvents();	
requesting containers on 

rm.drainEvents();	JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);	Job mockJob = mock(Job.class);	when(mockJob.getReport()).thenReturn( MRBuilderUtils.newJobReport(jobId, "job", "user", JobState.RUNNING, 0, 0, 0, 0, 0, 0, 0, "jobfile", null, false, ""));	MyContainerAllocator allocator = new MyContainerAllocator(rm, conf, appAttemptId, mockJob);	MockNM nodeManager1 = rm.registerNode("h1:1234", 10240);	MockNM nodeManager3 = rm.registerNode("h3:1234", 10240);	rm.drainEvents();	ContainerRequestEvent event1 = createReq(jobId, 1, 1024, new String[] { "h1" });	allocator.sendRequest(event1);	
rm heartbeat to send the container requests 

when(mockJob.getReport()).thenReturn( MRBuilderUtils.newJobReport(jobId, "job", "user", JobState.RUNNING, 0, 0, 0, 0, 0, 0, 0, "jobfile", null, false, ""));	MyContainerAllocator allocator = new MyContainerAllocator(rm, conf, appAttemptId, mockJob);	MockNM nodeManager1 = rm.registerNode("h1:1234", 10240);	MockNM nodeManager3 = rm.registerNode("h3:1234", 10240);	rm.drainEvents();	ContainerRequestEvent event1 = createReq(jobId, 1, 1024, new String[] { "h1" });	allocator.sendRequest(event1);	List<TaskAttemptContainerAssignedEvent> assigned = allocator.schedule();	rm.drainEvents();	Assert.assertEquals("No of assignments must be 0", 0, assigned.size());	
heartbeat to actually schedule the containers 

MockNM nodeManager1 = rm.registerNode("h1:1234", 10240);	MockNM nodeManager3 = rm.registerNode("h3:1234", 10240);	rm.drainEvents();	ContainerRequestEvent event1 = createReq(jobId, 1, 1024, new String[] { "h1" });	allocator.sendRequest(event1);	List<TaskAttemptContainerAssignedEvent> assigned = allocator.schedule();	rm.drainEvents();	Assert.assertEquals("No of assignments must be 0", 0, assigned.size());	nodeManager1.nodeHeartbeat(true);	rm.drainEvents();	
rm heartbeat to process the scheduled containers 

allocator.sendRequest(event1);	List<TaskAttemptContainerAssignedEvent> assigned = allocator.schedule();	rm.drainEvents();	Assert.assertEquals("No of assignments must be 0", 0, assigned.size());	nodeManager1.nodeHeartbeat(true);	rm.drainEvents();	assigned = allocator.schedule();	rm.drainEvents();	assertBlacklistAdditionsAndRemovals(0, 0, rm);	Assert.assertEquals("No of assignments must be 1", 1, assigned.size());	
failing container on should blacklist the node 

ContainerFailedEvent f1 = createFailEvent(jobId, 1, "h1", false);	allocator.sendFailure(f1);	ContainerRequestEvent event1f = createReq(jobId, 1, 1024, new String[] { "h1" }, true, false);	allocator.sendRequest(event1f);	assigned = allocator.schedule();	rm.drainEvents();	assertBlacklistAdditionsAndRemovals(1, 0, rm);	Assert.assertEquals("No of assignments must be 0", 0, assigned.size());	ContainerRequestEvent event3 = createReq(jobId, 3, 1024, new String[] { "h1", "h3" });	allocator.sendRequest(event3);	
heartbeat to actually schedule the containers 

ContainerRequestEvent event1f = createReq(jobId, 1, 1024, new String[] { "h1" }, true, false);	allocator.sendRequest(event1f);	assigned = allocator.schedule();	rm.drainEvents();	assertBlacklistAdditionsAndRemovals(1, 0, rm);	Assert.assertEquals("No of assignments must be 0", 0, assigned.size());	ContainerRequestEvent event3 = createReq(jobId, 3, 1024, new String[] { "h1", "h3" });	allocator.sendRequest(event3);	nodeManager1.nodeHeartbeat(true);	rm.drainEvents();	
rm heartbeat to process the scheduled containers 

assertBlacklistAdditionsAndRemovals(1, 0, rm);	Assert.assertEquals("No of assignments must be 0", 0, assigned.size());	ContainerRequestEvent event3 = createReq(jobId, 3, 1024, new String[] { "h1", "h3" });	allocator.sendRequest(event3);	nodeManager1.nodeHeartbeat(true);	rm.drainEvents();	assigned = allocator.schedule();	rm.drainEvents();	assertBlacklistAdditionsAndRemovals(0, 0, rm);	Assert.assertEquals("No of assignments must be 0", 0, assigned.size());	
rm heartbeat to process the re scheduled containers 

nodeManager1.nodeHeartbeat(true);	rm.drainEvents();	assigned = allocator.schedule();	rm.drainEvents();	assertBlacklistAdditionsAndRemovals(0, 0, rm);	Assert.assertEquals("No of assignments must be 0", 0, assigned.size());	assigned = allocator.schedule();	rm.drainEvents();	assertBlacklistAdditionsAndRemovals(0, 0, rm);	Assert.assertEquals("No of assignments must be 0", 0, assigned.size());	
heartbeat to re schedule the containers 

assigned = allocator.schedule();	rm.drainEvents();	assertBlacklistAdditionsAndRemovals(0, 0, rm);	Assert.assertEquals("No of assignments must be 0", 0, assigned.size());	assigned = allocator.schedule();	rm.drainEvents();	assertBlacklistAdditionsAndRemovals(0, 0, rm);	Assert.assertEquals("No of assignments must be 0", 0, assigned.size());	nodeManager3.nodeHeartbeat(true);	rm.drainEvents();	
rm heartbeat to process the re scheduled containers for 

assigned = allocator.schedule();	rm.drainEvents();	assertBlacklistAdditionsAndRemovals(0, 0, rm);	Assert.assertEquals("No of assignments must be 0", 0, assigned.size());	nodeManager3.nodeHeartbeat(true);	rm.drainEvents();	assigned = allocator.schedule();	assertBlacklistAdditionsAndRemovals(0, 0, rm);	rm.drainEvents();	for (TaskAttemptContainerAssignedEvent assig : assigned) {	
assgined to with priority 

public MyFifoScheduler(RMContext rmContext) {	super();	try {	Configuration conf = new Configuration();	reinitialize(conf, rmContext);	} catch (IOException ie) {	
add application failed with 

public ExcessReduceContainerAllocateScheduler(RMContext rmContext) {	super();	try {	Configuration conf = new Configuration();	reinitialize(conf, rmContext);	} catch (IOException ie) {	
add application failed with 

public void testCompletedTasksRecalculateSchedule() throws Exception {	
running testcompletedtasksrecalculateschedule 

public void testHeartbeatHandler() throws Exception {	
running testheartbeathandler 

public void testAMRMTokenUpdate() throws Exception {	
running testamrmtokenupdate 

public void testConcurrentTaskLimits() throws Exception {	final int MAP_COUNT = 5;	final int REDUCE_COUNT = 2;	final int MAP_LIMIT = 3;	final int REDUCE_LIMIT = 1;	
running testconcurrenttasklimits 

public void testUpdateAskOnRampDownAllReduces() throws Exception {	
running testupdateaskonrampdownallreduces 

public void testAvoidAskMoreReducersWhenReducerPreemptionIsRequired() throws Exception {	
running testavoidaskmorereducerswhenreducerpreemptionisrequired 

public void testExcludeSchedReducesFromHeadroom() throws Exception {	
running testexcludeschedreducesfromheadroom 

========================= hadoop sample_5151 =========================

private static ContainerRetryContext configureRetryContext( Configuration conf, ContainerLaunchContext launchContext, ContainerId containerId) {	ContainerRetryContext context;	if (launchContext != null && launchContext.getContainerRetryContext() != null) {	context = launchContext.getContainerRetryContext();	} else {	context = ContainerRetryContext.NEVER_RETRY_CONTEXT;	}	int minimumRestartInterval = conf.getInt( YarnConfiguration.NM_CONTAINER_RETRY_MINIMUM_INTERVAL_MS, YarnConfiguration.DEFAULT_NM_CONTAINER_RETRY_MINIMUM_INTERVAL_MS);	if (context.getRetryPolicy() != ContainerRetryPolicy.NEVER_RETRY && context.getRetryInterval() < minimumRestartInterval) {	
set restart interval to minimum value ms for container 

private void addDiagnostics(String... diags) {	for (String s : diags) {	this.diagnostics.append("[" + dateFormat.format(new Date()) + "]" + s);	}	if (diagnostics.length() > diagnosticsMaxSize) {	diagnostics.delete(0, diagnostics.length() - diagnosticsMaxSize);	}	try {	stateStore.storeContainerDiagnostics(containerId, diagnostics);	} catch (IOException e) {	
unable to update diagnostics in state store for 

public void transition( ContainerImpl container, ContainerEvent event) {	UpdateContainerTokenEvent updateEvent = (UpdateContainerTokenEvent)event;	container.setContainerTokenIdentifier(updateEvent.getUpdatedToken());	try {	container.context.getNMStateStore() .storeContainerUpdateToken(container.containerId, container.getContainerTokenIdentifier());	} catch (IOException e) {	
could not store container update 

container.dispatcher.getEventHandler().handle( new AuxServicesEvent(AuxServicesEventType.APPLICATION_INIT, container.user, container.containerId .getApplicationAttemptId().getApplicationId(), service.getKey().toString(), service.getValue()));	}	}	container.containerLocalizationStartTime = clock.getTime();	Map<String,LocalResource> cntrRsrc = ctxt.getLocalResources();	if (!cntrRsrc.isEmpty()) {	try {	Map<LocalResourceVisibility, Collection<LocalResourceRequest>> req = container.resourceSet.addResources(ctxt.getLocalResources());	container.dispatcher.getEventHandler().handle( new ContainerLocalizationRequestEvent(container, req));	} catch (URISyntaxException e) {	
failed to parse resource request 

public ContainerState transition(ContainerImpl container, ContainerEvent event) {	ContainerResourceLocalizedEvent rsrcEvent = (ContainerResourceLocalizedEvent) event;	LocalResourceRequest resourceRequest = rsrcEvent.getResource();	Path location = rsrcEvent.getLocation();	Set<String> syms = container.resourceSet.resourceLocalized(resourceRequest, location);	if (null == syms) {	
localized resource for container 

ResourceSet newResourceSet = container.reInitContext.newResourceSet;	if (!newResourceSet.getPendingResources().isEmpty()) {	container.dispatcher.getEventHandler().handle( new ContainerLocalizationRequestEvent( container, newResourceSet.getAllResourcesByVisibility()));	} else {	container.dispatcher.getEventHandler().handle( new ContainersLauncherEvent(container, ContainersLauncherEventType.CLEANUP_CONTAINER_FOR_REINIT));	resourcesPresent = true;	}	container.metrics.reInitingContainer();	NMAuditLogger.logSuccess(container.user, AuditConstants.START_CONTAINER_REINIT, "ContainerImpl", container.containerId.getApplicationAttemptId().getApplicationId(), container.containerId);	} catch (Exception e) {	
container re initialization failure 

protected ReInitializationContext createReInitContext(ContainerImpl container, ContainerEvent event) {	container.addDiagnostics("Container upgrade will be Rolled-back.\n");	
container about to be explicitly rolledback 

public void transition(ContainerImpl container, ContainerEvent event) {	ContainerResourceLocalizedEvent rsrcEvent = (ContainerResourceLocalizedEvent) event;	Set<String> links = container.resourceSet.resourceLocalized( rsrcEvent.getResource(), rsrcEvent.getLocation());	if (links == null) {	return;	}	for (String link : links) {	try {	String linkFile = new Path(container.workDir, link).toString();	if (new File(linkFile).exists()) {	
symlink file already exists 

Set<String> links = container.resourceSet.resourceLocalized( rsrcEvent.getResource(), rsrcEvent.getLocation());	if (links == null) {	return;	}	for (String link : links) {	try {	String linkFile = new Path(container.workDir, link).toString();	if (new File(linkFile).exists()) {	} else {	container.context.getContainerExecutor() .symLink(rsrcEvent.getLocation().toString(), linkFile);	
created symlink 

public void transition(ContainerImpl container, ContainerEvent event) {	ContainerResourceFailedEvent failedEvent = (ContainerResourceFailedEvent) event;	container.resourceSet.resourceLocalizationFailed( failedEvent.getResource());	container.addDiagnostics("Container aborting re-initialization.. " + failedEvent.getDiagnosticMessage());	
container re init failed resource could not be localized 

container.metrics.runningContainer();	container.wasLaunched  = true;	if (container.isReInitializing()) {	NMAuditLogger.logSuccess(container.user, AuditConstants.FINISH_CONTAINER_REINIT, "ContainerImpl", container.containerId.getApplicationAttemptId().getApplicationId(), container.containerId);	}	container.setIsReInitializing(false);	if (container.reInitContext != null && !container.reInitContext.canRollback()) {	container.reInitContext = null;	}	if (container.recoveredAsKilled) {	
killing due to recovered as killed 

container.addDiagnostics("Diagnostic message from attempt " + n + " : ", "\n");	}	container.addDiagnostics(exitEvent.getDiagnosticInfo(), "\n");	}	if (container.shouldRetry(container.exitCode)) {	if (container.remainingRetryAttempts > 0) {	container.remainingRetryAttempts--;	try {	container.stateStore.storeContainerRemainingRetryAttempts( container.getContainerId(), container.remainingRetryAttempts);	} catch (IOException e) {	
unable to update remainingretryattempts in state store for 

container.remainingRetryAttempts--;	try {	container.stateStore.storeContainerRemainingRetryAttempts( container.getContainerId(), container.remainingRetryAttempts);	} catch (IOException e) {	}	}	doRelaunch(container, container.remainingRetryAttempts, container.containerRetryContext.getRetryInterval());	return ContainerState.RELAUNCHING;	} else if (container.canRollback()) {	container.addDiagnostics("Container Re-init Auto Rolled-Back.");	
rolling back container reinitialization for 

private void doRelaunch(final ContainerImpl container, int remainingRetryAttempts, final int retryInterval) {	
relaunching container remaining retry attempts after relaunch interval between retries is ms 

public void transition(ContainerImpl container, ContainerEvent event) {	
relaunching container for re initialization 

public void transition(ContainerImpl container, ContainerEvent event) {	if (container.wasLaunched) {	container.metrics.endRunningContainer();	} else {	
container exited with success despite being killed and not actually running 

public void handle(ContainerEvent event) {	try {	this.writeLock.lock();	ContainerId containerID = event.getContainerID();	if (LOG.isDebugEnabled()) {	
processing of type 

try {	this.writeLock.lock();	ContainerId containerID = event.getContainerID();	if (LOG.isDebugEnabled()) {	}	ContainerState oldState = stateMachine.getCurrentState();	ContainerState newState = null;	try {	newState = stateMachine.doTransition(event.getType(), event);	} catch (InvalidStateTransitionException e) {	
can t handle this event at current state current eventtype container 

ContainerId containerID = event.getContainerID();	if (LOG.isDebugEnabled()) {	}	ContainerState oldState = stateMachine.getCurrentState();	ContainerState newState = null;	try {	newState = stateMachine.doTransition(event.getType(), event);	} catch (InvalidStateTransitionException e) {	}	if (newState != null && oldState != newState) {	
container transitioned from to 

========================= hadoop sample_1811 =========================

public static S3AInputPolicy getPolicy(String name) {	String trimmed = name.trim().toLowerCase(Locale.ENGLISH);	switch (trimmed) {	case INPUT_FADV_NORMAL: return Normal;	case INPUT_FADV_RANDOM: return Random;	case INPUT_FADV_SEQUENTIAL: return Sequential;	
unrecognized value 

========================= hadoop sample_6004 =========================

for (int i = 0; i < numDirs; i++) {	try {	Path tmpDir = new Path(dirStrings[i]);	if(ctx.localFS.mkdirs(tmpDir)|| ctx.localFS.exists(tmpDir)) {	try {	File tmpFile = tmpDir.isAbsolute() ? new File(ctx.localFS.makeQualified(tmpDir).toUri()) : new File(dirStrings[i]);	DiskChecker.checkDir(tmpFile);	dirs.add(new Path(tmpFile.getPath()));	dfList.add(new DF(tmpFile, 30000));	} catch (DiskErrorException de) {	
is not writable 

Path tmpDir = new Path(dirStrings[i]);	if(ctx.localFS.mkdirs(tmpDir)|| ctx.localFS.exists(tmpDir)) {	try {	File tmpFile = tmpDir.isAbsolute() ? new File(ctx.localFS.makeQualified(tmpDir).toUri()) : new File(dirStrings[i]);	DiskChecker.checkDir(tmpFile);	dirs.add(new Path(tmpFile.getPath()));	dfList.add(new DF(tmpFile, 30000));	} catch (DiskErrorException de) {	}	} else {	
failed to create 

try {	File tmpFile = tmpDir.isAbsolute() ? new File(ctx.localFS.makeQualified(tmpDir).toUri()) : new File(dirStrings[i]);	DiskChecker.checkDir(tmpFile);	dirs.add(new Path(tmpFile.getPath()));	dfList.add(new DF(tmpFile, 30000));	} catch (DiskErrorException de) {	}	} else {	}	} catch (IOException ie) {	
failed to create 

private Path createPath(Path dir, String path, boolean checkWrite) throws IOException {	Path file = new Path(dir, path);	if (checkWrite) {	try {	DiskChecker.checkDir(new File(file.getParent().toUri().getPath()));	return file;	} catch (DiskErrorException d) {	
disk error exception 

========================= hadoop sample_4193 =========================

public void announce() {	
running test 

========================= hadoop sample_2944 =========================

private void deleteAndEnsureInTrash(Path pathToDelete, File blockFile, File trashFile) throws Exception {	assertTrue(blockFile.exists());	assertFalse(trashFile.exists());	
deleting file during rolling upgrade 

private void startRollingUpgrade() throws Exception {	
starting rolling upgrade 

private void finalizeRollingUpgrade() throws Exception {	
finalizing rolling upgrade 

private void rollbackRollingUpgrade() throws Exception {	
starting rollback of the rolling upgrade 

private void rollbackRollingUpgrade() throws Exception {	MiniDFSCluster.DataNodeProperties dnprop = cluster.stopDataNode(0);	dnprop.setDnArgs("-rollback");	cluster.shutdownNameNodes();	cluster.restartNameNode("-rollingupgrade", "rollback");	cluster.restartDataNode(dnprop);	cluster.waitActive();	nn = cluster.getNameNode(0);	dn0 = cluster.getDataNodes().get(0);	triggerHeartBeats();	
the cluster is active after rollback 

Path[] paths = new Path[3];	File[] blockFiles = new File[3];	for (int i = 0; i < 2; ++i) {	paths[i] = new Path("/" + GenericTestUtils.getMethodName() + "." + i + ".dat");	DFSTestUtil.createFile(fs, paths[i], BLOCK_SIZE, (short) 2, seed);	}	startRollingUpgrade();	blockFiles[0] = getBlockForFile(paths[0], true);	File trashFile0 = getTrashFileForBlock(blockFiles[0], false);	deleteAndEnsureInTrash(paths[0], blockFiles[0], trashFile0);	
shutting down the datanode 

for (int i = 0; i < 2; ++i) {	paths[i] = new Path("/" + GenericTestUtils.getMethodName() + "." + i + ".dat");	DFSTestUtil.createFile(fs, paths[i], BLOCK_SIZE, (short) 2, seed);	}	startRollingUpgrade();	blockFiles[0] = getBlockForFile(paths[0], true);	File trashFile0 = getTrashFileForBlock(blockFiles[0], false);	deleteAndEnsureInTrash(paths[0], blockFiles[0], trashFile0);	MiniDFSCluster.DataNodeProperties dnprop = cluster.stopDataNode(0);	DFSTestUtil.addDataNodeLayoutVersion( DataNodeLayoutVersion.CURRENT_LAYOUT_VERSION - 1, "Test Layout for TestDataNodeRollingUpgrade");	
restarting the datanode 

}	startRollingUpgrade();	blockFiles[0] = getBlockForFile(paths[0], true);	File trashFile0 = getTrashFileForBlock(blockFiles[0], false);	deleteAndEnsureInTrash(paths[0], blockFiles[0], trashFile0);	MiniDFSCluster.DataNodeProperties dnprop = cluster.stopDataNode(0);	DFSTestUtil.addDataNodeLayoutVersion( DataNodeLayoutVersion.CURRENT_LAYOUT_VERSION - 1, "Test Layout for TestDataNodeRollingUpgrade");	cluster.restartDataNode(dnprop, true);	cluster.waitActive();	dn0 = cluster.getDataNodes().get(0);	
the dn has been restarted 

Path[] paths = new Path[3];	File[] blockFiles = new File[3];	for (int i = 0; i < 2; ++i) {	paths[i] = new Path("/" + GenericTestUtils.getMethodName() + "." + i + ".dat");	DFSTestUtil.createFile(fs, paths[i], BLOCK_SIZE, (short) 1, seed);	}	startRollingUpgrade();	blockFiles[0] = getBlockForFile(paths[0], true);	File trashFile0 = getTrashFileForBlock(blockFiles[0], false);	deleteAndEnsureInTrash(paths[0], blockFiles[0], trashFile0);	
shutting down the datanode 

for (int i = 0; i < 2; ++i) {	paths[i] = new Path("/" + GenericTestUtils.getMethodName() + "." + i + ".dat");	DFSTestUtil.createFile(fs, paths[i], BLOCK_SIZE, (short) 1, seed);	}	startRollingUpgrade();	blockFiles[0] = getBlockForFile(paths[0], true);	File trashFile0 = getTrashFileForBlock(blockFiles[0], false);	deleteAndEnsureInTrash(paths[0], blockFiles[0], trashFile0);	MiniDFSCluster.DataNodeProperties dnprop = cluster.stopDataNode(0);	DFSTestUtil.addDataNodeLayoutVersion( DataNodeLayoutVersion.CURRENT_LAYOUT_VERSION - 1, "Test Layout for TestDataNodeRollingUpgrade");	
restarting the datanode 

}	startRollingUpgrade();	blockFiles[0] = getBlockForFile(paths[0], true);	File trashFile0 = getTrashFileForBlock(blockFiles[0], false);	deleteAndEnsureInTrash(paths[0], blockFiles[0], trashFile0);	MiniDFSCluster.DataNodeProperties dnprop = cluster.stopDataNode(0);	DFSTestUtil.addDataNodeLayoutVersion( DataNodeLayoutVersion.CURRENT_LAYOUT_VERSION - 1, "Test Layout for TestDataNodeRollingUpgrade");	cluster.restartDataNode(dnprop, true);	cluster.waitActive();	dn0 = cluster.getDataNodes().get(0);	
the dn has been restarted 

========================= hadoop sample_7245 =========================

if (groupId != null) {	entitiesToEntityCache.add(entity);	} else {	entitiesToDBStore.add(entity);	}	}	}	if (!entitiesToSummaryCache.isEmpty()) {	Path summaryLogPath = new Path(attemptDir, SUMMARY_LOG_PREFIX + appAttemptId.toString());	if (LOG.isDebugEnabled()) {	
writing summary log for to 

}	if (!entitiesToSummaryCache.isEmpty()) {	Path summaryLogPath = new Path(attemptDir, SUMMARY_LOG_PREFIX + appAttemptId.toString());	if (LOG.isDebugEnabled()) {	}	this.logFDsCache.writeSummaryEntityLogs(fs, summaryLogPath, objMapper, appAttemptId, entitiesToSummaryCache, isAppendSupported);	}	if (!entitiesToEntityCache.isEmpty()) {	Path entityLogPath = new Path(attemptDir, ENTITY_LOG_PREFIX + groupId.toString());	if (LOG.isDebugEnabled()) {	
writing entity log for to 

public synchronized void close() throws Exception {	if (logFDsCache != null) {	
closing cache 

public void flush() throws IOException {	if (logFDsCache != null) {	
flushing cache 

private void writeDomain(ApplicationAttemptId appAttemptId, TimelineDomain domain) throws IOException {	Path domainLogPath = new Path(attemptDirCache.getAppAttemptDir(appAttemptId), DOMAIN_LOG_PREFIX + appAttemptId.toString());	if (LOG.isDebugEnabled()) {	
writing domains for to 

public void writeEntities(List<TimelineEntity> entities) throws IOException {	if (writerClosed()) {	prepareForWrite();	}	if (LOG.isDebugEnabled()) {	
writing entity list of size 

private Path createAttemptDir(ApplicationAttemptId appAttemptId) throws IOException {	Path appDir = createApplicationDir(appAttemptId.getApplicationId());	Path attemptDir = new Path(appDir, appAttemptId.toString());	if (FileSystem.mkdirs(fs, attemptDir, new FsPermission(APP_LOG_DIR_PERMISSIONS))) {	if (LOG.isDebugEnabled()) {	
new attempt directory created 

private Path createApplicationDir(ApplicationId appId) throws IOException {	Path appRootDir = getAppRootDir(authUgi.getShortUserName());	Path appDir = new Path(appRootDir, appId.toString());	if (FileSystem.mkdirs(fs, appDir, new FsPermission(APP_LOG_DIR_PERMISSIONS))) {	if (LOG.isDebugEnabled()) {	
new app directory created 

private Path getAppRootDir(String user) throws IOException {	if (!storeInsideUserDir) {	return activePath;	}	Path userDir = new Path(activePath, user);	if (FileSystem.mkdirs(fs, userDir, new FsPermission(APP_LOG_DIR_PERMISSIONS))) {	if (LOG.isDebugEnabled()) {	
new user directory created 

========================= hadoop sample_2552 =========================

public static String getProcessId(Path path) throws IOException {	if (path == null) {	throw new IOException("Trying to access process id from a null path");	}	if (LOG.isDebugEnabled()) {	
accessing pid from pid file 

========================= hadoop sample_1684 =========================

public SubmitterUserResolver() throws IOException {	
current user resolver is submitteruserresolver 

========================= hadoop sample_6141 =========================

SharedFileDescriptorFactory factory = SharedFileDescriptorFactory.create("shm_", new String[] { path.getAbsolutePath() });	FileInputStream stream = factory.createDescriptor("testAllocateSlots", 4096);	ShortCircuitShm shm = new ShortCircuitShm(ShmId.createRandom(), stream);	int numSlots = 0;	ArrayList<Slot> slots = new ArrayList<Slot>();	while (!shm.isFull()) {	Slot slot = shm.allocAndRegisterSlot(new ExtendedBlockId(123L, "test_bp1"));	slots.add(slot);	numSlots++;	}	
allocated slots before running out 

========================= hadoop sample_6784 =========================

public TestFileInputFormat(int numThreads) {	this.numThreads = numThreads;	
running with numthreads 

public void setup() throws IOException {	
using test dir 

========================= hadoop sample_4613 =========================

MetricDiff listRequests = new MetricDiff(fs, OBJECT_LIST_REQUESTS);	MetricDiff listContinueRequests = new MetricDiff(fs, OBJECT_CONTINUE_LIST_REQUESTS);	MetricDiff listStatusCalls = new MetricDiff(fs, INVOCATION_LIST_FILES);	MetricDiff getFileStatusCalls = new MetricDiff(fs, INVOCATION_GET_FILE_STATUS);	NanoTimer createTimer = new NanoTimer();	TreeScanResults created = createSubdirs(fs, listDir, depth, width, files, 0);	int emptyDepth = 1 * scale;	int emptyWidth = 3 * scale;	created.add(createSubdirs(fs, listDir, emptyDepth, emptyWidth, 0, 0, "empty", "f-", ""));	createTimer.end("Time to create %s", created);	
time per operation 

describe("Timing getFileStatus(\"%s\")", path);	S3AFileSystem fs = getFileSystem();	MetricDiff metadataRequests = new MetricDiff(fs, Statistic.OBJECT_METADATA_REQUESTS);	MetricDiff listRequests = new MetricDiff(fs, Statistic.OBJECT_LIST_REQUESTS);	long attempts = getOperationCount();	NanoTimer timer = new NanoTimer();	for (long l = 0; l < attempts; l++) {	fs.getFileStatus(path);	}	timer.end("Time to execute %d getFileStatusCalls", attempts);	
time per call 

describe("Timing getFileStatus(\"%s\")", path);	S3AFileSystem fs = getFileSystem();	MetricDiff metadataRequests = new MetricDiff(fs, Statistic.OBJECT_METADATA_REQUESTS);	MetricDiff listRequests = new MetricDiff(fs, Statistic.OBJECT_LIST_REQUESTS);	long attempts = getOperationCount();	NanoTimer timer = new NanoTimer();	for (long l = 0; l < attempts; l++) {	fs.getFileStatus(path);	}	timer.end("Time to execute %d getFileStatusCalls", attempts);	
metadata 

describe("Timing getFileStatus(\"%s\")", path);	S3AFileSystem fs = getFileSystem();	MetricDiff metadataRequests = new MetricDiff(fs, Statistic.OBJECT_METADATA_REQUESTS);	MetricDiff listRequests = new MetricDiff(fs, Statistic.OBJECT_LIST_REQUESTS);	long attempts = getOperationCount();	NanoTimer timer = new NanoTimer();	for (long l = 0; l < attempts; l++) {	fs.getFileStatus(path);	}	timer.end("Time to execute %d getFileStatusCalls", attempts);	
metadata per operation 

describe("Timing getFileStatus(\"%s\")", path);	S3AFileSystem fs = getFileSystem();	MetricDiff metadataRequests = new MetricDiff(fs, Statistic.OBJECT_METADATA_REQUESTS);	MetricDiff listRequests = new MetricDiff(fs, Statistic.OBJECT_LIST_REQUESTS);	long attempts = getOperationCount();	NanoTimer timer = new NanoTimer();	for (long l = 0; l < attempts; l++) {	fs.getFileStatus(path);	}	timer.end("Time to execute %d getFileStatusCalls", attempts);	
listobjects 

describe("Timing getFileStatus(\"%s\")", path);	S3AFileSystem fs = getFileSystem();	MetricDiff metadataRequests = new MetricDiff(fs, Statistic.OBJECT_METADATA_REQUESTS);	MetricDiff listRequests = new MetricDiff(fs, Statistic.OBJECT_LIST_REQUESTS);	long attempts = getOperationCount();	NanoTimer timer = new NanoTimer();	for (long l = 0; l < attempts; l++) {	fs.getFileStatus(path);	}	timer.end("Time to execute %d getFileStatusCalls", attempts);	
listobjects per operation 

========================= hadoop sample_5901 =========================

try {	stop(server, proxy);	assertEquals("Not enough clients", numClients, res.size());	for (Future<Void> f : res) {	try {	f.get();	fail("Future get should not return");	} catch (ExecutionException e) {	ServiceException se = (ServiceException) e.getCause();	assertTrue("Unexpected exception: " + se, se.getCause() instanceof IOException);	
expected exception 

========================= hadoop sample_3139 =========================

private void initializeSSLConf(Context context) throws IOException {	
initializing ssl configuration 

private void initializeSSLConf(Context context) throws IOException {	String workDir = conf.get(JobContext.JOB_LOCAL_DIR) + "/work";	Path[] cacheFiles = context.getLocalCacheFiles();	Configuration sslConfig = new Configuration(false);	String sslConfFileName = conf.get(DistCpConstants.CONF_LABEL_SSL_CONF);	Path sslClient = findCacheFile(cacheFiles, sslConfFileName);	if (sslClient == null) {	
ssl client config file not found was looking for in 

sslConfig.set("ssl.client.keystore.location", keyStorePath.toString());	try {	OutputStream out = new FileOutputStream(workDir + "/" + sslConfFileName);	try {	sslConfig.writeXml(out);	} finally {	out.close();	}	conf.set(DistCpConstants.CONF_LABEL_SSL_KEYSTORE, sslConfFileName);	} catch (IOException e) {	
unable to write out the ssl configuration will fall back to default ssl client xml in class path if there is one 

public void map(Text relPath, CopyListingFileStatus sourceFileStatus, Context context) throws IOException, InterruptedException {	Path sourcePath = sourceFileStatus.getPath();	
distcpmapper map received 

sourceFS = sourcePath.getFileSystem(conf);	final boolean preserveXAttrs = fileAttributes.contains(FileAttribute.XATTR);	sourceCurrStatus = DistCpUtils.toCopyListingFileStatusHelper(sourceFS, sourceFS.getFileStatus(sourcePath), fileAttributes.contains(FileAttribute.ACL), preserveXAttrs, preserveRawXattrs, sourceFileStatus.getChunkOffset(), sourceFileStatus.getChunkLength());	} catch (FileNotFoundException e) {	throw new IOException(new RetriableFileCopyCommand.CopyReadException(e));	}	FileStatus targetStatus = null;	try {	targetStatus = targetFS.getFileStatus(target);	} catch (FileNotFoundException ignore) {	
path could not be found 

if (targetStatus != null && (targetStatus.isDirectory() != sourceCurrStatus.isDirectory())) {	throw new IOException("Can't replace " + target + ". Target is " + getFileType(targetStatus) + ", Source is " + getFileType(sourceCurrStatus));	}	if (sourceCurrStatus.isDirectory()) {	createTargetDirsWithRetry(description, target, context);	return;	}	FileAction action = checkUpdate(sourceFS, sourceCurrStatus, target, targetStatus);	Path tmpTarget = target;	if (action == FileAction.SKIP) {	
skipping copy of to 

updateSkipCounters(context, sourceCurrStatus);	context.write(null, new Text("SKIP: " + sourceCurrStatus.getPath()));	if (verboseLog) {	context.write(null, new Text("FILE_SKIPPED: source=" + sourceFileStatus.getPath() + ", size=" + sourceFileStatus.getLen() + " --> " + "target=" + target + ", size=" + (targetStatus == null ? 0 : targetStatus.getLen())));	}	} else {	if (sourceCurrStatus.isSplit()) {	tmpTarget = DistCpUtils.getSplitChunkPath(target, sourceCurrStatus);	}	if (LOG.isDebugEnabled()) {	
copying 

========================= hadoop sample_6291 =========================

public ReplicaOutputStreams createStreams(boolean isCreate, DataChecksum requestedChecksum) throws IOException {	File blockFile = getBlockFile();	File metaFile = getMetaFile();	if (DataNode.LOG.isDebugEnabled()) {	
writeto blockfile is of size 

public ReplicaOutputStreams createStreams(boolean isCreate, DataChecksum requestedChecksum) throws IOException {	File blockFile = getBlockFile();	File metaFile = getMetaFile();	if (DataNode.LOG.isDebugEnabled()) {	
writeto metafile is of size 

public OutputStream createRestartMetaStream() throws IOException {	File blockFile = getBlockFile();	File restartMeta = new File(blockFile.getParent()  + File.pathSeparator + "." + blockFile.getName() + ".restart");	if (!getFileIoProvider().deleteWithExistsCheck(getVolume(), restartMeta)) {	
failed to delete restart meta file 

========================= hadoop sample_7879 =========================

protected void processLine(String line) throws Exception {	List<String> badRecords = MAPPER_BAD_RECORDS;	if(isReducer) {	badRecords = REDUCER_BAD_RECORDS;	}	if(badRecords.size()>0 && line.contains(badRecords.get(0))) {	
encountered bad record 

protected void processLine(String line) throws Exception {	List<String> badRecords = MAPPER_BAD_RECORDS;	if(isReducer) {	badRecords = REDUCER_BAD_RECORDS;	}	if(badRecords.size()>0 && line.contains(badRecords.get(0))) {	System.exit(-1);	}	else if(badRecords.size()>1 && line.contains(badRecords.get(1))) {	
encountered bad record 

if(isReducer) {	badRecords = REDUCER_BAD_RECORDS;	}	if(badRecords.size()>0 && line.contains(badRecords.get(0))) {	System.exit(-1);	}	else if(badRecords.size()>1 && line.contains(badRecords.get(1))) {	throw new Exception("Got bad record..crashing");	}	else if(badRecords.size()>2 && line.contains(badRecords.get(2))) {	
encountered bad record 

========================= hadoop sample_5794 =========================

doReturn("4.0.0").when(fakeNsInfo).getSoftwareVersion();	doReturn("3.0.0").when(mockDnConf).getMinimumNameNodeVersion();	assertEquals("4.0.0", actor.retrieveNamespaceInfo().getSoftwareVersion());	doReturn("3.0.0").when(fakeNsInfo).getSoftwareVersion();	doReturn("4.0.0").when(mockDnConf).getMinimumNameNodeVersion();	try {	actor.retrieveNamespaceInfo();	fail("Should have thrown an exception for NN with too-low version");	} catch (IncorrectVersionException ive) {	GenericTestUtils.assertExceptionContains( "The reported NameNode version is too low", ive);	
got expected exception 

========================= hadoop sample_7253 =========================

InvocationHandler dummyHandler = new LossyRetryInvocationHandler<>( numResponseToDrop, failoverProxyProvider, RetryPolicies.failoverOnNetworkException( RetryPolicies.TRY_ONCE_THEN_FAIL, maxFailoverAttempts, Math.max(numResponseToDrop + 1, maxRetryAttempts), delay, maxCap));	T proxy = (T) Proxy.newProxyInstance( failoverProxyProvider.getInterface().getClassLoader(), new Class[]{xface}, dummyHandler);	Text dtService;	if (failoverProxyProvider.useLogicalURI()) {	dtService = HAUtilClient.buildTokenServiceForLogicalUri(nameNodeUri, HdfsConstants.HDFS_URI_SCHEME);	} else {	dtService = SecurityUtil.buildTokenService( DFSUtilClient.getNNAddress(nameNodeUri));	}	return new ProxyAndInfo<>(proxy, dtService, DFSUtilClient.getNNAddress(nameNodeUri));	} else {	
currently creating proxy using lossyretryinvocationhandler requires nn ha setup 

========================= hadoop sample_6911 =========================

if (Shell.WINDOWS) {	return false;	}	ShellCommandExecutor shexec;	boolean supported = true;	try {	String[] args = {"bash", "-c", "echo 1000"};	shexec = new ShellCommandExecutor(args);	shexec.execute();	} catch (InterruptedIOException iioe) {	
interrupted unable to determine if bash is supported 

}	ShellCommandExecutor shexec;	boolean supported = true;	try {	String[] args = {"bash", "-c", "echo 1000"};	shexec = new ShellCommandExecutor(args);	shexec.execute();	} catch (InterruptedIOException iioe) {	throw iioe;	} catch (IOException ioe) {	
bash is not supported by the os 

boolean supported = true;	try {	String[] args = {"bash", "-c", "echo 1000"};	shexec = new ShellCommandExecutor(args);	shexec.execute();	} catch (InterruptedIOException iioe) {	throw iioe;	} catch (IOException ioe) {	supported = false;	} catch (SecurityException se) {	
bash execution is not allowed by the jvm security manager considering it not supported 

if (Shell.WINDOWS) {	return false;	}	ShellCommandExecutor shexec = null;	boolean setsidSupported = true;	try {	String[] args = {"setsid", "bash", "-c", "echo $$"};	shexec = new ShellCommandExecutor(args);	shexec.execute();	} catch (IOException ioe) {	
setsid is not available on this machine so not using it 

}	ShellCommandExecutor shexec = null;	boolean setsidSupported = true;	try {	String[] args = {"setsid", "bash", "-c", "echo $$"};	shexec = new ShellCommandExecutor(args);	shexec.execute();	} catch (IOException ioe) {	setsidSupported = false;	} catch (SecurityException se) {	
setsid is not allowed to run by the jvm security manager so not using it 

try {	String[] args = {"setsid", "bash", "-c", "echo $$"};	shexec = new ShellCommandExecutor(args);	shexec.execute();	} catch (IOException ioe) {	setsidSupported = false;	} catch (SecurityException se) {	setsidSupported = false;	} catch (Error err) {	if (err.getMessage() != null && err.getMessage().contains("posix_spawn is not " + "a supported process launch mechanism") && (Shell.FREEBSD || Shell.MAC)) {	
avoiding jdk on bsd based systems 

public void run() {	try {	String line = errReader.readLine();	while((line != null) && !isInterrupted()) {	errMsg.append(line);	errMsg.append(System.getProperty("line.separator"));	line = errReader.readLine();	}	} catch(IOException ioe) {	if (!isTimedOut()) {	
error reading the error stream 

try {	String line = errReader.readLine();	while((line != null) && !isInterrupted()) {	errMsg.append(line);	errMsg.append(System.getProperty("line.separator"));	line = errReader.readLine();	}	} catch(IOException ioe) {	if (!isTimedOut()) {	} else {	
error reading the error stream due to shell command timeout 

if (!isTimedOut()) {	} else {	}	}	}	};	try {	errThread.start();	} catch (IllegalStateException ise) {	} catch (OutOfMemoryError oe) {	
caught one possible reason is that ulimit setting of max user processes is too low if so do ulimit u largernum and try again 

} finally {	if (timeOutTimer != null) {	timeOutTimer.cancel();	}	try {	InputStream stdout = process.getInputStream();	synchronized (stdout) {	inReader.close();	}	} catch (IOException ioe) {	
error while closing the input stream 

if (!completed.get()) {	errThread.interrupt();	joinThread(errThread);	}	try {	InputStream stderr = process.getErrorStream();	synchronized (stderr) {	errReader.close();	}	} catch (IOException ioe) {	
error while closing the error stream 

private static void joinThread(Thread t) {	while (t.isAlive()) {	try {	t.join();	} catch (InterruptedException ie) {	if (LOG.isWarnEnabled()) {	
interrupted while joining on 

========================= hadoop sample_3640 =========================

public void handle(Signal signal) {	
received signal sig 

========================= hadoop sample_3705 =========================

public synchronized void pushMetric(final MetricsRecord mr) {	intervalHeartBeat();	try {	mr.incrMetric(getName() + "_num_ops", getPreviousIntervalNumOps());	mr.setMetric(getName() + "_avg_time", getPreviousIntervalAverageTime());	} catch (Exception e) {	
pushmetric failed for 

========================= hadoop sample_3355 =========================

do {	if (lastEntry == null) {	batchedEntries = nnRpc.listOpenFiles(0);	} else {	batchedEntries = nnRpc.listOpenFiles(lastEntry.getId());	}	assertTrue("Incorrect open files list size!", batchedEntries.size() <= BATCH_SIZE);	for (int i = 0; i < batchedEntries.size(); i++) {	lastEntry = batchedEntries.get(i);	String filePath = lastEntry.getFilePath();	
openfile 

final AtomicBoolean listOpenFilesError = new AtomicBoolean(false);	final int listingIntervalMsec = 250;	Thread clientThread = new Thread(new Runnable() {	public void run() {	while(!failoverCompleted.get()) {	try {	assertEquals(0, ToolRunner.run(dfsAdmin, new String[] {"-listOpenFiles"}));	Thread.sleep(listingIntervalMsec);	} catch (Exception e) {	listOpenFilesError.set(true);	
error listing open files 

Thread.sleep(listingIntervalMsec);	} catch (Exception e) {	listOpenFilesError.set(true);	break;	}	}	}	});	clientThread.start();	Thread.sleep(listingIntervalMsec * 2);	
shutting down active 

} catch (Exception e) {	listOpenFilesError.set(true);	break;	}	}	}	});	clientThread.start();	Thread.sleep(listingIntervalMsec * 2);	haCluster.shutdownNameNode(0);	
transitioning to active 

========================= hadoop sample_7316 =========================

private static <T> T newProxyInstance(final YarnConfiguration conf, final Class<T> protocol, RMProxy<T> instance, RetryPolicy retryPolicy) throws IOException{	if (HAUtil.isHAEnabled(conf) || HAUtil.isFederationEnabled(conf)) {	RMFailoverProxyProvider<T> provider = instance.createRMFailoverProxyProvider(conf, protocol);	return (T) RetryProxy.create(protocol, provider, retryPolicy);	} else {	InetSocketAddress rmAddress = instance.getRMAddress(conf, protocol);	
connecting to resourcemanager at 

protected static RetryPolicy createRetryPolicy(Configuration conf, long retryTime, long retryInterval, boolean isHAEnabled) {	long rmConnectWaitMS = retryTime;	long rmConnectionRetryIntervalMS = retryInterval;	boolean waitForEver = (rmConnectWaitMS == -1);	if (!waitForEver) {	if (rmConnectWaitMS < 0) {	throw new YarnRuntimeException("Invalid Configuration. " + YarnConfiguration.RESOURCEMANAGER_CONNECT_MAX_WAIT_MS + " can be -1, but can not be other negative numbers");	}	if (rmConnectWaitMS < rmConnectionRetryIntervalMS) {	
is smaller than only try connect once 

========================= hadoop sample_2544 =========================

log("Normal NameNode upgrade", numDirs);	UpgradeUtilities.createNameNodeStorageDirs(nameNodeDirs, "current");	cluster = createCluster();	try {	final DistributedFileSystem dfs = cluster.getFileSystem();	dfs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);	dfs.rollingUpgrade(RollingUpgradeAction.PREPARE);	fail();	} catch(RemoteException re) {	assertEquals(InconsistentFSStateException.class.getName(), re.getClassName());	
the exception is expected 

log("Normal NameNode upgrade", numDirs);	UpgradeUtilities.createNameNodeStorageDirs(nameNodeDirs, "current");	cluster = createCluster();	try {	final DistributedFileSystem dfs = cluster.getFileSystem();	dfs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);	dfs.rollingUpgrade(RollingUpgradeAction.PREPARE);	fail();	} catch(RemoteException re) {	assertEquals(InconsistentFSStateException.class.getName(), re.getClassName());	
the exception is expected 

========================= hadoop sample_7592 =========================

public List<InputSplit> split(Configuration conf, ResultSet results, String colName) throws SQLException {	
generating splits for a floating point index column due to the 

public List<InputSplit> split(Configuration conf, ResultSet results, String colName) throws SQLException {	
imprecise representation of floating point values in java this 

public List<InputSplit> split(Configuration conf, ResultSet results, String colName) throws SQLException {	
may result in an incomplete import 

public List<InputSplit> split(Configuration conf, ResultSet results, String colName) throws SQLException {	
you are strongly encouraged to choose an integral split column 

========================= hadoop sample_5055 =========================

protected boolean mkOneDirWithMode(Path path, File p2f, FsPermission permission) throws IOException {	if (LOG.isDebugEnabled()) {	
efs mkonedirwithmode s s 

if (LOG.isDebugEnabled()) {	}	boolean ret = false;	try {	Native.Elevated.mkdir(path);	setPermission(path, permission);	ret = true;	}	catch(Throwable e) {	if (LOG.isDebugEnabled()) {	
efs mkonedirwithmode s 

public void setPermission(Path p, FsPermission permission) throws IOException {	if (LOG.isDebugEnabled()) {	
efs setpermission s s 

public void setOwner(Path p, String username, String groupname) throws IOException {	if (LOG.isDebugEnabled()) {	
efs setowner s s s 

protected OutputStream createOutputStreamWithMode(Path f, boolean append, FsPermission permission) throws IOException {	if (LOG.isDebugEnabled()) {	
efs createoutputstreamwithmode s b s 

public boolean delete(Path p, boolean recursive) throws IOException {	if (LOG.isDebugEnabled()) {	
efs delete s b 

private Thread startStreamReader(final InputStream stream) throws IOException {	Thread streamReaderThread = new Thread() {	public void run() {	try (BufferedReader lines = new BufferedReader( new InputStreamReader(stream, Charset.forName("UTF-8")))) {	char[] buf = new char[512];	int nRead;	while ((nRead = lines.read(buf, 0, buf.length)) > 0) {	output.append(buf, 0, nRead);	}	} catch (Throwable t) {	
error occurred reading the process stdout 

protected String[] getRunCommand(String command, String groupId, String userName, Path pidFile, Configuration conf) {	File f = new File(command);	if (LOG.isDebugEnabled()) {	
getruncommand s exists b 

protected void copyFile(Path src, Path dst, String owner) throws IOException {	if (LOG.isDebugEnabled()) {	
copyfile s s owner s 

protected void createDir(Path dirPath, FsPermission perms, boolean createParent, String owner) throws IOException {	perms = new FsPermission(DIR_PERM);	if (LOG.isDebugEnabled()) {	
createdir s perm s owner s 

protected void setScriptExecutable(Path script, String owner) throws IOException {	if (LOG.isDebugEnabled()) {	
setscriptexecutable s owner s 

public Path localizeClasspathJar(Path jarPath, Path target, String owner) throws IOException {	if (LOG.isDebugEnabled()) {	
localizeclasspathjar s s o s 

createUserLocalDirs(localDirs, user);	createUserCacheDirs(localDirs, user);	createAppDirs(localDirs, user, appId);	createAppLogDirs(appId, logDirs, user);	Path appStorageDir = getWorkingDir(localDirs, user, appId);	String tokenFn = String.format( ContainerLocalizer.TOKEN_FILE_NAME_FMT, locId);	Path tokenDst = new Path(appStorageDir, tokenFn);	copyFile(nmPrivateContainerTokensPath, tokenDst, user);	File cwdApp = new File(appStorageDir.toString());	if (LOG.isDebugEnabled()) {	
cwdapp s 

WintuilsProcessStubExecutor stubExecutor = new WintuilsProcessStubExecutor( cwdApp.getAbsolutePath(), localizerPid, user, "nul:", cmdLine);	try {	stubExecutor.execute();	stubExecutor.validateResult();	} finally {	stubExecutor.close();	try {	killContainer(localizerPid, Signal.KILL);	}	catch(Throwable e) {	
an exception occurred during the cleanup of localizer job s n s 

========================= hadoop sample_1915 =========================

c.set(DFSConfigKeys.DFS_NAMENODE_NAME_DIR_KEY, dirs);	c.set(DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_KEY, "${" + DFSConfigKeys.DFS_NAMENODE_NAME_DIR_KEY + "}");	c.set(DFSConfigKeys.DFS_NAMENODE_BACKUP_ADDRESS_KEY, "127.0.0.1:0");	c.set(DFSConfigKeys.DFS_NAMENODE_BACKUP_HTTP_ADDRESS_KEY, "127.0.0.1:0");	NameNode nn;	try {	Configuration nnconf = new HdfsConfiguration(c);	DFSTestUtil.formatNameNode(nnconf);	nn = NameNode.createNameNode(new String[] {}, nnconf);	} catch (IOException e) {	
ioexception is thrown creating name node 

throw e;	}	c.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION, "kerberos");	c.set(DFSConfigKeys.DFS_NAMENODE_KEYTAB_FILE_KEY, "");	BackupNode bn = null;	try {	bn = (BackupNode)NameNode.createNameNode( new String[] {startupOpt.getName()}, c);	assertTrue("Namesystem in BackupNode should be null", bn.getNamesystem() == null);	fail("Incorrect authentication setting should throw IOException");	} catch (IOException e) {	
ioexception thrown 

backup.stop();	backup = null;	EditLogFile editsLog = FSImageTestUtil.findLatestEditsLog(sd);	assertEquals(editsLog.getFirstTxId(), nn.getFSImage().getEditLog().getCurSegmentTxId());	assertTrue("Should not have finalized " + editsLog, editsLog.isInProgress());	assertTrue(fileSys.mkdirs(new Path("/edit-while-bn-down")));	backup = startBackupNode(conf, StartupOption.BACKUP, 1);	testBNInSync(cluster, backup, 4);	assertNotNull(backup.getNamesystem().getFileInfo("/edit-while-bn-down", false));	} finally {	
shutting down 

private void testBNInSync(MiniDFSCluster cluster, final BackupNode backup, int testIdx) throws Exception {	final NameNode nn = cluster.getNameNode();	final FileSystem fs = cluster.getFileSystem();	for (int i = 0; i < 10; i++) {	final String src = "/test_" + testIdx + "_" + i;	
creating on nn 

private void testBNInSync(MiniDFSCluster cluster, final BackupNode backup, int testIdx) throws Exception {	final NameNode nn = cluster.getNameNode();	final FileSystem fs = cluster.getFileSystem();	for (int i = 0; i < 10; i++) {	final String src = "/test_" + testIdx + "_" + i;	Path p = new Path(src);	assertTrue(fs.mkdirs(p));	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	
checking for on bn 

conf.set(DFSConfigKeys.DFS_NAMESERVICE_ID, "bnCluster");	conf.set(DFSConfigKeys.DFS_HA_NAMENODES_KEY_PREFIX + ".bnCluster", "nnActive, nnBackup");	conf.set(rpcAddrKeyPreffix + ".nnActive", nnAddr);	conf.set(rpcAddrKeyPreffix + ".nnBackup", bnAddr);	cluster.startDataNodes(conf, 3, true, StartupOption.REGULAR, null);	DFSTestUtil.createFile( fileSys, file1, 8192, (short)3, 0);	FileSystem bnFS = FileSystem.get( new Path("hdfs: String nnData = DFSTestUtil.readFile(fileSys, file1);	String bnData = DFSTestUtil.readFile(bnFS, file1);	assertEquals("Data read from BackupNode and NameNode is not the same.", nnData, bnData);	} catch(IOException e) {	
error in testbackupnode 

========================= hadoop sample_7438 =========================

}	IOException ex = null;	int numFailovers = 0;	for (int i = 0;; i++, numFailovers++) {	KMSClientProvider provider = providers[(currPos + i) % providers.length];	try {	return op.call(provider);	} catch (AccessControlException ace) {	throw ace;	} catch (IOException ioe) {	
kms provider at threw an ioexception 

public void warmUpEncryptedKeys(String... keyNames) throws IOException {	Preconditions.checkArgument(providers.length > 0, "No providers are configured");	boolean success = false;	IOException e = null;	for (KMSClientProvider provider : providers) {	try {	provider.warmUpEncryptedKeys(keyNames);	success = true;	} catch (IOException ioe) {	e = ioe;	
error warming up keys for provider with url 

public void close() throws IOException {	for (KMSClientProvider provider : providers) {	try {	provider.close();	} catch (IOException ioe) {	
error closing provider with url 

public void flush() throws IOException {	for (KMSClientProvider provider : providers) {	try {	provider.flush();	} catch (IOException ioe) {	
error flushing provider with url 

========================= hadoop sample_3494 =========================

RMAppAttempt attempt = app.getCurrentAppAttempt();	MockAM am = rm.sendAMLaunched(attempt.getAppAttemptId());	am.registerAppAttempt();	int request = 2;	am.allocate("127.0.0.1" , 1000, request, new ArrayList<ContainerId>());	nm1.nodeHeartbeat(true);	List<Container> conts = am.allocate(new ArrayList<ResourceRequest>(), new ArrayList<ContainerId>()).getAllocatedContainers();	int contReceived = conts.size();	int waitCount = 0;	while (contReceived < request && waitCount++ < 200) {	
got containers waiting to get 

am.unregisterAppAttempt();	NodeHeartbeatResponse resp = nm1.nodeHeartbeat(attempt.getAppAttemptId(), 1, ContainerState.COMPLETE);	rm.waitForState(am.getApplicationAttemptId(), RMAppAttemptState.FINISHED);	resp = nm1.nodeHeartbeat(true);	List<ContainerId> containersToCleanup = resp.getContainersToCleanup();	List<ApplicationId> appsToCleanup = resp.getApplicationsToCleanup();	int numCleanedContainers = containersToCleanup.size();	int numCleanedApps = appsToCleanup.size();	waitCount = 0;	while ((numCleanedContainers < 2 || numCleanedApps < 1) && waitCount++ < 200) {	
waiting to get cleanup events cleanedconts cleanedapps 

MockAM am = rm.sendAMLaunched(attempt.getAppAttemptId());	am.registerAppAttempt();	int request = 2;	am.allocate("127.0.0.1" , 1000, request, new ArrayList<ContainerId>());	rm.drainEvents();	nm1.nodeHeartbeat(true);	List<Container> conts = am.allocate(new ArrayList<ResourceRequest>(), new ArrayList<ContainerId>()).getAllocatedContainers();	int contReceived = conts.size();	int waitCount = 0;	while (contReceived < request && waitCount++ < 200) {	
got containers waiting to get 

ArrayList<ContainerId> release = new ArrayList<ContainerId>();	release.add(conts.get(0).getId());	am.allocate(new ArrayList<ResourceRequest>(), release);	rm.drainEvents();	Map<ApplicationId, List<ContainerStatus>> containerStatuses = new HashMap<ApplicationId, List<ContainerStatus>>();	ArrayList<ContainerStatus> containerStatusList = new ArrayList<ContainerStatus>();	containerStatusList.add(BuilderUtils.newContainerStatus(conts.get(0) .getId(), ContainerState.RUNNING, "nothing", 0, conts.get(0).getResource()));	containerStatuses.put(app.getApplicationId(), containerStatusList);	NodeHeartbeatResponse resp = nm1.nodeHeartbeat(containerStatuses, true);	waitForContainerCleanup(rm, nm1, resp);	
testing container launch much after release and nm getting cleanup 

rm.drainEvents();	contsToClean = resp.getContainersToCleanup();	cleanedConts += contsToClean.size();	if (cleanedConts >= 1) {	break;	}	Thread.sleep(100);	resp = nm.nodeHeartbeat(true);	} while(waitCount++ < 200);	if (contsToClean.isEmpty()) {	
failed to get any containers to cleanup 

contsToClean = resp.getContainersToCleanup();	cleanedConts += contsToClean.size();	if (cleanedConts >= 1) {	break;	}	Thread.sleep(100);	resp = nm.nodeHeartbeat(true);	} while(waitCount++ < 200);	if (contsToClean.isEmpty()) {	} else {	
got cleanup for 

========================= hadoop sample_464 =========================

private static CGroupsCpuResourceHandlerImpl getCGroupsCpuResourceHandler( Configuration conf) throws ResourceHandlerException {	boolean cgroupsCpuEnabled = conf.getBoolean(YarnConfiguration.NM_CPU_RESOURCE_ENABLED, YarnConfiguration.DEFAULT_NM_CPU_RESOURCE_ENABLED);	boolean cgroupsLCEResourcesHandlerEnabled = conf.getClass(YarnConfiguration.NM_LINUX_CONTAINER_RESOURCES_HANDLER, DefaultLCEResourcesHandler.class) .equals(CgroupsLCEResourcesHandler.class);	if (cgroupsCpuEnabled || cgroupsLCEResourcesHandlerEnabled) {	if (cGroupsCpuResourceHandler == null) {	synchronized (CpuResourceHandler.class) {	if (cGroupsCpuResourceHandler == null) {	
creating new cgroups cpu handler 

private static CGroupsBlkioResourceHandlerImpl getCgroupsBlkioResourceHandler( Configuration conf) throws ResourceHandlerException {	if (cGroupsBlkioResourceHandler == null) {	synchronized (DiskResourceHandler.class) {	if (cGroupsBlkioResourceHandler == null) {	
creating new cgroups blkio handler 

}	Map<String, Set<String>> pathSubsystemMappings = new HashMap<>();	Set<String> validCGroups = CGroupsHandler.CGroupController.getValidCGroups();	for (File candidate: list) {	Set<String> cgroupList = new HashSet<>(Arrays.asList(candidate.getName().split(",")));	cgroupList.retainAll(validCGroups);	if (!cgroupList.isEmpty()) {	if (candidate.isDirectory()) {	pathSubsystemMappings.put(candidate.getAbsolutePath(), cgroupList);	} else {	
the following cgroup is not a directory 

========================= hadoop sample_1850 =========================

final Path foo = new Path("/foo");	final Path bar = new Path("/bar");	final Path baz = new Path("/baz");	final RollingUpgradeInfo info1;	{	final DistributedFileSystem dfs = cluster.getFileSystem();	dfs.mkdirs(foo);	dfs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);	info1 = dfs.rollingUpgrade(RollingUpgradeAction.PREPARE);	dfs.setSafeMode(SafeModeAction.SAFEMODE_LEAVE);	
start 

cluster.shutdown();	}	final Configuration conf2 = setConf(new Configuration(), nn2Dir, mjc);	cluster2 = new MiniDFSCluster.Builder(conf2) .numDataNodes(0) .format(false) .manageNameDfsDirs(false) .build();	final DistributedFileSystem dfs2 = cluster2.getFileSystem();	Assert.assertTrue(dfs2.exists(foo));	Assert.assertTrue(dfs2.exists(bar));	Assert.assertFalse(dfs2.exists(baz));	assertEquals(info1, dfs2.rollingUpgrade(RollingUpgradeAction.QUERY));	dfs2.mkdirs(baz);	
restart cluster 

assertEquals(info1, dfs2.rollingUpgrade(RollingUpgradeAction.QUERY));	dfs2.mkdirs(baz);	cluster2.restartNameNode();	assertEquals(info1, dfs2.rollingUpgrade(RollingUpgradeAction.QUERY));	Assert.assertTrue(dfs2.exists(foo));	Assert.assertTrue(dfs2.exists(bar));	Assert.assertTrue(dfs2.exists(baz));	try {	cluster2.restartNameNode("-upgrade");	} catch(IOException e) {	
the exception is expected 

dfs2.mkdirs(baz);	cluster2.restartNameNode();	assertEquals(info1, dfs2.rollingUpgrade(RollingUpgradeAction.QUERY));	Assert.assertTrue(dfs2.exists(foo));	Assert.assertTrue(dfs2.exists(bar));	Assert.assertTrue(dfs2.exists(baz));	try {	cluster2.restartNameNode("-upgrade");	} catch(IOException e) {	}	
restart cluster again 

cluster2.restartNameNode("-upgrade");	} catch(IOException e) {	}	cluster2.restartNameNode();	assertEquals(info1, dfs2.rollingUpgrade(RollingUpgradeAction.QUERY));	Assert.assertTrue(dfs2.exists(foo));	Assert.assertTrue(dfs2.exists(bar));	Assert.assertTrue(dfs2.exists(baz));	final RollingUpgradeInfo finalize = dfs2.rollingUpgrade( RollingUpgradeAction.FINALIZE);	Assert.assertTrue(finalize.isFinalized());	
restart cluster with regular startup option 

========================= hadoop sample_7654 =========================

public CompletedJob(Configuration conf, JobId jobId, Path historyFile, boolean loadTasks, String userName, HistoryFileInfo info, JobACLsManager aclsMgr) throws IOException {	
loading job from file 

if ( getTotalReduces() == 0 ) {	report.setReduceProgress(1.0f);	} else {	report.setReduceProgress((float) getCompletedReduces() / getTotalReduces());	}	report.setJobFile(getConfFile().toString());	String historyUrl = "N/A";	try {	historyUrl = MRWebAppUtil.getApplicationWebURLOnJHSWithScheme(conf, jobId.getAppId());	} catch (UnknownHostException e) {	
problem determining local host 

TaskAttemptCompletionEvent tace = Records.newRecord(TaskAttemptCompletionEvent.class);	int attemptRunTime = -1;	if (taskAttempt.getLaunchTime() != 0 && taskAttempt.getFinishTime() != 0) {	attemptRunTime = (int) (taskAttempt.getFinishTime() - taskAttempt.getLaunchTime());	}	TaskAttemptCompletionEventStatus taceStatus = TaskAttemptCompletionEventStatus.KILLED;	String taStateString = taskAttempt.getState().toString();	try {	taceStatus = TaskAttemptCompletionEventStatus.valueOf(taStateString);	} catch (Exception e) {	
cannot constuct tacestatus from taskatemptstate for taskattemptid defaulting to killed 

protected synchronized void loadFullHistoryData(boolean loadTasks, Path historyFileAbsolute) throws IOException {	
loading history file 

}	IOException parseException = parser.getParseException();	if (parseException != null) {	throw new YarnRuntimeException( "Could not parse history file " + historyFileAbsolute, parseException);	}	} else {	throw new IOException("History file not found");	}	if (loadTasks) {	loadAllTasks();	
taskinfo loaded 

========================= hadoop sample_5393 =========================

private void sendRecvData(String testDescription, boolean eofExpected) throws IOException {	Socket sock = null;	try {	if ( testDescription != null ) {	
testing 

private void sendRecvData(String testDescription, boolean eofExpected) throws IOException {	Socket sock = null;	try {	if ( testDescription != null ) {	}	
going to write 

sock.setSoTimeout(HdfsConstants.READ_TIMEOUT);	OutputStream out = sock.getOutputStream();	byte[] retBuf = new byte[recvBuf.size()];	DataInputStream in = new DataInputStream(sock.getInputStream());	out.write(sendBuf.toByteArray());	out.flush();	try {	in.readFully(retBuf);	} catch (EOFException eof) {	if ( eofExpected ) {	
got eof as expected 

try {	in.readFully(retBuf);	} catch (EOFException eof) {	if ( eofExpected ) {	return;	}	throw eof;	}	String received = StringUtils.byteToHexString(retBuf);	String expected = StringUtils.byteToHexString(recvBuf.toByteArray());	
received 

try {	in.readFully(retBuf);	} catch (EOFException eof) {	if ( eofExpected ) {	return;	}	throw eof;	}	String received = StringUtils.byteToHexString(retBuf);	String expected = StringUtils.byteToHexString(recvBuf.toByteArray());	
expected 

========================= hadoop sample_7204 =========================

protected void onSuccess(Job job) {	
job success 

switch (type) {	case MAP: runInputBytes[i] = counters.findCounter("FileSystemCounters", "HDFS_BYTES_READ").getValue() - counters.findCounter(TaskCounter.SPLIT_RAW_BYTES).getValue();	runInputRecords[i] = (int) counters.findCounter( TaskCounter.MAP_INPUT_RECORDS).getValue();	runOutputBytes[i] = counters .findCounter(TaskCounter.MAP_OUTPUT_BYTES).getValue();	runOutputRecords[i] = (int) counters.findCounter( TaskCounter.MAP_OUTPUT_RECORDS).getValue();	specInfo = spec.getTaskInfo(TaskType.MAP, i);	specInputRecords[i] = specInfo.getInputRecords();	specInputBytes[i] = specInfo.getInputBytes();	specOutputRecords[i] = specInfo.getOutputRecords();	specOutputBytes[i] = specInfo.getOutputBytes();	
spec 

switch (type) {	case MAP: runInputBytes[i] = counters.findCounter("FileSystemCounters", "HDFS_BYTES_READ").getValue() - counters.findCounter(TaskCounter.SPLIT_RAW_BYTES).getValue();	runInputRecords[i] = (int) counters.findCounter( TaskCounter.MAP_INPUT_RECORDS).getValue();	runOutputBytes[i] = counters .findCounter(TaskCounter.MAP_OUTPUT_BYTES).getValue();	runOutputRecords[i] = (int) counters.findCounter( TaskCounter.MAP_OUTPUT_RECORDS).getValue();	specInfo = spec.getTaskInfo(TaskType.MAP, i);	specInputRecords[i] = specInfo.getInputRecords();	specInputBytes[i] = specInfo.getInputBytes();	specOutputRecords[i] = specInfo.getOutputRecords();	specOutputBytes[i] = specInfo.getOutputBytes();	
run 

break;	case REDUCE: runInputBytes[i] = 0;	runInputRecords[i] = (int) counters.findCounter( TaskCounter.REDUCE_INPUT_RECORDS).getValue();	runOutputBytes[i] = counters.findCounter("FileSystemCounters", "HDFS_BYTES_WRITTEN").getValue();	runOutputRecords[i] = (int) counters.findCounter( TaskCounter.REDUCE_OUTPUT_RECORDS).getValue();	specInfo = spec.getTaskInfo(TaskType.REDUCE, i);	specInputBytes[i] = 0;	specInputRecords[i] = specInfo.getInputRecords();	specOutputRecords[i] = specInfo.getOutputRecords();	specOutputBytes[i] = specInfo.getOutputBytes();	
spec 

break;	case REDUCE: runInputBytes[i] = 0;	runInputRecords[i] = (int) counters.findCounter( TaskCounter.REDUCE_INPUT_RECORDS).getValue();	runOutputBytes[i] = counters.findCounter("FileSystemCounters", "HDFS_BYTES_WRITTEN").getValue();	runOutputRecords[i] = (int) counters.findCounter( TaskCounter.REDUCE_OUTPUT_RECORDS).getValue();	specInfo = spec.getTaskInfo(TaskType.REDUCE, i);	specInputBytes[i] = 0;	specInputRecords[i] = specInfo.getInputRecords();	specOutputRecords[i] = specInfo.getOutputRecords();	specOutputBytes[i] = specInfo.getOutputBytes();	
run 

========================= hadoop sample_6100 =========================

int tryPort = port;	int tries = 0;	while (true) {	if (tries > 0 || tryPort == 0) {	tryPort = port + rand.nextInt(65535 - port);	}	if (tryPort == 0) {	continue;	}	try (ServerSocket s = new ServerSocket(tryPort)) {	
using port 

tryPort = port + rand.nextInt(65535 - port);	}	if (tryPort == 0) {	continue;	}	try (ServerSocket s = new ServerSocket(tryPort)) {	return tryPort;	} catch (IOException e) {	tries++;	if (tries >= retries) {	
port is already in use giving up 

if (tryPort == 0) {	continue;	}	try (ServerSocket s = new ServerSocket(tryPort)) {	return tryPort;	} catch (IOException e) {	tries++;	if (tries >= retries) {	throw e;	} else {	
port is already in use trying again 

========================= hadoop sample_2840 =========================

principalClass = "com.ibm.security.auth.AIXPrincipal";	} else {	principalClass = "com.ibm.security.auth.LinuxPrincipal";	}	}	} else {	principalClass = windows ? "com.sun.security.auth.NTUserPrincipal" : "com.sun.security.auth.UnixPrincipal";	}	return (Class<? extends Principal>) cl.loadClass(principalClass);	} catch (ClassNotFoundException e) {	
unable to find jaas classes 

AppConfigurationEntry ace = new AppConfigurationEntry( KerberosUtil.getKrb5LoginModuleName(), LoginModuleControlFlag.REQUIRED, krbOptions);	DynamicConfiguration dynConf = new DynamicConfiguration(new AppConfigurationEntry[]{ ace });	LoginContext login = newLoginContext( HadoopConfiguration.USER_KERBEROS_CONFIG_NAME, null, dynConf);	login.login();	Subject loginSubject = login.getSubject();	Set<Principal> loginPrincipals = loginSubject.getPrincipals();	if (loginPrincipals.isEmpty()) {	throw new RuntimeException("No login principals found!");	}	if (loginPrincipals.size() != 1) {	
found more than one principal in the ticket cache file 

KerberosTicket tgt = getTGT();	if (tgt == null) {	return;	}	long nextRefresh = getRefreshTime(tgt);	RetryPolicy rp = null;	while (true) {	try {	long now = Time.now();	if (LOG.isDebugEnabled()) {	
current time is 

KerberosTicket tgt = getTGT();	if (tgt == null) {	return;	}	long nextRefresh = getRefreshTime(tgt);	RetryPolicy rp = null;	while (true) {	try {	long now = Time.now();	if (LOG.isDebugEnabled()) {	
next refresh is 

while (true) {	try {	long now = Time.now();	if (LOG.isDebugEnabled()) {	}	if (now < nextRefresh) {	Thread.sleep(nextRefresh - now);	}	Shell.execCommand(cmd, "-R");	if (LOG.isDebugEnabled()) {	
renewed ticket 

}	if (now < nextRefresh) {	Thread.sleep(nextRefresh - now);	}	Shell.execCommand(cmd, "-R");	if (LOG.isDebugEnabled()) {	}	reloginFromTicketCache();	tgt = getTGT();	if (tgt == null) {	
no tgt after renewal aborting renew thread for 

}	reloginFromTicketCache();	tgt = getTGT();	if (tgt == null) {	return;	}	nextRefresh = Math.max(getRefreshTime(tgt), now + kerberosMinSecondsBeforeRelogin);	metrics.renewalFailures.set(0);	rp = null;	} catch (InterruptedException ie) {	
terminating renewal thread 

return;	}	nextRefresh = Math.max(getRefreshTime(tgt), now + kerberosMinSecondsBeforeRelogin);	metrics.renewalFailures.set(0);	rp = null;	} catch (InterruptedException ie) {	return;	} catch (IOException ie) {	metrics.renewalFailuresTotal.incr();	final long tgtEndTime = tgt.getEndTime().getTime();	
exception encountered while running the renewal command for tgt end time renewalfailures renewalfailurestotal 

} catch (IOException ie) {	metrics.renewalFailuresTotal.incr();	final long tgtEndTime = tgt.getEndTime().getTime();	final long now = Time.now();	if (rp == null) {	rp = RetryPolicies.exponentialBackoffRetry(Long.SIZE - 2, kerberosMinSecondsBeforeRelogin, TimeUnit.MILLISECONDS);	}	try {	nextRefresh = getNextTgtRenewalTime(tgtEndTime, now, rp);	} catch (Exception e) {	
exception when calculating next tgt renewal time 

if (rp == null) {	rp = RetryPolicies.exponentialBackoffRetry(Long.SIZE - 2, kerberosMinSecondsBeforeRelogin, TimeUnit.MILLISECONDS);	}	try {	nextRefresh = getNextTgtRenewalTime(tgtEndTime, now, rp);	} catch (Exception e) {	return;	}	metrics.renewalFailures.incr();	if (now > nextRefresh) {	
tgt is expired aborting renew thread for 

private void fixKerberosTicketOrder() {	Set<Object> creds = getSubject().getPrivateCredentials();	synchronized (creds) {	for (Iterator<Object> iter = creds.iterator(); iter.hasNext();) {	Object cred = iter.next();	if (cred instanceof KerberosTicket) {	KerberosTicket ticket = (KerberosTicket) cred;	if (!ticket.getServer().getName().startsWith("krbtgt")) {	
the first kerberos ticket is not tgt the server principal is remove and destroy it 

synchronized (creds) {	for (Iterator<Object> iter = creds.iterator(); iter.hasNext();) {	Object cred = iter.next();	if (cred instanceof KerberosTicket) {	KerberosTicket ticket = (KerberosTicket) cred;	if (!ticket.getServer().getName().startsWith("krbtgt")) {	iter.remove();	try {	ticket.destroy();	} catch (DestroyFailedException e) {	
destroy ticket failed 

try {	ticket.destroy();	} catch (DestroyFailedException e) {	}	} else {	return;	}	}	}	}	
warning no kerberos ticket found while attempting to renew ticket 

public List<String> getGroups() {	ensureInitialized();	try {	return groups.getGroups(getShortUserName());	} catch (IOException ie) {	if (LOG.isDebugEnabled()) {	
failed to get groups for user by 

public List<String> getGroups() {	ensureInitialized();	try {	return groups.getGroups(getShortUserName());	} catch (IOException ie) {	if (LOG.isDebugEnabled()) {	
TRACE 

public <T> T doAs(PrivilegedExceptionAction<T> action ) throws IOException, InterruptedException {	try {	logPrivilegedAction(subject, action);	return Subject.doAs(subject, action);	} catch (PrivilegedActionException pae) {	Throwable cause = pae.getCause();	if (LOG.isDebugEnabled()) {	
privilegedactionexception as cause 

========================= hadoop sample_3752 =========================

protected void processPath(PathData item) throws IOException {	short newperms = pp.applyNewPermission(item.stat);	if (item.stat.getPermission().toShort() != newperms) {	try {	item.fs.setPermission(item.path, new FsPermission(newperms));	} catch (IOException e) {	
error changing permissions of 

protected void processPath(PathData item) throws IOException {	String newOwner = (owner == null || owner.equals(item.stat.getOwner())) ? null : owner;	String newGroup = (group == null || group.equals(item.stat.getGroup())) ? null : group;	if (newOwner != null || newGroup != null) {	try {	item.fs.setOwner(item.path, newOwner, newGroup);	} catch (IOException e) {	
error changing ownership of 

========================= hadoop sample_4097 =========================

private void updateInfoCache(Iterable<MetricsRecordImpl> lastRecs) {	Preconditions.checkNotNull(lastRecs, "LastRecs should not be null");	
updating info cache 

private void updateInfoCache(Iterable<MetricsRecordImpl> lastRecs) {	Preconditions.checkNotNull(lastRecs, "LastRecs should not be null");	infoCache = infoBuilder.reset(lastRecs).get();	
Done 

private int updateAttrCache(Iterable<MetricsRecordImpl> lastRecs) {	Preconditions.checkNotNull(lastRecs, "LastRecs should not be null");	
updating attr cache 

========================= hadoop sample_3403 =========================

public synchronized ExportedBlockKeys exportKeys() {	if (!isMaster) return null;	
exporting access keys 

public synchronized void addKeys(ExportedBlockKeys exportedKeys) throws IOException {	if (isMaster || exportedKeys == null) return;	
setting block keys 

protected byte[] createPassword(BlockTokenIdentifier identifier) {	BlockKey key = null;	synchronized (this) {	key = currentKey;	}	if (key == null) throw new IllegalStateException("currentKey hasn't been initialized.");	identifier.setExpiryDate(timer.now() + tokenLifetime);	identifier.setKeyId(key.getKeyId());	if (LOG.isDebugEnabled()) {	
generating block token for 

========================= hadoop sample_7856 =========================

public void setStatus(Status status) throws ServerException {	Check.notNull(status, "status");	if (status.settable) {	if (status != this.status) {	Status oldStatus = this.status;	this.status = status;	for (Service service : services.values()) {	try {	service.serverStatusChange(oldStatus, status);	} catch (Exception ex) {	
service exception during status change to server shutting down 

Properties serverInfo = new Properties();	try {	InputStream is = getResource(name + ".properties");	serverInfo.load(is);	is.close();	} catch (IOException ex) {	throw new RuntimeException("Could not load server information file: " + name + ".properties");	}	initLog();	log.info("++++++++++++++++++++++++++++++++++++++++++++++++++++++");	
server starting 

Properties serverInfo = new Properties();	try {	InputStream is = getResource(name + ".properties");	serverInfo.load(is);	is.close();	} catch (IOException ex) {	throw new RuntimeException("Could not load server information file: " + name + ".properties");	}	initLog();	log.info("++++++++++++++++++++++++++++++++++++++++++++++++++++++");	
built information 

Properties serverInfo = new Properties();	try {	InputStream is = getResource(name + ".properties");	serverInfo.load(is);	is.close();	} catch (IOException ex) {	throw new RuntimeException("Could not load server information file: " + name + ".properties");	}	initLog();	log.info("++++++++++++++++++++++++++++++++++++++++++++++++++++++");	
version version undef 

Properties serverInfo = new Properties();	try {	InputStream is = getResource(name + ".properties");	serverInfo.load(is);	is.close();	} catch (IOException ex) {	throw new RuntimeException("Could not load server information file: " + name + ".properties");	}	initLog();	log.info("++++++++++++++++++++++++++++++++++++++++++++++++++++++");	
source repository source repository undef 

Properties serverInfo = new Properties();	try {	InputStream is = getResource(name + ".properties");	serverInfo.load(is);	is.close();	} catch (IOException ex) {	throw new RuntimeException("Could not load server information file: " + name + ".properties");	}	initLog();	log.info("++++++++++++++++++++++++++++++++++++++++++++++++++++++");	
source revision source revision undef 

Properties serverInfo = new Properties();	try {	InputStream is = getResource(name + ".properties");	serverInfo.load(is);	is.close();	} catch (IOException ex) {	throw new RuntimeException("Could not load server information file: " + name + ".properties");	}	initLog();	log.info("++++++++++++++++++++++++++++++++++++++++++++++++++++++");	
built by build username undef 

Properties serverInfo = new Properties();	try {	InputStream is = getResource(name + ".properties");	serverInfo.load(is);	is.close();	} catch (IOException ex) {	throw new RuntimeException("Could not load server information file: " + name + ".properties");	}	initLog();	log.info("++++++++++++++++++++++++++++++++++++++++++++++++++++++");	
built timestamp build timestamp undef 

Properties serverInfo = new Properties();	try {	InputStream is = getResource(name + ".properties");	serverInfo.load(is);	is.close();	} catch (IOException ex) {	throw new RuntimeException("Could not load server information file: " + name + ".properties");	}	initLog();	log.info("++++++++++++++++++++++++++++++++++++++++++++++++++++++");	
runtime information 

Properties serverInfo = new Properties();	try {	InputStream is = getResource(name + ".properties");	serverInfo.load(is);	is.close();	} catch (IOException ex) {	throw new RuntimeException("Could not load server information file: " + name + ".properties");	}	initLog();	log.info("++++++++++++++++++++++++++++++++++++++++++++++++++++++");	
home dir 

try {	InputStream is = getResource(name + ".properties");	serverInfo.load(is);	is.close();	} catch (IOException ex) {	throw new RuntimeException("Could not load server information file: " + name + ".properties");	}	initLog();	log.info("++++++++++++++++++++++++++++++++++++++++++++++++++++++");	log.info("    Config dir: {}", (config == null) ? configDir : "-");	
log dir 

try {	InputStream is = getResource(name + ".properties");	serverInfo.load(is);	is.close();	} catch (IOException ex) {	throw new RuntimeException("Could not load server information file: " + name + ".properties");	}	initLog();	log.info("++++++++++++++++++++++++++++++++++++++++++++++++++++++");	log.info("    Config dir: {}", (config == null) ? configDir : "-");	
temp dir 

InputStream is = getResource(name + ".properties");	serverInfo.load(is);	is.close();	} catch (IOException ex) {	throw new RuntimeException("Could not load server information file: " + name + ".properties");	}	initLog();	log.info("++++++++++++++++++++++++++++++++++++++++++++++++++++++");	log.info("    Config dir: {}", (config == null) ? configDir : "-");	initConfig();	
loading services 

is.close();	} catch (IOException ex) {	throw new RuntimeException("Could not load server information file: " + name + ".properties");	}	initLog();	log.info("++++++++++++++++++++++++++++++++++++++++++++++++++++++");	log.info("    Config dir: {}", (config == null) ? configDir : "-");	initConfig();	List<Service> list = loadServices();	try {	
initializing services 

} catch (IOException ex) {	throw new RuntimeException("Could not load server information file: " + name + ".properties");	}	initLog();	log.info("++++++++++++++++++++++++++++++++++++++++++++++++++++++");	log.info("    Config dir: {}", (config == null) ? configDir : "-");	initConfig();	List<Service> list = loadServices();	try {	initServices(list);	
services initialized 

throw new RuntimeException("Could not load server information file: " + name + ".properties");	}	initLog();	log.info("++++++++++++++++++++++++++++++++++++++++++++++++++++++");	log.info("    Config dir: {}", (config == null) ? configDir : "-");	initConfig();	List<Service> list = loadServices();	try {	initServices(list);	} catch (ServerException ex) {	
services initialization failure destroying initialized services 

initConfig();	List<Service> list = loadServices();	try {	initServices(list);	} catch (ServerException ex) {	destroyServices();	throw ex;	}	Status status = Status.valueOf(getConfig().get(getPrefixedName(CONF_STARTUP_STATUS), Status.NORMAL.toString()));	setStatus(status);	
server started status 

protected void initConfig() throws ServerException {	verifyDir(configDir);	File file = new File(configDir);	Configuration defaultConf;	String defaultConfig = name + "-default.xml";	ClassLoader classLoader = Thread.currentThread().getContextClassLoader();	InputStream inputStream = classLoader.getResourceAsStream(defaultConfig);	if (inputStream == null) {	
default configuration file not available in classpath 

defaultConf = new Configuration(false);	ConfigurationUtils.load(defaultConf, inputStream);	} catch (Exception ex) {	throw new ServerException(ServerException.ERROR.S03, defaultConfig, ex.getMessage(), ex);	}	}	if (config == null) {	Configuration siteConf;	File siteFile = new File(file, name + "-site.xml");	if (!siteFile.exists()) {	
site configuration file not found in config directory 

if (config == null) {	Configuration siteConf;	File siteFile = new File(file, name + "-site.xml");	if (!siteFile.exists()) {	siteConf = new Configuration(false);	} else {	if (!siteFile.isFile()) {	throw new ServerException(ServerException.ERROR.S05, siteFile.getAbsolutePath());	}	try {	
loading site configuration from 

config = new Configuration(false);	ConfigurationUtils.copy(siteConf, config);	}	ConfigurationUtils.injectDefaults(defaultConf, config);	ConfigRedactor redactor = new ConfigRedactor(config);	for (String name : System.getProperties().stringPropertyNames()) {	String value = System.getProperty(name);	if (name.startsWith(getPrefix() + ".")) {	config.set(name, value);	String redacted = redactor.redact(name, value);	
system property sets 

}	ConfigurationUtils.injectDefaults(defaultConf, config);	ConfigRedactor redactor = new ConfigRedactor(config);	for (String name : System.getProperties().stringPropertyNames()) {	String value = System.getProperty(name);	if (name.startsWith(getPrefix() + ".")) {	config.set(name, value);	String redacted = redactor.redact(name, value);	}	}	
loaded configuration 

private void loadServices(Class[] classes, List<Service> list) throws ServerException {	for (Class klass : classes) {	try {	Service service = (Service) klass.newInstance();	
loading service implementation 

protected List<Service> loadServices() throws ServerException {	try {	Map<Class, Service> map = new LinkedHashMap<Class, Service>();	Class[] classes = getConfig().getClasses(getPrefixedName(CONF_SERVICES));	Class[] classesExt = getConfig().getClasses(getPrefixedName(CONF_SERVICES_EXT));	List<Service> list = new ArrayList<Service>();	loadServices(classes, list);	loadServices(classesExt, list);	for (Service service : list) {	if (map.containsKey(service.getInterface())) {	
replacing service implementation 

protected void initServices(List<Service> services) throws ServerException {	for (Service service : services) {	
initializing service 

protected void destroyServices() {	List<Service> list = new ArrayList<Service>(services.values());	Collections.reverse(list);	for (Service service : list) {	try {	
destroying service 

protected void destroyServices() {	List<Service> list = new ArrayList<Service>(services.values());	Collections.reverse(list);	for (Service service : list) {	try {	service.destroy();	} catch (Throwable ex) {	
could not destroy service 

protected void destroyServices() {	List<Service> list = new ArrayList<Service>(services.values());	Collections.reverse(list);	for (Service service : list) {	try {	service.destroy();	} catch (Throwable ex) {	}	}	
services destroyed 

public void destroy() {	ensureOperational();	destroyServices();	
server shutdown 

if (getStatus() == Status.SHUTTING_DOWN) {	throw new IllegalStateException("Server shutting down");	}	try {	Service newService = klass.newInstance();	Service oldService = services.get(newService.getInterface());	if (oldService != null) {	try {	oldService.destroy();	} catch (Throwable ex) {	
could not destroy service 

Service oldService = services.get(newService.getInterface());	if (oldService != null) {	try {	oldService.destroy();	} catch (Throwable ex) {	}	}	newService.init(this);	services.put(newService.getInterface(), newService);	} catch (Exception ex) {	
could not set service programmatically server shutting down 

========================= hadoop sample_6744 =========================

private void createTestFileAndSetLength() throws IOException {	FileSystem fs = accountUsingInputStreamV1.getFileSystem();	if (fs.exists(TEST_FILE_PATH)) {	testFileStatus = fs.getFileStatus(TEST_FILE_PATH);	testFileLength = testFileStatus.getLen();	
reusing test file 

testFileLength = testFileStatus.getLen();	return;	}	int sizeOfAlphabet = ('z' - 'a' + 1);	byte[] buffer = new byte[26 * KILOBYTE];	char character = 'a';	for (int i = 0; i < buffer.length; i++) {	buffer[i] = (byte) character;	character = (character == 'z') ? 'a' : (char) ((int) character + 1);	}	
creating test file of size 

buffer[i] = (byte) character;	character = (character == 'z') ? 'a' : (char) ((int) character + 1);	}	ContractTestUtils.NanoTimer timer = new ContractTestUtils.NanoTimer();	try(FSDataOutputStream outputStream = fs.create(TEST_FILE_PATH)) {	int bytesWritten = 0;	while (bytesWritten < TEST_FILE_SIZE) {	outputStream.write(buffer);	bytesWritten += buffer.length;	}	
closing stream 

========================= hadoop sample_6314 =========================

job.addInfo("PRIORITY", jobInfo.getPriority());	job.addInfo("TOTAL_MAPS", jobInfo.getTotalMaps());	job.addInfo("TOTAL_REDUCES", jobInfo.getTotalReduces());	job.addInfo("UBERIZED", jobInfo.getUberized());	job.addInfo("ERROR_INFO", jobInfo.getErrorInfo());	Counters totalCounters = jobInfo.getTotalCounters();	if (totalCounters != null) {	addMetrics(job, totalCounters);	}	addConfiguration(job, conf);	
converted job to a timeline entity 

private List<TimelineEntity> createTaskAndTaskAttemptEntities( JobInfo jobInfo) {	List<TimelineEntity> entities = new ArrayList<>();	Map<TaskID, TaskInfo> taskInfoMap = jobInfo.getAllTasks();	
job has tasks 

task.setCreatedTime(taskInfo.getStartTime());	task.addInfo("START_TIME", taskInfo.getStartTime());	task.addInfo("FINISH_TIME", taskInfo.getFinishTime());	task.addInfo("TASK_TYPE", taskInfo.getTaskType());	task.addInfo("TASK_STATUS", taskInfo.getTaskStatus());	task.addInfo("ERROR_INFO", taskInfo.getError());	Counters counters = taskInfo.getCounters();	if (counters != null) {	addMetrics(task, counters);	}	
converted task to a timeline entity 

private Set<TimelineEntity> createTaskAttemptEntities(TaskInfo taskInfo) {	Set<TimelineEntity> taskAttempts = new HashSet<TimelineEntity>();	Map<TaskAttemptID, TaskAttemptInfo> taskAttemptInfoMap = taskInfo.getAllTaskAttempts();	
task has task attempts 

taskAttempt.addInfo("SHUFFLE_FINISH_TIME", taskAttemptInfo.getShuffleFinishTime());	taskAttempt.addInfo("SORT_FINISH_TIME", taskAttemptInfo.getSortFinishTime());	taskAttempt.addInfo("TASK_STATUS", taskAttemptInfo.getTaskStatus());	taskAttempt.addInfo("STATE", taskAttemptInfo.getState());	taskAttempt.addInfo("ERROR", taskAttemptInfo.getError());	taskAttempt.addInfo("CONTAINER_ID", taskAttemptInfo.getContainerId().toString());	Counters counters = taskAttemptInfo.getCounters();	if (counters != null) {	addMetrics(taskAttempt, counters);	}	
converted task attempt to a timeline entity 

========================= hadoop sample_5611 =========================

return null;	}	} else {	parent = iip.getParentINodesInPath();	}	final String userName = dirPerms.getUserName();	long id = fsd.allocateNewInodeId();	PermissionStatus perm = new PermissionStatus( userName, null, FsPermission.getDefault());	INodeSymlink newNode = unprotectedAddSymlink(fsd, parent, iip.getLastLocalName(), id, target, mtime, mtime, perm);	if (newNode == null) {	
addsymlink failed to add 

}	final String userName = dirPerms.getUserName();	long id = fsd.allocateNewInodeId();	PermissionStatus perm = new PermissionStatus( userName, null, FsPermission.getDefault());	INodeSymlink newNode = unprotectedAddSymlink(fsd, parent, iip.getLastLocalName(), id, target, mtime, mtime, perm);	if (newNode == null) {	return null;	}	fsd.getEditLog().logSymlink(path, target, mtime, mtime, newNode, logRetryCache);	if(NameNode.stateChangeLog.isDebugEnabled()) {	
addsymlink is added 

========================= hadoop sample_8023 =========================

private boolean markMovedIfGoodBlock(DBlock block, StorageType targetStorageType) {	synchronized (block) {	synchronized (movedBlocks) {	if (isGoodBlockCandidate(source, target, targetStorageType, block)) {	this.block = block;	if (chooseProxySource()) {	movedBlocks.put(block);	if (LOG.isDebugEnabled()) {	
decided to move 

private void dispatch() {	
start moving 

Token<BlockTokenIdentifier> accessToken = km.getAccessToken(eb);	IOStreamPair saslStreams = saslClient.socketSend(sock, unbufOut, unbufIn, km, accessToken, target.getDatanodeInfo());	unbufOut = saslStreams.out;	unbufIn = saslStreams.in;	out = new DataOutputStream(new BufferedOutputStream(unbufOut, ioFileBufferSize));	in = new DataInputStream(new BufferedInputStream(unbufIn, ioFileBufferSize));	sendRequest(out, eb, accessToken);	receiveResponse(in);	nnc.getBytesMoved().addAndGet(block.getNumBytes());	target.getDDatanode().setHasSuccess();	
successfully moved 

IOStreamPair saslStreams = saslClient.socketSend(sock, unbufOut, unbufIn, km, accessToken, target.getDatanodeInfo());	unbufOut = saslStreams.out;	unbufIn = saslStreams.in;	out = new DataOutputStream(new BufferedOutputStream(unbufOut, ioFileBufferSize));	in = new DataInputStream(new BufferedInputStream(unbufIn, ioFileBufferSize));	sendRequest(out, eb, accessToken);	receiveResponse(in);	nnc.getBytesMoved().addAndGet(block.getNumBytes());	target.getDDatanode().setHasSuccess();	} catch (IOException e) {	
failed to move 

private long getBlockList() throws IOException {	final long size = Math.min(getBlocksSize, blocksToReceive);	final BlocksWithLocations newBlocks = nnc.getBlocks(getDatanodeInfo(), size);	if (LOG.isTraceEnabled()) {	
getblocks B returns blocks 

final StorageType[] storageTypes = blk.getStorageTypes();	for (int i = 0; i < datanodeUuids.length; i++) {	final StorageGroup g = storageGroupMap.get( datanodeUuids[i], storageTypes[i]);	if (g != null) {	block.addLocation(g);	}	}	}	if (!srcBlocks.contains(block) && isGoodBlockCandidate(block)) {	if (LOG.isTraceEnabled()) {	
add to 

if (p != null) {	previousMoveTimestamp = Time.monotonicNow();	executePendingMove(p);	continue;	}	removeMovedBlocks();	if (shouldFetchMoreBlocks()) {	try {	if(delay > 0) {	if (LOG.isDebugEnabled()) {	
sleeping msec 

Thread.sleep(delay);	}	final long received = getBlockList();	if (received == 0) {	return;	}	blocksToReceive -= received;	continue;	} catch (InterruptedException ignored) {	} catch (IOException e) {	
exception while getting block list 

continue;	} catch (InterruptedException ignored) {	} catch (IOException e) {	return;	} finally {	delay = 0L;	}	} else {	long noMoveInterval = Time.monotonicNow() - previousMoveTimestamp;	if (noMoveInterval > maxNoMoveInterval) {	
failed to find a pending move for ms skipping 

}	try {	synchronized (Dispatcher.this) {	Dispatcher.this.wait(1000);	}	Thread.sleep(100);	} catch (InterruptedException ignored) {	}	}	if (isIterationOver()) {	
the maximum iteration time seconds has been reached stopping 

public void executePendingMove(final PendingMove p) {	final DDatanode targetDn = p.target.getDDatanode();	ExecutorService moveExecutor = targetDn.getMoveExecutor();	if (moveExecutor == null) {	final int nThreads = moverThreadAllocator.allocate();	if (nThreads > 0) {	moveExecutor = targetDn.initMoveExecutor(nThreads);	}	}	if (moveExecutor == null) {	
no mover threads available skip moving 

LOG.debug("Balancer concurrent threads = " + concurrentThreads);	LOG.debug("Disperse Interval sec = " + concurrentThreads / BALANCER_NUM_RPC_PER_SEC);	}	int threadsPerTarget = maxMoverThreads/targets.size();	if (threadsPerTarget == 0) {	moverThreadAllocator.setLotSize(1);	LOG.warn(DFSConfigKeys.DFS_BALANCER_MOVERTHREADS_KEY + "=" + maxMoverThreads + " is too small for moving blocks to " + targets.size() + " targets. Balancing may be slower.");	} else {	if  (threadsPerTarget > maxConcurrentMovesPerNode) {	threadsPerTarget = maxConcurrentMovesPerNode;	
limiting threads per target to the specified max 

}	int threadsPerTarget = maxMoverThreads/targets.size();	if (threadsPerTarget == 0) {	moverThreadAllocator.setLotSize(1);	LOG.warn(DFSConfigKeys.DFS_BALANCER_MOVERTHREADS_KEY + "=" + maxMoverThreads + " is too small for moving blocks to " + targets.size() + " targets. Balancing may be slower.");	} else {	if  (threadsPerTarget > maxConcurrentMovesPerNode) {	threadsPerTarget = maxConcurrentMovesPerNode;	}	moverThreadAllocator.setLotSize(threadsPerTarget);	
allocating threads per target 

if(j >= concurrentThreads) {	dSec = 0;	} else if((j + 1) % BALANCER_NUM_RPC_PER_SEC == 0) {	dSec++;	}	}	for (Future<?> future : futures) {	try {	future.get();	} catch (ExecutionException e) {	
dispatcher thread failed 

========================= hadoop sample_8189 =========================

int result = cli.run(new String[] { "container", "-list", attemptId.toString() });	assertEquals(0, result);	verify(client).getContainers(attemptId);	ByteArrayOutputStream baos = new ByteArrayOutputStream();	OutputStreamWriter stream = new OutputStreamWriter(baos, "UTF-8");	PrintWriter pw = new PrintWriter(stream);	pw.println("Total number of containers :3");	pw.printf(ApplicationCLI.CONTAINER_PATTERN, "Container-Id", "Start Time", "Finish Time", "State", "Host", "Node Http Address", "LOG-URL");	pw.printf(ApplicationCLI.CONTAINER_PATTERN, "container_1234_0005_01_000001", Times.format(time1), Times.format(time2), "COMPLETE", "host:1234", "http: pw.printf(ApplicationCLI.CONTAINER_PATTERN, "container_1234_0005_01_000002", Times.format(time1), Times.format(time2), "COMPLETE", "host:1234", "http: pw.printf(ApplicationCLI.CONTAINER_PATTERN, "container_1234_0005_01_000003", Times.format(time1), "N/A", "RUNNING", "host:1234", "http: pw.close();	String appReportStr = baos.toString("UTF-8");	
ExpectedOutput 

assertEquals(0, result);	verify(client).getContainers(attemptId);	ByteArrayOutputStream baos = new ByteArrayOutputStream();	OutputStreamWriter stream = new OutputStreamWriter(baos, "UTF-8");	PrintWriter pw = new PrintWriter(stream);	pw.println("Total number of containers :3");	pw.printf(ApplicationCLI.CONTAINER_PATTERN, "Container-Id", "Start Time", "Finish Time", "State", "Host", "Node Http Address", "LOG-URL");	pw.printf(ApplicationCLI.CONTAINER_PATTERN, "container_1234_0005_01_000001", Times.format(time1), Times.format(time2), "COMPLETE", "host:1234", "http: pw.printf(ApplicationCLI.CONTAINER_PATTERN, "container_1234_0005_01_000002", Times.format(time1), Times.format(time2), "COMPLETE", "host:1234", "http: pw.printf(ApplicationCLI.CONTAINER_PATTERN, "container_1234_0005_01_000003", Times.format(time1), "N/A", "RUNNING", "host:1234", "http: pw.close();	String appReportStr = baos.toString("UTF-8");	Log.info("["+appReportStr+"]");	
outputfrom command 

========================= hadoop sample_2584 =========================

public static void cleanup(String action, FileSystem fileSystem, String cleanupPath) {	noteAction(action);	try {	if (fileSystem != null) {	fileSystem.delete(new Path(cleanupPath).makeQualified(fileSystem), true);	}	} catch (Exception e) {	
error deleting in 

public static void downgrade(String message, Throwable failure) {	
downgrading test 

========================= hadoop sample_6197 =========================

public void tearDown() throws IOException {	if (!tearDownDone && dn != null) {	try {	dn.shutdown();	} catch(Exception e) {	
cannot close 

private void waitForBlockReport( final DatanodeProtocolClientSideTranslatorPB mockNN) throws Exception {	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	try {	Mockito.verify(mockNN).blockReport( Mockito.eq(datanodeRegistration), Mockito.eq(POOL_ID), Mockito.<StorageBlockReport[]>anyObject(), Mockito.<BlockReportContext>anyObject());	return true;	} catch (Throwable t) {	
waiting on block report 

public void testDatanodeRegistrationRetry() throws Exception {	final DatanodeProtocolClientSideTranslatorPB namenode = mock(DatanodeProtocolClientSideTranslatorPB.class);	Mockito.doAnswer(new Answer<DatanodeRegistration>() {	int i = 0;	public DatanodeRegistration answer(InvocationOnMock invocation) throws Throwable {	i++;	if ( i > 1 && i < 5) {	
mockito exception 

final DatanodeProtocolClientSideTranslatorPB namenode = mock(DatanodeProtocolClientSideTranslatorPB.class);	Mockito.doAnswer(new Answer<DatanodeRegistration>() {	int i = 0;	public DatanodeRegistration answer(InvocationOnMock invocation) throws Throwable {	i++;	if ( i > 1 && i < 5) {	throw new EOFException("TestDatanodeProtocolRetryPolicy");	} else {	DatanodeRegistration dr = (DatanodeRegistration) invocation.getArguments()[0];	datanodeRegistration = new DatanodeRegistration(dr.getDatanodeUuid(), dr);	
mockito succeeded 

}	}	}).when(namenode).registerDatanode( Mockito.any(DatanodeRegistration.class));	when(namenode.versionRequest()).thenReturn( new NamespaceInfo(1, CLUSTER_ID, POOL_ID, 1L));	Mockito.doAnswer(new Answer<HeartbeatResponse>() {	int i = 0;	public HeartbeatResponse answer(InvocationOnMock invocation) throws Throwable {	i++;	HeartbeatResponse heartbeatResponse;	if ( i == 1 ) {	
mockito heartbeatresponse registration 

}).when(namenode).registerDatanode( Mockito.any(DatanodeRegistration.class));	when(namenode.versionRequest()).thenReturn( new NamespaceInfo(1, CLUSTER_ID, POOL_ID, 1L));	Mockito.doAnswer(new Answer<HeartbeatResponse>() {	int i = 0;	public HeartbeatResponse answer(InvocationOnMock invocation) throws Throwable {	i++;	HeartbeatResponse heartbeatResponse;	if ( i == 1 ) {	heartbeatResponse = new HeartbeatResponse( new DatanodeCommand[]{RegisterCommand.REGISTER}, new NNHAStatusHeartbeat(HAServiceState.ACTIVE, 1), null, ThreadLocalRandom.current().nextLong() | 1L);	} else {	
mockito heartbeatresponse 

========================= hadoop sample_7293 =========================

public void tree() {	if( !Shell.WINDOWS) {	
platform not windows not testing 

========================= hadoop sample_2091 =========================

public GetApplicationReportResponse getApplicationReport( GetApplicationReportRequest request) throws YarnException {	ApplicationId applicationId = request.getApplicationId();	if (applicationId == null) {	throw new ApplicationNotFoundException("Invalid application id: null");	}	UserGroupInformation callerUGI;	try {	callerUGI = UserGroupInformation.getCurrentUser();	} catch (IOException ie) {	
error getting ugi 

public SubmitApplicationResponse submitApplication( SubmitApplicationRequest request) throws YarnException, IOException {	ApplicationSubmissionContext submissionContext = request .getApplicationSubmissionContext();	ApplicationId applicationId = submissionContext.getApplicationId();	CallerContext callerContext = CallerContext.getCurrent();	String user = null;	try {	user = UserGroupInformation.getCurrentUser().getShortUserName();	} catch (IOException ie) {	
unable to get the current user 

if (YarnConfiguration.timelineServiceV2Enabled(getConfig())) {	String value = null;	try {	for (String tag : submissionContext.getApplicationTags()) {	if (tag.startsWith(TimelineUtils.FLOW_RUN_ID_TAG_PREFIX + ":") || tag.startsWith( TimelineUtils.FLOW_RUN_ID_TAG_PREFIX.toLowerCase() + ":")) {	value = tag.substring(TimelineUtils.FLOW_RUN_ID_TAG_PREFIX.length() + 1);	Long.valueOf(value);	}	}	} catch (NumberFormatException e) {	
invalid to flow run flow run should be a long integer 

value = tag.substring(TimelineUtils.FLOW_RUN_ID_TAG_PREFIX.length() + 1);	Long.valueOf(value);	}	}	} catch (NumberFormatException e) {	RMAuditLogger.logFailure(user, AuditConstants.SUBMIT_APP_REQUEST, e.getMessage(), "ClientRMService", "Exception in submitting application", applicationId);	throw RPCUtil.getRemoteException(e);	}	}	if (rmContext.getRMApps().get(applicationId) != null) {	
this is an earlier submitted application 

submissionContext .setApplicationType(YarnConfiguration.DEFAULT_APPLICATION_TYPE);	} else {	if (submissionContext.getApplicationType().length() > YarnConfiguration.APPLICATION_TYPE_LENGTH) {	submissionContext.setApplicationType(submissionContext .getApplicationType().substring(0, YarnConfiguration.APPLICATION_TYPE_LENGTH));	}	}	ReservationId reservationId = request.getApplicationSubmissionContext() .getReservationID();	checkReservationACLs(submissionContext.getQueue(), AuditConstants .SUBMIT_RESERVATION_REQUEST, reservationId);	try {	rmAppManager.submitApplication(submissionContext, System.currentTimeMillis(), user);	
application with id submitted by user 

if (submissionContext.getApplicationType().length() > YarnConfiguration.APPLICATION_TYPE_LENGTH) {	submissionContext.setApplicationType(submissionContext .getApplicationType().substring(0, YarnConfiguration.APPLICATION_TYPE_LENGTH));	}	}	ReservationId reservationId = request.getApplicationSubmissionContext() .getReservationID();	checkReservationACLs(submissionContext.getQueue(), AuditConstants .SUBMIT_RESERVATION_REQUEST, reservationId);	try {	rmAppManager.submitApplication(submissionContext, System.currentTimeMillis(), user);	RMAuditLogger.logSuccess(user, AuditConstants.SUBMIT_APP_REQUEST, "ClientRMService", applicationId, callerContext);	} catch (YarnException e) {	
exception in submitting 

public KillApplicationResponse forceKillApplication( KillApplicationRequest request) throws YarnException {	ApplicationId applicationId = request.getApplicationId();	CallerContext callerContext = CallerContext.getCurrent();	UserGroupInformation callerUGI;	try {	callerUGI = UserGroupInformation.getCurrentUser();	} catch (IOException ie) {	
error getting ugi 

public GetApplicationsResponse getApplications(GetApplicationsRequest request) throws YarnException {	UserGroupInformation callerUGI;	try {	callerUGI = UserGroupInformation.getCurrentUser();	} catch (IOException ie) {	
error getting ugi 

public GetQueueInfoResponse getQueueInfo(GetQueueInfoRequest request) throws YarnException {	UserGroupInformation callerUGI;	try {	callerUGI = UserGroupInformation.getCurrentUser();	} catch (IOException ie) {	
error getting ugi 

continue;	}	appReports.add( rmApp.createAndGetApplicationReport( callerUGI.getUserName(), true));	}	}	}	queueInfo.setApplications(appReports);	response.setQueueInfo(queueInfo);	RMAuditLogger.logSuccess(callerUGI.getUserName(), AuditConstants.GET_QUEUE_INFO_REQUEST, "ClientRMService", arguments);	} catch (IOException ioe) {	
failed to getqueueinfo for 

private void refreshScheduler(String planName, ReservationDefinition contract, String reservationId) {	if ((contract.getArrival() - clock.getTime()) < reservationSystem .getPlanFollowerTimeStep()) {	
reservation is within threshold so attempting to create synchronously 

private void refreshScheduler(String planName, ReservationDefinition contract, String reservationId) {	if ((contract.getArrival() - clock.getTime()) < reservationSystem .getPlanFollowerTimeStep()) {	reservationSystem.synchronizePlan(planName, true);	
created reservation synchronously 

public SignalContainerResponse signalToContainer( SignalContainerRequest request) throws YarnException, IOException {	ContainerId containerId = request.getContainerId();	UserGroupInformation callerUGI;	try {	callerUGI = UserGroupInformation.getCurrentUser();	} catch (IOException ie) {	
error getting ugi 

private UserGroupInformation getCallerUgi(ApplicationId applicationId, String operation) throws YarnException {	UserGroupInformation callerUGI;	try {	callerUGI = UserGroupInformation.getCurrentUser();	} catch (IOException ie) {	
error getting ugi 

========================= hadoop sample_670 =========================

request.setTrackingUrl(MRWebAppUtil .getAMWebappScheme(getConfig()) + serviceAddr.getHostName() + ":" + clientService.getHttpPort());	}	RegisterApplicationMasterResponse response = scheduler.registerApplicationMaster(request);	isApplicationMasterRegistered = true;	maxContainerCapability = response.getMaximumResourceCapability();	this.context.getClusterInfo().setMaxContainerCapability( maxContainerCapability);	if (UserGroupInformation.isSecurityEnabled()) {	setClientToAMToken(response.getClientToAMTokenMasterKey());	}	this.applicationACLs = response.getApplicationACLs();	
maxcontainercapability 

}	RegisterApplicationMasterResponse response = scheduler.registerApplicationMaster(request);	isApplicationMasterRegistered = true;	maxContainerCapability = response.getMaximumResourceCapability();	this.context.getClusterInfo().setMaxContainerCapability( maxContainerCapability);	if (UserGroupInformation.isSecurityEnabled()) {	setClientToAMToken(response.getClientToAMTokenMasterKey());	}	this.applicationACLs = response.getApplicationACLs();	String queue = response.getQueue();	
queue 

maxContainerCapability = response.getMaximumResourceCapability();	this.context.getClusterInfo().setMaxContainerCapability( maxContainerCapability);	if (UserGroupInformation.isSecurityEnabled()) {	setClientToAMToken(response.getClientToAMTokenMasterKey());	}	this.applicationACLs = response.getApplicationACLs();	String queue = response.getQueue();	job.setQueueName(queue);	this.schedulerResourceTypes.addAll(response.getSchedulerResourceTypes());	} catch (Exception are) {	
exception while registering 

protected void unregister() {	try {	doUnregistration();	} catch(Exception are) {	
exception while unregistering 

finishState = FinalApplicationStatus.SUCCEEDED;	} else if (jobImpl.getInternalState() == JobStateInternal.KILLED || (jobImpl.getInternalState() == JobStateInternal.RUNNING && isSignalled)) {	finishState = FinalApplicationStatus.KILLED;	} else if (jobImpl.getInternalState() == JobStateInternal.FAILED || jobImpl.getInternalState() == JobStateInternal.ERROR) {	finishState = FinalApplicationStatus.FAILED;	}	StringBuffer sb = new StringBuffer();	for (String s : job.getDiagnostics()) {	sb.append(s).append("\n");	}	
setting job diagnostics to 

} else if (jobImpl.getInternalState() == JobStateInternal.KILLED || (jobImpl.getInternalState() == JobStateInternal.RUNNING && isSignalled)) {	finishState = FinalApplicationStatus.KILLED;	} else if (jobImpl.getInternalState() == JobStateInternal.FAILED || jobImpl.getInternalState() == JobStateInternal.ERROR) {	finishState = FinalApplicationStatus.FAILED;	}	StringBuffer sb = new StringBuffer();	for (String s : job.getDiagnostics()) {	sb.append(s).append("\n");	}	String historyUrl = context.getHistoryUrl();	
history url is 

String historyUrl = context.getHistoryUrl();	FinishApplicationMasterRequest request = FinishApplicationMasterRequest.newInstance(finishState, sb.toString(), historyUrl);	try {	while (true) {	FinishApplicationMasterResponse response = scheduler.finishApplicationMaster(request);	if (response.getIsUnregistered()) {	RunningAppContext raContext = (RunningAppContext) context;	raContext.markSuccessfulUnregistration();	break;	}	
waiting for application to be successfully unregistered 

protected void serviceStop() throws Exception {	if (stopped.getAndSet(true)) {	return;	}	if (allocatorThread != null) {	allocatorThread.interrupt();	try {	allocatorThread.join();	} catch (InterruptedException ie) {	
interruptedexception while stopping 

public void run() {	while (!stopped.get() && !Thread.currentThread().isInterrupted()) {	try {	Thread.sleep(rmPollInterval);	try {	heartbeat();	} catch (RMContainerAllocationException e) {	
error communicating with rm 

public void run() {	while (!stopped.get() && !Thread.currentThread().isInterrupted()) {	try {	Thread.sleep(rmPollInterval);	try {	heartbeat();	} catch (RMContainerAllocationException e) {	return;	} catch (Exception e) {	
error in contacting rm 

heartbeat();	} catch (RMContainerAllocationException e) {	return;	} catch (Exception e) {	continue;	}	lastHeartbeatTime = context.getClock().getTime();	executeHeartbeatCallbacks();	} catch (InterruptedException e) {	if (!stopped.get()) {	
allocated thread interrupted returning 

public void setShouldUnregister(boolean shouldUnregister) {	this.shouldUnregister = shouldUnregister;	
rmcommunicator notified that shouldunregistered is 

public void setSignalled(boolean isSignalled) {	this.isSignalled = isSignalled;	
rmcommunicator notified that issignalled is 

========================= hadoop sample_5304 =========================

public Boolean call() throws Exception {	Path tempPath = null;	try {	if (!verifyAccess()) {	
user is not authorized to upload file 

try {	if (!verifyAccess()) {	return false;	}	Path actualPath = getActualPath();	String checksumVal = computeChecksum(actualPath);	Path directoryPath = new Path(SharedCacheUtil.getCacheEntryPath(nestedLevel, sharedCacheRootDir, checksumVal));	fs.mkdirs(directoryPath, DIRECTORY_PERMISSION);	tempPath = new Path(directoryPath, getTemporaryFileName(actualPath));	if (!uploadFile(actualPath, tempPath)) {	
could not copy the file to the shared cache at 

String checksumVal = computeChecksum(actualPath);	Path directoryPath = new Path(SharedCacheUtil.getCacheEntryPath(nestedLevel, sharedCacheRootDir, checksumVal));	fs.mkdirs(directoryPath, DIRECTORY_PERMISSION);	tempPath = new Path(directoryPath, getTemporaryFileName(actualPath));	if (!uploadFile(actualPath, tempPath)) {	return false;	}	fs.setPermission(tempPath, FILE_PERMISSION);	Path finalPath = new Path(directoryPath, actualPath.getName());	if (!fs.rename(tempPath, finalPath)) {	
the file already exists under ignoring this attempt 

if (!fs.rename(tempPath, finalPath)) {	deleteTempFile(tempPath);	return false;	}	if (!notifySharedCacheManager(checksumVal, actualPath.getName())) {	fs.delete(finalPath, false);	return false;	}	short replication = (short)conf.getInt(YarnConfiguration.SHARED_CACHE_NM_UPLOADER_REPLICATION_FACTOR, YarnConfiguration.DEFAULT_SHARED_CACHE_NM_UPLOADER_REPLICATION_FACTOR);	fs.setReplication(finalPath, replication);	
file was uploaded to the shared cache at 

return false;	}	if (!notifySharedCacheManager(checksumVal, actualPath.getName())) {	fs.delete(finalPath, false);	return false;	}	short replication = (short)conf.getInt(YarnConfiguration.SHARED_CACHE_NM_UPLOADER_REPLICATION_FACTOR, YarnConfiguration.DEFAULT_SHARED_CACHE_NM_UPLOADER_REPLICATION_FACTOR);	fs.setReplication(finalPath, replication);	return true;	} catch (IOException e) {	
exception while uploading the file 

private void deleteTempFile(Path tempPath) {	try {	if (tempPath != null) {	fs.delete(tempPath, false);	}	} catch (IOException ioe) {	
exception received while deleting temp files 

========================= hadoop sample_1890 =========================

public void run() {	try {	openFileCtx.executeWriteBack();	} catch (Throwable t) {	
async data service got error 

========================= hadoop sample_7047 =========================

public static void createControlFile(FileSystem fs, long megaBytes, int numFiles, long seed) throws Exception {	
creating control file bytes files 

Text name = new Text(Long.toString(random.nextLong()));	long size = random.nextLong();	if (size < 0) size = -size;	size = size % maxSize;	writer.append(name, new LongWritable(size));	totalSize += size;	}	} finally {	writer.close();	}	
created control file for bytes 

========================= hadoop sample_5580 =========================

this.peerStatsEnabled = getConf().getBoolean( DFSConfigKeys.DFS_DATANODE_PEER_STATS_ENABLED_KEY, DFSConfigKeys.DFS_DATANODE_PEER_STATS_ENABLED_DEFAULT);	this.diskStatsEnabled = Util.isDiskStatsEnabled(getConf().getInt( DFSConfigKeys.DFS_DATANODE_FILEIO_PROFILING_SAMPLING_PERCENTAGE_KEY, DFSConfigKeys. DFS_DATANODE_FILEIO_PROFILING_SAMPLING_PERCENTAGE_DEFAULT));	this.outliersReportIntervalMs = getConf().getTimeDuration( DFS_DATANODE_OUTLIERS_REPORT_INTERVAL_KEY, DFS_DATANODE_OUTLIERS_REPORT_INTERVAL_DEFAULT, TimeUnit.MILLISECONDS);	this.ibrInterval = getConf().getLong( DFSConfigKeys.DFS_BLOCKREPORT_INCREMENTAL_INTERVAL_MSEC_KEY, DFSConfigKeys.DFS_BLOCKREPORT_INCREMENTAL_INTERVAL_MSEC_DEFAULT);	this.blockReportSplitThreshold = getConf().getLong( DFS_BLOCKREPORT_SPLIT_THRESHOLD_KEY, DFS_BLOCKREPORT_SPLIT_THRESHOLD_DEFAULT);	this.cacheReportInterval = getConf().getLong( DFS_CACHEREPORT_INTERVAL_MSEC_KEY, DFS_CACHEREPORT_INTERVAL_MSEC_DEFAULT);	this.datanodeSlowIoWarningThresholdMs = getConf().getLong( DFSConfigKeys.DFS_DATANODE_SLOW_IO_WARNING_THRESHOLD_KEY, DFSConfigKeys.DFS_DATANODE_SLOW_IO_WARNING_THRESHOLD_DEFAULT);	long initBRDelay = getConf().getLong( DFS_BLOCKREPORT_INITIAL_DELAY_KEY, DFS_BLOCKREPORT_INITIAL_DELAY_DEFAULT) * 1000L;	if (initBRDelay >= blockReportInterval) {	initBRDelay = 0;	
dfs blockreport initialdelay is greater than or equal to dfs blockreport intervalmsec setting initial delay to msec 

this.datanodeSlowIoWarningThresholdMs = getConf().getLong( DFSConfigKeys.DFS_DATANODE_SLOW_IO_WARNING_THRESHOLD_KEY, DFSConfigKeys.DFS_DATANODE_SLOW_IO_WARNING_THRESHOLD_DEFAULT);	long initBRDelay = getConf().getLong( DFS_BLOCKREPORT_INITIAL_DELAY_KEY, DFS_BLOCKREPORT_INITIAL_DELAY_DEFAULT) * 1000L;	if (initBRDelay >= blockReportInterval) {	initBRDelay = 0;	}	initialBlockReportDelayMs = initBRDelay;	heartBeatInterval = getConf().getLong(DFS_HEARTBEAT_INTERVAL_KEY, DFS_HEARTBEAT_INTERVAL_DEFAULT) * 1000L;	long confLifelineIntervalMs = getConf().getLong(DFS_DATANODE_LIFELINE_INTERVAL_SECONDS_KEY, 3 * getConf().getLong(DFS_HEARTBEAT_INTERVAL_KEY, DFS_HEARTBEAT_INTERVAL_DEFAULT)) * 1000L;	if (confLifelineIntervalMs <= heartBeatInterval) {	confLifelineIntervalMs = 3 * heartBeatInterval;	
s must be set to a value greater than s resetting value to s which is d milliseconds 

========================= hadoop sample_7944 =========================

longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);	enableMultiObjectsDelete = conf.getBoolean(ENABLE_MULTI_DELETE, true);	readAhead = longBytesOption(conf, READAHEAD_RANGE, DEFAULT_READAHEAD_RANGE, 0);	storageStatistics = (S3AStorageStatistics) GlobalStorageStatistics.INSTANCE .put(S3AStorageStatistics.NAME, new GlobalStorageStatistics.StorageStatisticsProvider() {	public StorageStatistics provide() {	return new S3AStorageStatistics();	}	});	int maxThreads = conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);	if (maxThreads < 2) {	
must be at least forcing to 

serverSideEncryptionAlgorithm = getEncryptionAlgorithm(conf);	inputPolicy = S3AInputPolicy.getPolicy( conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));	blockUploadEnabled = conf.getBoolean(FAST_UPLOAD, DEFAULT_FAST_UPLOAD);	if (blockUploadEnabled) {	blockOutputBuffer = conf.getTrimmed(FAST_UPLOAD_BUFFER, DEFAULT_FAST_UPLOAD_BUFFER);	partSize = ensureOutputParameterInRange(MULTIPART_SIZE, partSize);	blockFactory = S3ADataBlocks.createFactory(this, blockOutputBuffer);	blockOutputActiveBlocks = intOption(conf, FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);	LOG.debug("Using S3ABlockOutputStream with buffer = {}; block={};" + " queue limit={}", blockOutputBuffer, partSize, blockOutputActiveBlocks);	} else {	
using 

private void initMultipartUploads(Configuration conf) throws IOException {	boolean purgeExistingMultipart = conf.getBoolean(PURGE_EXISTING_MULTIPART, DEFAULT_PURGE_EXISTING_MULTIPART);	long purgeExistingMultipartAge = longOption(conf, PURGE_EXISTING_MULTIPART_AGE, DEFAULT_PURGE_EXISTING_MULTIPART_AGE, 0);	if (purgeExistingMultipart) {	Date purgeBefore = new Date(new Date().getTime() - purgeExistingMultipartAge * 1000);	try {	transfers.abortMultipartUploads(bucket, purgeBefore);	} catch (AmazonServiceException e) {	if (e.getStatusCode() == 403) {	instrumentation.errorIgnored();	
failed to purging multipart uploads against fs may be read only 

public void setInputPolicy(S3AInputPolicy inputPolicy) {	Objects.requireNonNull(inputPolicy, "Null inputStrategy");	
setting input strategy 

public FSDataInputStream open(Path f, int bufferSize) throws IOException {	
opening for reading 

String key = pathToKey(f);	FileStatus status = null;	try {	status = getFileStatus(f);	if (status.isDirectory()) {	throw new FileAlreadyExistsException(f + " is a directory");	}	if (!overwrite) {	throw new FileAlreadyExistsException(f + " already exists");	}	
overwriting file 

private boolean innerRename(Path source, Path dest) throws RenameFailedException, FileNotFoundException, IOException, AmazonClientException {	Path src = qualify(source);	Path dst = qualify(dest);	
rename path to 

String srcKey = pathToKey(src);	String dstKey = pathToKey(dst);	if (srcKey.isEmpty()) {	throw new RenameFailedException(src, dst, "source is root directory");	}	if (dstKey.isEmpty()) {	throw new RenameFailedException(src, dst, "dest is root directory");	}	S3AFileStatus srcStatus = innerGetFileStatus(src, true);	if (srcKey.equals(dstKey)) {	
rename src and dest refer to the same file or directory 

throw new RenameFailedException(src, dst, "source is a directory and dest is a file") .withExitCode(srcStatus.isFile());	} else if (dstStatus.isEmptyDirectory() != Tristate.TRUE) {	throw new RenameFailedException(src, dst, "Destination is a non-empty directory") .withExitCode(false);	}	} else {	if (dstStatus.isFile()) {	throw new RenameFailedException(src, dst, "Cannot rename onto an existing file") .withExitCode(false);	}	}	} catch (FileNotFoundException e) {	
rename destination path not found 

}	}	}	Collection<Path> srcPaths = null;	List<PathMetadata> dstMetas = null;	if (hasMetadataStore()) {	srcPaths = new HashSet<>();	dstMetas = new ArrayList<>();	}	if (srcStatus.isFile()) {	
rename renaming file to 

String filename = srcKey.substring(pathToKey(src.getParent()).length()+1);	newDstKey = newDstKey + filename;	copyFile(srcKey, newDstKey, length);	S3Guard.addMoveFile(metadataStore, srcPaths, dstMetas, src, keyToQualifiedPath(newDstKey), length, getDefaultBlockSize(dst), username);	} else {	copyFile(srcKey, dstKey, srcStatus.getLen());	S3Guard.addMoveFile(metadataStore, srcPaths, dstMetas, src, dst, length, getDefaultBlockSize(dst), username);	}	innerDelete(srcStatus, false);	} else {	
rename renaming directory to 

private void deleteObjects(DeleteObjectsRequest deleteRequest) throws MultiObjectDeleteException, AmazonClientException {	incrementWriteOperations();	incrementStatistic(OBJECT_DELETE_REQUESTS, 1);	try {	s3.deleteObjects(deleteRequest);	} catch (MultiObjectDeleteException e) {	List<MultiObjectDeleteException.DeleteError> errors = e.getErrors();	
partial failure of delete errors 

public void incrementPutStartStatistics(long bytes) {	
put start bytes 

public boolean delete(Path f, boolean recursive) throws IOException {	try {	return innerDelete(innerGetFileStatus(f, true), recursive);	} catch (FileNotFoundException e) {	
couldn t delete does not exist 

private boolean innerDelete(S3AFileStatus status, boolean recursive) throws IOException, AmazonClientException {	Path f = status.getPath();	
delete path recursive 

private boolean innerDelete(S3AFileStatus status, boolean recursive) throws IOException, AmazonClientException {	Path f = status.getPath();	String key = pathToKey(f);	if (status.isDirectory()) {	
delete path is a directory 

if (!key.endsWith("/")) {	key = key + "/";	}	if (key.equals("/")) {	return rejectRootDirectoryDelete(status, recursive);	}	if (!recursive && status.isEmptyDirectory() == Tristate.FALSE) {	throw new PathIsNotEmptyDirectoryException(f.toString());	}	if (status.isEmptyDirectory() == Tristate.TRUE) {	
deleting fake empty directory 

return rejectRootDirectoryDelete(status, recursive);	}	if (!recursive && status.isEmptyDirectory() == Tristate.FALSE) {	throw new PathIsNotEmptyDirectoryException(f.toString());	}	if (status.isEmptyDirectory() == Tristate.TRUE) {	deleteObject(key);	metadataStore.delete(f);	instrumentation.directoryDeleted();	} else {	
getting objects for directory prefix to delete 

deleteObject(key);	metadataStore.delete(f);	instrumentation.directoryDeleted();	} else {	ListObjectsRequest request = createListObjectsRequest(key, null);	ObjectListing objects = listObjects(request);	List<DeleteObjectsRequest.KeyVersion> keys = new ArrayList<>(objects.getObjectSummaries().size());	while (true) {	for (S3ObjectSummary summary : objects.getObjectSummaries()) {	keys.add(new DeleteObjectsRequest.KeyVersion(summary.getKey()));	
got object to delete 

} else {	if (!keys.isEmpty()) {	removeKeys(keys, false, false);	}	break;	}	}	}	metadataStore.deleteSubtree(f);	} else {	
delete path is a file 

private boolean rejectRootDirectoryDelete(S3AFileStatus status, boolean recursive) throws IOException {	
delete the root directory of 

private void createFakeDirectoryIfNecessary(Path f) throws IOException, AmazonClientException {	String key = pathToKey(f);	if (!key.isEmpty() && !s3Exists(f)) {	
creating new fake directory at 

public FileStatus[] innerListStatus(Path f) throws FileNotFoundException, IOException, AmazonClientException {	Path path = qualify(f);	String key = pathToKey(path);	
list status for path 

final FileStatus fileStatus =  getFileStatus(path);	if (fileStatus.isDirectory()) {	if (!key.isEmpty()) {	key = key + '/';	}	DirListingMetadata dirMeta = metadataStore.listChildren(path);	if (allowAuthoritative && dirMeta != null && dirMeta.isAuthoritative()) {	return S3Guard.dirMetaToStatuses(dirMeta);	}	ListObjectsRequest request = createListObjectsRequest(key, "/");	
liststatus doing listobjects for directory 

return S3Guard.dirMetaToStatuses(dirMeta);	}	ListObjectsRequest request = createListObjectsRequest(key, "/");	Listing.FileStatusListingIterator files = listing.createFileStatusListingIterator(path, request, ACCEPT_ALL, new Listing.AcceptAllButSelfAndS3nDirs(path));	result = new ArrayList<>(files.getBatchSize());	while (files.hasNext()) {	result.add(files.next());	}	return S3Guard.dirListingUnion(metadataStore, path, result, dirMeta, allowAuthoritative);	} else {	
adding rd not a dir 

private boolean innerMkdirs(Path p, FsPermission permission) throws IOException, FileAlreadyExistsException, AmazonClientException {	Path f = qualify(p);	
making directory 

private S3AFileStatus s3GetFileStatus(final Path path, String key, Set<Path> tombstones) throws IOException {	if (!key.isEmpty()) {	try {	ObjectMetadata meta = getObjectMetadata(key);	if (objectRepresentsDirectory(key, meta.getContentLength())) {	
found exact file fake directory 

private S3AFileStatus s3GetFileStatus(final Path path, String key, Set<Path> tombstones) throws IOException {	if (!key.isEmpty()) {	try {	ObjectMetadata meta = getObjectMetadata(key);	if (objectRepresentsDirectory(key, meta.getContentLength())) {	return new S3AFileStatus(Tristate.TRUE, path, username);	} else {	
found exact file normal file 

throw translateException("getFileStatus", path, e);	}	} catch (AmazonClientException e) {	throw translateException("getFileStatus", path, e);	}	if (!key.endsWith("/")) {	String newKey = key + "/";	try {	ObjectMetadata meta = getObjectMetadata(newKey);	if (objectRepresentsDirectory(newKey, meta.getContentLength())) {	
found file with fake directory 

} catch (AmazonClientException e) {	throw translateException("getFileStatus", path, e);	}	if (!key.endsWith("/")) {	String newKey = key + "/";	try {	ObjectMetadata meta = getObjectMetadata(newKey);	if (objectRepresentsDirectory(newKey, meta.getContentLength())) {	return new S3AFileStatus(Tristate.TRUE, path, username);	} else {	
found file with real file should not happen 

ListObjectsRequest request = new ListObjectsRequest();	request.setBucketName(bucket);	request.setPrefix(key);	request.setDelimiter("/");	request.setMaxKeys(1);	ObjectListing objects = listObjects(request);	Collection<String> prefixes = objects.getCommonPrefixes();	Collection<S3ObjectSummary> summaries = objects.getObjectSummaries();	if (!isEmptyOfKeys(prefixes, tombstones) || !isEmptyOfObjects(summaries, tombstones)) {	if (LOG.isDebugEnabled()) {	
found path as directory with 

request.setBucketName(bucket);	request.setPrefix(key);	request.setDelimiter("/");	request.setMaxKeys(1);	ObjectListing objects = listObjects(request);	Collection<String> prefixes = objects.getCommonPrefixes();	Collection<S3ObjectSummary> summaries = objects.getObjectSummaries();	if (!isEmptyOfKeys(prefixes, tombstones) || !isEmptyOfObjects(summaries, tombstones)) {	if (LOG.isDebugEnabled()) {	for (S3ObjectSummary summary : summaries) {	
summary 

request.setDelimiter("/");	request.setMaxKeys(1);	ObjectListing objects = listObjects(request);	Collection<String> prefixes = objects.getCommonPrefixes();	Collection<S3ObjectSummary> summaries = objects.getObjectSummaries();	if (!isEmptyOfKeys(prefixes, tombstones) || !isEmptyOfObjects(summaries, tombstones)) {	if (LOG.isDebugEnabled()) {	for (S3ObjectSummary summary : summaries) {	}	for (String prefix : prefixes) {	
prefix 

Collection<S3ObjectSummary> summaries = objects.getObjectSummaries();	if (!isEmptyOfKeys(prefixes, tombstones) || !isEmptyOfObjects(summaries, tombstones)) {	if (LOG.isDebugEnabled()) {	for (S3ObjectSummary summary : summaries) {	}	for (String prefix : prefixes) {	}	}	return new S3AFileStatus(Tristate.FALSE, path, username);	} else if (key.isEmpty()) {	
found root directory 

} else if (key.isEmpty()) {	return new S3AFileStatus(Tristate.TRUE, path, username);	}	} catch (AmazonServiceException e) {	if (e.getStatusCode() != 404) {	throw translateException("getFileStatus", key, e);	}	} catch (AmazonClientException e) {	throw translateException("getFileStatus", key, e);	}	
not found 

private void innerCopyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst) throws IOException, FileAlreadyExistsException, AmazonClientException {	incrementStatistic(INVOCATION_COPY_FROM_LOCAL_FILE);	final String key = pathToKey(dst);	if (!overwrite && exists(dst)) {	throw new FileAlreadyExistsException(dst + " already exists");	}	
copying local file from to 

private void copyFile(String srcKey, String dstKey, long size) throws IOException, InterruptedIOException, AmazonClientException {	
copyfile 

private void deleteUnnecessaryFakeDirectories(Path path) {	List<DeleteObjectsRequest.KeyVersion> keysToRemove = new ArrayList<>();	while (!path.isRoot()) {	String key = pathToKey(path);	key = (key.endsWith("/")) ? key : (key + "/");	
to delete unnecessary fake directory for 

}	try {	removeKeys(keysToRemove, false, true);	} catch(AmazonClientException | InvalidRequestException e) {	instrumentation.errorIgnored();	if (LOG.isDebugEnabled()) {	StringBuilder sb = new StringBuilder();	for(DeleteObjectsRequest.KeyVersion kv : keysToRemove) {	sb.append(kv.getKey()).append(",");	}	
while deleting keys 

private RemoteIterator<LocatedFileStatus> innerListFiles(Path f, boolean recursive, Listing.FileStatusAcceptor acceptor) throws IOException {	incrementStatistic(INVOCATION_LIST_FILES);	Path path = qualify(f);	
listfiles 

private RemoteIterator<LocatedFileStatus> innerListFiles(Path f, boolean recursive, Listing.FileStatusAcceptor acceptor) throws IOException {	incrementStatistic(INVOCATION_LIST_FILES);	Path path = qualify(f);	try {	final FileStatus fileStatus = getFileStatus(path);	if (fileStatus.isFile()) {	
path is a file 

private RemoteIterator<LocatedFileStatus> innerListFiles(Path f, boolean recursive, Listing.FileStatusAcceptor acceptor) throws IOException {	incrementStatistic(INVOCATION_LIST_FILES);	Path path = qualify(f);	try {	final FileStatus fileStatus = getFileStatus(path);	if (fileStatus.isFile()) {	return new Listing.SingleStatusRemoteIterator( toLocatedFileStatus(fileStatus));	} else {	String key = maybeAddTrailingSlash(pathToKey(path));	String delimiter = recursive ? null : "/";	
requesting all entries under with delimiter 

public RemoteIterator<LocatedFileStatus> listLocatedStatus(final Path f, final PathFilter filter) throws FileNotFoundException, IOException {	incrementStatistic(INVOCATION_LIST_LOCATED_STATUS);	Path path = qualify(f);	
listlocatedstatus 

public RemoteIterator<LocatedFileStatus> listLocatedStatus(final Path f, final PathFilter filter) throws FileNotFoundException, IOException {	incrementStatistic(INVOCATION_LIST_LOCATED_STATUS);	Path path = qualify(f);	try {	final FileStatus fileStatus = getFileStatus(path);	if (fileStatus.isFile()) {	
path is a file 

========================= hadoop sample_5996 =========================

public void submitApplication(ApplicationId applicationId, String userName, String queue)  throws AccessControlException {	validateSubmitApplication(applicationId, userName, queue);	try {	getParent().submitApplication(applicationId, userName, queue);	} catch (AccessControlException ace) {	
failed to submit application to parent queue 

String msg = "Queue " + getQueuePath() + " already has " + user .getTotalApplications() + " applications from user " + userName + " cannot accept submission of application: " + applicationId;	LOG.info(msg);	throw new AccessControlException(msg);	}	} finally {	writeLock.unlock();	}	try {	getParent().validateSubmitApplication(applicationId, userName, queue);	} catch (AccessControlException ace) {	
failed to submit application to parent queue 

fsApp.hasNext(); ) {	FiCaSchedulerApp application = fsApp.next();	ApplicationId applicationId = application.getApplicationId();	String partitionName = application.getAppAMNodePartitionName();	Resource amLimit = getAMResourceLimitPerPartition(partitionName);	if (amLimit == null) {	amLimit = calculateAndGetAMResourceLimitPerPartition(partitionName);	}	Resource amIfStarted = Resources.add( application.getAMResource(partitionName), queueUsage.getAMUsed(partitionName));	if (LOG.isDebugEnabled()) {	
application amresource maxamresourceperqueuepercent amlimit lastclusterresource amifstarted am node partition name 

String partitionName = application.getAppAMNodePartitionName();	Resource amLimit = getAMResourceLimitPerPartition(partitionName);	if (amLimit == null) {	amLimit = calculateAndGetAMResourceLimitPerPartition(partitionName);	}	Resource amIfStarted = Resources.add( application.getAMResource(partitionName), queueUsage.getAMUsed(partitionName));	if (LOG.isDebugEnabled()) {	}	if (!Resources.lessThanOrEqual(resourceCalculator, lastClusterResource, amIfStarted, amLimit)) {	if (getNumActiveApplications() < 1 || (Resources.lessThanOrEqual( resourceCalculator, lastClusterResource, queueUsage.getAMUsed(partitionName), Resources.none()))) {	
maximum am resource percent is insufficient to start a single application in queue it is likely set too low skipping enforcement to allow at least one application to start 

amLimit = calculateAndGetAMResourceLimitPerPartition(partitionName);	}	Resource amIfStarted = Resources.add( application.getAMResource(partitionName), queueUsage.getAMUsed(partitionName));	if (LOG.isDebugEnabled()) {	}	if (!Resources.lessThanOrEqual(resourceCalculator, lastClusterResource, amIfStarted, amLimit)) {	if (getNumActiveApplications() < 1 || (Resources.lessThanOrEqual( resourceCalculator, lastClusterResource, queueUsage.getAMUsed(partitionName), Resources.none()))) {	} else{	application.updateAMContainerDiagnostics(AMState.INACTIVATED, CSAMContainerLaunchDiagnosticsConstants.QUEUE_AM_RESOURCE_LIMIT_EXCEED);	if (LOG.isDebugEnabled()) {	
not activating application as amifstarted exceeds amlimit 

}	User user = getUser(application.getUser());	Resource userAMLimit = userAmPartitionLimit.get(partitionName);	if (userAMLimit == null) {	userAMLimit = getUserAMResourceLimitPerPartition(partitionName, application.getUser());	userAmPartitionLimit.put(partitionName, userAMLimit);	}	Resource userAmIfStarted = Resources.add( application.getAMResource(partitionName), user.getConsumedAMResources(partitionName));	if (!Resources.lessThanOrEqual(resourceCalculator, lastClusterResource, userAmIfStarted, userAMLimit)) {	if (getNumActiveApplications() < 1 || (Resources.lessThanOrEqual( resourceCalculator, lastClusterResource, queueUsage.getAMUsed(partitionName), Resources.none()))) {	
maximum am resource percent is insufficient to start a single application in queue for user it is likely set too low skipping enforcement to allow at least one application to start 

if (userAMLimit == null) {	userAMLimit = getUserAMResourceLimitPerPartition(partitionName, application.getUser());	userAmPartitionLimit.put(partitionName, userAMLimit);	}	Resource userAmIfStarted = Resources.add( application.getAMResource(partitionName), user.getConsumedAMResources(partitionName));	if (!Resources.lessThanOrEqual(resourceCalculator, lastClusterResource, userAmIfStarted, userAMLimit)) {	if (getNumActiveApplications() < 1 || (Resources.lessThanOrEqual( resourceCalculator, lastClusterResource, queueUsage.getAMUsed(partitionName), Resources.none()))) {	} else{	application.updateAMContainerDiagnostics(AMState.INACTIVATED, CSAMContainerLaunchDiagnosticsConstants.USER_AM_RESOURCE_LIMIT_EXCEED);	if (LOG.isDebugEnabled()) {	
not activating application for user as useramifstarted exceeds useramlimit 

}	user.activateApplication();	orderingPolicy.addSchedulableEntity(application);	application.updateAMContainerDiagnostics(AMState.ACTIVATED, null);	queueUsage.incAMUsed(partitionName, application.getAMResource(partitionName));	user.getResourceUsage().incAMUsed(partitionName, application.getAMResource(partitionName));	user.getResourceUsage().setAMLimit(partitionName, userAMLimit);	metrics.incAMUsed(partitionName, application.getUser(), application.getAMResource(partitionName));	metrics.setAMResouceLimitForUser(partitionName, application.getUser(), userAMLimit);	fsApp.remove();	
application from user activated in queue 

private void addApplicationAttempt(FiCaSchedulerApp application, User user) {	try {	writeLock.lock();	user.submitApplication();	getPendingAppsOrderingPolicy().addSchedulableEntity(application);	applicationAttemptMap.put(application.getApplicationAttemptId(), application);	if (Resources.greaterThan(resourceCalculator, lastClusterResource, lastClusterResource, Resources.none())) {	activateApplications();	} else {	application.updateAMContainerDiagnostics(AMState.INACTIVATED, CSAMContainerLaunchDiagnosticsConstants.CLUSTER_RESOURCE_EMPTY);	
skipping activateapplications for since cluster resource is 

try {	writeLock.lock();	user.submitApplication();	getPendingAppsOrderingPolicy().addSchedulableEntity(application);	applicationAttemptMap.put(application.getApplicationAttemptId(), application);	if (Resources.greaterThan(resourceCalculator, lastClusterResource, lastClusterResource, Resources.none())) {	activateApplications();	} else {	application.updateAMContainerDiagnostics(AMState.INACTIVATED, CSAMContainerLaunchDiagnosticsConstants.CLUSTER_RESOURCE_EMPTY);	}	
application added appid user leaf queue user pending applications user active applications queue pending applications queue active applications 

queueUsage.decAMUsed(partitionName, application.getAMResource(partitionName));	user.getResourceUsage().decAMUsed(partitionName, application.getAMResource(partitionName));	metrics.decAMUsed(partitionName, application.getUser(), application.getAMResource(partitionName));	}	applicationAttemptMap.remove(application.getApplicationAttemptId());	user.finishApplication(wasActive);	if (user.getTotalApplications() == 0) {	usersManager.removeUser(application.getUser());	}	activateApplications();	
application removed appid user queue user pending applications user active applications queue pending applications queue active applications 

cul.reservation = appReserved;	}	}	if (!userAssignable) {	application.updateAMContainerDiagnostics(AMState.ACTIVATED, "User capacity has reached its maximum limit.");	ActivitiesLogger.APP.recordRejectedAppActivityFromLeafQueue( activitiesManager, node, application, application.getPriority(), ActivityDiagnosticConstant.USER_CAPACITY_MAXIMUM_LIMIT);	continue;	}	assignment = application.assignContainers(clusterResource, ps, currentResourceLimits, schedulingMode, null);	if (LOG.isDebugEnabled()) {	
post assigncontainers for application 

protected boolean canAssignToUser(Resource clusterResource, String userName, Resource limit, FiCaSchedulerApp application, String nodePartition, ResourceLimits currentResourceLimits) {	try {	readLock.lock();	User user = getUser(userName);	currentResourceLimits.setAmountNeededUnreserve(Resources.none());	if (Resources.greaterThan(resourceCalculator, clusterResource, user.getUsed(nodePartition), limit)) {	if (this.reservationsContinueLooking && nodePartition.equals( CommonNodeLabelsManager.NO_LABEL)) {	if (Resources.lessThanOrEqual(resourceCalculator, clusterResource, Resources.subtract(user.getUsed(), application.getCurrentReservation()), limit)) {	if (LOG.isDebugEnabled()) {	
user in queue will exceed limit based on reservations consumed reserved limit 

if (this.reservationsContinueLooking && nodePartition.equals( CommonNodeLabelsManager.NO_LABEL)) {	if (Resources.lessThanOrEqual(resourceCalculator, clusterResource, Resources.subtract(user.getUsed(), application.getCurrentReservation()), limit)) {	if (LOG.isDebugEnabled()) {	}	Resource amountNeededToUnreserve = Resources.subtract( user.getUsed(nodePartition), limit);	currentResourceLimits.setAmountNeededUnreserve( amountNeededToUnreserve);	return true;	}	}	if (LOG.isDebugEnabled()) {	
user in queue will exceed limit consumed limit 

========================= hadoop sample_918 =========================

List<Class<? extends CryptoCodec>> klasses = getCodecClasses( conf, cipherSuite);	if (klasses == null) {	return null;	}	CryptoCodec codec = null;	for (Class<? extends CryptoCodec> klass : klasses) {	try {	CryptoCodec c = ReflectionUtils.newInstance(klass, conf);	if (c.getCipherSuite().getName().equals(cipherSuite.getName())) {	if (codec == null) {	
using crypto codec 

}	CryptoCodec codec = null;	for (Class<? extends CryptoCodec> klass : klasses) {	try {	CryptoCodec c = ReflectionUtils.newInstance(klass, conf);	if (c.getCipherSuite().getName().equals(cipherSuite.getName())) {	if (codec == null) {	codec = c;	}	} else {	
crypto codec doesn t meet the cipher suite 

for (Class<? extends CryptoCodec> klass : klasses) {	try {	CryptoCodec c = ReflectionUtils.newInstance(klass, conf);	if (c.getCipherSuite().getName().equals(cipherSuite.getName())) {	if (codec == null) {	codec = c;	}	} else {	}	} catch (Exception e) {	
crypto codec is not available 

private static List<Class<? extends CryptoCodec>> getCodecClasses( Configuration conf, CipherSuite cipherSuite) {	List<Class<? extends CryptoCodec>> result = Lists.newArrayList();	String configName = HADOOP_SECURITY_CRYPTO_CODEC_CLASSES_KEY_PREFIX + cipherSuite.getConfigSuffix();	String codecString;	if (configName.equals(CommonConfigurationKeysPublic .HADOOP_SECURITY_CRYPTO_CODEC_CLASSES_AES_CTR_NOPADDING_KEY)) {	codecString = conf.get(configName, CommonConfigurationKeysPublic .HADOOP_SECURITY_CRYPTO_CODEC_CLASSES_AES_CTR_NOPADDING_DEFAULT);	} else {	codecString = conf.get(configName);	}	if (codecString == null) {	
no crypto codec classes with cipher suite configured 

codecString = conf.get(configName);	}	if (codecString == null) {	return null;	}	for (String c : Splitter.on(',').trimResults().omitEmptyStrings(). split(codecString)) {	try {	Class<?> cls = conf.getClassByName(c);	result.add(cls.asSubclass(CryptoCodec.class));	} catch (ClassCastException e) {	
class is not a cryptocodec 

}	if (codecString == null) {	return null;	}	for (String c : Splitter.on(',').trimResults().omitEmptyStrings(). split(codecString)) {	try {	Class<?> cls = conf.getClassByName(c);	result.add(cls.asSubclass(CryptoCodec.class));	} catch (ClassCastException e) {	} catch (ClassNotFoundException e) {	
crypto codec not found 

========================= hadoop sample_3511 =========================

public void reinit(Configuration conf) {	if (conf == null) {	return;	}	level = ZStandardCodec.getCompressionLevel(conf);	reset();	
reinit compressor with new compression configuration 

========================= hadoop sample_3842 =========================

public void testBlockIdGeneration() throws IOException {	Configuration conf = new HdfsConfiguration();	conf.setInt(DFSConfigKeys.DFS_REPLICATION_KEY, 1);	MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).build();	try {	cluster.waitActive();	FileSystem fs = cluster.getFileSystem();	Path path = new Path("testBlockIdGeneration.dat");	DFSTestUtil.createFile( fs, path, IO_SIZE, BLOCK_SIZE * 10, BLOCK_SIZE, REPLICATION, SEED);	List<LocatedBlock> blocks = DFSTestUtil.getAllBlocks(fs, path);	
id is 

MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).build();	try {	cluster.waitActive();	FileSystem fs = cluster.getFileSystem();	Path path = new Path("testBlockIdGeneration.dat");	DFSTestUtil.createFile( fs, path, IO_SIZE, BLOCK_SIZE * 10, BLOCK_SIZE, REPLICATION, SEED);	List<LocatedBlock> blocks = DFSTestUtil.getAllBlocks(fs, path);	long nextBlockExpectedId = blocks.get(0).getBlock().getBlockId() + 1;	for (int i = 1; i < blocks.size(); ++i) {	long nextBlockId = blocks.get(i).getBlock().getBlockId();	
Block id is 

========================= hadoop sample_7552 =========================

allocateRequest.setResponseId(1);	List<Container> containers = new ArrayList<Container>(numberOfResourceRequests);	List<ResourceRequest> askList = new ArrayList<ResourceRequest>(numberOfResourceRequests);	for (int id = 0; id < numberOfResourceRequests; id++) {	askList.add(createResourceRequest("test-node-" + Integer.toString(id), 6000, 2, id % 5, 1));	}	allocateRequest.setAskList(askList);	AllocateResponse allocateResponse = interceptor.allocate(allocateRequest);	Assert.assertNotNull("allocate() returned null response", allocateResponse);	containers.addAll(allocateResponse.getAllocatedContainers());	
number of allocated containers in the original request 

}	allocateRequest.setAskList(askList);	AllocateResponse allocateResponse = interceptor.allocate(allocateRequest);	Assert.assertNotNull("allocate() returned null response", allocateResponse);	containers.addAll(allocateResponse.getAllocatedContainers());	int numHeartbeat = 0;	while (containers.size() < numberOfAllocationExcepted && numHeartbeat++ < 10) {	allocateResponse = interceptor.allocate(Records.newRecord(AllocateRequest.class));	Assert.assertNotNull("allocate() returned null response", allocateResponse);	containers.addAll(allocateResponse.getAllocatedContainers());	
number of allocated containers in this request 

}	allocateRequest.setAskList(askList);	AllocateResponse allocateResponse = interceptor.allocate(allocateRequest);	Assert.assertNotNull("allocate() returned null response", allocateResponse);	containers.addAll(allocateResponse.getAllocatedContainers());	int numHeartbeat = 0;	while (containers.size() < numberOfAllocationExcepted && numHeartbeat++ < 10) {	allocateResponse = interceptor.allocate(Records.newRecord(AllocateRequest.class));	Assert.assertNotNull("allocate() returned null response", allocateResponse);	containers.addAll(allocateResponse.getAllocatedContainers());	
total number of allocated containers 

allocateRequest.setResponseId(1);	List<ContainerId> relList = new ArrayList<ContainerId>(containers.size());	for (Container container : containers) {	relList.add(container.getId());	}	allocateRequest.setReleaseList(relList);	AllocateResponse allocateResponse = interceptor.allocate(allocateRequest);	Assert.assertNotNull(allocateResponse);	List<Container> containersForReleasedContainerIds = new ArrayList<Container>();	containersForReleasedContainerIds .addAll(allocateResponse.getAllocatedContainers());	
number of containers received in the original request 

allocateRequest.setReleaseList(relList);	AllocateResponse allocateResponse = interceptor.allocate(allocateRequest);	Assert.assertNotNull(allocateResponse);	List<Container> containersForReleasedContainerIds = new ArrayList<Container>();	containersForReleasedContainerIds .addAll(allocateResponse.getAllocatedContainers());	int numHeartbeat = 0;	while (containersForReleasedContainerIds.size() < relList.size() && numHeartbeat++ < 10) {	allocateResponse = interceptor.allocate(Records.newRecord(AllocateRequest.class));	Assert.assertNotNull(allocateResponse);	containersForReleasedContainerIds .addAll(allocateResponse.getAllocatedContainers());	
number of containers received in this request 

allocateRequest.setReleaseList(relList);	AllocateResponse allocateResponse = interceptor.allocate(allocateRequest);	Assert.assertNotNull(allocateResponse);	List<Container> containersForReleasedContainerIds = new ArrayList<Container>();	containersForReleasedContainerIds .addAll(allocateResponse.getAllocatedContainers());	int numHeartbeat = 0;	while (containersForReleasedContainerIds.size() < relList.size() && numHeartbeat++ < 10) {	allocateResponse = interceptor.allocate(Records.newRecord(AllocateRequest.class));	Assert.assertNotNull(allocateResponse);	containersForReleasedContainerIds .addAll(allocateResponse.getAllocatedContainers());	
total number of containers received 

public void testConcurrentRegister() throws InterruptedException, ExecutionException {	ExecutorService threadpool = Executors.newCachedThreadPool();	ExecutorCompletionService<RegisterApplicationMasterResponse> compSvc = new ExecutorCompletionService<>(threadpool);	Object syncObj = MockResourceManagerFacade.getSyncObj();	synchronized (syncObj) {	
starting first register thread 

public void testConcurrentRegister() throws InterruptedException, ExecutionException {	ExecutorService threadpool = Executors.newCachedThreadPool();	ExecutorCompletionService<RegisterApplicationMasterResponse> compSvc = new ExecutorCompletionService<>(threadpool);	Object syncObj = MockResourceManagerFacade.getSyncObj();	synchronized (syncObj) {	compSvc.submit(new ConcurrentRegisterAMCallable());	try {	
test main starts waiting for the first thread to block 

public void testConcurrentRegister() throws InterruptedException, ExecutionException {	ExecutorService threadpool = Executors.newCachedThreadPool();	ExecutorCompletionService<RegisterApplicationMasterResponse> compSvc = new ExecutorCompletionService<>(threadpool);	Object syncObj = MockResourceManagerFacade.getSyncObj();	synchronized (syncObj) {	compSvc.submit(new ConcurrentRegisterAMCallable());	try {	syncObj.wait();	
test main wait finished 

public void testConcurrentRegister() throws InterruptedException, ExecutionException {	ExecutorService threadpool = Executors.newCachedThreadPool();	ExecutorCompletionService<RegisterApplicationMasterResponse> compSvc = new ExecutorCompletionService<>(threadpool);	Object syncObj = MockResourceManagerFacade.getSyncObj();	synchronized (syncObj) {	compSvc.submit(new ConcurrentRegisterAMCallable());	try {	syncObj.wait();	} catch (Exception e) {	
test main wait interrupted 

ExecutorService threadpool = Executors.newCachedThreadPool();	ExecutorCompletionService<RegisterApplicationMasterResponse> compSvc = new ExecutorCompletionService<>(threadpool);	Object syncObj = MockResourceManagerFacade.getSyncObj();	synchronized (syncObj) {	compSvc.submit(new ConcurrentRegisterAMCallable());	try {	syncObj.wait();	} catch (Exception e) {	}	}	
starting second register thread 

ExecutorCompletionService<RegisterApplicationMasterResponse> compSvc = new ExecutorCompletionService<>(threadpool);	Object syncObj = MockResourceManagerFacade.getSyncObj();	synchronized (syncObj) {	compSvc.submit(new ConcurrentRegisterAMCallable());	try {	syncObj.wait();	} catch (Exception e) {	}	}	compSvc.submit(new ConcurrentRegisterAMCallable());	
let first blocked register thread move on 

public RegisterApplicationMasterResponse call() throws Exception {	RegisterApplicationMasterResponse response = null;	try {	response = interceptor.registerApplicationMaster( RegisterApplicationMasterRequest.newInstance(null, 1001, null));	} catch (Exception e) {	
register thread exception 

========================= hadoop sample_1605 =========================

public void testFileStatusWritable() throws Exception {	FileStatus[] tests = {	new FileStatus(1,false,5,3,4,5,null,"","",new Path("/a/b")), new FileStatus(0,false,1,2,3,new Path("/")), new FileStatus(1,false,5,3,4,5,null,"","",new Path("/a/b")) };	
writing filestatuses to a bytearrayoutputstream 

public void testFileStatusWritable() throws Exception {	FileStatus[] tests = {	new FileStatus(1,false,5,3,4,5,null,"","",new Path("/a/b")), new FileStatus(0,false,1,2,3,new Path("/")), new FileStatus(1,false,5,3,4,5,null,"","",new Path("/a/b")) };	ByteArrayOutputStream baos = new ByteArrayOutputStream();	DataOutput out = new DataOutputStream(baos);	for (FileStatus fs : tests) {	fs.write(out);	}	
creating bytearrayinputstream object 

public void testFileStatusWritable() throws Exception {	FileStatus[] tests = {	new FileStatus(1,false,5,3,4,5,null,"","",new Path("/a/b")), new FileStatus(0,false,1,2,3,new Path("/")), new FileStatus(1,false,5,3,4,5,null,"","",new Path("/a/b")) };	ByteArrayOutputStream baos = new ByteArrayOutputStream();	DataOutput out = new DataOutputStream(baos);	for (FileStatus fs : tests) {	fs.write(out);	}	DataInput in = new DataInputStream(new ByteArrayInputStream(baos.toByteArray()));	
testing if read objects are equal to written ones 

========================= hadoop sample_3268 =========================

private void loadReservationSystemState(RMState rmState) throws Exception {	try {	final ReservationStateFileProcessor fileProcessor = new ReservationStateFileProcessor(rmState);	final Path rootDirectory = this.reservationRoot;	processDirectoriesOfFiles(fileProcessor, rootDirectory);	} catch (Exception e) {	
failed to load state 

List<ApplicationAttemptStateData> attempts = new ArrayList<>();	final RMAppStateFileProcessor rmAppStateFileProcessor = new RMAppStateFileProcessor(rmState, attempts);	final Path rootDirectory = this.rmAppRoot;	processDirectoriesOfFiles(rmAppStateFileProcessor, rootDirectory);	for (ApplicationAttemptStateData attemptState : attempts) {	ApplicationId appId = attemptState.getAttemptId().getApplicationId();	ApplicationStateData appState = rmState.appState.get(appId);	assert appState != null;	appState.attempts.put(attemptState.getAttemptId(), attemptState);	}	
done loading applications from fs state store 

final RMAppStateFileProcessor rmAppStateFileProcessor = new RMAppStateFileProcessor(rmState, attempts);	final Path rootDirectory = this.rmAppRoot;	processDirectoriesOfFiles(rmAppStateFileProcessor, rootDirectory);	for (ApplicationAttemptStateData attemptState : attempts) {	ApplicationId appId = attemptState.getAttemptId().getApplicationId();	ApplicationStateData appState = rmState.appState.get(appId);	assert appState != null;	appState.attempts.put(attemptState.getAttemptId(), attemptState);	}	} catch (Exception e) {	
failed to load state 

private boolean checkAndRemovePartialRecord(Path record) throws IOException {	if (record.getName().endsWith(".tmp")) {	
incomplete rm state store entry found 

} else if (childNodeName.startsWith(DELEGATION_TOKEN_PREFIX)) {	RMDelegationTokenIdentifierData identifierData = new RMDelegationTokenIdentifierData();	identifierData.readFields(fsIn);	RMDelegationTokenIdentifier identifier = identifierData.getTokenIdentifier();	long renewDate = identifierData.getRenewDate();	rmState.rmSecretManagerState.delegationTokenState.put(identifier, renewDate);	if (LOG.isDebugEnabled()) {	LOG.debug("Loaded RMDelegationTokenIdentifier: " + identifier + " renewDate=" + renewDate);	}	} else {	
unknown file for recovering rmdelegationtokensecretmanager 

public synchronized void storeApplicationStateInternal(ApplicationId appId, ApplicationStateData appStateDataPB) throws Exception {	Path appDirPath = getAppDir(rmAppRoot, appId);	mkdirsWithRetries(appDirPath);	Path nodeCreatePath = getNodePath(appDirPath, appId.toString());	
storing info for app at 

public synchronized void storeApplicationStateInternal(ApplicationId appId, ApplicationStateData appStateDataPB) throws Exception {	Path appDirPath = getAppDir(rmAppRoot, appId);	mkdirsWithRetries(appDirPath);	Path nodeCreatePath = getNodePath(appDirPath, appId.toString());	byte[] appStateData = appStateDataPB.getProto().toByteArray();	try {	writeFileWithRetries(nodeCreatePath, appStateData, true);	} catch (Exception e) {	
error storing info for app 

public synchronized void updateApplicationStateInternal(ApplicationId appId, ApplicationStateData appStateDataPB) throws Exception {	Path appDirPath = getAppDir(rmAppRoot, appId);	Path nodeCreatePath = getNodePath(appDirPath, appId.toString());	
updating info for app at 

public synchronized void updateApplicationStateInternal(ApplicationId appId, ApplicationStateData appStateDataPB) throws Exception {	Path appDirPath = getAppDir(rmAppRoot, appId);	Path nodeCreatePath = getNodePath(appDirPath, appId.toString());	byte[] appStateData = appStateDataPB.getProto().toByteArray();	try {	updateFile(nodeCreatePath, appStateData, true);	} catch (Exception e) {	
error updating info for app 

public synchronized void storeApplicationAttemptStateInternal( ApplicationAttemptId appAttemptId, ApplicationAttemptStateData attemptStateDataPB) throws Exception {	Path appDirPath = getAppDir(rmAppRoot, appAttemptId.getApplicationId());	Path nodeCreatePath = getNodePath(appDirPath, appAttemptId.toString());	
storing info for attempt at 

public synchronized void storeApplicationAttemptStateInternal( ApplicationAttemptId appAttemptId, ApplicationAttemptStateData attemptStateDataPB) throws Exception {	Path appDirPath = getAppDir(rmAppRoot, appAttemptId.getApplicationId());	Path nodeCreatePath = getNodePath(appDirPath, appAttemptId.toString());	byte[] attemptStateData = attemptStateDataPB.getProto().toByteArray();	try {	writeFileWithRetries(nodeCreatePath, attemptStateData, true);	} catch (Exception e) {	
error storing info for attempt 

public synchronized void updateApplicationAttemptStateInternal( ApplicationAttemptId appAttemptId, ApplicationAttemptStateData attemptStateDataPB) throws Exception {	Path appDirPath = getAppDir(rmAppRoot, appAttemptId.getApplicationId());	Path nodeCreatePath = getNodePath(appDirPath, appAttemptId.toString());	
updating info for attempt at 

public synchronized void updateApplicationAttemptStateInternal( ApplicationAttemptId appAttemptId, ApplicationAttemptStateData attemptStateDataPB) throws Exception {	Path appDirPath = getAppDir(rmAppRoot, appAttemptId.getApplicationId());	Path nodeCreatePath = getNodePath(appDirPath, appAttemptId.toString());	byte[] attemptStateData = attemptStateDataPB.getProto().toByteArray();	try {	updateFile(nodeCreatePath, attemptStateData, true);	} catch (Exception e) {	
error updating info for attempt 

public synchronized void removeApplicationAttemptInternal( ApplicationAttemptId appAttemptId) throws Exception {	Path appDirPath = getAppDir(rmAppRoot, appAttemptId.getApplicationId());	Path nodeRemovePath = getNodePath(appDirPath, appAttemptId.toString());	
removing info for attempt at 

public synchronized void removeApplicationStateInternal( ApplicationStateData appState) throws Exception {	ApplicationId appId = appState.getApplicationSubmissionContext().getApplicationId();	Path nodeRemovePath = getAppDir(rmAppRoot, appId);	
removing info for app at 

public synchronized void removeRMDelegationTokenState( RMDelegationTokenIdentifier identifier) throws Exception {	Path nodeCreatePath = getNodePath(rmDTSecretManagerRoot, DELEGATION_TOKEN_PREFIX + identifier.getSequenceNumber());	
removing rmdelegationtoken 

private void storeOrUpdateRMDelegationTokenState( RMDelegationTokenIdentifier identifier, Long renewDate, boolean isUpdate) throws Exception {	Path nodeCreatePath = getNodePath(rmDTSecretManagerRoot, DELEGATION_TOKEN_PREFIX + identifier.getSequenceNumber());	RMDelegationTokenIdentifierData identifierData = new RMDelegationTokenIdentifierData(identifier, renewDate);	if (isUpdate) {	
updating rmdelegationtoken 

private void storeOrUpdateRMDelegationTokenState( RMDelegationTokenIdentifier identifier, Long renewDate, boolean isUpdate) throws Exception {	Path nodeCreatePath = getNodePath(rmDTSecretManagerRoot, DELEGATION_TOKEN_PREFIX + identifier.getSequenceNumber());	RMDelegationTokenIdentifierData identifierData = new RMDelegationTokenIdentifierData(identifier, renewDate);	if (isUpdate) {	updateFile(nodeCreatePath, identifierData.toByteArray(), true);	} else {	
storing rmdelegationtoken 

private void storeOrUpdateRMDelegationTokenState( RMDelegationTokenIdentifier identifier, Long renewDate, boolean isUpdate) throws Exception {	Path nodeCreatePath = getNodePath(rmDTSecretManagerRoot, DELEGATION_TOKEN_PREFIX + identifier.getSequenceNumber());	RMDelegationTokenIdentifierData identifierData = new RMDelegationTokenIdentifierData(identifier, renewDate);	if (isUpdate) {	updateFile(nodeCreatePath, identifierData.toByteArray(), true);	} else {	writeFileWithRetries(nodeCreatePath, identifierData.toByteArray(), true);	Path latestSequenceNumberPath = getNodePath(rmDTSecretManagerRoot, DELEGATION_TOKEN_SEQUENCE_NUMBER_PREFIX + identifier.getSequenceNumber());	
storing 

public synchronized void storeRMDTMasterKeyState(DelegationKey masterKey) throws Exception {	Path nodeCreatePath = getNodePath(rmDTSecretManagerRoot, DELEGATION_KEY_PREFIX + masterKey.getKeyId());	ByteArrayOutputStream os = new ByteArrayOutputStream();	try (DataOutputStream fsOut = new DataOutputStream(os)) {	
storing rmdelegationkey 

protected void replaceFile(Path srcPath, Path dstPath) throws Exception {	if (existsWithRetries(dstPath)) {	deleteFileWithRetries(dstPath);	} else {	
file doesn t exist skip deleting the file 

protected void storeReservationState( ReservationAllocationStateProto reservationAllocation, String planName, String reservationIdName) throws Exception {	Path planCreatePath = getNodePath(reservationRoot, planName);	mkdirsWithRetries(planCreatePath);	Path reservationPath = getNodePath(planCreatePath, reservationIdName);	
storing state for reservation from plan at path 

protected void removeReservationState( String planName, String reservationIdName) throws Exception {	Path planCreatePath = getNodePath(reservationRoot, planName);	Path reservationPath = getNodePath(planCreatePath, reservationIdName);	
removing state for reservation from plan at path 

public void processChildNode(String appDirName, String childNodeName, byte[] childData) throws com.google.protobuf.InvalidProtocolBufferException {	if (childNodeName.startsWith(ApplicationId.appIdStrPrefix)) {	if (LOG.isDebugEnabled()) {	
loading application from node 

public void processChildNode(String appDirName, String childNodeName, byte[] childData) throws com.google.protobuf.InvalidProtocolBufferException {	if (childNodeName.startsWith(ApplicationId.appIdStrPrefix)) {	if (LOG.isDebugEnabled()) {	}	ApplicationStateDataPBImpl appState = new ApplicationStateDataPBImpl( ApplicationStateDataProto.parseFrom(childData));	ApplicationId appId = appState.getApplicationSubmissionContext().getApplicationId();	rmState.appState.put(appId, appState);	} else if (childNodeName.startsWith( ApplicationAttemptId.appAttemptIdStrPrefix)) {	if (LOG.isDebugEnabled()) {	
loading application attempt from node 

}	ApplicationStateDataPBImpl appState = new ApplicationStateDataPBImpl( ApplicationStateDataProto.parseFrom(childData));	ApplicationId appId = appState.getApplicationSubmissionContext().getApplicationId();	rmState.appState.put(appId, appState);	} else if (childNodeName.startsWith( ApplicationAttemptId.appAttemptIdStrPrefix)) {	if (LOG.isDebugEnabled()) {	}	ApplicationAttemptStateDataPBImpl attemptState = new ApplicationAttemptStateDataPBImpl( ApplicationAttemptStateDataProto.parseFrom(childData));	attempts.add(attemptState);	} else {	
unknown child node with name 

========================= hadoop sample_762 =========================

assertEquals("reader class is CombineFileRecordReader.", CombineFileRecordReader.class, reader.getClass());	MapContext<LongWritable,Text,LongWritable,Text> mcontext = new MapContextImpl<LongWritable,Text,LongWritable,Text>(job.getConfiguration(), context.getTaskAttemptID(), reader, null, null, MapReduceTestUtil.createDummyReporter(), split);	reader.initialize(split, mcontext);	try {	int count = 0;	while (reader.nextKeyValue()) {	LongWritable key = reader.getCurrentKey();	assertNotNull("Key should not be null.", key);	Text value = reader.getCurrentValue();	final int v = Integer.parseInt(value.toString());	
read 

========================= hadoop sample_5646 =========================

protected RpcProgram(String program, String host, int port, int progNumber, int lowProgVersion, int highProgVersion, DatagramSocket registrationSocket, boolean allowInsecurePorts, int portmapUdpTimeoutMillis) {	this.program = program;	this.host = host;	this.port = port;	this.progNumber = progNumber;	this.lowProgVersion = lowProgVersion;	this.highProgVersion = highProgVersion;	this.registrationSocket = registrationSocket;	this.allowInsecurePorts = allowInsecurePorts;	this.portmapUdpTimeoutMillis = portmapUdpTimeoutMillis;	
will not accept client connections from unprivileged ports 

public void register(int transport, int boundPort) {	if (boundPort != port) {	
the bound port is different with configured port 

public void unregister(int transport, int boundPort) {	if (boundPort != port) {	
the bound port is different with configured port 

protected void register(PortmapMapping mapEntry, boolean set) {	XDR mappingRequest = PortmapRequest.create(mapEntry, set);	SimpleUdpClient registrationClient = new SimpleUdpClient(host, RPCB_PORT, mappingRequest, true, registrationSocket, portmapUdpTimeoutMillis);	try {	registrationClient.run();	} catch (IOException e) {	String request = set ? "Registration" : "Unregistration";	
failure with portmap entry 

public void messageReceived(ChannelHandlerContext ctx, MessageEvent e) throws Exception {	RpcInfo info = (RpcInfo) e.getMessage();	RpcCall call = (RpcCall) info.header();	SocketAddress remoteAddress = info.remoteAddress();	if (LOG.isTraceEnabled()) {	
procedure 

public void messageReceived(ChannelHandlerContext ctx, MessageEvent e) throws Exception {	RpcInfo info = (RpcInfo) e.getMessage();	RpcCall call = (RpcCall) info.header();	SocketAddress remoteAddress = info.remoteAddress();	if (LOG.isTraceEnabled()) {	}	if (this.progNumber != call.getProgram()) {	
invalid rpc call program 

RpcCall call = (RpcCall) info.header();	SocketAddress remoteAddress = info.remoteAddress();	if (LOG.isTraceEnabled()) {	}	if (this.progNumber != call.getProgram()) {	sendAcceptedReply(call, remoteAddress, AcceptState.PROG_UNAVAIL, ctx);	return;	}	int ver = call.getVersion();	if (ver < lowProgVersion || ver > highProgVersion) {	
invalid rpc call version 

public boolean doPortMonitoring(SocketAddress remoteAddress) {	if (!allowInsecurePorts) {	if (LOG.isTraceEnabled()) {	
will not allow connections from unprivileged ports checking for valid client port 

public boolean doPortMonitoring(SocketAddress remoteAddress) {	if (!allowInsecurePorts) {	if (LOG.isTraceEnabled()) {	}	if (remoteAddress instanceof InetSocketAddress) {	InetSocketAddress inetRemoteAddress = (InetSocketAddress) remoteAddress;	if (inetRemoteAddress.getPort() > 1023) {	
connection attempted from which is an unprivileged port rejecting connection 

public boolean doPortMonitoring(SocketAddress remoteAddress) {	if (!allowInsecurePorts) {	if (LOG.isTraceEnabled()) {	}	if (remoteAddress instanceof InetSocketAddress) {	InetSocketAddress inetRemoteAddress = (InetSocketAddress) remoteAddress;	if (inetRemoteAddress.getPort() > 1023) {	return false;	}	} else {	
could not determine remote port of socket address rejecting connection 

========================= hadoop sample_4387 =========================

public void setup(VolumeScanner scanner) {	
starting volumescanner 

public void handle(ExtendedBlock block, IOException e) {	FsVolumeSpi volume = scanner.volume;	if (e == null) {	
successfully scanned on 

public void handle(ExtendedBlock block, IOException e) {	FsVolumeSpi volume = scanner.volume;	if (e == null) {	return;	}	if (!volume.getDataset().contains(block)) {	
volume block is no longer in the dataset 

public void handle(ExtendedBlock block, IOException e) {	FsVolumeSpi volume = scanner.volume;	if (e == null) {	return;	}	if (!volume.getDataset().contains(block)) {	return;	}	if (e instanceof FileNotFoundException ) {	
volume verification failed for because of filenotfoundexception this may be due to a race with write 

FsVolumeSpi volume = scanner.volume;	if (e == null) {	return;	}	if (!volume.getDataset().contains(block)) {	return;	}	if (e instanceof FileNotFoundException ) {	return;	}	
reporting bad with volume 

}	if (!volume.getDataset().contains(block)) {	return;	}	if (e instanceof FileNotFoundException ) {	return;	}	try {	scanner.datanode.reportBadBlocks(block, volume);	} catch (IOException ie) {	
cannot report bad block 

private void saveBlockIterator(BlockIterator iter) {	try {	iter.save();	} catch (IOException e) {	
error saving 

private synchronized long findNextUsableBlockIter() {	int numBlockIters = blockIters.size();	if (numBlockIters == 0) {	
no block pools are registered 

} else {	curIdx = blockIters.indexOf(curBlockIter);	Preconditions.checkState(curIdx >= 0);	}	long nowMs = Time.now();	long minTimeoutMs = Long.MAX_VALUE;	for (int i = 0; i < numBlockIters; i++) {	int idx = (curIdx + i + 1) % numBlockIters;	BlockIterator iter = blockIters.get(idx);	if (!iter.atEnd()) {	
now scanning bpid on volume 

int idx = (curIdx + i + 1) % numBlockIters;	BlockIterator iter = blockIters.get(idx);	if (!iter.atEnd()) {	curBlockIter = iter;	return 0L;	}	long iterStartMs = iter.getIterStartMs();	long waitMs = (iterStartMs + conf.scanPeriodMs) - nowMs;	if (waitMs <= 0) {	iter.rewind();	
now rescanning bpid on volume after more than hour s 

}	long iterStartMs = iter.getIterStartMs();	long waitMs = (iterStartMs + conf.scanPeriodMs) - nowMs;	if (waitMs <= 0) {	iter.rewind();	curBlockIter = iter;	return 0L;	}	minTimeoutMs = Math.min(minTimeoutMs, waitMs);	}	
no suitable block pools found to scan waiting ms 

private long scanBlock(ExtendedBlock cblock, long bytesPerSec) {	ExtendedBlock block = null;	try {	Block b = volume.getDataset().getStoredBlock( cblock.getBlockPoolId(), cblock.getBlockId());	if (b == null) {	
replica was not found in the volumemap for volume 

private long scanBlock(ExtendedBlock cblock, long bytesPerSec) {	ExtendedBlock block = null;	try {	Block b = volume.getDataset().getStoredBlock( cblock.getBlockPoolId(), cblock.getBlockId());	if (b == null) {	} else {	block = new ExtendedBlock(cblock.getBlockPoolId(), b);	}	} catch (FileNotFoundException e) {	
filenotfoundexception while finding block on volume 

private long scanBlock(ExtendedBlock cblock, long bytesPerSec) {	ExtendedBlock block = null;	try {	Block b = volume.getDataset().getStoredBlock( cblock.getBlockPoolId(), cblock.getBlockId());	if (b == null) {	} else {	block = new ExtendedBlock(cblock.getBlockPoolId(), b);	}	} catch (FileNotFoundException e) {	} catch (IOException e) {	
i o error while finding block on volume 

if (b == null) {	} else {	block = new ExtendedBlock(cblock.getBlockPoolId(), b);	}	} catch (FileNotFoundException e) {	} catch (IOException e) {	}	if (block == null) {	return -1;	}	
start scanning block 

expireOldScannedBytesRecords(monotonicMs);	if (!calculateShouldScan(volume.getStorageID(), conf.targetBytesPerSec, scannedBytesSum, startMinute, curMinute)) {	return 30000L;	}	if (suspectBlock != null) {	block = suspectBlock;	} else {	if ((curBlockIter == null) || curBlockIter.atEnd()) {	long timeout = findNextUsableBlockIter();	if (timeout > 0) {	
no block pools are ready to scan yet waiting ms 

synchronized (stats) {	stats.scansSinceRestart++;	stats.blocksScannedInCurrentPeriod = 0;	stats.nextBlockPoolScanStartMs = -1;	}	return 0L;	}	try {	block = curBlockIter.nextBlock();	} catch (IOException e) {	
nextblock error on 

stats.nextBlockPoolScanStartMs = -1;	}	return 0L;	}	try {	block = curBlockIter.nextBlock();	} catch (IOException e) {	return 0L;	}	if (block == null) {	
finished scanning block pool 

return 0L;	}	if (block == null) {	saveBlockIterator(curBlockIter);	return 0;	}	}	if (curBlockIter != null) {	long saveDelta = monotonicMs - curBlockIter.getLastSavedMs();	if (saveDelta >= conf.cursorSaveMs) {	
saving block iterator after ms 

public void run() {	this.startMinute = TimeUnit.MINUTES.convert(Time.monotonicNow(), TimeUnit.MILLISECONDS);	this.curMinute = startMinute;	try {	
thread starting 

resultHandler.setup(this);	try {	long timeout = 0;	while (true) {	ExtendedBlock suspectBlock = null;	synchronized (this) {	if (stopping) {	break;	}	if (timeout > 0) {	
wait for milliseconds 

wait(timeout);	if (stopping) {	break;	}	}	suspectBlock = popNextSuspectBlock();	}	timeout = runLoop(suspectBlock);	}	} catch (InterruptedException e) {	
exiting because of interruptedexception 

if (stopping) {	break;	}	}	suspectBlock = popNextSuspectBlock();	}	timeout = runLoop(suspectBlock);	}	} catch (InterruptedException e) {	} catch (Throwable e) {	
exiting because of exception 

break;	}	}	suspectBlock = popNextSuspectBlock();	}	timeout = runLoop(suspectBlock);	}	} catch (InterruptedException e) {	} catch (Throwable e) {	}	
exiting 

public synchronized void markSuspectBlock(ExtendedBlock block) {	if (stopping) {	
not scheduling suspect block for rescanning because this volume scanner is stopping 

public synchronized void markSuspectBlock(ExtendedBlock block) {	if (stopping) {	return;	}	Boolean recent = recentSuspectBlocks.getIfPresent(block);	if (recent != null) {	
not scheduling suspect block for rescanning because we rescanned it recently 

public synchronized void markSuspectBlock(ExtendedBlock block) {	if (stopping) {	return;	}	Boolean recent = recentSuspectBlocks.getIfPresent(block);	if (recent != null) {	return;	}	if (suspectBlocks.contains(block)) {	
suspect block is already queued for rescanning 

}	Boolean recent = recentSuspectBlocks.getIfPresent(block);	if (recent != null) {	return;	}	if (suspectBlocks.contains(block)) {	return;	}	suspectBlocks.add(block);	recentSuspectBlocks.put(block, true);	
scheduling suspect block for rescanning 

public synchronized void enableBlockPoolId(String bpid) {	for (BlockIterator iter : blockIters) {	if (iter.getBlockPoolId().equals(bpid)) {	
already enabled scanning on block pool 

public synchronized void enableBlockPoolId(String bpid) {	for (BlockIterator iter : blockIters) {	if (iter.getBlockPoolId().equals(bpid)) {	return;	}	}	BlockIterator iter = null;	try {	iter = volume.loadBlockIterator(bpid, BLOCK_ITERATOR_NAME);	
loaded block iterator for 

public synchronized void enableBlockPoolId(String bpid) {	for (BlockIterator iter : blockIters) {	if (iter.getBlockPoolId().equals(bpid)) {	return;	}	}	BlockIterator iter = null;	try {	iter = volume.loadBlockIterator(bpid, BLOCK_ITERATOR_NAME);	} catch (FileNotFoundException e) {	
failed to load block iterator 

for (BlockIterator iter : blockIters) {	if (iter.getBlockPoolId().equals(bpid)) {	return;	}	}	BlockIterator iter = null;	try {	iter = volume.loadBlockIterator(bpid, BLOCK_ITERATOR_NAME);	} catch (FileNotFoundException e) {	} catch (IOException e) {	
failed to load block iterator 

}	}	BlockIterator iter = null;	try {	iter = volume.loadBlockIterator(bpid, BLOCK_ITERATOR_NAME);	} catch (FileNotFoundException e) {	} catch (IOException e) {	}	if (iter == null) {	iter = volume.newBlockIterator(bpid, BLOCK_ITERATOR_NAME);	
created new block iterator for 

public synchronized void disableBlockPoolId(String bpid) {	Iterator<BlockIterator> i = blockIters.iterator();	while (i.hasNext()) {	BlockIterator iter = i.next();	if (iter.getBlockPoolId().equals(bpid)) {	
disabling scanning on block pool 

if (iter.getBlockPoolId().equals(bpid)) {	i.remove();	IOUtils.cleanup(null, iter);	if (curBlockIter == iter) {	curBlockIter = null;	}	notify();	return;	}	}	
can t remove block pool because it was never added 

========================= hadoop sample_7952 =========================

writeManager = new WriteManager(iug, config, aixCompatMode);	clientCache = new DFSClientCache(config);	replication = (short) config.getInt(DFSConfigKeys.DFS_REPLICATION_KEY, DFSConfigKeys.DFS_REPLICATION_DEFAULT);	blockSize = config.getLongBytes(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, DFSConfigKeys.DFS_BLOCK_SIZE_DEFAULT);	bufferSize = config.getInt( CommonConfigurationKeysPublic.IO_FILE_BUFFER_SIZE_KEY, CommonConfigurationKeysPublic.IO_FILE_BUFFER_SIZE_DEFAULT);	writeDumpDir = config.get(NfsConfigKeys.DFS_NFS_FILE_DUMP_DIR_KEY, NfsConfigKeys.DFS_NFS_FILE_DUMP_DIR_DEFAULT);	boolean enableDump = config.getBoolean(NfsConfigKeys.DFS_NFS_FILE_DUMP_KEY, NfsConfigKeys.DFS_NFS_FILE_DUMP_DEFAULT);	UserGroupInformation.setConfiguration(config);	SecurityUtil.login(config, NfsConfigKeys.DFS_NFS_KEYTAB_FILE_KEY, NfsConfigKeys.DFS_NFS_KERBEROS_PRINCIPAL_KEY);	superuser = config.get(NfsConfigKeys.NFS_SUPERUSER_KEY, NfsConfigKeys.NFS_SUPERUSER_DEFAULT);	
configured hdfs superuser is 

private void clearDirectory(String writeDumpDir) throws IOException {	File dumpDir = new File(writeDumpDir);	if (dumpDir.exists()) {	
delete current dump directory 

private void clearDirectory(String writeDumpDir) throws IOException {	File dumpDir = new File(writeDumpDir);	if (dumpDir.exists()) {	if (!(FileUtil.fullyDelete(dumpDir))) {	throw new IOException("Cannot remove current dump directory: " + dumpDir);	}	}	
create new dump directory 

if (pauseMonitor == null) {	pauseMonitor = new JvmPauseMonitor();	pauseMonitor.init(config);	pauseMonitor.start();	metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);	}	writeManager.startAsyncDataService();	try {	infoServer.start();	} catch (IOException e) {	
failed to start web server 

if (writeManager != null) {	writeManager.shutdownAsyncDataService();	}	if (pauseMonitor != null) {	pauseMonitor.stop();	}	if (infoServer != null) {	try {	infoServer.stop();	} catch (Exception e) {	
exception shutting down web server 

public NFS3Response nullProcedure() {	if (LOG.isDebugEnabled()) {	
nfs null 

private void setattrInternal(DFSClient dfsClient, String fileIdPath, SetAttr3 newAttr, boolean setMode) throws IOException {	EnumSet<SetAttrField> updateFields = newAttr.getUpdateFields();	if (setMode && updateFields.contains(SetAttrField.MODE)) {	if (LOG.isDebugEnabled()) {	
set new mode 

}	if (updateFields.contains(SetAttrField.UID) || updateFields.contains(SetAttrField.GID)) {	String uname = updateFields.contains(SetAttrField.UID) ? iug.getUserName( newAttr.getUid(), IdMappingConstant.UNKNOWN_USER) : null;	String gname = updateFields.contains(SetAttrField.GID) ? iug .getGroupName(newAttr.getGid(), IdMappingConstant.UNKNOWN_GROUP) : null;	dfsClient.setOwner(fileIdPath, uname, gname);	}	long atime = updateFields.contains(SetAttrField.ATIME) ? newAttr.getAtime() .getMilliSeconds() : -1;	long mtime = updateFields.contains(SetAttrField.MTIME) ? newAttr.getMtime() .getMilliSeconds() : -1;	if (atime != -1 || mtime != -1) {	if (LOG.isDebugEnabled()) {	
set atime mtime 

private DirectoryListing listPaths(DFSClient dfsClient, String dirFileIdPath, byte[] startAfter) throws IOException {	DirectoryListing dlisting;	try {	dlisting = dfsClient.listPaths(dirFileIdPath, startAfter);	} catch (RemoteException e) {	IOException io = e.unwrapRemoteException();	if (!(io instanceof DirectoryListingStartAfterNotFoundException)) {	throw io;	}	
cookie couldn t be found utf do listing from beginning 

}	DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser());	if (dfsClient == null) {	response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);	return response;	}	READDIR3Request request;	try {	request = READDIR3Request.deserialize(xdr);	} catch (IOException e) {	
invalid readdir request 

}	READDIR3Request request;	try {	request = READDIR3Request.deserialize(xdr);	} catch (IOException e) {	return new READDIR3Response(Nfs3Status.NFS3ERR_INVAL);	}	FileHandle handle = request.getHandle();	long cookie = request.getCookie();	if (cookie < 0) {	
invalid readdir request with negative cookie 

} catch (IOException e) {	return new READDIR3Response(Nfs3Status.NFS3ERR_INVAL);	}	FileHandle handle = request.getHandle();	long cookie = request.getCookie();	if (cookie < 0) {	return new READDIR3Response(Nfs3Status.NFS3ERR_INVAL);	}	long count = request.getCount();	if (count <= 0) {	
nonpositive count in invalid readdir request 

FileHandle handle = request.getHandle();	long cookie = request.getCookie();	if (cookie < 0) {	return new READDIR3Response(Nfs3Status.NFS3ERR_INVAL);	}	long count = request.getCount();	if (count <= 0) {	return new READDIR3Response(Nfs3Status.NFS3_OK);	}	if (LOG.isDebugEnabled()) {	
nfs readdir fileid cookie count client 

if (LOG.isDebugEnabled()) {	}	HdfsFileStatus dirStatus;	DirectoryListing dlisting;	Nfs3FileAttributes postOpAttr;	long dotdotFileId = 0;	try {	String dirFileIdPath = Nfs3Utils.getFileIdPath(handle);	dirStatus = dfsClient.getFileInfo(dirFileIdPath);	if (dirStatus == null) {	
can t get path for fileid 

DirectoryListing dlisting;	Nfs3FileAttributes postOpAttr;	long dotdotFileId = 0;	try {	String dirFileIdPath = Nfs3Utils.getFileIdPath(handle);	dirStatus = dfsClient.getFileInfo(dirFileIdPath);	if (dirStatus == null) {	return new READDIR3Response(Nfs3Status.NFS3ERR_STALE);	}	if (!dirStatus.isDir()) {	
can t readdir for regular file fileid 

dirStatus = dfsClient.getFileInfo(dirFileIdPath);	if (dirStatus == null) {	return new READDIR3Response(Nfs3Status.NFS3ERR_STALE);	}	if (!dirStatus.isDir()) {	return new READDIR3Response(Nfs3Status.NFS3ERR_NOTDIR);	}	long cookieVerf = request.getCookieVerf();	if ((cookieVerf != 0) && (cookieVerf != dirStatus.getModificationTime())) {	if (aixCompatMode) {	
aix compatibility mode enabled ignoring cookieverf mismatches 

if (dirStatus == null) {	return new READDIR3Response(Nfs3Status.NFS3ERR_STALE);	}	if (!dirStatus.isDir()) {	return new READDIR3Response(Nfs3Status.NFS3ERR_NOTDIR);	}	long cookieVerf = request.getCookieVerf();	if ((cookieVerf != 0) && (cookieVerf != dirStatus.getModificationTime())) {	if (aixCompatMode) {	} else {	
cookieverf mismatch request cookieverf dir cookieverf 

byte[] startAfter;	if(cookie == 0 ) {	startAfter = HdfsFileStatus.EMPTY_NAME;	} else {	String inodeIdPath = Nfs3Utils.getFileIdPath(cookie);	startAfter = inodeIdPath.getBytes(Charset.forName("UTF-8"));	}	dlisting = listPaths(dfsClient, dirFileIdPath, startAfter);	postOpAttr = Nfs3Utils.getFileAttr(dfsClient, dirFileIdPath, iug);	if (postOpAttr == null) {	
can t get path for fileid 

} else {	String inodeIdPath = Nfs3Utils.getFileIdPath(cookie);	startAfter = inodeIdPath.getBytes(Charset.forName("UTF-8"));	}	dlisting = listPaths(dfsClient, dirFileIdPath, startAfter);	postOpAttr = Nfs3Utils.getFileAttr(dfsClient, dirFileIdPath, iug);	if (postOpAttr == null) {	return new READDIR3Response(Nfs3Status.NFS3ERR_STALE);	}	} catch (IOException e) {	
exception 

final NFSPROC3 nfsproc3 = NFSPROC3.fromValue(rpcCall.getProcedure());	int xid = rpcCall.getXid();	byte[] data = new byte[info.data().readableBytes()];	info.data().readBytes(data);	XDR xdr = new XDR(data);	XDR out = new XDR();	InetAddress client = ((InetSocketAddress) info.remoteAddress()) .getAddress();	Credentials credentials = rpcCall.getCredential();	if (nfsproc3 != NFSPROC3.NULL) {	if (credentials.getFlavor() != AuthFlavor.AUTH_SYS && credentials.getFlavor() != AuthFlavor.RPCSEC_GSS) {	
wrong rpc auth flavor is not auth sys or rpcsec gss 

ChannelBuffer buf = ChannelBuffers.wrappedBuffer(reply.asReadOnlyWrap() .buffer());	RpcResponse rsp = new RpcResponse(buf, info.remoteAddress());	RpcUtil.sendRpcResponse(ctx, rsp);	return;	}	}	if (!isIdempotent(rpcCall)) {	RpcCallCache.CacheEntry entry = rpcCallCache.checkOrAddToCache(client, xid);	if (entry != null) {	if (entry.isCompleted()) {	
sending the cached reply to retransmitted request 

return;	}	}	if (!isIdempotent(rpcCall)) {	RpcCallCache.CacheEntry entry = rpcCallCache.checkOrAddToCache(client, xid);	if (entry != null) {	if (entry.isCompleted()) {	RpcUtil.sendRpcResponse(ctx, entry.getResponse());	return;	} else {	
retransmitted request transaction still in progress 

========================= hadoop sample_7051 =========================

};	resourceManager.registerServiceListener(rmStateChangeListener);	resourceManager.init(configuration);	new Thread() {	public void run() {	resourceManager.start();	}	}.start();	boolean rmStarted = rmStartedSignal.await(60000L, TimeUnit.MILLISECONDS);	Assert.assertTrue("ResourceManager failed to start up.", rmStarted);	
resourcemanager rmadmin address 

public static void tearDownResourceManager() throws InterruptedException {	if (resourceManager != null) {	
stopping resourcemanager 

========================= hadoop sample_2603 =========================

String logFile = jhLogFiles[i].getPath().toString();	writer.append(new Text(logFile), new LongWritable(0));	} catch(Exception e) {	throw new IOException(e);	} finally {	if (writer != null) writer.close();	writer = null;	}	}	} catch(IOException ex) {	
filecreatedaemon failed 

private void updateJob(StringTokenizer tokens) throws IOException {	while(tokens.hasMoreTokens()) {	String t = tokens.nextToken();	String[] keyVal = getKeyValue(t);	if(keyVal.length < 2) continue;	if(keyVal[0].equals("JOBID")) {	if(JOBID == null) JOBID = new String(keyVal[1]);	else if(!JOBID.equals(keyVal[1])) {	
incorrect jobid expect 

private static void analyzeResult( FileSystem fs, int testType, long execTime, Path resFileName ) throws IOException {	
analyzing results 

}	} finally {	if(lines != null) lines.close();	if(in != null) in.close();	}	}	} finally {	if(writer != null) writer.close();	if(out != null) out.close();	}	
analyzing results done 

private static void cleanup(Configuration conf) throws IOException {	
cleaning up test files 

========================= hadoop sample_5579 =========================

public void testDeleteThrowsExceptionWithLeaseExistsErrorMessage() throws Exception {	
starting test 

NativeAzureFileSystem nfs = fs;	final String fullKey = nfs.pathToKey(nfs.makeAbsolute(path));	final AzureNativeFileSystemStore store = nfs.getStore();	final CountDownLatch leaseAttemptComplete = new CountDownLatch(1);	final CountDownLatch beginningDeleteAttempt = new CountDownLatch(1);	Thread t = new Thread() {	public void run() {	SelfRenewingLease lease = null;	try {	lease = store.acquireLease(fullKey);	
lease acquired 

final String fullKey = nfs.pathToKey(nfs.makeAbsolute(path));	final AzureNativeFileSystemStore store = nfs.getStore();	final CountDownLatch leaseAttemptComplete = new CountDownLatch(1);	final CountDownLatch beginningDeleteAttempt = new CountDownLatch(1);	Thread t = new Thread() {	public void run() {	SelfRenewingLease lease = null;	try {	lease = store.acquireLease(fullKey);	} catch (AzureException e) {	
lease acqusition thread unable to acquire lease 

} catch (InterruptedException e) {	Thread.currentThread().interrupt();	}	try {	Thread.sleep(SelfRenewingLease.LEASE_ACQUIRE_RETRY_INTERVAL * 3);	} catch (InterruptedException ex) {	Thread.currentThread().interrupt();	}	try {	if (lease != null){	
freeing lease 

try {	Thread.sleep(SelfRenewingLease.LEASE_ACQUIRE_RETRY_INTERVAL * 3);	} catch (InterruptedException ex) {	Thread.currentThread().interrupt();	}	try {	if (lease != null){	lease.free();	}	} catch (StorageException se) {	
unable to free lease 

========================= hadoop sample_6355 =========================

RecordReader<IntWritable,BytesWritable> reader = format.createRecordReader(split, context);	MapContext<IntWritable,BytesWritable,IntWritable,BytesWritable> mcontext = new MapContextImpl<IntWritable,BytesWritable,IntWritable,BytesWritable>(job.getConfiguration(), context.getTaskAttemptID(), reader, null, null, MapReduceTestUtil.createDummyReporter(), split);	reader.initialize(split, mcontext);	assertEquals("reader class is CombineFileRecordReader.", CombineFileRecordReader.class, reader.getClass());	try {	while (reader.nextKeyValue()) {	IntWritable key = reader.getCurrentKey();	BytesWritable value = reader.getCurrentValue();	assertNotNull("Value should not be null.", value);	final int k = key.get();	
read 

========================= hadoop sample_5650 =========================

public void start() {	
starting standby checkpoint thread checkpointing active nn at serving checkpoints at 

public void stop() throws IOException {	cancelAndPreventCheckpoints("Stopping checkpointer");	thread.setShouldRun(false);	thread.interrupt();	try {	thread.join();	} catch (InterruptedException e) {	
edit log tailer thread exited with an exception 

final long txid;	final NameNodeFile imageType;	namesystem.cpLockInterruptibly();	try {	assert namesystem.getEditLog().isOpenForRead() : "Standby Checkpointer should only attempt a checkpoint when " + "NN is in standby mode, but the edit logs are in an unexpected state";	FSImage img = namesystem.getFSImage();	long prevCheckpointTxId = img.getStorage().getMostRecentCheckpointTxId();	long thisCheckpointTxId = img.getCorrectLastAppliedOrWrittenTxId();	assert thisCheckpointTxId >= prevCheckpointTxId;	if (thisCheckpointTxId == prevCheckpointTxId) {	
a checkpoint was triggered but the standby node has not received any transactions since the last checkpoint at txid skipping 

imageType = NameNodeFile.IMAGE;	}	img.saveNamespace(namesystem, imageType, canceler);	txid = img.getStorage().getMostRecentCheckpointTxId();	assert txid == thisCheckpointTxId : "expected to save checkpoint at txid=" + thisCheckpointTxId + " but instead saved at txid=" + txid;	String outputDir = checkpointConf.getLegacyOivImageDir();	if (outputDir != null && !outputDir.isEmpty()) {	try {	img.saveLegacyOIVImage(namesystem, outputDir, canceler);	} catch (IOException ioe) {	
exception encountered while saving legacy oiv image continuing with other checkpointing steps 

}	try {	if (UserGroupInformation.isSecurityEnabled()) {	UserGroupInformation.getCurrentUser().checkTGTAndReloginFromKeytab();	}	final long now = monotonicNow();	final long uncheckpointed = countUncheckpointedTxns();	final long secsSinceLast = (now - lastCheckpointTime) / 1000;	boolean needCheckpoint = needRollbackCheckpoint;	if (needCheckpoint) {	
triggering a rollback fsimage for rolling upgrade 

try {	if (UserGroupInformation.isSecurityEnabled()) {	UserGroupInformation.getCurrentUser().checkTGTAndReloginFromKeytab();	}	final long now = monotonicNow();	final long uncheckpointed = countUncheckpointedTxns();	final long secsSinceLast = (now - lastCheckpointTime) / 1000;	boolean needCheckpoint = needRollbackCheckpoint;	if (needCheckpoint) {	} else if (uncheckpointed >= checkpointConf.getTxnCount()) {	
triggering checkpoint because there have been txns since the last checkpoint which exceeds the configured threshold 

UserGroupInformation.getCurrentUser().checkTGTAndReloginFromKeytab();	}	final long now = monotonicNow();	final long uncheckpointed = countUncheckpointedTxns();	final long secsSinceLast = (now - lastCheckpointTime) / 1000;	boolean needCheckpoint = needRollbackCheckpoint;	if (needCheckpoint) {	} else if (uncheckpointed >= checkpointConf.getTxnCount()) {	needCheckpoint = true;	} else if (secsSinceLast >= checkpointConf.getPeriod()) {	
triggering checkpoint because it has been seconds since the last checkpoint which exceeds the configured interval 

final long secsSinceLast = (now - lastCheckpointTime) / 1000;	boolean needCheckpoint = needRollbackCheckpoint;	if (needCheckpoint) {	} else if (uncheckpointed >= checkpointConf.getTxnCount()) {	needCheckpoint = true;	} else if (secsSinceLast >= checkpointConf.getPeriod()) {	needCheckpoint = true;	}	synchronized (cancelLock) {	if (now < preventCheckpointsUntil) {	
but skipping this checkpoint since we are about to failover 

}	if (needCheckpoint) {	doCheckpoint();	if (needRollbackCheckpoint && namesystem.getFSImage().hasRollbackFSImage()) {	namesystem.setCreatedRollbackImages(true);	namesystem.setNeedRollbackFsImage(false);	}	lastCheckpointTime = now;	}	} catch (SaveNamespaceCancelledException ce) {	
checkpoint was cancelled 

doCheckpoint();	if (needRollbackCheckpoint && namesystem.getFSImage().hasRollbackFSImage()) {	namesystem.setCreatedRollbackImages(true);	namesystem.setNeedRollbackFsImage(false);	}	lastCheckpointTime = now;	}	} catch (SaveNamespaceCancelledException ce) {	canceledCount++;	} catch (InterruptedException ie) {	
interrupted during checkpointing 

namesystem.setCreatedRollbackImages(true);	namesystem.setNeedRollbackFsImage(false);	}	lastCheckpointTime = now;	}	} catch (SaveNamespaceCancelledException ce) {	canceledCount++;	} catch (InterruptedException ie) {	continue;	} catch (Throwable t) {	
exception in docheckpoint 

========================= hadoop sample_8062 =========================

private void doCrossFilter(HttpServletRequest req, HttpServletResponse res) {	String originsList = encodeHeader(req.getHeader(ORIGIN));	if (!isCrossOrigin(originsList)) {	if(LOG.isDebugEnabled()) {	
header origin is null returning 

private void doCrossFilter(HttpServletRequest req, HttpServletResponse res) {	String originsList = encodeHeader(req.getHeader(ORIGIN));	if (!isCrossOrigin(originsList)) {	if(LOG.isDebugEnabled()) {	}	return;	}	if (!areOriginsAllowed(originsList)) {	if(LOG.isDebugEnabled()) {	
header origins not allowed returning 

return;	}	if (!areOriginsAllowed(originsList)) {	if(LOG.isDebugEnabled()) {	}	return;	}	String accessControlRequestMethod = req.getHeader(ACCESS_CONTROL_REQUEST_METHOD);	if (!isMethodAllowed(accessControlRequestMethod)) {	if(LOG.isDebugEnabled()) {	
access control method not allowed returning 

}	String accessControlRequestMethod = req.getHeader(ACCESS_CONTROL_REQUEST_METHOD);	if (!isMethodAllowed(accessControlRequestMethod)) {	if(LOG.isDebugEnabled()) {	}	return;	}	String accessControlRequestHeaders = req.getHeader(ACCESS_CONTROL_REQUEST_HEADERS);	if (!areHeadersAllowed(accessControlRequestHeaders)) {	if(LOG.isDebugEnabled()) {	
access control headers not allowed returning 

}	return;	}	String accessControlRequestHeaders = req.getHeader(ACCESS_CONTROL_REQUEST_HEADERS);	if (!areHeadersAllowed(accessControlRequestHeaders)) {	if(LOG.isDebugEnabled()) {	}	return;	}	if(LOG.isDebugEnabled()) {	
completed cross origin filter checks populating HttpServletResponse 

private void initializeAllowedMethods(FilterConfig filterConfig) {	String allowedMethodsConfig = filterConfig.getInitParameter(ALLOWED_METHODS);	if (allowedMethodsConfig == null) {	allowedMethodsConfig = ALLOWED_METHODS_DEFAULT;	}	allowedMethods.addAll( Arrays.asList(allowedMethodsConfig.trim().split("\\s*,\\s*")));	
allowed methods 

private void initializeAllowedHeaders(FilterConfig filterConfig) {	String allowedHeadersConfig = filterConfig.getInitParameter(ALLOWED_HEADERS);	if (allowedHeadersConfig == null) {	allowedHeadersConfig = ALLOWED_HEADERS_DEFAULT;	}	allowedHeaders.addAll( Arrays.asList(allowedHeadersConfig.trim().split("\\s*,\\s*")));	
allowed headers 

private void initializeAllowedOrigins(FilterConfig filterConfig) {	String allowedOriginsConfig = filterConfig.getInitParameter(ALLOWED_ORIGINS);	if (allowedOriginsConfig == null) {	allowedOriginsConfig = ALLOWED_ORIGINS_DEFAULT;	}	allowedOrigins.addAll( Arrays.asList(allowedOriginsConfig.trim().split("\\s*,\\s*")));	allowAllOrigins = allowedOrigins.contains("*");	
allowed origins 

private void initializeAllowedOrigins(FilterConfig filterConfig) {	String allowedOriginsConfig = filterConfig.getInitParameter(ALLOWED_ORIGINS);	if (allowedOriginsConfig == null) {	allowedOriginsConfig = ALLOWED_ORIGINS_DEFAULT;	}	allowedOrigins.addAll( Arrays.asList(allowedOriginsConfig.trim().split("\\s*,\\s*")));	allowAllOrigins = allowedOrigins.contains("*");	
allow all origins 

private void initializeMaxAge(FilterConfig filterConfig) {	maxAge = filterConfig.getInitParameter(MAX_AGE);	if (maxAge == null) {	maxAge = MAX_AGE_DEFAULT;	}	
max age 

========================= hadoop sample_3784 =========================

private void disconnect(FTPClient client) throws IOException {	if (client != null) {	if (!client.isConnected()) {	throw new FTPException("Client not connected");	}	boolean logoutSuccess = client.logout();	client.disconnect();	if (!logoutSuccess) {	
logout failed while disconnecting error code 

========================= hadoop sample_4104 =========================

assertTrue("Data directory does not contain any blocks or there was an " + "IO error", metaFiles != null && !metaFiles.isEmpty());	File metaFile = metaFiles.get(0);	RandomAccessFile file = new RandomAccessFile(metaFile, "rw");	FileChannel channel = file.getChannel();	long position = channel.size() - 2;	int length = 2;	byte[] buffer = new byte[length];	random.nextBytes(buffer);	channel.write(ByteBuffer.wrap(buffer), position);	file.close();	
deliberately corrupting file at offset length 

channel.write(ByteBuffer.wrap(buffer), position);	file.close();	try {	util.checkFiles(fs, "/srcdat10");	} catch (BlockMissingException e) {	System.out.println("Received BlockMissingException as expected.");	} catch (IOException e) {	assertTrue("Corrupted replicas not handled properly. Expecting BlockMissingException " + " but received IOException " + e, false);	}	badFiles = namenode.getNamesystem().listCorruptFileBlocks("/", null);	
namenode has bad files 

assertTrue("Data directory does not contain any blocks or there was an " + "IO error", metaFiles != null && !metaFiles.isEmpty());	File metaFile = metaFiles.get(0);	RandomAccessFile file = new RandomAccessFile(metaFile, "rw");	FileChannel channel = file.getChannel();	long position = channel.size() - 2;	int length = 2;	byte[] buffer = new byte[length];	random.nextBytes(buffer);	channel.write(ByteBuffer.wrap(buffer), position);	file.close();	
deliberately corrupting file at offset length 

channel.write(ByteBuffer.wrap(buffer), position);	file.close();	try {	util.checkFiles(fs, "/srcdat10");	} catch (BlockMissingException e) {	System.out.println("Received BlockMissingException as expected.");	} catch (IOException e) {	assertTrue("Corrupted replicas not handled properly. " + "Expecting BlockMissingException " + " but received IOException " + e, false);	}	badFiles = cluster.getNameNode().getNamesystem(). listCorruptFileBlocks("/", null);	
namenode has bad files 

System.out.println("Received BlockMissingException as expected.");	} catch (IOException e) {	assertTrue("Corrupted replicas not handled properly. " + "Expecting BlockMissingException " + " but received IOException " + e, false);	}	badFiles = cluster.getNameNode().getNamesystem(). listCorruptFileBlocks("/", null);	assertTrue("Namenode has " + badFiles.size() + " bad files. Expecting 1.", badFiles.size() == 1);	cluster.restartNameNode(0);	fs = cluster.getFileSystem();	while (!cluster.getNameNode().namesystem.getBlockManager() .isPopulatingReplQueues()) {	try {	
waiting for replication queues 

}	}	try {	util.checkFiles(fs, "/srcdat10");	} catch (BlockMissingException e) {	System.out.println("Received BlockMissingException as expected.");	} catch (IOException e) {	assertTrue("Corrupted replicas not handled properly. " + "Expecting BlockMissingException " + " but received IOException " + e, false);	}	badFiles = cluster.getNameNode().getNamesystem(). listCorruptFileBlocks("/", null);	
namenode has bad files 

assertTrue(numCorrupt == 0);	String bpid = cluster.getNamesystem().getBlockPoolId();	for (int i = 0; i < 4; i++) {	for (int j = 0; j <= 1; j++) {	File storageDir = cluster.getInstanceStorageDir(i, j);	File data_dir = MiniDFSCluster.getFinalizedDir(storageDir, bpid);	List<File> metadataFiles = MiniDFSCluster.getAllBlockMetadataFiles( data_dir);	if (metadataFiles == null) continue;	for (File metadataFile : metadataFiles) {	File blockFile = Block.metaToBlockFile(metadataFile);	
deliberately removing file 

String bpid = cluster.getNamesystem().getBlockPoolId();	for (int i = 0; i < 4; i++) {	for (int j = 0; j <= 1; j++) {	File storageDir = cluster.getInstanceStorageDir(i, j);	File data_dir = MiniDFSCluster.getFinalizedDir(storageDir, bpid);	List<File> metadataFiles = MiniDFSCluster.getAllBlockMetadataFiles( data_dir);	if (metadataFiles == null) continue;	for (File metadataFile : metadataFiles) {	File blockFile = Block.metaToBlockFile(metadataFile);	assertTrue("Cannot remove file.", blockFile.delete());	
deliberately removing file 

int count = 0;	corruptFileBlocks = namenode.getNamesystem(). listCorruptFileBlocks("/corruptData", null);	numCorrupt = corruptFileBlocks.size();	while (numCorrupt < 3) {	Thread.sleep(1000);	corruptFileBlocks = namenode.getNamesystem() .listCorruptFileBlocks("/corruptData", null);	numCorrupt = corruptFileBlocks.size();	count++;	if (count > 30) break;	}	
namenode has bad files 

private int countPaths(RemoteIterator<Path> iter) throws IOException {	int i = 0;	while (iter.hasNext()) {	
path 

int numCorrupt = countPaths(corruptFileBlocks);	assertTrue(numCorrupt == 0);	String bpid = cluster.getNamesystem().getBlockPoolId();	for (int i = 0; i < 2; i++) {	File storageDir = cluster.getInstanceStorageDir(0, i);	File data_dir = MiniDFSCluster.getFinalizedDir(storageDir, bpid);	List<File> metadataFiles = MiniDFSCluster.getAllBlockMetadataFiles( data_dir);	if (metadataFiles == null) continue;	for (File metadataFile : metadataFiles) {	File blockFile = Block.metaToBlockFile(metadataFile);	
deliberately removing file 

assertTrue(numCorrupt == 0);	String bpid = cluster.getNamesystem().getBlockPoolId();	for (int i = 0; i < 2; i++) {	File storageDir = cluster.getInstanceStorageDir(0, i);	File data_dir = MiniDFSCluster.getFinalizedDir(storageDir, bpid);	List<File> metadataFiles = MiniDFSCluster.getAllBlockMetadataFiles( data_dir);	if (metadataFiles == null) continue;	for (File metadataFile : metadataFiles) {	File blockFile = Block.metaToBlockFile(metadataFile);	assertTrue("Cannot remove file.", blockFile.delete());	
deliberately removing file 

int count = 0;	corruptFileBlocks = dfs.listCorruptFileBlocks(new Path("/corruptData"));	numCorrupt = countPaths(corruptFileBlocks);	while (numCorrupt < 3) {	Thread.sleep(1000);	corruptFileBlocks = dfs.listCorruptFileBlocks(new Path("/corruptData"));	numCorrupt = countPaths(corruptFileBlocks);	count++;	if (count > 30) break;	}	
namenode has bad files 

util.createFiles(fs, "/srcdat2", (short) 1);	util.waitReplication(fs, "/srcdat2", (short) 1);	final NameNode namenode = cluster.getNameNode();	Collection<FSNamesystem.CorruptFileBlockInfo> badFiles = namenode. getNamesystem().listCorruptFileBlocks("/srcdat2", null);	assertTrue("Namenode has " + badFiles.size() + " corrupt files. Expecting none.", badFiles.size() == 0);	final String bpid = cluster.getNamesystem().getBlockPoolId();	for (int i=0; i<4; i++) {	for (int j=0; j<=1; j++) {	File storageDir = cluster.getInstanceStorageDir(i, j);	File data_dir = MiniDFSCluster.getFinalizedDir(storageDir, bpid);	
removing files from 

if (metadataFiles == null) continue;	for (File metadataFile : metadataFiles) {	File blockFile = Block.metaToBlockFile(metadataFile);	assertTrue("Cannot remove file.", blockFile.delete());	assertTrue("Cannot remove file.", metadataFile.delete());	}	}	}	DataNode dn = cluster.getDataNodes().get(0);	DataNodeTestUtils.runDirectoryScanner(dn);	
restarting datanode to trigger blockpoolslicescanner 

assertTrue("Cannot remove file.", metadataFile.delete());	}	}	}	DataNode dn = cluster.getDataNodes().get(0);	DataNodeTestUtils.runDirectoryScanner(dn);	cluster.restartDataNodes();	cluster.waitActive();	badFiles = namenode.getNamesystem().listCorruptFileBlocks("/srcdat2", null);	while (badFiles.size() < maxCorruptFileBlocks) {	
of corrupt files is 

DataNode dn = cluster.getDataNodes().get(0);	DataNodeTestUtils.runDirectoryScanner(dn);	cluster.restartDataNodes();	cluster.waitActive();	badFiles = namenode.getNamesystem().listCorruptFileBlocks("/srcdat2", null);	while (badFiles.size() < maxCorruptFileBlocks) {	Thread.sleep(10000);	badFiles = namenode.getNamesystem(). listCorruptFileBlocks("/srcdat2", null);	}	badFiles = namenode.getNamesystem(). listCorruptFileBlocks("/srcdat2", null);	
namenode has bad files 

int numCorrupt = countPaths(corruptFileBlocks);	assertTrue(numCorrupt == 0);	String bpid = cluster.getNamesystem().getBlockPoolId();	for (int i = 0; i < 2; i++) {	File storageDir = cluster.getInstanceStorageDir(0, i);	File data_dir = MiniDFSCluster.getFinalizedDir(storageDir, bpid);	List<File> metadataFiles = MiniDFSCluster .getAllBlockMetadataFiles(data_dir);	if (metadataFiles == null) continue;	for (File metadataFile : metadataFiles) {	File blockFile = Block.metaToBlockFile(metadataFile);	
deliberately removing file 

assertTrue(numCorrupt == 0);	String bpid = cluster.getNamesystem().getBlockPoolId();	for (int i = 0; i < 2; i++) {	File storageDir = cluster.getInstanceStorageDir(0, i);	File data_dir = MiniDFSCluster.getFinalizedDir(storageDir, bpid);	List<File> metadataFiles = MiniDFSCluster .getAllBlockMetadataFiles(data_dir);	if (metadataFiles == null) continue;	for (File metadataFile : metadataFiles) {	File blockFile = Block.metaToBlockFile(metadataFile);	assertTrue("Cannot remove file.", blockFile.delete());	
deliberately removing file 

int count = 0;	corruptFileBlocks = dfs.listCorruptFileBlocks(new Path("corruptData"));	numCorrupt = countPaths(corruptFileBlocks);	while (numCorrupt < 3) {	Thread.sleep(1000);	corruptFileBlocks = dfs.listCorruptFileBlocks(new Path("corruptData"));	numCorrupt = countPaths(corruptFileBlocks);	count++;	if (count > 30) break;	}	
namenode has bad files 

========================= hadoop sample_7378 =========================

public ReadaheadRequest submitReadahead( String identifier, FileDescriptor fd, long off, long len) {	ReadaheadRequestImpl req = new ReadaheadRequestImpl( identifier, fd, off, len);	pool.execute(req);	if (LOG.isTraceEnabled()) {	
submit readahead 

this.len = len;	}	public void run() {	if (canceled) return;	try {	NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(identifier, fd, off, len, POSIX_FADV_WILLNEED);	} catch (IOException ioe) {	if (canceled) {	return;	}	
failed readahead on 

========================= hadoop sample_3946 =========================

Preconditions .checkState(length >= 0, "Length must be set to a non-negative value");	BlockReader reader = tryToCreateExternalBlockReader();	if (reader != null) {	return reader;	}	final ShortCircuitConf scConf = conf.getShortCircuitConf();	if (scConf.isShortCircuitLocalReads() && allowShortCircuitLocalReads) {	if (clientContext.getUseLegacyBlockReaderLocal()) {	reader = getLegacyBlockReaderLocal();	if (reader != null) {	
returning new legacy block reader local 

final ShortCircuitConf scConf = conf.getShortCircuitConf();	if (scConf.isShortCircuitLocalReads() && allowShortCircuitLocalReads) {	if (clientContext.getUseLegacyBlockReaderLocal()) {	reader = getLegacyBlockReaderLocal();	if (reader != null) {	return reader;	}	} else {	reader = getBlockReaderLocal();	if (reader != null) {	
returning new block reader local 

} else {	reader = getBlockReaderLocal();	if (reader != null) {	return reader;	}	}	}	if (scConf.isDomainSocketDataTraffic()) {	reader = getRemoteBlockReaderFromDomain();	if (reader != null) {	
returning new remote block reader using unix domain socket on 

for (Class<? extends ReplicaAccessorBuilder> cls : clses) {	try {	ByteArrayDataOutput bado = ByteStreams.newDataOutput();	token.write(bado);	byte tokenBytes[] = bado.toByteArray();	Constructor<? extends ReplicaAccessorBuilder> ctor = cls.getConstructor();	ReplicaAccessorBuilder builder = ctor.newInstance();	long visibleLength = startOffset + length;	ReplicaAccessor accessor = builder. setAllowShortCircuitReads(allowShortCircuitLocalReads). setBlock(block.getBlockId(), block.getBlockPoolId()). setGenerationStamp(block.getGenerationStamp()). setBlockAccessToken(tokenBytes). setClientName(clientName). setConfiguration(configuration). setFileName(fileName). setVerifyChecksum(verifyChecksum). setVisibleLength(visibleLength). build();	if (accessor == null) {	
no replicaaccessor created by 

byte tokenBytes[] = bado.toByteArray();	Constructor<? extends ReplicaAccessorBuilder> ctor = cls.getConstructor();	ReplicaAccessorBuilder builder = ctor.newInstance();	long visibleLength = startOffset + length;	ReplicaAccessor accessor = builder. setAllowShortCircuitReads(allowShortCircuitLocalReads). setBlock(block.getBlockId(), block.getBlockPoolId()). setGenerationStamp(block.getGenerationStamp()). setBlockAccessToken(tokenBytes). setClientName(clientName). setConfiguration(configuration). setFileName(fileName). setVerifyChecksum(verifyChecksum). setVisibleLength(visibleLength). build();	if (accessor == null) {	} else {	return new ExternalBlockReader(accessor, visibleLength, startOffset);	}	} catch (Throwable t) {	
failed to construct new object of type 

private BlockReader getLegacyBlockReaderLocal() throws IOException {	
trying to construct blockreaderlocallegacy 

private BlockReader getLegacyBlockReaderLocal() throws IOException {	if (!DFSUtilClient.isLocalAddress(inetSocketAddress)) {	
can t construct blockreaderlocallegacy because the address is not local 

private BlockReader getLegacyBlockReaderLocal() throws IOException {	if (!DFSUtilClient.isLocalAddress(inetSocketAddress)) {	return null;	}	if (clientContext.getDisableLegacyBlockReaderLocal()) {	
can t construct blockreaderlocallegacy because disablelegacyblockreaderlocal is set 

try {	return BlockReaderLocalLegacy.newBlockReader(conf, userGroupInformation, configuration, fileName, block, token, datanode, startOffset, length, storageType, tracer);	} catch (RemoteException remoteException) {	ioe = remoteException.unwrapRemoteException( InvalidToken.class, AccessControlException.class);	} catch (IOException e) {	ioe = e;	}	if ((!(ioe instanceof AccessControlException)) && isSecurityException(ioe)) {	throw ioe;	}	
error creating legacy blockreaderlocal disabling legacy local reads 

private BlockReader getBlockReaderLocal() throws InvalidToken {	
trying to construct a blockreaderlocal for short circuit reads 

private BlockReader getBlockReaderLocal() throws InvalidToken {	if (pathInfo == null) {	pathInfo = clientContext.getDomainSocketFactory() .getPathInfo(inetSocketAddress, conf.getShortCircuitConf());	}	if (!pathInfo.getPathState().getUsableForShortCircuit()) {	
is not usable for short circuit giving up on blockreaderlocal 

pathInfo = clientContext.getDomainSocketFactory() .getPathInfo(inetSocketAddress, conf.getShortCircuitConf());	}	if (!pathInfo.getPathState().getUsableForShortCircuit()) {	return null;	}	ShortCircuitCache cache = clientContext.getShortCircuitCache();	ExtendedBlockId key = new ExtendedBlockId(block.getBlockId(), block.getBlockPoolId());	ShortCircuitReplicaInfo info = cache.fetchOrCreate(key, this);	InvalidToken exc = info.getInvalidTokenException();	if (exc != null) {	
got invalidtoken exception while trying to construct blockreaderlocal via 

return null;	}	ShortCircuitCache cache = clientContext.getShortCircuitCache();	ExtendedBlockId key = new ExtendedBlockId(block.getBlockId(), block.getBlockPoolId());	ShortCircuitReplicaInfo info = cache.fetchOrCreate(key, this);	InvalidToken exc = info.getInvalidTokenException();	if (exc != null) {	throw exc;	}	if (info.getReplica() == null) {	
failed to get shortcircuitreplica cannot construct blockreaderlocal via 

public ShortCircuitReplicaInfo createShortCircuitReplicaInfo() {	if (createShortCircuitReplicaInfoCallback != null) {	ShortCircuitReplicaInfo info = createShortCircuitReplicaInfoCallback.createShortCircuitReplicaInfo();	if (info != null) return info;	}	
trying to create shortcircuitreplicainfo 

curPeer = nextDomainPeer();	if (curPeer == null) break;	if (curPeer.fromCache) remainingCacheTries--;	DomainPeer peer = (DomainPeer)curPeer.peer;	Slot slot = null;	ShortCircuitCache cache = clientContext.getShortCircuitCache();	try {	MutableBoolean usedPeer = new MutableBoolean(false);	slot = cache.allocShmSlot(datanode, peer, usedPeer, new ExtendedBlockId(block.getBlockId(), block.getBlockPoolId()), clientName);	if (usedPeer.booleanValue()) {	
allocshmslot used up our previous socket allocating a new one 

peer = (DomainPeer)curPeer.peer;	}	ShortCircuitReplicaInfo info = requestFileDescriptors(peer, slot);	clientContext.getPeerCache().put(datanode, peer);	return info;	} catch (IOException e) {	if (slot != null) {	cache.freeSlot(slot);	}	if (curPeer.fromCache) {	
closing stale domain peer 

ShortCircuitReplicaInfo info = requestFileDescriptors(peer, slot);	clientContext.getPeerCache().put(datanode, peer);	return info;	} catch (IOException e) {	if (slot != null) {	cache.freeSlot(slot);	}	if (curPeer.fromCache) {	IOUtilsClient.cleanup(LOG, peer);	} else {	
i o error requesting file descriptors disabling domain socket 

DomainSocket sock = peer.getDomainSocket();	failureInjector.injectRequestFileDescriptorsFailure();	switch (resp.getStatus()) {	case SUCCESS: byte buf[] = new byte[1];	FileInputStream[] fis = new FileInputStream[2];	sock.recvFileInputStreams(fis, buf, 0, buf.length);	ShortCircuitReplica replica = null;	try {	ExtendedBlockId key = new ExtendedBlockId(block.getBlockId(), block.getBlockPoolId());	if (buf[0] == USE_RECEIPT_VERIFICATION.getNumber()) {	
sending receipt verification byte for slot 

sock.recvFileInputStreams(fis, buf, 0, buf.length);	ShortCircuitReplica replica = null;	try {	ExtendedBlockId key = new ExtendedBlockId(block.getBlockId(), block.getBlockPoolId());	if (buf[0] == USE_RECEIPT_VERIFICATION.getNumber()) {	sock.getOutputStream().write(0);	}	replica = new ShortCircuitReplica(key, fis[0], fis[1], cache, Time.monotonicNow(), slot);	return new ShortCircuitReplicaInfo(replica);	} catch (IOException e) {	
error creating shortcircuitreplica 

replica = new ShortCircuitReplica(key, fis[0], fis[1], cache, Time.monotonicNow(), slot);	return new ShortCircuitReplicaInfo(replica);	} catch (IOException e) {	return null;	} finally {	if (replica == null) {	IOUtilsClient.cleanup(DFSClient.LOG, fis[0], fis[1]);	}	}	case ERROR_UNSUPPORTED: if (!resp.hasShortCircuitAccessVersion()) {	
short circuit read access is disabled for datanode reason 

} catch (IOException e) {	return null;	} finally {	if (replica == null) {	IOUtilsClient.cleanup(DFSClient.LOG, fis[0], fis[1]);	}	}	case ERROR_UNSUPPORTED: if (!resp.hasShortCircuitAccessVersion()) {	clientContext.getDomainSocketFactory() .disableShortCircuitForPath(pathInfo.getPath());	} else {	
short circuit read access for the file is disabled for datanode reason 

}	}	case ERROR_UNSUPPORTED: if (!resp.hasShortCircuitAccessVersion()) {	clientContext.getDomainSocketFactory() .disableShortCircuitForPath(pathInfo.getPath());	} else {	}	return null;	case ERROR_ACCESS_TOKEN: String msg = "access control error while " + "attempting to set up short-circuit access to " + fileName + resp.getMessage();	LOG.debug("{}:{}", this, msg);	return new ShortCircuitReplicaInfo(new InvalidToken(msg));	
unknown response code while attempting to set up short circuit access disabling short circuit read for datanode temporarily 

private BlockReader getRemoteBlockReaderFromDomain() throws IOException {	if (pathInfo == null) {	pathInfo = clientContext.getDomainSocketFactory() .getPathInfo(inetSocketAddress, conf.getShortCircuitConf());	}	if (!pathInfo.getPathState().getUsableForDataTransfer()) {	
not trying to create a remote block reader because the unix domain socket at is not usable 

private BlockReader getRemoteBlockReaderFromDomain() throws IOException {	if (pathInfo == null) {	pathInfo = clientContext.getDomainSocketFactory() .getPathInfo(inetSocketAddress, conf.getShortCircuitConf());	}	if (!pathInfo.getPathState().getUsableForDataTransfer()) {	return null;	}	
trying to create a remote block reader from the unix domain socket at 

if (curPeer == null) break;	if (curPeer.fromCache) remainingCacheTries--;	DomainPeer peer = (DomainPeer)curPeer.peer;	BlockReader blockReader = null;	try {	blockReader = getRemoteBlockReader(peer);	return blockReader;	} catch (IOException ioe) {	IOUtilsClient.cleanup(LOG, peer);	if (isSecurityException(ioe)) {	
got security exception while constructing a remote block reader from the unix domain socket at 

BlockReader blockReader = null;	try {	blockReader = getRemoteBlockReader(peer);	return blockReader;	} catch (IOException ioe) {	IOUtilsClient.cleanup(LOG, peer);	if (isSecurityException(ioe)) {	throw ioe;	}	if (curPeer.fromCache) {	
closed potentially stale domain peer 

try {	blockReader = getRemoteBlockReader(peer);	return blockReader;	} catch (IOException ioe) {	IOUtilsClient.cleanup(LOG, peer);	if (isSecurityException(ioe)) {	throw ioe;	}	if (curPeer.fromCache) {	} else {	
i o error constructing remote block reader disabling domain socket 

private BlockReader getRemoteBlockReaderFromTcp() throws IOException {	
trying to create a remote block reader from a tcp socket 

BlockReaderPeer curPeer = null;	Peer peer = null;	try {	curPeer = nextTcpPeer();	if (curPeer.fromCache) remainingCacheTries--;	peer = curPeer.peer;	blockReader = getRemoteBlockReader(peer);	return blockReader;	} catch (IOException ioe) {	if (isSecurityException(ioe)) {	
got security exception while constructing a remote block reader from 

curPeer = nextTcpPeer();	if (curPeer.fromCache) remainingCacheTries--;	peer = curPeer.peer;	blockReader = getRemoteBlockReader(peer);	return blockReader;	} catch (IOException ioe) {	if (isSecurityException(ioe)) {	throw ioe;	}	if ((curPeer != null) && curPeer.fromCache) {	
closed potentially stale remote peer 

if (curPeer.fromCache) remainingCacheTries--;	peer = curPeer.peer;	blockReader = getRemoteBlockReader(peer);	return blockReader;	} catch (IOException ioe) {	if (isSecurityException(ioe)) {	throw ioe;	}	if ((curPeer != null) && curPeer.fromCache) {	} else {	
i o error constructing remote block reader 

private BlockReaderPeer nextDomainPeer() {	if (remainingCacheTries > 0) {	Peer peer = clientContext.getPeerCache().get(datanode, true);	if (peer != null) {	
nextdomainpeer reusing existing peer 

private BlockReaderPeer nextTcpPeer() throws IOException {	if (remainingCacheTries > 0) {	Peer peer = clientContext.getPeerCache().get(datanode, false);	if (peer != null) {	
nexttcppeer reusing existing peer 

private BlockReaderPeer nextTcpPeer() throws IOException {	if (remainingCacheTries > 0) {	Peer peer = clientContext.getPeerCache().get(datanode, false);	if (peer != null) {	return new BlockReaderPeer(peer, true);	}	}	try {	Peer peer = remotePeerFactory.newConnectedPeer(inetSocketAddress, token, datanode);	
nexttcppeer created newconnectedpeer 

if (remainingCacheTries > 0) {	Peer peer = clientContext.getPeerCache().get(datanode, false);	if (peer != null) {	return new BlockReaderPeer(peer, true);	}	}	try {	Peer peer = remotePeerFactory.newConnectedPeer(inetSocketAddress, token, datanode);	return new BlockReaderPeer(peer, false);	} catch (IOException e) {	
nexttcppeer failed to create newconnectedpeer connected to 

========================= hadoop sample_7006 =========================

public void write(DataOutput out) throws IOException {	
writing containertokenidentifier to rpc layer 

========================= hadoop sample_2204 =========================

public void testContainerResourceIncreaseIsSynchronizedWithRMResync() throws IOException, InterruptedException, YarnException {	NodeManager nm = new TestNodeManager4();	YarnConfiguration conf = createNMConfig();	conf.setBoolean( YarnConfiguration.RM_WORK_PRESERVING_RECOVERY_ENABLED, true);	nm.init(conf);	nm.start();	((TestNodeManager4)nm).startContainer();	((TestNodeManager4)nm).updateContainerResource();	
sending out resync event 

public void startContainer() throws IOException, InterruptedException, YarnException {	
start a container and wait until it is in running state 

public void updateContainerResource() throws InterruptedException {	
increase a container resource in a separate thread 

========================= hadoop sample_1582 =========================

if (bpRegistration == null) {	return;	}	DatanodeInfo[] dnArr = {new DatanodeInfoBuilder() .setNodeID(bpRegistration).build()};	String[] uuids = { storageUuid };	StorageType[] types = { storageType };	LocatedBlock[] locatedBlock = { new LocatedBlock(block, dnArr, uuids, types) };	try {	bpNamenode.reportBadBlocks(locatedBlock);	} catch (RemoteException re) {	
reportbadblock encountered remoteexception for block 

========================= hadoop sample_7905 =========================

public void tearDown() throws Exception {	LOG.debug("== Tear down. ==");	if (ms != null) {	try {	ms.destroy();	} catch (Exception e) {	
failed to destroy tables in teardown 

} else if (implementation == MetadataStoreListFilesIterator.class) {	iterator = new MetadataStoreListFilesIterator(ms, rootMeta, false);	} else {	throw new UnsupportedOperationException("Unrecognized class");	}	final Set<String> actual = new HashSet<>();	while (iterator.hasNext()) {	final Path p = iterator.next().getPath();	actual.add(Path.getPathWithoutSchemeAndAuthority(p).toString());	}	
we got by iterating descendantsiterator 

========================= hadoop sample_5938 =========================

svc = createDummyHAService();	hm = new HealthMonitor(conf, svc) {	protected HAServiceProtocol createProxy() throws IOException {	createProxyCount.incrementAndGet();	if (throwOOMEOnCreate) {	throw new OutOfMemoryError("oome");	}	return super.createProxy();	}	};	
starting health monitor 

hm = new HealthMonitor(conf, svc) {	protected HAServiceProtocol createProxy() throws IOException {	createProxyCount.incrementAndGet();	if (throwOOMEOnCreate) {	throw new OutOfMemoryError("oome");	}	return super.createProxy();	}	};	hm.start();	
waiting for healthy signal 

public void testMonitor() throws Exception {	
mocking bad health check waiting for unhealthy 

public void testMonitor() throws Exception {	svc.isHealthy = false;	waitForState(hm, HealthMonitor.State.SERVICE_UNHEALTHY);	
returning to healthy state waiting for healthy 

public void testMonitor() throws Exception {	svc.isHealthy = false;	waitForState(hm, HealthMonitor.State.SERVICE_UNHEALTHY);	svc.isHealthy = true;	waitForState(hm, HealthMonitor.State.SERVICE_HEALTHY);	
returning an ioexception as if node went down 

svc.isHealthy = false;	waitForState(hm, HealthMonitor.State.SERVICE_UNHEALTHY);	svc.isHealthy = true;	waitForState(hm, HealthMonitor.State.SERVICE_HEALTHY);	int countBefore = createProxyCount.get();	svc.actUnreachable = true;	waitForState(hm, HealthMonitor.State.SERVICE_NOT_RESPONDING);	while (createProxyCount.get() < countBefore + 3) {	Thread.sleep(10);	}	
returning to healthy state waiting for healthy 

public void testHealthMonitorDies() throws Exception {	
mocking rte in health monitor waiting for failed 

public void testCallbackThrowsRTE() throws Exception {	hm.addCallback(new Callback() {	public void enteredState(State newState) {	throw new RuntimeException("Injected RTE");	}	});	
mocking bad health check waiting for unhealthy 

========================= hadoop sample_3100 =========================

private FSQueue createNewQueues(FSQueueType queueType, FSParentQueue topParent, List<String> newQueueNames) {	AllocationConfiguration queueConf = scheduler.getAllocationConfiguration();	Iterator<String> i = newQueueNames.iterator();	FSParentQueue parent = topParent;	FSQueue queue = null;	while (i.hasNext()) {	FSParentQueue newParent = null;	String queueName = i.next();	SchedulingPolicy childPolicy = scheduler.getAllocationConfiguration(). getSchedulingPolicy(queueName);	if (!parent.getPolicy().isChildPolicyAllowed(childPolicy)) {	
can t create queue 

SchedulingPolicy childPolicy = scheduler.getAllocationConfiguration(). getSchedulingPolicy(queueName);	if (!parent.getPolicy().isChildPolicyAllowed(childPolicy)) {	return null;	}	if (!i.hasNext() && (queueType != FSQueueType.PARENT)) {	FSLeafQueue leafQueue = new FSLeafQueue(queueName, scheduler, parent);	leafQueues.add(leafQueue);	queue = leafQueue;	} else {	if (childPolicy instanceof FifoPolicy) {	
can t create queue since is only for leaf queues 

public void updateAllocationConfiguration(AllocationConfiguration queueConf) {	synchronized (queues) {	if (!rootQueue.verifyAndSetPolicyFromConf(queueConf)) {	
setting scheduling policies for existing queues failed 

========================= hadoop sample_959 =========================

public void init() throws ServiceException {	int threads = getServiceConfig().getInt(CONF_THREADS, 5);	scheduler = new ScheduledThreadPoolExecutor(threads);	
scheduler started 

public void destroy() {	try {	long limit = Time.now() + 30 * 1000;	scheduler.shutdownNow();	while (!scheduler.awaitTermination(1000, TimeUnit.MILLISECONDS)) {	
waiting for scheduler to shutdown 

public void destroy() {	try {	long limit = Time.now() + 30 * 1000;	scheduler.shutdownNow();	while (!scheduler.awaitTermination(1000, TimeUnit.MILLISECONDS)) {	if (Time.now() > limit) {	
gave up waiting for scheduler to shutdown 

public void destroy() {	try {	long limit = Time.now() + 30 * 1000;	scheduler.shutdownNow();	while (!scheduler.awaitTermination(1000, TimeUnit.MILLISECONDS)) {	if (Time.now() > limit) {	break;	}	}	if (scheduler.isTerminated()) {	
scheduler shutdown 

public void schedule(final Callable<?> callable, long delay, long interval, TimeUnit unit) {	Check.notNull(callable, "callable");	if (!scheduler.isShutdown()) {	
scheduling callable interval seconds delay in 

public void schedule(final Callable<?> callable, long delay, long interval, TimeUnit unit) {	Check.notNull(callable, "callable");	if (!scheduler.isShutdown()) {	Runnable r = new Runnable() {	public void run() {	String instrName = callable.getClass().getSimpleName();	Instrumentation instr = getServer().get(Instrumentation.class);	if (getServer().getStatus() == Server.Status.HALTED) {	
skipping server status 

public void schedule(final Callable<?> callable, long delay, long interval, TimeUnit unit) {	Check.notNull(callable, "callable");	if (!scheduler.isShutdown()) {	Runnable r = new Runnable() {	public void run() {	String instrName = callable.getClass().getSimpleName();	Instrumentation instr = getServer().get(Instrumentation.class);	if (getServer().getStatus() == Server.Status.HALTED) {	instr.incr(INST_GROUP, instrName + ".skips", 1);	} else {	
executing 

Instrumentation instr = getServer().get(Instrumentation.class);	if (getServer().getStatus() == Server.Status.HALTED) {	instr.incr(INST_GROUP, instrName + ".skips", 1);	} else {	instr.incr(INST_GROUP, instrName + ".execs", 1);	Instrumentation.Cron cron = instr.createCron().start();	try {	callable.call();	} catch (Exception ex) {	instr.incr(INST_GROUP, instrName + ".fails", 1);	
error executing 

========================= hadoop sample_6752 =========================

private void pread(DFSInputStream in, long pos, byte[] buffer, int offset, int length, byte[] authenticData) throws IOException {	Assert.assertTrue("Test buffer too small", buffer.length >= offset + length);	if (pos >= 0) in.seek(pos);	
reading from file of size at offset 

public void testReadFromOneDN() throws Exception {	HdfsConfiguration configuration = new HdfsConfiguration();	final String contextName = "testReadFromOneDNContext";	configuration.set(HdfsClientConfigKeys.DFS_CLIENT_CONTEXT, contextName);	configuration.setLong(HdfsClientConfigKeys.DFS_CLIENT_SOCKET_TIMEOUT_KEY, 100000000L);	BlockReaderTestUtil util = new BlockReaderTestUtil(1, configuration);	final Path testFile = new Path("/testConnCache.dat");	byte authenticData[] = util.writeFile(testFile, FILE_SIZE / 1024);	DFSClient client = new DFSClient( new InetSocketAddress("localhost", util.getCluster().getNameNodePort()), util.getConf());	DFSInputStream in = client.open(testFile.toString());	
opened 

========================= hadoop sample_7143 =========================

localFiles.add(pathString);	} else if (resource.getType() == LocalResourceType.PATTERN) {	throw new IllegalArgumentException("Resource type PATTERN is not " + "implemented yet. " + resource.getResource());	}	Path resourcePath;	try {	resourcePath = resource.getResource().toPath();	} catch (URISyntaxException e) {	throw new IOException(e);	}	
localized s as s 

private void symlink(File workDir, String target, String link) throws IOException {	if (link != null) {	link = workDir.toString() + Path.SEPARATOR + link;	File flink = new File(link);	if (!flink.exists()) {	
creating symlink s s 

private void symlink(File workDir, String target, String link) throws IOException {	if (link != null) {	link = workDir.toString() + Path.SEPARATOR + link;	File flink = new File(link);	if (!flink.exists()) {	if (0 != FileUtil.symLink(target, link)) {	
failed to create symlink s s 

public void close() throws IOException {	for (File symlink : symlinksCreated) {	if (!symlink.delete()) {	
failed to delete symlink created by the local job runner 

========================= hadoop sample_4446 =========================

private static ConnectionConfigurator getSSLConnectionConfiguration( final int connectTimeout, final int readTimeout, Configuration conf) {	ConnectionConfigurator conn;	try {	conn = newSslConnConfigurator(connectTimeout, readTimeout, conf);	} catch (Exception e) {	
cannot load customized ssl related configuration fallback to system generic settings 

public URLConnection openConnection(URL url) throws IOException {	try {	return openConnection(url, false);	} catch (AuthenticationException e) {	
open connection failed 

public URLConnection openConnection(URL url, boolean isSpnego) throws IOException, AuthenticationException {	if (isSpnego) {	
open authenticatedurl connection 

public URLConnection openConnection(URL url, boolean isSpnego) throws IOException, AuthenticationException {	if (isSpnego) {	UserGroupInformation.getCurrentUser().checkTGTAndReloginFromKeytab();	final AuthenticatedURL.Token authToken = new AuthenticatedURL.Token();	return new AuthenticatedURL(new KerberosUgiAuthenticator(), connConfigurator).openConnection(url, authToken);	} else {	
open url connection 

========================= hadoop sample_6882 =========================

}	localFS.setPermission(dbPath, LEVELDB_DIR_UMASK);	}	} finally {	IOUtils.cleanupWithLogger(LOG, localFS);	}	JniDBFactory factory = new JniDBFactory();	try {	options.createIfMissing(false);	db = factory.open(new File(dbPath.toString()), options);	
loading the existing database at th path 

JniDBFactory factory = new JniDBFactory();	try {	options.createIfMissing(false);	db = factory.open(new File(dbPath.toString()), options);	checkVersion();	} catch (NativeDB.DBException e) {	if (e.isNotFound() || e.getMessage().contains(" does not exist ")) {	try {	options.createIfMissing(true);	db = factory.open(new File(dbPath.toString()), options);	
creating a new database at th path 

public TimelineServiceState loadState() throws IOException {	
loading timeline service state from leveldb 

public TimelineServiceState loadState() throws IOException {	TimelineServiceState state = new TimelineServiceState();	int numKeys = loadTokenMasterKeys(state);	int numTokens = loadTokens(state);	loadLatestSequenceNumber(state);	
loaded master keys and tokens from leveldb and latest sequence number is 

private void checkVersion() throws IOException {	Version loadedVersion = loadVersion();	
loaded timeline state store version info 

private void checkVersion() throws IOException {	Version loadedVersion = loadVersion();	if (loadedVersion.equals(getCurrentVersion())) {	return;	}	if (loadedVersion.isCompatibleTo(getCurrentVersion())) {	
storing timeline state store version info 

========================= hadoop sample_2068 =========================

public void handle(RMAppAttemptEvent event) {	this.writeLock.lock();	try {	ApplicationAttemptId appAttemptID = event.getApplicationAttemptId();	
processing event for of type 

public void handle(RMAppAttemptEvent event) {	this.writeLock.lock();	try {	ApplicationAttemptId appAttemptID = event.getApplicationAttemptId();	final RMAppAttemptState oldState = getAppAttemptState();	try {	this.stateMachine.doTransition(event.getType(), event);	} catch (InvalidStateTransitionException e) {	
app attempt can t handle this event at current state 

public void recover(RMState state) {	ApplicationStateData appState = state.getApplicationState().get(getAppAttemptId().getApplicationId());	ApplicationAttemptStateData attemptState = appState.getAttempt(getAppAttemptId());	assert attemptState != null;	if (attemptState.getState() == null) {	
NONE 

public RMAppAttemptState transition(RMAppAttemptImpl appAttempt, RMAppAttemptEvent event) {	ApplicationSubmissionContext subCtx = appAttempt.submissionContext;	if (!subCtx.getUnmanagedAM()) {	for (ResourceRequest amReq : appAttempt.amReqs) {	amReq.setNumContainers(1);	amReq.setPriority(AM_CONTAINER_PRIORITY);	}	int numNodes = RMServerUtils.getApplicableNodeCountForAM(appAttempt.rmContext, appAttempt.conf, appAttempt.amReqs);	if (LOG.isDebugEnabled()) {	
setting node count for blacklist to 

for (ResourceRequest amReq : appAttempt.amReqs) {	amReq.setNumContainers(1);	amReq.setPriority(AM_CONTAINER_PRIORITY);	}	int numNodes = RMServerUtils.getApplicableNodeCountForAM(appAttempt.rmContext, appAttempt.conf, appAttempt.amReqs);	if (LOG.isDebugEnabled()) {	}	appAttempt.getAMBlacklistManager().refreshNodeHostCount(numNodes);	ResourceBlacklistRequest amBlacklist = appAttempt.getAMBlacklistManager().getBlacklistUpdates();	if (LOG.isDebugEnabled()) {	
using blacklist for am additions and removals 

private void retryFetchingAMContainer(final RMAppAttemptImpl appAttempt) {	new Thread() {	public void run() {	try {	Thread.sleep(500);	} catch (InterruptedException e) {	
interrupted while waiting to resend the containerallocated event 

if (appAttempt.submissionContext .getKeepContainersAcrossApplicationAttempts() && rmApp.getCurrentAppAttempt() != appAttempt) {	appAttempt.transferStateFromAttempt(rmApp.getCurrentAppAttempt());	}	if (rmApp.getCurrentAppAttempt() == appAttempt && !RMAppImpl.isAppInFinalState(rmApp)) {	appAttempt.scheduler.handle(new AppAttemptAddedSchedulerEvent( appAttempt.getAppAttemptId(), false, true));	(new BaseFinalTransition(appAttempt.recoveredFinalState)).transition( appAttempt, event);	}	return appAttempt.recoveredFinalState;	} else if (RMAppImpl.isAppInFinalState(rmApp))  {	RMAppState appState = ((RMAppImpl) rmApp).getRecoveredFinalState();	
final state was recorded but final state was not recorded 

case FAIL: diags.append(event.getDiagnosticMsg());	break;	case EXPIRE: diags.append(getAMExpiredDiagnostics(event));	break;	default: break;	}	AggregateAppResourceUsage resUsage = this.attemptMetrics.getAggregateAppResourceUsage();	RMStateStore rmStore = rmContext.getStateStore();	setFinishTime(System.currentTimeMillis());	ApplicationAttemptStateData attemptState = ApplicationAttemptStateData.newInstance( applicationAttemptId,  getMasterContainer(), rmStore.getCredentialsFromAppAttempt(this), startTime, stateToBeStored, finalTrackingUrl, diags.toString(), finalStatus, exitStatus, getFinishTime(), resUsage.getMemorySeconds(), resUsage.getVcoreSeconds(), this.attemptMetrics.getPreemptedMemory(), this.attemptMetrics.getPreemptedVcore());	
updating application attempt with final state and exit status 

if (appAttempt.submissionContext .getKeepContainersAcrossApplicationAttempts() && !appAttempt.submissionContext.getUnmanagedAM()) {	int numberOfFailure = ((RMAppImpl)appAttempt.rmApp) .getNumFailedAppAttempts();	if (numberOfFailure < appAttempt.rmApp.getMaxAppAttempts()) {	keepContainersAcrossAppAttempts = true;	}	}	appEvent = new RMAppFailedAttemptEvent(applicationId, RMAppEventType.ATTEMPT_FAILED, appAttempt.getDiagnostics(), keepContainersAcrossAppAttempts);	}	break;	default: {	
cannot get this state error 

private static void amContainerFinished(RMAppAttemptImpl appAttempt, RMAppAttemptContainerFinishedEvent containerFinishedEvent) {	NodeId nodeId = containerFinishedEvent.getNodeId();	ContainerStatus containerStatus = containerFinishedEvent.getContainerStatus();	if (containerStatus != null) {	int exitStatus = containerStatus.getExitStatus();	if (shouldCountTowardsNodeBlacklisting(exitStatus)) {	appAttempt.addAMNodeToBlackList(nodeId);	}	} else {	
no containerstatus in containerfinishedevent 

private void addAMNodeToBlackList(NodeId nodeId) {	SchedulerNode schedulerNode = scheduler.getSchedulerNode(nodeId);	if (schedulerNode != null) {	blacklistedNodesForAM.addNode(schedulerNode.getNodeName());	} else {	
is not added to am blacklist for because it has been removed 

private void storeAttempt() {	
storing attempt appid attemptid mastercontainer 

========================= hadoop sample_723 =========================

protected void serviceStart() throws Exception {	Configuration conf = getConfig();	YarnRPC rpc = YarnRPC.create(conf);	InetSocketAddress address = new InetSocketAddress(0);	server = rpc.getServer(MRClientProtocol.class, protocolHandler, address, conf, appContext.getClientToAMTokenSecretManager(), conf.getInt(MRJobConfig.MR_AM_JOB_CLIENT_THREAD_COUNT, MRJobConfig.DEFAULT_MR_AM_JOB_CLIENT_THREAD_COUNT), MRJobConfig.MR_AM_JOB_CLIENT_PORT_RANGE);	if (conf.getBoolean( CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION, false)) {	refreshServiceAcls(conf, new MRAMPolicyProvider());	}	server.start();	this.bindAddress = NetUtils.createSocketAddrForHost(appContext.getNMHostname(), server.getListenerAddress().getPort());	
instantiated mrclientservice at 

InetSocketAddress address = new InetSocketAddress(0);	server = rpc.getServer(MRClientProtocol.class, protocolHandler, address, conf, appContext.getClientToAMTokenSecretManager(), conf.getInt(MRJobConfig.MR_AM_JOB_CLIENT_THREAD_COUNT, MRJobConfig.DEFAULT_MR_AM_JOB_CLIENT_THREAD_COUNT), MRJobConfig.MR_AM_JOB_CLIENT_PORT_RANGE);	if (conf.getBoolean( CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION, false)) {	refreshServiceAcls(conf, new MRAMPolicyProvider());	}	server.start();	this.bindAddress = NetUtils.createSocketAddrForHost(appContext.getNMHostname(), server.getListenerAddress().getPort());	try {	webApp = WebApps.$for("mapreduce", AppContext.class, appContext, "ws") .withHttpPolicy(conf, Policy.HTTP_ONLY) .withPortRange(conf, MRJobConfig.MR_AM_WEBAPP_PORT_RANGE) .start(new AMWebApp());	} catch (Exception e) {	
webapps failed to start ignoring for now 

public GetTaskReportsResponse getTaskReports( GetTaskReportsRequest request) throws IOException {	JobId jobId = request.getJobId();	TaskType taskType = request.getTaskType();	GetTaskReportsResponse response = recordFactory.newRecordInstance(GetTaskReportsResponse.class);	Job job = verifyAndGetJob(jobId, JobACL.VIEW_JOB, true);	Collection<Task> tasks = job.getTasks(taskType).values();	
getting task report for report size will be 

========================= hadoop sample_5312 =========================

prog.endStep(Phase.LOADING_FSIMAGE, step);	}	break;	case CACHE_MANAGER: {	Step step = new Step(StepType.CACHE_POOLS);	prog.beginStep(Phase.LOADING_FSIMAGE, step);	loadCacheManagerSection(in, prog, step);	prog.endStep(Phase.LOADING_FSIMAGE, step);	}	break;	
unrecognized section 

========================= hadoop sample_7991 =========================

private void add(Configuration conf, String serializationName) {	try {	Class<? extends Serialization> serializionClass = (Class<? extends Serialization>) conf.getClassByName(serializationName);	serializations.add((Serialization) ReflectionUtils.newInstance(serializionClass, getConf()));	} catch (ClassNotFoundException e) {	
serialization class not found 

========================= hadoop sample_3878 =========================

bsize += cfg.getBlockSize() + " bytes";	LOG.info(bsize);	}	if (cfg.getRandomSeed() != null) {	LOG.info("Random seed = " + cfg.getRandomSeed());	}	if (cfg.getSleepRange() != null) {	LOG.info("Sleep range = " + cfg.getSleepRange() + " milliseconds");	}	LOG.info("Replication amount = " + cfg.getReplication());	
operations are 

========================= hadoop sample_5555 =========================

DatanodeInfo[] nodes = blk.getLocations();	for (int j = 0; j < nodes.length; j++) {	if (isNodeDown && nodes[j].getXferAddr().equals(downnode)) {	hasdown++;	if (!nodes[j].isDecommissioned()) {	return "For block " + blk.getBlock() + " replica on " + nodes[j] + " is given as downnode, " + "but is not decommissioned";	}	if (j != nodes.length - 1) {	return "For block " + blk.getBlock() + " decommissioned node " + nodes[j] + " was not last node in list: " + (j + 1) + " of " + nodes.length;	}	
block replica on is decommissioned 

}	if (j != nodes.length - 1) {	return "For block " + blk.getBlock() + " decommissioned node " + nodes[j] + " was not last node in list: " + (j + 1) + " of " + nodes.length;	}	} else {	if (nodes[j].isDecommissioned()) {	return "For block " + blk.getBlock() + " replica on " + nodes[j] + " is unexpectedly decommissioned";	}	}	}	
block has decommissioned replica 

public void testDecommission2() throws IOException {	
starting test testdecommission 

private void testDecommission(int numNamenodes, int numDatanodes) throws IOException {	
starting test testdecommission 

public void testRecommission() throws Exception {	final int numDatanodes = 6;	try {	
starting test testrecommission 

int count = 0;	StringBuilder sb = new StringBuilder("Replica locations: ");	for (int i = 0; i < info.numNodes(); i++) {	DatanodeDescriptor dn = info.getDatanode(i);	sb.append(dn + ", ");	if (!dn.getDatanodeUuid().equals(uuid)) {	count++;	}	}	LOG.info(sb.toString());	
count 

public void testClusterStats(int numNameNodes) throws IOException, InterruptedException {	
starting test testclusterstats 

public void testHostsFile(int numNameNodes) throws IOException, InterruptedException {	int numDatanodes = 1;	startCluster(numNameNodes, numDatanodes, true, null, false);	final String bogusIp = "127.0.30.1";	initIncludeHost(bogusIp);	for (int j = 0; j < numNameNodes; j++) {	refreshNodes(j);	DFSClient client = getDfsClient(j);	DatanodeInfo[] info = client.datanodeReport(DatanodeReportType.LIVE);	for (int i = 0 ; i < 5 && info.length != 0; i++) {	
waiting for datanode to be marked dead 

public void testDecommissionWithOpenfile() throws IOException, InterruptedException {	
starting test testdecommissionwithopenfile 

public void testCloseWhileDecommission() throws IOException, ExecutionException, InterruptedException {	
starting test testclosewhiledecommission 

public void testDecommissionWithNamenodeRestart() throws IOException, InterruptedException {	
starting test testdecommissionwithnamenoderestart 

public void testDeadNodeCountAfterNamenodeRestart()throws Exception {	
starting test testdeadnodecountafternamenoderestart 

public void testIncludeByRegistrationName() throws Exception {	final String registrationName = "127.0.0.100";	final String nonExistentDn = "127.0.0.10";	getConf().set(DFSConfigKeys.DFS_DATANODE_HOST_NAME_KEY, registrationName);	startCluster(1, 1, false, null, true);	initIncludeHost(nonExistentDn);	refreshNodes(0);	
waiting for dn to be marked as dead 

initIncludeHost(nonExistentDn);	refreshNodes(0);	final DFSClient client = getDfsClient(0);	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	BlockManagerTestUtil .checkHeartbeat(getCluster().getNamesystem().getBlockManager());	try {	DatanodeInfo info[] = client.datanodeReport(DatanodeReportType.DEAD);	return info.length == 1;	} catch (IOException e) {	
failed to check dead dns 

} catch (IOException e) {	return false;	}	}	}, 500, 5000);	int dnPort = getCluster().getDataNodes().get(0).getXferPort();	initIncludeHost(registrationName + ":" + dnPort);	refreshNodes(0);	getCluster().restartDataNode(0);	getCluster().triggerHeartbeats();	
waiting for dn to come back 

BlockManagerTestUtil .checkHeartbeat(getCluster().getNamesystem().getBlockManager());	try {	DatanodeInfo info[] = client.datanodeReport(DatanodeReportType.LIVE);	if (info.length == 1) {	Assert.assertFalse(info[0].isDecommissioned());	Assert.assertFalse(info[0].isDecommissionInProgress());	assertEquals(registrationName, info[0].getHostName());	return true;	}	} catch (IOException e) {	
failed to check dead dns 

final FileSystem fileSys = getCluster().getFileSystem(0);	final FSNamesystem ns = getCluster().getNamesystem(0);	final int repl = 3;	writeFile(fileSys, file, repl, 1);	final List<DatanodeInfo> decomDataNodes = takeNodeOutofService(0, Lists.newArrayList(getCluster().getDataNodes().get(0).getDatanodeUuid(), getCluster().getDataNodes().get(1).getDatanodeUuid()), Long.MAX_VALUE, null, null, AdminStates.DECOMMISSIONED);	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	try {	String errMsg = checkFile(fileSys, file, repl, decomDataNodes.get(0).getXferAddr(), 5);	if (errMsg != null) {	
check file 

writeFile(fileSys, file, repl, 1);	final List<DatanodeInfo> decomDataNodes = takeNodeOutofService(0, Lists.newArrayList(getCluster().getDataNodes().get(0).getDatanodeUuid(), getCluster().getDataNodes().get(1).getDatanodeUuid()), Long.MAX_VALUE, null, null, AdminStates.DECOMMISSIONED);	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	try {	String errMsg = checkFile(fileSys, file, repl, decomDataNodes.get(0).getXferAddr(), 5);	if (errMsg != null) {	}	return true;	} catch (IOException e) {	
check file 

========================= hadoop sample_7138 =========================

try {	fs.mkdirs(inputPaths[i]);	} catch (IOException e) {	}	}	}	}	job.submit();	this.state = State.RUNNING;	} catch (Exception ioe) {	
got an error while submitting 

========================= hadoop sample_5040 =========================

}	if (fairShareComparator.compare(copy[1], copy[2]) > 0) {	swap(copy, 1, 2);	if (fairShareComparator.compare(copy[0], copy[1]) > 0) {	swap(copy, 0, 1);	}	}	if (fairShareComparator.compare(copy[0], copy[2]) <= 0) {	return true;	} else {	
failure data 

========================= hadoop sample_534 =========================

if (unselectedFields == null) {	return;	}	for (String field : unselectedFields) {	if (!field.trim().isEmpty()) {	String[] literalsArray = field.split(",");	for (String literals : literalsArray) {	if (literals != null && !literals.trim().isEmpty()) {	DeSelectType type = DeSelectType.obtainType(literals);	if (type == null) {	
invalid deselects string 

========================= hadoop sample_867 =========================

public long renew(final Token<?> token, Configuration conf) throws IOException, InterruptedException {	
renewing the delegation token 

public void cancel(final Token<?> token, Configuration conf) throws IOException, InterruptedException {	
cancelling the delegation token 

========================= hadoop sample_6396 =========================

while (running) {	iterations++;	try {	if (check.call()) {	return iterations;	}	ex = null;	} catch (InterruptedException | FailFastException | VirtualMachineError e) {	throw e;	} catch (Throwable e) {	
eventually iteration 

Thread.sleep(sleeptime);	} else {	running = false;	}	}	}	Throwable evaluate;	try {	evaluate = timeoutHandler.evaluate(timeoutMillis, ex);	if (evaluate == null) {	
timeout handler did not throw an exception 

boolean running;	int sleeptime;	int iterations = 0;	do {	iterations++;	try {	return eval.call();	} catch (InterruptedException | FailFastException | VirtualMachineError e) {	throw e;	} catch (Throwable e) {	
evaluate iteration 

private static String robustToString(Object o) {	if (o == null) {	return NULL_RESULT;	} else {	try {	return o.toString();	} catch (Exception e) {	
exception calling tostring 

========================= hadoop sample_2855 =========================

throw new IllegalArgumentException("mode cannot be NULL");	}	this.mode = mode;	requireClientCert = conf.getBoolean(SSL_REQUIRE_CLIENT_CERT_KEY, DEFAULT_SSL_REQUIRE_CLIENT_CERT);	Configuration sslConf = readSSLConfiguration(mode);	Class<? extends KeyStoresFactory> klass = conf.getClass(KEYSTORES_FACTORY_CLASS_KEY, FileBasedKeyStoresFactory.class, KeyStoresFactory.class);	keystoresFactory = ReflectionUtils.newInstance(klass, sslConf);	enabledProtocols = conf.getStrings(SSL_ENABLED_PROTOCOLS_KEY, SSL_ENABLED_PROTOCOLS_DEFAULT);	excludeCiphers = Arrays.asList( sslConf.getTrimmedStrings(SSL_SERVER_EXCLUDE_CIPHER_LIST));	if (LOG.isDebugEnabled()) {	
will exclude cipher suites 

private void disableExcludedCiphers(SSLEngine sslEngine) {	String[] cipherSuites = sslEngine.getEnabledCipherSuites();	ArrayList<String> defaultEnabledCipherSuites = new ArrayList<String>(Arrays.asList(cipherSuites));	Iterator iterator = excludeCiphers.iterator();	while(iterator.hasNext()) {	String cipherName = (String)iterator.next();	if(defaultEnabledCipherSuites.contains(cipherName)) {	defaultEnabledCipherSuites.remove(cipherName);	
disabling cipher suite 

========================= hadoop sample_3747 =========================

job.setMapperClass(GenDCDataMapper.class);	job.setNumReduceTasks(0);	job.setMapOutputKeyClass(NullWritable.class);	job.setMapOutputValueClass(BytesWritable.class);	job.setInputFormatClass(GenDCDataFormat.class);	job.setOutputFormatClass(NullOutputFormat.class);	job.setJarByClass(GenerateDistCacheData.class);	try {	FileInputFormat.addInputPath(job, new Path("ignored"));	} catch (IOException e) {	
error while adding input path 

========================= hadoop sample_6115 =========================

amtype = "mapreduce";	for (ContainerSimulator cs : containerList) {	if (cs.getType().equals("map")) {	cs.setPriority(PRIORITY_MAP);	allMaps.add(cs);	} else if (cs.getType().equals("reduce")) {	cs.setPriority(PRIORITY_REDUCE);	allReduces.add(cs);	}	}	
added new job with mapper and reducers 

protected void processResponseQueue() throws Exception {	while (! responseQueue.isEmpty()) {	AllocateResponse response = responseQueue.take();	if (! response.getCompletedContainersStatuses().isEmpty()) {	for (ContainerStatus cs : response.getCompletedContainersStatuses()) {	ContainerId containerId = cs.getContainerId();	if (cs.getExitStatus() == ContainerExitStatus.SUCCESS) {	if (assignedMaps.containsKey(containerId)) {	
application has one mapper finished 

AllocateResponse response = responseQueue.take();	if (! response.getCompletedContainersStatuses().isEmpty()) {	for (ContainerStatus cs : response.getCompletedContainersStatuses()) {	ContainerId containerId = cs.getContainerId();	if (cs.getExitStatus() == ContainerExitStatus.SUCCESS) {	if (assignedMaps.containsKey(containerId)) {	assignedMaps.remove(containerId);	mapFinished ++;	finishedContainers ++;	} else if (assignedReduces.containsKey(containerId)) {	
application has one reducer finished 

if (assignedMaps.containsKey(containerId)) {	assignedMaps.remove(containerId);	mapFinished ++;	finishedContainers ++;	} else if (assignedReduces.containsKey(containerId)) {	assignedReduces.remove(containerId);	reduceFinished ++;	finishedContainers ++;	} else if (amContainer.getId().equals(containerId)){	isFinished = true;	
application goes to finish 

reduceFinished ++;	finishedContainers ++;	} else if (amContainer.getId().equals(containerId)){	isFinished = true;	}	if (mapFinished >= mapTotal && reduceFinished >= reduceTotal) {	lastStep();	}	} else {	if (assignedMaps.containsKey(containerId)) {	
application has one mapper killed 

} else if (amContainer.getId().equals(containerId)){	isFinished = true;	}	if (mapFinished >= mapTotal && reduceFinished >= reduceTotal) {	lastStep();	}	} else {	if (assignedMaps.containsKey(containerId)) {	pendingFailedMaps.add(assignedMaps.remove(containerId));	} else if (assignedReduces.containsKey(containerId)) {	
application has one reducer killed 

}	if (mapFinished >= mapTotal && reduceFinished >= reduceTotal) {	lastStep();	}	} else {	if (assignedMaps.containsKey(containerId)) {	pendingFailedMaps.add(assignedMaps.remove(containerId));	} else if (assignedReduces.containsKey(containerId)) {	pendingFailedReduces.add(assignedReduces.remove(containerId));	} else if (amContainer.getId().equals(containerId)){	
application s am is going to be killed waiting for rescheduling 

pendingFailedMaps.add(assignedMaps.remove(containerId));	} else if (assignedReduces.containsKey(containerId)) {	pendingFailedReduces.add(assignedReduces.remove(containerId));	} else if (amContainer.getId().equals(containerId)){	}	}	}	}	if (isAMContainerRunning && (mapFinished >= mapTotal) && (reduceFinished >= reduceTotal)) {	isAMContainerRunning = false;	
application sends out event to clean up its am container 

}	}	if (isAMContainerRunning && (mapFinished >= mapTotal) && (reduceFinished >= reduceTotal)) {	isAMContainerRunning = false;	isFinished = true;	break;	}	for (Container container : response.getAllocatedContainers()) {	if (! scheduledMaps.isEmpty()) {	ContainerSimulator cs = scheduledMaps.remove();	
application starts to launch a mapper 

isFinished = true;	break;	}	for (Container container : response.getAllocatedContainers()) {	if (! scheduledMaps.isEmpty()) {	ContainerSimulator cs = scheduledMaps.remove();	assignedMaps.put(container.getId(), cs);	se.getNmMap().get(container.getNodeId()) .addNewContainer(container, cs.getLifeTime());	} else if (! this.scheduledReduces.isEmpty()) {	ContainerSimulator cs = scheduledReduces.remove();	
application starts to launch a reducer 

protected void sendContainerRequest() throws YarnException, IOException, InterruptedException {	if (isFinished) {	return;	}	List<ResourceRequest> ask = null;	if (mapFinished != mapTotal) {	if (!pendingMaps.isEmpty()) {	ask = packageRequests(mergeLists(pendingMaps, scheduledMaps), PRIORITY_MAP);	
application sends out request for mappers 

return;	}	List<ResourceRequest> ask = null;	if (mapFinished != mapTotal) {	if (!pendingMaps.isEmpty()) {	ask = packageRequests(mergeLists(pendingMaps, scheduledMaps), PRIORITY_MAP);	scheduledMaps.addAll(pendingMaps);	pendingMaps.clear();	} else if (!pendingFailedMaps.isEmpty()) {	ask = packageRequests(mergeLists(pendingFailedMaps, scheduledMaps), PRIORITY_MAP);	
application sends out requests for failed mappers 

scheduledMaps.addAll(pendingMaps);	pendingMaps.clear();	} else if (!pendingFailedMaps.isEmpty()) {	ask = packageRequests(mergeLists(pendingFailedMaps, scheduledMaps), PRIORITY_MAP);	scheduledMaps.addAll(pendingFailedMaps);	pendingFailedMaps.clear();	}	} else if (reduceFinished != reduceTotal) {	if (!pendingReduces.isEmpty()) {	ask = packageRequests(mergeLists(pendingReduces, scheduledReduces), PRIORITY_REDUCE);	
application sends out requests for reducers 

scheduledMaps.addAll(pendingFailedMaps);	pendingFailedMaps.clear();	}	} else if (reduceFinished != reduceTotal) {	if (!pendingReduces.isEmpty()) {	ask = packageRequests(mergeLists(pendingReduces, scheduledReduces), PRIORITY_REDUCE);	scheduledReduces.addAll(pendingReduces);	pendingReduces.clear();	} else if (!pendingFailedReduces.isEmpty()) {	ask = packageRequests(mergeLists(pendingFailedReduces, scheduledReduces), PRIORITY_REDUCE);	
application sends out request for failed reducers 

========================= hadoop sample_6050 =========================

Configuration conf = new HdfsConfiguration();	cluster = new MiniDFSCluster.Builder(conf).numDataNodes(3).build();	FileSystem fs = cluster.getFileSystem();	util.createFiles(fs, "/srcdat");	String bpid = cluster.getNamesystem().getBlockPoolId();	DataNode dn = cluster.getDataNodes().get(2);	Map<DatanodeStorage, BlockListAsLongs> blockReports = dn.getFSDataset().getBlockReports(bpid);	assertTrue("Blocks do not exist on data-dir", !blockReports.isEmpty());	for (BlockListAsLongs report : blockReports.values()) {	for (BlockReportReplica brr : report) {	
deliberately removing block 

Path file = new Path(PathUtils.getTestDirName(getClass()), "corruptFile");	FileSystem fs = FileSystem.getLocal(conf);	DataOutputStream dos = fs.create(file);	dos.writeBytes("original bytes");	dos.close();	dos = new DataOutputStream(new FileOutputStream(file.toString()));	dos.writeBytes("corruption");	dos.close();	DataInputStream dis = fs.open(file, 512);	try {	
a checksumexception is expected to be logged 

========================= hadoop sample_7641 =========================

public synchronized void joinElection(byte[] data) throws HadoopIllegalArgumentException {	if (data == null) {	throw new HadoopIllegalArgumentException("data cannot be null");	}	if (wantToBeInElection) {	
already in election not re connecting 

public synchronized void joinElection(byte[] data) throws HadoopIllegalArgumentException {	if (data == null) {	throw new HadoopIllegalArgumentException("data cannot be null");	}	if (wantToBeInElection) {	return;	}	appData = new byte[data.length];	System.arraycopy(data, 0, appData, 0, data.length);	if (LOG.isDebugEnabled()) {	
attempting active election for 

public synchronized void ensureParentZNode() throws IOException, InterruptedException {	Preconditions.checkState(!wantToBeInElection, "ensureParentZNode() may not be called while in the election");	String pathParts[] = znodeWorkingDir.split("/");	Preconditions.checkArgument(pathParts.length >= 1 && pathParts[0].isEmpty(), "Invalid path: %s", znodeWorkingDir);	StringBuilder sb = new StringBuilder();	for (int i = 1; i < pathParts.length; i++) {	sb.append("/").append(pathParts[i]);	String prefixPath = sb.toString();	
ensuring existence of 

try {	setAclsWithRetries(prefixPath);	} catch (KeeperException e1) {	throw new IOException("Couldn't set ACLs on parent ZNode: " + prefixPath, e1);	}	} else {	throw new IOException("Couldn't create " + prefixPath, e);	}	}	}	
successfully created in zk 

public synchronized void clearParentZNode() throws IOException, InterruptedException {	Preconditions.checkState(!wantToBeInElection, "clearParentZNode() may not be called while in the election");	try {	
recursively deleting from zk 

try {	zkDoWithRetries(new ZKAction<Void>() {	public Void run() throws KeeperException, InterruptedException {	ZKUtil.deleteRecursive(zkClient, znodeWorkingDir);	return null;	}	});	} catch (KeeperException e) {	throw new IOException("Couldn't clear parent znode " + znodeWorkingDir, e);	}	
successfully deleted from zk 

public synchronized void quitElection(boolean needFence) {	
yielding from election 

public synchronized void processResult(int rc, String path, Object ctx, String name) {	if (isStaleClient(ctx)) return;	if (LOG.isDebugEnabled()) {	
createnode result for path connectionstate for 

if (createRetryCount == 0) {	becomeStandby();	}	monitorActiveStatus();	return;	}	String errorMessage = "Received create error from Zookeeper. code:" + code.toString() + " for path " + path;	LOG.debug(errorMessage);	if (shouldRetry(code)) {	if (createRetryCount < maxRetryNum) {	
retrying createnode createretrycount 

String errorMessage = "Received create error from Zookeeper. code:" + code.toString() + " for path " + path;	LOG.debug(errorMessage);	if (shouldRetry(code)) {	if (createRetryCount < maxRetryNum) {	++createRetryCount;	createLockNodeAsync();	return;	}	errorMessage = errorMessage + ". Not retrying further znode create connection errors.";	} else if (isSessionExpired(code)) {	
lock acquisition failed because session was lost 

public synchronized void processResult(int rc, String path, Object ctx, Stat stat) {	if (isStaleClient(ctx)) return;	monitorLockNodePending = false;	assert wantToBeInElection : "Got a StatNode result after quitting election";	if (LOG.isDebugEnabled()) {	
statnode result for path connectionstate for 

String errorMessage = "Received stat error from Zookeeper. code:" + code.toString();	LOG.debug(errorMessage);	if (shouldRetry(code)) {	if (statRetryCount < maxRetryNum) {	++statRetryCount;	monitorLockNodeAsync();	return;	}	errorMessage = errorMessage + ". Not retrying further znode monitoring connection errors.";	} else if (isSessionExpired(code)) {	
lock monitoring failed because session was lost 

private void monitorActiveStatus() {	assert wantToBeInElection;	if (LOG.isDebugEnabled()) {	
monitoring active leader for 

private void reJoinElection(int sleepTime) {	
trying to re establish zk session 

private void reJoinElection(int sleepTime) {	sessionReestablishLockForTests.lock();	try {	terminateConnection();	sleepFor(sleepTime);	if (appData != null) {	joinElectionInternal();	} else {	
not joining election since service has not yet been reported as healthy 

private boolean reEstablishSession() {	int connectionRetryCount = 0;	boolean success = false;	while(!success && connectionRetryCount < maxRetryNum) {	if (LOG.isDebugEnabled()) {	
establishing zookeeper connection for 

try {	zkClient.close();	} catch (InterruptedException e) {	throw new IOException("Interrupted while closing ZK", e);	}	zkClient = null;	watcher = null;	}	zkClient = connectToZooKeeper();	if (LOG.isDebugEnabled()) {	
created new connection for 

public synchronized void terminateConnection() {	if (zkClient == null) {	return;	}	if (LOG.isDebugEnabled()) {	
terminating zk connection for 

private boolean becomeActive() {	assert wantToBeInElection;	if (state == State.ACTIVE) {	return true;	}	try {	Stat oldBreadcrumbStat = fenceOldActive();	writeBreadCrumbNode(oldBreadcrumbStat);	if (LOG.isDebugEnabled()) {	
becoming active for 

}	try {	Stat oldBreadcrumbStat = fenceOldActive();	writeBreadCrumbNode(oldBreadcrumbStat);	if (LOG.isDebugEnabled()) {	}	appClient.becomeActive();	state = State.ACTIVE;	return true;	} catch (Exception e) {	
exception handling the winning of election 

private void writeBreadCrumbNode(Stat oldBreadcrumbStat) throws KeeperException, InterruptedException {	Preconditions.checkState(appData != null, "no appdata");	
writing znode to indicate that the local node is the most recent active 

private void tryDeleteOwnBreadCrumbNode() {	assert state == State.ACTIVE;	
deleting bread crumb of active node 

assert state == State.ACTIVE;	Stat stat = new Stat();	byte[] data = null;	try {	data = zkClient.getData(zkBreadCrumbPath, false, stat);	if (!Arrays.equals(data, appData)) {	throw new IllegalStateException( "We thought we were active, but in fact " + "the active znode had the wrong data: " + StringUtils.byteToHexString(data) + " (stat=" + stat + ")");	}	deleteWithRetries(zkBreadCrumbPath, stat.getVersion());	} catch (Exception e) {	
unable to delete our own bread crumb of being active at expecting to be fenced by the next active 

private Stat fenceOldActive() throws InterruptedException, KeeperException {	final Stat stat = new Stat();	byte[] data;	
checking for any old active which needs to be fenced 

final Stat stat = new Stat();	byte[] data;	try {	data = zkDoWithRetries(new ZKAction<byte[]>() {	public byte[] run() throws KeeperException, InterruptedException {	return zkClient.getData(zkBreadCrumbPath, false, stat);	}	});	} catch (KeeperException ke) {	if (isNodeDoesNotExist(ke.code())) {	
no old node to fence 

public byte[] run() throws KeeperException, InterruptedException {	return zkClient.getData(zkBreadCrumbPath, false, stat);	}	});	} catch (KeeperException ke) {	if (isNodeDoesNotExist(ke.code())) {	return null;	}	throw ke;	}	
old node exists 

return zkClient.getData(zkBreadCrumbPath, false, stat);	}	});	} catch (KeeperException ke) {	if (isNodeDoesNotExist(ke.code())) {	return null;	}	throw ke;	}	if (Arrays.equals(data, appData)) {	
but old node has our own data so don t need to fence it 

private void becomeStandby() {	if (state != State.STANDBY) {	if (LOG.isDebugEnabled()) {	
becoming standby for 

private void enterNeutralMode() {	if (state != State.NEUTRAL) {	if (LOG.isDebugEnabled()) {	
entering neutral mode for 

private void monitorLockNodeAsync() {	if (monitorLockNodePending && monitorLockNodeClient == zkClient) {	
ignore duplicate monitor lock node request 

private synchronized boolean isStaleClient(Object ctx) {	Preconditions.checkNotNull(ctx);	if (zkClient != (ZooKeeper)ctx) {	
ignoring stale result from old client with sessionid 

private void waitForZKConnectionEvent(int connectionTimeoutMs) throws KeeperException, IOException {	try {	if (!hasReceivedEvent.await(connectionTimeoutMs, TimeUnit.MILLISECONDS)) {	
connection timed out couldn t connect to zookeeper in milliseconds 

public void process(WatchedEvent event) {	hasReceivedEvent.countDown();	try {	if (!hasSetZooKeeper.await(zkSessionTimeout, TimeUnit.MILLISECONDS)) {	
event received with stale zk 

========================= hadoop sample_3985 =========================

private synchronized NodeData registerNode(DatanodeDescriptor dn) {	if (nodes.containsKey(dn.getDatanodeUuid())) {	
can t register dn because it is already registered 

private synchronized NodeData registerNode(DatanodeDescriptor dn) {	if (nodes.containsKey(dn.getDatanodeUuid())) {	return null;	}	NodeData node = new NodeData(dn.getDatanodeUuid());	deferredHead.addToBeginning(node);	nodes.put(dn.getDatanodeUuid(), node);	
registered dn 

public synchronized void unregister(DatanodeDescriptor dn) {	NodeData node = nodes.remove(dn.getDatanodeUuid());	if (node == null) {	
can t unregister dn because it is not currently registered 

public synchronized long requestLease(DatanodeDescriptor dn) {	NodeData node = nodes.get(dn.getDatanodeUuid());	if (node == null) {	
dn requested a lease even though it wasn t yet registered registering now 

public synchronized long requestLease(DatanodeDescriptor dn) {	NodeData node = nodes.get(dn.getDatanodeUuid());	if (node == null) {	node = registerNode(dn);	}	if (node.leaseId != 0) {	
removing existing br lease for dn in order to issue a new one 

pruneExpiredPending(monotonicNowMs);	if (numPending >= maxPending) {	if (LOG.isDebugEnabled()) {	StringBuilder allLeases = new StringBuilder();	String prefix = "";	for (NodeData cur = pendingHead.next; cur != pendingHead;	cur = cur.next) {	allLeases.append(prefix).append(cur.datanodeUuid);	prefix = ", ";	}	
can t create a new br lease for dn because numpending equals maxpending at current leases 

private synchronized boolean pruneIfExpired(long monotonicNowMs, NodeData node) {	if (monotonicNowMs < node.leaseTimeMs + leaseExpiryMs) {	return false;	}	
removing expired block report lease for dn 

private synchronized void pruneExpiredPending(long monotonicNowMs) {	NodeData cur = pendingHead.next;	while (cur != pendingHead) {	NodeData next = cur.next;	if (!pruneIfExpired(monotonicNowMs, cur)) {	return;	}	cur = next;	}	
no entries remaining in the pending list 

public synchronized boolean checkLease(DatanodeDescriptor dn, long monotonicNowMs, long id) {	if (id == 0) {	
datanode is using br lease id to bypass rate limiting 

public synchronized boolean checkLease(DatanodeDescriptor dn, long monotonicNowMs, long id) {	if (id == 0) {	return true;	}	NodeData node = nodes.get(dn.getDatanodeUuid());	if (node == null) {	
br lease is not valid for unknown datanode 

public synchronized boolean checkLease(DatanodeDescriptor dn, long monotonicNowMs, long id) {	if (id == 0) {	return true;	}	NodeData node = nodes.get(dn.getDatanodeUuid());	if (node == null) {	return false;	}	if (node.leaseId == 0) {	
br lease is not valid for dn because the dn is not in the pending set 

return true;	}	NodeData node = nodes.get(dn.getDatanodeUuid());	if (node == null) {	return false;	}	if (node.leaseId == 0) {	return false;	}	if (pruneIfExpired(monotonicNowMs, node)) {	
br lease is not valid for dn because the lease has expired 

if (node == null) {	return false;	}	if (node.leaseId == 0) {	return false;	}	if (pruneIfExpired(monotonicNowMs, node)) {	return false;	}	if (id != node.leaseId) {	
br lease is not valid for dn expected br lease 

if (node.leaseId == 0) {	return false;	}	if (pruneIfExpired(monotonicNowMs, node)) {	return false;	}	if (id != node.leaseId) {	return false;	}	if (LOG.isTraceEnabled()) {	
br lease is valid for dn 

public synchronized long removeLease(DatanodeDescriptor dn) {	NodeData node = nodes.get(dn.getDatanodeUuid());	if (node == null) {	
can t remove lease for unknown datanode 

public synchronized long removeLease(DatanodeDescriptor dn) {	NodeData node = nodes.get(dn.getDatanodeUuid());	if (node == null) {	return 0;	}	long id = node.leaseId;	if (id == 0) {	
dn has no lease to remove 

========================= hadoop sample_8330 =========================

public K[] getSample(InputFormat<K,V> inf, JobConf job) throws IOException {	InputSplit[] splits = inf.getSplits(job, job.getNumMapTasks());	ArrayList<K> samples = new ArrayList<K>(numSamples);	int splitsToSample = Math.min(maxSplitsSampled, splits.length);	Random r = new Random();	long seed = r.nextLong();	r.setSeed(seed);	
seed 

========================= hadoop sample_4743 =========================

private long doCopy(CopyListingFileStatus source, Path target, Mapper.Context context, EnumSet<FileAttribute> fileAttributes) throws IOException {	final boolean toAppend = action == FileAction.APPEND;	Path targetPath = toAppend ? target : getTmpFile(target, context);	final Configuration configuration = context.getConfiguration();	FileSystem targetFS = target.getFileSystem(configuration);	try {	if (LOG.isDebugEnabled()) {	
copying to 

private long doCopy(CopyListingFileStatus source, Path target, Mapper.Context context, EnumSet<FileAttribute> fileAttributes) throws IOException {	final boolean toAppend = action == FileAction.APPEND;	Path targetPath = toAppend ? target : getTmpFile(target, context);	final Configuration configuration = context.getConfiguration();	FileSystem targetFS = target.getFileSystem(configuration);	try {	if (LOG.isDebugEnabled()) {	
target file path 

private Path getTmpFile(Path target, Mapper.Context context) {	Path targetWorkPath = new Path(context.getConfiguration(). get(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH));	Path root = target.equals(targetWorkPath)? targetWorkPath.getParent() : targetWorkPath;	
creating temp file distcp tmp 

========================= hadoop sample_6284 =========================

public Optional<ListenableFuture<V>> schedule( final Checkable<K, V> target, final K context) {	
scheduling a check for 

public Optional<ListenableFuture<V>> schedule( final Checkable<K, V> target, final K context) {	if (checksInProgress.containsKey(target)) {	return Optional.absent();	}	if (completedChecks.containsKey(target)) {	final LastCheckResult<V> result = completedChecks.get(target);	final long msSinceLastCheck = timer.monotonicNow() - result.completedAt;	if (msSinceLastCheck < minMsBetweenChecks) {	
skipped checking time since last check ms is less than the min gap ms 

========================= hadoop sample_7883 =========================

private void updateCurrentMasterKey(MasterKeyData key) {	super.currentMasterKey = key;	try {	stateStore.storeNMTokenCurrentMasterKey(key.getMasterKey());	} catch (IOException e) {	
unable to update current master key in state store 

private void updatePreviousMasterKey(MasterKeyData key) {	previousMasterKey = key;	try {	stateStore.storeNMTokenPreviousMasterKey(key.getMasterKey());	} catch (IOException e) {	
unable to update previous master key in state store 

public synchronized void setMasterKey(MasterKey masterKey) {	if (super.currentMasterKey == null || super.currentMasterKey.getMasterKey() .getKeyId() != masterKey.getKeyId()) {	
rolling master key for container tokens got key with id 

if (previousMasterKey != null && keyId == previousMasterKey.getMasterKey().getKeyId()) {	masterKeyToUse = previousMasterKey;	} else if (keyId == currentMasterKey.getMasterKey().getKeyId()) {	masterKeyToUse = currentMasterKey;	}	if (nodeId != null && !identifier.getNodeId().equals(nodeId)) {	throw new InvalidToken("Given NMToken for application : " + appAttemptId.toString() + " is not valid for current node manager." + "expected : " + nodeId.toString() + " found : " + identifier.getNodeId().toString());	}	if (masterKeyToUse != null) {	byte[] password = retrivePasswordInternal(identifier, masterKeyToUse);	
nmtoken password retrieved successfully 

public synchronized void appFinished(ApplicationId appId) {	List<ApplicationAttemptId> appAttemptList = appToAppAttemptMap.get(appId);	if (appAttemptList != null) {	if (LOG.isDebugEnabled()) {	
removing application attempts nmtoken keys for application 

public synchronized void appFinished(ApplicationId appId) {	List<ApplicationAttemptId> appAttemptList = appToAppAttemptMap.get(appId);	if (appAttemptList != null) {	if (LOG.isDebugEnabled()) {	}	for (ApplicationAttemptId appAttemptId : appAttemptList) {	removeAppAttemptKey(appAttemptId);	}	appToAppAttemptMap.remove(appId);	} else {	
no application attempt for application started on this nm 

ApplicationAttemptId appAttemptId = identifier.getApplicationAttemptId();	if (!appToAppAttemptMap.containsKey(appAttemptId.getApplicationId())) {	appToAppAttemptMap.put(appAttemptId.getApplicationId(), new ArrayList<ApplicationAttemptId>());	}	MasterKeyData oldKey = oldMasterKeys.get(appAttemptId);	if (oldKey == null) {	appToAppAttemptMap.get(appAttemptId.getApplicationId()).add(appAttemptId);	}	if (oldKey == null || oldKey.getMasterKey().getKeyId() != identifier.getKeyId()) {	if (LOG.isDebugEnabled()) {	
nmtoken key updated for application attempt 

public synchronized void setNodeId(NodeId nodeId) {	if (LOG.isDebugEnabled()) {	
updating nodeid 

private void updateAppAttemptKey(ApplicationAttemptId attempt, MasterKeyData key) {	this.oldMasterKeys.put(attempt, key);	try {	stateStore.storeNMTokenApplicationMasterKey(attempt, key.getMasterKey());	} catch (IOException e) {	
unable to store master key for application 

private void removeAppAttemptKey(ApplicationAttemptId attempt) {	this.oldMasterKeys.remove(attempt);	try {	stateStore.removeNMTokenApplicationMasterKey(attempt);	} catch (IOException e) {	
unable to remove master key for application 

========================= hadoop sample_1687 =========================

public void run() {	
resource usage matcher thread started 

public void run() {	try {	while (progress.getProgress() < 1) {	match();	try {	Thread.sleep(sleepTime);	} catch (Exception e) {}	}	match();	
resource usage emulation complete matcher exiting 

public void run() {	try {	while (progress.getProgress() < 1) {	match();	try {	Thread.sleep(sleepTime);	} catch (Exception e) {}	}	match();	} catch (Exception e) {	
exception while running the resource usage emulation matcher thread exiting 

public void run() {	
status reporter thread started 

public void run() {	try {	while (!isInterrupted() && progress.getProgress() < 1) {	context.progress();	try {	Thread.sleep(100);	} catch (Exception e) {}	}	
status reporter thread exiting 

public void run() {	try {	while (!isInterrupted() && progress.getProgress() < 1) {	context.progress();	try {	Thread.sleep(100);	} catch (Exception e) {}	}	} catch (Exception e) {	
exception while running the status reporter thread 

final int maps = split.getMapCount();	final long[] reduceBytes = split.getOutputBytes();	final long[] reduceRecords = split.getOutputRecords();	long totalRecords = 0L;	final int nReduces = ctxt.getNumReduceTasks();	if (nReduces > 0) {	boolean emulateMapOutputCompression = CompressionEmulationUtil.isCompressionEmulationEnabled(conf) && conf.getBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, false);	float compressionRatio = 1.0f;	if (emulateMapOutputCompression) {	compressionRatio = CompressionEmulationUtil.getMapOutputCompressionEmulationRatio(conf);	
gridmix is configured to use a compression ratio of for the map output data 

mapOutputBytes /= compressionRatio;	}	reduces.add(new IntermediateRecordFactory( new AvgRecordFactory(mapOutputBytes, reduceRecords[i], conf, 5*1024), i, reduceRecords[i], spec, conf));	totalRecords += reduceRecords[i];	}	} else {	long mapOutputBytes = reduceBytes[0];	boolean emulateJobOutputCompression = CompressionEmulationUtil.isCompressionEmulationEnabled(conf) && conf.getBoolean(FileOutputFormat.COMPRESS, false);	if (emulateJobOutputCompression) {	float compressionRatio = CompressionEmulationUtil.getJobOutputCompressionEmulationRatio(conf);	
gridmix is configured to use a compression ratio of for the job output data 

final RecordFactory f = reduces.get(idx);	if (!f.next(key, val)) {	reduces.remove(idx);	continue;	}	context.write(key, val);	acc -= 1.0;	try {	matcher.match();	} catch (Exception e) {	
error in resource usage emulation message 

public void cleanup(Context context) throws IOException, InterruptedException {	
starting the cleanup phase 

public void cleanup(Context context) throws IOException, InterruptedException {	for (RecordFactory factory : reduces) {	key.setSeed(r.nextLong());	while (factory.next(key, val)) {	context.progress();	context.write(key, val);	key.setSeed(r.nextLong());	try {	matcher.match();	} catch (Exception e) {	
error in resource usage emulation message 

context.progress();	context.write(key, val);	key.setSeed(r.nextLong());	try {	matcher.match();	} catch (Exception e) {	}	}	}	if (context.getNumReduceTasks() > 0 && context.getCounter(TaskCounter.SPILLED_RECORDS).getValue() == 0) {	
boosting the map phase progress 

for (GridmixRecord ignored : context.getValues()) {	final GridmixKey spec = context.getCurrentKey();	inRecords += spec.getReduceInputRecords();	outBytes += spec.getReduceOutputBytes();	outRecords += spec.getReduceOutputRecords();	if (spec.getReduceResourceUsageMetrics() != null) {	metrics = spec.getReduceResourceUsageMetrics();	}	}	if (0 == outRecords && inRecords > 0) {	
spec output bytes w o records using input record count 

if (spec.getReduceResourceUsageMetrics() != null) {	metrics = spec.getReduceResourceUsageMetrics();	}	}	if (0 == outRecords && inRecords > 0) {	outRecords = inRecords;	}	Configuration conf = context.getConfiguration();	if (CompressionEmulationUtil.isCompressionEmulationEnabled(conf) && FileOutputFormat.getCompressOutput(context)) {	float compressionRatio = CompressionEmulationUtil .getJobOutputCompressionEmulationRatio(conf);	
gridmix is configured to use a compression ratio of for the reduce output data 

protected void reduce(GridmixKey key, Iterable<GridmixRecord> values, Context context) throws IOException, InterruptedException {	for (GridmixRecord ignored : values) {	acc += ratio;	while (acc >= 1.0 && factory.next(null, val)) {	context.write(NullWritable.get(), val);	acc -= 1.0;	try {	matcher.match();	} catch (Exception e) {	
error in resource usage emulation message 

protected void cleanup(Context context) throws IOException, InterruptedException {	val.setSeed(r.nextLong());	while (factory.next(null, val)) {	context.write(NullWritable.get(), val);	val.setSeed(r.nextLong());	try {	matcher.match();	} catch (Exception e) {	
error in resource usage emulation message 

========================= hadoop sample_6142 =========================

public void run() {	try {	server.shutdown();	} catch (Exception e) {	
httpserver fails to shut down 

========================= hadoop sample_6512 =========================

protected void serviceInit(Configuration conf) throws Exception {	timelineServiceVersion = conf.getFloat(YarnConfiguration.TIMELINE_SERVICE_VERSION, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_VERSION);	
timeline service address 

private static void putTimelineDataInJSONFile(String path, String type) {	File jsonFile = new File(path);	if (!jsonFile.exists()) {	
file doesn t exist 

YarnJacksonJaxbJsonProvider.configObjectMapper(MAPPER);	TimelineEntities entities = null;	TimelineDomains domains = null;	try {	if (type.equals(ENTITY_DATA_TYPE)) {	entities = MAPPER.readValue(jsonFile, TimelineEntities.class);	} else if (type.equals(DOMAIN_DATA_TYPE)){	domains = MAPPER.readValue(jsonFile, TimelineDomains.class);	}	} catch (Exception e) {	
error when reading 

client.init(conf);	client.start();	try {	if (UserGroupInformation.isSecurityEnabled() && conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED, false)) {	Token<TimelineDelegationTokenIdentifier> token = client.getDelegationToken( UserGroupInformation.getCurrentUser().getUserName());	UserGroupInformation.getCurrentUser().addToken(token);	}	if (type.equals(ENTITY_DATA_TYPE)) {	TimelinePutResponse response = client.putEntities( entities.getEntities().toArray( new TimelineEntity[entities.getEntities().size()]));	if (response.getErrors().size() == 0) {	
timeline entities are successfully put 

try {	if (UserGroupInformation.isSecurityEnabled() && conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED, false)) {	Token<TimelineDelegationTokenIdentifier> token = client.getDelegationToken( UserGroupInformation.getCurrentUser().getUserName());	UserGroupInformation.getCurrentUser().addToken(token);	}	if (type.equals(ENTITY_DATA_TYPE)) {	TimelinePutResponse response = client.putEntities( entities.getEntities().toArray( new TimelineEntity[entities.getEntities().size()]));	if (response.getErrors().size() == 0) {	} else {	for (TimelinePutResponse.TimelinePutError error : response.getErrors()) {	
timelineentity is not successfully put error code 

} else {	for (TimelinePutResponse.TimelinePutError error : response.getErrors()) {	}	}	} else if (type.equals(DOMAIN_DATA_TYPE) && domains != null) {	boolean hasError = false;	for (TimelineDomain domain : domains.getDomains()) {	try {	client.putDomain(domain);	} catch (Exception e) {	
error when putting domain 

} else if (type.equals(DOMAIN_DATA_TYPE) && domains != null) {	boolean hasError = false;	for (TimelineDomain domain : domains.getDomains()) {	try {	client.putDomain(domain);	} catch (Exception e) {	hasError = true;	}	}	if (!hasError) {	
timeline domains are successfully put 

try {	client.putDomain(domain);	} catch (Exception e) {	hasError = true;	}	}	if (!hasError) {	}	}	} catch(RuntimeException e) {	
error when putting the timeline data 

client.putDomain(domain);	} catch (Exception e) {	hasError = true;	}	}	if (!hasError) {	}	}	} catch(RuntimeException e) {	} catch (Exception e) {	
error when putting the timeline data 

========================= hadoop sample_2551 =========================

final long startReadValue = getLongCounter("TotalReadTime", rb);	final AtomicInteger x = new AtomicInteger(0);	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	x.getAndIncrement();	try {	DFSTestUtil.createFile(fs, new Path("/time.txt." + x.get()), LONG_FILE_LEN, (short) 1, Time.monotonicNow());	DFSTestUtil.readFile(fs, new Path("/time.txt." + x.get()));	fs.delete(new Path("/time.txt." + x.get()), true);	} catch (IOException ioe) {	
caught ioexception while ingesting dn metrics 

========================= hadoop sample_7303 =========================

public void shutDown() throws InterruptedException {	this.stopped = true;	interrupt();	try {	join(5000);	} catch (InterruptedException ie) {	
got interrupt while joining 

private DataInputStream openShuffleUrl(MapHost host, Set<TaskAttemptID> remaining, URL url) {	DataInputStream input = null;	try {	setupConnectionsWithRetry(url);	if (stopped) {	abortConnect(host, remaining);	} else {	input = new DataInputStream(connection.getInputStream());	}	} catch (TryAgainLaterException te) {	
connection rejected by the host will retry later 

if (stopped) {	abortConnect(host, remaining);	} else {	input = new DataInputStream(connection.getInputStream());	}	} catch (TryAgainLaterException te) {	scheduler.penalize(host, te.backoff);	} catch (IOException ie) {	boolean connectExcpt = ie instanceof ConnectException;	ioErrs.increment(1);	
failed to connect to with map outputs 

protected void copyFromHost(MapHost host) throws IOException {	retryStartTime = 0;	List<TaskAttemptID> maps = scheduler.getMapsForHost(host);	if (maps.size() == 0) {	return;	}	if(LOG.isDebugEnabled()) {	
fetcher going to fetch from for 

IOUtils.cleanup(LOG, input);	connection.disconnect();	url = getMapOutputURL(host, remaining);	input = openShuffleUrl(host, remaining, url);	if (input == null) {	return;	}	}	}	if(failedTasks != null && failedTasks.length > 0) {	
copymapoutput failed for tasks 

boolean shouldWait = true;	while (shouldWait) {	try {	openConnection(url);	shouldWait = false;	} catch (IOException e) {	if (!fetchRetryEnabled) {	throw e;	}	if ((Time.monotonicNow() - startTime) >= this.fetchRetryTimeout) {	
failed to connect to host after milliseconds 

private void verifyConnection(URL url, String msgToEncode, String encHash) throws IOException {	int rc = connection.getResponseCode();	if (rc == TOO_MANY_REQ_STATUS_CODE) {	long backoff = connection.getHeaderFieldLong(FETCH_RETRY_AFTER_HEADER, FETCH_RETRY_DELAY_DEFAULT);	if (backoff < 0) {	backoff = FETCH_RETRY_DELAY_DEFAULT;	
get a negative backoff value from shufflehandler setting it to the default value 

int forReduce = -1;	try {	ShuffleHeader header = new ShuffleHeader();	header.readFields(input);	mapId = TaskAttemptID.forName(header.mapId);	compressedLength = header.compressedLength;	decompressedLength = header.uncompressedLength;	forReduce = header.forReduce;	} catch (IllegalArgumentException e) {	badIdErrs.increment(1);	
invalid map id 

return remaining.toArray(new TaskAttemptID[remaining.size()]);	}	InputStream is = input;	is = CryptoUtils.wrapIfNecessary(jobConf, is, compressedLength);	compressedLength -= CryptoUtils.cryptoPadding(jobConf);	decompressedLength -= CryptoUtils.cryptoPadding(jobConf);	if (!verifySanity(compressedLength, decompressedLength, forReduce, remaining, mapId)) {	return new TaskAttemptID[] {mapId};	}	if(LOG.isDebugEnabled()) {	
header len decomp len 

if(LOG.isDebugEnabled()) {	}	try {	mapOutput = merger.reserve(mapId, decompressedLength, id);	} catch (IOException ioe) {	ioErrs.increment(1);	scheduler.reportLocalError(ioe);	return EMPTY_ATTEMPT_ID_ARRAY;	}	if (mapOutput == null) {	
fetcher mergemanager returned status wait 

mapOutput = merger.reserve(mapId, decompressedLength, id);	} catch (IOException ioe) {	ioErrs.increment(1);	scheduler.reportLocalError(ioe);	return EMPTY_ATTEMPT_ID_ARRAY;	}	if (mapOutput == null) {	return EMPTY_ATTEMPT_ID_ARRAY;	}	try {	
fetcher about to shuffle output of map decomp len to 

ioErrs.increment(1);	scheduler.reportLocalError(ioe);	return EMPTY_ATTEMPT_ID_ARRAY;	}	if (mapOutput == null) {	return EMPTY_ATTEMPT_ID_ARRAY;	}	try {	mapOutput.shuffle(host, is, compressedLength, decompressedLength, metrics, reporter);	} catch (java.lang.InternalError | Exception e) {	
failed to shuffle for fetcher 

return null;	} catch (IOException ioe) {	if (mapOutput != null) {	mapOutput.abort();	}	if (canRetry) {	checkTimeoutOrRetry(host, ioe);	}	ioErrs.increment(1);	if (mapId == null || mapOutput == null) {	
fetcher failed to read map header decomp 

checkTimeoutOrRetry(host, ioe);	}	ioErrs.increment(1);	if (mapId == null || mapOutput == null) {	if(mapId == null) {	return remaining.toArray(new TaskAttemptID[remaining.size()]);	} else {	return new TaskAttemptID[] {mapId};	}	}	
failed to shuffle output of from 

private void checkTimeoutOrRetry(MapHost host, IOException ioe) throws IOException {	long currentTime = Time.monotonicNow();	if (retryStartTime == 0) {	retryStartTime = currentTime;	}	if (currentTime - retryStartTime < this.fetchRetryTimeout) {	
shuffle output from failed retry it 

private void checkTimeoutOrRetry(MapHost host, IOException ioe) throws IOException {	long currentTime = Time.monotonicNow();	if (retryStartTime == 0) {	retryStartTime = currentTime;	}	if (currentTime - retryStartTime < this.fetchRetryTimeout) {	throw ioe;	} else {	
timeout for copying mapoutput with retry on host after milliseconds 

private boolean verifySanity(long compressedLength, long decompressedLength, int forReduce, Set<TaskAttemptID> remaining, TaskAttemptID mapId) {	if (compressedLength < 0 || decompressedLength < 0) {	wrongLengthErrs.increment(1);	
invalid lengths in map output header id len decomp len 

private boolean verifySanity(long compressedLength, long decompressedLength, int forReduce, Set<TaskAttemptID> remaining, TaskAttemptID mapId) {	if (compressedLength < 0 || decompressedLength < 0) {	wrongLengthErrs.increment(1);	return false;	}	if (forReduce != reduce) {	wrongReduceErrs.increment(1);	
data for the wrong reduce map len decomp len for reduce 

if (compressedLength < 0 || decompressedLength < 0) {	wrongLengthErrs.increment(1);	return false;	}	if (forReduce != reduce) {	wrongReduceErrs.increment(1);	return false;	}	if (!remaining.contains(mapId)) {	wrongMapErrs.increment(1);	
invalid map output received output for 

private URL getMapOutputURL(MapHost host, Collection<TaskAttemptID> maps )  throws MalformedURLException {	StringBuffer url = new StringBuffer(host.getBaseUrl());	boolean first = true;	for (TaskAttemptID mapId : maps) {	if (!first) {	url.append(",");	}	url.append(mapId);	first = false;	}	
mapoutput url for 

attempts++;	connection.connect();	break;	} catch (IOException ioe) {	long currentTime = Time.monotonicNow();	long retryTime = currentTime - startTime;	long leftTime = connectionTimeout - retryTime;	long timeSinceLastIteration = currentTime - lastTime;	if (leftTime <= 0) {	int retryTimeInSeconds = (int) retryTime/1000;	
connection retry failed with attempts in seconds 

throw ioe;	}	if (leftTime < unit) {	unit = (int)leftTime;	connection.setConnectTimeout(unit);	}	if (timeSinceLastIteration < unit) {	try {	sleep(unit - timeSinceLastIteration);	} catch (InterruptedException e) {	
sleep in connection retry get interrupted 

========================= hadoop sample_4939 =========================

cluster.waitActive();	DistributedFileSystem dfs = cluster.getFileSystem();	final String TEST_FILE = "/test_file2";	DFSTestUtil.createFile(dfs, new Path(TEST_FILE), BLOCK_SIZE, (short)1, 0xcafe);	dfs.addCachePool(new CachePoolInfo("pool"));	long cacheDirectiveId = dfs.addCacheDirective(new CacheDirectiveInfo.Builder(). setPool("pool").setPath(new Path(TEST_FILE)). setReplication((short) 1).build());	FsDatasetSpi<?> fsd = cluster.getDataNodes().get(0).getFSDataset();	DFSTestUtil.verifyExpectedCacheUsage(BLOCK_SIZE, 1, fsd);	FSDataInputStream in = dfs.open(new Path(TEST_FILE));	ByteBuffer buf = in.read(null, BLOCK_SIZE, EnumSet.noneOf(ReadOption.class));	
removing cache directive 

DistributedFileSystem dfs = cluster.getFileSystem();	final String TEST_FILE = "/test_file2";	DFSTestUtil.createFile(dfs, new Path(TEST_FILE), BLOCK_SIZE, (short)1, 0xcafe);	dfs.addCachePool(new CachePoolInfo("pool"));	long cacheDirectiveId = dfs.addCacheDirective(new CacheDirectiveInfo.Builder(). setPool("pool").setPath(new Path(TEST_FILE)). setReplication((short) 1).build());	FsDatasetSpi<?> fsd = cluster.getDataNodes().get(0).getFSDataset();	DFSTestUtil.verifyExpectedCacheUsage(BLOCK_SIZE, 1, fsd);	FSDataInputStream in = dfs.open(new Path(TEST_FILE));	ByteBuffer buf = in.read(null, BLOCK_SIZE, EnumSet.noneOf(ReadOption.class));	dfs.removeCacheDirective(cacheDirectiveId);	
finished removing cache directive 

========================= hadoop sample_7246 =========================

static Class<? extends MetadataStore> getMetadataStoreClass( Configuration conf) {	if (conf == null) {	return NullMetadataStore.class;	}	if (conf.get(S3_METADATA_STORE_IMPL) != null && LOG.isDebugEnabled()) {	
metastore option source 

children.add(new PathMetadata(prevStatus));	}	dirMeta = new DirListingMetadata(f, children, authoritative);	ms.put(dirMeta);	}	pathMetas.add(new PathMetadata(status));	prevStatus = status;	}	ms.put(pathMetas);	} catch (IOException ioe) {	
metadatastore put failure 

public static void addMoveAncestors(MetadataStore ms, Collection<Path> srcPaths, Collection<PathMetadata> dstMetas, Path srcRoot, Path srcPath, Path dstPath, String owner) {	if (isNullMetadataStore(ms)) {	return;	}	assertQualified(srcRoot, srcPath, dstPath);	if (srcPath.equals(srcRoot)) {	
skip moving ancestors of source root directory 

if (isNullMetadataStore(ms)) {	return;	}	assertQualified(srcRoot, srcPath, dstPath);	if (srcPath.equals(srcRoot)) {	return;	}	Path parentSrc = srcPath.getParent();	Path parentDst = dstPath.getParent();	while (parentSrc != null && !parentSrc.isRoot() && !parentSrc.equals(srcRoot) && !srcPaths.contains(parentSrc)) {	
renaming non listed parent to 

========================= hadoop sample_6007 =========================

try {	if (Shell.WINDOWS && linkFile.getParentFile() != null && !new Path(target).isAbsolute()) {	shExec = new ShellCommandExecutor(cmd, linkFile.getParentFile());	} else {	shExec = new ShellCommandExecutor(cmd);	}	shExec.execute();	} catch (Shell.ExitCodeException ec) {	int returnVal = ec.getExitCode();	if (Shell.WINDOWS && returnVal == SYMLINK_NO_PRIVILEGE) {	
fail to create symbolic links on windows the default security settings in windows disallow non elevated administrators and all non administrators from creating symbolic links this behavior can be changed in the local security policy management console 

if (Shell.WINDOWS && linkFile.getParentFile() != null && !new Path(target).isAbsolute()) {	shExec = new ShellCommandExecutor(cmd, linkFile.getParentFile());	} else {	shExec = new ShellCommandExecutor(cmd);	}	shExec.execute();	} catch (Shell.ExitCodeException ec) {	int returnVal = ec.getExitCode();	if (Shell.WINDOWS && returnVal == SYMLINK_NO_PRIVILEGE) {	} else if (returnVal != 0) {	
command failed with 

}	shExec.execute();	} catch (Shell.ExitCodeException ec) {	int returnVal = ec.getExitCode();	if (Shell.WINDOWS && returnVal == SYMLINK_NO_PRIVILEGE) {	} else if (returnVal != 0) {	}	return returnVal;	} catch (IOException e) {	if (LOG.isDebugEnabled()) {	
error while create symlink to exception 

public static int chmod(String filename, String perm, boolean recursive) throws IOException {	String [] cmd = Shell.getSetPermissionCommand(perm, recursive);	String[] args = new String[cmd.length + 1];	System.arraycopy(cmd, 0, args, 0, cmd.length);	args[cmd.length] = new File(filename).getPath();	ShellCommandExecutor shExec = new ShellCommandExecutor(args);	try {	shExec.execute();	}catch(IOException e) {	if(LOG.isDebugEnabled()) {	
error while changing permission exception 

public static String[] createJarWithClassPath(String inputClassPath, Path pwd, Path targetDir, Map<String, String> callerEnv) throws IOException {	Map<String, String> env = Shell.WINDOWS ? new CaseInsensitiveMap(callerEnv) : callerEnv;	String[] classPathEntries = inputClassPath.split(File.pathSeparator);	for (int i = 0; i < classPathEntries.length; ++i) {	classPathEntries[i] = StringUtils.replaceTokens(classPathEntries[i], StringUtils.ENV_VAR_PATTERN, env);	}	File workingDir = new File(pwd.toString());	if (!workingDir.mkdirs()) {	
mkdirs false for execution will continue 

String dstHost = dstUri.getHost();	if ((srcHost!=null) && (dstHost!=null)) {	if (srcHost.equals(dstHost)) {	return srcUri.getPort()==dstUri.getPort();	}	try {	srcHost = InetAddress.getByName(srcHost).getCanonicalHostName();	dstHost = InetAddress.getByName(dstHost).getCanonicalHostName();	} catch (UnknownHostException ue) {	if (LOG.isDebugEnabled()) {	
could not compare file systems unknown host 

========================= hadoop sample_4190 =========================

private void writeAndReadOneFile(int numWrites, int recordLength, int syncInterval) throws IOException {	final long MINIMUM_EXPECTED_TIME = 20;	
writing bytes to 

========================= hadoop sample_6338 =========================

private void cleanupTempFiles(JobContext context) {	try {	Configuration conf = context.getConfiguration();	Path targetWorkPath = new Path(conf.get(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH));	FileSystem targetFS = targetWorkPath.getFileSystem(conf);	String jobId = context.getJobID().toString();	deleteAttemptTempFiles(targetWorkPath, targetFS, jobId);	deleteAttemptTempFiles(targetWorkPath.getParent(), targetFS, jobId);	} catch (Throwable t) {	
unable to cleanup temp files 

private void deleteAttemptTempFiles(Path targetWorkPath, FileSystem targetFS, String jobId) throws IOException {	if (targetWorkPath == null) {	return;	}	FileStatus[] tempFiles = targetFS.globStatus( new Path(targetWorkPath, ".distcp.tmp." + jobId.replaceAll("job","attempt") + "*"));	if (tempFiles != null && tempFiles.length > 0) {	for (FileStatus file : tempFiles) {	
cleaning up 

private void cleanup(Configuration conf) {	Path metaFolder = new Path(conf.get(DistCpConstants.CONF_LABEL_META_FOLDER));	try {	FileSystem fs = metaFolder.getFileSystem(conf);	
cleaning up temporary work folder 

private void cleanup(Configuration conf) {	Path metaFolder = new Path(conf.get(DistCpConstants.CONF_LABEL_META_FOLDER));	try {	FileSystem fs = metaFolder.getFileSystem(conf);	fs.delete(metaFolder, true);	} catch (IOException ignore) {	
exception encountered 

private void concatFileChunks(Configuration conf) throws IOException {	
concat file chunks 

Text srcRelPath = new Text();	CopyListingFileStatus lastFileStatus = null;	LinkedList<Path> allChunkPaths = new LinkedList<Path>();	while (sourceReader.next(srcRelPath, srcFileStatus)) {	if (srcFileStatus.isDirectory()) {	continue;	}	Path targetFile = new Path(targetRoot.toString() + "/" + srcRelPath);	Path targetFileChunkPath = DistCpUtils.getSplitChunkPath(targetFile, srcFileStatus);	if (LOG.isDebugEnabled()) {	
add to concat 

lastFileStatus = null;	} else {	if (lastFileStatus == null) {	lastFileStatus = new CopyListingFileStatus(srcFileStatus);	} else {	if (!srcFileStatus.getPath().equals(lastFileStatus.getPath()) || srcFileStatus.getChunkOffset() != (lastFileStatus.getChunkOffset() + lastFileStatus.getChunkLength())) {	String emsg = "Inconsistent sequence file: current " + "chunk file " + srcFileStatus + " doesnt match prior " + "entry " + lastFileStatus;	if (!ignoreFailures) {	throw new IOException(emsg);	} else {	
skipping concat this set 

private void preserveFileAttributesForDirectories(Configuration conf) throws IOException {	String attrSymbols = conf.get(DistCpConstants.CONF_LABEL_PRESERVE_STATUS);	final boolean syncOrOverwrite = syncFolder || overwrite;	
about to preserve attributes 

Path targetFile = new Path(targetRoot.toString() + "/" + srcRelPath);	if (targetRoot.equals(targetFile) && syncOrOverwrite) continue;	FileSystem targetFS = targetFile.getFileSystem(conf);	DistCpUtils.preserve(targetFS, targetFile, srcFileStatus, attributes, preserveRawXattrs);	taskAttemptContext.progress();	taskAttemptContext.setStatus("Preserving status on directory entries. [" + sourceReader.getPosition() * 100 / totalLen + "%]");	}	} finally {	IOUtils.closeStream(sourceReader);	}	
preserved status on dir entries on target 

private void deleteMissing(Configuration conf) throws IOException {	
delete option is enabled about to remove entries from target that are missing in source 

Text trgtRelPath = new Text();	FileSystem targetFS = targetFinalPath.getFileSystem(conf);	boolean srcAvailable = sourceReader.next(srcRelPath, srcFileStatus);	while (targetReader.next(trgtRelPath, trgtFileStatus)) {	while (srcAvailable && trgtRelPath.compareTo(srcRelPath) > 0) {	srcAvailable = sourceReader.next(srcRelPath, srcFileStatus);	}	if (srcAvailable && trgtRelPath.equals(srcRelPath)) continue;	boolean result = targetFS.delete(trgtFileStatus.getPath(), true) || !targetFS.exists(trgtFileStatus.getPath());	if (result) {	
deleted missing at source 

} else {	throw new IOException("Unable to delete " + trgtFileStatus.getPath());	}	taskAttemptContext.progress();	taskAttemptContext.setStatus("Deleting missing files from target. [" + targetReader.getPosition() * 100 / totalLen + "%]");	}	} finally {	IOUtils.closeStream(sourceReader);	IOUtils.closeStream(targetReader);	}	
deleted from target 

private void commitData(Configuration conf) throws IOException {	Path workDir = new Path(conf.get(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH));	Path finalDir = new Path(conf.get(DistCpConstants.CONF_LABEL_TARGET_FINAL_PATH));	FileSystem targetFS = workDir.getFileSystem(conf);	
atomic commit enabled moving to 

private void commitData(Configuration conf) throws IOException {	Path workDir = new Path(conf.get(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH));	Path finalDir = new Path(conf.get(DistCpConstants.CONF_LABEL_TARGET_FINAL_PATH));	FileSystem targetFS = workDir.getFileSystem(conf);	if (targetFS.exists(finalDir) && targetFS.exists(workDir)) {	
pre existing final path found at 

private void commitData(Configuration conf) throws IOException {	Path workDir = new Path(conf.get(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH));	Path finalDir = new Path(conf.get(DistCpConstants.CONF_LABEL_TARGET_FINAL_PATH));	FileSystem targetFS = workDir.getFileSystem(conf);	if (targetFS.exists(finalDir) && targetFS.exists(workDir)) {	throw new IOException("Target-path can't be committed to because it " + "exists at " + finalDir + ". Copied data is in temp-dir: " + workDir + ". ");	}	boolean result = targetFS.rename(workDir, finalDir);	if (!result) {	
rename failed perhaps data already moved verifying 

Path finalDir = new Path(conf.get(DistCpConstants.CONF_LABEL_TARGET_FINAL_PATH));	FileSystem targetFS = workDir.getFileSystem(conf);	if (targetFS.exists(finalDir) && targetFS.exists(workDir)) {	throw new IOException("Target-path can't be committed to because it " + "exists at " + finalDir + ". Copied data is in temp-dir: " + workDir + ". ");	}	boolean result = targetFS.rename(workDir, finalDir);	if (!result) {	result = targetFS.exists(finalDir) && !targetFS.exists(workDir);	}	if (result) {	
data committed successfully to 

if (targetFS.exists(finalDir) && targetFS.exists(workDir)) {	throw new IOException("Target-path can't be committed to because it " + "exists at " + finalDir + ". Copied data is in temp-dir: " + workDir + ". ");	}	boolean result = targetFS.rename(workDir, finalDir);	if (!result) {	result = targetFS.exists(finalDir) && !targetFS.exists(workDir);	}	if (result) {	taskAttemptContext.setStatus("Data committed successfully to " + finalDir);	} else {	
unable to commit data to 

private void concatFileChunks(Configuration conf, Path targetFile, LinkedList<Path> allChunkPaths) throws IOException {	if (allChunkPaths.size() == 1) {	return;	}	if (LOG.isDebugEnabled()) {	
concat allchunksize 

if (allChunkPaths.size() == 1) {	return;	}	if (LOG.isDebugEnabled()) {	}	FileSystem dstfs = targetFile.getFileSystem(conf);	Path firstChunkFile = allChunkPaths.removeFirst();	Path[] restChunkFiles = new Path[allChunkPaths.size()];	allChunkPaths.toArray(restChunkFiles);	if (LOG.isDebugEnabled()) {	
concat firstchunk 

}	if (LOG.isDebugEnabled()) {	}	FileSystem dstfs = targetFile.getFileSystem(conf);	Path firstChunkFile = allChunkPaths.removeFirst();	Path[] restChunkFiles = new Path[allChunkPaths.size()];	allChunkPaths.toArray(restChunkFiles);	if (LOG.isDebugEnabled()) {	int i = 0;	for (Path f : restChunkFiles) {	
concat other chunk 

Path[] restChunkFiles = new Path[allChunkPaths.size()];	allChunkPaths.toArray(restChunkFiles);	if (LOG.isDebugEnabled()) {	int i = 0;	for (Path f : restChunkFiles) {	++i;	}	}	dstfs.concat(firstChunkFile, restChunkFiles);	if (LOG.isDebugEnabled()) {	
concat result 

========================= hadoop sample_6292 =========================

public void handle(ContainerSchedulerEvent event) {	switch (event.getType()) {	case SCHEDULE_CONTAINER: scheduleContainer(event.getContainer());	break;	case CONTAINER_PAUSED: case CONTAINER_COMPLETED: onResourcesReclaimed(event.getContainer());	break;	case UPDATE_CONTAINER: if (event instanceof UpdateContainerSchedulerEvent) {	onUpdateContainer((UpdateContainerSchedulerEvent) event);	} else {	
unknown event type on updatecontainer 

case CONTAINER_PAUSED: case CONTAINER_COMPLETED: onResourcesReclaimed(event.getContainer());	break;	case UPDATE_CONTAINER: if (event instanceof UpdateContainerSchedulerEvent) {	onUpdateContainer((UpdateContainerSchedulerEvent) event);	} else {	}	break;	case SHED_QUEUED_CONTAINERS: shedQueuedOpportunisticContainers();	break;	case RECOVERY_COMPLETED: startPendingContainers(maxOppQueueLength <= 0);	
unknown event arrived at containerscheduler 

public void recoverActiveContainer(Container container, RecoveredContainerStatus rcs) {	ExecutionType execType = container.getContainerTokenIdentifier().getExecutionType();	if (rcs == RecoveredContainerStatus.QUEUED || rcs == RecoveredContainerStatus.PAUSED) {	if (execType == ExecutionType.GUARANTEED) {	queuedGuaranteedContainers.put(container.getContainerId(), container);	} else if (execType == ExecutionType.OPPORTUNISTIC) {	queuedOpportunisticContainers .put(container.getContainerId(), container);	} else {	
unknown execution type received exectype 

private boolean enqueueContainer(Container container) {	boolean isGuaranteedContainer = container.getContainerTokenIdentifier(). getExecutionType() == ExecutionType.GUARANTEED;	boolean isQueued;	if (isGuaranteedContainer) {	queuedGuaranteedContainers.put(container.getContainerId(), container);	isQueued = true;	} else {	if (queuedOpportunisticContainers.size() < maxOppQueueLength) {	
opportunistic container will be queued at the nm 

boolean isGuaranteedContainer = container.getContainerTokenIdentifier(). getExecutionType() == ExecutionType.GUARANTEED;	boolean isQueued;	if (isGuaranteedContainer) {	queuedGuaranteedContainers.put(container.getContainerId(), container);	isQueued = true;	} else {	if (queuedOpportunisticContainers.size() < maxOppQueueLength) {	queuedOpportunisticContainers.put( container.getContainerId(), container);	isQueued = true;	} else {	
opportunistic container will not be queued at the nm since max queue length has been reached 

isQueued = true;	} else {	container.sendKillEvent( ContainerExitStatus.KILLED_BY_CONTAINER_SCHEDULER, "Opportunistic container queue is full.");	isQueued = false;	}	}	if (isQueued) {	try {	this.context.getNMStateStore().storeContainerQueued( container.getContainerId());	} catch (IOException e) {	
could not store container state the container has been queued 

private void reclaimOpportunisticContainerResources(Container container) {	List<Container> extraOppContainersToReclaim = pickOpportunisticContainersToReclaimResources( container.getContainerId());	for (Container contToReclaim : extraOppContainersToReclaim) {	String preemptionAction = usePauseEventForPreemption == true ? "paused" : "resumed";	
container will be to start the execution of guaranteed container 

private void startContainer(Container container) {	
starting container 

Container runningCont = lifoIterator.next();	if (runningCont.getContainerTokenIdentifier().getExecutionType() == ExecutionType.OPPORTUNISTIC) {	if (oppContainersToKill.containsKey( runningCont.getContainerId())) {	continue;	}	extraOpportContainersToKill.add(runningCont);	ContainersMonitor.ContainerManagerUtils.decreaseResourceUtilization( getContainersMonitor(), resourcesToFreeUp, runningCont.getResource());	}	}	if (!hasSufficientResources(resourcesToFreeUp)) {	
there are no sufficient resources to start guaranteed at the moment opportunistic containers are in the process of being killed to make room 

private void shedQueuedOpportunisticContainers() {	int numAllowed = this.queuingLimit.getMaxQueueLength();	Iterator<Container> containerIter = queuedOpportunisticContainers.values().iterator();	while (containerIter.hasNext()) {	Container container = containerIter.next();	if (container.getContainerState() != ContainerState.PAUSED) {	if (numAllowed <= 0) {	container.sendKillEvent( ContainerExitStatus.KILLED_BY_CONTAINER_SCHEDULER, "Container De-queued to meet NM queuing limits.");	containerIter.remove();	
opportunistic container will be killed to meet nm queuing limits 

========================= hadoop sample_1804 =========================

public boolean fence(HAServiceTarget fromSvc) {	LOG.info("====== Beginning Service Fencing Process... ======");	int i = 0;	for (FenceMethodWithArg method : methods) {	
trying method 

public boolean fence(HAServiceTarget fromSvc) {	LOG.info("====== Beginning Service Fencing Process... ======");	int i = 0;	for (FenceMethodWithArg method : methods) {	try {	if (method.method.tryFence(fromSvc, method.arg)) {	LOG.info("====== Fencing successful by method " + method + " ======");	return true;	}	} catch (BadFencingConfigurationException e) {	
fencing method misconfigured 

int i = 0;	for (FenceMethodWithArg method : methods) {	try {	if (method.method.tryFence(fromSvc, method.arg)) {	LOG.info("====== Fencing successful by method " + method + " ======");	return true;	}	} catch (BadFencingConfigurationException e) {	continue;	} catch (Throwable t) {	
fencing method failed with an unexpected error 

try {	if (method.method.tryFence(fromSvc, method.arg)) {	LOG.info("====== Fencing successful by method " + method + " ======");	return true;	}	} catch (BadFencingConfigurationException e) {	continue;	} catch (Throwable t) {	continue;	}	
fencing method was unsuccessful 

if (method.method.tryFence(fromSvc, method.arg)) {	LOG.info("====== Fencing successful by method " + method + " ======");	return true;	}	} catch (BadFencingConfigurationException e) {	continue;	} catch (Throwable t) {	continue;	}	}	
unable to fence service by any configured method 

========================= hadoop sample_3993 =========================

JobId jobId =  TypeConverter.toYarn( TypeConverter.fromYarn(applicationAttemptId.getApplicationId()));	Path start = MRApps.getStartJobCommitFile(conf, userName, jobId);	FileSystem fs = FileSystem.get(conf);	fs.create(start).close();	ContainerId containerId = ContainerId.fromString(containerIdStr);	MRAppMaster appMaster = new MRAppMasterTest(applicationAttemptId, containerId, "host", -1, -1, System.currentTimeMillis(), false, false);	boolean caught = false;	try {	MRAppMaster.initAndStartAppMaster(appMaster, conf, userName);	} catch (IOException e) {	
caught expected exception 

Path end = MRApps.getEndJobCommitSuccessFile(conf, userName, jobId);	FileSystem fs = FileSystem.get(conf);	fs.create(start).close();	fs.create(end).close();	ContainerId containerId = ContainerId.fromString(containerIdStr);	MRAppMaster appMaster = new MRAppMasterTest(applicationAttemptId, containerId, "host", -1, -1, System.currentTimeMillis(), false, false);	boolean caught = false;	try {	MRAppMaster.initAndStartAppMaster(appMaster, conf, userName);	} catch (IOException e) {	
caught expected exception 

Path end = MRApps.getEndJobCommitFailureFile(conf, userName, jobId);	FileSystem fs = FileSystem.get(conf);	fs.create(start).close();	fs.create(end).close();	ContainerId containerId = ContainerId.fromString(containerIdStr);	MRAppMaster appMaster = new MRAppMasterTest(applicationAttemptId, containerId, "host", -1, -1, System.currentTimeMillis(), false, false);	boolean caught = false;	try {	MRAppMaster.initAndStartAppMaster(appMaster, conf, userName);	} catch (IOException e) {	
caught expected exception 

File dir = new File(stagingDir);	if(dir.exists()) {	FileUtils.deleteDirectory(dir);	}	ContainerId containerId = ContainerId.fromString(containerIdStr);	MRAppMaster appMaster = new MRAppMasterTest(applicationAttemptId, containerId, "host", -1, -1, System.currentTimeMillis(), false, false);	boolean caught = false;	try {	MRAppMaster.initAndStartAppMaster(appMaster, conf, userName);	} catch (IOException e) {	
caught expected exception 

========================= hadoop sample_5144 =========================

protected void serviceInit(Configuration conf) throws Exception {	registryRoot = conf.getTrimmed(KEY_REGISTRY_ZK_ROOT, DEFAULT_ZK_REGISTRY_ROOT);	registrySecurity = new RegistrySecurity("registry security");	addService(registrySecurity);	if (LOG.isDebugEnabled()) {	
creating registry with root 

private CuratorFramework createCurator() throws IOException {	Configuration conf = getConfig();	createEnsembleProvider();	int sessionTimeout = conf.getInt(KEY_REGISTRY_ZK_SESSION_TIMEOUT, DEFAULT_ZK_SESSION_TIMEOUT);	int connectionTimeout = conf.getInt(KEY_REGISTRY_ZK_CONNECTION_TIMEOUT, DEFAULT_ZK_CONNECTION_TIMEOUT);	int retryTimes = conf.getInt(KEY_REGISTRY_ZK_RETRY_TIMES, DEFAULT_ZK_RETRY_TIMES);	int retryInterval = conf.getInt(KEY_REGISTRY_ZK_RETRY_INTERVAL, DEFAULT_ZK_RETRY_INTERVAL);	int retryCeiling = conf.getInt(KEY_REGISTRY_ZK_RETRY_CEILING, DEFAULT_ZK_RETRY_CEILING);	if (LOG.isDebugEnabled()) {	
creating curatorservice with connection 

public Stat zkStat(String path) throws IOException {	checkServiceLive();	String fullpath = createFullPath(path);	Stat stat;	try {	if (LOG.isDebugEnabled()) {	
stat 

public List<ACL> zkGetACLS(String path) throws IOException {	checkServiceLive();	String fullpath = createFullPath(path);	List<ACL> acls;	try {	if (LOG.isDebugEnabled()) {	
getacls 

public boolean zkMkPath(String path, CreateMode mode, boolean createParents, List<ACL> acls) throws IOException {	checkServiceLive();	path = createFullPath(path);	if (acls == null || acls.isEmpty()) {	throw new NoPathPermissionsException(path, "Empty ACL list");	}	try {	RegistrySecurity.AclListInfo aclInfo = new RegistrySecurity.AclListInfo(acls);	if (LOG.isDebugEnabled()) {	
creating path with mode and acl 

if (LOG.isDebugEnabled()) {	}	CreateBuilder createBuilder = curator.create();	createBuilder.withMode(mode).withACL(acls);	if (createParents) {	createBuilder.creatingParentsIfNeeded();	}	createBuilder.forPath(path);	} catch (KeeperException.NodeExistsException e) {	if (LOG.isDebugEnabled()) {	
path already present 

public void zkCreate(String path, CreateMode mode, byte[] data, List<ACL> acls) throws IOException {	Preconditions.checkArgument(data != null, "null data");	checkServiceLive();	String fullpath = createFullPath(path);	try {	if (LOG.isDebugEnabled()) {	
creating with bytes of data and acl 

public void zkUpdate(String path, byte[] data) throws IOException {	Preconditions.checkArgument(data != null, "null data");	checkServiceLive();	path = createFullPath(path);	try {	if (LOG.isDebugEnabled()) {	
updating with bytes 

public void zkDelete(String path, boolean recursive, BackgroundCallback backgroundCallback) throws IOException {	checkServiceLive();	String fullpath = createFullPath(path);	try {	if (LOG.isDebugEnabled()) {	
deleting 

public List<String> zkList(String path) throws IOException {	checkServiceLive();	String fullpath = createFullPath(path);	try {	if (LOG.isDebugEnabled()) {	
ls 

public byte[] zkRead(String path) throws IOException {	checkServiceLive();	String fullpath = createFullPath(path);	try {	if (LOG.isDebugEnabled()) {	
reading 

protected String dumpRegistryRobustly(boolean verbose) {	try {	ZKPathDumper pathDumper = dumpPath(verbose);	return pathDumper.toString();	} catch (Exception e) {	
ignoring exception 

========================= hadoop sample_2681 =========================

public void testAccessTimeParam() {	final AccessTimeParam p = new AccessTimeParam(AccessTimeParam.DEFAULT);	Assert.assertEquals(-1L, p.getValue().longValue());	new AccessTimeParam(-1L);	try {	new AccessTimeParam(-2L);	Assert.fail();	} catch(IllegalArgumentException e) {	
expected 

public void testBlockSizeParam() {	final BlockSizeParam p = new BlockSizeParam(BlockSizeParam.DEFAULT);	Assert.assertEquals(null, p.getValue());	Assert.assertEquals( conf.getLongBytes(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, DFSConfigKeys.DFS_BLOCK_SIZE_DEFAULT), p.getValue(conf));	new BlockSizeParam(1L);	try {	new BlockSizeParam(0L);	Assert.fail();	} catch(IllegalArgumentException e) {	
expected 

public void testBufferSizeParam() {	final BufferSizeParam p = new BufferSizeParam(BufferSizeParam.DEFAULT);	Assert.assertEquals(null, p.getValue());	Assert.assertEquals( conf.getInt(CommonConfigurationKeysPublic.IO_FILE_BUFFER_SIZE_KEY, CommonConfigurationKeysPublic.IO_FILE_BUFFER_SIZE_DEFAULT), p.getValue(conf));	new BufferSizeParam(1);	try {	new BufferSizeParam(0);	Assert.fail();	} catch(IllegalArgumentException e) {	
expected 

public void testDestinationParam() {	final DestinationParam p = new DestinationParam(DestinationParam.DEFAULT);	Assert.assertEquals(null, p.getValue());	new DestinationParam("/abc");	try {	new DestinationParam("abc");	Assert.fail();	} catch(IllegalArgumentException e) {	
expected 

public void testModificationTimeParam() {	final ModificationTimeParam p = new ModificationTimeParam(ModificationTimeParam.DEFAULT);	Assert.assertEquals(-1L, p.getValue().longValue());	new ModificationTimeParam(-1L);	try {	new ModificationTimeParam(-2L);	Assert.fail();	} catch(IllegalArgumentException e) {	
expected 

public void testOverwriteParam() {	final OverwriteParam p = new OverwriteParam(OverwriteParam.DEFAULT);	Assert.assertEquals(false, p.getValue());	new OverwriteParam("trUe");	try {	new OverwriteParam("abc");	Assert.fail();	} catch(IllegalArgumentException e) {	
expected 

public void testPermissionParam() {	final PermissionParam p = new PermissionParam(PermissionParam.DEFAULT);	Assert.assertEquals(new FsPermission((short)0755), p.getFsPermission());	new PermissionParam("0");	try {	new PermissionParam("-1");	Assert.fail();	} catch(IllegalArgumentException e) {	
expected 

try {	new PermissionParam("-1");	Assert.fail();	} catch(IllegalArgumentException e) {	}	new PermissionParam("1777");	try {	new PermissionParam("2000");	Assert.fail();	} catch(IllegalArgumentException e) {	
expected 

new PermissionParam("1777");	try {	new PermissionParam("2000");	Assert.fail();	} catch(IllegalArgumentException e) {	}	try {	new PermissionParam("8");	Assert.fail();	} catch(IllegalArgumentException e) {	
expected 

}	try {	new PermissionParam("8");	Assert.fail();	} catch(IllegalArgumentException e) {	}	try {	new PermissionParam("abc");	Assert.fail();	} catch(IllegalArgumentException e) {	
expected 

public void testRecursiveParam() {	final RecursiveParam p = new RecursiveParam(RecursiveParam.DEFAULT);	Assert.assertEquals(false, p.getValue());	new RecursiveParam("falSe");	try {	new RecursiveParam("abc");	Assert.fail();	} catch(IllegalArgumentException e) {	
expected 

public void testReplicationParam() {	final ReplicationParam p = new ReplicationParam(ReplicationParam.DEFAULT);	Assert.assertEquals(null, p.getValue());	Assert.assertEquals( (short)conf.getInt(DFSConfigKeys.DFS_REPLICATION_KEY, DFSConfigKeys.DFS_REPLICATION_DEFAULT), p.getValue(conf));	new ReplicationParam((short)1);	try {	new ReplicationParam((short)0);	Assert.fail();	} catch(IllegalArgumentException e) {	
expected 

public void testAclPermissionParam() {	final AclPermissionParam p = new AclPermissionParam("user::rwx,group::r--,other::rwx,user:user1:rwx");	List<AclEntry> setAclList = AclEntry.parseAclSpec("user::rwx,group::r--,other::rwx,user:user1:rwx", true);	Assert.assertEquals(setAclList.toString(), p.getAclPermission(true) .toString());	new AclPermissionParam("user::rw-,group::rwx,other::rw-,user:user1:rwx");	try {	new AclPermissionParam("user::rw--,group::rwx-,other::rw-");	Assert.fail();	} catch (IllegalArgumentException e) {	
expected 

try {	new AclPermissionParam("user::rw--,group::rwx-,other::rw-");	Assert.fail();	} catch (IllegalArgumentException e) {	}	new AclPermissionParam( "user::rw-,group::rwx,other::rw-,user:user1:rwx,group:group1:rwx,other::rwx,mask::rwx,default:user:user1:rwx");	try {	new AclPermissionParam("user:r-,group:rwx,other:rw-");	Assert.fail();	} catch (IllegalArgumentException e) {	
expected 

new AclPermissionParam( "user::rw-,group::rwx,other::rw-,user:user1:rwx,group:group1:rwx,other::rwx,mask::rwx,default:user:user1:rwx");	try {	new AclPermissionParam("user:r-,group:rwx,other:rw-");	Assert.fail();	} catch (IllegalArgumentException e) {	}	try {	new AclPermissionParam("default:::r-,default:group::rwx,other::rw-");	Assert.fail();	} catch (IllegalArgumentException e) {	
expected 

}	try {	new AclPermissionParam("default:::r-,default:group::rwx,other::rw-");	Assert.fail();	} catch (IllegalArgumentException e) {	}	try {	new AclPermissionParam("user:r-,group::rwx,other:rw-,mask:rw-,temp::rwx");	Assert.fail();	} catch (IllegalArgumentException e) {	
expected 

new FsActionParam("r-x");	new FsActionParam("-wx");	new FsActionParam("r--");	new FsActionParam("-w-");	new FsActionParam("--x");	new FsActionParam("---");	try {	new FsActionParam("rw");	Assert.fail();	} catch(IllegalArgumentException e) {	
expected 

new FsActionParam("---");	try {	new FsActionParam("rw");	Assert.fail();	} catch(IllegalArgumentException e) {	}	try {	new FsActionParam("qwx");	Assert.fail();	} catch(IllegalArgumentException e) {	
expected 

}	try {	new FsActionParam("qwx");	Assert.fail();	} catch(IllegalArgumentException e) {	}	try {	new FsActionParam("qrwx");	Assert.fail();	} catch(IllegalArgumentException e) {	
expected 

}	try {	new FsActionParam("qrwx");	Assert.fail();	} catch(IllegalArgumentException e) {	}	try {	new FsActionParam("rwxx");	Assert.fail();	} catch(IllegalArgumentException e) {	
expected 

}	try {	new FsActionParam("rwxx");	Assert.fail();	} catch(IllegalArgumentException e) {	}	try {	new FsActionParam("xwr");	Assert.fail();	} catch(IllegalArgumentException e) {	
expected 

}	try {	new FsActionParam("xwr");	Assert.fail();	} catch(IllegalArgumentException e) {	}	try {	new FsActionParam("r-w");	Assert.fail();	} catch(IllegalArgumentException e) {	
expected 

========================= hadoop sample_7179 =========================

private void waitForState(ApplicationId appId, EnumSet<RMAppState> finalStates) throws InterruptedException {	drainEventsImplicitly();	RMApp app = getRMContext().getRMApps().get(appId);	Assert.assertNotNull("app shouldn't be null", app);	final int timeoutMsecs = 80 * SECOND;	int timeWaiting = 0;	while (!finalStates.contains(app.getState())) {	if (timeWaiting >= timeoutMsecs) {	break;	}	
app state is waiting for state 

Assert.assertNotNull("app shouldn't be null", app);	final int timeoutMsecs = 80 * SECOND;	int timeWaiting = 0;	while (!finalStates.contains(app.getState())) {	if (timeWaiting >= timeoutMsecs) {	break;	}	Thread.sleep(WAIT_MS_PER_LOOP);	timeWaiting += WAIT_MS_PER_LOOP;	}	
app state is 

public void waitForState(ApplicationId appId, RMAppState finalState) throws InterruptedException {	drainEventsImplicitly();	RMApp app = getRMContext().getRMApps().get(appId);	Assert.assertNotNull("app shouldn't be null", app);	final int timeoutMsecs = 80 * SECOND;	int timeWaiting = 0;	while (!finalState.equals(app.getState())) {	if (timeWaiting >= timeoutMsecs) {	break;	}	
app state is waiting for state 

Assert.assertNotNull("app shouldn't be null", app);	final int timeoutMsecs = 80 * SECOND;	int timeWaiting = 0;	while (!finalState.equals(app.getState())) {	if (timeWaiting >= timeoutMsecs) {	break;	}	Thread.sleep(WAIT_MS_PER_LOOP);	timeWaiting += WAIT_MS_PER_LOOP;	}	
app state is 

public static void waitForState(RMAppAttempt attempt, RMAppAttemptState finalState, int timeoutMsecs) throws InterruptedException {	int timeWaiting = 0;	while (finalState != attempt.getAppAttemptState()) {	if (timeWaiting >= timeoutMsecs) {	break;	}	
appattempt state is waiting for state 

public static void waitForState(RMAppAttempt attempt, RMAppAttemptState finalState, int timeoutMsecs) throws InterruptedException {	int timeWaiting = 0;	while (finalState != attempt.getAppAttemptState()) {	if (timeWaiting >= timeoutMsecs) {	break;	}	Thread.sleep(WAIT_MS_PER_LOOP);	timeWaiting += WAIT_MS_PER_LOOP;	}	
attempt state is 

public void waitForContainerToComplete(RMAppAttempt attempt, NMContainerStatus completedContainer) throws InterruptedException {	drainEventsImplicitly();	int timeWaiting = 0;	while (timeWaiting < TIMEOUT_MS_FOR_CONTAINER_AND_NODE) {	List<ContainerStatus> containers = attempt.getJustFinishedContainers();	
received completed containers 

public MockAM waitForNewAMToLaunchAndRegister(ApplicationId appId, int attemptSize, MockNM nm) throws Exception {	RMApp app = getRMContext().getRMApps().get(appId);	Assert.assertNotNull(app);	int timeWaiting = 0;	while (app.getAppAttempts().size() != attemptSize) {	if (timeWaiting >= TIMEOUT_MS_FOR_ATTEMPT) {	break;	}	
application is waiting for am to restart current has attempts 

int timeWaiting = 0;	while (container == null) {	if (timeWaiting >= timeoutMsecs) {	return false;	}	for (MockNM nm : nms) {	nm.nodeHeartbeat(true);	}	drainEventsImplicitly();	container = getResourceScheduler().getRMContainer(containerId);	
waiting for container to be container is null right now 

}	drainEventsImplicitly();	container = getResourceScheduler().getRMContainer(containerId);	Thread.sleep(WAIT_MS_PER_LOOP);	timeWaiting += WAIT_MS_PER_LOOP;	}	while (!containerState.equals(container.getState())) {	if (timeWaiting >= timeoutMsecs) {	return false;	}	
container state is waiting for state 

if (timeWaiting >= timeoutMsecs) {	return false;	}	for (MockNM nm : nms) {	nm.nodeHeartbeat(true);	}	drainEventsImplicitly();	Thread.sleep(WAIT_MS_PER_LOOP);	timeWaiting += WAIT_MS_PER_LOOP;	}	
container state is 

}	node = getRMNode(nodeId);	Thread.sleep(WAIT_MS_PER_LOOP);	timeWaiting += WAIT_MS_PER_LOOP;	}	Assert.assertNotNull("node shouldn't be null (timedout)", node);	while (!finalState.equals(node.getState())) {	if (timeWaiting >= TIMEOUT_MS_FOR_CONTAINER_AND_NODE) {	break;	}	
node state is waiting for state 

timeWaiting += WAIT_MS_PER_LOOP;	}	Assert.assertNotNull("node shouldn't be null (timedout)", node);	while (!finalState.equals(node.getState())) {	if (timeWaiting >= TIMEOUT_MS_FOR_CONTAINER_AND_NODE) {	break;	}	Thread.sleep(WAIT_MS_PER_LOOP);	timeWaiting += WAIT_MS_PER_LOOP;	}	
node state is 

public static MockAM launchAM(RMApp app, MockRM rm, MockNM nm) throws Exception {	rm.drainEventsImplicitly();	RMAppAttempt attempt = waitForAttemptScheduled(app, rm);	
launch am 

public static MockAM launchUAM(RMApp app, MockRM rm, MockNM nm) throws Exception {	rm.drainEventsImplicitly();	rm.waitForState(app.getApplicationId(), RMAppState.ACCEPTED);	RMAppAttempt attempt = app.getCurrentAppAttempt();	waitForSchedulerAppAttemptAdded(attempt.getAppAttemptId(), rm);	
launch am 

public void waitForAppRemovedFromScheduler(ApplicationId appId) throws InterruptedException {	int timeWaiting = 0;	drainEventsImplicitly();	Map<ApplicationId, SchedulerApplication> apps  = ((AbstractYarnScheduler) getResourceScheduler()) .getSchedulerApplications();	while (apps.containsKey(appId)) {	if (timeWaiting >= TIMEOUT_MS_FOR_APP_REMOVED) {	break;	}	
wait for app removed 

drainEventsImplicitly();	Map<ApplicationId, SchedulerApplication> apps  = ((AbstractYarnScheduler) getResourceScheduler()) .getSchedulerApplications();	while (apps.containsKey(appId)) {	if (timeWaiting >= TIMEOUT_MS_FOR_APP_REMOVED) {	break;	}	Thread.sleep(WAIT_MS_PER_LOOP);	timeWaiting += WAIT_MS_PER_LOOP;	}	Assert.assertTrue("app is not removed from scheduler (timeout).", !apps.containsKey(appId));	
app is removed from scheduler 

========================= hadoop sample_575 =========================

public void testHistoryEvents() throws Exception {	Configuration conf = new Configuration();	MRApp app = new MRAppWithHistory(2, 1, true, this.getClass().getName(), true);	app.submit(conf);	Job job = app.getContext().getAllJobs().values().iterator().next();	JobId jobId = job.getID();	
jobid is 

public void testEventsFlushOnStop() throws Exception {	Configuration conf = new Configuration();	MRApp app = new MRAppWithSpecialHistoryHandler(1, 0, true, this .getClass().getName(), true);	app.submit(conf);	Job job = app.getContext().getAllJobs().values().iterator().next();	JobId jobId = job.getID();	
jobid is 

public void testAssignedQueue() throws Exception {	Configuration conf = new Configuration();	MRApp app = new MRAppWithHistory(2, 1, true, this.getClass().getName(), true, "assignedQueue");	app.submit(conf);	Job job = app.getContext().getAllJobs().values().iterator().next();	JobId jobId = job.getID();	
jobid is 

========================= hadoop sample_5328 =========================

public void testFileOutput() throws Throwable {	File f = new File("target/kdiag.txt");	kdiag(ARG_KEYLEN, KEYLEN, ARG_KEYTAB, keytab.getAbsolutePath(), ARG_PRINCIPAL, "foo@EXAMPLE.COM", ARG_OUTPUT, f.getAbsolutePath());	
output of 

========================= hadoop sample_2951 =========================

protected void serviceInit(Configuration conf) throws Exception {	
applicationhistory init 

protected void serviceStart() throws Exception {	
starting applicationhistory 

protected void serviceStop() throws Exception {	
stopping applicationhistory 

========================= hadoop sample_2052 =========================

savedBlock = block;	}	if (testedLoad && (loadedBlock == null)) {	loadedBlock = block;	assertEquals(savedBlock, loadedBlock);	}	boolean blockRemoved = blocks.remove(block);	assertTrue("Found unknown block " + block, blockRemoved);	if (blocksProcessed > (numFiles / 3)) {	if (!testedSave) {	
processed blocks out of saving iterator 

assertTrue("Found unknown block " + block, blockRemoved);	if (blocksProcessed > (numFiles / 3)) {	if (!testedSave) {	iter.save();	testedSave = true;	savedBlocksProcessed = blocksProcessed;	}	}	if (blocksProcessed > (numFiles / 2)) {	if (!testedRewind) {	
processed blocks out of rewinding iterator 

}	}	if (blocksProcessed > (numFiles / 2)) {	if (!testedRewind) {	iter.rewind();	break;	}	}	if (blocksProcessed > ((2 * numFiles) / 3)) {	if (!testedLoad) {	
processed blocks out of loading iterator 

if (!testedLoad) {	iter = volume.loadBlockIterator(ctx.bpids[0], "test");	iter.setMaxStalenessMs(maxStaleness);	break;	}	}	}	if (!testedRewind) {	testedRewind = true;	blocksProcessed = 0;	
starting again at the beginning 

}	}	if (!testedRewind) {	testedRewind = true;	blocksProcessed = 0;	continue;	}	if (!testedLoad) {	testedLoad = true;	blocksProcessed = savedBlocksProcessed;	
starting again at the load point 

public void setup(VolumeScanner scanner) {	this.scanner = scanner;	Info info = getInfo(scanner.volume);	
about to start scanning 

this.scanner = scanner;	Info info = getInfo(scanner.volume);	synchronized (info) {	while (!info.shouldRun) {	try {	info.wait();	} catch (InterruptedException e) {	}	}	}	
starting scanning 

public void handle(ExtendedBlock block, IOException e) {	
handling block exception 

}	ctx.cluster.restartDataNode(0);	synchronized (info) {	info.shouldRun = true;	info.notify();	}	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	synchronized (info) {	if (info.blocksScanned != 9) {	
waiting for blocksscanned to reach it is at 

final TestScanResultHandler.Info info = TestScanResultHandler.getInfo(ctx.volumes.get(0));	synchronized (info) {	info.shouldRun = true;	info.notify();	}	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	synchronized (info) {	Statistics stats = ctx.blockScanner.getVolumeStats( ctx.volumes.get(0).getStorageID());	if (stats.scansSinceRestart < 3) {	
waiting for scanssincerestart to reach it is 

info.notify();	}	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	synchronized (info) {	Statistics stats = ctx.blockScanner.getVolumeStats( ctx.volumes.get(0).getStorageID());	if (stats.scansSinceRestart < 3) {	return false;	}	if (!stats.eof) {	
waiting for eof 

final TestContext ctx = new TestContext(conf, 1);	final int NUM_EXPECTED_BLOCKS = 10;	ctx.createFiles(0, NUM_EXPECTED_BLOCKS, 1);	final TestScanResultHandler.Info info = TestScanResultHandler.getInfo(ctx.volumes.get(0));	String storageID = ctx.volumes.get(0).getStorageID();	synchronized (info) {	info.sem = new Semaphore(4);	info.shouldRun = true;	info.notify();	}	
waiting for the first blocks to be scanned 

synchronized (info) {	assertEquals("Expected 4 good blocks.", 4, info.goodBlocks.size());	info.goodBlocks.clear();	assertEquals("Expected 4 blocksScanned", 4, info.blocksScanned);	assertEquals("Did not expect bad blocks.", 0, info.badBlocks.size());	info.blocksScanned = 0;	}	ExtendedBlock first = ctx.getFileBlock(0, 0);	ctx.datanode.getBlockScanner().markSuspectBlock(storageID, first);	info.sem.release(2);	
waiting for more blocks to be scanned 

synchronized (info) {	assertTrue("Expected block " + first + " to have been scanned.", info.goodBlocks.contains(first));	assertEquals(2, info.goodBlocks.size());	info.goodBlocks.clear();	assertEquals("Did not expect bad blocks.", 0, info.badBlocks.size());	assertEquals(2, info.blocksScanned);	info.blocksScanned = 0;	}	ctx.datanode.getBlockScanner().markSuspectBlock(storageID, first);	info.sem.release(10);	
waiting for more blocks to be scanned 

MaterializedReplica unreachableReplica = ctx.getMaterializedReplica(0, 1);	ExtendedBlock unreachableBlock = ctx.getFileBlock(0, 1);	unreachableReplica.makeUnreachable();	final TestScanResultHandler.Info info = TestScanResultHandler.getInfo(ctx.volumes.get(0));	String storageID = ctx.volumes.get(0).getStorageID();	synchronized (info) {	info.sem = new Semaphore(NUM_FILES);	info.shouldRun = true;	info.notify();	}	
waiting for the blocks to be scanned 

private void waitForRescan(final TestScanResultHandler.Info info, final int numExpectedBlocks) throws TimeoutException, InterruptedException {	
waiting for the first blocks to be scanned 

========================= hadoop sample_7250 =========================

private void startApplication(OutputCollector<K3, V3> output, Reporter reporter) throws IOException {	if (application == null) {	try {	
starting application 

}	};	startApplication(nullCollector, Reporter.NULL);	}	try {	if (isOk) {	application.getDownlink().endOfInput();	} else {	application.getDownlink().abort();	}	
waiting for finish 

};	startApplication(nullCollector, Reporter.NULL);	}	try {	if (isOk) {	application.getDownlink().endOfInput();	} else {	application.getDownlink().abort();	}	application.waitForFinish();	
got done 

========================= hadoop sample_4702 =========================

public CSAssignment allocateContainersToNode( PlacementSet<FiCaSchedulerNode> ps, boolean withNodeHeartbeat) {	try {	Thread.sleep(1000);	} catch(InterruptedException e) {	
thread interrupted 

========================= hadoop sample_522 =========================

protected void closeConnection() {	try {	if (null != this.connection) {	this.connection.close();	this.connection = null;	}	} catch (SQLException sqlE) {	
exception on close 

========================= hadoop sample_5056 =========================

public boolean initDriver() {	String rootDir = getRootDir();	try {	if (rootDir == null) {	
invalid root directory unable to initialize driver 

public boolean initDriver() {	String rootDir = getRootDir();	try {	if (rootDir == null) {	return false;	}	if (!exists(rootDir)) {	if (!mkdir(rootDir)) {	
cannot create state store root directory 

try {	if (rootDir == null) {	return false;	}	if (!exists(rootDir)) {	if (!mkdir(rootDir)) {	return false;	}	}	} catch (Exception ex) {	
cannot initialize filesystem using root directory 

public <T extends BaseRecord> boolean initRecordStorage( String className, Class<T> recordClass) {	String dataDirPath = getRootDir() + "/" + className;	try {	if (!exists(dataDirPath)) {	
data directory doesn t exist creating it 

public <T extends BaseRecord> boolean initRecordStorage( String className, Class<T> recordClass) {	String dataDirPath = getRootDir() + "/" + className;	try {	if (!exists(dataDirPath)) {	if (!mkdir(dataDirPath)) {	
cannot create data directory 

String dataDirPath = getRootDir() + "/" + className;	try {	if (!exists(dataDirPath)) {	if (!mkdir(dataDirPath)) {	return false;	}	String dataFilePath = dataDirPath + "/" + DATA_FILE_NAME;	if (!exists(dataFilePath)) {	List<T> emtpyList = new ArrayList<>();	if(!writeAll(emtpyList, recordClass)) {	
cannot create data file 

}	String dataFilePath = dataDirPath + "/" + DATA_FILE_NAME;	if (!exists(dataFilePath)) {	List<T> emtpyList = new ArrayList<>();	if(!writeAll(emtpyList, recordClass)) {	return false;	}	}	}	} catch (Exception ex) {	
cannot create data directory 

private <T extends BaseRecord> List<T> getAllFile( BufferedReader reader, Class<T> clazz, boolean includeDates) throws IOException {	List<T> ret = new ArrayList<T>();	String line;	while ((line = reader.readLine()) != null) {	if (!line.startsWith("#") && line.length() > 0) {	try {	T record = newRecord(line, clazz, includeDates);	ret.add(record);	} catch (Exception ex) {	
cannot parse line in data source file 

public <T extends BaseRecord> QueryResult<T> get(Class<T> clazz, String sub) throws IOException {	verifyDriverReady();	BufferedReader reader = null;	lockRecordRead(clazz);	try {	reader = getReader(clazz, sub);	List<T> data = getAllFile(reader, clazz, true);	return new QueryResult<T>(data, getTime());	} catch (Exception ex) {	
cannot fetch records 

reader = getReader(clazz, sub);	List<T> data = getAllFile(reader, clazz, true);	return new QueryResult<T>(data, getTime());	} catch (Exception ex) {	throw new IOException("Cannot read from data store " + ex.getMessage());	} finally {	if (reader != null) {	try {	reader.close();	} catch (IOException e) {	
failed closing file 

private <T extends BaseRecord> boolean writeAllFile( Collection<T> records, BufferedWriter writer) {	try {	for (BaseRecord record : records) {	try {	String data = serializeString(record);	writer.write(data);	writer.newLine();	} catch (IllegalArgumentException ex) {	
cannot write record to file 

try {	String data = serializeString(record);	writer.write(data);	writer.newLine();	} catch (IllegalArgumentException ex) {	}	}	writer.flush();	return true;	} catch (IOException e) {	
cannot commit records to file 

public <T extends BaseRecord> boolean writeAll( Collection<T> records, Class<T> recordClass) throws StateStoreUnavailableException {	verifyDriverReady();	lockRecordWrite(recordClass);	BufferedWriter writer = null;	try {	writer = getWriter(recordClass, null);	return writeAllFile(records, writer);	} catch (Exception e) {	
cannot add records to file for 

try {	writer = getWriter(recordClass, null);	return writeAllFile(records, writer);	} catch (Exception e) {	return false;	} finally {	if (writer != null) {	try {	writer.close();	} catch (IOException e) {	
cannot close writer for 

for (T updatedRecord : records) {	try {	updatedRecord.validate();	String key = updatedRecord.getPrimaryKey();	if (writeList.containsKey(key) && allowUpdate) {	writeList.put(key, updatedRecord);	updatedRecord.setDateModified(this.getTime());	} else if (!writeList.containsKey(key)) {	writeList.put(key, updatedRecord);	} else if (errorIfExists) {	
attempt to insert record that already exists 

String key = updatedRecord.getPrimaryKey();	if (writeList.containsKey(key) && allowUpdate) {	writeList.put(key, updatedRecord);	updatedRecord.setDateModified(this.getTime());	} else if (!writeList.containsKey(key)) {	writeList.put(key, updatedRecord);	} else if (errorIfExists) {	return false;	}	} catch (IllegalArgumentException ex) {	
cannot write invalid record to state store 

final List<T> newRecords = new LinkedList<>();	for (T record : existingRecords) {	if (!recordsToRemove.contains(record)) {	newRecords.add(record);	}	}	if (!writeAll(newRecords, clazz)) {	throw new IOException( "Cannot remove record " + clazz + " query " + query);	}	} catch (IOException e) {	
cannot remove records query 

========================= hadoop sample_8234 =========================

static void lengthTest(GridmixRecord x, GridmixRecord y, int min, int max) throws Exception {	final Random r = new Random();	final long seed = r.nextLong();	r.setSeed(seed);	
length 

static void randomReplayTest(GridmixRecord x, GridmixRecord y, int min, int max) throws Exception {	final Random r = new Random();	final long seed = r.nextLong();	r.setSeed(seed);	
randreplay 

static void eqSeedTest(GridmixRecord x, GridmixRecord y, int max) throws Exception {	final Random r = new Random();	final long s = r.nextLong();	r.setSeed(s);	
eqseed 

static void binSortTest(GridmixRecord x, GridmixRecord y, int min, int max, WritableComparator cmp) throws Exception {	final Random r = new Random();	final long s = r.nextLong();	r.setSeed(s);	
sort 

static void checkSpec(GridmixKey a, GridmixKey b) throws Exception {	final Random r = new Random();	final long s = r.nextLong();	r.setSeed(s);	
spec 

========================= hadoop sample_6103 =========================

while (true) {	long start = Time.now();	try {	if (in != null) {	in.read(buf);	} else {	out.write(buf);	}	} catch (SocketTimeoutException e) {	long diff = Time.now() - start;	
got sockettimeoutexception as expected after millis 

doIO(in, null, TIMEOUT);	((SocketInputStream)in).setTimeout(TIMEOUT * 2);	doIO(in, null, TIMEOUT * 2);	((SocketInputStream)in).setTimeout(0);	TestingThread thread = new TestingThread(ctx) {	public void doWork() throws Exception {	try {	in.read();	fail("Did not fail with interrupt");	} catch (InterruptedIOException ste) {	
got expection while reading as expected 

========================= hadoop sample_2843 =========================

conf.setBoolean(HdfsClientConfigKeys.Read.ShortCircuit.KEY, false);	MiniDFSCluster cluster = null;	FSDataInputStream[] streams = new FSDataInputStream[NUM_OPENS];	try {	cluster = new MiniDFSCluster.Builder(conf).build();	DistributedFileSystem dfs = cluster.getFileSystem();	final Path TEST_PATH = new Path("/testFile");	DFSTestUtil.createFile(dfs, TEST_PATH, 131072, (short)1, 1);	for (int i = 0; i < NUM_OPENS; i++) {	streams[i] = dfs.open(TEST_PATH);	
opening file 

========================= hadoop sample_7712 =========================

protected void startStorage() throws IOException {	Path storeRoot = createStorageDir(getConfig());	Options options = new Options();	options.createIfMissing(false);	options.logger(new LeveldbLogger());	
using state database at for recovery 

protected void startStorage() throws IOException {	Path storeRoot = createStorageDir(getConfig());	Options options = new Options();	options.createIfMissing(false);	options.logger(new LeveldbLogger());	File dbfile = new File(storeRoot.toString());	try {	db = JniDBFactory.factory.open(dbfile, options);	} catch (NativeDB.DBException e) {	if (e.isNotFound() || e.getMessage().contains(" does not exist ")) {	
creating state database at 

public HistoryServerState loadState() throws IOException {	HistoryServerState state = new HistoryServerState();	int numKeys = loadTokenMasterKeys(state);	
recovered token master keys 

public HistoryServerState loadState() throws IOException {	HistoryServerState state = new HistoryServerState();	int numKeys = loadTokenMasterKeys(state);	int numTokens = loadTokens(state);	
recovered tokens 

try {	iter = new LeveldbIterator(db);	iter.seek(bytes(TOKEN_MASTER_KEY_KEY_PREFIX));	while (iter.hasNext()) {	Entry<byte[],byte[]> entry = iter.next();	String key = asString(entry.getKey());	if (!key.startsWith(TOKEN_MASTER_KEY_KEY_PREFIX)) {	break;	}	if (LOG.isDebugEnabled()) {	
loading master key from 

try {	iter = new LeveldbIterator(db);	iter.seek(bytes(TOKEN_STATE_KEY_PREFIX));	while (iter.hasNext()) {	Entry<byte[],byte[]> entry = iter.next();	String key = asString(entry.getKey());	if (!key.startsWith(TOKEN_STATE_KEY_PREFIX)) {	break;	}	if (LOG.isDebugEnabled()) {	
loading token from 

public void storeToken(MRDelegationTokenIdentifier tokenId, Long renewDate) throws IOException {	if (LOG.isDebugEnabled()) {	
storing token 

public void storeTokenMasterKey(DelegationKey masterKey) throws IOException {	if (LOG.isDebugEnabled()) {	
storing master key 

public void removeTokenMasterKey(DelegationKey masterKey) throws IOException {	if (LOG.isDebugEnabled()) {	
removing master key 

private void checkVersion() throws IOException {	Version loadedVersion = loadVersion();	
loaded state version info 

private void checkVersion() throws IOException {	Version loadedVersion = loadVersion();	if (loadedVersion.equals(getCurrentVersion())) {	return;	}	if (loadedVersion.isCompatibleTo(getCurrentVersion())) {	
storing state version info 

========================= hadoop sample_5390 =========================

public void tearDown() throws Exception {	
shutting down minidfscluster 

========================= hadoop sample_7207 =========================

protected void serviceInit(Configuration conf) throws Exception {	this.monitoringInterval = conf.getLong(YarnConfiguration.NM_CONTAINER_MON_INTERVAL_MS, conf.getLong(YarnConfiguration.NM_RESOURCE_MON_INTERVAL_MS, YarnConfiguration.DEFAULT_NM_RESOURCE_MON_INTERVAL_MS));	this.resourceCalculatorPlugin = ResourceCalculatorPlugin.getContainersMonitorPlugin(conf);	
using resourcecalculatorplugin 

protected void serviceInit(Configuration conf) throws Exception {	this.monitoringInterval = conf.getLong(YarnConfiguration.NM_CONTAINER_MON_INTERVAL_MS, conf.getLong(YarnConfiguration.NM_RESOURCE_MON_INTERVAL_MS, YarnConfiguration.DEFAULT_NM_RESOURCE_MON_INTERVAL_MS));	this.resourceCalculatorPlugin = ResourceCalculatorPlugin.getContainersMonitorPlugin(conf);	processTreeClass = conf.getClass(YarnConfiguration.NM_CONTAINER_MON_PROCESS_TREE, null, ResourceCalculatorProcessTree.class);	this.conf = conf;	
using resourcecalculatorprocesstree 

this.containerMetricsUnregisterDelayMs = conf.getLong( YarnConfiguration.NM_CONTAINER_METRICS_UNREGISTER_DELAY_MS, YarnConfiguration.DEFAULT_NM_CONTAINER_METRICS_UNREGISTER_DELAY_MS);	long configuredPMemForContainers = NodeManagerHardwareUtils.getContainerMemoryMB( this.resourceCalculatorPlugin, conf) * 1024 * 1024L;	long configuredVCoresForContainers = NodeManagerHardwareUtils.getVCores(this.resourceCalculatorPlugin, conf);	this.maxPmemAllottedForContainers = configuredPMemForContainers;	this.maxVCoresAllottedForContainers = configuredVCoresForContainers;	vmemRatio = conf.getFloat(YarnConfiguration.NM_VMEM_PMEM_RATIO, YarnConfiguration.DEFAULT_NM_VMEM_PMEM_RATIO);	Preconditions.checkArgument(vmemRatio > 0.99f, YarnConfiguration.NM_VMEM_PMEM_RATIO + " should be at least 1.0");	this.maxVmemAllottedForContainers = (long) (vmemRatio * configuredPMemForContainers);	pmemCheckEnabled = conf.getBoolean(YarnConfiguration.NM_PMEM_CHECK_ENABLED, YarnConfiguration.DEFAULT_NM_PMEM_CHECK_ENABLED);	vmemCheckEnabled = conf.getBoolean(YarnConfiguration.NM_VMEM_CHECK_ENABLED, YarnConfiguration.DEFAULT_NM_VMEM_CHECK_ENABLED);	
physical memory check enabled 

this.containerMetricsUnregisterDelayMs = conf.getLong( YarnConfiguration.NM_CONTAINER_METRICS_UNREGISTER_DELAY_MS, YarnConfiguration.DEFAULT_NM_CONTAINER_METRICS_UNREGISTER_DELAY_MS);	long configuredPMemForContainers = NodeManagerHardwareUtils.getContainerMemoryMB( this.resourceCalculatorPlugin, conf) * 1024 * 1024L;	long configuredVCoresForContainers = NodeManagerHardwareUtils.getVCores(this.resourceCalculatorPlugin, conf);	this.maxPmemAllottedForContainers = configuredPMemForContainers;	this.maxVCoresAllottedForContainers = configuredVCoresForContainers;	vmemRatio = conf.getFloat(YarnConfiguration.NM_VMEM_PMEM_RATIO, YarnConfiguration.DEFAULT_NM_VMEM_PMEM_RATIO);	Preconditions.checkArgument(vmemRatio > 0.99f, YarnConfiguration.NM_VMEM_PMEM_RATIO + " should be at least 1.0");	this.maxVmemAllottedForContainers = (long) (vmemRatio * configuredPMemForContainers);	pmemCheckEnabled = conf.getBoolean(YarnConfiguration.NM_PMEM_CHECK_ENABLED, YarnConfiguration.DEFAULT_NM_PMEM_CHECK_ENABLED);	vmemCheckEnabled = conf.getBoolean(YarnConfiguration.NM_VMEM_CHECK_ENABLED, YarnConfiguration.DEFAULT_NM_VMEM_CHECK_ENABLED);	
virtual memory check enabled 

long configuredPMemForContainers = NodeManagerHardwareUtils.getContainerMemoryMB( this.resourceCalculatorPlugin, conf) * 1024 * 1024L;	long configuredVCoresForContainers = NodeManagerHardwareUtils.getVCores(this.resourceCalculatorPlugin, conf);	this.maxPmemAllottedForContainers = configuredPMemForContainers;	this.maxVCoresAllottedForContainers = configuredVCoresForContainers;	vmemRatio = conf.getFloat(YarnConfiguration.NM_VMEM_PMEM_RATIO, YarnConfiguration.DEFAULT_NM_VMEM_PMEM_RATIO);	Preconditions.checkArgument(vmemRatio > 0.99f, YarnConfiguration.NM_VMEM_PMEM_RATIO + " should be at least 1.0");	this.maxVmemAllottedForContainers = (long) (vmemRatio * configuredPMemForContainers);	pmemCheckEnabled = conf.getBoolean(YarnConfiguration.NM_PMEM_CHECK_ENABLED, YarnConfiguration.DEFAULT_NM_PMEM_CHECK_ENABLED);	vmemCheckEnabled = conf.getBoolean(YarnConfiguration.NM_VMEM_CHECK_ENABLED, YarnConfiguration.DEFAULT_NM_VMEM_CHECK_ENABLED);	containersMonitorEnabled = isContainerMonitorEnabled() && monitoringInterval > 0;	
containersmonitor enabled 

this.maxVmemAllottedForContainers = (long) (vmemRatio * configuredPMemForContainers);	pmemCheckEnabled = conf.getBoolean(YarnConfiguration.NM_PMEM_CHECK_ENABLED, YarnConfiguration.DEFAULT_NM_PMEM_CHECK_ENABLED);	vmemCheckEnabled = conf.getBoolean(YarnConfiguration.NM_VMEM_CHECK_ENABLED, YarnConfiguration.DEFAULT_NM_VMEM_CHECK_ENABLED);	containersMonitorEnabled = isContainerMonitorEnabled() && monitoringInterval > 0;	nodeCpuPercentageForYARN = NodeManagerHardwareUtils.getNodeCpuPercentage(conf);	if (pmemCheckEnabled) {	long totalPhysicalMemoryOnNM = UNKNOWN_MEMORY_LIMIT;	if (this.resourceCalculatorPlugin != null) {	totalPhysicalMemoryOnNM = this.resourceCalculatorPlugin .getPhysicalMemorySize();	if (totalPhysicalMemoryOnNM <= 0) {	
nodemanager s totalpmem could not be calculated setting it to 

nodeCpuPercentageForYARN = NodeManagerHardwareUtils.getNodeCpuPercentage(conf);	if (pmemCheckEnabled) {	long totalPhysicalMemoryOnNM = UNKNOWN_MEMORY_LIMIT;	if (this.resourceCalculatorPlugin != null) {	totalPhysicalMemoryOnNM = this.resourceCalculatorPlugin .getPhysicalMemorySize();	if (totalPhysicalMemoryOnNM <= 0) {	totalPhysicalMemoryOnNM = UNKNOWN_MEMORY_LIMIT;	}	}	if (totalPhysicalMemoryOnNM != UNKNOWN_MEMORY_LIMIT && this.maxPmemAllottedForContainers > totalPhysicalMemoryOnNM * 0.80f) {	
nodemanager configured with physical memory allocated to containers which is more than of the total physical memory available thrashing might happen 

private boolean isResourceCalculatorAvailable() {	if (resourceCalculatorPlugin == null) {	
resourcecalculatorplugin is unavailable on this system is disabled 

private boolean isResourceCalculatorAvailable() {	if (resourceCalculatorPlugin == null) {	return false;	}	if (ResourceCalculatorProcessTree .getResourceCalculatorProcessTree("0", processTreeClass, conf) == null) {	
resourcecalculatorprocesstree is unavailable on this system is disabled 

public void run() {	while (!stopped && !Thread.currentThread().isInterrupted()) {	if (LOG.isDebugEnabled()) {	StringBuilder tmp = new StringBuilder("[ ");	for (ProcessTreeInfo p : trackingContainers.values()) {	tmp.append(p.getPID());	tmp.append(" ");	}	
current processtree list 

long cpuUsageTotalCoresByAllContainers = 0;	for (Entry<ContainerId, ProcessTreeInfo> entry : trackingContainers .entrySet()) {	ContainerId containerId = entry.getKey();	ProcessTreeInfo ptInfo = entry.getValue();	try {	String pId = ptInfo.getPID();	if (pId == null) {	pId = containerExecutor.getProcessId(ptInfo.getContainerId());	if (pId != null) {	if (LOG.isDebugEnabled()) {	
tracking processtree for the first time 

if (containerMetricsEnabled) {	ContainerMetrics usageMetrics = ContainerMetrics .forContainer(containerId, containerMetricsPeriodMs, containerMetricsUnregisterDelayMs);	usageMetrics.recordProcessId(pId);	}	Container container = context.getContainers().get(containerId);	String[] ipAndHost = containerExecutor.getIpAndHost(container);	if (ipAndHost != null && ipAndHost[0] != null && ipAndHost[1] != null) {	container.setIpAndHost(ipAndHost);	LOG.info(containerId + "'s ip = " + ipAndHost[0] + ", and hostname = " + ipAndHost[1]);	} else {	
can not get both ip and hostname 

}	if (LOG.isDebugEnabled()) {	LOG.debug("Constructing ProcessTree for : PID = " + pId + " ContainerId = " + containerId);	}	ResourceCalculatorProcessTree pTree = ptInfo.getProcessTree();	pTree.updateProcessTree();	long currentVmemUsage = pTree.getVirtualMemorySize();	long currentPmemUsage = pTree.getRssMemorySize();	float cpuUsagePercentPerCore = pTree.getCpuUsagePercent();	if (cpuUsagePercentPerCore < 0) {	
skipping monitoring container since cpu usage is not yet available 

float cpuUsagePercentPerCore = pTree.getCpuUsagePercent();	if (cpuUsagePercentPerCore < 0) {	continue;	}	float cpuUsageTotalCoresPercentage = cpuUsagePercentPerCore / resourceCalculatorPlugin.getNumProcessors();	int milliVcoresUsed = (int) (cpuUsageTotalCoresPercentage * 1000 long curMemUsageOfAgedProcesses = pTree.getVirtualMemorySize(1);	long curRssMemUsageOfAgedProcesses = pTree.getRssMemorySize(1);	long vmemLimit = ptInfo.getVmemLimit();	long pmemLimit = ptInfo.getPmemLimit();	if (LOG.isDebugEnabled()) {	
memory usage of processtree s for container id s 

isMemoryOverLimit = true;	containerExitStatus = ContainerExitStatus.KILLED_EXCEEDED_PMEM;	}	vmemUsageByAllContainers += currentVmemUsage;	pmemByAllContainers += currentPmemUsage;	cpuUsagePercentPerCoreByAllContainers += cpuUsagePercentPerCore;	cpuUsageTotalCoresByAllContainers += cpuUsagePercentPerCore;	if (isMemoryOverLimit) {	LOG.warn(msg);	if (!pTree.checkPidPgrpidForMatch()) {	
killed container process with pid but it is not a process group leader 

vmemUsageByAllContainers += currentVmemUsage;	pmemByAllContainers += currentPmemUsage;	cpuUsagePercentPerCoreByAllContainers += cpuUsagePercentPerCore;	cpuUsageTotalCoresByAllContainers += cpuUsagePercentPerCore;	if (isMemoryOverLimit) {	LOG.warn(msg);	if (!pTree.checkPidPgrpidForMatch()) {	}	eventDispatcher.getEventHandler().handle( new ContainerKillEvent(containerId, containerExitStatus, msg));	trackingContainers.remove(containerId);	
removed processtree with root 

}	eventDispatcher.getEventHandler().handle( new ContainerKillEvent(containerId, containerExitStatus, msg));	trackingContainers.remove(containerId);	}	ContainerImpl container = (ContainerImpl) context.getContainers().get(containerId);	NMTimelinePublisher nmMetricsPublisher = container.getNMTimelinePublisher();	if (nmMetricsPublisher != null) {	nmMetricsPublisher.reportContainerResourceUsage(container, currentPmemUsage, cpuUsagePercentPerCore);	}	} catch (Exception e) {	
uncaught exception in containersmonitorimpl while monitoring resource of 

} catch (Exception e) {	}	}	if (LOG.isDebugEnabled()) {	LOG.debug("Total Resource Usage stats in NM by all containers : " + "Virtual Memory= " + vmemUsageByAllContainers + ", Physical Memory= " + pmemByAllContainers + ", Total CPU usage= " + cpuUsageTotalCoresByAllContainers + ", Total CPU(% per core) usage" + cpuUsagePercentPerCoreByAllContainers);	}	setContainersUtilization(trackedContainersUtilization);	try {	Thread.sleep(monitoringInterval);	} catch (InterruptedException e) {	
is interrupted exiting 

protected void onChangeMonitoringContainerResource( ContainersMonitorEvent monitoringEvent, ContainerId containerId) {	ChangeMonitoringContainerResourceEvent changeEvent = (ChangeMonitoringContainerResourceEvent) monitoringEvent;	ProcessTreeInfo processTreeInfo = trackingContainers.get(containerId);	if (processTreeInfo == null) {	
failed to track container it may have already completed 

protected void onChangeMonitoringContainerResource( ContainersMonitorEvent monitoringEvent, ContainerId containerId) {	ChangeMonitoringContainerResourceEvent changeEvent = (ChangeMonitoringContainerResourceEvent) monitoringEvent;	ProcessTreeInfo processTreeInfo = trackingContainers.get(containerId);	if (processTreeInfo == null) {	return;	}	
changing resource monitoring for 

protected void onStopMonitoringContainer( ContainersMonitorEvent monitoringEvent, ContainerId containerId) {	
stopping resource monitoring for 

protected void onStartMonitoringContainer( ContainersMonitorEvent monitoringEvent, ContainerId containerId) {	ContainerStartMonitoringEvent startEvent = (ContainerStartMonitoringEvent) monitoringEvent;	
starting resource monitoring for 

========================= hadoop sample_1783 =========================

public void handle(AMLauncherEvent event) {	if (AMLauncherEventType.LAUNCH == event.getType()) {	ApplicationId appId = event.getAppAttempt().getAppAttemptId().getApplicationId();	for (AMSimulator ams : amMap.values()) {	if (ams.getApplicationId() != null && ams.getApplicationId().equals( appId)) {	try {	Container amContainer = event.getAppAttempt().getMasterContainer();	setupAMRMToken(event.getAppAttempt());	super.context.getDispatcher().getEventHandler().handle( new RMAppAttemptEvent(event.getAppAttempt().getAppAttemptId(), RMAppAttemptEventType.LAUNCHED));	ams.notifyAMContainerLaunched( event.getAppAttempt().getMasterContainer());	
notify am launcher launched 

========================= hadoop sample_6046 =========================

public void testAggregateMetricInit() {	
test aggregate metrics are initialized correctly 

public void testAggregateMetricInit() {	Assert.assertEquals(0, FederationStateStoreClientMetrics.getNumSucceededCalls());	Assert.assertEquals(0, FederationStateStoreClientMetrics.getNumFailedCalls());	
test aggregate metrics are updated correctly 

public void testSuccessfulCalls() {	
test aggregate and method successful calls updated correctly 

public void testSuccessfulCalls() {	long totalGoodBefore = FederationStateStoreClientMetrics.getNumSucceededCalls();	long apiGoodBefore = FederationStateStoreClientMetrics .getNumSucceessfulCallsForMethod("registerSubCluster");	goodStateStore.registerSubCluster(100);	Assert.assertEquals(totalGoodBefore + 1, FederationStateStoreClientMetrics.getNumSucceededCalls());	Assert.assertEquals(100, FederationStateStoreClientMetrics.getLatencySucceededCalls(), 0);	Assert.assertEquals(apiGoodBefore + 1, FederationStateStoreClientMetrics.getNumSucceededCalls());	Assert.assertEquals(100, FederationStateStoreClientMetrics .getLatencySucceessfulCallsForMethod("registerSubCluster"), 0);	
test running stats correctly calculated for metrics 

public void testFailedCalls() {	long totalBadbefore = FederationStateStoreClientMetrics.getNumFailedCalls();	long apiBadBefore = FederationStateStoreClientMetrics .getNumFailedCallsForMethod("registerSubCluster");	badStateStore.registerSubCluster();	
test aggregate and method failed calls updated correctly 

public void testCallsUnknownMethod() {	long totalBadbefore = FederationStateStoreClientMetrics.getNumFailedCalls();	long apiBadBefore = FederationStateStoreClientMetrics .getNumFailedCallsForMethod("registerSubCluster");	long totalGoodBefore = FederationStateStoreClientMetrics.getNumSucceededCalls();	long apiGoodBefore = FederationStateStoreClientMetrics .getNumSucceessfulCallsForMethod("registerSubCluster");	
calling metrics class directly 

public void testCallsUnknownMethod() {	long totalBadbefore = FederationStateStoreClientMetrics.getNumFailedCalls();	long apiBadBefore = FederationStateStoreClientMetrics .getNumFailedCallsForMethod("registerSubCluster");	long totalGoodBefore = FederationStateStoreClientMetrics.getNumSucceededCalls();	long apiGoodBefore = FederationStateStoreClientMetrics .getNumSucceessfulCallsForMethod("registerSubCluster");	FederationStateStoreClientMetrics.failedStateStoreCall();	FederationStateStoreClientMetrics.succeededStateStoreCall(100);	
test aggregate and method calls did not update 

public void registerSubCluster() {	
mocked failed registersubcluster call 

public void registerSubCluster(long duration) {	
mocked successful registersubcluster call with duration 

========================= hadoop sample_1279 =========================

if(oversleepMap.get(ctx.getContainer().getContainerId().toString()) == true) {	Thread.sleep(10000);	}	} catch (InterruptedException e) {	}	}	return super.launchContainer(ctx);	}	public void pauseContainer(Container container) {	oversleepMap.put(container.getContainerId().toString(), true);	
container was paused 

}	} catch (InterruptedException e) {	}	}	return super.launchContainer(ctx);	}	public void pauseContainer(Container container) {	oversleepMap.put(container.getContainerId().toString(), true);	}	public void resumeContainer(Container container) {	
container was resumed 

========================= hadoop sample_1629 =========================

public synchronized ProxyInfo<T> getProxy() {	AddressRpcProxyPair<T> current = proxies.get(currentProxyIndex);	if (current.namenode == null) {	try {	current.namenode = factory.createProxy(conf, current.address, xface, ugi, false, getFallbackToSimpleAuth());	} catch (IOException e) {	
failed to create rpc proxy to namenode 

========================= hadoop sample_6898 =========================

public void testDnRestartWithSavedReplicas() throws IOException, InterruptedException, TimeoutException {	getClusterBuilder().build();	final String METHOD_NAME = GenericTestUtils.getMethodName();	Path path1 = new Path("/" + METHOD_NAME + ".01.dat");	makeTestFile(path1, BLOCK_SIZE, true);	ensureFileReplicasOnStorageType(path1, RAM_DISK);	Thread.sleep(3 * LAZY_WRITER_INTERVAL_SEC * 1000);	ensureFileReplicasOnStorageType(path1, RAM_DISK);	
restarting the datanode 

public void testDnRestartWithUnsavedReplicas() throws IOException, InterruptedException, TimeoutException {	getClusterBuilder().build();	FsDatasetTestUtil.stopLazyWriter(cluster.getDataNodes().get(0));	final String METHOD_NAME = GenericTestUtils.getMethodName();	Path path1 = new Path("/" + METHOD_NAME + ".01.dat");	makeTestFile(path1, BLOCK_SIZE, true);	ensureFileReplicasOnStorageType(path1, RAM_DISK);	
restarting the datanode 

========================= hadoop sample_7270 =========================

store.storeNewClusterNodeLabels(storeNewClusterNodeLabelsEvent .getLabels());	break;	case REMOVE_LABELS: RemoveClusterNodeLabels removeClusterNodeLabelsEvent = (RemoveClusterNodeLabels) event;	store.removeClusterNodeLabels(removeClusterNodeLabelsEvent.getLabels());	break;	case STORE_NODE_TO_LABELS: UpdateNodeToLabelsMappingsEvent updateNodeToLabelsMappingsEvent = (UpdateNodeToLabelsMappingsEvent) event;	store.updateNodeToLabelsMappings(updateNodeToLabelsMappingsEvent .getNodeToLabels());	break;	}	} catch (IOException e) {	
failed to store label modification to storage 

}	for (NodeLabel label : labels) {	if (this.labelCollections.get(label.getName()) == null) {	this.labelCollections.put(label.getName(), new RMNodeLabel(label));	newLabels.add(label);	}	}	if (null != dispatcher && !newLabels.isEmpty()) {	dispatcher.getEventHandler().handle( new StoreNewClusterNodeLabels(newLabels));	}	
add labels 

}	}	}	}	for (String label : labelsToRemove) {	labelCollections.remove(label);	}	if (null != dispatcher) {	dispatcher.getEventHandler().handle( new RemoveClusterNodeLabels(labelsToRemove));	}	
remove labels 

if (nm.labels != null) {	nm.labels.removeAll(labels);	newNMToLabels.put(nodeId, nm.labels);	}	}	}	}	if (null != dispatcher && isCentralizedNodeLabelConfiguration) {	dispatcher.getEventHandler().handle( new UpdateNodeToLabelsMappingsEvent(newNMToLabels));	}	
labels on nodes 

if (nodeLabelInfo != null) {	Set<NodeId> nodeIds = nodeLabelInfo.getAssociatedNodeIds();	if (!nodeIds.isEmpty()) {	if (type.isAssignableFrom(String.class)) {	labelsToNodes.put(type.cast(label), nodeIds);	} else {	labelsToNodes.put(type.cast(nodeLabelInfo.getNodeLabel()), nodeIds);	}	}	} else {	
getlabelstonodes label cannot be found 

========================= hadoop sample_2567 =========================

private boolean hasDelegationToken(URL url, AuthenticatedURL.Token token) {	boolean hasDt = false;	if (token instanceof DelegationTokenAuthenticatedURL.Token) {	hasDt = ((DelegationTokenAuthenticatedURL.Token) token). getDelegationToken() != null;	if (hasDt) {	
delegation token found 

========================= hadoop sample_3726 =========================

public void testJceAesCtrCryptoCodec() throws Exception {	GenericTestUtils.assumeInNativeProfile();	if (!NativeCodeLoader.buildSupportsOpenssl()) {	
skipping test since openssl library not loaded 

public void testOpensslAesCtrCryptoCodec() throws Exception {	GenericTestUtils.assumeInNativeProfile();	if (!NativeCodeLoader.buildSupportsOpenssl()) {	
skipping test since openssl library not loaded 

private void cryptoCodecTest(Configuration conf, int seed, int count, String encCodecClass, String decCodecClass, byte[] iv) throws IOException, GeneralSecurityException {	CryptoCodec encCodec = null;	try {	encCodec = (CryptoCodec)ReflectionUtils.newInstance( conf.getClassByName(encCodecClass), conf);	} catch (ClassNotFoundException cnfe) {	throw new IOException("Illegal crypto codec!");	}	
created a codec object of type 

}	DataOutputBuffer data = new DataOutputBuffer();	RandomDatum.Generator generator = new RandomDatum.Generator(seed);	for(int i = 0; i < count; ++i) {	generator.next();	RandomDatum key = generator.getKey();	RandomDatum value = generator.getValue();	key.write(data);	value.write(data);	}	
generated records 

RandomDatum key = generator.getKey();	RandomDatum value = generator.getValue();	key.write(data);	value.write(data);	}	DataOutputBuffer encryptedDataBuffer = new DataOutputBuffer();	CryptoOutputStream out = new CryptoOutputStream(encryptedDataBuffer, encCodec, bufferSize, key, iv);	out.write(data.getData(), 0, data.getLength());	out.flush();	out.close();	
finished encrypting data 

CryptoOutputStream out = new CryptoOutputStream(encryptedDataBuffer, encCodec, bufferSize, key, iv);	out.write(data.getData(), 0, data.getLength());	out.flush();	out.close();	CryptoCodec decCodec = null;	try {	decCodec = (CryptoCodec)ReflectionUtils.newInstance( conf.getClassByName(decCodecClass), conf);	} catch (ClassNotFoundException cnfe) {	throw new IOException("Illegal crypto codec!");	}	
created a codec object of type 

decryptedDataBuffer.reset(encryptedDataBuffer.getData(), 0, encryptedDataBuffer.getLength());	in = new CryptoInputStream(new TestCryptoStreams.FakeInputStream( decryptedDataBuffer), decCodec, bufferSize, key, iv);	int seekPos = data.getLength() / 3;	in.seek(seekPos);	TestCryptoStreams.FakeInputStream originalInput = new TestCryptoStreams.FakeInputStream(originalData);	originalInput.seek(seekPos);	do {	expected = originalInput.read();	assertEquals("Decrypted stream read by byte does not match", expected, in.read());	} while (expected != -1);	
success completed checking records 

========================= hadoop sample_2834 =========================

private void delete(SwiftNativeFileSystem fs, Path path) {	try {	if (!fs.delete(path, false)) {	
failed to delete 

private void delete(SwiftNativeFileSystem fs, Path path) {	try {	if (!fs.delete(path, false)) {	}	} catch (IOException e) {	
deleting 

private void deleteR(SwiftNativeFileSystem fs, Path path) {	try {	if (!fs.delete(path, true)) {	
failed to delete 

private void deleteR(SwiftNativeFileSystem fs, Path path) {	try {	if (!fs.delete(path, true)) {	}	} catch (IOException e) {	
deleting 

public void testCreateDirWithFileParent() throws Throwable {	Path path = new Path("/test/CreateDirWithFileParent");	Path child = new Path(path, "subdir/child");	fs.mkdirs(path.getParent());	try {	writeTextFile(fs, path, "parent", true);	try {	fs.mkdirs(child);	} catch (ParentNotDirectoryException expected) {	
expected exception 

========================= hadoop sample_6174 =========================

List<R> newRecords = null;	long t = -1;	try {	QueryResult<R> result = getDriver().get(getRecordClass());	newRecords = result.getRecords();	t = result.getTimestamp();	if (this.override) {	overrideExpiredRecords(result);	}	} catch (IOException e) {	
cannot get records from the state store 

public void overrideExpiredRecords(QueryResult<R> query) throws IOException {	List<R> commitRecords = new ArrayList<>();	List<R> newRecords = query.getRecords();	long currentDriverTime = query.getTimestamp();	if (newRecords == null || currentDriverTime <= 0) {	
cannot check overrides for record 

public void overrideExpiredRecords(QueryResult<R> query) throws IOException {	List<R> commitRecords = new ArrayList<>();	List<R> newRecords = query.getRecords();	long currentDriverTime = query.getTimestamp();	if (newRecords == null || currentDriverTime <= 0) {	return;	}	for (R record : newRecords) {	if (record.checkExpired(currentDriverTime)) {	String recordName = StateStoreUtils.getRecordName(record.getClass());	
override state store record 

========================= hadoop sample_8243 =========================

for (long t = now;	(t < plan.getLastEndTime() && t < (now + lengthOfCheckZone));	t += plan.getStep()) {	Resource excessCap = Resources.subtract(plan.getTotalCommittedResources(t), totCap);	if (Resources.greaterThan(resCalc, totCap, excessCap, ZERO_RESOURCE)) {	Set<ReservationAllocation> curReservations = new TreeSet<ReservationAllocation>(plan.getReservationsAtTime(t));	for (Iterator<ReservationAllocation> resIter = curReservations.iterator(); resIter.hasNext() && Resources.greaterThan(resCalc, totCap, excessCap, ZERO_RESOURCE);) {	ReservationAllocation reservation = resIter.next();	plan.deleteReservation(reservation.getReservationId());	excessCap = Resources.subtract(excessCap, reservation.getResourcesAtTime(t));	
removing reservation to repair physical resource constraints in the plan 

========================= hadoop sample_1062 =========================

for (Map.Entry<NodeId, RMNode> entry : rmContext.getInactiveRMNodes().entrySet()) {	NodeId nodeId = entry.getKey();	RMNode rmNode = entry.getValue();	if (isUntrackedNode(rmNode.getHostName())) {	if (rmNode.getUntrackedTimeStamp() == 0) {	rmNode.setUntrackedTimeStamp(now);	} else if (now - rmNode.getUntrackedTimeStamp() > nodeRemovalTimeout) {	RMNode result = rmContext.getInactiveRMNodes().remove(nodeId);	if (result != null) {	decrInactiveNMMetrics(rmNode);	
removed node from inactive nodes list 

ClusterMetrics clusterMetrics = ClusterMetrics.getMetrics();	switch (rmNode.getState()) {	case SHUTDOWN: clusterMetrics.decrNumShutdownNMs();	break;	case DECOMMISSIONED: clusterMetrics.decrDecommisionedNMs();	break;	case LOST: clusterMetrics.decrNumLostNMs();	break;	case REBOOTED: clusterMetrics.decrNumRebootedNMs();	break;	
unexpected node state 

private void printConfiguredHosts() {	if (!LOG.isDebugEnabled()) {	return;	}	LOG.debug("hostsReader: in=" + conf.get(YarnConfiguration.RM_NODES_INCLUDE_FILE_PATH, YarnConfiguration.DEFAULT_RM_NODES_INCLUDE_FILE_PATH) + " out=" + conf.get(YarnConfiguration.RM_NODES_EXCLUDE_FILE_PATH, YarnConfiguration.DEFAULT_RM_NODES_EXCLUDE_FILE_PATH));	HostDetails hostDetails = hostsReader.getHostDetails();	for (String include : hostDetails.getIncludedHosts()) {	
include 

private void printConfiguredHosts() {	if (!LOG.isDebugEnabled()) {	return;	}	LOG.debug("hostsReader: in=" + conf.get(YarnConfiguration.RM_NODES_INCLUDE_FILE_PATH, YarnConfiguration.DEFAULT_RM_NODES_INCLUDE_FILE_PATH) + " out=" + conf.get(YarnConfiguration.RM_NODES_EXCLUDE_FILE_PATH, YarnConfiguration.DEFAULT_RM_NODES_EXCLUDE_FILE_PATH));	HostDetails hostDetails = hostsReader.getHostDetails();	for (String include : hostDetails.getIncludedHosts()) {	}	for (String exclude : hostDetails.getExcludedHosts()) {	
exclude 

private void refreshHostsReader( Configuration yarnConf, boolean graceful, Integer timeout) throws IOException, YarnException {	if (null == yarnConf) {	yarnConf = new YarnConfiguration();	}	includesFile = yarnConf.get(YarnConfiguration.RM_NODES_INCLUDE_FILE_PATH, YarnConfiguration.DEFAULT_RM_NODES_INCLUDE_FILE_PATH);	excludesFile = yarnConf.get(YarnConfiguration.RM_NODES_EXCLUDE_FILE_PATH, YarnConfiguration.DEFAULT_RM_NODES_EXCLUDE_FILE_PATH);	
refreshnodes excludesfile 

private void refreshHostsReader( Configuration yarnConf, boolean graceful, Integer timeout) throws IOException, YarnException {	if (null == yarnConf) {	yarnConf = new YarnConfiguration();	}	includesFile = yarnConf.get(YarnConfiguration.RM_NODES_INCLUDE_FILE_PATH, YarnConfiguration.DEFAULT_RM_NODES_INCLUDE_FILE_PATH);	excludesFile = yarnConf.get(YarnConfiguration.RM_NODES_EXCLUDE_FILE_PATH, YarnConfiguration.DEFAULT_RM_NODES_EXCLUDE_FILE_PATH);	hostsReader.refresh(includesFile, excludesFile);	printConfiguredHosts();	
hostsreader include exclude 

List<RMNode> nodesToDecom = new ArrayList<RMNode>();	HostDetails hostDetails = hostsReader.getHostDetails();	Set<String> includes = hostDetails.getIncludedHosts();	Set<String> excludes = hostDetails.getExcludedHosts();	for (RMNode n : this.rmContext.getRMNodes().values()) {	NodeState s = n.getState();	boolean isExcluded = !isValidNode( n.getHostName(), includes, excludes);	String nodeStr = "node " + n.getNodeID() + " with state " + s;	if (!isExcluded) {	if (s == NodeState.DECOMMISSIONING) {	
recommission 

NodeState s = n.getState();	boolean isExcluded = !isValidNode( n.getHostName(), includes, excludes);	String nodeStr = "node " + n.getNodeID() + " with state " + s;	if (!isExcluded) {	if (s == NodeState.DECOMMISSIONING) {	nodesToRecom.add(n);	}	} else {	if (graceful) {	if (s != NodeState.DECOMMISSIONED && s != NodeState.DECOMMISSIONING) {	
gracefully decommission 

String nodeStr = "node " + n.getNodeID() + " with state " + s;	if (!isExcluded) {	if (s == NodeState.DECOMMISSIONING) {	nodesToRecom.add(n);	}	} else {	if (graceful) {	if (s != NodeState.DECOMMISSIONED && s != NodeState.DECOMMISSIONING) {	nodesToDecom.add(n);	} else if (s == NodeState.DECOMMISSIONING && !Objects.equals(n.getDecommissioningTimeout(), timeout)) {	
update timeout to be 

if (s == NodeState.DECOMMISSIONING) {	nodesToRecom.add(n);	}	} else {	if (graceful) {	if (s != NodeState.DECOMMISSIONED && s != NodeState.DECOMMISSIONING) {	nodesToDecom.add(n);	} else if (s == NodeState.DECOMMISSIONING && !Objects.equals(n.getDecommissioningTimeout(), timeout)) {	nodesToDecom.add(n);	} else {	
no action for 

} else {	if (graceful) {	if (s != NodeState.DECOMMISSIONED && s != NodeState.DECOMMISSIONING) {	nodesToDecom.add(n);	} else if (s == NodeState.DECOMMISSIONING && !Objects.equals(n.getDecommissioningTimeout(), timeout)) {	nodesToDecom.add(n);	} else {	}	} else {	if (s != NodeState.DECOMMISSIONED) {	
forcefully decommission 

public void run() {	long currentTime = clock.getTime();	Iterator<Map.Entry<String, CacheEntry>> iterator = cache.entrySet().iterator();	while (iterator.hasNext()) {	Map.Entry<String, CacheEntry> entry = iterator.next();	if (currentTime > entry.getValue().resolveTime + CachedResolver.this.expiryIntervalMs) {	iterator.remove();	if (LOG.isDebugEnabled()) {	
expired after secs 

public void handle(NodesListManagerEvent event) {	RMNode eventNode = event.getNode();	switch (event.getType()) {	
reported unusable 

public void handle(NodesListManagerEvent event) {	RMNode eventNode = event.getNode();	switch (event.getType()) {	for(RMApp app: rmContext.getRMApps().values()) {	if (!app.isAppFinalStateStored()) {	this.rmContext .getDispatcher() .getEventHandler() .handle( new RMAppNodeUpdateEvent(app.getApplicationId(), eventNode, RMAppNodeUpdateType.NODE_UNUSABLE));	}	}	break;	
reported usable 

this.rmContext .getDispatcher() .getEventHandler() .handle( new RMAppNodeUpdateEvent(app.getApplicationId(), eventNode, RMAppNodeUpdateType.NODE_UNUSABLE));	}	}	break;	for (RMApp app : rmContext.getRMApps().values()) {	if (!app.isAppFinalStateStored()) {	this.rmContext .getDispatcher() .getEventHandler() .handle( new RMAppNodeUpdateEvent(app.getApplicationId(), eventNode, RMAppNodeUpdateType.NODE_USABLE));	}	}	break;	
ignoring invalid eventtype 

private void disableHostsFileReader(Exception ex) {	
failed to init hostsreader disabling 

========================= hadoop sample_1127 =========================

public synchronized void reserveResource( SchedulerApplicationAttempt application, SchedulerRequestKey priority, RMContainer container) {	RMContainer reservedContainer = getReservedContainer();	if (reservedContainer != null) {	if (!container.getContainer().getNodeId().equals(getNodeID())) {	throw new IllegalStateException("Trying to reserve" + " container " + container + " on node " + container.getReservedNode() + " when currently" + " reserved resource " + reservedContainer + " on node " + reservedContainer.getReservedNode());	}	if (!reservedContainer.getContainer().getId().getApplicationAttemptId() .equals(container.getContainer().getId().getApplicationAttemptId())) {	throw new IllegalStateException("Trying to reserve" + " container " + container + " for application " + application.getApplicationAttemptId() + " when currently" + " reserved container " + reservedContainer + " on node " + this);	}	if (LOG.isDebugEnabled()) {	
updated reserved container on node for application attempt 

if (!container.getContainer().getNodeId().equals(getNodeID())) {	throw new IllegalStateException("Trying to reserve" + " container " + container + " on node " + container.getReservedNode() + " when currently" + " reserved resource " + reservedContainer + " on node " + reservedContainer.getReservedNode());	}	if (!reservedContainer.getContainer().getId().getApplicationAttemptId() .equals(container.getContainer().getId().getApplicationAttemptId())) {	throw new IllegalStateException("Trying to reserve" + " container " + container + " for application " + application.getApplicationAttemptId() + " when currently" + " reserved container " + reservedContainer + " on node " + this);	}	if (LOG.isDebugEnabled()) {	}	} else {	if (LOG.isDebugEnabled()) {	
reserved container on node for application attempt 

========================= hadoop sample_943 =========================

BlockListAsLongs blockList = kvPair.getValue();	BlockListAsLongs.Builder builder = BlockListAsLongs.builder();	for (BlockReportReplica block : blockList) {	if (corruptOneBlockGs && !corruptedGs) {	long gsOld = block.getGenerationStamp();	long gsNew;	do {	gsNew = rand.nextInt();	} while (gsNew == gsOld);	block.setGenerationStamp(gsNew);	
corrupted the gs for block id 

} while (gsNew == gsOld);	block.setGenerationStamp(gsNew);	corruptedGs = true;	} else if (corruptOneBlockLen && !corruptedLen) {	long lenOld = block.getNumBytes();	long lenNew;	do {	lenNew = rand.nextInt((int)lenOld - 1);	} while (lenNew == lenOld);	block.setNumBytes(lenNew);	
corrupted the length for block id 

protected abstract void sendBlockReports(DatanodeRegistration dnR, String poolId, StorageBlockReport[] reports) throws IOException;	public void blockReport_01() throws IOException {	final String METHOD_NAME = GenericTestUtils.getMethodName();	Path filePath = new Path("/" + METHOD_NAME + ".dat");	ArrayList<Block> blocks = prepareForRide(filePath, METHOD_NAME, FILE_SIZE);	if(LOG.isDebugEnabled()) {	
number of blocks allocated 

final String METHOD_NAME = GenericTestUtils.getMethodName();	Path filePath = new Path("/" + METHOD_NAME + ".dat");	ArrayList<Block> blocks = prepareForRide(filePath, METHOD_NAME, FILE_SIZE);	if(LOG.isDebugEnabled()) {	}	long[] oldLengths = new long[blocks.size()];	int tempLen;	for (int i = 0; i < blocks.size(); i++) {	Block b = blocks.get(i);	if(LOG.isDebugEnabled()) {	
block before t size 

if(LOG.isDebugEnabled()) {	}	long[] oldLengths = new long[blocks.size()];	int tempLen;	for (int i = 0; i < blocks.size(); i++) {	Block b = blocks.get(i);	if(LOG.isDebugEnabled()) {	}	oldLengths[i] = b.getNumBytes();	if(LOG.isDebugEnabled()) {	
setting new length 

for (int i = 0; i < blocks.size(); i++) {	Block b = blocks.get(i);	if(LOG.isDebugEnabled()) {	}	oldLengths[i] = b.getNumBytes();	if(LOG.isDebugEnabled()) {	}	tempLen = rand.nextInt(BLOCK_SIZE);	b.set(b.getBlockId(), tempLen, b.getGenerationStamp());	if(LOG.isDebugEnabled()) {	
block after t size 

if(LOG.isDebugEnabled()) {	}	}	DataNode dn = cluster.getDataNodes().get(DN_N0);	String poolId = cluster.getNamesystem().getBlockPoolId();	DatanodeRegistration dnR = dn.getDNRegistrationForBP(poolId);	StorageBlockReport[] reports = getBlockReports(dn, poolId, false, false);	sendBlockReports(dnR, poolId, reports);	List<LocatedBlock> blocksAfterReport = DFSTestUtil.getAllBlocks(fs.open(filePath));	if(LOG.isDebugEnabled()) {	
after mods number of blocks allocated 

public void blockReport_02() throws IOException {	final String METHOD_NAME = GenericTestUtils.getMethodName();	
running test 

List<Integer> removedIndex = new ArrayList<Integer>();	List<LocatedBlock> lBlocks = cluster.getNameNodeRpc().getBlockLocations( filePath.toString(), FILE_START, FILE_SIZE).getLocatedBlocks();	while (removedIndex.size() != 2) {	int newRemoveIndex = rand.nextInt(lBlocks.size());	if (!removedIndex.contains(newRemoveIndex)) removedIndex.add(newRemoveIndex);	}	for (Integer aRemovedIndex : removedIndex) {	blocks2Remove.add(lBlocks.get(aRemovedIndex).getBlock());	}	if(LOG.isDebugEnabled()) {	
number of blocks allocated 

if (!removedIndex.contains(newRemoveIndex)) removedIndex.add(newRemoveIndex);	}	for (Integer aRemovedIndex : removedIndex) {	blocks2Remove.add(lBlocks.get(aRemovedIndex).getBlock());	}	if(LOG.isDebugEnabled()) {	}	final DataNode dn0 = cluster.getDataNodes().get(DN_N0);	for (ExtendedBlock b : blocks2Remove) {	if(LOG.isDebugEnabled()) {	
removing the block 

}	if(LOG.isDebugEnabled()) {	}	final DataNode dn0 = cluster.getDataNodes().get(DN_N0);	for (ExtendedBlock b : blocks2Remove) {	if(LOG.isDebugEnabled()) {	}	for (File f : findAllFiles(dataDir, new MyFileFilter(b.getBlockName(), true))) {	DataNodeTestUtils.getFSDataset(dn0).unfinalizeBlock(b);	if (!f.delete()) {	
couldn t delete 

if(LOG.isDebugEnabled()) {	}	final DataNode dn0 = cluster.getDataNodes().get(DN_N0);	for (ExtendedBlock b : blocks2Remove) {	if(LOG.isDebugEnabled()) {	}	for (File f : findAllFiles(dataDir, new MyFileFilter(b.getBlockName(), true))) {	DataNodeTestUtils.getFSDataset(dn0).unfinalizeBlock(b);	if (!f.delete()) {	} else {	
deleted file 

public void testInterleavedBlockReports() throws IOException, ExecutionException, InterruptedException {	int numConcurrentBlockReports = 3;	DataNode dn = cluster.getDataNodes().get(DN_N0);	final String poolId = cluster.getNamesystem().getBlockPoolId();	
block pool id 

private void waitForTempReplica(Block bl, int DN_N1) throws IOException {	final boolean tooLongWait = false;	final int TIMEOUT = 40000;	if(LOG.isDebugEnabled()) {	
wait for datanode to appear 

private void waitForTempReplica(Block bl, int DN_N1) throws IOException {	final boolean tooLongWait = false;	final int TIMEOUT = 40000;	if(LOG.isDebugEnabled()) {	}	while (cluster.getDataNodes().size() <= DN_N1) {	waitTil(20);	}	if(LOG.isDebugEnabled()) {	
total number of dns 

final DataNode dn1 = cluster.getDataNodes().get(DN_N1);	String bpid = cluster.getNamesystem().getBlockPoolId();	Replica r = DataNodeTestUtils.fetchReplicaInfo(dn1, bpid, bl.getBlockId());	long start = Time.monotonicNow();	int count = 0;	while (r == null) {	waitTil(5);	r = DataNodeTestUtils.fetchReplicaInfo(dn1, bpid, bl.getBlockId());	long waiting_period = Time.monotonicNow() - start;	if (count++ % 100 == 0) if(LOG.isDebugEnabled()) {	
has been waiting for ms 

while (r == null) {	waitTil(5);	r = DataNodeTestUtils.fetchReplicaInfo(dn1, bpid, bl.getBlockId());	long waiting_period = Time.monotonicNow() - start;	if (count++ % 100 == 0) if(LOG.isDebugEnabled()) {	}	if (waiting_period > TIMEOUT) assertTrue("Was waiting too long to get ReplicaInfo from a datanode", tooLongWait);	}	HdfsServerConstants.ReplicaState state = r.getState();	if(LOG.isDebugEnabled()) {	
replica state before the loop 

if (waiting_period > TIMEOUT) assertTrue("Was waiting too long to get ReplicaInfo from a datanode", tooLongWait);	}	HdfsServerConstants.ReplicaState state = r.getState();	if(LOG.isDebugEnabled()) {	}	start = Time.monotonicNow();	while (state != HdfsServerConstants.ReplicaState.TEMPORARY) {	waitTil(5);	state = r.getState();	if(LOG.isDebugEnabled()) {	
keep waiting for is in state 

}	start = Time.monotonicNow();	while (state != HdfsServerConstants.ReplicaState.TEMPORARY) {	waitTil(5);	state = r.getState();	if(LOG.isDebugEnabled()) {	}	if (Time.monotonicNow() - start > TIMEOUT) assertTrue("Was waiting too long for a replica to become TEMPORARY", tooLongWait);	}	if(LOG.isDebugEnabled()) {	
replica state after the loop 

private ArrayList<Block> writeFile(final String METHOD_NAME, final long fileSize, Path filePath) {	ArrayList<Block> blocks = null;	try {	REPL_FACTOR = 2;	blocks = prepareForRide(filePath, METHOD_NAME, fileSize);	} catch (IOException e) {	if(LOG.isDebugEnabled()) {	
caught exception 

private void startDNandWait(Path filePath, boolean waitReplicas) throws IOException, InterruptedException, TimeoutException {	if (LOG.isDebugEnabled()) {	
before next dn start 

private void startDNandWait(Path filePath, boolean waitReplicas) throws IOException, InterruptedException, TimeoutException {	if (LOG.isDebugEnabled()) {	}	cluster.startDataNodes(conf, 1, true, null, null);	cluster.waitClusterUp();	ArrayList<DataNode> datanodes = cluster.getDataNodes();	assertEquals(datanodes.size(), 2);	if (LOG.isDebugEnabled()) {	int lastDn = datanodes.size() - 1;	
new datanode has been started 

private ArrayList<Block> prepareForRide(final Path filePath, final String METHOD_NAME, long fileSize) throws IOException {	
running test 

private void printStats() {	BlockManagerTestUtil.updateState(cluster.getNamesystem().getBlockManager());	if(LOG.isDebugEnabled()) {	
missing 

private void printStats() {	BlockManagerTestUtil.updateState(cluster.getNamesystem().getBlockManager());	if(LOG.isDebugEnabled()) {	
corrupted 

private void printStats() {	BlockManagerTestUtil.updateState(cluster.getNamesystem().getBlockManager());	if(LOG.isDebugEnabled()) {	
under replicated 

private void printStats() {	BlockManagerTestUtil.updateState(cluster.getNamesystem().getBlockManager());	if(LOG.isDebugEnabled()) {	
pending delete 

private void printStats() {	BlockManagerTestUtil.updateState(cluster.getNamesystem().getBlockManager());	if(LOG.isDebugEnabled()) {	
pending replications 

private void printStats() {	BlockManagerTestUtil.updateState(cluster.getNamesystem().getBlockManager());	if(LOG.isDebugEnabled()) {	
excess 

private void printStats() {	BlockManagerTestUtil.updateState(cluster.getNamesystem().getBlockManager());	if(LOG.isDebugEnabled()) {	
total 

private ArrayList<Block> locatedToBlocks(final List<LocatedBlock> locatedBlks, List<Integer> positionsToRemove) {	ArrayList<Block> newList = new ArrayList<Block>();	for (int i = 0; i < locatedBlks.size(); i++) {	if (positionsToRemove != null && positionsToRemove.contains(i)) {	if(LOG.isDebugEnabled()) {	
block to be omitted 

========================= hadoop sample_7227 =========================

public static boolean shutdownThread(Thread thread, long timeoutInMilliSeconds) {	if (thread == null) {	return true;	}	try {	thread.interrupt();	thread.join(timeoutInMilliSeconds);	return true;	} catch (InterruptedException ie) {	
interrupted while shutting down thread 

========================= hadoop sample_3634 =========================

public void testAdditionsAndRemovals() {	IdentityHashStore<Key, Integer> store = new IdentityHashStore<Key, Integer>(0);	final int NUM_KEYS = 1000;	
generating keys 

========================= hadoop sample_2926 =========================

private static void scaleConfigParameter(Configuration sourceConf, Configuration destConf, String clusterValueKey, String jobValueKey, long defaultValue) {	long simulatedClusterDefaultValue = destConf.getLong(clusterValueKey, defaultValue);	long originalClusterDefaultValue = sourceConf.getLong(clusterValueKey, defaultValue);	long originalJobValue = sourceConf.getLong(jobValueKey, defaultValue);	double scaleFactor = (double)originalJobValue/originalClusterDefaultValue;	long simulatedJobValue = (long)(scaleFactor * simulatedClusterDefaultValue);	if (LOG.isDebugEnabled()) {	
for the job configuration parameter and the cluster configuration parameter the original job s configuration value is scaled from to using the default unit value of for the original cluster and for the simulated cluster 

========================= hadoop sample_6123 =========================

private List<String> getUnixGroups(String user) throws IOException {	ShellCommandExecutor executor = createGroupExecutor(user);	List<String> groups;	try {	executor.execute();	groups = resolveFullGroupNames(executor.getOutput());	} catch (ExitCodeException e) {	try {	groups = resolvePartialGroupNames(user, e.getMessage(), executor.getOutput());	} catch (PartialGroupNameException pge) {	
unable to return groups for user 

executor.execute();	groups = resolveFullGroupNames(executor.getOutput());	} catch (ExitCodeException e) {	try {	groups = resolvePartialGroupNames(user, e.getMessage(), executor.getOutput());	} catch (PartialGroupNameException pge) {	return EMPTY_GROUPS;	}	} catch (IOException ioe) {	if (executor.isTimedOut()) {	
unable to return groups for user as shell group lookup command ran longer than the configured timeout limit of seconds 

private List<String> resolvePartialGroupNames(String userName, String errMessage, String groupNames) throws PartialGroupNameException {	if (Shell.WINDOWS) {	throw new PartialGroupNameException("Does not support partial group" + " name resolution on Windows. " + errMessage);	}	if (groupNames.isEmpty()) {	throw new PartialGroupNameException("The user name '" + userName + "' is not found. " + errMessage);	} else {	
some group names for are not resolvable 

========================= hadoop sample_3771 =========================

public static void main(String[] args) {	JobConf job = new JobConf(HadoopArchives.class);	HadoopArchives harchives = new HadoopArchives(job);	int ret = 0;	try{	ret = ToolRunner.run(harchives, args);	} catch(Exception e) {	
exception in archives 

========================= hadoop sample_6573 =========================

public static YarnRPC create(Configuration conf) {	
creating yarnrpc for 

========================= hadoop sample_2530 =========================

static void persistBlocks( FSDirectory fsd, String path, INodeFile file, boolean logRetryCache) {	assert fsd.getFSNamesystem().hasWriteLock();	Preconditions.checkArgument(file.isUnderConstruction());	fsd.getEditLog().logUpdateBlocks(path, file, logRetryCache);	if(NameNode.stateChangeLog.isDebugEnabled()) {	
persistblocks with blocks is persisted to the file system 

if (newNode == null) {	throw new IOException("Unable to add " + src +  " to namespace");	}	fsn.leaseManager.addLease( newNode.getFileUnderConstructionFeature().getClientName(), newNode.getId());	if (feInfo != null) {	FSDirEncryptionZoneOp.setFileEncryptionInfo(fsd, iip, feInfo);	}	setNewINodeStoragePolicy(fsd.getBlockManager(), iip, isLazyPersist);	fsd.getEditLog().logOpenFile(src, newNode, overwrite, logRetryEntry);	if (NameNode.stateChangeLog.isDebugEnabled()) {	
dir namesystem startfile added inode 

fsd.writeLock();	try {	final INodeFile fileINode = inodesInPath.getLastINode().asFile();	Preconditions.checkState(fileINode.isUnderConstruction());	fsd.updateCount(inodesInPath, 0, fileINode.getPreferredBlockSize(), fileINode.getFileReplication(), true);	BlockInfo blockInfo = new BlockInfoContiguous(block, fileINode.getFileReplication());	blockInfo.convertToBlockUnderConstruction( HdfsServerConstants.BlockUCState.UNDER_CONSTRUCTION, targets);	fsd.getBlockManager().addBlockCollection(blockInfo, fileINode);	fileINode.addBlock(blockInfo);	if(NameNode.stateChangeLog.isDebugEnabled()) {	
dir fsdirectory addblock with block is added to the in memory file system 

newNode.setLocalName(localName);	newNode.toUnderConstruction(clientName, clientMachine);	INodesInPath newiip;	fsd.writeLock();	try {	newiip = fsd.addINode(existing, newNode);	} finally {	fsd.writeUnlock();	}	if (newiip == null) {	
dir addfile failed to add 

fsd.writeLock();	try {	newiip = fsd.addINode(existing, newNode);	} finally {	fsd.writeUnlock();	}	if (newiip == null) {	return null;	}	if(NameNode.stateChangeLog.isDebugEnabled()) {	
dir addfile is added 

if (!Block.matchingIdAndGenStamp(previousBlock, lastBlockInFile)) {	BlockInfo penultimateBlock = file.getPenultimateBlock();	if (previous == null && lastBlockInFile != null && lastBlockInFile.getNumBytes() >= file.getPreferredBlockSize() && lastBlockInFile.isComplete()) {	if (NameNode.stateChangeLog.isDebugEnabled()) {	NameNode.stateChangeLog.debug( "BLOCK* NameSystem.allocateBlock: handling block allocation" + " writing to a file with a complete previous block: src=" + src + " lastBlock=" + lastBlockInFile);	}	} else if (Block.matchingIdAndGenStamp(penultimateBlock, previousBlock)) {	if (lastBlockInFile.getNumBytes() != 0) {	throw new IOException( "Request looked like a retry to allocate block " + lastBlockInFile + " but it already contains " + lastBlockInFile.getNumBytes() + " bytes");	}	
block allocateblock caught retry for allocation of a new block in returning previously allocated block 

static boolean completeFile(FSNamesystem fsn, FSPermissionChecker pc, final String srcArg, String holder, ExtendedBlock last, long fileId) throws IOException {	String src = srcArg;	if (NameNode.stateChangeLog.isDebugEnabled()) {	
dir namesystem completefile for 

static boolean completeFile(FSNamesystem fsn, FSPermissionChecker pc, final String srcArg, String holder, ExtendedBlock last, long fileId) throws IOException {	String src = srcArg;	if (NameNode.stateChangeLog.isDebugEnabled()) {	}	checkBlock(fsn, last);	INodesInPath iip = fsn.dir.resolvePath(pc, src, fileId);	boolean success = completeFileInternal(fsn, iip, holder, ExtendedBlock.getLocalBlock(last), fileId);	if (success) {	
dir completefile is closed by 

final String src = iip.getPath();	final INodeFile pendingFile;	INode inode = null;	try {	inode = iip.getLastINode();	pendingFile = fsn.checkLease(iip, holder, fileId);	} catch (LeaseExpiredException lee) {	if (inode != null && inode.isFile() && !inode.asFile().isUnderConstruction()) {	final Block realLastBlock = inode.asFile().getLastBlock();	if (Block.matchingIdAndGenStamp(last, realLastBlock)) {	
dir completefile request from to complete inode which is already closed but it appears to be an rpc retry returning success 

private static void persistNewBlock( FSNamesystem fsn, String path, INodeFile file) {	Preconditions.checkArgument(file.isUnderConstruction());	fsn.getEditLog().logAddBlock(path, file);	if (NameNode.stateChangeLog.isDebugEnabled()) {	
persistnewblock with new block current total block count is 

========================= hadoop sample_8097 =========================

private BlockBalanceThrottler(long bandwidth, int maxThreads) {	super(bandwidth);	this.maxThreads.set(maxThreads);	
balancing bandwidth is bytes s 

private BlockBalanceThrottler(long bandwidth, int maxThreads) {	super(bandwidth);	this.maxThreads.set(maxThreads);	
number threads for balancing is 

try {	peer = peerServer.accept();	int curXceiverCount = datanode.getXceiverCount();	if (curXceiverCount > maxXceiverCount) {	throw new IOException("Xceiver count " + curXceiverCount + " exceeds the limit of concurrent xcievers: " + maxXceiverCount);	}	new Daemon(datanode.threadGroup, DataXceiver.create(peer, datanode, this)) .start();	} catch (SocketTimeoutException ignored) {	} catch (AsynchronousCloseException ace) {	if (datanode.shouldRun && !datanode.shutdownForUpgrade) {	
dataxceiverserver 

if (curXceiverCount > maxXceiverCount) {	throw new IOException("Xceiver count " + curXceiverCount + " exceeds the limit of concurrent xcievers: " + maxXceiverCount);	}	new Daemon(datanode.threadGroup, DataXceiver.create(peer, datanode, this)) .start();	} catch (SocketTimeoutException ignored) {	} catch (AsynchronousCloseException ace) {	if (datanode.shouldRun && !datanode.shutdownForUpgrade) {	}	} catch (IOException ie) {	IOUtils.cleanup(null, peer);	
dataxceiverserver 

}	new Daemon(datanode.threadGroup, DataXceiver.create(peer, datanode, this)) .start();	} catch (SocketTimeoutException ignored) {	} catch (AsynchronousCloseException ace) {	if (datanode.shouldRun && !datanode.shutdownForUpgrade) {	}	} catch (IOException ie) {	IOUtils.cleanup(null, peer);	} catch (OutOfMemoryError ie) {	IOUtils.cleanup(null, peer);	
datanode is out of memory will retry in seconds 

}	} catch (IOException ie) {	IOUtils.cleanup(null, peer);	} catch (OutOfMemoryError ie) {	IOUtils.cleanup(null, peer);	try {	Thread.sleep(30 * 1000);	} catch (InterruptedException e) {	}	} catch (Throwable te) {	
dataxceiverserver exiting due to 

} catch (InterruptedException e) {	}	} catch (Throwable te) {	datanode.shouldRun = false;	}	}	try {	peerServer.close();	closed = true;	} catch (IOException ie) {	
dataxceiverserver close exception 

datanode.shouldRun = false;	}	}	try {	peerServer.close();	closed = true;	} catch (IOException ie) {	}	if (datanode.shutdownForUpgrade) {	restartNotifyPeers();	
shutting down dataxceiverserver before restart 

public synchronized void sendOOBToPeers() {	if (!datanode.shutdownForUpgrade) {	return;	}	for (Peer p : peers.keySet()) {	try {	peersXceiver.get(p).sendOOB();	} catch (IOException e) {	
got error when sending oob message 

public synchronized void sendOOBToPeers() {	if (!datanode.shutdownForUpgrade) {	return;	}	for (Peer p : peers.keySet()) {	try {	peersXceiver.get(p).sendOOB();	} catch (IOException e) {	} catch (InterruptedException e) {	
interrupted when sending oob message 

========================= hadoop sample_7948 =========================

public synchronized Counter findCounter(String group, String name) {	if (name.equals("MAP_INPUT_BYTES")) {	
counter name map input bytes is deprecated use fileinputformatcounters as group name and bytes read as counter name instead 

========================= hadoop sample_4667 =========================

public void handle(RMAppAttemptEvent event) {	ApplicationId appId = event.getApplicationAttemptId().getApplicationId();	RMApp rmApp = this.rmContext.getRMApps().get(appId);	if (rmApp != null) {	try {	rmApp.getRMAppAttempt(event.getApplicationAttemptId()).handle(event);	} catch (Throwable t) {	
error in handling event type for application 

public void handle(RMAppEvent event) {	ApplicationId appID = event.getApplicationId();	RMApp rmApp = this.rmContext.getRMApps().get(appID);	if (rmApp != null) {	try {	rmApp.handle(event);	} catch (Throwable t) {	
error in handling event type for application 

public void testUnmanagedApp() throws IOException {	ApplicationSubmissionContext subContext = new ApplicationSubmissionContextPBImpl();	subContext.setUnmanagedAM(true);	
start testunmanagedappsuccesspath 

public void testUnmanagedApp() throws IOException {	ApplicationSubmissionContext subContext = new ApplicationSubmissionContextPBImpl();	subContext.setUnmanagedAM(true);	final String diagMsg = "some diagnostics";	RMApp application = testCreateAppFinished(subContext, diagMsg);	Assert.assertTrue("Finished app missing diagnostics", application.getDiagnostics().indexOf(diagMsg) != -1);	reset(writer);	reset(publisher);	
start testunmanagedappfailpath 

public void testAppSuccessPath() throws IOException {	
start testappsuccesspath 

public void testAppRecoverPath() throws IOException {	
start testapprecoverpath 

public void testAppNewKill() throws IOException {	
start testappnewkill 

public void testAppNewReject() throws IOException {	
start testappnewreject 

public void testAppNewRejectAddToStore() throws IOException {	
start testappnewrejectaddtostore 

public void testAppNewSavingKill() throws IOException {	
start testappnewsavingkill 

public void testAppNewSavingReject() throws IOException {	
start testappnewsavingreject 

public void testAppNewSavingSaveReject() throws IOException {	
start testappnewsavingsavereject 

public void testAppSubmittedRejected() throws IOException {	
start testappsubmittedrejected 

public void testAppSubmittedKill() throws IOException, InterruptedException {	
start testappsubmittedkill 

public void testAppAcceptedFailed() throws IOException {	
start testappacceptedfailed 

public void testAppAcceptedKill() throws IOException, InterruptedException {	
start testappacceptedkill 

public void testAppAcceptedAttemptKilled() throws IOException, InterruptedException {	
start testappacceptedattemptkilled 

public void testAppRunningKill() throws IOException {	
start testapprunningkill 

public void testAppRunningFailed() throws IOException {	
start testapprunningfailed 

public void testAppAtFinishingIgnoreKill() throws IOException {	
start testappatfinishingignorekill 

public void testAppFinalSavingToFinished() throws IOException {	
start testappfinalsavingtofinished 

public void testAppFinishedFinished() throws IOException {	
start testappfinishedfinished 

public void testAppFailedFailed() throws IOException {	
start testappfailedfailed 

public void testAppKilledKilled() throws IOException {	
start testappkilledkilled 

========================= hadoop sample_447 =========================

try {	writeLock.lock();	float capacity = entitlement.getCapacity();	if (capacity < 0 || capacity > 1.0f) {	throw new SchedulerDynamicEditException( "Capacity demand is not in the [0,1] range: " + capacity);	}	setCapacity(capacity);	setAbsoluteCapacity(getParent().getAbsoluteCapacity() * getCapacity());	setMaxCapacity(entitlement.getMaxCapacity());	if (LOG.isDebugEnabled()) {	
successfully changed to for queue 

========================= hadoop sample_914 =========================

String listAllowedUsers = conf.getInitParameter( YarnConfiguration.TIMELINE_SERVICE_READ_ALLOWED_USERS);	if (StringUtils.isEmpty(listAllowedUsers)) {	listAllowedUsers = YarnConfiguration.DEFAULT_TIMELINE_SERVICE_READ_ALLOWED_USERS;	}	LOG.info("listAllowedUsers=" + listAllowedUsers);	allowedUsersAclList = new AccessControlList(listAllowedUsers);	LOG.info("allowedUsersAclList=" + allowedUsersAclList.getUsers());	String adminAclListStr = conf.getInitParameter(YarnConfiguration.YARN_ADMIN_ACL);	if (StringUtils.isEmpty(adminAclListStr)) {	adminAclListStr = TimelineReaderWhitelistAuthorizationFilter.EMPTY_STRING;	
adminacllist not set hence setting it to 

========================= hadoop sample_328 =========================

doReturn("3.0.0").when(mockDnReg).getSoftwareVersion();	rpcServer.registerDatanode(mockDnReg);	doReturn("4.0.0").when(mockDnReg).getSoftwareVersion();	rpcServer.registerDatanode(mockDnReg);	doReturn("2.0.0").when(mockDnReg).getSoftwareVersion();	try {	rpcServer.registerDatanode(mockDnReg);	fail("Should not have been able to register DN with too-low version.");	} catch (IncorrectVersionException ive) {	GenericTestUtils.assertExceptionContains( "The reported DataNode version is too low", ive);	
got expected exception 

doReturn(123).when(mockDnReg).getXferPort();	rpcServer.registerDatanode(mockDnReg);	doReturn(nnCTime + 1).when(mockStorageInfo).getCTime();	rpcServer.registerDatanode(mockDnReg);	doReturn(VersionInfo.getVersion() + ".1").when(mockDnReg).getSoftwareVersion();	try {	rpcServer.registerDatanode(mockDnReg);	fail("Should not have been able to register DN with different software" + " versions and CTimes");	} catch (IncorrectVersionException ive) {	GenericTestUtils.assertExceptionContains( "does not match CTime of NN", ive);	
got expected exception 

========================= hadoop sample_7661 =========================

protected String execShellGetUserForNetgroup(final String netgroup) throws IOException {	String result = "";	try {	result = Shell.execCommand( Shell.getUsersForNetgroupCommand(netgroup.substring(1)));	} catch (ExitCodeException e) {	
error getting users for netgroup 

========================= hadoop sample_3767 =========================

public void run() {	try {	startFlag.await();	if (Thread.currentThread().isInterrupted()) {	return;	}	
start serial 

}	GridmixJob prevJob;	while (!Thread.currentThread().isInterrupted()) {	final JobStory job;	try {	job = getNextJobFiltered();	if (null == job) {	return;	}	if (LOG.isDebugEnabled()) {	
serial mode submitting job 

try {	job = getNextJobFiltered();	if (null == job) {	return;	}	if (LOG.isDebugEnabled()) {	}	prevJob = jobCreator.createGridmixJob( conf, 0L, job, scratch, userResolver.getTargetUgi( UserGroupInformation.createRemoteUser(job.getUser())), sequence.getAndIncrement());	lock.lock();	try {	
submitted the job 

error = e;	return;	}	if (prevJob != null) {	lock.lock();	try {	while (true) {	try {	jobCompleted.await();	} catch (InterruptedException ie) {	
error in serialjobfactory while waiting for job completion 

if (prevJob != null) {	lock.lock();	try {	while (true) {	try {	jobCompleted.await();	} catch (InterruptedException ie) {	return;	}	if (LOG.isDebugEnabled()) {	
job completed 

public void start() {	
starting serial submission 

========================= hadoop sample_6135 =========================

conf.setQueues(B, new String[] { "b1", "b2", "b3" });	conf.setCapacity(B1, B1_CAPACITY);	conf.setUserLimitFactor(B1, 100.0f);	conf.setCapacity(B2, B2_CAPACITY);	conf.setUserLimitFactor(B2, 100.0f);	conf.setCapacity(B3, B3_CAPACITY);	conf.setUserLimitFactor(B3, 100.0f);	conf.setReservable(A, true);	conf.setReservationWindow(A, 86400 * 1000);	conf.setAverageCapacity(A, 1.0f);	
setup a as a plan queue 

========================= hadoop sample_514 =========================

private boolean validMessageLength(int len) {	if (len < 28) {	if (LOG.isDebugEnabled()) {	
portmap mapping registration failed the response size is less than bytes 

private void handle(RpcDeniedReply deniedReply) {	
portmap mapping registration request was denied 

private void handle(RpcAcceptedReply acceptedReply, XDR xdr) {	AcceptState acceptState = acceptedReply.getAcceptState();	assert (acceptState == AcceptState.SUCCESS);	boolean answer = xdr.readBoolean();	if (answer != true) {	
portmap mapping registration failed accept state 

private void handle(RpcAcceptedReply acceptedReply, XDR xdr) {	AcceptState acceptState = acceptedReply.getAcceptState();	assert (acceptState == AcceptState.SUCCESS);	boolean answer = xdr.readBoolean();	if (answer != true) {	}	
portmap mapping registration succeeded 

========================= hadoop sample_4364 =========================

CSQueue childQueue = currentChildQueues.get(newChildQueueName);	if (childQueue != null) {	if ((childQueue instanceof LeafQueue && newChildQueue instanceof ParentQueue) || (childQueue instanceof ParentQueue && newChildQueue instanceof LeafQueue)) {	newChildQueue.setParent(this);	currentChildQueues.put(newChildQueueName, newChildQueue);	CapacitySchedulerQueueManager queueManager = this.csContext.getCapacitySchedulerQueueManager();	queueManager.addQueue(newChildQueueName, newChildQueue);	continue;	}	childQueue.reinitialize(newChildQueue, clusterResource);	
re configured queue 

newChildQueue.setParent(this);	currentChildQueues.put(newChildQueueName, newChildQueue);	CapacitySchedulerQueueManager queueManager = this.csContext.getCapacitySchedulerQueueManager();	queueManager.addQueue(newChildQueueName, newChildQueue);	continue;	}	childQueue.reinitialize(newChildQueue, clusterResource);	} else{	newChildQueue.setParent(this);	currentChildQueues.put(newChildQueueName, newChildQueue);	
added new child queue 

writeLock.lock();	validateSubmitApplication(applicationId, user, queue);	addApplication(applicationId, user);	} finally {	writeLock.unlock();	}	if (parent != null) {	try {	parent.submitApplication(applicationId, user, queue);	} catch (AccessControlException ace) {	
failed to submit application to parent queue 

private void addApplication(ApplicationId applicationId, String user) {	try {	writeLock.lock();	++numApplications;	
application added appid user leaf queue of parent applications 

private void removeApplication(ApplicationId applicationId, String user) {	try {	writeLock.lock();	--numApplications;	
application removed appid user leaf queue of parent applications 

}	ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager, node, getParentName(), getQueueName(), ActivityState.SKIPPED, ActivityDiagnosticConstant.QUEUE_DO_NOT_NEED_MORE_RESOURCE);	if (rootQueue) {	ActivitiesLogger.NODE.finishSkippedNodeAllocation(activitiesManager, node);	}	return CSAssignment.NULL_ASSIGNMENT;	}	CSAssignment assignment = new CSAssignment(Resources.createResource(0, 0), NodeType.NODE_LOCAL);	while (canAssign(clusterResource, node)) {	if (LOG.isDebugEnabled()) {	
trying to assign containers to child queue of 

private CSAssignment assignContainersToChildQueues(Resource cluster, PlacementSet<FiCaSchedulerNode> ps, ResourceLimits limits, SchedulingMode schedulingMode) {	CSAssignment assignment = CSAssignment.NULL_ASSIGNMENT;	Resource parentLimits = limits.getLimit();	printChildQueues();	for (Iterator<CSQueue> iter = sortAndGetChildrenAllocationIterator( ps.getPartition()); iter.hasNext(); ) {	CSQueue childQueue = iter.next();	if(LOG.isDebugEnabled()) {	
trying to assign to queue stats 

CSAssignment assignment = CSAssignment.NULL_ASSIGNMENT;	Resource parentLimits = limits.getLimit();	printChildQueues();	for (Iterator<CSQueue> iter = sortAndGetChildrenAllocationIterator( ps.getPartition()); iter.hasNext(); ) {	CSQueue childQueue = iter.next();	if(LOG.isDebugEnabled()) {	}	ResourceLimits childLimits = getResourceLimitsOfChild(childQueue, cluster, parentLimits, ps.getPartition());	CSAssignment childAssignment = childQueue.assignContainers(cluster, ps, childLimits, schedulingMode);	if(LOG.isDebugEnabled()) {	
assigned to queue stats 

private void printChildQueues() {	if (LOG.isDebugEnabled()) {	
printchildqueues queue child queues 

========================= hadoop sample_902 =========================

public boolean useWhitelistEnv(Map<String, String> env) {	try {	LinuxContainerRuntime runtime = pickContainerRuntime(env);	return runtime.useWhitelistEnv(env);	} catch (ContainerExecutionException e) {	
unable to determine runtime 

========================= hadoop sample_1865 =========================

public void handle(RMAppAttemptEvent event) {	ApplicationAttemptId appID = event.getApplicationAttemptId();	assertEquals(applicationAttempt.getAppAttemptId(), appID);	try {	applicationAttempt.handle(event);	} catch (Throwable t) {	
error in handling event type for application 

public void handle(RMAppEvent event) {	assertEquals(application.getApplicationId(), event.getApplicationId());	if (event instanceof RMAppFailedAttemptEvent) {	transferStateFromPreviousAttempt = ((RMAppFailedAttemptEvent) event) .getTransferStateFromPreviousAttempt();	}	try {	application.handle(event);	} catch (Throwable t) {	
error in handling event type for application 

========================= hadoop sample_451 =========================

public NamenodeBeanMetrics(Router router) {	this.router = router;	try {	StandardMBean bean = new StandardMBean(this, FSNamesystemMBean.class);	this.fsBeanName = MBeans.register("NameNode", "FSNamesystem", bean);	
registered fsnamesystem mbean 

this.router = router;	try {	StandardMBean bean = new StandardMBean(this, FSNamesystemMBean.class);	this.fsBeanName = MBeans.register("NameNode", "FSNamesystem", bean);	} catch (NotCompliantMBeanException e) {	throw new RuntimeException("Bad FSNamesystem MBean setup", e);	}	try {	StandardMBean bean = new StandardMBean(this, FSNamesystemMBean.class);	this.fsStateBeanName = MBeans.register("NameNode", "FSNamesystemState", bean);	
registered fsnamesystemstate mbean 

}	try {	StandardMBean bean = new StandardMBean(this, FSNamesystemMBean.class);	this.fsStateBeanName = MBeans.register("NameNode", "FSNamesystemState", bean);	} catch (NotCompliantMBeanException e) {	throw new RuntimeException("Bad FSNamesystemState MBean setup", e);	}	try {	StandardMBean bean = new StandardMBean(this, NameNodeMXBean.class);	this.nnInfoBeanName = MBeans.register("NameNode", "NameNodeInfo", bean);	
registered namenodeinfo mbean 

}	try {	StandardMBean bean = new StandardMBean(this, NameNodeMXBean.class);	this.nnInfoBeanName = MBeans.register("NameNode", "NameNodeInfo", bean);	} catch (NotCompliantMBeanException e) {	throw new RuntimeException("Bad NameNodeInfo MBean setup", e);	}	try {	StandardMBean bean = new StandardMBean(this, NameNodeStatusMXBean.class);	this.nnStatusBeanName = MBeans.register("NameNode", "NameNodeStatus", bean);	
registered namenodestatus mbean 

innerinfo.put("version", (node.getSoftwareVersion() == null ? "UNKNOWN" : node.getSoftwareVersion()));	innerinfo.put("used", node.getDfsUsed());	innerinfo.put("remaining", node.getRemaining());	innerinfo.put("blockScheduled", -1);	innerinfo.put("blockPoolUsed", node.getBlockPoolUsed());	innerinfo.put("blockPoolUsedPercent", node.getBlockPoolUsedPercent());	innerinfo.put("volfails", -1);	info.put(node.getHostName() + ":" + node.getXferPort(), Collections.unmodifiableMap(innerinfo));	}	} catch (StandbyException e) {	
cannot get nodes router in safe mode 

innerinfo.put("used", node.getDfsUsed());	innerinfo.put("remaining", node.getRemaining());	innerinfo.put("blockScheduled", -1);	innerinfo.put("blockPoolUsed", node.getBlockPoolUsed());	innerinfo.put("blockPoolUsedPercent", node.getBlockPoolUsedPercent());	innerinfo.put("volfails", -1);	info.put(node.getHostName() + ":" + node.getXferPort(), Collections.unmodifiableMap(innerinfo));	}	} catch (StandbyException e) {	} catch (IOException e) {	
cannot get nodes 

public String getClusterId() {	try {	return getNamespaceInfo("getClusterId").toString();	} catch (IOException e) {	
cannot fetch cluster id metrics 

public String getBlockPoolId() {	try {	return getNamespaceInfo("getBlockPoolId").toString();	} catch (IOException e) {	
cannot fetch block pool id metrics 

========================= hadoop sample_8193 =========================

private void callAsyncGet(long timeout, TimeUnit unit) {	if (!isCancelled() && called.compareAndSet(false, true)) {	try {	set(asyncGet.get(timeout, unit));	} catch (TimeoutException te) {	
TRACE 

private void callAsyncGet(long timeout, TimeUnit unit) {	if (!isCancelled() && called.compareAndSet(false, true)) {	try {	set(asyncGet.get(timeout, unit));	} catch (TimeoutException te) {	called.compareAndSet(true, false);	} catch (Throwable e) {	
TRACE 

========================= hadoop sample_3675 =========================

private SharedCacheUploaderMetrics() {	registry = new MetricsRegistry("SharedCacheUploaderRequests");	
initialized 

========================= hadoop sample_390 =========================

public static void assertNNHasCheckpoints(MiniDFSCluster cluster, int nnIdx, List<Integer> txids) {	for (File nameDir : getNameNodeCurrentDirs(cluster, nnIdx)) {	
examining name dir with files 

public static void assertNNHasCheckpoints(MiniDFSCluster cluster, int nnIdx, List<Integer> txids) {	for (File nameDir : getNameNodeCurrentDirs(cluster, nnIdx)) {	
examining storage dir with contents 

========================= hadoop sample_7323 =========================

protected void serviceInit(Configuration conf) throws Exception {	String jobId = TypeConverter.fromYarn(context.getApplicationID()).toString();	String stagingDirStr = null;	String doneDirStr = null;	String userDoneDirStr = null;	try {	stagingDirStr = JobHistoryUtils.getConfiguredHistoryStagingDirPrefix(conf, jobId);	doneDirStr = JobHistoryUtils.getConfiguredHistoryIntermediateDoneDirPrefix(conf);	userDoneDirStr = JobHistoryUtils.getHistoryIntermediateDoneDirForUser(conf);	} catch (IOException e) {	
failed while getting the configured log directories 

doneDirStr = JobHistoryUtils.getConfiguredHistoryIntermediateDoneDirPrefix(conf);	userDoneDirStr = JobHistoryUtils.getHistoryIntermediateDoneDirForUser(conf);	} catch (IOException e) {	throw new YarnRuntimeException(e);	}	try {	stagingDirPath = FileContext.getFileContext(conf).makeQualified(new Path(stagingDirStr));	stagingDirFS = FileSystem.get(stagingDirPath.toUri(), conf);	mkdir(stagingDirFS, stagingDirPath, new FsPermission( JobHistoryUtils.HISTORY_STAGING_DIR_PERMISSIONS));	} catch (IOException e) {	
failed while checking for creating history staging path 

mkdir(stagingDirFS, stagingDirPath, new FsPermission( JobHistoryUtils.HISTORY_STAGING_DIR_PERMISSIONS));	} catch (IOException e) {	throw new YarnRuntimeException(e);	}	Path doneDirPath = null;	try {	doneDirPath = FileContext.getFileContext(conf).makeQualified(new Path(doneDirStr));	doneDirFS = FileSystem.get(doneDirPath.toUri(), conf);	if (!doneDirFS.exists(doneDirPath)) {	if (JobHistoryUtils.shouldCreateNonUserDirectory(conf)) {	
creating intermediate history logdir based on conf should ideally be created by the jobhistoryserver 

if (!doneDirFS.exists(doneDirPath)) {	if (JobHistoryUtils.shouldCreateNonUserDirectory(conf)) {	mkdir( doneDirFS, doneDirPath, new FsPermission( JobHistoryUtils.HISTORY_INTERMEDIATE_DONE_DIR_PERMISSIONS .toShort()));	} else {	String message = "Not creating intermediate history logDir: [" + doneDirPath + "] based on conf: " + MRJobConfig.MR_AM_CREATE_JH_INTERMEDIATE_BASE_DIR + ". Either set to true or pre-create this directory with" + " appropriate permissions";	LOG.error(message);	throw new YarnRuntimeException(message);	}	}	} catch (IOException e) {	
failed checking for the existance of history intermediate done directory 

throw new YarnRuntimeException(message);	}	}	} catch (IOException e) {	throw new YarnRuntimeException(e);	}	try {	doneDirPrefixPath = FileContext.getFileContext(conf).makeQualified(new Path(userDoneDirStr));	mkdir(doneDirFS, doneDirPrefixPath, new FsPermission( JobHistoryUtils.HISTORY_INTERMEDIATE_USER_DIR_PERMISSIONS));	} catch (IOException e) {	
error creating user intermediate history done directory 

doneDirPrefixPath = FileContext.getFileContext(conf).makeQualified(new Path(userDoneDirStr));	mkdir(doneDirFS, doneDirPrefixPath, new FsPermission( JobHistoryUtils.HISTORY_INTERMEDIATE_USER_DIR_PERMISSIONS));	} catch (IOException e) {	throw new YarnRuntimeException(e);	}	maxUnflushedCompletionEvents = conf.getInt(MRJobConfig.MR_AM_HISTORY_MAX_UNFLUSHED_COMPLETE_EVENTS, MRJobConfig.DEFAULT_MR_AM_HISTORY_MAX_UNFLUSHED_COMPLETE_EVENTS);	postJobCompletionMultiplier = conf.getInt( MRJobConfig.MR_AM_HISTORY_JOB_COMPLETE_UNFLUSHED_MULTIPLIER, MRJobConfig.DEFAULT_MR_AM_HISTORY_JOB_COMPLETE_UNFLUSHED_MULTIPLIER);	flushTimeout = conf.getLong(MRJobConfig.MR_AM_HISTORY_COMPLETE_EVENT_FLUSH_TIMEOUT_MS, MRJobConfig.DEFAULT_MR_AM_HISTORY_COMPLETE_EVENT_FLUSH_TIMEOUT_MS);	minQueueSizeForBatchingFlushes = conf.getInt( MRJobConfig.MR_AM_HISTORY_USE_BATCHED_FLUSH_QUEUE_SIZE_THRESHOLD, MRJobConfig.DEFAULT_MR_AM_HISTORY_USE_BATCHED_FLUSH_QUEUE_SIZE_THRESHOLD);	if (conf.getBoolean(MRJobConfig.MAPREDUCE_JOB_EMIT_TIMELINE_DATA, MRJobConfig.DEFAULT_MAPREDUCE_JOB_EMIT_TIMELINE_DATA)) {	
emitting job history data to the timeline service is enabled 

if (conf.getBoolean(MRJobConfig.MAPREDUCE_JOB_EMIT_TIMELINE_DATA, MRJobConfig.DEFAULT_MAPREDUCE_JOB_EMIT_TIMELINE_DATA)) {	if (YarnConfiguration.timelineServiceEnabled(conf)) {	boolean timelineServiceV2Enabled = ((int) YarnConfiguration.getTimelineServiceVersion(conf) == 2);	if(timelineServiceV2Enabled) {	timelineV2Client = ((MRAppMaster.RunningAppContext)context).getTimelineV2Client();	timelineV2Client.init(conf);	} else {	timelineClient = ((MRAppMaster.RunningAppContext) context).getTimelineClient();	timelineClient.init(conf);	}	
timeline service is enabled version 

if (YarnConfiguration.timelineServiceEnabled(conf)) {	boolean timelineServiceV2Enabled = ((int) YarnConfiguration.getTimelineServiceVersion(conf) == 2);	if(timelineServiceV2Enabled) {	timelineV2Client = ((MRAppMaster.RunningAppContext)context).getTimelineV2Client();	timelineV2Client.init(conf);	} else {	timelineClient = ((MRAppMaster.RunningAppContext) context).getTimelineClient();	timelineClient.init(conf);	}	} else {	
timeline service is not enabled 

if(timelineServiceV2Enabled) {	timelineV2Client = ((MRAppMaster.RunningAppContext)context).getTimelineV2Client();	timelineV2Client.init(conf);	} else {	timelineClient = ((MRAppMaster.RunningAppContext) context).getTimelineClient();	timelineClient.init(conf);	}	} else {	}	} else {	
emitting job history data to the timeline server is not enabled 

} else {	}	} else {	}	String jhistFormat = conf.get(JHAdminConfig.MR_HS_JHIST_FORMAT, JHAdminConfig.DEFAULT_MR_HS_JHIST_FORMAT);	if (jhistFormat.equals("json")) {	jhistMode = EventWriter.WriteMode.JSON;	} else if (jhistFormat.equals("binary")) {	jhistMode = EventWriter.WriteMode.BINARY;	} else {	
unrecognized value for property valid values are json or binary falling back to default value 

private void mkdir(FileSystem fs, Path path, FsPermission fsp) throws IOException {	if (!fs.exists(path)) {	try {	fs.mkdirs(path, fsp);	FileStatus fsStatus = fs.getFileStatus(path);	
perms after creating expected 

private void mkdir(FileSystem fs, Path path, FsPermission fsp) throws IOException {	if (!fs.exists(path)) {	try {	fs.mkdirs(path, fsp);	FileStatus fsStatus = fs.getFileStatus(path);	if (fsStatus.getPermission().toShort() != fsp.toShort()) {	
explicitly setting permissions to 

private void mkdir(FileSystem fs, Path path, FsPermission fsp) throws IOException {	if (!fs.exists(path)) {	try {	fs.mkdirs(path, fsp);	FileStatus fsStatus = fs.getFileStatus(path);	if (fsStatus.getPermission().toShort() != fsp.toShort()) {	fs.setPermission(path, fsp);	}	} catch (FileAlreadyExistsException e) {	
directory already exists 

timelineClient.start();	} else if (timelineV2Client != null) {	timelineV2Client.start();	}	eventHandlingThread = new Thread(new Runnable() {	public void run() {	JobHistoryEvent event = null;	while (!stopped && !Thread.currentThread().isInterrupted()) {	if (eventCounter != 0 && eventCounter % 1000 == 0) {	eventCounter = 0;	
size of the jobhistory event queue is 

JobHistoryEvent event = null;	while (!stopped && !Thread.currentThread().isInterrupted()) {	if (eventCounter != 0 && eventCounter % 1000 == 0) {	eventCounter = 0;	} else {	eventCounter++;	}	try {	event = eventQueue.take();	} catch (InterruptedException e) {	
eventqueue take interrupted returning 

}	try {	event = eventQueue.take();	} catch (InterruptedException e) {	return;	}	synchronized (lock) {	boolean isInterrupted = Thread.interrupted();	handleEvent(event);	if (isInterrupted) {	
event handling interrupted 

protected void serviceStop() throws Exception {	
stopping jobhistoryeventhandler size of the outstanding queue size is 

protected void serviceStop() throws Exception {	stopped = true;	synchronized(lock) {	if (eventHandlingThread != null) {	
interrupting event handling thread 

protected void serviceStop() throws Exception {	stopped = true;	synchronized(lock) {	if (eventHandlingThread != null) {	eventHandlingThread.interrupt();	} else {	
null event handling thread 

protected void serviceStop() throws Exception {	stopped = true;	synchronized(lock) {	if (eventHandlingThread != null) {	eventHandlingThread.interrupt();	} else {	}	}	try {	if (eventHandlingThread != null) {	
waiting for event handling thread to complete 

if (eventHandlingThread != null) {	eventHandlingThread.interrupt();	} else {	}	}	try {	if (eventHandlingThread != null) {	eventHandlingThread.join();	}	} catch (InterruptedException ie) {	
interrupted exception while stopping 

}	try {	if (eventHandlingThread != null) {	eventHandlingThread.join();	}	} catch (InterruptedException ie) {	}	for (MetaInfo mi : fileMap.values()) {	try {	if (LOG.isDebugEnabled()) {	
shutting down timer for 

eventHandlingThread.join();	}	} catch (InterruptedException ie) {	}	for (MetaInfo mi : fileMap.values()) {	try {	if (LOG.isDebugEnabled()) {	}	mi.shutDownTimer();	} catch (IOException e) {	
exception while canceling delayed flush timer likely caused by a failed flush 

try {	if (LOG.isDebugEnabled()) {	}	mi.shutDownTimer();	} catch (IOException e) {	}	}	Iterator<JobHistoryEvent> it = eventQueue.iterator();	while(it.hasNext()) {	JobHistoryEvent ev = it.next();	
in stop writing event 

Iterator<JobHistoryEvent> it = eventQueue.iterator();	while(it.hasNext()) {	JobHistoryEvent ev = it.next();	handleEvent(ev);	}	if(forceJobCompletion) {	for (Map.Entry<JobId,MetaInfo> jobIt : fileMap.entrySet()) {	JobId toClose = jobIt.getKey();	MetaInfo mi = jobIt.getValue();	if(mi != null && mi.isWriterActive()) {	
found jobid to have not been closed will close 

JobUnsuccessfulCompletionEvent jucEvent = new JobUnsuccessfulCompletionEvent(TypeConverter.fromYarn(toClose), System.currentTimeMillis(), job.getCompletedMaps(), job.getCompletedReduces(), createJobStateForJobUnsuccessfulCompletionEvent( mi.getForcedJobStateOnShutDown()), job.getDiagnostics());	JobHistoryEvent jfEvent = new JobHistoryEvent(toClose, jucEvent);	handleEvent(jfEvent);	}	}	}	for (MetaInfo mi : fileMap.values()) {	try {	mi.closeWriter();	} catch (IOException e) {	
exception while closing file 

try {	mi.closeWriter();	} catch (IOException e) {	}	}	if (timelineClient != null) {	timelineClient.stop();	} else if (timelineV2Client != null) {	timelineV2Client.stop();	}	
stopped jobhistoryeventhandler super stop 

protected void setupEventWriter(JobId jobId, AMStartedEvent amStartedEvent) throws IOException {	if (stagingDirPath == null) {	
log directory is null returning 

String user = UserGroupInformation.getCurrentUser().getShortUserName();	if (user == null) {	throw new IOException( "User is null while setting up jobhistory eventwriter");	}	String jobName = context.getJob(jobId).getName();	EventWriter writer = (oldFi == null) ? null : oldFi.writer;	Path logDirConfPath = JobHistoryUtils.getStagingConfFile(stagingDirPath, jobId, startCount);	if (writer == null) {	try {	writer = createEventWriter(historyFile);	
event writer setup for jobid file 

if (user == null) {	throw new IOException( "User is null while setting up jobhistory eventwriter");	}	String jobName = context.getJob(jobId).getName();	EventWriter writer = (oldFi == null) ? null : oldFi.writer;	Path logDirConfPath = JobHistoryUtils.getStagingConfFile(stagingDirPath, jobId, startCount);	if (writer == null) {	try {	writer = createEventWriter(historyFile);	} catch (IOException ioe) {	
could not create log file for job 

} catch (IOException ioe) {	throw ioe;	}	if (conf != null) {	if (logDirConfPath != null) {	Configuration redactedConf = new Configuration(conf);	MRJobConfUtil.redact(redactedConf);	try (FSDataOutputStream jobFileOut = stagingDirFS .create(logDirConfPath, true)) {	redactedConf.writeXml(jobFileOut);	} catch (IOException e) {	
failed to write the job configuration file 

public void closeWriter(JobId id) throws IOException {	try {	final MetaInfo mi = fileMap.get(id);	if (mi != null) {	mi.closeWriter();	}	} catch (IOException e) {	
error closing writer for jobid 

public void handleEvent(JobHistoryEvent event) {	synchronized (lock) {	if (event.getHistoryEvent().getEventType() == EventType.AM_STARTED) {	try {	AMStartedEvent amStartedEvent = (AMStartedEvent) event.getHistoryEvent();	setupEventWriter(event.getJobID(), amStartedEvent);	} catch (IOException ioe) {	
error jobhistoryeventhandler in handleevent 

if (! (historyEvent instanceof NormalizedResourceEvent)) {	mi.writeEvent(historyEvent);	}	processEventForJobSummary(event.getHistoryEvent(), mi.getJobSummary(), event.getJobID());	if (timelineV2Client != null) {	processEventForNewTimelineService(historyEvent, event.getJobID(), event.getTimestamp());	} else if (timelineClient != null) {	processEventForTimelineServer(historyEvent, event.getJobID(), event.getTimestamp());	}	if (LOG.isDebugEnabled()) {	
in historyeventhandler 

}	processEventForJobSummary(event.getHistoryEvent(), mi.getJobSummary(), event.getJobID());	if (timelineV2Client != null) {	processEventForNewTimelineService(historyEvent, event.getJobID(), event.getTimestamp());	} else if (timelineClient != null) {	processEventForTimelineServer(historyEvent, event.getJobID(), event.getTimestamp());	}	if (LOG.isDebugEnabled()) {	}	} catch (IOException e) {	
error writing history event 

tEntity.setEntityId(jobId.toString());	tEntity.setEntityType(MAPREDUCE_JOB_ENTITY_TYPE);	break;	default: break;	}	try {	TimelinePutResponse response = timelineClient.putEntities(tEntity);	List<TimelinePutResponse.TimelinePutError> errors = response.getErrors();	if (errors.size() == 0) {	if (LOG.isDebugEnabled()) {	
timeline entities are successfully put in event 

default: break;	}	try {	TimelinePutResponse response = timelineClient.putEntities(tEntity);	List<TimelinePutResponse.TimelinePutError> errors = response.getErrors();	if (errors.size() == 0) {	if (LOG.isDebugEnabled()) {	}	} else {	for (TimelinePutResponse.TimelinePutError error : errors) {	
error when publishing entity server side error code 

TimelinePutResponse response = timelineClient.putEntities(tEntity);	List<TimelinePutResponse.TimelinePutError> errors = response.getErrors();	if (errors.size() == 0) {	if (LOG.isDebugEnabled()) {	}	} else {	for (TimelinePutResponse.TimelinePutError error : errors) {	}	}	} catch (YarnException | IOException | ClientHandlerException ex) {	
error putting entity to timeline Server 

configSize = size;	}	jobEntityForConfigs.addConfig(entry.getKey(), entry.getValue());	appEntityForConfigs.addConfig(entry.getKey(), entry.getValue());	}	if (configSize > 0) {	timelineV2Client.putEntities(jobEntityForConfigs);	timelineV2Client.putEntities(appEntityForConfigs);	}	} catch (IOException | YarnException e) {	
exception while publishing configs on job submitted event for the job 

taskAttemptId = ((MapAttemptFinishedEvent)event). getAttemptId().toString();	taskAttemptIdPrefix = TimelineServiceHelper. invertLong(((MapAttemptFinishedEvent)event).getStartTime());	break;	case REDUCE_ATTEMPT_FINISHED: taskId = ((ReduceAttemptFinishedEvent)event).getTaskId().toString();	taskAttemptId = ((ReduceAttemptFinishedEvent)event). getAttemptId().toString();	taskAttemptIdPrefix = TimelineServiceHelper. invertLong(((ReduceAttemptFinishedEvent)event).getStartTime());	break;	case SETUP_ATTEMPT_FINISHED: case CLEANUP_ATTEMPT_FINISHED: taskId = ((TaskAttemptFinishedEvent)event).getTaskId().toString();	taskAttemptId = ((TaskAttemptFinishedEvent)event). getAttemptId().toString();	break;	
eventtype cannot be recognized and handled by timeline service 

tEntity = createTaskAttemptEntity(event, timestamp, taskAttemptId, MAPREDUCE_TASK_ATTEMPT_ENTITY_TYPE, MAPREDUCE_TASK_ENTITY_TYPE, taskId, setCreatedTime, taskAttemptIdPrefix);	}	}	try {	if (appEntityWithJobMetrics == null) {	timelineV2Client.putEntitiesAsync(tEntity);	} else {	timelineV2Client.putEntities(tEntity, appEntityWithJobMetrics);	}	} catch (IOException | YarnException e) {	
failed to process event for the job 

final MetaInfo mi = fileMap.get(jobId);	if (mi == null) {	throw new IOException("No MetaInfo found for JobId: [" + jobId + "]");	}	if (!mi.isWriterActive()) {	throw new IOException( "Inactive Writer: Likely received multiple JobFinished / " + "JobUnsuccessful events for JobId: [" + jobId + "]");	}	try {	mi.closeWriter();	} catch (IOException e) {	
error closing writer for jobid 

protected void processDoneFiles(JobId jobId) throws IOException {	final MetaInfo mi = fileMap.get(jobId);	if (mi == null) {	throw new IOException("No MetaInfo found for JobId: [" + jobId + "]");	}	if (mi.getHistoryFile() == null) {	
no file for job history with found in cache 

protected void processDoneFiles(JobId jobId) throws IOException {	final MetaInfo mi = fileMap.get(jobId);	if (mi == null) {	throw new IOException("No MetaInfo found for JobId: [" + jobId + "]");	}	if (mi.getHistoryFile() == null) {	}	if (mi.getConfFile() == null) {	
no file for jobconf with found in cache 

Path qualifiedSummaryDoneFile = null;	FSDataOutputStream summaryFileOut = null;	try {	String doneSummaryFileName = getTempFileName(JobHistoryUtils .getIntermediateSummaryFileName(jobId));	qualifiedSummaryDoneFile = doneDirFS.makeQualified(new Path( doneDirPrefixPath, doneSummaryFileName));	summaryFileOut = doneDirFS.create(qualifiedSummaryDoneFile, true);	summaryFileOut.writeUTF(mi.getJobSummary().getJobSummaryString());	summaryFileOut.close();	doneDirFS.setPermission(qualifiedSummaryDoneFile, new FsPermission( JobHistoryUtils.HISTORY_INTERMEDIATE_FILE_PERMISSIONS));	} catch (IOException e) {	
unable to write out jobsummaryinfo to 

Path qualifiedDoneFile = null;	if (mi.getHistoryFile() != null) {	Path historyFile = mi.getHistoryFile();	Path qualifiedLogFile = stagingDirFS.makeQualified(historyFile);	int jobNameLimit = getConfig().getInt(JHAdminConfig.MR_HS_JOBNAME_LIMIT, JHAdminConfig.DEFAULT_MR_HS_JOBNAME_LIMIT);	String doneJobHistoryFileName = getTempFileName(FileNameIndexUtils.getDoneFileName(mi .getJobIndexInfo(), jobNameLimit));	qualifiedDoneFile = doneDirFS.makeQualified(new Path(doneDirPrefixPath, doneJobHistoryFileName));	if(moveToDoneNow(qualifiedLogFile, qualifiedDoneFile)) {	String historyUrl = MRWebAppUtil.getApplicationWebURLOnJHSWithScheme( getConfig(), context.getApplicationID());	context.setHistoryUrl(historyUrl);	
set historyurl to 

Path confFile = mi.getConfFile();	Path qualifiedConfFile = stagingDirFS.makeQualified(confFile);	String doneConfFileName = getTempFileName(JobHistoryUtils .getIntermediateConfFileName(jobId));	qualifiedConfDoneFile = doneDirFS.makeQualified(new Path(doneDirPrefixPath, doneConfFileName));	moveToDoneNow(qualifiedConfFile, qualifiedConfDoneFile);	}	moveTmpToDone(qualifiedSummaryDoneFile);	moveTmpToDone(qualifiedConfDoneFile);	moveTmpToDone(qualifiedDoneFile);	} catch (IOException e) {	
error closing writer for jobid 

public void run() {	
in flush timer task 

protected void moveTmpToDone(Path tmpPath) throws IOException {	if (tmpPath != null) {	String tmpFileName = tmpPath.getName();	String fileName = getFileNameFromTmpFN(tmpFileName);	Path path = new Path(tmpPath.getParent(), fileName);	doneDirFS.rename(tmpPath, path);	
moved tmp to done to 

protected boolean moveToDoneNow(Path fromPath, Path toPath) throws IOException {	boolean success = false;	if (stagingDirFS.exists(fromPath)) {	
copying to 

protected boolean moveToDoneNow(Path fromPath, Path toPath) throws IOException {	boolean success = false;	if (stagingDirFS.exists(fromPath)) {	doneDirFS.delete(toPath, true);	boolean copied = FileUtil.copy(stagingDirFS, fromPath, doneDirFS, toPath, false, getConfig());	doneDirFS.setPermission(toPath, new FsPermission( JobHistoryUtils.HISTORY_INTERMEDIATE_FILE_PERMISSIONS));	if (copied) {	
copied from to done location 

protected boolean moveToDoneNow(Path fromPath, Path toPath) throws IOException {	boolean success = false;	if (stagingDirFS.exists(fromPath)) {	doneDirFS.delete(toPath, true);	boolean copied = FileUtil.copy(stagingDirFS, fromPath, doneDirFS, toPath, false, getConfig());	doneDirFS.setPermission(toPath, new FsPermission( JobHistoryUtils.HISTORY_INTERMEDIATE_FILE_PERMISSIONS));	if (copied) {	success = true;	} else {	
copy failed from to done location 

public void setForcejobCompletion(boolean forceJobCompletion) {	this.forceJobCompletion = forceJobCompletion;	
jobhistoryeventhandler notified that forcejobcompletion is 

========================= hadoop sample_5172 =========================

public void testReadWriteHistoryData() throws IOException {	
starting testreadwritehistorydata 

public void testWriteAfterApplicationFinish() throws IOException {	
starting testwriteafterapplicationfinish 

public void testMassiveWriteContainerHistoryData() throws IOException {	
starting testmassivewritecontainerhistorydata 

public void testMissingContainerHistoryData() throws IOException {	
starting testmissingcontainerhistorydata 

public void testMissingApplicationAttemptHistoryData() throws IOException {	
starting testmissingapplicationattempthistorydata 

public void testInitExistingWorkingDirectoryInSafeMode() throws Exception {	
starting testinitexistingworkingdirectoryinsafemode 

public void testInitNonExistingWorkingDirectoryInSafeMode() throws Exception {	
starting testinitnonexistingworkingdirectoryinsafemode 

========================= hadoop sample_1998 =========================

protected void chooseFavouredNodes(String src, int numOfReplicas, List<DatanodeDescriptor> favoredNodes, Set<Node> favoriteAndExcludedNodes, long blocksize, int maxNodesPerRack, List<DatanodeStorageInfo> results, boolean avoidStaleNodes, EnumMap<StorageType, Integer> storageTypes) throws NotEnoughReplicasException {	for (int i = 0; i < favoredNodes.size() && results.size() < numOfReplicas;	i++) {	DatanodeDescriptor favoredNode = favoredNodes.get(i);	final DatanodeStorageInfo target = chooseLocalStorage(favoredNode, favoriteAndExcludedNodes, blocksize, maxNodesPerRack, results, avoidStaleNodes, storageTypes, false);	if (target == null) {	
could not find a target for file with favored node 

while (candidates.size() - expectedNumOfReplicas > excessReplicas.size()) {	final DatanodeStorageInfo cur;	if (firstOne && useDelHint(delNodeHintStorage, addedNodeStorage, moreThanOne, exactlyOne, excessTypes)) {	cur = delNodeHintStorage;	} else {	cur = chooseReplicaToDelete(moreThanOne, exactlyOne, excessTypes, rackMap);	}	firstOne = false;	if (cur == null) {	if (LOG.isDebugEnabled()) {	
no excess replica can be found excesstypes morethanone exactlyone 

========================= hadoop sample_8310 =========================

protected void serviceInit(Configuration conf) throws Exception {	int threadPoolSize = getAggregatorThreadPoolSize(conf);	this.threadPool = HadoopExecutors.newFixedThreadPool(threadPoolSize, new ThreadFactoryBuilder() .setNameFormat("LogAggregationService #%d") .build());	rollingMonitorInterval = conf.getLong( YarnConfiguration.NM_LOG_AGGREGATION_ROLL_MONITORING_INTERVAL_SECONDS, YarnConfiguration.DEFAULT_NM_LOG_AGGREGATION_ROLL_MONITORING_INTERVAL_SECONDS);	boolean logAggregationDebugMode = conf.getBoolean(NM_LOG_AGGREGATION_DEBUG_ENABLED, false);	if (rollingMonitorInterval > 0 && rollingMonitorInterval < MIN_LOG_ROLLING_INTERVAL) {	if (logAggregationDebugMode) {	LOG.info("Log aggregation debug mode enabled. rollingMonitorInterval = " + rollingMonitorInterval);	} else {	
rollingmonitorintervall should be more than or equal to seconds using seconds instead 

this.threadPool = HadoopExecutors.newFixedThreadPool(threadPoolSize, new ThreadFactoryBuilder() .setNameFormat("LogAggregationService #%d") .build());	rollingMonitorInterval = conf.getLong( YarnConfiguration.NM_LOG_AGGREGATION_ROLL_MONITORING_INTERVAL_SECONDS, YarnConfiguration.DEFAULT_NM_LOG_AGGREGATION_ROLL_MONITORING_INTERVAL_SECONDS);	boolean logAggregationDebugMode = conf.getBoolean(NM_LOG_AGGREGATION_DEBUG_ENABLED, false);	if (rollingMonitorInterval > 0 && rollingMonitorInterval < MIN_LOG_ROLLING_INTERVAL) {	if (logAggregationDebugMode) {	LOG.info("Log aggregation debug mode enabled. rollingMonitorInterval = " + rollingMonitorInterval);	} else {	this.rollingMonitorInterval = MIN_LOG_ROLLING_INTERVAL;	}	} else if (rollingMonitorInterval <= 0) {	
rollingmonitorinterval is set as the log rolling monitoring interval is disabled the logs will be aggregated after this application is finished 

rollingMonitorInterval = conf.getLong( YarnConfiguration.NM_LOG_AGGREGATION_ROLL_MONITORING_INTERVAL_SECONDS, YarnConfiguration.DEFAULT_NM_LOG_AGGREGATION_ROLL_MONITORING_INTERVAL_SECONDS);	boolean logAggregationDebugMode = conf.getBoolean(NM_LOG_AGGREGATION_DEBUG_ENABLED, false);	if (rollingMonitorInterval > 0 && rollingMonitorInterval < MIN_LOG_ROLLING_INTERVAL) {	if (logAggregationDebugMode) {	LOG.info("Log aggregation debug mode enabled. rollingMonitorInterval = " + rollingMonitorInterval);	} else {	this.rollingMonitorInterval = MIN_LOG_ROLLING_INTERVAL;	}	} else if (rollingMonitorInterval <= 0) {	} else {	
rollingmonitorinterval is set as the logs will be aggregated every seconds 

protected void serviceStop() throws Exception {	
waiting for pending aggregation during exit 

boolean shouldAbort = context.getNMStateStore().canRecover() && !context.getDecommissioned() && supervised;	for (AppLogAggregator aggregator : appLogAggregators.values()) {	if (shouldAbort) {	aggregator.abortLogAggregation();	} else {	aggregator.finishLogAggregation();	}	}	while (!threadPool.isTerminated()) {	for (ApplicationId appId : appLogAggregators.keySet()) {	
waiting for aggregation to complete for 

}	}	while (!threadPool.isTerminated()) {	for (ApplicationId appId : appLogAggregators.keySet()) {	}	try {	if (!threadPool.awaitTermination(30, TimeUnit.SECONDS)) {	threadPool.shutdownNow();	}	} catch (InterruptedException e) {	
aggregation stop interrupted 

}	try {	if (!threadPool.awaitTermination(30, TimeUnit.SECONDS)) {	threadPool.shutdownNow();	}	} catch (InterruptedException e) {	break;	}	}	for (ApplicationId appId : appLogAggregators.keySet()) {	
some logs may not have been aggregated for 

private void initApp(final ApplicationId appId, String user, Credentials credentials, Map<ApplicationAccessType, String> appAcls, LogAggregationContext logAggregationContext, long recoveredLogInitedTime) {	ApplicationEvent eventResponse;	try {	initAppAggregator(appId, user, credentials, appAcls, logAggregationContext, recoveredLogInitedTime);	eventResponse = new ApplicationEvent(appId, ApplicationEventType.APPLICATION_LOG_HANDLING_INITED);	} catch (YarnRuntimeException e) {	
application failed to init aggregation 

protected void closeFileSystems(final UserGroupInformation userUgi) {	try {	FileSystem.closeAllForUGI(userUgi);	} catch (IOException e) {	
failed to close filesystems 

private void stopContainer(ContainerId containerId, ContainerType containerType, int exitCode) {	AppLogAggregator aggregator = this.appLogAggregators.get( containerId.getApplicationAttemptId().getApplicationId());	if (aggregator == null) {	
log aggregation is not initialized for did it fail to start 

private void stopApp(ApplicationId appId) {	AppLogAggregator aggregator = this.appLogAggregators.get(appId);	if (aggregator == null) {	
log aggregation is not initialized for did it fail to start 

private int getAggregatorThreadPoolSize(Configuration conf) {	int threadPoolSize;	try {	threadPoolSize = conf.getInt(YarnConfiguration .NM_LOG_AGGREGATION_THREAD_POOL_SIZE, YarnConfiguration.DEFAULT_NM_LOG_AGGREGATION_THREAD_POOL_SIZE);	} catch (NumberFormatException ex) {	
invalid thread pool size setting it to the default value in yarnconfiguration 

private int getAggregatorThreadPoolSize(Configuration conf) {	int threadPoolSize;	try {	threadPoolSize = conf.getInt(YarnConfiguration .NM_LOG_AGGREGATION_THREAD_POOL_SIZE, YarnConfiguration.DEFAULT_NM_LOG_AGGREGATION_THREAD_POOL_SIZE);	} catch (NumberFormatException ex) {	threadPoolSize = YarnConfiguration. DEFAULT_NM_LOG_AGGREGATION_THREAD_POOL_SIZE;	}	if(threadPoolSize <= 0) {	
invalid thread pool size setting it to the default value in yarnconfiguration 

========================= hadoop sample_1832 =========================

public static void sendRedirect(HttpServletRequest request, HttpServletResponse response, String target) throws IOException {	if (LOG.isDebugEnabled()) {	
redirecting to 

========================= hadoop sample_1561 =========================

protected Nfs3Base(RpcProgram rpcProgram, Configuration conf) {	this.rpcProgram = rpcProgram;	
nfs server port set to 

public void start(boolean register) {	startTCPServer();	if (register) {	ShutdownHookManager.get().addShutdownHook(new NfsShutdownHook(), SHUTDOWN_HOOK_PRIORITY);	try {	rpcProgram.register(PortmapMapping.TRANSPORT_TCP, nfsBoundPort);	} catch (Throwable e) {	
failed to register the service 

private void startTCPServer() {	SimpleTcpServer tcpServer = new SimpleTcpServer(rpcProgram.getPort(), rpcProgram, 0);	rpcProgram.startDaemons();	try {	tcpServer.run();	} catch (Throwable e) {	
failed to start the tcp server 

========================= hadoop sample_4333 =========================

private boolean visit(String resourceName) {	if (resourceName.equals(ResourceRequest.ANY)) {	return visitAny();	}	List<FSSchedulerNode> nodes = nodeTracker.getNodesByResourceName(resourceName);	int numNodes = nodes.size();	if (numNodes == 0) {	
found resourcerequest for a non existent node rack named 

========================= hadoop sample_955 =========================

public void becomeStandby() {	cancelDisconnectTimer();	try {	rm.getRMContext().getRMAdminService().transitionToStandby(req);	} catch (Exception e) {	
rm could not transition to standby 

public void enterNeutralMode() {	
lost contact with zookeeper transitioning to standby in ms if connection is not reestablished 

public void fenceOldActive(byte[] oldActiveData) {	if (LOG.isDebugEnabled()) {	
request to fence old active being ignored as embedded leader election doesn t support fencing 

byte[] data;	try {	data = elector.getActiveData();	} catch (ActiveStandbyElector.ActiveNotFoundException e) {	return true;	}	YarnServerResourceManagerServiceProtos.ActiveRMInfoProto proto;	try {	proto = YarnServerResourceManagerServiceProtos.ActiveRMInfoProto .parseFrom(data);	} catch (InvalidProtocolBufferException e) {	
invalid data in zk 

========================= hadoop sample_1131 =========================

public void testJavaAssert() {	try {	assert false : "Good! Java assert is on.";	} catch(AssertionError ae) {	
the assertionerror is expected 

========================= hadoop sample_2859 =========================

dispatcher.getEventHandler().handle(new ContainerEvent(containerId, ContainerEventType.RECOVER_PAUSED_CONTAINER));	boolean notInterrupted = true;	try {	File pidFile = locatePidFile(appIdStr, containerIdStr);	if (pidFile != null) {	String pidPathStr = pidFile.getPath();	pidFilePath = new Path(pidPathStr);	exec.activateContainer(containerId, pidFilePath);	retCode = exec.reacquireContainer( new ContainerReacquisitionContext.Builder() .setContainer(container) .setUser(container.getUser()) .setContainerId(containerId) .build());	} else {	
unable to locate pid file for container 

try {	File pidFile = locatePidFile(appIdStr, containerIdStr);	if (pidFile != null) {	String pidPathStr = pidFile.getPath();	pidFilePath = new Path(pidPathStr);	exec.activateContainer(containerId, pidFilePath);	retCode = exec.reacquireContainer( new ContainerReacquisitionContext.Builder() .setContainer(container) .setUser(container.getUser()) .setContainerId(containerId) .build());	} else {	}	} catch (InterruptedException | InterruptedIOException e) {	
interrupted while waiting for exit code from 

if (pidFile != null) {	String pidPathStr = pidFile.getPath();	pidFilePath = new Path(pidPathStr);	exec.activateContainer(containerId, pidFilePath);	retCode = exec.reacquireContainer( new ContainerReacquisitionContext.Builder() .setContainer(container) .setUser(container.getUser()) .setContainerId(containerId) .build());	} else {	}	} catch (InterruptedException | InterruptedIOException e) {	notInterrupted = false;	} catch (IOException e) {	
unable to recover container 

} catch (InterruptedException | InterruptedIOException e) {	notInterrupted = false;	} catch (IOException e) {	} finally {	if (notInterrupted) {	this.completed.set(true);	exec.deactivateContainer(containerId);	try {	getContext().getNMStateStore().storeContainerCompleted(containerId, retCode);	} catch (IOException e) {	
unable to set exit code for container 

if (notInterrupted) {	this.completed.set(true);	exec.deactivateContainer(containerId);	try {	getContext().getNMStateStore().storeContainerCompleted(containerId, retCode);	} catch (IOException e) {	}	}	}	if (retCode != 0) {	
recovered container exited with a non zero exit code 

try {	getContext().getNMStateStore().storeContainerCompleted(containerId, retCode);	} catch (IOException e) {	}	}	}	if (retCode != 0) {	this.dispatcher.getEventHandler().handle(new ContainerExitEvent( containerId, ContainerEventType.CONTAINER_EXITED_WITH_FAILURE, retCode, "Container exited with a non-zero exit code " + retCode));	return retCode;	}	
recovered container succeeded 

========================= hadoop sample_1776 =========================

public void testAll() throws Throwable {	for (TestMatrixEntry testMatrixEntry : createTestMatrix()) {	Configuration conf = new HdfsConfiguration();	MiniDFSCluster cluster = setupTestCase( conf, testMatrixEntry.getProtectedPaths(), testMatrixEntry.getUnprotectedPaths());	try {	
running 

========================= hadoop sample_7358 =========================

localFS = FileSystem.getLocal(conf);	if (!localFS.exists(dbPath)) {	if (!localFS.mkdirs(dbPath)) {	throw new IOException("Couldn't create directory for leveldb " + "timeline store " + dbPath);	}	localFS.setPermission(dbPath, LeveldbUtils.LEVELDB_DIR_UMASK);	}	} finally {	IOUtils.cleanupWithLogger(LOG, localFS);	}	
using leveldb path 

public V get(K entityId) {	V result = null;	byte[] startTimeBytes = entityDb.get(getStartTimeKey(entityId));	if (startTimeBytes == null) {	return null;	}	try {	result = getEntityForKey(getEntityKey(entityId, startTimeBytes));	} catch (IOException e) {	
genericobjectmapper cannot read key from key into an object read aborted 

public void put(K entityId, V entity) {	Long startTime = entity.getStartTime();	if (startTime == null) {	startTime = System.currentTimeMillis();	}	byte[] startTimeBytes = GenericObjectMapper.writeReverseOrderedLong( startTime);	try {	byte[] valueBytes = GenericObjectMapper.write(entity);	entityDb.put(getEntityKey(entityId, startTimeBytes), valueBytes);	} catch (IOException e) {	
genericobjectmapper cannot write into a byte array write aborted 

}	return false;	}	public V next() {	if (hasNext()) {	Map.Entry<byte[], byte[]> nextRaw = internalDbIterator.next();	try {	V result = getEntityForKey(nextRaw.getKey());	return result;	} catch (IOException e) {	
genericobjectmapper cannot read key from key into an object read aborted 

try {	V result = getEntityForKey(nextRaw.getKey());	return result;	} catch (IOException e) {	LOG.error(e.getMessage());	}	}	return null;	}	public void remove() {	
leveldb map adapter does not support iterate and remove use cases 

========================= hadoop sample_304 =========================

if (iip.isRaw() || !fsd.ezManager.hasCreatedEncryptionZone() || !iip.getLastINode().isFile()) {	return null;	}	fsd.readLock();	try {	EncryptionZone encryptionZone = getEZForPath(fsd, iip);	if (encryptionZone == null) {	return null;	} else if(encryptionZone.getPath() == null || encryptionZone.getPath().isEmpty()) {	if (NameNode.LOG.isDebugEnabled()) {	
encryption zone does not have a valid path 

return null;	} else if(encryptionZone.getPath() == null || encryptionZone.getPath().isEmpty()) {	if (NameNode.LOG.isDebugEnabled()) {	}	}	final CryptoProtocolVersion version = encryptionZone.getVersion();	final CipherSuite suite = encryptionZone.getSuite();	final String keyName = encryptionZone.getKeyName();	XAttr fileXAttr = FSDirXAttrOp.unprotectedGetXAttrByPrefixedName( iip, CRYPTO_XATTR_FILE_ENCRYPTION_INFO);	if (fileXAttr == null) {	
could not find encryption xattr for file in encryption zone 

public void run() {	NameNode.LOG.info("Warming up {} EDEKs... (initialDelay={}, " + "retryInterval={})", keyNames.length, initialDelay, retryInterval);	try {	Thread.sleep(initialDelay);	} catch (InterruptedException ie) {	
edekcacheloader interrupted before warming up 

return;	}	final int logCoolDown = 10000;	int sinceLastLog = logCoolDown;	boolean success = false;	IOException lastSeenIOE = null;	long warmUpEDEKStartTime = monotonicNow();	while (true) {	try {	kp.warmUpEncryptedKeys(keyNames);	
successfully warmed up edeks 

IOException lastSeenIOE = null;	long warmUpEDEKStartTime = monotonicNow();	while (true) {	try {	kp.warmUpEncryptedKeys(keyNames);	success = true;	break;	} catch (IOException ioe) {	lastSeenIOE = ioe;	if (sinceLastLog >= logCoolDown) {	
failed to warm up edeks 

while (true) {	try {	kp.warmUpEncryptedKeys(keyNames);	success = true;	break;	} catch (IOException ioe) {	lastSeenIOE = ioe;	if (sinceLastLog >= logCoolDown) {	sinceLastLog = 0;	} else {	
failed to warm up edeks 

kp.warmUpEncryptedKeys(keyNames);	success = true;	break;	} catch (IOException ioe) {	lastSeenIOE = ioe;	if (sinceLastLog >= logCoolDown) {	sinceLastLog = 0;	} else {	}	} catch (Exception e) {	
cannot warm up edeks 

if (sinceLastLog >= logCoolDown) {	sinceLastLog = 0;	} else {	}	} catch (Exception e) {	throw e;	}	try {	Thread.sleep(retryInterval);	} catch (InterruptedException ie) {	
edekcacheloader interrupted during retry 

try {	Thread.sleep(retryInterval);	} catch (InterruptedException ie) {	break;	}	sinceLastLog += retryInterval;	}	long warmUpEDEKTime = monotonicNow() - warmUpEDEKStartTime;	NameNode.getNameNodeMetrics().addWarmUpEDEKTime(warmUpEDEKTime);	if (!success) {	
unable to warm up edeks 

Thread.sleep(retryInterval);	} catch (InterruptedException ie) {	break;	}	sinceLastLog += retryInterval;	}	long warmUpEDEKTime = monotonicNow() - warmUpEDEKStartTime;	NameNode.getNameNodeMetrics().addWarmUpEDEKTime(warmUpEDEKTime);	if (!success) {	if (lastSeenIOE != null) {	
last seen exception 

========================= hadoop sample_8042 =========================

public void pipeline_01() throws IOException {	final String METHOD_NAME = GenericTestUtils.getMethodName();	if(LOG.isDebugEnabled()) {	
running 

public void pipeline_01() throws IOException {	final String METHOD_NAME = GenericTestUtils.getMethodName();	if(LOG.isDebugEnabled()) {	}	Path filePath = new Path("/" + METHOD_NAME + ".dat");	DFSTestUtil.createFile(fs, filePath, FILE_SIZE, REPL_FACTOR, rand.nextLong());	if(LOG.isDebugEnabled()) {	
invoking append but doing nothing otherwise 

byte[] toWrite = new byte[1024];	int written = 0;	Random rb = new Random(rand.nextLong());	while (bytesToWrite > 0) {	rb.nextBytes(toWrite);	int bytesToWriteNext = (1024 < bytesToWrite) ? 1024 : bytesToWrite;	out.write(toWrite, 0, bytesToWriteNext);	System.arraycopy(toWrite, 0, ret, (ret.length - bytesToWrite), bytesToWriteNext);	written += bytesToWriteNext;	if(LOG.isDebugEnabled()) {	
written total 

========================= hadoop sample_7134 =========================

public void run() {	long cutoffMillis = System.currentTimeMillis() - retentionMillis;	
aggregated log deletion started 

FileSystem fs = remoteRootLogDir.getFileSystem(conf);	for(FileStatus userDir : fs.listStatus(remoteRootLogDir)) {	if(userDir.isDirectory()) {	Path userDirPath = new Path(userDir.getPath(), suffix);	deleteOldLogDirsFrom(userDirPath, cutoffMillis, fs, rmClient);	}	}	} catch (IOException e) {	logIOException("Error reading root log dir this deletion " + "attempt is being aborted", e);	}	
aggregated log deletion finished 

========================= hadoop sample_2336 =========================

private static FileDescriptor getFileDescriptorIfAvail(InputStream in) {	FileDescriptor fd = null;	try {	if (in instanceof HasFileDescriptor) {	fd = ((HasFileDescriptor)in).getFileDescriptor();	} else if (in instanceof FileInputStream) {	fd = ((FileInputStream)in).getFD();	}	} catch (IOException e) {	
unable to determine filedescriptor 

========================= hadoop sample_4718 =========================

return m;	}	String output = runResolveCommand(names, scriptName);	if (output != null) {	StringTokenizer allSwitchInfo = new StringTokenizer(output);	while (allSwitchInfo.hasMoreTokens()) {	String switchInfo = allSwitchInfo.nextToken();	m.add(switchInfo);	}	if (m.size() != names.size()) {	
script returned values when were expected 

File dir = null;	String userDir;	if ((userDir = System.getProperty("user.dir")) != null) {	dir = new File(userDir);	}	ShellCommandExecutor s = new ShellCommandExecutor( cmdList.toArray(new String[cmdList.size()]), dir);	try {	s.execute();	allOutput.append(s.getOutput()).append(" ");	} catch (Exception e) {	
exception running 

========================= hadoop sample_3527 =========================

final Collection<URI> sharedEditsDirs = FSNamesystem.getSharedEditsDirs(conf);	for (URI u : requiredEditsDirs) {	if (u.toString().compareTo( DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_DEFAULT) == 0) {	continue;	}	if (!editsDirs.contains(u) && !sharedEditsDirs.contains(u)) {	throw new IllegalArgumentException("Required edits directory " + u + " not found: " + DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_KEY + "=" + editsDirs + "; " + DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_REQUIRED_KEY + "=" + requiredEditsDirs + "; " + DFSConfigKeys.DFS_NAMENODE_SHARED_EDITS_DIR_KEY + "=" + sharedEditsDirs);	}	}	if (namespaceDirs.size() == 1) {	
only one image storage directory configured beware of data loss due to lack of redundant storage directories 

if (u.toString().compareTo( DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_DEFAULT) == 0) {	continue;	}	if (!editsDirs.contains(u) && !sharedEditsDirs.contains(u)) {	throw new IllegalArgumentException("Required edits directory " + u + " not found: " + DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_KEY + "=" + editsDirs + "; " + DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_REQUIRED_KEY + "=" + requiredEditsDirs + "; " + DFSConfigKeys.DFS_NAMENODE_SHARED_EDITS_DIR_KEY + "=" + sharedEditsDirs);	}	}	if (namespaceDirs.size() == 1) {	}	if (editsDirs.size() == 1) {	
only one namespace edits storage directory configured beware of data loss due to lack of redundant storage directories 

FSImage fsImage = new FSImage(conf, FSNamesystem.getNamespaceDirs(conf), FSNamesystem.getNamespaceEditsDirs(conf));	FSNamesystem namesystem = new FSNamesystem(conf, fsImage, false);	StartupOption startOpt = NameNode.getStartupOption(conf);	if (startOpt == StartupOption.RECOVER) {	namesystem.setSafeMode(SafeModeAction.SAFEMODE_ENTER);	}	long loadStart = monotonicNow();	try {	namesystem.loadFSImage(startOpt);	} catch (IOException ioe) {	
encountered exception loading fsimage 

namesystem.setSafeMode(SafeModeAction.SAFEMODE_ENTER);	}	long loadStart = monotonicNow();	try {	namesystem.loadFSImage(startOpt);	} catch (IOException ioe) {	fsImage.close();	throw ioe;	}	long timeTakenToLoadFSImage = monotonicNow() - loadStart;	
finished loading fsimage in msecs 

static RetryCache initRetryCache(Configuration conf) {	boolean enable = conf.getBoolean(DFS_NAMENODE_ENABLE_RETRY_CACHE_KEY, DFS_NAMENODE_ENABLE_RETRY_CACHE_DEFAULT);	
retry cache on namenode is enabled disabled 

static RetryCache initRetryCache(Configuration conf) {	boolean enable = conf.getBoolean(DFS_NAMENODE_ENABLE_RETRY_CACHE_KEY, DFS_NAMENODE_ENABLE_RETRY_CACHE_DEFAULT);	if (enable) {	float heapPercent = conf.getFloat( DFS_NAMENODE_RETRY_CACHE_HEAP_PERCENT_KEY, DFS_NAMENODE_RETRY_CACHE_HEAP_PERCENT_DEFAULT);	long entryExpiryMillis = conf.getLong( DFS_NAMENODE_RETRY_CACHE_EXPIRYTIME_MILLIS_KEY, DFS_NAMENODE_RETRY_CACHE_EXPIRYTIME_MILLIS_DEFAULT);	
retry cache will use of total heap and retry cache entry expiry time is millis 

private static Collection<URI> getStorageDirs(Configuration conf, String propertyName) {	Collection<String> dirNames = conf.getTrimmedStringCollection(propertyName);	StartupOption startOpt = NameNode.getStartupOption(conf);	if(startOpt == StartupOption.IMPORT) {	Configuration cE = new HdfsConfiguration(false);	cE.addResource("core-default.xml");	cE.addResource("core-site.xml");	cE.addResource("hdfs-default.xml");	Collection<String> dirNames2 = cE.getTrimmedStringCollection(propertyName);	dirNames.removeAll(dirNames2);	
warning tthe namenode currently runs without persistent storage tany changes to the file system meta data may be lost trecommended actions t t shutdown and restart namenode with configured propertyname in hdfs site xml t t use backup node as a persistent and up to date storage of the file system meta data 

public static List<URI> getNamespaceEditsDirs(Configuration conf, boolean includeShared) throws IOException {	LinkedHashSet<URI> editsDirs = new LinkedHashSet<URI>();	if (includeShared) {	List<URI> sharedDirs = getSharedEditsDirs(conf);	if (sharedDirs.size() > 1) {	throw new IOException( "Multiple shared edits directories are not yet supported");	}	for (URI dir : sharedDirs) {	if (!editsDirs.add(dir)) {	
edits uri listed multiple times in ignoring duplicates 

if (sharedDirs.size() > 1) {	throw new IOException( "Multiple shared edits directories are not yet supported");	}	for (URI dir : sharedDirs) {	if (!editsDirs.add(dir)) {	}	}	}	for (URI dir : getStorageDirs(conf, DFS_NAMENODE_EDITS_DIR_KEY)) {	if (!editsDirs.add(dir)) {	
edits uri listed multiple times in and ignoring duplicates 

final INodeFile bc = getBlockCollection(blockCollectionID);	if (bc == null || !bc.isUnderConstruction()) {	return false;	}	String fullName = bc.getName();	try {	if (fullName != null && fullName.startsWith(Path.SEPARATOR) && dir.getINode(fullName, DirOp.READ) == bc) {	return false;	}	} catch (IOException e) {	
error while resolving the path 

private void closeFile(String path, INodeFile file) {	assert hasWriteLock();	getEditLog().logCloseFile(path, file);	
closefile with bloks is persisted to the file system 

public void run () {	try {	while (fsRunning && shouldNNRmRun) {	checkAvailableResources();	if(!nameNodeHasResourcesAvailable()) {	String lowResourcesMsg = "NameNode low on available disk space. ";	if (!isInSafeMode()) {	
entering safe mode 

public void run () {	try {	while (fsRunning && shouldNNRmRun) {	checkAvailableResources();	if(!nameNodeHasResourcesAvailable()) {	String lowResourcesMsg = "NameNode low on available disk space. ";	if (!isInSafeMode()) {	} else {	
already in safe mode 

} else {	}	enterSafeMode(true);	}	try {	Thread.sleep(resourceRecheckInterval);	} catch (InterruptedException ie) {	}	}	} catch (Exception e) {	
exception in namenoderesourcemonitor 

public void run() {	while (fsRunning && shouldRun) {	try {	long numEdits = getCorrectTransactionsSinceLastLogRoll();	if (numEdits > rollThreshold) {	
namenode rolling its own edit log because number of edits in open segment exceeds threshold of 

public void run() {	while (fsRunning && shouldRun) {	try {	long numEdits = getCorrectTransactionsSinceLastLogRoll();	if (numEdits > rollThreshold) {	rollEditLog();	}	} catch (Exception e) {	
swallowing exception in 

try {	long numEdits = getCorrectTransactionsSinceLastLogRoll();	if (numEdits > rollThreshold) {	rollEditLog();	}	} catch (Exception e) {	}	try {	Thread.sleep(sleepIntervalMs);	} catch (InterruptedException e) {	
was interrupted exiting 

BlockStoragePolicy lpPolicy = blockManager.getStoragePolicy("LAZY_PERSIST");	List<BlockCollection> filesToDelete = new ArrayList<>();	boolean changed = false;	writeLock();	try {	final Iterator<BlockInfo> it = blockManager.getCorruptReplicaBlockIterator();	while (it.hasNext()) {	Block b = it.next();	BlockInfo blockInfo = blockManager.getStoredBlock(b);	if (blockInfo == null) {	
cannot find block info for block 

BlockInfo blockInfo = blockManager.getStoredBlock(b);	if (blockInfo == null) {	} else {	BlockCollection bc = getBlockCollection(blockInfo);	if (bc.getStoragePolicyID() == lpPolicy.getId()) {	filesToDelete.add(bc);	}	}	}	for (BlockCollection bc : filesToDelete) {	
removing lazypersist file with no replicas 

public void run() {	while (fsRunning && shouldRun) {	try {	if (!isInSafeMode()) {	clearCorruptLazyPersistFiles();	} else {	if (FSNamesystem.LOG.isDebugEnabled()) {	
namenode is in safemode skipping scrubbing of corrupted lazy persist files 

public void run() {	while (fsRunning && shouldRun) {	try {	if (!isInSafeMode()) {	clearCorruptLazyPersistFiles();	} else {	if (FSNamesystem.LOG.isDebugEnabled()) {	}	}	} catch (Exception e) {	
ignoring exception in lazypersistfilescrubber 

clearCorruptLazyPersistFiles();	} else {	if (FSNamesystem.LOG.isDebugEnabled()) {	}	}	} catch (Exception e) {	}	try {	Thread.sleep(scrubIntervalSec * 1000);	} catch (InterruptedException e) {	
lazypersistfilescrubber was interrupted exiting 

private void registerMBean() {	try {	StandardMBean bean = new StandardMBean(this, FSNamesystemMBean.class);	mbeanName = MBeans.register("NameNode", "FSNamesystemState", bean);	} catch (NotCompliantMBeanException e) {	throw new RuntimeException("Bad MBean setup", e);	}	
registered fsnamesystemstate mbean 

return null;	}	Date now = new Date();	final List<RollingWindowManager.TopWindow> topWindows = topMetrics.getTopWindows();	Map<String, Object> topMap = new TreeMap<String, Object>();	topMap.put("windows", topWindows);	topMap.put("timestamp", DFSUtil.dateToIso8601String(now));	try {	return JsonUtil.toJsonString(topMap);	} catch (IOException e) {	
failed to fetch topuser metrics 

try {	corruptFileBlocks = listCorruptFileBlocks("/", null);	int corruptFileCount = corruptFileBlocks.size();	if (corruptFileCount != 0) {	for (FSNamesystem.CorruptFileBlockInfo c : corruptFileBlocks) {	list.add(c.toString());	}	}	} catch (StandbyException e) {	if (LOG.isDebugEnabled()) {	
get corrupt file blocks returned error 

int corruptFileCount = corruptFileBlocks.size();	if (corruptFileCount != 0) {	for (FSNamesystem.CorruptFileBlockInfo c : corruptFileBlocks) {	list.add(c.toString());	}	}	} catch (StandbyException e) {	if (LOG.isDebugEnabled()) {	}	} catch (IOException e) {	
get corrupt file blocks returned error 

private void startRollingUpgradeInternalForNonHA(long startTime) throws IOException {	Preconditions.checkState(!haEnabled);	if (!isInSafeMode()) {	throw new IOException("Safe mode should be turned ON " + "in order to create namespace image.");	}	checkRollingUpgrade("start rolling upgrade");	getFSImage().checkUpgrade();	getFSImage().saveNamespace(this, NameNodeFile.IMAGE_ROLLBACK, null);	
successfully saved namespace for preparing rolling upgrade 

try {	upgradeInfo = getRollingUpgradeInfo();	if (upgradeInfo == null) {	return null;	}	if (!upgradeInfo.createdRollbackImages()) {	boolean hasRollbackImage = this.getFSImage().hasRollbackFSImage();	upgradeInfo.setCreatedRollbackImages(hasRollbackImage);	}	} catch (IOException ioe) {	
encountered exception setting rollback image 

========================= hadoop sample_7972 =========================

JvmMetrics.create("JournalNode", conf.get(DFSConfigKeys.DFS_METRICS_SESSION_ID_KEY), DefaultMetricsSystem.instance());	InetSocketAddress socAddr = JournalNodeRpcServer.getAddress(conf);	SecurityUtil.login(conf, DFSConfigKeys.DFS_JOURNALNODE_KEYTAB_FILE_KEY, DFSConfigKeys.DFS_JOURNALNODE_KERBEROS_PRINCIPAL_KEY, socAddr.getHostName());	registerJNMXBean();	httpServer = new JournalNodeHttpServer(conf, this);	httpServer.start();	httpServerURI = httpServer.getServerURI().toString();	rpcServer = new JournalNodeRpcServer(conf, this);	rpcServer.start();	} catch (IOException ioe) {	
failed to start journalnode 

public void stop(int rc) {	this.resultCode = rc;	if (rpcServer != null) {	rpcServer.stop();	}	if (httpServer != null) {	try {	httpServer.stop();	} catch (IOException ioe) {	
unable to stop http server for 

public void reportErrorOnFile(File f) {	
error reported on file exiting 

public static void main(String[] args) throws Exception {	StringUtils.startupShutdownMessage(JournalNode.class, args, LOG);	try {	System.exit(ToolRunner.run(new JournalNode(), args));	} catch (Throwable e) {	
failed to start journalnode 

========================= hadoop sample_8384 =========================

protected synchronized void cleanupRunningContainers() {	for (StartedContainer startedContainer : startedContainers.values()) {	try {	stopContainer(startedContainer.getContainerId(), startedContainer.getNodeId());	} catch (YarnException e) {	
failed to stop container when stopping nmclientimpl 

protected synchronized void cleanupRunningContainers() {	for (StartedContainer startedContainer : startedContainers.values()) {	try {	stopContainer(startedContainer.getContainerId(), startedContainer.getNodeId());	} catch (YarnException e) {	} catch (IOException e) {	
failed to stop container when stopping nmclientimpl 

========================= hadoop sample_2623 =========================

this.clock = clock;	this.startTime = clock.getTime();	this.appSubmitTime = appSubmitTime;	this.appAttemptID = applicationAttemptId;	this.containerID = containerId;	this.nmHost = nmHost;	this.nmPort = nmPort;	this.nmHttpPort = nmHttpPort;	this.metrics = MRAppMetrics.create();	logSyncer = TaskLog.createLogSyncer();	
created mrappmaster for application 

taskAttemptFinishingMonitor = createTaskAttemptFinishingMonitor(dispatcher.getEventHandler());	addIfService(taskAttemptFinishingMonitor);	context = new RunningAppContext(conf, taskAttemptFinishingMonitor);	appName = conf.get(MRJobConfig.JOB_NAME, "<missing app name>");	conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());	newApiCommitter = false;	jobId = MRBuilderUtils.newJobId(appAttemptID.getApplicationId(), appAttemptID.getApplicationId().getId());	int numReduceTasks = conf.getInt(MRJobConfig.NUM_REDUCES, 0);	if ((numReduceTasks > 0 && conf.getBoolean("mapred.reducer.new-api", false)) || (numReduceTasks == 0 && conf.getBoolean("mapred.mapper.new-api", false)))  {	newApiCommitter = true;	
using mapred newapicommitter 

FileSystem fs = getFileSystem(conf);	boolean stagingExists = fs.exists(stagingDir);	Path startCommitFile = MRApps.getStartJobCommitFile(conf, user, jobId);	boolean commitStarted = fs.exists(startCommitFile);	Path endCommitSuccessFile = MRApps.getEndJobCommitSuccessFile(conf, user, jobId);	boolean commitSuccess = fs.exists(endCommitSuccessFile);	Path endCommitFailureFile = MRApps.getEndJobCommitFailureFile(conf, user, jobId);	boolean commitFailure = fs.exists(endCommitFailureFile);	if(!stagingExists) {	isLastAMRetry = true;	
attempt num is last retry because the staging dir doesn t exist 

boolean commitFailure = fs.exists(endCommitFailureFile);	if(!stagingExists) {	isLastAMRetry = true;	errorHappenedShutDown = true;	forcedState = JobStateInternal.ERROR;	shutDownMessage = "Staging dir does not exist " + stagingDir;	LOG.fatal(shutDownMessage);	} else if (commitStarted) {	errorHappenedShutDown = true;	isLastAMRetry = true;	
attempt num is last retry because a commit was started 

private void cleanupInterruptedCommit(Configuration conf, FileSystem fs, Path startCommitFile) throws IOException {	
delete startjobcommitfile in case commit is not finished as successful or failed 

private OutputCommitter createOutputCommitter(Configuration conf) {	return callWithJobClassLoader(conf, new Action<OutputCommitter>() {	public OutputCommitter call(Configuration conf) {	OutputCommitter committer = null;	
outputcommitter set in config mapred output committer class 

OutputFormat outputFormat;	try {	outputFormat = ReflectionUtils.newInstance(taskContext .getOutputFormatClass(), conf);	committer = outputFormat.getOutputCommitter(taskContext);	} catch (Exception e) {	throw new YarnRuntimeException(e);	}	} else {	committer = ReflectionUtils.newInstance(conf.getClass( "mapred.output.committer.class", FileOutputCommitter.class, org.apache.hadoop.mapred.OutputCommitter.class), conf);	}	
outputcommitter is 

public void cleanupStagingDir() throws IOException {	String jobTempDir = getConfig().get(MRJobConfig.MAPREDUCE_JOB_DIR);	FileSystem fs = getFileSystem(getConfig());	try {	if (!keepJobFiles(new JobConf(getConfig()), jobTempDir)) {	jobTempDir = getConfig().get(MRJobConfig.MAPREDUCE_JOB_DIR);	if (jobTempDir == null) {	
job staging directory is null 

public void cleanupStagingDir() throws IOException {	String jobTempDir = getConfig().get(MRJobConfig.MAPREDUCE_JOB_DIR);	FileSystem fs = getFileSystem(getConfig());	try {	if (!keepJobFiles(new JobConf(getConfig()), jobTempDir)) {	jobTempDir = getConfig().get(MRJobConfig.MAPREDUCE_JOB_DIR);	if (jobTempDir == null) {	return;	}	Path jobTempDirPath = new Path(jobTempDir);	
deleting staging directory 

try {	if (!keepJobFiles(new JobConf(getConfig()), jobTempDir)) {	jobTempDir = getConfig().get(MRJobConfig.MAPREDUCE_JOB_DIR);	if (jobTempDir == null) {	return;	}	Path jobTempDirPath = new Path(jobTempDir);	fs.delete(jobTempDirPath, true);	}	} catch(IOException io) {	
failed to cleanup staging dir 

public void shutDownJob() {	JobEndNotifier notifier = null;	if (getConfig().get(MRJobConfig.MR_JOB_END_NOTIFICATION_URL) != null) {	notifier = new JobEndNotifier();	notifier.setConf(getConfig());	}	try {	if ( !isLastAMRetry){	if (((JobImpl)job).getInternalState() != JobStateInternal.REBOOT) {	
job finished cleanly recording last mrappmaster retry 

notifier = new JobEndNotifier();	notifier.setConf(getConfig());	}	try {	if ( !isLastAMRetry){	if (((JobImpl)job).getInternalState() != JobStateInternal.REBOOT) {	isLastAMRetry = true;	}	}	notifyIsLastAMRetry(isLastAMRetry);	
calling stop for all the services 

sendJobEndNotify(notifier);	notifier = null;	}	try {	Thread.sleep(5000);	} catch (InterruptedException e) {	e.printStackTrace();	}	clientService.stop();	} catch (Throwable t) {	
graceful stop failed exiting 

private void sendJobEndNotify(JobEndNotifier notifier) {	try {	
job end notification started for jobid 

private void sendJobEndNotify(JobEndNotifier notifier) {	try {	JobReport report = job.getReport();	if (!context.hasSuccessfullyUnregistered()) {	report.setJobState(JobState.FAILED);	}	notifier.notify(report);	} catch (InterruptedException ie) {	
job end notification interrupted for jobid 

protected Speculator createSpeculator(Configuration conf, final AppContext context) {	return callWithJobClassLoader(conf, new Action<Speculator>() {	public Speculator call(Configuration conf) {	Class<? extends Speculator> speculatorClass;	try {	speculatorClass = conf.getClass(MRJobConfig.MR_AM_JOB_SPECULATOR, DefaultSpeculator.class, Speculator.class);	Constructor<? extends Speculator> speculatorConstructor = speculatorClass.getConstructor (Configuration.class, AppContext.class);	Speculator result = speculatorConstructor.newInstance(conf, context);	return result;	} catch (InstantiationException ex) {	
can t make a speculator check 

public Speculator call(Configuration conf) {	Class<? extends Speculator> speculatorClass;	try {	speculatorClass = conf.getClass(MRJobConfig.MR_AM_JOB_SPECULATOR, DefaultSpeculator.class, Speculator.class);	Constructor<? extends Speculator> speculatorConstructor = speculatorClass.getConstructor (Configuration.class, AppContext.class);	Speculator result = speculatorConstructor.newInstance(conf, context);	return result;	} catch (InstantiationException ex) {	throw new YarnRuntimeException(ex);	} catch (IllegalAccessException ex) {	
can t make a speculator check 

try {	speculatorClass = conf.getClass(MRJobConfig.MR_AM_JOB_SPECULATOR, DefaultSpeculator.class, Speculator.class);	Constructor<? extends Speculator> speculatorConstructor = speculatorClass.getConstructor (Configuration.class, AppContext.class);	Speculator result = speculatorConstructor.newInstance(conf, context);	return result;	} catch (InstantiationException ex) {	throw new YarnRuntimeException(ex);	} catch (IllegalAccessException ex) {	throw new YarnRuntimeException(ex);	} catch (InvocationTargetException ex) {	
can t make a speculator check 

Constructor<? extends Speculator> speculatorConstructor = speculatorClass.getConstructor (Configuration.class, AppContext.class);	Speculator result = speculatorConstructor.newInstance(conf, context);	return result;	} catch (InstantiationException ex) {	throw new YarnRuntimeException(ex);	} catch (IllegalAccessException ex) {	throw new YarnRuntimeException(ex);	} catch (InvocationTargetException ex) {	throw new YarnRuntimeException(ex);	} catch (NoSuchMethodException ex) {	
can t make a speculator check 

protected void serviceStop() throws Exception {	try {	if(isLastAMRetry) {	cleanupStagingDir();	} else {	
skipping cleaning up the staging dir assuming am will be retried 

protected void serviceStop() throws Exception {	try {	if(isLastAMRetry) {	cleanupStagingDir();	} else {	}	} catch (IOException io) {	
failed to cleanup staging dir 

dispatcher.getEventHandler().handle( new JobHistoryEvent(job.getID(), new AMStartedEvent(amInfo .getAppAttemptId(), amInfo.getStartTime(), amInfo.getContainerId(), amInfo.getNodeManagerHost(), amInfo.getNodeManagerPort(), amInfo .getNodeManagerHttpPort(), this.forcedState == null ? null : this.forcedState.toString(), appSubmitTime)));	amInfos.add(amInfo);	DefaultMetricsSystem.initialize("MRAppMaster");	boolean initFailed = false;	if (!errorHappenedShutDown) {	JobEvent initJobEvent = new JobEvent(job.getID(), JobEventType.JOB_INIT);	jobEventDispatcher.handle(initJobEvent);	initFailed = (((JobImpl)job).getInternalState() != JobStateInternal.INITED);	if (job.isUber()) {	speculatorEventDispatcher.disableSpeculation();	
mrappmaster uberizing job in local container on node 

DefaultMetricsSystem.initialize("MRAppMaster");	boolean initFailed = false;	if (!errorHappenedShutDown) {	JobEvent initJobEvent = new JobEvent(job.getID(), JobEventType.JOB_INIT);	jobEventDispatcher.handle(initJobEvent);	initFailed = (((JobImpl)job).getInternalState() != JobStateInternal.INITED);	if (job.isUber()) {	speculatorEventDispatcher.disableSpeculation();	} else {	dispatcher.getEventHandler().handle( new SpeculatorEvent(job.getID(), clock.getTime()));	
mrappmaster launching normal non uberized multi container job 

private void processRecovery() throws IOException{	boolean attemptRecovery = shouldAttemptRecovery();	boolean recoverySucceeded = true;	if (attemptRecovery) {	
attempting to recover 

private void processRecovery() throws IOException{	boolean attemptRecovery = shouldAttemptRecovery();	boolean recoverySucceeded = true;	if (attemptRecovery) {	try {	parsePreviousJobHistory();	} catch (IOException e) {	
unable to parse prior job history aborting recovery 

private boolean shouldAttemptRecovery() throws IOException {	if (isFirstAttempt()) {	return false;	}	boolean recoveryEnabled = getConfig().getBoolean( MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE_DEFAULT);	if (!recoveryEnabled) {	
not attempting to recover recovery disabled to enable recovery set 

private boolean shouldAttemptRecovery() throws IOException {	if (isFirstAttempt()) {	return false;	}	boolean recoveryEnabled = getConfig().getBoolean( MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE_DEFAULT);	if (!recoveryEnabled) {	return false;	}	boolean recoverySupportedByCommitter = isRecoverySupported();	if (!recoverySupportedByCommitter) {	
not attempting to recover recovery is not supported by use an outputcommitter that supports recovery 

if (!recoveryEnabled) {	return false;	}	boolean recoverySupportedByCommitter = isRecoverySupported();	if (!recoverySupportedByCommitter) {	return false;	}	int reducerCount = getConfig().getInt(MRJobConfig.NUM_REDUCES, 0);	boolean shuffleKeyValidForRecovery = TokenCache.getShuffleSecretKey(jobCredentials) != null;	if (reducerCount > 0 && !shuffleKeyValidForRecovery) {	
not attempting to recover the shuffle key is invalid for recovery 

if (!recoverySupportedByCommitter) {	return false;	}	int reducerCount = getConfig().getInt(MRJobConfig.NUM_REDUCES, 0);	boolean shuffleKeyValidForRecovery = TokenCache.getShuffleSecretKey(jobCredentials) != null;	if (reducerCount > 0 && !shuffleKeyValidForRecovery) {	return false;	}	boolean spillEncrypted = CryptoUtils.isEncryptedSpillEnabled(getConfig());	if (reducerCount > 0 && spillEncrypted) {	
not attempting to recover intermediate spill encryption is enabled 

private static FSDataInputStream getPreviousJobHistoryStream( Configuration conf, ApplicationAttemptId appAttemptId) throws IOException {	Path historyFile = JobHistoryUtils.getPreviousJobHistoryPath(conf, appAttemptId);	
previous history file is at 

private void parsePreviousJobHistory() throws IOException {	FSDataInputStream in = getPreviousJobHistoryStream(getConfig(), appAttemptID);	JobHistoryParser parser = new JobHistoryParser(in);	JobInfo jobInfo = parser.parse();	Exception parseException = parser.getParseException();	if (parseException != null) {	
got an error parsing job history file ignoring incomplete events 

for (TaskInfo taskInfo : taskInfos.values()) {	if (TaskState.SUCCEEDED.toString().equals(taskInfo.getTaskStatus())) {	Iterator<Entry<TaskAttemptID, TaskAttemptInfo>> taskAttemptIterator = taskInfo.getAllTaskAttempts().entrySet().iterator();	while (taskAttemptIterator.hasNext()) {	Map.Entry<TaskAttemptID, TaskAttemptInfo> currentEntry = taskAttemptIterator.next();	if (!jobInfo.getAllCompletedTaskAttempts().containsKey(currentEntry.getKey())) {	taskAttemptIterator.remove();	}	}	completedTasksFromPreviousRun .put(TypeConverter.toYarn(taskInfo.getTaskId()), taskInfo);	
read from history task 

Iterator<Entry<TaskAttemptID, TaskAttemptInfo>> taskAttemptIterator = taskInfo.getAllTaskAttempts().entrySet().iterator();	while (taskAttemptIterator.hasNext()) {	Map.Entry<TaskAttemptID, TaskAttemptInfo> currentEntry = taskAttemptIterator.next();	if (!jobInfo.getAllCompletedTaskAttempts().containsKey(currentEntry.getKey())) {	taskAttemptIterator.remove();	}	}	completedTasksFromPreviousRun .put(TypeConverter.toYarn(taskInfo.getTaskId()), taskInfo);	}	}	
read completed tasks from history 

if (!amStartedEventsBegan) {	amStartedEventsBegan = true;	}	AMStartedEvent amStartedEvent = (AMStartedEvent) event;	amInfos.add(MRBuilderUtils.newAMInfo( amStartedEvent.getAppAttemptId(), amStartedEvent.getStartTime(), amStartedEvent.getContainerId(), StringInterner.weakIntern(amStartedEvent.getNodeManagerHost()), amStartedEvent.getNodeManagerPort(), amStartedEvent.getNodeManagerHttpPort()));	} else if (amStartedEventsBegan) {	break;	}	}	} catch (IOException e) {	
could not parse the old history file will not have old aminfos 

conf.addResource(new Path(MRJobConfig.JOB_CONF_FILE));	MRWebAppUtil.initialize(conf);	String systemPropsToLog = MRApps.getSystemPropertiesToLog(conf);	if (systemPropsToLog != null) {	LOG.info(systemPropsToLog);	}	String jobUserName = System .getenv(ApplicationConstants.Environment.USER.name());	conf.set(MRJobConfig.USER_NAME, jobUserName);	initAndStartAppMaster(appMaster, conf, jobUserName);	} catch (Throwable t) {	
error starting mrappmaster 

public void run() {	
mrappmaster received a signal signaling rmcommunicator and jobhistoryeventhandler 

public void notifyIsLastAMRetry(boolean isLastAMRetry){	if(containerAllocator instanceof ContainerAllocatorRouter) {	
notify rmcommunicator isamlastretry 

public void notifyIsLastAMRetry(boolean isLastAMRetry){	if(containerAllocator instanceof ContainerAllocatorRouter) {	((ContainerAllocatorRouter) containerAllocator) .setShouldUnregister(isLastAMRetry);	}	if(jobHistoryEventHandler != null) {	
notify jheh isamlastretry 

protected static void initAndStartAppMaster(final MRAppMaster appMaster, final JobConf conf, String jobUserName) throws IOException, InterruptedException {	UserGroupInformation.setConfiguration(conf);	SecurityUtil.setConfiguration(conf);	Credentials credentials = UserGroupInformation.getCurrentUser().getCredentials();	
executing with tokens 

========================= hadoop sample_5298 =========================

Path fullyResolvedPath = fs.resolvePath(p);	FileSystem fullyResolvedFs = FileSystem.get(fullyResolvedPath.toUri(), conf);	try {	long trashInterval = fullyResolvedFs.getServerDefaults( fullyResolvedPath).getTrashInterval();	if (0 != trashInterval) {	Configuration confCopy = new Configuration(conf);	confCopy.setLong(CommonConfigurationKeysPublic.FS_TRASH_INTERVAL_KEY, trashInterval);	conf = confCopy;	}	} catch (Exception e) {	
failed to get server trash configuration 

========================= hadoop sample_4173 =========================

protected void storeNewMasterKey(DelegationKey newKey) {	try {	
storing master key with keyid 

protected void storeNewMasterKey(DelegationKey newKey) {	try {	rm.getRMContext().getStateStore().storeRMDTMasterKey(newKey);	} catch (Exception e) {	
error in storing master key with keyid 

protected void removeStoredMasterKey(DelegationKey key) {	try {	
removing master key with keyid 

protected void removeStoredMasterKey(DelegationKey key) {	try {	rm.getRMContext().getStateStore().removeRMDTMasterKey(key);	} catch (Exception e) {	
error in removing master key with keyid 

protected void storeNewToken(RMDelegationTokenIdentifier identifier, long renewDate) {	try {	
storing rmdelegation token with sequence number 

protected void storeNewToken(RMDelegationTokenIdentifier identifier, long renewDate) {	try {	rm.getRMContext().getStateStore().storeRMDelegationToken(identifier, renewDate);	} catch (Exception e) {	
error in storing rmdelegationtoken with sequence number 

protected void updateStoredToken(RMDelegationTokenIdentifier id, long renewDate) {	try {	
updating rmdelegation token with sequence number 

protected void updateStoredToken(RMDelegationTokenIdentifier id, long renewDate) {	try {	rm.getRMContext().getStateStore().updateRMDelegationToken(id, renewDate);	} catch (Exception e) {	
error in updating persisted rmdelegationtoken with sequence number 

protected void removeStoredToken(RMDelegationTokenIdentifier ident) throws IOException {	try {	
removing rmdelegation token with sequence number 

protected void removeStoredToken(RMDelegationTokenIdentifier ident) throws IOException {	try {	rm.getRMContext().getStateStore().removeRMDelegationToken(ident);	} catch (Exception e) {	
error in removing rmdelegationtoken with sequence number 

public void recover(RMState rmState) throws Exception {	
recovering rmdelegationtokensecretmanager 

========================= hadoop sample_703 =========================

conf.set(CommonConfigurationKeys.FS_PERMISSIONS_UMASK_KEY,  "000");	try {	Path stagingPath = FileContext.getFileContext(conf).makeQualified( new Path(conf.get(MRJobConfig.MR_AM_STAGING_DIR)));	if (Path.WINDOWS) {	if (LocalFileSystem.class.isInstance(stagingPath.getFileSystem(conf))) {	conf.set(MRJobConfig.MR_AM_STAGING_DIR, new File(conf.get(MRJobConfig.MR_AM_STAGING_DIR)) .getAbsolutePath());	}	}	FileContext fc=FileContext.getFileContext(stagingPath.toUri(), conf);	if (fc.util().exists(stagingPath)) {	
exists deleting 

Path stagingPath = FileContext.getFileContext(conf).makeQualified( new Path(conf.get(MRJobConfig.MR_AM_STAGING_DIR)));	if (Path.WINDOWS) {	if (LocalFileSystem.class.isInstance(stagingPath.getFileSystem(conf))) {	conf.set(MRJobConfig.MR_AM_STAGING_DIR, new File(conf.get(MRJobConfig.MR_AM_STAGING_DIR)) .getAbsolutePath());	}	}	FileContext fc=FileContext.getFileContext(stagingPath.toUri(), conf);	if (fc.util().exists(stagingPath)) {	fc.delete(stagingPath, true);	}	
mkdir 

}, 1500, 60_000);	if (historyServer.getServiceState() != STATE.STARTED) {	throw new IOException("HistoryServer failed to start");	}	super.serviceStart();	} catch (Throwable t) {	throw new YarnRuntimeException(t);	}	getConfig().set(JHAdminConfig.MR_HISTORY_ADDRESS, historyServer.getConfig().get(JHAdminConfig.MR_HISTORY_ADDRESS));	MRWebAppUtil.setJHSWebappURLWithoutScheme(getConfig(), MRWebAppUtil.getJHSWebappURLWithoutScheme(historyServer.getConfig()));	
minimryarn resourcemanager address 

}, 1500, 60_000);	if (historyServer.getServiceState() != STATE.STARTED) {	throw new IOException("HistoryServer failed to start");	}	super.serviceStart();	} catch (Throwable t) {	throw new YarnRuntimeException(t);	}	getConfig().set(JHAdminConfig.MR_HISTORY_ADDRESS, historyServer.getConfig().get(JHAdminConfig.MR_HISTORY_ADDRESS));	MRWebAppUtil.setJHSWebappURLWithoutScheme(getConfig(), MRWebAppUtil.getJHSWebappURLWithoutScheme(historyServer.getConfig()));	
minimryarn resourcemanager web address 

}, 1500, 60_000);	if (historyServer.getServiceState() != STATE.STARTED) {	throw new IOException("HistoryServer failed to start");	}	super.serviceStart();	} catch (Throwable t) {	throw new YarnRuntimeException(t);	}	getConfig().set(JHAdminConfig.MR_HISTORY_ADDRESS, historyServer.getConfig().get(JHAdminConfig.MR_HISTORY_ADDRESS));	MRWebAppUtil.setJHSWebappURLWithoutScheme(getConfig(), MRWebAppUtil.getJHSWebappURLWithoutScheme(historyServer.getConfig()));	
minimryarn historyserver address 

========================= hadoop sample_5627 =========================

public void test() throws Exception {	Configuration conf = new Configuration();	TestProtoBufRpcServerHandoffServer serverImpl = new TestProtoBufRpcServerHandoffServer();	BlockingService blockingService = TestProtobufRpcHandoffProto.newReflectiveBlockingService(serverImpl);	RPC.setProtocolEngine(conf, TestProtoBufRpcServerHandoffProtocol.class, ProtobufRpcEngine.class);	RPC.Server server = new RPC.Builder(conf) .setProtocol(TestProtoBufRpcServerHandoffProtocol.class) .setInstance(blockingService) .setVerbose(true) .setNumHandlers(1) .build();	server.start();	InetSocketAddress address = server.getListenerAddress();	long serverStartTime = System.currentTimeMillis();	
server started at at time 

========================= hadoop sample_3120 =========================

public DynamicInputChunk acquire(TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException {	String taskId = taskAttemptContext.getTaskAttemptID().getTaskID().toString();	Path acquiredFilePath = new Path(getChunkRootPath(), taskId);	if (fs.exists(acquiredFilePath)) {	
acquiring pre assigned chunk 

public DynamicInputChunk acquire(TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException {	String taskId = taskAttemptContext.getTaskAttemptID().getTaskID().toString();	Path acquiredFilePath = new Path(getChunkRootPath(), taskId);	if (fs.exists(acquiredFilePath)) {	return new DynamicInputChunk(acquiredFilePath, taskAttemptContext, this);	}	for (FileStatus chunkFile : getListOfChunkFiles()) {	if (fs.rename(chunkFile.getPath(), acquiredFilePath)) {	
acquired 

========================= hadoop sample_6288 =========================

public Response toResponse(Exception e) {	if (LOG.isTraceEnabled()) {	
got excepition 

s = Response.Status.FORBIDDEN;	} else if (e instanceof FileNotFoundException) {	s = Response.Status.NOT_FOUND;	} else if (e instanceof IOException) {	s = Response.Status.FORBIDDEN;	} else if (e instanceof UnsupportedOperationException) {	s = Response.Status.BAD_REQUEST;	} else if (e instanceof IllegalArgumentException) {	s = Response.Status.BAD_REQUEST;	} else {	
internal server error 

========================= hadoop sample_7860 =========================

private Server createServer(Class<?> pbProtocol, InetSocketAddress addr, Configuration conf, SecretManager<? extends TokenIdentifier> secretManager, int numHandlers, BlockingService blockingService, String portRangeConfig) throws IOException {	RPC.setProtocolEngine(conf, pbProtocol, ProtobufRpcEngine.class);	RPC.Server server = new RPC.Builder(conf).setProtocol(pbProtocol) .setInstance(blockingService).setBindAddress(addr.getHostName()) .setPort(addr.getPort()).setNumHandlers(numHandlers).setVerbose(false) .setSecretManager(secretManager).setPortRangeConfig(portRangeConfig) .build();	
adding protocol to the server 

========================= hadoop sample_2361 =========================

public Token<ClientToAMTokenIdentifier> selectToken(Text service, Collection<Token<? extends TokenIdentifier>> tokens) {	if (service == null) {	return null;	}	
looking for a token with service 

public Token<ClientToAMTokenIdentifier> selectToken(Text service, Collection<Token<? extends TokenIdentifier>> tokens) {	if (service == null) {	return null;	}	for (Token<? extends TokenIdentifier> token : tokens) {	
token kind is and the token s service name is 

========================= hadoop sample_2224 =========================

public JobHistoryFileParser(FileSystem fs) {	
jobhistoryfileparser created with 

public JobInfo parseHistoryFile(Path path) throws IOException {	
parsing job history file 

public Configuration parseConfiguration(Path path) throws IOException {	
parsing job configuration file 

========================= hadoop sample_5631 =========================

this.applicationType = applicationType;	this.applicationTags = applicationTags;	this.amReqs = amReqs;	if (submissionContext.getPriority() != null) {	this.applicationPriority = Priority .newInstance(submissionContext.getPriority().getPriority());	}	int globalMaxAppAttempts = conf.getInt(YarnConfiguration.RM_AM_MAX_ATTEMPTS, YarnConfiguration.DEFAULT_RM_AM_MAX_ATTEMPTS);	int individualMaxAppAttempts = submissionContext.getMaxAppAttempts();	if (individualMaxAppAttempts <= 0 || individualMaxAppAttempts > globalMaxAppAttempts) {	this.maxAppAttempts = globalMaxAppAttempts;	
the specific max attempts for application is invalid because it is out of the range use the global max attempts instead 

}	int globalMaxAppAttempts = conf.getInt(YarnConfiguration.RM_AM_MAX_ATTEMPTS, YarnConfiguration.DEFAULT_RM_AM_MAX_ATTEMPTS);	int individualMaxAppAttempts = submissionContext.getMaxAppAttempts();	if (individualMaxAppAttempts <= 0 || individualMaxAppAttempts > globalMaxAppAttempts) {	this.maxAppAttempts = globalMaxAppAttempts;	} else {	this.maxAppAttempts = individualMaxAppAttempts;	}	this.attemptFailuresValidityInterval = submissionContext.getAttemptFailuresValidityInterval();	if (this.attemptFailuresValidityInterval > 0) {	
the attemptfailuresvalidityinterval for the application is 

public void handle(RMAppEvent event) {	this.writeLock.lock();	try {	ApplicationId appID = event.getApplicationId();	
processing event for of type 

public void handle(RMAppEvent event) {	this.writeLock.lock();	try {	ApplicationId appID = event.getApplicationId();	final RMAppState oldState = getState();	try {	this.stateMachine.doTransition(event.getType(), event);	} catch (InvalidStateTransitionException e) {	
app can t handle this event at current state 

public void recover(RMState state) {	ApplicationStateData appState = state.getApplicationState().get(getApplicationId());	this.recoveredFinalState = appState.getState();	if (recoveredFinalState == null) {	
NONE 

private void processNodeUpdate(RMAppNodeUpdateType type, RMNode node) {	NodeState nodeState = node.getState();	updatedNodes.add(node);	
received node update event for node with state 

public void transition(RMAppImpl app, RMAppEvent event) {	long applicationLifetime = app.getApplicationLifetime(ApplicationTimeoutType.LIFETIME);	applicationLifetime = app.scheduler .checkAndGetApplicationLifetime(app.queue, applicationLifetime);	if (applicationLifetime > 0) {	Long newTimeout = Long.valueOf(app.submitTime + (applicationLifetime * 1000));	app.rmContext.getRMAppLifetimeMonitor().registerApp(app.applicationId, ApplicationTimeoutType.LIFETIME, newTimeout);	app.applicationTimeouts.put(ApplicationTimeoutType.LIFETIME, newTimeout);	LOG.info("Application " + app.applicationId + " is registered for timeout monitor, type=" + ApplicationTimeoutType.LIFETIME + " value=" + applicationLifetime + " seconds");	}	
storing application with id 

private void rememberTargetTransitionsAndStoreState(RMAppEvent event, Object transitionToDo, RMAppState targetFinalState, RMAppState stateToBeStored) {	rememberTargetTransitions(event, transitionToDo, targetFinalState);	this.stateBeforeFinalSaving = getState();	this.storedFinishTime = this.systemClock.getTime();	
updating application with final state 

public RMAppState transition(RMAppImpl app, RMAppEvent event) {	int numberOfFailure = app.getNumFailedAppAttempts();	if (app.maxAppAttempts == 1) {	
max app attempts is for preventing further attempts 

public RMAppState transition(RMAppImpl app, RMAppEvent event) {	int numberOfFailure = app.getNumFailedAppAttempts();	if (app.maxAppAttempts == 1) {	numberOfFailure = app.maxAppAttempts;	} else {	
the number of failed attempts in previous milliseconds is the max attempts is 

private void removeExcessAttempts(RMAppImpl app) {	while (app.nextAttemptId - app.firstAttemptIdInStateStore > app.maxAppAttempts) {	ApplicationAttemptId attemptId = ApplicationAttemptId.newInstance( app.getApplicationId(), app.firstAttemptIdInStateStore);	RMAppAttempt rmAppAttempt = app.getRMAppAttempt(attemptId);	long endTime = app.systemClock.getTime();	if (rmAppAttempt.getFinishTime() < (endTime - app.attemptFailuresValidityInterval)) {	app.firstAttemptIdInStateStore++;	
remove attempt from state store 

========================= hadoop sample_715 =========================

public StopContainersResponse stopContainers(StopContainersRequest request) throws YarnException {	
container cleaned up by mycontainermanager 

Logger rootLogger = LogManager.getRootLogger();	rootLogger.setLevel(Level.DEBUG);	MyContainerManagerImpl containerManager = new MyContainerManagerImpl();	MockRMWithCustomAMLauncher rm = new MockRMWithCustomAMLauncher( containerManager);	rm.start();	MockNM nm1 = rm.registerNode("127.0.0.1:1234", 5120);	RMApp app = rm.submitApp(2000);	nm1.nodeHeartbeat(true);	int waitCount = 0;	while (containerManager.launched == false && waitCount++ < 20) {	
waiting for am launch to happen 

Assert.assertEquals(app.getRMAppAttempt(appAttemptId) .getMasterContainer().getId() .toString(), containerManager.containerIdAtContainerManager);	Assert.assertEquals(nm1.getNodeId().toString(), containerManager.nmHostAtContainerManager);	Assert.assertEquals(YarnConfiguration.DEFAULT_RM_AM_MAX_ATTEMPTS, containerManager.maxAppAttempts);	MockAM am = new MockAM(rm.getRMContext(), rm .getApplicationMasterService(), appAttemptId);	am.registerAppAttempt();	am.unregisterAppAttempt();	nm1.nodeHeartbeat(attempt.getAppAttemptId(), 1, ContainerState.COMPLETE);	rm.waitForState(am.getApplicationAttemptId(), RMAppAttemptState.FINISHED);	waitCount = 0;	while (containerManager.cleanedup == false && waitCount++ < 20) {	
waiting for am cleanup to happen 

========================= hadoop sample_596 =========================

public void reportTo(DatanodeProtocolClientSideTranslatorPB bpNamenode, DatanodeRegistration bpRegistration) throws BPServiceActorActionException {	try {	bpNamenode.errorReport(bpRegistration, errorCode, errorMessage);	} catch (RemoteException re) {	
trysenderrorreport encountered remoteexception errormessage errorcode 

========================= hadoop sample_7876 =========================

public void run() {	Client.setAsynchronousMode(true);	for (int i = 0; i < count; i++) {	try {	final long param = TestIPC.RANDOM.nextLong();	runCall(i, param);	} catch (Exception e) {	
caller d call d caught s 

private void waitForReturnValues(final int start, final int end) throws InterruptedException, ExecutionException {	for (int i = start; i < end; i++) {	LongWritable value = returnFutures.get(i).get();	if (expectedValues.get(i) != value.get()) {	
caller d call d failed 

========================= hadoop sample_3136 =========================

}	for (FSAppAttempt sched : nonRunnableApps) {	sched.updateDemand();	Resources.addTo(tmpDemand, sched.getDemand());	}	} finally {	readLock.unlock();	}	demand = Resources.componentwiseMin(tmpDemand, getMaxShare());	if (LOG.isDebugEnabled()) {	
the updated demand for is the max is 

}	for (FSAppAttempt sched : nonRunnableApps) {	sched.updateDemand();	Resources.addTo(tmpDemand, sched.getDemand());	}	} finally {	readLock.unlock();	}	demand = Resources.componentwiseMin(tmpDemand, getMaxShare());	if (LOG.isDebugEnabled()) {	
the updated fairshare for is 

public Resource assignContainer(FSSchedulerNode node) {	Resource assigned = none();	if (LOG.isDebugEnabled()) {	
node offered to queue fairshare 

if (!assignContainerPreCheck(node)) {	return assigned;	}	for (FSAppAttempt sched : fetchAppsWithDemand(true)) {	if (SchedulerAppUtils.isPlaceBlacklisted(sched, node, LOG)) {	continue;	}	assigned = sched.assignContainer(node);	if (!assigned.equals(none())) {	if (LOG.isDebugEnabled()) {	
assigned container in queue container 

========================= hadoop sample_980 =========================

}	if (Block.isMetaFilename(file.getName())) {	File metaFile = file;	File blockFile = Block.metaToBlockFile(metaFile);	long blockId = Block.filename2id(blockFile.getName());	File targetDir = DatanodeUtil.idToBlockDir(finalizedDir, blockId);	if (blockFile.exists()) {	try {	fileIoProvider.mkdirsWithExistsCheck(volume, targetDir);	} catch(IOException ioe) {	
failed to mkdirs 

if (blockFile.exists()) {	try {	fileIoProvider.mkdirsWithExistsCheck(volume, targetDir);	} catch(IOException ioe) {	continue;	}	final File targetMetaFile = new File(targetDir, metaFile.getName());	try {	fileIoProvider.rename(volume, metaFile, targetMetaFile);	} catch (IOException e) {	
failed to move meta file from to 

final File targetMetaFile = new File(targetDir, metaFile.getName());	try {	fileIoProvider.rename(volume, metaFile, targetMetaFile);	} catch (IOException e) {	continue;	}	final File targetBlockFile = new File(targetDir, blockFile.getName());	try {	fileIoProvider.rename(volume, blockFile, targetBlockFile);	} catch (IOException e) {	
failed to move block file from to 

}	final File targetBlockFile = new File(targetDir, blockFile.getName());	try {	fileIoProvider.rename(volume, blockFile, targetBlockFile);	} catch (IOException e) {	continue;	}	if (targetBlockFile.exists() && targetMetaFile.exists()) {	++numRecovered;	} else {	
failed to move to 

File restartMeta = new File(file.getParent()  + File.pathSeparator + "." + file.getName() + ".restart");	Scanner sc = null;	try {	sc = new Scanner(restartMeta, "UTF-8");	if (sc.hasNextLong() && (sc.nextLong() > timer.now())) {	newReplica = new ReplicaBeingWritten(blockId, validateIntegrityAndSetLength(file, genStamp), genStamp, volume, file.getParentFile(), null, 0);	loadRwr = false;	}	sc.close();	if (!fileIoProvider.delete(volume, restartMeta)) {	
failed to delete restart meta file 

replicaToKeep = replica1.getGenerationStamp() > replica2.getGenerationStamp() ? replica1 : replica2;	} else if (replica1.getNumBytes() != replica2.getNumBytes()) {	replicaToKeep = replica1.getNumBytes() > replica2.getNumBytes() ? replica1 : replica2;	} else if (replica1.getVolume().isTransientStorage() && !replica2.getVolume().isTransientStorage()) {	replicaToKeep = replica2;	} else {	replicaToKeep = replica1;	}	replicaToDelete = (replicaToKeep == replica1) ? replica2 : replica1;	if (LOG.isDebugEnabled()) {	
resolveduplicatereplicas decide to keep will try to delete 

private void deleteReplica(final ReplicaInfo replicaToDelete) {	final File blockFile = replicaToDelete.getBlockFile();	if (!fileIoProvider.delete(volume, blockFile)) {	
failed to delete block file 

private void deleteReplica(final ReplicaInfo replicaToDelete) {	final File blockFile = replicaToDelete.getBlockFile();	if (!fileIoProvider.delete(volume, blockFile)) {	}	final File metaFile = replicaToDelete.getMetaFile();	if (!fileIoProvider.delete(volume, metaFile)) {	
failed to delete meta file 

}	if (blockFile.length() > validFileLength) {	try (RandomAccessFile blockRAF = fileIoProvider.getRandomAccessFile( volume, blockFile, "rw")) {	blockRAF.setLength(validFileLength);	}	}	return validFileLength;	}	}	} catch (IOException e) {	
getting exception while validating integrity and setting length for blockfile 

private boolean readReplicasFromCache(ReplicaMap volumeMap, final RamDiskReplicaTracker lazyWriteReplicaMap) {	ReplicaMap tmpReplicaMap = new ReplicaMap(new AutoCloseableLock());	File replicaFile = new File(currentDir, REPLICA_CACHE_FILE);	if (!replicaFile.exists()) {	
replica cache file doesn t exist 

private boolean readReplicasFromCache(ReplicaMap volumeMap, final RamDiskReplicaTracker lazyWriteReplicaMap) {	ReplicaMap tmpReplicaMap = new ReplicaMap(new AutoCloseableLock());	File replicaFile = new File(currentDir, REPLICA_CACHE_FILE);	if (!replicaFile.exists()) {	return false;	}	long fileLastModifiedTime = replicaFile.lastModified();	if (System.currentTimeMillis() > fileLastModifiedTime + replicaCacheExpiry) {	
replica cache file has gone stale 

private boolean readReplicasFromCache(ReplicaMap volumeMap, final RamDiskReplicaTracker lazyWriteReplicaMap) {	ReplicaMap tmpReplicaMap = new ReplicaMap(new AutoCloseableLock());	File replicaFile = new File(currentDir, REPLICA_CACHE_FILE);	if (!replicaFile.exists()) {	return false;	}	long fileLastModifiedTime = replicaFile.lastModified();	if (System.currentTimeMillis() > fileLastModifiedTime + replicaCacheExpiry) {	if (!replicaFile.delete()) {	
replica cache file cannot be deleted 

break;	default: break;	}	}	inputStream.close();	for (Iterator<ReplicaInfo> iter = tmpReplicaMap.replicas(bpid).iterator(); iter.hasNext(); ) {	ReplicaInfo info = iter.next();	iter.remove();	volumeMap.add(bpid, info);	}	
successfully read replica from cache file 

}	}	inputStream.close();	for (Iterator<ReplicaInfo> iter = tmpReplicaMap.replicas(bpid).iterator(); iter.hasNext(); ) {	ReplicaInfo info = iter.next();	iter.remove();	volumeMap.add(bpid, info);	}	return true;	} catch (Exception e) {	
exception occured while reading the replicas cache file 

ReplicaInfo info = iter.next();	iter.remove();	volumeMap.add(bpid, info);	}	return true;	} catch (Exception e) {	return false;	}	finally {	if (!fileIoProvider.delete(volume, replicaFile)) {	
failed to delete replica cache file 

if (!fileIoProvider.deleteWithExistsCheck(volume, tmpFile) || !fileIoProvider.deleteWithExistsCheck(volume, replicaCacheFile)) {	return;	}	FileOutputStream out = null;	try {	out = fileIoProvider.getFileOutputStream(volume, tmpFile);	blocksListToPersist.writeTo(out);	out.close();	fileIoProvider.moveFile(volume, tmpFile, replicaCacheFile);	} catch (Exception e) {	
failed to write replicas to cache 

========================= hadoop sample_7933 =========================

injectToken();	ugi.doAs(new PrivilegedExceptionAction<Void>() {	public Void run() throws Exception {	try {	handle(ctx, req);	} finally {	String host = null;	try {	host = ((InetSocketAddress)ctx.channel().remoteAddress()). getAddress().getHostAddress();	} catch (Exception e) {	
error retrieving hostname 

public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) {	
error 

========================= hadoop sample_7899 =========================

static String getUsernameFromConf(Configuration conf) {	String oldStyleUgi = conf.get(DEPRECATED_UGI_KEY);	if (oldStyleUgi != null) {	
should not be used instead use 

========================= hadoop sample_3970 =========================

cluster.waitActive();	cluster.transitionToActive(0);	NameNode nn1 = cluster.getNameNode(0);	NameNode nn2 = cluster.getNameNode(1);	FileSystem fs = HATestUtil.configureFailoverFs(cluster, conf);	Thread.sleep(1000);	System.err.println("==================================");	DFSTestUtil.writeFile(fs, TEST_FILE_PATH, TEST_FILE_DATA);	nn1.getRpcServer().rollEditLog();	System.err.println("==================================");	
waiting for block locations to appear on standby node 

NameNode nn2 = cluster.getNameNode(1);	FileSystem fs = HATestUtil.configureFailoverFs(cluster, conf);	Thread.sleep(1000);	System.err.println("==================================");	DFSTestUtil.writeFile(fs, TEST_FILE_PATH, TEST_FILE_DATA);	nn1.getRpcServer().rollEditLog();	System.err.println("==================================");	waitForBlockLocations(cluster, nn2, TEST_FILE, 3);	cluster.triggerHeartbeats();	cluster.triggerBlockReports();	
changing replication to 

DFSTestUtil.writeFile(fs, TEST_FILE_PATH, TEST_FILE_DATA);	nn1.getRpcServer().rollEditLog();	System.err.println("==================================");	waitForBlockLocations(cluster, nn2, TEST_FILE, 3);	cluster.triggerHeartbeats();	cluster.triggerBlockReports();	fs.setReplication(TEST_FILE_PATH, (short)1);	BlockManagerTestUtil.computeAllPendingWork( nn1.getNamesystem().getBlockManager());	waitForBlockLocations(cluster, nn1, TEST_FILE, 1);	nn1.getRpcServer().rollEditLog();	
waiting for lowered replication to show up on standby 

nn1.getRpcServer().rollEditLog();	System.err.println("==================================");	waitForBlockLocations(cluster, nn2, TEST_FILE, 3);	cluster.triggerHeartbeats();	cluster.triggerBlockReports();	fs.setReplication(TEST_FILE_PATH, (short)1);	BlockManagerTestUtil.computeAllPendingWork( nn1.getNamesystem().getBlockManager());	waitForBlockLocations(cluster, nn1, TEST_FILE, 1);	nn1.getRpcServer().rollEditLog();	waitForBlockLocations(cluster, nn2, TEST_FILE, 1);	
changing replication to 

cluster.triggerHeartbeats();	cluster.triggerBlockReports();	fs.setReplication(TEST_FILE_PATH, (short)1);	BlockManagerTestUtil.computeAllPendingWork( nn1.getNamesystem().getBlockManager());	waitForBlockLocations(cluster, nn1, TEST_FILE, 1);	nn1.getRpcServer().rollEditLog();	waitForBlockLocations(cluster, nn2, TEST_FILE, 1);	fs.setReplication(TEST_FILE_PATH, (short)3);	BlockManagerTestUtil.computeAllPendingWork( nn1.getNamesystem().getBlockManager());	nn1.getRpcServer().rollEditLog();	
waiting for higher replication to show up on standby 

static void waitForBlockLocations(final MiniDFSCluster cluster, final NameNode nn, final String path, final int expectedReplicas) throws Exception {	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	try {	LocatedBlocks locs = NameNodeAdapter.getBlockLocations(nn, path, 0, 1000);	DatanodeInfo[] dnis = locs.getLastLocatedBlock().getLocations();	for (DatanodeInfo dni : dnis) {	Assert.assertNotNull(dni);	}	int numReplicas = dnis.length;	
got locs 

for (DatanodeInfo dni : dnis) {	Assert.assertNotNull(dni);	}	int numReplicas = dnis.length;	if (numReplicas > expectedReplicas) {	cluster.triggerDeletionReports();	}	cluster.triggerHeartbeats();	return numReplicas == expectedReplicas;	} catch (IOException e) {	
no block locations yet 

========================= hadoop sample_7460 =========================

public void reduce(Text key, Iterator<Text> values, Context context) throws IOException,InterruptedException {	
the key 

public void reduce(Text key, Iterator<Text> values, Context context) throws IOException,InterruptedException {	if(values.hasNext()) {	Text val = values.next();	
the value 

========================= hadoop sample_5689 =========================

private void initialize(Configuration conf) throws IOException {	shouldRun = true;	checkpointConf = new CheckpointConf(conf);	String fullInfoAddr = conf.get(DFS_NAMENODE_BACKUP_HTTP_ADDRESS_KEY, DFS_NAMENODE_BACKUP_HTTP_ADDRESS_DEFAULT);	infoBindAddress = fullInfoAddr.substring(0, fullInfoAddr.indexOf(":"));	
checkpoint period secs min 

private void initialize(Configuration conf) throws IOException {	shouldRun = true;	checkpointConf = new CheckpointConf(conf);	String fullInfoAddr = conf.get(DFS_NAMENODE_BACKUP_HTTP_ADDRESS_KEY, DFS_NAMENODE_BACKUP_HTTP_ADDRESS_DEFAULT);	infoBindAddress = fullInfoAddr.substring(0, fullInfoAddr.indexOf(":"));	
transactions count is to trigger checkpoint 

shouldCheckpoint = true;	} else {	long txns = countUncheckpointedTxns();	if(txns >= checkpointConf.getTxnCount()) shouldCheckpoint = true;	}	if(shouldCheckpoint) {	doCheckpoint();	lastCheckpointTime = now;	}	} catch(IOException e) {	
exception in docheckpoint 

} else {	long txns = countUncheckpointedTxns();	if(txns >= checkpointConf.getTxnCount()) shouldCheckpoint = true;	}	if(shouldCheckpoint) {	doCheckpoint();	lastCheckpointTime = now;	}	} catch(IOException e) {	} catch(Throwable e) {	
throwable exception in docheckpoint 

========================= hadoop sample_8111 =========================

try (OutputStream out = new FileOutputStream(file)) {	IOUtils.copyBytes(in, out, BUFFER_SIZE);	}	if (!file.setLastModified(entry.getTime())) {	numOfFailedLastModifiedSet++;	}	}	}	}	if (numOfFailedLastModifiedSet > 0) {	
could not set last modfied time for file s 

========================= hadoop sample_3618 =========================

public void setup() throws IOException {	localFS.delete(new Path(localDir.getAbsolutePath()), true);	localFS.delete(new Path(tmpDir.getAbsolutePath()), true);	localFS.delete(new Path(localLogDir.getAbsolutePath()), true);	localFS.delete(new Path(remoteLogDir.getAbsolutePath()), true);	localDir.mkdir();	tmpDir.mkdir();	localLogDir.mkdir();	remoteLogDir.mkdir();	
created localdir in 

public void setup() throws IOException {	localFS.delete(new Path(localDir.getAbsolutePath()), true);	localFS.delete(new Path(tmpDir.getAbsolutePath()), true);	localFS.delete(new Path(localLogDir.getAbsolutePath()), true);	localFS.delete(new Path(remoteLogDir.getAbsolutePath()), true);	localDir.mkdir();	tmpDir.mkdir();	localLogDir.mkdir();	remoteLogDir.mkdir();	
created tmpdir in 

========================= hadoop sample_1625 =========================

scmOut = null;	} else {	scm = SCM.GIT;	}	}	}	}	if (scmOut != null) {	getLog().debug(scmOut.toString());	}	
scm 

private byte[] computeMD5(List<File> files) throws IOException, NoSuchAlgorithmException {	MessageDigest md5 = MessageDigest.getInstance("MD5");	for (File file : files) {	
computing for 

private String computeMD5() throws Exception {	List<File> files = FileSetUtils.convertFileSetToFiles(source);	Collections.sort(files, new MD5Comparator());	byte[] md5 = computeMD5(files);	String md5str = byteArrayToString(md5);	
computed 

========================= hadoop sample_4427 =========================

try {	curatorTestingServer = new TestingServer();	curatorTestingServer.start();	String connectString = curatorTestingServer.getConnectString();	curatorFramework = CuratorFrameworkFactory.builder() .connectString(connectString) .retryPolicy(new RetryNTimes(100, 100)) .build();	curatorFramework.start();	Configuration conf = new YarnConfiguration();	conf.set(CommonConfigurationKeys.ZK_ADDRESS, connectString);	setConf(conf);	} catch (Exception e) {	
cannot initialize zookeeper store 

========================= hadoop sample_1275 =========================

public synchronized void unregister() {	try {	ShutdownHookManager.get().removeShutdownHook(this);	} catch (IllegalStateException e) {	
failed to unregister shutdown hook 

boolean result = false;	synchronized (this) {	service = serviceRef.get();	serviceRef.clear();	}	if (service != null) {	try {	service.stop();	result = true;	} catch (Throwable t) {	
error stopping 

========================= hadoop sample_4077 =========================

public void run() {	try {	Thread.sleep(delayTime);	if (failInRun) {	noteFailure(new Exception(FAILURE_MESSAGE));	}	} catch (InterruptedException e) {	interrupted = true;	
Interrupted 

========================= hadoop sample_3161 =========================

public void setUp() throws Exception {	ExitUtil.disableSystemExit();	ExitUtil.resetFirstExitException();	config = new HdfsConfiguration();	hdfsDir = new File(MiniDFSCluster.getBaseDirectory());	if ( hdfsDir.exists() && !FileUtil.fullyDelete(hdfsDir) ) {	throw new IOException("Could not delete hdfs directory '" + hdfsDir + "'");	}	
hdfsdir is 

public void createCheckPoint(int count) throws IOException {	
starting mini cluster 

public void createCheckPoint(int count) throws IOException {	MiniDFSCluster cluster = null;	SecondaryNameNode sn = null;	try {	cluster = new MiniDFSCluster.Builder(config) .manageDataDfsDirs(false) .manageNameDfsDirs(false).build();	cluster.waitActive();	
starting secondary node 

SecondaryNameNode sn = null;	try {	cluster = new MiniDFSCluster.Builder(config) .manageDataDfsDirs(false) .manageNameDfsDirs(false).build();	cluster.waitActive();	sn = new SecondaryNameNode(config);	assertNotNull(sn);	for (int i=0; i<count; i++) {	FileSystem fileSys = cluster.getFileSystem();	Path p = new Path("t" + i);	this.writeFile(fileSys, p, 1);	
file created 

SecondaryNameNode sn = null;	try {	cluster = new MiniDFSCluster.Builder(config) .manageDataDfsDirs(false) .manageNameDfsDirs(false).build();	cluster.waitActive();	sn = new SecondaryNameNode(config);	assertNotNull(sn);	for (int i=0; i<count; i++) {	FileSystem fileSys = cluster.getFileSystem();	Path p = new Path("t" + i);	this.writeFile(fileSys, p, 1);	
doing checkpoint 

try {	cluster = new MiniDFSCluster.Builder(config) .manageDataDfsDirs(false) .manageNameDfsDirs(false).build();	cluster.waitActive();	sn = new SecondaryNameNode(config);	assertNotNull(sn);	for (int i=0; i<count; i++) {	FileSystem fileSys = cluster.getFileSystem();	Path p = new Path("t" + i);	this.writeFile(fileSys, p, 1);	sn.doCheckpoint();	
done checkpoint 

this.writeFile(fileSys, p, 1);	sn.doCheckpoint();	}	} catch (IOException e) {	fail(StringUtils.stringifyException(e));	System.err.println("checkpoint failed");	throw e;	}  finally {	if(sn!=null) sn.shutdown();	if(cluster!=null) cluster.shutdown();	
cluster shutdown 

private void checkNameNodeFiles() throws IOException{	
about to start dfs cluster 

private void checkNameNodeFiles() throws IOException{	MiniDFSCluster cluster = null;	try {	cluster = new MiniDFSCluster.Builder(config) .format(false) .manageDataDfsDirs(false) .manageNameDfsDirs(false) .startupOption(IMPORT).build();	cluster.waitActive();	
nn started with checkpoint option 

public void testChkpointStartup2() throws IOException{	
starting same directory for checkpoint 

public void testChkpointStartup1() throws IOException{	
starting teststartup recovery 

public void testSNNStartup() throws IOException{	
starting secondnn startup test 

public void testSNNStartup() throws IOException{	config.set(DFSConfigKeys.DFS_NAMENODE_NAME_DIR_KEY, fileAsURI(new File(hdfsDir, "name")).toString());	config.set(DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_KEY, fileAsURI(new File(hdfsDir, "name")).toString());	config.set(DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_EDITS_DIR_KEY, fileAsURI(new File(hdfsDir, "chkpt_edits")).toString());	config.set(DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_DIR_KEY, fileAsURI(new File(hdfsDir, "chkpt")).toString());	
starting nn 

config.set(DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_EDITS_DIR_KEY, fileAsURI(new File(hdfsDir, "chkpt_edits")).toString());	config.set(DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_DIR_KEY, fileAsURI(new File(hdfsDir, "chkpt")).toString());	MiniDFSCluster cluster = null;	SecondaryNameNode sn = null;	NameNode nn = null;	try {	cluster = new MiniDFSCluster.Builder(config).manageDataDfsDirs(false) .manageNameDfsDirs(false) .build();	cluster.waitActive();	nn = cluster.getNameNode();	assertNotNull(nn);	
starting secondnn 

MiniDFSCluster cluster = null;	SecondaryNameNode sn = null;	NameNode nn = null;	try {	cluster = new MiniDFSCluster.Builder(config).manageDataDfsDirs(false) .manageNameDfsDirs(false) .build();	cluster.waitActive();	nn = cluster.getNameNode();	assertNotNull(nn);	sn = new SecondaryNameNode(config);	assertNotNull(sn);	
doing checkpoint 

SecondaryNameNode sn = null;	NameNode nn = null;	try {	cluster = new MiniDFSCluster.Builder(config).manageDataDfsDirs(false) .manageNameDfsDirs(false) .build();	cluster.waitActive();	nn = cluster.getNameNode();	assertNotNull(nn);	sn = new SecondaryNameNode(config);	assertNotNull(sn);	sn.doCheckpoint();	
done checkpoint 

public void testCompression() throws IOException {	
test compressing image 

public void testCompression() throws IOException {	Configuration conf = new Configuration();	FileSystem.setDefaultUri(conf, "hdfs: conf.set(DFSConfigKeys.DFS_NAMENODE_HTTP_ADDRESS_KEY, "127.0.0.1:0");	File base_dir = new File(PathUtils.getTestDir(getClass()), "dfs/");	conf.set(DFSConfigKeys.DFS_NAMENODE_NAME_DIR_KEY, new File(base_dir, "name").getPath());	conf.setBoolean(DFSConfigKeys.DFS_PERMISSIONS_ENABLED_KEY, false);	DFSTestUtil.formatNameNode(conf);	
create an uncompressed fsimage 

DFSTestUtil.formatNameNode(conf);	NameNode namenode = new NameNode(conf);	namenode.getNamesystem().mkdirs("/test", new PermissionStatus("hairong", null, FsPermission.getDefault()), true);	NamenodeProtocols nnRpc = namenode.getRpcServer();	assertTrue(nnRpc.getFileInfo("/test").isDir());	nnRpc.setSafeMode(SafeModeAction.SAFEMODE_ENTER, false);	nnRpc.saveNamespace();	namenode.stop();	namenode.join();	namenode.joinHttpServer();	
read an uncomressed image and store it compressed using default codec 

namenode.getNamesystem().mkdirs("/test", new PermissionStatus("hairong", null, FsPermission.getDefault()), true);	NamenodeProtocols nnRpc = namenode.getRpcServer();	assertTrue(nnRpc.getFileInfo("/test").isDir());	nnRpc.setSafeMode(SafeModeAction.SAFEMODE_ENTER, false);	nnRpc.saveNamespace();	namenode.stop();	namenode.join();	namenode.joinHttpServer();	conf.setBoolean(DFSConfigKeys.DFS_IMAGE_COMPRESS_KEY, true);	checkNameSpace(conf);	
read a compressed image and store it using a different codec 

assertTrue(nnRpc.getFileInfo("/test").isDir());	nnRpc.setSafeMode(SafeModeAction.SAFEMODE_ENTER, false);	nnRpc.saveNamespace();	namenode.stop();	namenode.join();	namenode.joinHttpServer();	conf.setBoolean(DFSConfigKeys.DFS_IMAGE_COMPRESS_KEY, true);	checkNameSpace(conf);	conf.set(DFSConfigKeys.DFS_IMAGE_COMPRESSION_CODEC_KEY, "org.apache.hadoop.io.compress.GzipCodec");	checkNameSpace(conf);	
read a compressed image and store it as uncompressed 

nnRpc.saveNamespace();	namenode.stop();	namenode.join();	namenode.joinHttpServer();	conf.setBoolean(DFSConfigKeys.DFS_IMAGE_COMPRESS_KEY, true);	checkNameSpace(conf);	conf.set(DFSConfigKeys.DFS_IMAGE_COMPRESSION_CODEC_KEY, "org.apache.hadoop.io.compress.GzipCodec");	checkNameSpace(conf);	conf.setBoolean(DFSConfigKeys.DFS_IMAGE_COMPRESS_KEY, false);	checkNameSpace(conf);	
read an uncompressed image and store it as uncompressed 

public void testImageChecksum() throws Exception {	
test uncompressed image checksum 

public void testImageChecksum() throws Exception {	testImageChecksum(false);	
test compressed image checksum 

MiniDFSCluster cluster = null;	if (compress) {	config.setBoolean(DFSConfigKeys.DFS_IMAGE_COMPRESSION_CODEC_KEY, true);	}	try {	LOG.info("\n===========================================\n" + "Starting empty cluster");	cluster = new MiniDFSCluster.Builder(config) .numDataNodes(0) .format(true) .build();	cluster.waitActive();	FileSystem fs = cluster.getFileSystem();	fs.mkdirs(new Path("/test"));	
shutting down cluster 

========================= hadoop sample_7437 =========================

checkpointDirs = FSImage.getCheckpointDirs(conf, "/tmp/hadoop/dfs/namesecondary");	checkpointEditsDirs = FSImage.getCheckpointEditsDirs(conf, "/tmp/hadoop/dfs/namesecondary");	checkpointImage = new CheckpointStorage(conf, checkpointDirs, checkpointEditsDirs);	checkpointImage.recoverCreate(commandLineOpts.shouldFormat());	checkpointImage.deleteTempEdits();	namesystem = new FSNamesystem(conf, checkpointImage, true);	namesystem.dir.disableQuotaChecks();	checkpointConf = new CheckpointConf(conf);	nameNodeStatusBeanName = MBeans.register("SecondaryNameNode", "SecondaryNameNodeInfo", this);	legacyOivImageDir = conf.get( DFSConfigKeys.DFS_NAMENODE_LEGACY_OIV_IMAGE_DIR_KEY);	
checkpoint period secs min 

checkpointDirs = FSImage.getCheckpointDirs(conf, "/tmp/hadoop/dfs/namesecondary");	checkpointEditsDirs = FSImage.getCheckpointEditsDirs(conf, "/tmp/hadoop/dfs/namesecondary");	checkpointImage = new CheckpointStorage(conf, checkpointDirs, checkpointEditsDirs);	checkpointImage.recoverCreate(commandLineOpts.shouldFormat());	checkpointImage.deleteTempEdits();	namesystem = new FSNamesystem(conf, checkpointImage, true);	namesystem.dir.disableQuotaChecks();	checkpointConf = new CheckpointConf(conf);	nameNodeStatusBeanName = MBeans.register("SecondaryNameNode", "SecondaryNameNodeInfo", this);	legacyOivImageDir = conf.get( DFSConfigKeys.DFS_NAMENODE_LEGACY_OIV_IMAGE_DIR_KEY);	
log size trigger txns 

private void join() {	try {	infoServer.join();	} catch (InterruptedException ie) {	
exception 

public void shutdown() {	shouldRun = false;	if (checkpointThread != null) {	checkpointThread.interrupt();	try {	checkpointThread.join(10000);	} catch (InterruptedException e) {	
interrupted waiting to join on checkpointer thread 

} catch (InterruptedException e) {	Thread.currentThread().interrupt();	}	}	try {	if (infoServer != null) {	infoServer.stop();	infoServer = null;	}	} catch (Exception e) {	
exception shutting down secondarynamenode 

if (nameNodeStatusBeanName != null) {	MBeans.unregister(nameNodeStatusBeanName);	nameNodeStatusBeanName = null;	}	try {	if (checkpointImage != null) {	checkpointImage.close();	checkpointImage = null;	}	} catch(IOException e) {	
exception while closing checkpointstorage 

try {	if(UserGroupInformation.isSecurityEnabled()) UserGroupInformation.getCurrentUser().checkTGTAndReloginFromKeytab();	final long monotonicNow = Time.monotonicNow();	final long now = Time.now();	if (shouldCheckpointBasedOnCount() || monotonicNow >= lastCheckpointTime + 1000 * checkpointConf.getPeriod()) {	doCheckpoint();	lastCheckpointTime = monotonicNow;	lastCheckpointWallclockTime = now;	}	} catch (IOException e) {	
exception in docheckpoint 

final long monotonicNow = Time.monotonicNow();	final long now = Time.now();	if (shouldCheckpointBasedOnCount() || monotonicNow >= lastCheckpointTime + 1000 * checkpointConf.getPeriod()) {	doCheckpoint();	lastCheckpointTime = monotonicNow;	lastCheckpointWallclockTime = now;	}	} catch (IOException e) {	e.printStackTrace();	if (checkpointImage.getMergeErrorCount() > maxRetries) {	
merging failed times 

doCheckpoint();	lastCheckpointTime = monotonicNow;	lastCheckpointWallclockTime = now;	}	} catch (IOException e) {	e.printStackTrace();	if (checkpointImage.getMergeErrorCount() > maxRetries) {	terminate(1);	}	} catch (Throwable e) {	
throwable exception in docheckpoint 

}	long expectedTxId = sig.mostRecentCheckpointTxId + 1;	if (manifest.getLogs().get(0).getStartTxId() != expectedTxId) {	throw new IOException("Bad edit log manifest (expected txid = " + expectedTxId + ": " + manifest);	}	try {	Boolean b = UserGroupInformation.getCurrentUser().doAs( new PrivilegedExceptionAction<Boolean>() {	public Boolean run() throws Exception {	dstImage.getStorage().cTime = sig.cTime;	if (sig.mostRecentCheckpointTxId == dstImage.getStorage().getMostRecentCheckpointTxId()) {	
image has not changed will not download image 

long expectedTxId = sig.mostRecentCheckpointTxId + 1;	if (manifest.getLogs().get(0).getStartTxId() != expectedTxId) {	throw new IOException("Bad edit log manifest (expected txid = " + expectedTxId + ": " + manifest);	}	try {	Boolean b = UserGroupInformation.getCurrentUser().doAs( new PrivilegedExceptionAction<Boolean>() {	public Boolean run() throws Exception {	dstImage.getStorage().cTime = sig.cTime;	if (sig.mostRecentCheckpointTxId == dstImage.getStorage().getMostRecentCheckpointTxId()) {	} else {	
image has changed downloading updated image from nn 

private URL getInfoServer() throws IOException {	URI fsName = FileSystem.getDefaultUri(conf);	if (!HdfsConstants.HDFS_URI_SCHEME.equalsIgnoreCase(fsName.getScheme())) {	throw new IOException("This is not a DFS");	}	final String scheme = DFSUtil.getHttpClientScheme(conf);	URI address = DFSUtil.getInfoServerWithDefaultHost(fsName.getHost(), conf, scheme);	
will connect to namenode at 

HttpServer2.Builder builder = DFSUtil.httpServerTemplateForNNAndJN(conf, httpAddr, httpsAddr, "secondary", DFSConfigKeys. DFS_SECONDARY_NAMENODE_KERBEROS_INTERNAL_SPNEGO_PRINCIPAL_KEY, DFSConfigKeys.DFS_SECONDARY_NAMENODE_KEYTAB_FILE_KEY);	final boolean xFrameEnabled = conf.getBoolean( DFSConfigKeys.DFS_XFRAME_OPTION_ENABLED, DFSConfigKeys.DFS_XFRAME_OPTION_ENABLED_DEFAULT);	final String xFrameOptionValue = conf.getTrimmed( DFSConfigKeys.DFS_XFRAME_OPTION_VALUE, DFSConfigKeys.DFS_XFRAME_OPTION_VALUE_DEFAULT);	builder.configureXFrame(xFrameEnabled).setXFrameOption(xFrameOptionValue);	infoServer = builder.build();	infoServer.setAttribute("secondary.name.node", this);	infoServer.setAttribute("name.system.image", checkpointImage);	infoServer.setAttribute(JspHelper.CURRENT_CONF, conf);	infoServer.addInternalServlet("imagetransfer", ImageServlet.PATH_SPEC, ImageServlet.class, true);	infoServer.start();	
web server init done 

try {	doMerge(sig, manifest, loadImage, checkpointImage, namesystem);	} catch (IOException ioe) {	checkpointImage.setMergeError();	throw ioe;	}	checkpointImage.clearMergeError();	long txid = checkpointImage.getLastAppliedTxId();	TransferFsImage.uploadImageFromStorage(fsName, conf, dstStorage, NameNodeFile.IMAGE, txid);	CheckpointFaultInjector.getInstance().afterSecondaryUploadsNewImage();	
checkpoint done new image size 

throw ioe;	}	checkpointImage.clearMergeError();	long txid = checkpointImage.getLastAppliedTxId();	TransferFsImage.uploadImageFromStorage(fsName, conf, dstStorage, NameNodeFile.IMAGE, txid);	CheckpointFaultInjector.getInstance().afterSecondaryUploadsNewImage();	if (legacyOivImageDir != null && !legacyOivImageDir.isEmpty()) {	try {	checkpointImage.saveLegacyOIVImage(namesystem, legacyOivImageDir, new Canceler());	} catch (IOException e) {	
failed to write legacy oiv image 

public static void main(String[] argv) throws Exception {	CommandLineOpts opts = SecondaryNameNode.parseArgs(argv);	if (opts == null) {	
failed to parse options 

secondary = new SecondaryNameNode(tconf, opts);	if (opts != null && opts.getCommand() != null) {	int ret = secondary.processStartupCommand(opts);	terminate(ret);	} else {	secondary.startInfoServer();	secondary.startCheckpointThread();	secondary.join();	}	} catch (Throwable e) {	
failed to start secondary namenode 

========================= hadoop sample_7964 =========================

protected synchronized void allocateContainer(RMContainer rmContainer, boolean launchedOnNode) {	Container container = rmContainer.getContainer();	if (rmContainer.getExecutionType() == ExecutionType.GUARANTEED) {	deductUnallocatedResource(container.getResource());	++numContainers;	}	launchedContainers.put(container.getId(), new ContainerInfo(rmContainer, launchedOnNode));	if (LOG.isDebugEnabled()) {	
assigned container of capacity on host which has containers used and available after allocation 

private synchronized void addUnallocatedResource(Resource resource) {	if (resource == null) {	
invalid resource addition of null resource for 

public synchronized void deductUnallocatedResource(Resource resource) {	if (resource == null) {	
invalid deduction of null resource for 

========================= hadoop sample_939 =========================

public static AzureBlobStorageTestAccount createForEmulator() throws Exception {	saveMetricsConfigFile();	NativeAzureFileSystem fs = null;	CloudBlobContainer container = null;	Configuration conf = createTestConfiguration();	if (!conf.getBoolean(USE_EMULATOR_PROPERTY_NAME, false)) {	
skipping emulator azure test because configuration doesn t indicate that it s running 

static CloudStorageAccount createStorageAccount(String accountName, Configuration conf, boolean allowAnonymous) throws URISyntaxException, KeyProviderException {	String accountKey = AzureNativeFileSystemStore .getAccountKeyFromConfiguration(accountName, conf);	final StorageCredentials credentials;	if (accountKey == null) {	if (allowAnonymous) {	credentials = StorageCredentialsAnonymous.ANONYMOUS;	} else {	
skipping live azure test because of missing key for account 

static CloudStorageAccount createTestAccount(Configuration conf) throws URISyntaxException, KeyProviderException {	String testAccountName = conf.get(TEST_ACCOUNT_NAME_PROPERTY_NAME);	if (testAccountName == null) {	
skipping live azure test because of missing test account 

========================= hadoop sample_6362 =========================

public void waitFor(Supplier<Boolean> check, int checkEveryMillis, int logInterval) throws InterruptedException {	Preconditions.checkNotNull(check, "check should not be null");	Preconditions.checkArgument(checkEveryMillis >= 0, "checkEveryMillis should be positive value");	Preconditions.checkArgument(logInterval >= 0, "logInterval should be positive value");	int loggingCounter = logInterval;	do {	if (LOG.isDebugEnabled()) {	
check the condition for main loop 

public void waitFor(Supplier<Boolean> check, int checkEveryMillis, int logInterval) throws InterruptedException {	Preconditions.checkNotNull(check, "check should not be null");	Preconditions.checkArgument(checkEveryMillis >= 0, "checkEveryMillis should be positive value");	Preconditions.checkArgument(logInterval >= 0, "logInterval should be positive value");	int loggingCounter = logInterval;	do {	if (LOG.isDebugEnabled()) {	}	boolean result = check.get();	if (result) {	
exits the main loop 

Preconditions.checkArgument(logInterval >= 0, "logInterval should be positive value");	int loggingCounter = logInterval;	do {	if (LOG.isDebugEnabled()) {	}	boolean result = check.get();	if (result) {	return;	}	if (--loggingCounter <= 0) {	
waiting in main loop 

========================= hadoop sample_2634 =========================

protected void serviceStop() throws Exception {	ExecutorCompletionService<KillApplicationResponse> completionService = new ExecutorCompletionService<>(this.threadpool);	if (this.unmanagedAppMasterMap.isEmpty()) {	return;	}	Set<String> addressList = new HashSet<>(this.unmanagedAppMasterMap.keySet());	
abnormal shutdown of uampoolmanager still uams in map 

protected void serviceStop() throws Exception {	ExecutorCompletionService<KillApplicationResponse> completionService = new ExecutorCompletionService<>(this.threadpool);	if (this.unmanagedAppMasterMap.isEmpty()) {	return;	}	Set<String> addressList = new HashSet<>(this.unmanagedAppMasterMap.keySet());	for (final String uamId : addressList) {	completionService.submit(new Callable<KillApplicationResponse>() {	public KillApplicationResponse call() throws Exception {	try {	
force killing uam id for application 

if (this.unmanagedAppMasterMap.isEmpty()) {	return;	}	Set<String> addressList = new HashSet<>(this.unmanagedAppMasterMap.keySet());	for (final String uamId : addressList) {	completionService.submit(new Callable<KillApplicationResponse>() {	public KillApplicationResponse call() throws Exception {	try {	return unmanagedAppMasterMap.remove(uamId).forceKillApplication();	} catch (Exception e) {	
failed to kill unmanaged application master 

return null;	}	}	});	}	for (int i = 0; i < addressList.size(); ++i) {	try {	Future<KillApplicationResponse> future = completionService.take();	future.get();	} catch (Exception e) {	
failed to kill unmanaged application master 

ApplicationId appId = null;	ApplicationClientProtocol rmClient;	try {	UserGroupInformation appSubmitter = UserGroupInformation.createRemoteUser(submitter);	rmClient = AMRMClientUtils.createRMProxy(conf, ApplicationClientProtocol.class, appSubmitter, null);	GetNewApplicationResponse response = rmClient.getNewApplication(GetNewApplicationRequest.newInstance());	if (response == null) {	throw new YarnException("getNewApplication got null response");	}	appId = response.getApplicationId();	
received new application id from rm 

public RegisterApplicationMasterResponse createAndRegisterNewUAM(String uamId, RegisterApplicationMasterRequest registerRequest, Configuration conf, ApplicationId appId, String queueName, String submitter, String appNameSuffix) throws YarnException, IOException {	if (this.unmanagedAppMasterMap.containsKey(uamId)) {	throw new YarnException("UAM " + uamId + " already exists");	}	UnmanagedApplicationManager uam = createUAM(conf, appId, queueName, submitter, appNameSuffix);	this.unmanagedAppMasterMap.put(uamId, uam);	RegisterApplicationMasterResponse response = null;	try {	
creating and registering uam id for application 

public FinishApplicationMasterResponse finishApplicationMaster(String uamId, FinishApplicationMasterRequest request) throws YarnException, IOException {	if (!this.unmanagedAppMasterMap.containsKey(uamId)) {	throw new YarnException("UAM " + uamId + " does not exist");	}	
finishing application for uam id 

public FinishApplicationMasterResponse finishApplicationMaster(String uamId, FinishApplicationMasterRequest request) throws YarnException, IOException {	if (!this.unmanagedAppMasterMap.containsKey(uamId)) {	throw new YarnException("UAM " + uamId + " does not exist");	}	FinishApplicationMasterResponse response = this.unmanagedAppMasterMap.get(uamId).finishApplicationMaster(request);	if (response.getIsUnregistered()) {	this.unmanagedAppMasterMap.remove(uamId);	this.attemptIdMap.remove(uamId);	
uam id is unregistered 

========================= hadoop sample_1331 =========================

public void init(Configuration conf) {	try {	super.init(conf);	} catch (YarnException e1) {	
error failed to init hsqldb 

public void init(Configuration conf) {	try {	super.init(conf);	} catch (YarnException e1) {	}	try {	conn = getConnection();	
database init start 

conn.prepareStatement(SP_GETSUBCLUSTER).execute();	conn.prepareStatement(SP_GETSUBCLUSTERS).execute();	conn.prepareStatement(SP_ADDAPPLICATIONHOMESUBCLUSTER).execute();	conn.prepareStatement(SP_UPDATEAPPLICATIONHOMESUBCLUSTER).execute();	conn.prepareStatement(SP_GETAPPLICATIONHOMESUBCLUSTER).execute();	conn.prepareStatement(SP_GETAPPLICATIONSHOMESUBCLUSTER).execute();	conn.prepareStatement(SP_DELETEAPPLICATIONHOMESUBCLUSTER).execute();	conn.prepareStatement(SP_SETPOLICYCONFIGURATION).execute();	conn.prepareStatement(SP_GETPOLICYCONFIGURATION).execute();	conn.prepareStatement(SP_GETPOLICIESCONFIGURATIONS).execute();	
database init complete 

conn.prepareStatement(SP_ADDAPPLICATIONHOMESUBCLUSTER).execute();	conn.prepareStatement(SP_UPDATEAPPLICATIONHOMESUBCLUSTER).execute();	conn.prepareStatement(SP_GETAPPLICATIONHOMESUBCLUSTER).execute();	conn.prepareStatement(SP_GETAPPLICATIONSHOMESUBCLUSTER).execute();	conn.prepareStatement(SP_DELETEAPPLICATIONHOMESUBCLUSTER).execute();	conn.prepareStatement(SP_SETPOLICYCONFIGURATION).execute();	conn.prepareStatement(SP_GETPOLICYCONFIGURATION).execute();	conn.prepareStatement(SP_GETPOLICIESCONFIGURATIONS).execute();	conn.close();	} catch (SQLException e) {	
error failed to inizialize hsqldb 

public void closeConnection() {	try {	conn.close();	} catch (SQLException e) {	
error failed to close connection to hsqldb db 

========================= hadoop sample_1274 =========================

public void uncaughtException(Thread thread, Throwable exception) {	if (ShutdownHookManager.get().isShutdownInProgress()) {	
thread threw an error during shutdown 

public void uncaughtException(Thread thread, Throwable exception) {	if (ShutdownHookManager.get().isShutdownInProgress()) {	} else if (exception instanceof Error) {	try {	
thread threw an error shutting down 

try {	System.err.println("Halting due to Out Of Memory Error...");	} catch (Throwable err) {	}	ExitUtil.haltOnOutOfMemory((OutOfMemoryError) exception);	} else {	ExitUtil.ExitException ee = ServiceLauncher.convertToExitException(exception);	ExitUtil.terminate(ee.status, ee);	}	} else {	
thread threw an exception 

========================= hadoop sample_4076 =========================

protected void flushAndSync(boolean durable) throws IOException {	assert out.getLength() == 0 : "Output buffer is not empty";	if (doubleBuf.isFlushed()) {	
nothing to flush 

========================= hadoop sample_8055 =========================

public void testJobControlWithFailJob() throws Exception {	
starting testjobcontrolwithfailjob 

public void testJobControlWithKillJob() throws Exception {	
starting testjobcontrolwithkilljob 

public void testJobControl() throws Exception {	
starting testjobcontrol 

public void testControlledJob() throws Exception {	
starting testcontrolledjob 

========================= hadoop sample_5667 =========================

public void periodicInvoke() {	
updating state store cache 

========================= hadoop sample_8281 =========================

public Path getTrashRoot(Path fullPath) {	Map<String, String> params = new HashMap<>();	params.put(OP_PARAM, Operation.GETTRASHROOT.toString());	try {	HttpURLConnection conn = getConnection( Operation.GETTRASHROOT.getMethod(), params, fullPath, true);	HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);	JSONObject json = (JSONObject) HttpFSUtils.jsonParse(conn);	return new Path((String) json.get(TRASH_DIR_JSON));	} catch (IOException ex) {	
cannot find trash root of 

========================= hadoop sample_6765 =========================

private void printReferenceTraceInfo(String op) {	StackTraceElement[] stack = Thread.currentThread().getStackTrace();	for (StackTraceElement ste : stack) {	switch (ste.getMethodName()) {	case "getDfsUsed": case "getBlockPoolUsed": case "getAvailable": case "getVolumeMap": return;	default: break;	}	}	
reference count 

private String getNextSubDir(String prev, File dir) throws IOException {	List<String> children = fileIoProvider.listDirectory( FsVolumeImpl.this, dir, SubdirFilter.INSTANCE);	cache = null;	cacheMs = 0;	if (children.size() == 0) {	
getnextsubdir no subdirectories found in 

private String getNextSubDir(String prev, File dir) throws IOException {	List<String> children = fileIoProvider.listDirectory( FsVolumeImpl.this, dir, SubdirFilter.INSTANCE);	cache = null;	cacheMs = 0;	if (children.size() == 0) {	return null;	}	Collections.sort(children);	String nextSubDir = nextSorted(children, prev);	if (nextSubDir == null) {	
getnextsubdir no more subdirectories found in 

List<String> children = fileIoProvider.listDirectory( FsVolumeImpl.this, dir, SubdirFilter.INSTANCE);	cache = null;	cacheMs = 0;	if (children.size() == 0) {	return null;	}	Collections.sort(children);	String nextSubDir = nextSorted(children, prev);	if (nextSubDir == null) {	} else {	
getnextsubdir picking next subdirectory within 

private List<String> getSubdirEntries() throws IOException {	if (state.curFinalizedSubDir == null) {	return null;	}	long now = Time.monotonicNow();	if (cache != null) {	long delta = now - cacheMs;	if (delta < maxStalenessMs) {	return cache;	} else {	
getsubdirentries purging entries cache for after ms 

}	}	File dir = Paths.get(bpidDir.getAbsolutePath(), "current", "finalized", state.curFinalizedDir, state.curFinalizedSubDir).toFile();	List<String> entries = fileIoProvider.listDirectory( FsVolumeImpl.this, dir, BlockFileFilter.INSTANCE);	if (entries.size() == 0) {	entries = null;	} else {	Collections.sort(entries);	}	if (entries == null) {	
getsubdirentries no entries found in 

}	File dir = Paths.get(bpidDir.getAbsolutePath(), "current", "finalized", state.curFinalizedDir, state.curFinalizedSubDir).toFile();	List<String> entries = fileIoProvider.listDirectory( FsVolumeImpl.this, dir, BlockFileFilter.INSTANCE);	if (entries.size() == 0) {	entries = null;	} else {	Collections.sort(entries);	}	if (entries == null) {	} else {	
getsubdirentries listed entries in 

public ExtendedBlock nextBlock() throws IOException {	if (state.atEnd) {	return null;	}	try {	while (true) {	List<String> entries = getSubdirEntries();	if (entries != null) {	state.curEntry = nextSorted(entries, state.curEntry);	if (state.curEntry == null) {	
nextblock advancing from to next subdirectory 

while (true) {	List<String> entries = getSubdirEntries();	if (entries != null) {	state.curEntry = nextSorted(entries, state.curEntry);	if (state.curEntry == null) {	} else {	ExtendedBlock block = new ExtendedBlock(bpid, Block.filename2id(state.curEntry));	File expectedBlockDir = DatanodeUtil.idToBlockDir( new File("."), block.getBlockId());	File actualBlockDir = Paths.get(".", state.curFinalizedDir, state.curFinalizedSubDir).toFile();	if (!expectedBlockDir.equals(actualBlockDir)) {	
nextblock block id found in invalid directory expected directory actual directory 

if (entries != null) {	state.curEntry = nextSorted(entries, state.curEntry);	if (state.curEntry == null) {	} else {	ExtendedBlock block = new ExtendedBlock(bpid, Block.filename2id(state.curEntry));	File expectedBlockDir = DatanodeUtil.idToBlockDir( new File("."), block.getBlockId());	File actualBlockDir = Paths.get(".", state.curFinalizedDir, state.curFinalizedSubDir).toFile();	if (!expectedBlockDir.equals(actualBlockDir)) {	continue;	}	
nextblock advancing to 

if (state.curFinalizedSubDir == null) {	state.curFinalizedDir = getNextFinalizedDir();	if (state.curFinalizedDir == null) {	state.atEnd = true;	return null;	}	}	}	} catch (IOException e) {	state.atEnd = true;	
nextblock i o error 

try (BufferedWriter writer = new BufferedWriter( new OutputStreamWriter(fileIoProvider.getFileOutputStream( FsVolumeImpl.this, getTempSaveFile()), "UTF-8"))) {	WRITER.writeValue(writer, state);	success = true;	} finally {	if (!success) {	fileIoProvider.delete(FsVolumeImpl.this, getTempSaveFile());	}	}	fileIoProvider.move(FsVolumeImpl.this, getTempSaveFile().toPath(), getSaveFile().toPath(), StandardCopyOption.ATOMIC_MOVE);	if (LOG.isTraceEnabled()) {	
save saved 

public void load() throws IOException {	File file = getSaveFile();	this.state = READER.readValue(file);	
load loaded iterator from 

========================= hadoop sample_7932 =========================

static DefaultFullHttpResponse exceptionCaught(Throwable cause) {	Exception e = cause instanceof Exception ? (Exception) cause : new Exception(cause);	if (LOG.isTraceEnabled()) {	
got exception 

s = FORBIDDEN;	} else if (e instanceof FileNotFoundException) {	s = NOT_FOUND;	} else if (e instanceof IOException) {	s = FORBIDDEN;	} else if (e instanceof UnsupportedOperationException) {	s = BAD_REQUEST;	} else if (e instanceof IllegalArgumentException) {	s = BAD_REQUEST;	} else {	
internal server error 

========================= hadoop sample_7898 =========================

}	}	if (conf == null) {	conf = new Configuration();	}	try {	if (application != null) {	webapp.setHostClass(application.getClass());	} else {	String cls = inferHostClass();	
setting webapp host class to 

try {	if (application != null) {	webapp.setHostClass(application.getClass());	} else {	String cls = inferHostClass();	webapp.setHostClass(Class.forName(cls));	}	if (devMode) {	if (port > 0) {	try {	
http log info instance 

webapp.setHostClass(application.getClass());	} else {	String cls = inferHostClass();	webapp.setHostClass(Class.forName(cls));	}	if (devMode) {	if (port > 0) {	try {	Thread.sleep(100);	} catch (ConnectException e) {	
no existing webapp instance found 

} else {	String cls = inferHostClass();	webapp.setHostClass(Class.forName(cls));	}	if (devMode) {	if (port > 0) {	try {	Thread.sleep(100);	} catch (ConnectException e) {	} catch (Exception e) {	
error stopping existing instance 

webapp.setHostClass(Class.forName(cls));	}	if (devMode) {	if (port > 0) {	try {	Thread.sleep(100);	} catch (ConnectException e) {	} catch (Exception e) {	}	} else {	
dev mode does not work with ephemeral port 

}	HttpServer2 server = builder.build();	for(ServletStruct struct: servlets) {	server.addServlet(struct.name, struct.spec, struct.clazz);	}	for(Map.Entry<String, Object> entry : attributes.entrySet()) {	server.setAttribute(entry.getKey(), entry.getValue());	}	Map<String, String> params = getConfigParameters(csrfConfigPrefix);	if (hasCSRFEnabled(params)) {	
csrf protection has been enabled for the application please ensure that there is an authentication mechanism enabled kerberos custom etc 

} catch (IOException e) {	throw new WebAppException("Error starting http server", e);	}	Injector injector = Guice.createInjector(webapp, new AbstractModule() {	protected void configure() {	if (api != null) {	bind(api).toInstance(application);	}	}	});	
registered webapp guice modules 

Injector injector = Guice.createInjector(webapp, new AbstractModule() {	protected void configure() {	if (api != null) {	bind(api).toInstance(application);	}	}	});	webapp.setGuiceFilter(injector.getInstance(GuiceFilter.class));	if (devMode) {	injector.getInstance(Dispatcher.class).setDevMode(devMode);	
in dev mode 

public WebApp start(WebApp webapp, WebAppContext ui2Context) {	WebApp webApp = build(webapp);	HttpServer2 httpServer = webApp.httpServer();	if (ui2Context != null) {	addFiltersForNewContext(ui2Context);	httpServer.addContext(ui2Context, true);	}	try {	httpServer.start();	
web app started at 

private void addFiltersForNewContext(WebAppContext ui2Context) {	Map<String, String> params = getConfigParameters(csrfConfigPrefix);	if (hasCSRFEnabled(params)) {	
csrf protection has been enabled for the application please ensure that there is an authentication mechanism enabled kerberos custom etc 

private String inferHostClass() {	String thisClass = this.getClass().getName();	Throwable t = new Throwable();	for (StackTraceElement e : t.getStackTrace()) {	if (e.getClassName().equals(thisClass)) continue;	return e.getClassName();	}	
could not infer host class from 

========================= hadoop sample_2233 =========================

conf.setInt(DFSConfigKeys.DFS_NAMENODE_NUM_CHECKPOINTS_RETAINED_KEY, 10);	conf.setInt(DFSConfigKeys.DFS_HA_TAILEDITS_PERIOD_KEY, 1);	conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_EDITS_ASYNC_LOGGING, useAsyncEditLogging);	HAUtil.setAllowStandbyReads(conf, true);	if (clusterType == TestType.SHARED_DIR_HA) {	int basePort = 10000;	int retryCount = 0;	while (true) {	try {	basePort = 10000 + RANDOM.nextInt(1000) * 4;	
set shared dir ha cluster s baseport to 

basePort = 10000 + RANDOM.nextInt(1000) * 4;	MiniDFSNNTopology topology = MiniQJMHACluster.createDefaultTopology(basePort);	cluster = new MiniDFSCluster.Builder(conf) .nnTopology(topology) .numDataNodes(0) .checkExitOnShutdown(false) .build();	break;	} catch (BindException e) {	if (cluster != null) {	cluster.shutdown(true);	cluster = null;	}	++retryCount;	
shared dir ha miniqjmhacluster port conflicts retried times 

========================= hadoop sample_7461 =========================

public void addComplete(int rc, LedgerHandle handle, long entryId, Object ctx) {	synchronized(this) {	outstandingRequests.decrementAndGet();	if (!transmitResult.compareAndSet(BKException.Code.OK, rc)) {	
tried to set transmit result to bkexception getmessage rc but is already bkexception getmessage transmitresult get 

========================= hadoop sample_7748 =========================

public void testDecommissionLosingData() throws Exception {	ArrayList<String> nodes = new ArrayList<String>(2);	FSNamesystem fsn = cluster.getNamesystem();	BlockManager bm = fsn.getBlockManager();	DatanodeManager dm = bm.getDatanodeManager();	Path file1 = new Path("decommissionLosingData.dat");	writeFile(fileSys, file1, (short)numDatanodes);	Thread.sleep(1000);	
shutdown 

DatanodeManager dm = bm.getDatanodeManager();	Path file1 = new Path("decommissionLosingData.dat");	writeFile(fileSys, file1, (short)numDatanodes);	Thread.sleep(1000);	DatanodeID dnID = cluster.getDataNodes().get(1).getDatanodeId();	String dnName = dnID.getXferAddr();	DatanodeDescriptor dnDescriptor1 = dm.getDatanode(dnID);	nodes.add(dnName);	DataNodeProperties stoppedDN1 = cluster.stopDataNode(1);	DFSTestUtil.waitForDatanodeState(cluster, dnID.getDatanodeUuid(), false, 30000);	
shutdown 

DatanodeDescriptor dnDescriptor1 = dm.getDatanode(dnID);	nodes.add(dnName);	DataNodeProperties stoppedDN1 = cluster.stopDataNode(1);	DFSTestUtil.waitForDatanodeState(cluster, dnID.getDatanodeUuid(), false, 30000);	dnID = cluster.getDataNodes().get(0).getDatanodeId();	dnName = dnID.getXferAddr();	DatanodeDescriptor dnDescriptor0 = dm.getDatanode(dnID);	nodes.add(dnName);	DataNodeProperties stoppedDN0 = cluster.stopDataNode(0);	DFSTestUtil.waitForDatanodeState(cluster, dnID.getDatanodeUuid(), false, 30000);	
decommissioning nodes 

DFSTestUtil.waitForDatanodeState(cluster, dnID.getDatanodeUuid(), false, 30000);	hostsFileWriter.initExcludeHosts(nodes);	dm.refreshNodes(conf);	BlockManagerTestUtil.recheckDecommissionState(dm);	assertTrue(dnDescriptor0.isDecommissioned());	assertTrue(dnDescriptor1.isDecommissioned());	long  missingBlocks = bm.getMissingBlocksCount();	long underreplicated = bm.getUnderReplicatedBlocksCount();	assertTrue(missingBlocks > 0);	assertTrue(underreplicated > 0);	
bring back 

assertTrue(missingBlocks > 0);	assertTrue(underreplicated > 0);	cluster.restartDataNode(stoppedDN0, true);	do {	dnID = cluster.getDataNodes().get(0).getDatanodeId();	} while (dnID == null);	dnDescriptor0 = dm.getDatanode(dnID);	while (dnDescriptor0.numBlocks() == 0) {	Thread.sleep(100);	}	
bring back 

cluster.restartDataNode(stoppedDN1, true);	do {	dnID = cluster.getDataNodes().get(1).getDatanodeId();	} while (dnID == null);	dnDescriptor1 = dm.getDatanode(dnID);	while (dnDescriptor1.numBlocks() == 0) {	Thread.sleep(100);	}	Thread.sleep(2000);	assertEquals(underreplicated, bm.getUnderReplicatedBlocksCount());	
starting two more nodes 

========================= hadoop sample_7318 =========================

public void setup() throws InterruptedException, IOException {	if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {	
mrappjar not found not running test 

public void tearDown() {	if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {	
mrappjar not found not running test 

public void testJobHistoryData() throws IOException, InterruptedException, AvroRemoteException, ClassNotFoundException {	if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {	
mrappjar not found not running test 

JobId jobId = TypeConverter.toYarn(job.getJobID());	ApplicationId appID = jobId.getAppId();	int pollElapsed = 0;	while (true) {	Thread.sleep(1000);	pollElapsed += 1000;	if (TERMINAL_RM_APP_STATES.contains( mrCluster.getResourceManager().getRMContext().getRMApps().get(appID) .getState())) {	break;	}	if (pollElapsed >= 60000) {	
application did not reach terminal state within seconds 

pollElapsed += 1000;	if (TERMINAL_RM_APP_STATES.contains( mrCluster.getResourceManager().getRMContext().getRMApps().get(appID) .getState())) {	break;	}	if (pollElapsed >= 60000) {	break;	}	}	Assert.assertEquals(RMAppState.FINISHED, mrCluster.getResourceManager() .getRMContext().getRMApps().get(appID).getState());	Counters counterHS = job.getCounters();	
counterhs 

pollElapsed += 1000;	if (TERMINAL_RM_APP_STATES.contains( mrCluster.getResourceManager().getRMContext().getRMApps().get(appID) .getState())) {	break;	}	if (pollElapsed >= 60000) {	break;	}	}	Assert.assertEquals(RMAppState.FINISHED, mrCluster.getResourceManager() .getRMContext().getRMApps().get(appID).getState());	Counters counterHS = job.getCounters();	
countermr 

========================= hadoop sample_5630 =========================

String fileOwner = fsStatus.getOwner();	if (!(fileOwner.equals(currentUser.getShortUserName()) || fileOwner .equalsIgnoreCase(currentUser.getUserName()) || fileOwner .equals(realUser.getShortUserName()) || fileOwner .equalsIgnoreCase(realUser.getUserName()))) {	String errorMessage = "The ownership on the staging directory " + stagingArea + " is not as expected. " + "It is owned by " + fileOwner + ". The directory must " + "be owned by the submitter " + currentUser.getShortUserName() + " or " + currentUser.getUserName();	if (!realUser.getUserName().equals(currentUser.getUserName())) {	throw new IOException( errorMessage + " or " + realUser.getShortUserName() + " or " + realUser.getUserName());	} else {	throw new IOException(errorMessage);	}	}	if (!fsStatus.getPermission().equals(JOB_DIR_PERMISSION)) {	
permissions on staging directory are incorrect fixing permissions to correct value 

========================= hadoop sample_4922 =========================

public abstract boolean signalContainer(ContainerSignalContext ctx) throws IOException;	public abstract void deleteAsUser(DeletionAsUserContext ctx) throws IOException, InterruptedException;	public abstract void symLink(String target, String symlink) throws IOException;	public abstract boolean isContainerAlive(ContainerLivenessContext ctx) throws IOException;	public int reacquireContainer(ContainerReacquisitionContext ctx) throws IOException, InterruptedException {	Container container = ctx.getContainer();	String user = ctx.getUser();	ContainerId containerId = ctx.getContainerId();	Path pidPath = getPidFilePath(containerId);	if (pidPath == null) {	
is not active returning terminated error 

String user = ctx.getUser();	ContainerId containerId = ctx.getContainerId();	Path pidPath = getPidFilePath(containerId);	if (pidPath == null) {	return ExitCode.TERMINATED.getExitCode();	}	String pid = ProcessIdFileReader.getProcessId(pidPath);	if (pid == null) {	throw new IOException("Unable to determine pid for " + containerId);	}	
reacquiring with pid 

ContainerLivenessContext livenessContext = new ContainerLivenessContext .Builder() .setContainer(container) .setUser(user) .setPid(pid) .build();	while (isContainerAlive(livenessContext)) {	Thread.sleep(1000);	}	final int sleepMsec = 100;	int msecLeft = 2000;	String exitCodeFile = ContainerLaunch.getExitCodeFile(pidPath.toString());	File file = new File(exitCodeFile);	while (!file.exists() && msecLeft >= 0) {	if (!isContainerActive(containerId)) {	
was deactivated 

public static String[] getLocalIpAndHost(Container container) {	String[] ipAndHost = new String[2];	try {	InetAddress address = InetAddress.getLocalHost();	ipAndHost[0] = address.getHostAddress();	ipAndHost[1] = address.getHostName();	} catch (UnknownHostException e) {	
unable to get local hostname and ip for 

public void pauseContainer(Container container) {	
doesn t support pausing 

public void resumeContainer(Container container) {	
doesn t support resume 

public String getProcessId(ContainerId containerID) {	String pid = null;	Path pidFile = pidFiles.get(containerID);	if (pidFile != null) {	try {	pid = ProcessIdFileReader.getProcessId(pidFile);	} catch (IOException e) {	
got exception reading pid from pid file 

========================= hadoop sample_1673 =========================

public void run() {	while (!Thread.currentThread().isInterrupted()) {	try {	continuousSchedulingAttempt();	Thread.sleep(getContinuousSchedulingSleepMs());	} catch (InterruptedException e) {	
continuous scheduling thread interrupted exiting 

private void dumpSchedulerState() {	FSQueue rootQueue = queueMgr.getRootQueue();	Resource clusterResource = getClusterResource();	
fairscheduler state cluster capacity allocations availability demand 

UserGroupInformation userUgi = UserGroupInformation.createRemoteUser( user);	if (!queue.hasAccess(QueueACL.SUBMIT_APPLICATIONS, userUgi) && !queue .hasAccess(QueueACL.ADMINISTER_QUEUE, userUgi)) {	String msg = "User " + userUgi.getUserName() + " cannot submit applications to queue " + queue.getName() + "(requested queuename is " + queueName + ")";	LOG.info(msg);	rmContext.getDispatcher().getEventHandler().handle( new RMAppEvent(applicationId, RMAppEventType.APP_REJECTED, msg));	return;	}	SchedulerApplication<FSAppAttempt> application = new SchedulerApplication<FSAppAttempt>(queue, user);	applications.put(applicationId, application);	queue.getMetrics().submitApp(user);	
accepted application from user in queue currently num of applications 

String msg = "User " + userUgi.getUserName() + " cannot submit applications to queue " + queue.getName() + "(requested queuename is " + queueName + ")";	LOG.info(msg);	rmContext.getDispatcher().getEventHandler().handle( new RMAppEvent(applicationId, RMAppEventType.APP_REJECTED, msg));	return;	}	SchedulerApplication<FSAppAttempt> application = new SchedulerApplication<FSAppAttempt>(queue, user);	applications.put(applicationId, application);	queue.getMetrics().submitApp(user);	if (isAppRecovering) {	if (LOG.isDebugEnabled()) {	
is recovering skip notifying app accepted 

}	application.setCurrentAppAttempt(attempt);	boolean runnable = maxRunningEnforcer.canAppBeRunnable(queue, attempt);	queue.addApp(attempt, runnable);	if (runnable) {	maxRunningEnforcer.trackRunnableApp(attempt);	} else{	maxRunningEnforcer.trackNonRunnableApp(attempt);	}	queue.getMetrics().submitAppAttempt(user);	
added application attempt to scheduler from user 

boolean runnable = maxRunningEnforcer.canAppBeRunnable(queue, attempt);	queue.addApp(attempt, runnable);	if (runnable) {	maxRunningEnforcer.trackRunnableApp(attempt);	} else{	maxRunningEnforcer.trackNonRunnableApp(attempt);	}	queue.getMetrics().submitAppAttempt(user);	if (isAttemptRecovering) {	if (LOG.isDebugEnabled()) {	
is recovering skipping notifying attempt added 

private void removeApplication(ApplicationId applicationId, RMAppState finalState) {	SchedulerApplication<FSAppAttempt> application = applications.remove( applicationId);	if (application == null) {	
couldn t find application 

private void removeApplicationAttempt( ApplicationAttemptId applicationAttemptId, RMAppAttemptState rmAppAttemptFinalState, boolean keepContainers) {	try {	writeLock.lock();	LOG.info("Application " + applicationAttemptId + " is done. finalState=" + rmAppAttemptFinalState);	FSAppAttempt attempt = getApplicationAttempt(applicationAttemptId);	if (attempt == null) {	
unknown application has completed 

private void removeApplicationAttempt( ApplicationAttemptId applicationAttemptId, RMAppAttemptState rmAppAttemptFinalState, boolean keepContainers) {	try {	writeLock.lock();	LOG.info("Application " + applicationAttemptId + " is done. finalState=" + rmAppAttemptFinalState);	FSAppAttempt attempt = getApplicationAttempt(applicationAttemptId);	if (attempt == null) {	return;	}	if (attempt.isStopped()) {	
application has already been stopped 

LOG.info("Application " + applicationAttemptId + " is done. finalState=" + rmAppAttemptFinalState);	FSAppAttempt attempt = getApplicationAttempt(applicationAttemptId);	if (attempt == null) {	return;	}	if (attempt.isStopped()) {	return;	}	for (RMContainer rmContainer : attempt.getLiveContainers()) {	if (keepContainers && rmContainer.getState().equals( RMContainerState.RUNNING)) {	
skip killing 

protected void completedContainerInternal( RMContainer rmContainer, ContainerStatus containerStatus, RMContainerEventType event) {	try {	writeLock.lock();	Container container = rmContainer.getContainer();	FSAppAttempt application = getCurrentAttemptForContainer( container.getId());	ApplicationId appId = container.getId().getApplicationAttemptId().getApplicationId();	if (application == null) {	
container of finished application completed with event 

}	FSSchedulerNode node = getFSSchedulerNode(container.getNodeId());	if (rmContainer.getState() == RMContainerState.RESERVED) {	application.unreserve(rmContainer.getReservedSchedulerKey(), node);	} else{	application.containerCompleted(rmContainer, containerStatus, event);	node.releaseContainer(rmContainer.getContainerId(), false);	updateRootQueueMetrics();	}	if (LOG.isDebugEnabled()) {	
application attempt released container on node with event 

private void addNode(List<NMContainerStatus> containerReports, RMNode node) {	try {	writeLock.lock();	FSSchedulerNode schedulerNode = new FSSchedulerNode(node, usePortForNodeName);	nodeTracker.addNode(schedulerNode);	triggerUpdate();	Resource clusterResource = getClusterResource();	queueMgr.getRootQueue().setSteadyFairShare(clusterResource);	queueMgr.getRootQueue().recomputeSteadyShares();	
added node cluster capacity 

private void removeNode(RMNode rmNode) {	try {	writeLock.lock();	NodeId nodeId = rmNode.getNodeID();	FSSchedulerNode node = nodeTracker.getNode(nodeId);	if (node == null) {	
attempting to remove non existent node 

RMContainer reservedContainer = node.getReservedContainer();	if (reservedContainer != null) {	super.completedContainer(reservedContainer, SchedulerUtils .createAbnormalContainerStatus(reservedContainer.getContainerId(), SchedulerUtils.LOST_CONTAINER), RMContainerEventType.KILL);	}	nodeTracker.removeNode(nodeId);	Resource clusterResource = getClusterResource();	queueMgr.getRootQueue().setSteadyFairShare(clusterResource);	queueMgr.getRootQueue().recomputeSteadyShares();	updateRootQueueMetrics();	triggerUpdate();	
removed node cluster capacity 

public Allocation allocate(ApplicationAttemptId appAttemptId, List<ResourceRequest> ask, List<ContainerId> release, List<String> blacklistAdditions, List<String> blacklistRemovals, ContainerUpdates updateRequests) {	FSAppAttempt application = getSchedulerApp(appAttemptId);	if (application == null) {	
calling allocate on removed or non existent application 

public Allocation allocate(ApplicationAttemptId appAttemptId, List<ResourceRequest> ask, List<ContainerId> release, List<String> blacklistAdditions, List<String> blacklistRemovals, ContainerUpdates updateRequests) {	FSAppAttempt application = getSchedulerApp(appAttemptId);	if (application == null) {	return EMPTY_ALLOCATION;	}	if (!application.getApplicationAttemptId().equals(appAttemptId)) {	
calling allocate on previous or removed or non existent application attempt 

application.showRequests();	application.updateResourceRequests(ask);	application.showRequests();	}	} finally {	lock.unlock();	}	Set<ContainerId> preemptionContainerIds = application.getPreemptionContainerIds();	if (LOG.isDebugEnabled()) {	LOG.debug( "allocate: post-update" + " applicationAttemptId=" + appAttemptId + " #ask=" + ask.size() + " reservation= " + application .getCurrentReservation());	
preempting container s 

RMContainer container = ((ReleaseContainerEvent) event).getContainer();	completedContainer(container, SchedulerUtils.createAbnormalContainerStatus( container.getContainerId(), SchedulerUtils.RELEASED_CONTAINER), RMContainerEventType.RELEASED);	break;	case CONTAINER_EXPIRED: if (!(event instanceof ContainerExpiredSchedulerEvent)) {	throw new RuntimeException("Unexpected event type: " + event);	}	ContainerExpiredSchedulerEvent containerExpiredEvent = (ContainerExpiredSchedulerEvent)event;	ContainerId containerId = containerExpiredEvent.getContainerId();	super.completedContainer(getRMContainer(containerId), SchedulerUtils.createAbnormalContainerStatus( containerId, SchedulerUtils.EXPIRED_CONTAINER), RMContainerEventType.EXPIRE);	break;	
unknown event arrived at fairscheduler 

rackLocalityDelayMs = this.conf.getLocalityDelayRackMs();	assignMultiple = this.conf.getAssignMultiple();	maxAssignDynamic = this.conf.isMaxAssignDynamic();	maxAssign = this.conf.getMaxAssign();	sizeBasedWeight = this.conf.getSizeBasedWeight();	usePortForNodeName = this.conf.getUsePortForNodeName();	reservableNodesRatio = this.conf.getReservableNodes();	updateInterval = this.conf.getUpdateInterval();	if (updateInterval < 0) {	updateInterval = FairSchedulerConfiguration.DEFAULT_UPDATE_INTERVAL_MS;	
is invalid so using default value ms instead 

public void reinitialize(Configuration conf, RMContext rmContext) throws IOException {	try {	allocsLoader.reloadAllocations();	} catch (Exception e) {	
failed to reload allocations file 

public boolean checkAccess(UserGroupInformation callerUGI, QueueACL acl, String queueName) {	try {	readLock.lock();	FSQueue queue = getQueueManager().getQueue(queueName);	if (queue == null) {	if (LOG.isDebugEnabled()) {	
acl not found for queue access type for queue 

writeLock.lock();	SchedulerApplication<FSAppAttempt> app = applications.get(appId);	if (app == null) {	throw new YarnException("App to be moved " + appId + " not found.");	}	FSAppAttempt attempt = (FSAppAttempt) app.getCurrentAppAttempt();	try {	attempt.getWriteLock().lock();	FSLeafQueue oldQueue = (FSLeafQueue) app.getQueue();	if (attempt.isStopped()) {	
application is stopped and can t be moved 

========================= hadoop sample_978 =========================

public void assignTo(TaskID taskId) throws IOException {	Path newPath = new Path(chunkContext.getChunkRootPath(), taskId.toString());	if (!chunkContext.getFs().rename(chunkFilePath, newPath)) {	
could not be assigned to 

public void release() throws IOException {	close();	if (!chunkContext.getFs().delete(chunkFilePath, false)) {	
unable to release chunk at path 

========================= hadoop sample_6289 =========================

private static boolean setField( BaseRecord record, String fieldName, Object data) {	Method m = locateSetter(record, fieldName);	if (m != null) {	try {	m.invoke(record, data);	} catch (Exception e) {	
cannot set field on object to data of type 

Map<String, Class<?>> getters = new HashMap<>();	for (Method m : record.getClass().getDeclaredMethods()) {	if (m.getName().startsWith("get")) {	try {	Class<?> type = m.getReturnType();	char[] c = m.getName().substring(3).toCharArray();	c[0] = Character.toLowerCase(c[0]);	String key = new String(c);	getters.put(key, type);	} catch (Exception e) {	
cannot execute getter on object 

private static Object getField(BaseRecord record, String fieldName) {	Object result = null;	Method m = locateGetter(record, fieldName);	if (m != null) {	try {	result = m.invoke(record);	} catch (Exception e) {	
cannot get field on object 

========================= hadoop sample_7532 =========================

public void bind(String path, ServiceRecord record, int flags) throws IOException {	Preconditions.checkArgument(record != null, "null record");	validatePath(path);	RegistryTypeUtils.validateServiceRecord(path, record);	
bound at 

========================= hadoop sample_2675 =========================

private static RandomDatum[] generate(int count) {	
generating records in memory 

private static RandomDatum[] generate(int count) {	RandomDatum[] data = new RandomDatum[count];	RandomDatum.Generator generator = new RandomDatum.Generator();	for (int i = 0; i < count; i++) {	generator.next();	data[i] = generator.getValue();	}	
sorting records 

private static void writeTest(FileSystem fs, RandomDatum[] data, String file, CompressionType compress) throws IOException {	MapFile.delete(fs, file);	
creating with records 

private static void readTest(FileSystem fs, RandomDatum[] data, String file) throws IOException {	RandomDatum v = new RandomDatum();	int sample = (int)Math.sqrt(data.length);	Random random = new Random();	
reading records 

private static void readTest(FileSystem fs, RandomDatum[] data, String file) throws IOException {	RandomDatum v = new RandomDatum();	int sample = (int)Math.sqrt(data.length);	Random random = new Random();	SetFile.Reader reader = new SetFile.Reader(fs, file, conf);	for (int i = 0; i < sample; i++) {	if (!reader.seek(data[random.nextInt(data.length)])) throw new RuntimeException("wrong value at " + i);	}	reader.close();	
done reading 

========================= hadoop sample_3064 =========================

public void submitSyncFileRangeRequest(FsVolumeImpl volume, final ReplicaOutputStreams streams, final long offset, final long nbytes, final int flags) {	execute(volume.getCurrentDir(), new Runnable() {	public void run() {	try {	streams.syncFileRangeIfPossible(offset, nbytes, flags);	} catch (NativeIOException e) {	
sync file range error 

private boolean moveFiles() {	File trashDirFile = new File(trashDirectory);	try {	fileIoProvider.mkdirsWithExistsCheck( volume, trashDirFile);	} catch (IOException e) {	return false;	}	if (LOG.isDebugEnabled()) {	
moving files and to trash 

boolean result;	result = (trashDirectory == null) ? deleteFiles() : moveFiles();	if (!result) {	LOG.warn("Unexpected error trying to " + (trashDirectory == null ? "delete" : "move") + " block " + block.getBlockPoolId() + " " + block.getLocalBlock() + " at file " + blockFile + ". Ignored.");	} else {	if(block.getLocalBlock().getNumBytes() != BlockCommand.NO_ACK){	datanode.notifyNamenodeDeletedBlock(block, volume.getStorageID());	}	volume.onBlockFileDeletion(block.getBlockPoolId(), blockLength);	volume.onMetaFileDeletion(block.getBlockPoolId(), metaLength);	
deleted file 

========================= hadoop sample_7924 =========================

private void putEntity(TimelineEntity entity, ApplicationId appId) {	try {	if (LOG.isDebugEnabled()) {	
publishing the entity json style content 

private void putEntity(TimelineEntity entity, ApplicationId appId) {	try {	if (LOG.isDebugEnabled()) {	}	TimelineCollector timelineCollector = rmTimelineCollectorManager.get(appId);	TimelineEntities entities = new TimelineEntities();	entities.addEntity(entity);	timelineCollector.putEntities(entities, UserGroupInformation.getCurrentUser());	} catch (Exception e) {	
error when publishing entity 

========================= hadoop sample_647 =========================

protected void checkOutput() throws IOException {	StringBuffer output = new StringBuffer(256);	Path[] fileList = FileUtil.stat2Paths(fileSys.listStatus( new Path(OUTPUT_DIR)));	for (int i = 0; i < fileList.length; i++){	
adding output from file 

========================= hadoop sample_5763 =========================

protected void handleNMTimelineEvent(NMTimelineEvent event) {	switch (event.getType()) {	case TIMELINE_ENTITY_PUBLISH: putEntity(((TimelinePublishEvent) event).getTimelineEntityToPublish(), ((TimelinePublishEvent) event).getApplicationId());	break;	
unknown nmtimelineevent type 

cpuMetric.addValue(currentTimeMillis, Math.round(cpuUsagePercentPerCore));	entity.addMetric(cpuMetric);	}	entity.setIdPrefix(TimelineServiceHelper. invertLong(container.getContainerStartTime()));	ApplicationId appId = container.getContainerId().getApplicationAttemptId() .getApplicationId();	try {	TimelineV2Client timelineClient = getTimelineClient(appId);	if (timelineClient != null) {	timelineClient.putEntitiesAsync(entity);	} else {	
seems like client has been removed before the container metric could be published for 

}	entity.setIdPrefix(TimelineServiceHelper. invertLong(container.getContainerStartTime()));	ApplicationId appId = container.getContainerId().getApplicationAttemptId() .getApplicationId();	try {	TimelineV2Client timelineClient = getTimelineClient(appId);	if (timelineClient != null) {	timelineClient.putEntitiesAsync(entity);	} else {	}	} catch (IOException | YarnException e) {	
failed to publish container metrics for container 

tEvent.setId(eventType);	tEvent.setTimestamp(event.getTimestamp());	entity.addEvent(tEvent);	entity.setIdPrefix(TimelineServiceHelper. invertLong(container.getContainerStartTime()));	ApplicationId appId = container.getContainerId().getApplicationAttemptId().getApplicationId();	try {	TimelineV2Client timelineClient = getTimelineClient(appId);	if (timelineClient != null) {	timelineClient.putEntitiesAsync(entity);	} else {	
seems like client has been removed before the event could be published for 

entity.addEvent(tEvent);	entity.setIdPrefix(TimelineServiceHelper. invertLong(container.getContainerStartTime()));	ApplicationId appId = container.getContainerId().getApplicationAttemptId().getApplicationId();	try {	TimelineV2Client timelineClient = getTimelineClient(appId);	if (timelineClient != null) {	timelineClient.putEntitiesAsync(entity);	} else {	}	} catch (IOException | YarnException e) {	
failed to publish container metrics for container 

private void putEntity(TimelineEntity entity, ApplicationId appId) {	try {	if (LOG.isDebugEnabled()) {	
publishing the entity json style content 

private void putEntity(TimelineEntity entity, ApplicationId appId) {	try {	if (LOG.isDebugEnabled()) {	}	TimelineV2Client timelineClient = getTimelineClient(appId);	if (timelineClient != null) {	timelineClient.putEntities(entity);	} else {	
seems like client has been removed before the entity could be published for 

private void putEntity(TimelineEntity entity, ApplicationId appId) {	try {	if (LOG.isDebugEnabled()) {	}	TimelineV2Client timelineClient = getTimelineClient(appId);	if (timelineClient != null) {	timelineClient.putEntities(entity);	} else {	}	} catch (Exception e) {	
error when publishing entity 

public void publishApplicationEvent(ApplicationEvent event) {	switch (event.getType()) {	case INIT_APPLICATION: case FINISH_APPLICATION: case APPLICATION_LOG_HANDLING_FAILED: break;	case APPLICATION_CONTAINER_FINISHED: ApplicationContainerFinishedEvent evnt = (ApplicationContainerFinishedEvent) event;	publishContainerFinishedEvent(evnt.getContainerStatus(), event.getTimestamp(), evnt.getContainerStartTime());	break;	default: if (LOG.isDebugEnabled()) {	
is not a desired applicationevent which needs to be published by nmtimelinepublisher 

public void publishContainerEvent(ContainerEvent event) {	if (this.httpAddress == null) {	this.httpAddress = nodeId.getHost() + ":" + context.getHttpPort();	}	switch (event.getType()) {	case INIT_CONTAINER: publishContainerCreatedEvent(event);	break;	default: if (LOG.isDebugEnabled()) {	
is not a desired containerevent which needs to be published by nmtimelinepublisher 

public void publishLocalizationEvent(LocalizationEvent event) {	switch (event.getType()) {	case CONTAINER_RESOURCES_LOCALIZED: publishContainerLocalizationEvent((ContainerLocalizationEvent) event, ContainerMetricsConstants.LOCALIZATION_FINISHED_EVENT_TYPE);	break;	case LOCALIZE_CONTAINER_RESOURCES: publishContainerLocalizationEvent((ContainerLocalizationEvent) event, ContainerMetricsConstants.LOCALIZATION_START_EVENT_TYPE);	break;	default: if (LOG.isDebugEnabled()) {	
is not a desired localizationevent which needs to be published by nmtimelinepublisher 

TimelineV2Client timelineClient = nmLoginUGI.doAs(new PrivilegedExceptionAction<TimelineV2Client>() {	public TimelineV2Client run() throws Exception {	TimelineV2Client timelineClient = TimelineV2Client.createTimelineClient(appId);	timelineClient.init(getConfig());	timelineClient.start();	return timelineClient;	}	});	appToClientMap.put(appId, timelineClient);	} catch (IOException | InterruptedException | RuntimeException | Error e) {	
unable to create timeline client for app 

========================= hadoop sample_1720 =========================

public void addPrioirityACLs(List<AppPriorityACLGroup> priorityACLGroups, String queueName) {	List<PriorityACL> priorityACL = allAcls.get(queueName);	if (null == priorityACL) {	priorityACL = new ArrayList<PriorityACL>();	allAcls.put(queueName, priorityACL);	}	Collections.sort(priorityACLGroups);	for (AppPriorityACLGroup priorityACLGroup : priorityACLGroups) {	priorityACL.add(new PriorityACL(priorityACLGroup.getMaxPriority(), priorityACLGroup.getDefaultPriority(), priorityACLGroup.getACLList()));	if (LOG.isDebugEnabled()) {	
priority acl group added max priority default priority 

========================= hadoop sample_702 =========================

public static void setup() throws IOException {	if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {	
mrappjar not found not running test 

public void testRMNMInfo() throws Exception {	if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {	
mrappjar not found not running test 

========================= hadoop sample_5626 =========================

public void testFailureWithMisconfiguredHaNNs() throws Exception {	String logicalHost = "misconfigured-ha-uri";	Configuration conf = new Configuration();	conf.set(HdfsClientConfigKeys.Failover.PROXY_PROVIDER_KEY_PREFIX + "." + logicalHost, ConfiguredFailoverProxyProvider.class.getName());	URI uri = new URI("hdfs: try {	FileSystem.get(uri, conf).exists(new Path("/test"));	fail("Successfully got proxy provider for misconfigured FS");	} catch (IOException ioe) {	
got expected exception 

Field f = InetAddress.class.getDeclaredField("nameServices");	f.setAccessible(true);	Assume.assumeNotNull(f);	List<NameService> nsList = (List<NameService>) f.get(null);	NameService ns = nsList.get(0);	Log log = LogFactory.getLog("NameServiceSpy");	ns = Mockito.mock(NameService.class, new GenericTestUtils.DelegateAnswer(log, ns));	nsList.set(0, ns);	return ns;	} catch (Throwable t) {	
unable to spy on dns skipping test 

========================= hadoop sample_7135 =========================

private String setupTestWorkDir() {	File testWorkDir = new File("target", this.getClass().getCanonicalName());	try {	FileContext.getLocalFSFileContext().delete( new Path(testWorkDir.getAbsolutePath()), true);	return testWorkDir.getAbsolutePath();	} catch (Exception e) {	
could not cleanup 

========================= hadoop sample_5108 =========================

public void authenticate(URL url, AuthenticatedURL.Token token) throws IOException, AuthenticationException {	if (!token.isSet()) {	this.url = url;	base64 = new Base64(0);	HttpURLConnection conn = token.openConnection(url, connConfigurator);	conn.setRequestMethod(AUTH_HTTP_METHOD);	conn.connect();	boolean needFallback = false;	if (conn.getResponseCode() == HttpURLConnection.HTTP_OK) {	
jdk performed authentication on our behalf 

conn.connect();	boolean needFallback = false;	if (conn.getResponseCode() == HttpURLConnection.HTTP_OK) {	AuthenticatedURL.extractToken(conn, token);	if (isTokenKerberos(token)) {	return;	}	needFallback = true;	}	if (!needFallback && isNegotiate(conn)) {	
performing our own spnego sequence 

if (conn.getResponseCode() == HttpURLConnection.HTTP_OK) {	AuthenticatedURL.extractToken(conn, token);	if (isTokenKerberos(token)) {	return;	}	needFallback = true;	}	if (!needFallback && isNegotiate(conn)) {	doSpnegoSequence(token);	} else {	
using fallback authenticator sequence 

private void doSpnegoSequence(final AuthenticatedURL.Token token) throws IOException, AuthenticationException {	try {	AccessControlContext context = AccessController.getContext();	Subject subject = Subject.getSubject(context);	if (subject == null || (!KerberosUtil.hasKerberosKeyTab(subject) && !KerberosUtil.hasKerberosTicket(subject))) {	
no subject in context logging in 

private void doSpnegoSequence(final AuthenticatedURL.Token token) throws IOException, AuthenticationException {	try {	AccessControlContext context = AccessController.getContext();	Subject subject = Subject.getSubject(context);	if (subject == null || (!KerberosUtil.hasKerberosKeyTab(subject) && !KerberosUtil.hasKerberosTicket(subject))) {	subject = new Subject();	LoginContext login = new LoginContext("", subject, null, new KerberosConfiguration());	login.login();	}	if (LOG.isDebugEnabled()) {	
using subject 

========================= hadoop sample_2767 =========================

if (device == null) {	throw new ResourceHandlerException("device cannot be null!");	}	String tmpDirBase = conf.get("hadoop.tmp.dir");	if (tmpDirBase == null) {	throw new ResourceHandlerException("hadoop.tmp.dir not set!");	}	tmpDirPath = tmpDirBase + "/nm-tc-rules";	File tmpDir = new File(tmpDirPath);	if (!(tmpDir.exists() || tmpDir.mkdirs())) {	
unable to create directory 

if (!(tmpDir.exists() || tmpDir.mkdirs())) {	throw new ResourceHandlerException("Unable to create directory: " + tmpDirPath);	}	this.device = device;	this.rootBandwidthMbit = rootBandwidthMbit;	this.yarnBandwidthMbit = yarnBandwidthMbit;	defaultClassBandwidthMbit = (rootBandwidthMbit - yarnBandwidthMbit) <= 0 ? rootBandwidthMbit : (rootBandwidthMbit - yarnBandwidthMbit);	boolean recoveryEnabled = conf.getBoolean(YarnConfiguration .NM_RECOVERY_ENABLED, YarnConfiguration.DEFAULT_NM_RECOVERY_ENABLED);	String state = null;	if (!recoveryEnabled) {	
nm recovery is not enabled we ll wipe tc state before proceeding 

this.device = device;	this.rootBandwidthMbit = rootBandwidthMbit;	this.yarnBandwidthMbit = yarnBandwidthMbit;	defaultClassBandwidthMbit = (rootBandwidthMbit - yarnBandwidthMbit) <= 0 ? rootBandwidthMbit : (rootBandwidthMbit - yarnBandwidthMbit);	boolean recoveryEnabled = conf.getBoolean(YarnConfiguration .NM_RECOVERY_ENABLED, YarnConfiguration.DEFAULT_NM_RECOVERY_ENABLED);	String state = null;	if (!recoveryEnabled) {	} else {	state = readState();	if (checkIfAlreadyBootstrapped(state)) {	
tc configuration is already in place not wiping state 

defaultClassBandwidthMbit = (rootBandwidthMbit - yarnBandwidthMbit) <= 0 ? rootBandwidthMbit : (rootBandwidthMbit - yarnBandwidthMbit);	boolean recoveryEnabled = conf.getBoolean(YarnConfiguration .NM_RECOVERY_ENABLED, YarnConfiguration.DEFAULT_NM_RECOVERY_ENABLED);	String state = null;	if (!recoveryEnabled) {	} else {	state = readState();	if (checkIfAlreadyBootstrapped(state)) {	reacquireContainerClasses(state);	return;	} else {	
tc configuration is incomplete wiping tc state before proceeding 

private void initializeState() throws ResourceHandlerException {	
initializing tc state 

private void initializeState() throws ResourceHandlerException {	BatchBuilder builder = new BatchBuilder(PrivilegedOperation. OperationType.TC_MODIFY_STATE) .addRootQDisc() .addCGroupFilter() .addClassToRootQDisc(rootBandwidthMbit) .addDefaultClass(defaultClassBandwidthMbit, rootBandwidthMbit) .addYARNRootClass(yarnBandwidthMbit, yarnBandwidthMbit);	PrivilegedOperation op = builder.commitBatchToTempFile();	try {	privilegedOperationExecutor.executePrivilegedOperation(op, false);	} catch (PrivilegedOperationException e) {	
failed to bootstrap outbound bandwidth configuration 

List<String> regexes = new ArrayList<>();	regexes.add(String.format("^qdisc htb %d: root(.)*$", ROOT_QDISC_HANDLE));	regexes.add(String.format("^filter parent %d: protocol ip " + "(.)*cgroup(.)*$", ROOT_QDISC_HANDLE));	regexes.add(String.format("^class htb %d:%d root(.)*$", ROOT_QDISC_HANDLE, ROOT_CLASS_ID));	regexes.add(String.format("^class htb %d:%d parent %d:%d(.)*$", ROOT_QDISC_HANDLE, DEFAULT_CLASS_ID, ROOT_QDISC_HANDLE, ROOT_CLASS_ID));	regexes.add(String.format("^class htb %d:%d parent %d:%d(.)*$", ROOT_QDISC_HANDLE, YARN_ROOT_CLASS_ID, ROOT_QDISC_HANDLE, ROOT_CLASS_ID));	for (String regex : regexes) {	Pattern pattern = Pattern.compile(regex, Pattern.MULTILINE);	if (pattern.matcher(state).find()) {	if (LOG.isDebugEnabled()) {	
matched regex 

Pattern pattern = Pattern.compile(regex, Pattern.MULTILINE);	if (pattern.matcher(state).find()) {	if (LOG.isDebugEnabled()) {	}	} else {	String logLine = new StringBuffer("Failed to match regex: ") .append(regex).append(" Current state: ").append(state).toString();	LOG.warn(logLine);	return false;	}	}	
bootstrap check succeeded 

private String readState() throws ResourceHandlerException {	BatchBuilder builder = new BatchBuilder(PrivilegedOperation. OperationType.TC_READ_STATE) .readState();	PrivilegedOperation op = builder.commitBatchToTempFile();	try {	String output = privilegedOperationExecutor.executePrivilegedOperation(op, true);	if (LOG.isDebugEnabled()) {	
tc state n 

private String readState() throws ResourceHandlerException {	BatchBuilder builder = new BatchBuilder(PrivilegedOperation. OperationType.TC_READ_STATE) .readState();	PrivilegedOperation op = builder.commitBatchToTempFile();	try {	String output = privilegedOperationExecutor.executePrivilegedOperation(op, true);	if (LOG.isDebugEnabled()) {	}	return output;	} catch (PrivilegedOperationException e) {	
failed to bootstrap outbound bandwidth rules 

private void wipeState() throws ResourceHandlerException {	BatchBuilder builder = new BatchBuilder(PrivilegedOperation. OperationType.TC_MODIFY_STATE) .wipeState();	PrivilegedOperation op = builder.commitBatchToTempFile();	try {	
wiping tc state 

private void wipeState() throws ResourceHandlerException {	BatchBuilder builder = new BatchBuilder(PrivilegedOperation. OperationType.TC_MODIFY_STATE) .wipeState();	PrivilegedOperation op = builder.commitBatchToTempFile();	try {	privilegedOperationExecutor.executePrivilegedOperation(op, false);	} catch (PrivilegedOperationException e) {	
failed to wipe tc state this could happen if the interface is already in its default state ignoring 

Pattern tcClassPattern = Pattern.compile(String.format( "class htb %d:(\\d+) .*", ROOT_QDISC_HANDLE));	synchronized (classIdSet) {	for (String tcClassSplit : tcClasses) {	String tcClass = tcClassSplit.trim();	if (!tcClass.isEmpty()) {	Matcher classMatcher = tcClassPattern.matcher(tcClass);	if (classMatcher.matches()) {	int classId = Integer.parseInt(classMatcher.group(1));	if (classId >= MIN_CONTAINER_CLASS_ID) {	classIdSet.set(classId - MIN_CONTAINER_CLASS_ID);	
reacquired container classid 

for (String tcClassSplit : tcClasses) {	String tcClass = tcClassSplit.trim();	if (!tcClass.isEmpty()) {	Matcher classMatcher = tcClassPattern.matcher(tcClass);	if (classMatcher.matches()) {	int classId = Integer.parseInt(classMatcher.group(1));	if (classId >= MIN_CONTAINER_CLASS_ID) {	classIdSet.set(classId - MIN_CONTAINER_CLASS_ID);	}	} else {	
unable to match classid in string 

public Map<Integer, Integer> readStats() throws ResourceHandlerException {	BatchBuilder builder = new BatchBuilder(PrivilegedOperation. OperationType.TC_READ_STATS) .readClasses();	PrivilegedOperation op = builder.commitBatchToTempFile();	try {	String output = privilegedOperationExecutor.executePrivilegedOperation(op, true);	if (LOG.isDebugEnabled()) {	
tc stats output 

public Map<Integer, Integer> readStats() throws ResourceHandlerException {	BatchBuilder builder = new BatchBuilder(PrivilegedOperation. OperationType.TC_READ_STATS) .readClasses();	PrivilegedOperation op = builder.commitBatchToTempFile();	try {	String output = privilegedOperationExecutor.executePrivilegedOperation(op, true);	if (LOG.isDebugEnabled()) {	}	Map<Integer, Integer> classIdBytesStats = parseStatsString(output);	if (LOG.isDebugEnabled()) {	
classid bytes sent n 

PrivilegedOperation op = builder.commitBatchToTempFile();	try {	String output = privilegedOperationExecutor.executePrivilegedOperation(op, true);	if (LOG.isDebugEnabled()) {	}	Map<Integer, Integer> classIdBytesStats = parseStatsString(output);	if (LOG.isDebugEnabled()) {	}	return classIdBytesStats;	} catch (PrivilegedOperationException e) {	
failed to get tc stats 

currentClassId = classId;	continue;	}	}	Matcher bytesMatcher = bytesPattern.matcher(line);	if (bytesMatcher.matches()) {	if (currentClassId != -1) {	int bytes = Integer.parseInt(bytesMatcher.group(1));	containerClassIdStats.put(currentClassId, bytes);	} else {	
matched a bytes sent line outside of a class stats segment 

public int getClassIdFromFileContents(String input) {	String classIdStr = String.format("%08x", Integer.parseInt(input));	if (LOG.isDebugEnabled()) {	
classid hex string 

File tcCmds = File.createTempFile(TMP_FILE_PREFIX, TMP_FILE_SUFFIX, new File(tmpDirPath));	try ( Writer writer = new OutputStreamWriter(new FileOutputStream(tcCmds), "UTF-8");	PrintWriter printWriter = new PrintWriter(writer)) {	for (String command : commands) {	printWriter.println(command);	}	}	operation.appendArgs(tcCmds.getAbsolutePath());	return operation;	} catch (IOException e) {	
failed to create or write to temporary file in dir 

========================= hadoop sample_1848 =========================

private static void saveMD5File(File dataFile, String digestString) throws IOException {	File md5File = getDigestFileForFile(dataFile);	String md5Line = digestString + " *" + dataFile.getName() + "\n";	AtomicFileOutputStream afos = new AtomicFileOutputStream(md5File);	afos.write(md5Line.getBytes(Charsets.UTF_8));	afos.close();	if (LOG.isDebugEnabled()) {	
saved to 

public static void renameMD5File(File oldDataFile, File newDataFile) throws IOException {	final File fromFile = getDigestFileForFile(oldDataFile);	if (!fromFile.exists()) {	throw new FileNotFoundException(fromFile + " does not exist.");	}	final String digestString = readStoredMd5(fromFile).group(1);	saveMD5File(newDataFile, digestString);	if (!fromFile.delete()) {	
deleting failed 

========================= hadoop sample_7836 =========================

public void run() {	ReentrantReadWriteLock.WriteLock writeLock = sortedNodesLock.writeLock();	writeLock.lock();	try {	try {	List<NodeId> nodeIds = sortNodes();	sortedNodes.clear();	sortedNodes.addAll(nodeIds);	} catch (Exception ex) {	
got exception while sorting nodes 

public void addNode(List<NMContainerStatus> containerStatuses, RMNode rmNode) {	if (LOG.isDebugEnabled()) {	
node added event from 

public void removeNode(RMNode removedRMNode) {	
node delete event for 

ReentrantReadWriteLock.WriteLock writeLock = clusterNodesLock.writeLock();	writeLock.lock();	ClusterNode node;	try {	node = this.clusterNodes.remove(removedRMNode.getNodeID());	} finally {	writeLock.unlock();	}	if (LOG.isDebugEnabled()) {	if (node != null) {	
delete clusternode 

writeLock.lock();	ClusterNode node;	try {	node = this.clusterNodes.remove(removedRMNode.getNodeID());	} finally {	writeLock.unlock();	}	if (LOG.isDebugEnabled()) {	if (node != null) {	} else {	
node not in list 

public void updateNode(RMNode rmNode) {	
node update event from 

}	int estimatedQueueWaitTime = opportunisticContainersStatus.getEstimatedQueueWaitTime();	int waitQueueLength = opportunisticContainersStatus.getWaitQueueLength();	ReentrantReadWriteLock.WriteLock writeLock = clusterNodesLock.writeLock();	writeLock.lock();	try {	ClusterNode currentNode = this.clusterNodes.get(rmNode.getNodeID());	if (currentNode == null) {	if (estimatedQueueWaitTime != -1 || comparator == LoadComparator.QUEUE_LENGTH) {	this.clusterNodes.put(rmNode.getNodeID(), new ClusterNode(rmNode.getNodeID()) .setQueueWaitTime(estimatedQueueWaitTime) .setQueueLength(waitQueueLength));	
inserting clusternode with queue wait time and wait queue length 

int estimatedQueueWaitTime = opportunisticContainersStatus.getEstimatedQueueWaitTime();	int waitQueueLength = opportunisticContainersStatus.getWaitQueueLength();	ReentrantReadWriteLock.WriteLock writeLock = clusterNodesLock.writeLock();	writeLock.lock();	try {	ClusterNode currentNode = this.clusterNodes.get(rmNode.getNodeID());	if (currentNode == null) {	if (estimatedQueueWaitTime != -1 || comparator == LoadComparator.QUEUE_LENGTH) {	this.clusterNodes.put(rmNode.getNodeID(), new ClusterNode(rmNode.getNodeID()) .setQueueWaitTime(estimatedQueueWaitTime) .setQueueLength(waitQueueLength));	} else {	
ignoring clusternode with queue wait time and wait queue length 

ClusterNode currentNode = this.clusterNodes.get(rmNode.getNodeID());	if (currentNode == null) {	if (estimatedQueueWaitTime != -1 || comparator == LoadComparator.QUEUE_LENGTH) {	this.clusterNodes.put(rmNode.getNodeID(), new ClusterNode(rmNode.getNodeID()) .setQueueWaitTime(estimatedQueueWaitTime) .setQueueLength(waitQueueLength));	} else {	}	} else {	if (estimatedQueueWaitTime != -1 || comparator == LoadComparator.QUEUE_LENGTH) {	currentNode .setQueueWaitTime(estimatedQueueWaitTime) .setQueueLength(waitQueueLength) .updateTimestamp();	if (LOG.isDebugEnabled()) {	
updating clusternode with queue wait time and wait queue length 

this.clusterNodes.put(rmNode.getNodeID(), new ClusterNode(rmNode.getNodeID()) .setQueueWaitTime(estimatedQueueWaitTime) .setQueueLength(waitQueueLength));	} else {	}	} else {	if (estimatedQueueWaitTime != -1 || comparator == LoadComparator.QUEUE_LENGTH) {	currentNode .setQueueWaitTime(estimatedQueueWaitTime) .setQueueLength(waitQueueLength) .updateTimestamp();	if (LOG.isDebugEnabled()) {	}	} else {	this.clusterNodes.remove(rmNode.getNodeID());	
deleting clusternode with queue wait time and wait queue length 

public void updateNodeResource(RMNode rmNode, ResourceOption resourceOption) {	
node resource update event from 

========================= hadoop sample_885 =========================

public void init(Class<?> protocol) {	if (protocolCache.contains(protocol)) return;	protocolCache.add(protocol);	for (Method method : protocol.getDeclaredMethods()) {	String name = method.getName();	LOG.debug(name);	try { registry.newRate(name, name, false, true); }	catch (Exception e) {	
error creating rate metrics for 

========================= hadoop sample_3463 =========================

private boolean setNextDirectoryInputStream() throws FileNotFoundException, IOException {	if (input != null) {	input.close();	
file closed 

CodecPool.returnDecompressor(inputDecompressor);	inputDecompressor = null;	inputCodec = null;	}	++inputDirectoryCursor;	if (inputDirectoryCursor >= inputDirectoryFiles.length) {	return false;	}	fileFirstLine = true;	currentFileName = inputDirectoryFiles[inputDirectoryCursor];	
opening file 

CodecPool.returnDecompressor(inputDecompressor);	inputDecompressor = null;	inputCodec = null;	}	++inputDirectoryCursor;	if (inputDirectoryCursor >= inputDirectoryFiles.length) {	return false;	}	fileFirstLine = true;	currentFileName = inputDirectoryFiles[inputDirectoryCursor];	
this file starts with line 

}	}	incorporateSpread(currentJobMapTimes, mapTimeSpreadDists, thisOutcome, thisJobType);	incorporateSpread(currentJobShuffleTimes, shuffleTimeSpreadDists, thisOutcome, thisJobType);	incorporateSpread(currentJobSortTimes, sortTimeSpreadDists, thisOutcome, thisJobType);	incorporateSpread(currentJobReduceTimes, reduceTimeSpreadDists, thisOutcome, thisJobType);	}	}	}	} catch (NumberFormatException e) {	
hadooplogsanalyzer processjobline bad numerical format at line 

task.setStartTime(Long.parseLong(startTime));	}	if (finishTime != null) {	task.setFinishTime(Long.parseLong(finishTime));	}	Pre21JobHistoryConstants.Values typ;	Pre21JobHistoryConstants.Values stat;	try {	stat = status == null ? null : Pre21JobHistoryConstants.Values .valueOf(status);	} catch (IllegalArgumentException e) {	
a task status you don t know about is status 

Pre21JobHistoryConstants.Values stat;	try {	stat = status == null ? null : Pre21JobHistoryConstants.Values .valueOf(status);	} catch (IllegalArgumentException e) {	stat = null;	}	task.setTaskStatus(stat);	try {	typ = taskType == null ? null : Pre21JobHistoryConstants.Values .valueOf(taskType);	} catch (IllegalArgumentException e) {	
a task type you don t know about is tasktype 

attempt.setAttemptID(attemptID);	}	if (!attemptAlreadyExists) {	attemptsInCurrentJob.put(attemptID, attempt);	task.getAttempts().add(attempt);	}	Pre21JobHistoryConstants.Values stat = null;	try {	stat = status == null ? null : Pre21JobHistoryConstants.Values .valueOf(status);	} catch (IllegalArgumentException e) {	
a map attempt status you don t know about is status 

} else {	taskAttemptStartTimes.remove(attemptID);	}	} else if (status != null && attemptFinishTime != null) {	long finishTime = Long.parseLong(attemptFinishTime);	if (status.equals("SUCCESS")) {	taskMapAttemptFinishTimes.put(attemptID, finishTime);	}	}	} catch (NumberFormatException e) {	
hadooplogsanalyzer processmapattemptline bad numerical format at line 

attempt.setAttemptID(attemptID);	}	if (!attemptAlreadyExists) {	attemptsInCurrentJob.put(attemptID, attempt);	task.getAttempts().add(attempt);	}	Pre21JobHistoryConstants.Values stat = null;	try {	stat = status == null ? null : Pre21JobHistoryConstants.Values .valueOf(status);	} catch (IllegalArgumentException e) {	
a map attempt status you don t know about is status 

long finishTime = Long.parseLong(attemptFinishTime);	taskReduceAttemptFinishTimes.put(attemptID, finishTime);	if (attemptShuffleFinished != null) {	taskReduceAttemptShuffleEndTimes.put(attemptID, Long .parseLong(attemptShuffleFinished));	}	if (attemptSortFinished != null) {	taskReduceAttemptSortEndTimes.put(attemptID, Long .parseLong(attemptSortFinished));	}	}	} catch (NumberFormatException e) {	
hadooplogsanalyzer processreduceattemptline bad numerical format at line 

========================= hadoop sample_6661 =========================

public HttpBodyContent getData(SwiftObjectPath path, long offset, long length) throws IOException {	if (offset < 0) {	throw new SwiftException("Invalid offset: " + offset + " in getDataAsInputStream( path=" + path + ", offset=" + offset + ", length =" + length + ")");	}	if (length <= 0) {	throw new SwiftException("Invalid length: " + length + " in getDataAsInputStream( path="+ path + ", offset=" + offset + ", length ="+ length + ")");	}	final String range = String.format(SWIFT_RANGE_HEADER_FORMAT_PATTERN, offset, offset + length - 1);	if (LOG.isDebugEnabled()) {	
getdata 

}	final InputStream responseBodyAsStream = method.getResponseBodyAsStream();	final byte[] locationData = new byte[1024];	return responseBodyAsStream.read(locationData) > 0 ? locationData : null;	}	protected void setup(GetMethod method) throws SwiftInternalStateException {	setHeaders(method, requestHeaders);	}	});	} catch (IOException e) {	
failed to get the location of 

public AccessToken authenticate() throws IOException {	final AuthenticationRequest authenticationRequest;	if (useKeystoneAuthentication) {	authenticationRequest = keystoneAuthRequest;	} else {	authenticationRequest = authRequest;	}	
started authentication 

URI endpointURI = null;	URI objectLocation;	Endpoint swiftEndpoint = null;	AccessToken accessToken;	for (Catalog catalog : serviceCatalog) {	String name = catalog.getName();	String type = catalog.getType();	String descr = String.format("[%s: %s]; ", name, type);	catList.append(descr);	if (LOG.isDebugEnabled()) {	
catalog entry 

if (LOG.isDebugEnabled()) {	LOG.debug("Found swift catalog as " + name + " => " + type);	}	for (Endpoint endpoint : catalog.getEndpoints()) {	String endpointRegion = endpoint.getRegion();	URI publicURL = endpoint.getPublicURL();	URI internalURL = endpoint.getInternalURL();	descr = String.format("[%s => %s / %s]; ", endpointRegion, publicURL, internalURL);	regionList.append(descr);	if (LOG.isDebugEnabled()) {	
endpoint 

accessToken = access.getToken();	String path = SWIFT_OBJECT_AUTH_ENDPOINT + swiftEndpoint.getTenantId();	String host = endpointURI.getHost();	try {	objectLocation = new URI(endpointURI.getScheme(), null, host, endpointURI.getPort(), path, null, null);	} catch (URISyntaxException e) {	throw new SwiftException("object endpoint URI is incorrect: " + endpointURI + " + " + path, e);	}	setAuthDetails(endpointURI, objectLocation, accessToken);	if (LOG.isDebugEnabled()) {	
authenticated against 

private StringRequestEntity getAuthenticationRequst(AuthenticationRequest authenticationRequest) throws IOException {	final String data = JSONUtil.toJSON(new AuthenticationRequestWrapper( authenticationRequest));	if (LOG.isDebugEnabled()) {	
authenticating with 

private void preRemoteCommand(String operation) throws IOException {	if (LOG.isTraceEnabled()) {	
executing 

private <M extends HttpMethod> int exec(M method) throws IOException {	final HttpClient client = new HttpClient();	if (proxyHost != null) {	client.getParams().setParameter(ConnRoutePNames.DEFAULT_PROXY, new HttpHost(proxyHost, proxyPort));	}	int statusCode = execWithDebugOutput(method, client);	if ((statusCode == HttpStatus.SC_UNAUTHORIZED || statusCode == HttpStatus.SC_BAD_REQUEST) && method instanceof AuthPostMethod && !useKeystoneAuthentication) {	if (LOG.isDebugEnabled()) {	
operation failed with status attempting keystone auth 

useKeystoneAuthentication = true;	final AuthPostMethod authentication = (AuthPostMethod) method;	authentication.setRequestEntity(getAuthenticationRequst(keystoneAuthRequest));	statusCode = execWithDebugOutput(method, client);	}	if (statusCode == HttpStatus.SC_UNAUTHORIZED ) {	if (method instanceof AuthPostMethod) {	throw new SwiftAuthenticationFailedException(authRequest.toString(), "auth", authUri, method);	}	if (LOG.isDebugEnabled()) {	
Reauthenticating 

statusCode = execWithDebugOutput(method, client);	}	if (statusCode == HttpStatus.SC_UNAUTHORIZED ) {	if (method instanceof AuthPostMethod) {	throw new SwiftAuthenticationFailedException(authRequest.toString(), "auth", authUri, method);	}	if (LOG.isDebugEnabled()) {	}	authenticate();	if (LOG.isDebugEnabled()) {	
retrying original request 

========================= hadoop sample_6231 =========================

if (token.getKind().equals(TimelineDelegationTokenIdentifier.KIND_NAME)) {	return;	}	}	org.apache.hadoop.security.token.Token<TimelineDelegationTokenIdentifier> timelineDelegationToken = getTimelineDelegationToken();	if (timelineDelegationToken == null) {	return;	}	credentials.addToken(timelineService, timelineDelegationToken);	if (LOG.isDebugEnabled()) {	
add timline delegation token into credentials 

public void failApplicationAttempt(ApplicationAttemptId attemptId) throws YarnException, IOException {	
failing application attempt 

request.setApplicationId(applicationId);	if (diagnostics != null) {	request.setDiagnostics(diagnostics);	}	try {	int pollCount = 0;	long startTime = System.currentTimeMillis();	while (true) {	KillApplicationResponse response = rmClient.forceKillApplication(request);	if (response.getIsKillCompleted()) {	
killed application 

while (true) {	KillApplicationResponse response = rmClient.forceKillApplication(request);	if (response.getIsKillCompleted()) {	break;	}	long elapsedMillis = System.currentTimeMillis() - startTime;	if (enforceAsyncAPITimeout() && elapsedMillis >= this.asyncApiPollTimeoutMillis) {	throw new YarnException("Timed out while waiting for application " + applicationId + " to be killed.");	}	if (++pollCount % 10 == 0) {	
waiting for application to be killed 

public void signalToContainer(ContainerId containerId, SignalContainerCommand command) throws YarnException, IOException {	
signalling container with command 

========================= hadoop sample_2622 =========================

public void update(ClusterStats item) {	try {	numBlacklistedTrackers = item.getStatus().getBlacklistedTrackers();	numActiveTrackers = item.getStatus().getTaskTrackers();	maxMapTasks = item.getStatus().getMaxMapTasks();	maxReduceTasks = item.getStatus().getMaxReduceTasks();	} catch (Exception e) {	long time = System.currentTimeMillis();	
error in processing cluster status at 

========================= hadoop sample_6118 =========================

blockIdManager.setLegacyGenerationStamp(genstamp);	if (NameNodeLayoutVersion.supports( LayoutVersion.Feature.SEQUENTIAL_BLOCK_ID, imgVersion)) {	genstamp = in.readLong();	blockIdManager.setGenerationStamp(genstamp);	long stampAtIdSwitch = in.readLong();	blockIdManager.setLegacyGenerationStampLimit(stampAtIdSwitch);	long maxSequentialBlockId = in.readLong();	blockIdManager.setLastAllocatedBlockId(maxSequentialBlockId);	} else {	long startingGenStamp = blockIdManager.upgradeLegacyGenerationStamp();	
upgrading to sequential block ids generation stamp for new blocks set to 

}	if (NameNodeLayoutVersion.supports( LayoutVersion.Feature.STORED_TXIDS, imgVersion)) {	imgTxId = in.readLong();	} else {	imgTxId = 0;	}	if (NameNodeLayoutVersion.supports( LayoutVersion.Feature.ADD_INODE_ID, imgVersion)) {	long lastInodeId = in.readLong();	namesystem.dir.resetLastInodeId(lastInodeId);	if (LOG.isDebugEnabled()) {	
load last allocated inodeid from fsimage 

} else {	imgTxId = 0;	}	if (NameNodeLayoutVersion.supports( LayoutVersion.Feature.ADD_INODE_ID, imgVersion)) {	long lastInodeId = in.readLong();	namesystem.dir.resetLastInodeId(lastInodeId);	if (LOG.isDebugEnabled()) {	}	} else {	if (LOG.isDebugEnabled()) {	
old layout version doesn t have inode id will assign new id for each inode 

if (supportSnapshot) {	snapshotMap = namesystem.getSnapshotManager().read(in, this);	}	FSImageCompression compression;	if (NameNodeLayoutVersion.supports( LayoutVersion.Feature.FSIMAGE_COMPRESSION, imgVersion)) {	compression = FSImageCompression.readCompressionHeader(conf, in);	} else {	compression = FSImageCompression.createNoopCompression();	}	in = compression.unwrapInputStream(fin);	
loading image file using 

prog.setCount(Phase.LOADING_FSIMAGE, step, numFiles);	loadSecretManagerState(in);	loadCacheManagerState(in);	boolean eof = (in.read() == -1);	assert eof : "Should have reached the end of image file " + curFile;	} finally {	in.close();	}	imgDigest = new MD5Hash(digester.digest());	loaded = true;	
image file of size bytes loaded in seconds 

FSDirectory fsDir = namesystem.dir;	INodeDirectory parentINode = fsDir.rootDir;	for (long i = 0; i < numFiles; i++) {	pathComponents = FSImageSerialization.readPathComponents(in);	for (int j=0; j < pathComponents.length; j++) {	byte[] newComponent = renameReservedComponentOnUpgrade (pathComponents[j], getLayoutVersion());	if (!Arrays.equals(newComponent, pathComponents[j])) {	String oldPath = DFSUtil.byteArray2PathString(pathComponents);	pathComponents[j] = newComponent;	String newPath = DFSUtil.byteArray2PathString(pathComponents);	
renaming reserved path to 

private static void setRenameReservedMapInternal(String renameReserved) {	Collection<String> pairs = StringUtils.getTrimmedStringCollection(renameReserved);	for (String p : pairs) {	String[] pair = StringUtils.split(p, '/', '=');	Preconditions.checkArgument(pair.length == 2, "Could not parse key-value pair " + p);	String key = pair[0];	String value = pair[1];	Preconditions.checkArgument(DFSUtil.isReservedPathComponent(key), "Unknown reserved path " + key);	Preconditions.checkArgument(DFSUtil.isValidNameForComponent(value), "Invalid rename path for " + key + ": " + value);	
will rename reserved path to 

String[] components = INode.getPathNames(path);	if (components.length == 0) {	return path;	}	for (int i=0; i<components.length; i++) {	components[i] = DFSUtil.bytes2String( renameReservedComponentOnUpgrade( DFSUtil.string2Bytes(components[i]), layoutVersion));	}	path = DFSUtil.strings2PathString(components);	}	if (!path.equals(oldPath)) {	
upgrade process renamed reserved path to 

private static byte[] renameReservedRootComponentOnUpgrade(byte[] component, final int layoutVersion) throws IllegalReservedPathException {	if (!NameNodeLayoutVersion.supports(Feature.ADD_INODE_ID, layoutVersion)) {	if (Arrays.equals(component, FSDirectory.DOT_RESERVED)) {	if (!renameReservedMap.containsKey(HdfsConstants.DOT_SNAPSHOT_DIR)) {	throw new IllegalReservedPathException(RESERVED_ERROR_MSG);	}	final String renameString = renameReservedMap .get(FSDirectory.DOT_RESERVED_STRING);	component = DFSUtil.string2Bytes(renameString);	
renamed root path to 

========================= hadoop sample_8106 =========================

private CheckpointSignature runOperations() throws IOException {	
creating edits by performing fs operations 

========================= hadoop sample_7359 =========================

}	try (TimelineDataManager tdm = new TimelineDataManager(store, aclManager)) {	tdm.init(config);	tdm.start();	appLogs.loadDetailLog(tdm, groupId);	}	}	updateRefreshTimeToNow();	metrics.addCacheRefreshTime(Time.monotonicNow() - startTime);	} else {	
cache new enough skip refreshing 

public synchronized void forceRelease() {	try {	if (store != null) {	store.close();	}	} catch (IOException e) {	
error closing timeline store 

store.close();	}	} catch (IOException e) {	}	store = null;	for (LogInfo log : appLogs.getDetailLogs()) {	if (log.getFilename().contains(groupId.toString())) {	log.setOffset(0);	}	}	
cache for group released 

========================= hadoop sample_305 =========================

private void shutdown() {	try {	connection.commit();	connection.close();	connection = null;	}catch (Throwable ex) {	
exception occurred while closing connection 

connection.commit();	connection.close();	connection = null;	}catch (Throwable ex) {	} finally {	try {	if(server != null) {	server.shutdown();	}	}catch (Throwable ex) {	
exception occurred while shutting down hsqldb 

========================= hadoop sample_5671 =========================

private static void warnForDeprecatedConfigs(Configuration conf) {	for (String key : ImmutableList.of( "fs.checkpoint.size", "dfs.namenode.checkpoint.size")) {	if (conf.get(key) != null) {	
configuration key is deprecated ignoring instead please specify a value for 

========================= hadoop sample_8043 =========================

for (String nameserviceId : addrMap.keySet()) {	if (bpByNameserviceId.containsKey(nameserviceId)) {	toRefresh.add(nameserviceId);	} else {	toAdd.add(nameserviceId);	}	}	toRemove = Sets.newHashSet(Sets.difference( bpByNameserviceId.keySet(), addrMap.keySet()));	assert toRefresh.size() + toAdd.size() == addrMap.size() : "toAdd: " + Joiner.on(",").useForNull("<default>").join(toAdd) + "  toRemove: " + Joiner.on(",").useForNull("<default>").join(toRemove) + "  toRefresh: " + Joiner.on(",").useForNull("<default>").join(toRefresh);	if (!toAdd.isEmpty()) {	
starting bpofferservices for nameservices default 

lifelineAddrs.add(nnIdToLifelineAddr != null ? nnIdToLifelineAddr.get(nnId) : null);	}	BPOfferService bpos = createBPOS(nsToAdd, addrs, lifelineAddrs);	bpByNameserviceId.put(nsToAdd, bpos);	offerServices.add(bpos);	}	}	startAll();	}	if (!toRemove.isEmpty()) {	
stopping bpofferservices for nameservices default 

startAll();	}	if (!toRemove.isEmpty()) {	for (String nsToRemove : toRemove) {	BPOfferService bpos = bpByNameserviceId.get(nsToRemove);	bpos.stop();	bpos.join();	}	}	if (!toRefresh.isEmpty()) {	
refreshing list of nns for nameservices default 

========================= hadoop sample_7908 =========================

public AmazonDynamoDB createDynamoDBClient(String defaultRegion) throws IOException {	startSingletonServer();	final Configuration conf = getConf();	final AWSCredentialsProvider credentials = createAWSCredentialProviderSet(null, conf);	final ClientConfiguration awsConf = DefaultS3ClientFactory.createAwsConf(conf);	awsConf.setMaxErrorRetry(3);	final String region = getRegion(conf, defaultRegion);	
creating dynamodblocal client using endpoint in region 

public synchronized static void startSingletonServer() throws IOException {	if (dynamoDBLocalServer != null) {	return;	}	if (StringUtils.isEmpty(System.getProperty(SYSPROP_SQLITE_LIB))) {	String projectBuildDir = System.getProperty("project.build.directory");	if (StringUtils.isEmpty(projectBuildDir)) {	projectBuildDir = "target";	}	System.setProperty(SYSPROP_SQLITE_LIB, projectBuildDir + File.separator + "native-libs");	
setting 

String projectBuildDir = System.getProperty("project.build.directory");	if (StringUtils.isEmpty(projectBuildDir)) {	projectBuildDir = "target";	}	System.setProperty(SYSPROP_SQLITE_LIB, projectBuildDir + File.separator + "native-libs");	}	try {	final String port = String.valueOf(ServerSocketUtil.getPort(0, 100));	ddbEndpoint = "http: dynamoDBLocalServer = ServerRunner.createServerFromCommandLineArgs( new String[]{"-inMemory", "-port", port});	dynamoDBLocalServer.start();	
dynamodblocal singleton server was started at 

public synchronized static void stopSingletonServer() throws IOException {	if (dynamoDBLocalServer != null) {	
shutting down the in memory dynamodblocal server 

========================= hadoop sample_5935 =========================

public GetSpaceUsed build() throws IOException {	GetSpaceUsed getSpaceUsed = null;	try {	Constructor<? extends GetSpaceUsed> cons = getKlass().getConstructor(Builder.class);	getSpaceUsed = cons.newInstance(this);	} catch (InstantiationException e) {	
error trying to create an instance of 

public GetSpaceUsed build() throws IOException {	GetSpaceUsed getSpaceUsed = null;	try {	Constructor<? extends GetSpaceUsed> cons = getKlass().getConstructor(Builder.class);	getSpaceUsed = cons.newInstance(this);	} catch (InstantiationException e) {	} catch (IllegalAccessException e) {	
error trying to create 

public GetSpaceUsed build() throws IOException {	GetSpaceUsed getSpaceUsed = null;	try {	Constructor<? extends GetSpaceUsed> cons = getKlass().getConstructor(Builder.class);	getSpaceUsed = cons.newInstance(this);	} catch (InstantiationException e) {	} catch (IllegalAccessException e) {	} catch (InvocationTargetException e) {	
error trying to create 

public GetSpaceUsed build() throws IOException {	GetSpaceUsed getSpaceUsed = null;	try {	Constructor<? extends GetSpaceUsed> cons = getKlass().getConstructor(Builder.class);	getSpaceUsed = cons.newInstance(this);	} catch (InstantiationException e) {	} catch (IllegalAccessException e) {	} catch (InvocationTargetException e) {	} catch (NoSuchMethodException e) {	
doesn t look like the class have the needed constructor 

========================= hadoop sample_4277 =========================

public Token<NMTokenIdentifier> selectToken(Text service, Collection<Token<? extends TokenIdentifier>> tokens) {	if (service == null) {	return null;	}	for (Token<? extends TokenIdentifier> token : tokens) {	if (LOG.isDebugEnabled()) {	
looking for service current token is 

========================= hadoop sample_2209 =========================

private void testSuccessfulJob(String filename, Class<? extends OutputCommitter> committer, String[] exclude) throws IOException {	JobConf jc = mr.createJobConf();	Path outDir = getNewOutputDir();	configureJob(jc, "job with cleanup()", 1, 0, outDir);	jc.setOutputCommitter(committer);	JobClient jobClient = new JobClient(jc);	RunningJob job = jobClient.submitJob(jc);	JobID id = job.getID();	job.waitForCompletion();	
job finished 

jc.setMapperClass(UtilsForTests.KillMapper.class);	jc.setOutputCommitter(committer);	JobClient jobClient = new JobClient(jc);	RunningJob job = jobClient.submitJob(jc);	JobID id = job.getID();	Counters counters = job.getCounters();	while (true) {	if (counters.getCounter(JobCounter.TOTAL_LAUNCHED_MAPS) == 1) {	break;	}	
waiting for a map task to be launched 

========================= hadoop sample_5489 =========================

private static void proxyLink(final HttpServletRequest req, final HttpServletResponse resp, final URI link, final Cookie c, final String proxyHost, final HTTP method) throws IOException {	DefaultHttpClient client = new DefaultHttpClient();	client .getParams() .setParameter(ClientPNames.COOKIE_POLICY, CookiePolicy.BROWSER_COMPATIBILITY) .setBooleanParameter(ClientPNames.ALLOW_CIRCULAR_REDIRECTS, true);	InetAddress localAddress = InetAddress.getByName(proxyHost);	if (LOG.isDebugEnabled()) {	
local inetaddress for proxy host 

} else {	resp.setStatus(HttpServletResponse.SC_METHOD_NOT_ALLOWED);	return;	}	Enumeration<String> names = req.getHeaderNames();	while (names.hasMoreElements()) {	String name = names.nextElement();	if (PASS_THROUGH_HEADERS.contains(name)) {	String value = req.getHeader(name);	if (LOG.isDebugEnabled()) {	
req header 

final String remoteUser = req.getRemoteUser();	String[] parts = null;	if (pathInfo != null) {	if (pathInfo.startsWith(REDIRECT)) {	pathInfo = pathInfo.substring(REDIRECT.length());	isRedirect = true;	}	parts = pathInfo.split("/", 3);	}	if ((parts == null) || (parts.length < 2)) {	
gave an invalid proxy path 

}	String appId = parts[1];	String rest = parts.length > 2 ? parts[2] : "";	ApplicationId id = null;	try {	id = Apps.toAppID(appId);	} catch (YarnRuntimeException e) {	throw new YarnRuntimeException("Error parsing Application Id");	}	if (id == null) {	
attempting to access that is invalid 

try {	fetchedAppReport = getFetchedAppReport(id);	} catch (ApplicationNotFoundException e) {	fetchedAppReport = null;	}	ApplicationReport applicationReport = null;	if (fetchedAppReport != null) {	applicationReport = fetchedAppReport.getApplicationReport();	}	if (applicationReport == null) {	
attempting to access that was not found 

}	notFound(resp, "Application " + appId + " could not be found " + "in RM or history server");	return;	}	URI trackingUri = getTrackingUri(req, resp, id, applicationReport.getOriginalTrackingUrl(), fetchedAppReport.getAppReportSource());	if (trackingUri == null) {	return;	}	String runningUser = applicationReport.getUser();	if (checkUser && !runningUser.equals(remoteUser)) {	
asking if they want to connect to the app master gui of owned by 

URI trackingUri = getTrackingUri(req, resp, id, applicationReport.getOriginalTrackingUrl(), fetchedAppReport.getAppReportSource());	if (trackingUri == null) {	return;	}	String runningUser = applicationReport.getUser();	if (checkUser && !runningUser.equals(remoteUser)) {	warnUserPage(resp, ProxyUriUtils.getPathAndQuery(id, rest, req.getQueryString(), true), runningUser, id);	return;	}	URI toFetch = buildTrackingUrl(trackingUri, req, rest);	
is accessing unchecked which is the app master gui of owned by 

private boolean handleRedirect(String id, HttpServletRequest req, HttpServletResponse resp) throws IOException {	boolean badRedirect = false;	try {	badRedirect = NetUtils.getLocalInetAddress(req.getRemoteHost()) != null;	} catch (SocketException ex) {	}	if (badRedirect) {	
the am s web app redirected the rm web proxy s request back to the web proxy the typical cause is that the am is resolving the rm s address as something other than what it expects check your network configuration and the value of the yarn web proxy address property once the host resolution issue has been resolved you will likely need to delete the misbehaving application 

private boolean handleRedirect(String id, HttpServletRequest req, HttpServletResponse resp) throws IOException {	boolean badRedirect = false;	try {	badRedirect = NetUtils.getLocalInetAddress(req.getRemoteHost()) != null;	} catch (SocketException ex) {	}	if (badRedirect) {	String redirect = StringHelper.pjoin(failurePageUrlBase, id);	
redirect sending redirect to 

========================= hadoop sample_1560 =========================

public void doPreUpgrade() throws IOException {	
starting upgrade of edits directory 

public void doPreUpgrade() throws IOException {	try {	NNUpgradeUtil.doPreUpgrade(conf, sd);	} catch (IOException ioe) {	
failed to move aside pre upgrade storage in image directory 

========================= hadoop sample_8038 =========================

for (Writer w : writers) {	w.stopWriter();	}	int filesCreated = 0;	int numFailures = 0;	for (Writer w : writers) {	w.join();	filesCreated += w.getFilesCreated();	numFailures += w.getNumFailures();	}	
stress test created files and hit failures 

});	FSDataOutputStream os = fs.create(file, replication);	os.write(new byte[1]);	os.close();	cluster.triggerBlockReports();	for (final DataNode dn : cluster.getDataNodes()) {	try (FsDatasetSpi.FsVolumeReferences volumes = dn.getFSDataset().getFsVolumeReferences()) {	final FsVolumeImpl volume = (FsVolumeImpl) volumes.get(0);	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	
dn space 

private void checkReservedSpace(final long expectedReserved) throws TimeoutException, InterruptedException, IOException {	for (final DataNode dn : cluster.getDataNodes()) {	try (FsDatasetSpi.FsVolumeReferences volumes = dn.getFSDataset() .getFsVolumeReferences()) {	final FsVolumeImpl volume = (FsVolumeImpl) volumes.get(0);	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	
dn space expected reservedspace 

========================= hadoop sample_7267 =========================

private void seekQuietly(long positiveTargetPos) {	try {	seek(positiveTargetPos);	} catch (IOException ioe) {	
ignoring ioe on seek of to 

return;	}	long diff = targetPos - pos;	if (diff > 0) {	int available = wrappedStream.available();	long forwardSeekRange = Math.max(readahead, available);	long remainingInCurrentRequest = remainingInCurrentRequest();	long forwardSeekLimit = Math.min(remainingInCurrentRequest, forwardSeekRange);	boolean skipForward = remainingInCurrentRequest > 0 && diff <= forwardSeekLimit;	if (skipForward) {	
forward seek on of bytes 

if (skipForward) {	streamStatistics.seekForwards(diff);	long skipped = wrappedStream.skip(diff);	if (skipped > 0) {	pos += skipped;	incrementBytesRead(diff);	}	if (pos == targetPos) {	return;	} else {	
failed to seek on to current position 

private void onReadFailure(IOException ioe, int length) throws IOException {	
got exception while trying to read from stream trying to recover 

private void onReadFailure(IOException ioe, int length) throws IOException {	
while trying to read from stream 

private void closeStream(String reason, long length, boolean forceAbort) {	if (wrappedStream != null) {	long remaining = remainingInCurrentRequest();	
closing stream abort soft 

private void closeStream(String reason, long length, boolean forceAbort) {	if (wrappedStream != null) {	long remaining = remainingInCurrentRequest();	boolean shouldAbort = forceAbort || remaining > readahead;	if (!shouldAbort) {	try {	long drained = 0;	while (wrappedStream.read() >= 0) {	drained++;	}	
drained stream of bytes 

boolean shouldAbort = forceAbort || remaining > readahead;	if (!shouldAbort) {	try {	long drained = 0;	while (wrappedStream.read() >= 0) {	drained++;	}	wrappedStream.close();	streamStatistics.streamClose(false, drained);	} catch (IOException e) {	
when closing stream for 

while (wrappedStream.read() >= 0) {	drained++;	}	wrappedStream.close();	streamStatistics.streamClose(false, drained);	} catch (IOException e) {	shouldAbort = true;	}	}	if (shouldAbort) {	
aborting stream 

public synchronized boolean resetConnection() throws IOException {	checkNotClosed();	boolean connectionOpen = wrappedStream != null;	if (connectionOpen) {	
forced reset of connection to 

========================= hadoop sample_6026 =========================

public void testNetgroups () throws IOException{	if(!NativeCodeLoader.isNativeCodeLoaded()) {	
not testing netgroups this test only runs when native code is compiled 

public void testNetgroups () throws IOException{	if(!NativeCodeLoader.isNativeCodeLoaded()) {	return;	}	String groupMappingClassName = System.getProperty("TestProxyUsersGroupMapping");	if(groupMappingClassName == null) {	LOG.info("Not testing netgroups, no group mapping class specified, " + "use -DTestProxyUsersGroupMapping=$className to specify " + "group mapping class (must implement GroupMappingServiceProvider " + "interface and support netgroups)");	return;	}	
testing netgroups using 

========================= hadoop sample_2966 =========================

protected void beforeExecute(Thread t, Runnable r) {	if (LOG.isDebugEnabled()) {	
beforeexecute in thread runnable type 

========================= hadoop sample_3674 =========================

needsMinMaxResetOperation = true;	metricsRateAttributeMod.put(o.getName() + NUM_OPS, o);	metricsRateAttributeMod.put(o.getName() + AVG_TIME, o);	metricsRateAttributeMod.put(o.getName() + MIN_TIME, o);	metricsRateAttributeMod.put(o.getName() + MAX_TIME, o);	}  else if ( MetricsIntValue.class.isInstance(o) || MetricsTimeVaryingInt.class.isInstance(o) ) {	attributesInfo.add(new MBeanAttributeInfo(o.getName(), "java.lang.Integer", o.getDescription(), true, false, false));	} else if ( MetricsLongValue.class.isInstance(o) || MetricsTimeVaryingLong.class.isInstance(o) ) {	attributesInfo.add(new MBeanAttributeInfo(o.getName(), "java.lang.Long", o.getDescription(), true, false, false));	} else {	
unknown metrics type 

else if (o instanceof MetricsLongValue) return ((MetricsLongValue) o).get();	else if (o instanceof MetricsTimeVaryingInt) return ((MetricsTimeVaryingInt) o).getPreviousIntervalValue();	else if (o instanceof MetricsTimeVaryingLong) return ((MetricsTimeVaryingLong) o).getPreviousIntervalValue();	else if (o instanceof MetricsTimeVaryingRate) {	MetricsTimeVaryingRate or = (MetricsTimeVaryingRate) o;	if (attributeName.endsWith(NUM_OPS)) return or.getPreviousIntervalNumOps();	else if (attributeName.endsWith(AVG_TIME)) return or.getPreviousIntervalAverageTime();	else if (attributeName.endsWith(MIN_TIME)) return or.getMinTime();	else if (attributeName.endsWith(MAX_TIME)) return or.getMaxTime();	else {	
unexpected attribute suffix 

else if (o instanceof MetricsTimeVaryingRate) {	MetricsTimeVaryingRate or = (MetricsTimeVaryingRate) o;	if (attributeName.endsWith(NUM_OPS)) return or.getPreviousIntervalNumOps();	else if (attributeName.endsWith(AVG_TIME)) return or.getPreviousIntervalAverageTime();	else if (attributeName.endsWith(MIN_TIME)) return or.getMinTime();	else if (attributeName.endsWith(MAX_TIME)) return or.getMaxTime();	else {	throw new AttributeNotFoundException();	}	} else {	
unknown metrics type 

========================= hadoop sample_3361 =========================

MiniDFSCluster cluster = null;	try {	cluster = new MiniDFSCluster.Builder(conf).numDataNodes(3).build();	cluster.waitActive();	final FileSystem fs = cluster.getFileSystem();	try (final FSDataOutputStream out = fs .create(new Path(baseDir, "test.data"), (short) 2)) {	out.write(0x31);	out.hflush();	out.hsync();	}	
delay info 

========================= hadoop sample_7288 =========================

public UseSharedCacheResourceResponse use( UseSharedCacheResourceRequest request) throws YarnException, IOException {	UseSharedCacheResourceResponse response = recordFactory.newRecordInstance(UseSharedCacheResourceResponse.class);	UserGroupInformation callerUGI;	try {	callerUGI = UserGroupInformation.getCurrentUser();	} catch (IOException ie) {	
error getting ugi 

public ReleaseSharedCacheResourceResponse release( ReleaseSharedCacheResourceRequest request) throws YarnException, IOException {	ReleaseSharedCacheResourceResponse response = recordFactory .newRecordInstance(ReleaseSharedCacheResourceResponse.class);	UserGroupInformation callerUGI;	try {	callerUGI = UserGroupInformation.getCurrentUser();	} catch (IOException ie) {	
error getting ugi 

========================= hadoop sample_391 =========================

public List<InputSplit> split(Configuration conf, ResultSet results, String colName) throws SQLException {	
generating splits for a textual index column 

public List<InputSplit> split(Configuration conf, ResultSet results, String colName) throws SQLException {	
if your database sorts in a case insensitive order this may result in a partial import or duplicate records 

public List<InputSplit> split(Configuration conf, ResultSet results, String colName) throws SQLException {	
you are strongly encouraged to choose an integral split column 

========================= hadoop sample_5061 =========================

public TimelineEntity readEntity(Configuration hbaseConf, Connection conn) throws IOException {	validateParams();	augmentParams(hbaseConf, conn);	FilterList filterList = constructFilterListBasedOnFields();	if (LOG.isDebugEnabled() && filterList != null) {	
filterlist created for get is 

public TimelineEntity readEntity(Configuration hbaseConf, Connection conn) throws IOException {	validateParams();	augmentParams(hbaseConf, conn);	FilterList filterList = constructFilterListBasedOnFields();	if (LOG.isDebugEnabled() && filterList != null) {	}	Result result = getResult(hbaseConf, conn, filterList);	if (result == null || result.isEmpty()) {	
cannot find matching entity of type 

public Set<TimelineEntity> readEntities(Configuration hbaseConf, Connection conn) throws IOException {	validateParams();	augmentParams(hbaseConf, conn);	Set<TimelineEntity> entities = new LinkedHashSet<>();	FilterList filterList = createFilterList();	if (LOG.isDebugEnabled() && filterList != null) {	
filterlist created for scan is 

========================= hadoop sample_1167 =========================

PrivilegedOperation launchOp = new PrivilegedOperation( PrivilegedOperation.OperationType.LAUNCH_CONTAINER);	launchOp.appendArgs(ctx.getExecutionAttribute(RUN_AS_USER), ctx.getExecutionAttribute(USER), Integer.toString(PrivilegedOperation. RunAsUserCommand.LAUNCH_CONTAINER.getValue()), ctx.getExecutionAttribute(APPID), ctx.getExecutionAttribute(CONTAINER_ID_STR), ctx.getExecutionAttribute(CONTAINER_WORK_DIR).toString(), ctx.getExecutionAttribute(NM_PRIVATE_CONTAINER_SCRIPT_PATH).toUri() .getPath(), ctx.getExecutionAttribute(NM_PRIVATE_TOKENS_PATH).toUri().getPath(), ctx.getExecutionAttribute(PID_FILE_PATH).toString(), StringUtils.join(PrivilegedOperation.LINUX_FILE_PATH_SEPARATOR, ctx.getExecutionAttribute(LOCAL_DIRS)), StringUtils.join(PrivilegedOperation.LINUX_FILE_PATH_SEPARATOR, ctx.getExecutionAttribute(LOG_DIRS)), ctx.getExecutionAttribute(RESOURCES_OPTIONS));	String tcCommandFile = ctx.getExecutionAttribute(TC_COMMAND_FILE);	if (tcCommandFile != null) {	launchOp.appendArgs(tcCommandFile);	}	List<String> prefixCommands = (List<String>) ctx.getExecutionAttribute( CONTAINER_LAUNCH_PREFIX_COMMANDS);	try {	privilegedOperationExecutor.executePrivilegedOperation(prefixCommands, launchOp, null, null, false, false);	} catch (PrivilegedOperationException e) {	
launch container failed exception 

========================= hadoop sample_1864 =========================

private void waitForRecovery(FileSystem fs, Path fn, long newLength) throws IOException {	
waiting on truncate file recovery for 

========================= hadoop sample_5574 =========================

public Boolean get() {	outs.clear();	errs.clear();	try {	getReconfigurationStatus(nodeType, address, outs, errs);	} catch (IOException e) {	
call getreconfigurationstatus on s s failed 

private void verifyOpenFilesListing(HashSet<Path> closedFileSet, HashMap<Path, FSDataOutputStream> openFilesMap) {	final String outStr = scanIntoString(out);	
dfsadmin listopenfiles output 

========================= hadoop sample_7119 =========================

return;	}	final Map<Block, BInfo> map = getMap(bpid);	for (Block b: invalidBlks) {	if (b == null) {	continue;	}	BInfo binfo = map.get(b);	if (binfo == null) {	error = true;	
invalidate missing block 

========================= hadoop sample_7230 =========================

if (mkl != null) {	minKeyLength = Integer.parseInt(mkl);	}	securityRequired = popOption(ARG_SECURE, args);	nofail = popOption(ARG_NOFAIL, args);	jaas = popOption(ARG_JAAS, args);	nologin = popOption(ARG_NOLOGIN, args);	checkShortName = popOption(ARG_VERIFYSHORTNAME, args);	String resource;	while (null != (resource = popOptionWithArgument(ARG_RESOURCE, args))) {	
loading resource 

endln();	title("Configuration Options");	for (String prop : new String[]{	KERBEROS_KINIT_COMMAND, HADOOP_SECURITY_AUTHENTICATION, HADOOP_SECURITY_AUTHORIZATION, "hadoop.kerberos.min.seconds.before.relogin", "hadoop.security.dns.interface", "hadoop.security.dns.nameserver", HADOOP_RPC_PROTECTION, HADOOP_SECURITY_SASL_PROPS_RESOLVER_CLASS, HADOOP_SECURITY_CRYPTO_CODEC_CLASSES_KEY_PREFIX, HADOOP_SECURITY_GROUP_MAPPING, "hadoop.security.impersonation.provider.class", DFS_DATA_TRANSFER_PROTECTION, DFS_DATA_TRANSFER_SASLPROPERTIES_RESOLVER_CLASS }) {	printConfOpt(prop);	}	Configuration conf = getConf();	if (isSimpleAuthentication(conf)) {	println(HADOOP_AUTHENTICATION_IS_DISABLED);	failif(securityRequired, CAT_CONFIG, HADOOP_AUTHENTICATION_IS_DISABLED);	
security is not enabled for the hadoop cluster 

for (String prop : new String[]{	KERBEROS_KINIT_COMMAND, HADOOP_SECURITY_AUTHENTICATION, HADOOP_SECURITY_AUTHORIZATION, "hadoop.kerberos.min.seconds.before.relogin", "hadoop.security.dns.interface", "hadoop.security.dns.nameserver", HADOOP_RPC_PROTECTION, HADOOP_SECURITY_SASL_PROPS_RESOLVER_CLASS, HADOOP_SECURITY_CRYPTO_CODEC_CLASSES_KEY_PREFIX, HADOOP_SECURITY_GROUP_MAPPING, "hadoop.security.impersonation.provider.class", DFS_DATA_TRANSFER_PROTECTION, DFS_DATA_TRANSFER_SASLPROPERTIES_RESOLVER_CLASS }) {	printConfOpt(prop);	}	Configuration conf = getConf();	if (isSimpleAuthentication(conf)) {	println(HADOOP_AUTHENTICATION_IS_DISABLED);	failif(securityRequired, CAT_CONFIG, HADOOP_AUTHENTICATION_IS_DISABLED);	} else {	if (isSimpleAuthentication(new Configuration())) {	
the default cluster security is insecure 

========================= hadoop sample_3766 =========================

public boolean rename(Path src, Path dst) throws IOException {	File srcFile = pathToFile(src);	File dstFile = pathToFile(dst);	if (srcFile.renameTo(dstFile)) {	return true;	}	if (Shell.WINDOWS && handleEmptyDstDirectoryOnWindows(src, srcFile, dst, dstFile)) {	return true;	}	if (LOG.isDebugEnabled()) {	
falling through to a copy of to 

public final boolean handleEmptyDstDirectoryOnWindows(Path src, File srcFile, Path dst, File dstFile) throws IOException {	try {	FileStatus sdst = this.getFileStatus(dst);	if (sdst.isDirectory() && dstFile.list().length == 0) {	if (LOG.isDebugEnabled()) {	
deleting empty destination and renaming to 

========================= hadoop sample_4109 =========================

private void initialize(QueueConfigurationParser cp) {	this.root = cp.getRoot();	leafQueues.clear();	allQueues.clear();	leafQueues = getRoot().getLeafQueues();	allQueues.putAll(getRoot().getInnerQueues());	allQueues.putAll(leafQueues);	
allqueues leafqueues 

public synchronized boolean hasAccess( String queueName, QueueACL qACL, UserGroupInformation ugi) {	Queue q = leafQueues.get(queueName);	if (q == null) {	
queue is not present 

public synchronized boolean hasAccess( String queueName, QueueACL qACL, UserGroupInformation ugi) {	Queue q = leafQueues.get(queueName);	if (q == null) {	return false;	}	if(q.getChildren() != null && !q.getChildren().isEmpty()) {	
cannot submit job to parent queue 

if (q == null) {	return false;	}	if(q.getChildren() != null && !q.getChildren().isEmpty()) {	return false;	}	if (!areAclsEnabled()) {	return true;	}	if (LOG.isDebugEnabled()) {	
checking access for the acl for user 

========================= hadoop sample_4666 =========================

}	String[] readStr;	while(read < masterStat.getLen()) {	int b = lin.readLine(line);	read += b;	readStr = line.toString().split(" ");	stores.add(new Store(Long.parseLong(readStr[2]), Long.parseLong(readStr[3])));	line.clear();	}	} catch (IOException ioe) {	
encountered exception 

========================= hadoop sample_4236 =========================

public void transition(ApplicationImpl app, ApplicationEvent event) {	app.dispatcher.getEventHandler().handle( new ApplicationLocalizationEvent( LocalizationEventType.INIT_APPLICATION_RESOURCES, app));	app.setAppLogInitedTimestamp(event.getTimestamp());	try {	app.appStateStore.storeApplication(app.appId, buildAppProto(app));	} catch (Exception ex) {	
failed to update application state in state store 

public void transition(ApplicationImpl app, ApplicationEvent event) {	
log aggregation service failed to initialize there will be no logs for this application 

public void transition(ApplicationImpl app, ApplicationEvent event) {	ApplicationContainerInitEvent initEvent = (ApplicationContainerInitEvent) event;	Container container = initEvent.getContainer();	app.containers.put(container.getContainerId(), container);	
adding to application 

public void transition(ApplicationImpl app, ApplicationEvent event) {	ApplicationContainerFinishedEvent containerEvent = (ApplicationContainerFinishedEvent) event;	if (null == app.containers.remove(containerEvent.getContainerID())) {	
removing unknown from application 

public void transition(ApplicationImpl app, ApplicationEvent event) {	ApplicationContainerFinishedEvent containerEvent = (ApplicationContainerFinishedEvent) event;	if (null == app.containers.remove(containerEvent.getContainerID())) {	} else {	
removing from application 

public ApplicationState transition(ApplicationImpl app, ApplicationEvent event) {	ApplicationContainerFinishedEvent containerFinishEvent = (ApplicationContainerFinishedEvent) event;	
removing from application 

public void transition(ApplicationImpl app, ApplicationEvent event) {	ApplicationId appId = event.getApplicationID();	app.context.getApplications().remove(appId);	app.aclsManager.removeApplication(appId);	try {	app.context.getNMStateStore().removeApplication(appId);	} catch (IOException e) {	
unable to remove application from state store 

public void handle(ApplicationEvent event) {	this.writeLock.lock();	try {	ApplicationId applicationID = event.getApplicationID();	if (LOG.isDebugEnabled()) {	
processing of type 

this.writeLock.lock();	try {	ApplicationId applicationID = event.getApplicationID();	if (LOG.isDebugEnabled()) {	}	ApplicationState oldState = stateMachine.getCurrentState();	ApplicationState newState = null;	try {	newState = stateMachine.doTransition(event.getType(), event);	} catch (InvalidStateTransitionException e) {	
can t handle this event at current state 

ApplicationId applicationID = event.getApplicationID();	if (LOG.isDebugEnabled()) {	}	ApplicationState oldState = stateMachine.getCurrentState();	ApplicationState newState = null;	try {	newState = stateMachine.doTransition(event.getType(), event);	} catch (InvalidStateTransitionException e) {	}	if (newState != null && oldState != newState) {	
application transitioned from to 

========================= hadoop sample_1872 =========================

}	final ExtendedBlock b = new ExtendedBlock(bpid, oldrbw.getBlockId(), oldrbw.getBytesAcked(), oldrbw.getGenerationStamp());	final BlockOpResponseProto s = DFSTestUtil.transferRbw( b, DFSClientAdapter.getDFSClient(fs), oldnodeinfo, newnodeinfo);	Assert.assertEquals(Status.SUCCESS, s.getStatus());	}	final ReplicaBeingWritten newrbw = getRbw(newnode, bpid);	LOG.info("newrbw = " + newrbw);	Assert.assertEquals(oldrbw.getBlockId(), newrbw.getBlockId());	Assert.assertEquals(oldrbw.getGenerationStamp(), newrbw.getGenerationStamp());	Assert.assertEquals(oldrbw.getVisibleLength(), newrbw.getVisibleLength());	
DONE 

========================= hadoop sample_7226 =========================

double deadlineFactorSample = (deadlineFactor != null) ? deadlineFactor.sample() : -1;	this.queueName = jobClass.workload.getQueueName();	this.submitTime = MILLISECONDS.convert(actualSubmissionTime, SECONDS);	this.deadline = hasDeadline ? MILLISECONDS.convert(actualSubmissionTime, SECONDS) + (long) Math.ceil(deadlineFactorSample * duration) : -1;	conf.set(QUEUE_NAME, queueName);	final long seed = rand.nextLong();	rand.setSeed(seed);	id = sequence.getAndIncrement();	name = String.format(jobClass.getClassName() + "_%06d", id);	LOG.debug(name + " (" + seed + ")");	
job timing job submission deadline duration deadline submission 

========================= hadoop sample_6067 =========================

public RegisterNodeManagerResponse registerNodeManager( RegisterNodeManagerRequest request) throws YarnException, IOException {	NodeId nodeId = request.getNodeId();	Resource resource = request.getResource();	
registering 

public NodeHeartbeatResponse nodeHeartbeat(NodeHeartbeatRequest request) throws YarnException, IOException {	NodeStatus nodeStatus = request.getNodeStatus();	
got heartbeat number 

public NodeHeartbeatResponse nodeHeartbeat(NodeHeartbeatRequest request) throws YarnException, IOException {	
got heartbeatid 

for (ApplicationId appId : nodeStatus.getKeepAliveApplications()) {	List<Long> list = keepAliveRequests.get(appId);	if (list == null) {	list = new LinkedList<Long>();	keepAliveRequests.put(appId, list);	}	list.add(System.currentTimeMillis());	}	}	if (heartBeatID == 2) {	
sending finish app for application 

public RegisterNodeManagerResponse registerNodeManager( RegisterNodeManagerRequest request) throws YarnException, IOException, IOException {	if (System.currentTimeMillis() - waitStartTime <= rmStartIntervalMS || rmNeverStart) {	throw new java.net.ConnectException("Faking RM start failure as start " + "delay timer has not expired.");	} else {	NodeId nodeId = request.getNodeId();	Resource resource = request.getResource();	
registering 

nm.start();	} catch (Throwable e) {	TestNodeStatusUpdater.this.nmStartError = e;	throw new YarnRuntimeException(e);	}	}	}.start();	System.out.println(" ----- thread already started.." + nm.getServiceState());	int waitCount = 0;	while (nm.getServiceState() == STATE.INITED && waitCount++ != 50) {	
waiting for nm to start 

} catch (Throwable e) {	TestNodeStatusUpdater.this.nmStartError = e;	throw new YarnRuntimeException(e);	}	}	}.start();	System.out.println(" ----- thread already started.." + nm.getServiceState());	int waitCount = 0;	while (nm.getServiceState() == STATE.INITED && waitCount++ != 50) {	if (nmStartError != null) {	
error during startup 

nm.init(conf);	nm.start();	int waitCount = 0;	while (heartBeatID < 1 && waitCount++ != 200) {	Thread.sleep(500);	}	Assert.assertFalse(heartBeatID < 1);	nm.stop();	waitCount = 0;	while (nm.getServiceState() != STATE.STOPPED && waitCount++ != 20) {	
waiting for nm to stop 

}	Assert.assertFalse(heartBeatID < 1);	nm.stop();	waitCount = 0;	while (nm.getServiceState() != STATE.STOPPED && waitCount++ != 20) {	Thread.sleep(1000);	}	Assert.assertEquals(STATE.STOPPED, nm.getServiceState());	waitCount = 0;	while (numCleanups.get() == 0 && waitCount++ != 20) {	
waiting for nm shutdown 

Assert.assertEquals(STATE.INITED, nm.getServiceState());	nm.start();	int waitCount = 0;	while (heartBeatID < 1 && waitCount++ != 200) {	Thread.sleep(500);	}	Assert.assertFalse(heartBeatID < 1);	Assert.assertTrue(nm.getNMContext().getDecommissioned());	waitCount = 0;	while (nm.getServiceState() != STATE.STOPPED && waitCount++ != 20) {	
waiting for nm to stop 

return nodeStatusUpdater;	}	};	nm.init(conf);	NodeStatusUpdater updater = nmWithUpdater.getUpdater();	Assert.assertNotNull("Updater not yet created ", updater);	waitStartTime = System.currentTimeMillis();	try {	nm.start();	} catch (Exception ex){	
nm should have started successfully after connecting to rm 

conf.setLong(YarnConfiguration.RM_NM_EXPIRY_INTERVAL_MS, 4000l);	nm.init(conf);	nm.start();	while (heartBeatID < 12) {	Thread.sleep(1000l);	}	MyResourceTracker3 rt = (MyResourceTracker3) nm.getNodeStatusUpdater().getRMClient();	rt.context.getApplications().remove(rt.appId);	Assert.assertEquals(1, rt.keepAliveRequests.size());	int numKeepAliveRequests = rt.keepAliveRequests.get(rt.appId).size();	
number of keep alive requests 

super.cleanUpApplicationsOnNMShutDown();	numCleanups.incrementAndGet();	}	};	}	};	nm.init(conf);	nm.start();	int waitCount = 0;	while (nm.getServiceState() != STATE.STARTED && waitCount++ != 20) {	
waiting for nm to stop 

protected ContainerManagerImpl createContainerManager(Context context, ContainerExecutor exec, DeletionService del, NodeStatusUpdater nodeStatusUpdater, ApplicationACLsManager aclsManager, LocalDirsHandlerService diskhandler) {	return new MyContainerManager(context, exec, del, nodeStatusUpdater, metrics, diskhandler);	}	};	YarnConfiguration conf = createNMConfig();	nm.init(conf);	nm.start();	System.out.println(" ----- thread already started.." + nm.getServiceState());	int waitCount = 0;	while (nm.getServiceState() == STATE.INITED && waitCount++ != 20) {	
waiting for nm to start 

return new MyContainerManager(context, exec, del, nodeStatusUpdater, metrics, diskhandler);	}	};	YarnConfiguration conf = createNMConfig();	nm.init(conf);	nm.start();	System.out.println(" ----- thread already started.." + nm.getServiceState());	int waitCount = 0;	while (nm.getServiceState() == STATE.INITED && waitCount++ != 20) {	if (nmStartError != null) {	
error during startup 

========================= hadoop sample_1608 =========================

public void submissionFailed(JobStats job) {	String jobID = job.getJob().getConfiguration().get(Gridmix.ORIGINAL_JOB_ID);	
job submission failed notification for job 

protected void onSuccess(Job job) {	
success 

protected void onFailure(Job job) {	
failure 

synchronized (mJobs) {	jobStats = mJobs.poll();	}	while (jobStats != null) {	Job job = jobStats.getJob();	try {	long start = System.currentTimeMillis();	JobStatus status = job.getStatus();	long end = System.currentTimeMillis();	if (LOG.isDebugEnabled()) {	
status polling for job took ms 

jobStats = mJobs.poll();	}	}	try {	TimeUnit.MILLISECONDS.sleep(pollDelayMillis);	} catch (InterruptedException e) {	shutdown = true;	continue;	}	} catch (Throwable e) {	
unexpected exception 

========================= hadoop sample_6122 =========================

public void setup() throws Exception {	localFS = FileContext.getLocalFSFileContext();	localActiveDir = new File("target", this.getClass().getSimpleName() + "-activeDir") .getAbsoluteFile();	localFS.delete(new Path(localActiveDir.getAbsolutePath()), true);	localActiveDir.mkdir();	
created activedir in 

========================= hadoop sample_2161 =========================

public Void answer(InvocationOnMock invocation) throws Throwable {	Object[] args = invocation.getArguments();	StorageDirectory sd = (StorageDirectory)args[1];	if (count++ == 1) {	
injecting fault for sd 

public Void answer(InvocationOnMock invocation) throws Throwable {	Object[] args = invocation.getArguments();	StorageDirectory sd = (StorageDirectory)args[1];	if (count++ == 1) {	if (throwRTE) {	throw new RuntimeException("Injected fault: saveFSImage second time");	} else {	throw new IOException("Injected fault: saveFSImage second time");	}	}	
not injecting fault for sd 

public Void answer(InvocationOnMock invocation) throws Throwable {	Object[] args = invocation.getArguments();	StorageDirectory sd = (StorageDirectory)args[0];	if (faultType == Fault.WRITE_STORAGE_ALL || (faultType==Fault.WRITE_STORAGE_ONE && count++==1)) {	
injecting fault for sd 

public Void answer(InvocationOnMock invocation) throws Throwable {	Object[] args = invocation.getArguments();	StorageDirectory sd = (StorageDirectory)args[0];	if (faultType == Fault.WRITE_STORAGE_ALL || (faultType==Fault.WRITE_STORAGE_ONE && count++==1)) {	throw new IOException("Injected fault: writeProperties second time");	}	
not injecting fault for sd 

fsn.setSafeMode(SafeModeAction.SAFEMODE_ENTER);	try {	fsn.saveNamespace();	if (shouldFail) {	fail("Did not fail!");	}	} catch (Exception e) {	if (! shouldFail) {	throw e;	} else {	
test caught expected exception 

Whitebox.setInternalState(fsn, "fsImage", spyImage);	FileSystem fs = FileSystem.getLocal(conf);	File rootDir = storage.getStorageDir(0).getRoot();	Path rootPath = new Path(rootDir.getPath(), "current");	final FsPermission permissionNone = new FsPermission((short) 0);	final FsPermission permissionAll = new FsPermission( FsAction.ALL, FsAction.READ_EXECUTE, FsAction.READ_EXECUTE);	fs.setPermission(rootPath, permissionNone);	try {	doAnEdit(fsn, 1);	fsn.setSafeMode(SafeModeAction.SAFEMODE_ENTER);	
doing the first savenamespace 

FileSystem fs = FileSystem.getLocal(conf);	File rootDir = storage.getStorageDir(0).getRoot();	Path rootPath = new Path(rootDir.getPath(), "current");	final FsPermission permissionNone = new FsPermission((short) 0);	final FsPermission permissionAll = new FsPermission( FsAction.ALL, FsAction.READ_EXECUTE, FsAction.READ_EXECUTE);	fs.setPermission(rootPath, permissionNone);	try {	doAnEdit(fsn, 1);	fsn.setSafeMode(SafeModeAction.SAFEMODE_ENTER);	fsn.saveNamespace();	
first savenamespace sucessful 

Path rootPath = new Path(rootDir.getPath(), "current");	final FsPermission permissionNone = new FsPermission((short) 0);	final FsPermission permissionAll = new FsPermission( FsAction.ALL, FsAction.READ_EXECUTE, FsAction.READ_EXECUTE);	fs.setPermission(rootPath, permissionNone);	try {	doAnEdit(fsn, 1);	fsn.setSafeMode(SafeModeAction.SAFEMODE_ENTER);	fsn.saveNamespace();	assertTrue("Savenamespace should have marked one directory as bad." + " But found " + storage.getRemovedStorageDirs().size() + " bad directories.", storage.getRemovedStorageDirs().size() == 1);	fs.setPermission(rootPath, permissionAll);	
doing the second savenamespace 

final FsPermission permissionNone = new FsPermission((short) 0);	final FsPermission permissionAll = new FsPermission( FsAction.ALL, FsAction.READ_EXECUTE, FsAction.READ_EXECUTE);	fs.setPermission(rootPath, permissionNone);	try {	doAnEdit(fsn, 1);	fsn.setSafeMode(SafeModeAction.SAFEMODE_ENTER);	fsn.saveNamespace();	assertTrue("Savenamespace should have marked one directory as bad." + " But found " + storage.getRemovedStorageDirs().size() + " bad directories.", storage.getRemovedStorageDirs().size() == 1);	fs.setPermission(rootPath, permissionAll);	fsn.saveNamespace();	
second savenamespace sucessful 

final FsPermission permissionAll = new FsPermission( FsAction.ALL, FsAction.READ_EXECUTE, FsAction.READ_EXECUTE);	fs.setPermission(rootPath, permissionNone);	try {	doAnEdit(fsn, 1);	fsn.setSafeMode(SafeModeAction.SAFEMODE_ENTER);	fsn.saveNamespace();	assertTrue("Savenamespace should have marked one directory as bad." + " But found " + storage.getRemovedStorageDirs().size() + " bad directories.", storage.getRemovedStorageDirs().size() == 1);	fs.setPermission(rootPath, permissionAll);	fsn.saveNamespace();	assertTrue("Savenamespace should have been successful in removing " + " bad directories from Image."  + " But found " + storage.getRemovedStorageDirs().size() + " bad directories.", storage.getRemovedStorageDirs().size() == 0);	
shutting down fsimage 

doAnEdit(fsn, 1);	fsn.setSafeMode(SafeModeAction.SAFEMODE_ENTER);	fsn.saveNamespace();	assertTrue("Savenamespace should have marked one directory as bad." + " But found " + storage.getRemovedStorageDirs().size() + " bad directories.", storage.getRemovedStorageDirs().size() == 1);	fs.setPermission(rootPath, permissionAll);	fsn.saveNamespace();	assertTrue("Savenamespace should have been successful in removing " + " bad directories from Image."  + " But found " + storage.getRemovedStorageDirs().size() + " bad directories.", storage.getRemovedStorageDirs().size() == 0);	originalImage.close();	fsn.close();	fsn = null;	
loading new fsmage from disk 

fsn.setSafeMode(SafeModeAction.SAFEMODE_ENTER);	fsn.saveNamespace();	assertTrue("Savenamespace should have marked one directory as bad." + " But found " + storage.getRemovedStorageDirs().size() + " bad directories.", storage.getRemovedStorageDirs().size() == 1);	fs.setPermission(rootPath, permissionAll);	fsn.saveNamespace();	assertTrue("Savenamespace should have been successful in removing " + " bad directories from Image."  + " But found " + storage.getRemovedStorageDirs().size() + " bad directories.", storage.getRemovedStorageDirs().size() == 0);	originalImage.close();	fsn.close();	fsn = null;	fsn = FSNamesystem.loadFromDisk(conf);	
checking reloaded image 

fsn.saveNamespace();	assertTrue("Savenamespace should have marked one directory as bad." + " But found " + storage.getRemovedStorageDirs().size() + " bad directories.", storage.getRemovedStorageDirs().size() == 1);	fs.setPermission(rootPath, permissionAll);	fsn.saveNamespace();	assertTrue("Savenamespace should have been successful in removing " + " bad directories from Image."  + " But found " + storage.getRemovedStorageDirs().size() + " bad directories.", storage.getRemovedStorageDirs().size() == 0);	originalImage.close();	fsn.close();	fsn = null;	fsn = FSNamesystem.loadFromDisk(conf);	checkEditExists(fsn, 1);	
reloaded image is good 

fsn = FSNamesystem.loadFromDisk(conf);	checkEditExists(fsn, 1);	} finally {	if (rootDir.exists()) {	fs.setPermission(rootPath, permissionAll);	}	if (fsn != null) {	try {	fsn.close();	} catch (Throwable t) {	
failed to shut down 

Whitebox.setInternalState(fsn, "fsImage", spyImage);	spyImage.storage.setStorageDirectories( FSNamesystem.getNamespaceDirs(conf), FSNamesystem.getNamespaceEditsDirs(conf));	doThrow(new IOException("Injected fault: saveFSImage")). when(spyImage).saveFSImage( (SaveNamespaceContext)anyObject(), (StorageDirectory)anyObject(), (NameNodeFile) anyObject());	try {	doAnEdit(fsn, 1);	fsn.setSafeMode(SafeModeAction.SAFEMODE_ENTER);	try {	fsn.saveNamespace();	fail("saveNamespace did not fail even when all directories failed!");	} catch (IOException ioe) {	
got expected exception 

public void testSaveWhileEditsRolled() throws Exception {	Configuration conf = getConf();	NameNode.initMetrics(conf, NamenodeRole.NAMENODE);	DFSTestUtil.formatNameNode(conf);	FSNamesystem fsn = FSNamesystem.loadFromDisk(conf);	try {	doAnEdit(fsn, 1);	CheckpointSignature sig = fsn.rollEditLog();	
checkpoint signature 

}	});	Thread.sleep(500);	delayer.proceed();	cancelFuture.get();	saverFuture.get();	fail("saveNamespace did not fail even though cancelled!");	} catch (Throwable t) {	GenericTestUtils.assertExceptionContains( "SaveNamespaceCancelledException", t);	}	
successfully cancelled a savenamespace 

========================= hadoop sample_7363 =========================

outputStream.close();	long uploadDurationMs = new Date().getTime() - start.getTime();	logOpResponseCount("Creating a 1K file", base);	base = assertWebResponsesInRange(base, 2, 15);	getBandwidthGaugeUpdater().triggerUpdate(true);	long bytesWritten = AzureMetricsTestUtil.getCurrentBytesWritten(getInstrumentation());	assertTrue("The bytes written in the last second " + bytesWritten + " is pretty far from the expected range of around " + FILE_SIZE + " bytes plus a little overhead.", bytesWritten > (FILE_SIZE / 2) && bytesWritten < (FILE_SIZE * 2));	long totalBytesWritten = AzureMetricsTestUtil.getCurrentTotalBytesWritten(getInstrumentation());	assertTrue("The total bytes written  " + totalBytesWritten + " is pretty far from the expected range of around " + FILE_SIZE + " bytes plus a little overhead.", totalBytesWritten >= FILE_SIZE && totalBytesWritten < (FILE_SIZE * 2));	long uploadRate = AzureMetricsTestUtil.getLongGaugeValue(getInstrumentation(), WASB_UPLOAD_RATE);	
upload rate bytes second 

base = assertWebResponsesInRange(base, 2, 15);	getBandwidthGaugeUpdater().triggerUpdate(true);	long bytesWritten = AzureMetricsTestUtil.getCurrentBytesWritten(getInstrumentation());	assertTrue("The bytes written in the last second " + bytesWritten + " is pretty far from the expected range of around " + FILE_SIZE + " bytes plus a little overhead.", bytesWritten > (FILE_SIZE / 2) && bytesWritten < (FILE_SIZE * 2));	long totalBytesWritten = AzureMetricsTestUtil.getCurrentTotalBytesWritten(getInstrumentation());	assertTrue("The total bytes written  " + totalBytesWritten + " is pretty far from the expected range of around " + FILE_SIZE + " bytes plus a little overhead.", totalBytesWritten >= FILE_SIZE && totalBytesWritten < (FILE_SIZE * 2));	long uploadRate = AzureMetricsTestUtil.getLongGaugeValue(getInstrumentation(), WASB_UPLOAD_RATE);	long expectedRate = (FILE_SIZE * 1000L) / uploadDurationMs;	assertTrue("The upload rate " + uploadRate + " is below the expected range of around " + expectedRate + " bytes/second that the unit test observed. This should never be" + " the case since the test underestimates the rate by looking at " + " end-to-end time instead of just block upload time.", uploadRate >= expectedRate);	long uploadLatency = AzureMetricsTestUtil.getLongGaugeValue(getInstrumentation(), WASB_UPLOAD_LATENCY);	
upload latency 

long downloadDurationMs = new Date().getTime() - start.getTime();	assertEquals(FILE_SIZE, count);	logOpResponseCount("Reading a 1K file", base);	base = assertWebResponsesInRange(base, 1, 10);	getBandwidthGaugeUpdater().triggerUpdate(false);	long totalBytesRead = AzureMetricsTestUtil.getCurrentTotalBytesRead(getInstrumentation());	assertEquals(FILE_SIZE, totalBytesRead);	long bytesRead = AzureMetricsTestUtil.getCurrentBytesRead(getInstrumentation());	assertTrue("The bytes read in the last second " + bytesRead + " is pretty far from the expected range of around " + FILE_SIZE + " bytes plus a little overhead.", bytesRead > (FILE_SIZE / 2) && bytesRead < (FILE_SIZE * 2));	long downloadRate = AzureMetricsTestUtil.getLongGaugeValue(getInstrumentation(), WASB_DOWNLOAD_RATE);	
download rate bytes second 

base = assertWebResponsesInRange(base, 1, 10);	getBandwidthGaugeUpdater().triggerUpdate(false);	long totalBytesRead = AzureMetricsTestUtil.getCurrentTotalBytesRead(getInstrumentation());	assertEquals(FILE_SIZE, totalBytesRead);	long bytesRead = AzureMetricsTestUtil.getCurrentBytesRead(getInstrumentation());	assertTrue("The bytes read in the last second " + bytesRead + " is pretty far from the expected range of around " + FILE_SIZE + " bytes plus a little overhead.", bytesRead > (FILE_SIZE / 2) && bytesRead < (FILE_SIZE * 2));	long downloadRate = AzureMetricsTestUtil.getLongGaugeValue(getInstrumentation(), WASB_DOWNLOAD_RATE);	expectedRate = (FILE_SIZE * 1000L) / downloadDurationMs;	assertTrue("The download rate " + downloadRate + " is below the expected range of around " + expectedRate + " bytes/second that the unit test observed. This should never be" + " the case since the test underestimates the rate by looking at " + " end-to-end time instead of just block download time.", downloadRate >= expectedRate);	long downloadLatency = AzureMetricsTestUtil.getLongGaugeValue(getInstrumentation(), WASB_DOWNLOAD_LATENCY);	
download latency 

getBandwidthGaugeUpdater().suppressAutoUpdate();	OutputStream outputStream = getFileSystem().create(filePath);	outputStream.write(new byte[FILE_SIZE]);	outputStream.close();	logOpResponseCount("Creating a 100 MB file", base);	base = assertWebResponsesInRange(base, 20, 50);	getBandwidthGaugeUpdater().triggerUpdate(true);	long totalBytesWritten = AzureMetricsTestUtil.getCurrentTotalBytesWritten(getInstrumentation());	assertTrue("The total bytes written  " + totalBytesWritten + " is pretty far from the expected range of around " + FILE_SIZE + " bytes plus a little overhead.", totalBytesWritten >= FILE_SIZE && totalBytesWritten < (FILE_SIZE * 2));	long uploadRate = AzureMetricsTestUtil.getLongGaugeValue(getInstrumentation(), WASB_UPLOAD_RATE);	
upload rate bytes second 

OutputStream outputStream = getFileSystem().create(filePath);	outputStream.write(new byte[FILE_SIZE]);	outputStream.close();	logOpResponseCount("Creating a 100 MB file", base);	base = assertWebResponsesInRange(base, 20, 50);	getBandwidthGaugeUpdater().triggerUpdate(true);	long totalBytesWritten = AzureMetricsTestUtil.getCurrentTotalBytesWritten(getInstrumentation());	assertTrue("The total bytes written  " + totalBytesWritten + " is pretty far from the expected range of around " + FILE_SIZE + " bytes plus a little overhead.", totalBytesWritten >= FILE_SIZE && totalBytesWritten < (FILE_SIZE * 2));	long uploadRate = AzureMetricsTestUtil.getLongGaugeValue(getInstrumentation(), WASB_UPLOAD_RATE);	long uploadLatency = AzureMetricsTestUtil.getLongGaugeValue(getInstrumentation(), WASB_UPLOAD_LATENCY);	
upload latency 

count++;	}	inputStream.close();	assertEquals(FILE_SIZE, count);	logOpResponseCount("Reading a 100 MB file", base);	base = assertWebResponsesInRange(base, 20, 40);	getBandwidthGaugeUpdater().triggerUpdate(false);	long totalBytesRead = AzureMetricsTestUtil.getCurrentTotalBytesRead(getInstrumentation());	assertEquals(FILE_SIZE, totalBytesRead);	long downloadRate = AzureMetricsTestUtil.getLongGaugeValue(getInstrumentation(), WASB_DOWNLOAD_RATE);	
download rate bytes second 

}	inputStream.close();	assertEquals(FILE_SIZE, count);	logOpResponseCount("Reading a 100 MB file", base);	base = assertWebResponsesInRange(base, 20, 40);	getBandwidthGaugeUpdater().triggerUpdate(false);	long totalBytesRead = AzureMetricsTestUtil.getCurrentTotalBytesRead(getInstrumentation());	assertEquals(FILE_SIZE, totalBytesRead);	long downloadRate = AzureMetricsTestUtil.getLongGaugeValue(getInstrumentation(), WASB_DOWNLOAD_RATE);	long downloadLatency = AzureMetricsTestUtil.getLongGaugeValue(getInstrumentation(), WASB_DOWNLOAD_LATENCY);	
download latency 

========================= hadoop sample_6319 =========================

if (rmContainerInFinalState(r.getRmContainer())) {	if (LOG.isDebugEnabled()) {	LOG.debug("To-release container=" + r.getRmContainer() + ", for to a new allocated container, is in final state");	}	return true;	}	}	if (null != c.getAllocateFromReservedContainer()) {	if (rmContainerInFinalState( c.getAllocateFromReservedContainer().getRmContainer())) {	if (LOG.isDebugEnabled()) {	
allocate from reserved container is in final state 

private boolean commonCheckContainerAllocation( Resource cluster, ContainerAllocationProposal<FiCaSchedulerApp, FiCaSchedulerNode> allocation, SchedulerContainer<FiCaSchedulerApp, FiCaSchedulerNode> schedulerContainer) {	RMContainer reservedContainerOnNode = schedulerContainer.getSchedulerNode().getReservedContainer();	if (reservedContainerOnNode != null) {	RMContainer fromReservedContainer = allocation.getAllocateFromReservedContainer().getRmContainer();	if (fromReservedContainer != reservedContainerOnNode) {	if (LOG.isDebugEnabled()) {	
try to allocate from a non existed reserved container 

if (LOG.isDebugEnabled()) {	LOG.debug("No pending resource for: nodeType=" + allocation .getAllocationLocalityType() + ", node=" + schedulerContainer .getSchedulerNode() + ", requestKey=" + schedulerContainer .getSchedulerRequestKey() + ", application=" + getApplicationAttemptId());	}	return false;	}	commonCheckContainerAllocation(cluster, allocation, schedulerContainer);	} else {	if (schedulerContainer.getRmContainer().getState() == RMContainerState.RESERVED) {	if (schedulerContainer.getRmContainer() != schedulerContainer .getSchedulerNode().getReservedContainer()) {	if (LOG.isDebugEnabled()) {	
try to re reserve a container but node is already reserved by another container 

public NodeId getNodeIdToUnreserve( SchedulerRequestKey schedulerKey, Resource resourceNeedUnreserve, ResourceCalculator rc, Resource clusterResource) {	Map<NodeId, RMContainer> reservedContainers = this.reservedContainers.get( schedulerKey);	if ((reservedContainers != null) && (!reservedContainers.isEmpty())) {	for (Map.Entry<NodeId, RMContainer> entry : reservedContainers .entrySet()) {	NodeId nodeId = entry.getKey();	RMContainer reservedContainer = entry.getValue();	Resource reservedResource = reservedContainer.getReservedResource();	if (Resources.fitsIn(rc, clusterResource, resourceNeedUnreserve, reservedResource)) {	if (LOG.isDebugEnabled()) {	
unreserving node with reservation size in order to allocate container with size 

public RMContainer findNodeToUnreserve(Resource clusterResource, FiCaSchedulerNode node, SchedulerRequestKey schedulerKey, Resource minimumUnreservedResource) {	try {	readLock.lock();	NodeId idToUnreserve = getNodeIdToUnreserve(schedulerKey, minimumUnreservedResource, rc, clusterResource);	if (idToUnreserve == null) {	if (LOG.isDebugEnabled()) {	
checked to see if could unreserve for app but nothing reserved that matches for this app 

try {	readLock.lock();	NodeId idToUnreserve = getNodeIdToUnreserve(schedulerKey, minimumUnreservedResource, rc, clusterResource);	if (idToUnreserve == null) {	if (LOG.isDebugEnabled()) {	}	return null;	}	FiCaSchedulerNode nodeToUnreserve = ((CapacityScheduler) scheduler).getNode(idToUnreserve);	if (nodeToUnreserve == null) {	
node to unreserve doesn t exist nodeid 

if (idToUnreserve == null) {	if (LOG.isDebugEnabled()) {	}	return null;	}	FiCaSchedulerNode nodeToUnreserve = ((CapacityScheduler) scheduler).getNode(idToUnreserve);	if (nodeToUnreserve == null) {	return null;	}	if (LOG.isDebugEnabled()) {	
unreserving for app on nodeid in order to replace reserved application and place it on node needing 

public CSAssignment assignContainers(Resource clusterResource, PlacementSet<FiCaSchedulerNode> ps, ResourceLimits currentResourceLimits, SchedulingMode schedulingMode, RMContainer reservedContainer) {	if (LOG.isDebugEnabled()) {	
pre assigncontainers for application 

public boolean moveReservation(RMContainer reservedContainer, FiCaSchedulerNode sourceNode, FiCaSchedulerNode targetNode) {	try {	writeLock.lock();	if (!sourceNode.getPartition().equals(targetNode.getPartition())) {	if (LOG.isDebugEnabled()) {	
failed to move reservation two nodes are in different partition 

try {	writeLock.lock();	if (!sourceNode.getPartition().equals(targetNode.getPartition())) {	if (LOG.isDebugEnabled()) {	}	return false;	}	Map<NodeId, RMContainer> map = reservedContainers.get( reservedContainer.getReservedSchedulerKey());	if (null == map) {	if (LOG.isDebugEnabled()) {	
cannot find reserved container map 

return false;	}	Map<NodeId, RMContainer> map = reservedContainers.get( reservedContainer.getReservedSchedulerKey());	if (null == map) {	if (LOG.isDebugEnabled()) {	}	return false;	}	if (sourceNode.getReservedContainer() != reservedContainer) {	if (LOG.isDebugEnabled()) {	
to be moved container already updated 

return false;	}	if (sourceNode.getReservedContainer() != reservedContainer) {	if (LOG.isDebugEnabled()) {	}	return false;	}	synchronized (targetNode){	if (targetNode.getReservedContainer() != null) {	if (LOG.isDebugEnabled()) {	
target node is already occupied before moving 

========================= hadoop sample_942 =========================

String[] property = StringUtils.getStrings(param, ":");	if (property == null || property.length != 2) {	continue;	}	if (property[0].equals(SAMPLE_RATE)) {	try {	float sampleRate = Float.parseFloat(property[1]);	if (sampleRate >= 0.0 && sampleRate <= 1.0) {	this.sampleRate = sampleRate;	} else {	
the format isn t valid sample rate falls back to the default value 

continue;	}	if (property[0].equals(SAMPLE_RATE)) {	try {	float sampleRate = Float.parseFloat(property[1]);	if (sampleRate >= 0.0 && sampleRate <= 1.0) {	this.sampleRate = sampleRate;	} else {	}	} catch (NumberFormatException nfe) {	
the format isn t valid sample rate falls back to the default value 

} else {	}	} catch (NumberFormatException nfe) {	}	} else if (property[0].equals(MIN_THRESHOLD)) {	try {	int minThreshold = Integer.parseInt(property[1]);	if (minThreshold >= 0) {	this.minThreshold = minThreshold;	} else {	
the format isn t valid min threshold falls back to the default value 

} catch (NumberFormatException nfe) {	}	} else if (property[0].equals(MIN_THRESHOLD)) {	try {	int minThreshold = Integer.parseInt(property[1]);	if (minThreshold >= 0) {	this.minThreshold = minThreshold;	} else {	}	} catch (NumberFormatException nfe) {	
the format isn t valid min threshold falls back to the default value 

========================= hadoop sample_1825 =========================

private void cleanupBeforeTestrun() throws IOException {	FileSystem tempFS = FileSystem.get(getConf());	if (operation.equals(OP_CREATE_WRITE)) {	
deleting data directory 

private void createControlFiles() throws IOException {	
creating control files 

checkArgs(i + 1, args.length);	baseDir = args[++i];	} else if (args[i].equals("-readFileAfterOpen")) {	checkArgs(i + 1, args.length);	readFileAfterOpen = Boolean.parseBoolean(args[++i]);	} else if (args[i].equals("-help")) {	displayUsage();	isHelpMessage = true;	}	}	
test inputs 

checkArgs(i + 1, args.length);	baseDir = args[++i];	} else if (args[i].equals("-readFileAfterOpen")) {	checkArgs(i + 1, args.length);	readFileAfterOpen = Boolean.parseBoolean(args[++i]);	} else if (args[i].equals("-help")) {	displayUsage();	isHelpMessage = true;	}	}	
test operation 

checkArgs(i + 1, args.length);	baseDir = args[++i];	} else if (args[i].equals("-readFileAfterOpen")) {	checkArgs(i + 1, args.length);	readFileAfterOpen = Boolean.parseBoolean(args[++i]);	} else if (args[i].equals("-help")) {	displayUsage();	isHelpMessage = true;	}	}	
start time 

checkArgs(i + 1, args.length);	baseDir = args[++i];	} else if (args[i].equals("-readFileAfterOpen")) {	checkArgs(i + 1, args.length);	readFileAfterOpen = Boolean.parseBoolean(args[++i]);	} else if (args[i].equals("-help")) {	displayUsage();	isHelpMessage = true;	}	}	
number of maps 

checkArgs(i + 1, args.length);	baseDir = args[++i];	} else if (args[i].equals("-readFileAfterOpen")) {	checkArgs(i + 1, args.length);	readFileAfterOpen = Boolean.parseBoolean(args[++i]);	} else if (args[i].equals("-help")) {	displayUsage();	isHelpMessage = true;	}	}	
number of reduces 

checkArgs(i + 1, args.length);	baseDir = args[++i];	} else if (args[i].equals("-readFileAfterOpen")) {	checkArgs(i + 1, args.length);	readFileAfterOpen = Boolean.parseBoolean(args[++i]);	} else if (args[i].equals("-help")) {	displayUsage();	isHelpMessage = true;	}	}	
block size 

checkArgs(i + 1, args.length);	baseDir = args[++i];	} else if (args[i].equals("-readFileAfterOpen")) {	checkArgs(i + 1, args.length);	readFileAfterOpen = Boolean.parseBoolean(args[++i]);	} else if (args[i].equals("-help")) {	displayUsage();	isHelpMessage = true;	}	}	
bytes to write 

checkArgs(i + 1, args.length);	baseDir = args[++i];	} else if (args[i].equals("-readFileAfterOpen")) {	checkArgs(i + 1, args.length);	readFileAfterOpen = Boolean.parseBoolean(args[++i]);	} else if (args[i].equals("-help")) {	displayUsage();	isHelpMessage = true;	}	}	
bytes per checksum 

checkArgs(i + 1, args.length);	baseDir = args[++i];	} else if (args[i].equals("-readFileAfterOpen")) {	checkArgs(i + 1, args.length);	readFileAfterOpen = Boolean.parseBoolean(args[++i]);	} else if (args[i].equals("-help")) {	displayUsage();	isHelpMessage = true;	}	}	
number of files 

checkArgs(i + 1, args.length);	baseDir = args[++i];	} else if (args[i].equals("-readFileAfterOpen")) {	checkArgs(i + 1, args.length);	readFileAfterOpen = Boolean.parseBoolean(args[++i]);	} else if (args[i].equals("-help")) {	displayUsage();	isHelpMessage = true;	}	}	
replication factor 

checkArgs(i + 1, args.length);	baseDir = args[++i];	} else if (args[i].equals("-readFileAfterOpen")) {	checkArgs(i + 1, args.length);	readFileAfterOpen = Boolean.parseBoolean(args[++i]);	} else if (args[i].equals("-help")) {	displayUsage();	isHelpMessage = true;	}	}	
base dir 

checkArgs(i + 1, args.length);	baseDir = args[++i];	} else if (args[i].equals("-readFileAfterOpen")) {	checkArgs(i + 1, args.length);	readFileAfterOpen = Boolean.parseBoolean(args[++i]);	} else if (args[i].equals("-help")) {	displayUsage();	isHelpMessage = true;	}	}	
read file after open 

private boolean barrier() {	long startTime = getConf().getLong("test.nnbench.starttime", 0l);	long currentTime = System.currentTimeMillis();	long sleepTime = startTime - currentTime;	boolean retVal = false;	if (sleepTime > 0) {	
waiting in barrier for ms 

out = filesystem.create(filePath, true, 512, replFactor, blkSize);	out.write(buffer);	totalTimeAL1 += (System.currentTimeMillis() - startTimeAL);	startTimeAL = System.currentTimeMillis();	out.close();	totalTimeAL2 += (System.currentTimeMillis() - startTimeAL);	successfulOp = true;	successfulFileOps ++;	reporter.setStatus("Finish "+ l + " files");	} catch (IOException e) {	
exception recorded in op create write close file filepath 

if (readFile) {	startTimeAL = System.currentTimeMillis();	input.readFully(buffer);	totalTimeAL2 += (System.currentTimeMillis() - startTimeAL);	}	input.close();	successfulOp = true;	successfulFileOps ++;	reporter.setStatus("Finish "+ l + " files");	} catch (IOException e) {	
exception recorded in op openread file filepath 

startTimeAL = System.currentTimeMillis();	boolean result = filesystem.rename(filePath, filePathR);	if (!result) {	throw new IOException("rename failed for " + filePath);	}	totalTimeAL1 += (System.currentTimeMillis() - startTimeAL);	successfulOp = true;	successfulFileOps ++;	reporter.setStatus("Finish "+ l + " files");	} catch (IOException e) {	
exception recorded in op rename file filepath 

startTimeAL = System.currentTimeMillis();	boolean result = filesystem.delete(filePath, true);	if (!result) {	throw new IOException("delete failed for " + filePath);	}	totalTimeAL1 += (System.currentTimeMillis() - startTimeAL);	successfulOp = true;	successfulFileOps ++;	reporter.setStatus("Finish "+ l + " files");	} catch (IOException e) {	
exception recorded in op delete file filepath 

public NNBenchReducer () {	
starting nnbenchreducer 

public NNBenchReducer () {	try {	hostName = java.net.InetAddress.getLocalHost().getHostName();	} catch(Exception e) {	hostName = "localhost";	}	
starting nnbenchreducer on 

========================= hadoop sample_5535 =========================

public static void main(String[] args) throws Throwable {	Thread.setDefaultUncaughtExceptionHandler(new YarnUncaughtExceptionHandler());	
child starting 

SecurityUtil.setConfiguration(job);	String host = args[0];	int port = Integer.parseInt(args[1]);	final InetSocketAddress address = NetUtils.createSocketAddrForHost(host, port);	final TaskAttemptID firstTaskid = TaskAttemptID.forName(args[2]);	long jvmIdLong = Long.parseLong(args[3]);	JVMId jvmId = new JVMId(firstTaskid.getJobID(), firstTaskid.getTaskType() == TaskType.MAP, jvmIdLong);	CallerContext.setCurrent( new CallerContext.Builder("mr_" + firstTaskid.toString()).build());	DefaultMetricsSystem.initialize( StringUtils.camelize(firstTaskid.getTaskType().name()) +"Task");	Credentials credentials = UserGroupInformation.getCurrentUser().getCredentials();	
executing with tokens 

UserGroupInformation taskOwner = UserGroupInformation.createRemoteUser(firstTaskid.getJobID().toString());	Token<JobTokenIdentifier> jt = TokenCache.getJobToken(credentials);	SecurityUtil.setTokenService(jt, address);	taskOwner.addToken(jt);	final TaskUmbilicalProtocol umbilical = taskOwner.doAs(new PrivilegedExceptionAction<TaskUmbilicalProtocol>() {	public TaskUmbilicalProtocol run() throws Exception {	return (TaskUmbilicalProtocol)RPC.getProxy(TaskUmbilicalProtocol.class, TaskUmbilicalProtocol.versionID, address, job);	}	});	JvmContext context = new JvmContext(jvmId, "-1000");	
pid jvm pid 

});	JvmContext context = new JvmContext(jvmId, "-1000");	Task task = null;	UserGroupInformation childUGI = null;	ScheduledExecutorService logSyncer = null;	try {	int idleLoopCount = 0;	JvmTask myTask = null;;	for (int idle = 0; null == myTask; ++idle) {	long sleepTimeMilliSecs = Math.min(idle * 500, 1500);	
sleeping for ms before retrying again got null now 

final Task taskFinal = task;	childUGI.doAs(new PrivilegedExceptionAction<Object>() {	public Object run() throws Exception {	setEncryptedSpillKeyIfRequired(taskFinal);	FileSystem.get(job).setWorkingDirectory(job.getWorkingDirectory());	taskFinal.run(job, umbilical);	return null;	}	});	} catch (FSError e) {	
fserror from child 

FileSystem.get(job).setWorkingDirectory(job.getWorkingDirectory());	taskFinal.run(job, umbilical);	return null;	}	});	} catch (FSError e) {	if (!ShutdownHookManager.get().isShutdownInProgress()) {	umbilical.fsError(taskid, e.getMessage());	}	} catch (Exception exception) {	
exception running child 

final Task taskFinal = task;	childUGI.doAs(new PrivilegedExceptionAction<Object>() {	public Object run() throws Exception {	taskFinal.taskCleanup(umbilical);	return null;	}	});	}	}	} catch (Exception e) {	
exception cleaning up 

}	}	} catch (Exception e) {	}	if (taskid != null) {	if (!ShutdownHookManager.get().isShutdownInProgress()) {	umbilical.fatalError(taskid, StringUtils.stringifyException(exception));	}	}	} catch (Throwable throwable) {	
error running child 

private static void configureLocalDirs(Task task, JobConf job) throws IOException {	String[] localSysDirs = StringUtils.getTrimmedStrings( System.getenv(Environment.LOCAL_DIRS.name()));	job.setStrings(MRConfig.LOCAL_DIR, localSysDirs);	
for child 

private static void configureTask(JobConf job, Task task, Credentials credentials, Token<JobTokenIdentifier> jt) throws IOException {	job.setCredentials(credentials);	ApplicationAttemptId appAttemptId = ContainerId.fromString( System.getenv(Environment.CONTAINER_ID.name())) .getApplicationAttemptId();	
application attempt id 

private static void configureTask(JobConf job, Task task, Credentials credentials, Token<JobTokenIdentifier> jt) throws IOException {	job.setCredentials(credentials);	ApplicationAttemptId appAttemptId = ContainerId.fromString( System.getenv(Environment.CONTAINER_ID.name())) .getApplicationAttemptId();	job.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptId.getAttemptId());	job.setBoolean("ipc.client.tcpnodelay", true);	job.setClass(MRConfig.TASK_LOCAL_OUTPUT_CLASS, YarnOutputFiles.class, MapOutputFile.class);	task.setJobTokenSecret( JobTokenSecretManager.createSecretKey(jt.getPassword()));	byte[] shuffleSecret = TokenCache.getShuffleSecretKey(credentials);	if (shuffleSecret == null) {	
shuffle secret missing from task credentials using job token secret as shuffle secret 

========================= hadoop sample_5165 =========================

return TimelineUtils.createTimelineAbout("Timeline API");	}	public TimelineEntities getEntities( init(res);	try {	return timelineDataManager.getEntities( parseStr(entityType), parsePairStr(primaryFilter, ":"), parsePairsStr(secondaryFilter, ",", ":"), parseLongStr(windowStart), parseLongStr(windowEnd), parseStr(fromId), parseLongStr(fromTs), parseLongStr(limit), parseFieldsStr(fields, ","), getUser(req));	} catch (NumberFormatException e) {	throw new BadRequestException( "windowStart, windowEnd, fromTs or limit is not a numeric value: " + e);	} catch (IllegalArgumentException e) {	throw new BadRequestException("requested invalid field: " + e);	} catch (Exception e) {	
error getting entities 

public TimelineEntity getEntity( init(res);	TimelineEntity entity = null;	try {	entity = timelineDataManager.getEntity( parseStr(entityType), parseStr(entityId), parseFieldsStr(fields, ","), getUser(req));	} catch (IllegalArgumentException e) {	throw new BadRequestException(e);	} catch (Exception e) {	
error getting entity 

public TimelineEvents getEvents( init(res);	try {	return timelineDataManager.getEvents( parseStr(entityType), parseArrayStr(entityId, ","), parseArrayStr(eventType, ","), parseLongStr(windowStart), parseLongStr(windowEnd), parseLongStr(limit), getUser(req));	} catch (NumberFormatException e) {	throw (BadRequestException)new BadRequestException( "windowStart, windowEnd or limit is not a numeric value.") .initCause(e);	} catch (Exception e) {	
error getting entity timelines 

if (callerUGI == null) {	String msg = "The owner of the posted timeline entities is not set";	LOG.error(msg);	throw new ForbiddenException(msg);	}	try {	return timelineDataManager.postEntities(entities, callerUGI);	} catch (BadRequestException bre) {	throw bre;	} catch (Exception e) {	
error putting entities 

LOG.error(msg);	throw new ForbiddenException(msg);	}	domain.setOwner(callerUGI.getShortUserName());	try {	timelineDataManager.putDomain(domain, callerUGI);	} catch (YarnException e) {	LOG.error(e.getMessage(), e);	throw new ForbiddenException(e);	} catch (RuntimeException e) {	
error putting domain 

}	domain.setOwner(callerUGI.getShortUserName());	try {	timelineDataManager.putDomain(domain, callerUGI);	} catch (YarnException e) {	LOG.error(e.getMessage(), e);	throw new ForbiddenException(e);	} catch (RuntimeException e) {	throw new WebApplicationException(e, Response.Status.INTERNAL_SERVER_ERROR);	} catch (IOException e) {	
error putting domain 

========================= hadoop sample_2073 =========================

public synchronized void addProcessor( ApplicationMasterServiceProcessor processor) {	
adding tp top of ams processing chain 

========================= hadoop sample_738 =========================

protected final LocatedBlocks ensureFileReplicasOnStorageType( final Path path, final StorageType storageType) throws IOException, TimeoutException, InterruptedException {	
ensure path is on storagetype 

public Boolean get() {	try {	LocatedBlocks locatedBlocks = client.getLocatedBlocks(path.toString(), 0, fileLength);	for (LocatedBlock locatedBlock : locatedBlocks.getLocatedBlocks()) {	if (locatedBlock.getStorageTypes()[0] != storageType) {	return false;	}	}	return true;	} catch (IOException ioe) {	
exception got in ensurefilereplicasonstoragetype 

capacities = new long[] { ramDiskStorageLimit, -1 };	cluster = new MiniDFSCluster .Builder(conf) .numDataNodes(numDatanodes) .storageCapacities(capacities) .storageTypes(storageTypes != null ? storageTypes : (hasTransientStorage ? new StorageType[]{RAM_DISK, DEFAULT} : null)) .build();	cluster.waitActive();	fs = cluster.getFileSystem();	client = fs.getClient();	try {	jmx = initJMX();	} catch (Exception e) {	fail("Failed initialize JMX for testing: " + e);	}	
cluster startup complete 

public static void initCacheManipulator() {	NativeIO.POSIX.setCacheManipulator(new NativeIO.POSIX.CacheManipulator() {	public void mlock(String identifier, ByteBuffer mmap, long length) throws IOException {	
lazypersisttestcase faking mlock of bytes 

public static void initCacheManipulator() {	NativeIO.POSIX.setCacheManipulator(new NativeIO.POSIX.CacheManipulator() {	public void mlock(String identifier, ByteBuffer mmap, long length) throws IOException {	}	public long getMemlockLimit() {	
lazypersisttestcase fake return 

public static void initCacheManipulator() {	NativeIO.POSIX.setCacheManipulator(new NativeIO.POSIX.CacheManipulator() {	public void mlock(String identifier, ByteBuffer mmap, long length) throws IOException {	}	public long getMemlockLimit() {	return Long.MAX_VALUE;	}	public boolean verifyCanMlock() {	
lazypersisttestcase fake return 

protected final boolean verifyBlockDeletedFromDir(File dir, LocatedBlocks locatedBlocks) {	for (LocatedBlock lb : locatedBlocks.getLocatedBlocks()) {	File targetDir = DatanodeUtil.idToBlockDir(dir, lb.getBlock().getBlockId());	File blockFile = new File(targetDir, lb.getBlock().getBlockName());	if (blockFile.exists()) {	
blockfile exists after deletion 

protected final boolean verifyBlockDeletedFromDir(File dir, LocatedBlocks locatedBlocks) {	for (LocatedBlock lb : locatedBlocks.getLocatedBlocks()) {	File targetDir = DatanodeUtil.idToBlockDir(dir, lb.getBlock().getBlockId());	File blockFile = new File(targetDir, lb.getBlock().getBlockName());	if (blockFile.exists()) {	return false;	}	File metaFile = new File(targetDir, DatanodeUtil.getMetaName(lb.getBlock().getBlockName(), lb.getBlock().getGenerationStamp()));	if (metaFile.exists()) {	
metafile exists after deletion 

protected final boolean verifyDeletedBlocks(LocatedBlocks locatedBlocks) throws IOException, InterruptedException {	
verifying replica has no saved copy after deletion 

========================= hadoop sample_7271 =========================

public TimelineEntity getEntity(TimelineReaderContext context, TimelineDataToRetrieve dataToRetrieve) throws IOException {	String flowRunPath = getFlowRunPath(context.getUserId(), context.getClusterId(), context.getFlowName(), context.getFlowRunId(), context.getAppId());	File dir = new File(new File(rootPath, ENTITIES_DIR), context.getClusterId() + File.separator + flowRunPath + File.separator + context.getAppId() + File.separator + context.getEntityType());	File entityFile = new File( dir, context.getEntityId() + TIMELINE_SERVICE_STORAGE_EXTENSION);	try (BufferedReader reader = new BufferedReader(new InputStreamReader( new FileInputStream(entityFile), Charset.forName("UTF-8")))) {	TimelineEntity entity = readEntityFromFile(reader);	return createEntityToBeReturned( entity, dataToRetrieve.getFieldsToRetrieve());	} catch (FileNotFoundException e) {	
cannot find entity id type will send http in response 

========================= hadoop sample_368 =========================

long totalbytes = 0L;	int totalrecs = 0;	for (int i = 0; i < recs.length; ++i) {	recs[i] = (int) Math.round(tot_recs * recs_dist[i]);	bytes[i] = Math.round(tot_bytes * bytes_dist[i]);	totalrecs += recs[i];	totalbytes += bytes[i];	}	recs[0] += totalrecs - tot_recs;	bytes[0] += totalbytes - tot_bytes;	
dist 

iMapBTotal += m_bytesIn[i];	oMapRTotal += m_recsOut[i];	oMapBTotal += m_bytesOut[i];	}	for (int i = 0; i < r_recsIn.length; ++i) {	iRedRTotal += r_recsIn[i];	iRedBTotal += r_bytesIn[i];	oRedRTotal += r_recsOut[i];	oRedBTotal += r_bytesOut[i];	}	
s m r d 

========================= hadoop sample_6102 =========================

public boolean isRunning() {	if (super.isRunning()) {	return true;	}	
httpserver acceptor isrunning is false rechecking 

public boolean isRunning() {	if (super.isRunning()) {	return true;	}	try {	Thread.sleep(10);	} catch (InterruptedException ie) {	Thread.currentThread().interrupt();	}	boolean runState = super.isRunning();	
httpserver acceptor isrunning is 

========================= hadoop sample_3750 =========================

public void testCachingRelaysSingleSwitchQueries() throws Throwable {	StaticMapping staticMapping = newInstance(null);	assertSingleSwitch(staticMapping);	CachedDNSToSwitchMapping cachedMap = new CachedDNSToSwitchMapping(staticMapping);	
mapping 

public void testCachingRelaysMultiSwitchQueries() throws Throwable {	StaticMapping staticMapping = newInstance("top");	assertMultiSwitch(staticMapping);	CachedDNSToSwitchMapping cachedMap = new CachedDNSToSwitchMapping(staticMapping);	
mapping 

========================= hadoop sample_2842 =========================

public void getFileStatusReturnsAsExpected() throws URISyntaxException, IOException {	getMockServer().enqueue(new MockResponse().setResponseCode(200) .setBody(TestADLResponseData.getGetFileStatusJSONResponse()));	long startTime = Time.monotonicNow();	FileStatus fileStatus = getMockAdlFileSystem() .getFileStatus(new Path("/test1/test2"));	long endTime = Time.monotonicNow();	
time 

public void getFileStatusAclBit() throws URISyntaxException, IOException {	getMockServer().enqueue(new MockResponse().setResponseCode(200) .setBody(TestADLResponseData.getGetFileStatusJSONResponse(true)));	long startTime = Time.monotonicNow();	FileStatus fileStatus = getMockAdlFileSystem() .getFileStatus(new Path("/test1/test2"));	long endTime = Time.monotonicNow();	
time 

getMockServer().enqueue(new MockResponse().setResponseCode(200) .setBody(TestADLResponseData.getGetFileStatusJSONResponse(true)));	long startTime = Time.monotonicNow();	FileStatus fileStatus = getMockAdlFileSystem() .getFileStatus(new Path("/test1/test2"));	long endTime = Time.monotonicNow();	Assert.assertTrue(fileStatus.isFile());	Assert.assertEquals(true, fileStatus.getPermission().getAclBit());	getMockServer().enqueue(new MockResponse().setResponseCode(200) .setBody(TestADLResponseData.getGetFileStatusJSONResponse(false)));	startTime = Time.monotonicNow();	fileStatus = getMockAdlFileSystem() .getFileStatus(new Path("/test1/test2"));	endTime = Time.monotonicNow();	
time 

========================= hadoop sample_6552 =========================

public void testSnapshot() throws Throwable {	try {	runTestSnapshot(SNAPSHOT_ITERATION_NUMBER);	} catch(Throwable t) {	
FAILED 

========================= hadoop sample_7343 =========================

private static void send(final DataOutputStream out, final Op opcode, final Message proto) throws IOException {	
sending datatransferop 

========================= hadoop sample_6944 =========================

protected void initStorage(Configuration conf) throws IOException {	final String storeUri = conf.get(JHAdminConfig.MR_HS_FS_STATE_STORE_URI);	if (storeUri == null) {	throw new IOException("No store location URI configured in " + JHAdminConfig.MR_HS_FS_STATE_STORE_URI);	}	
using for history server state storage 

public HistoryServerState loadState() throws IOException {	
loading history server state from 

public void storeToken(MRDelegationTokenIdentifier tokenId, Long renewDate) throws IOException {	if (LOG.isDebugEnabled()) {	
storing token 

public void updateToken(MRDelegationTokenIdentifier tokenId, Long renewDate) throws IOException {	if (LOG.isDebugEnabled()) {	
updating token 

public void removeToken(MRDelegationTokenIdentifier tokenId) throws IOException {	if (LOG.isDebugEnabled()) {	
removing token 

public void storeTokenMasterKey(DelegationKey key) throws IOException {	if (LOG.isDebugEnabled()) {	
storing master key 

public void removeTokenMasterKey(DelegationKey key) throws IOException {	if (LOG.isDebugEnabled()) {	
removing master key 

fs.delete(stat.getPath(), false);	} else {	loadTokenFromBucket(bucketId, state, stat.getPath(), stat.getLen());	fs.rename(stat.getPath(), new Path(stat.getPath().getParent(), tokenName));	loadedTokens.add(tokenName);	++numTokens;	}	} else if (name.startsWith(TMP_FILE_PREFIX)) {	fs.delete(stat.getPath(), false);	} else {	
skipping unexpected file in history server token bucket 

private int loadKeys(HistoryServerState state) throws IOException {	FileStatus[] stats = fs.listStatus(tokenKeysStatePath);	int numKeys = 0;	for (FileStatus stat : stats) {	String name = stat.getPath().getName();	if (name.startsWith(TOKEN_MASTER_KEY_FILE_PREFIX)) {	loadTokenMasterKey(state, stat.getPath(), stat.getLen());	++numKeys;	} else {	
skipping unexpected file in history server token state 

private int loadTokens(HistoryServerState state) throws IOException {	FileStatus[] stats = fs.listStatus(tokenStatePath);	int numTokens = 0;	for (FileStatus stat : stats) {	String name = stat.getPath().getName();	if (name.startsWith(TOKEN_BUCKET_DIR_PREFIX)) {	numTokens += loadTokensFromBucket(state, stat.getPath());	} else if (name.equals(TOKEN_KEYS_DIR_NAME)) {	continue;	} else {	
skipping unexpected file in history server token state 

private void loadTokenState(HistoryServerState state) throws IOException {	int numKeys = loadKeys(state);	int numTokens = loadTokens(state);	
loaded master keys and tokens from 

========================= hadoop sample_5391 =========================

setupDatanodeAddress(dnConf, setupHostsFile, checkDataNodeAddrConfig);	if (manageDfsDirs) {	String dirs = makeDataNodeDirs(i, storageTypes == null ? null : storageTypes[i]);	dnConf.set(DFSConfigKeys.DFS_DATANODE_DATA_DIR_KEY, dirs);	conf.set(DFSConfigKeys.DFS_DATANODE_DATA_DIR_KEY, dirs);	}	if (simulatedCapacities != null) {	SimulatedFSDataset.setFactory(dnConf);	dnConf.setLong(SimulatedFSDataset.CONFIG_PROPERTY_CAPACITY, simulatedCapacities[i-curDatanodesNum]);	}	
starting datanode with 

String dirs = makeDataNodeDirs(i, storageTypes == null ? null : storageTypes[i]);	dnConf.set(DFSConfigKeys.DFS_DATANODE_DATA_DIR_KEY, dirs);	conf.set(DFSConfigKeys.DFS_DATANODE_DATA_DIR_KEY, dirs);	}	if (simulatedCapacities != null) {	SimulatedFSDataset.setFactory(dnConf);	dnConf.setLong(SimulatedFSDataset.CONFIG_PROPERTY_CAPACITY, simulatedCapacities[i-curDatanodesNum]);	}	if (hosts != null) {	dnConf.set(DFSConfigKeys.DFS_DATANODE_HOST_NAME_KEY, hosts[i - curDatanodesNum]);	
starting datanode with hostname set to 

if (simulatedCapacities != null) {	SimulatedFSDataset.setFactory(dnConf);	dnConf.setLong(SimulatedFSDataset.CONFIG_PROPERTY_CAPACITY, simulatedCapacities[i-curDatanodesNum]);	}	if (hosts != null) {	dnConf.set(DFSConfigKeys.DFS_DATANODE_HOST_NAME_KEY, hosts[i - curDatanodesNum]);	}	if (racks != null) {	String name = hosts[i - curDatanodesNum];	if (nodeGroups == null) {	
adding node with hostname to rack 

dnConf.setLong(SimulatedFSDataset.CONFIG_PROPERTY_CAPACITY, simulatedCapacities[i-curDatanodesNum]);	}	if (hosts != null) {	dnConf.set(DFSConfigKeys.DFS_DATANODE_HOST_NAME_KEY, hosts[i - curDatanodesNum]);	}	if (racks != null) {	String name = hosts[i - curDatanodesNum];	if (nodeGroups == null) {	StaticMapping.addNodeToRack(name,racks[i-curDatanodesNum]);	} else {	
adding node with hostname to servergroup and rack 

} catch (Exception ex) {	ex.printStackTrace();	}	}	DataNode dn = DataNode.instantiateDataNode(dnArgs, dnConf, secureResources);	if(dn == null) throw new IOException("Cannot start DataNode in " + dnConf.get(DFSConfigKeys.DFS_DATANODE_DATA_DIR_KEY));	String ipAddr = dn.getXferAddress().getAddress().getHostAddress();	if (racks != null) {	int port = dn.getXferAddress().getPort();	if (nodeGroups == null) {	
adding node with ip port to rack 

}	}	DataNode dn = DataNode.instantiateDataNode(dnArgs, dnConf, secureResources);	if(dn == null) throw new IOException("Cannot start DataNode in " + dnConf.get(DFSConfigKeys.DFS_DATANODE_DATA_DIR_KEY));	String ipAddr = dn.getXferAddress().getAddress().getHostAddress();	if (racks != null) {	int port = dn.getXferAddress().getPort();	if (nodeGroups == null) {	StaticMapping.addNodeToRack(ipAddr + ":" + port, racks[i-curDatanodesNum]);	} else {	
adding node with ip port to nodegroup and rack 

========================= hadoop sample_7206 =========================

String line = null;	while ((line = bufferedReader.readLine()) != null) {	lines.add(line);	}	if (LOG.isDebugEnabled()) {	LOG.debug("Loaded IP list of size = " + lines.size() + " from file = " + fileName);	}	return (lines.toArray(new String[lines.size()]));	}	} else {	
missing ip list file 

========================= hadoop sample_3615 =========================

private void rDelete(File place) throws Exception {	if (place.isFile()) {	
deleting file 

private void deleteDir(File dir) throws Exception {	String fns[] = dir.list();	for (String afn : fns) {	File fn = new File(dir, afn);	rDelete(fn);	}	
deleting directory 

========================= hadoop sample_5542 =========================

public DockerClient(Configuration conf) throws ContainerExecutionException {	String tmpDirBase = conf.get("hadoop.tmp.dir");	if (tmpDirBase == null) {	throw new ContainerExecutionException("hadoop.tmp.dir not set!");	}	tmpDirPath = tmpDirBase + "/nm-docker-cmds";	File tmpDir = new File(tmpDirPath);	if (!(tmpDir.exists() || tmpDir.mkdirs())) {	
unable to create directory 

throw new ContainerExecutionException( "'=' found in entry for docker command file, key = " + entry .getKey() + "; value = " + entry.getValue());	}	if (entry.getValue().contains("\n")) {	throw new ContainerExecutionException( "'\\n' found in entry for docker command file, key = " + entry .getKey() + "; value = " + entry.getValue());	}	printWriter.println("  " + entry.getKey() + "=" + StringUtils .join(",", entry.getValue()));	}	printWriter.close();	return dockerCommandFile.getAbsolutePath();	} catch (IOException e) {	
unable to write docker command to temporary file 

========================= hadoop sample_1852 =========================

private FsPermission tryLoadFromPath(Path path, Path backupPath) throws NoSuchAlgorithmException, CertificateException, IOException {	FsPermission perm = null;	try {	perm = loadFromPath(path, password);	fs.delete(backupPath, true);	
keystore loaded successfully 

FsPermission perm = null;	try {	perm = loadFromPath(path, password);	fs.delete(backupPath, true);	} catch (IOException ioe) {	if (!isBadorWrongPassword(ioe)) {	perm = loadFromPath(backupPath, password);	renameOrFail(path, new Path(path.toString() + "_CORRUPTED_" + System.currentTimeMillis()));	renameOrFail(backupPath, path);	if (LOG.isDebugEnabled()) {	
keystore loaded successfully from s since s was corrupted 

private FsPermission tryLoadIncompleteFlush(Path oldPath, Path newPath) throws IOException, NoSuchAlgorithmException, CertificateException {	FsPermission perm = null;	if (fs.exists(newPath)) {	perm = loadAndReturnPerm(newPath, oldPath);	}	if ((perm == null) && fs.exists(oldPath)) {	perm = loadAndReturnPerm(oldPath, newPath);	}	if (perm == null) {	keyStore.load(null, password);	
keystore initialized anew successfully 

private FsPermission loadAndReturnPerm(Path pathToLoad, Path pathToDelete) throws NoSuchAlgorithmException, CertificateException, IOException {	FsPermission perm = null;	try {	perm = loadFromPath(pathToLoad, password);	renameOrFail(pathToLoad, path);	if (LOG.isDebugEnabled()) {	
keystore loaded successfully from s 

private void resetKeyStoreState(Path path) {	
could not flush keystore attempting to reset to previous state 

private void resetKeyStoreState(Path path) {	cache.clear();	try {	loadFromPath(path, password);	
keystore resetting to previously flushed state 

private void resetKeyStoreState(Path path) {	cache.clear();	try {	loadFromPath(path, password);	} catch (Exception e) {	
could not reset keystore to previous state 

========================= hadoop sample_3496 =========================

protected void doGet(final HttpServletRequest req, final HttpServletResponse resp) throws ServletException, IOException {	final UserGroupInformation ugi;	final ServletContext context = getServletContext();	final Configuration conf = NameNodeHttpServer.getConfFromContext(context);	try {	ugi = getUGI(req, conf);	} catch(IOException ioe) {	
request for token received with no authentication from 

========================= hadoop sample_7986 =========================

public static MD5Hash downloadImageToStorage(URL fsName, long imageTxId, Storage dstStorage, boolean needDigest, boolean isBootstrapStandby) throws IOException {	String fileid = ImageServlet.getParamStringForImage(null, imageTxId, dstStorage, isBootstrapStandby);	String fileName = NNStorage.getCheckpointImageFileName(imageTxId);	List<File> dstFiles = dstStorage.getFiles( NameNodeDirType.IMAGE, fileName);	if (dstFiles.isEmpty()) {	throw new IOException("No targets in destination storage!");	}	MD5Hash hash = getFileClient(fsName, fileid, dstFiles, dstStorage, needDigest);	
downloaded file size bytes 

static MD5Hash handleUploadImageRequest(HttpServletRequest request, long imageTxId, Storage dstStorage, InputStream stream, long advertisedSize, DataTransferThrottler throttler) throws IOException {	String fileName = NNStorage.getCheckpointImageFileName(imageTxId);	List<File> dstFiles = dstStorage.getFiles(NameNodeDirType.IMAGE, fileName);	if (dstFiles.isEmpty()) {	throw new IOException("No targets in destination storage!");	}	MD5Hash advertisedDigest = parseMD5Header(request);	MD5Hash hash = receiveFile(fileName, dstFiles, dstStorage, true, advertisedSize, advertisedDigest, fileName, stream, throttler);	
downloaded file size bytes 

public static void uploadImageFromStorage(URL fsName, Configuration conf, NNStorage storage, NameNodeFile nnf, long txid, Canceler canceler) throws IOException {	URL url = new URL(fsName, ImageServlet.PATH_SPEC);	long startTime = Time.monotonicNow();	try {	uploadImage(url, conf, storage, nnf, txid, canceler);	} catch (HttpPutFailedException e) {	if (e.getResponseCode() == HttpServletResponse.SC_CONFLICT) {	
image upload with txid conflicted with a previous image upload to the same namenode continuing 

try {	uploadImage(url, conf, storage, nnf, txid, canceler);	} catch (HttpPutFailedException e) {	if (e.getResponseCode() == HttpServletResponse.SC_CONFLICT) {	return;	} else {	throw e;	}	}	double xferSec = Math.max( ((float) (Time.monotonicNow() - startTime)) / 1000.0, 0.001);	
uploaded image with txid to namenode at in seconds 

}	while (num > 0) {	if (canceler != null && canceler.isCancelled()) {	throw new SaveNamespaceCancelledException( canceler.getCancellationReason());	}	num = infile.read(buf);	if (num <= 0) {	break;	}	if (CheckpointFaultInjector.getInstance() .shouldCorruptAByte(localfile)) {	
simulating a corrupt byte in image transfer 

static MD5Hash getFileClient(URL infoServer, String queryString, List<File> localPaths, Storage dstStorage, boolean getChecksum) throws IOException {	URL url = new URL(infoServer, ImageServlet.PATH_SPEC + "?" + queryString);	
opening connection to 

private static void setTimeout(HttpURLConnection connection) {	if (timeout <= 0) {	Configuration conf = new HdfsConfiguration();	timeout = conf.getInt(DFSConfigKeys.DFS_IMAGE_TRANSFER_TIMEOUT_KEY, DFSConfigKeys.DFS_IMAGE_TRANSFER_TIMEOUT_DEFAULT);	
image transfer timeout configured to milliseconds 

stream = new DigestInputStream(stream, digester);	}	boolean finishedReceiving = false;	int num = 1;	List<FileOutputStream> outputStreams = Lists.newArrayList();	try {	if (localPaths != null) {	for (File f : localPaths) {	try {	if (f.exists()) {	
overwriting existing file with file downloaded from 

try {	if (localPaths != null) {	for (File f : localPaths) {	try {	if (f.exists()) {	}	FileOutputStream fos = new FileOutputStream(f);	outputStreams.add(fos);	streamPathMap.put(fos, f);	} catch (IOException ioe) {	
unable to download file 

private static void deleteTmpFiles(List<File> files) {	if (files == null) {	return;	}	
deleting temporary files 

private static void deleteTmpFiles(List<File> files) {	if (files == null) {	return;	}	for (File file : files) {	if (!file.delete()) {	
deleting has failed 

========================= hadoop sample_7969 =========================

conf.set(SESSION_TOKEN, sessionToken);	conf.set(AWS_CREDENTIALS_PROVIDER, PROVIDER_CLASS);	try(S3AFileSystem fs = S3ATestUtils.createTestFileSystem(conf)) {	createAndVerifyFile(fs, path("testSTS"), TEST_FILE_SIZE);	}	conf.set(SESSION_TOKEN, "invalid-" + sessionToken);	try (S3AFileSystem fs = S3ATestUtils.createTestFileSystem(conf)) {	createAndVerifyFile(fs, path("testSTSInvalidToken"), TEST_FILE_SIZE);	fail("Expected an access exception, but file access to " + fs.getUri() + " was allowed: " + fs);	} catch (AWSS3IOException ex) {	
expected exception 

conf.set(SESSION_TOKEN, sessionToken);	conf.set(AWS_CREDENTIALS_PROVIDER, PROVIDER_CLASS);	try(S3AFileSystem fs = S3ATestUtils.createTestFileSystem(conf)) {	createAndVerifyFile(fs, path("testSTS"), TEST_FILE_SIZE);	}	conf.set(SESSION_TOKEN, "invalid-" + sessionToken);	try (S3AFileSystem fs = S3ATestUtils.createTestFileSystem(conf)) {	createAndVerifyFile(fs, path("testSTSInvalidToken"), TEST_FILE_SIZE);	fail("Expected an access exception, but file access to " + fs.getUri() + " was allowed: " + fs);	} catch (AWSS3IOException ex) {	
expected exception 

========================= hadoop sample_5954 =========================

BigDecimal maxVal = results.getBigDecimal(2);	String lowClausePrefix = colName + " >= ";	String highClausePrefix = colName + " < ";	BigDecimal numSplits = new BigDecimal(conf.getInt(MRJobConfig.NUM_MAPS, 1));	if (minVal == null && maxVal == null) {	List<InputSplit> splits = new ArrayList<InputSplit>();	splits.add(new DataDrivenDBInputFormat.DataDrivenDBInputSplit( colName + " IS NULL", colName + " IS NULL"));	return splits;	}	if (minVal == null || maxVal == null) {	
cannot find a range for numeric or decimal fields with one end null 

========================= hadoop sample_5057 =========================

public boolean initDriver() {	
initializing zookeeper connection 

public boolean initDriver() {	Configuration conf = getConf();	baseZNode = conf.get( FEDERATION_STORE_ZK_PARENT_PATH, FEDERATION_STORE_ZK_PARENT_PATH_DEFAULT);	try {	this.zkManager = new ZKCuratorManager(conf);	this.zkManager.start();	} catch (IOException e) {	
cannot initialize the zk connection 

public <T extends BaseRecord> boolean initRecordStorage( String className, Class<T> clazz) {	try {	String checkPath = getNodePath(baseZNode, className);	zkManager.createRootDirRecursively(checkPath);	return true;	} catch (Exception e) {	
cannot initialize zk node for 

Stat stat = new Stat();	String data = zkManager.getStringData(path, stat);	boolean corrupted = false;	if (data == null || data.equals("")) {	corrupted = true;	} else {	try {	T record = createRecord(data, stat, clazz);	ret.add(record);	} catch (IOException e) {	
cannot create record type from 

corrupted = true;	} else {	try {	T record = createRecord(data, stat, clazz);	ret.add(record);	} catch (IOException e) {	corrupted = true;	}	}	if (corrupted) {	
cannot get data for at cleaning corrupted data 

T record = createRecord(data, stat, clazz);	ret.add(record);	} catch (IOException e) {	corrupted = true;	}	}	if (corrupted) {	zkManager.delete(path);	}	} catch (Exception e) {	
cannot get data for 

verifyDriverReady();	if (query == null) {	return 0;	}	long start = monotonicNow();	List<T> records = null;	try {	QueryResult<T> result = get(clazz);	records = result.getRecords();	} catch (IOException ex) {	
cannot get existing records 

QueryResult<T> result = get(clazz);	records = result.getRecords();	} catch (IOException ex) {	getMetrics().addFailure(monotonicNow() - start);	return 0;	}	String znode = getZNodeForClass(clazz);	List<T> recordsToRemove = filterMultiple(query, records);	int removed = 0;	for (T existingRecord : recordsToRemove) {	
removing 

String znode = getZNodeForClass(clazz);	List<T> recordsToRemove = filterMultiple(query, records);	int removed = 0;	for (T existingRecord : recordsToRemove) {	try {	String primaryKey = getPrimaryKey(existingRecord);	String path = getNodePath(znode, primaryKey);	if (zkManager.delete(path)) {	removed++;	} else {	
did not remove 

int removed = 0;	for (T existingRecord : recordsToRemove) {	try {	String primaryKey = getPrimaryKey(existingRecord);	String path = getNodePath(znode, primaryKey);	if (zkManager.delete(path)) {	removed++;	} else {	}	} catch (Exception e) {	
cannot remove 

public <T extends BaseRecord> boolean removeAll(Class<T> clazz) throws IOException {	long start = monotonicNow();	boolean status = true;	String znode = getZNodeForClass(clazz);	
deleting all children under 

public <T extends BaseRecord> boolean removeAll(Class<T> clazz) throws IOException {	long start = monotonicNow();	boolean status = true;	String znode = getZNodeForClass(clazz);	try {	List<String> children = zkManager.getChildren(znode);	for (String child : children) {	String path = getNodePath(znode, child);	
deleting 

long start = monotonicNow();	boolean status = true;	String znode = getZNodeForClass(clazz);	try {	List<String> children = zkManager.getChildren(znode);	for (String child : children) {	String path = getNodePath(znode, child);	zkManager.delete(path);	}	} catch (Exception e) {	
cannot remove 

private boolean writeNode( String znode, byte[] bytes, boolean update, boolean error) {	try {	boolean created = zkManager.create(znode);	if (!update && !created && error) {	
cannot write record it already exists 

private boolean writeNode( String znode, byte[] bytes, boolean update, boolean error) {	try {	boolean created = zkManager.create(znode);	if (!update && !created && error) {	return false;	}	zkManager.setData(znode, bytes, -1);	return true;	} catch (Exception e) {	
cannot write record 

========================= hadoop sample_8235 =========================

nextTaskId.set(Math.max(nextTaskId.get(), info.getTask().getTaskId()));	successorTasks.addAll(info.getSuccessorTaskIds());	}	final long now = System.currentTimeMillis();	for (DeletionTaskRecoveryInfo info : idToInfoMap.values()) {	for (Integer successorId : info.getSuccessorTaskIds()){	DeletionTaskRecoveryInfo successor = idToInfoMap.get(successorId);	if (successor != null) {	info.getTask().addDeletionTaskDependency(successor.getTask());	} else {	
unable to locate dependency task for deletion task 

return;	}	task.setTaskId(generateTaskId());	DeletionTask[] successors = task.getSuccessorTasks();	for (DeletionTask successor : successors) {	recordDeletionTaskInStateStore(successor);	}	try {	stateStore.storeDeletionTask(task.getTaskId(), task.convertDeletionTaskToProto());	} catch (IOException e) {	
unable to store deletion task 

========================= hadoop sample_1917 =========================

public static boolean hasPerlSupport() {	boolean hasPerl = false;	ShellCommandExecutor shexec = new ShellCommandExecutor( new String[] { "perl", "-e", "print 42" });	try {	shexec.execute();	if (shexec.getOutput().equals("42")) {	hasPerl = true;	}	else {	
perl is installed but isn t behaving as expected 

boolean hasPerl = false;	ShellCommandExecutor shexec = new ShellCommandExecutor( new String[] { "perl", "-e", "print 42" });	try {	shexec.execute();	if (shexec.getOutput().equals("42")) {	hasPerl = true;	}	else {	}	} catch (Exception e) {	
could not run perl 

========================= hadoop sample_5779 =========================

conf.set(YarnConfiguration.RM_HA_ID, rmId);	nonRetriableProxy.put(rmId, super.getProxyInternal());	T proxy = createRetriableProxy();	ProxyInfo<T> pInfo = new ProxyInfo<T>(proxy, rmId);	retriableProxies.put(rmId, pInfo);	}	conf.set(YarnConfiguration.RM_HA_ID, originalId);	T proxyInstance = (T) Proxy.newProxyInstance( RMRequestHedgingInvocationHandler.class.getClassLoader(), new Class<?>[] {protocol}, new RMRequestHedgingInvocationHandler(retriableProxies));	String combinedInfo = Arrays.toString(rmServiceIds);	wrappedProxy = new ProxyInfo<T>(proxyInstance, combinedInfo);	
created wrapped proxy for 

protected T createRetriableProxy() {	try {	RetryPolicy retryPolicy = RMProxy.createRetryPolicy(conf, false);	InetSocketAddress rmAddress = rmProxy.getRMAddress(conf, protocol);	T proxy = rmProxy.getProxy(conf, protocol, rmAddress);	return (T) RetryProxy.create(protocol, proxy, retryPolicy);	} catch (IOException ioe) {	
unable to create proxy to the resourcemanager 

public Object invoke(Object proxy, final Method method, final Object[] args) throws Throwable {	if (successfulProxy != null) {	return invokeMethod(nonRetriableProxy.get(successfulProxy), method, args);	}	
looking for the active rm in 

}	};	proxyMap.put(completionService.submit(c), pInfo);	}	Future<Object> callResultFuture = completionService.take();	String pInfo = proxyMap.get(callResultFuture).proxyInfo;	successfulProxy = pInfo;	Object retVal;	try {	retVal = callResultFuture.get();	
found active rm 

}	Future<Object> callResultFuture = completionService.take();	String pInfo = proxyMap.get(callResultFuture).proxyInfo;	successfulProxy = pInfo;	Object retVal;	try {	retVal = callResultFuture.get();	return retVal;	} catch (Exception ex) {	Throwable rootCause = extraRootException(ex);	
invocation returned exception on so propagating back to caller 

public void performFailover(T currentProxy) {	
connection lost with trying to fail over 

========================= hadoop sample_2546 =========================

public synchronized void pushMetric(final MetricsRecord mr) {	intervalHeartBeat();	try {	mr.incrMetric(getName(), getPreviousIntervalValue());	} catch (Exception e) {	
pushmetric failed for 

========================= hadoop sample_3357 =========================

private FederationStateStoreClientMetrics() {	for (Method m : STATESTORE_API_METHODS) {	String methodName = m.getName();	
registering federation statestore client metrics for 

========================= hadoop sample_1385 =========================

private static DynamoDB createDynamoDB(Configuration conf, String s3Region) throws IOException {	Preconditions.checkNotNull(conf);	final Class<? extends DynamoDBClientFactory> cls = conf.getClass( S3GUARD_DDB_CLIENT_FACTORY_IMPL, S3GUARD_DDB_CLIENT_FACTORY_IMPL_DEFAULT, DynamoDBClientFactory.class);	
creating dynamodb client with region 

public void initialize(FileSystem fs) throws IOException {	Preconditions.checkArgument(fs instanceof S3AFileSystem, "DynamoDBMetadataStore only supports S3A filesystem.");	final S3AFileSystem s3afs = (S3AFileSystem) fs;	instrumentation = s3afs.getInstrumentation().getS3GuardInstrumentation();	final String bucket = s3afs.getBucket();	String confRegion = s3afs.getConf().getTrimmed(S3GUARD_DDB_REGION_KEY);	if (!StringUtils.isEmpty(confRegion)) {	region = confRegion;	
overriding region with configured dynamodb region 

public void initialize(FileSystem fs) throws IOException {	Preconditions.checkArgument(fs instanceof S3AFileSystem, "DynamoDBMetadataStore only supports S3A filesystem.");	final S3AFileSystem s3afs = (S3AFileSystem) fs;	instrumentation = s3afs.getInstrumentation().getS3GuardInstrumentation();	final String bucket = s3afs.getBucket();	String confRegion = s3afs.getConf().getTrimmed(S3GUARD_DDB_REGION_KEY);	if (!StringUtils.isEmpty(confRegion)) {	region = confRegion;	} else {	region = s3afs.getBucketLocation();	
inferring dynamodb region from bucket 

private void innerDelete(Path path, boolean tombstone) throws IOException {	path = checkPath(path);	
deleting from table in region 

private void innerDelete(Path path, boolean tombstone) throws IOException {	path = checkPath(path);	if (path.isRoot()) {	
skip deleting root directory as it does not exist in table 

public void deleteSubtree(Path path) throws IOException {	path = checkPath(path);	
deleting subtree from table in region 

public void deleteSubtree(Path path) throws IOException {	path = checkPath(path);	final PathMetadata meta = get(path);	if (meta == null || meta.isDeleted()) {	
subtree path does not exist this will be a no op 

public PathMetadata get(Path path, boolean wantEmptyDirectoryFlag) throws IOException {	path = checkPath(path);	
get from table in region 

public PathMetadata get(Path path, boolean wantEmptyDirectoryFlag) throws IOException {	path = checkPath(path);	try {	final PathMetadata meta;	if (path.isRoot()) {	meta = new PathMetadata(makeDirStatus(username, path));	} else {	final Item item = getConsistentItem(pathToKey(path));	meta = itemToPathMetadata(item, username);	
get from table in region returning for 

public DirListingMetadata listChildren(Path path) throws IOException {	path = checkPath(path);	
listing table in region 

public DirListingMetadata listChildren(Path path) throws IOException {	path = checkPath(path);	try {	final QuerySpec spec = new QuerySpec() .withHashKey(pathToParentKeyAttribute(path)) .withConsistentRead(true);	final ItemCollection<QueryOutcome> items = table.query(spec);	final List<PathMetadata> metas = new ArrayList<>();	for (Item item : items) {	PathMetadata meta = itemToPathMetadata(item, username);	metas.add(meta);	}	
listing table in region for returning 

private void retryBackoff(int retryCount) throws IOException {	try {	RetryPolicy.RetryAction action = dataAccessRetryPolicy.shouldRetry(null, retryCount, 0, true);	if (action.action == RetryPolicy.RetryAction.RetryDecision.FAIL) {	throw new IOException( String.format("Max retries exceeded (%d) for DynamoDB", retryCount));	} else {	
sleeping msec before next retry 

public void put(PathMetadata meta) throws IOException {	
saving to table in region 

public void put(Collection<PathMetadata> metas) throws IOException {	
saving batch to table in region 

public void put(DirListingMetadata meta) throws IOException {	
saving to table in region 

public synchronized void close() {	if (instrumentation != null) {	instrumentation.storeClosed();	}	if (dynamoDB != null) {	
shutting down 

public void destroy() throws IOException {	if (table == null) {	
in destroy no table to delete 

public void destroy() throws IOException {	if (table == null) {	return;	}	
deleting dynamodb table in region 

public void destroy() throws IOException {	if (table == null) {	return;	}	Preconditions.checkNotNull(dynamoDB, "Not connected to DynamoDB");	try {	table.delete();	table.waitForDelete();	} catch (ResourceNotFoundException rnfe) {	
resourcenotfoundexception while deleting dynamodb table in region this may indicate that the table does not exist or has been deleted by another concurrent thread or process 

if (table == null) {	return;	}	Preconditions.checkNotNull(dynamoDB, "Not connected to DynamoDB");	try {	table.delete();	table.waitForDelete();	} catch (ResourceNotFoundException rnfe) {	} catch (InterruptedException ie) {	Thread.currentThread().interrupt();	
interrupted while waiting for dynamodb table being deleted 

}	}	if (deletionBatch.size() > 0) {	Thread.sleep(delay);	processBatchWriteRequest(pathToKey(deletionBatch), null);	}	} catch (InterruptedException e) {	Thread.currentThread().interrupt();	throw new InterruptedIOException("Pruning was interrupted");	}	
finished pruning items in batches of 

private Item getVersionMarkerItem() throws IOException {	final PrimaryKey versionMarkerKey = createVersionMarkerPrimaryKey(VERSION_MARKER);	int retryCount = 0;	Item versionMarker = table.getItem(versionMarkerKey);	while (versionMarker == null) {	try {	RetryPolicy.RetryAction action = dataAccessRetryPolicy.shouldRetry(null, retryCount, 0, true);	if (action.action == RetryPolicy.RetryAction.RetryDecision.FAIL) {	break;	} else {	
sleeping ms before next retry 

static void verifyVersionCompatibility(String tableName, Item versionMarker) throws IOException {	if (versionMarker == null) {	
table contains no version marker 

private void waitForTableActive(Table t) throws IOException {	try {	t.waitForActive();	} catch (InterruptedException e) {	
interrupted while waiting for table in region active 

private void createTable(ProvisionedThroughput capacity) throws IOException {	try {	
creating non existent dynamodb table in region 

private void createTable(ProvisionedThroughput capacity) throws IOException {	try {	table = dynamoDB.createTable(new CreateTableRequest() .withTableName(tableName) .withKeySchema(keySchema()) .withAttributeDefinitions(attributeDefinitions()) .withProvisionedThroughput(capacity));	
awaiting table becoming active 

private void createTable(ProvisionedThroughput capacity) throws IOException {	try {	table = dynamoDB.createTable(new CreateTableRequest() .withTableName(tableName) .withKeySchema(keySchema()) .withAttributeDefinitions(attributeDefinitions()) .withProvisionedThroughput(capacity));	} catch (ResourceInUseException e) {	
resourceinuseexception while creating dynamodb table in region this may indicate that the table was created by another concurrent thread or process 

public void updateParameters(Map<String, String> parameters) throws IOException {	Preconditions.checkNotNull(table, "Not initialized");	TableDescription desc = getTableDescription(true);	ProvisionedThroughputDescription current = desc.getProvisionedThroughput();	long currentRead = current.getReadCapacityUnits();	long newRead = getLongParam(parameters, S3GUARD_DDB_TABLE_CAPACITY_READ_KEY, currentRead);	long currentWrite = current.getWriteCapacityUnits();	long newWrite = getLongParam(parameters, S3GUARD_DDB_TABLE_CAPACITY_WRITE_KEY, currentWrite);	ProvisionedThroughput throughput = new ProvisionedThroughput() .withReadCapacityUnits(newRead) .withWriteCapacityUnits(newWrite);	if (newRead != currentRead || newWrite != currentWrite) {	
current table capacity is read write 

public void updateParameters(Map<String, String> parameters) throws IOException {	Preconditions.checkNotNull(table, "Not initialized");	TableDescription desc = getTableDescription(true);	ProvisionedThroughputDescription current = desc.getProvisionedThroughput();	long currentRead = current.getReadCapacityUnits();	long newRead = getLongParam(parameters, S3GUARD_DDB_TABLE_CAPACITY_READ_KEY, currentRead);	long currentWrite = current.getWriteCapacityUnits();	long newWrite = getLongParam(parameters, S3GUARD_DDB_TABLE_CAPACITY_WRITE_KEY, currentWrite);	ProvisionedThroughput throughput = new ProvisionedThroughput() .withReadCapacityUnits(newRead) .withWriteCapacityUnits(newWrite);	if (newRead != currentRead || newWrite != currentWrite) {	
changing capacity of table to read write 

TableDescription desc = getTableDescription(true);	ProvisionedThroughputDescription current = desc.getProvisionedThroughput();	long currentRead = current.getReadCapacityUnits();	long newRead = getLongParam(parameters, S3GUARD_DDB_TABLE_CAPACITY_READ_KEY, currentRead);	long currentWrite = current.getWriteCapacityUnits();	long newWrite = getLongParam(parameters, S3GUARD_DDB_TABLE_CAPACITY_WRITE_KEY, currentWrite);	ProvisionedThroughput throughput = new ProvisionedThroughput() .withReadCapacityUnits(newRead) .withWriteCapacityUnits(newWrite);	if (newRead != currentRead || newWrite != currentWrite) {	table.updateTable(throughput);	} else {	
table capacity unchanged at read write 

========================= hadoop sample_6010 =========================

public void serviceStart() throws Exception {	Configuration conf = getConfig();	Path fsWorkingPath = new Path(conf.get(YarnConfiguration.FS_APPLICATION_HISTORY_STORE_URI, conf.get("hadoop.tmp.dir") + "/yarn/timeline/generic-history"));	rootDirPath = new Path(fsWorkingPath, ROOT_DIR_NAME);	try {	fs = getFileSystem(fsWorkingPath, conf);	fs.mkdirs(rootDirPath, ROOT_DIR_UMASK);	} catch (IOException e) {	
error when initializing filesystemhistorystorage 

ApplicationFinishData finishData = parseApplicationFinishData(entry.value);	mergeApplicationHistoryData(historyData, finishData);	readFinishData = true;	}	}	}	if (!readStartData && !readFinishData) {	return null;	}	if (!readStartData) {	
start information is missing for application 

readFinishData = true;	}	}	}	if (!readStartData && !readFinishData) {	return null;	}	if (!readStartData) {	}	if (!readFinishData) {	
finish information is missing for application 

}	}	}	if (!readStartData && !readFinishData) {	return null;	}	if (!readStartData) {	}	if (!readFinishData) {	}	
completed reading history information of application 

}	if (!readStartData && !readFinishData) {	return null;	}	if (!readStartData) {	}	if (!readFinishData) {	}	return historyData;	} catch (IOException e) {	
error when reading history file of application 

Map<ApplicationId, ApplicationHistoryData> historyDataMap = new HashMap<ApplicationId, ApplicationHistoryData>();	FileStatus[] files = fs.listStatus(rootDirPath);	for (FileStatus file : files) {	ApplicationId appId = ApplicationId.fromString(file.getPath().getName());	try {	ApplicationHistoryData historyData = getApplication(appId);	if (historyData != null) {	historyDataMap.put(appId, historyData);	}	} catch (IOException e) {	
history information of application is not included into the result due to the exception 

ApplicationAttemptFinishData finishData = parseApplicationAttemptFinishData(entry.value);	mergeApplicationAttemptHistoryData(historyData, finishData);	readFinishData = true;	}	}	}	if (!readStartData && !readFinishData) {	return null;	}	if (!readStartData) {	
start information is missing for application attempt 

readFinishData = true;	}	}	}	if (!readStartData && !readFinishData) {	return null;	}	if (!readStartData) {	}	if (!readFinishData) {	
finish information is missing for application attempt 

}	}	}	if (!readStartData && !readFinishData) {	return null;	}	if (!readStartData) {	}	if (!readFinishData) {	}	
completed reading history information of application attempt 

}	if (!readStartData && !readFinishData) {	return null;	}	if (!readStartData) {	}	if (!readFinishData) {	}	return historyData;	} catch (IOException e) {	
error when reading history file of application attempt 

ContainerFinishData finishData = parseContainerFinishData(entry.value);	mergeContainerHistoryData(historyData, finishData);	readFinishData = true;	}	}	}	if (!readStartData && !readFinishData) {	return null;	}	if (!readStartData) {	
start information is missing for container 

readFinishData = true;	}	}	}	if (!readStartData && !readFinishData) {	return null;	}	if (!readStartData) {	}	if (!readFinishData) {	
finish information is missing for container 

}	}	}	if (!readStartData && !readFinishData) {	return null;	}	if (!readStartData) {	}	if (!readFinishData) {	}	
completed reading history information of container 

}	if (!readStartData && !readFinishData) {	return null;	}	if (!readStartData) {	}	if (!readFinishData) {	}	return historyData;	} catch (IOException e) {	
error when reading history file of container 

historyDataMap.put(containerId, historyData);	}	if (entry.key.suffix.equals(START_DATA_SUFFIX)) {	mergeContainerHistoryData(historyData, parseContainerStartData(entry.value));	} else if (entry.key.suffix.equals(FINISH_DATA_SUFFIX)) {	mergeContainerHistoryData(historyData, parseContainerFinishData(entry.value));	}	}	}	}	
completed reading history information of all containers of application attempt 

}	if (entry.key.suffix.equals(START_DATA_SUFFIX)) {	mergeContainerHistoryData(historyData, parseContainerStartData(entry.value));	} else if (entry.key.suffix.equals(FINISH_DATA_SUFFIX)) {	mergeContainerHistoryData(historyData, parseContainerFinishData(entry.value));	}	}	}	}	} catch (IOException e) {	
error when reading history information of some containers of application attempt 

public void applicationStarted(ApplicationStartData appStart) throws IOException {	HistoryFileWriter hfWriter = outstandingWriters.get(appStart.getApplicationId());	if (hfWriter == null) {	Path applicationHistoryFile = new Path(rootDirPath, appStart.getApplicationId().toString());	try {	hfWriter = new HistoryFileWriter(applicationHistoryFile);	
opened history file of application 

public void applicationStarted(ApplicationStartData appStart) throws IOException {	HistoryFileWriter hfWriter = outstandingWriters.get(appStart.getApplicationId());	if (hfWriter == null) {	Path applicationHistoryFile = new Path(rootDirPath, appStart.getApplicationId().toString());	try {	hfWriter = new HistoryFileWriter(applicationHistoryFile);	} catch (IOException e) {	
error when openning history file of application 

} catch (IOException e) {	throw e;	}	outstandingWriters.put(appStart.getApplicationId(), hfWriter);	} else {	throw new IOException("History file of application " + appStart.getApplicationId() + " is already opened");	}	assert appStart instanceof ApplicationStartDataPBImpl;	try {	hfWriter.writeHistoryData(new HistoryDataKey(appStart.getApplicationId() .toString(), START_DATA_SUFFIX), ((ApplicationStartDataPBImpl) appStart).getProto().toByteArray());	
start information of application is written 

throw e;	}	outstandingWriters.put(appStart.getApplicationId(), hfWriter);	} else {	throw new IOException("History file of application " + appStart.getApplicationId() + " is already opened");	}	assert appStart instanceof ApplicationStartDataPBImpl;	try {	hfWriter.writeHistoryData(new HistoryDataKey(appStart.getApplicationId() .toString(), START_DATA_SUFFIX), ((ApplicationStartDataPBImpl) appStart).getProto().toByteArray());	} catch (IOException e) {	
error when writing start information of application 

public void applicationFinished(ApplicationFinishData appFinish) throws IOException {	HistoryFileWriter hfWriter = getHistoryFileWriter(appFinish.getApplicationId());	assert appFinish instanceof ApplicationFinishDataPBImpl;	try {	hfWriter.writeHistoryData(new HistoryDataKey(appFinish.getApplicationId() .toString(), FINISH_DATA_SUFFIX), ((ApplicationFinishDataPBImpl) appFinish).getProto().toByteArray());	
finish information of application is written 

public void applicationFinished(ApplicationFinishData appFinish) throws IOException {	HistoryFileWriter hfWriter = getHistoryFileWriter(appFinish.getApplicationId());	assert appFinish instanceof ApplicationFinishDataPBImpl;	try {	hfWriter.writeHistoryData(new HistoryDataKey(appFinish.getApplicationId() .toString(), FINISH_DATA_SUFFIX), ((ApplicationFinishDataPBImpl) appFinish).getProto().toByteArray());	} catch (IOException e) {	
error when writing finish information of application 

public void applicationAttemptStarted( ApplicationAttemptStartData appAttemptStart) throws IOException {	HistoryFileWriter hfWriter = getHistoryFileWriter(appAttemptStart.getApplicationAttemptId() .getApplicationId());	assert appAttemptStart instanceof ApplicationAttemptStartDataPBImpl;	try {	hfWriter.writeHistoryData(new HistoryDataKey(appAttemptStart .getApplicationAttemptId().toString(), START_DATA_SUFFIX), ((ApplicationAttemptStartDataPBImpl) appAttemptStart).getProto() .toByteArray());	
start information of application attempt is written 

public void applicationAttemptStarted( ApplicationAttemptStartData appAttemptStart) throws IOException {	HistoryFileWriter hfWriter = getHistoryFileWriter(appAttemptStart.getApplicationAttemptId() .getApplicationId());	assert appAttemptStart instanceof ApplicationAttemptStartDataPBImpl;	try {	hfWriter.writeHistoryData(new HistoryDataKey(appAttemptStart .getApplicationAttemptId().toString(), START_DATA_SUFFIX), ((ApplicationAttemptStartDataPBImpl) appAttemptStart).getProto() .toByteArray());	} catch (IOException e) {	
error when writing start information of application attempt 

public void applicationAttemptFinished( ApplicationAttemptFinishData appAttemptFinish) throws IOException {	HistoryFileWriter hfWriter = getHistoryFileWriter(appAttemptFinish.getApplicationAttemptId() .getApplicationId());	assert appAttemptFinish instanceof ApplicationAttemptFinishDataPBImpl;	try {	hfWriter.writeHistoryData(new HistoryDataKey(appAttemptFinish .getApplicationAttemptId().toString(), FINISH_DATA_SUFFIX), ((ApplicationAttemptFinishDataPBImpl) appAttemptFinish).getProto() .toByteArray());	
finish information of application attempt is written 

public void applicationAttemptFinished( ApplicationAttemptFinishData appAttemptFinish) throws IOException {	HistoryFileWriter hfWriter = getHistoryFileWriter(appAttemptFinish.getApplicationAttemptId() .getApplicationId());	assert appAttemptFinish instanceof ApplicationAttemptFinishDataPBImpl;	try {	hfWriter.writeHistoryData(new HistoryDataKey(appAttemptFinish .getApplicationAttemptId().toString(), FINISH_DATA_SUFFIX), ((ApplicationAttemptFinishDataPBImpl) appAttemptFinish).getProto() .toByteArray());	} catch (IOException e) {	
error when writing finish information of application attempt 

public void containerStarted(ContainerStartData containerStart) throws IOException {	HistoryFileWriter hfWriter = getHistoryFileWriter(containerStart.getContainerId() .getApplicationAttemptId().getApplicationId());	assert containerStart instanceof ContainerStartDataPBImpl;	try {	hfWriter.writeHistoryData(new HistoryDataKey(containerStart .getContainerId().toString(), START_DATA_SUFFIX), ((ContainerStartDataPBImpl) containerStart).getProto().toByteArray());	
start information of container is written 

public void containerStarted(ContainerStartData containerStart) throws IOException {	HistoryFileWriter hfWriter = getHistoryFileWriter(containerStart.getContainerId() .getApplicationAttemptId().getApplicationId());	assert containerStart instanceof ContainerStartDataPBImpl;	try {	hfWriter.writeHistoryData(new HistoryDataKey(containerStart .getContainerId().toString(), START_DATA_SUFFIX), ((ContainerStartDataPBImpl) containerStart).getProto().toByteArray());	} catch (IOException e) {	
error when writing start information of container 

public void containerFinished(ContainerFinishData containerFinish) throws IOException {	HistoryFileWriter hfWriter = getHistoryFileWriter(containerFinish.getContainerId() .getApplicationAttemptId().getApplicationId());	assert containerFinish instanceof ContainerFinishDataPBImpl;	try {	hfWriter.writeHistoryData(new HistoryDataKey(containerFinish .getContainerId().toString(), FINISH_DATA_SUFFIX), ((ContainerFinishDataPBImpl) containerFinish).getProto().toByteArray());	
finish information of container is written 

public void containerFinished(ContainerFinishData containerFinish) throws IOException {	HistoryFileWriter hfWriter = getHistoryFileWriter(containerFinish.getContainerId() .getApplicationAttemptId().getApplicationId());	assert containerFinish instanceof ContainerFinishDataPBImpl;	try {	hfWriter.writeHistoryData(new HistoryDataKey(containerFinish .getContainerId().toString(), FINISH_DATA_SUFFIX), ((ContainerFinishDataPBImpl) containerFinish).getProto().toByteArray());	} catch (IOException e) {	
error when writing finish information of container 

========================= hadoop sample_2053 =========================

private void waitForBlockReport(final DatanodeProtocolClientSideTranslatorPB mockNN) throws Exception {	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	try {	Mockito.verify(mockNN).blockReport( Mockito.<DatanodeRegistration>anyObject(), Mockito.eq(FAKE_BPID), Mockito.<StorageBlockReport[]>anyObject(), Mockito.<BlockReportContext>anyObject());	return true;	} catch (Throwable t) {	
waiting on block report 

private void waitForBlockReport( final DatanodeProtocolClientSideTranslatorPB mockNN1, final DatanodeProtocolClientSideTranslatorPB mockNN2) throws Exception {	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	return get(mockNN1) || get(mockNN2);	}	private Boolean get(DatanodeProtocolClientSideTranslatorPB mockNN) {	try {	Mockito.verify(mockNN).blockReport( Mockito.<DatanodeRegistration>anyObject(), Mockito.eq(FAKE_BPID), Mockito.<StorageBlockReport[]>anyObject(), Mockito.<BlockReportContext>anyObject());	return true;	} catch (Throwable t) {	
waiting on block report 

========================= hadoop sample_7251 =========================

public ResourceBlacklistRequest getBlacklistUpdates() {	ResourceBlacklistRequest ret;	List<String> blacklist = new ArrayList<>(blacklistNodes);	final int currentBlacklistSize = blacklist.size();	final double failureThreshold = this.blacklistDisableFailureThreshold * numberOfNodeManagerHosts;	if (currentBlacklistSize < failureThreshold) {	if (LOG.isDebugEnabled()) {	
blacklist size is less than failure threshold ratio out of total usable nodes 

public ResourceBlacklistRequest getBlacklistUpdates() {	ResourceBlacklistRequest ret;	List<String> blacklist = new ArrayList<>(blacklistNodes);	final int currentBlacklistSize = blacklist.size();	final double failureThreshold = this.blacklistDisableFailureThreshold * numberOfNodeManagerHosts;	if (currentBlacklistSize < failureThreshold) {	if (LOG.isDebugEnabled()) {	}	ret = ResourceBlacklistRequest.newInstance(blacklist, EMPTY_LIST);	} else {	
ignoring blacklists blacklist size is more than failure threshold ratio out of total usable nodes 

========================= hadoop sample_1106 =========================

protected String cleanName(String name) {	name = name.trim();	if (name.contains(".")) {	String converted = name.replaceAll("\\.", "_dot_");	
name is converted to when it is used as a queue name 

========================= hadoop sample_962 =========================

BufferedReader reader = null;	try {	InputStream is = new FileInputStream(filtersFile);	reader = new BufferedReader(new InputStreamReader(is, Charset.forName("UTF-8")));	String line;	while ((line = reader.readLine()) != null) {	Pattern pattern = Pattern.compile(line);	filters.add(pattern);	}	} catch (FileNotFoundException notFound) {	
can t find filters file 

try {	InputStream is = new FileInputStream(filtersFile);	reader = new BufferedReader(new InputStreamReader(is, Charset.forName("UTF-8")));	String line;	while ((line = reader.readLine()) != null) {	Pattern pattern = Pattern.compile(line);	filters.add(pattern);	}	} catch (FileNotFoundException notFound) {	} catch (IOException cantRead) {	
an error occurred while attempting to read from 

========================= hadoop sample_6293 =========================

final Runner[] runners = new Runner[Runner.NUM_RUNNERS];	final Thread[] threads = new Thread[runners.length];	final int num = 1 << 10;	for(int i = 0; i < runners.length; i++) {	runners[i] = new Runner(i, countThreshold, countLimit, pool, i, bam);	threads[i] = runners[i].start(num);	}	final List<Exception> exceptions = new ArrayList<Exception>();	final Thread randomRecycler = new Thread() {	public void run() {	
randomrecycler start 

runners[j].recycle();	} catch (Exception e) {	e.printStackTrace();	exceptions.add(new Exception(this + " has an exception", e));	}	if ((i & 0xFF) == 0) {	LOG.info("randomRecycler sleep, i=" + i);	sleepMs(100);	}	}	
randomrecycler done 

return false;	}	};	randomRecycler.start();	randomRecycler.join();	Assert.assertTrue(exceptions.isEmpty());	Assert.assertNull(counters.get(0, false));	for(int i = 1; i < runners.length; i++) {	if (!runners[i].assertionErrors.isEmpty()) {	for(AssertionError e : runners[i].assertionErrors) {	
assertionerror 

========================= hadoop sample_6769 =========================

public int run(final String[] args) throws Exception {	if (!localTarget.isAutoFailoverEnabled()) {	
automatic failover is not enabled for please ensure that automatic failover is enabled in the configuration before running the zk failover controller 

private int doRun(String[] args) throws Exception {	try {	initZK();	} catch (KeeperException ke) {	
unable to start failover controller unable to connect to zookeeper quorum at please check the configured value for and ensure that zookeeper is running 

badArg(args[i]);	}	}	return formatZK(force, interactive);	}	else {	badArg(args[0]);	}	}	} catch (Exception e){	
the failover controller encounters runtime error 

return formatZK(force, interactive);	}	else {	badArg(args[0]);	}	}	} catch (Exception e){	throw e;	}	if (!elector.parentZNodeExists()) {	
unable to start failover controller parent znode does not exist run with formatzk flag to initialize zookeeper 

}	} catch (Exception e){	throw e;	}	if (!elector.parentZNodeExists()) {	return ERR_CODE_NO_PARENT_ZNODE;	}	try {	localTarget.checkFencingConfigured();	} catch (BadFencingConfigurationException e) {	
fencing is not configured for you must configure a fencing method before using automatic failover 

localTarget.checkFencingConfigured();	} catch (BadFencingConfigurationException e) {	return ERR_CODE_NO_FENCER;	}	try {	initRPC();	initHM();	startRPC();	mainLoop();	} catch (Exception e) {	
the failover controller encounters runtime error 

private int formatZK(boolean force, boolean interactive) throws IOException, InterruptedException {	if (elector.parentZNodeExists()) {	if (!force && (!interactive || !confirmFormat())) {	return ERR_CODE_FORMAT_DENIED;	}	try {	elector.clearParentZNode();	} catch (IOException e) {	
unable to clear zk parent znode 

private boolean confirmFormat() {	String parentZnode = getParentZnode();	System.err.println( "===============================================\n" + "The configured parent znode " + parentZnode + " already exists.\n" + "Are you sure you want to clear all failover information from\n" + "ZooKeeper?\n" + "WARNING: Before proceeding, ensure that all HDFS services and\n" + "failover controllers are stopped!\n" + "===============================================");	try {	return ToolRunner.confirmPrompt("Proceed formatting " + parentZnode + "?");	} catch (IOException e) {	
failed to confirm 

private synchronized void fatalError(String err) {	
fatal error occurred 

private synchronized void becomeActive() throws ServiceFailedException {	
trying to make active 

return null;	}	}	synchronized (activeAttemptRecordLock) {	if ((lastActiveAttemptRecord != null && lastActiveAttemptRecord.nanoTime >= st)) {	return lastActiveAttemptRecord;	}	activeAttemptRecordLock.wait(1000);	}	} while (System.nanoTime() < waitUntil);	
ms timeout elapsed waiting for an attempt to become active 

private synchronized void becomeStandby() {	
zk election indicated that should become standby 

private synchronized void becomeStandby() {	try {	int timeout = FailoverController.getGracefulFenceTimeout(conf);	localTarget.getProxy(conf, timeout).transitionToStandby(createReqInfo());	
successfully transitioned to standby state 

private synchronized void becomeStandby() {	try {	int timeout = FailoverController.getGracefulFenceTimeout(conf);	localTarget.getProxy(conf, timeout).transitionToStandby(createReqInfo());	} catch (Exception e) {	
couldn t transition to standby state 

private void doFence(HAServiceTarget target) {	
should fence 

private void doFence(HAServiceTarget target) {	boolean gracefulWorked = new FailoverController(conf, RequestSource.REQUEST_BY_ZKFC).tryGracefulFence(target);	if (gracefulWorked) {	
successfully transitioned to standby state without fencing 

private void doFence(HAServiceTarget target) {	boolean gracefulWorked = new FailoverController(conf, RequestSource.REQUEST_BY_ZKFC).tryGracefulFence(target);	if (gracefulWorked) {	return;	}	try {	target.checkFencingConfigured();	} catch (BadFencingConfigurationException e) {	
couldn t fence old active 

private void doCedeActive(int millisToCede) throws AccessControlException, ServiceFailedException, IOException {	int timeout = FailoverController.getGracefulFenceTimeout(conf);	synchronized (elector) {	synchronized (this) {	if (millisToCede <= 0) {	delayJoiningUntilNanotime = 0;	recheckElectability();	return;	}	
requested by at to cede active role 

synchronized (elector) {	synchronized (this) {	if (millisToCede <= 0) {	delayJoiningUntilNanotime = 0;	recheckElectability();	return;	}	boolean needFence = false;	try {	localTarget.getProxy(conf, timeout).transitionToStandby(createReqInfo());	
successfully ensured local node is in standby mode 

synchronized (this) {	if (millisToCede <= 0) {	delayJoiningUntilNanotime = 0;	recheckElectability();	return;	}	boolean needFence = false;	try {	localTarget.getProxy(conf, timeout).transitionToStandby(createReqInfo());	} catch (IOException ioe) {	
unable to transition local node to standby 

synchronized (this) {	if (millisToCede <= 0) {	delayJoiningUntilNanotime = 0;	recheckElectability();	return;	}	boolean needFence = false;	try {	localTarget.getProxy(conf, timeout).transitionToStandby(createReqInfo());	} catch (IOException ioe) {	
quitting election but indicating that fencing is necessary 

private void doGracefulFailover() throws ServiceFailedException, IOException, InterruptedException {	int timeout = FailoverController.getGracefulFenceTimeout(conf) * 2;	checkEligibleForFailover();	HAServiceTarget oldActive = getCurrentActive();	if (oldActive == null) {	throw new ServiceFailedException( "No other node is currently active.");	}	if (oldActive.getAddress().equals(localTarget.getAddress())) {	
local node is already active no need to failover returning success 

private void doGracefulFailover() throws ServiceFailedException, IOException, InterruptedException {	int timeout = FailoverController.getGracefulFenceTimeout(conf) * 2;	checkEligibleForFailover();	HAServiceTarget oldActive = getCurrentActive();	if (oldActive == null) {	throw new ServiceFailedException( "No other node is currently active.");	}	if (oldActive.getAddress().equals(localTarget.getAddress())) {	return;	}	
asking to cede its active state for ms 

if (attempt == null) {	synchronized(this) {	if (lastHealthState != State.SERVICE_HEALTHY) {	throw new ServiceFailedException("Unable to become active. " + "Service became unhealthy while trying to failover.");	}	}	throw new ServiceFailedException("Unable to become active. " + "Local node did not get an opportunity to do so from ZooKeeper, " + "or the local node took too long to transition to active.");	}	oldZkfc.cedeActive(-1);	if (attempt.succeeded) {	
successfully became active 

private void recheckElectability() {	synchronized (elector) {	synchronized (this) {	boolean healthy = lastHealthState == State.SERVICE_HEALTHY;	long remainingDelay = delayJoiningUntilNanotime - System.nanoTime();	if (remainingDelay > 0) {	if (healthy) {	
would have joined master election but this node is prohibited from doing so for more ms 

}	scheduleRecheck(remainingDelay);	return;	}	switch (lastHealthState) {	case SERVICE_HEALTHY: elector.joinElection(targetToData(localTarget));	if (quitElectionOnBadState) {	quitElectionOnBadState = false;	}	break;	
ensuring that does not participate in active master election 

}	switch (lastHealthState) {	case SERVICE_HEALTHY: elector.joinElection(targetToData(localTarget));	if (quitElectionOnBadState) {	quitElectionOnBadState = false;	}	break;	elector.quitElection(false);	serviceState = HAServiceState.INITIALIZING;	break;	
quitting master election for and marking that fencing is necessary 

protected synchronized void setLastHealthState(HealthMonitor.State newState) {	
local service entered state 

========================= hadoop sample_3992 =========================

throw new ServiceStateException(E_NO_USER_DETERMINED_FOR_ACLS);	}	digest(id, pass);	ACL acl = new ACL(ZooDefs.Perms.ALL, toDigestId(id, pass));	userACLs.add(acl);	digestAuthUser = id;	digestAuthPassword = pass;	String authPair = id + ":" + pass;	digestAuthData = authPair.getBytes("UTF-8");	if (LOG.isDebugEnabled()) {	
auth is digest acl 

ACL acl = new ACL(ZooDefs.Perms.ALL, toDigestId(id, pass));	userACLs.add(acl);	digestAuthUser = id;	digestAuthPassword = pass;	String authPair = id + ":" + pass;	digestAuthData = authPair.getBytes("UTF-8");	if (LOG.isDebugEnabled()) {	}	break;	case anon: if (LOG.isDebugEnabled()) {	
auth is anonymous 

}	break;	case anon: if (LOG.isDebugEnabled()) {	}	userACLs = new ArrayList<ACL>(0);	break;	}	systemACLs.addAll(userACLs);	} else {	if (LOG.isDebugEnabled()) {	
registry has no security 

public boolean addDigestACL(ACL acl) {	if (secureRegistry) {	if (LOG.isDebugEnabled()) {	
added acl 

public boolean addDigestACL(ACL acl) {	if (secureRegistry) {	if (LOG.isDebugEnabled()) {	}	digestACLs.add(acl);	return true;	} else {	if (LOG.isDebugEnabled()) {	
ignoring added acl registry is insecure 

public void resetDigestACLs() {	if (LOG.isDebugEnabled()) {	
cleared digest acls 

public static void bindJVMtoJAASFile(File jaasFile) {	String path = jaasFile.getAbsolutePath();	if (LOG.isDebugEnabled()) {	
binding to 

public ACL createACLForUser(UserGroupInformation ugi, int perms) {	if (LOG.isDebugEnabled()) {	
creating acl for 

public ACL createACLfromUsername(String username, int perms) {	if (usesRealm && !username.contains("@")) {	username = username + "@" + kerberosRealm;	if (LOG.isDebugEnabled()) {	
appending kerberos realm to make 

public static UgiInfo fromCurrentUser() {	try {	return new UgiInfo(UserGroupInformation.getCurrentUser());	} catch (IOException e) {	
failed to get current user 

========================= hadoop sample_2680 =========================

TaskAttempt task1Attempt1 = mapTask1.getAttempts().values().iterator().next();	TaskAttempt task2Attempt = mapTask2.getAttempts().values().iterator().next();	app.waitForState(task1Attempt1, TaskAttemptState.RUNNING);	app.waitForState(task2Attempt, TaskAttemptState.RUNNING);	app.waitForState(reduceTask, TaskState.RUNNING);	app.getContext().getEventHandler().handle( new TaskAttemptEvent( task1Attempt1.getID(), TaskAttemptEventType.TA_FAILMSG));	app.waitForState(task1Attempt1, TaskAttemptState.FAILED);	int timeOut = 0;	while (mapTask1.getAttempts().size() != 2 && timeOut++ < 10) {	Thread.sleep(2000);	
waiting for next attempt to start 

Assert.assertEquals(2, mapTask1.getAttempts().size());	Iterator<TaskAttempt> itr = mapTask1.getAttempts().values().iterator();	itr.next();	TaskAttempt task1Attempt2 = itr.next();	waitForContainerAssignment(task1Attempt2);	app.getContext().getEventHandler().handle( new TaskAttemptEvent(task1Attempt2.getID(), TaskAttemptEventType.TA_CONTAINER_LAUNCH_FAILED));	app.waitForState(task1Attempt2, TaskAttemptState.FAILED);	timeOut = 0;	while (mapTask1.getAttempts().size() != 3 && timeOut++ < 10) {	Thread.sleep(2000);	
waiting for next attempt to start 

itr = mapTask1.getAttempts().values().iterator();	itr.next();	itr.next();	TaskAttempt task1Attempt3 = itr.next();	app.waitForState(task1Attempt3, TaskAttemptState.RUNNING);	app.getContext().getEventHandler().handle( new TaskAttemptEvent( task1Attempt3.getID(), TaskAttemptEventType.TA_KILL));	app.waitForState(task1Attempt3, TaskAttemptState.KILLED);	timeOut = 0;	while (mapTask1.getAttempts().size() != 4 && timeOut++ < 10) {	Thread.sleep(2000);	
waiting for next attempt to start 

Iterator<Task> it = job.getTasks().values().iterator();	Task mapTask1 = it.next();	Task mapTask2 = it.next();	Task reduceTask = it.next();	app.waitForState(mapTask1, TaskState.RUNNING);	app.waitForState(mapTask2, TaskState.RUNNING);	app.getContext().getEventHandler().handle( new TaskEvent(mapTask1.getID(), TaskEventType.T_ADD_SPEC_ATTEMPT));	int timeOut = 0;	while (mapTask1.getAttempts().size() != 2 && timeOut++ < 10) {	Thread.sleep(1000);	
waiting for next attempt to start 

public void testRecoverySuccessAttempt() {	
start testrecoverysuccessattempt 

public void testRecoveryAllFailAttempts() {	
start testrecoveryallfailattempts 

public void testRecoveryTaskSuccessAllAttemptsFail() {	
start testrecoverytasksuccessallattemptsfail 

public void testRecoveryTaskSuccessAllAttemptsSucceed() {	
start testrecoverytasksuccessallattemptsfail 

public void testRecoveryAllAttemptsKilled() {	
start testrecoveryallattemptskilled 

LOG.info(eventNum + " " + current.getClass().getName());	if (current instanceof JobHistoryEvent) {	JobHistoryEvent jhe = (JobHistoryEvent) current;	LOG.info(expectedJobHistoryEvents.get(0).toString() + " " + jhe.getHistoryEvent().getEventType().toString() + " " + jhe.getJobID());	assertEquals(expectedJobHistoryEvents.get(0), jhe.getHistoryEvent().getEventType());	expectedJobHistoryEvents.remove(0);	}  else if (current instanceof JobCounterUpdateEvent) {	JobCounterUpdateEvent jcue = (JobCounterUpdateEvent) current;	boolean containsUpdates = jcue.getCounterUpdates().size() > 0;	if(containsUpdates) {	
jobcounterupdateevent 

========================= hadoop sample_5145 =========================

public void addToQueue(PathDeletionContext... contexts) {	for (PathDeletionContext context : contexts) {	try {	if (!deletePath(context)) {	
stale path 

public void addToQueue(PathDeletionContext... contexts) {	for (PathDeletionContext context : contexts) {	try {	if (!deletePath(context)) {	stalePaths.add(context.fullPath);	}	} catch (IOException e) {	
caught exception while deleting path 

========================= hadoop sample_5450 =========================

public Response submitApplication(ApplicationSubmissionContextInfo newApp, HttpServletRequest hsr) throws AuthorizationException, IOException, InterruptedException {	validateRunning();	ApplicationId appId = ApplicationId.fromString(newApp.getApplicationId());	
application submitted 

public Response updateAppState(AppState targetState, HttpServletRequest hsr, String appId) throws AuthorizationException, YarnException, InterruptedException, IOException {	validateRunning();	ApplicationId applicationId = ApplicationId.fromString(appId);	if (!applicationMap.remove(applicationId)) {	throw new ApplicationNotFoundException( "Trying to kill an absent application: " + appId);	}	if (targetState == null) {	return Response.status(Status.BAD_REQUEST).build();	}	
force killing application 

========================= hadoop sample_1940 =========================

assertEquals(file3.toString(), itor.next().getPath().toString());	assertFalse(itor.hasNext());	{	fs.setPermission(dir, new FsPermission((short)0));	try {	final String username = UserGroupInformation.getCurrentUser().getShortUserName() + "1";	final HftpFileSystem hftp2 = cluster.getHftpFileSystemAs(username, conf, 0, "somegroup");	hftp2.getContentSummary(dir);	fail();	} catch(IOException ioe) {	
good getting an exception 

========================= hadoop sample_7197 =========================

final FileSystem fs = file.getFileSystem(job);	fileIn = fs.open(file);	CompressionCodec codec = new CompressionCodecFactory(job).getCodec(file);	if (null != codec) {	isCompressedInput = true;	decompressor = CodecPool.getDecompressor(codec);	CompressionInputStream cIn = codec.createInputStream(fileIn, decompressor);	filePosition = cIn;	inputStream = cIn;	numRecordsRemainingInSplit = Long.MAX_VALUE;	
compressed input cannot compute number of records in the split 

numRecordsRemainingInSplit = Long.MAX_VALUE;	} else {	fileIn.seek(start);	filePosition = fileIn;	inputStream = fileIn;	long splitSize = end - start - numBytesToSkip;	numRecordsRemainingInSplit = (splitSize + recordLength - 1)/recordLength;	if (numRecordsRemainingInSplit < 0) {	numRecordsRemainingInSplit = 0;	}	
expecting records each with a length of bytes in the split with an effective size of bytes 

========================= hadoop sample_5000 =========================

public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) {	
exception in 

========================= hadoop sample_7894 =========================

private void initialize(Configuration conf) throws YarnException {	
initializing reservation system 

private void loadPlan(String planName, Map<ReservationId, ReservationAllocationStateProto> reservations) throws PlanningException {	Plan plan = plans.get(planName);	Resource minAllocation = getMinAllocation();	ResourceCalculator rescCalculator = getResourceCalculator();	for (Entry<ReservationId, ReservationAllocationStateProto> currentReservation : reservations .entrySet()) {	plan.addReservation(ReservationSystemUtil.toInMemoryAllocation(planName, currentReservation.getKey(), currentReservation.getValue(), minAllocation, rescCalculator), true);	resQMap.put(currentReservation.getKey(), planName);	}	
recovered reservations for plan 

public void recover(RMState state) throws Exception {	
recovering reservation system 

private void initializeNewPlans(Configuration conf) {	
refreshing reservation system 

private void initializeNewPlans(Configuration conf) {	writeLock.lock();	try {	Set<String> planQueueNames = scheduler.getPlanQueues();	for (String planQueueName : planQueueNames) {	if (!plans.containsKey(planQueueName)) {	Plan plan = initializePlan(planQueueName);	plans.put(planQueueName, plan);	} else {	
plan based on reservation queue already exists 

if (!plans.containsKey(planQueueName)) {	Plan plan = initializePlan(planQueueName);	plans.put(planQueueName, plan);	} else {	}	}	if (planFollower != null) {	planFollower.setPlans(plans.values());	}	} catch (YarnException e) {	
exception while trying to refresh reservable queues 

private PlanFollower createPlanFollower() {	String planFollowerPolicyClassName = conf.get(YarnConfiguration.RM_RESERVATION_SYSTEM_PLAN_FOLLOWER, getDefaultPlanFollower());	if (planFollowerPolicyClassName == null) {	return null;	}	
using planfollowerpolicy 

public ReservationId getNewReservationId() {	writeLock.lock();	try {	ReservationId resId = ReservationId.newInstance( ResourceManager.getClusterTimeStamp(), resCounter.incrementAndGet());	
allocated new reservationid 

protected Plan initializePlan(String planQueueName) throws YarnException {	String planQueuePath = getPlanQueuePath(planQueueName);	SharingPolicy adPolicy = getAdmissionPolicy(planQueuePath);	adPolicy.init(planQueuePath, getReservationSchedulerConfiguration());	Resource minAllocation = getMinAllocation();	Resource maxAllocation = getMaxAllocation();	ResourceCalculator rescCalc = getResourceCalculator();	Resource totCap = getPlanQueueCapacity(planQueueName);	Plan plan = new InMemoryPlan(getRootQueueMetrics(), adPolicy, getAgent(planQueuePath), totCap, planStepSize, rescCalc, minAllocation, maxAllocation, planQueueName, getReplanner(planQueuePath), getReservationSchedulerConfiguration().getMoveOnExpiry(planQueuePath), maxPeriodicity, rmContext);	
initialized plan based on reservable queue 

protected Planner getReplanner(String planQueueName) {	ReservationSchedulerConfiguration reservationConfig = getReservationSchedulerConfiguration();	String plannerClassName = reservationConfig.getReplanner(planQueueName);	
using replanner for queue 

protected ReservationAgent getAgent(String queueName) {	ReservationSchedulerConfiguration reservationConfig = getReservationSchedulerConfiguration();	String agentClassName = reservationConfig.getReservationAgent(queueName);	
using agent for queue 

protected SharingPolicy getAdmissionPolicy(String queueName) {	ReservationSchedulerConfiguration reservationConfig = getReservationSchedulerConfiguration();	String admissionPolicyClassName = reservationConfig.getReservationAdmissionPolicy(queueName);	
using admissionpolicy for queue 

========================= hadoop sample_1043 =========================

createDirTree(BUCKET_ROOT, depth, width, origMetas);	List<Path> origPaths = metasToPaths(origMetas);	List<PathMetadata> movedMetas = moveMetas(origMetas, BUCKET_ROOT, new Path(BUCKET_ROOT, "moved-here"));	List<Path> movedPaths = metasToPaths(movedMetas);	long count = 1;	try (MetadataStore ms = createMetadataStore()) {	try {	count = populateMetadataStore(origMetas, ms);	describe("Running move workload");	NanoTimer moveTimer = new NanoTimer();	
running moves of paths each 

========================= hadoop sample_5903 =========================

static ResourceEstimatorServer startResourceEstimatorServer() throws IOException, InterruptedException {	Configuration config = new YarnConfiguration();	config.addResource(ResourceEstimatorConfiguration.CONFIG_FILE);	ResourceEstimatorServer resourceEstimatorServer = null;	try {	resourceEstimatorServer = new ResourceEstimatorServer();	ShutdownHookManager.get().addShutdownHook( new CompositeServiceShutdownHook(resourceEstimatorServer), 30);	resourceEstimatorServer.init(config);	resourceEstimatorServer.start();	} catch (Throwable t) {	
error starting resourceestimatorserver 

public void shutdown() throws Exception {	
stopping resourceestimator service at 

========================= hadoop sample_6513 =========================

public void setTimelineCollectorInfo(CollectorInfo collectorInfo) {	if (collectorInfo == null) {	
not setting collector info as it is null 

public void setTimelineCollectorInfo(CollectorInfo collectorInfo) {	if (collectorInfo == null) {	return;	}	if (collectorInfo.getCollectorToken() != null) {	setTimelineDelegationToken( collectorInfo.getCollectorToken(), collectorInfo.getCollectorAddr());	}	if (collectorInfo.getCollectorAddr() != null && !collectorInfo.getCollectorAddr().isEmpty() && !collectorInfo.getCollectorAddr().equals(timelineServiceAddress)) {	this.timelineServiceAddress = collectorInfo.getCollectorAddr();	
updated timeline service address to 

private void setTimelineDelegationToken(Token delegationToken, String collectorAddr) {	if (!delegationToken.getKind().equals( TimelineDelegationTokenIdentifier.KIND_NAME.toString())) {	
timeline token to be updated should be of kind 

private void setTimelineDelegationToken(Token delegationToken, String collectorAddr) {	if (!delegationToken.getKind().equals( TimelineDelegationTokenIdentifier.KIND_NAME.toString())) {	return;	}	if (collectorAddr == null || collectorAddr.isEmpty()) {	collectorAddr = timelineServiceAddress;	}	String service = delegationToken.getService();	if ((service == null || service.isEmpty()) && (collectorAddr == null || collectorAddr.isEmpty())) {	
timeline token does not have service and timeline service address is not yet set not updating the token 

return;	}	if (currentTimelineToken != null && currentTimelineToken.equals(delegationToken)) {	return;	}	currentTimelineToken = delegationToken;	org.apache.hadoop.security.token. Token<TimelineDelegationTokenIdentifier> timelineToken = new org.apache.hadoop.security.token. Token<TimelineDelegationTokenIdentifier>( delegationToken.getIdentifier().array(), delegationToken.getPassword().array(), new Text(delegationToken.getKind()), service == null ? new Text() : new Text(service));	InetSocketAddress serviceAddr = (collectorAddr != null && !collectorAddr.isEmpty()) ? NetUtils.createSocketAddr(collectorAddr) : SecurityUtil.getTokenServiceAddr(timelineToken);	SecurityUtil.setTokenService(timelineToken, serviceAddr);	authUgi.addToken(timelineToken);	
updated timeline delegation token 

public void run() {	try {	EntitiesHolder entitiesHolder;	while (!Thread.currentThread().isInterrupted()) {	try {	entitiesHolder = timelineEntityQueue.take();	} catch (InterruptedException ie) {	
timeline dispatcher thread was interrupted 

} catch (InterruptedException ie) {	Thread.currentThread().interrupt();	return;	}	if (entitiesHolder != null) {	publishWithoutBlockingOnQueue(entitiesHolder);	}	}	} finally {	if (!timelineEntityQueue.isEmpty()) {	
yet to publish timelineentities draining them now 

}	}	} finally {	if (!timelineEntityQueue.isEmpty()) {	}	long timeTillweDrain = System.currentTimeMillis() + drainTimeoutPeriod;	while (!timelineEntityQueue.isEmpty()) {	publishWithoutBlockingOnQueue(timelineEntityQueue.poll());	if (System.currentTimeMillis() > timeTillweDrain) {	if (!timelineEntityQueue.isEmpty()) {	
time to drain elapsed remaining timelineentities will not be published 

public void stop() {	
stopping timelineclient 

========================= hadoop sample_2550 =========================

private void doCopy(Set<TaskAttemptID> maps) throws IOException {	Iterator<TaskAttemptID> iter = maps.iterator();	while (iter.hasNext()) {	TaskAttemptID map = iter.next();	
localfetcher going to fetch 

Path mapOutputFileName = localMapFiles.get(mapTaskId).getOutputFile();	Path indexFileName = mapOutputFileName.suffix(".index");	SpillRecord sr = new SpillRecord(indexFileName, job);	IndexRecord ir = sr.getIndex(reduce);	long compressedLength = ir.partLength;	long decompressedLength = ir.rawLength;	compressedLength -= CryptoUtils.cryptoPadding(job);	decompressedLength -= CryptoUtils.cryptoPadding(job);	MapOutput<K, V> mapOutput = merger.reserve(mapTaskId, decompressedLength, id);	if (mapOutput == null) {	
fetcher mergemanager returned status wait 

SpillRecord sr = new SpillRecord(indexFileName, job);	IndexRecord ir = sr.getIndex(reduce);	long compressedLength = ir.partLength;	long decompressedLength = ir.rawLength;	compressedLength -= CryptoUtils.cryptoPadding(job);	decompressedLength -= CryptoUtils.cryptoPadding(job);	MapOutput<K, V> mapOutput = merger.reserve(mapTaskId, decompressedLength, id);	if (mapOutput == null) {	return false;	}	
localfetcher about to shuffle output of map decomp len to 

========================= hadoop sample_4938 =========================

if (!isTokenRenewalExcluded(fs, conf)) {	delegTokenRenewer = Master.getMasterPrincipal(conf);	if (delegTokenRenewer == null || delegTokenRenewer.length() == 0) {	throw new IOException( "Can't get Master Kerberos principal for use as renewer");	}	}	mergeBinaryTokens(credentials, conf);	final Token<?> tokens[] = fs.addDelegationTokens(delegTokenRenewer, credentials);	if (tokens != null) {	for (Token<?> token : tokens) {	
got dt for 

========================= hadoop sample_4892 =========================

public void move(Collection<Path> pathsToDelete, Collection<PathMetadata> pathsToCreate) throws IOException {	Preconditions.checkNotNull(pathsToDelete, "pathsToDelete is null");	Preconditions.checkNotNull(pathsToCreate, "pathsToCreate is null");	Preconditions.checkArgument(pathsToDelete.size() == pathsToCreate.size(), "Must supply same number of paths to delete/create.");	synchronized (this) {	for (Path meta : pathsToDelete) {	
move deleting metadata 

public void move(Collection<Path> pathsToDelete, Collection<PathMetadata> pathsToCreate) throws IOException {	Preconditions.checkNotNull(pathsToDelete, "pathsToDelete is null");	Preconditions.checkNotNull(pathsToCreate, "pathsToCreate is null");	Preconditions.checkArgument(pathsToDelete.size() == pathsToCreate.size(), "Must supply same number of paths to delete/create.");	synchronized (this) {	for (Path meta : pathsToDelete) {	delete(meta);	}	for (PathMetadata meta : pathsToCreate) {	
move adding metadata 

public void put(PathMetadata meta) throws IOException {	Preconditions.checkNotNull(meta);	FileStatus status = meta.getFileStatus();	Path path = standardize(status.getPath());	synchronized (this) {	if (LOG.isDebugEnabled()) {	
put 

public synchronized void put(DirListingMetadata meta) throws IOException {	if (LOG.isDebugEnabled()) {	
put dirmeta 

private void deleteHashEntries(Path path, boolean tombstone) {	
delete file entry for 

private void deleteHashEntries(Path path, boolean tombstone) {	if (tombstone) {	fileHash.put(path, PathMetadata.tombstone(path));	} else {	fileHash.remove(path);	}	
removing listing of 

if (tombstone) {	fileHash.put(path, PathMetadata.tombstone(path));	} else {	fileHash.remove(path);	}	dirHash.remove(path);	Path parent = path.getParent();	if (parent != null) {	DirListingMetadata dir = dirHash.get(parent);	if (dir != null) {	
removing parent s entry for 

========================= hadoop sample_6006 =========================

private void runDelegatedTasks(SSLEngineResult result, SSLEngine engine) throws Exception {	Runnable runnable;	if (result.getHandshakeStatus() == SSLEngineResult.HandshakeStatus.NEED_TASK) {	while ((runnable = engine.getDelegatedTask()) != null) {	
running delegated task 

int netBufferMax = session.getPacketBufferSize();	ByteBuffer clientOut = ByteBuffer.wrap("client".getBytes());	ByteBuffer clientIn = ByteBuffer.allocate(appBufferMax);	ByteBuffer serverOut = ByteBuffer.wrap("server".getBytes());	ByteBuffer serverIn = ByteBuffer.allocate(appBufferMax);	ByteBuffer cTOs = ByteBuffer.allocateDirect(netBufferMax);	ByteBuffer sTOc = ByteBuffer.allocateDirect(netBufferMax);	boolean dataDone = false;	try {	while (!isEngineClosed(clientSSLEngine) || !isEngineClosed(serverSSLEngine)) {	
client wrap 

int netBufferMax = session.getPacketBufferSize();	ByteBuffer clientOut = ByteBuffer.wrap("client".getBytes());	ByteBuffer clientIn = ByteBuffer.allocate(appBufferMax);	ByteBuffer serverOut = ByteBuffer.wrap("server".getBytes());	ByteBuffer serverIn = ByteBuffer.allocate(appBufferMax);	ByteBuffer cTOs = ByteBuffer.allocateDirect(netBufferMax);	ByteBuffer sTOc = ByteBuffer.allocateDirect(netBufferMax);	boolean dataDone = false;	try {	while (!isEngineClosed(clientSSLEngine) || !isEngineClosed(serverSSLEngine)) {	
server wrap 

ByteBuffer clientIn = ByteBuffer.allocate(appBufferMax);	ByteBuffer serverOut = ByteBuffer.wrap("server".getBytes());	ByteBuffer serverIn = ByteBuffer.allocate(appBufferMax);	ByteBuffer cTOs = ByteBuffer.allocateDirect(netBufferMax);	ByteBuffer sTOc = ByteBuffer.allocateDirect(netBufferMax);	boolean dataDone = false;	try {	while (!isEngineClosed(clientSSLEngine) || !isEngineClosed(serverSSLEngine)) {	cTOs.flip();	sTOc.flip();	
client unwrap 

ByteBuffer clientIn = ByteBuffer.allocate(appBufferMax);	ByteBuffer serverOut = ByteBuffer.wrap("server".getBytes());	ByteBuffer serverIn = ByteBuffer.allocate(appBufferMax);	ByteBuffer cTOs = ByteBuffer.allocateDirect(netBufferMax);	ByteBuffer sTOc = ByteBuffer.allocateDirect(netBufferMax);	boolean dataDone = false;	try {	while (!isEngineClosed(clientSSLEngine) || !isEngineClosed(serverSSLEngine)) {	cTOs.flip();	sTOc.flip();	
server unwrap 

boolean dataDone = false;	try {	while (!isEngineClosed(clientSSLEngine) || !isEngineClosed(serverSSLEngine)) {	cTOs.flip();	sTOc.flip();	cTOs.compact();	sTOc.compact();	if (!dataDone && (clientOut.limit() == serverIn.position()) && (serverOut.limit() == clientIn.position())) {	checkTransfer(serverOut, clientIn);	checkTransfer(clientOut, serverIn);	
closing client 

========================= hadoop sample_2946 =========================

public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException {	if (filterConfig == null) return;	uri = ((HttpServletRequest)request).getRequestURI();	
filtering 

static void access(String urlstring) throws IOException {	
access 

========================= hadoop sample_3087 =========================

private List<InputSplit> getSplits(Configuration configuration, int numSplits, long totalSizeBytes) throws IOException {	List<InputSplit> splits = new ArrayList<InputSplit>(numSplits);	long nBytesPerSplit = (long) Math.ceil(totalSizeBytes * 1.0 / numSplits);	CopyListingFileStatus srcFileStatus = new CopyListingFileStatus();	Text srcRelPath = new Text();	long currentSplitSize = 0;	long lastSplitStart = 0;	long lastPosition = 0;	final Path listingFilePath = getListingFilePath(configuration);	if (LOG.isDebugEnabled()) {	
average bytes per map number of maps total size 

final Path listingFilePath = getListingFilePath(configuration);	if (LOG.isDebugEnabled()) {	}	SequenceFile.Reader reader=null;	try {	reader = getListingFileReader(configuration);	while (reader.next(srcRelPath, srcFileStatus)) {	if (currentSplitSize + srcFileStatus.getChunkLength() > nBytesPerSplit && lastPosition != 0) {	FileSplit split = new FileSplit(listingFilePath, lastSplitStart, lastPosition - lastSplitStart, null);	if (LOG.isDebugEnabled()) {	
creating split bytes in split 

splits.add(split);	lastSplitStart = lastPosition;	currentSplitSize = 0;	}	currentSplitSize += srcFileStatus.getChunkLength();	lastPosition = reader.getPosition();	}	if (lastPosition > lastSplitStart) {	FileSplit split = new FileSplit(listingFilePath, lastSplitStart, lastPosition - lastSplitStart, null);	if (LOG.isDebugEnabled()) {	
creating split bytes in split 

private SequenceFile.Reader getListingFileReader(Configuration configuration) {	final Path listingFilePath = getListingFilePath(configuration);	try {	final FileSystem fileSystem = listingFilePath.getFileSystem(configuration);	if (!fileSystem.exists(listingFilePath)) throw new IllegalArgumentException("Listing file doesn't exist at: " + listingFilePath);	return new SequenceFile.Reader(configuration, SequenceFile.Reader.file(listingFilePath));	}	catch (IOException exception) {	
couldn t find listing file at 

========================= hadoop sample_6285 =========================

try {	FSAppAttempt starvedApp = context.getStarvedApps().take();	schedulerReadLock.lock();	try {	preemptContainers(identifyContainersToPreempt(starvedApp));	} finally {	schedulerReadLock.unlock();	}	starvedApp.preemptionTriggered(delayBeforeNextStarvationCheck);	} catch (InterruptedException e) {	
preemption thread interrupted exiting 

private void preemptContainers(List<RMContainer> containers) {	for (RMContainer container : containers) {	ApplicationAttemptId appAttemptId = container.getApplicationAttemptId();	FSAppAttempt app = scheduler.getSchedulerApp(appAttemptId);	
preempting container from queue 

public void run() {	for (RMContainer container : containers) {	ContainerStatus status = SchedulerUtils.createPreemptedContainerStatus( container.getContainerId(), SchedulerUtils.PREEMPTED_CONTAINER);	
killing container 

========================= hadoop sample_974 =========================

public void testReduceOutOfDiskSpace() throws Throwable {	
testReduceOutOfDiskSpace 

bin.close();	}	byte[] corrupted = bout.toByteArray();	corrupted[headerSize + (dataSize / 2)] = 0x0;	try {	bin = new ByteArrayInputStream(corrupted);	bin.read(new byte[headerSize], 0, headerSize);	odmo.shuffle(host, bin, dataSize, dataSize, metrics, Reporter.NULL);	fail("OnDiskMapOutput.shuffle didn't detect the corrupted map partition file");	} catch(ChecksumException e) {	
the expected checksum exception was thrown 

========================= hadoop sample_4607 =========================

for (int j = 0; j < listdir.length; j++) {	String path = listdir[j].getName();	if (!path.startsWith(blockName)) {	continue;	}	if (blockFile.getCanonicalPath().equals(listdir[j].getCanonicalPath())) {	continue;	}	return Block.getGenerationStamp(listdir[j].getName());	}	
block does not have a metafile 

========================= hadoop sample_7928 =========================

public ProfilingFileIoEvents(@Nullable Configuration conf) {	if (conf != null) {	int fileIOSamplingPercentage = conf.getInt( DFSConfigKeys.DFS_DATANODE_FILEIO_PROFILING_SAMPLING_PERCENTAGE_KEY, DFSConfigKeys .DFS_DATANODE_FILEIO_PROFILING_SAMPLING_PERCENTAGE_DEFAULT);	isEnabled = Util.isDiskStatsEnabled(fileIOSamplingPercentage);	if (fileIOSamplingPercentage > 100) {	
value cannot be more than setting value to 

========================= hadoop sample_7882 =========================

public TestFileInputFormat(int numThreads) {	this.numThreads = numThreads;	
running with numthreads 

public void setup() throws IOException {	
using test dir 

========================= hadoop sample_4579 =========================

public void check(String[] host, X509Certificate cert) throws SSLException {	String[] cns = Certificates.getCNs(cert);	String[] subjectAlts = Certificates.getDNSSubjectAlts(cert);	try {	check(host, cns, subjectAlts);	} catch (SSLException e) {	
host check error 

public void check(final String[] hosts, final String[] cns, final String[] subjectAlts, final boolean ie6, final boolean strictWithSubDomains) throws SSLException {	if (LOG.isTraceEnabled()) {	
hosts cns subjectalts strictwithsubdomains 

========================= hadoop sample_3746 =========================

public void tearDown() throws Exception {	if (fc != null) {	final Path testRoot = fileContextTestHelper.getAbsoluteTestRootPath(fc);	
deleting test root path 

public void tearDown() throws Exception {	if (fc != null) {	final Path testRoot = fileContextTestHelper.getAbsoluteTestRootPath(fc);	try {	fc.delete(testRoot, true);	} catch (Exception e) {	
error when deleting test root path 

public void tearDown() throws Exception {	if (fc != null) {	final Path testRoot = fileContextTestHelper.getAbsoluteTestRootPath(fc);	try {	fc.delete(testRoot, true);	} catch (Exception e) {	}	try {	fc.delete(localFsRootPath, true);	} catch (Exception e) {	
error when deleting localfsrootpath 

========================= hadoop sample_3316 =========================

public void firstStep() throws Exception {	simulateStartTimeMS = System.currentTimeMillis() - baselineTimeMS;	ReservationId reservationId = null;	try {	reservationId = submitReservationWhenSpecified();	} catch (UndeclaredThrowableException y) {	
unable to place reservation 

private ReservationId submitReservationWhenSpecified() throws IOException, InterruptedException {	if (reservationRequest != null) {	UserGroupInformation ugi = UserGroupInformation.createRemoteUser(user);	ugi.doAs(new PrivilegedExceptionAction<Object>() {	public Object run() throws YarnException, IOException {	rm.getClientRMService().submitReservation(reservationRequest);	
reservation successfully submitted 

public void lastStep() throws Exception {	
application is shutting down 

public void lastStep() throws Exception {	if (isTracked) {	untrackApp();	}	if (amContainer != null) {	LOG.info("AM container = {} reported to finish", amContainer.getId());	se.getNmMap().get(amContainer.getNodeId()).cleanupContainer( amContainer.getId());	} else {	
am container is null 

appSubContext.setReservationID(reservationId);	}	subAppRequest.setApplicationSubmissionContext(appSubContext);	UserGroupInformation ugi = UserGroupInformation.createRemoteUser(user);	ugi.doAs(new PrivilegedExceptionAction<Object>() {	public Object run() throws YarnException, IOException {	rm.getClientRMService().submitApplication(subAppRequest);	return null;	}	});	
submit a new application 

amRegisterRequest.setRpcPort(1000);	amRegisterRequest.setTrackingUrl("localhost:1000");	UserGroupInformation ugi = UserGroupInformation.createRemoteUser(appAttemptId.toString());	Token<AMRMTokenIdentifier> token = rm.getRMContext().getRMApps().get(appId) .getRMAppAttempt(appAttemptId).getAMRMToken();	ugi.addTokenIdentifier(token.decodeIdentifier());	ugi.doAs( new PrivilegedExceptionAction<RegisterApplicationMasterResponse>() {	public RegisterApplicationMasterResponse run() throws Exception {	return rm.getApplicationMasterService() .registerApplicationMaster(amRegisterRequest);	}	});	
register the application master for application 

========================= hadoop sample_6051 =========================

public synchronized Configuration retrieve() {	byte[] serializedSchedConf;	try {	serializedSchedConf = zkManager.getData(confStorePath);	} catch (Exception e) {	
failed to retrieve configuration from zookeeper store 

return null;	}	try {	Map<String, String> map = (HashMap<String, String>) deserializeObject(serializedSchedConf);	Configuration c = new Configuration();	for (Map.Entry<String, String> e : map.entrySet()) {	c.set(e.getKey(), e.getValue());	}	return c;	} catch (Exception e) {	
exception while deserializing scheduler configuration from store 

========================= hadoop sample_923 =========================

public List<PrivilegedOperation> reacquireContainer(ContainerId containerId) throws ResourceHandlerException {	String containerIdStr = containerId.toString();	if (LOG.isDebugEnabled()) {	
attempting to reacquire classid for container 

public List<PrivilegedOperation> reacquireContainer(ContainerId containerId) throws ResourceHandlerException {	String containerIdStr = containerId.toString();	if (LOG.isDebugEnabled()) {	}	String classIdStrFromFile = cGroupsHandler.getCGroupParam( CGroupsHandler.CGroupController.NET_CLS, containerIdStr, CGroupsHandler.CGROUP_PARAM_CLASSID);	int classId = trafficController .getClassIdFromFileContents(classIdStrFromFile);	
reacquired containerid classid mapping 

public Map<ContainerId, Integer> getBytesSentPerContainer() throws ResourceHandlerException {	Map<Integer, Integer> classIdStats = trafficController.readStats();	Map<ContainerId, Integer> containerIdStats = new HashMap<>();	for (Map.Entry<ContainerId, Integer> entry : containerIdClassIdMap .entrySet()) {	ContainerId containerId = entry.getKey();	Integer classId = entry.getValue();	Integer bytesSent = classIdStats.get(classId);	if (bytesSent == null) {	
no bytes sent metric found for container with classid 

public List<PrivilegedOperation> postComplete(ContainerId containerId) throws ResourceHandlerException {	
postcomplete for container 

public List<PrivilegedOperation> postComplete(ContainerId containerId) throws ResourceHandlerException {	cGroupsHandler.deleteCGroup(CGroupsHandler.CGroupController.NET_CLS, containerId.toString());	Integer classId = containerIdClassIdMap.get(containerId);	if (classId != null) {	PrivilegedOperation op = trafficController.new BatchBuilder(PrivilegedOperation.OperationType.TC_MODIFY_STATE) .deleteContainerClass(classId).commitBatchToTempFile();	try {	privilegedOperationExecutor.executePrivilegedOperation(op, false);	trafficController.releaseClassId(classId);	} catch (PrivilegedOperationException e) {	
failed to delete tc rule for classid 

Integer classId = containerIdClassIdMap.get(containerId);	if (classId != null) {	PrivilegedOperation op = trafficController.new BatchBuilder(PrivilegedOperation.OperationType.TC_MODIFY_STATE) .deleteContainerClass(classId).commitBatchToTempFile();	try {	privilegedOperationExecutor.executePrivilegedOperation(op, false);	trafficController.releaseClassId(classId);	} catch (PrivilegedOperationException e) {	throw new ResourceHandlerException( "Failed to delete tc rule for classId:" + classId);	}	} else {	
not cleaning up tc rules classid unknown for container 

public List<PrivilegedOperation> teardown() throws ResourceHandlerException {	if (LOG.isDebugEnabled()) {	
teardown nothing to do 

========================= hadoop sample_1844 =========================

}	Path file = new Path(dir, "part-0");	SequenceFile.Writer writer = SequenceFile.createWriter(fs, conf, file, BytesWritable.class, BytesWritable.class, CompressionType.NONE);	long numBytesToWrite = fileSizeInMB * 1024 * 1024;	int minKeySize = conf.getInt(MIN_KEY, 10);;	int keySizeRange = conf.getInt(MAX_KEY, 1000) - minKeySize;	int minValueSize = conf.getInt(MIN_VALUE, 0);	int valueSizeRange = conf.getInt(MAX_VALUE, 20000) - minValueSize;	BytesWritable randomKey = new BytesWritable();	BytesWritable randomValue = new BytesWritable();	
writing bytes to with minkeysize keysizerange minvaluesize valuesizerange 

randomKey.setSize(keyLength);	randomizeBytes(randomKey.getBytes(), 0, randomKey.getLength());	int valueLength = minValueSize + (valueSizeRange != 0 ? random.nextInt(valueSizeRange) : 0);	randomValue.setSize(valueLength);	randomizeBytes(randomValue.getBytes(), 0, randomValue.getLength());	writer.append(randomKey, randomValue);	numBytesToWrite -= keyLength + valueLength;	}	writer.close();	long end = System.currentTimeMillis();	
created of size mb in secs 

========================= hadoop sample_5411 =========================

public void run(Context context) throws IOException, InterruptedException {	outer = context;	int numberOfThreads = getNumberOfThreads(context);	mapClass = getMapperClass(context);	if (LOG.isDebugEnabled()) {	
configuring multithread runner to use threads 

========================= hadoop sample_5041 =========================

static Comparer<byte[]> getBestComparer() {	if (System.getProperty("os.arch").equals("sparc")) {	if (LOG.isTraceEnabled()) {	
lexicographical comparer selected for byte aligned system architecture 

static Comparer<byte[]> getBestComparer() {	if (System.getProperty("os.arch").equals("sparc")) {	if (LOG.isTraceEnabled()) {	}	return lexicographicalComparerJavaImpl();	}	try {	Class<?> theClass = Class.forName(UNSAFE_COMPARER_NAME);	Comparer<byte[]> comparer = (Comparer<byte[]>) theClass.getEnumConstants()[0];	if (LOG.isTraceEnabled()) {	
unsafe comparer selected for byte unaligned system architecture 

}	try {	Class<?> theClass = Class.forName(UNSAFE_COMPARER_NAME);	Comparer<byte[]> comparer = (Comparer<byte[]>) theClass.getEnumConstants()[0];	if (LOG.isTraceEnabled()) {	}	return comparer;	} catch (Throwable t) {	if (LOG.isTraceEnabled()) {	LOG.trace(t.getMessage());	
lexicographical comparer selected 

========================= hadoop sample_3896 =========================

String uid = "";	String gid = "";	String[] groups = {};	Shell.ShellCommandExecutor shexec1 = new Shell.ShellCommandExecutor( new String[]{"id", "-u", runAsUser});	Shell.ShellCommandExecutor shexec2 = new Shell.ShellCommandExecutor( new String[]{"id", "-g", runAsUser});	Shell.ShellCommandExecutor shexec3 = new Shell.ShellCommandExecutor( new String[]{"id", "-G", runAsUser});	try {	shexec1.execute();	uid = shexec1.getOutput().replaceAll("\n$", "");	} catch (Exception e) {	
could not run id u command 

Shell.ShellCommandExecutor shexec3 = new Shell.ShellCommandExecutor( new String[]{"id", "-G", runAsUser});	try {	shexec1.execute();	uid = shexec1.getOutput().replaceAll("\n$", "");	} catch (Exception e) {	}	try {	shexec2.execute();	gid = shexec2.getOutput().replaceAll("\n$", "");	} catch (Exception e) {	
could not run id g command 

}	try {	shexec2.execute();	gid = shexec2.getOutput().replaceAll("\n$", "");	} catch (Exception e) {	}	try {	shexec3.execute();	groups = shexec3.getOutput().replace("\n", " ").split(" ");	} catch (Exception e) {	
could not run id g command 

runtime.initialize(conf);	String[] networks = {"host", "none", "bridge", "sdn1"};	String invalidDefaultNetwork = "sdn2";	conf.setStrings(YarnConfiguration.NM_DOCKER_ALLOWED_CONTAINER_NETWORKS, networks);	conf.set(YarnConfiguration.NM_DOCKER_DEFAULT_CONTAINER_NETWORK, invalidDefaultNetwork);	try {	runtime = new DockerLinuxContainerRuntime(mockExecutor, mockCGroupsHandler);	runtime.initialize(conf);	Assert.fail("Invalid default network configuration should did not " + "trigger initialization failure.");	} catch (ContainerExecutionException e) {	
caught expected exception 

public void testContainerLaunchWithNetworkingDefaults() throws ContainerExecutionException, IOException, PrivilegedOperationException {	DockerLinuxContainerRuntime runtime = new DockerLinuxContainerRuntime(mockExecutor, mockCGroupsHandler);	runtime.initialize(conf);	Random randEngine = new Random();	String disallowedNetwork = "sdn" + Integer.toString(randEngine.nextInt());	try {	env.put(DockerLinuxContainerRuntime.ENV_DOCKER_CONTAINER_NETWORK, disallowedNetwork);	runtime.launchContainer(builder.build());	Assert.fail("Network was expected to be disallowed: " + disallowedNetwork);	} catch (ContainerExecutionException e) {	
caught expected exception 

Assert.assertEquals("  name=container_id", dockerCommands.get(counter++));	Assert.assertEquals("  net=sdn2", dockerCommands.get(counter++));	Assert.assertEquals( "  rw-mounts=/test_container_local_dir:/test_container_local_dir," + "/test_filecache_dir:/test_filecache_dir," + "/test_container_work_dir:/test_container_work_dir," + "/test_container_log_dir:/test_container_log_dir," + "/test_user_local_dir:/test_user_local_dir", dockerCommands.get(counter++));	Assert.assertEquals("  user=run_as_user", dockerCommands.get(counter++));	Assert.assertEquals("  workdir=/test_container_work_dir", dockerCommands.get(counter++));	env.put(DockerLinuxContainerRuntime.ENV_DOCKER_CONTAINER_NETWORK, customNetwork3);	try {	runtime.launchContainer(builder.build());	Assert.fail("Disallowed network : " + customNetwork3 + "did not trigger launch failure.");	} catch (ContainerExecutionException e) {	
caught expected exception 

public void testLaunchPrivilegedContainersWithDisabledSetting() throws ContainerExecutionException, PrivilegedOperationException, IOException{	DockerLinuxContainerRuntime runtime = new DockerLinuxContainerRuntime( mockExecutor, mockCGroupsHandler);	runtime.initialize(conf);	env.put(DockerLinuxContainerRuntime .ENV_DOCKER_CONTAINER_RUN_PRIVILEGED_CONTAINER, "true");	try {	runtime.launchContainer(builder.build());	Assert.fail("Expected a privileged launch container failure.");	} catch (ContainerExecutionException e) {	
caught expected exception 

public void testLaunchPrivilegedContainersWithEnabledSettingAndDefaultACL() throws ContainerExecutionException, PrivilegedOperationException, IOException{	conf.setBoolean(YarnConfiguration.NM_DOCKER_ALLOW_PRIVILEGED_CONTAINERS, true);	DockerLinuxContainerRuntime runtime = new DockerLinuxContainerRuntime( mockExecutor, mockCGroupsHandler);	runtime.initialize(conf);	env.put(DockerLinuxContainerRuntime .ENV_DOCKER_CONTAINER_RUN_PRIVILEGED_CONTAINER, "true");	try {	runtime.launchContainer(builder.build());	Assert.fail("Expected a privileged launch container failure.");	} catch (ContainerExecutionException e) {	
caught expected exception 

public void testMountSourceOnly() throws ContainerExecutionException, PrivilegedOperationException, IOException{	DockerLinuxContainerRuntime runtime = new DockerLinuxContainerRuntime( mockExecutor, mockCGroupsHandler);	runtime.initialize(conf);	env.put( DockerLinuxContainerRuntime.ENV_DOCKER_CONTAINER_LOCAL_RESOURCE_MOUNTS, "source");	try {	runtime.launchContainer(builder.build());	Assert.fail("Expected a launch container failure due to invalid mount.");	} catch (ContainerExecutionException e) {	
caught expected exception 

public void testMountInvalid() throws ContainerExecutionException, PrivilegedOperationException, IOException{	DockerLinuxContainerRuntime runtime = new DockerLinuxContainerRuntime( mockExecutor, mockCGroupsHandler);	runtime.initialize(conf);	env.put( DockerLinuxContainerRuntime.ENV_DOCKER_CONTAINER_LOCAL_RESOURCE_MOUNTS, "source:target:other");	try {	runtime.launchContainer(builder.build());	Assert.fail("Expected a launch container failure due to invalid mount.");	} catch (ContainerExecutionException e) {	
caught expected exception 

========================= hadoop sample_1645 =========================

protected void beforeExecute(Thread t, Runnable r) {	if (LOG.isDebugEnabled()) {	
beforeexecute in thread runnable type 

========================= hadoop sample_3678 =========================

public void run() {	try {	TaskAttemptID mapId = new TaskAttemptID(new TaskID( jobId, TaskType.MAP, taskId), 0);	
starting task 

map.localizeConfiguration(localConf);	map.setConf(localConf);	try {	map_tasks.getAndIncrement();	myMetrics.launchMap(mapId);	map.run(localConf, Job.this);	myMetrics.completeMap(mapId);	} finally {	map_tasks.getAndDecrement();	}	
finishing task 

public void run() {	try {	TaskAttemptID reduceId = new TaskAttemptID(new TaskID( jobId, TaskType.REDUCE, taskId), 0);	
starting task 

reduce.localizeConfiguration(localConf);	reduce.setConf(localConf);	try {	reduce_tasks.getAndIncrement();	myMetrics.launchReduce(reduce.getTaskID());	reduce.run(localConf, Job.this);	myMetrics.completeReduce(reduce.getTaskID());	} finally {	reduce_tasks.getAndDecrement();	}	
finishing task 

protected synchronized ExecutorService createMapExecutor() {	int maxMapThreads = job.getInt(LOCAL_MAX_MAPS, 1);	if (maxMapThreads < 1) {	throw new IllegalArgumentException( "Configured " + LOCAL_MAX_MAPS + " must be >= 1");	}	maxMapThreads = Math.min(maxMapThreads, this.numMapTasks);	maxMapThreads = Math.max(maxMapThreads, 1);	
starting mapper thread pool executor 

protected synchronized ExecutorService createMapExecutor() {	int maxMapThreads = job.getInt(LOCAL_MAX_MAPS, 1);	if (maxMapThreads < 1) {	throw new IllegalArgumentException( "Configured " + LOCAL_MAX_MAPS + " must be >= 1");	}	maxMapThreads = Math.min(maxMapThreads, this.numMapTasks);	maxMapThreads = Math.max(maxMapThreads, 1);	
max local threads 

protected synchronized ExecutorService createMapExecutor() {	int maxMapThreads = job.getInt(LOCAL_MAX_MAPS, 1);	if (maxMapThreads < 1) {	throw new IllegalArgumentException( "Configured " + LOCAL_MAX_MAPS + " must be >= 1");	}	maxMapThreads = Math.min(maxMapThreads, this.numMapTasks);	maxMapThreads = Math.max(maxMapThreads, 1);	
map tasks to process 

protected synchronized ExecutorService createReduceExecutor() {	int maxReduceThreads = job.getInt(LOCAL_MAX_REDUCES, 1);	if (maxReduceThreads < 1) {	throw new IllegalArgumentException( "Configured " + LOCAL_MAX_REDUCES + " must be >= 1");	}	maxReduceThreads = Math.min(maxReduceThreads, this.numReduceTasks);	maxReduceThreads = Math.max(maxReduceThreads, 1);	
starting reduce thread pool executor 

protected synchronized ExecutorService createReduceExecutor() {	int maxReduceThreads = job.getInt(LOCAL_MAX_REDUCES, 1);	if (maxReduceThreads < 1) {	throw new IllegalArgumentException( "Configured " + LOCAL_MAX_REDUCES + " must be >= 1");	}	maxReduceThreads = Math.min(maxReduceThreads, this.numReduceTasks);	maxReduceThreads = Math.max(maxReduceThreads, 1);	
max local threads 

protected synchronized ExecutorService createReduceExecutor() {	int maxReduceThreads = job.getInt(LOCAL_MAX_REDUCES, 1);	if (maxReduceThreads < 1) {	throw new IllegalArgumentException( "Configured " + LOCAL_MAX_REDUCES + " must be >= 1");	}	maxReduceThreads = Math.min(maxReduceThreads, this.numReduceTasks);	maxReduceThreads = Math.max(maxReduceThreads, 1);	
reduce tasks to process 

private void runTasks(List<RunnableWithThrowable> runnables, ExecutorService service, String taskType) throws Exception {	for (Runnable r : runnables) {	service.submit(r);	}	try {	service.shutdown();	
waiting for tasks 

for (Runnable r : runnables) {	service.submit(r);	}	try {	service.shutdown();	service.awaitTermination(Long.MAX_VALUE, TimeUnit.NANOSECONDS);	} catch (InterruptedException ie) {	service.shutdownNow();	throw ie;	}	
task executor complete 

public void run() {	JobID jobId = profile.getJobID();	JobContext jContext = new JobContextImpl(job, jobId);	org.apache.hadoop.mapreduce.OutputCommitter outputCommitter = null;	try {	outputCommitter = createOutputCommitter(conf.getUseNewMapper(), jobId, conf);	} catch (Exception e) {	
failed to createoutputcommitter 

if (killed) {	this.status.setRunState(JobStatus.KILLED);	} else {	this.status.setRunState(JobStatus.SUCCEEDED);	}	JobEndNotifier.localRunnerNotification(job, status);	} catch (Throwable t) {	try {	outputCommitter.abortJob(jContext, org.apache.hadoop.mapreduce.JobStatus.State.FAILED);	} catch (IOException ioe) {	
error cleaning up job 

this.status.setRunState(JobStatus.FAILED);	}	LOG.warn(id, t);	JobEndNotifier.localRunnerNotification(job, status);	} finally {	try {	fs.delete(systemJobFile.getParent(), true);	localFs.delete(localJobFile, true);	localDistributedCacheManager.close();	} catch (IOException e) {	
error cleaning up 

public void reportNextRecordRange(TaskAttemptID taskid, SortedRanges.Range range) throws IOException {	
task reportednextrecordrange 

public synchronized void fsError(TaskAttemptID taskId, String message) throws IOException {	
fserror from task 

public void shuffleError(TaskAttemptID taskId, String message) throws IOException {	
shuffleerror from task 

public synchronized void fatalError(TaskAttemptID taskId, String msg) throws IOException {	
fatal from task 

static void setupChildMapredLocalDirs(Task t, JobConf conf) {	String[] localDirs = conf.getTrimmedStrings(MRConfig.LOCAL_DIR);	String jobId = t.getJobID().toString();	String taskId = t.getTaskID().toString();	boolean isCleanup = t.isTaskCleanupTask();	String user = t.getUser();	StringBuffer childMapredLocalDir = new StringBuffer(localDirs[0] + Path.SEPARATOR + getLocalTaskDir(user, jobId, taskId, isCleanup));	for (int i = 1; i < localDirs.length; i++) {	childMapredLocalDir.append("," + localDirs[i] + Path.SEPARATOR + getLocalTaskDir(user, jobId, taskId, isCleanup));	}	
for child 

========================= hadoop sample_4447 =========================

public void testFilterFileSystem() throws Exception {	for (Method m : AbstractFileSystem.class.getDeclaredMethods()) {	if (Modifier.isStatic(m.getModifiers())) continue;	if (Modifier.isPrivate(m.getModifiers())) continue;	if (Modifier.isFinal(m.getModifiers())) continue;	try {	DontCheck.class.getMethod(m.getName(), m.getParameterTypes());	
skipping 

public void testFilterFileSystem() throws Exception {	for (Method m : AbstractFileSystem.class.getDeclaredMethods()) {	if (Modifier.isStatic(m.getModifiers())) continue;	if (Modifier.isPrivate(m.getModifiers())) continue;	if (Modifier.isFinal(m.getModifiers())) continue;	try {	DontCheck.class.getMethod(m.getName(), m.getParameterTypes());	} catch (NoSuchMethodException exc) {	
testing 

if (Modifier.isStatic(m.getModifiers())) continue;	if (Modifier.isPrivate(m.getModifiers())) continue;	if (Modifier.isFinal(m.getModifiers())) continue;	try {	DontCheck.class.getMethod(m.getName(), m.getParameterTypes());	} catch (NoSuchMethodException exc) {	try{	FilterFs.class.getDeclaredMethod(m.getName(), m.getParameterTypes());	}	catch(NoSuchMethodException exc2){	
filterfilesystem doesn t implement 

========================= hadoop sample_3228 =========================

} else {	return n1.ordinal() - n2.ordinal();	}	}	});	ImmutableList<Long> refIdList = null;	for (FileSummary.Section section : sections) {	fin.getChannel().position(section.getOffset());	is = FSImageUtil.wrapInputStreamForCompression(conf, summary.getCodec(), new BufferedInputStream(new LimitInputStream( fin, section.getLength())));	switch (SectionName.fromString(section.getName())) {	
loading string table 

}	}	});	ImmutableList<Long> refIdList = null;	for (FileSummary.Section section : sections) {	fin.getChannel().position(section.getOffset());	is = FSImageUtil.wrapInputStreamForCompression(conf, summary.getCodec(), new BufferedInputStream(new LimitInputStream( fin, section.getLength())));	switch (SectionName.fromString(section.getName())) {	stringTable = FSImageLoader.loadStringTable(is);	break;	
loading inode references 

long startTime = Time.monotonicNow();	out.println(getHeader());	for (FileSummary.Section section : sections) {	if (SectionName.fromString(section.getName()) == SectionName.INODE) {	fin.getChannel().position(section.getOffset());	is = FSImageUtil.wrapInputStreamForCompression(conf, summary.getCodec(), new BufferedInputStream(new LimitInputStream( fin, section.getLength())));	outputINodes(is);	}	}	long timeTaken = Time.monotonicNow() - startTime;	
time to output inodes ms 

private void loadDirectories( FileInputStream fin, List<FileSummary.Section> sections, FileSummary summary, Configuration conf) throws IOException {	
loading directories 

private void loadDirectories( FileInputStream fin, List<FileSummary.Section> sections, FileSummary summary, Configuration conf) throws IOException {	long startTime = Time.monotonicNow();	for (FileSummary.Section section : sections) {	if (SectionName.fromString(section.getName()) == SectionName.INODE) {	fin.getChannel().position(section.getOffset());	InputStream is = FSImageUtil.wrapInputStreamForCompression(conf, summary.getCodec(), new BufferedInputStream(new LimitInputStream( fin, section.getLength())));	loadDirectoriesInINodeSection(is);	}	}	long timeTaken = Time.monotonicNow() - startTime;	
finished loading directories in ms 

private void loadINodeDirSection( FileInputStream fin, List<FileSummary.Section> sections, FileSummary summary, Configuration conf, List<Long> refIdList) throws IOException {	
loading inode directory section 

private void loadINodeDirSection( FileInputStream fin, List<FileSummary.Section> sections, FileSummary summary, Configuration conf, List<Long> refIdList) throws IOException {	long startTime = Time.monotonicNow();	for (FileSummary.Section section : sections) {	if (SectionName.fromString(section.getName()) == SectionName.INODE_DIR) {	fin.getChannel().position(section.getOffset());	InputStream is = FSImageUtil.wrapInputStreamForCompression(conf, summary.getCodec(), new BufferedInputStream( new LimitInputStream(fin, section.getLength())));	buildNamespace(is, refIdList);	}	}	long timeTaken = Time.monotonicNow() - startTime;	
finished loading inode directory section in ms 

private void loadDirectoriesInINodeSection(InputStream in) throws IOException {	INodeSection s = INodeSection.parseDelimitedFrom(in);	
loading directories in inode section 

private void loadDirectoriesInINodeSection(InputStream in) throws IOException {	INodeSection s = INodeSection.parseDelimitedFrom(in);	int numDirs = 0;	for (int i = 0; i < s.getNumInodes(); ++i) {	INode p = INode.parseDelimitedFrom(in);	if (LOG.isDebugEnabled() && i % 10000 == 0) {	
scanned inodes 

int numDirs = 0;	for (int i = 0; i < s.getNumInodes(); ++i) {	INode p = INode.parseDelimitedFrom(in);	if (LOG.isDebugEnabled() && i % 10000 == 0) {	}	if (p.hasDirectory()) {	metadataMap.putDir(p);	numDirs++;	}	}	
found directories in inode section 

private void buildNamespace(InputStream in, List<Long> refIdList) throws IOException {	int count = 0;	while (true) {	FsImageProto.INodeDirectorySection.DirEntry e = FsImageProto.INodeDirectorySection.DirEntry.parseDelimitedFrom(in);	if (e == null) {	break;	}	count++;	if (LOG.isDebugEnabled() && count % 10000 == 0) {	
scanned directories 

for (int i = 0; i < e.getChildrenCount(); i++) {	long childId = e.getChildren(i);	metadataMap.putDirChild(parentId, childId);	}	for (int i = e.getChildrenCount();	i < e.getChildrenCount() + e.getRefChildrenCount(); i++) {	int refId = e.getRefChildren(i - e.getChildrenCount());	metadataMap.putDirChild(parentId, refIdList.get(refId));	}	}	
scanned inode directories to build namespace 

private void outputINodes(InputStream in) throws IOException {	INodeSection s = INodeSection.parseDelimitedFrom(in);	
found inodes in the inode section 

long ignored = 0;	long ignoredSnapshots = 0;	for (int i = 0; i < s.getNumInodes(); ++i) {	INode p = INode.parseDelimitedFrom(in);	try {	String parentPath = metadataMap.getParentPath(p.getId());	out.println(getEntry(parentPath, p));	} catch (IOException ioe) {	ignored++;	if (!(ioe instanceof IgnoreSnapshotException)) {	
exception caught ignoring node 

INode p = INode.parseDelimitedFrom(in);	try {	String parentPath = metadataMap.getParentPath(p.getId());	out.println(getEntry(parentPath, p));	} catch (IOException ioe) {	ignored++;	if (!(ioe instanceof IgnoreSnapshotException)) {	} else {	ignoredSnapshots++;	if (LOG.isDebugEnabled()) {	
exception caught ignoring node 

} catch (IOException ioe) {	ignored++;	if (!(ioe instanceof IgnoreSnapshotException)) {	} else {	ignoredSnapshots++;	if (LOG.isDebugEnabled()) {	}	}	}	if (LOG.isDebugEnabled() && i % 100000 == 0) {	
outputted inodes 

} else {	ignoredSnapshots++;	if (LOG.isDebugEnabled()) {	}	}	}	if (LOG.isDebugEnabled() && i % 100000 == 0) {	}	}	if (ignored > 0) {	
ignored nodes including in snapshots please turn on debug log for details 

ignoredSnapshots++;	if (LOG.isDebugEnabled()) {	}	}	}	if (LOG.isDebugEnabled() && i % 100000 == 0) {	}	}	if (ignored > 0) {	}	
outputted inodes 

static void ignoreSnapshotName(long inode) throws IOException {	if (LOG.isDebugEnabled()) {	
no snapshot name found for inode 

========================= hadoop sample_7791 =========================

}	final BlockInfo truncatedBlockUC;	BlockManager blockManager = fsn.getFSDirectory().getBlockManager();	if (shouldCopyOnTruncate) {	truncatedBlockUC = new BlockInfoContiguous(newBlock, file.getPreferredBlockReplication());	truncatedBlockUC.convertToBlockUnderConstruction( BlockUCState.UNDER_CONSTRUCTION, blockManager.getStorages(oldBlock));	truncatedBlockUC.setNumBytes(oldBlock.getNumBytes() - lastBlockDelta);	truncatedBlockUC.getUnderConstructionFeature().setTruncateBlock(oldBlock);	file.setLastBlock(truncatedBlockUC);	blockManager.addBlockCollection(truncatedBlockUC, file);	
block preparefilefortruncate scheduling copy on truncate to new size new block old block 

blockManager.addBlockCollection(truncatedBlockUC, file);	} else {	blockManager.convertLastBlockToUnderConstruction(file, lastBlockDelta);	oldBlock = file.getLastBlock();	assert !oldBlock.isComplete() : "oldBlock should be under construction";	BlockUnderConstructionFeature uc = oldBlock.getUnderConstructionFeature();	uc.setTruncateBlock(new Block(oldBlock));	uc.getTruncateBlock().setNumBytes(oldBlock.getNumBytes() - lastBlockDelta);	uc.getTruncateBlock().setGenerationStamp(newBlock.getGenerationStamp());	truncatedBlockUC = oldBlock;	
block preparefilefortruncate scheduling in place block truncate to new size 

========================= hadoop sample_8034 =========================

entity.addMetric(metric);	entity.addConfig("foo", "bar");	TimelineEntities entities = new TimelineEntities();	entities.addEntity(entity);	UserGroupInformation ugi = UserGroupInformation.getCurrentUser();	long startWrite = System.nanoTime();	try {	collector.putEntities(entities, ugi);	} catch (Exception e) {	context.getCounter(PerfCounters.TIMELINE_SERVICE_WRITE_FAILURES). increment(1);	
writing to the timeline service failed 

UserGroupInformation ugi = UserGroupInformation.getCurrentUser();	long startWrite = System.nanoTime();	try {	collector.putEntities(entities, ugi);	} catch (Exception e) {	context.getCounter(PerfCounters.TIMELINE_SERVICE_WRITE_FAILURES). increment(1);	}	long endWrite = System.nanoTime();	totalTime += TimeUnit.NANOSECONDS.toMillis(endWrite-startWrite);	}	
wrote entities kb in ms 

========================= hadoop sample_5606 =========================

static LocalResource createJar(FileContext files, Path p, LocalResourceVisibility vis) throws IOException {	
create jar file 

static LocalResource createJar(FileContext files, Path p, LocalResourceVisibility vis) throws IOException {	File jarFile = new File((files.makeQualified(p)).toUri());	FileOutputStream stream = new FileOutputStream(jarFile);	
create jar out stream 

static LocalResource createJar(FileContext files, Path p, LocalResourceVisibility vis) throws IOException {	File jarFile = new File((files.makeQualified(p)).toUri());	FileOutputStream stream = new FileOutputStream(jarFile);	JarOutputStream out = new JarOutputStream(stream, new Manifest());	
done writing jar stream 

========================= hadoop sample_2086 =========================

public void testBackgroundDelete() throws Throwable {	mkPath("/rm", CreateMode.PERSISTENT);	mkPath("/rm/child", CreateMode.PERSISTENT);	CuratorEventCatcher events = new CuratorEventCatcher();	curatorService.zkDelete("/rm", true, events);	CuratorEvent taken = events.take();	
took 

========================= hadoop sample_2655 =========================

private synchronized ClientDatanodeProtocol getDatanodeProxy( UserGroupInformation ugi, final DatanodeInfo node, final Configuration conf, final int socketTimeout, final boolean connectToDnViaHostname) throws IOException {	if (proxy == null) {	try {	proxy = ugi.doAs(new PrivilegedExceptionAction<ClientDatanodeProtocol>() {	public ClientDatanodeProtocol run() throws Exception {	return DFSUtilClient.createClientDatanodeProtocolProxy(node, conf, socketTimeout, connectToDnViaHostname);	}	});	} catch (InterruptedException e) {	
encountered exception 

}	pathinfo = getBlockPathInfo(userGroupInformation, blk, node, configuration, conf.getSocketTimeout(), token, conf.isConnectToDnViaHostname(), storageType);	}	FileInputStream dataIn = null;	FileInputStream checksumIn = null;	BlockReaderLocalLegacy localBlockReader = null;	final boolean skipChecksumCheck = scConf.isSkipShortCircuitChecksums() || storageType.isTransient();	try {	File blkfile = new File(pathinfo.getBlockPath());	dataIn = new FileInputStream(blkfile);	
new blockreaderlocallegacy for file of size startoffset length short circuit checksum 

File metafile = new File(pathinfo.getMetaPath());	checksumIn = new FileInputStream(metafile);	final DataChecksum checksum = BlockMetadataHeader.readDataChecksum( new DataInputStream(checksumIn), blk);	long firstChunkOffset = startOffset - (startOffset % checksum.getBytesPerChecksum());	localBlockReader = new BlockReaderLocalLegacy(scConf, file, blk, startOffset, checksum, true, dataIn, firstChunkOffset, checksumIn, tracer);	} else {	localBlockReader = new BlockReaderLocalLegacy(scConf, file, blk, startOffset, dataIn, tracer);	}	} catch (IOException e) {	localDatanodeInfo.removeBlockLocalPathInfo(blk);	
blockreaderlocallegacy removing from cache because local file could not be opened 

private static BlockLocalPathInfo getBlockPathInfo(UserGroupInformation ugi, ExtendedBlock blk, DatanodeInfo node, Configuration conf, int timeout, Token<BlockTokenIdentifier> token, boolean connectToDnViaHostname, StorageType storageType) throws IOException {	LocalDatanodeInfo localDatanodeInfo = getLocalDatanodeInfo(node.getIpcPort());	BlockLocalPathInfo pathinfo;	ClientDatanodeProtocol proxy = localDatanodeInfo.getDatanodeProxy(ugi, node, conf, timeout, connectToDnViaHostname);	try {	pathinfo = proxy.getBlockLocalPathInfo(blk, token);	if (pathinfo != null && !storageType.isTransient()) {	
cached location of block as 

public synchronized int read(byte[] buf, int off, int len) throws IOException {	
read off len 

public synchronized long skip(long n) throws IOException {	
skip 

========================= hadoop sample_7007 =========================

public Token<ContainerTokenIdentifier> selectToken(Text service, Collection<Token<? extends TokenIdentifier>> tokens) {	if (service == null) {	return null;	}	for (Token<? extends TokenIdentifier> token : tokens) {	if (LOG.isDebugEnabled()) {	
looking for service current token is 

========================= hadoop sample_2205 =========================

private void largeFiles(FileSystem srcFS, Path srcDir, FileSystem dstFS, Path dstDir) throws Exception {	Path inputDir = new Path(srcDir, "inputDir");	Path inputFile1 = new Path(inputDir, "file1");	Path inputFile2 = new Path(inputDir, "file2");	Path inputFile3 = new Path(inputDir, "file3");	mkdirs(srcFS, inputDir);	int fileSizeKb = conf.getInt("scale.test.distcp.file.size.kb", 10 * 1024);	int fileSizeMb = fileSizeKb / 1024;	
with file size 

========================= hadoop sample_6246 =========================

Configuration conf = getConf();	conf.setInt(SOCKET_SEND_BUFFER, 16 * 1024);	conf.setInt(SOCKET_RECV_BUFFER, 16 * 1024);	String testFile =  conf.getTrimmed(KEY_CSVTEST_FILE, DEFAULT_CSVTEST_FILE);	if (testFile.isEmpty()) {	assumptionMessage = "Empty test property: " + KEY_CSVTEST_FILE;	LOG.warn(assumptionMessage);	testDataAvailable = false;	} else {	testData = new Path(testFile);	
using as input stream source 

assumptionMessage = "Empty test property: " + KEY_CSVTEST_FILE;	LOG.warn(assumptionMessage);	testDataAvailable = false;	} else {	testData = new Path(testFile);	Path path = this.testData;	bindS3aFS(path);	try {	testDataStatus = s3aFS.getFileStatus(this.testData);	} catch (IOException e) {	
failed to read file specified in 

if (bytesRead == 1) {	break;	}	remaining -= bytesRead;	offset += bytesRead;	count += bytesRead;	readTimer.end();	if (bytesRead != 0) {	LOG.debug("Bytes in read #{}: {} , block bytes: {}," + " remaining in block: {}" + " duration={} nS; ns/byte: {}, bandwidth={} MB/s", reads, bytesRead, blockSize - remaining, remaining, readTimer.duration(), readTimer.nanosPerOperation(bytesRead), readTimer.bandwidthDescription(bytesRead));	} else {	
bytes returned by read operation 

offset += bytesRead;	count += bytesRead;	readTimer.end();	if (bytesRead != 0) {	LOG.debug("Bytes in read #{}: {} , block bytes: {}," + " remaining in block: {}" + " duration={} nS; ns/byte: {}, bandwidth={} MB/s", reads, bytesRead, blockSize - remaining, remaining, readTimer.duration(), readTimer.nanosPerOperation(bytesRead), readTimer.bandwidthDescription(bytesRead));	} else {	}	}	blockTimer.end("Reading block %d in %d reads", blockId, reads);	String bw = blockTimer.bandwidthDescription(blockSize);	
bandwidth of block mb s 

count += bytesRead;	readTimer.end();	if (bytesRead != 0) {	LOG.debug("Bytes in read #{}: {} , block bytes: {}," + " remaining in block: {}" + " duration={} nS; ns/byte: {}, bandwidth={} MB/s", reads, bytesRead, blockSize - remaining, remaining, readTimer.duration(), readTimer.nanosPerOperation(bytesRead), readTimer.bandwidthDescription(bytesRead));	} else {	}	}	blockTimer.end("Reading block %d in %d reads", blockId, reads);	String bw = blockTimer.bandwidthDescription(blockSize);	if (bandwidth(blockTimer, blockSize) < minimumBandwidth) {	
bandwidth too low on block resetting connection 

}	blockTimer.end("Reading block %d in %d reads", blockId, reads);	String bw = blockTimer.bandwidthDescription(blockSize);	if (bandwidth(blockTimer, blockSize) < minimumBandwidth) {	Assert.assertTrue("Bandwidth of " + bw +" too low after  " + resetCount + " attempts", resetCount <= maxResetCount);	resetCount++;	getS3AInputStream(in).resetConnection();	}	}	timer2.end("Time to read %d bytes in %d blocks", totalToRead, blockCount);	
overall bandwidth mb s reset connections 

long len = testDataStatus.getLen();	NanoTimer timer = new NanoTimer();	long blockCount = len / blockSize;	LOG.info("Reading {} blocks, readahead = {}", blockCount, readahead);	for (long i = 0; i < blockCount; i++) {	in.seek(in.getPos() + blockSize - 1);	assertTrue(in.read() >= 0);	}	timer.end("Time to execute %d seeks of distance %d with readahead = %d", blockCount, blockSize, readahead);	logTimePerIOP("seek(pos + " + blockCount+"); read()", timer, blockCount);	
effective bandwidth mb s 

int position = action[0];	int range = action[1];	in.readFully(position, buffer, 0, range);	totalBytesRead += range;	}	int reads = RANDOM_IO_SEQUENCE.length;	timer.end("Time to execute %d reads of total size %d bytes", reads, totalBytesRead);	in.close();	assertOpenOperationCount(expectedOpenCount);	logTimePerIOP("byte read", timer, totalBytesRead);	
effective bandwidth mb s 

describe("read over a buffer, making sure that the requests" + " spans readahead ranges");	int datasetLen = _32K;	S3AFileSystem fs = getFileSystem();	Path dataFile = path("testReadOverBuffer.bin");	byte[] sourceData = dataset(datasetLen, 0, 64);	writeDataset(fs, dataFile, sourceData, datasetLen, _16K, true);	byte[] buffer = new byte[datasetLen];	int readahead = _8K;	int halfReadahead = _4K;	in = openDataFile(fs, dataFile, S3AInputPolicy.Random, readahead);	
starting initial reads 

assertEquals("open operations on request #" + readOps + " after reading " + bytesRead + " current position in stream " + currentPos + " in\n" + fs + "\n " + in, 1, streamStatistics.openOperations);	for (int i = currentPos; i < currentPos + read; i++) {	assertEquals("Wrong value from byte " + i, sourceData[i], buffer[i]);	}	currentPos += read;	}	assertStreamOpenedExactlyOnce();	assertEquals(readahead, currentPos);	readTimer.end("read %d in %d operations", bytesRead, readOps);	bandwidth(readTimer, bytesRead);	
time per byte ns 

assertEquals("open operations on request #" + readOps + " after reading " + bytesRead + " current position in stream " + currentPos + " in\n" + fs + "\n " + in, 1, streamStatistics.openOperations);	for (int i = currentPos; i < currentPos + read; i++) {	assertEquals("Wrong value from byte " + i, sourceData[i], buffer[i]);	}	currentPos += read;	}	assertStreamOpenedExactlyOnce();	assertEquals(readahead, currentPos);	readTimer.end("read %d in %d operations", bytesRead, readOps);	bandwidth(readTimer, bytesRead);	
time per read ns 

bandwidth(readTimer, bytesRead);	describe("read last byte");	int read = in.read(currentPos, buffer, bytesRead, 1);	assertTrue("-1 from last read", read >= 0);	assertOpenOperationCount(2);	assertEquals("Wrong value from read ", sourceData[currentPos], (int) buffer[currentPos]);	currentPos++;	describe("read() to EOF over \n%s", in);	long readCount = 0;	NanoTimer timer = new NanoTimer();	
seeking 

describe("read last byte");	int read = in.read(currentPos, buffer, bytesRead, 1);	assertTrue("-1 from last read", read >= 0);	assertOpenOperationCount(2);	assertEquals("Wrong value from read ", sourceData[currentPos], (int) buffer[currentPos]);	currentPos++;	describe("read() to EOF over \n%s", in);	long readCount = 0;	NanoTimer timer = new NanoTimer();	in.seek(currentPos);	
reading 

while(currentPos < datasetLen) {	int r = in.read();	assertTrue("Negative read() at position " + currentPos + " in\n" + in, r >= 0);	buffer[currentPos] = (byte)r;	assertEquals("Wrong value from read from\n" + in, sourceData[currentPos], r);	currentPos++;	readCount++;	}	timer.end("read %d bytes", readCount);	bandwidth(timer, readCount);	
time per read ns 

========================= hadoop sample_5893 =========================

public void close() throws IOException {	if (ris.getDataInFd() != null && ((dropCacheBehindAllReads) || (dropCacheBehindLargeReads && isLongRead()))) {	try {	ris.dropCacheBehindReads(block.getBlockName(), lastCacheDropOffset, offset - lastCacheDropOffset, POSIX_FADV_DONTNEED);	} catch (Exception e) {	
unable to drop cache on file close 

datanode.metrics.addSendDataPacketTransferNanos(transferTime.get());	blockInPosition += dataLen;	} else {	out.write(buf, headerOff, dataOff + dataLen - headerOff);	}	} catch (IOException e) {	if (e instanceof SocketTimeoutException) {	} else {	String ioem = e.getMessage();	if (!ioem.startsWith("Broken pipe") && !ioem.startsWith("Connection reset")) {	
blocksender sendchunks exception 

private void readChecksum(byte[] buf, final int checksumOffset, final int checksumLen) throws IOException {	if (checksumSize <= 0 && ris.getChecksumIn() == null) {	return;	}	try {	ris.readChecksumFully(buf, checksumOffset, checksumLen);	} catch (IOException e) {	
could not read or failed to verify checksum for data at offset for block 

========================= hadoop sample_7904 =========================

protected boolean removeEldestEntry(Map.Entry<Collection<MetricsTag>, Record> eldest) {	boolean overflow = size() > maxRecsPerName;	if (overflow && !gotOverflow) {	
metrics cache overflow at for 

========================= hadoop sample_3414 =========================

public static SubClusterState fromString(String x) {	try {	return SubClusterState.valueOf(x);	} catch (Exception e) {	
invalid subcluster state value in the statestore does not match with the yarn federation standard 

========================= hadoop sample_1429 =========================

public void testConfAsEnvironment() {	if (!Shell.WINDOWS) {	fencer.tryFence(TEST_TARGET, "echo $in_fencing_tests");	
echo in ing tests yessir 

public void testConfAsEnvironment() {	if (!Shell.WINDOWS) {	fencer.tryFence(TEST_TARGET, "echo $in_fencing_tests");	} else {	fencer.tryFence(TEST_TARGET, "echo %in_fencing_tests%");	
echo in ng tests yessir 

public void testTargetAsEnvironment() {	if (!Shell.WINDOWS) {	fencer.tryFence(TEST_TARGET, "echo $target_host $target_port");	
echo ta rget port dummyhost 

public void testTargetAsEnvironment() {	if (!Shell.WINDOWS) {	fencer.tryFence(TEST_TARGET, "echo $target_host $target_port");	} else {	fencer.tryFence(TEST_TARGET, "echo %target_host% %target_port%");	
echo ta get port dummyhost 

========================= hadoop sample_3101 =========================

public void updateDemand() {	writeLock.lock();	try {	demand = Resources.createResource(0);	for (FSQueue childQueue : childQueues) {	childQueue.updateDemand();	Resource toAdd = childQueue.getDemand();	demand = Resources.add(demand, toAdd);	if (LOG.isDebugEnabled()) {	
counting resource from total resource demand for now 

Resource toAdd = childQueue.getDemand();	demand = Resources.add(demand, toAdd);	if (LOG.isDebugEnabled()) {	}	}	demand = Resources.componentwiseMin(demand, getMaxShare());	} finally {	writeLock.unlock();	}	if (LOG.isDebugEnabled()) {	
the updated demand for is the max is 

========================= hadoop sample_958 =========================

try {	Thread.sleep(SLEEP_TIME);	} catch (InterruptedException e) {}	System.exit(100);	}	result = appMaster.finish();	} catch (Throwable t) {	System.exit(1);	}	if (result) {	
application master completed successfully exiting 

} catch (InterruptedException e) {}	System.exit(100);	}	result = appMaster.finish();	} catch (Throwable t) {	System.exit(1);	}	if (result) {	System.exit(0);	} else {	
application master failed exiting 

========================= hadoop sample_1 =========================

public void startMerge(Set<T> inputs) {	if (!closed) {	numPending.incrementAndGet();	List<T> toMergeInputs = new ArrayList<T>();	Iterator<T> iter=inputs.iterator();	for (int ctr = 0; iter.hasNext() && ctr < mergeFactor; ++ctr) {	toMergeInputs.add(iter.next());	iter.remove();	}	
starting merge with segments while ignoring segments 

========================= hadoop sample_4943 =========================

public void stop() throws IOException {	rollEditsRpcExecutor.shutdown();	tailerThread.setShouldRun(false);	tailerThread.interrupt();	try {	tailerThread.join();	} catch (InterruptedException e) {	
edit log tailer thread exited with an exception 

break;	}	namesystem.cpLockInterruptibly();	try {	doTailEdits();	} finally {	namesystem.cpUnlock();	}	namesystem.getFSImage().getStorage().updateNameDirSize();	} catch (EditLogInputException elie) {	
error while reading edits from disk will try again 

try {	doTailEdits();	} finally {	namesystem.cpUnlock();	}	namesystem.getFSImage().getStorage().updateNameDirSize();	} catch (EditLogInputException elie) {	} catch (InterruptedException ie) {	continue;	} catch (Throwable t) {	
unknown error encountered while tailing edits shutting down standby nn 

namesystem.getFSImage().getStorage().updateNameDirSize();	} catch (EditLogInputException elie) {	} catch (InterruptedException ie) {	continue;	} catch (Throwable t) {	terminate(1, t);	}	try {	Thread.sleep(sleepTimeMs);	} catch (InterruptedException e) {	
edit log tailer interrupted 

========================= hadoop sample_8063 =========================

public Response createKey(Map jsonKey) throws Exception {	try{	
entering createkey method 

KMSWebApp.getAdminCallsMeter().mark();	UserGroupInformation user = HttpUserGroupInformation.get();	final String name = (String) jsonKey.get(KMSRESTConstants.NAME_FIELD);	KMSClientProvider.checkNotEmpty(name, KMSRESTConstants.NAME_FIELD);	assertAccess(KMSACLs.Type.CREATE, user, KMSOp.CREATE_KEY, name);	String cipher = (String) jsonKey.get(KMSRESTConstants.CIPHER_FIELD);	final String material;	material = (String) jsonKey.get(KMSRESTConstants.MATERIAL_FIELD);	int length = (jsonKey.containsKey(KMSRESTConstants.LENGTH_FIELD)) ? (Integer) jsonKey.get(KMSRESTConstants.LENGTH_FIELD) : 0;	String description = (String) jsonKey.get(KMSRESTConstants.DESCRIPTION_FIELD);	
creating key with name cipher being used length of key description of key 

}	);	kmsAudit.ok(user, KMSOp.CREATE_KEY, name, "UserProvidedMaterial:" + (material != null) + " Description:" + description);	if (!KMSWebApp.getACLs().hasAccess(KMSACLs.Type.GET, user)) {	keyVersion = removeKeyMaterial(keyVersion);	}	Map json = KMSServerJSONUtils.toJSON(keyVersion);	String requestURL = KMSMDCFilter.getURL();	int idx = requestURL.lastIndexOf(KMSRESTConstants.KEYS_RESOURCE);	requestURL = requestURL.substring(0, idx);	
exiting createkey method 

kmsAudit.ok(user, KMSOp.CREATE_KEY, name, "UserProvidedMaterial:" + (material != null) + " Description:" + description);	if (!KMSWebApp.getACLs().hasAccess(KMSACLs.Type.GET, user)) {	keyVersion = removeKeyMaterial(keyVersion);	}	Map json = KMSServerJSONUtils.toJSON(keyVersion);	String requestURL = KMSMDCFilter.getURL();	int idx = requestURL.lastIndexOf(KMSRESTConstants.KEYS_RESOURCE);	requestURL = requestURL.substring(0, idx);	return Response.created(getKeyURI(KMSRESTConstants.SERVICE_VERSION, name)) .type(MediaType.APPLICATION_JSON) .header("Location", getKeyURI(requestURL, name)).entity(json).build();	} catch (Exception e) {	
exception in createkey 

public Response deleteKey(@PathParam("name") final String name) throws Exception {	try {	
entering deletekey method 

public Response deleteKey(@PathParam("name") final String name) throws Exception {	try {	KMSWebApp.getAdminCallsMeter().mark();	UserGroupInformation user = HttpUserGroupInformation.get();	assertAccess(KMSACLs.Type.DELETE, user, KMSOp.DELETE_KEY, name);	KMSClientProvider.checkNotEmpty(name, "name");	
deleting key with name 

assertAccess(KMSACLs.Type.DELETE, user, KMSOp.DELETE_KEY, name);	KMSClientProvider.checkNotEmpty(name, "name");	user.doAs(new PrivilegedExceptionAction<Void>() {	public Void run() throws Exception {	provider.deleteKey(name);	provider.flush();	return null;	}	});	kmsAudit.ok(user, KMSOp.DELETE_KEY, name, "");	
exiting deletekey method 

user.doAs(new PrivilegedExceptionAction<Void>() {	public Void run() throws Exception {	provider.deleteKey(name);	provider.flush();	return null;	}	});	kmsAudit.ok(user, KMSOp.DELETE_KEY, name, "");	return Response.ok().build();	} catch (Exception e) {	
exception in deletekey 

public Response rolloverKey(@PathParam("name") final String name, Map jsonMaterial) throws Exception {	try {	
entering rolloverkey method 

public Response rolloverKey(@PathParam("name") final String name, Map jsonMaterial) throws Exception {	try {	KMSWebApp.getAdminCallsMeter().mark();	UserGroupInformation user = HttpUserGroupInformation.get();	assertAccess(KMSACLs.Type.ROLLOVER, user, KMSOp.ROLL_NEW_VERSION, name);	KMSClientProvider.checkNotEmpty(name, "name");	
rolling key with name 

provider.flush();	return keyVersion;	}	}	);	kmsAudit.ok(user, KMSOp.ROLL_NEW_VERSION, name, "UserProvidedMaterial:" + (material != null) + " NewVersion:" + keyVersion.getVersionName());	if (!KMSWebApp.getACLs().hasAccess(KMSACLs.Type.GET, user)) {	keyVersion = removeKeyMaterial(keyVersion);	}	Map json = KMSServerJSONUtils.toJSON(keyVersion);	
exiting rolloverkey method 

}	}	);	kmsAudit.ok(user, KMSOp.ROLL_NEW_VERSION, name, "UserProvidedMaterial:" + (material != null) + " NewVersion:" + keyVersion.getVersionName());	if (!KMSWebApp.getACLs().hasAccess(KMSACLs.Type.GET, user)) {	keyVersion = removeKeyMaterial(keyVersion);	}	Map json = KMSServerJSONUtils.toJSON(keyVersion);	return Response.ok().type(MediaType.APPLICATION_JSON).entity(json) .build();	} catch (Exception e) {	
exception in rolloverkey 

public Response getKeysMetadata(@QueryParam(KMSRESTConstants.KEY) List<String> keyNamesList) throws Exception {	try {	
entering getkeysmetadata method 

final String[] keyNames = keyNamesList.toArray( new String[keyNamesList.size()]);	assertAccess(KMSACLs.Type.GET_METADATA, user, KMSOp.GET_KEYS_METADATA);	KeyProvider.Metadata[] keysMeta = user.doAs( new PrivilegedExceptionAction<KeyProvider.Metadata[]>() {	public KeyProvider.Metadata[] run() throws Exception {	return provider.getKeysMetadata(keyNames);	}	}	);	Object json = KMSServerJSONUtils.toJSON(keyNames, keysMeta);	kmsAudit.ok(user, KMSOp.GET_KEYS_METADATA, "");	
exiting getkeysmetadata method 

KeyProvider.Metadata[] keysMeta = user.doAs( new PrivilegedExceptionAction<KeyProvider.Metadata[]>() {	public KeyProvider.Metadata[] run() throws Exception {	return provider.getKeysMetadata(keyNames);	}	}	);	Object json = KMSServerJSONUtils.toJSON(keyNames, keysMeta);	kmsAudit.ok(user, KMSOp.GET_KEYS_METADATA, "");	return Response.ok().type(MediaType.APPLICATION_JSON).entity(json) .build();	} catch (Exception e) {	
exception in getkeysmetadata 

public Response getKeyNames() throws Exception {	try {	
entering getkeynames method 

KMSWebApp.getAdminCallsMeter().mark();	UserGroupInformation user = HttpUserGroupInformation.get();	assertAccess(KMSACLs.Type.GET_KEYS, user, KMSOp.GET_KEYS);	List<String> json = user.doAs( new PrivilegedExceptionAction<List<String>>() {	public List<String> run() throws Exception {	return provider.getKeys();	}	}	);	kmsAudit.ok(user, KMSOp.GET_KEYS, "");	
exiting getkeynames method 

assertAccess(KMSACLs.Type.GET_KEYS, user, KMSOp.GET_KEYS);	List<String> json = user.doAs( new PrivilegedExceptionAction<List<String>>() {	public List<String> run() throws Exception {	return provider.getKeys();	}	}	);	kmsAudit.ok(user, KMSOp.GET_KEYS, "");	return Response.ok().type(MediaType.APPLICATION_JSON).entity(json) .build();	} catch (Exception e) {	
exception in getkeynames 

public Response getKey(@PathParam("name") String name) throws Exception {	try {	
entering getkey method 

public Response getKey(@PathParam("name") String name) throws Exception {	try {	
getting key information for key with name 

public Response getKey(@PathParam("name") String name) throws Exception {	try {	
exiting getkey method 

public Response getKey(@PathParam("name") String name) throws Exception {	try {	return getMetadata(name);	} catch (Exception e) {	
exception in getkey 

public Response getMetadata(@PathParam("name") final String name) throws Exception {	try {	
entering getmetadata method 

public Response getMetadata(@PathParam("name") final String name) throws Exception {	try {	UserGroupInformation user = HttpUserGroupInformation.get();	KMSClientProvider.checkNotEmpty(name, "name");	KMSWebApp.getAdminCallsMeter().mark();	assertAccess(KMSACLs.Type.GET_METADATA, user, KMSOp.GET_METADATA, name);	
getting metadata for key with name 

KMSWebApp.getAdminCallsMeter().mark();	assertAccess(KMSACLs.Type.GET_METADATA, user, KMSOp.GET_METADATA, name);	KeyProvider.Metadata metadata = user.doAs( new PrivilegedExceptionAction<KeyProvider.Metadata>() {	public KeyProvider.Metadata run() throws Exception {	return provider.getMetadata(name);	}	}	);	Object json = KMSServerJSONUtils.toJSON(name, metadata);	kmsAudit.ok(user, KMSOp.GET_METADATA, name, "");	
exiting getmetadata method 

KeyProvider.Metadata metadata = user.doAs( new PrivilegedExceptionAction<KeyProvider.Metadata>() {	public KeyProvider.Metadata run() throws Exception {	return provider.getMetadata(name);	}	}	);	Object json = KMSServerJSONUtils.toJSON(name, metadata);	kmsAudit.ok(user, KMSOp.GET_METADATA, name, "");	return Response.ok().type(MediaType.APPLICATION_JSON).entity(json) .build();	} catch (Exception e) {	
exception in getmetadata 

public Response getCurrentVersion(@PathParam("name") final String name) throws Exception {	try {	
entering getcurrentversion method 

public Response getCurrentVersion(@PathParam("name") final String name) throws Exception {	try {	UserGroupInformation user = HttpUserGroupInformation.get();	KMSClientProvider.checkNotEmpty(name, "name");	KMSWebApp.getKeyCallsMeter().mark();	assertAccess(KMSACLs.Type.GET, user, KMSOp.GET_CURRENT_KEY, name);	
getting key version for key with name 

KMSWebApp.getKeyCallsMeter().mark();	assertAccess(KMSACLs.Type.GET, user, KMSOp.GET_CURRENT_KEY, name);	KeyVersion keyVersion = user.doAs( new PrivilegedExceptionAction<KeyVersion>() {	public KeyVersion run() throws Exception {	return provider.getCurrentKey(name);	}	}	);	Object json = KMSServerJSONUtils.toJSON(keyVersion);	kmsAudit.ok(user, KMSOp.GET_CURRENT_KEY, name, "");	
exiting getcurrentversion method 

KeyVersion keyVersion = user.doAs( new PrivilegedExceptionAction<KeyVersion>() {	public KeyVersion run() throws Exception {	return provider.getCurrentKey(name);	}	}	);	Object json = KMSServerJSONUtils.toJSON(keyVersion);	kmsAudit.ok(user, KMSOp.GET_CURRENT_KEY, name, "");	return Response.ok().type(MediaType.APPLICATION_JSON).entity(json) .build();	} catch (Exception e) {	
exception in getcurrentversion 

public Response getKeyVersion( try {	
entering getkeyversion method 

public Response getKeyVersion( try {	UserGroupInformation user = HttpUserGroupInformation.get();	KMSClientProvider.checkNotEmpty(versionName, "versionName");	KMSWebApp.getKeyCallsMeter().mark();	assertAccess(KMSACLs.Type.GET, user, KMSOp.GET_KEY_VERSION);	
getting key with version name 

KeyVersion keyVersion = user.doAs( new PrivilegedExceptionAction<KeyVersion>() {	public KeyVersion run() throws Exception {	return provider.getKeyVersion(versionName);	}	}	);	if (keyVersion != null) {	kmsAudit.ok(user, KMSOp.GET_KEY_VERSION, keyVersion.getName(), "");	}	Object json = KMSServerJSONUtils.toJSON(keyVersion);	
exiting getkeyversion method 

return provider.getKeyVersion(versionName);	}	}	);	if (keyVersion != null) {	kmsAudit.ok(user, KMSOp.GET_KEY_VERSION, keyVersion.getName(), "");	}	Object json = KMSServerJSONUtils.toJSON(keyVersion);	return Response.ok().type(MediaType.APPLICATION_JSON).entity(json) .build();	} catch (Exception e) {	
exception in getkeyversion 

public Response generateEncryptedKeys( throws Exception {	try {	
entering generateencryptedkeys method 

public Response generateEncryptedKeys( throws Exception {	try {	UserGroupInformation user = HttpUserGroupInformation.get();	KMSClientProvider.checkNotEmpty(name, "name");	KMSClientProvider.checkNotNull(edekOp, "eekOp");	
generating encrypted key with name the edek operation is 

public Response generateEncryptedKeys( throws Exception {	try {	UserGroupInformation user = HttpUserGroupInformation.get();	KMSClientProvider.checkNotEmpty(name, "name");	KMSClientProvider.checkNotNull(edekOp, "eekOp");	Object retJSON;	if (edekOp.equals(KMSRESTConstants.EEK_GENERATE)) {	
edek operation is generate 

UserGroupInformation user = HttpUserGroupInformation.get();	KMSClientProvider.checkNotEmpty(name, "name");	KMSClientProvider.checkNotNull(edekOp, "eekOp");	Object retJSON;	if (edekOp.equals(KMSRESTConstants.EEK_GENERATE)) {	assertAccess(KMSACLs.Type.GENERATE_EEK, user, KMSOp.GENERATE_EEK, name);	final List<EncryptedKeyVersion> retEdeks = new LinkedList<EncryptedKeyVersion>();	try {	user.doAs( new PrivilegedExceptionAction<Void>() {	public Void run() throws Exception {	
generated encrypted key for number of keys 

user.doAs( new PrivilegedExceptionAction<Void>() {	public Void run() throws Exception {	for (int i = 0; i < numKeys; i++) {	retEdeks.add(provider.generateEncryptedKey(name));	}	return null;	}	}	);	} catch (Exception e) {	
exception in generateencryptedkeys 

error = new StringBuilder("IllegalArgumentException Wrong ");	error.append(KMSRESTConstants.EEK_OP);	error.append(" value, it must be ");	error.append(KMSRESTConstants.EEK_GENERATE);	error.append(" or ");	error.append(KMSRESTConstants.EEK_DECRYPT);	LOG.error(error.toString());	throw new IllegalArgumentException(error.toString());	}	KMSWebApp.getGenerateEEKCallsMeter().mark();	
exiting generateencryptedkeys method 

error.append(" value, it must be ");	error.append(KMSRESTConstants.EEK_GENERATE);	error.append(" or ");	error.append(KMSRESTConstants.EEK_DECRYPT);	LOG.error(error.toString());	throw new IllegalArgumentException(error.toString());	}	KMSWebApp.getGenerateEEKCallsMeter().mark();	return Response.ok().type(MediaType.APPLICATION_JSON).entity(retJSON) .build();	} catch (Exception e) {	
exception in generateencryptedkeys 

public Response decryptEncryptedKey( Map jsonPayload) throws Exception {	try {	
entering decryptencryptedkey method 

public Response decryptEncryptedKey( Map jsonPayload) throws Exception {	try {	UserGroupInformation user = HttpUserGroupInformation.get();	KMSClientProvider.checkNotEmpty(versionName, "versionName");	KMSClientProvider.checkNotNull(eekOp, "eekOp");	
decrypting key for the edek operation is 

error = new StringBuilder("IllegalArgumentException Wrong ");	error.append(KMSRESTConstants.EEK_OP);	error.append(" value, it must be ");	error.append(KMSRESTConstants.EEK_GENERATE);	error.append(" or ");	error.append(KMSRESTConstants.EEK_DECRYPT);	LOG.error(error.toString());	throw new IllegalArgumentException(error.toString());	}	KMSWebApp.getDecryptEEKCallsMeter().mark();	
exiting decryptencryptedkey method 

error.append(" value, it must be ");	error.append(KMSRESTConstants.EEK_GENERATE);	error.append(" or ");	error.append(KMSRESTConstants.EEK_DECRYPT);	LOG.error(error.toString());	throw new IllegalArgumentException(error.toString());	}	KMSWebApp.getDecryptEEKCallsMeter().mark();	return Response.ok().type(MediaType.APPLICATION_JSON).entity(retJSON) .build();	} catch (Exception e) {	
exception in decryptencryptedkey 

public Response getKeyVersions(@PathParam("name") final String name) throws Exception {	try {	
entering getkeyversions method 

public Response getKeyVersions(@PathParam("name") final String name) throws Exception {	try {	UserGroupInformation user = HttpUserGroupInformation.get();	KMSClientProvider.checkNotEmpty(name, "name");	KMSWebApp.getKeyCallsMeter().mark();	assertAccess(KMSACLs.Type.GET, user, KMSOp.GET_KEY_VERSIONS, name);	
getting key versions for key 

KMSWebApp.getKeyCallsMeter().mark();	assertAccess(KMSACLs.Type.GET, user, KMSOp.GET_KEY_VERSIONS, name);	List<KeyVersion> ret = user.doAs( new PrivilegedExceptionAction<List<KeyVersion>>() {	public List<KeyVersion> run() throws Exception {	return provider.getKeyVersions(name);	}	}	);	Object json = KMSServerJSONUtils.toJSON(ret);	kmsAudit.ok(user, KMSOp.GET_KEY_VERSIONS, name, "");	
exiting getkeyversions method 

List<KeyVersion> ret = user.doAs( new PrivilegedExceptionAction<List<KeyVersion>>() {	public List<KeyVersion> run() throws Exception {	return provider.getKeyVersions(name);	}	}	);	Object json = KMSServerJSONUtils.toJSON(ret);	kmsAudit.ok(user, KMSOp.GET_KEY_VERSIONS, name, "");	return Response.ok().type(MediaType.APPLICATION_JSON).entity(json) .build();	} catch (Exception e) {	
exception in getkeyversions 

========================= hadoop sample_4410 =========================

public EchoUserResolver() {	
current user resolver is echouserresolver 

========================= hadoop sample_6138 =========================

conf.setBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, true);	TaskAttemptID taskId = new TaskAttemptID();	FakeRecordLLReader reader = new FakeRecordLLReader();	LoadRecordGkNullWriter writer = new LoadRecordGkNullWriter();	OutputCommitter committer = new CustomOutputCommitter();	StatusReporter reporter = new TaskAttemptContextImpl.DummyReporter();	SleepSplit split = getSleepSplit();	MapContext<LongWritable, LongWritable, GridmixKey, NullWritable> mapcontext = new MapContextImpl<LongWritable, LongWritable, GridmixKey, NullWritable>( conf, taskId, reader, writer, committer, reporter, split);	Context context = new WrappedMapper<LongWritable, LongWritable, GridmixKey, NullWritable>() .getMapContext(mapcontext);	long start = System.currentTimeMillis();	
start 

LoadRecordGkNullWriter writer = new LoadRecordGkNullWriter();	OutputCommitter committer = new CustomOutputCommitter();	StatusReporter reporter = new TaskAttemptContextImpl.DummyReporter();	SleepSplit split = getSleepSplit();	MapContext<LongWritable, LongWritable, GridmixKey, NullWritable> mapcontext = new MapContextImpl<LongWritable, LongWritable, GridmixKey, NullWritable>( conf, taskId, reader, writer, committer, reporter, split);	Context context = new WrappedMapper<LongWritable, LongWritable, GridmixKey, NullWritable>() .getMapContext(mapcontext);	long start = System.currentTimeMillis();	LongWritable key = new LongWritable(start + 2000);	LongWritable value = new LongWritable(start + 2000);	test.map(key, value, context);	
finish 

========================= hadoop sample_6092 =========================

private void handleMirrorOutError(IOException ioe) throws IOException {	String bpid = block.getBlockPoolId();	
exception writing to mirror 

private void verifyChunks(ByteBuffer dataBuf, ByteBuffer checksumBuf) throws IOException {	try {	clientChecksum.verifyChunkedSums(dataBuf, checksumBuf, clientname, 0);	} catch (ChecksumException ce) {	PacketHeader header = packetReceiver.getHeader();	String specificOffset = "specific offsets are:" + " offsetInBlock = " + header.getOffsetInBlock() + " offsetInPacket = " + ce.getPos();	
checksum error in block from 

private void verifyChunks(ByteBuffer dataBuf, ByteBuffer checksumBuf) throws IOException {	try {	clientChecksum.verifyChunkedSums(dataBuf, checksumBuf, clientname, 0);	} catch (ChecksumException ce) {	PacketHeader header = packetReceiver.getHeader();	String specificOffset = "specific offsets are:" + " offsetInBlock = " + header.getOffsetInBlock() + " offsetInPacket = " + ce.getPos();	if (srcDataNode != null && isDatanode) {	try {	
report corrupt from datanode to namenode 

private void verifyChunks(ByteBuffer dataBuf, ByteBuffer checksumBuf) throws IOException {	try {	clientChecksum.verifyChunkedSums(dataBuf, checksumBuf, clientname, 0);	} catch (ChecksumException ce) {	PacketHeader header = packetReceiver.getHeader();	String specificOffset = "specific offsets are:" + " offsetInBlock = " + header.getOffsetInBlock() + " offsetInPacket = " + ce.getPos();	if (srcDataNode != null && isDatanode) {	try {	datanode.reportRemoteBadBlock(srcDataNode, block);	} catch (IOException e) {	
failed to report bad from datanode to namenode 

private int receivePacket() throws IOException {	packetReceiver.receiveNextPacket(in);	PacketHeader header = packetReceiver.getHeader();	if (LOG.isDebugEnabled()){	
receiving one packet for block 

LOG.warn("Slow BlockReceiver write packet to mirror took " + duration + "ms (threshold=" + datanodeSlowLogThresholdMs + "ms), " + "downstream DNs=" + Arrays.toString(downstreamDNs));	}	} catch (IOException e) {	handleMirrorOutError(e);	}	}	ByteBuffer dataBuf = packetReceiver.getDataSlice();	ByteBuffer checksumBuf = packetReceiver.getChecksumSlice();	if (lastPacketInBlock || len == 0) {	if(LOG.isDebugEnabled()) {	
receiving an empty packet or the end of the block 

if (dropPos > 0 && dropCacheBehindWrites) {	streams.dropCacheBehindWrites(block.getBlockName(), 0, dropPos, POSIX_FADV_DONTNEED);	}	lastCacheManagementOffset = offsetInBlock;	long duration = Time.monotonicNow() - begin;	if (duration > datanodeSlowLogThresholdMs && LOG.isWarnEnabled()) {	LOG.warn("Slow manageWriterOsCache took " + duration + "ms (threshold=" + datanodeSlowLogThresholdMs + "ms), volume=" + getVolumeBasePath());	}	}	} catch (Throwable t) {	
error managing cache for writer of block 

private void initPerfMonitoring(DatanodeInfo[] downstreams) {	if (downstreams != null && downstreams.length > 0) {	downstreamDNs = downstreams;	isPenultimateNode = (downstreams.length == 1);	if (isPenultimateNode && datanode.getPeerMetrics() != null) {	mirrorNameForMetrics = (downstreams[0].getInfoSecurePort() != 0 ? downstreams[0].getInfoSecureAddr() : downstreams[0].getInfoAddr());	
will collect peer metrics for downstream node 

}	byte[] buf = new byte[sizePartialChunk];	byte[] crcbuf = new byte[checksumSize];	try (ReplicaInputStreams instr = datanode.data.getTmpInputStreams(block, blkoff, ckoff)) {	instr.readDataFully(buf, 0, sizePartialChunk);	instr.readChecksumFully(crcbuf, 0, crcbuf.length);	}	final Checksum partialCrc = DataChecksum.newDataChecksum( diskChecksum.getChecksumType(), diskChecksum.getBytesPerChecksum());	partialCrc.update(buf, 0, sizePartialChunk);	if (LOG.isDebugEnabled()) {	
read in partial crc chunk from disk for 

synchronized(ackQueue) {	while (isRunning() && ackQueue.size() != 0) {	try {	ackQueue.wait();	} catch (InterruptedException e) {	running = false;	Thread.currentThread().interrupt();	}	}	if(LOG.isDebugEnabled()) {	
closing 

long expected = -2;	PipelineAck ack = new PipelineAck();	long seqno = PipelineAck.UNKOWN_SEQNO;	long ackRecvNanoTime = 0;	try {	if (type != PacketResponderType.LAST_IN_PIPELINE && !mirrorError) {	DataNodeFaultInjector.get().failPipeline(replicaInfo, mirrorAddr);	ack.readFields(downstreamIn);	ackRecvNanoTime = System.nanoTime();	if (LOG.isDebugEnabled()) {	
got 

long ackRecvNanoTime = 0;	try {	if (type != PacketResponderType.LAST_IN_PIPELINE && !mirrorError) {	DataNodeFaultInjector.get().failPipeline(replicaInfo, mirrorAddr);	ack.readFields(downstreamIn);	ackRecvNanoTime = System.nanoTime();	if (LOG.isDebugEnabled()) {	}	Status oobStatus = ack.getOOBStatus();	if (oobStatus != null) {	
relaying an out of band ack of type 

}	expected = pkt.seqno;	if (type == PacketResponderType.HAS_DOWNSTREAM_IN_PIPELINE && seqno != expected) {	throw new IOException(myString + "seqno: expected=" + expected + ", received=" + seqno);	}	if (type == PacketResponderType.HAS_DOWNSTREAM_IN_PIPELINE) {	totalAckTimeNanos = ackRecvNanoTime - pkt.ackEnqueueNanoTime;	long ackTimeNanos = totalAckTimeNanos - ack.getDownstreamAckTimeNanos();	if (ackTimeNanos < 0) {	if (LOG.isDebugEnabled()) {	
calculated invalid ack time ns 

}	}	lastPacketInBlock = pkt.lastPacketInBlock;	}	} catch (InterruptedException ine) {	isInterrupted = true;	} catch (IOException ioe) {	if (Thread.interrupted()) {	isInterrupted = true;	} else if (ioe instanceof EOFException && !packetSentInTime()) {	
the downstream error might be due to congestion in upstream including this node propagating the error 

if (Thread.interrupted()) {	isInterrupted = true;	} else if (ioe instanceof EOFException && !packetSentInTime()) {	throw ioe;	} else {	mirrorError = true;	LOG.info(myString, ioe);	}	}	if (Thread.interrupted() || isInterrupted) {	
thread is interrupted 

}	if (lastPacketInBlock) {	finalizeBlock(startTime);	}	Status myStatus = pkt != null ? pkt.ackStatus : Status.SUCCESS;	sendAckUpstream(ack, expected, totalAckTimeNanos, (pkt != null ? pkt.offsetInBlock : 0), PipelineAck.combineHeader(datanode.getECN(), myStatus));	if (pkt != null) {	removeAckHead();	}	} catch (IOException e) {	
ioexception in blockreceiver run 

}	}	} catch (Throwable e) {	if (running) {	LOG.info(myString, e);	running = false;	receiverThread.interrupt();	}	}	}	
terminating 

block.setNumBytes(replicaInfo.getNumBytes());	datanode.data.finalizeBlock(block, dirSyncOnFinalize);	}	if (pinning) {	datanode.data.setPinning(block);	}	datanode.closeBlock(block, null, replicaInfo.getStorageUuid(), replicaInfo.isOnTransientStorage());	if (ClientTraceLog.isInfoEnabled() && isClient) {	long offset = 0;	DatanodeRegistration dnR = datanode.getDNRegistrationForBP(block .getBlockPoolId());	
hdfs write 

datanode.data.finalizeBlock(block, dirSyncOnFinalize);	}	if (pinning) {	datanode.data.setPinning(block);	}	datanode.closeBlock(block, null, replicaInfo.getStorageUuid(), replicaInfo.isOnTransientStorage());	if (ClientTraceLog.isInfoEnabled() && isClient) {	long offset = 0;	DatanodeRegistration dnR = datanode.getDNRegistrationForBP(block .getBlockPoolId());	} else {	
received size from 

========================= hadoop sample_7953 =========================

protected void setup(Context context) {	FileSplit split = (FileSplit) context.getInputSplit();	Path filePath = split.getPath();	String name = filePath.getName();	this.threadId = Integer.valueOf(name);	
thread 

protected void cleanup(Context context) {	
busy loop counter 

FileOutputFormat.setOutputPath(job, outputPath);	final Thread toInterrupt = Thread.currentThread();	Thread interrupter = new Thread() {	public void run() {	try {	Thread.sleep(120*1000);	toInterrupt.interrupt();	} catch (InterruptedException ie) {}	}	};	
submitting job 

final Thread toInterrupt = Thread.currentThread();	Thread interrupter = new Thread() {	public void run() {	try {	Thread.sleep(120*1000);	toInterrupt.interrupt();	} catch (InterruptedException ie) {}	}	};	job.submit();	
starting thread to interrupt main thread in minutes 

Thread interrupter = new Thread() {	public void run() {	try {	Thread.sleep(120*1000);	toInterrupt.interrupt();	} catch (InterruptedException ie) {}	}	};	job.submit();	interrupter.start();	
waiting for job to complete 

Thread.sleep(120*1000);	toInterrupt.interrupt();	} catch (InterruptedException ie) {}	}	};	job.submit();	interrupter.start();	try {	job.waitForCompletion(true);	} catch (InterruptedException ie) {	
interrupted while waiting for job completion 

toInterrupt.interrupt();	} catch (InterruptedException ie) {}	}	};	job.submit();	interrupter.start();	try {	job.waitForCompletion(true);	} catch (InterruptedException ie) {	for (int i = 0; i < 10; i++) {	
dumping stacks 

interrupter.start();	try {	job.waitForCompletion(true);	} catch (InterruptedException ie) {	for (int i = 0; i < 10; i++) {	ReflectionUtils.logThreadInfo(LOG, "multimap threads", 0);	Thread.sleep(1000);	}	throw ie;	}	
job completed stopping interrupter 

ReflectionUtils.logThreadInfo(LOG, "multimap threads", 0);	Thread.sleep(1000);	}	throw ie;	}	interrupter.interrupt();	try {	interrupter.join();	} catch (InterruptedException ie) {	}	
verifying output 

BufferedReader r = new BufferedReader(new InputStreamReader(istream));	String line = null;	while ((line = r.readLine()) != null) {	valueSum += Integer.valueOf(line.trim());	}	r.close();	}	int maxVal = NUMBER_FILE_VAL - 1;	int expectedPerMapper = maxVal * (maxVal + 1) / 2;	int expectedSum = expectedPerMapper * numMaps;	
expected sum got 

========================= hadoop sample_5597 =========================

public void testExcessReservationThanNodeManagerCapacity() throws Exception {	MockRM rm = new MockRM(conf);	rm.start();	MockNM nm1 = rm.registerNode("127.0.0.1:1234", 2 * GB, 4);	MockNM nm2 = rm.registerNode("127.0.0.1:2234", 3 * GB, 4);	nm1.nodeHeartbeat(true);	nm2.nodeHeartbeat(true);	int waitCount = 20;	int size = rm.getRMContext().getRMNodes().size();	while ((size = rm.getRMContext().getRMNodes().size()) != 2 && waitCount-- > 0) {	
waiting for node managers to register 

int size = rm.getRMContext().getRMNodes().size();	while ((size = rm.getRMContext().getRMNodes().size()) != 2 && waitCount-- > 0) {	Thread.sleep(100);	}	Assert.assertEquals(2, rm.getRMContext().getRMNodes().size());	RMApp app1 = rm.submitApp(128);	nm1.nodeHeartbeat(true);	RMAppAttempt attempt1 = app1.getCurrentAppAttempt();	MockAM am1 = rm.sendAMLaunched(attempt1.getAppAttemptId());	am1.registerAppAttempt();	
sending container requests 

Assert.assertEquals(2, rm.getRMContext().getRMNodes().size());	RMApp app1 = rm.submitApp(128);	nm1.nodeHeartbeat(true);	RMAppAttempt attempt1 = app1.getCurrentAppAttempt();	MockAM am1 = rm.sendAMLaunched(attempt1.getAppAttemptId());	am1.registerAppAttempt();	am1.addRequests(new String[] {"*"}, 2 * GB, 1, 1);	AllocateResponse alloc1Response = am1.schedule();	nm1.nodeHeartbeat(true);	int waitCounter = 20;	
heartbeating 

RMApp app1 = rm.submitApp(128);	nm1.nodeHeartbeat(true);	RMAppAttempt attempt1 = app1.getCurrentAppAttempt();	MockAM am1 = rm.sendAMLaunched(attempt1.getAppAttemptId());	am1.registerAppAttempt();	am1.addRequests(new String[] {"*"}, 2 * GB, 1, 1);	AllocateResponse alloc1Response = am1.schedule();	nm1.nodeHeartbeat(true);	int waitCounter = 20;	while (alloc1Response.getAllocatedContainers().size() < 1 && waitCounter-- > 0) {	
waiting for containers to be created for app 

MockAM am1 = rm.sendAMLaunched(attempt1.getAppAttemptId());	am1.registerAppAttempt();	am1.addRequests(new String[] {"*"}, 2 * GB, 1, 1);	AllocateResponse alloc1Response = am1.schedule();	nm1.nodeHeartbeat(true);	int waitCounter = 20;	while (alloc1Response.getAllocatedContainers().size() < 1 && waitCounter-- > 0) {	Thread.sleep(500);	alloc1Response = am1.schedule();	}	
received container 

am1.registerAppAttempt();	am1.addRequests(new String[] {"*"}, 2 * GB, 1, 1);	AllocateResponse alloc1Response = am1.schedule();	nm1.nodeHeartbeat(true);	int waitCounter = 20;	while (alloc1Response.getAllocatedContainers().size() < 1 && waitCounter-- > 0) {	Thread.sleep(500);	alloc1Response = am1.schedule();	}	Assert.assertTrue(alloc1Response.getAllocatedContainers().size() == 0);	
heartbeating 

nm1.nodeHeartbeat(true);	int waitCounter = 20;	while (alloc1Response.getAllocatedContainers().size() < 1 && waitCounter-- > 0) {	Thread.sleep(500);	alloc1Response = am1.schedule();	}	Assert.assertTrue(alloc1Response.getAllocatedContainers().size() == 0);	waitCounter = 20;	nm2.nodeHeartbeat(true);	while (alloc1Response.getAllocatedContainers().size() < 1 && waitCounter-- > 0) {	
waiting for containers to be created for app 

Thread.sleep(500);	alloc1Response = am1.schedule();	}	Assert.assertTrue(alloc1Response.getAllocatedContainers().size() == 0);	waitCounter = 20;	nm2.nodeHeartbeat(true);	while (alloc1Response.getAllocatedContainers().size() < 1 && waitCounter-- > 0) {	Thread.sleep(500);	alloc1Response = am1.schedule();	}	
received container 

========================= hadoop sample_519 =========================

public static synchronized boolean isNativeBzip2Loaded(Configuration conf) {	String libname = conf.get("io.compression.codec.bzip2.library", "system-native");	if (!bzip2LibraryName.equals(libname)) {	nativeBzip2Loaded = false;	bzip2LibraryName = libname;	if (libname.equals("java-builtin")) {	
using pure java version of library 

String libname = conf.get("io.compression.codec.bzip2.library", "system-native");	if (!bzip2LibraryName.equals(libname)) {	nativeBzip2Loaded = false;	bzip2LibraryName = libname;	if (libname.equals("java-builtin")) {	} else if (conf.getBoolean( CommonConfigurationKeys.IO_NATIVE_LIB_AVAILABLE_KEY, CommonConfigurationKeys.IO_NATIVE_LIB_AVAILABLE_DEFAULT) && NativeCodeLoader.isNativeCodeLoaded()) {	try {	Bzip2Compressor.initSymbols(libname);	Bzip2Decompressor.initSymbols(libname);	nativeBzip2Loaded = true;	
successfully loaded initialized native library 

if (!bzip2LibraryName.equals(libname)) {	nativeBzip2Loaded = false;	bzip2LibraryName = libname;	if (libname.equals("java-builtin")) {	} else if (conf.getBoolean( CommonConfigurationKeys.IO_NATIVE_LIB_AVAILABLE_KEY, CommonConfigurationKeys.IO_NATIVE_LIB_AVAILABLE_DEFAULT) && NativeCodeLoader.isNativeCodeLoaded()) {	try {	Bzip2Compressor.initSymbols(libname);	Bzip2Decompressor.initSymbols(libname);	nativeBzip2Loaded = true;	} catch (Throwable t) {	
failed to load initialize native library will use pure java version 

========================= hadoop sample_3855 =========================

protected void setupConfig(Configuration conf) {	delayKeySubstring = conf.get(FAIL_INJECT_INCONSISTENCY_KEY, DEFAULT_DELAY_KEY_SUBSTRING);	if (delayKeySubstring.equals(MATCH_ALL_KEYS)) {	delayKeySubstring = "";	}	delayKeyProbability = conf.getFloat(FAIL_INJECT_INCONSISTENCY_PROBABILITY, DEFAULT_DELAY_KEY_PROBABILITY);	delayKeyMsec = conf.getLong(FAIL_INJECT_INCONSISTENCY_MSEC, DEFAULT_DELAY_KEY_MSEC);	
enabled with msec delay substring probability 

public void clearInconsistency() {	
clearing all delayed puts deletes 

public void deleteObject(DeleteObjectRequest deleteObjectRequest) throws AmazonClientException, AmazonServiceException {	String key = deleteObjectRequest.getKey();	
key 

public PutObjectResult putObject(PutObjectRequest putObjectRequest) throws AmazonClientException, AmazonServiceException {	
key 

public ObjectListing listObjects(ListObjectsRequest listObjectsRequest) throws AmazonClientException, AmazonServiceException {	
prefix 

private boolean isKeyDelayed(Long enqueueTime, String key) {	if (enqueueTime == null) {	
no delay for key 

private boolean isKeyDelayed(Long enqueueTime, String key) {	if (enqueueTime == null) {	return false;	}	long currentTime = System.currentTimeMillis();	long deadline = enqueueTime + delayKeyMsec;	if (currentTime >= deadline) {	delayedDeletes.remove(key);	
no longer delaying 

private boolean isKeyDelayed(Long enqueueTime, String key) {	if (enqueueTime == null) {	return false;	}	long currentTime = System.currentTimeMillis();	long deadline = enqueueTime + delayKeyMsec;	if (currentTime >= deadline) {	delayedDeletes.remove(key);	return false;	} else  {	
delaying 

private void enqueueDelayedPut(String key) {	
delaying put of 

========================= hadoop sample_5981 =========================

throw new IllegalArgumentException(msg);	}	ArrayList<RefreshResponse> responses = new ArrayList<RefreshResponse>(handlers.size());	for(RefreshHandler handler : handlers) {	RefreshResponse response;	try {	response = handler.handleRefresh(identifier, args);	if (response == null) {	throw new NullPointerException("Handler returned null.");	}	
responds to says returns 

========================= hadoop sample_4056 =========================

protected DB openDatabase() throws Exception {	Path storeRoot = createStorageDir();	Options options = new Options();	options.createIfMissing(false);	options.logger(new LeveldbLogger());	
using state database at for recovery 

protected DB openDatabase() throws Exception {	Path storeRoot = createStorageDir();	Options options = new Options();	options.createIfMissing(false);	options.logger(new LeveldbLogger());	File dbfile = new File(storeRoot.toString());	try {	db = JniDBFactory.factory.open(dbfile, options);	} catch (NativeDB.DBException e) {	if (e.isNotFound() || e.getMessage().contains(" does not exist ")) {	
creating state database at 

iter.seek(bytes(RM_RESERVATION_KEY_PREFIX));	while (iter.hasNext()) {	Entry<byte[],byte[]> entry = iter.next();	String key = asString(entry.getKey());	if (!key.startsWith(RM_RESERVATION_KEY_PREFIX)) {	break;	}	String planReservationString = key.substring(RM_RESERVATION_KEY_PREFIX.length());	String[] parts = planReservationString.split(SEPARATOR);	if (parts.length != 2) {	
incorrect reservation state key 

rmState.getReservationState().get(planName).put(reservationId, allocationState);	numReservations++;	}	} catch (DBException e) {	throw new IOException(e);	} finally {	if (iter != null) {	iter.close();	}	}	
recovered reservations 

private void loadRMDTSecretManagerState(RMState state) throws IOException {	int numKeys = loadRMDTSecretManagerKeys(state);	
recovered rm delegation token master keys 

private void loadRMDTSecretManagerState(RMState state) throws IOException {	int numKeys = loadRMDTSecretManagerKeys(state);	int numTokens = loadRMDTSecretManagerTokens(state);	
recovered rm delegation tokens 

iter = new LeveldbIterator(db);	iter.seek(bytes(RM_APP_KEY_PREFIX));	while (iter.hasNext()) {	Entry<byte[],byte[]> entry = iter.next();	String key = asString(entry.getKey());	if (!key.startsWith(RM_APP_KEY_PREFIX)) {	break;	}	String appIdStr = key.substring(RM_APP_ROOT.length() + 1);	if (appIdStr.contains(SEPARATOR)) {	
skipping extraneous data 

numAppAttempts += loadRMApp(state, iter, appIdStr, entry.getValue());	++numApps;	}	} catch (DBException e) {	throw new IOException(e);	} finally {	if (iter != null) {	iter.close();	}	}	
recovered applications and application attempts 

Entry<byte[],byte[]> entry = iter.peekNext();	String key = asString(entry.getKey());	if (!key.startsWith(attemptNodePrefix)) {	break;	}	String attemptId = key.substring(attemptNodePrefix.length());	if (attemptId.startsWith(ApplicationAttemptId.appAttemptIdStrPrefix)) {	ApplicationAttemptStateData attemptState = createAttemptState(attemptId, entry.getValue());	appState.attempts.put(attemptState.getAttemptId(), attemptState);	} else {	
ignoring unknown application key 

String attemptId = key.substring(attemptNodePrefix.length());	if (attemptId.startsWith(ApplicationAttemptId.appAttemptIdStrPrefix)) {	ApplicationAttemptStateData attemptState = createAttemptState(attemptId, entry.getValue());	appState.attempts.put(attemptState.getAttemptId(), attemptState);	} else {	}	iter.next();	}	int numAttempts = appState.attempts.size();	if (LOG.isDebugEnabled()) {	
loaded application with attempts 

protected void storeApplicationStateInternal(ApplicationId appId, ApplicationStateData appStateData) throws IOException {	String key = getApplicationNodeKey(appId);	if (LOG.isDebugEnabled()) {	
storing state for app at 

protected void storeApplicationAttemptStateInternal( ApplicationAttemptId attemptId, ApplicationAttemptStateData attemptStateData) throws IOException {	String key = getApplicationAttemptNodeKey(attemptId);	if (LOG.isDebugEnabled()) {	
storing state for attempt at 

public synchronized void removeApplicationAttemptInternal( ApplicationAttemptId attemptId) throws IOException {	String attemptKey = getApplicationAttemptNodeKey(attemptId);	if (LOG.isDebugEnabled()) {	
removing state for attempt at 

String appKey = getApplicationNodeKey(appId);	try {	WriteBatch batch = db.createWriteBatch();	try {	batch.delete(bytes(appKey));	for (ApplicationAttemptId attemptId : appState.attempts.keySet()) {	String attemptKey = getApplicationAttemptNodeKey(appKey, attemptId);	batch.delete(bytes(attemptKey));	}	if (LOG.isDebugEnabled()) {	
removing state for app and attempts at 

protected void storeReservationState( ReservationAllocationStateProto reservationAllocation, String planName, String reservationIdName) throws Exception {	try {	WriteBatch batch = db.createWriteBatch();	try {	String key = getReservationNodeKey(planName, reservationIdName);	if (LOG.isDebugEnabled()) {	
storing state for reservation plan at 

protected void removeReservationState(String planName, String reservationIdName) throws Exception {	try {	WriteBatch batch = db.createWriteBatch();	try {	String reservationKey = getReservationNodeKey(planName, reservationIdName);	batch.delete(bytes(reservationKey));	if (LOG.isDebugEnabled()) {	
removing state for reservation plan at 

private void storeOrUpdateRMDT(RMDelegationTokenIdentifier tokenId, Long renewDate, boolean isUpdate) throws IOException {	String tokenKey = getRMDTTokenNodeKey(tokenId);	RMDelegationTokenIdentifierData tokenData = new RMDelegationTokenIdentifierData(tokenId, renewDate);	if (LOG.isDebugEnabled()) {	
storing token to 

try {	WriteBatch batch = db.createWriteBatch();	try {	batch.put(bytes(tokenKey), tokenData.toByteArray());	if(!isUpdate) {	ByteArrayOutputStream bs = new ByteArrayOutputStream();	try (DataOutputStream ds = new DataOutputStream(bs)) {	ds.writeInt(tokenId.getSequenceNumber());	}	if (LOG.isDebugEnabled()) {	
storing to 

protected void removeRMDelegationTokenState( RMDelegationTokenIdentifier tokenId) throws IOException {	String tokenKey = getRMDTTokenNodeKey(tokenId);	if (LOG.isDebugEnabled()) {	
removing token at 

protected void storeRMDTMasterKeyState(DelegationKey masterKey) throws IOException {	String dbKey = getRMDTMasterKeyNodeKey(masterKey);	if (LOG.isDebugEnabled()) {	
storing token master key to 

protected void removeRMDTMasterKeyState(DelegationKey masterKey) throws IOException {	String dbKey = getRMDTMasterKeyNodeKey(masterKey);	if (LOG.isDebugEnabled()) {	
removing token master key at 

public void deleteStore() throws IOException {	Path root = getStorageDir();	
deleting state database at 

public synchronized void removeApplication(ApplicationId removeAppId) throws IOException {	String appKey = getApplicationNodeKey(removeAppId);	
removing state for app 

public void run() {	long start = Time.monotonicNow();	
starting full compaction cycle 

public void run() {	long start = Time.monotonicNow();	try {	db.compactRange(null, null);	} catch (DBException e) {	
error compacting database 

public void run() {	long start = Time.monotonicNow();	try {	db.compactRange(null, null);	} catch (DBException e) {	}	long duration = Time.monotonicNow() - start;	
full compaction cycle completed in msec 

========================= hadoop sample_763 =========================

}	FileInputFormat.setMaxInputSplitSize(job, fs.getFileStatus(inFile).getLen() / numSplits);	TaskAttemptContext context = MapReduceTestUtil. createDummyMapTaskAttemptContext(job.getConfiguration());	int count = 0;	for (InputSplit split : format.getSplits(job)) {	RecordReader<Text, BytesWritable> reader = format.createRecordReader(split, context);	MapContext<Text, BytesWritable, Text, BytesWritable> mcontext = new MapContextImpl<Text, BytesWritable, Text, BytesWritable>( job.getConfiguration(), context.getTaskAttemptID(), reader, null, null, MapReduceTestUtil.createDummyReporter(), split);	reader.initialize(split, mcontext);	try {	while (reader.nextKeyValue()) {	
accept record 

public void testRegexFilter() throws Exception {	
testing regex filter with patter 

public void testRegexFilter() throws Exception {	SequenceFileInputFilter.setFilterClass(job, SequenceFileInputFilter.RegexFilter.class);	SequenceFileInputFilter.RegexFilter.setPattern( job.getConfiguration(), "\\A10*");	fs.delete(inDir, true);	for (int length = 1; length < MAX_LENGTH;	length += random.nextInt(MAX_LENGTH / 10) + 1) {	
number of records 

public void testPercentFilter() throws Exception {	
testing percent filter with frequency 

public void testPercentFilter() throws Exception {	SequenceFileInputFilter.setFilterClass(job, SequenceFileInputFilter.PercentFilter.class);	SequenceFileInputFilter.PercentFilter.setFrequency( job.getConfiguration(), 1000);	fs.delete(inDir, true);	for (int length = 0; length < MAX_LENGTH;	length += random.nextInt(MAX_LENGTH / 10) + 1) {	
number of records 

public void testPercentFilter() throws Exception {	SequenceFileInputFilter.setFilterClass(job, SequenceFileInputFilter.PercentFilter.class);	SequenceFileInputFilter.PercentFilter.setFrequency( job.getConfiguration(), 1000);	fs.delete(inDir, true);	for (int length = 0; length < MAX_LENGTH;	length += random.nextInt(MAX_LENGTH / 10) + 1) {	createSequenceFile(length);	int count = countRecords(1);	
accepted records 

public void testMD5Filter() throws Exception {	
testing filter with frequency 

public void testMD5Filter() throws Exception {	SequenceFileInputFilter.setFilterClass(job, SequenceFileInputFilter.MD5Filter.class);	SequenceFileInputFilter.MD5Filter.setFrequency( job.getConfiguration(), 1000);	fs.delete(inDir, true);	for (int length = 0; length < MAX_LENGTH;	length += random.nextInt(MAX_LENGTH / 10) + 1) {	
number of records 

public void testMD5Filter() throws Exception {	SequenceFileInputFilter.setFilterClass(job, SequenceFileInputFilter.MD5Filter.class);	SequenceFileInputFilter.MD5Filter.setFrequency( job.getConfiguration(), 1000);	fs.delete(inDir, true);	for (int length = 0; length < MAX_LENGTH;	length += random.nextInt(MAX_LENGTH / 10) + 1) {	createSequenceFile(length);	
accepted records 

========================= hadoop sample_5647 =========================

private Map<String, LocalResource> setupLocalResources(Configuration jobConf, String jobSubmitDir) throws IOException {	Map<String, LocalResource> localResources = new HashMap<>();	Path jobConfPath = new Path(jobSubmitDir, MRJobConfig.JOB_CONF_FILE);	URL yarnUrlForJobSubmitDir = URL.fromPath(defaultFileContext .getDefaultFileSystem().resolvePath( defaultFileContext.makeQualified(new Path(jobSubmitDir))));	
creating setup context jobsubmitdir url is 

localResources.put(MRJobConfig.JOB_CONF_FILE, createApplicationResource(defaultFileContext, jobConfPath, LocalResourceType.FILE));	if (jobConf.get(MRJobConfig.JAR) != null) {	Path jobJarPath = new Path(jobConf.get(MRJobConfig.JAR));	FileContext fccc = FileContext.getFileContext(jobJarPath.toUri(), jobConf);	LocalResourceVisibility jobJarViz = jobConf.getBoolean(MRJobConfig.JOBJAR_VISIBILITY, MRJobConfig.JOBJAR_VISIBILITY_DEFAULT) ? LocalResourceVisibility.PUBLIC : LocalResourceVisibility.APPLICATION;	LocalResource rc = createApplicationResource( FileContext.getFileContext(jobJarPath.toUri(), jobConf), jobJarPath, MRJobConfig.JOB_JAR, LocalResourceType.PATTERN, jobJarViz, jobConf.getBoolean( MRJobConfig.JOBJAR_SHARED_CACHE_UPLOAD_POLICY, MRJobConfig.JOBJAR_SHARED_CACHE_UPLOAD_POLICY_DEFAULT));	String pattern = conf.getPattern(JobContext.JAR_UNPACK_PATTERN, JobConf.UNPACK_JAR_PATTERN_DEFAULT).pattern();	rc.setPattern(pattern);	localResources.put(MRJobConfig.JOB_JAR, rc);	} else {	
job jar is not present not adding any jar to the list of resources 

private ContainerLaunchContext setupContainerLaunchContextForAM( Configuration jobConf, Map<String, LocalResource> localResources, ByteBuffer securityTokens, List<String> vargs) throws IOException {	Vector<String> vargsFinal = new Vector<>(8);	StringBuilder mergedCommand = new StringBuilder();	for (CharSequence str : vargs) {	mergedCommand.append(str).append(" ");	}	vargsFinal.add(mergedCommand.toString());	
command to launch container for applicationmaster is 

ReservationId reservationID = null;	try {	reservationID = ReservationId.parseReservationId(jobConf .get(JobContext.RESERVATION_ID));	} catch (NumberFormatException e) {	String errMsg = "Invalid reservationId: " + jobConf.get(JobContext.RESERVATION_ID) + " specified for the app: " + applicationId;	LOG.warn(errMsg);	throw new IOException(errMsg);	}	if (reservationID != null) {	appContext.setReservationID(reservationID);	
submitting applicationsubmissioncontext app to queue with reservationid 

for (Map.Entry<String, String> map : conf) {	String key = map.getKey();	String val = map.getValue();	if (key.matches(regex)) {	copy.set(key, val);	count++;	}	}	copy.write(dob);	ByteBuffer appConf = ByteBuffer.wrap(dob.getData(), 0, dob.getLength());	
send configurations that match regex expression total number of configs total size bytes 

break;	}	currentTimeMillis = System.currentTimeMillis();	status = clientCache.getClient(arg0).getJobStatus(arg0);	if (status == null) {	killUnFinishedApplication(appId);	return;	}	}	} catch(IOException io) {	
error when checking for application status 

private static void warnForJavaLibPath(String opts, String component, String javaConf, String envConf) {	if (opts != null && opts.contains("-Djava.library.path")) {	
usage of djava library path in can cause programs to no longer function if hadoop native libraries are used these values should be set as part of the ld library path in the jvm env using config settings 

========================= hadoop sample_5702 =========================

private static void checkNotSymlink(INode inode, byte[][] components, int i) throws UnresolvedPathException {	if (inode != null && inode.isSymlink()) {	final int last = components.length - 1;	final String path = getPath(components, 0, last);	final String preceding = getPath(components, 0, i - 1);	final String remainder = getPath(components, i + 1, last);	final String target = inode.asSymlink().getSymlinkString();	if (LOG.isDebugEnabled()) {	final String link = inode.getLocalName();	
unresolvedpathexception path preceding count link target remainder 

========================= hadoop sample_8130 =========================

public void testFailFullyDelete() throws IOException {	if(Shell.WINDOWS) {	return;	}	
running test to verify failure of fullydelete 

public boolean delete() {	
trying to delete myfile 

public boolean delete() {	boolean bool = false;	if (getName().equals(file1Name)) {	bool = false;	} else {	bool = super.delete();	}	if (bool) {	
deleted successfully 

public boolean delete() {	boolean bool = false;	if (getName().equals(file1Name)) {	bool = false;	} else {	bool = super.delete();	}	if (bool) {	} else {	
cannot delete 

public void testFailFullyDeleteContents() throws IOException {	if(Shell.WINDOWS) {	return;	}	
running test to verify failure of fullydeletecontents 

}	List<String> actualClassPaths = Arrays.asList(classPathAttr.split(" "));	Collections.sort(expectedClassPaths);	Collections.sort(actualClassPaths);	Assert.assertEquals(expectedClassPaths, actualClassPaths);	} finally {	if (jarFile != null) {	try {	jarFile.close();	} catch (IOException e) {	
exception closing jarfile 

========================= hadoop sample_3286 =========================

protected void serviceInit(Configuration conf) throws Exception {	enabled = conf.getBoolean(YarnConfiguration.SHARED_CACHE_ENABLED, YarnConfiguration.DEFAULT_SHARED_CACHE_ENABLED);	if (enabled) {	int threadCount = conf.getInt(YarnConfiguration.SHARED_CACHE_NM_UPLOADER_THREAD_COUNT, YarnConfiguration.DEFAULT_SHARED_CACHE_NM_UPLOADER_THREAD_COUNT);	uploaderPool = HadoopExecutors.newFixedThreadPool(threadCount, new ThreadFactoryBuilder(). setNameFormat("Shared cache uploader #%d"). build());	scmClient = createSCMClient(conf);	try {	fs = FileSystem.get(conf);	localFs = FileSystem.getLocal(conf);	} catch (IOException e) {	
unexpected exception in getting the filesystem 

========================= hadoop sample_1891 =========================

private static void createControlFile( FileSystem fs, int fileSize, int nrFiles ) throws IOException {	
creating control file mega bytes files 

try {	writer = SequenceFile.createWriter(fs, fsConfig, controlFile, Text.class, LongWritable.class, CompressionType.NONE);	writer.append(new Text(name), new LongWritable(fileSize));	} catch(Exception e) {	throw new IOException(e.getLocalizedMessage());	} finally {	if (writer != null) writer.close();	writer = null;	}	}	
created control files for files 

private static void cleanup(FileSystem fs) throws Exception {	
cleaning up test files 

========================= hadoop sample_5581 =========================

public BlockScanner(DataNode datanode, Configuration conf) {	this.datanode = datanode;	this.conf = new Conf(conf);	if (isEnabled()) {	
initialized block scanner with targetbytespersec 

public BlockScanner(DataNode datanode, Configuration conf) {	this.datanode = datanode;	this.conf = new Conf(conf);	if (isEnabled()) {	} else {	
disabled block scanner 

public synchronized void addVolumeScanner(FsVolumeReference ref) {	boolean success = false;	try {	FsVolumeSpi volume = ref.getVolume();	if (!isEnabled()) {	
not adding volume scanner for because the block scanner is disabled 

public synchronized void addVolumeScanner(FsVolumeReference ref) {	boolean success = false;	try {	FsVolumeSpi volume = ref.getVolume();	if (!isEnabled()) {	return;	}	VolumeScanner scanner = scanners.get(volume.getStorageID());	if (scanner != null) {	
already have a scanner for volume 

boolean success = false;	try {	FsVolumeSpi volume = ref.getVolume();	if (!isEnabled()) {	return;	}	VolumeScanner scanner = scanners.get(volume.getStorageID());	if (scanner != null) {	return;	}	
adding scanner for volume storageid 

public synchronized void removeVolumeScanner(FsVolumeSpi volume) {	if (!isEnabled()) {	
not removing volume scanner for because the block scanner is disabled 

public synchronized void removeVolumeScanner(FsVolumeSpi volume) {	if (!isEnabled()) {	return;	}	VolumeScanner scanner = scanners.get(volume.getStorageID());	if (scanner == null) {	
no scanner found to remove for volumeid 

public synchronized void removeVolumeScanner(FsVolumeSpi volume) {	if (!isEnabled()) {	return;	}	VolumeScanner scanner = scanners.get(volume.getStorageID());	if (scanner == null) {	return;	}	
removing scanner for volume storageid 

public void doGet(HttpServletRequest request, HttpServletResponse response) throws IOException {	response.setContentType("text/plain");	DataNode datanode = (DataNode) getServletContext().getAttribute("datanode");	BlockScanner blockScanner = datanode.getBlockScanner();	StringBuilder buffer = new StringBuilder(8 * 1024);	if (!blockScanner.isEnabled()) {	
periodic block scanner is not running 

DataNode datanode = (DataNode) getServletContext().getAttribute("datanode");	BlockScanner blockScanner = datanode.getBlockScanner();	StringBuilder buffer = new StringBuilder(8 * 1024);	if (!blockScanner.isEnabled()) {	buffer.append("Periodic block scanner is not running. " + "Please check the datanode log if this is unexpected.");	} else {	buffer.append("Block Scanner Statistics\n\n");	blockScanner.printStats(buffer);	}	String resp = buffer.toString();	
returned servlet info 

========================= hadoop sample_7945 =========================

public static BlockingThreadPoolExecutorService newInstance( int activeTasks, int waitingTasks, long keepAliveTime, TimeUnit unit, String prefixName) {	queue, we need to be able to buffer all tasks in case dequeueing is slower than enqueueing. */ final BlockingQueue<Runnable> workQueue = new LinkedBlockingQueue<>(waitingTasks + activeTasks);	ThreadPoolExecutor eventProcessingExecutor = new ThreadPoolExecutor(activeTasks, activeTasks, keepAliveTime, unit, workQueue, newDaemonThreadFactory(prefixName), new RejectedExecutionHandler() {	public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) {	
could not submit task to executor 

========================= hadoop sample_5997 =========================

private void testRemoveElementsFromList(Random r, CachedBlocksList list, CachedBlock[] blocks) {	int i = 0;	for (Iterator<CachedBlock> iter = list.iterator(); iter.hasNext(); ) {	Assert.assertEquals(blocks[i], iter.next());	i++;	}	if (r.nextBoolean()) {	
removing via iterator 

for (Iterator<CachedBlock> iter = list.iterator(); iter.hasNext(); ) {	Assert.assertEquals(blocks[i], iter.next());	i++;	}	if (r.nextBoolean()) {	for (Iterator<CachedBlock> iter = list.iterator(); iter.hasNext() ;) {	iter.next();	iter.remove();	}	} else {	
removing in pseudo random order 

========================= hadoop sample_7553 =========================

public void start() throws IOException, FileNotFoundException, URISyntaxException {	if (!noDFS) {	dfs = new MiniDFSCluster.Builder(conf).nameNodePort(nnPort) .nameNodeHttpPort(nnHttpPort).numDataNodes(numDataNodes) .startupOption(dfsOpts).build();	
started minidfscluster namenode on port 

if (fs == null && dfs != null) {	fs = dfs.getFileSystem().getUri().toString();	} else if (fs == null) {	fs = "file: }	FileSystem.setDefaultUri(conf, new URI(fs));	conf.setBoolean(YarnConfiguration.YARN_MINICLUSTER_FIXED_PORTS, true);	conf.setBoolean(JHAdminConfig.MR_HISTORY_MINICLUSTER_FIXED_PORTS, true);	conf.set(YarnConfiguration.RM_ADDRESS, MiniYARNCluster.getHostname() + ":" + this.rmPort);	conf.set(JHAdminConfig.MR_HISTORY_ADDRESS, MiniYARNCluster.getHostname() + ":" + this.jhsPort);	mr = MiniMRClientClusterFactory.create(this.getClass(), numNodeManagers, conf);	
started minimrcluster 

private boolean parseArguments(String[] args) {	Options options = makeOptions();	CommandLine cli;	try {	CommandLineParser parser = new GnuParser();	cli = parser.parse(options, args);	} catch (ParseException e) {	
options parsing failed 

private void updateConfiguration(JobConf conf, String[] keyvalues) {	int num_confs_updated = 0;	if (keyvalues != null) {	for (String prop : keyvalues) {	String[] keyval = prop.split("=", 2);	if (keyval.length == 2) {	conf.set(keyval[0], keyval[1]);	num_confs_updated++;	} else {	
ignoring d option 

if (keyvalues != null) {	for (String prop : keyvalues) {	String[] keyval = prop.split("=", 2);	if (keyval.length == 2) {	conf.set(keyval[0], keyval[1]);	num_confs_updated++;	} else {	}	}	}	
updated configuration settings from command line 

========================= hadoop sample_5610 =========================

protected void serviceStop() throws Exception {	appAggregationExecutor.shutdown();	if (!appAggregationExecutor.awaitTermination(10, TimeUnit.SECONDS)) {	
app level aggregator shutdown timed out shutdown now 

private void aggregate() {	
app level real time aggregating 

private void aggregate() {	if (!isReadyToAggregate()) {	
app level collector is not ready skip aggregation 

private void aggregate() {	if (!isReadyToAggregate()) {	return;	}	try {	TimelineCollectorContext currContext = getTimelineEntityContext();	Map<String, AggregationStatusTable> aggregationGroups = getAggregationGroups();	if (aggregationGroups == null || aggregationGroups.isEmpty()) {	
app level collector is empty skip aggregation 

TimelineCollectorContext currContext = getTimelineEntityContext();	Map<String, AggregationStatusTable> aggregationGroups = getAggregationGroups();	if (aggregationGroups == null || aggregationGroups.isEmpty()) {	return;	}	TimelineEntity resultEntity = TimelineCollector.aggregateWithoutGroupId( aggregationGroups, currContext.getAppId(), TimelineEntityType.YARN_APPLICATION.toString());	TimelineEntities entities = new TimelineEntities();	entities.addEntity(resultEntity);	putEntitiesAsync(entities, getCurrentUser());	} catch (Exception e) {	
error aggregating timeline metrics 

Map<String, AggregationStatusTable> aggregationGroups = getAggregationGroups();	if (aggregationGroups == null || aggregationGroups.isEmpty()) {	return;	}	TimelineEntity resultEntity = TimelineCollector.aggregateWithoutGroupId( aggregationGroups, currContext.getAppId(), TimelineEntityType.YARN_APPLICATION.toString());	TimelineEntities entities = new TimelineEntities();	entities.addEntity(resultEntity);	putEntitiesAsync(entities, getCurrentUser());	} catch (Exception e) {	}	
app level real time aggregation complete 

========================= hadoop sample_364 =========================

private List<File> getLatestEditsFiles() {	if (latestNameCheckpointTime > latestEditsCheckpointTime) {	
name checkpoint time is newer than edits not loading edits 

========================= hadoop sample_8022 =========================

protected void serviceInit(Configuration conf) throws Exception {	super.serviceInit(conf);	taskTimeOut = conf.getLong( MRJobConfig.TASK_TIMEOUT, MRJobConfig.DEFAULT_TASK_TIMEOUT_MILLIS);	long taskProgressReportIntervalMillis = MRJobConfUtil. getTaskProgressReportInterval(conf);	long minimumTaskTimeoutAllowed = taskProgressReportIntervalMillis * 2;	if(taskTimeOut < minimumTaskTimeoutAllowed) {	taskTimeOut = minimumTaskTimeoutAllowed;	
task timeout must be as least twice as long as the task status report interval setting task timeout to 

boolean taskTimedOut = (taskTimeOut > 0) && (currentTime > (entry.getValue().getLastProgress() + taskTimeOut));	if(taskTimedOut) {	iterator.remove();	eventHandler.handle(new TaskAttemptDiagnosticsUpdateEvent(entry .getKey(), "AttemptID:" + entry.getKey().toString() + " Timed out after " + taskTimeOut / 1000 + " secs"));	eventHandler.handle(new TaskAttemptEvent(entry.getKey(), TaskAttemptEventType.TA_TIMED_OUT));	}	}	try {	Thread.sleep(taskTimeOutCheckInterval);	} catch (InterruptedException e) {	
taskheartbeathandler thread interrupted 

========================= hadoop sample_5240 =========================

private void matchSourceWithTargetToMove(Source source, StorageGroup target) {	long size = Math.min(source.availableSizeToMove(), target.availableSizeToMove());	final Task task = new Task(target, size);	source.addTask(task);	target.incScheduledSize(task.getSize());	dispatcher.add(source, target);	
decided to move bytes from to 

final Balancer b = new Balancer(nnc, p, conf);	final Result r = b.runOneIteration();	r.print(iteration, System.out);	b.resetData(conf);	if (r.exitStatus == ExitStatus.IN_PROGRESS) {	done = false;	} else if (r.exitStatus != ExitStatus.SUCCESS) {	return r.exitStatus.getExitCode();	}	} else {	
skipping blockpool 

private static void checkKeytabAndInit(Configuration conf) throws IOException {	if (conf.getBoolean(DFSConfigKeys.DFS_BALANCER_KEYTAB_ENABLED_KEY, DFSConfigKeys.DFS_BALANCER_KEYTAB_ENABLED_DEFAULT)) {	
keytab is configured will login using keytab 

if (args != null) {	try {	for(int i = 0; i < args.length; i++) {	if ("-threshold".equalsIgnoreCase(args[i])) {	checkArgument(++i < args.length, "Threshold value is missing: args = " + Arrays.toString(args));	try {	double threshold = Double.parseDouble(args[i]);	if (threshold < 1 || threshold > 100) {	throw new IllegalArgumentException( "Number out of range: threshold = " + threshold);	}	
using a threshold of 

includedNodes = new HashSet<>();	i = processHostList(args, i, "include", includedNodes);	b.setIncludedNodes(includedNodes);	} else if ("-source".equalsIgnoreCase(args[i])) {	Set<String> sourceNodes = new HashSet<>();	i = processHostList(args, i, "source", sourceNodes);	b.setSourceNodes(sourceNodes);	} else if ("-blockpools".equalsIgnoreCase(args[i])) {	checkArgument( ++i < args.length, "blockpools value is missing: args = " + Arrays.toString(args));	Set<String> blockpools = parseBlockPoolList(args[i]);	
balancer will run on the following blockpools 

Set<String> sourceNodes = new HashSet<>();	i = processHostList(args, i, "source", sourceNodes);	b.setSourceNodes(sourceNodes);	} else if ("-blockpools".equalsIgnoreCase(args[i])) {	checkArgument( ++i < args.length, "blockpools value is missing: args = " + Arrays.toString(args));	Set<String> blockpools = parseBlockPoolList(args[i]);	b.setBlockpools(blockpools);	} else if ("-idleiterations".equalsIgnoreCase(args[i])) {	checkArgument(++i < args.length, "idleiterations value is missing: args = " + Arrays .toString(args));	int maxIdleIteration = Integer.parseInt(args[i]);	
using a idleiterations of 

} else if ("-blockpools".equalsIgnoreCase(args[i])) {	checkArgument( ++i < args.length, "blockpools value is missing: args = " + Arrays.toString(args));	Set<String> blockpools = parseBlockPoolList(args[i]);	b.setBlockpools(blockpools);	} else if ("-idleiterations".equalsIgnoreCase(args[i])) {	checkArgument(++i < args.length, "idleiterations value is missing: args = " + Arrays .toString(args));	int maxIdleIteration = Integer.parseInt(args[i]);	b.setMaxIdleIteration(maxIdleIteration);	} else if ("-runDuringUpgrade".equalsIgnoreCase(args[i])) {	b.setRunDuringUpgrade(true);	
will run the balancer even during an ongoing hdfs upgrade most users will not want to run the balancer during an upgrade since it will not affect used space on over utilized machines 

public static void main(String[] args) {	if (DFSUtil.parseHelpArgument(args, USAGE, System.out, true)) {	System.exit(0);	}	try {	System.exit(ToolRunner.run(new HdfsConfiguration(), new Cli(), args));	} catch (Throwable e) {	
exiting balancer due an exception 

========================= hadoop sample_8188 =========================

return -1;	} else {	return n1.ordinal() - n2.ordinal();	}	}	});	for (FsImageProto.FileSummary.Section s : sections) {	fin.getChannel().position(s.getOffset());	InputStream is = FSImageUtil.wrapInputStreamForCompression(conf, summary.getCodec(), new BufferedInputStream(new LimitInputStream( fin, s.getLength())));	if (LOG.isDebugEnabled()) {	
loading section length 

static ImmutableList<Long> loadINodeReferenceSection(InputStream in) throws IOException {	
loading inode references 

ImmutableList.Builder<Long> builder = ImmutableList.builder();	long counter = 0;	while (true) {	FsImageProto.INodeReferenceSection.INodeReference e = FsImageProto.INodeReferenceSection.INodeReference .parseDelimitedFrom(in);	if (e == null) {	break;	}	++counter;	builder.add(e.getReferredId());	}	
loaded inode references 

private static byte[][] loadINodeSection(InputStream in) throws IOException {	FsImageProto.INodeSection s = FsImageProto.INodeSection .parseDelimitedFrom(in);	
loading inodes 

private static byte[][] loadINodeSection(InputStream in) throws IOException {	FsImageProto.INodeSection s = FsImageProto.INodeSection .parseDelimitedFrom(in);	final byte[][] inodes = new byte[(int) s.getNumInodes()][];	for (int i = 0; i < s.getNumInodes(); ++i) {	int size = CodedInputStream.readRawVarint32(in.read(), in);	byte[] bytes = new byte[size];	IOUtils.readFully(in, bytes, 0, size);	inodes[i] = bytes;	}	
sorting inodes 

private static byte[][] loadINodeSection(InputStream in) throws IOException {	FsImageProto.INodeSection s = FsImageProto.INodeSection .parseDelimitedFrom(in);	final byte[][] inodes = new byte[(int) s.getNumInodes()][];	for (int i = 0; i < s.getNumInodes(); ++i) {	int size = CodedInputStream.readRawVarint32(in.read(), in);	byte[] bytes = new byte[size];	IOUtils.readFully(in, bytes, 0, size);	inodes[i] = bytes;	}	Arrays.sort(inodes, INODE_BYTES_COMPARATOR);	
finished sorting inodes 

static String[] loadStringTable(InputStream in) throws IOException {	FsImageProto.StringTableSection s = FsImageProto.StringTableSection .parseDelimitedFrom(in);	
loading strings 

========================= hadoop sample_7787 =========================

public void testRenameFileIntoExistingDirectory() throws Exception {	assumeRenameSupported();	Path src = path("/test/olddir/file");	createFile(src);	Path dst = path("/test/new/newdir");	fs.mkdirs(dst);	rename(src, dst, true, false, true);	Path newFile = path("/test/new/newdir/file");	if (!fs.exists(newFile)) {	String ls = ls(dst);	
test new 

public void testRenameFileIntoExistingDirectory() throws Exception {	assumeRenameSupported();	Path src = path("/test/olddir/file");	createFile(src);	Path dst = path("/test/new/newdir");	fs.mkdirs(dst);	rename(src, dst, true, false, true);	Path newFile = path("/test/new/newdir/file");	if (!fs.exists(newFile)) {	String ls = ls(dst);	
test hadoop 

========================= hadoop sample_6179 =========================

if (dropBehind != null) {	fos.setDropBehind(dropBehind);	}	byte buf[] = new byte[8196];	while (length > 0) {	int amt = (length > buf.length) ? buf.length : (int)length;	fos.write(buf, 0, amt);	length -= amt;	}	} catch (IOException e) {	
ioexception 

while (length > 0) {	int amt = (length > buf.length) ? buf.length : (int)length;	int ret = fis.read(buf, 0, amt);	if (ret == -1) {	return totalRead;	}	totalRead += ret;	length -= ret;	}	} catch (IOException e) {	
ioexception 

public void testFadviseAfterWriteThenRead() throws Exception {	
testFadviseAfterWriteThenRead 

public void testClientDefaults() throws Exception {	
testClientDefaults 

public void testFadviseSkippedForSmallReads() throws Exception {	
testFadviseSkippedForSmallReads 

public void testNoFadviseAfterWriteThenRead() throws Exception {	
testNoFadviseAfterWriteThenRead 

public void testSeekAfterSetDropBehind() throws Exception {	
testSeekAfterSetDropBehind 

========================= hadoop sample_7292 =========================

private void shutdown() {	try {	connection.commit();	connection.close();	}catch (Throwable ex) {	
exception occurred while closing connection 

try {	connection.commit();	connection.close();	}catch (Throwable ex) {	} finally {	try {	if(server != null) {	server.shutdown();	}	}catch (Throwable ex) {	
exception occurred while shutting down hsqldb 

========================= hadoop sample_5755 =========================

private DFSZKFailoverController(Configuration conf, NNHAServiceTarget localTarget) {	super(conf, localTarget);	this.localNNTarget = localTarget;	adminAcl = new AccessControlList( conf.get(DFSConfigKeys.DFS_ADMIN, " "));	
failover controller configured for namenode 

StringUtils.startupShutdownMessage(DFSZKFailoverController.class, args, LOG);	if (DFSUtil.parseHelpArgument(args, ZKFailoverController.USAGE, System.out, true)) {	System.exit(0);	}	GenericOptionsParser parser = new GenericOptionsParser( new HdfsConfiguration(), args);	DFSZKFailoverController zkfc = DFSZKFailoverController.create( parser.getConfiguration());	int retCode = 0;	try {	retCode = zkfc.run(parser.getRemainingArgs());	} catch (Throwable t) {	
dfszkfailovercontroller exiting due to earlier exception 

protected void checkRpcAdminAccess() throws IOException, AccessControlException {	UserGroupInformation ugi = UserGroupInformation.getCurrentUser();	UserGroupInformation zkfcUgi = UserGroupInformation.getLoginUser();	if (adminAcl.isUserAllowed(ugi) || ugi.getShortUserName().equals(zkfcUgi.getShortUserName())) {	
allowed rpc access from at 

conn.setConnectTimeout(httpTimeOut);	conn.connect();	ByteArrayOutputStream out = new ByteArrayOutputStream();	IOUtils.copyBytes(conn.getInputStream(), out, 4096, true);	StringBuilder localNNThreadDumpContent = new StringBuilder("-- Local NN thread dump -- \n");	localNNThreadDumpContent.append(out);	localNNThreadDumpContent.append("\n -- Local NN thread dump -- ");	LOG.info(localNNThreadDumpContent);	isThreadDumpCaptured = true;	} catch (IOException e) {	
can t get local nn thread dump due to 

========================= hadoop sample_7768 =========================

private static boolean isSetsidSupported() {	ShellCommandExecutor shexec = null;	boolean setsidSupported = true;	try {	String[] args = {"setsid", "bash", "-c", "echo $$"};	shexec = new ShellCommandExecutor(args);	shexec.execute();	} catch (IOException ioe) {	
setsid is not available on this machine so not using it 

private static boolean isSetsidSupported() {	ShellCommandExecutor shexec = null;	boolean setsidSupported = true;	try {	String[] args = {"setsid", "bash", "-c", "echo $$"};	shexec = new ShellCommandExecutor(args);	shexec.execute();	} catch (IOException ioe) {	setsidSupported = false;	} finally {	
setsid exited with exit code 

private static void sendSignal(String pid, int signalNum, String signalName) {	ShellCommandExecutor shexec = null;	try {	String[] args = { "kill", "-" + signalNum, pid };	shexec = new ShellCommandExecutor(args);	shexec.execute();	} catch (IOException ioe) {	
error executing shell command 

private static void sendSignal(String pid, int signalNum, String signalName) {	ShellCommandExecutor shexec = null;	try {	String[] args = { "kill", "-" + signalNum, pid };	shexec = new ShellCommandExecutor(args);	shexec.execute();	} catch (IOException ioe) {	} finally {	if (pid.startsWith("-")) {	
sending signal to all members of process group exit code 

private static void sendSignal(String pid, int signalNum, String signalName) {	ShellCommandExecutor shexec = null;	try {	String[] args = { "kill", "-" + signalNum, pid };	shexec = new ShellCommandExecutor(args);	shexec.execute();	} catch (IOException ioe) {	} finally {	if (pid.startsWith("-")) {	} else {	
signaling process with exit code 

private static void sigKillInCurrentThread(String pid, boolean isProcessGroup, long sleepTimeBeforeSigKill) {	if (isProcessGroup || ProcessTree.isAlive(pid)) {	try {	Thread.sleep(sleepTimeBeforeSigKill);	} catch (InterruptedException i) {	
thread sleep is interrupted 

public static boolean isAlive(String pid) {	ShellCommandExecutor shexec = null;	try {	String[] args = { "kill", "-0", pid };	shexec = new ShellCommandExecutor(args);	shexec.execute();	} catch (ExitCodeException ee) {	return false;	} catch (IOException ioe) {	
error executing shell command 

public static boolean isProcessGroupAlive(String pgrpId) {	ShellCommandExecutor shexec = null;	try {	String[] args = { "kill", "-0", "-"+pgrpId };	shexec = new ShellCommandExecutor(args);	shexec.execute();	} catch (ExitCodeException ee) {	return false;	} catch (IOException ioe) {	
error executing shell command 

========================= hadoop sample_4847 =========================

protected void serviceInit(Configuration conf) throws Exception {	super.serviceInit(conf);	nodeBlacklistingEnabled = conf.getBoolean(MRJobConfig.MR_AM_JOB_NODE_BLACKLISTING_ENABLE, true);	
nodeblacklistingenabled 

protected void serviceInit(Configuration conf) throws Exception {	super.serviceInit(conf);	nodeBlacklistingEnabled = conf.getBoolean(MRJobConfig.MR_AM_JOB_NODE_BLACKLISTING_ENABLE, true);	maxTaskFailuresPerNode = conf.getInt(MRJobConfig.MAX_TASK_FAILURES_PER_TRACKER, 3);	blacklistDisablePercent = conf.getInt( MRJobConfig.MR_AM_IGNORE_BLACKLISTING_BLACKLISTED_NODE_PERECENT, MRJobConfig.DEFAULT_MR_AM_IGNORE_BLACKLISTING_BLACKLISTED_NODE_PERCENT);	
maxtaskfailurespernode is 

protected void serviceInit(Configuration conf) throws Exception {	super.serviceInit(conf);	nodeBlacklistingEnabled = conf.getBoolean(MRJobConfig.MR_AM_JOB_NODE_BLACKLISTING_ENABLE, true);	maxTaskFailuresPerNode = conf.getInt(MRJobConfig.MAX_TASK_FAILURES_PER_TRACKER, 3);	blacklistDisablePercent = conf.getInt( MRJobConfig.MR_AM_IGNORE_BLACKLISTING_BLACKLISTED_NODE_PERECENT, MRJobConfig.DEFAULT_MR_AM_IGNORE_BLACKLISTING_BLACKLISTED_NODE_PERCENT);	if (blacklistDisablePercent < -1 || blacklistDisablePercent > 100) {	throw new YarnRuntimeException("Invalid blacklistDisablePercent: " + blacklistDisablePercent + ". Should be an integer between 0 and 100 or -1 to disabled");	}	
blacklistdisablepercent is 

int limit = reqLimit.getNumContainers();	Map<String, Map<Resource, ResourceRequest>> remoteRequests = remoteRequestsTable.get(reqLimit.getPriority());	Map<Resource, ResourceRequest> reqMap = (remoteRequests != null) ? remoteRequests.get(ResourceRequest.ANY) : null;	ResourceRequest req = (reqMap != null) ? reqMap.get(reqLimit.getCapability()) : null;	if (req == null) {	continue;	}	if (ask.remove(req) || requestLimitsToUpdate.contains(req)) {	ResourceRequest newReq = req.getNumContainers() > limit ? reqLimit : req;	ask.add(newReq);	
applying ask limit of for priority and capability 

protected void computeIgnoreBlacklisting() {	if (!nodeBlacklistingEnabled) {	return;	}	if (blacklistDisablePercent != -1 && (blacklistedNodeCount != blacklistedNodes.size() || clusterNmCount != lastClusterNmCount)) {	blacklistedNodeCount = blacklistedNodes.size();	if (clusterNmCount == 0) {	
knownnode count at not computing ignoreblacklisting 

return;	}	if (blacklistDisablePercent != -1 && (blacklistedNodeCount != blacklistedNodes.size() || clusterNmCount != lastClusterNmCount)) {	blacklistedNodeCount = blacklistedNodes.size();	if (clusterNmCount == 0) {	return;	}	int val = (int) ((float) blacklistedNodes.size() / clusterNmCount * 100);	if (val >= blacklistDisablePercent) {	if (ignoreBlacklisting.compareAndSet(false, true)) {	
ignore blacklisting set to true known blacklisted 

return;	}	int val = (int) ((float) blacklistedNodes.size() / clusterNmCount * 100);	if (val >= blacklistDisablePercent) {	if (ignoreBlacklisting.compareAndSet(false, true)) {	blacklistAdditions.clear();	blacklistRemovals.addAll(blacklistedNodes);	}	} else {	if (ignoreBlacklisting.compareAndSet(true, false)) {	
ignore blacklisting set to false known blacklisted 

protected void containerFailedOnHost(String hostName) {	if (!nodeBlacklistingEnabled) {	return;	}	if (blacklistedNodes.contains(hostName)) {	if (LOG.isDebugEnabled()) {	
host is already blacklisted 

return;	}	if (blacklistedNodes.contains(hostName)) {	if (LOG.isDebugEnabled()) {	}	return;	}	Integer failures = nodeFailures.remove(hostName);	failures = failures == null ? Integer.valueOf(0) : failures;	failures++;	
failures on node 

return;	}	Integer failures = nodeFailures.remove(hostName);	failures = failures == null ? Integer.valueOf(0) : failures;	failures++;	if (failures >= maxTaskFailuresPerNode) {	blacklistedNodes.add(hostName);	if (!ignoreBlacklisting.get()) {	blacklistAdditions.add(hostName);	}	
blacklisted host 

private void decResourceRequest(Priority priority, String resourceName, Resource capability) {	Map<String, Map<Resource, ResourceRequest>> remoteRequests = this.remoteRequestsTable.get(priority);	Map<Resource, ResourceRequest> reqMap = remoteRequests.get(resourceName);	if (reqMap == null) {	if (LOG.isDebugEnabled()) {	
not decrementing resource as is not present in request table 

========================= hadoop sample_5305 =========================

public static Path createSnapshot(DistributedFileSystem hdfs, Path snapshotRoot, String snapshotName) throws Exception {	
createsnapshot for 

public static void compareDumpedTreeInFile(File file1, File file2, boolean compareQuota) throws IOException {	try {	compareDumpedTreeInFile(file1, file2, compareQuota, false);	} catch(Throwable t) {	
failed comparedumpedtreeinfile 

========================= hadoop sample_7338 =========================

public boolean deleteWithExistsCheck(@Nullable FsVolumeSpi volume, File f) {	final long begin = profilingEventHook.beforeMetadataOp(volume, DELETE);	try {	faultInjectorEventHook.beforeMetadataOp(volume, DELETE);	boolean deleted = !f.exists() || f.delete();	profilingEventHook.afterMetadataOp(volume, DELETE, begin);	if (!deleted) {	
failed to delete file 

========================= hadoop sample_7912 =========================

public static YarnAuthorizationProvider getInstance(Configuration conf) {	synchronized (YarnAuthorizationProvider.class) {	if (authorizer == null) {	Class<?> authorizerClass = conf.getClass(YarnConfiguration.YARN_AUTHORIZATION_PROVIDER, ConfiguredYarnAuthorizer.class);	authorizer = (YarnAuthorizationProvider) ReflectionUtils.newInstance( authorizerClass, conf);	authorizer.init(conf);	
is instantiated 

public static void destroy() {	synchronized (YarnAuthorizationProvider.class) {	if (authorizer != null) {	
is destroyed 

========================= hadoop sample_2213 =========================

private void parse() throws IOException {	FSDataInputStream in = null;	try {	in =  getPreviousJobHistoryFileStream(getConfig(), applicationAttemptId);	} catch (IOException e) {	
error trying to open previous history file no history data will be copied over 

FSDataInputStream in = null;	try {	in =  getPreviousJobHistoryFileStream(getConfig(), applicationAttemptId);	} catch (IOException e) {	return;	}	JobHistoryParser parser = new JobHistoryParser(in);	parser.parse(this);	Exception parseException = parser.getParseException();	if (parseException != null) {	
got an error parsing job history file ignoring incomplete events 

public static FSDataInputStream getPreviousJobHistoryFileStream( Configuration conf, ApplicationAttemptId applicationAttemptId) throws IOException {	FSDataInputStream in = null;	Path historyFile = null;	String jobId = TypeConverter.fromYarn(applicationAttemptId.getApplicationId()) .toString();	String jobhistoryDir = JobHistoryUtils.getConfiguredHistoryStagingDirPrefix(conf, jobId);	Path histDirPath = FileContext.getFileContext(conf).makeQualified(new Path(jobhistoryDir));	FileContext fc = FileContext.getFileContext(histDirPath.toUri(), conf);	historyFile = fc.makeQualified(JobHistoryUtils.getStagingJobHistoryFile(histDirPath, jobId, (applicationAttemptId.getAttemptId() - 1)));	
history file is at 

========================= hadoop sample_5169 =========================

public void run() {	
starting reconfiguration task 

final Configuration newConf = parent.getNewConf();	final Collection<PropertyChange> changes = parent.getChangedProperties(newConf, oldConf);	Map<PropertyChange, Optional<String>> results = Maps.newHashMap();	ConfigRedactor oldRedactor = new ConfigRedactor(oldConf);	ConfigRedactor newRedactor = new ConfigRedactor(newConf);	for (PropertyChange change : changes) {	String errorMessage = null;	String oldValRedacted = oldRedactor.redact(change.prop, change.oldVal);	String newValRedacted = newRedactor.redact(change.prop, change.newVal);	if (!parent.isPropertyReconfigurable(change.prop)) {	
property s is not configurable old value s new value s 

public final void reconfigureProperty(String property, String newVal) throws ReconfigurationException {	if (isPropertyReconfigurable(property)) {	
changing property to 

========================= hadoop sample_4282 =========================

final ExtendedBlock eb = new ExtendedBlock(BLOCK_POOL_IDS[0], 0);	final CountDownLatch startFinalizeLatch = new CountDownLatch(1);	final CountDownLatch blockReportReceivedLatch = new CountDownLatch(1);	final CountDownLatch volRemoveStartedLatch = new CountDownLatch(1);	final CountDownLatch volRemoveCompletedLatch = new CountDownLatch(1);	class BlockReportThread extends Thread {	public void run() {	try {	volRemoveStartedLatch.await();	} catch (Exception e) {	
unexpected exception when waiting for vol removal 

final CountDownLatch startFinalizeLatch = new CountDownLatch(1);	final CountDownLatch blockReportReceivedLatch = new CountDownLatch(1);	final CountDownLatch volRemoveStartedLatch = new CountDownLatch(1);	final CountDownLatch volRemoveCompletedLatch = new CountDownLatch(1);	class BlockReportThread extends Thread {	public void run() {	try {	volRemoveStartedLatch.await();	} catch (Exception e) {	}	
getting block report 

final CountDownLatch blockReportReceivedLatch = new CountDownLatch(1);	final CountDownLatch volRemoveStartedLatch = new CountDownLatch(1);	final CountDownLatch volRemoveCompletedLatch = new CountDownLatch(1);	class BlockReportThread extends Thread {	public void run() {	try {	volRemoveStartedLatch.await();	} catch (Exception e) {	}	dataset.getBlockReports(eb.getBlockPoolId());	
successfully received block report 

volRemoveStartedLatch.await();	} catch (Exception e) {	}	dataset.getBlockReports(eb.getBlockPoolId());	blockReportReceivedLatch.countDown();	}	}	class ResponderThread extends Thread {	public void run() {	try (ReplicaHandler replica = dataset .createRbw(StorageType.DEFAULT, eb, false)) {	
createrbw finished 

blockReportReceivedLatch.countDown();	}	}	class ResponderThread extends Thread {	public void run() {	try (ReplicaHandler replica = dataset .createRbw(StorageType.DEFAULT, eb, false)) {	startFinalizeLatch.countDown();	try {	Thread.sleep(1000);	} catch (InterruptedException ie) {	
ignoring 

class ResponderThread extends Thread {	public void run() {	try (ReplicaHandler replica = dataset .createRbw(StorageType.DEFAULT, eb, false)) {	startFinalizeLatch.countDown();	try {	Thread.sleep(1000);	} catch (InterruptedException ie) {	}	blockReportReceivedLatch.await();	dataset.finalizeBlock(eb, false);	
finalizeblock finished 

public void run() {	try (ReplicaHandler replica = dataset .createRbw(StorageType.DEFAULT, eb, false)) {	startFinalizeLatch.countDown();	try {	Thread.sleep(1000);	} catch (InterruptedException ie) {	}	blockReportReceivedLatch.await();	dataset.finalizeBlock(eb, false);	} catch (Exception e) {	
exception caught this should not affect the test 

} catch (Exception e) {	}	}	}	class VolRemoveThread extends Thread {	public void run() {	Set<File> volumesToRemove = new HashSet<>();	try {	volumesToRemove.add(StorageLocation.parse( dataset.getVolume(eb).getBasePath()).getFile());	} catch (Exception e) {	
problem preparing volumes to remove 

}	}	class VolRemoveThread extends Thread {	public void run() {	Set<File> volumesToRemove = new HashSet<>();	try {	volumesToRemove.add(StorageLocation.parse( dataset.getVolume(eb).getBasePath()).getFile());	} catch (Exception e) {	Assert.fail("Exception in remove volume thread, check log for " + "details.");	}	
removing volume 

class VolRemoveThread extends Thread {	public void run() {	Set<File> volumesToRemove = new HashSet<>();	try {	volumesToRemove.add(StorageLocation.parse( dataset.getVolume(eb).getBasePath()).getFile());	} catch (Exception e) {	Assert.fail("Exception in remove volume thread, check log for " + "details.");	}	dataset.removeVolumes(volumesToRemove, true);	volRemoveCompletedLatch.countDown();	
removed volume 

========================= hadoop sample_7276 =========================

private MiniJournalCluster(Builder b) throws IOException {	
starting minijournalcluster with journal nodes 

private MiniJournalCluster(Builder b) throws IOException {	if (b.baseDir != null) {	this.baseDir = new File(b.baseDir);	} else {	this.baseDir = new File(MiniDFSCluster.getBaseDirectory());	}	nodes = new JNInfo[b.numJournalNodes];	for (int i = 0; i < b.numJournalNodes; i++) {	if (b.format) {	File dir = getStorageDir(i);	
fully deleting jn directory 

public URI getQuorumJournalURI(String jid) {	List<String> addrs = Lists.newArrayList();	for (JNInfo info : nodes) {	addrs.add("127.0.0.1:" + info.ipcAddr.getPort());	}	String addrsVal = Joiner.on(";").join(addrs);	
setting logger addresses to 

public void shutdown() throws IOException {	boolean failed = false;	for (JNInfo info : nodes) {	try {	info.node.stopAndJoin(0);	} catch (Exception e) {	failed = true;	
unable to stop journal node 

qjm.close();	} catch (IOException e) {	return false;	}	return true;	}	}, 50, 3000);	} catch (TimeoutException e) {	fail("Time out while waiting for journal node " + index + " to start.");	} catch (InterruptedException ite) {	
thread interrupted when waiting for node start 

========================= hadoop sample_7626 =========================

public boolean isSchedulerReadyForAllocatingContainers() {	if (isSchedulerReady) {	return isSchedulerReady;	}	isSchedulerReady = (systemClock.getTime() - schedulerRecoveryStartTime) > schedulerRecoveryWaitTime;	if (!isSchedulerReady && printLog) {	
skip allocating containers scheduler is waiting for recovery 

public boolean isSchedulerReadyForAllocatingContainers() {	if (isSchedulerReady) {	return isSchedulerReady;	}	isSchedulerReady = (systemClock.getTime() - schedulerRecoveryStartTime) > schedulerRecoveryWaitTime;	if (!isSchedulerReady && printLog) {	printLog = false;	}	if (isSchedulerReady) {	
scheduler recovery is done start allocating new containers 

========================= hadoop sample_695 =========================

public long renewToken( Token<? extends AbstractDelegationTokenIdentifier> token, String renewer) throws IOException {	
renewing token with renewer 

public void cancelToken( Token<? extends AbstractDelegationTokenIdentifier> token, String canceler) throws IOException {	
cancelling token with canceler 

========================= hadoop sample_3720 =========================

final int seed = new Random().nextInt();	final DataOutputBuffer dataBuf = new DataOutputBuffer();	final RandomDatum.Generator generator = new RandomDatum.Generator(seed);	for(int i = 0; i < count; ++i) {	generator.next();	final RandomDatum key = generator.getKey();	final RandomDatum value = generator.getValue();	key.write(dataBuf);	value.write(dataBuf);	}	
generated records 

========================= hadoop sample_2832 =========================

private void resetStateTest(Configuration conf, int seed, int count, String codecClass) throws IOException {	CompressionCodec codec = null;	try {	codec = (CompressionCodec) ReflectionUtils.newInstance(conf .getClassByName(codecClass), conf);	} catch (ClassNotFoundException cnfe) {	throw new IOException("Illegal codec!");	}	
created a codec object of type 

}	DataOutputBuffer data = new DataOutputBuffer();	RandomDatum.Generator generator = new RandomDatum.Generator(seed);	for (int i = 0; i < count; ++i) {	generator.next();	RandomDatum key = generator.getKey();	RandomDatum value = generator.getValue();	key.write(data);	value.write(data);	}	
generated records 

RandomDatum value = generator.getValue();	key.write(data);	value.write(data);	}	DataOutputBuffer compressedDataBuffer = new DataOutputBuffer();	DataOutputStream deflateOut = new DataOutputStream( new BufferedOutputStream(compressedDataBuffer));	CompressionOutputStream deflateFilter = codec .createOutputStream(deflateOut);	deflateFilter.write(data.getData(), 0, data.getLength());	deflateFilter.finish();	deflateFilter.flush();	
finished compressing data 

key.write(data);	value.write(data);	}	DataOutputBuffer compressedDataBuffer = new DataOutputBuffer();	DataOutputStream deflateOut = new DataOutputStream( new BufferedOutputStream(compressedDataBuffer));	CompressionOutputStream deflateFilter = codec .createOutputStream(deflateOut);	deflateFilter.write(data.getData(), 0, data.getLength());	deflateFilter.finish();	deflateFilter.flush();	deflateFilter.resetState();	
finished reseting deflator 

}	DataInputBuffer originalData = new DataInputBuffer();	DataInputStream originalIn = new DataInputStream( new BufferedInputStream(originalData));	originalData.reset(data.getData(), 0, data.getLength());	compressedDataBuffer.reset();	deflateOut = new DataOutputStream(new BufferedOutputStream( compressedDataBuffer));	deflateFilter = codec.createOutputStream(deflateOut);	deflateFilter.write(data.getData(), 0, data.getLength());	deflateFilter.finish();	deflateFilter.flush();	
finished re compressing data 

RandomDatum k1 = new RandomDatum();	RandomDatum v1 = new RandomDatum();	k1.readFields(originalIn);	v1.readFields(originalIn);	RandomDatum k2 = new RandomDatum();	RandomDatum v2 = new RandomDatum();	k2.readFields(inflateIn);	v2.readFields(inflateIn);	assertTrue( "original and compressed-then-decompressed-output not equal", k1.equals(k2) && v1.equals(v2));	}	
success completed checking records 

========================= hadoop sample_2998 =========================

public ReplicaInputStreams( InputStream dataStream, InputStream checksumStream, FsVolumeReference volumeRef, FileIoProvider fileIoProvider) {	this.volumeRef = volumeRef;	this.fileIoProvider = fileIoProvider;	this.dataIn = dataStream;	this.checksumIn = checksumStream;	if (dataIn instanceof FileInputStream) {	try {	dataInFd = ((FileInputStream) dataIn).getFD();	} catch (Exception e) {	
could not get file descriptor for inputstream of class 

this.volumeRef = volumeRef;	this.fileIoProvider = fileIoProvider;	this.dataIn = dataStream;	this.checksumIn = checksumStream;	if (dataIn instanceof FileInputStream) {	try {	dataInFd = ((FileInputStream) dataIn).getFD();	} catch (Exception e) {	}	} else {	
could not get file descriptor for inputstream of class 

========================= hadoop sample_7919 =========================

public void testPipes() throws IOException {	if (System.getProperty("compile.c++") == null) {	
compile c is not defined so skipping testpipes 

========================= hadoop sample_5477 =========================

reader = new BufferedReader(isr);	StringBuilder sb = new StringBuilder();	String line = null;	while ((line = reader.readLine()) != null) {	sb.append(line);	}	String jmxOutput = sb.toString();	JSONObject json = new JSONObject(jmxOutput);	ret = json.getJSONArray("beans");	} catch (IOException e) {	
cannot read jmx bean from server 

StringBuilder sb = new StringBuilder();	String line = null;	while ((line = reader.readLine()) != null) {	sb.append(line);	}	String jmxOutput = sb.toString();	JSONObject json = new JSONObject(jmxOutput);	ret = json.getJSONArray("beans");	} catch (IOException e) {	} catch (JSONException e) {	
cannot parse jmx output for from server 

String line = null;	while ((line = reader.readLine()) != null) {	sb.append(line);	}	String jmxOutput = sb.toString();	JSONObject json = new JSONObject(jmxOutput);	ret = json.getJSONArray("beans");	} catch (IOException e) {	} catch (JSONException e) {	} catch (Exception e) {	
cannot parse jmx output for from server 

JSONObject json = new JSONObject(jmxOutput);	ret = json.getJSONArray("beans");	} catch (IOException e) {	} catch (JSONException e) {	} catch (Exception e) {	} finally {	if (reader != null) {	try {	reader.close();	} catch (IOException e) {	
problem closing 

private static <T, R> T newInstance(final Configuration conf, final R context, final Class<R> contextClass, final Class<T> clazz) {	try {	if (contextClass == null) {	Constructor<T> constructor = clazz.getConstructor();	return constructor.newInstance();	} else {	Constructor<T> constructor = clazz.getConstructor( Configuration.class, contextClass);	return constructor.newInstance(conf, context);	}	} catch (ReflectiveOperationException e) {	
could not instantiate 

========================= hadoop sample_8300 =========================

MockAM am = rm.sendAMLaunched(attempt.getAppAttemptId());	am.registerAppAttempt();	int request = 13;	am.allocate("h1" , 1000, request, new ArrayList<ContainerId>());	List<Container> conts = am.allocate(new ArrayList<ResourceRequest>(), new ArrayList<ContainerId>()).getAllocatedContainers();	int contReceived = conts.size();	while (contReceived < 3) {	nm1.nodeHeartbeat(true);	conts.addAll(am.allocate(new ArrayList<ResourceRequest>(), new ArrayList<ContainerId>()).getAllocatedContainers());	contReceived = conts.size();	
got containers waiting to get 

contReceived = conts.size();	Thread.sleep(WAIT_SLEEP_MS);	}	Assert.assertEquals(3, conts.size());	conts = am.allocate(new ArrayList<ResourceRequest>(), new ArrayList<ContainerId>()).getAllocatedContainers();	contReceived = conts.size();	while (contReceived < 10) {	nm2.nodeHeartbeat(true);	conts.addAll(am.allocate(new ArrayList<ResourceRequest>(), new ArrayList<ContainerId>()).getAllocatedContainers());	contReceived = conts.size();	
got containers waiting to get 

Assert.assertEquals(0, response.getAllocatedContainers().size());	allocateContainersAndValidateNMTokens(am, containersReceivedForNM2, 2, nmTokens, nm2);	Assert.assertEquals(2, nmTokens.size());	nm2 = rm.registerNode("h2:1234", 10000);	Map<NodeId, RMNode> nodes = rm.getRMContext().getRMNodes();	while (nodes.get(nm2.getNodeId()).getLastNodeHeartBeatResponse() .getResponseId() > 0) {	Thread.sleep(WAIT_SLEEP_MS);	}	int interval = 40;	while (nmTokenSecretManager .isApplicationAttemptNMTokenPresent(attempt.getAppAttemptId(), nm2.getNodeId()) && interval-- > 0) {	
waiting for nmtoken to be cleared for 

protected void allocateContainersAndValidateNMTokens(MockAM am, ArrayList<Container> containersReceived, int totalContainerRequested, HashMap<String, Token> nmTokens, MockNM nm) throws Exception, InterruptedException {	ArrayList<ContainerId> releaseContainerList = new ArrayList<ContainerId>();	AllocateResponse response;	ArrayList<ResourceRequest> resourceRequest = new ArrayList<ResourceRequest>();	while (containersReceived.size() < totalContainerRequested) {	nm.nodeHeartbeat(true);	
requesting containers 

containersReceived.addAll(response.getAllocatedContainers());	if (!response.getNMTokens().isEmpty()) {	for (NMToken nmToken : response.getNMTokens()) {	String nodeId = nmToken.getNodeId().toString();	if (nmTokens.containsKey(nodeId)) {	Assert.fail("Duplicate NMToken received for : " + nodeId);	}	nmTokens.put(nodeId, nmToken.getToken());	}	}	
got containers waiting to get 

========================= hadoop sample_416 =========================

public LocalContainerLauncher(AppContext context, TaskUmbilicalProtocol umbilical, ClassLoader jobClassLoader) {	super(LocalContainerLauncher.class.getName());	this.context = context;	this.umbilical = umbilical;	this.jobClassLoader = jobClassLoader;	try {	curFC = FileContext.getFileContext(curDir.toURI());	} catch (UnsupportedFileSystemException ufse) {	
local filesystem is unsupported should never happen 

public void serviceStart() throws Exception {	taskRunner = HadoopExecutors.newSingleThreadExecutor(new ThreadFactoryBuilder(). setDaemon(true).setNameFormat("uber-SubtaskRunner").build());	eventHandler = new Thread(new EventHandler(), "uber-EventHandler");	if (jobClassLoader != null) {	
setting as the context classloader of thread 

public void serviceStart() throws Exception {	taskRunner = HadoopExecutors.newSingleThreadExecutor(new ThreadFactoryBuilder(). setDaemon(true).setNameFormat("uber-SubtaskRunner").build());	eventHandler = new Thread(new EventHandler(), "uber-EventHandler");	if (jobClassLoader != null) {	eventHandler.setContextClassLoader(jobClassLoader);	} else {	
context classloader of thread 

public void run() {	ContainerLauncherEvent event = null;	final Map<TaskAttemptID, MapOutputFile> localMapFiles = new HashMap<TaskAttemptID, MapOutputFile>();	while (!Thread.currentThread().isInterrupted()) {	try {	event = eventQueue.take();	} catch (InterruptedException e) {	
returning interrupted 

public void run() {	ContainerLauncherEvent event = null;	final Map<TaskAttemptID, MapOutputFile> localMapFiles = new HashMap<TaskAttemptID, MapOutputFile>();	while (!Thread.currentThread().isInterrupted()) {	try {	event = eventQueue.take();	} catch (InterruptedException e) {	break;	}	
processing the event 

for (ThreadInfo ti : tInfos) {	System.out.println(ti.toString());	}	} catch (Throwable t) {	System.out.println("Could not create full thread dump: " + t.getMessage());	}	}	TaskAttemptId taId = event.getTaskAttemptID();	Future<?> future = futures.remove(taId);	if (future != null) {	
canceling the task attempt 

System.out.println("Could not create full thread dump: " + t.getMessage());	}	}	TaskAttemptId taId = event.getTaskAttemptID();	Future<?> future = futures.remove(taId);	if (future != null) {	future.cancel(true);	}	context.getEventHandler().handle( new TaskAttemptEvent(taId, TaskAttemptEventType.TA_CONTAINER_CLEANED));	} else if (event.getType() == EventType.CONTAINER_COMPLETED) {	
container completed 

}	}	TaskAttemptId taId = event.getTaskAttemptID();	Future<?> future = futures.remove(taId);	if (future != null) {	future.cancel(true);	}	context.getEventHandler().handle( new TaskAttemptEvent(taId, TaskAttemptEventType.TA_CONTAINER_CLEANED));	} else if (event.getType() == EventType.CONTAINER_COMPLETED) {	} else {	
ignoring unexpected event 

context.getEventHandler().handle(jce);	}	runSubtask(remoteTask, ytask.getType(), attemptID, numMapTasks, (numReduceTasks > 0), localMapFiles);	context.getEventHandler().handle(new TaskAttemptEvent(attemptID, TaskAttemptEventType.TA_CONTAINER_COMPLETED));	} catch (RuntimeException re) {	JobCounterUpdateEvent jce = new JobCounterUpdateEvent(attemptID.getTaskId().getJobId());	jce.addCounterUpdate(JobCounter.NUM_FAILED_UBERTASKS, 1);	context.getEventHandler().handle(jce);	context.getEventHandler().handle(new TaskAttemptEvent(attemptID, TaskAttemptEventType.TA_CONTAINER_COMPLETED));	} catch (IOException ioe) {	
oopsie this can never happen 

context.getEventHandler().handle(new TaskAttemptEvent(attemptID, TaskAttemptEventType.TA_CONTAINER_COMPLETED));	} catch (RuntimeException re) {	JobCounterUpdateEvent jce = new JobCounterUpdateEvent(attemptID.getTaskId().getJobId());	jce.addCounterUpdate(JobCounter.NUM_FAILED_UBERTASKS, 1);	context.getEventHandler().handle(jce);	context.getEventHandler().handle(new TaskAttemptEvent(attemptID, TaskAttemptEventType.TA_CONTAINER_COMPLETED));	} catch (IOException ioe) {	ExitUtil.terminate(-1);	} finally {	if (futures.remove(attemptID) != null) {	
removed attempt from the futures to keep track of 

org.apache.hadoop.mapred.TaskAttemptID classicAttemptID = TypeConverter.fromYarn(attemptID);	try {	JobConf conf = new JobConf(getConfig());	conf.set(JobContext.TASK_ID, task.getTaskID().toString());	conf.set(JobContext.TASK_ATTEMPT_ID, classicAttemptID.toString());	conf.setBoolean(JobContext.TASK_ISMAP, (taskType == TaskType.MAP));	conf.setInt(JobContext.TASK_PARTITION, task.getPartition());	conf.set(JobContext.ID, task.getJobID().toString());	String[] localSysDirs = StringUtils.getTrimmedStrings( System.getenv(Environment.LOCAL_DIRS.name()));	conf.setStrings(MRConfig.LOCAL_DIR, localSysDirs);	
for uber task 

conf.setBoolean(JobContext.TASK_ISMAP, (taskType == TaskType.MAP));	conf.setInt(JobContext.TASK_PARTITION, task.getPartition());	conf.set(JobContext.ID, task.getJobID().toString());	String[] localSysDirs = StringUtils.getTrimmedStrings( System.getenv(Environment.LOCAL_DIRS.name()));	conf.setStrings(MRConfig.LOCAL_DIR, localSysDirs);	conf.setBoolean("mapreduce.task.uberized", true);	task.setEncryptedSpillKey(encryptedSpillKey);	YarnChild.setEncryptedSpillKeyIfRequired(task);	if (taskType == TaskType.MAP) {	if (doneWithMaps) {	
container remote launch contains a map task but should be finished with maps 

if (renameOutputs) {	MapOutputFile renamed = renameMapOutputForReduce(conf, attemptID, map.getMapOutputFile());	localMapFiles.put(classicAttemptID, renamed);	}	relocalize();	if (++finishedSubMaps == numMapTasks) {	doneWithMaps = true;	}	} else /* TaskType.REDUCE */ {	if (!doneWithMaps) {	
container remote launch contains a reduce task but not yet finished with maps 

}	conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.LOCAL_FRAMEWORK_NAME);	conf.set(MRConfig.MASTER_ADDRESS, "local");	ReduceTask reduce = (ReduceTask)task;	reduce.setLocalMapFiles(localMapFiles);	reduce.setConf(conf);	reduce.run(conf, umbilical);	relocalize();	}	} catch (FSError e) {	
fserror from child 

reduce.setConf(conf);	reduce.run(conf, umbilical);	relocalize();	}	} catch (FSError e) {	if (!ShutdownHookManager.get().isShutdownInProgress()) {	umbilical.fsError(classicAttemptID, e.getMessage());	}	throw new RuntimeException();	} catch (Exception exception) {	
exception running local uberized child 

if (!ShutdownHookManager.get().isShutdownInProgress()) {	umbilical.fsError(classicAttemptID, e.getMessage());	}	throw new RuntimeException();	} catch (Exception exception) {	try {	if (task != null) {	task.taskCleanup(umbilical);	}	} catch (Exception e) {	
exception cleaning up 

} catch (Exception exception) {	try {	if (task != null) {	task.taskCleanup(umbilical);	}	} catch (Exception e) {	}	umbilical.reportDiagnosticInfo(classicAttemptID, StringUtils.stringifyException(exception));	throw new RuntimeException();	} catch (Throwable throwable) {	
error running local uberized child 

if (!localizedFiles.contains(curLocalFiles[j])) {	boolean deleted = false;	try {	if (curFC != null) {	deleted = curFC.delete(new Path(curLocalFiles[j].getName()),true);	}	} catch (IOException e) {	deleted = false;	}	if (!deleted) {	
unable to delete unexpected local file dir insufficient permissions 

protected static MapOutputFile renameMapOutputForReduce(JobConf conf, TaskAttemptId mapId, MapOutputFile subMapOutputFile) throws IOException {	FileSystem localFs = FileSystem.getLocal(conf);	Path mapOut = subMapOutputFile.getOutputFile();	FileStatus mStatus = localFs.getFileStatus(mapOut);	Path reduceIn = subMapOutputFile.getInputFileForWrite( TypeConverter.fromYarn(mapId).getTaskID(), mStatus.getLen());	Path mapOutIndex = subMapOutputFile.getOutputIndexFile();	Path reduceInIndex = new Path(reduceIn.toString() + ".index");	if (LOG.isDebugEnabled()) {	
renaming map output file for task attempt from original location to destination 

========================= hadoop sample_5162 =========================

private void readProcMemInfoFile(boolean readAgain) {	if (readMemInfoFile && !readAgain) {	return;	}	BufferedReader in;	InputStreamReader fReader;	try {	fReader = new InputStreamReader( new FileInputStream(procfsMemFile), Charset.forName("UTF-8"));	in = new BufferedReader(fReader);	} catch (FileNotFoundException f) {	
couldn t read can t determine memory settings 

hardwareCorruptSize = Long.parseLong(mat.group(2));	} else if (mat.group(1).equals(HUGEPAGESTOTAL_STRING)) {	hugePagesTotal = Long.parseLong(mat.group(2));	} else if (mat.group(1).equals(HUGEPAGESIZE_STRING)) {	hugePageSize = Long.parseLong(mat.group(2));	}	}	str = in.readLine();	}	} catch (IOException io) {	
error reading the stream 

}	str = in.readLine();	}	} catch (IOException io) {	} finally {	try {	fReader.close();	try {	in.close();	} catch (IOException i) {	
error closing the stream 

}	} catch (IOException io) {	} finally {	try {	fReader.close();	try {	in.close();	} catch (IOException i) {	}	} catch (IOException i) {	
error closing the stream 

if (readCpuInfoFile) {	return;	}	HashSet<String> coreIdSet = new HashSet<>();	BufferedReader in;	InputStreamReader fReader;	try {	fReader = new InputStreamReader( new FileInputStream(procfsCpuFile), Charset.forName("UTF-8"));	in = new BufferedReader(fReader);	} catch (FileNotFoundException f) {	
couldn t read can t determine cpu info 

currentPhysicalId = str;	}	mat = CORE_ID_FORMAT.matcher(str);	if (mat.find()) {	coreIdSet.add(currentPhysicalId + " " + str);	numCores = coreIdSet.size();	}	str = in.readLine();	}	} catch (IOException io) {	
error reading the stream 

}	str = in.readLine();	}	} catch (IOException io) {	} finally {	try {	fReader.close();	try {	in.close();	} catch (IOException i) {	
error closing the stream 

}	} catch (IOException io) {	} finally {	try {	fReader.close();	try {	in.close();	} catch (IOException i) {	}	} catch (IOException i) {	
error closing the stream 

if (mat.find()) {	long uTime = Long.parseLong(mat.group(1));	long nTime = Long.parseLong(mat.group(2));	long sTime = Long.parseLong(mat.group(3));	cpuTimeTracker.updateElapsedJiffies( BigInteger.valueOf(uTime + nTime + sTime), getCurrentTime());	break;	}	str = in.readLine();	}	} catch (IOException io) {	
error reading the stream 

}	str = in.readLine();	}	} catch (IOException io) {	} finally {	try {	fReader.close();	try {	in.close();	} catch (IOException i) {	
error closing the stream 

}	} catch (IOException io) {	} finally {	try {	fReader.close();	try {	in.close();	} catch (IOException i) {	}	} catch (IOException i) {	
error closing the stream 

if (mat.group(1).equals("lo")) {	str = in.readLine();	continue;	}	numNetBytesRead += Long.parseLong(mat.group(2));	numNetBytesWritten += Long.parseLong(mat.group(10));	}	str = in.readLine();	}	} catch (IOException io) {	
error reading the stream 

}	str = in.readLine();	}	} catch (IOException io) {	} finally {	try {	fReader.close();	try {	in.close();	} catch (IOException i) {	
error closing the stream 

}	} catch (IOException io) {	} finally {	try {	fReader.close();	try {	in.close();	} catch (IOException i) {	}	} catch (IOException i) {	
error closing the stream 

String sectorsWritten = mat.group(11);	if (null == sectorsRead || null == sectorsWritten) {	return;	}	numDisksBytesRead += Long.parseLong(sectorsRead) * sectorSize;	numDisksBytesWritten += Long.parseLong(sectorsWritten) * sectorSize;	}	str = in.readLine();	}	} catch (IOException e) {	
error reading the stream 

numDisksBytesRead += Long.parseLong(sectorsRead) * sectorSize;	numDisksBytesWritten += Long.parseLong(sectorsWritten) * sectorSize;	}	str = in.readLine();	}	} catch (IOException e) {	} finally {	try {	in.close();	} catch (IOException e) {	
error closing the stream 

========================= hadoop sample_3632 =========================

private void waitForActiveNN() {	try {	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	return dn.getAllBpOs().get(0).getActiveNN() != null;	}	}, 1000, 15 * 1000);	} catch (TimeoutException e) {	
failed to get active nn 

private void waitForActiveNN() {	try {	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	return dn.getAllBpOs().get(0).getActiveNN() != null;	}	}, 1000, 15 * 1000);	} catch (TimeoutException e) {	} catch (InterruptedException e) {	
interruptedexception while waiting to see active nn 

public void tearDown() throws IOException {	if (dn != null) {	try {	dn.shutdown();	} catch(Exception e) {	
cannot close 

public void testFinalizedReplicas () throws IOException {	if(LOG.isDebugEnabled()) {	
running 

public void testFinalizedRbwReplicas() throws IOException {	if(LOG.isDebugEnabled()) {	
running 

public void testFinalizedRwrReplicas() throws IOException {	if(LOG.isDebugEnabled()) {	
running 

public void testRBWReplicas() throws IOException {	if(LOG.isDebugEnabled()) {	
running 

public void testRBW_RWRReplicas() throws IOException {	if(LOG.isDebugEnabled()) {	
running 

public void testRWRReplicas() throws IOException {	if(LOG.isDebugEnabled()) {	
running 

public void testRecoveryInProgressException() throws IOException, InterruptedException {	if(LOG.isDebugEnabled()) {	
running 

public void testErrorReplicas() throws IOException, InterruptedException {	if(LOG.isDebugEnabled()) {	
running 

public void testZeroLenReplicas() throws IOException, InterruptedException {	if(LOG.isDebugEnabled()) {	
running 

public void testFailedReplicaUpdate() throws IOException {	if(LOG.isDebugEnabled()) {	
running 

public void testNoReplicaUnderRecovery() throws IOException {	if(LOG.isDebugEnabled()) {	
running 

public void testNotMatchedReplicaID() throws IOException {	if(LOG.isDebugEnabled()) {	
running 

public void testRURReplicas() throws Exception {	if (LOG.isDebugEnabled()) {	
running 

private void testStopWorker(final TestStopWorkerRunnable tswr) throws Exception {	
running 

Assert.assertEquals( TEST_STOP_WORKER_XCEIVER_STOP_TIMEOUT_MILLIS, dn.getDnConf().getXceiverStopTimeout());	final TestStopWorkerSemaphore progressParent = new TestStopWorkerSemaphore();	final TestStopWorkerSemaphore terminateSlowWriter = new TestStopWorkerSemaphore();	final AtomicReference<String> failure = new AtomicReference<String>(null);	Collection<RecoveringBlock> recoveringBlocks = initRecoveringBlocks();	final RecoveringBlock recoveringBlock = Iterators.get(recoveringBlocks.iterator(), 0);	final ExtendedBlock block = recoveringBlock.getBlock();	Thread slowWriterThread = new Thread(new Runnable() {	public void run() {	try {	
slowwriter creating rbw 

final TestStopWorkerSemaphore terminateSlowWriter = new TestStopWorkerSemaphore();	final AtomicReference<String> failure = new AtomicReference<String>(null);	Collection<RecoveringBlock> recoveringBlocks = initRecoveringBlocks();	final RecoveringBlock recoveringBlock = Iterators.get(recoveringBlocks.iterator(), 0);	final ExtendedBlock block = recoveringBlock.getBlock();	Thread slowWriterThread = new Thread(new Runnable() {	public void run() {	try {	ReplicaHandler replicaHandler = spyDN.data.createRbw(StorageType.DISK, block, false);	replicaHandler.close();	
slowwriter created rbw 

Collection<RecoveringBlock> recoveringBlocks = initRecoveringBlocks();	final RecoveringBlock recoveringBlock = Iterators.get(recoveringBlocks.iterator(), 0);	final ExtendedBlock block = recoveringBlock.getBlock();	Thread slowWriterThread = new Thread(new Runnable() {	public void run() {	try {	ReplicaHandler replicaHandler = spyDN.data.createRbw(StorageType.DISK, block, false);	replicaHandler.close();	progressParent.sem.release();	terminateSlowWriter.uninterruptiblyAcquire(60000);	
slowwriter exiting 

final RecoveringBlock recoveringBlock = Iterators.get(recoveringBlocks.iterator(), 0);	final ExtendedBlock block = recoveringBlock.getBlock();	Thread slowWriterThread = new Thread(new Runnable() {	public void run() {	try {	ReplicaHandler replicaHandler = spyDN.data.createRbw(StorageType.DISK, block, false);	replicaHandler.close();	progressParent.sem.release();	terminateSlowWriter.uninterruptiblyAcquire(60000);	} catch (Throwable t) {	
slowwriter got exception 

} catch (Throwable t) {	failure.compareAndSet(null, "slowWriter got exception " + t.getMessage());	}	}	});	slowWriterThread.start();	progressParent.uninterruptiblyAcquire(60000);	Thread stopWriterThread = new Thread(new Runnable() {	public void run() {	try {	
initiating 

failure.compareAndSet(null, "slowWriter got exception " + t.getMessage());	}	}	});	slowWriterThread.start();	progressParent.uninterruptiblyAcquire(60000);	Thread stopWriterThread = new Thread(new Runnable() {	public void run() {	try {	tswr.run(recoveringBlock);	
finished 

}	}	});	slowWriterThread.start();	progressParent.uninterruptiblyAcquire(60000);	Thread stopWriterThread = new Thread(new Runnable() {	public void run() {	try {	tswr.run(recoveringBlock);	} catch (Throwable t) {	
stopwriterthread got unexpected exception for 

========================= hadoop sample_7221 =========================

public void testDefaultURIWithoutScheme() throws Exception {	final Configuration conf = new Configuration();	conf.set(FileSystem.FS_DEFAULT_NAME_KEY, "/");	try {	FileContext.getFileContext(conf);	fail(UnsupportedFileSystemException.class + " not thrown!");	} catch (UnsupportedFileSystemException ufse) {	
expected exception 

========================= hadoop sample_3327 =========================

private FileContext(final AbstractFileSystem defFs, final FsPermission theUmask, final Configuration aConf) {	defaultFS = defFs;	umask = theUmask;	conf = aConf;	tracer = FsTracer.get(aConf);	try {	ugi = UserGroupInformation.getCurrentUser();	} catch (IOException e) {	
exception in getcurrentuser 

static void processDeleteOnExit() {	synchronized (DELETE_ON_EXIT) {	Set<Entry<FileContext, Set<Path>>> set = DELETE_ON_EXIT.entrySet();	for (Entry<FileContext, Set<Path>> entry : set) {	FileContext fc = entry.getKey();	Set<Path> paths = entry.getValue();	for (Path path : paths) {	try {	fc.delete(path, true);	} catch (IOException e) {	
ignoring failure to deleteonexit for path 

========================= hadoop sample_4231 =========================

protected void containerLaunchedOnNode( ContainerId containerId, SchedulerNode node) {	try {	readLock.lock();	SchedulerApplicationAttempt application = getCurrentAttemptForContainer(containerId);	if (application == null) {	
unknown application launched container on node 

protected void containerIncreasedOnNode(ContainerId containerId, SchedulerNode node, Container increasedContainerReportedByNM) {	SchedulerApplicationAttempt application = getCurrentAttemptForContainer( containerId);	if (application == null) {	
unknown application increased container on node 

public SchedulerAppReport getSchedulerAppInfo( ApplicationAttemptId appAttemptId) {	SchedulerApplicationAttempt attempt = getApplicationAttempt(appAttemptId);	if (attempt == null) {	if (LOG.isDebugEnabled()) {	
request for appinfo of unknown attempt 

public ApplicationResourceUsageReport getAppResourceUsageReport( ApplicationAttemptId appAttemptId) {	SchedulerApplicationAttempt attempt = getApplicationAttempt(appAttemptId);	if (attempt == null) {	if (LOG.isDebugEnabled()) {	
request for appinfo of unknown attempt 

public void recoverContainersOnNode(List<NMContainerStatus> containerReports, RMNode nm) {	try {	writeLock.lock();	if (!rmContext.isWorkPreservingRecoveryEnabled() || containerReports == null || (containerReports != null && containerReports.isEmpty())) {	return;	}	for (NMContainerStatus container : containerReports) {	ApplicationId appId = container.getContainerId().getApplicationAttemptId() .getApplicationId();	RMApp rmApp = rmContext.getRMApps().get(appId);	if (rmApp == null) {	
skip recovering container for unknown application 

}	for (NMContainerStatus container : containerReports) {	ApplicationId appId = container.getContainerId().getApplicationAttemptId() .getApplicationId();	RMApp rmApp = rmContext.getRMApps().get(appId);	if (rmApp == null) {	killOrphanContainerOnNode(nm, container);	continue;	}	SchedulerApplication<T> schedulerApp = applications.get(appId);	if (schedulerApp == null) {	
skip recovering container for unknown schedulerapplication application current state is 

RMApp rmApp = rmContext.getRMApps().get(appId);	if (rmApp == null) {	killOrphanContainerOnNode(nm, container);	continue;	}	SchedulerApplication<T> schedulerApp = applications.get(appId);	if (schedulerApp == null) {	killOrphanContainerOnNode(nm, container);	continue;	}	
recovering container 

continue;	}	SchedulerApplication<T> schedulerApp = applications.get(appId);	if (schedulerApp == null) {	killOrphanContainerOnNode(nm, container);	continue;	}	SchedulerApplicationAttempt schedulerAttempt = schedulerApp.getCurrentAppAttempt();	if (!rmApp.getApplicationSubmissionContext() .getKeepContainersAcrossApplicationAttempts()) {	if (schedulerAttempt.isStopped() || !schedulerAttempt .getApplicationAttemptId().equals( container.getContainerId().getApplicationAttemptId())) {	
skip recovering container for already stopped attempt 

schedulerAttempt.recoverContainer(schedulerNode, rmContainer);	RMAppAttempt appAttempt = rmApp.getCurrentAppAttempt();	if (appAttempt != null) {	Container masterContainer = appAttempt.getMasterContainer();	if (masterContainer != null && masterContainer.getId().equals( rmContainer.getContainerId())) {	((RMContainerImpl) rmContainer).setAMContainer(true);	}	}	if (schedulerAttempt.getPendingRelease().remove( container.getContainerId())) {	rmContainer.handle( new RMContainerFinishedEvent(container.getContainerId(), SchedulerUtils .createAbnormalContainerStatus(container.getContainerId(), SchedulerUtils.RELEASED_CONTAINER), RMContainerEventType.RELEASED));	
is released by application 

protected void createReleaseCache() {	new Timer().schedule(new TimerTask() {	public void run() {	clearPendingContainerCache();	
release request cache is cleaned up 

public void completedContainer(RMContainer rmContainer, ContainerStatus containerStatus, RMContainerEventType event) {	if (rmContainer == null) {	
container completed with event but corresponding rmcontainer doesn t exist 

completedContainerInternal(rmContainer, containerStatus, event);	completeOustandingUpdatesWhichAreReserved( rmContainer, containerStatus, event);	} else {	ContainerId containerId = rmContainer.getContainerId();	rmContainer.handle( new RMContainerFinishedEvent(containerId, containerStatus, event));	SchedulerApplicationAttempt schedulerAttempt = getCurrentAttemptForContainer(containerId);	if (schedulerAttempt != null) {	schedulerAttempt.removeRMContainer(containerId);	}	if (LOG.isDebugEnabled()) {	
completed container in state event 

protected abstract void completedContainerInternal(RMContainer rmContainer, ContainerStatus containerStatus, RMContainerEventType event);	protected void releaseContainers(List<ContainerId> containers, SchedulerApplicationAttempt attempt) {	for (ContainerId containerId : containers) {	RMContainer rmContainer = getRMContainer(containerId);	if (rmContainer == null) {	if (System.currentTimeMillis() - ResourceManager.getClusterTimeStamp() < nmExpireInterval) {	
doesn t exist add the container to the release request cache as it maybe on recovery 

public void updateNodeResource(RMNode nm, ResourceOption resourceOption) {	try {	writeLock.lock();	SchedulerNode node = getSchedulerNode(nm.getNodeID());	Resource newResource = resourceOption.getResource();	Resource oldResource = node.getTotalResource();	if (!oldResource.equals(newResource)) {	rmContext.getNodeLabelManager().updateNodeResource(nm.getNodeID(), newResource);	
update resource on node from to 

writeLock.lock();	SchedulerNode node = getSchedulerNode(nm.getNodeID());	Resource newResource = resourceOption.getResource();	Resource oldResource = node.getTotalResource();	if (!oldResource.equals(newResource)) {	rmContext.getNodeLabelManager().updateNodeResource(nm.getNodeID(), newResource);	nodeTracker.removeNode(nm.getNodeID());	node.updateTotalResource(newResource);	nodeTracker.addNode((N) node);	} else{	
update resource on node with the same resource 

protected int updateCompletedContainers(List<ContainerStatus> completedContainers, Resource releasedResources, NodeId nodeId) {	int releasedContainers = 0;	SchedulerNode node = getNode(nodeId);	List<ContainerId> untrackedContainerIdList = new ArrayList<ContainerId>();	for (ContainerStatus completedContainer : completedContainers) {	ContainerId containerId = completedContainer.getContainerId();	
container finished 

protected void nodeUpdate(RMNode nm) {	if (LOG.isDebugEnabled()) {	
nodeupdate cluster capacity 

List<ContainerStatus> completedContainers = updateNewContainerInfo(nm);	Resource releasedResources = Resource.newInstance(0, 0);	int releasedContainers = updateCompletedContainers(completedContainers, releasedResources, nm.getNodeID());	if (nm.getState() == NodeState.DECOMMISSIONING) {	this.rmContext .getDispatcher() .getEventHandler() .handle( new RMNodeResourceUpdateEvent(nm.getNodeID(), ResourceOption .newInstance(getSchedulerNode(nm.getNodeID()) .getAllocatedResource(), 0)));	}	updateSchedulerHealthInformation(releasedResources, releasedContainers);	updateNodeResourceUtilization(nm);	if(LOG.isDebugEnabled()) {	SchedulerNode node = getNode(nm.getNodeID());	
node being looked for scheduling availableresource 

protected void handleContainerUpdates( SchedulerApplicationAttempt appAttempt, ContainerUpdates updates) {	List<UpdateContainerRequest> promotionRequests = updates.getPromotionRequests();	if (promotionRequests != null && !promotionRequests.isEmpty()) {	
promotion update requests 

protected void handleContainerUpdates( SchedulerApplicationAttempt appAttempt, ContainerUpdates updates) {	List<UpdateContainerRequest> promotionRequests = updates.getPromotionRequests();	if (promotionRequests != null && !promotionRequests.isEmpty()) {	handleIncreaseRequests(appAttempt, promotionRequests);	}	List<UpdateContainerRequest> increaseRequests = updates.getIncreaseRequests();	if (increaseRequests != null && !increaseRequests.isEmpty()) {	
resource increase requests 

List<UpdateContainerRequest> promotionRequests = updates.getPromotionRequests();	if (promotionRequests != null && !promotionRequests.isEmpty()) {	handleIncreaseRequests(appAttempt, promotionRequests);	}	List<UpdateContainerRequest> increaseRequests = updates.getIncreaseRequests();	if (increaseRequests != null && !increaseRequests.isEmpty()) {	handleIncreaseRequests(appAttempt, increaseRequests);	}	List<UpdateContainerRequest> demotionRequests = updates.getDemotionRequests();	if (demotionRequests != null && !demotionRequests.isEmpty()) {	
demotion update requests 

List<UpdateContainerRequest> increaseRequests = updates.getIncreaseRequests();	if (increaseRequests != null && !increaseRequests.isEmpty()) {	handleIncreaseRequests(appAttempt, increaseRequests);	}	List<UpdateContainerRequest> demotionRequests = updates.getDemotionRequests();	if (demotionRequests != null && !demotionRequests.isEmpty()) {	handleDecreaseRequests(appAttempt, demotionRequests);	}	List<UpdateContainerRequest> decreaseRequests = updates.getDecreaseRequests();	if (decreaseRequests != null && !decreaseRequests.isEmpty()) {	
resource decrease requests 

private void handleIncreaseRequests( SchedulerApplicationAttempt applicationAttempt, List<UpdateContainerRequest> updateContainerRequests) {	for (UpdateContainerRequest uReq : updateContainerRequests) {	RMContainer rmContainer = rmContext.getScheduler().getRMContainer(uReq.getContainerId());	if (rmContainer != null) {	SchedulerNode schedulerNode = rmContext.getScheduler() .getSchedulerNode(rmContainer.getContainer().getNodeId());	if (!applicationAttempt.getUpdateContext() .checkAndAddToOutstandingIncreases( rmContainer, schedulerNode, uReq)) {	applicationAttempt.addToUpdateContainerErrors( UpdateContainerError.newInstance( RMServerUtils.UPDATE_OUTSTANDING_ERROR, uReq));	}	} else {	
cannot promote non existent or completed container 

RMContainer demotedRMContainer = createDemotedRMContainer(appAttempt, oppCntxt, rmContainer);	appAttempt.addToNewlyDemotedContainers( uReq.getContainerId(), demotedRMContainer);	} else {	RMContainer demotedRMContainer = createDecreasedRMContainer( appAttempt, uReq, rmContainer);	appAttempt.addToNewlyDecreasedContainers( uReq.getContainerId(), demotedRMContainer);	}	} else {	appAttempt.addToUpdateContainerErrors( UpdateContainerError.newInstance( RMServerUtils.UPDATE_OUTSTANDING_ERROR, uReq));	}	} else {	
cannot demote decrease non existent or completed container 

protected void rollbackContainerUpdate( ContainerId containerId) {	RMContainer rmContainer = getRMContainer(containerId);	if (rmContainer == null) {	
cannot rollback resource for container the container does not exist 

protected void rollbackContainerUpdate( ContainerId containerId) {	RMContainer rmContainer = getRMContainer(containerId);	if (rmContainer == null) {	return;	}	T app = getCurrentAttemptForContainer(containerId);	if (getCurrentAttemptForContainer(containerId) == null) {	
cannot rollback resource for container the application that the container belongs to does not exist 

protected void rollbackContainerUpdate( ContainerId containerId) {	RMContainer rmContainer = getRMContainer(containerId);	if (rmContainer == null) {	return;	}	T app = getCurrentAttemptForContainer(containerId);	if (getCurrentAttemptForContainer(containerId) == null) {	return;	}	if (Resources.fitsIn(rmContainer.getLastConfirmedResource(), rmContainer.getContainer().getResource())) {	
roll back resource for container 

public void run() {	while (!Thread.currentThread().isInterrupted()) {	try {	synchronized (updateThreadMonitor) {	updateThreadMonitor.wait(updateInterval);	}	update();	} catch (InterruptedException ie) {	
scheduler updatethread interrupted exiting 

public void run() {	while (!Thread.currentThread().isInterrupted()) {	try {	synchronized (updateThreadMonitor) {	updateThreadMonitor.wait(updateInterval);	}	update();	} catch (InterruptedException ie) {	return;	} catch (Exception e) {	
exception in scheduler updatethread 

========================= hadoop sample_1033 =========================

public static void deleteQuietly(FileSystem fs, Path path, boolean recursive) throws IOException {	if (fs != null && path != null) {	try {	fs.delete(path, recursive);	} catch (IOException e) {	
when deleting 

public static AzureBlobStorageTestAccount cleanupTestAccount( AzureBlobStorageTestAccount testAccount) {	if (testAccount != null) {	try {	testAccount.cleanup();	} catch (Exception e) {	
while cleaning up test account 

========================= hadoop sample_6349 =========================

rm.start();	final Configuration conf = rm.getConfig();	final YarnRPC rpc = YarnRPC.create(conf);	ApplicationMasterProtocol rmClient = null;	try {	MockNM nm1 = rm.registerNode("localhost:1234", 5120);	RMApp app = rm.submitApp(1024);	nm1.nodeHeartbeat(true);	int waitCount = 0;	while (containerManager.containerTokens == null && waitCount++ < 20) {	
waiting for am launch to happen 

count++;	}	Assert.assertTrue(attempt.getState() == RMAppAttemptState.FINISHED);	rpc.stopProxy(rmClient, conf);	rmClient = createRMClient(rm, conf, rpc, currentUser);	AllocateRequest allocateRequest = Records.newRecord(AllocateRequest.class);	try {	rmClient.allocate(allocateRequest);	Assert.fail("You got to be kidding me! " + "Using App tokens after app-finish should fail!");	} catch (Throwable t) {	
exception found is 

ApplicationMasterProtocol rmClient = null;	AMRMTokenSecretManager appTokenSecretManager = rm.getRMContext().getAMRMTokenSecretManager();	MasterKeyData oldKey = appTokenSecretManager.getMasterKey();	Assert.assertNotNull(oldKey);	try {	MockNM nm1 = rm.registerNode("localhost:1234", 5120);	RMApp app = rm.submitApp(1024);	nm1.nodeHeartbeat(true);	int waitCount = 0;	while (containerManager.containerTokens == null && waitCount++ < maxWaitAttempts) {	
waiting for am launch to happen 

========================= hadoop sample_441 =========================

UserGroupInformation.createRemoteUser("JobTracker");	DistributedFileSystem dfs = cluster.getFileSystem();	KeyProvider keyProvider = Mockito.mock(KeyProvider.class, withSettings().extraInterfaces( DelegationTokenExtension.class, CryptoExtension.class));	Mockito.when(keyProvider.getConf()).thenReturn(conf);	byte[] testIdentifier = "Test identifier for delegation token".getBytes();	Token<?> testToken = new Token(testIdentifier, new byte[0], new Text(), new Text());	Mockito.when(((DelegationTokenExtension)keyProvider). addDelegationTokens(anyString(), (Credentials)any())). thenReturn(new Token<?>[] { testToken });	dfs.getClient().setKeyProvider(keyProvider);	Credentials creds = new Credentials();	final Token<?> tokens[] = dfs.addDelegationTokens("JobTracker", creds);	
delegation tokens 

========================= hadoop sample_7108 =========================

slowwriters[i].start();	}	waitForBlockReplication(slowwriters);	for(SlowWriter s : slowwriters) {	s.checkReplication();	s.interruptRunning();	}	for(SlowWriter s : slowwriters) {	s.joinAndClose();	}	
verify the file 

static void sleepSeconds(final int waittime) throws InterruptedException {	
wait seconds 

public void run() {	int i = 0;	try {	sleep(sleepms);	for(; running; i++) {	
writes 

public void run() {	int i = 0;	try {	sleep(sleepms);	for(; running; i++) {	out.write(i);	out.hflush();	sleep(sleepms);	}	} catch(InterruptedException e) {	
interrupted 

public void testAppend() throws Exception {	final Configuration conf = new HdfsConfiguration();	final short REPLICATION = (short)3;	final MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf ).numDataNodes(1).build();	try {	final DistributedFileSystem fs = cluster.getFileSystem();	final Path f = new Path(DIR, "testAppend");	{	
create an empty file 

final DistributedFileSystem fs = cluster.getFileSystem();	final Path f = new Path(DIR, "testAppend");	{	fs.create(f, REPLICATION).close();	final FileStatus status = fs.getFileStatus(f);	Assert.assertEquals(REPLICATION, status.getReplication());	Assert.assertEquals(0L, status.getLen());	}	final byte[] bytes = new byte[1000];	{	
append bytes to 

final byte[] bytes = new byte[1000];	{	final FSDataOutputStream out = fs.append(f);	out.write(bytes);	out.close();	final FileStatus status = fs.getFileStatus(f);	Assert.assertEquals(REPLICATION, status.getReplication());	Assert.assertEquals(bytes.length, status.getLen());	}	{	
append another bytes to 

Assert.assertEquals(REPLICATION, status.getReplication());	Assert.assertEquals(bytes.length, status.getLen());	}	{	try {	final FSDataOutputStream out = fs.append(f);	out.write(bytes);	out.close();	Assert.fail();	} catch(IOException ioe) {	
this exception is expected 

public void testBestEffort() throws Exception {	final Configuration conf = new HdfsConfiguration();	ReplaceDatanodeOnFailure.write(Policy.ALWAYS, true, conf);	final MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf ).numDataNodes(1).build();	try {	final DistributedFileSystem fs = cluster.getFileSystem();	final Path f = new Path(DIR, "testIgnoreReplaceFailure");	final byte[] bytes = new byte[1000];	{	
write bytes to 

final byte[] bytes = new byte[1000];	{	final FSDataOutputStream out = fs.create(f, REPLICATION);	out.write(bytes);	out.close();	final FileStatus status = fs.getFileStatus(f);	Assert.assertEquals(REPLICATION, status.getReplication());	Assert.assertEquals(bytes.length, status.getLen());	}	{	
append another bytes to 

========================= hadoop sample_7667 =========================

long fileCount = 0;	RemoteIterator<LocatedFileStatus> iter = fs.listFiles(inputDir, true);	PathFilter filter = new Utils.OutputFileUtils.OutputFilesFilter();	while (iter.hasNext()) {	LocatedFileStatus lStatus = iter.next();	if (filter.accept(lStatus.getPath())) {	dataSize += lStatus.getLen();	++fileCount;	}	}	
total size of input data 

long fileCount = 0;	RemoteIterator<LocatedFileStatus> iter = fs.listFiles(inputDir, true);	PathFilter filter = new Utils.OutputFileUtils.OutputFilesFilter();	while (iter.hasNext()) {	LocatedFileStatus lStatus = iter.next();	if (filter.accept(lStatus.getPath())) {	dataSize += lStatus.getLen();	++fileCount;	}	}	
total number of input data files 

job.setMapperClass(GenDataMapper.class);	job.setNumReduceTasks(0);	job.setMapOutputKeyClass(NullWritable.class);	job.setMapOutputValueClass(BytesWritable.class);	job.setInputFormatClass(GenDataFormat.class);	job.setOutputFormatClass(RawBytesOutputFormat.class);	job.setJarByClass(GenerateData.class);	try {	FileInputFormat.addInputPath(job, new Path("ignored"));	} catch (IOException e) {	
error while adding input path 

========================= hadoop sample_6133 =========================

public void testLongLivedClient() throws IOException, InterruptedException {	FileChecksum checksum = writeUnencryptedAndThenRestartEncryptedCluster();	BlockTokenSecretManager btsm = cluster.getNamesystem().getBlockManager() .getBlockTokenSecretManager();	btsm.setKeyUpdateIntervalForTesting(2 * 1000);	btsm.setTokenLifetime(2 * 1000);	btsm.clearAllKeysForTesting();	assertEquals(PLAIN_TEXT, DFSTestUtil.readFile(fs, TEST_PATH));	assertEquals(checksum, fs.getFileChecksum(TEST_PATH));	
sleeping so that encryption keys expire 

public void testLongLivedClient() throws IOException, InterruptedException {	FileChecksum checksum = writeUnencryptedAndThenRestartEncryptedCluster();	BlockTokenSecretManager btsm = cluster.getNamesystem().getBlockManager() .getBlockTokenSecretManager();	btsm.setKeyUpdateIntervalForTesting(2 * 1000);	btsm.setTokenLifetime(2 * 1000);	btsm.clearAllKeysForTesting();	assertEquals(PLAIN_TEXT, DFSTestUtil.readFile(fs, TEST_PATH));	assertEquals(checksum, fs.getFileChecksum(TEST_PATH));	Thread.sleep(15 * 1000);	
done sleeping 

cluster = new MiniDFSCluster.Builder(conf) .numDataNodes(numDataNodes) .build();	fs = getFileSystem(conf);	DFSClient client = DFSClientAdapter.getDFSClient((DistributedFileSystem)fs);	DFSClient spyClient = Mockito.spy(client);	DFSClientAdapter.setDFSClient((DistributedFileSystem) fs, spyClient);	writeTestDataToFile(fs);	BlockTokenSecretManager btsm = cluster.getNamesystem().getBlockManager() .getBlockTokenSecretManager();	btsm.setKeyUpdateIntervalForTesting(2 * 1000);	btsm.setTokenLifetime(2 * 1000);	btsm.clearAllKeysForTesting();	
wait until encryption keys become invalid 

btsm.clearAllKeysForTesting();	final DataEncryptionKey encryptionKey = spyClient.getEncryptionKey();	List<DataNode> dataNodes = cluster.getDataNodes();	for (final DataNode dn: dataNodes) {	GenericTestUtils.waitFor( new Supplier<Boolean>() {	public Boolean get() {	return !dn.getBlockPoolTokenSecretManager(). get(encryptionKey.blockPoolId) .hasKey(encryptionKey.keyId);	}	}, 100, 30*1000 );	}	
the encryption key is invalid on all nodes now 

========================= hadoop sample_7688 =========================

public void run() {	try {	Thread.sleep(1000);	
Deleting 

public void run() {	try {	Thread.sleep(1000);	final FSDirectory fsdir = cluster.getNamesystem().dir;	INode fileINode = fsdir.getINode4Write(path.toString());	INodeMap inodeMap = (INodeMap) Whitebox.getInternalState(fsdir, "inodeMap");	fs.delete(path, false);	inodeMap.put(fileINode);	
Deleted 

public void run() {	try {	Thread.sleep(1000);	
renaming to 

public void run() {	try {	Thread.sleep(1000);	fs.rename(from, to);	
renamed to 

private void testDeleteAndCommitBlockSynchronizationRace(boolean hasSnapshot) throws Exception {	
start testing hassnapshot 

FSDataOutputStream stm = null;	Map<DataNode, DatanodeProtocolClientSideTranslatorPB> dnMap = new HashMap<DataNode, DatanodeProtocolClientSideTranslatorPB>();	try {	cluster = new MiniDFSCluster.Builder(conf) .numDataNodes(3) .build();	cluster.waitActive();	DistributedFileSystem fs = cluster.getFileSystem();	int stId = 0;	for(AbstractMap.SimpleImmutableEntry<String, Boolean> stest : testList) {	String testPath = stest.getKey();	Boolean mkSameDir = stest.getValue();	
test on mksamedir snapshot 

int stId = 0;	for(AbstractMap.SimpleImmutableEntry<String, Boolean> stest : testList) {	String testPath = stest.getKey();	Boolean mkSameDir = stest.getValue();	Path fPath = new Path(testPath);	Path grandestNonRootParent = fPath;	while (!grandestNonRootParent.getParent().equals(rootPath)) {	grandestNonRootParent = grandestNonRootParent.getParent();	}	stm = fs.create(fPath);	
test on created 

stm = fs.create(fPath);	AppendTestUtil.write(stm, 0, BLOCK_SIZE / 2);	stm.hflush();	if (hasSnapshot) {	SnapshotTestHelper.createSnapshot(fs, rootPath, "st" + String.valueOf(stId));	++stId;	}	NameNode nn = cluster.getNameNode();	ExtendedBlock blk = DFSTestUtil.getFirstBlock(fs, fPath);	DatanodeDescriptor expectedPrimary = DFSTestUtil.getExpectedPrimaryNode(nn, blk);	
expecting block recovery to be triggered on dn 

DatanodeDescriptor expectedPrimary = DFSTestUtil.getExpectedPrimaryNode(nn, blk);	DataNode primaryDN = cluster.getDataNode(expectedPrimary.getIpcPort());	DatanodeProtocolClientSideTranslatorPB nnSpy = dnMap.get(primaryDN);	if (nnSpy == null) {	nnSpy = InternalDataNodeTestUtils.spyOnBposToNN(primaryDN, nn);	dnMap.put(primaryDN, nnSpy);	}	DelayAnswer delayer = new DelayAnswer(LOG);	Mockito.doAnswer(delayer).when(nnSpy).commitBlockSynchronization( Mockito.eq(blk), Mockito.anyInt(), Mockito.anyLong(), Mockito.eq(true), Mockito.eq(false), (DatanodeID[]) Mockito.anyObject(), (String[]) Mockito.anyObject());	fs.recoverLease(fPath);	
waiting for commitblocksynchronization call from primary 

DataNode primaryDN = cluster.getDataNode(expectedPrimary.getIpcPort());	DatanodeProtocolClientSideTranslatorPB nnSpy = dnMap.get(primaryDN);	if (nnSpy == null) {	nnSpy = InternalDataNodeTestUtils.spyOnBposToNN(primaryDN, nn);	dnMap.put(primaryDN, nnSpy);	}	DelayAnswer delayer = new DelayAnswer(LOG);	Mockito.doAnswer(delayer).when(nnSpy).commitBlockSynchronization( Mockito.eq(blk), Mockito.anyInt(), Mockito.anyLong(), Mockito.eq(true), Mockito.eq(false), (DatanodeID[]) Mockito.anyObject(), (String[]) Mockito.anyObject());	fs.recoverLease(fPath);	delayer.waitForCall();	
deleting recursively 

if (nnSpy == null) {	nnSpy = InternalDataNodeTestUtils.spyOnBposToNN(primaryDN, nn);	dnMap.put(primaryDN, nnSpy);	}	DelayAnswer delayer = new DelayAnswer(LOG);	Mockito.doAnswer(delayer).when(nnSpy).commitBlockSynchronization( Mockito.eq(blk), Mockito.anyInt(), Mockito.anyLong(), Mockito.eq(true), Mockito.eq(false), (DatanodeID[]) Mockito.anyObject(), (String[]) Mockito.anyObject());	fs.recoverLease(fPath);	delayer.waitForCall();	fs.delete(grandestNonRootParent, true);	if (mkSameDir && !grandestNonRootParent.toString().equals(testPath)) {	
recreate dir testpath 

}	DelayAnswer delayer = new DelayAnswer(LOG);	Mockito.doAnswer(delayer).when(nnSpy).commitBlockSynchronization( Mockito.eq(blk), Mockito.anyInt(), Mockito.anyLong(), Mockito.eq(true), Mockito.eq(false), (DatanodeID[]) Mockito.anyObject(), (String[]) Mockito.anyObject());	fs.recoverLease(fPath);	delayer.waitForCall();	fs.delete(grandestNonRootParent, true);	if (mkSameDir && !grandestNonRootParent.toString().equals(testPath)) {	fs.mkdirs(grandestNonRootParent);	}	delayer.proceed();	
now wait for result 

fs.recoverLease(fPath);	delayer.waitForCall();	fs.delete(grandestNonRootParent, true);	if (mkSameDir && !grandestNonRootParent.toString().equals(testPath)) {	fs.mkdirs(grandestNonRootParent);	}	delayer.proceed();	delayer.waitForResult();	Throwable t = delayer.getThrown();	if (t != null) {	
result exception snapshot 

fs.delete(grandestNonRootParent, true);	if (mkSameDir && !grandestNonRootParent.toString().equals(testPath)) {	fs.mkdirs(grandestNonRootParent);	}	delayer.proceed();	delayer.waitForResult();	Throwable t = delayer.getThrown();	if (t != null) {	}	}	
now check we can restart 

if (mkSameDir && !grandestNonRootParent.toString().equals(testPath)) {	fs.mkdirs(grandestNonRootParent);	}	delayer.proceed();	delayer.waitForResult();	Throwable t = delayer.getThrown();	if (t != null) {	}	}	cluster.restartNameNodes();	
restart finished 

final Configuration config = new Configuration();	config.setBoolean(DFSConfigKeys.DFS_PERMISSIONS_ENABLED_KEY, false);	config.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLOCK_SIZE);	FSDataOutputStream stm = null;	try {	cluster = new MiniDFSCluster.Builder(config).numDataNodes(3).build();	cluster.waitActive();	final DistributedFileSystem fs = cluster.getFileSystem();	final Path testPath = new Path("/testfile");	stm = fs.create(testPath);	
test on 

});	while (!leases.isEmpty()) {	final Lease lease = leases.first();	final Lease spyLease = Mockito.spy(lease);	Mockito.doReturn(true).when(spyLease).expiredHardLimit();	spyLeases.add(spyLease);	leases.remove(lease);	}	Whitebox.setInternalState(lm, "sortedLeases", spyLeases);	Thread.sleep(2 * conf.getLong(DFS_NAMENODE_LEASE_RECHECK_INTERVAL_MS_KEY, DFS_NAMENODE_LEASE_RECHECK_INTERVAL_MS_DEFAULT));	
now check we can restart 

while (!leases.isEmpty()) {	final Lease lease = leases.first();	final Lease spyLease = Mockito.spy(lease);	Mockito.doReturn(true).when(spyLease).expiredHardLimit();	spyLeases.add(spyLease);	leases.remove(lease);	}	Whitebox.setInternalState(lm, "sortedLeases", spyLeases);	Thread.sleep(2 * conf.getLong(DFS_NAMENODE_LEASE_RECHECK_INTERVAL_MS_KEY, DFS_NAMENODE_LEASE_RECHECK_INTERVAL_MS_DEFAULT));	cluster.restartNameNodes();	
restart finished 

========================= hadoop sample_7372 =========================

public void report(long currTime, String userName, String cmd) {	
a metric is reported cmd user 

========================= hadoop sample_7958 =========================

if (displayScreen == DisplayScreen.SORT) {	handleSortScreenKeyPress(in);	} else if (displayScreen == DisplayScreen.TOP) {	handleTopScreenKeyPress(in);	} else if (displayScreen == DisplayScreen.FIELDS) {	handleFieldsScreenKeyPress(in);	} else {	handleHelpScreenKeyPress();	}	} catch (Exception e) {	
caught exception 

public int run(String[] args) throws Exception {	try {	parseOptions(args);	if (cliParser.hasOption("help")) {	printUsage();	return 0;	}	} catch (Exception e) {	
unable to parse options 

setTerminalWidth();	}	if (cliParser.hasOption("rows")) {	terminalHeight = Integer.parseInt(cliParser.getOptionValue("rows"));	} else {	setTerminalHeight();	}	if (cliParser.hasOption("delay")) {	int delay = Integer.parseInt(cliParser.getOptionValue("delay"));	if (delay < 1) {	
delay set too low using default 

private void setTerminalWidth() throws IOException, InterruptedException {	if (terminalWidth != -1) {	return;	}	String[] command = { "tput", "cols" };	String op = getCommandOutput(command).trim();	try {	terminalWidth = Integer.parseInt(op);	} catch (NumberFormatException ne) {	
couldn t determine terminal width setting to 

private void setTerminalHeight() throws IOException, InterruptedException {	if (terminalHeight != -1) {	return;	}	String[] command = { "tput", "lines" };	String op = getCommandOutput(command).trim();	try {	terminalHeight = Integer.parseInt(op);	} catch (NumberFormatException ne) {	
couldn t determine terminal height setting to 

protected NodesInformation getNodesInfo() {	NodesInformation nodeInfo = new NodesInformation();	YarnClusterMetrics yarnClusterMetrics;	try {	yarnClusterMetrics = client.getYarnClusterMetrics();	} catch (IOException ie) {	
unable to fetch cluster metrics 

protected NodesInformation getNodesInfo() {	NodesInformation nodeInfo = new NodesInformation();	YarnClusterMetrics yarnClusterMetrics;	try {	yarnClusterMetrics = client.getYarnClusterMetrics();	} catch (IOException ie) {	return nodeInfo;	} catch (YarnException ye) {	
unable to fetch cluster metrics 

protected QueueMetrics getQueueMetrics() {	QueueMetrics queueMetrics = new QueueMetrics();	List<QueueInfo> queuesInfo;	if (queues.isEmpty()) {	try {	queuesInfo = client.getRootQueueInfos();	} catch (Exception ie) {	
unable to get queue information 

} catch (Exception ie) {	return queueMetrics;	}	} else {	queuesInfo = new ArrayList<>();	for (String queueName : queues) {	try {	QueueInfo qInfo = client.getQueueInfo(queueName);	queuesInfo.add(qInfo);	} catch (Exception ie) {	
unable to get queue information 

protected void showTopScreen() {	List<ApplicationInformation> appsInfo = new ArrayList<>();	List<ApplicationReport> apps;	try {	apps = fetchAppReports();	} catch (Exception e) {	
unable to get application information 

========================= hadoop sample_2609 =========================

public void testModifyAndRead() throws Exception {	
test 

========================= hadoop sample_512 =========================

nonDatanodeReader = true;	List<String> hosts = new ArrayList<>(1);	hosts.add(targethost);	List<String> resolvedHosts = dnsToSwitchMapping.resolve(hosts);	if (resolvedHosts != null && !resolvedHosts.isEmpty()) {	String rName = resolvedHosts.get(0);	if (rName != null) {	client = new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR + targethost);	}	} else {	
node resolution failed please make sure that rack awareness scripts are functional 

public DatanodeDescriptor getDatanode(DatanodeID nodeID) throws UnregisteredNodeException {	final DatanodeDescriptor node = getDatanode(nodeID.getDatanodeUuid());	if (node == null) return null;	if (!node.getXferAddr().equals(nodeID.getXferAddr())) {	final UnregisteredNodeException e = new UnregisteredNodeException( nodeID, node);	
block namesystem getdatanode 

private void removeDatanode(DatanodeDescriptor nodeInfo, boolean removeBlocksFromBlocksMap) {	assert namesystem.hasWriteLock();	heartbeatManager.removeDatanode(nodeInfo);	if (removeBlocksFromBlocksMap) {	blockManager.removeBlocksAssociatedTo(nodeInfo);	}	networktopology.remove(nodeInfo);	decrementVersionCount(nodeInfo.getSoftwareVersion());	blockManager.getBlockReportLeaseManager().unregister(nodeInfo);	if (LOG.isDebugEnabled()) {	
remove datanode 

public void removeDatanode(final DatanodeID node) throws UnregisteredNodeException {	namesystem.writeLock();	try {	final DatanodeDescriptor descriptor = getDatanode(node);	if (descriptor != null) {	removeDatanode(descriptor, true);	} else {	
block removedatanode does not exist 

private void wipeDatanode(final DatanodeID node) {	final String key = node.getDatanodeUuid();	synchronized (this) {	host2DatanodeMap.remove(datanodeMap.remove(key));	}	if (LOG.isDebugEnabled()) {	
wipedatanode storage is removed from datanodemap 

private String resolveNetworkLocationWithFallBackToDefaultLocation ( DatanodeID node) {	String networkLocation;	try {	networkLocation = resolveNetworkLocation(node);	} catch (UnresolvedTopologyException e) {	
unresolved topology mapping using for host 

private String resolveNetworkLocation (DatanodeID node) throws UnresolvedTopologyException {	List<String> names = new ArrayList<>(1);	if (dnsToSwitchMapping instanceof CachedDNSToSwitchMapping) {	names.add(node.getIpAddr());	} else {	names.add(node.getHostName());	}	List<String> rName = resolveNetworkLocation(names);	String networkLocation;	if (rName == null) {	
the resolve call returned null 

private List<String> getNetworkDependenciesWithDefault(DatanodeInfo node) {	List<String> dependencies;	try {	dependencies = getNetworkDependencies(node);	} catch (UnresolvedTopologyException e) {	
unresolved dependency mapping for host continuing with an empty dependency list 

private List<String> getNetworkDependencies(DatanodeInfo node) throws UnresolvedTopologyException {	List<String> dependencies = Collections.emptyList();	if (dnsToSwitchMapping instanceof DNSToSwitchMappingWithDependency) {	dependencies = ((DNSToSwitchMappingWithDependency)dnsToSwitchMapping).getDependency( node.getHostName());	if(dependencies == null) {	
the dependency call returned null for host 

public void registerDatanode(DatanodeRegistration nodeReg) throws DisallowedDatanodeException, UnresolvedTopologyException {	InetAddress dnAddress = Server.getRemoteIp();	if (dnAddress != null) {	String hostname = dnAddress.getHostName();	String ip = dnAddress.getHostAddress();	if (checkIpHostnameInRegistration && !isNameResolved(dnAddress)) {	final String message = "hostname cannot be resolved (ip=" + ip + ", hostname=" + hostname + ")";	
unresolved datanode registration 

throw new DisallowedDatanodeException(nodeReg, message);	}	nodeReg.setIpAddr(ip);	nodeReg.setPeerHostName(hostname);	}	try {	nodeReg.setExportedKeys(blockManager.getBlockKeys());	if (!hostConfigManager.isIncluded(nodeReg)) {	throw new DisallowedDatanodeException(nodeReg);	}	
block registerdatanode from storage 

nodeReg.setPeerHostName(hostname);	}	try {	nodeReg.setExportedKeys(blockManager.getBlockKeys());	if (!hostConfigManager.isIncluded(nodeReg)) {	throw new DisallowedDatanodeException(nodeReg);	}	DatanodeDescriptor nodeS = getDatanode(nodeReg.getDatanodeUuid());	DatanodeDescriptor nodeN = host2DatanodeMap.getDatanodeByXferAddr( nodeReg.getIpAddr(), nodeReg.getXferPort());	if (nodeN != null && nodeN != nodeS) {	
block registerdatanode 

DatanodeDescriptor nodeS = getDatanode(nodeReg.getDatanodeUuid());	DatanodeDescriptor nodeN = host2DatanodeMap.getDatanodeByXferAddr( nodeReg.getIpAddr(), nodeReg.getXferPort());	if (nodeN != null && nodeN != nodeS) {	removeDatanode(nodeN);	wipeDatanode(nodeN);	nodeN = null;	}	if (nodeS != null) {	if (nodeN == nodeS) {	if(NameNode.stateChangeLog.isDebugEnabled()) {	
block registerdatanode node restarted 

if (nodeN != null && nodeN != nodeS) {	removeDatanode(nodeN);	wipeDatanode(nodeN);	nodeN = null;	}	if (nodeS != null) {	if (nodeN == nodeS) {	if(NameNode.stateChangeLog.isDebugEnabled()) {	}	} else {	
VERSION block registerdatanode is replaced by with the same storageid 

hostStr = hostLine.substring(0, idx);	port = Integer.parseInt(hostLine.substring(idx+1));	}	if (InetAddresses.isInetAddress(hostStr)) {	dnId = new DatanodeID(hostStr, "", "", port, DFSConfigKeys.DFS_DATANODE_HTTP_DEFAULT_PORT, DFSConfigKeys.DFS_DATANODE_HTTPS_DEFAULT_PORT, DFSConfigKeys.DFS_DATANODE_IPC_DEFAULT_PORT);	} else {	String ipAddr = "";	try {	ipAddr = InetAddress.getByName(hostStr).getHostAddress();	} catch (UnknownHostException e) {	
invalid hostname in hosts file 

if (!storage.getDatanodeDescriptor().isStale(staleInterval)) {	recoveryLocations.add(storage);	}	}	boolean truncateRecovery = uc.getTruncateBlock() != null;	boolean copyOnTruncateRecovery = truncateRecovery && uc.getTruncateBlock().getBlockId() != b.getBlockId();	ExtendedBlock primaryBlock = (copyOnTruncateRecovery) ? new ExtendedBlock(blockPoolId, uc.getTruncateBlock()) : new ExtendedBlock(blockPoolId, b);	DatanodeInfo[] recoveryInfos;	if (recoveryLocations.size() > 1) {	if (recoveryLocations.size() != storages.length) {	
skipped stale nodes for recovery 

public void markAllDatanodesStale() {	
marking all datandoes as stale 

========================= hadoop sample_8341 =========================

private void mayBeSkip() throws IOException {	hasNext = skipIt.hasNext();	if(!hasNext) {	
further groups got skipped 

runTaskCleanupTask(umbilical, reporter);	return;	}	codec = initCodec();	RawKeyValueIterator rIter = null;	ShuffleConsumerPlugin shuffleConsumerPlugin = null;	Class combinerClass = conf.getCombinerClass();	CombineOutputCollector combineCollector = (null != combinerClass) ? new CombineOutputCollector(reduceCombineOutputCounter, reporter, conf) : null;	Class<? extends ShuffleConsumerPlugin> clazz = job.getClass(MRConfig.SHUFFLE_CONSUMER_PLUGIN, Shuffle.class, ShuffleConsumerPlugin.class);	shuffleConsumerPlugin = ReflectionUtils.newInstance(clazz, job);	
using shuffleconsumerplugin 

========================= hadoop sample_4698 =========================

protected void reportFatalError(TaskAttemptID id, Throwable throwable, String logMsg) {	LOG.fatal(logMsg);	if (ShutdownHookManager.get().isShutdownInProgress()) {	return;	}	Throwable tCause = throwable.getCause();	String cause = tCause == null ? StringUtils.stringifyException(throwable) : StringUtils.stringifyException(tCause);	try {	umbilical.fatalError(id, cause);	} catch (IOException ioe) {	
failed to contact the tasktracker 

public static String normalizeStatus(String status, Configuration conf) {	int progressStatusLength = conf.getInt( MRConfig.PROGRESS_STATUS_LEN_LIMIT_KEY, MRConfig.PROGRESS_STATUS_LEN_LIMIT_DEFAULT);	if (status.length() > progressStatusLength) {	
task status status truncated to max limit characters 

protected void checkTaskLimits() throws TaskLimitException {	long limit = conf.getLong(MRJobConfig.TASK_LOCAL_WRITE_LIMIT_BYTES, MRJobConfig.DEFAULT_TASK_LOCAL_WRITE_LIMIT_BYTES);	if (limit >= 0) {	Counters.Counter localWritesCounter = null;	try {	LocalFileSystem localFS = FileSystem.getLocal(conf);	localWritesCounter = counters.findCounter(localFS.getScheme(), FileSystemCounter.BYTES_WRITTEN);	} catch (IOException e) {	
could not get localfilesystem bytes written counter 

updateCounters();	checkTaskLimits();	taskStatus.statusUpdate(taskProgress.get(), taskProgress.toString(), counters);	taskFound = umbilical.statusUpdate(taskId, taskStatus);	taskStatus.clearStatus();	}	else {	taskFound = umbilical.ping(taskId);	}	if (!taskFound) {	
parent died exiting 

System.exit(66);	}	sendProgress = resetProgressFlag();	remainingRetries = MAX_RETRIES;	} catch (TaskLimitException e) {	String errMsg = "Task exceeded the limits: " + StringUtils.stringifyException(e);	LOG.fatal(errMsg);	try {	umbilical.fatalError(taskId, errMsg);	} catch (IOException ioe) {	
failed to update failure diagnosis 

}	sendProgress = resetProgressFlag();	remainingRetries = MAX_RETRIES;	} catch (TaskLimitException e) {	String errMsg = "Task exceeded the limits: " + StringUtils.stringifyException(e);	LOG.fatal(errMsg);	try {	umbilical.fatalError(taskId, errMsg);	} catch (IOException ioe) {	}	
killing 

} catch (TaskLimitException e) {	String errMsg = "Task exceeded the limits: " + StringUtils.stringifyException(e);	LOG.fatal(errMsg);	try {	umbilical.fatalError(taskId, errMsg);	} catch (IOException ioe) {	}	resetDoneFlag();	ExitUtil.terminate(69);	} catch (Throwable t) {	
communication exception 

try {	umbilical.fatalError(taskId, errMsg);	} catch (IOException ioe) {	}	resetDoneFlag();	ExitUtil.terminate(69);	} catch (Throwable t) {	remainingRetries -=1;	if (remainingRetries == 0) {	ReflectionUtils.logThreadInfo(LOG, "Communication exception", 0);	
last retry killing 

protected void reportNextRecordRange(final TaskUmbilicalProtocol umbilical, long nextRecIndex) throws IOException{	long len = nextRecIndex - currentRecStartIndex +1;	SortedRanges.Range range = new SortedRanges.Range(currentRecStartIndex, len);	taskStatus.setNextRecordRange(range);	if (LOG.isDebugEnabled()) {	
sending reportnextrecordrange 

public void done(TaskUmbilicalProtocol umbilical, TaskReporter reporter ) throws IOException, InterruptedException {	
task is done and is in the process of committing 

boolean commitRequired = isCommitRequired();	if (commitRequired) {	int retries = MAX_RETRIES;	setState(TaskStatus.State.COMMIT_PENDING);	while (true) {	try {	umbilical.commitPending(taskId, taskStatus);	break;	} catch (InterruptedException ie) {	} catch (IOException ie) {	
failure sending commit pending 

public void statusUpdate(TaskUmbilicalProtocol umbilical) throws IOException {	int retries = MAX_RETRIES;	while (true) {	try {	if (!umbilical.statusUpdate(getTaskID(), taskStatus)) {	
parent died exiting 

while (true) {	try {	if (!umbilical.statusUpdate(getTaskID(), taskStatus)) {	System.exit(66);	}	taskStatus.clearStatus();	return;	} catch (InterruptedException ie) {	Thread.currentThread().interrupt();	} catch (IOException ie) {	
failure sending status update 

private long calculateOutputSize() throws IOException {	if (!isMapOrReduce()) {	return -1;	}	if (isMapTask() && conf.getNumReduceTasks() > 0) {	try {	Path mapOutput =  mapOutputFile.getOutputFile();	FileSystem localFS = FileSystem.getLocal(conf);	return localFS.getFileStatus(mapOutput).getLen();	} catch (IOException e) {	
could not find output size 

private void sendDone(TaskUmbilicalProtocol umbilical) throws IOException {	int retries = MAX_RETRIES;	while (true) {	try {	umbilical.done(getTaskID());	
task done 

private void sendDone(TaskUmbilicalProtocol umbilical) throws IOException {	int retries = MAX_RETRIES;	while (true) {	try {	umbilical.done(getTaskID());	return;	} catch (IOException ie) {	
failure signalling completion 

try {	while (!umbilical.canCommit(taskId)) {	try {	Thread.sleep(1000);	} catch(InterruptedException ie) {	}	reporter.setProgressFlag();	}	break;	} catch (IOException ie) {	
failure asking whether task can commit 

}	break;	} catch (IOException ie) {	if (--retries == 0) {	discardOutput(taskContext);	System.exit(68);	}	}	}	try {	
task is allowed to commit now 

if (--retries == 0) {	discardOutput(taskContext);	System.exit(68);	}	}	}	try {	committer.commitTask(taskContext);	return;	} catch (IOException iee) {	
failure committing 

protected void runJobCleanupTask(TaskUmbilicalProtocol umbilical, TaskReporter reporter ) throws IOException, InterruptedException {	setPhase(TaskStatus.Phase.CLEANUP);	getProgress().setStatus("cleanup");	statusUpdate(umbilical);	
cleaning up job 

protected void runJobCleanupTask(TaskUmbilicalProtocol umbilical, TaskReporter reporter ) throws IOException, InterruptedException {	setPhase(TaskStatus.Phase.CLEANUP);	getProgress().setStatus("cleanup");	statusUpdate(umbilical);	if (jobRunStateForCleanup == JobStatus.State.FAILED || jobRunStateForCleanup == JobStatus.State.KILLED) {	
aborting job with runstate 

getProgress().setStatus("cleanup");	statusUpdate(umbilical);	if (jobRunStateForCleanup == JobStatus.State.FAILED || jobRunStateForCleanup == JobStatus.State.KILLED) {	if (conf.getUseNewMapper()) {	committer.abortJob(jobContext, jobRunStateForCleanup);	} else {	org.apache.hadoop.mapred.OutputCommitter oldCommitter = (org.apache.hadoop.mapred.OutputCommitter)committer;	oldCommitter.abortJob(jobContext, jobRunStateForCleanup);	}	} else if (jobRunStateForCleanup == JobStatus.State.SUCCEEDED){	
committing job 

========================= hadoop sample_4732 =========================

private static void initProxySupport(Configuration conf, ClientConfiguration awsConf) throws IllegalArgumentException {	String proxyHost = conf.getTrimmed(PROXY_HOST, "");	int proxyPort = conf.getInt(PROXY_PORT, -1);	if (!proxyHost.isEmpty()) {	awsConf.setProxyHost(proxyHost);	if (proxyPort >= 0) {	awsConf.setProxyPort(proxyPort);	} else {	if (conf.getBoolean(SECURE_CONNECTIONS, DEFAULT_SECURE_CONNECTIONS)) {	
proxy host set without port using https default 

String proxyHost = conf.getTrimmed(PROXY_HOST, "");	int proxyPort = conf.getInt(PROXY_PORT, -1);	if (!proxyHost.isEmpty()) {	awsConf.setProxyHost(proxyHost);	if (proxyPort >= 0) {	awsConf.setProxyPort(proxyPort);	} else {	if (conf.getBoolean(SECURE_CONNECTIONS, DEFAULT_SECURE_CONNECTIONS)) {	awsConf.setProxyPort(443);	} else {	
proxy host set without port using http default 

if ((proxyUsername == null) != (proxyPassword == null)) {	String msg = "Proxy error: " + PROXY_USERNAME + " or " + PROXY_PASSWORD + " set without the other.";	LOG.error(msg);	throw new IllegalArgumentException(msg);	}	awsConf.setProxyUsername(proxyUsername);	awsConf.setProxyPassword(proxyPassword);	awsConf.setProxyDomain(conf.getTrimmed(PROXY_DOMAIN));	awsConf.setProxyWorkstation(conf.getTrimmed(PROXY_WORKSTATION));	if (LOG.isDebugEnabled()) {	
using proxy server as user with password on domain as workstation 

private static void initUserAgent(Configuration conf, ClientConfiguration awsConf) {	String userAgent = "Hadoop " + VersionInfo.getVersion();	String userAgentPrefix = conf.getTrimmed(USER_AGENT_PREFIX, "");	if (!userAgentPrefix.isEmpty()) {	userAgent = userAgentPrefix + ", " + userAgent;	}	
using user agent 

private static void enablePathStyleAccessIfRequired(AmazonS3 s3, Configuration conf) {	final boolean pathStyleAccess = conf.getBoolean(PATH_STYLE_ACCESS, false);	if (pathStyleAccess) {	
enabling path style access 

========================= hadoop sample_6021 =========================

static MetricsConfig loadFirst(String prefix, String... fileNames) {	for (String fname : fileNames) {	try {	Configuration cf = new PropertiesConfiguration(fname) .interpolatedConfiguration();	
loaded properties from 

LOG.debug(mc.toString());	return mc;	}	catch (ConfigurationException e) {	if (e.getMessage().startsWith("Cannot locate configuration")) {	continue;	}	throw new MetricsConfigException(e);	}	}	
cannot locate configuration tried 

public Object getProperty(String key) {	Object value = super.getProperty(key);	if (value == null) {	if (LOG.isDebugEnabled()) {	
poking parent for key 

public Object getProperty(String key) {	Object value = super.getProperty(key);	if (value == null) {	if (LOG.isDebugEnabled()) {	}	return getParent().getProperty(key.startsWith(PREFIX_DEFAULT) ? key : PREFIX_DEFAULT + key);	}	if (LOG.isDebugEnabled()) {	
returning for key 

========================= hadoop sample_3389 =========================

if (aclExt == null) {	aclExt = loadDomainFromTimelineStore(entity.getDomainId());	}	if (aclExt == null) {	throw new YarnException("Domain information of the timeline entity " + new EntityIdentifier(entity.getEntityId(), entity.getEntityType()) + " doesn't exist.");	}	String owner = aclExt.owner;	AccessControlList domainACL = aclExt.acls.get(applicationAccessType);	if (domainACL == null) {	if (LOG.isDebugEnabled()) {	
acl not found for access type for domain owned by using default 

========================= hadoop sample_2062 =========================

public void transferSuccessful() {	if (manageOsCache && getCount() > 0) {	try {	NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(identifier, fd, getPosition(), getCount(), POSIX_FADV_DONTNEED);	} catch (Throwable t) {	
failed to manage os cache for 

========================= hadoop sample_5318 =========================

protected RequestInterceptorChainWrapper getInterceptorChain( final HttpServletRequest hsr) {	String user = "";	if (hsr != null) {	user = hsr.getRemoteUser();	}	try {	if (user == null || user.equals("")) {	user = UserGroupInformation.getCurrentUser().getUserName();	}	} catch (IOException e) {	
cannot get user 

private void initializePipeline(String user) {	RequestInterceptorChainWrapper chainWrapper = null;	synchronized (this.userPipelineMap) {	if (this.userPipelineMap.containsKey(user)) {	
request to start an already existing user was received so ignoring 

private void initializePipeline(String user) {	RequestInterceptorChainWrapper chainWrapper = null;	synchronized (this.userPipelineMap) {	if (this.userPipelineMap.containsKey(user)) {	return;	}	chainWrapper = new RequestInterceptorChainWrapper();	this.userPipelineMap.put(user, chainWrapper);	}	
initializing request processing pipeline for the user 

========================= hadoop sample_1971 =========================

if (rmStateStore != null) {	rmStateStore.storeNewReservation( ReservationSystemUtil.buildStateProto(inMemReservation), getQueueName(), inMemReservation.getReservationId().toString());	}	}	ReservationInterval searchInterval = new ReservationInterval( inMemReservation.getStartTime(), inMemReservation.getEndTime());	Set<InMemoryReservationAllocation> reservations = currentReservations.get(searchInterval);	if (reservations == null) {	reservations = new HashSet<InMemoryReservationAllocation>();	}	if (!reservations.add(inMemReservation)) {	
unable to add reservation to plan 

Set<InMemoryReservationAllocation> reservations = currentReservations.get(searchInterval);	if (reservations == null) {	reservations = new HashSet<InMemoryReservationAllocation>();	}	if (!reservations.add(inMemReservation)) {	return false;	}	currentReservations.put(searchInterval, reservations);	reservationTable.put(inMemReservation.getReservationId(), inMemReservation);	incrementAllocation(inMemReservation);	
successfully added reservation to plan 

try {	ReservationId resId = reservation.getReservationId();	ReservationAllocation currReservation = getReservationById(resId);	if (currReservation == null) {	String errMsg = "The specified Reservation with ID " + resId + " does not exist in the plan";	LOG.error(errMsg);	throw new IllegalArgumentException(errMsg);	}	policy.validate(this, reservation);	if (!removeReservation(currReservation)) {	
unable to replace reservation from plan 

LOG.error(errMsg);	throw new IllegalArgumentException(errMsg);	}	policy.validate(this, reservation);	if (!removeReservation(currReservation)) {	return result;	}	try {	result = addReservation(reservation, false);	} catch (PlanningException e) {	
unable to update reservation from plan due to 

}	policy.validate(this, reservation);	if (!removeReservation(currReservation)) {	return result;	}	try {	result = addReservation(reservation, false);	} catch (PlanningException e) {	}	if (result) {	
successfully updated reservation in plan 

return result;	}	try {	result = addReservation(reservation, false);	} catch (PlanningException e) {	}	if (result) {	return result;	} else {	addReservation(currReservation, false);	
rollbacked update reservation from plan 

private boolean removeReservation(ReservationAllocation reservation) {	assert (readWriteLock.isWriteLockedByCurrentThread());	ReservationInterval searchInterval = new ReservationInterval( reservation.getStartTime(), reservation.getEndTime());	Set<InMemoryReservationAllocation> reservations = currentReservations.get(searchInterval);	if (reservations != null) {	if (rmStateStore != null) {	rmStateStore.removeReservation(getQueueName(), reservation.getReservationId().toString());	}	if (!reservations.remove(reservation)) {	
unable to remove reservation from plan 

if (reservations.isEmpty()) {	currentReservations.remove(searchInterval);	}	} else {	String errMsg = "The specified Reservation with ID " + reservation.getReservationId() + " does not exist in the plan";	LOG.error(errMsg);	throw new IllegalArgumentException(errMsg);	}	reservationTable.remove(reservation.getReservationId());	decrementAllocation(reservation);	
sucessfully deleted reservation in plan 

public void archiveCompletedReservations(long tick) {	
running archival at time 

if (userResAlloc != null && userPeriodicResAlloc != null) {	return RLESparseResourceAllocation.merge(resCalc, totalCapacity, userResAlloc, userPeriodicResAlloc, RLEOperator.add, start, end);	}	if (userResAlloc != null) {	return userResAlloc.getRangeOverlapping(start, end);	}	if (userPeriodicResAlloc != null) {	return userPeriodicResAlloc.getRangeOverlapping(start, end);	}	} catch (PlanningException e) {	
exception while trying to merge periodic and non periodic user allocations 

========================= hadoop sample_1072 =========================

next.getQueue().addApp(appSched, true);	noLongerPendingApps.add(appSched);	if (noLongerPendingApps.size() >= maxRunnableApps) {	break;	}	}	prev = next;	}	for (FSAppAttempt appSched : noLongerPendingApps) {	if (!appSched.getQueue().removeNonRunnableApp(appSched)) {	
can t make app runnable that does not already exist in queue as non runnable this should never happen 

if (noLongerPendingApps.size() >= maxRunnableApps) {	break;	}	}	prev = next;	}	for (FSAppAttempt appSched : noLongerPendingApps) {	if (!appSched.getQueue().removeNonRunnableApp(appSched)) {	}	if (!usersNonRunnableApps.remove(appSched.getUser(), appSched)) {	
waiting app expected to be in usersnonrunnableapps but was not this should never happen 

========================= hadoop sample_953 =========================

response.setHeader(ACCESS_CONTROL_ALLOW_ORIGIN, "*");	JsonFactory jsonFactory = new JsonFactory();	jg = jsonFactory.createJsonGenerator(writer);	jg.disable(JsonGenerator.Feature.AUTO_CLOSE_TARGET);	jg.useDefaultPrettyPrinter();	jg.writeStartObject();	if (mBeanServer == null) {	jg.writeStringField("result", "ERROR");	jg.writeStringField("message", "No MBeanServer could be found");	jg.close();	
no mbeanserver could be found 

listBeans(jg, new ObjectName(qry), null, response);	} finally {	if (jg != null) {	jg.close();	}	if (writer != null) {	writer.close();	}	}	} catch ( IOException e ) {	
caught an exception while processing jmx request 

if (jg != null) {	jg.close();	}	if (writer != null) {	writer.close();	}	}	} catch ( IOException e ) {	response.setStatus(HttpServletResponse.SC_INTERNAL_SERVER_ERROR);	} catch ( MalformedObjectNameException e ) {	
caught an exception while processing jmx request 

private void listBeans(JsonGenerator jg, ObjectName qry, String attribute, HttpServletResponse response) throws IOException {	
listing beans for 

try {	if ("org.apache.commons.modeler.BaseModelMBean".equals(code)) {	prs = "modelerType";	code = (String) mBeanServer.getAttribute(oname, prs);	}	if (attribute!=null) {	prs = attribute;	attributeinfo = mBeanServer.getAttribute(oname, prs);	}	} catch (AttributeNotFoundException e) {	
getting attribute of threw an exception 

if ("org.apache.commons.modeler.BaseModelMBean".equals(code)) {	prs = "modelerType";	code = (String) mBeanServer.getAttribute(oname, prs);	}	if (attribute!=null) {	prs = attribute;	attributeinfo = mBeanServer.getAttribute(oname, prs);	}	} catch (AttributeNotFoundException e) {	} catch (MBeanException e) {	
getting attribute of threw an exception 

prs = "modelerType";	code = (String) mBeanServer.getAttribute(oname, prs);	}	if (attribute!=null) {	prs = attribute;	attributeinfo = mBeanServer.getAttribute(oname, prs);	}	} catch (AttributeNotFoundException e) {	} catch (MBeanException e) {	} catch (RuntimeException e) {	
getting attribute of threw an exception 

code = (String) mBeanServer.getAttribute(oname, prs);	}	if (attribute!=null) {	prs = attribute;	attributeinfo = mBeanServer.getAttribute(oname, prs);	}	} catch (AttributeNotFoundException e) {	} catch (MBeanException e) {	} catch (RuntimeException e) {	} catch ( ReflectionException e ) {	
getting attribute of threw an exception 

attributeinfo = mBeanServer.getAttribute(oname, prs);	}	} catch (AttributeNotFoundException e) {	} catch (MBeanException e) {	} catch (RuntimeException e) {	} catch ( ReflectionException e ) {	}	} catch (InstanceNotFoundException e) {	continue;	} catch ( IntrospectionException e ) {	
problem while trying to process jmx query with mbean 

} catch (AttributeNotFoundException e) {	} catch (MBeanException e) {	} catch (RuntimeException e) {	} catch ( ReflectionException e ) {	}	} catch (InstanceNotFoundException e) {	continue;	} catch ( IntrospectionException e ) {	continue;	} catch ( ReflectionException e ) {	
problem while trying to process jmx query with mbean 

return;	}	if (attName.indexOf("=") >= 0 || attName.indexOf(":") >= 0 || attName.indexOf(" ") >= 0) {	return;	}	Object value = null;	try {	value = mBeanServer.getAttribute(oname, attName);	} catch (RuntimeMBeanException e) {	if (e.getCause() instanceof UnsupportedOperationException) {	
getting attribute of threw an exception 

}	if (attName.indexOf("=") >= 0 || attName.indexOf(":") >= 0 || attName.indexOf(" ") >= 0) {	return;	}	Object value = null;	try {	value = mBeanServer.getAttribute(oname, attName);	} catch (RuntimeMBeanException e) {	if (e.getCause() instanceof UnsupportedOperationException) {	} else {	
getting attribute of threw an exception 

}	Object value = null;	try {	value = mBeanServer.getAttribute(oname, attName);	} catch (RuntimeMBeanException e) {	if (e.getCause() instanceof UnsupportedOperationException) {	} else {	}	return;	} catch (RuntimeErrorException e) {	
getting attribute of threw an exception 

} catch (RuntimeMBeanException e) {	if (e.getCause() instanceof UnsupportedOperationException) {	} else {	}	return;	} catch (RuntimeErrorException e) {	return;	} catch (AttributeNotFoundException e) {	return;	} catch (MBeanException e) {	
getting attribute of threw an exception 

} else {	}	return;	} catch (RuntimeErrorException e) {	return;	} catch (AttributeNotFoundException e) {	return;	} catch (MBeanException e) {	return;	} catch (RuntimeException e) {	
getting attribute of threw an exception 

return;	} catch (RuntimeErrorException e) {	return;	} catch (AttributeNotFoundException e) {	return;	} catch (MBeanException e) {	return;	} catch (RuntimeException e) {	return;	} catch (ReflectionException e) {	
getting attribute of threw an exception 

========================= hadoop sample_3809 =========================

private void runBalancer(Configuration conf, long totalUsedSpace, long totalCapacity) throws Exception {	waitForHeartBeat(totalUsedSpace, totalCapacity);	Collection<URI> namenodes = DFSUtil.getInternalNsRpcUris(conf);	final int r = Balancer.run(namenodes, BalancerParameters.DEFAULT, conf);	assertEquals(ExitStatus.SUCCESS.getExitCode(), r);	waitForHeartBeat(totalUsedSpace, totalCapacity);	
rebalancing with default factor 

private void runBalancerCanFinish(Configuration conf, long totalUsedSpace, long totalCapacity) throws Exception {	waitForHeartBeat(totalUsedSpace, totalCapacity);	Collection<URI> namenodes = DFSUtil.getInternalNsRpcUris(conf);	final int r = Balancer.run(namenodes, BalancerParameters.DEFAULT, conf);	Assert.assertTrue(r == ExitStatus.SUCCESS.getExitCode() || (r == ExitStatus.NO_MOVE_PROGRESS.getExitCode()));	waitForHeartBeat(totalUsedSpace, totalCapacity);	
rebalancing with default factor 

========================= hadoop sample_7519 =========================

} else {	synchronized(info) {	while (isUnderConstruction(info)) {	try {	info.wait();	} catch (InterruptedException e) {	throw new IOException("Interrupted waiting for construction", e);	}	}	}	
indexcache hit mapid found 

if ((info = cache.putIfAbsent(mapId, newInd)) != null) {	synchronized(info) {	while (isUnderConstruction(info)) {	try {	info.wait();	} catch (InterruptedException e) {	throw new IOException("Interrupted waiting for construction", e);	}	}	}	
indexcache hit mapid found 

while (isUnderConstruction(info)) {	try {	info.wait();	} catch (InterruptedException e) {	throw new IOException("Interrupted waiting for construction", e);	}	}	}	return info;	}	
indexcache miss mapid not found 

public void removeMap(String mapId) {	IndexInformation info = cache.get(mapId);	if (info == null || isUnderConstruction(info)) {	return;	}	info = cache.remove(mapId);	if (info != null) {	totalMemoryUsed.addAndGet(-info.getSize());	if (!queue.remove(mapId)) {	
map id not found in queue 

IndexInformation info = cache.get(mapId);	if (info == null || isUnderConstruction(info)) {	return;	}	info = cache.remove(mapId);	if (info != null) {	totalMemoryUsed.addAndGet(-info.getSize());	if (!queue.remove(mapId)) {	}	} else {	
map id not found in cache 

========================= hadoop sample_4821 =========================

public synchronized ClientServiceDelegate getClient(JobID jobId) {	if (hsProxy == null) {	try {	hsProxy = instantiateHistoryProxy();	} catch (IOException e) {	
could not connect to history server 

protected MRClientProtocol instantiateHistoryProxy() throws IOException {	final String serviceAddr = conf.get(JHAdminConfig.MR_HISTORY_ADDRESS);	if (StringUtils.isEmpty(serviceAddr)) {	return null;	}	
connecting to historyserver at 

protected MRClientProtocol instantiateHistoryProxy() throws IOException {	final String serviceAddr = conf.get(JHAdminConfig.MR_HISTORY_ADDRESS);	if (StringUtils.isEmpty(serviceAddr)) {	return null;	}	final YarnRPC rpc = YarnRPC.create(conf);	
connected to historyserver at 

========================= hadoop sample_5699 =========================

public void stateChanged(Service service) {	
entry to state for 

========================= hadoop sample_4088 =========================

TestCase.assertEquals(2, localFiles.length);	TestCase.assertEquals(2, localArchives.length);	TestCase.assertEquals(2, files.length);	TestCase.assertEquals(2, archives.length);	TestCase.assertTrue(files[0].getPath().endsWith("distributed.first"));	TestCase.assertTrue(files[1].getPath().endsWith("distributed.second.jar"));	TestCase.assertEquals(1, fs.getFileStatus(localFiles[0]).getLen());	TestCase.assertTrue(fs.getFileStatus(localFiles[1]).getLen() > 1);	TestCase.assertTrue(fs.exists(new Path(localArchives[0], "distributed.jar.inside3")));	TestCase.assertTrue(fs.exists(new Path(localArchives[1], "distributed.jar.inside4")));	
java classpath java class path 

========================= hadoop sample_4437 =========================

private void testKeySpecs(String input, String expectedOutput, KeyFieldHelper helper, int s1, int e1) {	
input 

private void testKeySpecs(String input, String expectedOutput, KeyFieldHelper helper, int s1, int e1) {	String keySpecs = helper.keySpecs().get(0).toString();	
keyspecs 

private void testKeySpecs(String input, String expectedOutput, KeyFieldHelper helper, int s1, int e1) {	String keySpecs = helper.keySpecs().get(0).toString();	byte[] inputBytes = input.getBytes();	if (e1 == -1) {	e1 = inputBytes.length;	}	
length 

private void testKeySpecs(String input, String expectedOutput, KeyFieldHelper helper, int s1, int e1) {	String keySpecs = helper.keySpecs().get(0).toString();	byte[] inputBytes = input.getBytes();	if (e1 == -1) {	e1 = inputBytes.length;	}	int[] indices = helper.getWordLengths(inputBytes, s1, e1);	int start = helper.getStartOffset(inputBytes, s1, e1, indices, helper.keySpecs().get(0));	
start 

if (e1 == -1) {	e1 = inputBytes.length;	}	int[] indices = helper.getWordLengths(inputBytes, s1, e1);	int start = helper.getStartOffset(inputBytes, s1, e1, indices, helper.keySpecs().get(0));	if (expectedOutput == null) {	assertEquals("Expected -1 when the start index is invalid", -1, start);	return;	}	int end = helper.getEndOffset(inputBytes, s1, e1, indices, helper.keySpecs().get(0));	
end 

}	int[] indices = helper.getWordLengths(inputBytes, s1, e1);	int start = helper.getStartOffset(inputBytes, s1, e1, indices, helper.keySpecs().get(0));	if (expectedOutput == null) {	assertEquals("Expected -1 when the start index is invalid", -1, start);	return;	}	int end = helper.getEndOffset(inputBytes, s1, e1, indices, helper.keySpecs().get(0));	end = (end >= inputBytes.length) ? inputBytes.length -1 : end;	int length = end + 1 - start;	
length 

if (expectedOutput == null) {	assertEquals("Expected -1 when the start index is invalid", -1, start);	return;	}	int end = helper.getEndOffset(inputBytes, s1, e1, indices, helper.keySpecs().get(0));	end = (end >= inputBytes.length) ? inputBytes.length -1 : end;	int length = end + 1 - start;	byte[] outputBytes = new byte[length];	System.arraycopy(inputBytes, start, outputBytes, 0, length);	String output = new String(outputBytes);	
output 

if (expectedOutput == null) {	assertEquals("Expected -1 when the start index is invalid", -1, start);	return;	}	int end = helper.getEndOffset(inputBytes, s1, e1, indices, helper.keySpecs().get(0));	end = (end >= inputBytes.length) ? inputBytes.length -1 : end;	int length = end + 1 - start;	byte[] outputBytes = new byte[length];	System.arraycopy(inputBytes, start, outputBytes, 0, length);	String output = new String(outputBytes);	
expected output 

========================= hadoop sample_5676 =========================

protected void serviceInit(Configuration conf) throws Exception {	metrics = EntityGroupFSTimelineStoreMetrics.create();	summaryStore = createSummaryStore();	addService(summaryStore);	long logRetainSecs = conf.getLong( YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_RETAIN_SECONDS, YarnConfiguration .TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_RETAIN_SECONDS_DEFAULT);	logRetainMillis = logRetainSecs * 1000;	
cleaner set to delete logs older than seconds 

protected void serviceInit(Configuration conf) throws Exception {	metrics = EntityGroupFSTimelineStoreMetrics.create();	summaryStore = createSummaryStore();	addService(summaryStore);	long logRetainSecs = conf.getLong( YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_RETAIN_SECONDS, YarnConfiguration .TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_RETAIN_SECONDS_DEFAULT);	logRetainMillis = logRetainSecs * 1000;	long unknownActiveSecs = conf.getLong( YarnConfiguration .TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_UNKNOWN_ACTIVE_SECONDS, YarnConfiguration. TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_UNKNOWN_ACTIVE_SECONDS_DEFAULT );	unknownActiveMillis = unknownActiveSecs * 1000;	
unknown apps will be treated as complete after seconds 

protected void serviceInit(Configuration conf) throws Exception {	metrics = EntityGroupFSTimelineStoreMetrics.create();	summaryStore = createSummaryStore();	addService(summaryStore);	long logRetainSecs = conf.getLong( YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_RETAIN_SECONDS, YarnConfiguration .TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_RETAIN_SECONDS_DEFAULT);	logRetainMillis = logRetainSecs * 1000;	long unknownActiveSecs = conf.getLong( YarnConfiguration .TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_UNKNOWN_ACTIVE_SECONDS, YarnConfiguration. TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_UNKNOWN_ACTIVE_SECONDS_DEFAULT );	unknownActiveMillis = unknownActiveSecs * 1000;	appCacheMaxSize = conf.getInt( YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_APP_CACHE_SIZE, YarnConfiguration .TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_APP_CACHE_SIZE_DEFAULT);	
application cache size is 

addService(summaryStore);	long logRetainSecs = conf.getLong( YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_RETAIN_SECONDS, YarnConfiguration .TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_RETAIN_SECONDS_DEFAULT);	logRetainMillis = logRetainSecs * 1000;	long unknownActiveSecs = conf.getLong( YarnConfiguration .TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_UNKNOWN_ACTIVE_SECONDS, YarnConfiguration. TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_UNKNOWN_ACTIVE_SECONDS_DEFAULT );	unknownActiveMillis = unknownActiveSecs * 1000;	appCacheMaxSize = conf.getInt( YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_APP_CACHE_SIZE, YarnConfiguration .TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_APP_CACHE_SIZE_DEFAULT);	cachedLogs = Collections.synchronizedMap( new LinkedHashMap<TimelineEntityGroupId, EntityCacheItem>( appCacheMaxSize + 1, 0.75f, true) {	protected boolean removeEldestEntry( Map.Entry<TimelineEntityGroupId, EntityCacheItem> eldest) {	if (super.size() > appCacheMaxSize) {	TimelineEntityGroupId groupId = eldest.getKey();	
evicting due to space limitations 

long logRetainSecs = conf.getLong( YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_RETAIN_SECONDS, YarnConfiguration .TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_RETAIN_SECONDS_DEFAULT);	logRetainMillis = logRetainSecs * 1000;	long unknownActiveSecs = conf.getLong( YarnConfiguration .TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_UNKNOWN_ACTIVE_SECONDS, YarnConfiguration. TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_UNKNOWN_ACTIVE_SECONDS_DEFAULT );	unknownActiveMillis = unknownActiveSecs * 1000;	appCacheMaxSize = conf.getInt( YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_APP_CACHE_SIZE, YarnConfiguration .TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_APP_CACHE_SIZE_DEFAULT);	cachedLogs = Collections.synchronizedMap( new LinkedHashMap<TimelineEntityGroupId, EntityCacheItem>( appCacheMaxSize + 1, 0.75f, true) {	protected boolean removeEldestEntry( Map.Entry<TimelineEntityGroupId, EntityCacheItem> eldest) {	if (super.size() > appCacheMaxSize) {	TimelineEntityGroupId groupId = eldest.getKey();	EntityCacheItem cacheItem = eldest.getValue();	
force release cache 

private List<TimelineEntityGroupPlugin> loadPlugIns(Configuration conf) throws RuntimeException {	Collection<String> pluginNames = conf.getTrimmedStringCollection( YarnConfiguration.TIMELINE_SERVICE_ENTITY_GROUP_PLUGIN_CLASSES);	String pluginClasspath = conf.getTrimmed( YarnConfiguration.TIMELINE_SERVICE_ENTITY_GROUP_PLUGIN_CLASSPATH);	String[] systemClasses = conf.getTrimmedStrings( YarnConfiguration.TIMELINE_SERVICE_ENTITY_GROUP_PLUGIN_SYSTEM_CLASSES);	List<TimelineEntityGroupPlugin> pluginList = new LinkedList<TimelineEntityGroupPlugin>();	ClassLoader customClassLoader = null;	if (pluginClasspath != null && pluginClasspath.length() > 0) {	try {	customClassLoader = createPluginClassLoader(pluginClasspath, systemClasses);	} catch (IOException ioe) {	
error loading classloader 

String[] systemClasses = conf.getTrimmedStrings( YarnConfiguration.TIMELINE_SERVICE_ENTITY_GROUP_PLUGIN_SYSTEM_CLASSES);	List<TimelineEntityGroupPlugin> pluginList = new LinkedList<TimelineEntityGroupPlugin>();	ClassLoader customClassLoader = null;	if (pluginClasspath != null && pluginClasspath.length() > 0) {	try {	customClassLoader = createPluginClassLoader(pluginClasspath, systemClasses);	} catch (IOException ioe) {	}	}	for (final String name : pluginNames) {	
trying to load plugin class 

if (pluginClasspath != null && pluginClasspath.length() > 0) {	try {	customClassLoader = createPluginClassLoader(pluginClasspath, systemClasses);	} catch (IOException ioe) {	}	}	for (final String name : pluginNames) {	TimelineEntityGroupPlugin cacheIdPlugin = null;	try {	if (customClassLoader != null) {	
load plugin with classpath 

}	}	for (final String name : pluginNames) {	TimelineEntityGroupPlugin cacheIdPlugin = null;	try {	if (customClassLoader != null) {	Class<?> clazz = Class.forName(name, true, customClassLoader);	Class<? extends TimelineEntityGroupPlugin> sClass = clazz.asSubclass( TimelineEntityGroupPlugin.class);	cacheIdPlugin = ReflectionUtils.newInstance(sClass, conf);	} else {	
load plugin class with system classpath 

try {	if (customClassLoader != null) {	Class<?> clazz = Class.forName(name, true, customClassLoader);	Class<? extends TimelineEntityGroupPlugin> sClass = clazz.asSubclass( TimelineEntityGroupPlugin.class);	cacheIdPlugin = ReflectionUtils.newInstance(sClass, conf);	} else {	Class<?> clazz = conf.getClassByName(name);	cacheIdPlugin = (TimelineEntityGroupPlugin) ReflectionUtils.newInstance( clazz, conf);	}	} catch (Exception e) {	
error loading plugin 

Class<?> clazz = Class.forName(name, true, customClassLoader);	Class<? extends TimelineEntityGroupPlugin> sClass = clazz.asSubclass( TimelineEntityGroupPlugin.class);	cacheIdPlugin = ReflectionUtils.newInstance(sClass, conf);	} else {	Class<?> clazz = conf.getClassByName(name);	cacheIdPlugin = (TimelineEntityGroupPlugin) ReflectionUtils.newInstance( clazz, conf);	}	} catch (Exception e) {	throw new RuntimeException("No class defined for " + name, e);	}	
load plugin class 

protected void serviceStart() throws Exception {	super.serviceStart();	
starting 

if (!fs.exists(doneRootPath)) {	fs.mkdirs(doneRootPath);	fs.setPermission(doneRootPath, DONE_DIR_PERMISSION);	}	objMapper = new ObjectMapper();	objMapper.setAnnotationIntrospector(new JaxbAnnotationIntrospector());	jsonFactory = new MappingJsonFactory(objMapper);	final long scanIntervalSecs = conf.getLong( YarnConfiguration .TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_SCAN_INTERVAL_SECONDS, YarnConfiguration .TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_SCAN_INTERVAL_SECONDS_DEFAULT );	final long cleanerIntervalSecs = conf.getLong( YarnConfiguration .TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_CLEANER_INTERVAL_SECONDS, YarnConfiguration .TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_CLEANER_INTERVAL_SECONDS_DEFAULT );	final int numThreads = conf.getInt( YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_THREADS, YarnConfiguration .TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_THREADS_DEFAULT);	
scanning active directory every seconds 

if (!fs.exists(doneRootPath)) {	fs.mkdirs(doneRootPath);	fs.setPermission(doneRootPath, DONE_DIR_PERMISSION);	}	objMapper = new ObjectMapper();	objMapper.setAnnotationIntrospector(new JaxbAnnotationIntrospector());	jsonFactory = new MappingJsonFactory(objMapper);	final long scanIntervalSecs = conf.getLong( YarnConfiguration .TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_SCAN_INTERVAL_SECONDS, YarnConfiguration .TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_SCAN_INTERVAL_SECONDS_DEFAULT );	final long cleanerIntervalSecs = conf.getLong( YarnConfiguration .TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_CLEANER_INTERVAL_SECONDS, YarnConfiguration .TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_CLEANER_INTERVAL_SECONDS_DEFAULT );	final int numThreads = conf.getInt( YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_THREADS, YarnConfiguration .TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_THREADS_DEFAULT);	
cleaning logs every seconds 

protected void serviceStop() throws Exception {	
stopping 

protected void serviceStop() throws Exception {	stopExecutors.set(true);	if (executor != null) {	executor.shutdown();	if (executor.isTerminating()) {	
waiting for executor to terminate 

protected void serviceStop() throws Exception {	stopExecutors.set(true);	if (executor != null) {	executor.shutdown();	if (executor.isTerminating()) {	boolean terminated = executor.awaitTermination(10, TimeUnit.SECONDS);	if (terminated) {	
executor terminated 

protected void serviceStop() throws Exception {	stopExecutors.set(true);	if (executor != null) {	executor.shutdown();	if (executor.isTerminating()) {	boolean terminated = executor.awaitTermination(10, TimeUnit.SECONDS);	if (terminated) {	} else {	
executor did not terminate 

public synchronized void moveToDone() throws IOException {	Path doneAppPath = getDoneAppPath(appId);	if (!doneAppPath.equals(appDirPath)) {	Path donePathParent = doneAppPath.getParent();	if (!fs.exists(donePathParent)) {	fs.mkdirs(donePathParent);	}	
application is done trying to move to done dir 

public synchronized void moveToDone() throws IOException {	Path doneAppPath = getDoneAppPath(appId);	if (!doneAppPath.equals(appDirPath)) {	Path donePathParent = doneAppPath.getParent();	if (!fs.exists(donePathParent)) {	fs.mkdirs(donePathParent);	}	if (!fs.rename(appDirPath, doneAppPath)) {	throw new IOException("Rename " + appDirPath + " to " + doneAppPath + " failed");	} else {	
moved to 

public void run() {	
active scan starting 

public void run() {	try {	int scanned = scanActiveLogs();	
scanned active applications 

public void run() {	try {	int scanned = scanActiveLogs();	} catch (Exception e) {	Throwable t = extract(e);	if (t instanceof InterruptedException) {	
file scanner interrupted 

public void run() {	try {	int scanned = scanActiveLogs();	} catch (Exception e) {	Throwable t = extract(e);	if (t instanceof InterruptedException) {	} else {	
error scanning active files 

public void run() {	try {	int scanned = scanActiveLogs();	} catch (Exception e) {	Throwable t = extract(e);	if (t instanceof InterruptedException) {	} else {	}	}	
active scan complete 

public void run() {	try {	
begin parsing summary logs 

public void run() {	try {	appLogs.parseSummaryLogs();	if (appLogs.isDone()) {	appLogs.moveToDone();	appIdLogMap.remove(appLogs.getAppId());	}	
end parsing summary logs 

public void run() {	try {	appLogs.parseSummaryLogs();	if (appLogs.isDone()) {	appLogs.moveToDone();	appIdLogMap.remove(appLogs.getAppId());	}	} catch (Exception e) {	Throwable t = extract(e);	if (t instanceof InterruptedException) {	
log parser interrupted 

try {	appLogs.parseSummaryLogs();	if (appLogs.isDone()) {	appLogs.moveToDone();	appIdLogMap.remove(appLogs.getAppId());	}	} catch (Exception e) {	Throwable t = extract(e);	if (t instanceof InterruptedException) {	} else {	
error processing logs for 

public void run() {	
cleaner starting 

public void run() {	long startTime = Time.monotonicNow();	try {	cleanLogs(doneRootPath, fs, logRetainMillis);	} catch (Exception e) {	Throwable t = extract(e);	if (t instanceof InterruptedException) {	
cleaner interrupted 

public void run() {	long startTime = Time.monotonicNow();	try {	cleanLogs(doneRootPath, fs, logRetainMillis);	} catch (Exception e) {	Throwable t = extract(e);	if (t instanceof InterruptedException) {	} else {	
error cleaning files 

try {	cleanLogs(doneRootPath, fs, logRetainMillis);	} catch (Exception e) {	Throwable t = extract(e);	if (t instanceof InterruptedException) {	} else {	}	} finally {	metrics.addLogCleanTime(Time.monotonicNow() - startTime);	}	
cleaner finished 

private List<TimelineStore> getTimelineStoresFromCacheIds( Set<TimelineEntityGroupId> groupIds, String entityType, List<EntityCacheItem> cacheItems) throws IOException {	List<TimelineStore> stores = new LinkedList<TimelineStore>();	for (TimelineEntityGroupId groupId : groupIds) {	TimelineStore storeForId = getCachedStore(groupId, cacheItems);	if (storeForId != null) {	
adding as a store for the query 

private List<TimelineStore> getTimelineStoresFromCacheIds( Set<TimelineEntityGroupId> groupIds, String entityType, List<EntityCacheItem> cacheItems) throws IOException {	List<TimelineStore> stores = new LinkedList<TimelineStore>();	for (TimelineEntityGroupId groupId : groupIds) {	TimelineStore storeForId = getCachedStore(groupId, cacheItems);	if (storeForId != null) {	stores.add(storeForId);	metrics.incrGetEntityToDetailOps();	}	}	if (stores.size() == 0) {	
using summary store for 

protected List<TimelineStore> getTimelineStoresForRead(String entityId, String entityType, List<EntityCacheItem> cacheItems) throws IOException {	Set<TimelineEntityGroupId> groupIds = new HashSet<TimelineEntityGroupId>();	for (TimelineEntityGroupPlugin cacheIdPlugin : cacheIdPlugins) {	
trying plugin for id and type 

protected List<TimelineStore> getTimelineStoresForRead(String entityId, String entityType, List<EntityCacheItem> cacheItems) throws IOException {	Set<TimelineEntityGroupId> groupIds = new HashSet<TimelineEntityGroupId>();	for (TimelineEntityGroupPlugin cacheIdPlugin : cacheIdPlugins) {	Set<TimelineEntityGroupId> idsFromPlugin = cacheIdPlugin.getTimelineEntityGroupId(entityId, entityType);	if (idsFromPlugin == null) {	
plugin returned null 

protected List<TimelineStore> getTimelineStoresForRead(String entityId, String entityType, List<EntityCacheItem> cacheItems) throws IOException {	Set<TimelineEntityGroupId> groupIds = new HashSet<TimelineEntityGroupId>();	for (TimelineEntityGroupPlugin cacheIdPlugin : cacheIdPlugins) {	Set<TimelineEntityGroupId> idsFromPlugin = cacheIdPlugin.getTimelineEntityGroupId(entityId, entityType);	if (idsFromPlugin == null) {	} else {	
plugin returned ids 

protected List<TimelineStore> getTimelineStoresForRead(String entityId, String entityType, List<EntityCacheItem> cacheItems) throws IOException {	Set<TimelineEntityGroupId> groupIds = new HashSet<TimelineEntityGroupId>();	for (TimelineEntityGroupPlugin cacheIdPlugin : cacheIdPlugins) {	Set<TimelineEntityGroupId> idsFromPlugin = cacheIdPlugin.getTimelineEntityGroupId(entityId, entityType);	if (idsFromPlugin == null) {	} else {	}	if (idsFromPlugin != null) {	groupIds.addAll(idsFromPlugin);	
plugin returns a non null value on query 

private List<TimelineStore> getTimelineStoresForRead(String entityType, NameValuePair primaryFilter, Collection<NameValuePair> secondaryFilters, List<EntityCacheItem> cacheItems) throws IOException {	Set<TimelineEntityGroupId> groupIds = new HashSet<TimelineEntityGroupId>();	for (TimelineEntityGroupPlugin cacheIdPlugin : cacheIdPlugins) {	Set<TimelineEntityGroupId> idsFromPlugin = cacheIdPlugin.getTimelineEntityGroupId(entityType, primaryFilter, secondaryFilters);	if (idsFromPlugin != null) {	
plugin returns a non null value on query 

private TimelineStore getCachedStore(TimelineEntityGroupId groupId, List<EntityCacheItem> cacheItems) throws IOException {	EntityCacheItem cacheItem;	synchronized (this.cachedLogs) {	cacheItem = this.cachedLogs.get(groupId);	if (cacheItem == null) {	
set up new cache item for id 

private TimelineStore getCachedStore(TimelineEntityGroupId groupId, List<EntityCacheItem> cacheItems) throws IOException {	EntityCacheItem cacheItem;	synchronized (this.cachedLogs) {	cacheItem = this.cachedLogs.get(groupId);	if (cacheItem == null) {	cacheItem = new EntityCacheItem(groupId, getConfig());	AppLogs appLogs = getAndSetAppLogs(groupId.getApplicationId());	if (appLogs != null) {	
set applogs for group id 

EntityCacheItem cacheItem;	synchronized (this.cachedLogs) {	cacheItem = this.cachedLogs.get(groupId);	if (cacheItem == null) {	cacheItem = new EntityCacheItem(groupId, getConfig());	AppLogs appLogs = getAndSetAppLogs(groupId.getApplicationId());	if (appLogs != null) {	cacheItem.setAppLogs(appLogs);	this.cachedLogs.put(groupId, cacheItem);	} else {	
applogs for groupid is set to null 

if (appLogs != null) {	cacheItem.setAppLogs(appLogs);	this.cachedLogs.put(groupId, cacheItem);	} else {	}	}	}	TimelineStore store = null;	if (cacheItem.getAppLogs() != null) {	AppLogs appLogs = cacheItem.getAppLogs();	
try refresh cache 

} else {	}	}	}	TimelineStore store = null;	if (cacheItem.getAppLogs() != null) {	AppLogs appLogs = cacheItem.getAppLogs();	cacheItems.add(cacheItem);	store = cacheItem.refreshCache(aclManager, metrics);	} else {	
applogs for group id is null 

public TimelineEntities getEntities(String entityType, Long limit, Long windowStart, Long windowEnd, String fromId, Long fromTs, NameValuePair primaryFilter, Collection<NameValuePair> secondaryFilters, EnumSet<Field> fieldsToRetrieve, CheckAcl checkAcl) throws IOException {	LOG.debug("getEntities type={} primary={}", entityType, primaryFilter);	List<EntityCacheItem> relatedCacheItems = new ArrayList<>();	List<TimelineStore> stores = getTimelineStoresForRead(entityType, primaryFilter, secondaryFilters, relatedCacheItems);	TimelineEntities returnEntities = new TimelineEntities();	for (TimelineStore store : stores) {	
try timeline store for the request 

public TimelineEntity getEntity(String entityId, String entityType, EnumSet<Field> fieldsToRetrieve) throws IOException {	LOG.debug("getEntity type={} id={}", entityType, entityId);	List<EntityCacheItem> relatedCacheItems = new ArrayList<>();	List<TimelineStore> stores = getTimelineStoresForRead(entityId, entityType, relatedCacheItems);	for (TimelineStore store : stores) {	
try timeline store for the request 

public TimelineEntity getEntity(String entityId, String entityType, EnumSet<Field> fieldsToRetrieve) throws IOException {	LOG.debug("getEntity type={} id={}", entityType, entityId);	List<EntityCacheItem> relatedCacheItems = new ArrayList<>();	List<TimelineStore> stores = getTimelineStoresForRead(entityId, entityType, relatedCacheItems);	for (TimelineStore store : stores) {	TimelineEntity e = store.getEntity(entityId, entityType, fieldsToRetrieve);	if (e != null) {	return e;	}	}	
getentity found nothing 

public TimelineEvents getEntityTimelines(String entityType, SortedSet<String> entityIds, Long limit, Long windowStart, Long windowEnd, Set<String> eventTypes) throws IOException {	LOG.debug("getEntityTimelines type={} ids={}", entityType, entityIds);	TimelineEvents returnEvents = new TimelineEvents();	List<EntityCacheItem> relatedCacheItems = new ArrayList<>();	for (String entityId : entityIds) {	LOG.debug("getEntityTimeline type={} id={}", entityType, entityId);	List<TimelineStore> stores = getTimelineStoresForRead(entityId, entityType, relatedCacheItems);	for (TimelineStore store : stores) {	
try timeline store for the request 

========================= hadoop sample_302 =========================

public static boolean isPlaceBlacklisted( SchedulerApplicationAttempt application, SchedulerNode node, Log log) {	if (application.isPlaceBlacklisted(node.getNodeName())) {	if (log.isDebugEnabled()) {	
skipping host for since it has been blacklisted 

public static boolean isPlaceBlacklisted( SchedulerApplicationAttempt application, SchedulerNode node, Log log) {	if (application.isPlaceBlacklisted(node.getNodeName())) {	if (log.isDebugEnabled()) {	}	return true;	}	if (application.isPlaceBlacklisted(node.getRackName())) {	if (log.isDebugEnabled()) {	
skipping rack for since it has been blacklisted 

========================= hadoop sample_882 =========================

public void tearDown() throws Exception {	
shutting down minidfscluster 

========================= hadoop sample_7132 =========================

throw new IllegalArgumentException("Handler is already specified for " + scheme + " authentication scheme.");	}	String authHandlerPropName = String.format(AUTH_HANDLER_PROPERTY, scheme).toLowerCase();	String authHandlerName = config.getProperty(authHandlerPropName);	Preconditions.checkNotNull(authHandlerName, "No auth handler configured for scheme %s.", scheme);	String authHandlerClassName = AuthenticationHandlerUtil .getAuthenticationHandlerClassName(authHandlerName);	AuthenticationHandler handler = initializeAuthHandler(authHandlerClassName, config);	schemeToAuthHandlerMapping.put(scheme, handler);	types.add(handler.getType());	}	
successfully initialized multischemeauthenticationhandler 

protected AuthenticationHandler initializeAuthHandler( String authHandlerClassName, Properties config) throws ServletException {	try {	Preconditions.checkNotNull(authHandlerClassName);	
initializing authentication handler of type 

protected AuthenticationHandler initializeAuthHandler( String authHandlerClassName, Properties config) throws ServletException {	try {	Preconditions.checkNotNull(authHandlerClassName);	Class<?> klass = Thread.currentThread().getContextClassLoader() .loadClass(authHandlerClassName);	AuthenticationHandler authHandler = (AuthenticationHandler) klass.newInstance();	authHandler.init(config);	
successfully initialized authentication handler of type 

protected AuthenticationHandler initializeAuthHandler( String authHandlerClassName, Properties config) throws ServletException {	try {	Preconditions.checkNotNull(authHandlerClassName);	Class<?> klass = Thread.currentThread().getContextClassLoader() .loadClass(authHandlerClassName);	AuthenticationHandler authHandler = (AuthenticationHandler) klass.newInstance();	authHandler.init(config);	return authHandler;	} catch (ClassNotFoundException | InstantiationException | IllegalAccessException ex) {	
failed to initialize authentication handler 

public AuthenticationToken authenticate(HttpServletRequest request, HttpServletResponse response) throws IOException, AuthenticationException {	String authorization = request.getHeader(HttpConstants.AUTHORIZATION_HEADER);	if (authorization != null) {	for (String scheme : schemeToAuthHandlerMapping.keySet()) {	if (AuthenticationHandlerUtil.matchAuthScheme(scheme, authorization)) {	AuthenticationHandler handler = schemeToAuthHandlerMapping.get(scheme);	AuthenticationToken token = handler.authenticate(request, response);	
token generated with type 

========================= hadoop sample_2760 =========================

private static void setFailure(AtomicReference<String> failure, String what) {	failure.compareAndSet("", what);	
test error 

synchronized (this) {	numLeases--;	}	}	};	BlockManagerFaultInjector.instance = injector;	final int NUM_DATANODES = 5;	MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DATANODES).build();	cluster.waitActive();	for (int n = 1; n <= NUM_DATANODES; n++) {	
waiting for datanode s to report in 

========================= hadoop sample_7562 =========================

private void sleepForever() {	while (true) {	try {	Thread.sleep(SLEEP_INTERVAL_MS);	if (!dfs.isClusterUp()) {	
cluster is no longer up exiting 

public void start() throws IOException, FileNotFoundException {	dfs = new MiniDFSCluster.Builder(conf).nameNodePort(nameNodePort) .nameNodeHttpPort(nameNodeHttpPort) .numDataNodes(numDataNodes) .startupOption(dfsOpts) .format(format) .build();	dfs.waitActive();	
started minidfscluster namenode on port 

private boolean parseArguments(String[] args) {	Options options = makeOptions();	CommandLine cli;	try {	CommandLineParser parser = new GnuParser();	cli = parser.parse(options, args);	} catch(ParseException e) {	
options parsing failed 

} catch(ParseException e) {	new HelpFormatter().printHelp("...", options);	return false;	}	if (cli.hasOption("help")) {	new HelpFormatter().printHelp("...", options);	return false;	}	if (cli.getArgs().length > 0) {	for (String arg : cli.getArgs()) {	
unrecognized option 

private void updateConfiguration(Configuration conf2, String[] keyvalues) {	int num_confs_updated = 0;	if (keyvalues != null) {	for (String prop : keyvalues) {	String[] keyval = prop.split("=", 2);	if (keyval.length == 2) {	conf2.set(keyval[0], keyval[1]);	num_confs_updated++;	} else {	
ignoring d option 

if (keyvalues != null) {	for (String prop : keyvalues) {	String[] keyval = prop.split("=", 2);	if (keyval.length == 2) {	conf2.set(keyval[0], keyval[1]);	num_confs_updated++;	} else {	}	}	}	
updated configuration settings from command line 

private int intArgument(CommandLine cli, String argName, int defaultValue) {	String o = cli.getOptionValue(argName);	try {	if (o != null) {	return Integer.parseInt(o);	}	} catch (NumberFormatException ex) {	
couldn t parse value for option using default 

========================= hadoop sample_7077 =========================

private void waitForLaunchedState(RMAppAttempt attempt) throws InterruptedException {	int waitCount = 0;	while (attempt.getAppAttemptState() != RMAppAttemptState.LAUNCHED && waitCount++ < 20) {	
waiting for appattempt to reach launched state current state is 

========================= hadoop sample_494 =========================

instream.seek(shortLen + 1024);	int c = instream.read();	assertIsEOF("read()", c);	byte[] buf = new byte[256];	assertIsEOF("read(buffer)", instream.read(buf));	assertIsEOF("read(offset)", instream.read(instream.getPos(), buf, 0, buf.length));	try {	instream.readFully(shortLen + 512, buf);	fail("Expected readFully to fail");	} catch (EOFException expected) {	
expected eof 

========================= hadoop sample_5949 =========================

InetSocketAddress collectorServerAddress = conf.getSocketAddr( YarnConfiguration.NM_BIND_HOST, YarnConfiguration.NM_COLLECTOR_SERVICE_ADDRESS, YarnConfiguration.DEFAULT_NM_COLLECTOR_SERVICE_ADDRESS, YarnConfiguration.DEFAULT_NM_COLLECTOR_SERVICE_PORT);	Configuration serverConf = new Configuration(conf);	YarnRPC rpc = YarnRPC.create(conf);	server = rpc.getServer(CollectorNodemanagerProtocol.class, this, collectorServerAddress, serverConf, null, conf.getInt(YarnConfiguration.NM_COLLECTOR_SERVICE_THREAD_COUNT, YarnConfiguration.DEFAULT_NM_COLLECTOR_SERVICE_THREAD_COUNT));	if (conf.getBoolean( CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION, false)) {	server.refreshServiceAcl(conf, new NMPolicyProvider());	}	server.start();	collectorServerAddress = conf.updateConnectAddr( YarnConfiguration.NM_BIND_HOST, YarnConfiguration.NM_COLLECTOR_SERVICE_ADDRESS, YarnConfiguration.DEFAULT_NM_COLLECTOR_SERVICE_ADDRESS, server.getListenerAddress());	super.serviceStart();	
nmcollectorservice started at 

newCollectorsMap.put(appId, collector);	NMTimelinePublisher nmTimelinePublisher = context.getNMTimelinePublisher();	if (nmTimelinePublisher != null) {	nmTimelinePublisher.setTimelineServiceAddress(appId, collector.getCollectorAddr());	}	}	Map<ApplicationId, AppCollectorData> registeringCollectors = context.getRegisteringCollectors();	if (registeringCollectors != null) {	registeringCollectors.putAll(newCollectorsMap);	} else {	
collectors are added when the registered collectors are initialized 

========================= hadoop sample_1727 =========================

public static void main(String[] args) {	JobConf job = new JobConf(HadoopArchiveLogs.class);	HadoopArchiveLogs hal = new HadoopArchiveLogs(job);	int ret = 0;	try{	ret = ToolRunner.run(hal, args);	} catch(Exception e) {	
Exception 

public int run(String[] args) throws Exception {	int exitCode = 1;	handleOpts(args);	FileSystem fs = null;	Path remoteRootLogDir = new Path(conf.get( YarnConfiguration.NM_REMOTE_APP_LOG_DIR, YarnConfiguration.DEFAULT_NM_REMOTE_APP_LOG_DIR));	String suffix = LogAggregationUtils.getRemoteNodeLogDirSuffix(conf);	Path workingDir = new Path(remoteRootLogDir, "archive-logs-work");	if (verbose) {	
remote log dir root 

public int run(String[] args) throws Exception {	int exitCode = 1;	handleOpts(args);	FileSystem fs = null;	Path remoteRootLogDir = new Path(conf.get( YarnConfiguration.NM_REMOTE_APP_LOG_DIR, YarnConfiguration.DEFAULT_NM_REMOTE_APP_LOG_DIR));	String suffix = LogAggregationUtils.getRemoteNodeLogDirSuffix(conf);	Path workingDir = new Path(remoteRootLogDir, "archive-logs-work");	if (verbose) {	
log suffix 

public int run(String[] args) throws Exception {	int exitCode = 1;	handleOpts(args);	FileSystem fs = null;	Path remoteRootLogDir = new Path(conf.get( YarnConfiguration.NM_REMOTE_APP_LOG_DIR, YarnConfiguration.DEFAULT_NM_REMOTE_APP_LOG_DIR));	String suffix = LogAggregationUtils.getRemoteNodeLogDirSuffix(conf);	Path workingDir = new Path(remoteRootLogDir, "archive-logs-work");	if (verbose) {	
working dir 

Path workingDir = new Path(remoteRootLogDir, "archive-logs-work");	if (verbose) {	}	try {	fs = FileSystem.get(conf);	if (prepareWorkingDir(fs, workingDir)) {	checkFilesAndSeedApps(fs, remoteRootLogDir, suffix);	filterAppsByAggregatedStatus();	checkMaxEligible();	if (eligibleApplications.isEmpty()) {	
no eligible applications to process 

CommandLineParser parser = new GnuParser();	CommandLine commandLine = parser.parse(opts, args);	if (commandLine.hasOption(HELP_OPTION)) {	HelpFormatter formatter = new HelpFormatter();	formatter.printHelp("mapred archive-logs", opts);	System.exit(0);	}	if (commandLine.hasOption(MAX_ELIGIBLE_APPS_OPTION)) {	maxEligible = Integer.parseInt( commandLine.getOptionValue(MAX_ELIGIBLE_APPS_OPTION));	if (maxEligible == 0) {	
setting to accomplishes nothing please either set it to a negative value default all or a more reasonable value 

private boolean runDistributedShell(File localScript) throws Exception {	String[] dsArgs = {	"--appname", "ArchiveLogs", "--jar", ApplicationMaster.class.getProtectionDomain().getCodeSource() .getLocation().getPath(), "--num_containers", Integer.toString(eligibleApplications.size()), "--container_memory", Long.toString(memory), "--shell_script", localScript.getAbsolutePath() };	if (verbose) {	
running distributed shell with arguments 

========================= hadoop sample_6518 =========================

private TaskAttemptInfo getSuccessfulAttemptInfo(TaskType type, int task) {	TaskAttemptInfo ret;	for (int i = 0; true; ++i) {	ret = jobdesc.getTaskAttemptInfo(type, task, i);	if (ret.getRunState() == TaskStatus.State.SUCCEEDED) {	break;	}	}	if(ret.getRunState() != TaskStatus.State.SUCCEEDED) {	
no sucessful attempts tasktype task 

========================= hadoop sample_6148 =========================

public void resolve(TaskCompletionEvent event) {	switch (event.getTaskStatus()) {	case SUCCEEDED: URI u = getBaseURI(reduceId, event.getTaskTrackerHttp());	addKnownMapOutput(u.getHost() + ":" + u.getPort(), u.toString(), event.getTaskAttemptId());	maxMapRuntime = Math.max(maxMapRuntime, event.getTaskRunTime());	break;	case FAILED: case KILLED: case OBSOLETE: obsoleteMapOutput(event.getTaskAttemptId());	
ignoring obsolete output of map task 

public void resolve(TaskCompletionEvent event) {	switch (event.getTaskStatus()) {	case SUCCEEDED: URI u = getBaseURI(reduceId, event.getTaskTrackerHttp());	addKnownMapOutput(u.getHost() + ":" + u.getPort(), u.toString(), event.getTaskAttemptId());	maxMapRuntime = Math.max(maxMapRuntime, event.getTaskRunTime());	break;	case FAILED: case KILLED: case OBSOLETE: obsoleteMapOutput(event.getTaskAttemptId());	break;	case TIPFAILED: tipFailed(event.getTaskAttemptId().getTaskID());	
ignoring output of failed map tip 

long copyMillis = (endMillis - startMillis);	if (copyMillis == 0) copyMillis = 1;	float bytesPerMillis = (float) bytes / copyMillis;	float transferRate = bytesPerMillis * BYTES_PER_MILLIS_TO_MBS;	String individualProgress = "copy task(" + mapId + " succeeded" + " at " + mbpsFormat.format(transferRate) + " MB/s)";	copyTimeTracker.add(startMillis, endMillis);	totalBytesShuffledTillNow += bytes;	updateStatus(individualProgress);	reduceShuffleBytes.increment(bytes);	lastProgressTime = Time.monotonicNow();	
map done 

if (copyMillis == 0) copyMillis = 1;	float bytesPerMillis = (float) bytes / copyMillis;	float transferRate = bytesPerMillis * BYTES_PER_MILLIS_TO_MBS;	String individualProgress = "copy task(" + mapId + " succeeded" + " at " + mbpsFormat.format(transferRate) + " MB/s)";	copyTimeTracker.add(startMillis, endMillis);	totalBytesShuffledTillNow += bytes;	updateStatus(individualProgress);	reduceShuffleBytes.increment(bytes);	lastProgressTime = Time.monotonicNow();	} else {	
aborting already finished mapoutput for 

public void reportLocalError(IOException ioe) {	try {	
shuffle failed local error on this node 

public void reportLocalError(IOException ioe) {	try {	} catch (UnknownHostException e) {	
shuffle failed local error on this node 

private void checkAndInformMRAppMaster( int failures, TaskAttemptID mapId, boolean readError, boolean connectExcpt, boolean hostFailed) {	if (connectExcpt || (reportReadErrorImmediately && readError) || ((failures % maxFetchFailuresBeforeReporting) == 0) || hostFailed) {	
reporting fetch failure for to mrappmaster 

final float MAX_ALLOWED_STALL_TIME_PERCENT = 0.5f;	long totalFailures = failedShuffleCounter.getValue();	int doneMaps = totalMaps - remainingMaps;	boolean reducerHealthy = (((float)totalFailures / (totalFailures + doneMaps)) < MAX_ALLOWED_FAILED_FETCH_ATTEMPT_PERCENT);	boolean reducerProgressedEnough = (((float)doneMaps / totalMaps) >= MIN_REQUIRED_PROGRESS_PERCENT);	int stallDuration = (int)(Time.monotonicNow() - lastProgressTime);	int shuffleProgressDuration = (int)(lastProgressTime - startTime);	int minShuffleRunDuration = Math.max(shuffleProgressDuration, maxMapRuntime);	boolean reducerStalled = (((float)stallDuration / minShuffleRunDuration) >= MAX_ALLOWED_STALL_TIME_PERCENT);	if ((failureCounts.size() >= maxFailedUniqueFetches || failureCounts.size() == (totalMaps - doneMaps)) && !reducerHealthy && (!reducerProgressedEnough || reducerStalled)) {	
shuffle failed with too many fetch failures and insufficient progress 

}	Iterator<MapHost> iter = pendingHosts.iterator();	MapHost host = iter.next();	int numToPick = random.nextInt(pendingHosts.size());	for (int i = 0; i < numToPick; ++i) {	host = iter.next();	}	pendingHosts.remove(host);	host.markBusy();	if (LOG.isDebugEnabled()) {	
assigning with to 

}	}	}	while (itr.hasNext()) {	TaskAttemptID id = itr.next();	if (!obsoleteMaps.contains(id) && !finishedMaps[id.getTaskID().getId()]) {	host.addKnownMap(id);	}	}	if (LOG.isDebugEnabled()) {	
assigned of to to 

public synchronized void freeHost(MapHost host) {	if (host.getState() != State.PENALIZED) {	if (host.markAvailable() == State.PENDING) {	pendingHosts.add(host);	notifyAll();	}	}	
freed by in ms 

========================= hadoop sample_4933 =========================

while (iterator.hasNext()) {	Entry<String, String> kvPair = iterator.next();	if (kvPair.getKey().startsWith(AuthFilter.CONF_PREFIX)) {	params.put(kvPair.getKey(), kvPair.getValue());	}	}	String principalInConf = conf .get(DFSConfigKeys.DFS_WEB_AUTHENTICATION_KERBEROS_PRINCIPAL_KEY);	if (principalInConf != null && !principalInConf.isEmpty()) {	params .put( DFSConfigKeys.DFS_WEB_AUTHENTICATION_KERBEROS_PRINCIPAL_KEY, SecurityUtil.getServerPrincipal(principalInConf, bindAddress.getHostName()));	} else if (UserGroupInformation.isSecurityEnabled()) {	
webhdfs and security are enabled but configuration property is not set 

}	String principalInConf = conf .get(DFSConfigKeys.DFS_WEB_AUTHENTICATION_KERBEROS_PRINCIPAL_KEY);	if (principalInConf != null && !principalInConf.isEmpty()) {	params .put( DFSConfigKeys.DFS_WEB_AUTHENTICATION_KERBEROS_PRINCIPAL_KEY, SecurityUtil.getServerPrincipal(principalInConf, bindAddress.getHostName()));	} else if (UserGroupInformation.isSecurityEnabled()) {	}	String httpKeytab = conf.get(DFSUtil.getSpnegoKeytabKey(conf, DFSConfigKeys.DFS_NAMENODE_KEYTAB_FILE_KEY));	if (httpKeytab != null && !httpKeytab.isEmpty()) {	params.put( DFSConfigKeys.DFS_WEB_AUTHENTICATION_KERBEROS_KEYTAB_KEY, httpKeytab);	} else if (UserGroupInformation.isSecurityEnabled()) {	
webhdfs and security are enabled but configuration property is not set 

========================= hadoop sample_8013 =========================

private static FileContext getDefaultFileContext() {	FileContext fc = null;	Configuration defaultConf = new Configuration();	String[] sources;	sources = defaultConf.getPropertySources( CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY);	if (sources != null && (!Arrays.asList(sources).contains("core-default.xml") || sources.length > 1)) {	try {	fc = FileContext.getFileContext(defaultConf);	
default file system 

private static FileContext getDefaultFileContext() {	FileContext fc = null;	Configuration defaultConf = new Configuration();	String[] sources;	sources = defaultConf.getPropertySources( CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY);	if (sources != null && (!Arrays.asList(sources).contains("core-default.xml") || sources.length > 1)) {	try {	fc = FileContext.getFileContext(defaultConf);	} catch (UnsupportedFileSystemException e) {	
unable to create default file context 

Configuration defaultConf = new Configuration();	String[] sources;	sources = defaultConf.getPropertySources( CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY);	if (sources != null && (!Arrays.asList(sources).contains("core-default.xml") || sources.length > 1)) {	try {	fc = FileContext.getFileContext(defaultConf);	} catch (UnsupportedFileSystemException e) {	}	}	else {	
default file system is set solely by core default xml therefore ignoring 

========================= hadoop sample_4460 =========================

this.conf = config;	this.ugi = user;	this.namenodeAddress = address;	this.connectionPoolId = new ConnectionPoolId(this.ugi, this.namenodeAddress);	this.minSize = minPoolSize;	this.maxSize = maxPoolSize;	for (int i=0; i<this.minSize; i++) {	ConnectionContext newConnection = newConnection();	this.connections.add(newConnection);	}	
created connection pool with connections 

protected synchronized void close() {	long timeSinceLastActive = TimeUnit.MILLISECONDS.toSeconds( Time.now() - getLastActiveTime());	
shutting down connection pool used seconds ago 

========================= hadoop sample_8290 =========================

ipList.add(addr.getHostAddress());	}	}	StringBuilder builder = new StringBuilder();	for (String ip : ipList) {	builder.append(ip);	builder.append(',');	}	builder.append("127.0.1.1,");	builder.append(InetAddress.getLocalHost().getCanonicalHostName());	
local ip addresses 

public void testWebHdfsDoAs() throws Exception {	
start testwebhdfsdoas 

========================= hadoop sample_7165 =========================

private ExitStatus run() {	try {	init();	return new Processor().processNamespace().getExitStatus();	} catch (IllegalArgumentException e) {	System.out.println(e + ".  Exiting ...");	return ExitStatus.ILLEGAL_ARGUMENTS;	} catch (IOException e) {	System.out.println(e + ".  Exiting ...");	
exiting 

private void getSnapshottableDirs() {	SnapshottableDirectoryStatus[] dirs = null;	try {	dirs = dfs.getSnapshottableDirListing();	} catch (IOException e) {	
failed to get snapshottable directories ignore and continue 

getSnapshottableDirs();	Result result = new Result();	for (Path target : targetPaths) {	processPath(target.toUri().getPath(), result);	}	boolean hasFailed = Dispatcher.waitForMoveCompletion(storages.targets .values());	boolean hasSuccess = Dispatcher.checkForSuccess(storages.targets .values());	if (hasFailed && !hasSuccess) {	if (retryCount.get() == retryMaxAttempts) {	result.setRetryFailed();	
failed to move some block s after retries 

private void processPath(String fullPath, Result result) {	for (byte[] lastReturnedName = HdfsFileStatus.EMPTY_NAME;;) {	final DirectoryListing children;	try {	children = dfs.listPaths(fullPath, lastReturnedName, true);	} catch(IOException e) {	
failed to list directory ignore the directory and continue 

if (snapshottableDirs.contains(fullPath)) {	final String dirSnapshot = fullPath + HdfsConstants.DOT_SNAPSHOT_DIR;	processPath(dirSnapshot, result);	}	} else if (!status.isSymlink()) {	try {	if (!isSnapshotPathInCurrent(fullPath)) {	processFile(fullPath, (HdfsLocatedFileStatus) status, result);	}	} catch (IOException e) {	
failed to check the status of ignore it and continue 

private void processFile(String fullPath, HdfsLocatedFileStatus status, Result result) {	byte policyId = status.getStoragePolicy();	if (policyId == HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED) {	try {	policyId = dfs.getServerDefaults().getDefaultStoragePolicyId();	} catch (IOException e) {	
failed to get default policy for 

byte policyId = status.getStoragePolicy();	if (policyId == HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED) {	try {	policyId = dfs.getServerDefaults().getDefaultStoragePolicyId();	} catch (IOException e) {	return;	}	}	final BlockStoragePolicy policy = blockStoragePolicies[policyId];	if (policy == null) {	
failed to get the storage policy of file 

public static void main(String[] args) {	if (DFSUtil.parseHelpArgument(args, Cli.USAGE, System.out, true)) {	System.exit(0);	}	try {	System.exit(ToolRunner.run(new HdfsConfiguration(), new Cli(), args));	} catch (Throwable e) {	
exiting due to an exception 

========================= hadoop sample_7866 =========================

public void init(SubsetConfiguration conf) {	
initializing the gangliasink for ganglia metrics 

metricsServers = Servers.parse(conf.getString(SERVERS_PROPERTY), DEFAULT_PORT);	multicastEnabled = conf.getBoolean(MULTICAST_ENABLED_PROPERTY, DEFAULT_MULTICAST_ENABLED);	multicastTtl = conf.getInt(MULTICAST_TTL_PROPERTY, DEFAULT_MULTICAST_TTL);	gangliaConfMap = new HashMap<String, GangliaConf>();	loadGangliaConf(GangliaConfType.units);	loadGangliaConf(GangliaConfType.tmax);	loadGangliaConf(GangliaConfType.dmax);	loadGangliaConf(GangliaConfType.slope);	try {	if (multicastEnabled) {	
enabling multicast for ganglia with ttl 

private void loadGangliaConf(GangliaConfType gtype) {	String propertyarr[] = conf.getStringArray(gtype.name());	if (propertyarr != null && propertyarr.length > 0) {	for (String metricNValue : propertyarr) {	String metricNValueArr[] = metricNValue.split(EQUAL);	if (metricNValueArr.length != 2 || metricNValueArr[0].length() == 0) {	
invalid propertylist for 

========================= hadoop sample_3433 =========================

private void addVolume(Collection<StorageLocation> dataLocations, Storage.StorageDirectory sd) throws IOException {	final File dir = sd.getCurrentDir();	final StorageType storageType = getStorageTypeFromLocations(dataLocations, sd.getRoot());	FsVolumeImpl fsVolume = new FsVolumeImpl( this, sd.getStorageUuid(), dir, this.conf, storageType);	FsVolumeReference ref = fsVolume.obtainReference();	ReplicaMap tempVolumeMap = new ReplicaMap(datasetLock);	fsVolume.getVolumeMap(tempVolumeMap, ramDiskReplicaTracker);	activateVolume(tempVolumeMap, sd, storageType, ref);	
added volume storagetype 

StorageType storageType = location.getStorageType();	final FsVolumeImpl fsVolume = createFsVolume(sd.getStorageUuid(), sd.getCurrentDir(), storageType);	final ReplicaMap tempVolumeMap = new ReplicaMap(new AutoCloseableLock());	ArrayList<IOException> exceptions = Lists.newArrayList();	for (final NamespaceInfo nsInfo : nsInfos) {	String bpid = nsInfo.getBlockPoolID();	try {	fsVolume.addBlockPool(bpid, this.conf, this.timer);	fsVolume.getVolumeMap(bpid, tempVolumeMap, ramDiskReplicaTracker);	} catch (IOException e) {	
caught exception when adding will throw later 

sd.unlock();	} catch (IOException e) {	exceptions.add(e);	}	throw MultipleIOException.createIOException(exceptions);	}	final FsVolumeReference ref = fsVolume.obtainReference();	setupAsyncLazyPersistThread(fsVolume);	builder.build();	activateVolume(tempVolumeMap, sd, storageType, ref);	
added volume storagetype 

for (File vol : storageLocationsToRemove) {	Preconditions.checkArgument(vol.isAbsolute(), String.format("%s is not absolute path.", vol.getPath()));	}	Map<String, List<ReplicaInfo>> blkToInvalidate = new HashMap<>();	List<String> storageToRemove = new ArrayList<>();	try(AutoCloseableLock lock = datasetLock.acquire()) {	for (int idx = 0; idx < dataStorage.getNumStorageDirs(); idx++) {	Storage.StorageDirectory sd = dataStorage.getStorageDir(idx);	final File absRoot = sd.getRoot().getAbsoluteFile();	if (storageLocationsToRemove.contains(absRoot)) {	
removing from fsdataset 

public void getMetrics(MetricsCollector collector, boolean all) {	try {	DataNodeMetricHelper.getMetrics(collector, this, "FSDatasetState");	} catch (Exception e) {	
exception thrown while metric collection exception 

throw new IOException("Failed to copy " + srcMeta + " to " + dstMeta, e);	}	}	try {	Storage.nativeCopyFileUnbuffered(srcFile, dstFile, true);	} catch (IOException e) {	throw new IOException("Failed to copy " + srcFile + " to " + dstFile, e);	}	if (LOG.isDebugEnabled()) {	if (calculateChecksum) {	
copied to and calculated checksum 

}	}	try {	Storage.nativeCopyFileUnbuffered(srcFile, dstFile, true);	} catch (IOException e) {	throw new IOException("Failed to copy " + srcFile + " to " + dstFile, e);	}	if (LOG.isDebugEnabled()) {	if (calculateChecksum) {	} else {	
copied to 

public ReplicaHandler append(ExtendedBlock b, long newGS, long expectedBlockLen) throws IOException {	try(AutoCloseableLock lock = datasetLock.acquire()) {	if (newGS < b.getGenerationStamp()) {	throw new IOException("The new generation stamp " + newGS + " should be greater than the replica " + b + "'s generation stamp");	}	ReplicaInfo replicaInfo = getReplicaInfo(b);	
appending to 

if (v.getAvailable() < bytesReserved) {	throw new DiskOutOfSpaceException("Insufficient space for appending to " + replicaInfo);	}	File newBlkFile = new File(v.getRbwDir(bpid), replicaInfo.getBlockName());	File oldmeta = replicaInfo.getMetaFile();	ReplicaBeingWritten newReplicaInfo = new ReplicaBeingWritten( replicaInfo.getBlockId(), replicaInfo.getNumBytes(), newGS, v, newBlkFile.getParentFile(), Thread.currentThread(), bytesReserved);	byte[] lastChunkChecksum = v.loadLastPartialChunkChecksum( replicaInfo.getBlockFile(), replicaInfo.getMetaFile());	newReplicaInfo.setLastChecksumAndDataLen( replicaInfo.getNumBytes(), lastChunkChecksum);	File newmeta = newReplicaInfo.getMetaFile();	if (LOG.isDebugEnabled()) {	
renaming to 

}	if (LOG.isDebugEnabled()) {	LOG.debug("Renaming " + blkfile + " to " + newBlkFile + ", file length=" + blkfile.length());	}	try {	datanode.getFileIoProvider().renameTo( replicaInfo.getVolume(), blkfile, newBlkFile);	} catch (IOException e) {	try {	datanode.getFileIoProvider().renameTo( replicaInfo.getVolume(), newmeta, oldmeta);	} catch (IOException ex) {	
cannot move meta file back to the finalized directory 

public ReplicaHandler recoverAppend( ExtendedBlock b, long newGS, long expectedBlockLen) throws IOException {	
recover failed append to 

public Replica recoverClose(ExtendedBlock b, long newGS, long expectedBlockLen) throws IOException {	
recover failed close 

private void bumpReplicaGS(ReplicaInfo replicaInfo, long newGS) throws IOException {	long oldGS = replicaInfo.getGenerationStamp();	File oldmeta = replicaInfo.getMetaFile();	replicaInfo.setGenerationStamp(newGS);	File newmeta = replicaInfo.getMetaFile();	if (LOG.isDebugEnabled()) {	
renaming to 

public ReplicaHandler recoverRbw( ExtendedBlock b, long newGS, long minBytesRcvd, long maxBytesRcvd) throws IOException {	
recover rbw replica 

try {	try(AutoCloseableLock lock = datasetLock.acquire()) {	ReplicaInfo replicaInfo = getReplicaInfo(b.getBlockPoolId(), b.getBlockId());	if (replicaInfo.getState() != ReplicaState.RBW) {	throw new ReplicaNotFoundException( ReplicaNotFoundException.NON_RBW_REPLICA + replicaInfo);	}	ReplicaBeingWritten rbw = (ReplicaBeingWritten)replicaInfo;	if (!rbw.attemptToSetWriter(null, Thread.currentThread())) {	throw new MustStopExistingWriter(rbw);	}	
at recovering 

throw new ReplicaAlreadyExistsException("Block " + b + " already exists in state " + currentReplicaInfo.getState() + " and thus cannot be created.");	}	lastFoundReplicaInfo = currentReplicaInfo;	}	}	if (!isInPipeline) {	continue;	}	long writerStopMs = Time.monotonicNow() - startTimeMs;	if (writerStopMs > writerStopTimeoutMs) {	
unable to stop existing writer for block after miniseconds 

public void adjustCrcChannelPosition(ExtendedBlock b, ReplicaOutputStreams streams, int checksumSize) throws IOException {	FileOutputStream file = (FileOutputStream)streams.getChecksumOut();	FileChannel channel = file.getChannel();	long oldPos = channel.position();	long newPos = oldPos - checksumSize;	if (LOG.isDebugEnabled()) {	
changing meta file offset of block from to 

public void unfinalizeBlock(ExtendedBlock b) throws IOException {	try(AutoCloseableLock lock = datasetLock.acquire()) {	ReplicaInfo replicaInfo = volumeMap.get(b.getBlockPoolId(), b.getLocalBlock());	if (replicaInfo != null && replicaInfo.getState() == ReplicaState.TEMPORARY) {	volumeMap.remove(b.getBlockPoolId(), b.getLocalBlock());	if (delBlockFromDisk(replicaInfo.getBlockFile(), replicaInfo.getMetaFile(), b.getLocalBlock())) {	
block unfinalized and removed 

private boolean delBlockFromDisk(File blockFile, File metaFile, Block b) {	if (blockFile == null) {	
no file exists for block 

private boolean delBlockFromDisk(File blockFile, File metaFile, Block b) {	if (blockFile == null) {	return true;	}	if (!blockFile.delete()) {	
not able to delete the block file 

private boolean delBlockFromDisk(File blockFile, File metaFile, Block b) {	if (blockFile == null) {	return true;	}	if (!blockFile.delete()) {	return false;	} else {	if (metaFile != null && !metaFile.delete()) {	
not able to delete the meta block file 

try(AutoCloseableLock lock = datasetLock.acquire()) {	curVolumes = volumes.getVolumes();	for (FsVolumeSpi v : curVolumes) {	builders.put(v.getStorageID(), BlockListAsLongs.builder(maxDataLength));	}	Set<String> missingVolumesReported = new HashSet<>();	for (ReplicaInfo b : volumeMap.replicas(bpid)) {	String volStorageID = b.getVolume().getStorageID();	if (!builders.containsKey(volStorageID)) {	if (!missingVolumesReported.contains(volStorageID)) {	
storage volume missing for the replica block probably being removed 

private void invalidate(String bpid, Block[] invalidBlks, boolean async) throws IOException {	final List<String> errors = new ArrayList<String>();	for (int i = 0; i < invalidBlks.length; i++) {	final File f;	final FsVolumeImpl v;	try(AutoCloseableLock lock = datasetLock.acquire()) {	final ReplicaInfo info = volumeMap.get(bpid, invalidBlks[i]);	if (info == null) {	ReplicaInfo infoByBlockId = volumeMap.get(bpid, invalidBlks[i].getBlockId());	if (infoByBlockId == null) {	
failed to delete replica replicainfo not found 

continue;	}	File parent = f.getParentFile();	if (parent == null) {	errors.add("Failed to delete replica " + invalidBlks[i] +  ". Parent not found for file " + f);	continue;	}	ReplicaInfo removing = volumeMap.remove(bpid, invalidBlks[i]);	addDeletingBlock(bpid, removing.getBlockId());	if (LOG.isDebugEnabled()) {	
block file is to be deleted 

}	datanode.getShortCircuitRegistry().processBlockInvalidation( new ExtendedBlockId(invalidBlks[i].getBlockId(), bpid));	cacheManager.uncacheBlock(bpid, invalidBlks[i].getBlockId());	try {	if (async) {	asyncDiskService.deleteAsync(v.obtainReference(), f, FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()), new ExtendedBlock(bpid, invalidBlks[i]), dataStorage.getTrashDirectoryForBlockFile(bpid, f));	} else {	asyncDiskService.deleteSync(v.obtainReference(), f, FsDatasetUtil.getMetaFile(f, invalidBlks[i].getGenerationStamp()), new ExtendedBlock(bpid, invalidBlks[i]), dataStorage.getTrashDirectoryForBlockFile(bpid, f));	}	} catch (ClosedChannelException e) {	
volume is closed ignore the deletion task for block 

private void cacheBlock(String bpid, long blockId) {	FsVolumeImpl volume;	String blockFileName;	long length, genstamp;	Executor volumeExecutor;	try(AutoCloseableLock lock = datasetLock.acquire()) {	ReplicaInfo info = volumeMap.get(bpid, blockId);	boolean success = false;	try {	if (info == null) {	
failed to cache block with id pool replicainfo not found 

long length, genstamp;	Executor volumeExecutor;	try(AutoCloseableLock lock = datasetLock.acquire()) {	ReplicaInfo info = volumeMap.get(bpid, blockId);	boolean success = false;	try {	if (info == null) {	return;	}	if (info.getState() != ReplicaState.FINALIZED) {	
failed to cache block with id pool replica is not finalized it is in state 

try {	if (info == null) {	return;	}	if (info.getState() != ReplicaState.FINALIZED) {	return;	}	try {	volume = (FsVolumeImpl)info.getVolume();	if (volume == null) {	
failed to cache block with id pool volume not found 

}	if (info.getState() != ReplicaState.FINALIZED) {	return;	}	try {	volume = (FsVolumeImpl)info.getVolume();	if (volume == null) {	return;	}	} catch (ClassCastException e) {	
failed to cache block with id volume was not an instance of fsvolumeimpl 

}	try {	volume = (FsVolumeImpl)info.getVolume();	if (volume == null) {	return;	}	} catch (ClassCastException e) {	return;	}	if (volume.isTransientStorage()) {	
caching not supported on block with id since the volume is backed by ram 

if (asyncLazyPersistService != null) {	asyncLazyPersistService.shutdown();	}	if(volumes != null) {	volumes.shutdown();	}	if (lazyWriter != null) {	try {	lazyWriter.join();	} catch (InterruptedException ie) {	
fsdatasetimpl shutdown ignoring interruptedexception from lazywriter join 

if (memBlockInfo != null && memBlockInfo.getState() != ReplicaState.FINALIZED) {	return;	}	final FileIoProvider fileIoProvider = datanode.getFileIoProvider();	final boolean diskMetaFileExists = diskMetaFile != null && fileIoProvider.exists(vol, diskMetaFile);	final boolean diskFileExists = diskFile != null && fileIoProvider.exists(vol, diskFile);	final long diskGS = diskMetaFileExists ? Block.getGenerationStamp(diskMetaFile.getName()) : HdfsConstants.GRANDFATHER_GENERATION_STAMP;	if (!diskFileExists) {	if (memBlockInfo == null) {	if (diskMetaFileExists && fileIoProvider.delete(vol, diskMetaFile)) {	
deleted a metadata file without a block 

if (memBlockInfo == null) {	if (diskMetaFileExists && fileIoProvider.delete(vol, diskMetaFile)) {	}	return;	}	if (!memBlockInfo.getBlockFile().exists()) {	volumeMap.remove(bpid, blockId);	if (vol.isTransientStorage()) {	ramDiskReplicaTracker.discardReplica(bpid, blockId, true);	}	
removed block from memory with missing block file on the disk 

if (diskMetaFileExists && fileIoProvider.delete(vol, diskMetaFile)) {	}	return;	}	if (!memBlockInfo.getBlockFile().exists()) {	volumeMap.remove(bpid, blockId);	if (vol.isTransientStorage()) {	ramDiskReplicaTracker.discardReplica(bpid, blockId, true);	}	if (diskMetaFileExists && fileIoProvider.delete(vol, diskMetaFile)) {	
deleted a metadata file for the deleted block 

}	return;	}	if (memBlockInfo == null) {	ReplicaInfo diskBlockInfo = new FinalizedReplica(blockId, diskFile.length(), diskGS, vol, diskFile.getParentFile());	volumeMap.add(bpid, diskBlockInfo);	if (vol.isTransientStorage()) {	long lockedBytesReserved = cacheManager.reserve(diskBlockInfo.getNumBytes()) > 0 ? diskBlockInfo.getNumBytes() : 0;	ramDiskReplicaTracker.addReplica( bpid, blockId, (FsVolumeImpl) vol, lockedBytesReserved);	}	
added missing block to memory 

final boolean memFileExists = memFile != null && fileIoProvider.exists(vol, memFile);	if (memFileExists) {	if (memFile.compareTo(diskFile) != 0) {	if (diskMetaFile.exists()) {	if (fileIoProvider.exists(vol, memBlockInfo.getMetaFile())) {	ReplicaInfo diskBlockInfo = new FinalizedReplica( blockId, diskFile.length(), diskGS, vol, diskFile.getParentFile());	((FsVolumeImpl) vol).getBlockPoolSlice(bpid).resolveDuplicateReplicas( memBlockInfo, diskBlockInfo, volumeMap);	}	} else {	if (!fileIoProvider.delete(vol, diskFile)) {	
failed to delete will retry on next scan 

if (fileIoProvider.exists(vol, memBlockInfo.getMetaFile())) {	ReplicaInfo diskBlockInfo = new FinalizedReplica( blockId, diskFile.length(), diskGS, vol, diskFile.getParentFile());	((FsVolumeImpl) vol).getBlockPoolSlice(bpid).resolveDuplicateReplicas( memBlockInfo, diskBlockInfo, volumeMap);	}	} else {	if (!fileIoProvider.delete(vol, diskFile)) {	}	}	}	} else {	
block file in volumemap does not exist updating it to the file found during scan 

((FsVolumeImpl) vol).getBlockPoolSlice(bpid).resolveDuplicateReplicas( memBlockInfo, diskBlockInfo, volumeMap);	}	} else {	if (!fileIoProvider.delete(vol, diskFile)) {	}	}	}	} else {	memBlockInfo.setDir(diskFile.getParentFile());	memFile = diskFile;	
updating generation stamp for block from to 

} else {	memBlockInfo.setDir(diskFile.getParentFile());	memFile = diskFile;	memBlockInfo.setGenerationStamp(diskGS);	}	if (memBlockInfo.getGenerationStamp() != diskGS) {	File memMetaFile = FsDatasetUtil.getMetaFile(diskFile, memBlockInfo.getGenerationStamp());	if (fileIoProvider.exists(vol, memMetaFile)) {	String warningPrefix = "Metadata file in memory " + memMetaFile.getAbsolutePath() + " does not match file found by scan ";	if (!diskMetaFileExists) {	
null 

if (memBlockInfo.getGenerationStamp() != diskGS) {	File memMetaFile = FsDatasetUtil.getMetaFile(diskFile, memBlockInfo.getGenerationStamp());	if (fileIoProvider.exists(vol, memMetaFile)) {	String warningPrefix = "Metadata file in memory " + memMetaFile.getAbsolutePath() + " does not match file found by scan ";	if (!diskMetaFileExists) {	} else if (memMetaFile.compareTo(diskMetaFile) != 0) {	LOG.warn(warningPrefix + diskMetaFile.getAbsolutePath());	}	} else {	long gs = diskMetaFile != null && diskMetaFile.exists() && diskMetaFile.getParent().equals(memFile.getParent()) ? diskGS : HdfsConstants.GRANDFATHER_GENERATION_STAMP;	
updating generation stamp for block from to 

} else if (memMetaFile.compareTo(diskMetaFile) != 0) {	LOG.warn(warningPrefix + diskMetaFile.getAbsolutePath());	}	} else {	long gs = diskMetaFile != null && diskMetaFile.exists() && diskMetaFile.getParent().equals(memFile.getParent()) ? diskGS : HdfsConstants.GRANDFATHER_GENERATION_STAMP;	memBlockInfo.setGenerationStamp(gs);	}	}	if (memBlockInfo.getNumBytes() != memFile.length()) {	corruptBlock = new Block(memBlockInfo);	
updating size of block from to 

long gs = diskMetaFile != null && diskMetaFile.exists() && diskMetaFile.getParent().equals(memFile.getParent()) ? diskGS : HdfsConstants.GRANDFATHER_GENERATION_STAMP;	memBlockInfo.setGenerationStamp(gs);	}	}	if (memBlockInfo.getNumBytes() != memFile.length()) {	corruptBlock = new Block(memBlockInfo);	memBlockInfo.setNumBytes(memFile.length());	}	}	if (corruptBlock != null) {	
reporting the block as corrupt due to length mismatch 

}	if (memBlockInfo.getNumBytes() != memFile.length()) {	corruptBlock = new Block(memBlockInfo);	memBlockInfo.setNumBytes(memFile.length());	}	}	if (corruptBlock != null) {	try {	datanode.reportBadBlocks(new ExtendedBlock(bpid, corruptBlock), memBlockInfo.getVolume());	} catch (IOException e) {	
failed to repot bad block 

throw new IOException("THIS IS NOT SUPPOSED TO HAPPEN:" + " replica.getGenerationStamp() >= recoveryId = " + recoveryId + ", block=" + block + ", replica=" + replica);	}	final ReplicaUnderRecovery rur;	if (replica.getState() == ReplicaState.RUR) {	rur = (ReplicaUnderRecovery)replica;	if (rur.getRecoveryID() >= recoveryId) {	throw new RecoveryInProgressException( "rur.getRecoveryID() >= recoveryId = " + recoveryId + ", block=" + block + ", rur=" + rur);	}	final long oldRecoveryID = rur.getRecoveryID();	rur.setRecoveryID(recoveryId);	
initreplicarecovery update recovery id for from to 

rur = (ReplicaUnderRecovery)replica;	if (rur.getRecoveryID() >= recoveryId) {	throw new RecoveryInProgressException( "rur.getRecoveryID() >= recoveryId = " + recoveryId + ", block=" + block + ", rur=" + rur);	}	final long oldRecoveryID = rur.getRecoveryID();	rur.setRecoveryID(recoveryId);	}	else {	rur = new ReplicaUnderRecovery(replica, recoveryId);	map.add(bpid, rur);	
initreplicarecovery changing replica state for from to 

public void addBlockPool(String bpid, Configuration conf) throws IOException {	
adding block pool 

public void shutdownBlockPool(String bpid) {	try(AutoCloseableLock lock = datasetLock.acquire()) {	
removing block pool 

public void deleteBlockPool(String bpid, boolean force) throws IOException {	try(AutoCloseableLock lock = datasetLock.acquire()) {	List<FsVolumeImpl> curVolumes = volumes.getVolumes();	if (!force) {	for (FsVolumeImpl volume : curVolumes) {	try (FsVolumeReference ref = volume.obtainReference()) {	if (!volume.isBPDirEmpty(bpid)) {	
has some block files cannot delete unless forced 

public void onCompleteLazyPersist(String bpId, long blockId, long creationTime, File[] savedFiles, FsVolumeImpl targetVolume) {	try(AutoCloseableLock lock = datasetLock.acquire()) {	ramDiskReplicaTracker.recordEndLazyPersist(bpId, blockId, savedFiles);	targetVolume.incDfsUsedAndNumBlocks(bpId, savedFiles[0].length() + savedFiles[1].length());	datanode.getMetrics().incrRamDiskBlocksLazyPersisted();	datanode.getMetrics().incrRamDiskBytesLazyPersisted(savedFiles[1].length());	datanode.getMetrics().addRamDiskBlocksLazyPersistWindowMs( Time.monotonicNow() - creationTime);	if (LOG.isDebugEnabled()) {	
lazywriter finish persisting ramdisk block block pool id block id to block file and meta file on target volume 

public void onFailLazyPersist(String bpId, long blockId) {	RamDiskReplica block = null;	block = ramDiskReplicaTracker.getReplica(bpId, blockId);	if (block != null) {	
failed to save replica re enqueueing it 

try {	block = ramDiskReplicaTracker.dequeueNextReplicaToPersist();	if (block != null) {	try(AutoCloseableLock lock = datasetLock.acquire()) {	replicaInfo = volumeMap.get(block.getBlockPoolId(), block.getBlockId());	if (replicaInfo != null && replicaInfo.getVolume().isTransientStorage()) {	targetReference = volumes.getNextVolume( StorageType.DEFAULT, replicaInfo.getNumBytes());	targetVolume = (FsVolumeImpl) targetReference.getVolume();	ramDiskReplicaTracker.recordStartLazyPersist( block.getBlockPoolId(), block.getBlockId(), targetVolume);	if (LOG.isDebugEnabled()) {	
lazywriter start persisting ramdisk block block pool id block id on target volume 

targetVolume = (FsVolumeImpl) targetReference.getVolume();	ramDiskReplicaTracker.recordStartLazyPersist( block.getBlockPoolId(), block.getBlockId(), targetVolume);	if (LOG.isDebugEnabled()) {	}	asyncLazyPersistService.submitLazyPersistTask( block.getBlockPoolId(), block.getBlockId(), replicaInfo.getGenerationStamp(), block.getCreationTime(), replicaInfo.getMetaFile(), replicaInfo.getBlockFile(), targetReference);	}	}	}	succeeded = true;	} catch(IOException ioe) {	
exception saving replica 

if (LOG.isDebugEnabled()) {	}	asyncLazyPersistService.submitLazyPersistTask( block.getBlockPoolId(), block.getBlockId(), replicaInfo.getGenerationStamp(), block.getCreationTime(), replicaInfo.getMetaFile(), replicaInfo.getBlockFile(), targetReference);	}	}	}	succeeded = true;	} catch(IOException ioe) {	} finally {	if (!succeeded && block != null) {	
failed to save replica re enqueueing it 

public void evictBlocks(long bytesNeeded) throws IOException {	int iterations = 0;	final long cacheCapacity = cacheManager.getCacheCapacity();	while (iterations++ < MAX_BLOCK_EVICTIONS_PER_ITERATION && (cacheCapacity - cacheManager.getCacheUsed()) < bytesNeeded) {	RamDiskReplica replicaState = ramDiskReplicaTracker.getNextCandidateForEviction();	if (replicaState == null) {	break;	}	if (LOG.isDebugEnabled()) {	
evicting block 

public void run() {	int numSuccessiveFailures = 0;	while (fsRunning && shouldRun) {	try {	numSuccessiveFailures = saveNextReplica() ? 0 : (numSuccessiveFailures + 1);	if (numSuccessiveFailures >= ramDiskReplicaTracker.numReplicasNotPersisted()) {	Thread.sleep(checkpointerInterval * 1000);	numSuccessiveFailures = 0;	}	} catch (InterruptedException e) {	
lazywriter was interrupted exiting 

while (fsRunning && shouldRun) {	try {	numSuccessiveFailures = saveNextReplica() ? 0 : (numSuccessiveFailures + 1);	if (numSuccessiveFailures >= ramDiskReplicaTracker.numReplicasNotPersisted()) {	Thread.sleep(checkpointerInterval * 1000);	numSuccessiveFailures = 0;	}	} catch (InterruptedException e) {	break;	} catch (Exception e) {	
ignoring exception in lazywriter 

public void evictLazyPersistBlocks(long bytesNeeded) {	try {	((LazyWriter) lazyWriter.getRunnable()).evictBlocks(bytesNeeded);	} catch(IOException ioe) {	
ignoring exception 

========================= hadoop sample_7923 =========================

public Token<TimelineDelegationTokenIdentifier> selectToken(Text service, Collection<Token<? extends TokenIdentifier>> tokens) {	if (service == null) {	return null;	}	if (LOG.isDebugEnabled()) {	
looking for a token with service 

public Token<TimelineDelegationTokenIdentifier> selectToken(Text service, Collection<Token<? extends TokenIdentifier>> tokens) {	if (service == null) {	return null;	}	if (LOG.isDebugEnabled()) {	}	for (Token<? extends TokenIdentifier> token : tokens) {	if (LOG.isDebugEnabled()) {	
token kind is and the token s service name is 

========================= hadoop sample_2222 =========================

static Socket createSocketForPipeline(final DatanodeInfo first, final int length, final DFSClient client) throws IOException {	final DfsClientConf conf = client.getConf();	final String dnAddr = first.getXferAddr(conf.isConnectToDnViaHostname());	
connecting to datanode 

final InetSocketAddress isa = NetUtils.createSocketAddr(dnAddr);	final Socket sock = client.socketFactory.createSocket();	final int timeout = client.getDatanodeReadTimeout(length);	NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(), conf.getSocketTimeout());	sock.setTcpNoDelay(conf.getDataTransferTcpNoDelay());	sock.setSoTimeout(timeout);	sock.setKeepAlive(true);	if (conf.getSocketSendBufferSize() > 0) {	sock.setSendBufferSize(conf.getSocketSendBufferSize());	}	
send buf size 

private void initDataStreaming() {	this.setName("DataStreamer for file " + src + " block " + block);	if (LOG.isDebugEnabled()) {	
nodes storagetypes storageids 

private void endBlock() {	
closing old block 

public void run() {	long lastPacket = Time.monotonicNow();	TraceScope scope = null;	while (!streamerClosed && dfsClient.clientRunning) {	if (errorState.hasError() && response != null) {	try {	response.close();	response.join();	response = null;	} catch (InterruptedException  e) {	
caught exception 

final int halfSocketTimeout = dfsClient.getConf().getSocketTimeout()/2;	synchronized (dataQueue) {	long now = Time.monotonicNow();	while ((!shouldStop() && dataQueue.size() == 0 && (stage != BlockConstructionStage.DATA_STREAMING || stage == BlockConstructionStage.DATA_STREAMING && now - lastPacket < halfSocketTimeout)) || doSleep ) {	long timeout = halfSocketTimeout - (now-lastPacket);	timeout = timeout <= 0 ? 1000 : timeout;	timeout = (stage == BlockConstructionStage.DATA_STREAMING)? timeout : 1000;	try {	dataQueue.wait(timeout);	} catch (InterruptedException  e) {	
caught exception 

}	if (shouldStop()) {	continue;	}	if (dataQueue.isEmpty()) {	one = createHeartbeatPacket();	} else {	try {	backOffIfNecessary();	} catch (InterruptedException e) {	
caught exception 

}	one = dataQueue.getFirst();	SpanId[] parents = one.getTraceParents();	if (parents.length > 0) {	scope = dfsClient.getTracer(). newScope("dataStreamer", parents[0]);	scope.getSpan().setParents(parents);	}	}	}	if (stage == BlockConstructionStage.PIPELINE_SETUP_CREATE) {	
allocating new block 

if (parents.length > 0) {	scope = dfsClient.getTracer(). newScope("dataStreamer", parents[0]);	scope.getSpan().setParents(parents);	}	}	}	if (stage == BlockConstructionStage.PIPELINE_SETUP_CREATE) {	setPipeline(nextBlockOutputStream());	initDataStreaming();	} else if (stage == BlockConstructionStage.PIPELINE_SETUP_APPEND) {	
append to block 

long lastByteOffsetInBlock = one.getLastByteOffsetBlock();	if (lastByteOffsetInBlock > stat.getBlockSize()) {	throw new IOException("BlockSize " + stat.getBlockSize() + " is smaller than data size. " + " Offset of packet in block " + lastByteOffsetInBlock + " Aborting file " + src);	}	if (one.isLastPacketInBlock()) {	synchronized (dataQueue) {	while (!shouldStop() && ackQueue.size() != 0) {	try {	dataQueue.wait(1000);	} catch (InterruptedException  e) {	
caught exception 

scope.detach();	one.setTraceScope(scope);	}	scope = null;	dataQueue.removeFirst();	ackQueue.addLast(one);	packetSendTime.put(one.getSeqno(), Time.monotonicNow());	dataQueue.notifyAll();	}	}	
datastreamer block sending packet 

}	endBlock();	}	if (progress != null) { progress.progress(); }	if (artificialSlowdown != 0 && dfsClient.clientRunning) {	Thread.sleep(artificialSlowdown);	}	} catch (Throwable e) {	if (!errorState.isRestartingNode()) {	if (e instanceof QuotaExceededException) {	
datastreamer quota exception 

endBlock();	}	if (progress != null) { progress.progress(); }	if (artificialSlowdown != 0 && dfsClient.clientRunning) {	Thread.sleep(artificialSlowdown);	}	} catch (Throwable e) {	if (!errorState.isRestartingNode()) {	if (e instanceof QuotaExceededException) {	} else {	
datastreamer exception 

private void closeResponder() {	if (response != null) {	try {	response.close();	response.join();	} catch (InterruptedException  e) {	
caught exception 

if (ack.getSeqno() != DFSPacket.HEART_BEAT_SEQNO) {	Long begin = packetSendTime.get(ack.getSeqno());	if (begin != null) {	long duration = Time.monotonicNow() - begin;	if (duration > dfsclientSlowLogThresholdMs) {	LOG.info("Slow ReadProcessor read fields for block " + block + " took " + duration + "ms (threshold=" + dfsclientSlowLogThresholdMs + "ms); ack: " + ack + ", targets: " + Arrays.asList(targets));	}	}	}	if (LOG.isDebugEnabled()) {	
dfsclient 

}	} catch (Exception e) {	if (!responderClosed) {	lastException.set(e);	errorState.setError(true);	errorState.markFirstNodeIfNotMarked();	synchronized (dataQueue) {	dataQueue.notifyAll();	}	if (!errorState.isRestartingNode()) {	
exception for 

private boolean processDatanodeError() throws IOException {	if (!errorState.hasDatanodeError()) {	return false;	}	if (response != null) {	
error recovery for waiting for responder to exit 

if (response != null) {	return true;	}	closeStream();	synchronized (dataQueue) {	dataQueue.addAll(0, ackQueue);	ackQueue.clear();	packetSendTime.clear();	}	if (!errorState.isRestartingNode() && ++pipelineRecoveryCount > 5) {	
error recovering pipeline for writing already retried times for the same packet 

ArrayList<DatanodeInfo> exclude = new ArrayList<>(failed);	while (tried < 3) {	LocatedBlock lb;	lb = dfsClient.namenode.getAdditionalDatanode( src, stat.getFileId(), block.getCurrentBlock(), nodes, storageIDs, exclude.toArray(new DatanodeInfo[exclude.size()]), 1, dfsClient.clientName);	setPipeline(lb);	final int d;	try {	d = findNewDatanode(original);	} catch (IOException ioe) {	if (dfsClient.dtpReplaceDatanodeOnFailureReplication > 0 && nodes.length >= dfsClient.dtpReplaceDatanodeOnFailureReplication) {	
failed to find a new datanode to add to the write pipeline continue to write to the pipeline with nodes since it s no less than minimum replication configured by 

return;	}	throw ioe;	}	final DatanodeInfo src = original[tried % original.length];	final DatanodeInfo[] targets = {nodes[d]};	final StorageType[] targetStorageTypes = {storageTypes[d]};	try {	transfer(src, targets, targetStorageTypes, lb.getBlockToken());	} catch (IOException ioe) {	
error transferring data from to 

if (nodes.length <= 1) {	lastException.set(new IOException("All datanodes " + Arrays.toString(nodes) + " are bad. Aborting..."));	streamerClosed = true;	return false;	}	String reason = "bad.";	if (errorState.getRestartingNodeIndex() == badNodeIndex) {	reason = "restarting.";	restartingNodes.add(nodes[badNodeIndex]);	}	
error recovery for in pipeline datanode is 

private void handleDatanodeReplacement() throws IOException {	if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(stat.getReplication(), nodes, isAppend, isHflushed)) {	try {	addDatanode2ExistingPipeline();	} catch(IOException ioe) {	if (!dfsClient.dtpReplaceDatanodeOnFailure.isBestEffort()) {	throw ioe;	}	
failed to replace datanode continue with the remaining datanodes since is set to true 

DatanodeInfo[] excluded = getExcludedNodes();	lb = locateFollowingBlock( excluded.length > 0 ? excluded : null, oldBlock);	block.setCurrentBlock(lb.getBlock());	block.setNumBytes(0);	bytesSent = 0;	accessToken = lb.getBlockToken();	nodes = lb.getLocations();	storageTypes = lb.getStorageTypes();	success = createBlockOutputStream(nodes, storageTypes, 0L, false);	if (!success) {	
abandoning 

block.setNumBytes(0);	bytesSent = 0;	accessToken = lb.getBlockToken();	nodes = lb.getLocations();	storageTypes = lb.getStorageTypes();	success = createBlockOutputStream(nodes, storageTypes, 0L, false);	if (!success) {	dfsClient.namenode.abandonBlock(block.getCurrentBlock(), stat.getFileId(), src, dfsClient.clientName);	block.setCurrentBlock(null);	final DatanodeInfo badNode = nodes[errorState.getBadNodeIndex()];	
excluding datanode 

private boolean createBlockOutputStream(DatanodeInfo[] nodes, StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {	if (nodes.length == 0) {	
nodes are empty for write pipeline of 

String logInfo = "ack with firstBadLink as " + firstBadLink;	DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);	assert null == blockStream : "Previous blockStream unclosed";	blockStream = out;	result =  true;	errorState.reset();	failed.removeAll(restartingNodes);	restartingNodes.clear();	} catch (IOException ie) {	if (!errorState.isRestartingNode()) {	
exception in createblockoutputstream 

assert null == blockStream : "Previous blockStream unclosed";	blockStream = out;	result =  true;	errorState.reset();	failed.removeAll(restartingNodes);	restartingNodes.clear();	} catch (IOException ie) {	if (!errorState.isRestartingNode()) {	}	if (ie instanceof InvalidEncryptionKeyException && refetchEncryptionKey > 0) {	
will fetch a new encryption key and retry encryption key was invalid when connecting to 

if (favoredNodes == null) {	return null;	} else {	boolean[] pinnings = new boolean[nodes.length];	HashSet<String> favoredSet = new HashSet<>(Arrays.asList(favoredNodes));	for (int i = 0; i < nodes.length; i++) {	pinnings[i] = favoredSet.remove(nodes[i].getXferAddrWithHostname());	LOG.debug("{} was chosen by name node (favored={}).", nodes[i].getXferAddrWithHostname(), pinnings[i]);	}	if (!favoredSet.isEmpty()) {	
these favored nodes were specified but not chosen specified favored nodes 

} catch (RemoteException e) {	IOException ue = e.unwrapRemoteException(FileNotFoundException.class, AccessControlException.class, NSQuotaExceededException.class, DSQuotaExceededException.class, QuotaByStorageTypeExceededException.class, UnresolvedPathException.class);	if (ue != e) {	throw ue;	}	if (NotReplicatedYetException.class.getName(). equals(e.getClassName())) {	if (retries == 0) {	throw e;	} else {	--retries;	
exception while adding a block 

if (ue != e) {	throw ue;	}	if (NotReplicatedYetException.class.getName(). equals(e.getClassName())) {	if (retries == 0) {	throw e;	} else {	--retries;	long elapsed = Time.monotonicNow() - localstart;	if (elapsed > 5000) {	
waiting for replication for seconds 

}	if (NotReplicatedYetException.class.getName(). equals(e.getClassName())) {	if (retries == 0) {	throw e;	} else {	--retries;	long elapsed = Time.monotonicNow() - localstart;	if (elapsed > 5000) {	}	try {	
notreplicatedyetexception sleeping retries left 

throw e;	} else {	--retries;	long elapsed = Time.monotonicNow() - localstart;	if (elapsed > 5000) {	}	try {	Thread.sleep(sleeptime);	sleeptime *= 2;	} catch (InterruptedException ie) {	
caught exception 

private static LoadingCache<DatanodeInfo, DatanodeInfo> initExcludedNodes( long excludedNodesCacheExpiry) {	return CacheBuilder.newBuilder() .expireAfterWrite(excludedNodesCacheExpiry, TimeUnit.MILLISECONDS) .removalListener(new RemovalListener<DatanodeInfo, DatanodeInfo>() {	public void onRemoval( notification) {	
removing node from the excluded nodes list 

========================= hadoop sample_7020 =========================

public void testSeekReadClosedFile() throws Throwable {	instream = getFileSystem().open(smallSeekFile);	
stream is of type 

public void testNegativeSeek() throws Throwable {	instream = getFileSystem().open(smallSeekFile);	assertEquals(0, instream.getPos());	try {	instream.seek(-1);	long p = instream.getPos();	
seek to returned a position of 

try {	instream.readFully(TEST_FILE_LEN + 1, buffer);	fail("Expected an exception");	} catch (EOFException e) {	handleExpectedException(e);	} catch (IOException e) {	handleRelaxedException("readFully with an offset past EOF ", "EOFException", e);	}	try {	instream.readFully(TEST_FILE_LEN + 1, buffer, 0, 0);	
filesystem short circuits byte reads 

public void testReadFullyZeroBytebufferPastEOF() throws Throwable {	describe("readFully zero bytes from an offset past EOF");	assumeSupportsPositionedReadable();	instream = getFileSystem().open(smallSeekFile);	byte[] buffer = new byte[256];	try {	instream.readFully(TEST_FILE_LEN + 1, buffer, 0, 0);	
filesystem short circuits byte reads 

========================= hadoop sample_3219 =========================

file.setFileReplication(replication, iip.getLatestSnapshotId());	short targetReplication = (short) Math.max( replication, file.getPreferredBlockReplication());	if (oldBR > replication) {	fsd.updateCount(iip, 0L, size, oldBR, targetReplication, true);	}	for (BlockInfo b : file.getBlocks()) {	bm.setReplication(oldBR, targetReplication, b);	}	if (oldBR != -1) {	if (oldBR > targetReplication) {	
decreasing replication from to for 

short targetReplication = (short) Math.max( replication, file.getPreferredBlockReplication());	if (oldBR > replication) {	fsd.updateCount(iip, 0L, size, oldBR, targetReplication, true);	}	for (BlockInfo b : file.getBlocks()) {	bm.setReplication(oldBR, targetReplication, b);	}	if (oldBR != -1) {	if (oldBR > targetReplication) {	} else {	
increasing replication from to for 

========================= hadoop sample_8005 =========================

protected void setNextRollingTimeMillis(final long timestamp) {	this.nextRollingCheckMillis = timestamp;	
next rolling time for is 

public void init(final Configuration config) throws Exception {	
initializing rollingleveldb for 

protected synchronized void initHistoricalDBs() throws IOException {	Path rollingDBGlobPath = new Path(rollingDBPath, getName() + ".*");	FileStatus[] statuses = lfs.globStatus(rollingDBGlobPath);	for (FileStatus status : statuses) {	String dbName = FilenameUtils.getExtension(status.getPath().toString());	try {	Long dbStartTime = sdf.parse(dbName).getTime();	initRollingLevelDB(dbStartTime, status.getPath());	} catch (ParseException pe) {	
failed to initialize rolling leveldb for 

private void initRollingLevelDB(Long dbStartTime, Path rollingInstanceDBPath) {	if (rollingdbs.containsKey(dbStartTime)) {	return;	}	Options options = new Options();	options.createIfMissing(true);	options.cacheSize(conf.getLong( YarnConfiguration.TIMELINE_SERVICE_LEVELDB_READ_CACHE_SIZE, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_LEVELDB_READ_CACHE_SIZE));	options.maxOpenFiles(conf.getInt( YarnConfiguration.TIMELINE_SERVICE_LEVELDB_MAX_OPEN_FILES, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_LEVELDB_MAX_OPEN_FILES));	options.writeBufferSize(conf.getInt( YarnConfiguration.TIMELINE_SERVICE_LEVELDB_WRITE_BUFFER_SIZE, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_LEVELDB_WRITE_BUFFER_SIZE));	
initializing rolling leveldb instance for start time 

Options options = new Options();	options.createIfMissing(true);	options.cacheSize(conf.getLong( YarnConfiguration.TIMELINE_SERVICE_LEVELDB_READ_CACHE_SIZE, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_LEVELDB_READ_CACHE_SIZE));	options.maxOpenFiles(conf.getInt( YarnConfiguration.TIMELINE_SERVICE_LEVELDB_MAX_OPEN_FILES, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_LEVELDB_MAX_OPEN_FILES));	options.writeBufferSize(conf.getInt( YarnConfiguration.TIMELINE_SERVICE_LEVELDB_WRITE_BUFFER_SIZE, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_LEVELDB_WRITE_BUFFER_SIZE));	DB db = null;	try {	db = factory.open( new File(rollingInstanceDBPath.toUri().getPath()), options);	rollingdbs.put(dbStartTime, db);	String dbName = fdf.format(dbStartTime);	
added rolling leveldb instance to 

options.createIfMissing(true);	options.cacheSize(conf.getLong( YarnConfiguration.TIMELINE_SERVICE_LEVELDB_READ_CACHE_SIZE, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_LEVELDB_READ_CACHE_SIZE));	options.maxOpenFiles(conf.getInt( YarnConfiguration.TIMELINE_SERVICE_LEVELDB_MAX_OPEN_FILES, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_LEVELDB_MAX_OPEN_FILES));	options.writeBufferSize(conf.getInt( YarnConfiguration.TIMELINE_SERVICE_LEVELDB_WRITE_BUFFER_SIZE, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_LEVELDB_WRITE_BUFFER_SIZE));	DB db = null;	try {	db = factory.open( new File(rollingInstanceDBPath.toUri().getPath()), options);	rollingdbs.put(dbStartTime, db);	String dbName = fdf.format(dbStartTime);	} catch (IOException ioe) {	
failed to open rolling leveldb instance 

private void roll(long startTime) {	
rolling new db instance for 

private synchronized void scheduleOldDBsForEviction() {	long evictionThreshold = computeCurrentCheckMillis(currentTimeMillis() - getTimeToLive());	
scheduling dbs older than for eviction 

private synchronized void scheduleOldDBsForEviction() {	long evictionThreshold = computeCurrentCheckMillis(currentTimeMillis() - getTimeToLive());	Iterator<Entry<Long, DB>> iterator = rollingdbs.entrySet().iterator();	while (iterator.hasNext()) {	Entry<Long, DB> entry = iterator.next();	if (entry.getKey() < evictionThreshold) {	
scheduling eviction for 

public synchronized void evictOldDBs() {	
evicting dbs scheduled for eviction 

public synchronized void evictOldDBs() {	Iterator<Entry<Long, DB>> iterator = rollingdbsToEvict.entrySet() .iterator();	while (iterator.hasNext()) {	Entry<Long, DB> entry = iterator.next();	IOUtils.cleanupWithLogger(LOG, entry.getValue());	String dbName = fdf.format(entry.getKey());	Path path = new Path(rollingDBPath, getName() + "." + dbName);	try {	
removing old db directory contents in 

public synchronized void evictOldDBs() {	Iterator<Entry<Long, DB>> iterator = rollingdbsToEvict.entrySet() .iterator();	while (iterator.hasNext()) {	Entry<Long, DB> entry = iterator.next();	IOUtils.cleanupWithLogger(LOG, entry.getValue());	String dbName = fdf.format(entry.getKey());	Path path = new Path(rollingDBPath, getName() + "." + dbName);	try {	lfs.delete(path, true);	} catch (IOException ioe) {	
failed to evict old db 

========================= hadoop sample_2058 =========================

protected void serviceInit(Configuration conf) throws Exception {	long monitorInterval = conf.getLong(YarnConfiguration.RM_APPLICATION_MONITOR_INTERVAL_MS, YarnConfiguration.DEFAULT_RM_APPLICATION_MONITOR_INTERVAL_MS);	if (monitorInterval <= 0) {	monitorInterval = YarnConfiguration.DEFAULT_RM_APPLICATION_MONITOR_INTERVAL_MS;	}	setMonitorInterval(monitorInterval);	setExpireInterval(0);	setResetTimeOnStart(false);	
application lifelime monitor interval set to ms 

========================= hadoop sample_713 =========================

public final void buildListing(Path pathToListFile, DistCpOptions options) throws IOException {	validatePaths(options);	doBuildListing(pathToListFile, options);	Configuration config = getConf();	config.set(DistCpConstants.CONF_LABEL_LISTING_FILE_PATH, pathToListFile.toString());	config.setLong(DistCpConstants.CONF_LABEL_TOTAL_BYTES_TO_BE_COPIED, getBytesToCopy());	config.setLong(DistCpConstants.CONF_LABEL_TOTAL_NUMBER_OF_RECORDS, getNumberOfPaths());	validateFinalListing(pathToListFile, options);	
number of paths in the copy list 

DistCpUtils.checkFileSystemXAttrSupport(lastFs);	xAttrSupportCheckFsSet.add(lastFsUri);	}	}	lastKey.set(currentKey);	if (splitLargeFile) {	lastChunkOffset = lastFileStatus.getChunkOffset();	lastChunkLength = lastFileStatus.getChunkLength();	}	if (options.shouldUseDiff() && LOG.isDebugEnabled()) {	
copy list entry 

========================= hadoop sample_6298 =========================

public Map<String, Double> getOutliers() {	final Map<String, Double> stats = sendPacketDownstreamRollingAverages.getStats( MIN_OUTLIER_DETECTION_SAMPLES);	
datanodepeermetrics got stats 

========================= hadoop sample_7870 =========================

public boolean tryFence(HAServiceTarget target, String argsStr) throws BadFencingConfigurationException {	Args args = new Args(argsStr);	InetSocketAddress serviceAddr = target.getAddress();	String host = serviceAddr.getHostName();	Session session;	try {	session = createSession(serviceAddr.getHostName(), args);	} catch (JSchException e) {	
unable to create ssh session 

public boolean tryFence(HAServiceTarget target, String argsStr) throws BadFencingConfigurationException {	Args args = new Args(argsStr);	InetSocketAddress serviceAddr = target.getAddress();	String host = serviceAddr.getHostName();	Session session;	try {	session = createSession(serviceAddr.getHostName(), args);	} catch (JSchException e) {	return false;	}	
connecting to 

String host = serviceAddr.getHostName();	Session session;	try {	session = createSession(serviceAddr.getHostName(), args);	} catch (JSchException e) {	return false;	}	try {	session.connect(getSshConnectTimeout());	} catch (JSchException e) {	
unable to connect to as user 

try {	session = createSession(serviceAddr.getHostName(), args);	} catch (JSchException e) {	return false;	}	try {	session.connect(getSshConnectTimeout());	} catch (JSchException e) {	return false;	}	
connected to 

return false;	}	try {	session.connect(getSshConnectTimeout());	} catch (JSchException e) {	return false;	}	try {	return doFence(session, serviceAddr);	} catch (JSchException e) {	
unable to achieve fencing on remote host 

private boolean doFence(Session session, InetSocketAddress serviceAddr) throws JSchException {	int port = serviceAddr.getPort();	try {	
looking for process running on port 

private boolean doFence(Session session, InetSocketAddress serviceAddr) throws JSchException {	int port = serviceAddr.getPort();	try {	int rc = execCommand(session, "PATH=$PATH:/sbin:/usr/sbin fuser -v -k -n tcp " + port);	if (rc == 0) {	
successfully killed process that was listening on port 

private boolean doFence(Session session, InetSocketAddress serviceAddr) throws JSchException {	int port = serviceAddr.getPort();	try {	int rc = execCommand(session, "PATH=$PATH:/sbin:/usr/sbin fuser -v -k -n tcp " + port);	if (rc == 0) {	return true;	} else if (rc == 1) {	
indeterminate response from trying to kill service verifying whether it is running using nc 

private boolean doFence(Session session, InetSocketAddress serviceAddr) throws JSchException {	int port = serviceAddr.getPort();	try {	int rc = execCommand(session, "PATH=$PATH:/sbin:/usr/sbin fuser -v -k -n tcp " + port);	if (rc == 0) {	return true;	} else if (rc == 1) {	rc = execCommand(session, "nc -z " + serviceAddr.getHostName() + " " + serviceAddr.getPort());	if (rc == 0) {	
unable to fence it is running but we cannot kill it 

int port = serviceAddr.getPort();	try {	int rc = execCommand(session, "PATH=$PATH:/sbin:/usr/sbin fuser -v -k -n tcp " + port);	if (rc == 0) {	return true;	} else if (rc == 1) {	rc = execCommand(session, "nc -z " + serviceAddr.getHostName() + " " + serviceAddr.getPort());	if (rc == 0) {	return false;	} else {	
verified that the service is down 

return true;	} else if (rc == 1) {	rc = execCommand(session, "nc -z " + serviceAddr.getHostName() + " " + serviceAddr.getPort());	if (rc == 0) {	return false;	} else {	return true;	}	} else {	}	
rc 

rc = execCommand(session, "nc -z " + serviceAddr.getHostName() + " " + serviceAddr.getPort());	if (rc == 0) {	return false;	} else {	return true;	}	} else {	}	return rc == 0;	} catch (InterruptedException e) {	
interrupted while trying to fence via ssh 

return false;	} else {	return true;	}	} else {	}	return rc == 0;	} catch (InterruptedException e) {	return false;	} catch (IOException e) {	
unknown failure while trying to fence via ssh 

private int execCommand(Session session, String cmd) throws JSchException, InterruptedException, IOException {	
running cmd 

private static void cleanup(ChannelExec exec) {	if (exec != null) {	try {	exec.disconnect();	} catch (Throwable t) {	
couldn t disconnect ssh channel 

========================= hadoop sample_3999 =========================

public static <T extends RecordStore<?>> T newInstance( final Class<T> clazz, final StateStoreDriver driver) {	try {	Constructor<T> constructor = clazz.getConstructor(StateStoreDriver.class);	T recordStore = constructor.newInstance(driver);	return recordStore;	} catch (Exception e) {	
cannot create new instance for 

========================= hadoop sample_8286 =========================

public void reinit(Configuration conf) {	reset();	if (conf == null) {	return;	}	end(stream);	level = ZlibFactory.getCompressionLevel(conf);	strategy = ZlibFactory.getCompressionStrategy(conf);	stream = init(level.compressionLevel(), strategy.compressionStrategy(), windowBits.windowBits());	if(LOG.isDebugEnabled()) {	
reinit compressor with new compression configuration 

========================= hadoop sample_3833 =========================

private void testManualFailoverFailback(MiniDFSCluster cluster, Configuration conf, int nsIndex) throws Exception {	int nn0 = 2 * nsIndex, nn1 = 2 * nsIndex + 1;	cluster.transitionToActive(nn0);	
starting with nn active in namespace 

private void testManualFailoverFailback(MiniDFSCluster cluster, Configuration conf, int nsIndex) throws Exception {	int nn0 = 2 * nsIndex, nn1 = 2 * nsIndex + 1;	cluster.transitionToActive(nn0);	FileSystem fs = HATestUtil.configureFailoverFs(cluster, conf);	fs.mkdirs(TEST_DIR);	
failing over to nn in namespace 

private void testManualFailoverFailback(MiniDFSCluster cluster, Configuration conf, int nsIndex) throws Exception {	int nn0 = 2 * nsIndex, nn1 = 2 * nsIndex + 1;	cluster.transitionToActive(nn0);	FileSystem fs = HATestUtil.configureFailoverFs(cluster, conf);	fs.mkdirs(TEST_DIR);	cluster.transitionToStandby(nn0);	cluster.transitionToActive(nn1);	assertTrue(fs.exists(TEST_DIR));	DFSTestUtil.writeFile(fs, TEST_FILE_PATH, TEST_FILE_DATA);	
failing over to nn in namespace 

FileSystem fs = HATestUtil.configureFailoverFs(cluster, conf);	fs.mkdirs(TEST_DIR);	cluster.transitionToStandby(nn0);	cluster.transitionToActive(nn1);	assertTrue(fs.exists(TEST_DIR));	DFSTestUtil.writeFile(fs, TEST_FILE_PATH, TEST_FILE_DATA);	cluster.transitionToStandby(nn1);	cluster.transitionToActive(nn0);	assertTrue(fs.exists(TEST_DIR));	assertEquals(TEST_FILE_DATA, DFSTestUtil.readFile(fs, TEST_FILE_PATH));	
removing test file 

cluster.transitionToStandby(nn0);	cluster.transitionToActive(nn1);	assertTrue(fs.exists(TEST_DIR));	DFSTestUtil.writeFile(fs, TEST_FILE_PATH, TEST_FILE_DATA);	cluster.transitionToStandby(nn1);	cluster.transitionToActive(nn0);	assertTrue(fs.exists(TEST_DIR));	assertEquals(TEST_FILE_DATA, DFSTestUtil.readFile(fs, TEST_FILE_PATH));	fs.delete(TEST_DIR, true);	assertFalse(fs.exists(TEST_DIR));	
failing over to nn in namespace 

Configuration conf = new Configuration();	conf.setInt(DFSConfigKeys.DFS_HA_TAILEDITS_PERIOD_KEY, 1);	MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf) .nnTopology(MiniDFSNNTopology.simpleHATopology()) .numDataNodes(1) .build();	FSDataOutputStream stm = null;	FileSystem fs = HATestUtil.configureFailoverFs(cluster, conf);	NameNode nn0 = cluster.getNameNode(0);	NameNode nn1 = cluster.getNameNode(1);	try {	cluster.waitActive();	cluster.transitionToActive(0);	
starting with nn active 

stm = fs.create(TEST_FILE_PATH);	long nn0t0 = NameNodeAdapter.getLeaseRenewalTime(nn0, TEST_FILE_STR);	assertTrue(nn0t0 > 0);	long nn1t0 = NameNodeAdapter.getLeaseRenewalTime(nn1, TEST_FILE_STR);	assertEquals("Lease should not yet exist on nn1", -1, nn1t0);	Thread.sleep(5);	HATestUtil.waitForStandbyToCatchUp(nn0, nn1);	long nn1t1 = NameNodeAdapter.getLeaseRenewalTime(nn1, TEST_FILE_STR);	assertTrue("Lease should have been created on standby. Time was: " + nn1t1, nn1t1 > nn0t0);	Thread.sleep(5);	
failing over to nn 

Configuration conf = new Configuration();	conf.setBoolean( DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_ALWAYS_USE_KEY, true);	MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf) .nnTopology(MiniDFSNNTopology.simpleHATopology()) .numDataNodes(0) .build();	try {	cluster.waitActive();	cluster.transitionToActive(0);	NameNode nn1 = cluster.getNameNode(0);	NameNode nn2 = cluster.getNameNode(1);	String renewer = UserGroupInformation.getLoginUser().getUserName();	Token<DelegationTokenIdentifier> token = nn1.getRpcServer() .getDelegationToken(new Text(renewer));	
failing over to nn 

========================= hadoop sample_7466 =========================

public void onAMContainerFinished(ContainerId containerId) throws IOException {	
am container finished purging application attempt records 

public void onApplicationAttemptUnregistered(ApplicationAttemptId attemptId) throws IOException {	
application attempt unregistered purging app attempt records 

public void onApplicationCompleted(ApplicationId id) throws IOException {	
application completed purging application level records 

public void onContainerFinished(ContainerId id) throws IOException {	
container finished purging container level records 

public Future<Integer> purgeRecordsAsync(String path, String id, String persistencePolicyMatch, PurgePolicy purgePolicy, BackgroundCallback callback) {	
records under with id and policy 

========================= hadoop sample_2664 =========================

public void shutdown() {	if (hasWork()) {	
shutdown is called but there are still unprocessed work 

public void put(WorkRequest<T> workRequest) {	boolean isDone = false;	while (!isDone) {	try {	inputQueue.put(workRequest);	workCnt.incrementAndGet();	isDone = true;	} catch (InterruptedException ie) {	
could not put workrequest into inputqueue retrying 

public WorkReport<R> blockingTake() {	while (true) {	try {	WorkReport<R> report = outputQueue.take();	workCnt.decrementAndGet();	return report;	} catch (InterruptedException ie) {	
retrying in blockingtake 

public void run() {	while (true) {	WorkRequest<T> work;	try {	work = inputQueue.take();	} catch (InterruptedException e) {	
interrupted while waiting for requests from inputqueue 

} catch (InterruptedException e) {	return;	}	boolean isDone = false;	while (!isDone) {	try {	WorkReport<R> result = processor.processItem(work);	outputQueue.put(result);	isDone = true;	} catch (InterruptedException ie) {	
worker thread was interrupted while processing an item or putting into outputqueue retrying 

========================= hadoop sample_6277 =========================

final AtomicBoolean failed = new AtomicBoolean(false);	final DomainSocket[] socks = DomainSocket.socketpair();	Runnable reader = new Runnable() {	public void run() {	while (true) {	try {	int ret = socks[1].getInputStream().read();	if (ret == -1) return;	bytesRead.addAndGet(1);	} catch (IOException e) {	
reader error 

========================= hadoop sample_2849 =========================

public static void setup() throws IOException {	final Configuration conf = new Configuration();	final FileSystem fs = FileSystem.getLocal(conf).getRaw();	fs.delete(base, true);	final Random r = new Random();	final long seed = r.nextLong();	r.setSeed(seed);	
seed 

========================= hadoop sample_6109 =========================

public void testDeadlockShutdownBecomeActive() throws InterruptedException {	MockRM rm = new MockRMWithElector(conf, 1000);	rm.start();	
waiting for callback 

public void testDeadlockShutdownBecomeActive() throws InterruptedException {	MockRM rm = new MockRMWithElector(conf, 1000);	rm.start();	while (!callbackCalled.get());	
stopping rm 

public void testDeadlockShutdownBecomeActive() throws InterruptedException {	MockRM rm = new MockRMWithElector(conf, 1000);	rm.start();	while (!callbackCalled.get());	rm.stop();	
stopped rm 

protected EmbeddedElector createEmbeddedElector() {	return new ActiveStandbyElectorBasedElectorService(this) {	public void becomeActive() throws ServiceFailedException {	try {	callbackCalled.set(true);	
callback called sleeping now 

protected EmbeddedElector createEmbeddedElector() {	return new ActiveStandbyElectorBasedElectorService(this) {	public void becomeActive() throws ServiceFailedException {	try {	callbackCalled.set(true);	Thread.sleep(delayMs);	
sleep done 

========================= hadoop sample_601 =========================

BytesWritable bkey = new BytesWritable();	BytesWritable bval = new BytesWritable();	TaskAttemptContext context = MapReduceTestUtil.createDummyMapTaskAttemptContext(job.getConfiguration());	OutputFormat<BytesWritable, BytesWritable> outputFormat = new SequenceFileAsBinaryOutputFormat();	OutputCommitter committer = outputFormat.getOutputCommitter(context);	committer.setupJob(job);	RecordWriter<BytesWritable, BytesWritable> writer = outputFormat. getRecordWriter(context);	IntWritable iwritable = new IntWritable();	DoubleWritable dwritable = new DoubleWritable();	DataOutputBuffer outbuf = new DataOutputBuffer();	
creating data by sequencefileasbinaryoutputformat 

}	} finally {	writer.close(context);	}	committer.commitTask(context);	committer.commitJob(job);	InputFormat<IntWritable, DoubleWritable> iformat = new SequenceFileInputFormat<IntWritable, DoubleWritable>();	int count = 0;	r.setSeed(seed);	SequenceFileInputFormat.setInputPaths(job, outdir);	
reading data by sequencefileinputformat 

========================= hadoop sample_5660 =========================

status = Response.Status.BAD_REQUEST;	} else if (exception instanceof IllegalArgumentException) {	status = Response.Status.BAD_REQUEST;	} else {	status = Response.Status.INTERNAL_SERVER_ERROR;	log(status, throwable);	}	if (doAudit) {	KMSWebApp.getKMSAudit().error(KMSMDCFilter.getUgi(), KMSMDCFilter.getMethod(), KMSMDCFilter.getURL(), getOneLineMessage(exception));	}	
user request caused exception 

========================= hadoop sample_4421 =========================

private int runTool(String ... args) throws Exception {	errOutBytes.reset();	
running dfshaadmin 

private int runTool(String ... args) throws Exception {	errOutBytes.reset();	int ret = tool.run(args);	errOutput = new String(errOutBytes.toByteArray(), Charsets.UTF_8);	
output 

========================= hadoop sample_7124 =========================

public void render() {	int nestLevel = context().nestLevel();	
rendering 

========================= hadoop sample_2263 =========================

private long getQuorumTimeoutIncreaseMillis(long offset, int millis) {	long elapsed = quorumStopWatch.now(TimeUnit.MILLISECONDS);	long pauseTime = elapsed + offset;	if (pauseTime > (millis * WAIT_PROGRESS_INFO_THRESHOLD)) {	
pause detected while waiting for quorumcall response increasing timeout threshold by pause time of ms 

========================= hadoop sample_8394 =========================

public String exec(S3GuardTool cmd, String...args) throws Exception {	ByteArrayOutputStream buf = new ByteArrayOutputStream();	try {	exec(cmd, buf, args);	return buf.toString();	} catch (AssertionError e) {	throw e;	} catch (Exception e) {	
command failed 

protected void exec(S3GuardTool cmd, ByteArrayOutputStream buf, String...args) throws Exception {	
exec 

========================= hadoop sample_5928 =========================

public synchronized void synchronizePlan(Plan plan, boolean shouldReplan) {	String planQueueName = plan.getQueueName();	if (LOG.isDebugEnabled()) {	
running plan follower edit policy for plan 

String defReservationId = getReservationIdFromQueueName(planQueueName) + ReservationConstants.DEFAULT_QUEUE_SUFFIX;	String defReservationQueue = getReservationQueueName(planQueueName, defReservationId);	createDefaultReservationQueue(planQueueName, planQueue, defReservationId);	curReservationNames.add(defReservationId);	boolean shouldResize = false;	if (arePlanResourcesLessThanReservations(plan.getResourceCalculator(), clusterResources, planResources, reservedResources)) {	if (shouldReplan) {	try {	plan.getReplanner().plan(plan, null);	} catch (PlanningException e) {	
exception while trying to replan 

} else {	expired.add(reservationId);	}	}	cleanupExpiredQueues(planQueueName, plan.getMoveOnExpiry(), expired, defReservationQueue);	float totalAssignedCapacity = 0f;	if (currentReservations != null) {	try {	setQueueEntitlement(planQueueName, defReservationQueue, 0f, 1.0f);	} catch (YarnException e) {	
exception while trying to release default queue capacity for plan 

}	Resource capToAssign = res.getResourcesAtTime(now);	float targetCapacity = 0f;	if (planResources.getMemorySize() > 0 && planResources.getVirtualCores() > 0) {	if (shouldResize) {	capToAssign = calculateReservationToPlanProportion( plan.getResourceCalculator(), planResources, reservedResources, capToAssign);	}	targetCapacity = calculateReservationToPlanRatio(plan.getResourceCalculator(), clusterResources, planResources, capToAssign);	}	if (LOG.isDebugEnabled()) {	
assigning capacity of to queue with target capacity 

}	if (LOG.isDebugEnabled()) {	}	float maxCapacity = 1.0f;	if (res.containsGangs()) {	maxCapacity = targetCapacity;	}	try {	setQueueEntitlement(planQueueName, currResId, targetCapacity, maxCapacity);	} catch (YarnException e) {	
exception while trying to size reservation for plan 

}	try {	setQueueEntitlement(planQueueName, currResId, targetCapacity, maxCapacity);	} catch (YarnException e) {	}	totalAssignedCapacity += targetCapacity;	}	}	float defQCap = 1.0f - totalAssignedCapacity;	if (LOG.isDebugEnabled()) {	
planfollowereditpolicytask total plan capacity currreservation default queue capacity 

}	totalAssignedCapacity += targetCapacity;	}	}	float defQCap = 1.0f - totalAssignedCapacity;	if (LOG.isDebugEnabled()) {	}	try {	setQueueEntitlement(planQueueName, defReservationQueue, defQCap, 1.0f);	} catch (YarnException e) {	
exception while trying to reclaim default queue capacity for plan 

float defQCap = 1.0f - totalAssignedCapacity;	if (LOG.isDebugEnabled()) {	}	try {	setQueueEntitlement(planQueueName, defReservationQueue, defQCap, 1.0f);	} catch (YarnException e) {	}	try {	plan.archiveCompletedReservations(now);	} catch (PlanningException e) {	
exception in archiving completed reservations 

if (LOG.isDebugEnabled()) {	}	try {	setQueueEntitlement(planQueueName, defReservationQueue, defQCap, 1.0f);	} catch (YarnException e) {	}	try {	plan.archiveCompletedReservations(now);	} catch (PlanningException e) {	}	
finished iteration of plan follower edit policy for plan 

protected void cleanupExpiredQueues(String planQueueName, boolean shouldMove, Set<String> toRemove, String defReservationQueue) {	for (String expiredReservationId : toRemove) {	try {	String expiredReservation = getReservationQueueName(planQueueName, expiredReservationId);	setQueueEntitlement(planQueueName, expiredReservation, 0.0f, 0.0f);	if (shouldMove) {	moveAppsInQueueSync(expiredReservation, defReservationQueue);	}	if (scheduler.getAppsInQueue(expiredReservation).size() > 0) {	scheduler.killAllAppsInQueue(expiredReservation);	
killing applications in queue 

try {	String expiredReservation = getReservationQueueName(planQueueName, expiredReservationId);	setQueueEntitlement(planQueueName, expiredReservation, 0.0f, 0.0f);	if (shouldMove) {	moveAppsInQueueSync(expiredReservation, defReservationQueue);	}	if (scheduler.getAppsInQueue(expiredReservation).size() > 0) {	scheduler.killAllAppsInQueue(expiredReservation);	} else {	scheduler.removeQueue(expiredReservation);	
queue removed 

setQueueEntitlement(planQueueName, expiredReservation, 0.0f, 0.0f);	if (shouldMove) {	moveAppsInQueueSync(expiredReservation, defReservationQueue);	}	if (scheduler.getAppsInQueue(expiredReservation).size() > 0) {	scheduler.killAllAppsInQueue(expiredReservation);	} else {	scheduler.removeQueue(expiredReservation);	}	} catch (YarnException e) {	
exception while trying to expire reservation 

private void moveAppsInQueueSync(String expiredReservation, String defReservationQueue) {	List<ApplicationAttemptId> activeApps = scheduler.getAppsInQueue(expiredReservation);	if (activeApps.isEmpty()) {	return;	}	for (ApplicationAttemptId app : activeApps) {	try {	scheduler.moveApplication(app.getApplicationId(), defReservationQueue);	} catch (YarnException e) {	
encountered unexpected error during migration of application from reservation 

========================= hadoop sample_1068 =========================

public void configure(JobConf jobConf) {	int numberOfThreads = jobConf.getInt(MultithreadedMapper.NUM_THREADS, 10);	if (LOG.isDebugEnabled()) {	
configuring jobconf to use threads 

try {	K1 key = input.createKey();	V1 value = input.createValue();	while (input.next(key, value)) {	executorService.execute(new MapperInvokeRunable(key, value, output, reporter));	checkForExceptionsFromProcessingThreads();	key = input.createKey();	value = input.createValue();	}	if (LOG.isDebugEnabled()) {	
finished dispatching all mappper map calls job 

checkForExceptionsFromProcessingThreads();	key = input.createKey();	value = input.createValue();	}	if (LOG.isDebugEnabled()) {	}	executorService.shutdown();	try {	while (!executorService.awaitTermination(100, TimeUnit.MILLISECONDS)) {	if (LOG.isDebugEnabled()) {	
awaiting all running mappper map calls to finish job 

========================= hadoop sample_4790 =========================

public void testZlibSequenceFile() throws Exception {	
testing sequencefile with defaultcodec 

public void testZlibSequenceFile() throws Exception {	compressedSeqFileTest(new DefaultCodec());	
successfully tested sequencefile with defaultcodec 

private void writeTest(FileSystem fs, int count, int seed, Path file, CompressionType compressionType, CompressionCodec codec) throws IOException {	fs.delete(file, true);	
creating records with compression 

private void readTest(FileSystem fs, int count, int seed, Path file) throws IOException {	
reading records 

if ((i%2) == 0) {	reader.next(k);	reader.getCurrentValue(v);	} else {	reader.next(k, v);	}	if (!k.equals(key)) throw new RuntimeException("wrong key at " + i);	if (!v.equals(value)) throw new RuntimeException("wrong value at " + i);	}	} catch (IOException ioe) {	
problem on row 

}	} catch (IOException ioe) {	LOG.info("Expected key = " + key);	LOG.info("Expected len = " + key.getLength());	LOG.info("Actual key = " + k);	LOG.info("Actual len = " + k.getLength());	LOG.info("Expected value = " + value);	LOG.info("Expected len = " + value.getLength());	LOG.info("Actual value = " + v);	LOG.info("Actual len = " + v.getLength());	
key equals 

}	} catch (IOException ioe) {	LOG.info("Expected key = " + key);	LOG.info("Expected len = " + key.getLength());	LOG.info("Actual key = " + k);	LOG.info("Actual len = " + k.getLength());	LOG.info("Expected value = " + value);	LOG.info("Expected len = " + value.getLength());	LOG.info("Actual value = " + v);	LOG.info("Actual len = " + v.getLength());	
value equals 

private void sortTest(FileSystem fs, int count, int megabytes, int factor, boolean fast, Path file) throws IOException {	fs.delete(new Path(file+".sorted"), true);	SequenceFile.Sorter sorter = newSorter(fs, fast, megabytes, factor);	
sorting records 

private void sortTest(FileSystem fs, int count, int megabytes, int factor, boolean fast, Path file) throws IOException {	fs.delete(new Path(file+".sorted"), true);	SequenceFile.Sorter sorter = newSorter(fs, fast, megabytes, factor);	sorter.sort(file, file.suffix(".sorted"));	
done sorting debug 

private void checkSort(FileSystem fs, int count, int seed, Path file) throws IOException {	
sorting records in memory for debug 

private void checkSort(FileSystem fs, int count, int seed, Path file) throws IOException {	RandomDatum.Generator generator = new RandomDatum.Generator(seed);	SortedMap<RandomDatum, RandomDatum> map = new TreeMap<RandomDatum, RandomDatum>();	for (int i = 0; i < count; i++) {	generator.next();	RandomDatum key = generator.getKey();	RandomDatum value = generator.getValue();	map.put(key, value);	}	
checking order of records 

SequenceFile.Reader reader = new SequenceFile.Reader(fs, file.suffix(".sorted"), conf);	for (int i = 0; i < count; i++) {	Map.Entry<RandomDatum, RandomDatum> entry = iterator.next();	RandomDatum key = entry.getKey();	RandomDatum value = entry.getValue();	reader.next(k, v);	if (!k.equals(key)) throw new RuntimeException("wrong key at " + i);	if (!v.equals(value)) throw new RuntimeException("wrong value at " + i);	}	reader.close();	
sucessfully checked records 

private void mergeTest(FileSystem fs, int count, int seed, Path file, CompressionType compressionType, boolean fast, int factor, int megabytes) throws IOException {	
creating files with records 

}	RandomDatum.Generator generator = new RandomDatum.Generator(seed);	for (int i = 0; i < count; i++) {	generator.next();	RandomDatum key = generator.getKey();	RandomDatum value = generator.getValue();	writers[i%factor].append(key, value);	}	for (int i = 0; i < factor; i++) writers[i].close();	for (int i = 0; i < factor; i++) {	
sorting file with records 

for (int i = 0; i < count; i++) {	generator.next();	RandomDatum key = generator.getKey();	RandomDatum value = generator.getValue();	writers[i%factor].append(key, value);	}	for (int i = 0; i < factor; i++) writers[i].close();	for (int i = 0; i < factor; i++) {	newSorter(fs, fast, megabytes, factor).sort(names[i], sortedNames[i]);	}	
merging files with debug 

public void testSequenceFileMetadata() throws Exception {	
testing sequencefile with metadata 

SequenceFile.Metadata theMetadata = new SequenceFile.Metadata();	theMetadata.set(new Text("name_1"), new Text("value_1"));	theMetadata.set(new Text("name_2"), new Text("value_2"));	theMetadata.set(new Text("name_3"), new Text("value_3"));	theMetadata.set(new Text("name_4"), new Text("value_4"));	int seed = new Random().nextInt();	try {	writeMetadataTest(fs, count, seed, file, CompressionType.NONE, null, theMetadata);	SequenceFile.Metadata aMetadata = readMetadata(fs, file);	if (!theMetadata.equals(aMetadata)) {	
the original metadata 

SequenceFile.Metadata theMetadata = new SequenceFile.Metadata();	theMetadata.set(new Text("name_1"), new Text("value_1"));	theMetadata.set(new Text("name_2"), new Text("value_2"));	theMetadata.set(new Text("name_3"), new Text("value_3"));	theMetadata.set(new Text("name_4"), new Text("value_4"));	int seed = new Random().nextInt();	try {	writeMetadataTest(fs, count, seed, file, CompressionType.NONE, null, theMetadata);	SequenceFile.Metadata aMetadata = readMetadata(fs, file);	if (!theMetadata.equals(aMetadata)) {	
the retrieved metadata 

int seed = new Random().nextInt();	try {	writeMetadataTest(fs, count, seed, file, CompressionType.NONE, null, theMetadata);	SequenceFile.Metadata aMetadata = readMetadata(fs, file);	if (!theMetadata.equals(aMetadata)) {	throw new RuntimeException("metadata not match:  " + 1);	}	writeMetadataTest(fs, count, seed, recordCompressedFile, CompressionType.RECORD, codec, theMetadata);	aMetadata = readMetadata(fs, recordCompressedFile);	if (!theMetadata.equals(aMetadata)) {	
the original metadata 

int seed = new Random().nextInt();	try {	writeMetadataTest(fs, count, seed, file, CompressionType.NONE, null, theMetadata);	SequenceFile.Metadata aMetadata = readMetadata(fs, file);	if (!theMetadata.equals(aMetadata)) {	throw new RuntimeException("metadata not match:  " + 1);	}	writeMetadataTest(fs, count, seed, recordCompressedFile, CompressionType.RECORD, codec, theMetadata);	aMetadata = readMetadata(fs, recordCompressedFile);	if (!theMetadata.equals(aMetadata)) {	
the retrieved metadata 

throw new RuntimeException("metadata not match:  " + 1);	}	writeMetadataTest(fs, count, seed, recordCompressedFile, CompressionType.RECORD, codec, theMetadata);	aMetadata = readMetadata(fs, recordCompressedFile);	if (!theMetadata.equals(aMetadata)) {	throw new RuntimeException("metadata not match:  " + 2);	}	writeMetadataTest(fs, count, seed, blockCompressedFile, CompressionType.BLOCK, codec, theMetadata);	aMetadata =readMetadata(fs, blockCompressedFile);	if (!theMetadata.equals(aMetadata)) {	
the original metadata 

throw new RuntimeException("metadata not match:  " + 1);	}	writeMetadataTest(fs, count, seed, recordCompressedFile, CompressionType.RECORD, codec, theMetadata);	aMetadata = readMetadata(fs, recordCompressedFile);	if (!theMetadata.equals(aMetadata)) {	throw new RuntimeException("metadata not match:  " + 2);	}	writeMetadataTest(fs, count, seed, blockCompressedFile, CompressionType.BLOCK, codec, theMetadata);	aMetadata =readMetadata(fs, blockCompressedFile);	if (!theMetadata.equals(aMetadata)) {	
the retrieved metadata 

throw new RuntimeException("metadata not match:  " + 2);	}	writeMetadataTest(fs, count, seed, blockCompressedFile, CompressionType.BLOCK, codec, theMetadata);	aMetadata =readMetadata(fs, blockCompressedFile);	if (!theMetadata.equals(aMetadata)) {	throw new RuntimeException("metadata not match:  " + 3);	}	sortMetadataTest(fs, file, sortedFile, theMetadata);	aMetadata = readMetadata(fs, recordCompressedFile);	if (!theMetadata.equals(aMetadata)) {	
the original metadata 

throw new RuntimeException("metadata not match:  " + 2);	}	writeMetadataTest(fs, count, seed, blockCompressedFile, CompressionType.BLOCK, codec, theMetadata);	aMetadata =readMetadata(fs, blockCompressedFile);	if (!theMetadata.equals(aMetadata)) {	throw new RuntimeException("metadata not match:  " + 3);	}	sortMetadataTest(fs, file, sortedFile, theMetadata);	aMetadata = readMetadata(fs, recordCompressedFile);	if (!theMetadata.equals(aMetadata)) {	
the retrieved metadata 

throw new RuntimeException("metadata not match:  " + 3);	}	sortMetadataTest(fs, file, sortedFile, theMetadata);	aMetadata = readMetadata(fs, recordCompressedFile);	if (!theMetadata.equals(aMetadata)) {	throw new RuntimeException("metadata not match:  " + 4);	}	} finally {	fs.close();	}	
successfully tested sequencefile with metadata 

private SequenceFile.Metadata readMetadata(FileSystem fs, Path file) throws IOException {	
reading file 

private void writeMetadataTest(FileSystem fs, int count, int seed, Path file, CompressionType compressionType, CompressionCodec codec, SequenceFile.Metadata metadata) throws IOException {	fs.delete(file, true);	
creating records with metadata and with compression 

private void sortMetadataTest(FileSystem fs, Path unsortedFile, Path sortedFile, SequenceFile.Metadata metadata) throws IOException {	fs.delete(sortedFile, true);	
sorting to 

========================= hadoop sample_3055 =========================

public void rejoinElection() {	try {	closeLeaderLatch();	Thread.sleep(1000);	initAndStartLeaderLatch();	} catch (Exception e) {	
fail to re join election 

public void isLeader() {	
is elected leader transitioning to active 

public void isLeader() {	try {	rm.getRMContext().getRMAdminService() .transitionToActive( new HAServiceProtocol.StateChangeRequestInfo( HAServiceProtocol.RequestSource.REQUEST_BY_ZKFC));	} catch (Exception e) {	
failed to transition to active giving up leadership 

public void notLeader() {	
relinquish leadership 

public void notLeader() {	try {	rm.getRMContext().getRMAdminService() .transitionToStandby( new HAServiceProtocol.StateChangeRequestInfo( HAServiceProtocol.RequestSource.REQUEST_BY_ZKFC));	} catch (Exception e) {	
did not transition to standby successfully 

========================= hadoop sample_1087 =========================

protected TestableZooKeeper createClient(CountdownWatcher watcher, String hp, int timeout) throws IOException, InterruptedException {	watcher.reset();	TestableZooKeeper zk = new TestableZooKeeper(hp, timeout, watcher);	if (!watcher.clientConnected.await(timeout, TimeUnit.MILLISECONDS)) {	Assert.fail("Unable to connect to server");	}	synchronized(this) {	if (!allClientsSetup) {	
allclients never setup 

public static String send4LetterWord(String host, int port, String cmd) throws IOException {	
connecting to 

public static boolean waitForServerUp(String hp, long timeout) {	long start = Time.now();	while (true) {	try {	HostPort hpobj = parseHostPortList(hp).get(0);	String result = send4LetterWord(hpobj.host, hpobj.port, "stat");	if (result.startsWith("Zookeeper version:") && !result.contains("READ-ONLY")) {	return true;	}	} catch (IOException e) {	
server not up 

if (factory != null) {	ZKDatabase zkDb;	{	ZooKeeperServer zs = getServer(factory);	zkDb = zs.getZKDatabase();	}	factory.shutdown();	try {	zkDb.close();	} catch (IOException ie) {	
error closing logs 

public void setUp() throws Exception {	BASETEST.mkdirs();	setupTestEnv();	setUpAll();	tmpDir = createTmpDir(BASETEST);	startServer();	
client test setup finished 

protected void startServer() throws Exception {	
starting server 

protected void stopServer() throws Exception {	
stopping server 

protected void tearDownAll() throws Exception {	synchronized (this) {	if (allClients != null) for (ZooKeeper zk : allClients) {	try {	if (zk != null) zk.close();	} catch (InterruptedException e) {	
ignoring interrupt 

public void tearDown() throws Exception {	
teardown starting 

========================= hadoop sample_3110 =========================

public void testEmptyBasic() {	
test empty basic 

public void testEmptyBasic() {	Iterator<Integer> iter = set.iterator();	assertFalse(iter.hasNext());	assertEquals(0, set.size());	assertTrue(set.isEmpty());	assertNull(set.pollFirst());	assertEquals(0, set.pollAll().size());	assertEquals(0, set.pollN(10).size());	
test empty done 

public void testOneElementBasic() {	
test one element basic 

public void testOneElementBasic() {	set.add(list.get(0));	assertEquals(1, set.size());	assertFalse(set.isEmpty());	Iterator<Integer> iter = set.iterator();	assertTrue(iter.hasNext());	assertEquals(list.get(0), iter.next());	assertFalse(iter.hasNext());	
test one element basic done 

public void testMultiBasic() {	
test multi element basic 

}	for (Integer i : list) {	assertTrue(set.contains(i));	}	Iterator<Integer> iter = set.iterator();	int num = 0;	while (iter.hasNext()) {	assertEquals(list.get(num++), iter.next());	}	assertEquals(list.size(), num);	
test multi element basic done 

public void testRemoveOne() {	
test remove one 

assertEquals(0, set.size());	Iterator<Integer> iter = set.iterator();	assertFalse(iter.hasNext());	assertNull(set.pollFirst());	assertEquals(0, set.pollAll().size());	assertEquals(0, set.pollN(10).size());	assertTrue(set.add(list.get(0)));	assertEquals(1, set.size());	iter = set.iterator();	assertTrue(iter.hasNext());	
test remove one done 

public void testRemoveMulti() {	
test remove multi 

}	for (int i = NUM / 2; i < NUM; i++) {	assertTrue(set.contains(list.get(i)));	}	Iterator<Integer> iter = set.iterator();	int num = NUM / 2;	while (iter.hasNext()) {	assertEquals(list.get(num++), iter.next());	}	assertEquals(num, NUM);	
test remove multi done 

public void testRemoveAll() {	
test remove all 

}	for (int i = 0; i < NUM; i++) {	assertTrue(set.remove(list.get(i)));	}	for (int i = 0; i < NUM; i++) {	assertFalse(set.contains(list.get(i)));	}	Iterator<Integer> iter = set.iterator();	assertFalse(iter.hasNext());	assertTrue(set.isEmpty());	
test remove all done 

public void testPollOneElement() {	
test poll one element 

public void testPollOneElement() {	set.add(list.get(0));	assertEquals(list.get(0), set.pollFirst());	assertNull(set.pollFirst());	
test poll one element done 

public void testPollMulti() {	
test poll multi 

}	assertEquals(NUM, set.size());	for (int i = NUM / 2; i < NUM; i++) {	assertEquals(list.get(i), set.pollFirst());	}	for (int i = 0; i < NUM / 2; i++) {	assertEquals(list.get(i), set.pollFirst());	}	assertEquals(0, set.size());	assertTrue(set.isEmpty());	
test poll multi done 

public void testPollAll() {	
test poll all 

assertTrue(set.add(i));	}	while (set.pollFirst() != null);	assertEquals(0, set.size());	assertTrue(set.isEmpty());	for (int i = 0; i < NUM; i++) {	assertFalse(set.contains(list.get(i)));	}	Iterator<Integer> iter = set.iterator();	assertFalse(iter.hasNext());	
test poll all done 

public void testPollNOne() {	
test polln one 

public void testPollNOne() {	set.add(list.get(0));	List<Integer> l = set.pollN(10);	assertEquals(1, l.size());	assertEquals(list.get(0), l.get(0));	
test polln one done 

public void testPollNMulti() {	
test polln multi 

for (int i = 0; i < 10; i++) {	assertEquals(list.get(i), l.get(i));	}	l = set.pollN(1000);	assertEquals(NUM - 10, l.size());	for (int i = 10; i < NUM; i++) {	assertEquals(list.get(i), l.get(i - 10));	}	assertTrue(set.isEmpty());	assertEquals(0, set.size());	
test polln multi done 

public void testClear() {	
test clear 

set.clear();	assertEquals(0, set.size());	assertTrue(set.isEmpty());	bkmrkIt = set.getBookmark();	assertFalse(bkmrkIt.hasNext());	assertEquals(0, set.pollAll().size());	assertEquals(0, set.pollN(10).size());	assertNull(set.pollFirst());	Iterator<Integer> iter = set.iterator();	assertFalse(iter.hasNext());	
test clear done 

public void testOther() {	
test other 

assertEquals(NUM, array.length);	for (int i = 0; i < array.length; i++) {	assertTrue(list.contains(array[i]));	}	assertEquals(NUM, set.size());	Object[] array2 = set.toArray();	assertEquals(NUM, array2.length);	for (int i = 0; i < array2.length; i++) {	assertTrue(list.contains(array2[i]));	}	
test capacity done 

public void testGetBookmarkReturnsBookmarkIterator() {	
test getbookmark returns proper iterator 

public void testBookmarkAdvancesOnRemoveOfSameElement() {	
test that the bookmark advances if we remove its element 

public void testBookmarkSetToHeadOnAddToEmpty() {	
test bookmark is set after adding to previously empty set 

========================= hadoop sample_7153 =========================

public void initialize(URI fsuri, Configuration conf) throws IOException {	super.initialize(fsuri, conf);	setConf(conf);	if (store == null) {	store = new SwiftNativeFileSystemStore();	}	this.uri = fsuri;	String username = System.getProperty("user.name");	this.workingDir = new Path("/user", username) .makeQualified(uri, new Path(username));	if (LOG.isDebugEnabled()) {	
initializing swiftnativefilesystem against uri and working dir 

setConf(conf);	if (store == null) {	store = new SwiftNativeFileSystemStore();	}	this.uri = fsuri;	String username = System.getProperty("user.name");	this.workingDir = new Path("/user", username) .makeQualified(uri, new Path(username));	if (LOG.isDebugEnabled()) {	}	store.initialize(uri, conf);	
swiftfilesystem initialized 

public void setWorkingDirectory(Path dir) {	workingDir = makeAbsolute(dir);	if (LOG.isDebugEnabled()) {	
swiftfilesystem setworkingdirectory to 

for (FileStatus fileStatus : listOfFileBlocks) {	if (SwiftObjectPath.fromPath(uri, fileStatus.getPath()) .equals(SwiftObjectPath.fromPath(uri, file.getPath()))) {	continue;	}	locations.addAll(store.getObjectLocation(fileStatus.getPath()));	}	} else {	locations = store.getObjectLocation(file.getPath());	}	if (locations.isEmpty()) {	
no locations returned for 

public boolean mkdirs(Path path, FsPermission permission) throws IOException {	if (LOG.isDebugEnabled()) {	
swiftfilesystem mkdirs 

boolean shouldCreate;	if (isRoot(directory)) {	return false;	}	try {	fileStatus = getFileStatus(directory);	if (!SwiftUtils.isDirectory(fileStatus)) {	throw new ParentNotDirectoryException( String.format("%s: can't mkdir since it exists and is not a directory: %s", directory, fileStatus));	} else {	if (LOG.isDebugEnabled()) {	
skipping mkdir as it exists already 

private void forceMkdir(Path absolutePath) throws IOException {	if (LOG.isDebugEnabled()) {	
making dir in swift 

public FileStatus[] listStatus(Path path) throws IOException {	if (LOG.isDebugEnabled()) {	
swiftfilesystem liststatus for 

public FSDataOutputStream append(Path f, int bufferSize, Progressable progress) throws IOException {	
swiftfilesystem append 

public FSDataOutputStream create(Path file, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress) throws IOException {	
swiftfilesystem create 

FileStatus fileStatus = null;	Path absolutePath = makeAbsolute(file);	try {	fileStatus = getFileStatus(absolutePath);	} catch (FileNotFoundException e) {	}	if (fileStatus != null) {	if (fileStatus.isDirectory()) {	a file from the dir throw new SwiftPathExistsException("Cannot create a file over a directory:" + file);	if (LOG.isDebugEnabled()) {	
overwriting either an empty file or a directory 

========================= hadoop sample_6191 =========================

throw new IOException("Got unexpected end event " + "while looking for " + expected);	}	return ev;	case XMLEvent.START_ELEMENT: if (!expected.startsWith("[")) {	if (!ev.asStartElement().getName().getLocalPart(). equals(expected)) {	throw new IOException("Failed to find <" + expected + ">; " + "got " + ev.asStartElement().getName().getLocalPart() + " instead.");	}	}	return ev;	default: if (LOG.isTraceEnabled()) {	
skipping xmlevent of type 

Node node = new Node();	parent.addChild(key, node);	loadNodeChildrenHelper(node, expected, new String[0]);	break;	case XMLEvent.CHARACTERS: String val = XMLUtils. unmangleXmlString(ev.asCharacters().getData(), true);	parent.setVal(val);	events.nextEvent();	break;	case XMLEvent.ATTRIBUTE: throw new IOException("Unexpected XML event " + ev);	default: if (LOG.isTraceEnabled()) {	
skipping xmlevent 

if (lval != null)  {	b.setRollingUpgradeStartTime(lval);	}	lval = node.removeChildLong( NAME_SECTION_LAST_ALLOCATED_STRIPED_BLOCK_ID);	if (lval != null)  {	throw new IOException("can't handle lastAllocatedStripedBlockId " + "in NameSection; XML file is too new.\n");	}	node.verifyNoRemainingKeys("NameSection");	NameSystemSection s = b.build();	if (LOG.isDebugEnabled()) {	
writing header 

public void process() throws IOException {	
processing snapshotdiffsection 

private void processDirDiffEntry() throws IOException {	
processing dirdiffentry 

private void processFileDiffEntry() throws IOException {	
processing filediffentry 

private void writeStringTableSection() throws IOException {	FsImageProto.StringTableSection sectionHeader = FsImageProto.StringTableSection.newBuilder(). setNumEntry(stringTable.size()).build();	if (LOG.isDebugEnabled()) {	
writing header 

private void writeStringTableSection() throws IOException {	FsImageProto.StringTableSection sectionHeader = FsImageProto.StringTableSection.newBuilder(). setNumEntry(stringTable.size()).build();	if (LOG.isDebugEnabled()) {	}	sectionHeader.writeDelimitedTo(out);	for (Map.Entry<String, Integer> entry : stringTable.entrySet()) {	FsImageProto.StringTableSection.Entry stEntry = FsImageProto.StringTableSection.Entry.newBuilder(). setStr(entry.getKey()). setId(entry.getValue()). build();	if (LOG.isTraceEnabled()) {	
writing string table entry 

private void processXml() throws Exception {	
loading fsimage 

if (sectionProcessor == null) {	throw new IOException("Unknown FSImage section " + sectionName + ".  Valid section names are [" + StringUtils.join(", ", sections.keySet()) + "]");	}	unprocessedSections.remove(sectionName);	sectionProcessor.process();	}	writeStringTableSection();	long prevOffset = out.getCount();	FileSummary fileSummary = fileSummaryBld.build();	if (LOG.isDebugEnabled()) {	
writing filesummary 

========================= hadoop sample_7780 =========================

Collection<TempAppPerPartition> appsOrderedByPriority = tq.getApps();	Resource actualPreemptNeeded = resToObtainByPartition.get(partition);	if (actualPreemptNeeded == null) {	actualPreemptNeeded = Resources.createResource(0, 0);	resToObtainByPartition.put(partition, actualPreemptNeeded);	}	for (TempAppPerPartition a1 : appsOrderedByPriority) {	Resources.addTo(actualPreemptNeeded, a1.getActuallyToBePreempted());	}	if (LOG.isDebugEnabled()) {	
selected to preempt resource from partition 

if (Resources.greaterThan(rc, clusterResource, maxIntraQueuePreemptable, tq.getActuallyToBePreempted())) {	Resources.subtractFrom(maxIntraQueuePreemptable, tq.getActuallyToBePreempted());	} else {	maxIntraQueuePreemptable = Resource.newInstance(0, 0);	}	Resource preemptionLimit = Resources.min(rc, clusterResource, maxIntraQueuePreemptable, totalPreemptedResourceAllowed);	calculateToBePreemptedResourcePerApp(clusterResource, orderedApps, Resources.clone(preemptionLimit));	tq.addAllApps(orderedApps);	validateOutSameAppPriorityFromDemand(clusterResource, (TreeSet<TempAppPerPartition>) orderedApps, tq.getUsersPerPartition(), context.getIntraQueuePreemptionOrderPolicy());	if (LOG.isDebugEnabled()) {	
queue name partition 

String userName = app.getUser();	if (!usersPerPartition.containsKey(userName)) {	ResourceUsage userResourceUsage = tq.leafQueue.getUser(userName) .getResourceUsage();	Resource userSpecificAmUsed = perUserAMUsed.get(userName);	amUsed = (userSpecificAmUsed == null) ? Resources.none() : userSpecificAmUsed;	TempUserPerPartition tmpUser = new TempUserPerPartition( tq.leafQueue.getUser(userName), tq.queueName, Resources.clone(userResourceUsage.getUsed(partition)), Resources.clone(userSpecificAmUsed), Resources.clone(userResourceUsage.getReserved(partition)), Resources.none());	Resource userLimitResource = Resources.clone( tq.leafQueue.getResourceLimitForAllUsers(userName, clusterResource, partition, SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY));	userLimitResource = Resources.subtract(userLimitResource, tmpUser.amUsed);	tmpUser.setUserLimit(userLimitResource);	if (LOG.isDebugEnabled()) {	
tempuser 

========================= hadoop sample_676 =========================

Configuration conf = new Configuration();	conf.set("io.compression.codec.bzip2.library", "system-native");	if (NativeCodeLoader.isNativeCodeLoaded()) {	if (Bzip2Factory.isNativeBzip2Loaded(conf)) {	codecTest(conf, seed, 0, "org.apache.hadoop.io.compress.BZip2Codec");	codecTest(conf, seed, count, "org.apache.hadoop.io.compress.BZip2Codec");	conf.set("io.compression.codec.bzip2.library", "java-builtin");	codecTest(conf, seed, 0, "org.apache.hadoop.io.compress.BZip2Codec");	codecTest(conf, seed, count, "org.apache.hadoop.io.compress.BZip2Codec");	} else {	
native hadoop library available but native is not 

private static void codecTest(Configuration conf, int seed, int count, String codecClass) throws IOException {	CompressionCodec codec = null;	try {	codec = (CompressionCodec) ReflectionUtils.newInstance(conf.getClassByName(codecClass), conf);	} catch (ClassNotFoundException cnfe) {	throw new IOException("Illegal codec!");	}	
created a codec object of type 

}	DataOutputBuffer data = new DataOutputBuffer();	RandomDatum.Generator generator = new RandomDatum.Generator(seed);	for(int i=0; i < count; ++i) {	generator.next();	RandomDatum key = generator.getKey();	RandomDatum value = generator.getValue();	key.write(data);	value.write(data);	}	
generated records 

int leasedCompressorsBefore = codec.getCompressorType() == null ? -1 : CodecPool.getLeasedCompressorsCount(codec);	try (CompressionOutputStream deflateFilter = codec.createOutputStream(compressedDataBuffer);	DataOutputStream deflateOut = new DataOutputStream(new BufferedOutputStream(deflateFilter))) {	deflateOut.write(data.getData(), 0, data.getLength());	deflateOut.flush();	deflateFilter.finish();	}	if (leasedCompressorsBefore > -1) {	assertEquals("leased compressor not returned to the codec pool", leasedCompressorsBefore, CodecPool.getLeasedCompressorsCount(codec));	}	
finished compressing data 

originalData.reset(data.getData(), 0, data.getLength());	deCompressedDataBuffer.reset(compressedDataBuffer.getData(), 0, compressedDataBuffer.getLength());	try (CompressionInputStream inflateFilter = codec.createInputStream(deCompressedDataBuffer);	DataInputStream originalIn = new DataInputStream(new BufferedInputStream(originalData))) {	int expected;	do {	expected = originalIn.read();	assertEquals("Inflated stream read by byte does not match", expected, inflateFilter.read());	} while (expected != -1);	}	
success completed checking records 

private void testSplitableCodec( Class<? extends SplittableCompressionCodec> codecClass) throws IOException {	final long DEFLBYTES = 2 * 1024 * 1024;	final Configuration conf = new Configuration();	final Random rand = new Random();	final long seed = rand.nextLong();	
seed 

}	final int flen = (int) infile.getLen();	final Text line = new Text();	final Decompressor dcmp = CodecPool.getDecompressor(codec);	try {	for (int pos = 0; pos < infile.getLen(); pos += rand.nextInt(flen / 8)) {	final SplitCompressionInputStream in = codec.createInputStream(fs.open(infile.getPath()), dcmp, pos, flen, SplittableCompressionCodec.READ_MODE.BYBLOCK);	if (in.getAdjustedStart() >= flen) {	break;	}	
sample 

rand.nextBytes(b);	final byte[] b64enc = b64.encode(b);	dob.reset();	dob.writeInt(seq);	System.arraycopy(dob.getData(), 0, b64enc, 0, dob.getLength());	fout.write(b64enc);	fout.write('\n');	++seq;	infLen -= b64enc.length;	}	
wrote records to 

public void testCodecPoolGzipReuse() throws Exception {	Configuration conf = new Configuration();	conf.setBoolean(CommonConfigurationKeys.IO_NATIVE_LIB_AVAILABLE_KEY, true);	if (!ZlibFactory.isNativeZlibLoaded(conf)) {	
testcodecpoolgzipreuse skipped native libs not loaded 

public void testCodecInitWithCompressionLevel() throws Exception {	Configuration conf = new Configuration();	conf.setBoolean(CommonConfigurationKeys.IO_NATIVE_LIB_AVAILABLE_KEY, true);	if (ZlibFactory.isNativeZlibLoaded(conf)) {	
testcodecinitwithcompressionlevel with native 

public void testCodecInitWithCompressionLevel() throws Exception {	Configuration conf = new Configuration();	conf.setBoolean(CommonConfigurationKeys.IO_NATIVE_LIB_AVAILABLE_KEY, true);	if (ZlibFactory.isNativeZlibLoaded(conf)) {	codecTestWithNOCompression(conf, "org.apache.hadoop.io.compress.GzipCodec");	codecTestWithNOCompression(conf, "org.apache.hadoop.io.compress.DefaultCodec");	} else {	
testcodecinitwithcompressionlevel for native skipped native libs not loaded 

public void testCodecPoolCompressorReinit() throws Exception {	Configuration conf = new Configuration();	conf.setBoolean(CommonConfigurationKeys.IO_NATIVE_LIB_AVAILABLE_KEY, true);	if (ZlibFactory.isNativeZlibLoaded(conf)) {	GzipCodec gzc = ReflectionUtils.newInstance(GzipCodec.class, conf);	gzipReinitTest(conf, gzc);	} else {	
testcodecpoolcompressorreinit skipped native libs not loaded 

public void testSequenceFileBZip2NativeCodec() throws IOException, ClassNotFoundException, InstantiationException, IllegalAccessException {	Configuration conf = new Configuration();	conf.set("io.compression.codec.bzip2.library", "system-native");	if (NativeCodeLoader.isNativeCodeLoaded()) {	if (Bzip2Factory.isNativeBzip2Loaded(conf)) {	sequenceFileCodecTest(conf, 0, "org.apache.hadoop.io.compress.BZip2Codec", 100);	sequenceFileCodecTest(conf, 100, "org.apache.hadoop.io.compress.BZip2Codec", 100);	sequenceFileCodecTest(conf, 200000, "org.apache.hadoop.io.compress.BZip2Codec", 1000000);	} else {	
native hadoop library available but native is not 

private static void sequenceFileCodecTest(Configuration conf, int lines, String codecClass, int blockSize) throws IOException, ClassNotFoundException, InstantiationException, IllegalAccessException {	Path filePath = new Path("SequenceFileCodecTest." + codecClass);	conf.setInt("io.seqfile.compress.blocksize", blockSize);	FileSystem fs = FileSystem.get(conf);	
creating sequencefile with codec codecclass 

private static void sequenceFileCodecTest(Configuration conf, int lines, String codecClass, int blockSize) throws IOException, ClassNotFoundException, InstantiationException, IllegalAccessException {	Path filePath = new Path("SequenceFileCodecTest." + codecClass);	conf.setInt("io.seqfile.compress.blocksize", blockSize);	FileSystem fs = FileSystem.get(conf);	SequenceFile.Writer writer = SequenceFile.createWriter(fs, conf, filePath, Text.class, Text.class, CompressionType.BLOCK, (CompressionCodec)Class.forName(codecClass).newInstance());	
writing to sequencefile 

Path filePath = new Path("SequenceFileCodecTest." + codecClass);	conf.setInt("io.seqfile.compress.blocksize", blockSize);	FileSystem fs = FileSystem.get(conf);	SequenceFile.Writer writer = SequenceFile.createWriter(fs, conf, filePath, Text.class, Text.class, CompressionType.BLOCK, (CompressionCodec)Class.forName(codecClass).newInstance());	for (int i=0; i<lines; i++) {	Text key = new Text("key" + i);	Text value = new Text("value" + i);	writer.append(key, value);	}	writer.close();	
reading from the sequencefile 

while (reader.next(key, value)) {	assertEquals("key" + lc, key.toString());	assertEquals("value" + lc, value.toString());	lc ++;	}	} finally {	reader.close();	}	assertEquals(lines, lc);	fs.delete(filePath, false);	
success completed sequencefilecodectest with codec codecclass 

private void codecTestMapFile(Class<? extends CompressionCodec> clazz, CompressionType type, int records) throws Exception {	FileSystem fs = FileSystem.get(conf);	
creating mapfiles with records using codec 

private void codecTestMapFile(Class<? extends CompressionCodec> clazz, CompressionType type, int records) throws Exception {	FileSystem fs = FileSystem.get(conf);	Path path = new Path(GenericTestUtils.getTempPath( clazz.getSimpleName() + "-" + type + "-" + records));	
writing 

public void testGzipCompatibility() throws IOException {	Random r = new Random();	long seed = r.nextLong();	r.setSeed(seed);	
seed 

public void testNativeGzipConcat() throws IOException {	Configuration conf = new Configuration();	conf.setBoolean(CommonConfigurationKeys.IO_NATIVE_LIB_AVAILABLE_KEY, true);	if (!ZlibFactory.isNativeZlibLoaded(conf)) {	
skipped native libs not loaded 

public void testGzipLongOverflow() throws IOException {	
testGzipLongOverflow 

private void testGzipCodecWrite(boolean useNative) throws IOException {	Configuration conf = new Configuration();	conf.setBoolean(CommonConfigurationKeys.IO_NATIVE_LIB_AVAILABLE_KEY, useNative);	if (useNative) {	if (!ZlibFactory.isNativeZlibLoaded(conf)) {	
testgzipcodecwrite skipped native libs not loaded 

========================= hadoop sample_2994 =========================

hasHadoopAuthFilterInitializer = true;	break;	}	targets.add(initializer.getName());	}	}	if (!hasHadoopAuthFilterInitializer) {	targets.add(AuthenticationFilterInitializer.class.getName());	conf.set(filterInitializerConfKey, StringUtils.join(",", targets));	}	
instantiating nmwebapp at 

protected void serviceStop() throws Exception {	if (this.webApp != null) {	
stopping webapp 

========================= hadoop sample_1711 =========================

WritableUtils.writeVLong(buf, val);	try {	inbuf.reset(buf.getData(), 0, buf.getLength());	long val2 = WritableUtils.readVIntInRange(inbuf, lower, upper);	if (!expectSuccess) {	fail("expected readVIntInRange to throw an exception");	}	assertEquals(val, val2);	} catch(IOException e) {	if (expectSuccess) {	
unexpected exception 

========================= hadoop sample_3002 =========================

public ReplicaOutputStreams( OutputStream dataOut, OutputStream checksumOut, DataChecksum checksum, FsVolumeSpi volume, FileIoProvider fileIoProvider) {	this.dataOut = dataOut;	this.checksum = checksum;	this.checksumOut = checksumOut;	this.volume = volume;	this.fileIoProvider = fileIoProvider;	try {	if (this.dataOut instanceof FileOutputStream) {	this.outFd = ((FileOutputStream)this.dataOut).getFD();	} else {	
could not get file descriptor for outputstream of class 

this.checksum = checksum;	this.checksumOut = checksumOut;	this.volume = volume;	this.fileIoProvider = fileIoProvider;	try {	if (this.dataOut instanceof FileOutputStream) {	this.outFd = ((FileOutputStream)this.dataOut).getFD();	} else {	}	} catch (IOException e) {	
could not get file descriptor for outputstream of class 

========================= hadoop sample_7942 =========================

public void setUp() throws Exception {	File logFile = new File(historyLog);	
cannot create dirs for history log file 

public void setUp() throws Exception {	File logFile = new File(historyLog);	
cannot create history log file 

public void tearDown() throws Exception {	File logFile = new File(historyLog);	
cannot delete history log file 

public void tearDown() throws Exception {	File logFile = new File(historyLog);	
cannot delete history log dir 

========================= hadoop sample_5586 =========================

int numHedgedReadPoolThreads = 5;	final int hedgedReadTimeoutMillis = 50;	conf.setInt(HdfsClientConfigKeys.HedgedRead.THREADPOOL_SIZE_KEY, numHedgedReadPoolThreads);	conf.setLong(HdfsClientConfigKeys.HedgedRead.THRESHOLD_MILLIS_KEY, hedgedReadTimeoutMillis);	conf.setInt(HdfsClientConfigKeys.Retry.WINDOW_BASE_KEY, 0);	DFSClientFaultInjector.set(Mockito.mock(DFSClientFaultInjector.class));	DFSClientFaultInjector injector = DFSClientFaultInjector.get();	Mockito.doAnswer(new Answer<Void>() {	public Void answer(InvocationOnMock invocation) throws Throwable {	if (true) {	
throw checksum exception 

DFSInputStream din = dfsClient.open(p.toString());	final DatanodeInfo source = locations[1];	final DatanodeInfo destination = toMove;	DFSTestUtil.replaceBlock(lb.getBlock(), source, locations[1], toMove);	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	try {	LocatedBlocks lbs = dfsClient.getLocatedBlocks(p.toString(), 0);	LocatedBlock lb = lbs.get(0);	List<DatanodeInfo> locations = Arrays.asList(lb.getLocations());	
source destination 

DFSInputStream din = dfsClient.open(p.toString());	final DatanodeInfo source = locations[1];	final DatanodeInfo destination = toMove;	DFSTestUtil.replaceBlock(lb.getBlock(), source, locations[1], toMove);	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	try {	LocatedBlocks lbs = dfsClient.getLocatedBlocks(p.toString(), 0);	LocatedBlock lb = lbs.get(0);	List<DatanodeInfo> locations = Arrays.asList(lb.getLocations());	
got updated locations 

final DatanodeInfo destination = toMove;	DFSTestUtil.replaceBlock(lb.getBlock(), source, locations[1], toMove);	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	try {	LocatedBlocks lbs = dfsClient.getLocatedBlocks(p.toString(), 0);	LocatedBlock lb = lbs.get(0);	List<DatanodeInfo> locations = Arrays.asList(lb.getLocations());	return locations.contains(destination) && !locations.contains(source);	} catch (IOException e) {	
problem in getting block locations 

LocatedBlock lb = lbs.get(0);	List<DatanodeInfo> locations = Arrays.asList(lb.getLocations());	return locations.contains(destination) && !locations.contains(source);	} catch (IOException e) {	}	return null;	}	}, 1000, 10000);	DFSTestUtil.waitForReplication(cluster, lb.getBlock(), 1, 2, 0);	cluster.stopDataNode(validDownLocation.getName());	
starting read 

return null;	}	}, 1000, 10000);	DFSTestUtil.waitForReplication(cluster, lb.getBlock(), 1, 2, 0);	cluster.stopDataNode(validDownLocation.getName());	byte[] buf = new byte[1024];	int n = din.read(0, buf, 0, data.length());	assertEquals(data.length(), n);	assertEquals("Data should be read", data, new String(buf, 0, n));	assertTrue("Read should complete with maximum " + maxFailures + " failures, but completed with " + din.failures, din.failures <= maxFailures);	
read completed 

========================= hadoop sample_7104 =========================

public void testInit() {	
start testinit 

public void testScheduleTask() {	
start testscheduletask 

public void testKillScheduledTask() {	
start testkillscheduledtask 

public void testKillScheduledTaskAttempt() {	
start testkillscheduledtaskattempt 

public void testLaunchTaskAttempt() {	
start testlaunchtaskattempt 

public void testKillRunningTaskAttempt() {	
start testkillrunningtaskattempt 

public void testKillSuccessfulTask() {	
start testkillsuccesfultask 

public void testKillAttemptForSuccessfulTask() {	
start testkillattemptforsuccessfultask 

public void testTaskProgress() {	
start testtaskprogress 

========================= hadoop sample_5139 =========================

private boolean sourceHasNext() throws IOException {	if (statusBatchIterator.hasNext() || requestNextBatch()) {	return true;	} else {	if (providedStatusIterator == null) {	
start iterating the provided status 

public FileStatus next() throws IOException {	final FileStatus status;	if (sourceHasNext()) {	status = statusBatchIterator.next();	
removing the status from provided file status 

public FileStatus next() throws IOException {	final FileStatus status;	if (sourceHasNext()) {	status = statusBatchIterator.next();	providedStatus.remove(status);	} else {	if (providedStatusIterator.hasNext()) {	status = providedStatusIterator.next();	
returning provided file status 

private boolean requestNextBatch() throws IOException {	while (source.hasNext()) {	if (buildNextStatusBatch(source.next())) {	return true;	} else {	
all entries in batch were filtered continuing 

int added = 0, ignored = 0;	List<FileStatus> stats = new ArrayList<>( objects.getObjectSummaries().size() + objects.getCommonPrefixes().size());	for (S3ObjectSummary summary : objects.getObjectSummaries()) {	String key = summary.getKey();	Path keyPath = owner.keyToQualifiedPath(key);	if (LOG.isDebugEnabled()) {	LOG.debug("{}: {}", keyPath, stringify(summary));	}	if (acceptor.accept(keyPath, summary) && filter.accept(keyPath)) {	FileStatus status = createFileStatus(keyPath, summary, owner.getDefaultBlockSize(keyPath), owner.getUsername());	
adding 

String key = summary.getKey();	Path keyPath = owner.keyToQualifiedPath(key);	if (LOG.isDebugEnabled()) {	LOG.debug("{}: {}", keyPath, stringify(summary));	}	if (acceptor.accept(keyPath, summary) && filter.accept(keyPath)) {	FileStatus status = createFileStatus(keyPath, summary, owner.getDefaultBlockSize(keyPath), owner.getUsername());	stats.add(status);	added++;	} else {	
ignoring 

stats.add(status);	added++;	} else {	ignored++;	}	}	for (String prefix : objects.getCommonPrefixes()) {	Path keyPath = owner.keyToQualifiedPath(prefix);	if (acceptor.accept(keyPath, prefix) && filter.accept(keyPath)) {	FileStatus status = new S3AFileStatus(Tristate.FALSE, keyPath, owner.getUsername());	
adding directory 

ignored++;	}	}	for (String prefix : objects.getCommonPrefixes()) {	Path keyPath = owner.keyToQualifiedPath(prefix);	if (acceptor.accept(keyPath, prefix) && filter.accept(keyPath)) {	FileStatus status = new S3AFileStatus(Tristate.FALSE, keyPath, owner.getUsername());	added++;	stats.add(status);	} else {	
ignoring directory 

public ObjectListing next() throws IOException {	if (firstListing) {	firstListing = false;	} else {	try {	if (!objects.isTruncated()) {	throw new NoSuchElementException("No more results in listing of " + listPath);	}	
requesting next objects under 

public ObjectListing next() throws IOException {	if (firstListing) {	firstListing = false;	} else {	try {	if (!objects.isTruncated()) {	throw new NoSuchElementException("No more results in listing of " + listPath);	}	objects = owner.continueListObjects(objects);	listingCount++;	
new listing status 

========================= hadoop sample_5990 =========================

public boolean add(Node n) {	
adding node 

public boolean remove(Node n) {	
removing node 

public synchronized void childAddStorage( String childName, StorageType type) {	
child add storage 

public synchronized void childRemoveStorage( String childName, StorageType type) {	
child remove storage 

========================= hadoop sample_7807 =========================

public synchronized void updateApplicationStateInternal( ApplicationId appId, ApplicationStateData appState) throws Exception {	
updating final state for app 

public synchronized void updateApplicationAttemptStateInternal( ApplicationAttemptId appAttemptId, ApplicationAttemptStateData attemptState) throws Exception {	ApplicationStateData appState = state.getApplicationState().get(appAttemptId.getApplicationId());	if (appState == null) {	throw new YarnRuntimeException("Application doesn't exist");	}	
updating final state for attempt 

public synchronized void removeApplicationAttemptInternal( ApplicationAttemptId appAttemptId) throws Exception {	ApplicationStateData appState = state.getApplicationState().get(appAttemptId.getApplicationId());	ApplicationAttemptStateData attemptState = appState.attempts.remove(appAttemptId);	
removing state for attempt 

private void storeOrUpdateRMDT(RMDelegationTokenIdentifier rmDTIdentifier, Long renewDate, boolean isUpdate) throws Exception {	Map<RMDelegationTokenIdentifier, Long> rmDTState = state.rmSecretManagerState.getTokenState();	if (rmDTState.containsKey(rmDTIdentifier)) {	IOException e = new IOException("RMDelegationToken: " + rmDTIdentifier + "is already stored.");	
error storing info for rmdelegationtoken 

private void storeOrUpdateRMDT(RMDelegationTokenIdentifier rmDTIdentifier, Long renewDate, boolean isUpdate) throws Exception {	Map<RMDelegationTokenIdentifier, Long> rmDTState = state.rmSecretManagerState.getTokenState();	if (rmDTState.containsKey(rmDTIdentifier)) {	IOException e = new IOException("RMDelegationToken: " + rmDTIdentifier + "is already stored.");	throw e;	}	rmDTState.put(rmDTIdentifier, renewDate);	if(!isUpdate) {	state.rmSecretManagerState.dtSequenceNumber = rmDTIdentifier.getSequenceNumber();	}	
store rmdt with sequence number 

public synchronized void removeRMDelegationTokenState( RMDelegationTokenIdentifier rmDTIdentifier) throws Exception{	Map<RMDelegationTokenIdentifier, Long> rmDTState = state.rmSecretManagerState.getTokenState();	rmDTState.remove(rmDTIdentifier);	
remove rmdt with sequence number 

protected synchronized void updateRMDelegationTokenState( RMDelegationTokenIdentifier rmDTIdentifier, Long renewDate) throws Exception {	removeRMDelegationTokenState(rmDTIdentifier);	storeOrUpdateRMDT(rmDTIdentifier, renewDate, true);	
update rmdt with sequence number 

public synchronized void storeRMDTMasterKeyState(DelegationKey delegationKey) throws Exception {	Set<DelegationKey> rmDTMasterKeyState = state.rmSecretManagerState.getMasterKeyState();	if (rmDTMasterKeyState.contains(delegationKey)) {	IOException e = new IOException("RMDTMasterKey with keyID: " + delegationKey.getKeyId() + " is already stored");	
error storing info for rmdtmasterkey with keyid 

public synchronized void storeRMDTMasterKeyState(DelegationKey delegationKey) throws Exception {	Set<DelegationKey> rmDTMasterKeyState = state.rmSecretManagerState.getMasterKeyState();	if (rmDTMasterKeyState.contains(delegationKey)) {	IOException e = new IOException("RMDTMasterKey with keyID: " + delegationKey.getKeyId() + " is already stored");	throw e;	}	state.getRMDTSecretManagerState().getMasterKeyState().add(delegationKey);	
store rmdt master key with key id currently rmdtmasterkeystate size 

public synchronized void removeRMDTMasterKeyState(DelegationKey delegationKey) throws Exception {	
remove rmdt master key with key id 

protected synchronized void storeReservationState( ReservationAllocationStateProto reservationAllocation, String planName, String reservationIdName) throws Exception {	
storing reservationallocation for for plan 

protected synchronized void removeReservationState( String planName, String reservationIdName) throws Exception {	
removing reservationallocation for plan 

========================= hadoop sample_764 =========================

public MiniKdc(Properties conf, File workDir) throws Exception {	if (!conf.keySet().containsAll(PROPERTIES)) {	Set<String> missingProperties = new HashSet<String>(PROPERTIES);	missingProperties.removeAll(conf.keySet());	throw new IllegalArgumentException("Missing configuration properties: " + missingProperties);	}	this.workDir = new File(workDir, Long.toString(System.currentTimeMillis()));	if (! workDir.exists() && ! workDir.mkdirs()) {	throw new RuntimeException("Cannot create directory " + workDir);	}	
configuration 

System.setProperty(JAVA_SECURITY_KRB5_CONF, krb5conf.getAbsolutePath());	System.setProperty(SUN_SECURITY_KRB5_DEBUG, conf.getProperty(DEBUG, "false"));	Class<?> classRef;	if (System.getProperty("java.vendor").contains("IBM")) {	classRef = Class.forName("com.ibm.security.krb5.internal.Config");	} else {	classRef = Class.forName("sun.security.krb5.Config");	}	Method refreshMethod = classRef.getMethod("refresh", new Class[0]);	refreshMethod.invoke(classRef, new Object[0]);	
minikdc listening at port 

System.setProperty(JAVA_SECURITY_KRB5_CONF, krb5conf.getAbsolutePath());	System.setProperty(SUN_SECURITY_KRB5_DEBUG, conf.getProperty(DEBUG, "false"));	Class<?> classRef;	if (System.getProperty("java.vendor").contains("IBM")) {	classRef = Class.forName("com.ibm.security.krb5.internal.Config");	} else {	classRef = Class.forName("sun.security.krb5.Config");	}	Method refreshMethod = classRef.getMethod("refresh", new Class[0]);	refreshMethod.invoke(classRef, new Object[0]);	
minikdc setting jvm conf to 

public synchronized void stop() {	if (kdc != null) {	System.getProperties().remove(JAVA_SECURITY_KRB5_CONF);	System.getProperties().remove(SUN_SECURITY_KRB5_DEBUG);	kdc.stop();	try {	ds.shutdown();	} catch (Exception ex) {	
could not shutdown apacheds properly 

private void delete(File f) {	if (f.isFile()) {	if (! f.delete()) {	
warning cannot delete file 

private void delete(File f) {	if (f.isFile()) {	if (! f.delete()) {	}	} else {	for (File c: f.listFiles()) {	delete(c);	}	if (! f.delete()) {	
warning cannot delete directory 

========================= hadoop sample_4401 =========================

return get(conf);	}	if (scheme != null && authority == null) {	URI defaultUri = getDefaultUri(conf);	if (scheme.equals(defaultUri.getScheme()) && defaultUri.getAuthority() != null) {	return get(defaultUri, conf);	}	}	String disableCacheName = String.format("fs.%s.impl.disable.cache", scheme);	if (conf.getBoolean(disableCacheName, false)) {	
bypassing cache to create filesystem 

protected void processDeleteOnExit() {	synchronized (deleteOnExit) {	for (Iterator<Path> iter = deleteOnExit.iterator(); iter.hasNext();) {	Path path = iter.next();	try {	if (exists(path)) {	delete(path, true);	}	}	catch (IOException e) {	
ignoring failure to deleteonexit for path 

for (FileStatus candidate : candidates) {	Path userTrash = new Path(candidate.getPath(), TRASH_PREFIX);	if (exists(userTrash)) {	candidate.setPath(userTrash);	ret.add(candidate);	}	}	}	}	} catch (IOException e) {	
cannot get all trash roots 

private static void loadFileSystems() {	
loading filesystems 

if (!FILE_SYSTEMS_LOADED) {	ServiceLoader<FileSystem> serviceLoader = ServiceLoader.load(FileSystem.class);	Iterator<FileSystem> it = serviceLoader.iterator();	while (it.hasNext()) {	FileSystem fs;	try {	fs = it.next();	try {	SERVICE_FILE_SYSTEMS.put(fs.getScheme(), fs.getClass());	if (LOGGER.isDebugEnabled()) {	
fs getscheme fs getclass classutil findcontainingjar fs getclass 

Iterator<FileSystem> it = serviceLoader.iterator();	while (it.hasNext()) {	FileSystem fs;	try {	fs = it.next();	try {	SERVICE_FILE_SYSTEMS.put(fs.getScheme(), fs.getClass());	if (LOGGER.isDebugEnabled()) {	}	} catch (Exception e) {	
cannot load from 

Iterator<FileSystem> it = serviceLoader.iterator();	while (it.hasNext()) {	FileSystem fs;	try {	fs = it.next();	try {	SERVICE_FILE_SYSTEMS.put(fs.getScheme(), fs.getClass());	if (LOGGER.isDebugEnabled()) {	}	} catch (Exception e) {	
full exception loading 

FileSystem fs;	try {	fs = it.next();	try {	SERVICE_FILE_SYSTEMS.put(fs.getScheme(), fs.getClass());	if (LOGGER.isDebugEnabled()) {	}	} catch (Exception e) {	}	} catch (ServiceConfigurationError ee) {	
cannot load filesystem 

if (LOGGER.isDebugEnabled()) {	}	} catch (Exception e) {	}	} catch (ServiceConfigurationError ee) {	Throwable cause = ee.getCause();	while (cause != null) {	LOG.warn(cause.toString());	cause = cause.getCause();	}	
stack trace 

public static Class<? extends FileSystem> getFileSystemClass(String scheme, Configuration conf) throws IOException {	if (!FILE_SYSTEMS_LOADED) {	loadFileSystems();	}	
looking for fs supporting 

public static Class<? extends FileSystem> getFileSystemClass(String scheme, Configuration conf) throws IOException {	if (!FILE_SYSTEMS_LOADED) {	loadFileSystems();	}	Class<? extends FileSystem> clazz = null;	if (conf != null) {	String property = "fs." + scheme + ".impl";	
looking for configuration option 

public static Class<? extends FileSystem> getFileSystemClass(String scheme, Configuration conf) throws IOException {	if (!FILE_SYSTEMS_LOADED) {	loadFileSystems();	}	Class<? extends FileSystem> clazz = null;	if (conf != null) {	String property = "fs." + scheme + ".impl";	clazz = (Class<? extends FileSystem>) conf.getClass( property, null);	} else {	
no configuration skipping check for fs impl 

if (!FILE_SYSTEMS_LOADED) {	loadFileSystems();	}	Class<? extends FileSystem> clazz = null;	if (conf != null) {	String property = "fs." + scheme + ".impl";	clazz = (Class<? extends FileSystem>) conf.getClass( property, null);	} else {	}	if (clazz == null) {	
looking in service filesystems for implementation class 

}	Class<? extends FileSystem> clazz = null;	if (conf != null) {	String property = "fs." + scheme + ".impl";	clazz = (Class<? extends FileSystem>) conf.getClass( property, null);	} else {	}	if (clazz == null) {	clazz = SERVICE_FILE_SYSTEMS.get(scheme);	} else {	
filesystem defined in configuration option 

clazz = (Class<? extends FileSystem>) conf.getClass( property, null);	} else {	}	if (clazz == null) {	clazz = SERVICE_FILE_SYSTEMS.get(scheme);	} else {	}	if (clazz == null) {	throw new UnsupportedFileSystemException("No FileSystem for scheme " + "\"" + scheme + "\"");	}	
fs for is 

public synchronized void run() {	try {	closeAll(true);	} catch (IOException e) {	
filesystem cache closeall threw an exception 

public void run() {	while (!Thread.interrupted()) {	try {	StatisticsDataReference ref = (StatisticsDataReference)STATS_DATA_REF_QUEUE.remove();	ref.cleanUp();	} catch (InterruptedException ie) {	
cleaner thread interrupted will stop 

public void run() {	while (!Thread.interrupted()) {	try {	StatisticsDataReference ref = (StatisticsDataReference)STATS_DATA_REF_QUEUE.remove();	ref.cleanUp();	} catch (InterruptedException ie) {	Thread.currentThread().interrupt();	} catch (Throwable th) {	
exception in the cleaner thread but it will continue to run 

========================= hadoop sample_4114 =========================

nm1.nodeHeartbeat(true);	RMAppAttempt attempt = app.getCurrentAppAttempt();	MockAM am = rm.sendAMLaunched(attempt.getAppAttemptId());	am.registerAppAttempt();	final int request = 2;	am.allocate("h1" , 1000, request, new ArrayList<ContainerId>());	nm1.nodeHeartbeat(true);	List<Container> conts = new ArrayList<>(request);	int waitCount = 0;	while (conts.size() < request && waitCount++ < 200) {	
got containers waiting to get 

}	Assert.assertEquals(request, conts.size());	for(Container container : conts) {	rm.signalToContainer(container.getId(), SignalContainerCommand.OUTPUT_THREAD_DUMP);	}	NodeHeartbeatResponse resp;	List<SignalContainerRequest> contsToSignal;	int signaledConts = 0;	waitCount = 0;	while ( signaledConts < request && waitCount++ < 200) {	
waiting to get signalcontainer events signaledconts 

========================= hadoop sample_621 =========================

public void run() {	try {	String[] args = new String[1];	args[0] = String.valueOf(numBookies);	
starting bk 

public void run() {	try {	String[] args = new String[1];	args[0] = String.valueOf(numBookies);	LocalBookKeeper.main(args);	} catch (InterruptedException e) {	} catch (Exception e) {	
error starting local bk 

========================= hadoop sample_7738 =========================

public synchronized void handle(ResourceEvent event) {	LocalResourceRequest req = event.getLocalResourceRequest();	LocalizedResource rsrc = localrsrc.get(req);	switch (event.getType()) {	case LOCALIZED: if (useLocalCacheDirectoryManager) {	inProgressLocalResourcesMap.remove(req);	}	break;	case REQUEST: if (rsrc != null && (!isResourcePresent(rsrc))) {	
resource is missing localizing it again 

removeResource(req);	rsrc = null;	}	if (null == rsrc) {	rsrc = new LocalizedResource(req, dispatcher);	localrsrc.put(req, rsrc);	}	break;	case RELEASE: if (null == rsrc) {	ResourceReleaseEvent relEvent = (ResourceReleaseEvent) event;	
container sent release event on a resource request not present in cache 

}	break;	case RELEASE: if (null == rsrc) {	ResourceReleaseEvent relEvent = (ResourceReleaseEvent) event;	return;	}	break;	case LOCALIZATION_FAILED: removeResource(req);	break;	case RECOVERED: if (rsrc != null) {	
ignoring attempt to recover existing resource 

case LOCALIZATION_FAILED: removeResource(req);	break;	case RECOVERED: if (rsrc != null) {	return;	}	rsrc = recoverResource(req, (ResourceRecoveredEvent) event);	localrsrc.put(req, rsrc);	break;	}	if (rsrc == null) {	
received event for request but localized resource is missing 

if (event.getType() == ResourceEventType.RELEASE) {	if (rsrc.getState() == ResourceState.DOWNLOADING && rsrc.getRefCount() <= 0 && rsrc.getRequest().getVisibility() != LocalResourceVisibility.PUBLIC) {	removeResource(req);	}	}	if (event.getType() == ResourceEventType.LOCALIZED) {	if (rsrc.getLocalPath() != null) {	try {	stateStore.finishResourceLocalization(user, appId, buildLocalizedResourceProto(rsrc));	} catch (IOException ioe) {	
error storing resource state for 

removeResource(req);	}	}	if (event.getType() == ResourceEventType.LOCALIZED) {	if (rsrc.getLocalPath() != null) {	try {	stateStore.finishResourceLocalization(user, appId, buildLocalizedResourceProto(rsrc));	} catch (IOException ioe) {	}	} else {	
resource localized without a location 

public boolean remove(LocalizedResource rem, DeletionService delService) {	LocalizedResource rsrc = localrsrc.get(rem.getRequest());	if (null == rsrc) {	
attempt to remove absent resource from 

public boolean remove(LocalizedResource rem, DeletionService delService) {	LocalizedResource rsrc = localrsrc.get(rem.getRequest());	if (null == rsrc) {	return true;	}	if (rsrc.getRefCount() > 0 || ResourceState.DOWNLOADING.equals(rsrc.getState()) || rsrc != rem) {	
attempt to remove resource with non zero refcount 

return true;	}	if (rsrc.getRefCount() > 0 || ResourceState.DOWNLOADING.equals(rsrc.getState()) || rsrc != rem) {	return false;	} else {	if (ResourceState.LOCALIZED.equals(rsrc.getState())) {	FileDeletionTask deletionTask = new FileDeletionTask(delService, getUser(), getPathToDelete(rsrc.getLocalPath()), null);	delService.delete(deletionTask);	}	removeResource(rem.getRequest());	
removed from localized cache 

private void removeResource(LocalResourceRequest req) {	LocalizedResource rsrc = localrsrc.remove(req);	decrementFileCountForLocalCacheDirectory(req, rsrc);	if (rsrc != null) {	Path localPath = rsrc.getLocalPath();	if (localPath != null) {	try {	stateStore.removeLocalizedResource(user, appId, localPath);	} catch (IOException e) {	
unable to remove resource from state store 

private Path getPathToDelete(Path localPath) {	Path delPath = localPath.getParent();	String name = delPath.getName();	Matcher matcher = RANDOM_DIR_PATTERN.matcher(name);	if (matcher.matches()) {	return delPath;	} else {	
random directory component did not match deleting localized path only 

}	inProgressLocalResourcesMap.put(req, rPath);	}	while (true) {	Path uniquePath = new Path(rPath, Long.toString(uniqueNumberGenerator.incrementAndGet()));	File file = new File(uniquePath.toUri().getRawPath());	if (!file.exists()) {	rPath = uniquePath;	break;	}	
directory already exists try next one 

delService.delete(deletionTask);	}	}	Path localPath = new Path(rPath, req.getPath().getName());	LocalizedResource rsrc = localrsrc.get(req);	rsrc.setLocalPath(localPath);	LocalResource lr = LocalResource.newInstance(req.getResource(), req.getType(), req.getVisibility(), req.getSize(), req.getTimestamp());	try {	stateStore.startResourceLocalization(user, appId, ((LocalResourcePBImpl) lr).getProto(), localPath);	} catch (IOException e) {	
unable to record localization start for 

========================= hadoop sample_1880 =========================

boolean found = false;	for (RemoteLocation dest : original) {	if (dest.getNameserviceId().equals(nsId)) {	found = true;	newDestinations.addFirst(dest);	} else {	newDestinations.add(dest);	}	}	if (!found) {	
cannot find location with namespace in 

========================= hadoop sample_8208 =========================

if (closed) {	return;	}	if (blockStream != null) {	blockStream.close();	blockStream = null;	}	if (blockFile != null) {	boolean b = blockFile.delete();	if (!b) {	
ignoring failed delete 

========================= hadoop sample_5969 =========================

private void nextOp() throws IOException {	double rn = r.nextDouble();	int i = currentIndex;	
thread moving to index 

} else {	if (LOG.isDebugEnabled()) {	LOG.debug("Moving to index " + currentIndex + ": r = " + readProbs[currentIndex] + ", w = " + writeProbs + " for duration " + durations[currentIndex]);	}	currentIndex++;	}	}	}	}	if(LOG.isDebugEnabled()) {	
done with testing waiting for threads to finish 

} else if (args[i].equals("-elapsedTime")) {	if (scriptFile != null) {	System.err.println("Can't specify elapsedTime and use script.");	return -1;	}	durations[0] = Long.parseLong(args[++i]);	} else if (args[i].equals("-seed")) {	seed = Long.parseLong(args[++i]);	r = new Random(seed);	}  else if (args[i].equals("-flagFile")) {	
got flagfile 

========================= hadoop sample_3278 =========================

private IdentityProvider parseIdentityProvider(String ns, Configuration conf) {	List<IdentityProvider> providers = conf.getInstances( ns + "." + CommonConfigurationKeys.IPC_IDENTITY_PROVIDER_KEY, IdentityProvider.class);	if (providers.size() < 1) {	
identityprovider not specified defaulting to useridentityprovider 

private static double parseDecayFactor(String ns, Configuration conf) {	double factor = conf.getDouble(ns + "." + IPC_FCQ_DECAYSCHEDULER_FACTOR_KEY, 0.0);	if (factor == 0.0) {	factor = conf.getDouble(ns + "." + IPC_SCHEDULER_DECAYSCHEDULER_FACTOR_KEY, IPC_SCHEDULER_DECAYSCHEDULER_FACTOR_DEFAULT);	} else if ((factor > 0.0) && (factor < 1)) {	
is deprecated please use 

private static long parseDecayPeriodMillis(String ns, Configuration conf) {	long period = conf.getLong(ns + "." + IPC_FCQ_DECAYSCHEDULER_PERIOD_KEY, 0);	if (period == 0) {	period = conf.getLong(ns + "." + IPC_SCHEDULER_DECAYSCHEDULER_PERIOD_KEY, IPC_SCHEDULER_DECAYSCHEDULER_PERIOD_DEFAULT);	} else if (period > 0) {	
is deprecated please use 

private static double[] parseThresholds(String ns, Configuration conf, int numLevels) {	int[] percentages = conf.getInts(ns + "." + IPC_FCQ_DECAYSCHEDULER_THRESHOLDS_KEY);	if (percentages.length == 0) {	percentages = conf.getInts(ns + "." + IPC_DECAYSCHEDULER_THRESHOLDS_KEY);	if (percentages.length == 0) {	return getDefaultThresholds(numLevels);	}	} else {	
is deprecated please use 

decayedCount.set(nextValue);	if (nextValue == 0) {	it.remove();	}	}	totalDecayedCallCount.set(totalDecayedCount);	totalRawCallCount.set(totalRawCount);	recomputeScheduleCache();	updateAverageResponseTime(true);	} catch (Exception ex) {	
decaycurrentcounts exception 

private int cachedOrComputedPriorityLevel(Object identity) {	try {	long occurrences = this.getAndIncrementCallCounts(identity);	Map<Object, Integer> scheduleCache = scheduleCacheRef.get();	if (scheduleCache != null) {	Integer priority = scheduleCache.get(identity);	if (priority != null) {	
cache priority for with priority 

try {	long occurrences = this.getAndIncrementCallCounts(identity);	Map<Object, Integer> scheduleCache = scheduleCacheRef.get();	if (scheduleCache != null) {	Integer priority = scheduleCache.get(identity);	if (priority != null) {	return priority;	}	}	int priority = computePriorityLevel(occurrences);	
compute priority for priority 

Map<Object, Integer> scheduleCache = scheduleCacheRef.get();	if (scheduleCache != null) {	Integer priority = scheduleCache.get(identity);	if (priority != null) {	return priority;	}	}	int priority = computePriorityLevel(occurrences);	return priority;	} catch (InterruptedException ie) {	
caught interruptedexception returning low priority level 

Map<Object, Integer> scheduleCache = scheduleCacheRef.get();	if (scheduleCache != null) {	Integer priority = scheduleCache.get(identity);	if (priority != null) {	return priority;	}	}	int priority = computePriorityLevel(occurrences);	return priority;	} catch (InterruptedException ie) {	
fallback priority for with priority 

public boolean shouldBackOff(Schedulable obj) {	Boolean backOff = false;	if (backOffByResponseTimeEnabled) {	int priorityLevel = obj.getPriorityLevel();	if (LOG.isDebugEnabled()) {	double[] responseTimes = getAverageResponseTime();	
current caller priority 

public boolean shouldBackOff(Schedulable obj) {	Boolean backOff = false;	if (backOffByResponseTimeEnabled) {	int priorityLevel = obj.getPriorityLevel();	if (LOG.isDebugEnabled()) {	double[] responseTimes = getAverageResponseTime();	for (int i = 0; i < numLevels; i++) {	
queue responsetime backoffthreshold 

public void addResponseTime(String name, int priorityLevel, int queueTime, int processingTime) {	responseTimeCountInCurrWindow.getAndIncrement(priorityLevel);	responseTimeTotalInCurrWindow.getAndAdd(priorityLevel, queueTime+processingTime);	if (LOG.isDebugEnabled()) {	
addresponsetime for call priority queuetime processingtime 

public void getMetrics(MetricsCollector collector, boolean all) {	try {	MetricsRecordBuilder rb = collector.addRecord(getClass().getName()) .setContext(namespace);	addDecayedCallVolume(rb);	addUniqueIdentityCount(rb);	addTopNCallerSummary(rb);	addAvgResponseTimePerPriority(rb);	addCallVolumePerPriority(rb);	addRawCallVolume(rb);	} catch (Exception e) {	
exception thrown while metric collection exception 

========================= hadoop sample_4047 =========================

public Resource normalize(Resource r, Resource minimumResource, Resource maximumResource, Resource stepFactor) {	if (stepFactor.getMemorySize() == 0) {	
memory cannot be allocated in increments of zero assuming mb increment size please ensure the scheduler configuration is correct 

========================= hadoop sample_2193 =========================

static RenameResult renameToInt( FSDirectory fsd, final String src, final String dst, boolean logRetryCache) throws IOException {	if (NameNode.stateChangeLog.isDebugEnabled()) {	
dir namesystem renameto to 

String dst = dstIIP.getPath();	if (dst.equals(src)) {	return dstIIP;	}	try {	validateDestination(src, dst, srcInode);	} catch (IOException ignored) {	return null;	}	if (dstIIP.getLastINode() != null) {	
dir fsdirectory unprotectedrenameto failed to rename to because destination exists 

try {	validateDestination(src, dst, srcInode);	} catch (IOException ignored) {	return null;	}	if (dstIIP.getLastINode() != null) {	return null;	}	INode dstParent = dstIIP.getINode(-2);	if (dstParent == null) {	
dir fsdirectory unprotectedrenameto failed to rename to because destination s parent does not exist 

boolean added = false;	INodesInPath renamedIIP = null;	try {	if (!tx.removeSrc4OldRename()) {	return null;	}	renamedIIP = tx.addSourceToDestination();	added = (renamedIIP != null);	if (added) {	if (NameNode.stateChangeLog.isDebugEnabled()) {	
dir fsdirectory unprotectedrenameto is renamed to 

}	tx.updateMtimeAndLease(timestamp);	tx.updateQuotasInSourceTree(fsd.getBlockStoragePolicySuite());	return renamedIIP;	}	} finally {	if (!added) {	tx.restoreSource();	}	}	
dir fsdirectory unprotectedrenameto failed to rename to 

static RenameResult renameToInt( FSDirectory fsd, final String srcArg, final String dstArg, boolean logRetryCache, Options.Rename... options) throws IOException {	String src = srcArg;	String dst = dstArg;	if (NameNode.stateChangeLog.isDebugEnabled()) {	
dir namesystem renameto with options to 

renameToTrash = true;	}	if(renameToTrash) {	fsd.checkPermission(pc, srcIIP, false, null, FsAction.WRITE, null, FsAction.ALL, true);	} else {	fsd.checkPermission(pc, srcIIP, false, null, FsAction.WRITE, null, null, false);	}	fsd.checkPermission(pc, dstIIP, false, FsAction.WRITE, null, null, null, false);	}	if (NameNode.stateChangeLog.isDebugEnabled()) {	
dir fsdirectory renameto to 

final String dst = dstIIP.getPath();	final String error;	final INode srcInode = srcIIP.getLastINode();	validateRenameSource(fsd, srcIIP);	if (dst.equals(src)) {	throw new FileAlreadyExistsException("The source " + src + " and destination " + dst + " are the same");	}	validateDestination(src, dst, srcInode);	if (dstIIP.length() == 1) {	error = "rename destination cannot be the root";	
dir fsdirectory unprotectedrenameto 

fsd.ezManager.checkMoveValidity(srcIIP, dstIIP);	final INode dstInode = dstIIP.getLastINode();	List<INodeDirectory> snapshottableDirs = new ArrayList<>();	if (dstInode != null) {	validateOverwrite(src, dst, overwrite, srcInode, dstInode);	FSDirSnapshotOp.checkSnapshot(fsd, dstIIP, snapshottableDirs);	}	INode dstParent = dstIIP.getINode(-2);	if (dstParent == null) {	error = "rename destination parent " + dst + " not found.";	
dir fsdirectory unprotectedrenameto 

validateOverwrite(src, dst, overwrite, srcInode, dstInode);	FSDirSnapshotOp.checkSnapshot(fsd, dstIIP, snapshottableDirs);	}	INode dstParent = dstIIP.getINode(-2);	if (dstParent == null) {	error = "rename destination parent " + dst + " not found.";	throw new FileNotFoundException(error);	}	if (!dstParent.isDirectory()) {	error = "rename destination parent " + dst + " is a file.";	
dir fsdirectory unprotectedrenameto 

if (dstInode != null) {	removedNum = tx.removeDst();	if (removedNum != -1) {	undoRemoveDst = true;	}	}	INodesInPath renamedIIP = tx.addSourceToDestination();	if (renamedIIP != null) {	undoRemoveSrc = false;	if (NameNode.stateChangeLog.isDebugEnabled()) {	
dir fsdirectory unprotectedrenameto is renamed to 

return createRenameResult( fsd, renamedIIP, filesDeleted, collectedBlocks);	}	} finally {	if (undoRemoveSrc) {	tx.restoreSource();	}	if (undoRemoveDst) {	tx.restoreDst(bsps);	}	}	
dir fsdirectory unprotectedrenameto failed to rename to 

private static RenameResult renameTo(FSDirectory fsd, FSPermissionChecker pc, INodesInPath srcIIP, INodesInPath dstIIP, boolean logRetryCache) throws IOException {	if (fsd.isPermissionEnabled()) {	fsd.checkPermission(pc, srcIIP, false, null, FsAction.WRITE, null, null, false);	fsd.checkPermission(pc, dstIIP, false, FsAction.WRITE, null, null, null, false);	}	if (NameNode.stateChangeLog.isDebugEnabled()) {	
dir fsdirectory renameto to 

private static void validateDestination( String src, String dst, INode srcInode) throws IOException {	String error;	if (srcInode.isSymlink() && dst.equals(srcInode.asSymlink().getSymlinkString())) {	throw new FileAlreadyExistsException("Cannot rename symlink " + src + " to its target " + dst);	}	if (dst.startsWith(src) && dst.charAt(src.length()) == Path.SEPARATOR_CHAR) {	error = "Rename destination " + dst + " is a directory or file under source " + src;	
dir fsdirectory unprotectedrenameto 

private static void validateOverwrite( String src, String dst, boolean overwrite, INode srcInode, INode dstInode) throws IOException {	String error;	if (dstInode.isDirectory() != srcInode.isDirectory()) {	error = "Source " + src + " and destination " + dst + " must both be directories";	
dir fsdirectory unprotectedrenameto 

private static void validateOverwrite( String src, String dst, boolean overwrite, INode srcInode, INode dstInode) throws IOException {	String error;	if (dstInode.isDirectory() != srcInode.isDirectory()) {	error = "Source " + src + " and destination " + dst + " must both be directories";	throw new IOException(error);	}	if (!overwrite) {	error = "rename destination " + dst + " already exists";	
dir fsdirectory unprotectedrenameto 

throw new IOException(error);	}	if (!overwrite) {	error = "rename destination " + dst + " already exists";	throw new FileAlreadyExistsException(error);	}	if (dstInode.isDirectory()) {	final ReadOnlyList<INode> children = dstInode.asDirectory() .getChildrenList(Snapshot.CURRENT_STATE_ID);	if (!children.isEmpty()) {	error = "rename destination directory is not empty: " + dst;	
dir fsdirectory unprotectedrenameto 

private static void validateRenameSource(FSDirectory fsd, INodesInPath srcIIP) throws IOException {	String error;	final INode srcInode = srcIIP.getLastINode();	if (srcInode == null) {	error = "rename source " + srcIIP.getPath() + " is not found.";	
dir fsdirectory unprotectedrenameto 

private static void validateRenameSource(FSDirectory fsd, INodesInPath srcIIP) throws IOException {	String error;	final INode srcInode = srcIIP.getLastINode();	if (srcInode == null) {	error = "rename source " + srcIIP.getPath() + " is not found.";	throw new FileNotFoundException(error);	}	if (srcIIP.length() == 1) {	error = "rename source cannot be the root";	
dir fsdirectory unprotectedrenameto 

========================= hadoop sample_8121 =========================

when(app.getName()).thenReturn("Multiline\n\n\r\rAppName");	when(app.getUser()).thenReturn("Multiline\n\n\r\rUserName");	when(app.getQueue()).thenReturn("Multiline\n\n\r\rQueueName");	when(app.getState()).thenReturn(RMAppState.RUNNING);	when(app.getApplicationType()).thenReturn("MAPREDUCE");	when(app.getSubmitTime()).thenReturn(1000L);	RMAppMetrics metrics = new RMAppMetrics(Resource.newInstance(1234, 56), 10, 1, 16384, 64, 0, 0);	when(app.getRMAppMetrics()).thenReturn(metrics);	RMAppManager.ApplicationSummary.SummaryBuilder summary = new RMAppManager.ApplicationSummary().createAppSummary(app);	String msg = summary.toString();	
summary 

========================= hadoop sample_637 =========================

if (length == 0) {	return 0;	}	synchronized (this) {	long oldPos = getPos();	int nread = -1;	try {	seek(position);	nread = read(buffer, offset, length);	} catch (EOFException e) {	
downgrading eofexception raised trying to read bytes at offset 

========================= hadoop sample_4102 =========================

public Token<AMRMTokenIdentifier> selectToken(Text service, Collection<Token<? extends TokenIdentifier>> tokens) {	if (service == null) {	return null;	}	
looking for a token with service 

public Token<AMRMTokenIdentifier> selectToken(Text service, Collection<Token<? extends TokenIdentifier>> tokens) {	if (service == null) {	return null;	}	for (Token<? extends TokenIdentifier> token : tokens) {	
token kind is and the token s service name is 

========================= hadoop sample_2202 =========================

this.fileId = stat.getFileId();	this.blockSize = stat.getBlockSize();	this.blockReplication = stat.getReplication();	this.fileEncryptionInfo = stat.getFileEncryptionInfo();	this.cachingStrategy = new AtomicReference<>( dfsClient.getDefaultWriteCachingStrategy());	this.addBlockFlags = EnumSet.noneOf(AddBlockFlag.class);	if (flag.contains(CreateFlag.NO_LOCAL_WRITE)) {	this.addBlockFlags.add(AddBlockFlag.NO_LOCAL_WRITE);	}	if (progress != null) {	
set non null progress callback on dfsoutputstream 

private void initWritePacketSize() {	writePacketSize = dfsClient.getConf().getWritePacketSize();	if (writePacketSize > PacketReceiver.MAX_PACKET_SIZE) {	
configured write packet exceeds bytes as max using bytes 

final ExtendedBlock block = getStreamer().getBlock();	if (!getStreamer().streamerClosed() && block != null) {	lastBlockLength = block.getNumBytes();	}	}	}	if (getStreamer().getPersistBlocks().getAndSet(false) || updateLength) {	try {	dfsClient.namenode.fsync(src, fileId, dfsClient.clientName, lastBlockLength);	} catch (IOException ioe) {	
unable to persist blocks in hflush for 

}	}	synchronized(this) {	if (!getStreamer().streamerClosed()) {	getStreamer().setHflush();	}	}	} catch (InterruptedIOException interrupt) {	throw interrupt;	} catch (IOException e) {	
error while syncing 

throw new IOException(msg);	}	try {	if (retries == 0) {	throw new IOException("Unable to close file because the last block" + last + " does not have enough number of replicas.");	}	retries--;	Thread.sleep(sleeptime);	sleeptime *= 2;	if (Time.monotonicNow() - localstart > 5000) {	
could not complete retrying 

try {	if (retries == 0) {	throw new IOException("Unable to close file because the last block" + last + " does not have enough number of replicas.");	}	retries--;	Thread.sleep(sleeptime);	sleeptime *= 2;	if (Time.monotonicNow() - localstart > 5000) {	}	} catch (InterruptedException ie) {	
caught exception 

========================= hadoop sample_7000 =========================

LOG.info("newCapacity= " +  newCapacity);	LOG.info("newRack    = " +  newRack);	LOG.info("useTool    = " +  useTool);	assertEquals(capacities.length, racks.length);	int numOfDatanodes = capacities.length;	try {	cluster = new MiniDFSCluster.Builder(conf) .numDataNodes(0) .build();	cluster.getConfiguration(0).setInt(DFSConfigKeys.DFS_REPLICATION_KEY, DFSConfigKeys.DFS_REPLICATION_DEFAULT);	conf.setInt(DFSConfigKeys.DFS_REPLICATION_KEY, DFSConfigKeys.DFS_REPLICATION_DEFAULT);	if(useNamesystemSpy) {	
using spy namesystem 

private static int runBalancer(Collection<URI> namenodes, final BalancerParameters p, Configuration conf) throws IOException, InterruptedException {	final long sleeptime = conf.getLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY, DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT) * 2000 + conf.getLong(DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_KEY, DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_DEFAULT) * 1000;	LOG.info("namenodes  = " + namenodes);	LOG.info("parameters = " + p);	
print stack trace 

args.add(includeHostsFile.getAbsolutePath());	} else {	args.add(StringUtils.join(p.getIncludedNodes(), ','));	}	}	final Tool tool = new Cli();	tool.setConf(conf);	final int r = tool.run(args.toArray(new String[0]));	assertEquals("Tools should exit 0 on success", 0, r);	waitForHeartBeat(totalUsedSpace, totalCapacity, client, cluster);	
rebalancing with default ctor 

========================= hadoop sample_7515 =========================

protected synchronized List<String> getUsersForNetgroup(String netgroup) {	String[] users = null;	try {	users = getUsersForNetgroupJNI(netgroup.substring(1));	} catch (Exception e) {	if (LOG.isDebugEnabled()) {	
error getting users for netgroup 

protected synchronized List<String> getUsersForNetgroup(String netgroup) {	String[] users = null;	try {	users = getUsersForNetgroupJNI(netgroup.substring(1));	} catch (Exception e) {	if (LOG.isDebugEnabled()) {	} else {	
error getting users for netgroup 

========================= hadoop sample_3805 =========================

private void setupInternal(int numNodeManager, float timelineVersion) throws Exception {	
starting up yarn cluster 

args = mergeArgs(args, domainArgs);	}	boolean isTestingTimelineV2 = false;	if (timelineVersionWatcher.getTimelineVersion() == 2.0f) {	isTestingTimelineV2 = true;	if (!defaultFlow) {	String[] flowArgs = {	"--flow_name", "test_flow_name", "--flow_version", "test_flow_version", "--flow_run_id", "12345678" };	args = mergeArgs(args, flowArgs);	}	
setup using timeline 

}	boolean isTestingTimelineV2 = false;	if (timelineVersionWatcher.getTimelineVersion() == 2.0f) {	isTestingTimelineV2 = true;	if (!defaultFlow) {	String[] flowArgs = {	"--flow_name", "test_flow_name", "--flow_version", "test_flow_version", "--flow_run_id", "12345678" };	args = mergeArgs(args, flowArgs);	}	}	
initializing ds client 

isTestingTimelineV2 = true;	if (!defaultFlow) {	String[] flowArgs = {	"--flow_name", "test_flow_name", "--flow_version", "test_flow_version", "--flow_run_id", "12345678" };	args = mergeArgs(args, flowArgs);	}	}	final Client client = new Client(new Configuration(yarnCluster.getConfig()));	boolean initSuccess = client.init(args);	Assert.assertTrue(initSuccess);	
running ds client 

private void checkTimelineV2(boolean haveDomain, ApplicationId appId, boolean defaultFlow, ApplicationReport appReport) throws Exception {	
started 

private void checkTimelineV2(boolean haveDomain, ApplicationId appId, boolean defaultFlow, ApplicationReport appReport) throws Exception {	String tmpRoot = timelineV2StorageDir + File.separator + "entities" + File.separator;	File tmpRootFolder = new File(tmpRoot);	try {	Assert.assertTrue(tmpRootFolder.isDirectory());	String basePath = tmpRoot + YarnConfiguration.DEFAULT_RM_CLUSTER_ID + File.separator + UserGroupInformation.getCurrentUser().getShortUserName() + (defaultFlow ? File.separator + appReport.getName() + File.separator + TimelineUtils.DEFAULT_FLOW_VERSION + File.separator + appReport.getStartTime() + File.separator : File.separator + "test_flow_name" + File.separator + "test_flow_version" + File.separator + "12345678" + File.separator) + appId.toString();	
basepath 

public void testDSRestartWithPreviousRunningContainers() throws Exception {	String[] args = {	"--jar", APPMASTER_JAR, "--num_containers", "1", "--shell_command", "sleep 8", "--master_memory", "512", "--container_memory", "128", "--keep_containers_across_application_attempts" };	
initializing ds client 

public void testDSRestartWithPreviousRunningContainers() throws Exception {	String[] args = {	"--jar", APPMASTER_JAR, "--num_containers", "1", "--shell_command", "sleep 8", "--master_memory", "512", "--container_memory", "128", "--keep_containers_across_application_attempts" };	Client client = new Client(TestDSFailedAppMaster.class.getName(), new Configuration(yarnCluster.getConfig()));	client.init(args);	
running ds client 

public void testDSAttemptFailuresValidityIntervalSucess() throws Exception {	String[] args = {	"--jar", APPMASTER_JAR, "--num_containers", "1", "--shell_command", "sleep 8", "--master_memory", "512", "--container_memory", "128", "--attempt_failures_validity_interval", "2500" };	
initializing ds client 

public void testDSAttemptFailuresValidityIntervalSucess() throws Exception {	String[] args = {	"--jar", APPMASTER_JAR, "--num_containers", "1", "--shell_command", "sleep 8", "--master_memory", "512", "--container_memory", "128", "--attempt_failures_validity_interval", "2500" };	Configuration conf = yarnCluster.getConfig();	conf.setInt(YarnConfiguration.RM_AM_MAX_ATTEMPTS, 2);	Client client = new Client(TestDSSleepingAppMaster.class.getName(), new Configuration(conf));	client.init(args);	
running ds client 

public void testDSAttemptFailuresValidityIntervalFailed() throws Exception {	String[] args = {	"--jar", APPMASTER_JAR, "--num_containers", "1", "--shell_command", "sleep 8", "--master_memory", "512", "--container_memory", "128", "--attempt_failures_validity_interval", "15000" };	
initializing ds client 

public void testDSAttemptFailuresValidityIntervalFailed() throws Exception {	String[] args = {	"--jar", APPMASTER_JAR, "--num_containers", "1", "--shell_command", "sleep 8", "--master_memory", "512", "--container_memory", "128", "--attempt_failures_validity_interval", "15000" };	Configuration conf = yarnCluster.getConfig();	conf.setInt(YarnConfiguration.RM_AM_MAX_ATTEMPTS, 2);	Client client = new Client(TestDSSleepingAppMaster.class.getName(), new Configuration(conf));	client.init(args);	
running ds client 

public void testDSShellWithCommands() throws Exception {	String[] args = {	"--jar", APPMASTER_JAR, "--num_containers", "2", "--shell_command", "\"echo output_ignored;echo output_expected\"", "--master_memory", "512", "--master_vcores", "2", "--container_memory", "128", "--container_vcores", "1" };	
initializing ds client 

public void testDSShellWithCommands() throws Exception {	String[] args = {	"--jar", APPMASTER_JAR, "--num_containers", "2", "--shell_command", "\"echo output_ignored;echo output_expected\"", "--master_memory", "512", "--master_vcores", "2", "--container_memory", "128", "--container_vcores", "1" };	final Client client = new Client(new Configuration(yarnCluster.getConfig()));	boolean initSuccess = client.init(args);	Assert.assertTrue(initSuccess);	
running ds client 

public void testDSShellWithMultipleArgs() throws Exception {	String[] args = {	"--jar", APPMASTER_JAR, "--num_containers", "4", "--shell_command", "echo", "--shell_args", "HADOOP YARN MAPREDUCE HDFS", "--master_memory", "512", "--master_vcores", "2", "--container_memory", "128", "--container_vcores", "1" };	
initializing ds client 

public void testDSShellWithMultipleArgs() throws Exception {	String[] args = {	"--jar", APPMASTER_JAR, "--num_containers", "4", "--shell_command", "echo", "--shell_args", "HADOOP YARN MAPREDUCE HDFS", "--master_memory", "512", "--master_vcores", "2", "--container_memory", "128", "--container_vcores", "1" };	final Client client = new Client(new Configuration(yarnCluster.getConfig()));	boolean initSuccess = client.init(args);	Assert.assertTrue(initSuccess);	
running ds client 

}	if (!customShellScript.createNewFile()) {	Assert.fail("Can not create custom shell script file.");	}	PrintWriter fileWriter = new PrintWriter(customShellScript);	fileWriter.write("echo testDSShellWithShellScript");	fileWriter.close();	System.out.println(customShellScript.getAbsolutePath());	String[] args = {	"--jar", APPMASTER_JAR, "--num_containers", "1", "--shell_script", customShellScript.getAbsolutePath(), "--master_memory", "512", "--master_vcores", "2", "--container_memory", "128", "--container_vcores", "1" };	
initializing ds client 

}	PrintWriter fileWriter = new PrintWriter(customShellScript);	fileWriter.write("echo testDSShellWithShellScript");	fileWriter.close();	System.out.println(customShellScript.getAbsolutePath());	String[] args = {	"--jar", APPMASTER_JAR, "--num_containers", "1", "--shell_script", customShellScript.getAbsolutePath(), "--master_memory", "512", "--master_vcores", "2", "--container_memory", "128", "--container_vcores", "1" };	final Client client = new Client(new Configuration(yarnCluster.getConfig()));	boolean initSuccess = client.init(args);	Assert.assertTrue(initSuccess);	
running ds client 

public void testDSShellWithInvalidArgs() throws Exception {	Client client = new Client(new Configuration(yarnCluster.getConfig()));	
initializing ds client with no args 

public void testDSShellWithInvalidArgs() throws Exception {	Client client = new Client(new Configuration(yarnCluster.getConfig()));	try {	client.init(new String[]{});	Assert.fail("Exception is expected");	} catch (IllegalArgumentException e) {	Assert.assertTrue("The throw exception is not expected", e.getMessage().contains("No args"));	}	
initializing ds client with no jar file 

Assert.assertTrue("The throw exception is not expected", e.getMessage().contains("No args"));	}	try {	String[] args = {	"--num_containers", "2", "--shell_command", Shell.WINDOWS ? "dir" : "ls", "--master_memory", "512", "--container_memory", "128" };	client.init(args);	Assert.fail("Exception is expected");	} catch (IllegalArgumentException e) {	Assert.assertTrue("The throw exception is not expected", e.getMessage().contains("No jar"));	}	
initializing ds client with no shell command 

Assert.assertTrue("The throw exception is not expected", e.getMessage().contains("No jar"));	}	try {	String[] args = {	"--jar", APPMASTER_JAR, "--num_containers", "2", "--master_memory", "512", "--container_memory", "128" };	client.init(args);	Assert.fail("Exception is expected");	} catch (IllegalArgumentException e) {	Assert.assertTrue("The throw exception is not expected", e.getMessage().contains("No shell command"));	}	
initializing ds client with invalid no of containers 

Assert.assertTrue("The throw exception is not expected", e.getMessage().contains("No shell command"));	}	try {	String[] args = {	"--jar", APPMASTER_JAR, "--num_containers", "-1", "--shell_command", Shell.WINDOWS ? "dir" : "ls", "--master_memory", "512", "--container_memory", "128" };	client.init(args);	Assert.fail("Exception is expected");	} catch (IllegalArgumentException e) {	Assert.assertTrue("The throw exception is not expected", e.getMessage().contains("Invalid no. of containers"));	}	
initializing ds client with invalid no of vcores 

Assert.assertTrue("The throw exception is not expected", e.getMessage().contains("Invalid no. of containers"));	}	try {	String[] args = {	"--jar", APPMASTER_JAR, "--num_containers", "2", "--shell_command", Shell.WINDOWS ? "dir" : "ls", "--master_memory", "512", "--master_vcores", "-2", "--container_memory", "128", "--container_vcores", "1" };	client.init(args);	Assert.fail("Exception is expected");	} catch (IllegalArgumentException e) {	Assert.assertTrue("The throw exception is not expected", e.getMessage().contains("Invalid virtual cores specified"));	}	
initializing ds client with shell command and shell script 

Assert.assertTrue("The throw exception is not expected", e.getMessage().contains("Invalid virtual cores specified"));	}	try {	String[] args = {	"--jar", APPMASTER_JAR, "--num_containers", "2", "--shell_command", Shell.WINDOWS ? "dir" : "ls", "--master_memory", "512", "--master_vcores", "2", "--container_memory", "128", "--container_vcores", "1", "--shell_script", "test.sh" };	client.init(args);	Assert.fail("Exception is expected");	} catch (IllegalArgumentException e) {	Assert.assertTrue("The throw exception is not expected", e.getMessage().contains("Can not specify shell_command option " + "and shell_script option at the same time"));	}	
initializing ds client without shell command and shell script 

public void testContainerLaunchFailureHandling() throws Exception {	String[] args = {	"--jar", APPMASTER_JAR, "--num_containers", "2", "--shell_command", Shell.WINDOWS ? "dir" : "ls", "--master_memory", "512", "--container_memory", "128" };	
initializing ds client 

public void testContainerLaunchFailureHandling() throws Exception {	String[] args = {	"--jar", APPMASTER_JAR, "--num_containers", "2", "--shell_command", Shell.WINDOWS ? "dir" : "ls", "--master_memory", "512", "--container_memory", "128" };	Client client = new Client(ContainerLaunchFailAppMaster.class.getName(), new Configuration(yarnCluster.getConfig()));	boolean initSuccess = client.init(args);	Assert.assertTrue(initSuccess);	
running ds client 

public void testDebugFlag() throws Exception {	String[] args = {	"--jar", APPMASTER_JAR, "--num_containers", "2", "--shell_command", Shell.WINDOWS ? "dir" : "ls", "--master_memory", "512", "--master_vcores", "2", "--container_memory", "128", "--container_vcores", "1", "--debug" };	
initializing ds client 

public void testDebugFlag() throws Exception {	String[] args = {	"--jar", APPMASTER_JAR, "--num_containers", "2", "--shell_command", Shell.WINDOWS ? "dir" : "ls", "--master_memory", "512", "--master_vcores", "2", "--container_memory", "128", "--container_vcores", "1", "--debug" };	Client client = new Client(new Configuration(yarnCluster.getConfig()));	Assert.assertTrue(client.init(args));	
running ds client 

========================= hadoop sample_6 =========================

Text key = reader.createKey();	Class keyClass = key.getClass();	Text value = reader.createValue();	Class valueClass = value.getClass();	assertEquals("Key class is Text.", Text.class, keyClass);	assertEquals("Value class is Text.", Text.class, valueClass);	try {	int count = 0;	while (reader.next(key, value)) {	int v = Integer.parseInt(value.toString());	
read 

Class keyClass = key.getClass();	Text value = reader.createValue();	Class valueClass = value.getClass();	assertEquals("Key class is Text.", Text.class, keyClass);	assertEquals("Value class is Text.", Text.class, valueClass);	try {	int count = 0;	while (reader.next(key, value)) {	int v = Integer.parseInt(value.toString());	if (bits.get(v)) {	
conflict with in split at position 

========================= hadoop sample_5482 =========================

try {	remoteFS = getFileSystem(conf);	} catch (IOException e) {	throw new YarnRuntimeException( "Unable to get Remote FileSystem instance", e);	}	boolean remoteExists = true;	Path remoteRootLogDir = getRemoteRootLogDir();	try {	FsPermission perms = remoteFS.getFileStatus(remoteRootLogDir).getPermission();	if (!perms.equals(TLDIR_PERMISSIONS) && logPermError) {	
remote root log dir already exist but with incorrect permissions expected found the cluster may have problems with multiple users 

logPermError = false;	} else {	logPermError = true;	}	} catch (FileNotFoundException e) {	remoteExists = false;	} catch (IOException e) {	throw new YarnRuntimeException( "Failed to check permissions for dir [" + remoteRootLogDir + "]", e);	}	if (!remoteExists) {	
remote root log dir does not exist attempting to create it 

if (!remoteExists) {	try {	Path qualified = remoteRootLogDir.makeQualified(remoteFS.getUri(), remoteFS.getWorkingDirectory());	remoteFS.mkdirs(qualified, new FsPermission(TLDIR_PERMISSIONS));	remoteFS.setPermission(qualified, new FsPermission(TLDIR_PERMISSIONS));	UserGroupInformation loginUser = UserGroupInformation.getLoginUser();	String primaryGroupName = null;	try {	primaryGroupName = loginUser.getPrimaryGroupName();	} catch (IOException e) {	
no primary group found the remote root log directory will be created with the hdfs superuser being its group owner jobhistoryserver may be unable to read the directory 

Path userDir = LogAggregationUtils.getRemoteLogUserDir( remoteRootLogDir, user);	userDir = userDir.makeQualified(remoteFS.getUri(), remoteFS.getWorkingDirectory());	if (!checkExists(remoteFS, userDir, APP_DIR_PERMISSIONS)) {	createDir(remoteFS, userDir, APP_DIR_PERMISSIONS);	}	createDir(remoteFS, suffixDir, APP_DIR_PERMISSIONS);	}	createDir(remoteFS, appDir, APP_DIR_PERMISSIONS);	}	} catch (IOException e) {	
failed to setup application log directory for 

========================= hadoop sample_2347 =========================

public boolean tryFence(HAServiceTarget target, String args) throws BadFencingConfigurationException {	
tryfence 

public boolean tryFence(HAServiceTarget target, String args) throws BadFencingConfigurationException {	DummyHAService svc = (DummyHAService)target;	synchronized (svc) {	svc.fenceCount++;	}	if (svc.failToFence) {	
injected failure to fence 

========================= hadoop sample_3106 =========================

public static void afterClass() {	try {	if (localFs != null) {	localFs.close();	}	} catch (IOException ioe) {	
io exception in closing file system 

========================= hadoop sample_2594 =========================

public void abort() {	if (stream == null) return;	try {	stream.abort();	} catch (IOException ioe) {	
unable to abort stream 

private void disableAndReportErrorOnJournals(List<JournalAndStream> badJournals) {	if (badJournals == null || badJournals.isEmpty()) {	return;	}	for (JournalAndStream j : badJournals) {	
disabling journal 

for (JournalAndStream jas : journals) {	try {	closure.apply(jas);	} catch (Throwable t) {	if (jas.isRequired()) {	final String msg = "Error: " + status + " failed for required journal (" + jas + ")";	LOG.fatal(msg, t);	abortAllJournals();	terminate(1, msg);	} else {	
error failed for journal 

abortAllJournals();	terminate(1, msg);	} else {	badJAS.add(jas);	}	}	}	disableAndReportErrorOnJournals(badJAS);	if (!NameNodeResourcePolicy.areResourcesAvailable(journals, minimumRedundantJournals)) {	String message = status + " failed for too many journals";	
error 

public void setOutputBufferCapacity(final int size) {	try {	mapJournalsAndReportErrors(new JournalClosure() {	public void apply(JournalAndStream jas) throws IOException {	jas.getManager().setOutputBufferCapacity(size);	}	}, "setOutputBufferCapacity");	} catch (IOException e) {	
error in setting outputbuffer capacity 

========================= hadoop sample_8033 =========================

newEnv.put(ApplicationConstants.Environment.HADOOP_CLASSPATH.name(), "hadoop.tgz");	setEnv(newEnv);	Map<String, String> environment = new HashMap<String, String>();	MRApps.setClasspath(environment, conf);	assertTrue(environment.get(ApplicationConstants.Environment.CLASSPATH.name()).startsWith( ApplicationConstants.Environment.PWD.$$() + ApplicationConstants.CLASS_PATH_SEPARATOR));	assertTrue(environment.get(ApplicationConstants.Environment. HADOOP_CLASSPATH.name()).startsWith( ApplicationConstants.Environment.PWD.$$() + ApplicationConstants.CLASS_PATH_SEPARATOR));	String confClasspath = job.getConfiguration().get( YarnConfiguration.YARN_APPLICATION_CLASSPATH, StringUtils.join(",", YarnConfiguration.DEFAULT_YARN_CROSS_PLATFORM_APPLICATION_CLASSPATH));	if (confClasspath != null) {	confClasspath = confClasspath.replaceAll(",\\s*", ApplicationConstants.CLASS_PATH_SEPARATOR) .trim();	}	
classpath 

newEnv.put(ApplicationConstants.Environment.HADOOP_CLASSPATH.name(), "hadoop.tgz");	setEnv(newEnv);	Map<String, String> environment = new HashMap<String, String>();	MRApps.setClasspath(environment, conf);	assertTrue(environment.get(ApplicationConstants.Environment.CLASSPATH.name()).startsWith( ApplicationConstants.Environment.PWD.$$() + ApplicationConstants.CLASS_PATH_SEPARATOR));	assertTrue(environment.get(ApplicationConstants.Environment. HADOOP_CLASSPATH.name()).startsWith( ApplicationConstants.Environment.PWD.$$() + ApplicationConstants.CLASS_PATH_SEPARATOR));	String confClasspath = job.getConfiguration().get( YarnConfiguration.YARN_APPLICATION_CLASSPATH, StringUtils.join(",", YarnConfiguration.DEFAULT_YARN_CROSS_PLATFORM_APPLICATION_CLASSPATH));	if (confClasspath != null) {	confClasspath = confClasspath.replaceAll(",\\s*", ApplicationConstants.CLASS_PATH_SEPARATOR) .trim();	}	
confclasspath 

setEnv(newEnv);	Map<String, String> environment = new HashMap<String, String>();	MRApps.setClasspath(environment, conf);	assertTrue(environment.get(ApplicationConstants.Environment.CLASSPATH.name()).startsWith( ApplicationConstants.Environment.PWD.$$() + ApplicationConstants.CLASS_PATH_SEPARATOR));	assertTrue(environment.get(ApplicationConstants.Environment. HADOOP_CLASSPATH.name()).startsWith( ApplicationConstants.Environment.PWD.$$() + ApplicationConstants.CLASS_PATH_SEPARATOR));	String confClasspath = job.getConfiguration().get( YarnConfiguration.YARN_APPLICATION_CLASSPATH, StringUtils.join(",", YarnConfiguration.DEFAULT_YARN_CROSS_PLATFORM_APPLICATION_CLASSPATH));	if (confClasspath != null) {	confClasspath = confClasspath.replaceAll(",\\s*", ApplicationConstants.CLASS_PATH_SEPARATOR) .trim();	}	assertTrue(environment.get( ApplicationConstants.Environment.CLASSPATH.name()).contains( confClasspath));	
hadoop classpath 

for (Class cl : classes) {	if ("java.util.Collections$UnmodifiableMap".equals(cl.getName())) {	Field field = cl.getDeclaredField("m");	field.setAccessible(true);	Object obj = field.get(env);	Map<String, String> map = (Map<String, String>) obj;	map.putAll(newEnv);	}	}	} catch (Exception e1) {	
hack env on linux doesn t work 

field.setAccessible(true);	Object obj = field.get(env);	Map<String, String> map = (Map<String, String>) obj;	map.putAll(newEnv);	}	}	} catch (Exception e1) {	throw new RuntimeException(e1);	}	} catch (Exception e) {	
hack env on windows doesn t work 

========================= hadoop sample_4440 =========================

public void testOpenNonExistingFile() throws IOException {	final Path p = new Path("/test/testOpenNonExistingFile");	try {	final FSDataInputStream in = fs.open(p);	in.close();	fail("didn't expect to get here");	} catch (FileNotFoundException fnfe) {	
expected 

========================= hadoop sample_6187 =========================

float percentUsed, percentRemaining, percentBpUsed;	for (final DatanodeDescriptor datanode : live) {	used = datanode.getDfsUsed();	remaining = datanode.getRemaining();	nonDFSUsed = datanode.getNonDfsUsed();	configCapacity = datanode.getCapacity();	percentUsed = datanode.getDfsUsedPercent();	percentRemaining = datanode.getRemainingPercent();	bpUsed = datanode.getBlockPoolUsed();	percentBpUsed = datanode.getBlockPoolUsedPercent();	
datanode configcapacity used non dfs used remaining perentused percentremaining 

long diskCapacity = numOfDataDirs * utils.getRawCapacity();	reserved *= numOfDataDirs;	configCapacity = namesystem.getCapacityTotal();	used = namesystem.getCapacityUsed();	nonDFSUsed = namesystem.getNonDfsUsedSpace();	remaining = namesystem.getCapacityRemaining();	percentUsed = namesystem.getPercentUsed();	percentRemaining = namesystem.getPercentRemaining();	bpUsed = namesystem.getBlockPoolUsedSpace();	percentBpUsed = namesystem.getPercentBlockPoolUsed();	
data node directory 

long diskCapacity = numOfDataDirs * utils.getRawCapacity();	reserved *= numOfDataDirs;	configCapacity = namesystem.getCapacityTotal();	used = namesystem.getCapacityUsed();	nonDFSUsed = namesystem.getNonDfsUsedSpace();	remaining = namesystem.getCapacityRemaining();	percentUsed = namesystem.getPercentUsed();	percentRemaining = namesystem.getPercentRemaining();	bpUsed = namesystem.getBlockPoolUsedSpace();	percentBpUsed = namesystem.getPercentBlockPoolUsed();	
name node diskcapacity configcapacity reserved used remaining nondfsused remaining percentused percentremaining bpused percentbpused 

========================= hadoop sample_7329 =========================

localFS = FileSystem.getLocal(conf);	if (!localFS.exists(dbPath)) {	if (!localFS.mkdirs(dbPath)) {	throw new IOException("Couldn't create directory for leveldb " + "timeline store " + dbPath);	}	localFS.setPermission(dbPath, LEVELDB_DIR_UMASK);	}	} finally {	IOUtils.cleanupWithLogger(LOG, localFS);	}	
using leveldb path 

localFS.setPermission(dbPath, LEVELDB_DIR_UMASK);	}	} finally {	IOUtils.cleanupWithLogger(LOG, localFS);	}	try {	db = factory.open(new File(dbPath.toString()), options);	} catch (IOException ioe) {	File dbFile = new File(dbPath.toString());	File backupPath = new File( dbPath.toString() + BACKUP_EXT + Time.monotonicNow());	
incurred exception while loading leveldb database backing up at 

}	} finally {	IOUtils.cleanupWithLogger(LOG, localFS);	}	try {	db = factory.open(new File(dbPath.toString()), options);	} catch (IOException ioe) {	File dbFile = new File(dbPath.toString());	File backupPath = new File( dbPath.toString() + BACKUP_EXT + Time.monotonicNow());	FileUtils.copyDirectory(dbFile, backupPath);	
going to try repair 

protected void serviceStop() throws Exception {	if (deletionThread != null) {	deletionThread.interrupt();	
waiting for deletion thread to complete its current action 

protected void serviceStop() throws Exception {	if (deletionThread != null) {	deletionThread.interrupt();	try {	deletionThread.join();	} catch (InterruptedException e) {	
interrupted while waiting for deletion thread to complete closing db now 

public EntityDeletionThread(Configuration conf) {	ttl  = conf.getLong(YarnConfiguration.TIMELINE_SERVICE_TTL_MS, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_TTL_MS);	ttlInterval = conf.getLong( YarnConfiguration.TIMELINE_SERVICE_LEVELDB_TTL_INTERVAL_MS, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_LEVELDB_TTL_INTERVAL_MS);	
starting deletion thread with ttl and cycle interval 

public void run() {	while (true) {	long timestamp = System.currentTimeMillis() - ttl;	try {	discardOldEntities(timestamp);	Thread.sleep(ttlInterval);	} catch (IOException e) {	LOG.error(e.toString());	} catch (InterruptedException e) {	
deletion thread received interrupt exiting 

if (event != null) {	entity.addEvent(event);	}	}	} else if (key[prefixlen] == DOMAIN_ID_COLUMN[0]) {	byte[] v = iterator.peekNext().getValue();	String domainId = new String(v, Charset.forName("UTF-8"));	entity.setDomainId(domainId);	} else {	if (key[prefixlen] != INVISIBLE_REVERSE_RELATED_ENTITIES_COLUMN[0]) {	
found unexpected column for entity s of type s 

if (!allowEmptyDomainId) {	handleError(entity, response, TimelinePutError.NO_DOMAIN);	return;	}	} else {	writeBatch.put(key, entity.getDomainId().getBytes(Charset.forName("UTF-8")));	writePrimaryFilterEntries(writeBatch, primaryFilters, key, entity.getDomainId().getBytes(Charset.forName("UTF-8")));	}	db.write(writeBatch);	} catch (DBException de) {	
error putting entity of type 

return;	}	} else {	writeBatch.put(key, entity.getDomainId().getBytes(Charset.forName("UTF-8")));	writePrimaryFilterEntries(writeBatch, primaryFilters, key, entity.getDomainId().getBytes(Charset.forName("UTF-8")));	}	db.write(writeBatch);	} catch (DBException de) {	handleError(entity, response, TimelinePutError.IO_EXCEPTION);	} catch (IOException e) {	
error putting entity of type 

StartAndInsertTime relatedEntityStartAndInsertTime = getAndSetStartTime(relatedEntity.getId(), relatedEntity.getType(), readReverseOrderedLong(revStartTime, 0), null);	if (relatedEntityStartAndInsertTime == null) {	throw new IOException("Error setting start time for related entity");	}	byte[] relatedEntityStartTime = writeReverseOrderedLong( relatedEntityStartAndInsertTime.startTime);	byte[] key = createDomainIdKey(relatedEntity.getId(), relatedEntity.getType(), relatedEntityStartTime);	db.put(key, entity.getDomainId().getBytes(Charset.forName("UTF-8")));	db.put(createRelatedEntityKey(relatedEntity.getId(), relatedEntity.getType(), relatedEntityStartTime, entity.getEntityId(), entity.getEntityType()), EMPTY_BYTES);	db.put(createEntityMarkerKey(relatedEntity.getId(), relatedEntity.getType(), relatedEntityStartTime), writeReverseOrderedLong(relatedEntityStartAndInsertTime .insertTime));	} catch (DBException de) {	
error putting related entity of type for entity of type 

throw new IOException("Error setting start time for related entity");	}	byte[] relatedEntityStartTime = writeReverseOrderedLong( relatedEntityStartAndInsertTime.startTime);	byte[] key = createDomainIdKey(relatedEntity.getId(), relatedEntity.getType(), relatedEntityStartTime);	db.put(key, entity.getDomainId().getBytes(Charset.forName("UTF-8")));	db.put(createRelatedEntityKey(relatedEntity.getId(), relatedEntity.getType(), relatedEntityStartTime, entity.getEntityId(), entity.getEntityType()), EMPTY_BYTES);	db.put(createEntityMarkerKey(relatedEntity.getId(), relatedEntity.getType(), relatedEntityStartTime), writeReverseOrderedLong(relatedEntityStartAndInsertTime .insertTime));	} catch (DBException de) {	handleError(entity, response, TimelinePutError.IO_EXCEPTION);	} catch (IOException e) {	
error putting related entity of type for entity of type 

private void checkVersion() throws IOException {	Version loadedVersion = loadVersion();	
loaded timeline store version info 

private void checkVersion() throws IOException {	Version loadedVersion = loadVersion();	if (loadedVersion.equals(getCurrentVersion())) {	return;	}	if (loadedVersion.isCompatibleTo(getCurrentVersion())) {	
storing timeline store version info 

} else if (key[prefix.length] == OWNER_COLUMN[0]) {	domain.setOwner(new String(value, Charset.forName("UTF-8")));	} else if (key[prefix.length] == READER_COLUMN[0]) {	domain.setReaders(new String(value, Charset.forName("UTF-8")));	} else if (key[prefix.length] == WRITER_COLUMN[0]) {	domain.setWriters(new String(value, Charset.forName("UTF-8")));	} else if (key[prefix.length] == TIMESTAMP_COLUMN[0]) {	domain.setCreatedTime(readReverseOrderedLong(value, 0));	domain.setModifiedTime(readReverseOrderedLong(value, 8));	} else {	
unrecognized domain column 

========================= hadoop sample_2081 =========================

public void start(String fsimage) throws IOException {	try {	initServer(fsimage);	channel.closeFuture().await();	} catch (InterruptedException e) {	
interrupted stopping the webimageviewer 

final FSImageLoader loader = FSImageLoader.load(fsimage);	bootstrap.childHandler(new ChannelInitializer<SocketChannel>() {	protected void initChannel(SocketChannel ch) throws Exception {	ChannelPipeline p = ch.pipeline();	p.addLast(new HttpRequestDecoder(), new StringEncoder(), new HttpResponseEncoder(), new FSImageHandler(loader, allChannels));	}	});	channel = bootstrap.bind(address).sync().channel();	allChannels.add(channel);	address = (InetSocketAddress) channel.localAddress();	
webimageviewer started listening on press ctrl c to stop the viewer 

========================= hadoop sample_7779 =========================

public static void main(String[] args) throws IOException {	TeraScheduler problem = new TeraScheduler("block-loc.txt", "nodes");	for(Host host: problem.hosts) {	System.out.println(host);	}	
starting solve 

if (problem.splits[i].isAssigned) {	System.out.println("sched: " + problem.splits[i]);	} else {	leftOvers.add(problem.splits[i]);	}	}	for(Split cur: leftOvers) {	System.out.println("left: " + cur);	}	System.out.println("left over: " + leftOvers.size());	
done 

========================= hadoop sample_5744 =========================

int[] ret = new int[2];	if (yarnProcessors < 0.01f) {	throw new IllegalArgumentException("Number of processors can't be <= 0.");	}	int quotaUS = MAX_QUOTA_US;	int periodUS = (int) (MAX_QUOTA_US / yarnProcessors);	if (yarnProcessors < 1.0f) {	periodUS = MAX_QUOTA_US;	quotaUS = (int) (periodUS * yarnProcessors);	if (quotaUS < MIN_PERIOD_US) {	
the quota calculated for the cgroup was too low the minimum value is calculated value is setting quota to minimum value 

int quotaUS = MAX_QUOTA_US;	int periodUS = (int) (MAX_QUOTA_US / yarnProcessors);	if (yarnProcessors < 1.0f) {	periodUS = MAX_QUOTA_US;	quotaUS = (int) (periodUS * yarnProcessors);	if (quotaUS < MIN_PERIOD_US) {	quotaUS = MIN_PERIOD_US;	}	}	if (periodUS < MIN_PERIOD_US) {	
the period calculated for the cgroup was too low the minimum value is calculated value is using all available cpu 

if (strictResourceUsageMode) {	if (nodeVCores != containerVCores) {	float containerCPU = (containerVCores * yarnProcessors) / (float) nodeVCores;	int[] limits = getOverallLimits(containerCPU);	cGroupsHandler.updateCGroupParam(CPU, cgroupId, CGroupsHandler.CGROUP_CPU_PERIOD_US, String.valueOf(limits[0]));	cGroupsHandler.updateCGroupParam(CPU, cgroupId, CGroupsHandler.CGROUP_CPU_QUOTA_US, String.valueOf(limits[1]));	}	}	} catch (ResourceHandlerException re) {	cGroupsHandler.deleteCGroup(CPU, cgroupId);	
could not update cgroup for container 

========================= hadoop sample_1843 =========================

public abstract void confirmMutation(boolean isValid) throws Exception;	public abstract Configuration retrieve();	public abstract List<LogMutation> getConfirmedConfHistory(long fromId);	protected abstract Version getConfStoreVersion() throws Exception;	protected abstract void storeVersion() throws Exception;	protected abstract Version getCurrentVersion();	public void checkVersion() throws Exception {	Version loadedVersion = getConfStoreVersion();	
loaded configuration store version info 

protected abstract Version getCurrentVersion();	public void checkVersion() throws Exception {	Version loadedVersion = getConfStoreVersion();	if (loadedVersion != null && loadedVersion.equals(getCurrentVersion())) {	return;	}	if (loadedVersion == null) {	loadedVersion = getCurrentVersion();	}	if (loadedVersion.isCompatibleTo(getCurrentVersion())) {	
storing configuration store version info 

========================= hadoop sample_924 =========================

public void replaceLabelsOnNode(Map<NodeId, Set<String>> replaceLabelsToNode) throws IOException {	try {	writeLock.lock();	Map<NodeId, Set<String>> effectiveModifiedLabelMappings = getModifiedNodeLabelsMappings(replaceLabelsToNode);	if(effectiveModifiedLabelMappings.isEmpty()) {	
no modified node label mapping to replace 

public void activateNode(NodeId nodeId, Resource resource) {	try {	writeLock.lock();	Map<String, Host> before = cloneNodeMap(ImmutableSet.of(nodeId));	createHostIfNonExisted(nodeId.getHost());	try {	createNodeIfNonExisted(nodeId);	} catch (IOException e) {	
this shouldn t happen cannot get host in nodecollection associated to the node being activated 

========================= hadoop sample_1140 =========================

this.rawOut = out;	this.start = this.rawOut.getPos();	if (codec != null) {	this.compressor = CodecPool.getCompressor(codec);	if (this.compressor != null) {	this.compressor.reset();	this.compressedOut = codec.createOutputStream(checksumOut, compressor);	this.out = new FSDataOutputStream(this.compressedOut,  null);	this.compressOutput = true;	} else {	
could not obtain compressor from codecpool 

public Reader(Configuration conf, FSDataInputStream in, long length, CompressionCodec codec, Counters.Counter readsCounter) throws IOException {	readRecordsCounter = readsCounter;	checksumIn = new IFileInputStream(in,length, conf);	if (codec != null) {	decompressor = CodecPool.getDecompressor(codec);	if (decompressor != null) {	this.in = codec.createInputStream(checksumIn, decompressor);	} else {	
could not obtain decompressor from codecpool 

========================= hadoop sample_4712 =========================

public void testWithWritable() throws Exception {	conf.set("io.serializations", "org.apache.hadoop.io.serializer.WritableSerialization");	
testing defaultstringifier with text 

for(int i=0;i<10;i++) {	StringBuilder builder = new StringBuilder();	int strLen = random.nextInt(40);	for(int j=0; j< strLen; j++) {	builder.append(alphabet[random.nextInt(alphabet.length)]);	}	Text text = new Text(builder.toString());	DefaultStringifier<Text> stringifier = new DefaultStringifier<Text>(conf, Text.class);	String str = stringifier.toString(text);	Text claimedText = stringifier.fromString(str);	
object 

for(int i=0;i<10;i++) {	StringBuilder builder = new StringBuilder();	int strLen = random.nextInt(40);	for(int j=0; j< strLen; j++) {	builder.append(alphabet[random.nextInt(alphabet.length)]);	}	Text text = new Text(builder.toString());	DefaultStringifier<Text> stringifier = new DefaultStringifier<Text>(conf, Text.class);	String str = stringifier.toString(text);	Text claimedText = stringifier.fromString(str);	
string representation of the object 

public void testWithJavaSerialization() throws Exception {	conf.set("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization");	
testing defaultstringifier with serializable integer 

public void testWithJavaSerialization() throws Exception {	conf.set("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization");	Integer testInt = Integer.valueOf(42);	DefaultStringifier<Integer> stringifier = new DefaultStringifier<Integer>(conf, Integer.class);	String str = stringifier.toString(testInt);	Integer claimedInt = stringifier.fromString(str);	
string representation of the object 

public void testStoreLoad() throws IOException {	
testing defaultstringifier store and load 

public void testStoreLoadArray() throws IOException {	
testing defaultstringifier storearray and loadarray 

========================= hadoop sample_3038 =========================

private void setupQueueConfiguration(CapacitySchedulerConfiguration conf) {	conf.setQueues(CapacitySchedulerConfiguration.ROOT, new String[] { Q1, Q2 });	conf.setCapacity(Q1_PATH, 10);	conf.setCapacity(Q2_PATH, 90);	
setup top level queues and 

========================= hadoop sample_498 =========================

protected void serviceInit(Configuration conf) throws Exception {	this.maxThreadPoolSize = conf.getInt( YarnConfiguration.NM_CLIENT_ASYNC_THREAD_POOL_MAX_SIZE, YarnConfiguration.DEFAULT_NM_CLIENT_ASYNC_THREAD_POOL_MAX_SIZE);	
upper bound of the thread pool size is 

threadPool = new ThreadPoolExecutor(initSize, Integer.MAX_VALUE, 1, TimeUnit.HOURS, new LinkedBlockingQueue<Runnable>(), tf);	eventDispatcherThread = new Thread() {	public void run() {	ContainerEvent event = null;	Set<String> allNodes = new HashSet<String>();	while (!stopped.get() && !Thread.currentThread().isInterrupted()) {	try {	event = events.take();	} catch (InterruptedException e) {	if (!stopped.get()) {	
returning thread interrupted 

}	return;	}	allNodes.add(event.getNodeId().toString());	int threadPoolSize = threadPool.getCorePoolSize();	if (threadPoolSize != maxThreadPoolSize) {	int nodeNum = allNodes.size();	int idealThreadPoolSize = Math.min(maxThreadPoolSize, nodeNum);	if (threadPoolSize < idealThreadPoolSize) {	int newThreadPoolSize = Math.min(maxThreadPoolSize, idealThreadPoolSize + INITIAL_THREAD_POOL_SIZE);	
set nmclientasync thread pool size to as the number of nodes to talk to is 

protected void serviceStop() throws Exception {	if (stopped.getAndSet(true)) {	return;	}	if (eventDispatcherThread != null) {	eventDispatcherThread.interrupt();	try {	eventDispatcherThread.join();	} catch (InterruptedException e) {	
the thread of didn t finish normally 

public void startContainerAsync( Container container, ContainerLaunchContext containerLaunchContext) {	if (containers.putIfAbsent(container.getId(), new StatefulContainer(this, container.getId())) != null) {	callbackHandler.onStartContainerError(container.getId(), RPCUtil.getRemoteException("Container " + container.getId() + " is already started or scheduled to start"));	}	try {	events.put(new StartContainerEvent(container, containerLaunchContext));	} catch (InterruptedException e) {	
exception when scheduling the event of starting container 

public void increaseContainerResourceAsync(Container container) {	if (!(callbackHandler instanceof AbstractCallbackHandler)) {	
callback handler does not implement container resource increase callback methods 

if (!(callbackHandler instanceof AbstractCallbackHandler)) {	return;	}	AbstractCallbackHandler handler = (AbstractCallbackHandler) callbackHandler;	if (containers.get(container.getId()) == null) {	handler.onIncreaseContainerResourceError( container.getId(), RPCUtil.getRemoteException( "Container " + container.getId() + " is neither started nor scheduled to start"));	}	try {	events.put(new UpdateContainerResourceEvent(container, true));	} catch (InterruptedException e) {	
exception when scheduling the event of increasing resource of container 

public void updateContainerResourceAsync(Container container) {	if (!(callbackHandler instanceof AbstractCallbackHandler)) {	
callback handler does not implement container resource increase callback methods 

if (!(callbackHandler instanceof AbstractCallbackHandler)) {	return;	}	AbstractCallbackHandler handler = (AbstractCallbackHandler) callbackHandler;	if (containers.get(container.getId()) == null) {	handler.onUpdateContainerResourceError( container.getId(), RPCUtil.getRemoteException( "Container " + container.getId() + " is neither started nor scheduled to start"));	}	try {	events.put(new UpdateContainerResourceEvent(container, false));	} catch (InterruptedException e) {	
exception when scheduling the event of increasing resource of container 

public void reInitializeContainerAsync(ContainerId containerId, ContainerLaunchContext containerLaunchContex, boolean autoCommit){	if (!(callbackHandler instanceof AbstractCallbackHandler)) {	
callback handler does not implement container re initialize callback methods 

if (!(callbackHandler instanceof AbstractCallbackHandler)) {	return;	}	AbstractCallbackHandler handler = (AbstractCallbackHandler) callbackHandler;	if (containers.get(containerId) == null) {	handler.onContainerReInitializeError( containerId, RPCUtil.getRemoteException( "Container " + containerId + " is not started"));	}	try {	events.put(new ReInitializeContainerEvevnt(containerId, client.getNodeIdOfStartedContainer(containerId), containerLaunchContex, autoCommit));	} catch (InterruptedException e) {	
exception when scheduling the event of re initializing of container 

public void restartContainerAsync(ContainerId containerId){	if (!(callbackHandler instanceof AbstractCallbackHandler)) {	
callback handler does not implement container restart callback methods 

if (!(callbackHandler instanceof AbstractCallbackHandler)) {	return;	}	AbstractCallbackHandler handler = (AbstractCallbackHandler) callbackHandler;	if (containers.get(containerId) == null) {	handler.onContainerRestartError( containerId, RPCUtil.getRemoteException( "Container " + containerId + " is not started"));	}	try {	events.put(new ContainerEvent(containerId, client.getNodeIdOfStartedContainer(containerId), null, ContainerEventType.RESTART_CONTAINER));	} catch (InterruptedException e) {	
exception when scheduling the event of restart of container 

public void rollbackLastReInitializationAsync(ContainerId containerId){	if (!(callbackHandler instanceof AbstractCallbackHandler)) {	
callback handler does not implement container rollback callback methods 

if (!(callbackHandler instanceof AbstractCallbackHandler)) {	return;	}	AbstractCallbackHandler handler = (AbstractCallbackHandler) callbackHandler;	if (containers.get(containerId) == null) {	handler.onRollbackLastReInitializationError( containerId, RPCUtil.getRemoteException( "Container " + containerId + " is not started"));	}	try {	events.put(new ContainerEvent(containerId, client.getNodeIdOfStartedContainer(containerId), null, ContainerEventType.ROLLBACK_LAST_REINIT));	} catch (InterruptedException e) {	
exception when scheduling the event rollback re initialization of container 

public void commitLastReInitializationAsync(ContainerId containerId){	if (!(callbackHandler instanceof AbstractCallbackHandler)) {	
callback handler does not implement container commit last re initialization callback methods 

if (!(callbackHandler instanceof AbstractCallbackHandler)) {	return;	}	AbstractCallbackHandler handler = (AbstractCallbackHandler) callbackHandler;	if (containers.get(containerId) == null) {	handler.onCommitLastReInitializationError( containerId, RPCUtil.getRemoteException( "Container " + containerId + " is not started"));	}	try {	events.put(new ContainerEvent(containerId, client.getNodeIdOfStartedContainer(containerId), null, ContainerEventType.COMMIT_LAST_REINT));	} catch (InterruptedException e) {	
exception when scheduling the event commit re initialization of container 

public void stopContainerAsync(ContainerId containerId, NodeId nodeId) {	if (containers.get(containerId) == null) {	callbackHandler.onStopContainerError(containerId, RPCUtil.getRemoteException("Container " + containerId + " is neither started nor scheduled to start"));	}	try {	events.put(new ContainerEvent(containerId, nodeId, null, ContainerEventType.STOP_CONTAINER));	} catch (InterruptedException e) {	
exception when scheduling the event of stopping container 

public void getContainerStatusAsync(ContainerId containerId, NodeId nodeId) {	try {	events.put(new ContainerEvent(containerId, nodeId, null, ContainerEventType.QUERY_CONTAINER));	} catch (InterruptedException e) {	
exception when scheduling the event of querying the status of container 

try {	StartContainerEvent scEvent = null;	if (event instanceof StartContainerEvent) {	scEvent = (StartContainerEvent) event;	}	assert scEvent != null;	Map<String, ByteBuffer> allServiceResponse = container.nmClientAsync.getClient().startContainer( scEvent.getContainer(), scEvent.getContainerLaunchContext());	try {	container.nmClientAsync.getCallbackHandler().onContainerStarted( containerId, allServiceResponse);	} catch (Throwable thr) {	
unchecked exception is thrown from oncontainerstarted for container 

private ContainerState onExceptionRaised(StatefulContainer container, ContainerEvent event, Throwable t) {	try {	container.nmClientAsync.getCallbackHandler().onStartContainerError( event.getContainerId(), t);	} catch (Throwable thr) {	
unchecked exception is thrown from onstartcontainererror for container 

public void transition( StatefulContainer container, ContainerEvent event) {	boolean isIncreaseEvent = false;	if (!(container.nmClientAsync.getCallbackHandler() instanceof AbstractCallbackHandler)) {	
callback handler does not implement container resource update callback methods 

UpdateContainerResourceEvent updateEvent = (UpdateContainerResourceEvent) event;	container.nmClientAsync.getClient().updateContainerResource( updateEvent.getContainer());	isIncreaseEvent = updateEvent.isIncreaseEvent;	try {	if (isIncreaseEvent) {	handler.onContainerResourceIncreased(updateEvent.getContainerId(), updateEvent.getContainer().getResource());	} else {	handler.onContainerResourceUpdated(updateEvent.getContainerId(), updateEvent.getContainer().getResource());	}	} catch (Throwable thr) {	
unchecked exception is thrown from oncontainerresourceupdated for container 

} catch (Throwable thr) {	}	} catch (Exception e) {	try {	if (isIncreaseEvent) {	handler .onIncreaseContainerResourceError(event.getContainerId(), e);	} else {	handler.onUpdateContainerResourceError(event.getContainerId(), e);	}	} catch (Throwable thr) {	
unchecked exception is thrown from onupdatecontainerresourceerror for container 

public ContainerState transition(StatefulContainer container, ContainerEvent containerEvent) {	ContainerId containerId = containerEvent.getContainerId();	AbstractCallbackHandler handler = (AbstractCallbackHandler) container .nmClientAsync.getCallbackHandler();	Throwable handlerError = null;	try {	switch(containerEvent.getType()) {	case REINITIALIZE_CONTAINER: if (!(containerEvent instanceof ReInitializeContainerEvevnt)) {	
unexpected event 

handlerError = tr;	}	break;	case COMMIT_LAST_REINT: container.nmClientAsync.getClient() .commitLastReInitialization(containerId);	try {	handler.onCommitLastReInitialization(containerId);	} catch (Throwable tr) {	handlerError = tr;	}	break;	
event of type not expected here 

case COMMIT_LAST_REINT: container.nmClientAsync.getClient() .commitLastReInitialization(containerId);	try {	handler.onCommitLastReInitialization(containerId);	} catch (Throwable tr) {	handlerError = tr;	}	break;	break;	}	if (handlerError != null) {	
unchecked exception is thrown in handler for event for container 

} catch (Throwable tr) {	handlerError = tr;	}	break;	case COMMIT_LAST_REINT: try {	handler.onCommitLastReInitializationError(containerId, t);	} catch (Throwable tr) {	handlerError = tr;	}	break;	
event of type not expected here 

break;	case COMMIT_LAST_REINT: try {	handler.onCommitLastReInitializationError(containerId, t);	} catch (Throwable tr) {	handlerError = tr;	}	break;	break;	}	if (handlerError != null) {	
unchecked exception is thrown in handler for event for container 

public ContainerState transition( StatefulContainer container, ContainerEvent event) {	ContainerId containerId = event.getContainerId();	try {	container.nmClientAsync.getClient().stopContainer( containerId, event.getNodeId());	try {	container.nmClientAsync.getCallbackHandler().onContainerStopped( event.getContainerId());	} catch (Throwable thr) {	
unchecked exception is thrown from oncontainerstopped for container 

private ContainerState onExceptionRaised(StatefulContainer container, ContainerEvent event, Throwable t) {	try {	container.nmClientAsync.getCallbackHandler().onStopContainerError( event.getContainerId(), t);	} catch (Throwable thr) {	
unchecked exception is thrown from onstopcontainererror for container 

public void transition(StatefulContainer container, ContainerEvent event) {	try {	container.nmClientAsync.getCallbackHandler().onStartContainerError( event.getContainerId(), RPCUtil.getRemoteException(STOP_BEFORE_START_ERROR_MSG));	} catch (Throwable thr) {	
unchecked exception is thrown from onstartcontainererror for container 

public void handle(ContainerEvent event) {	writeLock.lock();	try {	try {	this.stateMachine.doTransition(event.getType(), event);	} catch (InvalidStateTransitionException e) {	
can t handle this event at current state 

public void run() {	ContainerId containerId = event.getContainerId();	
processing event for container 

public void run() {	ContainerId containerId = event.getContainerId();	if (event.getType() == ContainerEventType.QUERY_CONTAINER) {	try {	ContainerStatus containerStatus = client.getContainerStatus( containerId, event.getNodeId());	try {	callbackHandler.onContainerStatusReceived( containerId, containerStatus);	} catch (Throwable thr) {	
unchecked exception is thrown from oncontainerstatusreceived for container 

} catch (YarnException e) {	onExceptionRaised(containerId, e);	} catch (IOException e) {	onExceptionRaised(containerId, e);	} catch (Throwable t) {	onExceptionRaised(containerId, t);	}	} else {	StatefulContainer container = containers.get(containerId);	if (container == null) {	
container is already stopped or failed 

private void onExceptionRaised(ContainerId containerId, Throwable t) {	try {	callbackHandler.onGetContainerStatusError(containerId, t);	} catch (Throwable thr) {	
unchecked exception is thrown from ongetcontainerstatuserror for container 

========================= hadoop sample_2629 =========================

public void testGzip() throws IOException {	JobConf jobConf = new JobConf(defaultConf);	CompressionCodec gzip = new GzipCodec();	ReflectionUtils.setConf(gzip, jobConf);	localFs.delete(workDir, true);	assertFalse("[native (C/C++) codec]", (org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.class == gzip.getDecompressorType()) );	System.out.println(COLOR_BR_RED + "testGzip() using native-zlib Decompressor (" + gzip.getDecompressorType() + ")" + COLOR_NORMAL);	if (org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.class == gzip.getDecompressorType()) {	System.out.println(COLOR_BR_RED + "testGzip() using native-zlib Decompressor (" + gzip.getDecompressorType() + ")" + COLOR_NORMAL);	} else {	
testgzip skipped native c c libs not loaded 

========================= hadoop sample_5416 =========================

private RetryInfo handleException(final Method method, final int callId, final RetryPolicy policy, final Counters counters, final long expectFailoverCount, final Exception e) throws Exception {	final RetryInfo retryInfo = RetryInfo.newRetryInfo(policy, e, counters, proxyDescriptor.idempotentOrAtMostOnce(method), expectFailoverCount);	if (retryInfo.isFail()) {	if (retryInfo.action.reason != null) {	if (LOG.isDebugEnabled()) {	
exception while invoking call not retrying because 

========================= hadoop sample_3891 =========================

protected void addService(Service service) {	if (LOG.isDebugEnabled()) {	
adding service 

private void stop(int numOfServicesStarted, boolean stopOnlyStartedServices) {	Exception firstException = null;	List<Service> services = getServices();	for (int i = numOfServicesStarted - 1; i >= 0; i--) {	Service service = services.get(i);	if (LOG.isDebugEnabled()) {	
stopping service 

========================= hadoop sample_4084 =========================

hdfs.createSnapshot(dir, snapshotName);	if (s % step == 0) {	final Path file = new Path(dirStr, "f" + s);	DFSTestUtil.createFile(hdfs, file, BLOCKSIZE, REPLICATION, SEED);	}	}	try {	hdfs.createSnapshot(dir, "s" + s);	Assert.fail("Expected to fail to create snapshot, but didn't.");	} catch(IOException ioe) {	
the exception is expected 

========================= hadoop sample_7352 =========================

public void testOutliersFromTestMatrix() {	for (Map.Entry<Map<String, Double>, Set<String>> entry : outlierTestMatrix.entrySet()) {	
verifying set 

========================= hadoop sample_7217 =========================

public void setup(Context context) throws IOException, InterruptedException {	Configuration conf = context.getConfiguration();	this.fieldSeparator = conf.get(FieldSelectionHelper.DATA_FIELD_SEPERATOR, "\t");	this.mapOutputKeyValueSpec = conf.get(FieldSelectionHelper.MAP_OUTPUT_KEY_VALUE_SPEC, "0-:");	try {	this.ignoreInputKey = TextInputFormat.class.getCanonicalName().equals( context.getInputFormatClass().getCanonicalName());	} catch (ClassNotFoundException e) {	throw new IOException("Input format class not found", e);	}	allMapValueFieldsFrom = FieldSelectionHelper.parseOutputKeyValueSpec( mapOutputKeyValueSpec, mapOutputKeyFieldList, mapOutputValueFieldList);	
ignoreinputkey 

========================= hadoop sample_4968 =========================

protected void storeNewMasterKey(DelegationKey key) throws IOException {	if (LOG.isDebugEnabled()) {	
storing master key 

protected void storeNewMasterKey(DelegationKey key) throws IOException {	if (LOG.isDebugEnabled()) {	}	try {	store.storeTokenMasterKey(key);	} catch (IOException e) {	
unable to store master key 

protected void removeStoredMasterKey(DelegationKey key) {	if (LOG.isDebugEnabled()) {	
removing master key 

protected void removeStoredMasterKey(DelegationKey key) {	if (LOG.isDebugEnabled()) {	}	try {	store.removeTokenMasterKey(key);	} catch (IOException e) {	
unable to remove master key 

protected void storeNewToken(MRDelegationTokenIdentifier tokenId, long renewDate) {	if (LOG.isDebugEnabled()) {	
storing token 

protected void storeNewToken(MRDelegationTokenIdentifier tokenId, long renewDate) {	if (LOG.isDebugEnabled()) {	}	try {	store.storeToken(tokenId, renewDate);	} catch (IOException e) {	
unable to store token 

protected void removeStoredToken(MRDelegationTokenIdentifier tokenId) throws IOException {	if (LOG.isDebugEnabled()) {	
storing token 

protected void removeStoredToken(MRDelegationTokenIdentifier tokenId) throws IOException {	if (LOG.isDebugEnabled()) {	}	try {	store.removeToken(tokenId);	} catch (IOException e) {	
unable to remove token 

protected void updateStoredToken(MRDelegationTokenIdentifier tokenId, long renewDate) {	if (LOG.isDebugEnabled()) {	
updating token 

protected void updateStoredToken(MRDelegationTokenIdentifier tokenId, long renewDate) {	if (LOG.isDebugEnabled()) {	}	try {	store.updateToken(tokenId, renewDate);	} catch (IOException e) {	
unable to update token 

public void recover(HistoryServerState state) throws IOException {	
recovering 

========================= hadoop sample_5380 =========================

expectedOp.appendArgs(hierarchy, controllerKV);	cGroupsHandler.initializeCGroupController(controller);	try {	ArgumentCaptor<PrivilegedOperation> opCaptor = ArgumentCaptor.forClass( PrivilegedOperation.class);	verify(privilegedOperationExecutorMock) .executePrivilegedOperation(opCaptor.capture(), eq(false));	Assert.assertEquals(expectedOp, opCaptor.getValue());	verifyNoMoreInteractions(privilegedOperationExecutorMock);	cGroupsHandler.initializeCGroupController(controller);	verifyNoMoreInteractions(privilegedOperationExecutorMock);	} catch (PrivilegedOperationException e) {	
caught exception 

ArgumentCaptor<PrivilegedOperation> opCaptor = ArgumentCaptor.forClass( PrivilegedOperation.class);	verify(privilegedOperationExecutorMock) .executePrivilegedOperation(opCaptor.capture(), eq(false));	Assert.assertEquals(expectedOp, opCaptor.getValue());	verifyNoMoreInteractions(privilegedOperationExecutorMock);	cGroupsHandler.initializeCGroupController(controller);	verifyNoMoreInteractions(privilegedOperationExecutorMock);	} catch (PrivilegedOperationException e) {	assertTrue("Unexpected PrivilegedOperationException from mock!", false);	}	} catch (ResourceHandlerException e) {	
caught exception 

public void testCGroupPaths() throws IOException {	verifyZeroInteractions(privilegedOperationExecutorMock);	CGroupsHandler cGroupsHandler = null;	File mtab = createEmptyCgroups();	assertTrue("Sample subsystem should be created", new File(controllerPath).mkdirs());	try {	cGroupsHandler = new CGroupsHandlerImpl(createMountConfiguration(), privilegedOperationExecutorMock, mtab.getAbsolutePath());	cGroupsHandler.initializeCGroupController(controller);	} catch (ResourceHandlerException e) {	
caught exception 

public void testCGroupOperations() throws IOException {	verifyZeroInteractions(privilegedOperationExecutorMock);	CGroupsHandler cGroupsHandler = null;	File mtab = createEmptyCgroups();	assertTrue("Sample subsystem should be created", new File(controllerPath).mkdirs());	try {	cGroupsHandler = new CGroupsHandlerImpl(createMountConfiguration(), privilegedOperationExecutorMock, mtab.getAbsolutePath());	cGroupsHandler.initializeCGroupController(controller);	} catch (ResourceHandlerException e) {	
caught exception 

Assert.assertEquals(expectedPath, path);	String param = "test_param";	String paramValue = "test_param_value";	cGroupsHandler .updateCGroupParam(controller, testCGroup, param, paramValue);	String paramPath = expectedPath + Path.SEPARATOR + controller.getName() + "." + param;	File paramFile = new File(paramPath);	assertTrue(paramFile.exists());	try {	Assert.assertEquals(paramValue, new String(Files.readAllBytes( paramFile.toPath())));	} catch (IOException e) {	
caught exception 

String paramPath = expectedPath + Path.SEPARATOR + controller.getName() + "." + param;	File paramFile = new File(paramPath);	assertTrue(paramFile.exists());	try {	Assert.assertEquals(paramValue, new String(Files.readAllBytes( paramFile.toPath())));	} catch (IOException e) {	Assert.fail("Unexpected IOException trying to read cgroup param!");	}	Assert.assertEquals(paramValue, cGroupsHandler.getCGroupParam(controller, testCGroup, param));	} catch (ResourceHandlerException e) {	
caught exception 

========================= hadoop sample_1639 =========================

public void testAuthzDelegationToProvider() throws Exception {	
test not bypassing provider 

public void testAuthzBypassingProvider() throws Exception {	
test bypassing provider 

========================= hadoop sample_7493 =========================

public void initializeBlockRecovery(BlockInfo blockInfo, long recoveryId, boolean startRecovery) {	setBlockUCState(BlockUCState.UNDER_RECOVERY);	blockRecoveryId = recoveryId;	if (!startRecovery) {	return;	}	if (replicas.length == 0) {	
block blockunderconstructionfeature initializeblockrecovery no blocks found lease removed 

========================= hadoop sample_8336 =========================

public DFSClient getClient(UserGroupInformation user) throws IOException, URISyntaxException, InterruptedException {	
connecting to router at 

public RouterClient getAdminClient() throws IOException {	if (adminClient == null) {	InetSocketAddress routerSocket = router.getAdminServerAddress();	
connecting to router admin at 

public DFSClient getClient() throws IOException, URISyntaxException {	if (client == null) {	
connecting to router at 

public DFSClient getClient(UserGroupInformation user) throws IOException, URISyntaxException, InterruptedException {	
connecting to namenode at 

public DFSClient getClient() throws IOException, URISyntaxException {	if (client == null) {	
connecting to namenode at 

if (overrideConf != null) {	nnConf.addResource(overrideConf);	}	cluster = new MiniDFSCluster.Builder(nnConf) .numDataNodes(nameservices.size()*2) .nnTopology(topology) .build();	cluster.waitActive();	for (int i = 0; i < namenodes.size(); i++) {	NameNode nn = cluster.getNameNode(i);	namenodes.get(i).setNamenode(nn);	}	} catch (Exception e) {	
cannot start router dfs cluster 

public void waitRouterRegistrationQuorum(RouterContext router, FederationNamenodeServiceState state, String nsId, String nnId) throws InterruptedException, IOException {	
waiting for nn to transition to 

try {	int total = cluster.getNumNameNodes();	NameNodeInfo[] nns = cluster.getNameNodeInfos();	for (int i = 0; i < total; i++) {	NameNodeInfo nn = nns[i];	if (nn.getNameserviceId().equals(nsId) && nn.getNamenodeId().equals(nnId)) {	cluster.transitionToActive(i);	}	}	} catch (Throwable e) {	
cannot transition to active 

try {	int total = cluster.getNumNameNodes();	NameNodeInfo[] nns = cluster.getNameNodeInfos();	for (int i = 0; i < total; i++) {	NameNodeInfo nn = nns[i];	if (nn.getNameserviceId().equals(nsId) && nn.getNamenodeId().equals(nnId)) {	cluster.transitionToStandby(i);	}	}	} catch (Throwable e) {	
cannot transition to standby 

public void stopRouter(RouterContext router) {	try {	router.router.shutDown();	int loopCount = 0;	while (router.router.getServiceState() != STATE.STOPPED) {	loopCount++;	Thread.sleep(1000);	if (loopCount > 20) {	
cannot shutdown router 

========================= hadoop sample_7539 =========================

private AllocateResponse waitForAllocResponse(MockRM rm, MockAM am, MockNM nm, int size) throws Exception {	AllocateResponse allocResponse = am.doHeartbeat();	while (allocResponse.getAllocatedContainers().size() < size) {	
waiting for containers to be created for app 

========================= hadoop sample_565 =========================

assertEquals("We got more than one splits!", 1, splits.length);	InputSplit split = splits[0];	assertEquals("It should be CombineFileSplit", CombineFileSplit.class, split.getClass());	BitSet bits = new BitSet(length);	LOG.debug("split= " + split);	RecordReader<LongWritable, Text> reader = format.getRecordReader(split, job, voidReporter);	try {	int count = 0;	while (reader.next(key, value)) {	int v = Integer.parseInt(value.toString());	
read 

InputSplit split = splits[0];	assertEquals("It should be CombineFileSplit", CombineFileSplit.class, split.getClass());	BitSet bits = new BitSet(length);	LOG.debug("split= " + split);	RecordReader<LongWritable, Text> reader = format.getRecordReader(split, job, voidReporter);	try {	int count = 0;	while (reader.next(key, value)) {	int v = Integer.parseInt(value.toString());	if (bits.get(v)) {	
conflict with at position 

========================= hadoop sample_5441 =========================

public static void init() {	
test aggregate metrics are initialized correctly 

public static void init() {	Assert.assertEquals(0, metrics.getNumSucceededAppsCreated());	Assert.assertEquals(0, metrics.getNumSucceededAppsSubmitted());	Assert.assertEquals(0, metrics.getNumSucceededAppsKilled());	Assert.assertEquals(0, metrics.getNumSucceededAppsRetrieved());	Assert.assertEquals(0, metrics.getAppsFailedCreated());	Assert.assertEquals(0, metrics.getAppsFailedSubmitted());	Assert.assertEquals(0, metrics.getAppsFailedKilled());	Assert.assertEquals(0, metrics.getAppsFailedRetrieved());	
test aggregate metrics are updated correctly 

public void getNewApplication() {	
mocked failed getnewapplication call 

public void submitApplication() {	
mocked failed submitapplication call 

public void forceKillApplication() {	
mocked failed forcekillapplication call 

public void getApplicationReport() {	
mocked failed getapplicationreport call 

public void getApplicationsReport() {	
mocked failed getapplicationsreport call 

public void getNewApplication(long duration) {	
mocked successful getnewapplication call with duration 

public void submitApplication(long duration) {	
mocked successful submitapplication call with duration 

public void forceKillApplication(long duration) {	
mocked successful forcekillapplication call with duration 

public void getApplicationReport(long duration) {	
mocked successful getapplicationreport call with duration 

public void getApplicationsReport(long duration) {	
mocked successful getapplicationsreport call with duration 

========================= hadoop sample_1951 =========================

public java.net.URLStreamHandler createURLStreamHandler(String protocol) {	
creating handler for protocol 

public java.net.URLStreamHandler createURLStreamHandler(String protocol) {	if (!protocols.containsKey(protocol)) {	boolean known = true;	try {	Class<? extends FileSystem> impl = FileSystem.getFileSystemClass(protocol, conf);	
found implementation of 

boolean known = true;	try {	Class<? extends FileSystem> impl = FileSystem.getFileSystemClass(protocol, conf);	}	catch (IOException ex) {	known = false;	}	protocols.put(protocol, known);	}	if (protocols.get(protocol)) {	
using handler for protocol 

Class<? extends FileSystem> impl = FileSystem.getFileSystemClass(protocol, conf);	}	catch (IOException ex) {	known = false;	}	protocols.put(protocol, known);	}	if (protocols.get(protocol)) {	return handler;	} else {	
unknown protocol delegating to default implementation 

========================= hadoop sample_4250 =========================

}	final long checkStartTimeMs = timer.monotonicNow();	for (Map.Entry<StorageLocation, ListenableFuture<VolumeCheckResult>> entry : futures.entrySet()) {	final long waitSoFarMs = (timer.monotonicNow() - checkStartTimeMs);	final long timeLeftMs = Math.max(0, maxAllowedTimeForCheckMs - waitSoFarMs);	final StorageLocation location = entry.getKey();	try {	final VolumeCheckResult result = entry.getValue().get(timeLeftMs, TimeUnit.MILLISECONDS);	switch (result) {	case HEALTHY: break;	
storagelocation appears to be degraded 

final long checkStartTimeMs = timer.monotonicNow();	for (Map.Entry<StorageLocation, ListenableFuture<VolumeCheckResult>> entry : futures.entrySet()) {	final long waitSoFarMs = (timer.monotonicNow() - checkStartTimeMs);	final long timeLeftMs = Math.max(0, maxAllowedTimeForCheckMs - waitSoFarMs);	final StorageLocation location = entry.getKey();	try {	final VolumeCheckResult result = entry.getValue().get(timeLeftMs, TimeUnit.MILLISECONDS);	switch (result) {	case HEALTHY: break;	break;	
storagelocation detected as failed 

final long timeLeftMs = Math.max(0, maxAllowedTimeForCheckMs - waitSoFarMs);	final StorageLocation location = entry.getKey();	try {	final VolumeCheckResult result = entry.getValue().get(timeLeftMs, TimeUnit.MILLISECONDS);	switch (result) {	case HEALTHY: break;	break;	failedLocations.add(location);	goodLocations.remove(location);	break;	
unexpected health check result for storagelocation 

try {	final VolumeCheckResult result = entry.getValue().get(timeLeftMs, TimeUnit.MILLISECONDS);	switch (result) {	case HEALTHY: break;	break;	failedLocations.add(location);	goodLocations.remove(location);	break;	}	} catch (ExecutionException|TimeoutException e) {	
exception checking storagelocation 

public void shutdownAndWait(int gracePeriod, TimeUnit timeUnit) {	try {	delegateChecker.shutdownAndWait(gracePeriod, timeUnit);	} catch (InterruptedException e) {	
storagelocationchecker interrupted during shutdown 

========================= hadoop sample_7885 =========================

public String getPrediction( throws SolverException, SkylineStoreException {	RLESparseResourceAllocation result = skylineStore.getEstimation(pipelineId);	if (result == null) {	RecurrenceId recurrenceId = new RecurrenceId(pipelineId, "*");	Map<RecurrenceId, List<ResourceSkyline>> jobHistory = skylineStore.getHistory(recurrenceId);	result = solver.solve(jobHistory);	}	final String prediction = gson.toJson(result, rleType);	
predict resource requests for pipelineid 

public String getEstimatedResourceAllocation( RLESparseResourceAllocation result = skylineStore.getEstimation(pipelineId);	final String skyline = gson.toJson(result, rleType);	
query the skyline store for pipelineid 

========================= hadoop sample_6514 =========================

public S3AOutputStream(Configuration conf, S3AFileSystem fs, String key, Progressable progress) throws IOException {	this.key = key;	this.progress = progress;	this.fs = fs;	backupFile = fs.createTmpFileForWrite("output-", LocalDirAllocator.SIZE_UNKNOWN, conf);	
outputstream for key writing to tempfile 

public void close() throws IOException {	if (closed.getAndSet(true)) {	return;	}	backupStream.close();	
outputstream for key closed now beginning upload 

info.getUpload().addProgressListener(listener);	info.getUpload().waitForUploadResult();	listener.uploadCompleted();	fs.finishedWrite(key, info.getLength());	} catch (InterruptedException e) {	throw (InterruptedIOException) new InterruptedIOException(e.toString()) .initCause(e);	} catch (AmazonClientException e) {	throw translateException("saving output", key , e);	} finally {	if (!backupFile.delete()) {	
could not delete temporary file 

fs.finishedWrite(key, info.getLength());	} catch (InterruptedException e) {	throw (InterruptedIOException) new InterruptedIOException(e.toString()) .initCause(e);	} catch (AmazonClientException e) {	throw translateException("saving output", key , e);	} finally {	if (!backupFile.delete()) {	}	super.close();	}	
outputstream for key upload complete 

========================= hadoop sample_6001 =========================

private TimelineWriter createTimelineWriter(final Configuration conf) {	String timelineWriterClassName = conf.get( YarnConfiguration.TIMELINE_SERVICE_WRITER_CLASS, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WRITER_CLASS);	
using timelinewriter 

public TimelineCollector putIfAbsent(ApplicationId appId, TimelineCollector collector) {	TimelineCollector collectorInTable = null;	synchronized (collectors) {	collectorInTable = collectors.get(appId);	if (collectorInTable == null) {	try {	collector.init(getConfig());	collector.setWriter(writer);	collector.start();	collectors.put(appId, collector);	
the collector for was added 

collector.init(getConfig());	collector.setWriter(writer);	collector.start();	collectors.put(appId, collector);	collectorInTable = collector;	postPut(appId, collectorInTable);	} catch (Exception e) {	throw new YarnRuntimeException(e);	}	} else {	
the collector for already exists 

public boolean remove(ApplicationId appId) {	TimelineCollector collector = collectors.remove(appId);	if (collector == null) {	
the collector for does not exist 

public boolean remove(ApplicationId appId) {	TimelineCollector collector = collectors.remove(appId);	if (collector == null) {	} else {	synchronized (collector) {	postRemove(appId, collector);	collector.stop();	}	
the collector service for was removed 

synchronized (collectors) {	for (TimelineCollector c : collectors.values()) {	c.serviceStop();	}	}	}	if (writerFlusher != null) {	writerFlusher.shutdown();	writerFlusherRunning = false;	if (!writerFlusher.awaitTermination(30, TimeUnit.SECONDS)) {	
failed to stop the flusher task in time will still proceed to close the writer 

public void run() {	try {	synchronized (writer) {	writer.flush();	}	} catch (Throwable th) {	
exception during timeline writer flush 

========================= hadoop sample_359 =========================

public static void setupCluster(int replicationFactor, HdfsConfiguration conf) throws Exception {	util = new BlockReaderTestUtil(replicationFactor, conf);	dfsClient = util.getDFSClient();	long seed = Time.now();	
random seed 

if (p < PROPORTION_NON_POSITIONAL_READ) {	len = Math.min(rand.nextInt(64), (int) fileSize - startOff);	read(startOff, len);	bytesRead += len;	} else {	len = rand.nextInt((int) (fileSize - startOff));	pRead(startOff, len);	bytesRead += len;	}	} catch (Throwable t) {	
error while testing read at length 

========================= hadoop sample_7610 =========================

public void testDiskCheckTimeout() throws Exception {	
executing 

========================= hadoop sample_7240 =========================

} else {	builder = new ProcessBuilder("cmd.exe", "/c", cmd);	}	setConfAsEnvVars(builder.environment());	addTargetInfoAsEnvVars(target, builder.environment());	Process p;	try {	p = builder.start();	p.getOutputStream().close();	} catch (IOException e) {	
unable to execute 

StreamPumper errPumper = new StreamPumper( LOG, logPrefix, p.getErrorStream(), StreamPumper.StreamType.STDERR);	errPumper.start();	StreamPumper outPumper = new StreamPumper( LOG, logPrefix, p.getInputStream(), StreamPumper.StreamType.STDOUT);	outPumper.start();	int rc;	try {	rc = p.waitFor();	errPumper.join();	outPumper.join();	} catch (InterruptedException ie) {	
interrupted while waiting for fencing command 

private static String tryGetPid(Process p) {	try {	Class<? extends Process> clazz = p.getClass();	if (clazz.getName().equals("java.lang.UNIXProcess")) {	Field f = clazz.getDeclaredField("pid");	f.setAccessible(true);	return String.valueOf(f.getInt(p));	} else {	
unable to determine pid for since it is not a unixprocess 

try {	Class<? extends Process> clazz = p.getClass();	if (clazz.getName().equals("java.lang.UNIXProcess")) {	Field f = clazz.getDeclaredField("pid");	f.setAccessible(true);	return String.valueOf(f.getInt(p));	} else {	return null;	}	} catch (Throwable t) {	
unable to determine pid for 

========================= hadoop sample_3995 =========================

private Path getInodePath(long inodeId, String remainingPath) {	StringBuilder b = new StringBuilder();	b.append(Path.SEPARATOR).append(FSDirectory.DOT_RESERVED_STRING) .append(Path.SEPARATOR).append(FSDirectory.DOT_INODES_STRING) .append(Path.SEPARATOR).append(inodeId).append(Path.SEPARATOR) .append(remainingPath);	Path p = new Path(b.toString());	
inode path is 

private void ensureClusterRestartFails(MiniDFSCluster cluster) {	try {	cluster.restartNameNode();	fail("Cluster should not have successfully started");	} catch (Exception expected) {	
expected exception thrown 

========================= hadoop sample_7485 =========================

public ChunkChecksum getLastChecksumAndDataLen() throws IOException {	ChunkChecksum chunkChecksum = null;	try {	byte[] lastChecksum = getVolume().loadLastPartialChunkChecksum( getBlockFile(), getMetaFile());	if (lastChecksum != null) {	chunkChecksum = new ChunkChecksum(getVisibleLength(), lastChecksum);	}	} catch (FileNotFoundException e) {	
meta file is missing 

public ChunkChecksum getLastChecksumAndDataLen() throws IOException {	ChunkChecksum chunkChecksum = null;	try {	byte[] lastChecksum = getVolume().loadLastPartialChunkChecksum( getBlockFile(), getMetaFile());	if (lastChecksum != null) {	chunkChecksum = new ChunkChecksum(getVisibleLength(), lastChecksum);	}	} catch (FileNotFoundException e) {	} catch (IOException ioe) {	
unable to read checksum from meta file 

========================= hadoop sample_7893 =========================

displayError(cmd, "Null exception message");	e.printStackTrace(System.err);	} else {	displayError(cmd, e.getLocalizedMessage());	}	printUsage(System.err);	if (instance != null) {	printInstanceUsage(System.err, instance);	}	} catch (Exception e) {	
Error 

========================= hadoop sample_4246 =========================

public void writeMetric(String line) {	try {	statsd.write(line);	} catch (IOException e) {	
error sending metrics to statsd 

public void write(String msg) throws IOException {	if (null == socket) {	createSocket();	}	
sending metric 

========================= hadoop sample_3429 =========================

final RecordWriter rw = tof[taskIdx].getRecordWriter(taCtx[taskIdx]);	writeOutput(rw, taCtx[taskIdx]);	outputCommitter.commitTask(taCtx[taskIdx]);	return null;	}	});	}	} finally {	executor.shutdown();	while (!executor.awaitTermination(1, TimeUnit.SECONDS)) {	
awaiting thread termination 

========================= hadoop sample_4616 =========================

public void tearDown() throws Exception {	IOUtils.cleanup(LOG, clients.toArray(new SocketChannel[clients.size()]));	IOUtils.cleanup(LOG, fs);	if (serverSocket != null) {	try {	serverSocket.close();	} catch (IOException e) {	
exception in closing 

br = new BufferedReader(isr);	for (;;) {	String line = br.readLine();	if (line == null || line.isEmpty()) {	break;	}	}	out = clientSocket.getOutputStream();	out.write(temporaryRedirect().getBytes("UTF-8"));	} catch (IOException e) {	
unexpected ioexception in server thread 

========================= hadoop sample_7190 =========================

public void processResult(CuratorFramework client, CuratorEvent event) throws Exception {	if (LOG.isDebugEnabled()) {	
delete event 

========================= hadoop sample_2668 =========================

MiniDFSNNTopology topology = new MiniDFSNNTopology() .addNameservice(new MiniDFSNNTopology.NSConf("ns1") .addNN(new MiniDFSNNTopology.NNConf("nn1").setHttpPort(basePort)) .addNN(new MiniDFSNNTopology.NNConf("nn2").setHttpPort(basePort + 1)));	cluster = new MiniDFSCluster.Builder(conf) .nnTopology(topology) .numDataNodes(1) .manageNameDfsSharedDirs(false) .build();	cluster.waitActive();	nn0 = cluster.getNameNode(0);	nn1 = cluster.getNameNode(1);	fs = HATestUtil.configureFailoverFs(cluster, conf);	cluster.transitionToActive(0);	++retryCount;	break;	} catch (BindException e) {	
set up minidfscluster failed due to port conflicts retry times 

========================= hadoop sample_7743 =========================

StringUtils.startupShutdownMessage(ApplicationHistoryServer.class, args, LOG);	ApplicationHistoryServer appHistoryServer = null;	try {	appHistoryServer = new ApplicationHistoryServer();	ShutdownHookManager.get().addShutdownHook( new CompositeServiceShutdownHook(appHistoryServer), SHUTDOWN_HOOK_PRIORITY);	YarnConfiguration conf = new YarnConfiguration();	new GenericOptionsParser(conf, args);	appHistoryServer.init(conf);	appHistoryServer.start();	} catch (Throwable t) {	
error starting applicationhistoryserver 

private ApplicationHistoryManager createApplicationHistoryManager( Configuration conf) {	if (conf.get(YarnConfiguration.APPLICATION_HISTORY_STORE) == null || conf.get(YarnConfiguration.APPLICATION_HISTORY_STORE).length() == 0 || conf.get(YarnConfiguration.APPLICATION_HISTORY_STORE).equals( NullApplicationHistoryStore.class.getName())) {	return new ApplicationHistoryManagerOnTimelineStore( timelineDataManager, aclsManager);	} else {	
the filesystem based application history store is deprecated 

WebAppContext uiWebAppContext = new WebAppContext();	uiWebAppContext.setContextPath(webPath);	uiWebAppContext.setWar(onDiskPath);	final String[] ALL_URLS = { "/*" };	FilterHolder[] filterHolders = webAppContext.getServletHandler().getFilters();	for (FilterHolder filterHolder: filterHolders) {	if (!"guice".equals(filterHolder.getName())) {	HttpServer2.defineFilter(uiWebAppContext, filterHolder.getName(), filterHolder.getClassName(), filterHolder.getInitParameters(), ALL_URLS);	}	}	
hosting from at 

FilterHolder[] filterHolders = webAppContext.getServletHandler().getFilters();	for (FilterHolder filterHolder: filterHolders) {	if (!"guice".equals(filterHolder.getName())) {	HttpServer2.defineFilter(uiWebAppContext, filterHolder.getName(), filterHolder.getClassName(), filterHolder.getInitParameters(), ALL_URLS);	}	}	httpServer.addContext(uiWebAppContext, true);	}	httpServer.start();	conf.updateConnectAddr(YarnConfiguration.TIMELINE_SERVICE_BIND_HOST, YarnConfiguration.TIMELINE_SERVICE_WEBAPP_ADDRESS, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WEBAPP_ADDRESS, this.getListenerAddress());	
instantiating ahswebapp at 

========================= hadoop sample_2054 =========================

GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	final MutableBoolean done = new MutableBoolean(false);	try {	cache.getDfsClientShmManager().visit(new Visitor() {	public void visit(HashMap<DatanodeInfo, PerDatanodeVisitorInfo> info) throws IOException {	done.setValue(info.get(datanode).full.isEmpty() && info.get(datanode).notFull.isEmpty());	}	});	} catch (IOException e) {	
error running visitor 

for (Iterator<Slot> iter = shm.slotIterator();	iter.hasNext(); ) {	Slot slot = iter.next();	if (slot.isValid()) {	done.setValue(false);	}	}	}	});	} catch (IOException e) {	
error running visitor 

public void testDataXceiverHandlesRequestShortCircuitShmFailure() throws Exception {	BlockReaderTestUtil.enableShortCircuitShmTracing();	TemporarySocketDirectory sockDir = new TemporarySocketDirectory();	Configuration conf = createShortCircuitConf( "testDataXceiverHandlesRequestShortCircuitShmFailure", sockDir);	conf.setLong(HdfsClientConfigKeys.Read.ShortCircuit.STREAMS_CACHE_EXPIRY_MS_KEY, 1000000000L);	MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).build();	cluster.waitActive();	DistributedFileSystem fs = cluster.getFileSystem();	final Path TEST_PATH1 = new Path("/test_file1");	DFSTestUtil.createFile(fs, TEST_PATH1, 4096, (short)1, 0xFADE1);	
setting failure injector and performing a read which should fail 

}).when(failureInjector).sendShortCircuitShmResponse();	DataNodeFaultInjector prevInjector = DataNodeFaultInjector.get();	DataNodeFaultInjector.set(failureInjector);	try {	DFSTestUtil.readFileBuffer(fs, TEST_PATH1);	Assert.fail("expected readFileBuffer to fail, but it succeeded.");	} catch (Throwable t) {	GenericTestUtils.assertExceptionContains("TCP reads were disabled for " + "testing, but we failed to do a non-TCP read.", t);	}	checkNumberOfSegmentsAndSlots(0, 0, cluster.getDataNodes().get(0).getShortCircuitRegistry());	
clearing failure injector and performing another read 

========================= hadoop sample_7606 =========================

public void init(Configuration conf) throws YarnException {	
initializing zookeeper connection 

public void init(Configuration conf) throws YarnException {	baseZNode = conf.get( YarnConfiguration.FEDERATION_STATESTORE_ZK_PARENT_PATH, YarnConfiguration.DEFAULT_FEDERATION_STATESTORE_ZK_PARENT_PATH);	try {	this.zkManager = new ZKCuratorManager(conf);	this.zkManager.start();	} catch (IOException e) {	
cannot initialize the zk connection 

public GetSubClusterInfoResponse getSubCluster( GetSubClusterInfoRequest request) throws YarnException {	FederationMembershipStateStoreInputValidator.validate(request);	SubClusterId subClusterId = request.getSubClusterId();	SubClusterInfo subClusterInfo = null;	try {	subClusterInfo = getSubclusterInfo(subClusterId);	if (subClusterInfo == null) {	
the queried subcluster does not exist 

FederationPolicyStoreInputValidator.validate(request);	String queue = request.getQueue();	SubClusterPolicyConfiguration policy = null;	try {	policy = getPolicy(queue);	} catch (Exception e) {	String errMsg = "Cannot get policy: " + e.getMessage();	FederationStateStoreUtils.logAndThrowStoreException(LOG, errMsg);	}	if (policy == null) {	
policy for queue does not exist 

private byte[] get(String znode) throws YarnException {	boolean exists = false;	try {	exists = zkManager.exists(znode);	} catch (Exception e) {	String errMsg = "Cannot find znode " + znode;	FederationStateStoreUtils.logAndThrowStoreException(LOG, errMsg);	}	if (!exists) {	
does not exist 

private void put(String znode, byte[] data, boolean update) throws YarnException {	boolean created = false;	try {	created = zkManager.create(znode);	} catch (Exception e) {	String errMsg = "Cannot create znode " + znode + ": " + e.getMessage();	FederationStateStoreUtils.logAndThrowStoreException(LOG, errMsg);	}	if (!created) {	
not created 

private void put(String znode, byte[] data, boolean update) throws YarnException {	boolean created = false;	try {	created = zkManager.create(znode);	} catch (Exception e) {	String errMsg = "Cannot create znode " + znode + ": " + e.getMessage();	FederationStateStoreUtils.logAndThrowStoreException(LOG, errMsg);	}	if (!created) {	if (!update) {	
already existed and we are not updating 

========================= hadoop sample_1382 =========================

public static List<ACL> getZKAcls(Configuration conf) throws IOException {	String zkAclConf = conf.get(CommonConfigurationKeys.ZK_ACL, CommonConfigurationKeys.ZK_ACL_DEFAULT);	try {	zkAclConf = ZKUtil.resolveConfIndirection(zkAclConf);	return ZKUtil.parseACLs(zkAclConf);	} catch (IOException | ZKUtil.BadAclFormatException e) {	
couldn t read acls based on 

public static List<ZKUtil.ZKAuthInfo> getZKAuths(Configuration conf) throws IOException {	String zkAuthConf = conf.get(CommonConfigurationKeys.ZK_AUTH);	try {	zkAuthConf = ZKUtil.resolveConfIndirection(zkAuthConf);	if (zkAuthConf != null) {	return ZKUtil.parseAuth(zkAuthConf);	} else {	return Collections.emptyList();	}	} catch (IOException | ZKUtil.BadAuthFormatException e) {	
couldn t read auth based on 

========================= hadoop sample_3645 =========================

resourcesHandler = getResourcesHandler(conf);	containerSchedPriorityIsSet = false;	if (conf.get(YarnConfiguration.NM_CONTAINER_EXECUTOR_SCHED_PRIORITY) != null) {	containerSchedPriorityIsSet = true;	containerSchedPriorityAdjustment = conf .getInt(YarnConfiguration.NM_CONTAINER_EXECUTOR_SCHED_PRIORITY, YarnConfiguration.DEFAULT_NM_CONTAINER_EXECUTOR_SCHED_PRIORITY);	}	nonsecureLocalUser = conf.get( YarnConfiguration.NM_NONSECURE_MODE_LOCAL_USER_KEY, YarnConfiguration.DEFAULT_NM_NONSECURE_MODE_LOCAL_USER);	nonsecureLocalUserPattern = Pattern.compile( conf.get(YarnConfiguration.NM_NONSECURE_MODE_USER_PATTERN_KEY, YarnConfiguration.DEFAULT_NM_NONSECURE_MODE_USER_PATTERN));	containerLimitUsers = conf.getBoolean( YarnConfiguration.NM_NONSECURE_MODE_LIMIT_USERS, YarnConfiguration.DEFAULT_NM_NONSECURE_MODE_LIMIT_USERS);	if (!containerLimitUsers) {	
impersonation without authentication enabled 

public void init() throws IOException {	Configuration conf = super.getConf();	try {	PrivilegedOperation checkSetupOp = new PrivilegedOperation( PrivilegedOperation.OperationType.CHECK_SETUP);	PrivilegedOperationExecutor privilegedOperationExecutor = getPrivilegedOperationExecutor();	privilegedOperationExecutor.executePrivilegedOperation(checkSetupOp, false);	} catch (PrivilegedOperationException e) {	int exitCode = e.getExitCode();	
exit code from container executor initialization is 

} catch (PrivilegedOperationException e) {	int exitCode = e.getExitCode();	throw new IOException("Linux container executor not configured properly" + " (error=" + exitCode + ")", e);	}	try {	resourceHandlerChain = ResourceHandlerModule .getConfiguredResourceHandlerChain(conf);	if (LOG.isDebugEnabled()) {	LOG.debug("Resource handler chain enabled = " + (resourceHandlerChain != null));	}	if (resourceHandlerChain != null) {	
bootstrapping resource handler chain 

}	try {	resourceHandlerChain = ResourceHandlerModule .getConfiguredResourceHandlerChain(conf);	if (LOG.isDebugEnabled()) {	LOG.debug("Resource handler chain enabled = " + (resourceHandlerChain != null));	}	if (resourceHandlerChain != null) {	resourceHandlerChain.bootstrap(conf);	}	} catch (ResourceHandlerException e) {	
failed to bootstrap configured resource subsystems 

} catch (ResourceHandlerException e) {	throw new IOException( "Failed to bootstrap configured resource subsystems!");	}	try {	if (linuxContainerRuntime == null) {	LinuxContainerRuntime runtime = new DelegatingLinuxContainerRuntime();	runtime.initialize(conf);	this.linuxContainerRuntime = runtime;	}	} catch (ContainerExecutionException e) {	
failed to initialize linux container runtime s 

initializeContainerOp.appendArgs(ContainerLocalizer.getJavaOpts(getConf()));	List<String> localizerArgs = new ArrayList<>();	buildMainArgs(localizerArgs, user, appId, locId, nmAddr, localDirs);	initializeContainerOp.appendArgs(localizerArgs);	try {	Configuration conf = super.getConf();	PrivilegedOperationExecutor privilegedOperationExecutor = getPrivilegedOperationExecutor();	privilegedOperationExecutor.executePrivilegedOperation(prefixCommands, initializeContainerOp, null, null, false, true);	} catch (PrivilegedOperationException e) {	int exitCode = e.getExitCode();	
exit code from container startlocalizer is 

List<PrivilegedOperation> ops = resourceHandlerChain .preStart(container);	if (ops != null) {	List<PrivilegedOperation> resourceOps = new ArrayList<>();	resourceOps.add(new PrivilegedOperation( PrivilegedOperation.OperationType.ADD_PID_TO_CGROUP, resourcesOptions));	for (PrivilegedOperation op : ops) {	switch (op.getOperationType()) {	case ADD_PID_TO_CGROUP: resourceOps.add(op);	break;	case TC_MODIFY_STATE: tcCommandFile = op.getArguments().get(0);	break;	
privilegedoperation type unsupported in launch 

break;	case TC_MODIFY_STATE: tcCommandFile = op.getArguments().get(0);	break;	}	}	if (resourceOps.size() > 1) {	try {	PrivilegedOperation operation = PrivilegedOperationExecutor .squashCGroupOperations(resourceOps);	resourcesOptions = operation.getArguments().get(0);	} catch (PrivilegedOperationException e) {	
failed to squash cgroup operations 

try {	PrivilegedOperation operation = PrivilegedOperationExecutor .squashCGroupOperations(resourceOps);	resourcesOptions = operation.getArguments().get(0);	} catch (PrivilegedOperationException e) {	throw new ResourceHandlerException( "Failed to squash cgroup operations!");	}	}	}	}	} catch (ResourceHandlerException e) {	
resourcehandlerchain prestart failed 

addSchedPriorityCommand(prefixCommands);	if (prefixCommands.size() > 0) {	builder.setExecutionAttribute(CONTAINER_LAUNCH_PREFIX_COMMANDS, prefixCommands);	}	builder.setExecutionAttribute(LOCALIZED_RESOURCES, localizedResources) .setExecutionAttribute(RUN_AS_USER, runAsUser) .setExecutionAttribute(USER, user) .setExecutionAttribute(APPID, appId) .setExecutionAttribute(CONTAINER_ID_STR, containerIdStr) .setExecutionAttribute(CONTAINER_WORK_DIR, containerWorkDir) .setExecutionAttribute(NM_PRIVATE_CONTAINER_SCRIPT_PATH, nmPrivateContainerScriptPath) .setExecutionAttribute(NM_PRIVATE_TOKENS_PATH, nmPrivateTokensPath) .setExecutionAttribute(PID_FILE_PATH, pidFilePath) .setExecutionAttribute(LOCAL_DIRS, localDirs) .setExecutionAttribute(LOG_DIRS, logDirs) .setExecutionAttribute(FILECACHE_DIRS, filecacheDirs) .setExecutionAttribute(USER_LOCAL_DIRS, userLocalDirs) .setExecutionAttribute(CONTAINER_LOCAL_DIRS, containerLocalDirs) .setExecutionAttribute(CONTAINER_LOG_DIRS, containerLogDirs) .setExecutionAttribute(RESOURCES_OPTIONS, resourcesOptions);	if (tcCommandFile != null) {	builder.setExecutionAttribute(TC_COMMAND_FILE, tcCommandFile);	}	linuxContainerRuntime.launchContainer(builder.build());	} else {	
container was marked as inactive returning terminated error 

builder.setExecutionAttribute(LOCALIZED_RESOURCES, localizedResources) .setExecutionAttribute(RUN_AS_USER, runAsUser) .setExecutionAttribute(USER, user) .setExecutionAttribute(APPID, appId) .setExecutionAttribute(CONTAINER_ID_STR, containerIdStr) .setExecutionAttribute(CONTAINER_WORK_DIR, containerWorkDir) .setExecutionAttribute(NM_PRIVATE_CONTAINER_SCRIPT_PATH, nmPrivateContainerScriptPath) .setExecutionAttribute(NM_PRIVATE_TOKENS_PATH, nmPrivateTokensPath) .setExecutionAttribute(PID_FILE_PATH, pidFilePath) .setExecutionAttribute(LOCAL_DIRS, localDirs) .setExecutionAttribute(LOG_DIRS, logDirs) .setExecutionAttribute(FILECACHE_DIRS, filecacheDirs) .setExecutionAttribute(USER_LOCAL_DIRS, userLocalDirs) .setExecutionAttribute(CONTAINER_LOCAL_DIRS, containerLocalDirs) .setExecutionAttribute(CONTAINER_LOG_DIRS, containerLogDirs) .setExecutionAttribute(RESOURCES_OPTIONS, resourcesOptions);	if (tcCommandFile != null) {	builder.setExecutionAttribute(TC_COMMAND_FILE, tcCommandFile);	}	linuxContainerRuntime.launchContainer(builder.build());	} else {	return ContainerExecutor.ExitCode.TERMINATED.getExitCode();	}	} catch (ContainerExecutionException e) {	int exitCode = e.getExitCode();	
exit code from container is 

if (tcCommandFile != null) {	builder.setExecutionAttribute(TC_COMMAND_FILE, tcCommandFile);	}	linuxContainerRuntime.launchContainer(builder.build());	} else {	return ContainerExecutor.ExitCode.TERMINATED.getExitCode();	}	} catch (ContainerExecutionException e) {	int exitCode = e.getExitCode();	if (exitCode != ContainerExecutor.ExitCode.FORCE_KILLED.getExitCode() && exitCode != ContainerExecutor.ExitCode.TERMINATED.getExitCode()) {	
exception from container launch with container id and exit code 

container.handle(new ContainerDiagnosticsUpdateEvent(containerId, "Container killed on request. Exit code is " + exitCode));	}	return exitCode;	} finally {	resourcesHandler.postExecute(containerId);	try {	if (resourceHandlerChain != null) {	resourceHandlerChain.postComplete(containerId);	}	} catch (ResourceHandlerException e) {	
resourcehandlerchain postcomplete failed for containerid exception 

public int reacquireContainer(ContainerReacquisitionContext ctx) throws IOException, InterruptedException {	ContainerId containerId = ctx.getContainerId();	try {	if (resourceHandlerChain != null) {	try {	resourceHandlerChain.reacquireContainer(containerId);	} catch (ResourceHandlerException e) {	
resourcehandlerchain reacquirecontainer failed for containerid exception 

} catch (ResourceHandlerException e) {	}	}	return super.reacquireContainer(ctx);	} finally {	resourcesHandler.postExecute(containerId);	if (resourceHandlerChain != null) {	try {	resourceHandlerChain.postComplete(containerId);	} catch (ResourceHandlerException e) {	
resourcehandlerchain postcomplete failed for containerid exception 

String user = ctx.getUser();	Path dir = ctx.getSubDir();	List<Path> baseDirs = ctx.getBasedirs();	verifyUsernamePattern(user);	String runAsUser = getRunAsUser(user);	String dirString = dir == null ? "" : dir.toUri().getPath();	PrivilegedOperation deleteAsUserOp = new PrivilegedOperation( PrivilegedOperation.OperationType.DELETE_AS_USER, (String) null);	deleteAsUserOp.appendArgs( runAsUser, user, Integer.toString(PrivilegedOperation. RunAsUserCommand.DELETE_AS_USER.getValue()), dirString);	List<String> pathsToDelete = new ArrayList<String>();	if (baseDirs == null || baseDirs.size() == 0) {	
deleting absolute path 

String runAsUser = getRunAsUser(user);	String dirString = dir == null ? "" : dir.toUri().getPath();	PrivilegedOperation deleteAsUserOp = new PrivilegedOperation( PrivilegedOperation.OperationType.DELETE_AS_USER, (String) null);	deleteAsUserOp.appendArgs( runAsUser, user, Integer.toString(PrivilegedOperation. RunAsUserCommand.DELETE_AS_USER.getValue()), dirString);	List<String> pathsToDelete = new ArrayList<String>();	if (baseDirs == null || baseDirs.size() == 0) {	pathsToDelete.add(dirString);	} else {	for (Path baseDir : baseDirs) {	Path del = dir == null ? baseDir : new Path(baseDir, dir);	
deleting path 

pathsToDelete.add(del.toString());	deleteAsUserOp.appendArgs(baseDir.toUri().getPath());	}	}	try {	Configuration conf = super.getConf();	PrivilegedOperationExecutor privilegedOperationExecutor = getPrivilegedOperationExecutor();	privilegedOperationExecutor.executePrivilegedOperation(deleteAsUserOp, false);	}   catch (PrivilegedOperationException e) {	int exitCode = e.getExitCode();	
deleteasuser for returned with exit code 

listAsUserOp.appendArgs(runAsUser, user, Integer.toString( PrivilegedOperation.RunAsUserCommand.LIST_AS_USER.getValue()), dirString);	try {	PrivilegedOperationExecutor privOpExecutor = getPrivilegedOperationExecutor();	String results = privOpExecutor.executePrivilegedOperation(listAsUserOp, true);	for (String file: results.split("\n")) {	if (!file.startsWith("main :")) {	files.add(new File(new File(dirString), file));	}	}	} catch (PrivilegedOperationException e) {	
listasuser for returned with exit code 

public void mountCgroups(List<String> cgroupKVs, String hierarchy) throws IOException {	try {	PrivilegedOperation mountCGroupsOp = new PrivilegedOperation( PrivilegedOperation.OperationType.MOUNT_CGROUPS, hierarchy);	Configuration conf = super.getConf();	mountCGroupsOp.appendArgs(cgroupKVs);	PrivilegedOperationExecutor privilegedOperationExecutor = getPrivilegedOperationExecutor();	privilegedOperationExecutor.executePrivilegedOperation(mountCGroupsOp, false);	} catch (PrivilegedOperationException e) {	int exitCode = e.getExitCode();	
exception in linuxcontainerexecutor mountcgroups 

========================= hadoop sample_1678 =========================

init();	preProcessArgs();	parseArgv();	if (printUsage) {	printUsage(detailedUsage_);	return 0;	}	postProcessArgs();	setJobConf();	} catch (IllegalArgumentException ex) {	
error in streaming job 

protected void setJobConf() throws IOException {	if (additionalConfSpec_ != null) {	
additionalconfspec option is deprecated please use conf instead 

if (jar_ != null && isLocalHadoop()) {	File wd = new File(".").getAbsoluteFile();	RunJar.unJar(new File(jar_), wd);	}	jc_ = new JobClient(jobConf_);	running_ = null;	try {	running_ = jc_.submitJob(jobConf_);	jobId_ = running_.getID();	if (background_) {	
job is running in background 

File wd = new File(".").getAbsoluteFile();	RunJar.unJar(new File(jar_), wd);	}	jc_ = new JobClient(jobConf_);	running_ = null;	try {	running_ = jc_.submitJob(jobConf_);	jobId_ = running_.getID();	if (background_) {	} else if (!jc_.monitorAndPrintJob(jobConf_, running_)) {	
job not successful 

}	jc_ = new JobClient(jobConf_);	running_ = null;	try {	running_ = jc_.submitJob(jobConf_);	jobId_ = running_.getID();	if (background_) {	} else if (!jc_.monitorAndPrintJob(jobConf_, running_)) {	return 1;	}	
output directory 

jc_ = new JobClient(jobConf_);	running_ = null;	try {	running_ = jc_.submitJob(jobConf_);	jobId_ = running_.getID();	if (background_) {	} else if (!jc_.monitorAndPrintJob(jobConf_, running_)) {	return 1;	}	} catch(FileNotFoundException fe) {	
error launching job bad input path 

try {	running_ = jc_.submitJob(jobConf_);	jobId_ = running_.getID();	if (background_) {	} else if (!jc_.monitorAndPrintJob(jobConf_, running_)) {	return 1;	}	} catch(FileNotFoundException fe) {	return 2;	} catch(InvalidJobConfException je) {	
error launching job invalid job conf 

jobId_ = running_.getID();	if (background_) {	} else if (!jc_.monitorAndPrintJob(jobConf_, running_)) {	return 1;	}	} catch(FileNotFoundException fe) {	return 2;	} catch(InvalidJobConfException je) {	return 3;	} catch(FileAlreadyExistsException fae) {	
error launching job output path already exists 

} else if (!jc_.monitorAndPrintJob(jobConf_, running_)) {	return 1;	}	} catch(FileNotFoundException fe) {	return 2;	} catch(InvalidJobConfException je) {	return 3;	} catch(FileAlreadyExistsException fae) {	return 4;	} catch(IOException ioe) {	
error launching job 

}	} catch(FileNotFoundException fe) {	return 2;	} catch(InvalidJobConfException je) {	return 3;	} catch(FileAlreadyExistsException fae) {	return 4;	} catch(IOException ioe) {	return 5;	} catch (InterruptedException ie) {	
error monitoring job 

========================= hadoop sample_5816 =========================

private void verifyFilesUnreadablebyHDFS(MiniDFSCluster cluster, Path root) throws Exception{	DistributedFileSystem fs = cluster.getFileSystem();	Queue<Path> paths = new LinkedList<>();	paths.add(root);	while (!paths.isEmpty()) {	Path p = paths.poll();	FileStatus stat = fs.getFileStatus(p);	if (!stat.isDirectory()) {	try {	
testing path 

========================= hadoop sample_461 =========================

Path invalidXMLDir = new Path("/dirContainingInvalidXMLChar\u0000here");	hdfs.mkdirs(invalidXMLDir);	dirCount++;	Path stickyBitDir = new Path("/stickyBit");	hdfs.mkdirs(stickyBitDir);	hdfs.setPermission(stickyBitDir, new FsPermission(FsAction.ALL, FsAction.ALL, FsAction.ALL, true));	dirCount++;	writtenFiles.put(stickyBitDir.toString(), hdfs.getFileStatus(stickyBitDir));	Token<?>[] delegationTokens = hdfs .addDelegationTokens(TEST_RENEWER, null);	for (Token<?> t : delegationTokens) {	
got token 

hdfs.setXAttr(xattr, "user.a3", null);	hdfs.setXAttr(xattr, "user.a4", new byte[]{ -0x3d, 0x28 });	writtenFiles.put(xattr.toString(), hdfs.getFileStatus(xattr));	hdfs.setAcl( xattr, Lists.newArrayList(aclEntry(ACCESS, USER, ALL), aclEntry(ACCESS, USER, "foo", ALL), aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(ACCESS, GROUP, "bar", READ_EXECUTE), aclEntry(ACCESS, OTHER, EXECUTE)));	hdfs.setSafeMode(SafeModeAction.SAFEMODE_ENTER, false);	hdfs.saveNamespace();	originalFsimage = FSImageTestUtil.findLatestImageFile(FSImageTestUtil .getFSImage(cluster.getNameNode()).getStorage().getStorageDir(0));	if (originalFsimage == null) {	throw new RuntimeException("Didn't generate or can't find fsimage");	}	
original fs image file is 

========================= hadoop sample_7128 =========================

public MyAppMaster(Clock clock) {	super(MyAppMaster.class.getName());	if (clock == null) {	clock = SystemClock.getInstance();	}	this.clock = clock;	
created myappmaster 

========================= hadoop sample_5115 =========================

final long submitNanos = System.nanoTime();	ListenableFuture<Void> ret = null;	try {	ret = singleThreadExecutor.submit(new Callable<Void>() {	public Void call() throws IOException {	throwIfOutOfSync();	long rpcSendTimeNanos = System.nanoTime();	try {	getProxy().journal(createReqInfo(), segmentTxId, firstTxnId, numTxns, data);	} catch (IOException e) {	
remote journal failed to write txns will try to write to this jn again after the next log roll 

outOfSync = true;	}	throw e;	} finally {	long now = System.nanoTime();	long rpcTime = TimeUnit.MICROSECONDS.convert( now - rpcSendTimeNanos, TimeUnit.NANOSECONDS);	long endToEndTime = TimeUnit.MICROSECONDS.convert( now - submitNanos, TimeUnit.NANOSECONDS);	metrics.addWriteEndToEndLatency(endToEndTime);	metrics.addWriteRpcLatency(rpcTime);	if (rpcTime / 1000 > WARN_JOURNAL_MILLIS_THRESHOLD) {	
took ms to send a batch of edits bytes to remote journal 

public Void call() throws IOException {	getProxy().startLogSegment(createReqInfo(), txid, layoutVersion);	synchronized (IPCLoggerChannel.this) {	if (outOfSync) {	outOfSync = false;	
restarting previously stopped writes to in segment starting at txid 

========================= hadoop sample_8398 =========================

Map<String, MetricsConfig> map = c.getInstanceConfigs("t1");	Map<String, MetricsConfig> map2 = c.getInstanceConfigs("t2");	assertEquals("number of t1 instances", 2, map.size());	assertEquals("number of t2 instances", 1, map2.size());	assertTrue("contains t1 instance i1", map.containsKey("i1"));	assertTrue("contains t1 instance 42", map.containsKey("42"));	assertTrue("contains t2 instance i1", map2.containsKey("i1"));	MetricsConfig t1i1 = map.get("i1");	MetricsConfig t1i42 = map.get("42");	MetricsConfig t2i1 = map2.get("i1");	
instance 

Map<String, MetricsConfig> map = c.getInstanceConfigs("t1");	Map<String, MetricsConfig> map2 = c.getInstanceConfigs("t2");	assertEquals("number of t1 instances", 2, map.size());	assertEquals("number of t2 instances", 1, map2.size());	assertTrue("contains t1 instance i1", map.containsKey("i1"));	assertTrue("contains t1 instance 42", map.containsKey("42"));	assertTrue("contains t2 instance i1", map2.containsKey("i1"));	MetricsConfig t1i1 = map.get("i1");	MetricsConfig t1i42 = map.get("42");	MetricsConfig t2i1 = map2.get("i1");	
instance 

Map<String, MetricsConfig> map = c.getInstanceConfigs("t1");	Map<String, MetricsConfig> map2 = c.getInstanceConfigs("t2");	assertEquals("number of t1 instances", 2, map.size());	assertEquals("number of t2 instances", 1, map2.size());	assertTrue("contains t1 instance i1", map.containsKey("i1"));	assertTrue("contains t1 instance 42", map.containsKey("42"));	assertTrue("contains t2 instance i1", map2.containsKey("i1"));	MetricsConfig t1i1 = map.get("i1");	MetricsConfig t1i42 = map.get("42");	MetricsConfig t2i1 = map2.get("i1");	
instance 

========================= hadoop sample_2795 =========================

DFSTestUtil.waitReplication(dfs, filePath, repl);	final LocatedBlocks locatedblocks = dfs.dfs.getNamenode() .getBlockLocations(filePath.toString(), 0L, BLOCK_SIZE);	Assert.assertEquals(repl, locatedblocks.get(0).getLocations().length);	LocatedBlock lblock = locatedblocks.get(0);	DatanodeInfo[] datanodeinfos = lblock.getLocations();	ExtendedBlock block = lblock.getBlock();	for (int i = 0; i < corruptBlockCount; i++) {	DatanodeInfo dninfo = datanodeinfos[i];	final DataNode dn = cluster.getDataNode(dninfo.getIpcPort());	cluster.corruptReplica(dn, block);	
corrupted block on data node 

private void dfsClientReadFile(Path corruptedFile) throws IOException, UnresolvedLinkException {	DFSInputStream in = dfs.dfs.open(corruptedFile.toUri().getPath());	byte[] buf = new byte[buffersize];	int nRead = 0;	try {	do {	nRead = in.read(buf, 0, buf.length);	} while (nRead > 0);	} catch (ChecksumException ce) {	
dfsclientreadfile caught checksumexception 

private void dfsClientReadFile(Path corruptedFile) throws IOException, UnresolvedLinkException {	DFSInputStream in = dfs.dfs.open(corruptedFile.toUri().getPath());	byte[] buf = new byte[buffersize];	int nRead = 0;	try {	do {	nRead = in.read(buf, 0, buf.length);	} while (nRead > 0);	} catch (ChecksumException ce) {	} catch (BlockMissingException bme) {	
dfsclientreadfile caught blockmissingexception 

DFSInputStream in = dfs.dfs.open(corruptedFile.toUri().getPath());	byte[] buf = new byte[buffersize];	int startPosition = 2;	int nRead = 0;	try {	do {	nRead = in.read(startPosition, buf, 0, buf.length);	startPosition += buf.length;	} while (nRead > 0);	} catch (BlockMissingException bme) {	
dfsclientreadfile caught blockmissingexception 

private static void testFsckListCorruptFilesBlocks(Path filePath, int errorCode) throws Exception{	String outStr = runFsck(conf, errorCode, true, filePath.toString(), "-list-corruptfileblocks");	
fsck list corruptfileblocks out 

========================= hadoop sample_7597 =========================

public static void init() {	sockDir = new TemporarySocketDirectory();	DomainSocket.disableBindPathValidation();	prevCacheManipulator = NativeIO.POSIX.getCacheManipulator();	NativeIO.POSIX.setCacheManipulator(new CacheManipulator() {	public void mlock(String identifier, ByteBuffer mmap, long length) throws IOException {	
mlocking 

========================= hadoop sample_7714 =========================

private UserGroupInformation checkAcls(String method) throws IOException {	UserGroupInformation user;	try {	user = UserGroupInformation.getCurrentUser();	} catch (IOException ioe) {	
couldn t get current user 

private UserGroupInformation checkAcls(String method) throws IOException {	UserGroupInformation user;	try {	user = UserGroupInformation.getCurrentUser();	} catch (IOException ioe) {	HSAuditLogger.logFailure("UNKNOWN", method, adminAcl.toString(), HISTORY_ADMIN_SERVER, "Couldn't get current user");	throw ioe;	}	if (!adminAcl.isUserAllowed(user)) {	
user doesn t have permission to call 

try {	user = UserGroupInformation.getCurrentUser();	} catch (IOException ioe) {	HSAuditLogger.logFailure("UNKNOWN", method, adminAcl.toString(), HISTORY_ADMIN_SERVER, "Couldn't get current user");	throw ioe;	}	if (!adminAcl.isUserAllowed(user)) {	HSAuditLogger.logFailure(user.getShortUserName(), method, adminAcl.toString(), HISTORY_ADMIN_SERVER, AuditConstants.UNAUTHORIZED_USER);	throw new AccessControlException("User " + user.getShortUserName() + " doesn't have permission" + " to call '" + method + "'");	}	
hs admin invoked by user 

========================= hadoop sample_5379 =========================

private void printAtime(Path path, Path ssRoot, String ssName) throws IOException {	Path ssPath = getSSpath(path, ssRoot, ssName);	
access time 

SnapshotTestHelper.createSnapshot(hdfs, root, "s2");	printAtime(filePreSS, root, "s2");	printAtime(dirPreSS, root, "s2");	printAtime(filePostSS, root, "s2");	printAtime(dirPostSS, root, "s2");	Thread.sleep(3000);	now = Time.now();	hdfs.setReplication(filePostSS, (short) (REPLICATION - 1));	hdfs.setTimes(filePostSS, -1, now);	SnapshotTestHelper.createSnapshot(hdfs, root, "s3");	
snapshotdiff 

SnapshotTestHelper.createSnapshot(hdfs, root, "s2");	printAtime(filePreSS, root, "s2");	printAtime(dirPreSS, root, "s2");	printAtime(filePostSS, root, "s2");	printAtime(dirPostSS, root, "s2");	Thread.sleep(3000);	now = Time.now();	hdfs.setReplication(filePostSS, (short) (REPLICATION - 1));	hdfs.setTimes(filePostSS, -1, now);	SnapshotTestHelper.createSnapshot(hdfs, root, "s3");	
s0 s1 

SnapshotTestHelper.createSnapshot(hdfs, root, "s2");	printAtime(filePreSS, root, "s2");	printAtime(dirPreSS, root, "s2");	printAtime(filePostSS, root, "s2");	printAtime(dirPostSS, root, "s2");	Thread.sleep(3000);	now = Time.now();	hdfs.setReplication(filePostSS, (short) (REPLICATION - 1));	hdfs.setTimes(filePostSS, -1, now);	SnapshotTestHelper.createSnapshot(hdfs, root, "s3");	
snapshotdiff 

SnapshotTestHelper.createSnapshot(hdfs, root, "s2");	printAtime(filePreSS, root, "s2");	printAtime(dirPreSS, root, "s2");	printAtime(filePostSS, root, "s2");	printAtime(dirPostSS, root, "s2");	Thread.sleep(3000);	now = Time.now();	hdfs.setReplication(filePostSS, (short) (REPLICATION - 1));	hdfs.setTimes(filePostSS, -1, now);	SnapshotTestHelper.createSnapshot(hdfs, root, "s3");	
s1 s2 

========================= hadoop sample_7344 =========================

public void testSerialSubmit() throws Exception {	policy = GridmixJobSubmissionPolicy.SERIAL;	
serial started at 

public void testSerialSubmit() throws Exception {	policy = GridmixJobSubmissionPolicy.SERIAL;	doSubmission(JobCreator.LOADJOB.name(), false);	
serial ended at 

public void testReplaySubmit() throws Exception {	policy = GridmixJobSubmissionPolicy.REPLAY;	
replay started at 

public void testReplaySubmit() throws Exception {	policy = GridmixJobSubmissionPolicy.REPLAY;	doSubmission(JobCreator.LOADJOB.name(), false);	
replay ended at 

========================= hadoop sample_6105 =========================

public static void setConfiguration(Configuration conf) {	
updating configuration 

public static void setTokenServiceUseIp(boolean flag) {	if (LOG.isDebugEnabled()) {	
setting to 

public static void setTokenService(Token<?> token, InetSocketAddress addr) {	Text service = buildTokenService(addr);	if (token != null) {	token.setService(service);	if (LOG.isDebugEnabled()) {	
acquired token 

public static void setTokenService(Token<?> token, InetSocketAddress addr) {	Text service = buildTokenService(addr);	if (token != null) {	token.setService(service);	if (LOG.isDebugEnabled()) {	}	} else {	
failed to get token for service 

========================= hadoop sample_3757 =========================

public void testRDNS() throws Exception {	InetAddress localhost = getLocalIPAddr();	try {	String s = DNS.reverseDns(localhost, null);	
local reverse dns hostname is 

public void testRDNS() throws Exception {	InetAddress localhost = getLocalIPAddr();	try {	String s = DNS.reverseDns(localhost, null);	} catch (NameNotFoundException | CommunicationException e) {	if (!localhost.isLinkLocalAddress() || localhost.isLoopbackAddress()) {	
reverse dns failing as due to incomplete networking 

public void testLocalhostResolves() throws Exception {	InetAddress localhost = InetAddress.getByName("localhost");	assertNotNull("localhost is null", localhost);	
localhost ipaddr is 

========================= hadoop sample_2845 =========================

for (LocatedBlock blk : locations.getLocatedBlocks()) {	DatanodeInfo[] datanodes = blk.getLocations();	if (datanodes.length <= 1) break;	if (datanodes.length == 2) {	isNotOnSameRack = !(datanodes[0].getNetworkLocation().equals( datanodes[1].getNetworkLocation()));	break;	}	isOnSameRack = false;	isNotOnSameRack = false;	for (int i = 0; i < datanodes.length-1; i++) {	
datanode 

DFSTestUtil.createFile(fs, file1, 1024, replFactor, 0);	DFSTestUtil.waitReplication(fs, file1, replFactor);	ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, file1);	int blockFilesCorrupted = corruptBlockByDeletingBlockFile? cluster.corruptBlockOnDataNodesByDeletingBlockFile(block) : cluster.corruptBlockOnDataNodes(block);	assertEquals("Corrupted too few blocks", replFactor, blockFilesCorrupted);	replFactor = 2;	fs.setReplication(file1, replFactor);	blocks = dfsClient.getNamenode(). getBlockLocations(file1.toString(), 0, Long.MAX_VALUE);	while (blocks.get(0).isCorrupt() != true) {	try {	
waiting until block is marked as corrupt 

private void waitForBlockReplication(String filename, ClientProtocol namenode, int expected, long maxWaitSec, boolean isUnderConstruction, boolean noOverReplication) throws IOException {	long start = Time.monotonicNow();	
checking for block replication for 

iter.hasNext();) {	LocatedBlock block = iter.next();	if (isUnderConstruction && !iter.hasNext()) {	break;	}	int actual = block.getLocations().length;	if (noOverReplication) {	assertTrue(actual <= expected);	}	if ( actual < expected ) {	
not enough replicas for yet expecting got 

ExtendedBlock block = dfsClient.getNamenode().getBlockLocations(testFile, 0, Long.MAX_VALUE).get(0).getBlock();	List<MaterializedReplica> replicas = new ArrayList<>();	for (int dnIndex=0; dnIndex<3; dnIndex++) {	replicas.add(cluster.getMaterializedReplica(dnIndex, block));	}	assertEquals(3, replicas.size());	cluster.shutdown();	int fileCount = 0;	for (MaterializedReplica replica : replicas) {	if (fileCount == 0) {	
deleting block 

for (int dnIndex=0; dnIndex<3; dnIndex++) {	replicas.add(cluster.getMaterializedReplica(dnIndex, block));	}	assertEquals(3, replicas.size());	cluster.shutdown();	int fileCount = 0;	for (MaterializedReplica replica : replicas) {	if (fileCount == 0) {	replica.deleteData();	} else {	
corrupting file 

cluster.shutdown();	int fileCount = 0;	for (MaterializedReplica replica : replicas) {	if (fileCount == 0) {	replica.deleteData();	} else {	replica.corruptData();	}	fileCount++;	}	
restarting minicluster after deleting a replica and corrupting crcs 

public void testNoExtraReplicationWhenBlockReceivedIsLate() throws Exception {	
test block replication when blockreceived is late 

public void testReplicationWhileUnderConstruction() throws Exception {	
test block replication in under construction 

========================= hadoop sample_7651 =========================

public void testDefaultSendBufferSize() throws IOException {	final int sendBufferSize = getSendBufferSize(new Configuration());	
if not specified the auto tuned send buffer size is 

public void testSpecifiedSendBufferSize() throws IOException {	final Configuration conf1 = new Configuration();	conf1.setInt(DFS_CLIENT_SOCKET_SEND_BUFFER_SIZE_KEY, 256 * 1024);	final int sendBufferSize1 = getSendBufferSize(conf1);	final Configuration conf2 = new Configuration();	conf2.setInt(DFS_CLIENT_SOCKET_SEND_BUFFER_SIZE_KEY, 1024);	final int sendBufferSize2 = getSendBufferSize(conf2);	
large buf size is small is 

public void testAutoTuningSendBufferSize() throws IOException {	final Configuration conf = new Configuration();	conf.setInt(DFS_CLIENT_SOCKET_SEND_BUFFER_SIZE_KEY, 0);	final int sendBufferSize = getSendBufferSize(conf);	
the auto tuned send buffer size is 

private int getSendBufferSize(Configuration conf) throws IOException {	final MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf) .numDataNodes(1) .build();	try {	cluster.waitActive();	
minidfscluster started 

========================= hadoop sample_7201 =========================

public void serviceInit(Configuration conf) throws Exception {	super.serviceInit(conf);	
cachedhistorystorage init 

private void createLoadedJobCache(Configuration conf) {	loadedJobCacheSize = conf.getInt( JHAdminConfig.MR_HISTORY_LOADED_JOB_CACHE_SIZE, JHAdminConfig.DEFAULT_MR_HISTORY_LOADED_JOB_CACHE_SIZE);	useLoadedTasksCache = false;	try {	String taskSizeString = conf .get(JHAdminConfig.MR_HISTORY_LOADED_TASKS_CACHE_SIZE);	if (taskSizeString != null) {	loadedTasksCacheSize = Math.max(Integer.parseInt(taskSizeString), 1);	useLoadedTasksCache = true;	}	} catch (NumberFormatException nfe) {	
the property is not an integer value please set it to a positive integer value 

public void refreshLoadedJobCache() {	if (getServiceState() == STATE.STARTED) {	setConfig(createConf());	createLoadedJobCache(getConfig());	} else {	
failed to execute refreshloadedjobcache cachedhistorystorage is not started 

private Job loadJob(JobId jobId) throws RuntimeException, IOException {	if (LOG.isDebugEnabled()) {	
looking for job 

public Map<JobId, Job> getAllPartialJobs() {	
called getallpartialjobs 

public Map<JobId, Job> getAllPartialJobs() {	SortedMap<JobId, Job> result = new TreeMap<JobId, Job>();	try {	for (HistoryFileInfo mi : hsManager.getAllFileInfo()) {	if (mi != null) {	JobId id = mi.getJobId();	result.put(id, new PartialJob(mi.getJobIndexInfo(), id));	}	}	} catch (IOException e) {	
error trying to scan for all fileinfos 

========================= hadoop sample_5378 =========================

public static void sleepAtLeastIgnoreInterrupts(long millis) {	long start = Time.now();	while (Time.now() - start < millis) {	long timeToSleep = millis - (Time.now() - start);	try {	Thread.sleep(timeToSleep);	} catch (InterruptedException ie) {	
interrupted while sleeping 

========================= hadoop sample_3613 =========================

public void init(Configuration conf) throws IOException {	
initializing remotewasbauthorizerimpl instance 

========================= hadoop sample_6411 =========================

protected void serviceStart() throws Exception {	boolean recoveryEnabled = getConfig().getBoolean( JHAdminConfig.MR_HS_RECOVERY_ENABLE, JHAdminConfig.DEFAULT_MR_HS_RECOVERY_ENABLE);	if (recoveryEnabled) {	assert stateStore.isInState(STATE.STARTED);	HistoryServerState state = stateStore.loadState();	jhsDTSecretManager.recover(state);	}	try {	jhsDTSecretManager.startThreads();	} catch(IOException io) {	
error while starting the secret manager threads 

StringUtils.startupShutdownMessage(JobHistoryServer.class, args, LOG);	JobHistoryServer jobHistoryServer = null;	try {	jobHistoryServer = new JobHistoryServer();	ShutdownHookManager.get().addShutdownHook( new CompositeServiceShutdownHook(jobHistoryServer), SHUTDOWN_HOOK_PRIORITY);	YarnConfiguration conf = new YarnConfiguration(new JobConf());	new GenericOptionsParser(conf, args);	jobHistoryServer.init(conf);	jobHistoryServer.start();	} catch (Throwable t) {	
error starting jobhistoryserver 

========================= hadoop sample_5397 =========================

public void testPolicyProviderForServer() {	List<?> ifaces = ClassUtils.getAllInterfaces(rpcServerClass);	Set<Class<?>> serverProtocols = new HashSet<>(ifaces.size());	for (Object obj : ifaces) {	Class<?> iface = (Class<?>)obj;	if (iface.getSimpleName().endsWith("Protocol")) {	serverProtocols.add(iface);	}	}	
running test for rpc server found server protocols and policy provider protocols 

========================= hadoop sample_7650 =========================

}	if (results.hasOption("lazyOutput")) {	if (Boolean.parseBoolean(results.getOptionValue("lazyOutput"))) {	LazyOutputFormat.setOutputFormatClass(job, job.getOutputFormat().getClass());	}	}	if (results.hasOption("program")) {	setExecutable(job, results.getOptionValue("program"));	}	if (results.hasOption("jobconf")) {	
jobconf option is deprecated please use d instead 

public ClassLoader run() {	return new URLClassLoader(urls);	}	}	);	job.setClassLoader(loader);	}	runJob(job);	return 0;	} catch (ParseException pe) {	
error 

========================= hadoop sample_4705 =========================

public long renew(Token<?> token, Configuration conf) throws IOException {	
renewing delegation token 

public void cancel(Token<?> token, Configuration conf) throws IOException {	
canceling delegation token 

try {	final String doAsUser = getDoAsUser();	conn = getActualUgi().doAs(new PrivilegedExceptionAction <HttpURLConnection>() {	public HttpURLConnection run() throws Exception {	DelegationTokenAuthenticatedURL authUrl = new DelegationTokenAuthenticatedURL(configurator);	return authUrl.openConnection(url, authToken, doAsUser);	}	});	} catch (IOException ex) {	if (ex instanceof SocketTimeoutException) {	
failed to connect to 

public long renewDelegationToken(final Token<?> dToken) throws IOException {	try {	final String doAsUser = getDoAsUser();	final DelegationTokenAuthenticatedURL.Token token = generateDelegationToken(dToken);	final URL url = createURL(null, null, null, null);	
renewing delegation token with url as 

public Void cancelDelegationToken(final Token<?> dToken) throws IOException {	try {	final String doAsUser = getDoAsUser();	final DelegationTokenAuthenticatedURL.Token token = generateDelegationToken(dToken);	return getActualUgi().doAs( new PrivilegedExceptionAction<Void>() {	public Void run() throws Exception {	final URL url = createURL(null, null, null, null);	
cancelling delegation token with url as 

Token<?>[] tokens = null;	Text dtService = getDelegationTokenService();	Token<?> token = credentials.getToken(dtService);	if (token == null) {	final URL url = createURL(null, null, null, null);	final DelegationTokenAuthenticatedURL authUrl = new DelegationTokenAuthenticatedURL(configurator);	try {	final String doAsUser = getDoAsUser();	token = getActualUgi().doAs(new PrivilegedExceptionAction<Token<?>>() {	public Token<?> run() throws Exception {	
getting new token from renewer 

final URL url = createURL(null, null, null, null);	final DelegationTokenAuthenticatedURL authUrl = new DelegationTokenAuthenticatedURL(configurator);	try {	final String doAsUser = getDoAsUser();	token = getActualUgi().doAs(new PrivilegedExceptionAction<Token<?>>() {	public Token<?> run() throws Exception {	return authUrl.getDelegationToken(url, new DelegationTokenAuthenticatedURL.Token(), renewer, doAsUser);	}	});	if (token != null) {	
new token received 

private UserGroupInformation getActualUgi() throws IOException {	final UserGroupInformation currentUgi = UserGroupInformation .getCurrentUser();	if (LOG.isDebugEnabled()) {	UserGroupInformation.logAllUserInfo(currentUgi);	}	UserGroupInformation actualUgi = currentUgi;	if (currentUgi.getRealUser() != null) {	actualUgi = currentUgi.getRealUser();	}	if (UserGroupInformation.isSecurityEnabled() && !containsKmsDt(actualUgi) && !actualUgi.hasKerberosCredentials()) {	
using loginuser when kerberos is enabled but the actual user does not have either kms delegation token or kerberos credentials 

========================= hadoop sample_3490 =========================

}	ShellDecryptionKeyProvider provider = new ShellDecryptionKeyProvider();	Configuration conf = new Configuration();	String account = "testacct";	String key = "key";	conf.set(SimpleKeyProvider.KEY_ACCOUNT_KEY_PREFIX + account, key);	try {	provider.getStorageAccountKey(account, conf);	Assert .fail("fs.azure.shellkeyprovider.script is not specified, we should throw");	} catch (KeyProviderException e) {	
received an expected exception 

========================= hadoop sample_6368 =========================

this.writeLock.lock();	Set<ApplicationId> userApps = usersApplications.get(user);	if (userApps == null) {	userApps = new HashSet<ApplicationId>();	usersApplications.put(user, userApps);	activeUsers.incrementAndGet();	metrics.incrActiveUsers();	userLimitNeedsRecompute();	updateActiveUsersResourceUsage(user);	if (LOG.isDebugEnabled()) {	
user added to activeusers currently 

if (userApps.remove(applicationId)) {	metrics.deactivateApp(user);	}	if (userApps.isEmpty()) {	usersApplications.remove(user);	activeUsers.decrementAndGet();	metrics.decrActiveUsers();	userLimitNeedsRecompute();	updateNonActiveUsersResourceUsage(user);	if (LOG.isDebugEnabled()) {	
user removed from activeusers currently 

private ResourceUsage getTotalResourceUsagePerUser(String userName) {	if (nonActiveUsersSet.contains(userName)) {	return totalResUsageForNonActiveUsers;	} else if (activeUsersSet.contains(userName)) {	return totalResUsageForActiveUsers;	} else {	
user is not present in active non active this is highly unlikely we can consider this user in non active list in this case 

========================= hadoop sample_933 =========================

public void run() {	MetricsCollectorImpl builder = new MetricsCollectorImpl();	try {	sa.getMetrics(builder, true);	
reset lastrecs 

public void run() {	MetricsCollectorImpl builder = new MetricsCollectorImpl();	try {	sa.getMetrics(builder, true);	} catch (Exception e) {	hasError.set(true);	LOG.error(e.getStackTrace());	} finally {	if (hasError.get()) {	
hit error stopping now 

========================= hadoop sample_2794 =========================

if (percentage == 0.0 || maxMemory == 0) {	return 0;	}	final String vmBit = System.getProperty("sun.arch.data.model");	final double percentDivisor = 100.0/percentage;	final double percentMemory = maxMemory/percentDivisor;	final int e1 = (int)(Math.log(percentMemory)/Math.log(2.0) + 0.5);	final int e2 = e1 - ("32".equals(vmBit)? 2: 3);	final int exponent = e2 < 0? 0: e2 > 30? 30: e2;	final int c = 1 << exponent;	
computing capacity for map 

========================= hadoop sample_3701 =========================

if (drained) {	waitForDrained.notify();	}	}	}	Event event;	try {	event = eventQueue.take();	} catch(InterruptedException ie) {	if (!stopped) {	
asyncdispatcher thread interrupted 

protected void serviceStop() throws Exception {	if (drainEventsOnStop) {	blockNewEvents = true;	
asyncdispatcher is draining to stop ignoring any new events 

protected void serviceStop() throws Exception {	if (drainEventsOnStop) {	blockNewEvents = true;	long endTime = System.currentTimeMillis() + getConfig() .getLong(YarnConfiguration.DISPATCHER_DRAIN_EVENTS_TIMEOUT, YarnConfiguration.DEFAULT_DISPATCHER_DRAIN_EVENTS_TIMEOUT);	synchronized (waitForDrained) {	while (!isDrained() && eventHandlingThread != null && eventHandlingThread.isAlive() && System.currentTimeMillis() < endTime) {	waitForDrained.wait(100);	
waiting for asyncdispatcher to drain thread state is 

waitForDrained.wait(100);	}	}	}	stopped = true;	if (eventHandlingThread != null) {	eventHandlingThread.interrupt();	try {	eventHandlingThread.join();	} catch (InterruptedException ie) {	
interrupted exception while stopping 

protected void dispatch(Event event) {	if (LOG.isDebugEnabled()) {	
dispatching the event 

}	Class<? extends Enum> type = event.getType().getDeclaringClass();	try{	EventHandler handler = eventDispatchers.get(type);	if(handler != null) {	handler.handle(event);	} else {	throw new Exception("No handler for registered for " + type);	}	} catch (Throwable t) {	
error in dispatcher thread 

public void register(Class<? extends Enum> eventType, EventHandler handler) {	EventHandler<Event> registeredHandler = (EventHandler<Event>) eventDispatchers.get(eventType);	
registering for 

public void handle(Event event) {	if (blockNewEvents) {	return;	}	drained = false;	int qSize = eventQueue.size();	if (qSize != 0 && qSize % 1000 == 0 && lastEventQueueSizeLogged != qSize) {	lastEventQueueSizeLogged = qSize;	
size of event queue is 

if (blockNewEvents) {	return;	}	drained = false;	int qSize = eventQueue.size();	if (qSize != 0 && qSize % 1000 == 0 && lastEventQueueSizeLogged != qSize) {	lastEventQueueSizeLogged = qSize;	}	int remCapacity = eventQueue.remainingCapacity();	if (remCapacity < 1000) {	
very low remaining capacity in the event queue 

if (qSize != 0 && qSize % 1000 == 0 && lastEventQueueSizeLogged != qSize) {	lastEventQueueSizeLogged = qSize;	}	int remCapacity = eventQueue.remainingCapacity();	if (remCapacity < 1000) {	}	try {	eventQueue.put(event);	} catch (InterruptedException e) {	if (!stopped) {	
asyncdispatcher thread interrupted 

public void run() {	
exiting bbye 

========================= hadoop sample_2540 =========================

final byte[] computed = IO_BUF.get();	final byte[] expected = VERIFY_BUF.get();	try(FSDataInputStream in = dfs.open(f)) {	for(int i = 0; i < numBlocks; i++) {	in.read(computed);	nextBytes(i, seed, expected);	Assert.assertArrayEquals(expected, computed);	}	return true;	} catch(Exception e) {	
failed to verify file 

========================= hadoop sample_7257 =========================

this.ioSortFactor = jobConf.getInt(MRJobConfig.IO_SORT_FACTOR, MRJobConfig.DEFAULT_IO_SORT_FACTOR);	final float singleShuffleMemoryLimitPercent = jobConf.getFloat(MRJobConfig.SHUFFLE_MEMORY_LIMIT_PERCENT, DEFAULT_SHUFFLE_MEMORY_LIMIT_PERCENT);	if (singleShuffleMemoryLimitPercent < 0.0f || singleShuffleMemoryLimitPercent > 1.0f) {	throw new IllegalArgumentException("Invalid value for " + MRJobConfig.SHUFFLE_MEMORY_LIMIT_PERCENT + ": " + singleShuffleMemoryLimitPercent);	}	usedMemory = 0L;	commitMemory = 0L;	long maxSingleShuffleLimitConfiged = (long)(memoryLimit * singleShuffleMemoryLimitPercent);	if(maxSingleShuffleLimitConfiged > Integer.MAX_VALUE) {	maxSingleShuffleLimitConfiged = Integer.MAX_VALUE;	
the max number of bytes for a single in memory shuffle cannot be larger than integer max value setting it to integer max value 

public synchronized MapOutput<K,V> reserve(TaskAttemptID mapId, long requestedSize, int fetcher ) throws IOException {	if (requestedSize > maxSingleShuffleLimit) {	
shuffling to disk since is greater than maxsingleshufflelimit 

public synchronized MapOutput<K,V> reserve(TaskAttemptID mapId, long requestedSize, int fetcher ) throws IOException {	if (requestedSize > maxSingleShuffleLimit) {	return new OnDiskMapOutput<K,V>(mapId, this, requestedSize, jobConf, fetcher, true, FileSystem.getLocal(jobConf).getRaw(), mapOutputFile.getInputFileForWrite(mapId.getTaskID(), requestedSize));	}	if (usedMemory > memoryLimit) {	
stalling shuffle since usedmemory is greater than memorylimit commitmemory is 

public synchronized MapOutput<K,V> reserve(TaskAttemptID mapId, long requestedSize, int fetcher ) throws IOException {	if (requestedSize > maxSingleShuffleLimit) {	return new OnDiskMapOutput<K,V>(mapId, this, requestedSize, jobConf, fetcher, true, FileSystem.getLocal(jobConf).getRaw(), mapOutputFile.getInputFileForWrite(mapId.getTaskID(), requestedSize));	}	if (usedMemory > memoryLimit) {	return null;	}	
proceeding with shuffle since usedmemory is lesser than memorylimit commitmemory is 

public synchronized void closeInMemoryFile(InMemoryMapOutput<K,V> mapOutput) {	inMemoryMapOutputs.add(mapOutput);	
closeinmemoryfile map output of size inmemorymapoutputs size commitmemory usedmemory 

public synchronized void closeInMemoryMergedFile(InMemoryMapOutput<K,V> mapOutput) {	inMemoryMergedMapOutputs.add(mapOutput);	
closeinmemorymergedfile size inmemorymergedmapoutputs size 

public void merge(List<InMemoryMapOutput<K, V>> inputs) throws IOException {	if (inputs == null || inputs.size() == 0) {	return;	}	TaskAttemptID dummyMapId = inputs.get(0).getMapId();	List<Segment<K, V>> inMemorySegments = new ArrayList<Segment<K, V>>();	long mergeOutputSize = createInMemorySegments(inputs, inMemorySegments, 0);	int noInMemorySegments = inMemorySegments.size();	InMemoryMapOutput<K, V> mergedMapOutputs = unconditionalReserve(dummyMapId, mergeOutputSize, false);	Writer<K, V> writer = new InMemoryWriter<K, V>(mergedMapOutputs.getArrayStream());	
initiating memory to memory merge with segments of total size 

}	TaskAttemptID dummyMapId = inputs.get(0).getMapId();	List<Segment<K, V>> inMemorySegments = new ArrayList<Segment<K, V>>();	long mergeOutputSize = createInMemorySegments(inputs, inMemorySegments, 0);	int noInMemorySegments = inMemorySegments.size();	InMemoryMapOutput<K, V> mergedMapOutputs = unconditionalReserve(dummyMapId, mergeOutputSize, false);	Writer<K, V> writer = new InMemoryWriter<K, V>(mergedMapOutputs.getArrayStream());	RawKeyValueIterator rIter = Merger.merge(jobConf, rfs, (Class<K>)jobConf.getMapOutputKeyClass(), (Class<V>)jobConf.getMapOutputValueClass(), inMemorySegments, inMemorySegments.size(), new Path(reduceId.toString()), (RawComparator<K>)jobConf.getOutputKeyComparator(), reporter, null, null, null);	Merger.writeFile(rIter, writer, reporter, jobConf);	writer.close();	
memory to memory merge of the files in memory complete 

TaskID mapTaskId = mapId.getTaskID();	List<Segment<K, V>> inMemorySegments = new ArrayList<Segment<K, V>>();	long mergeOutputSize = createInMemorySegments(inputs, inMemorySegments,0);	int noInMemorySegments = inMemorySegments.size();	Path outputPath = mapOutputFile.getInputFileForWrite(mapTaskId, mergeOutputSize).suffix( Task.MERGED_OUTPUT_PREFIX);	FSDataOutputStream out = CryptoUtils.wrapIfNecessary(jobConf, rfs.create(outputPath));	Writer<K, V> writer = new Writer<K, V>(jobConf, out, (Class<K>) jobConf.getMapOutputKeyClass(), (Class<V>) jobConf.getMapOutputValueClass(), codec, null, true);	RawKeyValueIterator rIter = null;	CompressAwarePath compressAwarePath;	try {	
initiating in memory merge with segments 

try {	rIter = Merger.merge(jobConf, rfs, (Class<K>)jobConf.getMapOutputKeyClass(), (Class<V>)jobConf.getMapOutputValueClass(), inMemorySegments, inMemorySegments.size(), new Path(reduceId.toString()), (RawComparator<K>)jobConf.getOutputKeyComparator(), reporter, spilledRecordsCounter, null, null);	if (null == combinerClass) {	Merger.writeFile(rIter, writer, reporter, jobConf);	} else {	combineCollector.setWriter(writer);	combineAndSpill(rIter, reduceCombineInputCounter);	}	writer.close();	compressAwarePath = new CompressAwarePath(outputPath, writer.getRawLength(), writer.getCompressedLength());	
merge of the files in memory complete local file is of size 

public void merge(List<CompressAwarePath> inputs) throws IOException {	if (inputs == null || inputs.isEmpty()) {	
no ondisk files to merge 

public void merge(List<CompressAwarePath> inputs) throws IOException {	if (inputs == null || inputs.isEmpty()) {	return;	}	long approxOutputSize = 0;	int bytesPerSum = jobConf.getInt("io.bytes.per.checksum", 512);	
ondiskmerger we have map outputs on disk triggering merge 

try {	iter = Merger.merge(jobConf, rfs, (Class<K>) jobConf.getMapOutputKeyClass(), (Class<V>) jobConf.getMapOutputValueClass(), codec, inputs.toArray(new Path[inputs.size()]), true, ioSortFactor, tmpDir, (RawComparator<K>) jobConf.getOutputKeyComparator(), reporter, spilledRecordsCounter, null, mergedMapOutputsCounter, null);	Merger.writeFile(iter, writer, reporter, jobConf);	writer.close();	compressAwarePath = new CompressAwarePath(outputPath, writer.getRawLength(), writer.getCompressedLength());	} catch (IOException e) {	localFS.delete(outputPath, true);	throw e;	}	closeOnDiskFile(compressAwarePath);	
finished merging map output files on disk of total size local output file is of size 

private RawKeyValueIterator finalMerge(JobConf job, FileSystem fs, List<InMemoryMapOutput<K,V>> inMemoryMapOutputs, List<CompressAwarePath> onDiskMapOutputs ) throws IOException {	
finalmerge called with in memory map outputs and on disk map outputs 

fs.delete(outputPath, true);	} catch (IOException ie) {	}	}	throw e;	} finally {	if (null != writer) {	writer.close();	}	}	
merged segments bytes to disk to satisfy reduce memory limit 

}	throw e;	} finally {	if (null != writer) {	writer.close();	}	}	inMemToDiskBytes = 0;	memDiskSegments.clear();	} else if (inMemToDiskBytes != 0) {	
keeping segments bytes in memory for intermediate on disk merge 

}	}	List<Segment<K,V>> diskSegments = new ArrayList<Segment<K,V>>();	long onDiskBytes = inMemToDiskBytes;	long rawBytes = inMemToDiskBytes;	CompressAwarePath[] onDisk = onDiskMapOutputs.toArray( new CompressAwarePath[onDiskMapOutputs.size()]);	for (CompressAwarePath file : onDisk) {	long fileLength = fs.getFileStatus(file).getLen();	onDiskBytes += fileLength;	rawBytes += (file.getRawDataLength() > 0) ? file.getRawDataLength() : fileLength;	
disk file length is 

List<Segment<K,V>> diskSegments = new ArrayList<Segment<K,V>>();	long onDiskBytes = inMemToDiskBytes;	long rawBytes = inMemToDiskBytes;	CompressAwarePath[] onDisk = onDiskMapOutputs.toArray( new CompressAwarePath[onDiskMapOutputs.size()]);	for (CompressAwarePath file : onDisk) {	long fileLength = fs.getFileStatus(file).getLen();	onDiskBytes += fileLength;	rawBytes += (file.getRawDataLength() > 0) ? file.getRawDataLength() : fileLength;	diskSegments.add(new Segment<K, V>(job, fs, file, codec, keepInputs, (file.toString().endsWith( Task.MERGED_OUTPUT_PREFIX) ? null : mergedMapOutputsCounter), file.getRawDataLength() ));	}	
merging files bytes from disk 

Collections.sort(diskSegments, new Comparator<Segment<K,V>>() {	public int compare(Segment<K, V> o1, Segment<K, V> o2) {	if (o1.getLength() == o2.getLength()) {	return 0;	}	return o1.getLength() < o2.getLength() ? -1 : 1;	}	});	List<Segment<K,V>> finalSegments = new ArrayList<Segment<K,V>>();	long inMemBytes = createInMemorySegments(inMemoryMapOutputs, finalSegments, 0);	
merging segments bytes from memory into reduce 

========================= hadoop sample_4928 =========================

private synchronized void applyEdits(long firstTxId, int numTxns, byte[] data) throws IOException {	Preconditions.checkArgument(firstTxId == lastAppliedTxId + 1, "Received txn batch starting at %s but expected %s", firstTxId, lastAppliedTxId + 1);	assert backupInputStream.length() == 0 : "backup input stream is not empty";	try {	if (LOG.isTraceEnabled()) {	
data 

private boolean tryConvergeJournalSpool() throws IOException {	Preconditions.checkState(bnState == BNState.JOURNAL_ONLY, "bad state: %s", bnState);	while (lastAppliedTxId < editLog.getCurSegmentTxId() - 1) {	long target = editLog.getCurSegmentTxId();	
loading edits into backupnode to try to catch up from txid to 

List<EditLogInputStream> editStreams = Lists.newArrayList();	for (EditLogInputStream s : editStreamsAll) {	if (s.getFirstTxId() != editLog.getCurSegmentTxId()) {	editStreams.add(s);	}	}	loadEdits(editStreams, getNamesystem());	}	synchronized (this) {	if (lastAppliedTxId != editLog.getCurSegmentTxId() - 1) {	
logs rolled while catching up to current segment 

}	EditLogInputStream stream = null;	Collection<EditLogInputStream> editStreams = getEditLog().selectInputStreams( getEditLog().getCurSegmentTxId(), getEditLog().getCurSegmentTxId());	for (EditLogInputStream s : editStreams) {	if (s.getFirstTxId() == getEditLog().getCurSegmentTxId()) {	stream = s;	}	break;	}	if (stream == null) {	
unable to find stream starting with this indicates that there is an error in synchronization in backupimage 

if (s.getFirstTxId() == getEditLog().getCurSegmentTxId()) {	stream = s;	}	break;	}	if (stream == null) {	return false;	}	try {	long remainingTxns = getEditLog().getLastWrittenTxId() - lastAppliedTxId;	
going to finish converging with remaining txns from in progress stream 

}	try {	long remainingTxns = getEditLog().getLastWrittenTxId() - lastAppliedTxId;	FSEditLogLoader loader = new FSEditLogLoader(getNamesystem(), lastAppliedTxId);	loader.loadFSEdits(stream, lastAppliedTxId + 1);	lastAppliedTxId = loader.getLastAppliedTxId();	assert lastAppliedTxId == getEditLog().getLastWrittenTxId();	} finally {	FSEditLog.closeAllStreams(editStreams);	}	
successfully synced backupnode with namenode at txnid 

private synchronized void setState(BNState newState) {	if (LOG.isDebugEnabled()) {	
state transition 

========================= hadoop sample_8008 =========================

Class.forName(driverClass);	} catch (ClassNotFoundException e) {	FederationStateStoreUtils.logAndThrowException(LOG, "Driver class not found.", e);	}	dataSource = new HikariDataSource();	dataSource.setDataSourceClassName(driverClass);	FederationStateStoreUtils.setUsername(dataSource, userName);	FederationStateStoreUtils.setPassword(dataSource, password);	FederationStateStoreUtils.setProperty(dataSource, FederationStateStoreUtils.FEDERATION_STORE_URL, url);	dataSource.setMaximumPoolSize(maximumPoolSize);	
initialized connection pool to the federation statestore database at address 

cstmt.executeUpdate();	long stopTime = clock.getTime();	if (cstmt.getInt(9) == 0) {	String errMsg = "SubCluster " + subClusterId + " was not registered into the StateStore";	FederationStateStoreUtils.logAndThrowStoreException(LOG, errMsg);	}	if (cstmt.getInt(9) != 1) {	String errMsg = "Wrong behavior during registration of SubCluster " + subClusterId + " into the StateStore";	FederationStateStoreUtils.logAndThrowStoreException(LOG, errMsg);	}	
registered the subcluster into the statestore 

cstmt.executeUpdate();	long stopTime = clock.getTime();	if (cstmt.getInt(3) == 0) {	String errMsg = "SubCluster " + subClusterId + " not found";	FederationStateStoreUtils.logAndThrowStoreException(LOG, errMsg);	}	if (cstmt.getInt(3) != 1) {	String errMsg = "Wrong behavior during deregistration of SubCluster " + subClusterId + " from the StateStore";	FederationStateStoreUtils.logAndThrowStoreException(LOG, errMsg);	}	
deregistered the subcluster state to 

cstmt.executeUpdate();	long stopTime = clock.getTime();	if (cstmt.getInt(4) == 0) {	String errMsg = "SubCluster " + subClusterId.toString() + " does not exist; cannot heartbeat";	FederationStateStoreUtils.logAndThrowStoreException(LOG, errMsg);	}	if (cstmt.getInt(4) != 1) {	String errMsg = "Wrong behavior during the heartbeat of SubCluster " + subClusterId;	FederationStateStoreUtils.logAndThrowStoreException(LOG, errMsg);	}	
heartbeated the statestore for the specified subcluster 

cstmt.registerOutParameter(8, java.sql.Types.BIGINT);	cstmt.registerOutParameter(9, java.sql.Types.VARCHAR);	long startTime = clock.getTime();	cstmt.execute();	long stopTime = clock.getTime();	String amRMAddress = cstmt.getString(2);	String clientRMAddress = cstmt.getString(3);	String rmAdminAddress = cstmt.getString(4);	String webAppAddress = cstmt.getString(5);	if((amRMAddress == null) || (clientRMAddress == null)) {	
the queried subcluster does not exist 

String capability = cstmt.getString(9);	subClusterInfo = SubClusterInfo.newInstance(subClusterId, amRMAddress, clientRMAddress, rmAdminAddress, webAppAddress, lastHeartBeat, state, lastStartTime, capability);	FederationStateStoreClientMetrics .succeededStateStoreCall(stopTime - startTime);	try {	FederationMembershipStateStoreInputValidator .checkSubClusterInfo(subClusterInfo);	} catch (FederationStateStoreInvalidInputException e) {	String errMsg = "SubCluster " + subClusterId.toString() + " does not exist";	FederationStateStoreUtils.logAndThrowStoreException(LOG, errMsg);	}	if (LOG.isDebugEnabled()) {	
got the information about the specified subcluster 

FederationStateStoreClientMetrics .succeededStateStoreCall(stopTime - startTime);	if (subClusterId.equals(subClusterIdHome)) {	if (cstmt.getInt(4) == 0) {	String errMsg = "The application " + appId + " was not insert into the StateStore";	FederationStateStoreUtils.logAndThrowStoreException(LOG, errMsg);	}	if (cstmt.getInt(4) != 1) {	String errMsg = "Wrong behavior during the insertion of SubCluster " + subClusterId;	FederationStateStoreUtils.logAndThrowStoreException(LOG, errMsg);	}	
insert into the statestore the application in subcluster 

}	if (cstmt.getInt(4) != 1) {	String errMsg = "Wrong behavior during the insertion of SubCluster " + subClusterId;	FederationStateStoreUtils.logAndThrowStoreException(LOG, errMsg);	}	} else {	if (cstmt.getInt(4) != 0) {	String errMsg = "The application " + appId + " does exist but was overwritten";	FederationStateStoreUtils.logAndThrowStoreException(LOG, errMsg);	}	
application already present with subcluster 

cstmt.executeUpdate();	long stopTime = clock.getTime();	if (cstmt.getInt(3) == 0) {	String errMsg = "Application " + appId + " does not exist";	FederationStateStoreUtils.logAndThrowStoreException(LOG, errMsg);	}	if (cstmt.getInt(3) != 1) {	String errMsg = "Wrong behavior during the update of SubCluster " + subClusterId;	FederationStateStoreUtils.logAndThrowStoreException(LOG, errMsg);	}	
update the subcluster to for application in the statestore 

long startTime = clock.getTime();	cstmt.execute();	long stopTime = clock.getTime();	if (cstmt.getString(2) != null) {	homeRM = SubClusterId.newInstance(cstmt.getString(2));	} else {	String errMsg = "Application " + request.getApplicationId() + " does not exist";	FederationStateStoreUtils.logAndThrowStoreException(LOG, errMsg);	}	if (LOG.isDebugEnabled()) {	
got the information about the specified application the am is running in 

cstmt.executeUpdate();	long stopTime = clock.getTime();	if (cstmt.getInt(2) == 0) {	String errMsg = "Application " + request.getApplicationId() + " does not exist";	FederationStateStoreUtils.logAndThrowStoreException(LOG, errMsg);	}	if (cstmt.getInt(2) != 1) {	String errMsg = "Wrong behavior during deleting the application " + request.getApplicationId();	FederationStateStoreUtils.logAndThrowStoreException(LOG, errMsg);	}	
delete from the statestore the application 

cstmt = conn.prepareCall(CALL_SP_GET_POLICY_CONFIGURATION);	cstmt.setString(1, request.getQueue());	cstmt.registerOutParameter(2, java.sql.Types.VARCHAR);	cstmt.registerOutParameter(3, java.sql.Types.VARBINARY);	long startTime = clock.getTime();	cstmt.executeUpdate();	long stopTime = clock.getTime();	if (cstmt.getString(2) != null && cstmt.getBytes(3) != null) {	subClusterPolicyConfiguration = SubClusterPolicyConfiguration.newInstance(request.getQueue(), cstmt.getString(2), ByteBuffer.wrap(cstmt.getBytes(3)));	if (LOG.isDebugEnabled()) {	
selected from statestore the policy for the queue 

cstmt.registerOutParameter(2, java.sql.Types.VARCHAR);	cstmt.registerOutParameter(3, java.sql.Types.VARBINARY);	long startTime = clock.getTime();	cstmt.executeUpdate();	long stopTime = clock.getTime();	if (cstmt.getString(2) != null && cstmt.getBytes(3) != null) {	subClusterPolicyConfiguration = SubClusterPolicyConfiguration.newInstance(request.getQueue(), cstmt.getString(2), ByteBuffer.wrap(cstmt.getBytes(3)));	if (LOG.isDebugEnabled()) {	}	} else {	
policy for queue does not exist 

cstmt.executeUpdate();	long stopTime = clock.getTime();	if (cstmt.getInt(4) == 0) {	String errMsg = "The policy " + policyConf.getQueue() + " was not insert into the StateStore";	FederationStateStoreUtils.logAndThrowStoreException(LOG, errMsg);	}	if (cstmt.getInt(4) != 1) {	String errMsg = "Wrong behavior during insert the policy " + policyConf.getQueue();	FederationStateStoreUtils.logAndThrowStoreException(LOG, errMsg);	}	
insert into the state store the policy for the queue 

========================= hadoop sample_1383 =========================

Path missing = path("testRenameNonexistentFileSrc");	Path target = path("testRenameNonexistentFileDest");	boolean renameReturnsFalseOnFailure = isSupported(ContractOptions.RENAME_RETURNS_FALSE_IF_SOURCE_MISSING);	mkdirs(missing.getParent());	try {	boolean renamed = rename(missing, target);	if (!renameReturnsFalseOnFailure) {	String destDirLS = generateAndLogErrorListing(missing, target);	fail("expected rename(" + missing + ", " + target + " ) to fail," + " got a result of " + renamed + " and a destination directory of " + destDirLS);	} else {	
rename returned renaming a nonexistent file 

boolean renameReturnsFalseOnRenameDestExists = !isSupported(RENAME_RETURNS_FALSE_IF_DEST_EXISTS);	boolean destUnchanged = true;	try {	boolean renamed = rename(srcFile, destFile);	if (renameOverwritesDest) {	assertTrue("Rename returned false", renamed);	destUnchanged = false;	} else {	if (renamed && !renameReturnsFalseOnRenameDestExists) {	String destDirLS = generateAndLogErrorListing(srcFile, destFile);	
dest dir 

========================= hadoop sample_3214 =========================

static InetSocketAddress parseEntry(String type, String fn, String line) {	try {	URI uri = new URI("dummy", line, null, null, null);	int port = uri.getPort() == -1 ? 0 : uri.getPort();	InetSocketAddress addr = new InetSocketAddress(uri.getHost(), port);	if (addr.isUnresolved()) {	
failed to resolve address s in s ignoring in the s list 

static InetSocketAddress parseEntry(String type, String fn, String line) {	try {	URI uri = new URI("dummy", line, null, null, null);	int port = uri.getPort() == -1 ? 0 : uri.getPort();	InetSocketAddress addr = new InetSocketAddress(uri.getHost(), port);	if (addr.isUnresolved()) {	return null;	}	return addr;	} catch (URISyntaxException e) {	
failed to parse s in s ignoring in the s list 

========================= hadoop sample_8321 =========================

public void run() {	try {	pump();	} catch (Throwable t) {	
unable to pump output from 

========================= hadoop sample_3994 =========================

public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException {	if (filterConfig == null) return;	String uri = ((HttpServletRequest)request).getRequestURI();	
filtering 

static void access(String urlstring) throws IOException {	
access 

========================= hadoop sample_3097 =========================

public static void start() throws Exception {	server = new Server(0);	Context context = new Context();	context.setContextPath("/foo");	server.setHandler(context);	context.addServlet(new ServletHolder(TestServlet.class), "/bar");	server.getConnectors()[0].setHost("localhost");	server.start();	originalPort = server.getConnectors()[0].getLocalPort();	
running embedded servlet container at http originalport 

appReportFetcher.answer = 3;	proxyConn = (HttpURLConnection) url.openConnection();	proxyConn.setRequestProperty("Cookie", "checked_application_0_0000=true");	proxyConn.connect();	assertEquals(HttpURLConnection.HTTP_OK, proxyConn.getResponseCode());	appReportFetcher.answer = 5;	URL clientUrl = new URL("http: + "/proxy/application_00_0/test/tez?x=y&h=p");	proxyConn = (HttpURLConnection) clientUrl.openConnection();	proxyConn.connect();	LOG.info("" + proxyConn.getURL());	
proxyconn getheaderfield 

proxyServer = new HttpServer2.Builder() .setName("proxy") .addEndpoint( URI.create(WebAppUtils.getHttpSchemePrefix(conf) + bindAddress + ":0")).setFindPort(true) .setConf(conf) .setACL(acl) .build();	proxyServer.addServlet(ProxyUriUtils.PROXY_SERVLET_NAME, ProxyUriUtils.PROXY_PATH_SPEC, WebAppProxyServlet.class);	appReportFetcher = new AppReportFetcherForTest(conf);	proxyServer.setAttribute(FETCHER_ATTRIBUTE, appReportFetcher );	proxyServer.setAttribute(IS_SECURITY_ENABLED_ATTRIBUTE, Boolean.TRUE);	String proxy = WebAppUtils.getProxyHostAndPort(conf);	String[] proxyParts = proxy.split(":");	String proxyHost = proxyParts[0];	proxyServer.setAttribute(PROXY_HOST_ATTRIBUTE, proxyHost);	proxyServer.start();	
proxy server is started at port 

========================= hadoop sample_1550 =========================

private static void parseBlocksPerChunk(CommandLine command, DistCpOptions option) {	boolean hasOption = command.hasOption(DistCpOptionSwitch.BLOCKS_PER_CHUNK.getSwitch());	
parsechunksize 

private static void parseBlocksPerChunk(CommandLine command, DistCpOptions option) {	boolean hasOption = command.hasOption(DistCpOptionSwitch.BLOCKS_PER_CHUNK.getSwitch());	if (hasOption) {	String chunkSizeString = getVal(command, DistCpOptionSwitch.BLOCKS_PER_CHUNK.getSwitch().trim());	try {	int csize = Integer.parseInt(chunkSizeString);	if (csize < 0) {	csize = 0;	}	
set distcp blocksperchunk to 

private static void parseSizeLimit(CommandLine command) {	if (command.hasOption(DistCpOptionSwitch.SIZE_LIMIT.getSwitch())) {	String sizeLimitString = getVal(command, DistCpOptionSwitch.SIZE_LIMIT.getSwitch().trim());	try {	Long.parseLong(sizeLimitString);	}	catch (NumberFormatException e) {	throw new IllegalArgumentException("Size-limit is invalid: " + sizeLimitString, e);	}	
is a deprecated option ignoring 

private static void parseFileLimit(CommandLine command) {	if (command.hasOption(DistCpOptionSwitch.FILE_LIMIT.getSwitch())) {	String fileLimitString = getVal(command, DistCpOptionSwitch.FILE_LIMIT.getSwitch().trim());	try {	Integer.parseInt(fileLimitString);	} catch (NumberFormatException e) {	throw new IllegalArgumentException("File-limit is invalid: " + fileLimitString, e);	}	
is a deprecated option ignoring 

========================= hadoop sample_6295 =========================

metricsPathPrefix.append(tag.value());	}	}	long timestamp = record.timestamp() / 1000L;	for (AbstractMetric metric : record.metrics()) {	lines.append( metricsPathPrefix.toString() + "." + metric.name().replace(' ', '.')).append(" ") .append(metric.value()).append(" ").append(timestamp) .append("\n");	}	try {	graphite.write(lines.toString());	} catch (Exception e) {	
error sending metrics to graphite 

public void flush() {	try {	graphite.flush();	} catch (Exception e) {	
error flushing metrics to graphite 

}	if (tooManyConnectionFailures()) {	return;	}	try {	socket = new Socket(serverHost, serverPort);	writer = new OutputStreamWriter(socket.getOutputStream(), StandardCharsets.UTF_8);	} catch (Exception e) {	connectionFailures++;	if (tooManyConnectionFailures()) {	
too many connection failures would not try to connect again 

========================= hadoop sample_3428 =========================

public void setup() throws Exception {	super.setup();	fs = getRestrictedFileSystem();	auxFs = getNormalFileSystem();	testRoot = path("/ITestS3AConcurrentOps");	testRoot = S3ATestUtils.createTestPath(testRoot);	for (int i = 0; i < concurrentRenames; i++){	source[i] = new Path(testRoot, "source" + i);	target[i] = new Path(testRoot, "target" + i);	}	
generating data 

testRoot = S3ATestUtils.createTestPath(testRoot);	for (int i = 0; i < concurrentRenames; i++){	source[i] = new Path(testRoot, "source" + i);	target[i] = new Path(testRoot, "target" + i);	}	auxFs.mkdirs(testRoot);	byte[] zeroes = ContractTestUtils.dataset(1024*1024, 0, Integer.MAX_VALUE);	for (Path aSource : source) {	try(FSDataOutputStream out = auxFs.create(aSource)) {	for (int mb = 0; mb < 20; mb++) {	
block 

}	auxFs.mkdirs(testRoot);	byte[] zeroes = ContractTestUtils.dataset(1024*1024, 0, Integer.MAX_VALUE);	for (Path aSource : source) {	try(FSDataOutputStream out = auxFs.create(aSource)) {	for (int mb = 0; mb < 20; mb++) {	out.write(zeroes);	}	}	}	
data generated 

});	((ThreadPoolExecutor)executor).prestartAllCoreThreads();	Future<Boolean>[] futures = new Future[concurrentRenames];	for (int i = 0; i < concurrentRenames; i++) {	final int index = i;	futures[i] = executor.submit(new Callable<Boolean>() {	public Boolean call() throws Exception {	NanoTimer timer = new NanoTimer();	boolean result = fs.rename(source[index], target[index]);	timer.end("parallel rename %d", index);	
rename ran from to 

final int index = i;	futures[i] = executor.submit(new Callable<Boolean>() {	public Boolean call() throws Exception {	NanoTimer timer = new NanoTimer();	boolean result = fs.rename(source[index], target[index]);	timer.end("parallel rename %d", index);	return result;	}	});	}	
waiting for tasks to complete 

final int index = i;	futures[i] = executor.submit(new Callable<Boolean>() {	public Boolean call() throws Exception {	NanoTimer timer = new NanoTimer();	boolean result = fs.rename(source[index], target[index]);	timer.end("parallel rename %d", index);	return result;	}	});	}	
deadlock may have occurred if nothing else is logged or the test times out 

timer.end("parallel rename %d", index);	return result;	}	});	}	for (int i = 0; i < concurrentRenames; i++) {	assertTrue("No future " + i, futures[i].get());	assertPathExists("target path", target[i]);	assertPathDoesNotExist("source path", source[i]);	}	
all tasks have completed successfully 

========================= hadoop sample_5905 =========================

public void setConf(Configuration conf) {	balancedSpaceThreshold = conf.getLong( DFS_DATANODE_AVAILABLE_SPACE_VOLUME_CHOOSING_POLICY_BALANCED_SPACE_THRESHOLD_KEY, DFS_DATANODE_AVAILABLE_SPACE_VOLUME_CHOOSING_POLICY_BALANCED_SPACE_THRESHOLD_DEFAULT);	balancedPreferencePercent = conf.getFloat( DFS_DATANODE_AVAILABLE_SPACE_VOLUME_CHOOSING_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY, DFS_DATANODE_AVAILABLE_SPACE_VOLUME_CHOOSING_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_DEFAULT);	LOG.info("Available space volume choosing policy initialized: " + DFS_DATANODE_AVAILABLE_SPACE_VOLUME_CHOOSING_POLICY_BALANCED_SPACE_THRESHOLD_KEY + " = " + balancedSpaceThreshold + ", " + DFS_DATANODE_AVAILABLE_SPACE_VOLUME_CHOOSING_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY + " = " + balancedPreferencePercent);	if (balancedPreferencePercent > 1.0) {	
the value of is greater than but should be in the range 

public void setConf(Configuration conf) {	balancedSpaceThreshold = conf.getLong( DFS_DATANODE_AVAILABLE_SPACE_VOLUME_CHOOSING_POLICY_BALANCED_SPACE_THRESHOLD_KEY, DFS_DATANODE_AVAILABLE_SPACE_VOLUME_CHOOSING_POLICY_BALANCED_SPACE_THRESHOLD_DEFAULT);	balancedPreferencePercent = conf.getFloat( DFS_DATANODE_AVAILABLE_SPACE_VOLUME_CHOOSING_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY, DFS_DATANODE_AVAILABLE_SPACE_VOLUME_CHOOSING_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_DEFAULT);	LOG.info("Available space volume choosing policy initialized: " + DFS_DATANODE_AVAILABLE_SPACE_VOLUME_CHOOSING_POLICY_BALANCED_SPACE_THRESHOLD_KEY + " = " + balancedSpaceThreshold + ", " + DFS_DATANODE_AVAILABLE_SPACE_VOLUME_CHOOSING_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY + " = " + balancedPreferencePercent);	if (balancedPreferencePercent > 1.0) {	}	if (balancedPreferencePercent < 0.5) {	
the value of is less than so volumes with less available disk space will receive more block allocations 

private V doChooseVolume(final List<V> volumes, long replicaSize) throws IOException {	AvailableSpaceVolumeList volumesWithSpaces = new AvailableSpaceVolumeList(volumes);	if (volumesWithSpaces.areAllVolumesWithinFreeSpaceThreshold()) {	V volume = roundRobinPolicyBalanced.chooseVolume(volumes, replicaSize);	if (LOG.isDebugEnabled()) {	
all volumes are within the configured free space balance threshold selecting for write of block size 

} else {	V volume = null;	long mostAvailableAmongLowVolumes = volumesWithSpaces .getMostAvailableSpaceAmongVolumesWithLowAvailableSpace();	List<V> highAvailableVolumes = extractVolumesFromPairs( volumesWithSpaces.getVolumesWithHighAvailableSpace());	List<V> lowAvailableVolumes = extractVolumesFromPairs( volumesWithSpaces.getVolumesWithLowAvailableSpace());	float preferencePercentScaler = (highAvailableVolumes.size() * balancedPreferencePercent) + (lowAvailableVolumes.size() * (1 - balancedPreferencePercent));	float scaledPreferencePercent = (highAvailableVolumes.size() * balancedPreferencePercent) / preferencePercentScaler;	if (mostAvailableAmongLowVolumes < replicaSize || random.nextFloat() < scaledPreferencePercent) {	volume = roundRobinPolicyHighAvailable.chooseVolume( highAvailableVolumes, replicaSize);	if (LOG.isDebugEnabled()) {	
volumes are imbalanced selecting from high available space volumes for write of block size 

List<V> lowAvailableVolumes = extractVolumesFromPairs( volumesWithSpaces.getVolumesWithLowAvailableSpace());	float preferencePercentScaler = (highAvailableVolumes.size() * balancedPreferencePercent) + (lowAvailableVolumes.size() * (1 - balancedPreferencePercent));	float scaledPreferencePercent = (highAvailableVolumes.size() * balancedPreferencePercent) / preferencePercentScaler;	if (mostAvailableAmongLowVolumes < replicaSize || random.nextFloat() < scaledPreferencePercent) {	volume = roundRobinPolicyHighAvailable.chooseVolume( highAvailableVolumes, replicaSize);	if (LOG.isDebugEnabled()) {	}	} else {	volume = roundRobinPolicyLowAvailable.chooseVolume( lowAvailableVolumes, replicaSize);	if (LOG.isDebugEnabled()) {	
volumes are imbalanced selecting from low available space volumes for write of block size 

========================= hadoop sample_7938 =========================

boolean requestContinues = true;	LOG.trace("Processing operation for req=({}), token: {}", request, token);	String op = ServletUtils.getParameter(request, KerberosDelegationTokenAuthenticator.OP_PARAM);	op = (op != null) ? StringUtils.toUpperCase(op) : null;	if (isManagementOperation(request)) {	KerberosDelegationTokenAuthenticator.DelegationTokenOperation dtOp = KerberosDelegationTokenAuthenticator. DelegationTokenOperation.valueOf(op);	if (dtOp.getHttpMethod().equals(request.getMethod())) {	boolean doManagement;	if (dtOp.requiresKerberosCredentials() && token == null) {	token = authHandler.authenticate(request, response);	
got token 

public AuthenticationToken authenticate(HttpServletRequest request, HttpServletResponse response) throws IOException, AuthenticationException {	AuthenticationToken token;	String delegationParam = getDelegationToken(request);	if (delegationParam != null) {	
authenticating with dt param 

========================= hadoop sample_3717 =========================

protected void serviceStop() throws Exception {	scheduler.shutdown();	if (!scheduler.awaitTermination(collectorLingerPeriod, TimeUnit.MILLISECONDS)) {	
scheduler terminated before removing the application collectors 

========================= hadoop sample_358 =========================

public void testCheckOneVolume() throws Exception {	
executing 

public void testCheckAllVolumes() throws Exception {	
executing 

public void testCheckAllVolumes() throws Exception {	final List<FsVolumeSpi> volumes = makeVolumes( NUM_VOLUMES, expectedVolumeHealth);	final FsDatasetSpi<FsVolumeSpi> dataset = makeDataset(volumes);	final DatasetVolumeChecker checker = new DatasetVolumeChecker(new HdfsConfiguration(), new FakeTimer());	checker.setDelegateChecker(new DummyChecker());	Set<FsVolumeSpi> failedVolumes = checker.checkAllVolumes(dataset);	
got back failed volumes 

public Optional<ListenableFuture<VolumeCheckResult>> schedule( Checkable<VolumeCheckContext, VolumeCheckResult> target, VolumeCheckContext context) {	try {	return Optional.of( Futures.immediateFuture(target.check(context)));	} catch (Exception e) {	
check routine threw exception 

========================= hadoop sample_7241 =========================

public boolean checkAccess(UserGroupInformation callerUGI, QueueACL acl, RMApp app, String remoteAddress, List<String> forwardedAddresses) {	if (!isACLsEnable) {	return true;	}	if (scheduler instanceof CapacityScheduler) {	CSQueue queue = ((CapacityScheduler) scheduler).getQueue(app.getQueue());	if (queue == null) {	
queue does not exist for 

========================= hadoop sample_708 =========================

final CountDownLatch allDone = new CountDownLatch(numThreads);	final AtomicReference<Throwable> childError = new AtomicReference<>();	for (int i = 0; i < numThreads; i++) {	threadPool.submit(new Runnable() {	public void run() {	allReady.countDown();	try {	startBlocker.await();	incrementOpsCountByRandomNumbers();	} catch (Throwable t) {	
child failed when calling mkdir 

========================= hadoop sample_6779 =========================

public NodeHeartbeatResponse nodeHeartbeat(ApplicationAttemptId attemptId, long containerId, ContainerState containerState) throws Exception {	ContainerStatus containerStatus = BuilderUtils.newContainerStatus( BuilderUtils.newContainerId(attemptId, containerId), containerState, "Success", 0, BuilderUtils.newResource(memory, vCores));	ArrayList<ContainerStatus> containerStatusList = new ArrayList<ContainerStatus>(1);	containerStatusList.add(containerStatus);	
containerstatus 

========================= hadoop sample_421 =========================

public void reinit(Configuration conf) {	reset();	if (conf == null) {	return;	}	setLevel(ZlibFactory.getCompressionLevel(conf).compressionLevel());	final ZlibCompressor.CompressionStrategy strategy = ZlibFactory.getCompressionStrategy(conf);	try {	setStrategy(strategy.compressionStrategy());	} catch (IllegalArgumentException ill) {	
not supported by builtinzlibdeflater 

return;	}	setLevel(ZlibFactory.getCompressionLevel(conf).compressionLevel());	final ZlibCompressor.CompressionStrategy strategy = ZlibFactory.getCompressionStrategy(conf);	try {	setStrategy(strategy.compressionStrategy());	} catch (IllegalArgumentException ill) {	setStrategy(DEFAULT_STRATEGY);	}	if(LOG.isDebugEnabled()) {	
reinit compressor with new compression configuration 

========================= hadoop sample_3828 =========================

public void run() {	long startTimeMs = 0;	Thread.currentThread().setName("CacheReplicationMonitor(" + System.identityHashCode(this) + ")");	
starting cachereplicationmonitor with interval milliseconds 

public void run() {	long startTimeMs = 0;	Thread.currentThread().setName("CacheReplicationMonitor(" + System.identityHashCode(this) + ")");	try {	long curTimeMs = Time.monotonicNow();	while (true) {	lock.lock();	try {	while (true) {	if (shutdown) {	
shutting down cachereplicationmonitor 

try {	long curTimeMs = Time.monotonicNow();	while (true) {	lock.lock();	try {	while (true) {	if (shutdown) {	return;	}	if (completedScanCount < neededScanCount) {	
rescanning because of pending operations 

try {	while (true) {	if (shutdown) {	return;	}	if (completedScanCount < neededScanCount) {	break;	}	long delta = (startTimeMs + intervalMs) - curTimeMs;	if (delta <= 0) {	
rescanning after milliseconds 

rescan();	curTimeMs = Time.monotonicNow();	lock.lock();	try {	completedScanCount = curScanCount;	curScanCount = -1;	scanFinished.signalAll();	} finally {	lock.unlock();	}	
scanned directive s and block s in millisecond s 

lock.lock();	try {	completedScanCount = curScanCount;	curScanCount = -1;	scanFinished.signalAll();	} finally {	lock.unlock();	}	}	} catch (InterruptedException e) {	
shutting down cachereplicationmonitor 

completedScanCount = curScanCount;	curScanCount = -1;	scanFinished.signalAll();	} finally {	lock.unlock();	}	}	} catch (InterruptedException e) {	return;	} catch (Throwable t) {	
thread exiting 

if (neededScanCount <= completedScanCount) {	return;	}	if (curScanCount < 0) {	doRescan.signal();	}	while ((!shutdown) && (completedScanCount < neededScanCount)) {	try {	scanFinished.await();	} catch (InterruptedException e) {	
interrupted while waiting for cachereplicationmonitor rescan 

scannedDirectives++;	if (directive.getExpiryTime() > 0 && directive.getExpiryTime() <= now) {	LOG.debug("Directive {}: the directive expired at {} (now = {})", directive.getId(), directive.getExpiryTime(), now);	continue;	}	String path = directive.getPath();	INode node;	try {	node = fsDir.getINode(path, DirOp.READ);	} catch (IOException e) {	
directive failed to resolve path 

continue;	}	String path = directive.getPath();	INode node;	try {	node = fsDir.getINode(path, DirOp.READ);	} catch (IOException e) {	continue;	}	if (node == null)  {	
directive no inode found at 

INodeDirectory dir = node.asDirectory();	ReadOnlyList<INode> children = dir .getChildrenList(Snapshot.CURRENT_STATE_ID);	for (INode child : children) {	if (child.isFile()) {	rescanFile(directive, child.asFile());	}	}	} else if (node.isFile()) {	rescanFile(directive, node.asFile());	} else {	
directive ignoring non directive non file inode 

private void rescanFile(CacheDirective directive, INodeFile file) {	BlockInfo[] blockInfos = file.getBlocks();	directive.addFilesNeeded(1);	long neededTotal = file.computeFileSizeNotIncludingLastUcBlock() * directive.getReplication();	directive.addBytesNeeded(neededTotal);	CachePool pool = directive.getPool();	if (pool.getBytesNeeded() > pool.getLimit()) {	
directive not scanning file because bytesneeded for pool is but the pool s limit is 

directive.addFilesNeeded(1);	long neededTotal = file.computeFileSizeNotIncludingLastUcBlock() * directive.getReplication();	directive.addBytesNeeded(neededTotal);	CachePool pool = directive.getPool();	if (pool.getBytesNeeded() > pool.getLimit()) {	return;	}	long cachedTotal = 0;	for (BlockInfo blockInfo : blockInfos) {	if (!blockInfo.getBlockUCState().equals(BlockUCState.COMPLETE)) {	
directive can t cache block because it is in state not complete 

cachedBlocks.put(ncblock);	ocblock = ncblock;	} else {	List<DatanodeDescriptor> cachedOn = ocblock.getDatanodes(Type.CACHED);	long cachedByBlock = Math.min(cachedOn.size(), directive.getReplication()) * blockInfo.getNumBytes();	cachedTotal += cachedByBlock;	if ((mark != ocblock.getMark()) || (ocblock.getReplication() < directive.getReplication())) {	ocblock.setReplicationAndMark(directive.getReplication(), mark);	}	}	
directive setting replication for block to 

cachedTotal += cachedByBlock;	if ((mark != ocblock.getMark()) || (ocblock.getReplication() < directive.getReplication())) {	ocblock.setReplicationAndMark(directive.getReplication(), mark);	}	}	}	directive.addBytesCached(cachedTotal);	if (cachedTotal == neededTotal) {	directive.addFilesCached(1);	}	
directive caching bytes 

private void rescanCachedBlockMap() {	Set<DatanodeDescriptor> datanodes = blockManager.getDatanodeManager().getDatanodes();	for (DatanodeDescriptor dn : datanodes) {	long remaining = dn.getCacheRemaining();	for (Iterator<CachedBlock> it = dn.getPendingCached().iterator();	it.hasNext();) {	CachedBlock cblock = it.next();	BlockInfo blockInfo = blockManager. getStoredBlock(new Block(cblock.getBlockId()));	if (blockInfo == null) {	
block cannot be found in block manager and hence skipped from calculation for node 

for (DatanodeDescriptor dn : datanodes) {	long remaining = dn.getCacheRemaining();	for (Iterator<CachedBlock> it = dn.getPendingCached().iterator();	it.hasNext();) {	CachedBlock cblock = it.next();	BlockInfo blockInfo = blockManager. getStoredBlock(new Block(cblock.getBlockId()));	if (blockInfo == null) {	continue;	}	if (blockInfo.getNumBytes() > remaining) {	
block removing from pending cached for node because it cannot fit in remaining cache size 

cbIter.hasNext(); ) {	scannedBlocks++;	CachedBlock cblock = cbIter.next();	List<DatanodeDescriptor> pendingCached = cblock.getDatanodes(Type.PENDING_CACHED);	List<DatanodeDescriptor> cached = cblock.getDatanodes(Type.CACHED);	List<DatanodeDescriptor> pendingUncached = cblock.getDatanodes(Type.PENDING_UNCACHED);	for (Iterator<DatanodeDescriptor> iter = pendingUncached.iterator();	iter.hasNext(); ) {	DatanodeDescriptor datanode = iter.next();	if (!cblock.isInList(datanode.getCached())) {	
block removing from pending uncached for node because the datanode uncached it 

DatanodeDescriptor datanode = iter.next();	if (!cblock.isInList(datanode.getCached())) {	datanode.getPendingUncached().remove(cblock);	iter.remove();	}	}	BlockInfo blockInfo = blockManager. getStoredBlock(new Block(cblock.getBlockId()));	String reason = findReasonForNotCaching(cblock, blockInfo);	int neededCached = 0;	if (reason != null) {	
block can t cache block because it is 

} else {	neededCached = cblock.getReplication();	}	int numCached = cached.size();	if (numCached >= neededCached) {	for (Iterator<DatanodeDescriptor> iter = pendingCached.iterator();	iter.hasNext(); ) {	DatanodeDescriptor datanode = iter.next();	datanode.getPendingCached().remove(cblock);	iter.remove();	
block removing from pending cached for node because we already have cached replicas and we only need 

datanode.getPendingCached().remove(cblock);	iter.remove();	}	}	if (numCached < neededCached) {	for (Iterator<DatanodeDescriptor> iter = pendingUncached.iterator();	iter.hasNext(); ) {	DatanodeDescriptor datanode = iter.next();	datanode.getPendingUncached().remove(cblock);	iter.remove();	
block removing from pending uncached for node because we only have cached replicas and we need 

private void addNewPendingUncached(int neededUncached, CachedBlock cachedBlock, List<DatanodeDescriptor> cached, List<DatanodeDescriptor> pendingUncached) {	LinkedList<DatanodeDescriptor> possibilities = new LinkedList<DatanodeDescriptor>();	for (DatanodeDescriptor datanode : cached) {	if (!pendingUncached.contains(datanode)) {	possibilities.add(datanode);	}	}	while (neededUncached > 0) {	if (possibilities.isEmpty()) {	
logic error we re trying to uncache more replicas than actually exist for 

private void addNewPendingCached(final int neededCached, CachedBlock cachedBlock, List<DatanodeDescriptor> cached, List<DatanodeDescriptor> pendingCached) {	BlockInfo blockInfo = blockManager. getStoredBlock(new Block(cachedBlock.getBlockId()));	if (blockInfo == null) {	
block can t add new cached replicas because there is no record of this block on the namenode 

private void addNewPendingCached(final int neededCached, CachedBlock cachedBlock, List<DatanodeDescriptor> cached, List<DatanodeDescriptor> pendingCached) {	BlockInfo blockInfo = blockManager. getStoredBlock(new Block(cachedBlock.getBlockId()));	if (blockInfo == null) {	return;	}	if (!blockInfo.isComplete()) {	
block can t cache this block because it is not yet complete 

it = datanode.getPendingUncached().iterator();	while (it.hasNext()) {	CachedBlock cBlock = it.next();	BlockInfo info = blockManager.getStoredBlock(new Block(cBlock.getBlockId()));	if (info != null) {	pendingBytes += info.getNumBytes();	}	}	long pendingCapacity = pendingBytes + datanode.getCacheRemaining();	if (pendingCapacity < blockInfo.getNumBytes()) {	
block datanode is not a valid possibility because the block has size but the datanode only has bytes of cache remaining pending bytes already cached 

}	long pendingCapacity = pendingBytes + datanode.getCacheRemaining();	if (pendingCapacity < blockInfo.getNumBytes()) {	outOfCapacity++;	continue;	}	possibilities.add(datanode);	}	List<DatanodeDescriptor> chosen = chooseDatanodesForCaching(possibilities, neededCached, blockManager.getDatanodeManager().getStaleInterval());	for (DatanodeDescriptor datanode : chosen) {	
block added to pending cached on datanode 

}	possibilities.add(datanode);	}	List<DatanodeDescriptor> chosen = chooseDatanodesForCaching(possibilities, neededCached, blockManager.getDatanodeManager().getStaleInterval());	for (DatanodeDescriptor datanode : chosen) {	pendingCached.add(datanode);	boolean added = datanode.getPendingCached().add(cachedBlock);	assert added;	}	if (neededCached > chosen.size()) {	
block we only have of cached replicas datanodes have insufficient cache capacity 

========================= hadoop sample_8337 =========================

assertEquals(counters.findCounter(TaskCounter.REDUCE_SKIPPED_GROUPS). getCounter(),redBadRecords.size());	assertEquals(counters.findCounter(TaskCounter.REDUCE_INPUT_GROUPS). getCounter(),redRecs);	assertEquals(counters.findCounter(TaskCounter.REDUCE_INPUT_RECORDS). getCounter(),redRecs);	assertEquals(counters.findCounter(TaskCounter.REDUCE_OUTPUT_RECORDS). getCounter(),redRecs);	Path skipDir = SkipBadRecords.getSkipOutputPath(conf);	assertNotNull(skipDir);	Path[] skips = FileUtil.stat2Paths(getFileSystem().listStatus(skipDir));	List<String> mapSkipped = new ArrayList<String>();	List<String> redSkipped = new ArrayList<String>();	for(Path skipPath : skips) {	
skippath 

Path[] skips = FileUtil.stat2Paths(getFileSystem().listStatus(skipDir));	List<String> mapSkipped = new ArrayList<String>();	List<String> redSkipped = new ArrayList<String>();	for(Path skipPath : skips) {	SequenceFile.Reader reader = new SequenceFile.Reader( getFileSystem(), skipPath, conf);	Object key = ReflectionUtils.newInstance(reader.getKeyClass(), conf);	Object value = ReflectionUtils.newInstance(reader.getValueClass(), conf);	key = reader.next(key);	while(key!=null) {	value = reader.getCurrentValue(value);	
key value 

mapSkipped.add(value.toString());	}	key = reader.next(key);	}	reader.close();	}	assertTrue(mapSkipped.containsAll(mapperBadRecords));	assertTrue(redSkipped.containsAll(redBadRecords));	Path[] outputFiles = FileUtil.stat2Paths( getFileSystem().listStatus(getOutputDir(), new Utils.OutputFileUtils.OutputFilesFilter()));	List<String> mapperOutput=getProcessed(input, mapperBadRecords);	
mapperoutput 

}	key = reader.next(key);	}	reader.close();	}	assertTrue(mapSkipped.containsAll(mapperBadRecords));	assertTrue(redSkipped.containsAll(redBadRecords));	Path[] outputFiles = FileUtil.stat2Paths( getFileSystem().listStatus(getOutputDir(), new Utils.OutputFileUtils.OutputFilesFilter()));	List<String> mapperOutput=getProcessed(input, mapperBadRecords);	List<String> reducerOutput=getProcessed(mapperOutput, redBadRecords);	
reduceroutput 

if (outputFiles.length > 0) {	InputStream is = getFileSystem().open(outputFiles[0]);	BufferedReader reader = new BufferedReader(new InputStreamReader(is));	String line = reader.readLine();	int counter = 0;	while (line != null) {	counter++;	StringTokenizer tokeniz = new StringTokenizer(line, "\t");	String key = tokeniz.nextToken();	String value = tokeniz.nextToken();	
output key value 

public void map(LongWritable key, Text val, OutputCollector<LongWritable, Text> output, Reporter reporter) throws IOException {	String str = val.toString();	
map key value 

public void map(LongWritable key, Text val, OutputCollector<LongWritable, Text> output, Reporter reporter) throws IOException {	String str = val.toString();	if(MAPPER_BAD_RECORDS.get(0).equals(str)) {	
map encountered bad record 

public void map(LongWritable key, Text val, OutputCollector<LongWritable, Text> output, Reporter reporter) throws IOException {	String str = val.toString();	if(MAPPER_BAD_RECORDS.get(0).equals(str)) {	System.exit(-1);	}	else if(MAPPER_BAD_RECORDS.get(1).equals(str)) {	
map encountered bad record 

public void map(LongWritable key, Text val, OutputCollector<LongWritable, Text> output, Reporter reporter) throws IOException {	String str = val.toString();	if(MAPPER_BAD_RECORDS.get(0).equals(str)) {	System.exit(-1);	}	else if(MAPPER_BAD_RECORDS.get(1).equals(str)) {	throw new RuntimeException("Bad record "+str);	}	else if(MAPPER_BAD_RECORDS.get(2).equals(str)) {	try {	
map encountered bad record 

public void reduce(LongWritable key, Iterator<Text> values, OutputCollector<LongWritable, Text> output, Reporter reporter) throws IOException {	while(values.hasNext()) {	Text value = values.next();	
reduce key value 

public void reduce(LongWritable key, Iterator<Text> values, OutputCollector<LongWritable, Text> output, Reporter reporter) throws IOException {	while(values.hasNext()) {	Text value = values.next();	if(REDUCER_BAD_RECORDS.get(0).equals(value.toString())) {	
reduce encountered bad record 

public void reduce(LongWritable key, Iterator<Text> values, OutputCollector<LongWritable, Text> output, Reporter reporter) throws IOException {	while(values.hasNext()) {	Text value = values.next();	if(REDUCER_BAD_RECORDS.get(0).equals(value.toString())) {	System.exit(-1);	}	else if(REDUCER_BAD_RECORDS.get(1).equals(value.toString())) {	try {	
reduce encountered bad record 

========================= hadoop sample_5505 =========================

public Set<FsVolumeSpi> checkAllVolumes( final FsDatasetSpi<? extends FsVolumeSpi> dataset) throws InterruptedException {	final long gap = timer.monotonicNow() - lastAllVolumesCheck;	if (gap < minDiskCheckGapMs) {	numSkippedChecks.incrementAndGet();	
skipped checking all volumes time since last check is less than the minimum gap between checks ms 

public Set<FsVolumeSpi> checkAllVolumes( final FsDatasetSpi<? extends FsVolumeSpi> dataset) throws InterruptedException {	final long gap = timer.monotonicNow() - lastAllVolumesCheck;	if (gap < minDiskCheckGapMs) {	numSkippedChecks.incrementAndGet();	return Collections.emptySet();	}	final FsDatasetSpi.FsVolumeReferences references = dataset.getFsVolumeReferences();	if (references.size() == 0) {	
checkallvolumesasync no volumes can be referenced 

}	lastAllVolumesCheck = timer.monotonicNow();	final Set<FsVolumeSpi> healthyVolumes = new HashSet<>();	final Set<FsVolumeSpi> failedVolumes = new HashSet<>();	final Set<FsVolumeSpi> allVolumes = new HashSet<>();	final AtomicLong numVolumes = new AtomicLong(references.size());	final CountDownLatch latch = new CountDownLatch(1);	for (int i = 0; i < references.size(); ++i) {	final FsVolumeReference reference = references.getReference(i);	Optional<ListenableFuture<VolumeCheckResult>> olf = delegateChecker.schedule(reference.getVolume(), IGNORED_CONTEXT);	
scheduled health check for volume 

}	}));	} else {	IOUtils.cleanup(null, reference);	if (numVolumes.decrementAndGet() == 0) {	latch.countDown();	}	}	}	if (!latch.await(maxAllowedTimeForCheckMs, TimeUnit.MILLISECONDS)) {	
checkallvolumes timed out after ms 

public boolean checkVolume( final FsVolumeSpi volume, Callback callback) {	if (volume == null) {	
cannot schedule check on null volume 

public void onSuccess(@Nonnull VolumeCheckResult result) {	switch(result) {	
volume is 

public void onSuccess(@Nonnull VolumeCheckResult result) {	switch(result) {	markHealthy();	break;	
volume detected as being unhealthy 

public void onSuccess(@Nonnull VolumeCheckResult result) {	switch(result) {	markHealthy();	break;	markFailed();	break;	
unexpected health check result for volume 

public void onFailure(@Nonnull Throwable t) {	Throwable exception = (t instanceof ExecutionException) ? t.getCause() : t;	
exception running disk checks against volume 

private void invokeCallback() {	try {	final long remaining = volumeCounter.decrementAndGet();	if (callback != null && remaining == 0) {	callback.call(healthyVolumes, failedVolumes);	}	} catch(Exception e) {	
unexpected exception 

public void shutdownAndWait(int gracePeriod, TimeUnit timeUnit) {	try {	delegateChecker.shutdownAndWait(gracePeriod, timeUnit);	} catch (InterruptedException e) {	
datasetvolumechecker interrupted during shutdown 

========================= hadoop sample_7884 =========================

if (isInState(STATE.STARTED)) {	return;	}	synchronized (stateChangeLock) {	if (stateModel.enterState(STATE.STARTED) != STATE.STARTED) {	try {	startTime = System.currentTimeMillis();	serviceStart();	if (isInState(STATE.STARTED)) {	if (LOG.isDebugEnabled()) {	
service is started 

throw ServiceStateException.convert(e);	} finally {	terminationNotification.set(true);	synchronized (terminationNotification) {	terminationNotification.notifyAll();	}	notifyListeners();	}	} else {	if (LOG.isDebugEnabled()) {	
ignoring re entrant call to stop 

protected final void noteFailure(Exception exception) {	if (LOG.isDebugEnabled()) {	
notefailure 

protected final void noteFailure(Exception exception) {	if (LOG.isDebugEnabled()) {	}	if (exception == null) {	return;	}	synchronized (this) {	if (failureCause == null) {	failureCause = exception;	failureState = getServiceState();	
service failed in state cause 

protected void serviceInit(Configuration conf) throws Exception {	if (conf != config) {	
config has been overridden during init 

private void notifyListeners() {	try {	listeners.notifyListeners(this);	globalListeners.notifyListeners(this);	} catch (Throwable e) {	
exception while notifying listeners of 

private STATE enterState(STATE newState) {	assert stateModel != null : "null state in " + name + " " + this.getClass();	STATE oldState = stateModel.enterState(newState);	if (oldState != newState) {	if (LOG.isDebugEnabled()) {	
service entered state 

========================= hadoop sample_4085 =========================

MetricsTestHelper.replaceRollingAveragesScheduler( peerMetrics.getSendPacketDownstreamRollingAverages(), ROLLING_AVERAGE_WINDOWS, WINDOW_INTERVAL_SECONDS, TimeUnit.SECONDS);	injectFastNodesSamples(peerMetrics);	injectSlowNodeSamples(peerMetrics, slowNodeName);	peerMetrics.dumpSendPacketDownstreamAvgInfoAsJson();	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	return peerMetrics.getOutliers().size() > 0;	}	}, 500, 100_000);	final Map<String, Double> outliers = peerMetrics.getOutliers();	
got back outlier nodes 

public void injectFastNodesSamples(DataNodePeerMetrics peerMetrics) {	for (int nodeIndex = 0;	nodeIndex < MIN_OUTLIER_DETECTION_PEERS; ++nodeIndex) {	final String nodeName = "FastNode-" + nodeIndex;	
generating stats for node 

========================= hadoop sample_7216 =========================

public synchronized void add(String serialPart, String timestampPart) {	if (!cache.containsKey(serialPart)) {	cache.put(serialPart, new HashSet<String>());	if (cache.size() > maxSize) {	String key = cache.firstKey();	
dropping from the serialnumberindex we will no longer be able to see jobs that are in that serial index for 

public HistoryFileInfo addIfAbsent(HistoryFileInfo fileInfo) {	JobId jobId = fileInfo.getJobId();	if (LOG.isDebugEnabled()) {	
adding to job list cache with 

while (cache.size() > maxSize && keys.hasNext()) {	JobId key = keys.next();	HistoryFileInfo firstValue = cache.get(key);	if (firstValue != null) {	if (firstValue.isMovePending()) {	if (firstValue.didMoveFail() && firstValue.jobIndexInfo.getFinishTime() <= cutoff) {	cache.remove(key);	try {	firstValue.delete();	} catch (IOException e) {	
error while trying to delete history files that could not be moved to done 

}	inIntermediateCount += 1;	}	}	} else {	cache.remove(key);	}	}	}	if (inIntermediateCount > 0) {	
waiting to remove in intermediate state histories e g from joblistcache because it is not in done yet total count is 

}	}	} else {	cache.remove(key);	}	}	}	if (inIntermediateCount > 0) {	}	if (moveFailedCount > 0) {	
waiting to remove move failed state histories e g from joblistcache because it is not in done yet total count is 

public void delete(HistoryFileInfo fileInfo) {	if (LOG.isDebugEnabled()) {	
removing from cache 

public synchronized void scanIfNeeded(FileStatus fs) {	long newModTime = fs.getModificationTime();	if (modTime != newModTime || (scanTime/1000) == (modTime/1000) || (scanTime/1000 + 1) == (modTime/1000)) {	scanTime = System.currentTimeMillis();	Path p = fs.getPath();	try {	scanIntermediateDirectory(p);	modTime = newModTime;	} catch (IOException e) {	
error while trying to scan the directory 

if (modTime != newModTime || (scanTime/1000) == (modTime/1000) || (scanTime/1000 + 1) == (modTime/1000)) {	scanTime = System.currentTimeMillis();	Path p = fs.getPath();	try {	scanIntermediateDirectory(p);	modTime = newModTime;	} catch (IOException e) {	}	} else {	if (LOG.isDebugEnabled()) {	
scan not needed of 

protected synchronized void delete() throws IOException {	if (LOG.isDebugEnabled()) {	
deleting and 

private void mkdir(FileContext fc, Path path, FsPermission fsp) throws IOException {	if (!fc.util().exists(path)) {	try {	fc.mkdir(path, fsp, true);	FileStatus fsStatus = fc.getFileStatus(path);	
perms after creating expected 

private void mkdir(FileContext fc, Path path, FsPermission fsp) throws IOException {	if (!fc.util().exists(path)) {	try {	fc.mkdir(path, fsp, true);	FileStatus fsStatus = fc.getFileStatus(path);	if (fsStatus.getPermission().toShort() != fsp.toShort()) {	
explicitly setting permissions to 

private void mkdir(FileContext fc, Path path, FsPermission fsp) throws IOException {	if (!fc.util().exists(path)) {	try {	fc.mkdir(path, fsp, true);	FileStatus fsStatus = fc.getFileStatus(path);	if (fsStatus.getPermission().toShort() != fsp.toShort()) {	fc.setPermission(path, fsp);	}	} catch (FileAlreadyExistsException e) {	
directory already exists 

private void removeDirectoryFromSerialNumberIndex(Path serialDirPath) {	String serialPart = serialDirPath.getName();	String timeStampPart = JobHistoryUtils .getTimestampPartFromPath(serialDirPath.toString());	if (timeStampPart == null) {	
could not find timestamp portion from path continuing with next 

private void removeDirectoryFromSerialNumberIndex(Path serialDirPath) {	String serialPart = serialDirPath.getName();	String timeStampPart = JobHistoryUtils .getTimestampPartFromPath(serialDirPath.toString());	if (timeStampPart == null) {	return;	}	if (serialPart == null) {	
could not find serial portion from path continuing with next 

private void addDirectoryToSerialNumberIndex(Path serialDirPath) {	if (LOG.isDebugEnabled()) {	
adding to serial index 

private void addDirectoryToSerialNumberIndex(Path serialDirPath) {	if (LOG.isDebugEnabled()) {	}	String serialPart = serialDirPath.getName();	String timestampPart = JobHistoryUtils .getTimestampPartFromPath(serialDirPath.toString());	if (timestampPart == null) {	
could not find timestamp portion from path continuing with next 

private void addDirectoryToSerialNumberIndex(Path serialDirPath) {	if (LOG.isDebugEnabled()) {	}	String serialPart = serialDirPath.getName();	String timestampPart = JobHistoryUtils .getTimestampPartFromPath(serialDirPath.toString());	if (timestampPart == null) {	return;	}	if (serialPart == null) {	
could not find serial portion from path continuing with next 

private void addDirectoryToJobListCache(Path path) throws IOException {	if (LOG.isDebugEnabled()) {	
adding to job list cache 

private void addDirectoryToJobListCache(Path path) throws IOException {	if (LOG.isDebugEnabled()) {	}	List<FileStatus> historyFileList = scanDirectoryForHistoryFiles(path, doneDirFc);	for (FileStatus fs : historyFileList) {	if (LOG.isDebugEnabled()) {	
adding in history for 

try {	RemoteIterator<FileStatus> fileStatusIter = fc.listStatus(path);	while (fileStatusIter.hasNext()) {	FileStatus fileStatus = fileStatusIter.next();	Path filePath = fileStatus.getPath();	if (fileStatus.isFile() && pathFilter.accept(filePath)) {	jhStatusList.add(fileStatus);	}	}	} catch (FileNotFoundException fe) {	
error while scanning directory 

private void scanIntermediateDirectory(final Path absPath) throws IOException {	if (LOG.isDebugEnabled()) {	
scanning intermediate dir 

private void scanIntermediateDirectory(final Path absPath) throws IOException {	if (LOG.isDebugEnabled()) {	}	List<FileStatus> fileStatusList = scanDirectoryForHistoryFiles(absPath, intermediateDoneDirFc);	if (LOG.isDebugEnabled()) {	
found files 

private void scanIntermediateDirectory(final Path absPath) throws IOException {	if (LOG.isDebugEnabled()) {	}	List<FileStatus> fileStatusList = scanDirectoryForHistoryFiles(absPath, intermediateDoneDirFc);	if (LOG.isDebugEnabled()) {	}	for (FileStatus fs : fileStatusList) {	if (LOG.isDebugEnabled()) {	
scanning file 

String summaryFileName = JobHistoryUtils .getIntermediateSummaryFileName(jobIndexInfo.getJobId());	HistoryFileInfo fileInfo = createHistoryFileInfo(fs.getPath(), new Path(fs .getPath().getParent(), confFileName), new Path(fs.getPath() .getParent(), summaryFileName), jobIndexInfo, false);	final HistoryFileInfo old = jobListCache.addIfAbsent(fileInfo);	if (old == null || old.didMoveFail()) {	final HistoryFileInfo found = (old == null) ? fileInfo : old;	long cutoff = System.currentTimeMillis() - maxHistoryAge;	if(found.getJobIndexInfo().getFinishTime() <= cutoff) {	try {	found.delete();	} catch (IOException e) {	
error cleaning up a historyfile that is out of date 

if (old == null || old.didMoveFail()) {	final HistoryFileInfo found = (old == null) ? fileInfo : old;	long cutoff = System.currentTimeMillis() - maxHistoryAge;	if(found.getJobIndexInfo().getFinishTime() <= cutoff) {	try {	found.delete();	} catch (IOException e) {	}	} else {	if (LOG.isDebugEnabled()) {	
scheduling move to done of 

} catch (IOException e) {	}	} else {	if (LOG.isDebugEnabled()) {	}	moveToDoneExecutor.execute(new Runnable() {	public void run() {	try {	found.moveToDone();	} catch (IOException e) {	
failed to process fileinfo for job 

public void run() {	try {	found.moveToDone();	} catch (IOException e) {	}	}	});	}	} else if (!old.isMovePending()) {	if (LOG.isDebugEnabled()) {	
duplicate deleting 

private void moveToDoneNow(final Path src, final Path target) throws IOException {	
moving to 

private void makeDoneSubdir(Path path) throws IOException {	try {	doneDirFc.getFileStatus(path);	existingDoneSubdirs.add(path);	} catch (FileNotFoundException fnfE) {	try {	FsPermission fsp = new FsPermission( JobHistoryUtils.HISTORY_DONE_DIR_PERMISSION);	doneDirFc.mkdir(path, fsp, true);	FileStatus fsStatus = doneDirFc.getFileStatus(path);	
perms after creating expected 

private void makeDoneSubdir(Path path) throws IOException {	try {	doneDirFc.getFileStatus(path);	existingDoneSubdirs.add(path);	} catch (FileNotFoundException fnfE) {	try {	FsPermission fsp = new FsPermission( JobHistoryUtils.HISTORY_DONE_DIR_PERMISSION);	doneDirFc.mkdir(path, fsp, true);	FileStatus fsStatus = doneDirFc.getFileStatus(path);	if (fsStatus.getPermission().toShort() != fsp.toShort()) {	
explicitly setting permissions to 

========================= hadoop sample_5381 =========================

public ApplicationClassLoader(URL[] urls, ClassLoader parent, List<String> systemClasses) {	super(urls, parent);	this.parent = parent;	if (parent == null) {	throw new IllegalArgumentException("No parent classloader!");	}	this.systemClasses = (systemClasses == null || systemClasses.isEmpty()) ? Arrays.asList(StringUtils.getTrimmedStrings(SYSTEM_CLASSES_DEFAULT)) : systemClasses;	
classpath 

public ApplicationClassLoader(URL[] urls, ClassLoader parent, List<String> systemClasses) {	super(urls, parent);	this.parent = parent;	if (parent == null) {	throw new IllegalArgumentException("No parent classloader!");	}	this.systemClasses = (systemClasses == null || systemClasses.isEmpty()) ? Arrays.asList(StringUtils.getTrimmedStrings(SYSTEM_CLASSES_DEFAULT)) : systemClasses;	
system classes 

public URL getResource(String name) {	URL url = null;	if (!isSystemClass(name, systemClasses)) {	url= findResource(name);	if (url == null && name.startsWith("/")) {	if (LOG.isDebugEnabled()) {	
remove leading off 

protected synchronized Class<?> loadClass(String name, boolean resolve) throws ClassNotFoundException {	if (LOG.isDebugEnabled()) {	
loading class 

protected synchronized Class<?> loadClass(String name, boolean resolve) throws ClassNotFoundException {	if (LOG.isDebugEnabled()) {	}	Class<?> c = findLoadedClass(name);	ClassNotFoundException ex = null;	if (c == null && !isSystemClass(name, systemClasses)) {	try {	c = findClass(name);	if (LOG.isDebugEnabled() && c != null) {	
loaded class 

} catch (ClassNotFoundException e) {	if (LOG.isDebugEnabled()) {	LOG.debug(e.toString());	}	ex = e;	}	}	if (c == null) {	c = parent.loadClass(name);	if (LOG.isDebugEnabled() && c != null) {	
loaded class from parent 

========================= hadoop sample_3605 =========================

public void testKillJob() throws Exception {	JobConf conf = new JobConf();	AppContext context = mock(AppContext.class);	final CountDownLatch isDone = new CountDownLatch(1);	EventHandler handler = new EventHandler() {	public void handle(Event event) {	
handling event with type 

when(ytask.getType()).thenReturn(TaskType.MAP);	when(job.getTask(taskId)).thenReturn(ytask);	MapTask mapTask = mock(MapTask.class);	when(mapTask.isMapOrReduce()).thenReturn(true);	when(mapTask.isMapTask()).thenReturn(true);	TaskAttemptID taskID = TypeConverter.fromYarn(taId);	when(mapTask.getTaskID()).thenReturn(taskID);	when(mapTask.getJobID()).thenReturn(taskID.getJobID());	doAnswer(new Answer<Void>() {	public Void answer(InvocationOnMock invocation) throws Throwable {	
sleeping for minutes 

========================= hadoop sample_5102 =========================

public static YarnConfigurationStore getStore(Configuration conf) {	Class<? extends YarnConfigurationStore> storeClass = conf.getClass(YarnConfiguration.SCHEDULER_CONFIGURATION_STORE_CLASS, InMemoryConfigurationStore.class, YarnConfigurationStore.class);	
using yarnconfigurationstore implementation 

========================= hadoop sample_925 =========================

public static void setProperty(HikariDataSource dataSource, String property, String value) {	
setting property with value 

public static void setUsername(HikariDataSource dataSource, String userNameDB) {	if (userNameDB != null) {	dataSource.setUsername(userNameDB);	
setting non null username for store connection 

public static void setUsername(HikariDataSource dataSource, String userNameDB) {	if (userNameDB != null) {	dataSource.setUsername(userNameDB);	} else {	
null username specified for store connection so ignoring 

public static void setPassword(HikariDataSource dataSource, String password) {	if (password != null) {	dataSource.setPassword(password);	
setting non null credentials for store connection 

public static void setPassword(HikariDataSource dataSource, String password) {	if (password != null) {	dataSource.setPassword(password);	} else {	
null credentials specified for store connection so ignoring 

========================= hadoop sample_1454 =========================

public String toString() {	try {	return WRITER.writeValueAsString(innerMetrics);	} catch (IOException e) {	
failed to dump to json 

========================= hadoop sample_3469 =========================

protected void serviceStop() throws Exception {	keepRunning = false;	heartbeatThread.interrupt();	try {	heartbeatThread.join();	} catch (InterruptedException ex) {	
error joining with heartbeat thread 

while (true) {	Object response = null;	synchronized (unregisterHeartbeatLock) {	if (!keepRunning) {	return;	}	try {	response = client.allocate(progress);	} catch (ApplicationAttemptNotFoundException e) {	handler.onShutdownRequest();	
shutdown requested stopping callback 

synchronized (unregisterHeartbeatLock) {	if (!keepRunning) {	return;	}	try {	response = client.allocate(progress);	} catch (ApplicationAttemptNotFoundException e) {	handler.onShutdownRequest();	return;	} catch (Throwable ex) {	
exception on heartbeat 

return;	} catch (Throwable ex) {	response = ex;	}	if (response != null) {	while (true) {	try {	responseQueue.put(response);	break;	} catch (InterruptedException ex) {	
interrupted while waiting to put on response queue 

responseQueue.put(response);	break;	} catch (InterruptedException ex) {	}	}	}	}	try {	Thread.sleep(heartbeatIntervalMs.get());	} catch (InterruptedException ex) {	
heartbeater interrupted 

public void run() {	while (true) {	if (!keepRunning) {	return;	}	try {	Object object;	try {	object = responseQueue.take();	} catch (InterruptedException ex) {	
interrupted while waiting for queue 

========================= hadoop sample_2628 =========================

private int countRecords(int numSplits) throws IOException {	InputFormat<Text, BytesWritable> format = new SequenceFileInputFilter<Text, BytesWritable>();	Text key = new Text();	BytesWritable value = new BytesWritable();	if (numSplits==0) {	numSplits = random.nextInt(MAX_LENGTH/(SequenceFile.SYNC_INTERVAL/20))+1;	}	InputSplit[] splits = format.getSplits(job, numSplits);	int count = 0;	
generated splits 

BytesWritable value = new BytesWritable();	if (numSplits==0) {	numSplits = random.nextInt(MAX_LENGTH/(SequenceFile.SYNC_INTERVAL/20))+1;	}	InputSplit[] splits = format.getSplits(job, numSplits);	int count = 0;	for (int j = 0; j < splits.length; j++) {	RecordReader<Text, BytesWritable> reader = format.getRecordReader(splits[j], job, reporter);	try {	while (reader.next(key, value)) {	
accept record 

public void testRegexFilter() throws Exception {	
testing regex filter with patter 

public void testRegexFilter() throws Exception {	SequenceFileInputFilter.setFilterClass(job, SequenceFileInputFilter.RegexFilter.class);	SequenceFileInputFilter.RegexFilter.setPattern(job, "\\A10*");	fs.delete(inDir, true);	for (int length = 1; length < MAX_LENGTH;	length+= random.nextInt(MAX_LENGTH/10)+1) {	
number of records 

public void testPercentFilter() throws Exception {	
testing percent filter with frequency 

public void testPercentFilter() throws Exception {	SequenceFileInputFilter.setFilterClass(job, SequenceFileInputFilter.PercentFilter.class);	SequenceFileInputFilter.PercentFilter.setFrequency(job, 1000);	fs.delete(inDir, true);	for (int length = 0; length < MAX_LENGTH;	length+= random.nextInt(MAX_LENGTH/10)+1) {	
number of records 

public void testPercentFilter() throws Exception {	SequenceFileInputFilter.setFilterClass(job, SequenceFileInputFilter.PercentFilter.class);	SequenceFileInputFilter.PercentFilter.setFrequency(job, 1000);	fs.delete(inDir, true);	for (int length = 0; length < MAX_LENGTH;	length+= random.nextInt(MAX_LENGTH/10)+1) {	createSequenceFile(length);	int count = countRecords(1);	
accepted records 

public void testMD5Filter() throws Exception {	
testing filter with frequency 

public void testMD5Filter() throws Exception {	SequenceFileInputFilter.setFilterClass(job, SequenceFileInputFilter.MD5Filter.class);	SequenceFileInputFilter.MD5Filter.setFrequency(job, 1000);	fs.delete(inDir, true);	for (int length = 0; length < MAX_LENGTH;	length+= random.nextInt(MAX_LENGTH/10)+1) {	
number of records 

public void testMD5Filter() throws Exception {	SequenceFileInputFilter.setFilterClass(job, SequenceFileInputFilter.MD5Filter.class);	SequenceFileInputFilter.MD5Filter.setFrequency(job, 1000);	fs.delete(inDir, true);	for (int length = 0; length < MAX_LENGTH;	length+= random.nextInt(MAX_LENGTH/10)+1) {	createSequenceFile(length);	
accepted records 

========================= hadoop sample_5417 =========================

private Set<ReportForJson> getAndDeserializeJson() throws IOException {	final String json = tracker.getJson();	
got json 

========================= hadoop sample_7580 =========================

public void generate(Class<?> specClass, Class<?> implClass, String outputName, String outputPkg) throws IOException {	
generating using and 

hamlet = basename(outputName);	String pkg = pkgName(outputPkg, implClass.getPackage().getName());	puts(0, " "package ", pkg, ";\n", "import java.io.PrintWriter;\n", "import java.util.EnumSet;\n", "import static java.util.EnumSet.*;\n", "import static ", implClass.getName(), ".EOpt.*;\n", "import org.apache.hadoop.yarn.webapp.SubView;");	String implClassName = implClass.getSimpleName();	if (!implClass.getPackage().getName().equals(pkg)) {	puts(0, "import ", implClass.getName(), ';');	}	puts(0, "\n", "public class ", hamlet, " extends ", implClassName, " implements ", specClass.getSimpleName(), "._Html {\n", "  public ", hamlet, "(PrintWriter out, int nestLevel,", " boolean wasInline) {\n", "    super(out, nestLevel, wasInline);\n", "  }\n\n", "  static EnumSet<EOpt> opt(boolean endTag, boolean inline, ", "boolean pre) {\n", "    EnumSet<EOpt> opts = of(ENDTAG);\n", "    if (!endTag) opts.remove(ENDTAG);\n", "    if (inline) opts.add(INLINE);\n", "    if (pre) opts.add(PRE);\n", "    return opts;\n", "  }");	initLut(specClass);	genImpl(specClass, implClassName, 1);	
generating methods 

String implClassName = implClass.getSimpleName();	if (!implClass.getPackage().getName().equals(pkg)) {	puts(0, "import ", implClass.getName(), ';');	}	puts(0, "\n", "public class ", hamlet, " extends ", implClassName, " implements ", specClass.getSimpleName(), "._Html {\n", "  public ", hamlet, "(PrintWriter out, int nestLevel,", " boolean wasInline) {\n", "    super(out, nestLevel, wasInline);\n", "  }\n\n", "  static EnumSet<EOpt> opt(boolean endTag, boolean inline, ", "boolean pre) {\n", "    EnumSet<EOpt> opts = of(ENDTAG);\n", "    if (!endTag) opts.remove(ENDTAG);\n", "    if (inline) opts.add(INLINE);\n", "    if (pre) opts.add(PRE);\n", "    return opts;\n", "  }");	initLut(specClass);	genImpl(specClass, implClassName, 1);	genMethods(hamlet, top, 1);	puts(0, "}");	out.close();	
wrote bytes to java 

========================= hadoop sample_2278 =========================

if (connectUgi == null) {	connectUgi = ugi;	}	return connectUgi.doAs(new PrivilegedExceptionAction<Token<?>>() {	public Token<?> run() throws IOException {	Credentials c;	try {	c = DelegationUtilsClient.getDTfromRemote(connectionFactory, nnUri, renewer, proxyUser);	} catch (IOException e) {	if (e.getCause() instanceof ConnectException) {	
couldn t connect to assuming security is disabled 

return connectUgi.doAs(new PrivilegedExceptionAction<Token<?>>() {	public Token<?> run() throws IOException {	Credentials c;	try {	c = DelegationUtilsClient.getDTfromRemote(connectionFactory, nnUri, renewer, proxyUser);	} catch (IOException e) {	if (e.getCause() instanceof ConnectException) {	return null;	}	if (LOG.isDebugEnabled()) {	
exception getting delegation token 

========================= hadoop sample_6885 =========================

conf.setBoolean(YarnConfiguration.NM_RECOVERY_ENABLED, false);	TrafficController trafficController = new TrafficController(conf, privilegedOperationExecutorMock);	try {	trafficController .bootstrap(DEVICE, ROOT_BANDWIDTH_MBIT, YARN_BANDWIDTH_MBIT);	ArgumentCaptor<PrivilegedOperation> opCaptor = ArgumentCaptor.forClass (PrivilegedOperation.class);	verify(privilegedOperationExecutorMock, times(2)) .executePrivilegedOperation(opCaptor.capture(), eq(false));	List<PrivilegedOperation> ops = opCaptor.getAllValues();	verifyTrafficControlOperation(ops.get(0), PrivilegedOperation.OperationType.TC_MODIFY_STATE, Arrays.asList(WIPE_STATE_CMD));	verifyTrafficControlOperation(ops.get(1), PrivilegedOperation.OperationType.TC_MODIFY_STATE, Arrays.asList(ADD_ROOT_QDISC_CMD, ADD_CGROUP_FILTER_CMD, ADD_ROOT_CLASS_CMD, ADD_DEFAULT_CLASS_CMD, ADD_YARN_CLASS_CMD));	} catch (ResourceHandlerException | PrivilegedOperationException | IOException e) {	
unexpected exception 

ArgumentCaptor<PrivilegedOperation> readOpCaptor = ArgumentCaptor.forClass (PrivilegedOperation.class);	verify(privilegedOperationExecutorMock, times(1)) .executePrivilegedOperation(readOpCaptor.capture(), eq(true));	List<PrivilegedOperation> readOps = readOpCaptor.getAllValues();	verifyTrafficControlOperation(readOps.get(0), PrivilegedOperation.OperationType.TC_READ_STATE, Arrays.asList(READ_QDISC_CMD, READ_FILTER_CMD, READ_CLASS_CMD));	ArgumentCaptor<PrivilegedOperation> writeOpCaptor = ArgumentCaptor .forClass(PrivilegedOperation.class);	verify(privilegedOperationExecutorMock, times(2)) .executePrivilegedOperation(writeOpCaptor.capture(), eq(false));	List<PrivilegedOperation> writeOps = writeOpCaptor.getAllValues();	verifyTrafficControlOperation(writeOps.get(0), PrivilegedOperation.OperationType.TC_MODIFY_STATE, Arrays.asList(WIPE_STATE_CMD));	verifyTrafficControlOperation(writeOps.get(1), PrivilegedOperation.OperationType.TC_MODIFY_STATE, Arrays.asList(ADD_ROOT_QDISC_CMD, ADD_CGROUP_FILTER_CMD, ADD_ROOT_CLASS_CMD, ADD_DEFAULT_CLASS_CMD, ADD_YARN_CLASS_CMD));	} catch (ResourceHandlerException | PrivilegedOperationException | IOException e) {	
unexpected exception 

conf.setBoolean(YarnConfiguration.NM_RECOVERY_ENABLED, false);	TrafficController trafficController = new TrafficController(conf, privilegedOperationExecutorMock);	try {	trafficController .bootstrap(DEVICE, ROOT_BANDWIDTH_MBIT, YARN_BANDWIDTH_MBIT);	try {	TrafficController.BatchBuilder invalidBuilder = trafficController. new BatchBuilder( PrivilegedOperation.OperationType.ADD_PID_TO_CGROUP);	Assert.fail("Invalid builder check failed!");	} catch (ResourceHandlerException e) {	}	} catch (ResourceHandlerException e) {	
unexpected exception 

verifyTrafficControlOperation(addClassOp, PrivilegedOperation.OperationType.TC_MODIFY_STATE, Arrays.asList(expectedAddClassCmd));	TrafficController.BatchBuilder strictModeBuilder = trafficController. new BatchBuilder(PrivilegedOperation.OperationType.TC_MODIFY_STATE) .addContainerClass(classId, CONTAINER_BANDWIDTH_MBIT, true);	PrivilegedOperation addClassStrictModeOp = strictModeBuilder .commitBatchToTempFile();	String expectedAddClassStrictModeCmd = String.format (FORMAT_ADD_CONTAINER_CLASS_TO_DEVICE, classId, CONTAINER_BANDWIDTH_MBIT);	verifyTrafficControlOperation(addClassStrictModeOp, PrivilegedOperation.OperationType.TC_MODIFY_STATE, Arrays.asList(expectedAddClassStrictModeCmd));	TrafficController.BatchBuilder deleteBuilder = trafficController.new BatchBuilder(PrivilegedOperation.OperationType.TC_MODIFY_STATE) .deleteContainerClass(classId);	PrivilegedOperation deleteClassOp = deleteBuilder.commitBatchToTempFile();	String expectedDeleteClassCmd = String.format (FORAMT_DELETE_CONTAINER_CLASS_FROM_DEVICE, classId);	verifyTrafficControlOperation(deleteClassOp, PrivilegedOperation.OperationType.TC_MODIFY_STATE, Arrays.asList(expectedDeleteClassCmd));	} catch (ResourceHandlerException | IOException e) {	
unexpected exception 

========================= hadoop sample_1643 =========================

MockFiller filler = new MockFiller();	final ValueQueue<String> vq = new ValueQueue<String>(10, 0.1f, 300, 1, SyncGenerationPolicy.ALL, filler);	Assert.assertEquals("test", vq.getNext("k1"));	Assert.assertEquals(1, filler.getTop().num);	Assert.assertEquals("Failed in sync call.", 10, vq.getAtMost("k1", 10).size());	Assert.assertEquals("Sync call filler got wrong number.", 10, filler.getTop().num);	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	int size = vq.getSize("k1");	if (size != 10) {	
current valuequeue size is 

========================= hadoop sample_2829 =========================

private static void customShuffleTransferCornerCases( FadvisedFileRegion fileRegion, WritableByteChannel target, int count) {	try {	fileRegion.customShuffleTransfer(target, -1);	Assert.fail("Expected a IllegalArgumentException");	} catch (IllegalArgumentException ie) {	
expected illegal argument is passed 

fileRegion.customShuffleTransfer(target, -1);	Assert.fail("Expected a IllegalArgumentException");	} catch (IllegalArgumentException ie) {	} catch (Exception e) {	Assert.fail("Expected a IllegalArgumentException");	}	try {	fileRegion.customShuffleTransfer(target, count + 1);	Assert.fail("Expected a IllegalArgumentException");	} catch (IllegalArgumentException ie) {	
expected illegal argument is passed 

========================= hadoop sample_5315 =========================

StorageLocation sl = StorageLocation.parse(dir);	File lockFile = new File(sl.getFile(), Storage.STORAGE_FILE_LOCK);	try (RandomAccessFile raf = new RandomAccessFile(lockFile, "rws");	FileChannel channel = raf.getChannel()) {	FileLock lock = channel.tryLock();	assertNotNull(String.format( "Lock file at %s appears to be held by a different process.", lockFile.getAbsolutePath()), lock);	if (lock != null) {	try {	lock.release();	} catch (IOException e) {	
i o error releasing file lock s 

========================= hadoop sample_7282 =========================

public synchronized void parse(EventReader reader, HistoryEventHandler handler) throws IOException {	int eventCtr = 0;	HistoryEvent event;	try {	while ((event = reader.getNextEvent()) != null) {	handler.handleEvent(event);	++eventCtr;	}	} catch (IOException ioe) {	
caught exception parsing history file after events 

private void handleTaskAttemptFailedEvent( TaskAttemptUnsuccessfulCompletionEvent event) {	TaskInfo taskInfo = info.tasksMap.get(event.getTaskId());	if(taskInfo == null) {	
taskinfo is null for taskattemptunsuccessfulcompletionevent taskid 

private void handleTaskAttemptFailedEvent( TaskAttemptUnsuccessfulCompletionEvent event) {	TaskInfo taskInfo = info.tasksMap.get(event.getTaskId());	if(taskInfo == null) {	return;	}	TaskAttemptInfo attemptInfo = taskInfo.attemptsMap.get(event.getTaskAttemptId());	if(attemptInfo == null) {	
attemptinfo is null for taskattemptunsuccessfulcompletionevent taskattemptid 

========================= hadoop sample_4857 =========================

Statistic putRequestsActive = Statistic.OBJECT_PUT_REQUESTS_ACTIVE;	Statistic putBytesPending = Statistic.OBJECT_PUT_BYTES_PENDING;	ContractTestUtils.NanoTimer timer = new ContractTestUtils.NanoTimer();	S3AInstrumentation.OutputStreamStatistics streamStatistics;	long blocksPer10MB = blocksPerMB * 10;	ProgressCallback progress = new ProgressCallback(timer);	try (FSDataOutputStream out = fs.create(hugefile, true, uploadBlockSize, progress)) {	try {	streamStatistics = getOutputStreamStatistics(out);	} catch (ClassCastException e) {	
wrapped output stream is not block stream 

for (long block = 1; block <= blocks; block++) {	out.write(data);	long written = block * uploadBlockSize;	if (block % blocksPer10MB == 0 || written == filesize) {	long percentage = written * 100 / filesize;	double elapsedTime = timer.elapsedTime() / 1.0e9;	double writtenMB = 1.0 * written / _1MB;	LOG.info(String.format("[%02d%%] Buffered %.2f MB out of %d MB;" + " PUT %d bytes (%d pending) in %d operations (%d active);" + " elapsedTime=%.2fs; write to buffer bandwidth=%.2f MB/s", percentage, writtenMB, filesizeMB, storageStatistics.getLong(putBytes), gaugeValue(putBytesPending), storageStatistics.getLong(putRequests), gaugeValue(putRequestsActive), elapsedTime, writtenMB / elapsedTime));	}	}	
closing stream 

for (long block = 1; block <= blocks; block++) {	out.write(data);	long written = block * uploadBlockSize;	if (block % blocksPer10MB == 0 || written == filesize) {	long percentage = written * 100 / filesize;	double elapsedTime = timer.elapsedTime() / 1.0e9;	double writtenMB = 1.0 * written / _1MB;	LOG.info(String.format("[%02d%%] Buffered %.2f MB out of %d MB;" + " PUT %d bytes (%d pending) in %d operations (%d active);" + " elapsedTime=%.2fs; write to buffer bandwidth=%.2f MB/s", percentage, writtenMB, filesizeMB, storageStatistics.getLong(putBytes), gaugeValue(putBytesPending), storageStatistics.getLong(putRequests), gaugeValue(putRequestsActive), elapsedTime, writtenMB / elapsedTime));	}	}	
statistics 

LOG.info(String.format("[%02d%%] Buffered %.2f MB out of %d MB;" + " PUT %d bytes (%d pending) in %d operations (%d active);" + " elapsedTime=%.2fs; write to buffer bandwidth=%.2f MB/s", percentage, writtenMB, filesizeMB, storageStatistics.getLong(putBytes), gaugeValue(putBytesPending), storageStatistics.getLong(putRequests), gaugeValue(putRequestsActive), elapsedTime, writtenMB / elapsedTime));	}	}	ContractTestUtils.NanoTimer closeTimer = new ContractTestUtils.NanoTimer();	out.close();	closeTimer.end("time to close() output stream");	}	timer.end("time to write %d MB in blocks of %d", filesizeMB, uploadBlockSize);	logFSState();	bandwidth(timer, filesize);	
statistics after stream closed 

}	ContractTestUtils.NanoTimer closeTimer = new ContractTestUtils.NanoTimer();	out.close();	closeTimer.end("time to close() output stream");	}	timer.end("time to write %d MB in blocks of %d", filesizeMB, uploadBlockSize);	logFSState();	bandwidth(timer, filesize);	long putRequestCount = storageStatistics.getLong(putRequests);	Long putByteCount = storageStatistics.getLong(putBytes);	
put bytes in operations mb operation 

}	ContractTestUtils.NanoTimer closeTimer = new ContractTestUtils.NanoTimer();	out.close();	closeTimer.end("time to close() output stream");	}	timer.end("time to write %d MB in blocks of %d", filesizeMB, uploadBlockSize);	logFSState();	bandwidth(timer, filesize);	long putRequestCount = storageStatistics.getLong(putRequests);	Long putByteCount = storageStatistics.getLong(putBytes);	
time per put ns 

public void progressChanged(ProgressEvent progressEvent) {	ProgressEventType eventType = progressEvent.getEventType();	if (eventType.isByteCountEvent()) {	bytesTransferred.addAndGet(progressEvent.getBytesTransferred());	}	switch (eventType) {	case TRANSFER_PART_FAILED_EVENT: failures.incrementAndGet();	
transfer failure 

switch (eventType) {	case TRANSFER_PART_FAILED_EVENT: failures.incrementAndGet();	break;	case TRANSFER_PART_COMPLETED_EVENT: long elapsedTime = timer.elapsedTime();	double elapsedTimeS = elapsedTime / 1.0e9;	long written = bytesTransferred.get();	long writtenMB = written / _1MB;	LOG.info(String.format( "Event %s; total uploaded=%d MB in %.1fs;" + " effective upload bandwidth = %.2f MB/s", progressEvent, writtenMB, elapsedTimeS, writtenMB / elapsedTimeS));	break;	default: if (eventType.isByteCountEvent()) {	
event 

case TRANSFER_PART_FAILED_EVENT: failures.incrementAndGet();	break;	case TRANSFER_PART_COMPLETED_EVENT: long elapsedTime = timer.elapsedTime();	double elapsedTimeS = elapsedTime / 1.0e9;	long written = bytesTransferred.get();	long writtenMB = written / _1MB;	LOG.info(String.format( "Event %s; total uploaded=%d MB in %.1fs;" + " effective upload bandwidth = %.2f MB/s", progressEvent, writtenMB, elapsedTimeS, writtenMB / elapsedTimeS));	break;	default: if (eventType.isByteCountEvent()) {	} else {	
event 

public void test_040_PositionedReadHugeFile() throws Throwable {	assumeHugeFileExists();	final String encryption = getConf().getTrimmed( SERVER_SIDE_ENCRYPTION_ALGORITHM);	boolean encrypted = encryption != null;	if (encrypted) {	
file is encrypted with algorithm 

readAtByte0.end("time to read data at start of file");	ops++;	readAtEOF = new ContractTestUtils.NanoTimer();	in.readFully(eof - bufferSize, buffer);	readAtEOF.end("time to read data at end of file");	ops++;	readAtByte0Again = new ContractTestUtils.NanoTimer();	in.readFully(0, buffer);	readAtByte0Again.end("time to read data at start of file again");	ops++;	
final stream state 

S3AFileSystem fs = getFileSystem();	FileStatus status = fs.getFileStatus(hugefile);	long filesize = status.getLen();	long blocks = filesize / uploadBlockSize;	byte[] data = new byte[uploadBlockSize];	ContractTestUtils.NanoTimer timer = new ContractTestUtils.NanoTimer();	try (FSDataInputStream in = fs.open(hugefile, uploadBlockSize)) {	for (long block = 0; block < blocks; block++) {	in.readFully(data);	}	
final stream state 

========================= hadoop sample_5895 =========================

public void setup() {	securityManager = System.getSecurityManager();	System.setSecurityManager(new NoExitSecurityManager());	try {	fs = FileSystem.get(getConf());	root = new Path("target/tmp").makeQualified(fs.getUri(), fs.getWorkingDirectory()).toString();	TestDistCpUtils.delete(fs, root);	} catch (IOException e) {	
exception encountered 

========================= hadoop sample_6240 =========================

public void channelRead(final ChannelHandlerContext ctx, Object msg) {	client.writeAndFlush(msg).addListener(new ChannelFutureListener() {	public void operationComplete(ChannelFuture future) {	if (future.isSuccess()) {	ctx.channel().read();	} else {	
proxy failed cause 

public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) {	
proxy for failed cause 

public void operationComplete(ChannelFuture future) throws Exception {	if (future.isSuccess()) {	ctx.channel().pipeline().remove(HttpResponseEncoder.class);	HttpRequest newReq = new DefaultFullHttpRequest(HTTP_1_1, req.getMethod(), req.getUri());	newReq.headers().add(req.headers());	newReq.headers().set(CONNECTION, Values.CLOSE);	future.channel().writeAndFlush(newReq);	} else {	DefaultHttpResponse resp = new DefaultHttpResponse(HTTP_1_1, INTERNAL_SERVER_ERROR);	resp.headers().set(CONNECTION, Values.CLOSE);	
proxy failed cause 

public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) {	if (LOG.isDebugEnabled()) {	
proxy for failed cause 

========================= hadoop sample_7902 =========================

public void testKerberosAuth() throws Throwable {	File krb5conf = getKdc().getKrb5conf();	String krbConfig = FileUtils.readFileToString(krb5conf);	
conf at 

public void testDefaultRealmValid() throws Throwable {	String defaultRealm = KerberosUtil.getDefaultRealm();	assertNotEmpty("No default Kerberos Realm", defaultRealm);	
default realm 

========================= hadoop sample_2653 =========================

public void write(DataOutput out) throws IOException {	
writing nmtokenidentifier to rpc layer 

========================= hadoop sample_2203 =========================

public synchronized void put(final long inodeId, final DFSOutputStream out, final DFSClient dfsc) {	if (dfsc.isClientRunning()) {	if (!isRunning() || isRenewerExpired()) {	final int id = ++currentId;	daemon = new Daemon(new Runnable() {	public void run() {	try {	if (LOG.isDebugEnabled()) {	
lease renewer daemon for with renew id started 

if (dfsc.isClientRunning()) {	if (!isRunning() || isRenewerExpired()) {	final int id = ++currentId;	daemon = new Daemon(new Runnable() {	public void run() {	try {	if (LOG.isDebugEnabled()) {	}	LeaseRenewer.this.run(id);	} catch(InterruptedException e) {	
leaserenewer is interrupted 

try {	if (LOG.isDebugEnabled()) {	}	LeaseRenewer.this.run(id);	} catch(InterruptedException e) {	} finally {	synchronized(LeaseRenewer.this) {	Factory.INSTANCE.remove(LeaseRenewer.this);	}	if (LOG.isDebugEnabled()) {	
lease renewer daemon for with renew id exited 

public void interruptAndJoin() throws InterruptedException {	Daemon daemonCopy = null;	synchronized (this) {	if (isRunning()) {	daemon.interrupt();	daemonCopy = daemon;	}	}	if (daemonCopy != null) {	
wait for lease checker to terminate 

}	Collections.sort(copies, new Comparator<DFSClient>() {	public int compare(final DFSClient left, final DFSClient right) {	return left.getClientName().compareTo(right.getClientName());	}	});	String previousName = "";	for (final DFSClient c : copies) {	if (!c.getClientName().equals(previousName)) {	if (!c.renewLease()) {	
did not renew lease for client 

return left.getClientName().compareTo(right.getClientName());	}	});	String previousName = "";	for (final DFSClient c : copies) {	if (!c.getClientName().equals(previousName)) {	if (!c.renewLease()) {	continue;	}	previousName = c.getClientName();	
lease renewed for client 

private void run(final int id) throws InterruptedException {	for(long lastRenewed = Time.monotonicNow(); !Thread.interrupted();	Thread.sleep(getSleepPeriod())) {	final long elapsed = Time.monotonicNow() - lastRenewed;	if (elapsed >= getRenewalTime()) {	try {	renew();	if (LOG.isDebugEnabled()) {	
lease renewer daemon for with renew id executed 

for(long lastRenewed = Time.monotonicNow(); !Thread.interrupted();	Thread.sleep(getSleepPeriod())) {	final long elapsed = Time.monotonicNow() - lastRenewed;	if (elapsed >= getRenewalTime()) {	try {	renew();	if (LOG.isDebugEnabled()) {	}	lastRenewed = Time.monotonicNow();	} catch (SocketTimeoutException ie) {	
failed to renew lease for for seconds aborting 

synchronized (this) {	while (!dfsclients.isEmpty()) {	DFSClient dfsClient = dfsclients.get(0);	dfsClient.closeAllFilesBeingWritten(true);	closeClient(dfsClient);	}	emptyTime = 0;	}	break;	} catch (IOException ie) {	
failed to renew lease for for seconds will retry shortly 

emptyTime = 0;	}	break;	} catch (IOException ie) {	}	}	synchronized(this) {	if (id != currentId || isRenewerExpired()) {	if (LOG.isDebugEnabled()) {	if (id != currentId) {	
lease renewer daemon for with renew id is not current 

}	break;	} catch (IOException ie) {	}	}	synchronized(this) {	if (id != currentId || isRenewerExpired()) {	if (LOG.isDebugEnabled()) {	if (id != currentId) {	} else {	
lease renewer daemon for with renew id expired 

========================= hadoop sample_7001 =========================

ThreadFactory tf = tfBuilder.build();	launcherPool = new HadoopThreadPoolExecutor(5, 5, 1, TimeUnit.HOURS, new LinkedBlockingQueue<Runnable>(), tf);	eventHandlingThread = new Thread(new Runnable() {	public void run() {	CommitterEvent event = null;	while (!stopped.get() && !Thread.currentThread().isInterrupted()) {	try {	event = eventQueue.take();	} catch (InterruptedException e) {	if (!stopped.get()) {	
returning interrupted 

private synchronized void cancelJobCommit() {	Thread threadCommitting = jobCommitThread;	if (threadCommitting != null && threadCommitting.isAlive()) {	
cancelling commit 

public void run() {	
processing the event 

protected void handleJobSetup(CommitterJobSetupEvent event) {	try {	committer.setupJob(event.getJobContext());	context.getEventHandler().handle( new JobSetupCompletedEvent(event.getJobID()));	} catch (Exception e) {	
job setup failed 

protected void handleJobCommit(CommitterJobCommitEvent event) {	boolean commitJobIsRepeatable = false;	try {	commitJobIsRepeatable = committer.isCommitJobRepeatable( event.getJobContext());	} catch (IOException e) {	
exception in committer iscommitjobrepeatable 

} catch (IOException e) {	}	try {	touchz(startCommitFile, commitJobIsRepeatable);	jobCommitStarted();	waitForValidCommitWindow();	committer.commitJob(event.getJobContext());	touchz(endCommitSuccessFile, commitJobIsRepeatable);	context.getEventHandler().handle( new JobCommitCompletedEvent(event.getJobID()));	} catch (Exception e) {	
could not commit job 

touchz(startCommitFile, commitJobIsRepeatable);	jobCommitStarted();	waitForValidCommitWindow();	committer.commitJob(event.getJobContext());	touchz(endCommitSuccessFile, commitJobIsRepeatable);	context.getEventHandler().handle( new JobCommitCompletedEvent(event.getJobID()));	} catch (Exception e) {	try {	touchz(endCommitFailureFile, commitJobIsRepeatable);	} catch (Exception e2) {	
could not create failure file 

protected void handleJobAbort(CommitterJobAbortEvent event) {	cancelJobCommit();	try {	committer.abortJob(event.getJobContext(), event.getFinalState());	} catch (Exception e) {	
could not abort job 

protected void handleTaskAbort(CommitterTaskAbortEvent event) {	try {	committer.abortTask(event.getAttemptContext());	} catch (Exception e) {	
task cleanup failed for attempt 

========================= hadoop sample_5179 =========================

protected void serviceStart() throws Exception {	
starting router rmadmin service 

Configuration conf = getConfig();	YarnRPC rpc = YarnRPC.create(conf);	UserGroupInformation.setConfiguration(conf);	this.listenerEndpoint = conf.getSocketAddr(YarnConfiguration.ROUTER_BIND_HOST, YarnConfiguration.ROUTER_RMADMIN_ADDRESS, YarnConfiguration.DEFAULT_ROUTER_RMADMIN_ADDRESS, YarnConfiguration.DEFAULT_ROUTER_RMADMIN_PORT);	int maxCacheSize = conf.getInt(YarnConfiguration.ROUTER_PIPELINE_CACHE_MAX_SIZE, YarnConfiguration.DEFAULT_ROUTER_PIPELINE_CACHE_MAX_SIZE);	this.userPipelineMap = Collections.synchronizedMap( new LRUCacheHashMap<String, RequestInterceptorChainWrapper>( maxCacheSize, true));	Configuration serverConf = new Configuration(conf);	int numWorkerThreads = serverConf.getInt(YarnConfiguration.RM_ADMIN_CLIENT_THREAD_COUNT, YarnConfiguration.DEFAULT_RM_ADMIN_CLIENT_THREAD_COUNT);	this.server = rpc.getServer(ResourceManagerAdministrationProtocol.class, this, listenerEndpoint, serverConf, null, numWorkerThreads);	this.server.start();	
router rmadminservice listening on address 

protected void serviceStop() throws Exception {	
stopping router rmadminservice 

private void initializePipeline(String user) {	RequestInterceptorChainWrapper chainWrapper = null;	synchronized (this.userPipelineMap) {	if (this.userPipelineMap.containsKey(user)) {	
request to start an already existing user was received so ignoring 

private void initializePipeline(String user) {	RequestInterceptorChainWrapper chainWrapper = null;	synchronized (this.userPipelineMap) {	if (this.userPipelineMap.containsKey(user)) {	return;	}	chainWrapper = new RequestInterceptorChainWrapper();	this.userPipelineMap.put(user, chainWrapper);	}	
initializing request processing pipeline for the user 

========================= hadoop sample_1993 =========================

public static void main(String[] args) {	boolean result = false;	try {	Client client = new Client();	
initializing client 

if (!doRun) {	System.exit(0);	}	} catch (IllegalArgumentException e) {	System.err.println(e.getLocalizedMessage());	client.printUsage();	System.exit(-1);	}	result = client.run();	} catch (Throwable t) {	
error running client 

} catch (IllegalArgumentException e) {	System.err.println(e.getLocalizedMessage());	client.printUsage();	System.exit(-1);	}	result = client.run();	} catch (Throwable t) {	System.exit(1);	}	if (result) {	
application completed successfully 

client.printUsage();	System.exit(-1);	}	result = client.run();	} catch (Throwable t) {	System.exit(1);	}	if (result) {	System.exit(0);	}	
application failed to complete successfully 

public boolean init(String[] args) throws ParseException {	CommandLine cliParser = new GnuParser().parse(opts, args);	if (args.length == 0) {	throw new IllegalArgumentException("No args specified for client to initialize");	}	if (cliParser.hasOption("log_properties")) {	String log4jPath = cliParser.getOptionValue("log_properties");	try {	Log4jPropertyHelper.updateLog4jConfiguration(Client.class, log4jPath);	} catch (Exception e) {	
can not set up custom properties 

}	}	if (cliParser.hasOption("help")) {	printUsage();	return false;	}	if (cliParser.hasOption("debug")) {	debugFlag = true;	}	if (cliParser.hasOption("keep_containers_across_application_attempts")) {	
keep containers across application attempts 

public boolean run() throws IOException, YarnException {	
running client 

public boolean run() throws IOException, YarnException {	yarnClient.start();	YarnClusterMetrics clusterMetrics = yarnClient.getYarnClusterMetrics();	LOG.info("Got Cluster metric info from ASM" + ", numNodeManagers=" + clusterMetrics.getNumNodeManagers());	List<NodeReport> clusterNodeReports = yarnClient.getNodeReports( NodeState.RUNNING);	
got cluster node info from asm 

for (QueueACL userAcl : aclInfo.getUserAcls()) {	LOG.info("User ACL Info for Queue" + ", queueName=" + aclInfo.getQueueName() + ", userAcl=" + userAcl.name());	}	}	if (domainId != null && domainId.length() > 0 && toCreateDomain) {	prepareTimelineDomain();	}	YarnClientApplication app = yarnClient.createApplication();	GetNewApplicationResponse appResponse = app.getNewApplicationResponse();	long maxMem = appResponse.getMaximumResourceCapability().getMemorySize();	
max mem capability of resources in this cluster 

prepareTimelineDomain();	}	YarnClientApplication app = yarnClient.createApplication();	GetNewApplicationResponse appResponse = app.getNewApplicationResponse();	long maxMem = appResponse.getMaximumResourceCapability().getMemorySize();	if (amMemory > maxMem) {	LOG.info("AM memory specified above max threshold of cluster. Using max value." + ", specified=" + amMemory + ", max=" + maxMem);	amMemory = maxMem;	}	int maxVCores = appResponse.getMaximumResourceCapability().getVirtualCores();	
max virtual cores capability of resources in this cluster 

tags.add(TimelineUtils.generateFlowNameTag(flowName));	}	if (flowVersion != null) {	tags.add(TimelineUtils.generateFlowVersionTag(flowVersion));	}	if (flowRunId != 0) {	tags.add(TimelineUtils.generateFlowRunIdTag(flowRunId));	}	appContext.setApplicationTags(tags);	Map<String, LocalResource> localResources = new HashMap<String, LocalResource>();	
copy app master jar from local filesystem and add to local environment 

FileStatus shellFileStatus = fs.getFileStatus(shellDst);	hdfsShellScriptLen = shellFileStatus.getLen();	hdfsShellScriptTimestamp = shellFileStatus.getModificationTime();	}	if (!shellCommand.isEmpty()) {	addToLocalResources(fs, null, shellCommandPath, appId.toString(), localResources, shellCommand);	}	if (shellArgs.length > 0) {	addToLocalResources(fs, null, shellArgsPath, appId.toString(), localResources, StringUtils.join(shellArgs, " "));	}	
set the environment for the application master 

classPathEnv.append(ApplicationConstants.CLASS_PATH_SEPARATOR);	classPathEnv.append(c.trim());	}	classPathEnv.append(ApplicationConstants.CLASS_PATH_SEPARATOR).append( "./log4j.properties");	if (conf.getBoolean(YarnConfiguration.IS_MINI_YARN_CLUSTER, false)) {	classPathEnv.append(':');	classPathEnv.append(System.getProperty("java.class.path"));	}	env.put("CLASSPATH", classPathEnv.toString());	Vector<CharSequence> vargs = new Vector<CharSequence>(30);	
setting up app master command 

if (debugFlag) {	vargs.add("--debug");	}	vargs.addAll(containerRetryOptions);	vargs.add("1>" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + "/AppMaster.stdout");	vargs.add("2>" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + "/AppMaster.stderr");	StringBuilder command = new StringBuilder();	for (CharSequence str : vargs) {	command.append(str).append(" ");	}	
completed setting up app master command 

appContext.setResource(capability);	if (UserGroupInformation.isSecurityEnabled()) {	Credentials credentials = new Credentials();	String tokenRenewer = YarnClientUtils.getRmPrincipal(conf);	if (tokenRenewer == null || tokenRenewer.length() == 0) {	throw new IOException( "Can't get Master Kerberos principal for the RM to use as renewer");	}	final Token<?> tokens[] = fs.addDelegationTokens(tokenRenewer, credentials);	if (tokens != null) {	for (Token<?> token : tokens) {	
got dt for 

}	DataOutputBuffer dob = new DataOutputBuffer();	credentials.writeTokenStorageToStream(dob);	ByteBuffer fsTokens = ByteBuffer.wrap(dob.getData(), 0, dob.getLength());	amContainer.setTokens(fsTokens);	}	appContext.setAMContainerSpec(amContainer);	Priority pri = Priority.newInstance(amPriority);	appContext.setPriority(pri);	appContext.setQueue(amQueue);	
submitting application to asm 

private boolean monitorApplication(ApplicationId appId) throws YarnException, IOException {	while (true) {	try {	Thread.sleep(1000);	} catch (InterruptedException e) {	
thread sleep in monitoring loop interrupted 

try {	Thread.sleep(1000);	} catch (InterruptedException e) {	}	ApplicationReport report = yarnClient.getApplicationReport(appId);	LOG.info("Got application report from ASM for" + ", appId=" + appId.getId() + ", clientToAMToken=" + report.getClientToAMToken() + ", appDiagnostics=" + report.getDiagnostics() + ", appMasterHost=" + report.getHost() + ", appQueue=" + report.getQueue() + ", appMasterRpcPort=" + report.getRpcPort() + ", appStartTime=" + report.getStartTime() + ", yarnAppState=" + report.getYarnApplicationState().toString() + ", distributedFinalState=" + report.getFinalApplicationStatus().toString() + ", appTrackingUrl=" + report.getTrackingUrl() + ", appUser=" + report.getUser());	YarnApplicationState state = report.getYarnApplicationState();	FinalApplicationStatus dsStatus = report.getFinalApplicationStatus();	if (YarnApplicationState.FINISHED == state) {	if (FinalApplicationStatus.SUCCEEDED == dsStatus) {	
application has completed successfully breaking monitoring loop 

else {	LOG.info("Application did finished unsuccessfully." + " YarnState=" + state.toString() + ", DSFinalStatus=" + dsStatus.toString() + ". Breaking monitoring loop");	return false;	}	}	else if (YarnApplicationState.KILLED == state || YarnApplicationState.FAILED == state) {	LOG.info("Application did not finish." + " YarnState=" + state.toString() + ", DSFinalStatus=" + dsStatus.toString() + ". Breaking monitoring loop");	return false;	}	if (System.currentTimeMillis() > (clientStartTime + clientTimeout)) {	
reached client specified timeout for application killing application 

private void prepareTimelineDomain() {	TimelineClient timelineClient = null;	if (conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_ENABLED)) {	timelineClient = TimelineClient.createTimelineClient();	timelineClient.init(conf);	timelineClient.start();	} else {	
cannot put the domain because the timeline service is not enabled 

timelineClient.start();	} else {	return;	}	try {	TimelineDomain domain = new TimelineDomain();	domain.setId(domainId);	domain.setReaders( viewACLs != null && viewACLs.length() > 0 ? viewACLs : " ");	domain.setWriters( modifyACLs != null && modifyACLs.length() > 0 ? modifyACLs : " ");	timelineClient.putDomain(domain);	
put the timeline domain 

} else {	return;	}	try {	TimelineDomain domain = new TimelineDomain();	domain.setId(domainId);	domain.setReaders( viewACLs != null && viewACLs.length() > 0 ? viewACLs : " ");	domain.setWriters( modifyACLs != null && modifyACLs.length() > 0 ? modifyACLs : " ");	timelineClient.putDomain(domain);	} catch (Exception e) {	
error when putting the timeline domain 

========================= hadoop sample_7 =========================

public void generateTextFile(FileSystem fs, Path inputFile, long numLines, Order sortOrder) throws IOException {	
creating control file numlines sortorder 

}	break;	case DESCENDING: for (long l = numLines; l > 0; l--) {	output.println(pad(l, padding));	}	break;	}	} finally {	if (output != null) output.close();	}	
created control file 

========================= hadoop sample_5483 =========================

private Object runTool(String ... args) throws Exception {	errOutBytes.reset();	outBytes.reset();	
running haadmin 

private Object runTool(String ... args) throws Exception {	errOutBytes.reset();	outBytes.reset();	int ret = tool.run(args);	errOutput = new String(errOutBytes.toByteArray(), Charsets.UTF_8);	output = new String(outBytes.toByteArray(), Charsets.UTF_8);	
err output output 

========================= hadoop sample_3107 =========================

LightWeightGSet<TestElement, TestElement> set = new LightWeightGSet<TestElement, TestElement>(16);	for (Integer i : list) {	set.put(new TestElement(i));	}	long sum = 0;	for (Iterator<TestElement> iter = set.iterator();	iter.hasNext(); ) {	sum += iter.next().getVal();	}	long mode = sum / set.size();	
removing all elements above 

========================= hadoop sample_2891 =========================

public synchronized void set(float progress) {	if (Float.isNaN(progress)) {	progress = 0;	
illegal progress value found progress is float nan progress will be changed to 

public synchronized void set(float progress) {	if (Float.isNaN(progress)) {	progress = 0;	}	else if (progress == Float.NEGATIVE_INFINITY) {	progress = 0;	
illegal progress value found progress is float negative infinity progress will be changed to 

public synchronized void set(float progress) {	if (Float.isNaN(progress)) {	progress = 0;	}	else if (progress == Float.NEGATIVE_INFINITY) {	progress = 0;	}	else if (progress < 0) {	progress = 0;	
illegal progress value found progress is less than progress will be changed to 

progress = 0;	}	else if (progress == Float.NEGATIVE_INFINITY) {	progress = 0;	}	else if (progress < 0) {	progress = 0;	}	else if (progress > 1) {	progress = 1;	
illegal progress value found progress is larger than progress will be changed to 

progress = 0;	}	else if (progress < 0) {	progress = 0;	}	else if (progress > 1) {	progress = 1;	}	else if (progress == Float.POSITIVE_INFINITY) {	progress = 1;	
illegal progress value found progress is float positive infinity progress will be changed to 

========================= hadoop sample_3629 =========================

Assert.assertEquals(result.get(4), "/bin/nmPrivateCTokensPath");	Assert.assertEquals(result.get(8), "-classpath" );	Assert.assertEquals(result.get(11), "-Xmx256m" );	Assert.assertEquals(result.get(12),"org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer" );	Assert.assertEquals(result.get(13), "test");	Assert.assertEquals(result.get(14), "application_0");	Assert.assertEquals(result.get(15),"12345" );	Assert.assertEquals(result.get(16),"localhost" );	Assert.assertEquals(result.get(17),"8040" );	} catch (InterruptedException e) {	
error 

========================= hadoop sample_1614 =========================

private TimelineReader createTimelineReaderStore(final Configuration conf) {	String timelineReaderClassName = conf.get( YarnConfiguration.TIMELINE_SERVICE_READER_CLASS, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_READER_CLASS);	
using store 

private void startTimelineReaderWebApp() {	Configuration conf = getConfig();	addFilters(conf);	String bindAddress = WebAppUtils.getWebAppBindURL(conf, YarnConfiguration.TIMELINE_SERVICE_BIND_HOST, WebAppUtils.getTimelineReaderWebAppURL(conf));	
instantiating timelinereaderwebapp at 

static TimelineReaderServer startTimelineReaderServer(String[] args, Configuration conf) {	Thread.setDefaultUncaughtExceptionHandler( new YarnUncaughtExceptionHandler());	StringUtils.startupShutdownMessage(TimelineReaderServer.class, args, LOG);	TimelineReaderServer timelineReaderServer = null;	try {	timelineReaderServer = new TimelineReaderServer();	ShutdownHookManager.get().addShutdownHook( new CompositeServiceShutdownHook(timelineReaderServer), SHUTDOWN_HOOK_PRIORITY);	timelineReaderServer.init(conf);	timelineReaderServer.start();	} catch (Throwable t) {	
error starting timelinereaderwebserver 

========================= hadoop sample_323 =========================

public Resource normalize(Resource r, Resource minimumResource, Resource maximumResource, Resource stepFactor) {	if (stepFactor.getMemorySize() == 0 || stepFactor.getVirtualCores() == 0) {	Resource step = Resources.clone(stepFactor);	if (stepFactor.getMemorySize() == 0) {	
memory cannot be allocated in increments of zero assuming mb increment size please ensure the scheduler configuration is correct 

public Resource normalize(Resource r, Resource minimumResource, Resource maximumResource, Resource stepFactor) {	if (stepFactor.getMemorySize() == 0 || stepFactor.getVirtualCores() == 0) {	Resource step = Resources.clone(stepFactor);	if (stepFactor.getMemorySize() == 0) {	step.setMemorySize(minimumResource.getMemorySize());	}	if (stepFactor.getVirtualCores() == 0) {	
vcore cannot be allocated in increments of zero assuming vcores increment size please ensure the scheduler configuration is correct 

========================= hadoop sample_2192 =========================

stateFilter = NodeState.valueOf(StringUtils.toUpperCase(type));	}	Collection<RMNode> rmNodes = this.rm.getRMContext().getRMNodes().values();	boolean isInactive = false;	if (stateFilter != null) {	switch (stateFilter) {	case DECOMMISSIONED: case LOST: case REBOOTED: case SHUTDOWN: rmNodes = this.rm.getRMContext().getInactiveRMNodes().values();	isInactive = true;	break;	case DECOMMISSIONING: break;	
unexpected state filter for inactive rm node 

========================= hadoop sample_773 =========================

public PeerCache(int c, long e) {	this.capacity = c;	this.expiryPeriod = e;	if (capacity == 0 ) {	
socketcache disabled 

Iterator<Value> iter = sockStreamList.iterator();	while (iter.hasNext()) {	Value candidate = iter.next();	iter.remove();	long ageMs = Time.monotonicNow() - candidate.getTime();	Peer peer = candidate.getPeer();	if (ageMs >= expiryPeriod) {	try {	peer.close();	} catch (IOException e) {	
got ioexception closing stale peer which is ms old 

========================= hadoop sample_6997 =========================

public synchronized void run() {	try {	closeAll(true);	} catch (IOException e) {	
dfsclientcache closeall threw an exception 

private RemovalListener<String, DFSClient> clientRemovalListener() {	return new RemovalListener<String, DFSClient>() {	public void onRemoval(RemovalNotification<String, DFSClient> notification) {	DFSClient client = notification.getValue();	try {	client.close();	} catch (IOException e) {	
ioexception when closing the dfsclient s cause s 

========================= hadoop sample_7056 =========================

throw new IOException("Cannot initialize for null containerId");	}	this.lfs = lfs;	this.user = user;	this.appId = appId;	this.localDirs = localDirs;	this.localizerId = localizerId;	this.recordFactory = recordFactory;	this.conf = new YarnConfiguration();	this.diskValidator = DiskValidatorFactory.getInstance( conf.get(YarnConfiguration.DISK_VALIDATOR, YarnConfiguration.DEFAULT_DISK_VALIDATOR));	
disk validator is loaded 

protected void closeFileSystems(UserGroupInformation ugi) {	try {	FileSystem.closeAllForUGI(ugi);	} catch (IOException e) {	
failed to close filesystems 

}	break;	case DIE: for (Future<Path> pending : pendingResources.values()) {	pending.cancel(true);	}	status = createStatus();	try {	nodemanager.heartbeat(status);	} catch (YarnException e) {	e.printStackTrace(System.out);	
heartbeat failed while dying 

String appId = argv[1];	String locId = argv[2];	InetSocketAddress nmAddr = new InetSocketAddress(argv[3], Integer.parseInt(argv[4]));	String[] sLocaldirs = Arrays.copyOfRange(argv, 5, argv.length);	ArrayList<Path> localDirs = new ArrayList<Path>(sLocaldirs.length);	for (String sLocaldir : sLocaldirs) {	localDirs.add(new Path(sLocaldir));	}	final String uid = UserGroupInformation.getCurrentUser().getShortUserName();	if (!user.equals(uid)) {	
localization running as not 

localDirs.add(new Path(sLocaldir));	}	final String uid = UserGroupInformation.getCurrentUser().getShortUserName();	if (!user.equals(uid)) {	}	ContainerLocalizer localizer = new ContainerLocalizer(FileContext.getLocalFSFileContext(), user, appId, locId, localDirs, RecordFactoryProvider.getRecordFactory(null));	localizer.runLocalization(nmAddr);	return;	} catch (Throwable e) {	e.printStackTrace(System.out);	
exception in main 

========================= hadoop sample_1881 =========================

protected void writeEntities(Configuration tlConf, TimelineCollectorManager manager, Context context) throws IOException {	JobHistoryFileReplayHelper helper = new JobHistoryFileReplayHelper(context);	int replayMode = helper.getReplayMode();	JobHistoryFileParser parser = helper.getParser();	TimelineEntityConverterV2 converter = new TimelineEntityConverterV2();	Collection<JobFiles> jobs = helper.getJobFiles();	if (jobs.isEmpty()) {	
will process no jobs 

protected void writeEntities(Configuration tlConf, TimelineCollectorManager manager, Context context) throws IOException {	JobHistoryFileReplayHelper helper = new JobHistoryFileReplayHelper(context);	int replayMode = helper.getReplayMode();	JobHistoryFileParser parser = helper.getParser();	TimelineEntityConverterV2 converter = new TimelineEntityConverterV2();	Collection<JobFiles> jobs = helper.getJobFiles();	if (jobs.isEmpty()) {	} else {	
will process jobs 

int replayMode = helper.getReplayMode();	JobHistoryFileParser parser = helper.getParser();	TimelineEntityConverterV2 converter = new TimelineEntityConverterV2();	Collection<JobFiles> jobs = helper.getJobFiles();	if (jobs.isEmpty()) {	} else {	}	for (JobFiles job: jobs) {	String jobIdStr = job.getJobId();	if (job.getJobConfFilePath() == null || job.getJobHistoryFilePath() == null) {	
missing either the job history file or the configuration file skipping 

TimelineEntityConverterV2 converter = new TimelineEntityConverterV2();	Collection<JobFiles> jobs = helper.getJobFiles();	if (jobs.isEmpty()) {	} else {	}	for (JobFiles job: jobs) {	String jobIdStr = job.getJobId();	if (job.getJobConfFilePath() == null || job.getJobHistoryFilePath() == null) {	continue;	}	
processing 

if (job.getJobConfFilePath() == null || job.getJobHistoryFilePath() == null) {	continue;	}	JobId jobId = TypeConverter.toYarn(JobID.forName(jobIdStr));	ApplicationId appId = jobId.getAppId();	AppLevelTimelineCollector collector = new AppLevelTimelineCollector(appId);	manager.putIfAbsent(appId, collector);	try {	JobInfo jobInfo = parser.parseHistoryFile(job.getJobHistoryFilePath());	Configuration jobConf = parser.parseConfiguration(job.getJobConfFilePath());	
parsed the job history file and the configuration file for job 

manager.putIfAbsent(appId, collector);	try {	JobInfo jobInfo = parser.parseHistoryFile(job.getJobHistoryFilePath());	Configuration jobConf = parser.parseConfiguration(job.getJobConfFilePath());	TimelineCollectorContext tlContext = collector.getTimelineEntityContext();	tlContext.setFlowName(jobInfo.getJobname());	tlContext.setFlowRunId(jobInfo.getSubmitTime());	tlContext.setUserId(jobInfo.getUsername());	long totalTime = 0;	List<TimelineEntity> entitySet = converter.createTimelineEntities(jobInfo, jobConf);	
converted them into timeline entities for job 

try {	switch (replayMode) {	case JobHistoryFileReplayHelper.WRITE_ALL_AT_ONCE: writeAllEntities(collector, entitySet, ugi);	break;	case JobHistoryFileReplayHelper.WRITE_PER_ENTITY: writePerEntity(collector, entitySet, ugi);	break;	default: break;	}	} catch (Exception e) {	context.getCounter(PerfCounters.TIMELINE_SERVICE_WRITE_FAILURES). increment(1);	
writing to the timeline service failed 

case JobHistoryFileReplayHelper.WRITE_PER_ENTITY: writePerEntity(collector, entitySet, ugi);	break;	default: break;	}	} catch (Exception e) {	context.getCounter(PerfCounters.TIMELINE_SERVICE_WRITE_FAILURES). increment(1);	}	long endWrite = System.nanoTime();	totalTime += TimeUnit.NANOSECONDS.toMillis(endWrite-startWrite);	int numEntities = entitySet.size();	
wrote entities in ms 

private void writePerEntity(AppLevelTimelineCollector collector, List<TimelineEntity> entitySet, UserGroupInformation ugi) throws IOException {	for (TimelineEntity entity : entitySet) {	TimelineEntities entities = new TimelineEntities();	entities.addEntity(entity);	collector.putEntities(entities, ugi);	
wrote entity 

========================= hadoop sample_5591 =========================

public void mlock(String identifier, ByteBuffer buffer, long len) throws IOException {	
mlocking 

public static void chmod(String path, int mode) throws IOException {	if (!Shell.WINDOWS) {	chmodImpl(path, mode);	} else {	try {	chmodImpl(path, mode);	} catch (NativeIOException nioe) {	if (nioe.getErrorCode() == 3) {	throw new NativeIOException("No such file or directory", Errno.ENOENT);	} else {	
nativeio chmod error d s 

stat = fstat(fd);	stat.owner = getName(IdCache.USER, stat.ownerId);	stat.group = getName(IdCache.GROUP, stat.groupId);	} else {	try {	stat = fstat(fd);	} catch (NativeIOException nioe) {	if (nioe.getErrorCode() == 6) {	throw new NativeIOException("The handle is invalid.", Errno.EBADF);	} else {	
nativeio getfstat error d s 

Map<Integer, CachedName> idNameCache = (domain == IdCache.USER) ? USER_ID_NAME_CACHE : GROUP_ID_NAME_CACHE;	String name;	CachedName cachedName = idNameCache.get(id);	long now = System.currentTimeMillis();	if (cachedName != null && (cachedName.timestamp + cacheTimeout) > now) {	name = cachedName.name;	} else {	name = (domain == IdCache.USER) ? getUserName(id) : getGroupName(id);	if (LOG.isDebugEnabled()) {	String type = (domain == IdCache.USER) ? "UserName" : "GroupName";	
got for id from the native implementation 

public static boolean access(String path, AccessRight desiredAccess) throws IOException {	return access0(path, desiredAccess.accessRight());	}	public static native void extendWorkingSetSize(long delta) throws IOException;	static {	if (NativeCodeLoader.isNativeCodeLoaded()) {	try {	initNative();	nativeLoaded = true;	} catch (Throwable t) {	
unable to initialize nativeio libraries 

private static native long getMemlockLimit0();	static long getOperatingSystemPageSize() {	try {	Field f = Unsafe.class.getDeclaredField("theUnsafe");	f.setAccessible(true);	Unsafe unsafe = (Unsafe)f.get(null);	return unsafe.pageSize();	} catch (Throwable e) {	
unable to get operating system page size guessing 

owner = stripDomain(owner);	return owner;	} else {	long uid = POSIX.getUIDforFDOwnerforOwner(fd);	CachedUid cUid = uidCache.get(uid);	long now = System.currentTimeMillis();	if (cUid != null && (cUid.timestamp + cacheTimeout) > now) {	return cUid.username;	}	String user = POSIX.getUserName(uid);	
got username for uid from the native implementation 

private synchronized static void ensureInitialized() {	if (!initialized) {	cacheTimeout = new Configuration().getLong("hadoop.security.uid.cache.secs", 4*60*60) * 1000;	
initialized cache for uid to user mapping with a cache timeout of seconds 

========================= hadoop sample_3916 =========================

public void process(HistoryEvent event) {	if (finalized) {	throw new IllegalStateException( "JobBuilder.process(HistoryEvent event) called after ParsedJob built");	}	if (event instanceof AMStartedEvent) {	return;	} else if (event instanceof NormalizedResourceEvent) {	
normalizedresourceevent should be ignored in history server 

========================= hadoop sample_6647 =========================

public Configuration bindArgs(Configuration config, List<String> args) throws Exception {	Assert.assertEquals(STATE.NOTINITED, getServiceState());	for (String arg : args) {	LOG.info(arg);	}	Configuration newConf = new Configuration(config);	if (args.contains(ARG_FAILING)) {	
cli contains 

========================= hadoop sample_3150 =========================

public synchronized void setJobConf(JobConf jobConf) {	try {	super.setJob(org.apache.hadoop.mapreduce.Job.getInstance(jobConf));	} catch (IOException ioe) {	
Exception 

========================= hadoop sample_4695 =========================

final byte[] fileContents = AppendTestUtil.initBuffer(len);	{	FSDataOutputStream out = fs2.create(p, true, 4096, (short)1, len);	out.write(fileContents, 0, len);	out.close();	}	fs2.append(p);	fs1.append(p);	Assert.fail();	} catch(RemoteException re) {	
got an exception 

final byte[] fileContents = AppendTestUtil.initBuffer(len);	{	FSDataOutputStream out = fs2.create(p, true, 4096, (short)1, len);	out.write(fileContents, 0, len);	out.close();	}	((DistributedFileSystem) fs2).append(p, EnumSet.of(CreateFlag.APPEND, CreateFlag.NEW_BLOCK), 4096, null);	fs1.append(p);	Assert.fail();	} catch(RemoteException re) {	
got an exception 

========================= hadoop sample_7113 =========================

rmAppRootHierarchies = new HashMap<>(5);	rmAppRootHierarchies.put(0, rmAppRoot);	for (int splitIndex = 1; splitIndex <= 4; splitIndex++) {	rmAppRootHierarchies.put(splitIndex, getNodePath(hierarchiesPath, Integer.toString(splitIndex)));	}	fencingNodePath = getNodePath(zkRootNodePath, FENCING_LOCK);	zkSessionTimeout = conf.getInt(YarnConfiguration.RM_ZK_TIMEOUT_MS, YarnConfiguration.DEFAULT_RM_ZK_TIMEOUT_MS);	zknodeLimit = conf.getInt(YarnConfiguration.RM_ZK_ZNODE_SIZE_LIMIT_BYTES, YarnConfiguration.DEFAULT_RM_ZK_ZNODE_SIZE_LIMIT_BYTES);	appIdNodeSplitIndex = conf.getInt(YarnConfiguration.ZK_APPID_NODE_SPLIT_INDEX, YarnConfiguration.DEFAULT_ZK_APPID_NODE_SPLIT_INDEX);	if (appIdNodeSplitIndex < 0 || appIdNodeSplitIndex > 4) {	
invalid value for config specified resetting it to 

appIdNodeSplitIndex = YarnConfiguration.DEFAULT_ZK_APPID_NODE_SPLIT_INDEX;	}	zkAcl = ZKCuratorManager.getZKAcls(conf);	if (HAUtil.isHAEnabled(conf)) {	String zkRootNodeAclConf = HAUtil.getConfValueForRMInstance (YarnConfiguration.ZK_RM_STATE_STORE_ROOT_NODE_ACL, conf);	if (zkRootNodeAclConf != null) {	zkRootNodeAclConf = ZKUtil.resolveConfIndirection(zkRootNodeAclConf);	try {	zkRootNodeAcl = ZKUtil.parseACLs(zkRootNodeAclConf);	} catch (ZKUtil.BadAclFormatException bafe) {	
invalid format for 

}	dtSequenceNumberPath = getNodePath(rmDTSecretManagerRoot, RM_DT_SEQUENTIAL_NUMBER_ZNODE_NAME);	amrmTokenSecretManagerRoot = getNodePath(zkRootNodePath, AMRMTOKEN_SECRET_MANAGER_ROOT);	reservationRoot = getNodePath(zkRootNodePath, RESERVATION_SYSTEM_ROOT);	zkManager = resourceManager.getZKManager();	if(zkManager==null) {	zkManager = resourceManager.createAndStartZKManager(conf);	}	delegationTokenNodeSplitIndex = conf.getInt(YarnConfiguration.ZK_DELEGATION_TOKEN_NODE_SPLIT_INDEX, YarnConfiguration.DEFAULT_ZK_DELEGATION_TOKEN_NODE_SPLIT_INDEX);	if (delegationTokenNodeSplitIndex < 0 || delegationTokenNodeSplitIndex > 4) {	
invalid value for config specified resetting it to 

private void loadReservationSystemState(RMState rmState) throws Exception {	List<String> planNodes = getChildren(reservationRoot);	for (String planName : planNodes) {	if (LOG.isDebugEnabled()) {	
loading plan from znode 

private void loadReservationSystemState(RMState rmState) throws Exception {	List<String> planNodes = getChildren(reservationRoot);	for (String planName : planNodes) {	if (LOG.isDebugEnabled()) {	}	String planNodePath = getNodePath(reservationRoot, planName);	List<String> reservationNodes = getChildren(planNodePath);	for (String reservationNodeName : reservationNodes) {	String reservationNodePath = getNodePath(planNodePath, reservationNodeName);	if (LOG.isDebugEnabled()) {	
loading reservation from znode 

private void loadAMRMTokenSecretManagerState(RMState rmState) throws Exception {	byte[] data = getData(amrmTokenSecretManagerRoot);	if (data == null) {	
there is no data saved 

private void loadRMDelegationKeyState(RMState rmState) throws Exception {	List<String> childNodes = getChildren(dtMasterKeysRootPath);	for (String childNodeName : childNodes) {	String childNodePath = getNodePath(dtMasterKeysRootPath, childNodeName);	byte[] childData = getData(childNodePath);	if (childData == null) {	
content of is broken 

String parentNodePath = getNodePath(tokenRoot, childNodeName);	if (splitIndex == 0) {	loadDelegationTokenFromNode(rmState, parentNodePath);	} else {	List<String> leafNodes = getChildren(parentNodePath);	for (String leafNodeName : leafNodes) {	loadDelegationTokenFromNode(rmState, getNodePath(parentNodePath, leafNodeName));	}	}	} else if (splitIndex == 0 && !(childNodeName.equals("1") || childNodeName.equals("2") || childNodeName.equals("3") || childNodeName.equals("4"))) {	
unknown child node with name under 

private void loadDelegationTokenFromNode(RMState rmState, String path) throws Exception {	byte[] data = getData(path);	if (data == null) {	
content of is broken 

private void loadRMAppStateFromAppNode(RMState rmState, String appNodePath, String appIdStr) throws Exception {	byte[] appData = getData(appNodePath);	if (LOG.isDebugEnabled()) {	
loading application from znode 

loadRMAppStateFromAppNode(rmState, getNodePath(appRoot, childNodeName), childNodeName);	} else {	String parentNodePath = getNodePath(appRoot, childNodeName);	List<String> leafNodes = getChildren(parentNodePath);	for (String leafNodeName : leafNodes) {	String appIdStr = childNodeName + leafNodeName;	loadRMAppStateFromAppNode(rmState, getNodePath(parentNodePath, leafNodeName), appIdStr);	}	}	} else if (!childNodeName.equals(RM_APP_ROOT_HIERARCHIES)){	
unknown child node with name under 

private void loadApplicationAttemptState(ApplicationStateData appState, String appPath) throws Exception {	List<String> attempts = getChildren(appPath);	for (String attemptIDStr : attempts) {	if (attemptIDStr.startsWith(ApplicationAttemptId.appAttemptIdStrPrefix)) {	String attemptPath = getNodePath(appPath, attemptIDStr);	byte[] attemptData = getData(attemptPath);	ApplicationAttemptStateDataPBImpl attemptState = new ApplicationAttemptStateDataPBImpl( ApplicationAttemptStateDataProto.parseFrom(attemptData));	appState.attempts.put(attemptState.getAttemptId(), attemptState);	}	}	
done loading applications from zk state store 

private void checkRemoveParentZnode(String path, int splitIndex) throws Exception {	if (splitIndex != 0) {	String parentZnode = getSplitZnodeParent(path, splitIndex);	List<String> children = null;	try {	children = getChildren(parentZnode);	} catch (KeeperException.NoNodeException ke) {	if (LOG.isDebugEnabled()) {	
unable to remove parent node as it does not exist 

children = getChildren(parentZnode);	} catch (KeeperException.NoNodeException ke) {	if (LOG.isDebugEnabled()) {	}	return;	}	if (children != null && children.isEmpty()) {	try {	zkManager.safeDelete(parentZnode, zkAcl, fencingNodePath);	if (LOG.isDebugEnabled()) {	
no leaf znode exists removing parent node 

}	return;	}	if (children != null && children.isEmpty()) {	try {	zkManager.safeDelete(parentZnode, zkAcl, fencingNodePath);	if (LOG.isDebugEnabled()) {	}	} catch (KeeperException.NotEmptyException ke) {	if (LOG.isDebugEnabled()) {	
unable to remove app parent node as it has children 

public synchronized void storeApplicationStateInternal(ApplicationId appId, ApplicationStateData appStateDataPB) throws Exception {	String nodeCreatePath = getLeafAppIdNodePath(appId.toString(), true);	if (LOG.isDebugEnabled()) {	
storing info for app at 

public synchronized void storeApplicationStateInternal(ApplicationId appId, ApplicationStateData appStateDataPB) throws Exception {	String nodeCreatePath = getLeafAppIdNodePath(appId.toString(), true);	if (LOG.isDebugEnabled()) {	}	byte[] appStateData = appStateDataPB.getProto().toByteArray();	if (appStateData.length <= zknodeLimit) {	zkManager.safeCreate(nodeCreatePath, appStateData, zkAcl, CreateMode.PERSISTENT, zkAcl, fencingNodePath);	} else {	if (LOG.isDebugEnabled()) {	
application state data size for is 

pathExists = false;	if (appIdNodeSplitIndex != 0) {	String rootNode = getSplitZnodeParent(nodeUpdatePath, appIdNodeSplitIndex);	if (!exists(rootNode)) {	zkManager.safeCreate(rootNode, null, zkAcl, CreateMode.PERSISTENT, zkAcl, fencingNodePath);	}	}	}	}	if (LOG.isDebugEnabled()) {	
storing final state info for app at 

}	}	if (LOG.isDebugEnabled()) {	}	byte[] appStateData = appStateDataPB.getProto().toByteArray();	if (pathExists) {	zkManager.safeSetData(nodeUpdatePath, appStateData, -1, zkAcl, fencingNodePath);	} else {	zkManager.safeCreate(nodeUpdatePath, appStateData, zkAcl, CreateMode.PERSISTENT, zkAcl, fencingNodePath);	if (LOG.isDebugEnabled()) {	
path for didn t exist creating a new znode to update the application state 

} else {	throw new YarnRuntimeException("Unexpected Exception. App node for " + "app " + appId + " not found");	}	} else {	appDirPath = alternatePathInfo.path;	}	}	String path = getNodePath(appDirPath, appAttemptId.toString());	byte[] attemptStateData = (attemptStateDataPB == null) ? null : attemptStateDataPB.getProto().toByteArray();	if (LOG.isDebugEnabled()) {	
info for attempt at 

String path = getNodePath(appDirPath, appAttemptId.toString());	byte[] attemptStateData = (attemptStateDataPB == null) ? null : attemptStateDataPB.getProto().toByteArray();	if (LOG.isDebugEnabled()) {	}	switch (operation) {	case UPDATE: if (exists(path)) {	zkManager.safeSetData(path, attemptStateData, -1, zkAcl, fencingNodePath);	} else {	zkManager.safeCreate(path, attemptStateData, zkAcl, CreateMode.PERSISTENT, zkAcl, fencingNodePath);	if (LOG.isDebugEnabled()) {	
path for didn t exist created a new znode to update the application attempt state 

ZnodeSplitInfo alternatePathInfo = getAlternateAppPath(removeAppId);	if (alternatePathInfo != null) {	appIdRemovePath = alternatePathInfo.path;	splitIndex = alternatePathInfo.splitIndex;	} else {	return;	}	}	if (safeRemove) {	if (LOG.isDebugEnabled()) {	
removing info for app at and its attempts 

protected synchronized void storeRMDelegationTokenState( RMDelegationTokenIdentifier rmDTIdentifier, Long renewDate) throws Exception {	String nodeCreatePath = getLeafDelegationTokenNodePath( rmDTIdentifier.getSequenceNumber(), true);	if (LOG.isDebugEnabled()) {	
storing 

String nodeCreatePath = getLeafDelegationTokenNodePath( rmDTIdentifier.getSequenceNumber(), true);	if (LOG.isDebugEnabled()) {	}	RMDelegationTokenIdentifierData identifierData = new RMDelegationTokenIdentifierData(rmDTIdentifier, renewDate);	ByteArrayOutputStream seqOs = new ByteArrayOutputStream();	try (DataOutputStream seqOut = new DataOutputStream(seqOs)) {	SafeTransaction trx = zkManager.createTransaction(zkAcl, fencingNodePath);	trx.create(nodeCreatePath, identifierData.toByteArray(), zkAcl, CreateMode.PERSISTENT);	seqOut.writeInt(rmDTIdentifier.getSequenceNumber());	if (LOG.isDebugEnabled()) {	
storing sequencenumber 

if (!exists(nodeRemovePath)) {	ZnodeSplitInfo alternatePathInfo = getAlternateDTPath(rmDTIdentifier.getSequenceNumber());	if (alternatePathInfo != null) {	nodeRemovePath = alternatePathInfo.path;	splitIndex = alternatePathInfo.splitIndex;	} else {	return;	}	}	if (LOG.isDebugEnabled()) {	
removing rmdelegationtoken 

if (!exists(nodeUpdatePath)) {	ZnodeSplitInfo alternatePathInfo = getAlternateDTPath(rmDTIdentifier.getSequenceNumber());	if (alternatePathInfo != null) {	nodeUpdatePath = alternatePathInfo.path;	} else {	pathExists = false;	}	}	if (pathExists) {	if (LOG.isDebugEnabled()) {	
updating 

protected synchronized void storeRMDTMasterKeyState( DelegationKey delegationKey) throws Exception {	String nodeCreatePath = getNodePath(dtMasterKeysRootPath, DELEGATION_KEY_PREFIX + delegationKey.getKeyId());	if (LOG.isDebugEnabled()) {	
storing rmdelegationkey 

protected synchronized void removeRMDTMasterKeyState( DelegationKey delegationKey) throws Exception {	String nodeRemovePath = getNodePath(dtMasterKeysRootPath, DELEGATION_KEY_PREFIX + delegationKey.getKeyId());	if (LOG.isDebugEnabled()) {	
removing rmdelegationkey 

protected synchronized void removeReservationState(String planName, String reservationIdName) throws Exception {	String planNodePath = getNodePath(reservationRoot, planName);	String reservationPath = getNodePath(planNodePath, reservationIdName);	if (LOG.isDebugEnabled()) {	
removing reservationallocation for plan 

private void addOrUpdateReservationState( ReservationAllocationStateProto reservationAllocation, String planName, String reservationIdName, SafeTransaction trx, boolean isUpdate) throws Exception {	String planCreatePath = getNodePath(reservationRoot, planName);	String reservationPath = getNodePath(planCreatePath, reservationIdName);	byte[] reservationData = reservationAllocation.toByteArray();	if (!exists(planCreatePath)) {	if (LOG.isDebugEnabled()) {	
creating plan node at 

String planCreatePath = getNodePath(reservationRoot, planName);	String reservationPath = getNodePath(planCreatePath, reservationIdName);	byte[] reservationData = reservationAllocation.toByteArray();	if (!exists(planCreatePath)) {	if (LOG.isDebugEnabled()) {	}	trx.create(planCreatePath, null, zkAcl, CreateMode.PERSISTENT);	}	if (isUpdate) {	if (LOG.isDebugEnabled()) {	
updating reservation in plan at 

if (LOG.isDebugEnabled()) {	}	trx.create(planCreatePath, null, zkAcl, CreateMode.PERSISTENT);	}	if (isUpdate) {	if (LOG.isDebugEnabled()) {	}	trx.setData(reservationPath, reservationData, -1);	} else {	if (LOG.isDebugEnabled()) {	
storing reservation in plan at 

if (splitIdx == 0) {	return getNodePath(rootNode, nodeName);	}	int split = nodeName.length() - splitIdx;	String rootNodePath = getNodePath(rootNode, nodeName.substring(0, split));	if (createParentIfNotExists && !exists(rootNodePath)) {	try {	zkManager.safeCreate(rootNodePath, null, zkAcl, CreateMode.PERSISTENT, zkAcl, fencingNodePath);	} catch (KeeperException.NodeExistsException e) {	if (LOG.isDebugEnabled()) {	
unable to create app parent node as it already exists 

public void run() {	try {	while (!isFencedState()) {	zkManager.createTransaction(zkAcl, fencingNodePath).commit();	Thread.sleep(zkSessionTimeout);	}	} catch (InterruptedException ie) {	
thread interrupted exiting 

========================= hadoop sample_765 =========================

private ArrayList<String> CreateTestRequestIdentifiers( int numberOfRequests) {	ArrayList<String> testContexts = new ArrayList<String>();	
creating contexts for testing 

private ArrayList<String> CreateTestRequestIdentifiers( int numberOfRequests) {	ArrayList<String> testContexts = new ArrayList<String>();	for (int ep = 0; ep < numberOfRequests; ep++) {	testContexts.add("test-endpoint-" + Integer.toString(ep));	
created test context 

RegisterApplicationMasterResponse registerResponse = registerApplicationMaster(testAppId);	Assert.assertNotNull(registerResponse);	Assert.assertEquals(Integer.toString(testAppId), registerResponse.getQueue());	FinishApplicationMasterResponse finshResponse = finishApplicationMaster(testAppId, FinalApplicationStatus.FAILED);	Assert.assertNotNull(finshResponse);	Assert.assertEquals(false, finshResponse.getIsUnregistered());	try {	finishApplicationMaster(testAppId, FinalApplicationStatus.SUCCEEDED);	Assert .fail("The request to finish application master should have failed");	} catch (Throwable ex) {	
finish registration failed as expected because it was not registered 

public void testFinishInvalidApplicationMaster() throws Exception {	try {	finishApplicationMaster(4, FinalApplicationStatus.SUCCEEDED);	Assert .fail("The request to finish application master should have failed");	} catch (Throwable ex) {	
finish registration failed as expected because it was not registered 

for (int index = numberOfRequests - 1; index >= 0; index--) {	FinishApplicationMasterResponse finshResponse = finishApplicationMaster(index, FinalApplicationStatus.SUCCEEDED);	Assert.assertNotNull(finshResponse);	Assert.assertEquals(true, finshResponse.getIsUnregistered());	Assert.assertTrue(this.getAMRMProxyService() .getPipelines().size() == index);	}	try {	finishApplicationMaster(1, FinalApplicationStatus.SUCCEEDED);	Assert .fail("The request to finish application master should have failed");	} catch (Throwable ex) {	
finish registration failed as expected because it was not registered 

}	try {	finishApplicationMaster(1, FinalApplicationStatus.SUCCEEDED);	Assert .fail("The request to finish application master should have failed");	} catch (Throwable ex) {	}	try {	finishApplicationMaster(4, FinalApplicationStatus.SUCCEEDED);	Assert .fail("The request to finish application master should have failed");	} catch (Throwable ex) {	
finish registration failed as expected because it was not registered 

public void testFinishMulitpleApplicationMastersInParallel() throws Exception {	int numberOfRequests = 5;	ArrayList<String> testContexts = new ArrayList<String>();	
creating contexts for testing 

public void testFinishMulitpleApplicationMastersInParallel() throws Exception {	int numberOfRequests = 5;	ArrayList<String> testContexts = new ArrayList<String>();	for (int i = 0; i < numberOfRequests; i++) {	testContexts.add("test-endpoint-" + Integer.toString(i));	
created test context 

public void testAllocateRequestWithoutRegistering() throws Exception {	try {	allocate(1);	Assert .fail("The request to allocate application master should have failed");	} catch (Throwable ex) {	
allocaterequest failed as expected because am was not registered 

tempAppIds.add(new Integer(i));	}	final ArrayList<Integer> appIds = tempAppIds;	List<Integer> responses = runInParallel(appIds, new Function<Integer, Integer>() {	public Integer invoke(Integer testAppId) {	try {	RegisterApplicationMasterResponse registerResponse = registerApplicationMaster(testAppId);	Assert.assertNotNull("response is null", registerResponse);	List<Container> containers = getContainersAndAssert(testAppId, 10);	releaseContainersAndAssert(testAppId, containers);	
sucessfully registered application master with appid 

}	final ArrayList<Integer> appIds = tempAppIds;	List<Integer> responses = runInParallel(appIds, new Function<Integer, Integer>() {	public Integer invoke(Integer testAppId) {	try {	RegisterApplicationMasterResponse registerResponse = registerApplicationMaster(testAppId);	Assert.assertNotNull("response is null", registerResponse);	List<Container> containers = getContainersAndAssert(testAppId, 10);	releaseContainersAndAssert(testAppId, containers);	} catch (Throwable ex) {	
failed to register application master with appid 

AllocateResponse allocateResponse = allocate(appId, allocateRequest);	Assert.assertNotNull("allocate() returned null response", allocateResponse);	Assert.assertNull( "new AMRMToken from RM should have been nulled by AMRMProxyService", allocateResponse.getAMRMToken());	containers.addAll(allocateResponse.getAllocatedContainers());	int numHeartbeat = 0;	while (containers.size() < askList.size() && numHeartbeat++ < 10) {	allocateResponse = allocate(appId, Records.newRecord(AllocateRequest.class));	Assert.assertNotNull("allocate() returned null response", allocateResponse);	Assert.assertNull( "new AMRMToken from RM should have been nulled by AMRMProxyService", allocateResponse.getAMRMToken());	containers.addAll(allocateResponse.getAllocatedContainers());	
number of allocated containers in this request 

AllocateResponse allocateResponse = allocate(appId, allocateRequest);	Assert.assertNotNull("allocate() returned null response", allocateResponse);	Assert.assertNull( "new AMRMToken from RM should have been nulled by AMRMProxyService", allocateResponse.getAMRMToken());	containers.addAll(allocateResponse.getAllocatedContainers());	int numHeartbeat = 0;	while (containers.size() < askList.size() && numHeartbeat++ < 10) {	allocateResponse = allocate(appId, Records.newRecord(AllocateRequest.class));	Assert.assertNotNull("allocate() returned null response", allocateResponse);	Assert.assertNull( "new AMRMToken from RM should have been nulled by AMRMProxyService", allocateResponse.getAMRMToken());	containers.addAll(allocateResponse.getAllocatedContainers());	
total number of allocated containers 

Assert.assertNotNull(allocateResponse);	Assert.assertNull( "new AMRMToken from RM should have been nulled by AMRMProxyService", allocateResponse.getAMRMToken());	List<Container> containersForReleasedContainerIds = new ArrayList<Container>();	containersForReleasedContainerIds.addAll(allocateResponse .getAllocatedContainers());	int numHeartbeat = 0;	while (containersForReleasedContainerIds.size() < relList.size() && numHeartbeat++ < 10) {	allocateResponse = allocate(appId, Records.newRecord(AllocateRequest.class));	Assert.assertNotNull(allocateResponse);	Assert.assertNull( "new AMRMToken from RM should have been nulled by AMRMProxyService", allocateResponse.getAMRMToken());	containersForReleasedContainerIds.addAll(allocateResponse .getAllocatedContainers());	
number of containers received in this request 

Assert.assertNotNull(allocateResponse);	Assert.assertNull( "new AMRMToken from RM should have been nulled by AMRMProxyService", allocateResponse.getAMRMToken());	List<Container> containersForReleasedContainerIds = new ArrayList<Container>();	containersForReleasedContainerIds.addAll(allocateResponse .getAllocatedContainers());	int numHeartbeat = 0;	while (containersForReleasedContainerIds.size() < relList.size() && numHeartbeat++ < 10) {	allocateResponse = allocate(appId, Records.newRecord(AllocateRequest.class));	Assert.assertNotNull(allocateResponse);	Assert.assertNull( "new AMRMToken from RM should have been nulled by AMRMProxyService", allocateResponse.getAMRMToken());	containersForReleasedContainerIds.addAll(allocateResponse .getAllocatedContainers());	
total number of containers received 

========================= hadoop sample_1602 =========================

private void testExceptionCases(boolean resizable) {	{	final LightWeightGSet<Integer, Integer> gset = createGSet(16, resizable);	try {	gset.contains(null);	Assert.fail();	} catch(NullPointerException e) {	
good getting 

Assert.fail();	} catch(NullPointerException e) {	}	}	{	final LightWeightGSet<Integer, Integer> gset = createGSet(16, resizable);	try {	gset.get(null);	Assert.fail();	} catch(NullPointerException e) {	
good getting 

Assert.fail();	} catch(NullPointerException e) {	}	}	{	final LightWeightGSet<Integer, Integer> gset = createGSet(16, resizable);	try {	gset.put(null);	Assert.fail();	} catch(NullPointerException e) {	
good getting 

final LightWeightGSet<Integer, Integer> gset = createGSet(16, resizable);	try {	gset.put(null);	Assert.fail();	} catch(NullPointerException e) {	}	try {	gset.put(1);	Assert.fail();	} catch(IllegalArgumentException e) {	
good getting 

}	}	try {	for(IntElement i : gset) {	if (i.value == v) {	gset.remove(data[1]);	}	}	Assert.fail();	} catch(ConcurrentModificationException e) {	
good getting 

{	final GSet<IntElement, IntElement> gset = createGSet(data, resizable);	try {	for(IntElement i : gset) {	if (i.value == v) {	gset.put(data[0]);	}	}	Assert.fail();	} catch(ConcurrentModificationException e) {	
good getting 

{	final GSet<IntElement, IntElement> gset = createGSet(data, resizable);	try {	for(IntElement i : gset) {	if (i.value == v) {	gset.put(data[3]);	}	}	Assert.fail();	} catch(ConcurrentModificationException e) {	
good getting 

private static void testCapacity(long maxMemory, double percent) {	int capacity = LightWeightGSet.computeCapacity(maxMemory, percent, "map");	
validating total memory percent returned capacity 

========================= hadoop sample_2887 =========================

if (taskType != Pre21JobHistoryConstants.Values.MAP) {	LOG.warn("TaskType for a MapTask is not Map. task=" + mapTask.getTaskID() + " type=" + ((taskType == null) ? "null" : taskType.toString()));	continue;	}	List<LoggedLocation> locations = mapTask.getPreferredLocations();	List<String> hostList = new ArrayList<String>();	if (locations != null) {	for (LoggedLocation location : locations) {	List<NodeName> layers = location.getLayers();	if (layers.size() == 0) {	
bad location layer format for task 

List<LoggedLocation> locations = mapTask.getPreferredLocations();	List<String> hostList = new ArrayList<String>();	if (locations != null) {	for (LoggedLocation location : locations) {	List<NodeName> layers = location.getLayers();	if (layers.size() == 0) {	continue;	}	String host = layers.get(layers.size() - 1).getValue();	if (host == null) {	
bad location layer format for task 

if (host == null) {	continue;	}	hostList.add(host);	}	}	String[] hosts = hostList.toArray(new String[hostList.size()]);	totalHosts += hosts.length;	long mapInputBytes = getTaskInfo(mapTask).getInputBytes();	if (mapInputBytes < 0) {	
inputbytes for task is not defined 

String[] hosts = hostList.toArray(new String[hostList.size()]);	totalHosts += hosts.length;	long mapInputBytes = getTaskInfo(mapTask).getInputBytes();	if (mapInputBytes < 0) {	mapInputBytes = 0;	}	splitsList.add(new FileSplit(emptyPath, 0, mapInputBytes, hosts));	}	int totalMaps = job.getTotalMaps();	if (totalMaps < splitsList.size()) {	
totalmaps for job is less than the total number of map task descriptions 

private int sanitizeValue(int oldVal, int defaultVal, String name, JobID id) {	if (oldVal == -1) {	
not defined for 

private long sanitizeTaskRuntime(long time, ID id) {	if (time < 0) {	
negative running time for task 

========================= hadoop sample_6597 =========================

byte[] version = new byte[CHECKSUM_VERSION.length];	sums.readFully(version);	if (!Arrays.equals(version, CHECKSUM_VERSION)) {	throw new IOException("Not a checksum file: "+sumFile);	}	this.bytesPerSum = sums.readInt();	set(fs.verifyChecksum, DataChecksum.newCrc32(), bytesPerSum, 4);	} catch (FileNotFoundException e) {	set(fs.verifyChecksum, null, 1, 0);	} catch (IOException e) {	
problem opening checksum file ignoring exception 

========================= hadoop sample_4115 =========================

public DomainSocketFactory(ShortCircuitConf conf) {	final String feature;	if (conf.isShortCircuitLocalReads() && (!conf.isUseLegacyBlockReaderLocal())) {	feature = "The short-circuit local reads feature";	} else if (conf.isDomainSocketDataTraffic()) {	feature = "UNIX domain socket data traffic";	} else {	feature = null;	}	if (feature == null) {	
both short circuit local reads and unix domain socket are disabled 

} else if (conf.isDomainSocketDataTraffic()) {	feature = "UNIX domain socket data traffic";	} else {	feature = null;	}	if (feature == null) {	} else {	if (conf.getDomainSocketPath().isEmpty()) {	throw new HadoopIllegalArgumentException(feature + " is enabled but " + HdfsClientConfigKeys.DFS_DOMAIN_SOCKET_PATH_KEY + " is not set.");	} else if (DomainSocket.getLoadingFailureReason() != null) {	
cannot be used because 

feature = "UNIX domain socket data traffic";	} else {	feature = null;	}	if (feature == null) {	} else {	if (conf.getDomainSocketPath().isEmpty()) {	throw new HadoopIllegalArgumentException(feature + " is enabled but " + HdfsClientConfigKeys.DFS_DOMAIN_SOCKET_PATH_KEY + " is not set.");	} else if (DomainSocket.getLoadingFailureReason() != null) {	} else {	
is enabled 

public DomainSocket createSocket(PathInfo info, int socketTimeout) {	Preconditions.checkArgument(info.getPathState() != PathState.UNUSABLE);	boolean success = false;	DomainSocket sock = null;	try {	sock = DomainSocket.connect(info.getPath());	sock.setAttribute(DomainSocket.RECEIVE_TIMEOUT, socketTimeout);	success = true;	} catch (IOException e) {	
error creating domainsocket 

========================= hadoop sample_6981 =========================

public static void setupZooKeeper() throws Exception {	
starting zk server 

public static void setupZooKeeper() throws Exception {	ZkTmpDir = File.createTempFile("zookeeper", "test");	ZkTmpDir.delete();	ZkTmpDir.mkdir();	try {	zks = new ZooKeeperServer(ZkTmpDir, ZkTmpDir, ZooKeeperDefaultPort);	serverFactory = new NIOServerCnxnFactory();	serverFactory.configure(new InetSocketAddress(ZooKeeperDefaultPort), 10);	serverFactory.startup(zks);	} catch (Exception e) {	
exception while instantiating zookeeper 

ZkTmpDir.delete();	ZkTmpDir.mkdir();	try {	zks = new ZooKeeperServer(ZkTmpDir, ZkTmpDir, ZooKeeperDefaultPort);	serverFactory = new NIOServerCnxnFactory();	serverFactory.configure(new InetSocketAddress(ZooKeeperDefaultPort), 10);	serverFactory.startup(zks);	} catch (Exception e) {	}	boolean b = LocalBookKeeper.waitForServerUp(HOSTPORT, CONNECTION_TIMEOUT);	
zookeeper server up 

public void setup() throws Exception {	zkc = connectZooKeeper(HOSTPORT);	try {	ZKUtil.deleteRecursive(zkc, BK_ROOT_PATH);	} catch (KeeperException.NoNodeException e) {	
ignoring no node exception on cleanup 

public void setup() throws Exception {	zkc = connectZooKeeper(HOSTPORT);	try {	ZKUtil.deleteRecursive(zkc, BK_ROOT_PATH);	} catch (KeeperException.NoNodeException e) {	} catch (Exception e) {	
exception when deleting bookie root path in zk 

========================= hadoop sample_7739 =========================

MetricsRegistry r = null;	for (Field field : ReflectionUtils.getDeclaredFieldsIncludingInherited(cls)) {	if (field.getType() != MetricsRegistry.class) continue;	try {	field.setAccessible(true);	r = (MetricsRegistry) field.get(source);	hasRegistry = r != null;	break;	}	catch (Exception e) {	
error accessing field 

private void add(Object source, Field field) {	for (Annotation annotation : field.getAnnotations()) {	if (!(annotation instanceof Metric)) continue;	try {	field.setAccessible(true);	if (field.get(source) != null) continue;	}	catch (Exception e) {	
error accessing field annotated with 

========================= hadoop sample_3453 =========================

public AccumulatingReducer () {	try {	hostName = java.net.InetAddress.getLocalHost().getHostName();	} catch(Exception e) {	hostName = "localhost";	}	
starting accumulatingreducer on 

========================= hadoop sample_5587 =========================

public void testBlockListMoveToHead() throws Exception {	
blockinfo movetohead tests 

public void testBlockListMoveToHead() throws Exception {	final int MAX_BLOCKS = 10;	DatanodeStorageInfo dd = DFSTestUtil.createDatanodeStorageInfo("s1", "1.1.1.1");	ArrayList<Block> blockList = new ArrayList<Block>(MAX_BLOCKS);	ArrayList<BlockInfo> blockInfoList = new ArrayList<BlockInfo>();	int headIndex;	int curIndex;	
building block list 

ArrayList<Block> blockList = new ArrayList<Block>(MAX_BLOCKS);	ArrayList<BlockInfo> blockInfoList = new ArrayList<BlockInfo>();	int headIndex;	int curIndex;	for (int i = 0; i < MAX_BLOCKS; i++) {	blockList.add(new Block(i, 0, GenerationStamp.LAST_RESERVED_STAMP));	blockInfoList.add(new BlockInfoContiguous(blockList.get(i), (short) 3));	dd.addBlock(blockInfoList.get(i));	assertEquals("Find datanode should be 0", 0, blockInfoList.get(i) .findStorageInfo(dd));	}	
checking list length 

}	assertEquals("Length should be MAX_BLOCK", MAX_BLOCKS, dd.numBlocks());	Iterator<BlockInfo> it = dd.getBlockIterator();	int len = 0;	while (it.hasNext()) {	it.next();	len++;	}	assertEquals("There should be MAX_BLOCK blockInfo's", MAX_BLOCKS, len);	headIndex = dd.getBlockListHeadForTesting().findStorageInfo(dd);	
moving each block to the head of the list 

it.next();	len++;	}	assertEquals("There should be MAX_BLOCK blockInfo's", MAX_BLOCKS, len);	headIndex = dd.getBlockListHeadForTesting().findStorageInfo(dd);	for (int i = 0; i < MAX_BLOCKS; i++) {	curIndex = blockInfoList.get(i).findStorageInfo(dd);	headIndex = dd.moveBlockToHead(blockInfoList.get(i), curIndex, headIndex);	assertEquals("Block should be at the head of the list now.", blockInfoList.get(i), dd.getBlockListHeadForTesting());	}	
moving head to the head 

for (int i = 0; i < MAX_BLOCKS; i++) {	curIndex = blockInfoList.get(i).findStorageInfo(dd);	headIndex = dd.moveBlockToHead(blockInfoList.get(i), curIndex, headIndex);	assertEquals("Block should be at the head of the list now.", blockInfoList.get(i), dd.getBlockListHeadForTesting());	}	BlockInfo temp = dd.getBlockListHeadForTesting();	curIndex = 0;	headIndex = 0;	dd.moveBlockToHead(temp, curIndex, headIndex);	assertEquals( "Moving head to the head of the list shopuld not change the list", temp, dd.getBlockListHeadForTesting());	
checking elements of the list 

headIndex = 0;	dd.moveBlockToHead(temp, curIndex, headIndex);	assertEquals( "Moving head to the head of the list shopuld not change the list", temp, dd.getBlockListHeadForTesting());	temp = dd.getBlockListHeadForTesting();	assertNotNull("Head should not be null", temp);	int c = MAX_BLOCKS - 1;	while (temp != null) {	assertEquals("Expected element is not on the list", blockInfoList.get(c--), temp);	temp = temp.getNext(0);	}	
moving random blocks to the head of the list 

========================= hadoop sample_7555 =========================

public AuthenticationToken alternateAuthenticate(HttpServletRequest request, HttpServletResponse response) throws IOException, AuthenticationException {	AuthenticationToken token = null;	String serializedJWT = null;	HttpServletRequest req = (HttpServletRequest) request;	serializedJWT = getJWTFromCookie(req);	if (serializedJWT == null) {	String loginURL = constructLoginURL(request);	
sending redirect to 

((HttpServletResponse) response).sendRedirect(loginURL);	} else {	String userName = null;	SignedJWT jwtToken = null;	boolean valid = false;	try {	jwtToken = SignedJWT.parse(serializedJWT);	valid = validateToken(jwtToken);	if (valid) {	userName = jwtToken.getJWTClaimsSet().getSubject();	
username 

} else {	String userName = null;	SignedJWT jwtToken = null;	boolean valid = false;	try {	jwtToken = SignedJWT.parse(serializedJWT);	valid = validateToken(jwtToken);	if (valid) {	userName = jwtToken.getJWTClaimsSet().getSubject();	} else {	
jwttoken failed validation 

SignedJWT jwtToken = null;	boolean valid = false;	try {	jwtToken = SignedJWT.parse(serializedJWT);	valid = validateToken(jwtToken);	if (valid) {	userName = jwtToken.getJWTClaimsSet().getSubject();	} else {	}	} catch(ParseException pe) {	
unable to parse the jwt token 

try {	jwtToken = SignedJWT.parse(serializedJWT);	valid = validateToken(jwtToken);	if (valid) {	userName = jwtToken.getJWTClaimsSet().getSubject();	} else {	}	} catch(ParseException pe) {	}	if (valid) {	
issuing authenticationtoken for user 

if (valid) {	userName = jwtToken.getJWTClaimsSet().getSubject();	} else {	}	} catch(ParseException pe) {	}	if (valid) {	token = new AuthenticationToken(userName, userName, getType());	} else {	String loginURL = constructLoginURL(request);	
token validation failed sending redirect to 

protected String getJWTFromCookie(HttpServletRequest req) {	String serializedJWT = null;	Cookie[] cookies = req.getCookies();	if (cookies != null) {	for (Cookie cookie : cookies) {	if (cookieName.equals(cookie.getName())) {	
cookie has been found and is being processed 

protected boolean validateToken(SignedJWT jwtToken) {	boolean sigValid = validateSignature(jwtToken);	if (!sigValid) {	
signature could not be verified 

protected boolean validateToken(SignedJWT jwtToken) {	boolean sigValid = validateSignature(jwtToken);	if (!sigValid) {	}	boolean audValid = validateAudiences(jwtToken);	if (!audValid) {	
audience validation failed 

protected boolean validateToken(SignedJWT jwtToken) {	boolean sigValid = validateSignature(jwtToken);	if (!sigValid) {	}	boolean audValid = validateAudiences(jwtToken);	if (!audValid) {	}	boolean expValid = validateExpiration(jwtToken);	if (!expValid) {	
expiration validation failed 

protected boolean validateSignature(SignedJWT jwtToken) {	boolean valid = false;	if (JWSObject.State.SIGNED == jwtToken.getState()) {	
jwt token is in a signed state 

protected boolean validateSignature(SignedJWT jwtToken) {	boolean valid = false;	if (JWSObject.State.SIGNED == jwtToken.getState()) {	if (jwtToken.getSignature() != null) {	
jwt token signature is not null 

protected boolean validateSignature(SignedJWT jwtToken) {	boolean valid = false;	if (JWSObject.State.SIGNED == jwtToken.getState()) {	if (jwtToken.getSignature() != null) {	try {	JWSVerifier verifier = new RSASSAVerifier(publicKey);	if (jwtToken.verify(verifier)) {	valid = true;	
jwt token has been successfully verified 

protected boolean validateSignature(SignedJWT jwtToken) {	boolean valid = false;	if (JWSObject.State.SIGNED == jwtToken.getState()) {	if (jwtToken.getSignature() != null) {	try {	JWSVerifier verifier = new RSASSAVerifier(publicKey);	if (jwtToken.verify(verifier)) {	valid = true;	} else {	
jwt signature verification failed 

boolean valid = false;	if (JWSObject.State.SIGNED == jwtToken.getState()) {	if (jwtToken.getSignature() != null) {	try {	JWSVerifier verifier = new RSASSAVerifier(publicKey);	if (jwtToken.verify(verifier)) {	valid = true;	} else {	}	} catch (JOSEException je) {	
error while validating signature 

protected boolean validateAudiences(SignedJWT jwtToken) {	boolean valid = false;	try {	List<String> tokenAudienceList = jwtToken.getJWTClaimsSet() .getAudience();	if (audiences == null) {	valid = true;	} else {	boolean found = false;	for (String aud : tokenAudienceList) {	if (audiences.contains(aud)) {	
jwt token audience has been successfully validated 

valid = true;	} else {	boolean found = false;	for (String aud : tokenAudienceList) {	if (audiences.contains(aud)) {	valid = true;	break;	}	}	if (!valid) {	
jwt audience validation failed 

for (String aud : tokenAudienceList) {	if (audiences.contains(aud)) {	valid = true;	break;	}	}	if (!valid) {	}	}	} catch (ParseException pe) {	
unable to parse the jwt token 

protected boolean validateExpiration(SignedJWT jwtToken) {	boolean valid = false;	try {	Date expires = jwtToken.getJWTClaimsSet().getExpirationTime();	if (expires == null || new Date().before(expires)) {	
jwt token expiration date has been successfully validated 

protected boolean validateExpiration(SignedJWT jwtToken) {	boolean valid = false;	try {	Date expires = jwtToken.getJWTClaimsSet().getExpirationTime();	if (expires == null || new Date().before(expires)) {	valid = true;	} else {	
jwt expiration date validation failed 

protected boolean validateExpiration(SignedJWT jwtToken) {	boolean valid = false;	try {	Date expires = jwtToken.getJWTClaimsSet().getExpirationTime();	if (expires == null || new Date().before(expires)) {	valid = true;	} else {	}	} catch (ParseException pe) {	
jwt expiration date validation failed 

========================= hadoop sample_2757 =========================

return Channels.pipeline(RpcUtil.constructRpcFrameDecoder(), RpcUtil.STAGE_RPC_MESSAGE_PARSER, rpcProgram, RpcUtil.STAGE_RPC_TCP_RESPONSE);	}	});	server.setOption("child.tcpNoDelay", true);	server.setOption("child.keepAlive", true);	server.setOption("child.reuseAddress", true);	server.setOption("reuseAddress", true);	ch = server.bind(new InetSocketAddress(port));	InetSocketAddress socketAddr = (InetSocketAddress) ch.getLocalAddress();	boundPort = socketAddr.getPort();	
started listening to tcp requests at port for with workercount 

========================= hadoop sample_4379 =========================

private void awaitResponseTimeout(FutureTask<Writable> future) throws ExecutionException, InterruptedException {	long sleepTime = 3000L;	while (sleepTime > 0) {	try {	future.get(200L, TimeUnit.MILLISECONDS);	Assert.fail("Expected to timeout since" + " the deferred response hasn't been registered");	} catch (TimeoutException e) {	}	sleepTime -= 200L;	}	
done sleeping 

========================= hadoop sample_3146 =========================

public void close() throws IOException {	try {	sock.close();	} catch (IOException e) {	
error closing domainpeerserver 

========================= hadoop sample_7806 =========================

public boolean putMetricsImmediate(MetricsBuffer buffer) {	WaitableMetricsBuffer waitableBuffer = new WaitableMetricsBuffer(buffer);	if (queue.enqueue(waitableBuffer)) {	refreshQueueSizeGauge();	} else {	
has a full queue and can t consume the given metrics 

public boolean putMetricsImmediate(MetricsBuffer buffer) {	WaitableMetricsBuffer waitableBuffer = new WaitableMetricsBuffer(buffer);	if (queue.enqueue(waitableBuffer)) {	refreshQueueSizeGauge();	} else {	dropped.incr();	return false;	}	if (!waitableBuffer.waitTillNotified(oobPutTimeout)) {	
couldn t fulfill an immediate putmetrics request in time abandoning 

public void consume(MetricsBuffer buffer) {	long ts = 0;	for (MetricsBuffer.Entry entry : buffer) {	if (sourceFilter == null || sourceFilter.accepts(entry.name())) {	for (MetricsRecordImpl record : entry.records()) {	if ((context == null || context.equals(record.context())) && (recordFilter == null || recordFilter.accepts(record))) {	if (LOG.isDebugEnabled()) {	
pushing record to 

}	}	}	if (ts > 0) {	sink.flush();	latency.add(Time.now() - ts);	}	if (buffer instanceof WaitableMetricsBuffer) {	((WaitableMetricsBuffer)buffer).notifyAnyWaiters();	}	
Done 

========================= hadoop sample_3392 =========================

public void registerApplicationMaster( ApplicationAttemptId applicationAttemptId, RegisterApplicationMasterRequest request, RegisterApplicationMasterResponse response) throws IOException {	RMApp app = getRmContext().getRMApps().get( applicationAttemptId.getApplicationId());	
am registration 

public void registerApplicationMaster( ApplicationAttemptId applicationAttemptId, RegisterApplicationMasterRequest request, RegisterApplicationMasterResponse response) throws IOException {	RMApp app = getRmContext().getRMApps().get( applicationAttemptId.getApplicationId());	getRmContext().getDispatcher().getEventHandler() .handle( new RMAppAttemptRegistrationEvent(applicationAttemptId, request .getHost(), request.getRpcPort(), request.getTrackingUrl()));	RMAuditLogger.logSuccess(app.getUser(), RMAuditLogger.AuditConstants.REGISTER_AM, "ApplicationMasterService", app.getApplicationId(), applicationAttemptId);	response.setMaximumResourceCapability(getScheduler() .getMaximumResourceCapability(app.getQueue()));	response.setApplicationACLs(app.getRMAppAttempt(applicationAttemptId) .getSubmissionContext().getAMContainerSpec().getApplicationACLs());	response.setQueue(app.getQueue());	if (UserGroupInformation.isSecurityEnabled()) {	
setting client token master key 

if (null != token) {	nmTokens.add(token);	}	} catch (IllegalArgumentException e) {	if (e.getCause() instanceof UnknownHostException) {	throw (UnknownHostException) e.getCause();	}	}	}	response.setNMTokensFromPreviousAttempts(nmTokens);	
application retrieved containers from previous attempts and nm tokens 

ApplicationSubmissionContext asc = app.getApplicationSubmissionContext();	for (ResourceRequest req : ask) {	if (null == req.getNodeLabelExpression() && ResourceRequest.ANY.equals(req.getResourceName())) {	req.setNodeLabelExpression(asc.getNodeLabelExpression());	}	}	Resource maximumCapacity = getScheduler().getMaximumResourceCapability();	try {	RMServerUtils.normalizeAndValidateRequests(ask, maximumCapacity, app.getQueue(), getScheduler(), getRmContext());	} catch (InvalidResourceRequestException e) {	
invalid resource ask by application 

}	Resource maximumCapacity = getScheduler().getMaximumResourceCapability();	try {	RMServerUtils.normalizeAndValidateRequests(ask, maximumCapacity, app.getQueue(), getScheduler(), getRmContext());	} catch (InvalidResourceRequestException e) {	throw e;	}	try {	RMServerUtils.validateBlacklistRequest(blacklistRequest);	}  catch (InvalidResourceBlacklistRequestException e) {	
invalid blacklist request by application 

}	try {	RMServerUtils.validateBlacklistRequest(blacklistRequest);	}  catch (InvalidResourceBlacklistRequestException e) {	throw e;	}	if (!app.getApplicationSubmissionContext() .getKeepContainersAcrossApplicationAttempts()) {	try {	RMServerUtils.validateContainerReleaseRequest(release, appAttemptId);	} catch (InvalidContainerReleaseException e) {	
invalid container release by application 

RMServerUtils.validateContainerReleaseRequest(release, appAttemptId);	} catch (InvalidContainerReleaseException e) {	throw e;	}	}	List<UpdateContainerError> updateErrors = new ArrayList<>();	ContainerUpdates containerUpdateRequests = RMServerUtils.validateAndSplitUpdateResourceRequests( getRmContext(), request, maximumCapacity, updateErrors);	Allocation allocation;	RMAppAttemptState state = app.getRMAppAttempt(appAttemptId).getAppAttemptState();	if (state.equals(RMAppAttemptState.FINAL_SAVING) || state.equals(RMAppAttemptState.FINISHING) || app.isAppFinalStateStored()) {	
is in state ignore container allocate request 

List<UpdateContainerError> updateErrors = new ArrayList<>();	ContainerUpdates containerUpdateRequests = RMServerUtils.validateAndSplitUpdateResourceRequests( getRmContext(), request, maximumCapacity, updateErrors);	Allocation allocation;	RMAppAttemptState state = app.getRMAppAttempt(appAttemptId).getAppAttemptState();	if (state.equals(RMAppAttemptState.FINAL_SAVING) || state.equals(RMAppAttemptState.FINISHING) || app.isAppFinalStateStored()) {	allocation = EMPTY_ALLOCATION;	} else {	allocation = getScheduler().allocate(appAttemptId, ask, release, blacklistAdditions, blacklistRemovals, containerUpdateRequests);	}	if (!blacklistAdditions.isEmpty() || !blacklistRemovals.isEmpty()) {	
blacklist are updated in scheduler blacklistadditions blacklistremovals 

========================= hadoop sample_698 =========================

private IOStreamPair getEncryptedStreams(InetAddress addr, OutputStream underlyingOut, InputStream underlyingIn, DataEncryptionKey encryptionKey) throws IOException {	Map<String, String> saslProps = createSaslPropertiesForEncryption( encryptionKey.encryptionAlgorithm);	
client using encryption algorithm 

SaslResponseWithNegotiatedCipherOption response = readSaslMessageAndNegotiatedCipherOption(in);	localResponse = sasl.evaluateChallengeOrResponse(response.payload);	assert localResponse == null;	checkSaslComplete(sasl, saslProps);	CipherOption cipherOption = null;	if (sasl.isNegotiatedQopPrivacy()) {	cipherOption = unwrap(response.cipherOption, sasl);	if (LOG.isDebugEnabled()) {	if (cipherOption == null) {	if (cipherSuites != null && !cipherSuites.isEmpty()) {	
client accepts cipher suites but server does not accept any of them 

assert localResponse == null;	checkSaslComplete(sasl, saslProps);	CipherOption cipherOption = null;	if (sasl.isNegotiatedQopPrivacy()) {	cipherOption = unwrap(response.cipherOption, sasl);	if (LOG.isDebugEnabled()) {	if (cipherOption == null) {	if (cipherSuites != null && !cipherSuites.isEmpty()) {	}	} else {	
client using cipher suite with server 

========================= hadoop sample_6939 =========================

public static DataChecksum readDataChecksum(final DataInputStream metaIn, final Object name) throws IOException {	final BlockMetadataHeader header = readHeader(metaIn);	if (header.getVersion() != VERSION) {	
unexpected meta file version for version in file is but expected version is 

========================= hadoop sample_6893 =========================

assertThat(locations.get(5).getUri(), is(dir5.toURI()));	assertThat(locations.get(6).getStorageType(), is(StorageType.DISK));	assertThat(locations.get(6).getUri(), is(dir6.toURI()));	assertThat(locations.get(7).getStorageType(), is(StorageType.DISK));	String locations2 = "[BadMediaType]/dir0,[ssd]/dir1,[disk]/dir2";	conf.set(DFS_DATANODE_DATA_DIR_KEY, locations2);	try {	locations = DataNode.getStorageLocations(conf);	fail();	} catch (IllegalArgumentException iae) {	
the exception is expected 

========================= hadoop sample_7294 =========================

private static JSONJAXBContext initContext() {	try {	return new JSONJAXBContext(JSONConfiguration.DEFAULT, WeightedPolicyInfo.class);	} catch (JAXBException e) {	
error parsing the policy 

========================= hadoop sample_1340 =========================

}	).start();	try {	GenericTestUtils.waitFor(new Supplier<Boolean>() {	public Boolean get() {	return bootStrapped.get();	}	}, 50, timeOut);	fail("Did not timeout");	} catch (TimeoutException e) {	
encountered expected timeout 

private void removeStandbyNameDirs() {	for (URI u : cluster.getNameDirs(1)) {	assertTrue(u.getScheme().equals("file"));	File dir = new File(u.getPath());	
removing standby dir 

========================= hadoop sample_7447 =========================

if (count == positions.length) {	positions = Arrays.copyOf(positions, positions.length * 2);	}	keyBuilder.add(k);	positions[count] = position.get();	count++;	}	this.keys = keyBuilder.toArray(new WritableComparable[count]);	positions = Arrays.copyOf(positions, count);	} catch (EOFException e) {	
unexpected eof reading at entry ignoring 

========================= hadoop sample_3957 =========================

public void testPurgeEntryCuratorCallback() throws Throwable {	String path = "/users/example/hbase/hbase1/";	ServiceRecord written = buildExampleServiceEntry( PersistencePolicies.APPLICATION_ATTEMPT);	written.set(YarnRegistryAttributes.YARN_ID, "testAsyncPurgeEntry_attempt_001");	operations.mknode(RegistryPathUtils.parentOf(path), true);	operations.bind(path, written, 0);	ZKPathDumper dump = registry.dumpPath(false);	CuratorEventCatcher events = new CuratorEventCatcher();	
initial state 

operations.mknode(RegistryPathUtils.parentOf(path), true);	operations.bind(path, written, 0);	ZKPathDumper dump = registry.dumpPath(false);	CuratorEventCatcher events = new CuratorEventCatcher();	String id = written.get(YarnRegistryAttributes.YARN_ID, "");	int opcount = purge("/", id, PersistencePolicies.CONTAINER, RegistryAdminService.PurgePolicy.PurgeAll, events);	assertPathExists(path);	assertEquals(0, opcount);	assertEquals("Event counter", 0, events.getCount());	opcount = purge("/", id, PersistencePolicies.APPLICATION_ATTEMPT, RegistryAdminService.PurgePolicy.PurgeAll, events);	
final state 

public void testAsyncPurgeEntry() throws Throwable {	String path = "/users/example/hbase/hbase1/";	ServiceRecord written = buildExampleServiceEntry( PersistencePolicies.APPLICATION_ATTEMPT);	written.set(YarnRegistryAttributes.YARN_ID, "testAsyncPurgeEntry_attempt_001");	operations.mknode(RegistryPathUtils.parentOf(path), true);	operations.bind(path, written, 0);	ZKPathDumper dump = registry.dumpPath(false);	
initial state 

ZKPathDumper dump = registry.dumpPath(false);	DeleteCompletionCallback deletions = new DeleteCompletionCallback();	int opcount = purge("/", written.get(YarnRegistryAttributes.YARN_ID, ""), PersistencePolicies.CONTAINER, RegistryAdminService.PurgePolicy.PurgeAll, deletions);	assertPathExists(path);	dump = registry.dumpPath(false);	assertEquals("wrong no of delete operations in " + dump, 0, deletions.getEventCount());	assertEquals("wrong no of delete operations in " + dump, 0, opcount);	deletions = new DeleteCompletionCallback();	opcount = purge("/", written.get(YarnRegistryAttributes.YARN_ID, ""), PersistencePolicies.APPLICATION_ATTEMPT, RegistryAdminService.PurgePolicy.PurgeAll, deletions);	dump = registry.dumpPath(false);	
final state 

========================= hadoop sample_2645 =========================

Mapper<Text, CopyListingFileStatus, Text, Text>.Context context = stubContext.getContext();	mkdirs(SOURCE_PATH + "/src/file");	touchFile(TARGET_PATH + "/src/file");	try {	copyMapper.setup(context);	copyMapper.map(new Text("/src/file"), new CopyListingFileStatus(fs.getFileStatus( new Path(SOURCE_PATH + "/src/file"))), context);	} catch (IOException e) {	Assert.assertTrue(e.getMessage().startsWith("Can't replace"));	}	} catch (Exception e) {	
exception encountered 

deleteState();	createSourceData();	UserGroupInformation tmpUser = UserGroupInformation.createRemoteUser("guest");	final CopyMapper copyMapper = new CopyMapper();	final Mapper<Text, CopyListingFileStatus, Text, Text>.Context context = tmpUser.doAs( new PrivilegedAction<Mapper<Text, CopyListingFileStatus, Text, Text>.Context>() {	public Mapper<Text, CopyListingFileStatus, Text, Text>.Context run() {	try {	StubContext stubContext = new StubContext(getConfiguration(), null, 0);	return stubContext.getContext();	} catch (Exception e) {	
exception encountered 

preserveStatus.remove(DistCpOptions.FileAttribute.XATTR);	context.getConfiguration().set(DistCpConstants.CONF_LABEL_PRESERVE_STATUS, DistCpUtils.packAttributes(preserveStatus));	touchFile(SOURCE_PATH + "/src/file");	mkdirs(TARGET_PATH);	cluster.getFileSystem().setPermission(new Path(TARGET_PATH), new FsPermission((short)511));	final FileSystem tmpFS = tmpUser.doAs(new PrivilegedAction<FileSystem>() {	public FileSystem run() {	try {	return FileSystem.get(configuration);	} catch (IOException e) {	
exception encountered 

Assert.fail("Expected copy to fail");	} catch (AccessControlException e) {	Assert.assertTrue("Got exception: " + e.getMessage(), true);	} catch (Exception e) {	throw new RuntimeException(e);	}	return null;	}	});	} catch (Exception e) {	
exception encountered 

deleteState();	createSourceData();	UserGroupInformation tmpUser = UserGroupInformation.createRemoteUser("guest");	final CopyMapper copyMapper = new CopyMapper();	final Mapper<Text, CopyListingFileStatus, Text, Text>.Context context = tmpUser.doAs( new PrivilegedAction<Mapper<Text, CopyListingFileStatus, Text, Text>.Context>() {	public Mapper<Text, CopyListingFileStatus, Text, Text>.Context run() {	try {	StubContext stubContext = new StubContext(getConfiguration(), null, 0);	return stubContext.getContext();	} catch (Exception e) {	
exception encountered 

});	touchFile(SOURCE_PATH + "/src/file");	mkdirs(TARGET_PATH);	cluster.getFileSystem().setPermission(new Path(SOURCE_PATH + "/src/file"), new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ));	cluster.getFileSystem().setPermission(new Path(TARGET_PATH), new FsPermission((short)511));	final FileSystem tmpFS = tmpUser.doAs(new PrivilegedAction<FileSystem>() {	public FileSystem run() {	try {	return FileSystem.get(configuration);	} catch (IOException e) {	
exception encountered 

try {	copyMapper.setup(context);	copyMapper.map(new Text("/src/file"), new CopyListingFileStatus(tmpFS.getFileStatus( new Path(SOURCE_PATH + "/src/file"))), context);	} catch (Exception e) {	throw new RuntimeException(e);	}	return null;	}	});	} catch (Exception e) {	
exception encountered 

try {	deleteState();	createSourceData();	UserGroupInformation tmpUser = UserGroupInformation.createRemoteUser("guest");	final CopyMapper copyMapper = new CopyMapper();	final StubContext stubContext =  tmpUser. doAs(new PrivilegedAction<StubContext>() {	public StubContext run() {	try {	return new StubContext(getConfiguration(), null, 0);	} catch (Exception e) {	
exception encountered 

context.getConfiguration().set(DistCpConstants.CONF_LABEL_PRESERVE_STATUS, DistCpUtils.packAttributes(preserveStatus));	touchFile(SOURCE_PATH + "/src/file");	touchFile(TARGET_PATH + "/src/file");	cluster.getFileSystem().setPermission(new Path(SOURCE_PATH + "/src/file"), new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ));	cluster.getFileSystem().setPermission(new Path(TARGET_PATH + "/src/file"), new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ));	final FileSystem tmpFS = tmpUser.doAs(new PrivilegedAction<FileSystem>() {	public FileSystem run() {	try {	return FileSystem.get(configuration);	} catch (IOException e) {	
exception encountered 

Assert.assertEquals(stubContext.getWriter().values().size(), 1);	Assert.assertTrue(stubContext.getWriter().values().get(0).toString().startsWith("SKIP"));	Assert.assertTrue(stubContext.getWriter().values().get(0).toString(). contains(SOURCE_PATH + "/src/file"));	} catch (Exception e) {	throw new RuntimeException(e);	}	return null;	}	});	} catch (Exception e) {	
exception encountered 

try {	deleteState();	createSourceData();	UserGroupInformation tmpUser = UserGroupInformation.createRemoteUser("guest");	final CopyMapper copyMapper = new CopyMapper();	final StubContext stubContext =  tmpUser. doAs(new PrivilegedAction<StubContext>() {	public StubContext run() {	try {	return new StubContext(getConfiguration(), null, 0);	} catch (Exception e) {	
exception encountered 

OutputStream out = cluster.getFileSystem().create(new Path(TARGET_PATH + "/src/file"));	out.write("hello world".getBytes());	out.close();	cluster.getFileSystem().setPermission(new Path(SOURCE_PATH + "/src/file"), new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ));	cluster.getFileSystem().setPermission(new Path(TARGET_PATH + "/src/file"), new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ));	final FileSystem tmpFS = tmpUser.doAs(new PrivilegedAction<FileSystem>() {	public FileSystem run() {	try {	return FileSystem.get(configuration);	} catch (IOException e) {	
exception encountered 

} catch (AccessControlException ignore) {	} catch (Exception e) {	if (e.getCause() == null || e.getCause().getCause() == null || !(e.getCause().getCause() instanceof AccessControlException)) {	throw new RuntimeException(e);	}	}	return null;	}	});	} catch (Exception e) {	
exception encountered 

Mapper<Text, CopyListingFileStatus, Text, Text>.Context context = stubContext.getContext();	touchFile(SOURCE_PATH + "/src/file");	mkdirs(TARGET_PATH + "/src/file");	try {	copyMapper.setup(context);	copyMapper.map(new Text("/src/file"), new CopyListingFileStatus(fs.getFileStatus( new Path(SOURCE_PATH + "/src/file"))), context);	} catch (IOException e) {	Assert.assertTrue(e.getMessage().startsWith("Can't replace"));	}	} catch (Exception e) {	
exception encountered 

deleteState();	createSourceData();	final UserGroupInformation tmpUser = UserGroupInformation .createRemoteUser("guest");	final CopyMapper copyMapper = new CopyMapper();	final Mapper<Text, CopyListingFileStatus, Text, Text>.Context context = tmpUser.doAs(new PrivilegedAction< Mapper<Text, CopyListingFileStatus, Text, Text>.Context>() {	public Mapper<Text, CopyListingFileStatus, Text, Text>.Context run() {	try {	StubContext stubContext = new StubContext( getConfiguration(), null, 0);	return stubContext.getContext();	} catch (Exception e) {	
exception encountered when get stub context 

touchFile(SOURCE_PATH + "/src/file");	mkdirs(TARGET_PATH);	cluster.getFileSystem().setPermission(new Path(SOURCE_PATH + "/src/file"), new FsPermission(FsAction.NONE, FsAction.NONE, FsAction.NONE));	cluster.getFileSystem().setPermission(new Path(TARGET_PATH), new FsPermission((short)511));	context.getConfiguration().setBoolean( DistCpOptionSwitch.IGNORE_FAILURES.getConfigLabel(), ignoreFailures);	final FileSystem tmpFS = tmpUser.doAs(new PrivilegedAction<FileSystem>() {	public FileSystem run() {	try {	return FileSystem.get(configuration);	} catch (IOException e) {	
exception encountered when get filesystem 

}	}	});	tmpUser.doAs(new PrivilegedAction<Integer>() {	public Integer run() {	try {	copyMapper.setup(context);	copyMapper.map(new Text("/src/file"), new CopyListingFileStatus(tmpFS.getFileStatus( new Path(SOURCE_PATH + "/src/file"))), context);	Assert.assertTrue("Should have thrown an IOException if not " + "ignoring failures", ignoreFailures);	} catch (IOException e) {	
unexpected exception encountered 

});	tmpUser.doAs(new PrivilegedAction<Integer>() {	public Integer run() {	try {	copyMapper.setup(context);	copyMapper.map(new Text("/src/file"), new CopyListingFileStatus(tmpFS.getFileStatus( new Path(SOURCE_PATH + "/src/file"))), context);	Assert.assertTrue("Should have thrown an IOException if not " + "ignoring failures", ignoreFailures);	} catch (IOException e) {	Assert.assertFalse("Should not have thrown an IOException if " + "ignoring failures", ignoreFailures);	} catch (Exception e) {	
exception encountered when the mapper copies file 

Assert.assertTrue("Should have thrown an IOException if not " + "ignoring failures", ignoreFailures);	} catch (IOException e) {	Assert.assertFalse("Should not have thrown an IOException if " + "ignoring failures", ignoreFailures);	} catch (Exception e) {	throw new RuntimeException(e);	}	return null;	}	});	} catch (Exception e) {	
unexpected exception encountered 

========================= hadoop sample_6256 =========================

private Reconfigurable getReconfigurable(HttpServletRequest req) {	
servlet path 

private Reconfigurable getReconfigurable(HttpServletRequest req) {	
getting attribute 

out.println("<p>Changed \"" + StringEscapeUtils.escapeHtml(param) + "\" from \"" + StringEscapeUtils.escapeHtml(oldConf.getRaw(param)) + "\" to default</p>");	reconf.reconfigureProperty(param, null);	} else if (!value.equals("default") && !value.equals("null") && !value.isEmpty() && (oldConf.getRaw(param) == null || !oldConf.getRaw(param).equals(value))) {	if (oldConf.getRaw(param) == null) {	out.println("<p>Changed \"" + StringEscapeUtils.escapeHtml(param) + "\" from default to \"" + StringEscapeUtils.escapeHtml(value) + "\"</p>");	} else {	out.println("<p>Changed \"" + StringEscapeUtils.escapeHtml(param) + "\" from \"" + StringEscapeUtils.escapeHtml(oldConf. getRaw(param)) + "\" to \"" + StringEscapeUtils.escapeHtml(value) + "\"</p>");	}	reconf.reconfigureProperty(param, value);	} else {	
property unchanged 

protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException {	
GET 

protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException {	
POST 

========================= hadoop sample_4284 =========================

public void testActiveStandbyTransition() throws Exception {	
starting test with parentdir 

========================= hadoop sample_3111 =========================

public static void closeSocket(Socket sock) {	if (sock != null) {	try {	sock.close();	} catch (IOException ignored) {	
ignoring exception while closing socket 

========================= hadoop sample_3812 =========================

o.write(23);	o.close();	hdfs.setAcl(file, Lists.newArrayList( aclEntry(ACCESS, USER, READ_WRITE), aclEntry(ACCESS, USER, "foo", READ_WRITE), aclEntry(ACCESS, USER, "bar", READ), aclEntry(ACCESS, GROUP, READ), aclEntry(ACCESS, GROUP, "group", READ), aclEntry(ACCESS, OTHER, NONE)));	writtenAcls.put(file.toString(), hdfs.getAclStatus(file));	hdfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_ENTER, false);	hdfs.saveNamespace();	originalFsimage = FSImageTestUtil.findLatestImageFile(FSImageTestUtil .getFSImage(cluster.getNameNode()).getStorage().getStorageDir(0));	if (originalFsimage == null) {	throw new RuntimeException("Didn't generate or can't find fsimage");	}	
original fs image file is 

========================= hadoop sample_7125 =========================

public FileHandle(String s) {	MessageDigest digest;	try {	digest = MessageDigest.getInstance("MD5");	handle = new byte[HANDLE_LEN];	} catch (NoSuchAlgorithmException e) {	
messagedigest unavailable 

========================= hadoop sample_4359 =========================

} catch (ClosedChannelException e) {	return;	}	boolean didShutdown = false;	boolean interrupted = false;	while (count > 0) {	if (!didShutdown) {	try {	shutdown0(fd);	} catch (IOException e) {	
shutdown error 

========================= hadoop sample_3536 =========================

do {	nowMs = timer.monotonicNow();	localTimeStampOfLastReadLockReport = timeStampOfLastReadLockReportMs.get();	if (nowMs - localTimeStampOfLastReadLockReport < lockSuppressWarningIntervalMs) {	numReadLockWarningsSuppressed.incrementAndGet();	return;	}	} while (!timeStampOfLastReadLockReportMs.compareAndSet( localTimeStampOfLastReadLockReport, nowMs));	int numSuppressedWarnings = numReadLockWarningsSuppressed.getAndSet(0);	long longestLockHeldIntervalMs = longestReadLockHeldIntervalMs.getAndSet(0);	
fsnamesystem read lock held for ms via tnumber of suppressed read lock reports tlongest read lock held interval 

timeStampOfLastWriteLockReportMs = currentTimeMs;	} else {	numWriteLockWarningsSuppressed++;	}	}	coarseLock.writeLock().unlock();	if (needReport) {	addMetric(opName, writeLockIntervalNanos, true);	}	if (logReport) {	
fsnamesystem write lock held for ms via tnumber of suppressed write lock reports tlongest write lock held interval 

========================= hadoop sample_7963 =========================

public void waitFor(Supplier<Boolean> check, int checkEveryMillis, int logInterval) throws InterruptedException {	Preconditions.checkNotNull(check, "check should not be null");	Preconditions.checkArgument(checkEveryMillis >= 0, "checkEveryMillis should be positive value");	Preconditions.checkArgument(logInterval >= 0, "logInterval should be positive value");	int loggingCounter = logInterval;	do {	if (LOG.isDebugEnabled()) {	
check the condition for main loop 

public void waitFor(Supplier<Boolean> check, int checkEveryMillis, int logInterval) throws InterruptedException {	Preconditions.checkNotNull(check, "check should not be null");	Preconditions.checkArgument(checkEveryMillis >= 0, "checkEveryMillis should be positive value");	Preconditions.checkArgument(logInterval >= 0, "logInterval should be positive value");	int loggingCounter = logInterval;	do {	if (LOG.isDebugEnabled()) {	}	boolean result = check.get();	if (result) {	
exits the main loop 

Preconditions.checkArgument(logInterval >= 0, "logInterval should be positive value");	int loggingCounter = logInterval;	do {	if (LOG.isDebugEnabled()) {	}	boolean result = check.get();	if (result) {	return;	}	if (--loggingCounter <= 0) {	
waiting in main loop 

========================= hadoop sample_2632 =========================

private void recoverUnclosedSegment(long segmentTxId) throws IOException {	Preconditions.checkArgument(segmentTxId > 0);	
beginning recovery of unclosed segment starting at txid 

private void recoverUnclosedSegment(long segmentTxId) throws IOException {	Preconditions.checkArgument(segmentTxId > 0);	QuorumCall<AsyncLogger,PrepareRecoveryResponseProto> prepare = loggers.prepareRecovery(segmentTxId);	Map<AsyncLogger, PrepareRecoveryResponseProto> prepareResponses= loggers.waitForWriteQuorum(prepare, prepareRecoveryTimeoutMs, "prepareRecovery(" + segmentTxId + ")");	
recovery prepare phase complete responses 

private void recoverUnclosedSegment(long segmentTxId) throws IOException {	Preconditions.checkArgument(segmentTxId > 0);	QuorumCall<AsyncLogger,PrepareRecoveryResponseProto> prepare = loggers.prepareRecovery(segmentTxId);	Map<AsyncLogger, PrepareRecoveryResponseProto> prepareResponses= loggers.waitForWriteQuorum(prepare, prepareRecoveryTimeoutMs, "prepareRecovery(" + segmentTxId + ")");	Entry<AsyncLogger, PrepareRecoveryResponseProto> bestEntry = Collections.max( prepareResponses.entrySet(), SegmentRecoveryComparator.INSTANCE);	AsyncLogger bestLogger = bestEntry.getKey();	PrepareRecoveryResponseProto bestResponse = bestEntry.getValue();	if (bestResponse.hasAcceptedInEpoch()) {	
using already accepted recovery for segment starting at txid 

private void recoverUnclosedSegment(long segmentTxId) throws IOException {	Preconditions.checkArgument(segmentTxId > 0);	QuorumCall<AsyncLogger,PrepareRecoveryResponseProto> prepare = loggers.prepareRecovery(segmentTxId);	Map<AsyncLogger, PrepareRecoveryResponseProto> prepareResponses= loggers.waitForWriteQuorum(prepare, prepareRecoveryTimeoutMs, "prepareRecovery(" + segmentTxId + ")");	Entry<AsyncLogger, PrepareRecoveryResponseProto> bestEntry = Collections.max( prepareResponses.entrySet(), SegmentRecoveryComparator.INSTANCE);	AsyncLogger bestLogger = bestEntry.getKey();	PrepareRecoveryResponseProto bestResponse = bestEntry.getValue();	if (bestResponse.hasAcceptedInEpoch()) {	} else if (bestResponse.hasSegmentState()) {	
using longest log 

Map<AsyncLogger, PrepareRecoveryResponseProto> prepareResponses= loggers.waitForWriteQuorum(prepare, prepareRecoveryTimeoutMs, "prepareRecovery(" + segmentTxId + ")");	Entry<AsyncLogger, PrepareRecoveryResponseProto> bestEntry = Collections.max( prepareResponses.entrySet(), SegmentRecoveryComparator.INSTANCE);	AsyncLogger bestLogger = bestEntry.getKey();	PrepareRecoveryResponseProto bestResponse = bestEntry.getValue();	if (bestResponse.hasAcceptedInEpoch()) {	} else if (bestResponse.hasSegmentState()) {	} else {	for (PrepareRecoveryResponseProto resp : prepareResponses.values()) {	assert !resp.hasSegmentState() : "One of the loggers had a response, but no best logger " + "was found.";	}	
none of the responders had a log to recover 

public void recoverUnfinalizedSegments() throws IOException {	Preconditions.checkState(!isActiveWriter, "already active writer");	
starting recovery process for unclosed journal segments 

public void recoverUnfinalizedSegments() throws IOException {	Preconditions.checkState(!isActiveWriter, "already active writer");	Map<AsyncLogger, NewEpochResponseProto> resps = createNewUniqueEpoch();	
successfully started new epoch 

public void recoverUnfinalizedSegments() throws IOException {	Preconditions.checkState(!isActiveWriter, "already active writer");	Map<AsyncLogger, NewEpochResponseProto> resps = createNewUniqueEpoch();	if (LOG.isDebugEnabled()) {	
newepoch responses 

========================= hadoop sample_8395 =========================

public void initialize(URI storeUri, Configuration conf) throws IOException {	super.initialize(storeUri, conf);	this.setConf(conf);	this.uri = URI .create(storeUri.getScheme() + ": try {	userName = UserGroupInformation.getCurrentUser().getShortUserName();	} catch (IOException e) {	userName = "hadoop";	
got exception when getting hadoop user name set the user name to 

========================= hadoop sample_6558 =========================

public static void createHomeAndStagingDirectory(String user, Configuration conf) {	try {	FileSystem fs = dfsCluster.getFileSystem();	String path = "/user/" + user;	Path homeDirectory = new Path(path);	if (!fs.exists(homeDirectory)) {	
creating home directory 

try {	FileSystem fs = dfsCluster.getFileSystem();	String path = "/user/" + user;	Path homeDirectory = new Path(path);	if (!fs.exists(homeDirectory)) {	fs.mkdirs(homeDirectory);	changePermission(user, homeDirectory, fs);	}	changePermission(user, homeDirectory, fs);	Path stagingArea = new Path( conf.get("mapreduce.jobtracker.staging.root.dir", "/tmp/hadoop/mapred/staging"));	
creating staging root directory 

========================= hadoop sample_6108 =========================

public void setClientNamenodeAddress(Configuration conf) {	String nnAddr = conf.get(FS_DEFAULT_NAME_KEY);	if (nnAddr == null) {	clientNamenodeAddress = null;	return;	}	
is 

return;	}	if (DFSUtilClient.getNameServiceIds(conf).contains(nnHost)) {	clientNamenodeAddress = nnHost;	} else if (nnUri.getPort() > 0) {	clientNamenodeAddress = nnUri.getAuthority();	} else {	clientNamenodeAddress = null;	return;	}	
clients are to use to access this namenode service 

public static void setServiceAddress(Configuration conf, String address) {	
setting address 

pauseMonitor.start();	metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);	if (NamenodeRole.NAMENODE == role) {	startHttpServer(conf);	}	loadNamesystem(conf);	rpcServer = createRpcServer(conf);	initReconfigurableBackoffKey();	if (clientNamenodeAddress == null) {	clientNamenodeAddress = NetUtils.getHostPortString(getNameNodeAddress());	
clients are to use to access this namenode service 

if (NamenodeRole.NAMENODE != role) {	startHttpServer(conf);	httpServer.setNameNodeAddress(getNameNodeAddress());	httpServer.setFSImage(getFSImage());	}	rpcServer.start();	try {	plugins = conf.getInstances(DFS_NAMENODE_PLUGINS_KEY, ServicePlugin.class);	} catch (RuntimeException e) {	String pluginsValue = conf.get(DFS_NAMENODE_PLUGINS_KEY);	
unable to load namenode plugins specified list of plugins 

try {	plugins = conf.getInstances(DFS_NAMENODE_PLUGINS_KEY, ServicePlugin.class);	} catch (RuntimeException e) {	String pluginsValue = conf.get(DFS_NAMENODE_PLUGINS_KEY);	throw e;	}	for (ServicePlugin p: plugins) {	try {	p.start(this);	} catch (Throwable t) {	
serviceplugin could not be started 

} catch (RuntimeException e) {	String pluginsValue = conf.get(DFS_NAMENODE_PLUGINS_KEY);	throw e;	}	for (ServicePlugin p: plugins) {	try {	p.start(this);	} catch (Throwable t) {	}	}	
rpc up at 

String pluginsValue = conf.get(DFS_NAMENODE_PLUGINS_KEY);	throw e;	}	for (ServicePlugin p: plugins) {	try {	p.start(this);	} catch (Throwable t) {	}	}	if (rpcServer.getServiceRpcAddress() != null) {	
service rpc up at 

private void stopCommonServices() {	if(rpcServer != null) rpcServer.stop();	if(namesystem != null) namesystem.close();	if (pauseMonitor != null) pauseMonitor.stop();	if (plugins != null) {	for (ServicePlugin p : plugins) {	try {	p.stop();	} catch (Throwable t) {	
serviceplugin could not be stopped 

private void stopHttpServer() {	try {	if (httpServer != null) httpServer.stop();	} catch (Exception e) {	
exception while stopping httpserver 

private void stopAtException(Exception e){	try {	this.stop();	} catch (Exception ex) {	
encountered exception when handling exception 

public void join() {	try {	rpcServer.join();	} catch (InterruptedException ie) {	
caught interrupted exception 

public void stop() {	synchronized(this) {	if (stopRequested) return;	stopRequested = true;	}	try {	if (state != null) {	state.exitState(haContext);	}	} catch (ServiceFailedException e) {	
encountered exception while exiting state 

public void joinHttpServer() {	if (httpServer != null) {	try {	httpServer.join();	} catch (InterruptedException e) {	
caught interruptedexception joining namenodehttpserver 

if (sd.hasSomeData()) {	throw new NameNodeFormatException( "NameNode format aborted as reformat is disabled for " + "this cluster.");	}	}	}	if (!fsImage.confirmFormat(force, isInteractive)) {	return true;	}	fsImage.format(fsn, clusterId);	} catch (IOException ioe) {	
encountered exception during format 

private static boolean initializeSharedEdits(Configuration conf, boolean force, boolean interactive) throws IOException {	String nsId = DFSUtil.getNamenodeNameServiceId(conf);	String namenodeId = HAUtil.getNameNodeId(conf, nsId);	initializeGenericKeys(conf, nsId, namenodeId);	if (conf.get(DFSConfigKeys.DFS_NAMENODE_SHARED_EDITS_DIR_KEY) == null) {	
no shared edits directory configured for namespace namenode 

return true;	}	NNStorage newSharedStorage = sharedEditsImage.getStorage();	newSharedStorage.format(nsInfo);	sharedEditsImage.getEditLog().formatNonFileJournals(nsInfo);	fsns.getFSImage().getEditLog().close();	fsns.getFSImage().getEditLog().initJournalsForWrite();	fsns.getFSImage().getEditLog().recoverUnclosedStreams();	copyEditLogSegmentsToSharedDir(fsns, sharedEditsDirs, newSharedStorage, conf);	} catch (IOException ioe) {	
could not initialize shared edits dir 

fsns.getFSImage().getEditLog().initJournalsForWrite();	fsns.getFSImage().getEditLog().recoverUnclosedStreams();	copyEditLogSegmentsToSharedDir(fsns, sharedEditsDirs, newSharedStorage, conf);	} catch (IOException ioe) {	return true;	} finally {	if (sharedEditsImage != null) {	try {	sharedEditsImage.close();	}  catch (IOException ioe) {	
could not close sharededitsimage 

if (sharedEditsImage != null) {	try {	sharedEditsImage.close();	}  catch (IOException ioe) {	}	}	if (existingStorage != null) {	try {	existingStorage.unlockAll();	} catch (IOException ioe) {	
could not unlock storage directories 

int argsLen = (args == null) ? 0 : args.length;	StartupOption startOpt = StartupOption.REGULAR;	for(int i=0; i < argsLen; i++) {	String cmd = args[i];	if (StartupOption.FORMAT.getName().equalsIgnoreCase(cmd)) {	startOpt = StartupOption.FORMAT;	for (i = i + 1; i < argsLen; i++) {	if (args[i].equalsIgnoreCase(StartupOption.CLUSTERID.getName())) {	i++;	if (i >= argsLen) {	
must specify a valid cluster id after the flag 

if (StartupOption.FORMAT.getName().equalsIgnoreCase(cmd)) {	startOpt = StartupOption.FORMAT;	for (i = i + 1; i < argsLen; i++) {	if (args[i].equalsIgnoreCase(StartupOption.CLUSTERID.getName())) {	i++;	if (i >= argsLen) {	return null;	}	String clusterId = args[i];	if (clusterId.isEmpty() || clusterId.equalsIgnoreCase(StartupOption.FORCE.getName()) || clusterId.equalsIgnoreCase( StartupOption.NONINTERACTIVE.getName())) {	
must specify a valid cluster id after the flag 

startOpt = StartupOption.CHECKPOINT;	} else if (StartupOption.UPGRADE.getName().equalsIgnoreCase(cmd) || StartupOption.UPGRADEONLY.getName().equalsIgnoreCase(cmd)) {	startOpt = StartupOption.UPGRADE.getName().equalsIgnoreCase(cmd) ? StartupOption.UPGRADE : StartupOption.UPGRADEONLY;	while (i + 1 < argsLen) {	String flag = args[i + 1];	if (flag.equalsIgnoreCase(StartupOption.CLUSTERID.getName())) {	if (i + 2 < argsLen) {	i += 2;	startOpt.setClusterId(args[i]);	} else {	
must specify a valid cluster id after the flag 

}	} else if (flag.equalsIgnoreCase(StartupOption.RENAMERESERVED .getName())) {	if (i + 2 < argsLen) {	FSImageFormat.setRenameReservedPairs(args[i + 2]);	i += 2;	} else {	FSImageFormat.useDefaultRenameReservedPairs();	i += 1;	}	} else {	
unknown upgrade flag 

i += 1;	}	} else {	return null;	}	}	} else if (StartupOption.ROLLINGUPGRADE.getName().equalsIgnoreCase(cmd)) {	startOpt = StartupOption.ROLLINGUPGRADE;	++i;	if (i >= argsLen) {	
must specify a rolling upgrade startup option 

startOpt = StartupOption.BOOTSTRAPSTANDBY;	return startOpt;	} else if (StartupOption.INITIALIZESHAREDEDITS.getName().equalsIgnoreCase(cmd)) {	startOpt = StartupOption.INITIALIZESHAREDEDITS;	for (i = i + 1 ; i < argsLen; i++) {	if (StartupOption.NONINTERACTIVE.getName().equals(args[i])) {	startOpt.setInteractiveFormat(false);	} else if (StartupOption.FORCE.getName().equals(args[i])) {	startOpt.setForceFormat(true);	} else {	
invalid argument 

private static void doRecovery(StartupOption startOpt, Configuration conf) throws IOException {	String nsId = DFSUtil.getNamenodeNameServiceId(conf);	String namenodeId = HAUtil.getNameNodeId(conf, nsId);	initializeGenericKeys(conf, nsId, namenodeId);	if (startOpt.getForce() < MetaRecoveryContext.FORCE_ALL) {	if (!confirmPrompt("You have selected Metadata Recovery mode.  " + "This mode is intended to recover lost metadata on a corrupt " + "filesystem.  Metadata recovery mode often permanently deletes " + "data from your HDFS filesystem.  Please back up your edit log " + "and fsimage before trying this!\n\n" + "Are you ready to proceed? (Y/N)\n")) {	System.err.println("Recovery aborted at user request.\n");	return;	}	}	
starting recovery 

System.err.println("Recovery aborted at user request.\n");	return;	}	}	UserGroupInformation.setConfiguration(conf);	NameNode.initMetrics(conf, startOpt.toNodeRole());	FSNamesystem fsn = null;	try {	fsn = FSNamesystem.loadFromDisk(conf);	fsn.getFSImage().saveNamespace(fsn);	
recovery complete 

return;	}	}	UserGroupInformation.setConfiguration(conf);	NameNode.initMetrics(conf, startOpt.toNodeRole());	FSNamesystem fsn = null;	try {	fsn = FSNamesystem.loadFromDisk(conf);	fsn.getFSImage().saveNamespace(fsn);	} catch (IOException e) {	
recovery failed caught exception 

}	UserGroupInformation.setConfiguration(conf);	NameNode.initMetrics(conf, startOpt.toNodeRole());	FSNamesystem fsn = null;	try {	fsn = FSNamesystem.loadFromDisk(conf);	fsn.getFSImage().saveNamespace(fsn);	} catch (IOException e) {	throw e;	} catch (RuntimeException e) {	
recovery failed caught exception 

public static NameNode createNameNode(String argv[], Configuration conf) throws IOException {	
createnamenode 

if (namenodeId != null) {	conf.set(DFS_HA_NAMENODE_ID_KEY, namenodeId);	}	DFSUtil.setGenericConf(conf, nameserviceId, namenodeId, NAMENODE_SPECIFIC_KEYS);	DFSUtil.setGenericConf(conf, nameserviceId, null, NAMESERVICE_SPECIFIC_KEYS);	}	if (conf.get(DFS_NAMENODE_RPC_ADDRESS_KEY) != null) {	URI defaultUri = URI.create(HdfsConstants.HDFS_URI_SCHEME + ": + conf.get(DFS_NAMENODE_RPC_ADDRESS_KEY));	conf.set(FS_DEFAULT_NAME_KEY, defaultUri.toString());	if (LOG.isDebugEnabled()) {	
setting to 

if (DFSUtil.parseHelpArgument(argv, NameNode.USAGE, System.out, true)) {	System.exit(0);	}	try {	StringUtils.startupShutdownMessage(NameNode.class, argv, LOG);	NameNode namenode = createNameNode(argv, null);	if (namenode != null) {	namenode.join();	}	} catch (Throwable e) {	
failed to start namenode 

datanodeManager.setHeartbeatInterval(DFS_HEARTBEAT_INTERVAL_DEFAULT);	return String.valueOf(DFS_HEARTBEAT_INTERVAL_DEFAULT);	} else {	datanodeManager.setHeartbeatInterval(Long.parseLong(newVal));	return String.valueOf(datanodeManager.getHeartbeatInterval());	}	} catch (NumberFormatException nfe) {	throw new ReconfigurationException(property, newVal, getConf().get( property), nfe);	} finally {	namesystem.writeUnlock();	
reconfigure changed heartbeatinterval to 

datanodeManager.setHeartbeatRecheckInterval( DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_DEFAULT);	return String.valueOf(DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_DEFAULT);	} else {	datanodeManager.setHeartbeatRecheckInterval(Integer.parseInt(newVal));	return String.valueOf(datanodeManager.getHeartbeatRecheckInterval());	}	} catch (NumberFormatException nfe) {	throw new ReconfigurationException(property, newVal, getConf().get( property), nfe);	} finally {	namesystem.writeUnlock();	
reconfigure changed heartbeatrecheckinterval to 

========================= hadoop sample_8053 =========================

ContainerTokenIdentifier containerIdentifier = new ContainerTokenIdentifier(cId, context.getNodeId().toString(), user, r, System.currentTimeMillis() + 120000, 123, DUMMY_RM_IDENTIFIER, Priority.newInstance(0), 0);	Token containerToken = BuilderUtils.newContainerToken(context.getNodeId(), containerManager.getContext().getContainerTokenSecretManager() .createPassword(containerIdentifier), containerIdentifier);	StartContainerRequest scRequest = StartContainerRequest.newInstance(containerLaunchContext, containerToken);	List<StartContainerRequest> list = new ArrayList<StartContainerRequest>();	list.add(scRequest);	StartContainersRequest allRequests = StartContainersRequest.newInstance(list);	containerManager.startContainers(allRequests);	int timeoutSecs = 0;	while (!processStartFile.exists() && timeoutSecs++ < 20) {	Thread.sleep(1000);	
waiting for process start file to be created 

========================= hadoop sample_1622 =========================

public Configuration bindArgs(Configuration config, List<String> args) throws Exception {	if (LOG.isDebugEnabled()) {	
service passed in arguments 

========================= hadoop sample_4070 =========================

public boolean isChildPolicyAllowed(SchedulingPolicy childPolicy) {	if (childPolicy instanceof DominantResourceFairnessPolicy) {	
queue policy can t be if the parent policy is choose or for child queues instead please note that is only for leaf queues 

========================= hadoop sample_968 =========================

public void periodicInvoke() {	
checking state store connection 

public void periodicInvoke() {	if (!stateStore.isDriverReady()) {	
attempting to open state store driver 

========================= hadoop sample_8287 =========================

public void shutdown() {	
stopping healthmonitor thread 

private void tryConnect() {	Preconditions.checkState(proxy == null);	try {	synchronized (this) {	proxy = createProxy();	}	} catch (IOException e) {	
could not connect to local service at 

private void doHealthChecks() throws InterruptedException {	while (shouldRun) {	HAServiceStatus status = null;	boolean healthy = false;	try {	status = proxy.getServiceStatus();	proxy.monitorHealth();	healthy = true;	} catch (Throwable t) {	if (isHealthCheckFailedException(t)) {	
service health check failed for 

HAServiceStatus status = null;	boolean healthy = false;	try {	status = proxy.getServiceStatus();	proxy.monitorHealth();	healthy = true;	} catch (Throwable t) {	if (isHealthCheckFailedException(t)) {	enterState(State.SERVICE_UNHEALTHY);	} else {	
transport level exception trying to monitor health of 

private synchronized void enterState(State newState) {	if (newState != state) {	
entering state 

private MonitorDaemon() {	super();	setName("Health Monitor for " + targetToMonitor);	setUncaughtExceptionHandler(new UncaughtExceptionHandler() {	public void uncaughtException(Thread t, Throwable e) {	
health monitor failed 

========================= hadoop sample_3998 =========================

public void joinThread() {	try {	checkNotNull(httpServer, "httpServer").join();	} catch (InterruptedException e) {	
interrupted 

========================= hadoop sample_2235 =========================

public void testDelegationTokenDFSApi() throws Exception {	final Token<DelegationTokenIdentifier> token = getDelegationToken(fs, "JobTracker");	DelegationTokenIdentifier identifier = new DelegationTokenIdentifier();	byte[] tokenId = token.getIdentifier();	identifier.readFields(new DataInputStream( new ByteArrayInputStream(tokenId)));	
a valid token should have non null password and should be renewed successfully 

public void catchupDuringFailover() throws IOException {	synchronized (TestDelegationTokensWithHA.this) {	while (!catchup) {	try {	
the editlog tailer is waiting to catchup 

public void testDelegationTokenDuringNNFailover() throws Exception {	EditLogTailer editLogTailer = nn1.getNamesystem().getEditLogTailer();	editLogTailer.stop();	Configuration conf = (Configuration) Whitebox.getInternalState( editLogTailer, "conf");	nn1.getNamesystem().setEditLogTailerForTests( new EditLogTailerForTest(nn1.getNamesystem(), conf));	final Token<DelegationTokenIdentifier> token = getDelegationToken(fs, "JobTracker");	DelegationTokenIdentifier identifier = new DelegationTokenIdentifier();	byte[] tokenId = token.getIdentifier();	identifier.readFields(new DataInputStream( new ByteArrayInputStream(tokenId)));	
a valid token should have non null password and should be renewed successfully 

cluster.getNameNodeRpc(0).renewDelegationToken(token);	fail("StandbyException is expected since nn0 is in standby state");	} catch (StandbyException e) {	GenericTestUtils.assertExceptionContains( HAServiceState.STANDBY.toString(), e);	}	new Thread() {	public void run() {	try {	cluster.transitionToActive(1);	} catch (Exception e) {	
transition to active failed 

} catch (Exception e) {	}	}	}.start();	Thread.sleep(1000);	try {	nn1.getNamesystem().verifyToken(token.decodeIdentifier(), token.getPassword());	fail("RetriableException/StandbyException is expected since nn1 is in transition");	} catch (IOException e) {	assertTrue(e instanceof StandbyException || e instanceof RetriableException);	
got expected exception 

final Token<DelegationTokenIdentifier> token = getDelegationToken(fs, "JobTracker");	UserGroupInformation ugi = UserGroupInformation.createRemoteUser("test");	URI haUri = new URI("hdfs: token.setService(HAUtilClient.buildTokenServiceForLogicalUri(haUri, HdfsConstants.HDFS_URI_SCHEME));	ugi.addToken(token);	Collection<InetSocketAddress> nnAddrs = new HashSet<InetSocketAddress>();	nnAddrs.add(new InetSocketAddress("localhost", nn0.getNameNodeAddress().getPort()));	nnAddrs.add(new InetSocketAddress("localhost", nn1.getNameNodeAddress().getPort()));	HAUtilClient.cloneDelegationTokenForLogicalUri(ugi, haUri, nnAddrs);	Collection<Token<? extends TokenIdentifier>> tokens = ugi.getTokens();	assertEquals(3, tokens.size());	
tokens 

========================= hadoop sample_7467 =========================

private boolean checkManualStateManagementOK(HAServiceTarget target) {	if (target.isAutoFailoverEnabled()) {	if (requestSource != RequestSource.REQUEST_BY_USER_FORCED) {	errOut.println( "Automatic failover is enabled for " + target + "\n" + "Refusing to manually manage HA state, since it may cause\n" + "a split-brain scenario or other incorrect state.\n" + "If you are very sure you know what you are doing, please \n" + "specify the --" + FORCEMANUAL + " flag.");	return false;	} else {	
proceeding with manual ha state management even though automatic failover is enabled for 

public int run(String[] argv) throws Exception {	try {	return runCmd(argv);	} catch (IllegalArgumentException iae) {	errOut.println("Illegal argument: " + iae.getLocalizedMessage());	return -1;	} catch (IOException ioe) {	errOut.println("Operation failed: " + ioe.getLocalizedMessage());	if (LOG.isDebugEnabled()) {	
operation failed 

}	if ("-transitionToActive".equals(cmd) || "-transitionToStandby".equals(cmd) || "-failover".equals(cmd)) {	opts.addOption(FORCEMANUAL, false, "force manual control even if auto-failover is enabled");	}	CommandLine cmdLine = parseOpts(cmd, opts, argv);	if (cmdLine == null) {	return -1;	}	if (cmdLine.hasOption(FORCEMANUAL)) {	if (!confirmForceManual()) {	
Aborted 

========================= hadoop sample_3977 =========================

S3AFileSystem s3a = getFileSystem();	MetricDiff copyLocalOps = new MetricDiff(s3a, INVOCATION_COPY_FROM_LOCAL_FILE);	MetricDiff putRequests = new MetricDiff(s3a, OBJECT_PUT_REQUESTS);	MetricDiff putBytes = new MetricDiff(s3a, OBJECT_PUT_BYTES);	Path remotePath = path("copied");	s3a.copyFromLocalFile(false, true, localPath, remotePath);	verifyFileContents(s3a, remotePath, data);	copyLocalOps.assertDiffEquals(1);	putRequests.assertDiffEquals(1);	putBytes.assertDiffEquals(len);	
filesystem 

========================= hadoop sample_5909 =========================

public static long getFileSize(Path path, Configuration configuration) throws IOException {	
retrieving file size for 

public static LinkedList<CopyListingFileStatus> toCopyListingFileStatus( FileSystem fileSystem, FileStatus fileStatus, boolean preserveAcls, boolean preserveXAttrs, boolean preserveRawXAttrs, int blocksPerChunk) throws IOException {	LinkedList<CopyListingFileStatus> copyListingFileStatus = new LinkedList<CopyListingFileStatus>();	final CopyListingFileStatus clfs = toCopyListingFileStatusHelper( fileSystem, fileStatus, preserveAcls, preserveXAttrs, preserveRawXAttrs, 0, fileStatus.getLen());	final long blockSize = fileStatus.getBlockSize();	if (LOG.isDebugEnabled()) {	
tocopylisting chunksize isdfs 

final long blockSize = fileStatus.getBlockSize();	if (LOG.isDebugEnabled()) {	}	if ((blocksPerChunk > 0) && !fileStatus.isDirectory() && (fileStatus.getLen() > blockSize * blocksPerChunk)) {	final BlockLocation[] blockLocations;	blockLocations = fileSystem.getFileBlockLocations(fileStatus, 0, fileStatus.getLen());	int numBlocks = blockLocations.length;	long curPos = 0;	if (numBlocks <= blocksPerChunk) {	if (LOG.isDebugEnabled()) {	
add file 

while (i < numBlocks) {	long curLength = 0;	for (int j = 0; j < blocksPerChunk && i < numBlocks; ++j, ++i) {	curLength += blockLocations[i].getLength();	}	if (curLength > 0) {	CopyListingFileStatus clfs1 = new CopyListingFileStatus(clfs);	clfs1.setChunkOffset(curPos);	clfs1.setChunkLength(curLength);	if (LOG.isDebugEnabled()) {	
add file chunk 

clfs1.setChunkLength(curLength);	if (LOG.isDebugEnabled()) {	}	copyListingFileStatus.add(clfs1);	curPos += curLength;	}	}	}	} else {	if (LOG.isDebugEnabled()) {	
add file dir 

public static boolean checksumsAreEqual(FileSystem sourceFS, Path source, FileChecksum sourceChecksum, FileSystem targetFS, Path target) throws IOException {	FileChecksum targetChecksum = null;	try {	sourceChecksum = sourceChecksum != null ? sourceChecksum : sourceFS .getFileChecksum(source);	targetChecksum = targetFS.getFileChecksum(target);	} catch (IOException e) {	
unable to retrieve checksum for or 

========================= hadoop sample_6276 =========================

public void init(Configuration conf) {	this.rollingInterval = conf.getLong( YarnConfiguration.RM_AMRM_TOKEN_MASTER_KEY_ROLLING_INTERVAL_SECS, YarnConfiguration.DEFAULT_RM_AMRM_TOKEN_MASTER_KEY_ROLLING_INTERVAL_SECS) * 1000;	this.activationDelay = (long) (conf.getLong(YarnConfiguration.RM_AM_EXPIRY_INTERVAL_MS, YarnConfiguration.DEFAULT_RM_AM_EXPIRY_INTERVAL_MS) * 1.5);	
amrmtokenkeyrollinginterval ms and amrmtokenkeyactivationdelay ms 

public void start() {	if (this.currentMasterKey == null) {	this.currentMasterKey = createNewMasterKey();	if (this.nmStateStore != null) {	try {	this.nmStateStore.storeAMRMProxyCurrentMasterKey( this.currentMasterKey.getMasterKey());	} catch (IOException e) {	
unable to update current master key in state store 

public void applicationMasterFinished(ApplicationAttemptId appAttemptId) {	this.writeLock.lock();	try {	
application finished removing password for 

public void rollMasterKey() {	this.writeLock.lock();	try {	
rolling master key for amrm tokens 

public void rollMasterKey() {	this.writeLock.lock();	try {	this.nextMasterKey = createNewMasterKey();	if (this.nmStateStore != null) {	try {	this.nmStateStore .storeAMRMProxyNextMasterKey(this.nextMasterKey.getMasterKey());	} catch (IOException e) {	
unable to update next master key in state store 

public void activateNextMasterKey() {	this.writeLock.lock();	try {	
activating next master key with id 

public void activateNextMasterKey() {	this.writeLock.lock();	try {	this.currentMasterKey = this.nextMasterKey;	this.nextMasterKey = null;	if (this.nmStateStore != null) {	try {	this.nmStateStore.storeAMRMProxyCurrentMasterKey( this.currentMasterKey.getMasterKey());	this.nmStateStore.storeAMRMProxyNextMasterKey(null);	} catch (IOException e) {	
unable to update current master key in state store 

public Token<AMRMTokenIdentifier> createAndGetAMRMToken( ApplicationAttemptId appAttemptId) {	this.writeLock.lock();	try {	
create amrmtoken for applicationattempt 

public byte[] retrievePassword(AMRMTokenIdentifier identifier) throws InvalidToken {	this.readLock.lock();	try {	ApplicationAttemptId applicationAttemptId = identifier.getApplicationAttemptId();	if (LOG.isDebugEnabled()) {	
trying to retrieve password for 

protected byte[] createPassword(AMRMTokenIdentifier identifier) {	this.readLock.lock();	try {	ApplicationAttemptId applicationAttemptId = identifier.getApplicationAttemptId();	
creating password for 

public void recover(RecoveredAMRMProxyState state) {	if (state != null) {	MasterKey currentKey = state.getCurrentMasterKey();	if (currentKey != null) {	this.currentMasterKey = new MasterKeyData(currentKey, createSecretKey(currentKey.getBytes().array()));	} else {	
no current master key recovered from nm statestore for amrmproxytokensecretmanager 

========================= hadoop sample_1730 =========================

List<String> logDirs = dirsHandler.getLogDirs();	ContainerLocalizer localizer = new ContainerLocalizer(lfs, user, appId, locId, getPaths(localDirs), RecordFactoryProvider.getRecordFactory(getConf()));	createUserLocalDirs(localDirs, user);	createUserCacheDirs(localDirs, user);	createAppDirs(localDirs, user, appId);	createAppLogDirs(appId, logDirs, user);	Path appStorageDir = getWorkingDir(localDirs, user, appId);	String tokenFn = String.format(ContainerLocalizer.TOKEN_FILE_NAME_FMT, locId);	Path tokenDst = new Path(appStorageDir, tokenFn);	copyFile(nmPrivateContainerTokensPath, tokenDst, user);	
copying from to 

public int launchContainer(ContainerStartContext ctx) throws IOException {	Container container = ctx.getContainer();	Path nmPrivateContainerScriptPath = ctx.getNmPrivateContainerScriptPath();	Path nmPrivateTokensPath = ctx.getNmPrivateTokensPath();	String userName = ctx.getUser();	Path containerWorkDir = ctx.getContainerWorkDir();	List<String> localDirs = ctx.getLocalDirs();	List<String> logDirs = ctx.getLogDirs();	String containerImageName = container.getLaunchContext().getEnvironment() .get(YarnConfiguration.NM_DOCKER_CONTAINER_EXECUTOR_IMAGE_NAME);	if (LOG.isDebugEnabled()) {	
containerimagename from launchcontext 

String logDirMount = toMount(logDirs);	String containerWorkDirMount = toMount(Collections.singletonList( containerWorkDir.toUri().getPath()));	StringBuilder commands = new StringBuilder();	String commandStr = commands.append(dockerExecutor) .append(" ") .append("run") .append(" ") .append("--rm --net=host") .append(" ") .append(" --name " + containerIdStr) .append(localDirMount) .append(logDirMount) .append(containerWorkDirMount) .append(" ") .append(containerImageName) .toString();	String dockerPidScript = "`" + dockerExecutor + " inspect --format {{.State.Pid}} " + containerIdStr + "`";	LocalWrapperScriptBuilder sb = new UnixLocalWrapperScriptBuilder( containerWorkDir, commandStr, dockerPidScript);	Path pidFile = getPidFilePath(containerId);	if (pidFile != null) {	sb.writeLocalWrapperScript(launchDst, pidFile);	} else {	
container was marked as inactive returning terminated error 

sb.writeLocalWrapperScript(launchDst, pidFile);	} else {	return ExitCode.TERMINATED.getExitCode();	}	ShellCommandExecutor shExec = null;	try {	lfs.setPermission(launchDst, ContainerExecutor.TASK_LAUNCH_SCRIPT_PERMISSION);	lfs.setPermission(sb.getWrapperScriptPath(), ContainerExecutor.TASK_LAUNCH_SCRIPT_PERMISSION);	String[] command = getRunCommand(sb.getWrapperScriptPath().toString(), containerIdStr, userName, pidFile, this.getConf());	if (LOG.isDebugEnabled()) {	
launchcontainer 

try {	lfs.setPermission(launchDst, ContainerExecutor.TASK_LAUNCH_SCRIPT_PERMISSION);	lfs.setPermission(sb.getWrapperScriptPath(), ContainerExecutor.TASK_LAUNCH_SCRIPT_PERMISSION);	String[] command = getRunCommand(sb.getWrapperScriptPath().toString(), containerIdStr, userName, pidFile, this.getConf());	if (LOG.isDebugEnabled()) {	}	shExec = new ShellCommandExecutor( command, new File(containerWorkDir.toUri().getPath()), container.getLaunchContext().getEnvironment(), 0L, false);	if (isContainerActive(containerId)) {	shExec.execute();	} else {	
container was marked as inactive returning terminated error 

if (isContainerActive(containerId)) {	shExec.execute();	} else {	return ExitCode.TERMINATED.getExitCode();	}	} catch (IOException e) {	if (null == shExec) {	return -1;	}	int exitCode = shExec.getExitCode();	
exit code from container is 

shExec.execute();	} else {	return ExitCode.TERMINATED.getExitCode();	}	} catch (IOException e) {	if (null == shExec) {	return -1;	}	int exitCode = shExec.getExitCode();	if (exitCode != ExitCode.FORCE_KILLED.getExitCode() && exitCode != ExitCode.TERMINATED.getExitCode()) {	
exception from container launch with container id and exit code 

}	sb.command(command);	try (PrintStream pout = new PrintStream(out, false, "UTF-8")) {	sb.write(pout);	}	if (LOG.isDebugEnabled()) {	ByteArrayOutputStream baos = new ByteArrayOutputStream();	try (PrintStream ps = new PrintStream(baos, false, "UTF-8")) {	sb.write(ps);	}	
script utf 

public boolean signalContainer(ContainerSignalContext ctx) throws IOException {	String user = ctx.getUser();	String pid = ctx.getPid();	Signal signal = ctx.getSignal();	if (LOG.isDebugEnabled()) {	
sending signal to pid as user 

public void deleteAsUser(DeletionAsUserContext ctx) throws IOException, InterruptedException {	Path subDir = ctx.getSubDir();	List<Path> baseDirs = ctx.getBasedirs();	if (baseDirs == null || baseDirs.size() == 0) {	
deleting absolute path 

public void deleteAsUser(DeletionAsUserContext ctx) throws IOException, InterruptedException {	Path subDir = ctx.getSubDir();	List<Path> baseDirs = ctx.getBasedirs();	if (baseDirs == null || baseDirs.size() == 0) {	if (!lfs.delete(subDir, true)) {	
delete returned false for path 

public void deleteAsUser(DeletionAsUserContext ctx) throws IOException, InterruptedException {	Path subDir = ctx.getSubDir();	List<Path> baseDirs = ctx.getBasedirs();	if (baseDirs == null || baseDirs.size() == 0) {	if (!lfs.delete(subDir, true)) {	}	return;	}	for (Path baseDir : baseDirs) {	Path del = subDir == null ? baseDir : new Path(baseDir, subDir);	
deleting path 

List<Path> baseDirs = ctx.getBasedirs();	if (baseDirs == null || baseDirs.size() == 0) {	if (!lfs.delete(subDir, true)) {	}	return;	}	for (Path baseDir : baseDirs) {	Path del = subDir == null ? baseDir : new Path(baseDir, subDir);	try {	if (!lfs.delete(del, true)) {	
delete returned false for path 

Path appStorageDir = null;	long totalAvailable = 0L;	long[] availableOnDisk = new long[localDirs.size()];	int i = 0;	for (String localDir : localDirs) {	Path curBase = getApplicationDir(new Path(localDir), user, appId);	long space = 0L;	try {	space = getDiskFreeSpace(curBase);	} catch (IOException e) {	
unable to get free space for 

========================= hadoop sample_1675 =========================

job.addOtherInfo("JOB_QUEUE_NAME", jobInfo.getJobQueueName());	job.addOtherInfo("SUBMIT_TIME", jobInfo.getSubmitTime());	job.addOtherInfo("LAUNCH_TIME", jobInfo.getLaunchTime());	job.addOtherInfo("FINISH_TIME", jobInfo.getFinishTime());	job.addOtherInfo("JOB_STATUS", jobInfo.getJobStatus());	job.addOtherInfo("PRIORITY", jobInfo.getPriority());	job.addOtherInfo("TOTAL_MAPS", jobInfo.getTotalMaps());	job.addOtherInfo("TOTAL_REDUCES", jobInfo.getTotalReduces());	job.addOtherInfo("UBERIZED", jobInfo.getUberized());	job.addOtherInfo("ERROR_INFO", jobInfo.getErrorInfo());	
converted job to a timeline entity 

private TimelineEntity createTaskEntity(TaskInfo taskInfo) {	TimelineEntity task = new TimelineEntity();	task.setEntityType(TASK);	task.setEntityId(taskInfo.getTaskId().toString());	task.setStartTime(taskInfo.getStartTime());	task.addOtherInfo("START_TIME", taskInfo.getStartTime());	task.addOtherInfo("FINISH_TIME", taskInfo.getFinishTime());	task.addOtherInfo("TASK_TYPE", taskInfo.getTaskType());	task.addOtherInfo("TASK_STATUS", taskInfo.getTaskStatus());	task.addOtherInfo("ERROR_INFO", taskInfo.getError());	
converted task to a timeline entity 

private Set<TimelineEntity> createTaskAttemptEntities(TaskInfo taskInfo) {	Set<TimelineEntity> taskAttempts = new HashSet<TimelineEntity>();	Map<TaskAttemptID, TaskAttemptInfo> taskAttemptInfoMap = taskInfo.getAllTaskAttempts();	
task has task attempts 

========================= hadoop sample_5636 =========================

public void setup() throws Exception {	long seed = rand.nextLong();	rand.setSeed(seed);	
running with seed 

========================= hadoop sample_587 =========================

public GetReplicaVisibleLengthResponseProto answer( InvocationOnMock invocation) throws IOException {	Object args[] = invocation.getArguments();	assertEquals(2, args.length);	GetReplicaVisibleLengthRequestProto req = (GetReplicaVisibleLengthRequestProto) args[1];	Set<TokenIdentifier> tokenIds = UserGroupInformation.getCurrentUser() .getTokenIdentifiers();	assertEquals("Only one BlockTokenIdentifier expected", 1, tokenIds.size());	long result = 0;	for (TokenIdentifier tokenId : tokenIds) {	BlockTokenIdentifier id = (BlockTokenIdentifier) tokenId;	
got 

ClientDatanodeProtocol proxy = null;	int fdsAtStart = countOpenFileDescriptors();	try {	long endTime = Time.now() + 3000;	while (Time.now() < endTime) {	proxy = DFSUtilClient.createClientDatanodeProtocolProxy(fakeDnId, conf, 1000, false, fakeBlock);	assertEquals(block3.getBlockId(), proxy.getReplicaVisibleLength(block3));	if (proxy != null) {	RPC.stopProxy(proxy);	}	
num open fds 

========================= hadoop sample_7164 =========================

if (conf.getBoolean( DFSConfigKeys.DFS_ROUTER_HTTP_ENABLE, DFSConfigKeys.DFS_ROUTER_HTTP_ENABLE_DEFAULT)) {	this.httpServer = createHttpServer();	addService(this.httpServer);	}	if (conf.getBoolean( DFSConfigKeys.DFS_ROUTER_HEARTBEAT_ENABLE, DFSConfigKeys.DFS_ROUTER_HEARTBEAT_ENABLE_DEFAULT)) {	this.namenodeHearbeatServices = createNamenodeHearbeatServices();	for (NamenodeHeartbeatService hearbeatService : this.namenodeHearbeatServices) {	addService(hearbeatService);	}	if (this.namenodeHearbeatServices.isEmpty()) {	
heartbeat is enabled but there are no namenodes to monitor 

protected void setRpcServerAddress(InetSocketAddress address) {	this.rpcAddress = address;	if (this.rpcAddress != null) {	try {	String hostname = InetAddress.getLocalHost().getHostName();	setRouterId(hostname + ":" + this.rpcAddress.getPort());	} catch (UnknownHostException ex) {	
cannot set unique router id address not resolvable 

protected NamenodeHeartbeatService createLocalNamenodeHearbeatService() {	String nsId = DFSUtil.getNamenodeNameServiceId(conf);	String nnId = null;	if (HAUtil.isHAEnabled(conf, nsId)) {	nnId = HAUtil.getNameNodeId(conf, nsId);	if (nnId == null) {	
cannot find namenode id for local 

protected NamenodeHeartbeatService createNamenodeHearbeatService( String nsId, String nnId) {	
creating heartbeat service for namenode in 

========================= hadoop sample_8291 =========================

public void run() {	for (int i = 0; i < count; i++) {	try {	int byteSize = RANDOM.nextInt(BYTE_COUNT);	byte[] bytes = new byte[byteSize];	System.arraycopy(BYTES, 0, bytes, 0, byteSize);	Writable param = new BytesWritable(bytes);	call(client, param, address);	Thread.sleep(RANDOM.nextInt(20));	} catch (Exception e) {	
caught exception 

========================= hadoop sample_3127 =========================

this.datanode = datanode;	this.dataXceiverServer = dataXceiverServer;	this.connectToDnViaHostname = datanode.getDnConf().connectToDnViaHostname;	this.ioFileBufferSize = DFSUtilClient.getIoFileBufferSize(datanode.getConf());	this.smallBufferSize = DFSUtilClient.getSmallBufferSize(datanode.getConf());	remoteAddress = peer.getRemoteAddressString();	final int colonIdx = remoteAddress.indexOf(':');	remoteAddressWithoutPort = (colonIdx < 0) ? remoteAddress : remoteAddress.substring(0, colonIdx);	localAddress = peer.getLocalAddressString();	if (LOG.isDebugEnabled()) {	
number of active connections is 

public void sendOOB() throws IOException, InterruptedException {	BlockReceiver br = getCurrentBlockReceiver();	if (br == null) {	return;	}	
sending oob to peer 

public void stopWriter() {	synchronized(this) {	if (getCurrentBlockReceiver() == null) {	return;	}	xceiver.interrupt();	}	
stopped the writer 

}	dataXceiverServer.addPeer(peer, Thread.currentThread(), this);	peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);	InputStream input = socketIn;	try {	IOStreamPair saslStreams = datanode.saslServer.receive(peer, socketOut, socketIn, datanode.getXferAddress().getPort(), datanode.getDatanodeId());	input = new BufferedInputStream(saslStreams.in, smallBufferSize);	socketOut = saslStreams.out;	} catch (InvalidMagicNumberException imne) {	if (imne.isHandshake4Encryption()) {	
failed to read expected encryption handshake from client at perhaps the client is running an older version of hadoop which does not support encryption 

dataXceiverServer.addPeer(peer, Thread.currentThread(), this);	peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);	InputStream input = socketIn;	try {	IOStreamPair saslStreams = datanode.saslServer.receive(peer, socketOut, socketIn, datanode.getXferAddress().getPort(), datanode.getDatanodeId());	input = new BufferedInputStream(saslStreams.in, smallBufferSize);	socketOut = saslStreams.out;	} catch (InvalidMagicNumberException imne) {	if (imne.isHandshake4Encryption()) {	} else {	
failed to read expected sasl data transfer protection handshake from client at perhaps the client is running an older version of hadoop which does not support sasl data transfer protection 

if (opsProcessed != 0) {	assert dnConf.socketKeepaliveTimeout > 0;	peer.setReadTimeout(dnConf.socketKeepaliveTimeout);	} else {	peer.setReadTimeout(dnConf.socketTimeout);	}	op = readOp();	} catch (InterruptedIOException ignored) {	break;	} catch (EOFException | ClosedChannelException e) {	
cached closing after ops this message is usually benign 

} else if (t instanceof InvalidToken) {	if (LOG.isTraceEnabled()) {	LOG.trace(s, t);	}	} else {	LOG.error(s, t);	}	} finally {	collectThreadLocalStates();	if (LOG.isDebugEnabled()) {	
number of active connections is 

} catch (ShortCircuitFdsVersionException e) {	bld.setStatus(ERROR_UNSUPPORTED);	bld.setShortCircuitAccessVersion(DataNode.CURRENT_BLOCK_FORMAT_VERSION);	bld.setMessage(e.getMessage());	} catch (ShortCircuitFdsUnsupportedException e) {	bld.setStatus(ERROR_UNSUPPORTED);	bld.setMessage(e.getMessage());	} catch (IOException e) {	bld.setStatus(ERROR);	bld.setMessage(e.getMessage());	
request short circuit read file descriptor failed with unknown error 

}	byte buf[] = new byte[1];	if (supportsReceiptVerification) {	buf[0] = (byte)USE_RECEIPT_VERIFICATION.getNumber();	} else {	buf[0] = (byte)DO_NOT_USE_RECEIPT_VERIFICATION.getNumber();	}	DomainSocket sock = peer.getDomainSocket();	sock.sendFileDescriptors(fds, buf, 0, buf.length);	if (supportsReceiptVerification) {	
reading receipt verification byte for 

buf[0] = (byte)DO_NOT_USE_RECEIPT_VERIFICATION.getNumber();	}	DomainSocket sock = peer.getDomainSocket();	sock.sendFileDescriptors(fds, buf, 0, buf.length);	if (supportsReceiptVerification) {	int val = sock.getInputStream().read();	if (val < 0) {	throw new EOFException();	}	} else {	
receipt verification is not enabled on the datanode not verifying 

int val = sock.getInputStream().read();	if (val < 0) {	throw new EOFException();	}	} else {	}	success = true;	}	} finally {	if ((!success) && (registeredSlotId != null)) {	
unregistering because the requestshortcircuitfdsforread operation failed 

} else {	}	success = true;	}	} finally {	if ((!success) && (registeredSlotId != null)) {	datanode.shortCircuitRegistry.unregisterSlot(registeredSlotId);	}	if (ClientTraceLog.isInfoEnabled()) {	DatanodeRegistration dnR = datanode.getDNRegistrationForBP(blk .getBlockPoolId());	
src dest op request short circuit fds blockid s srvid s success b 

}	ReleaseShortCircuitAccessResponseProto.Builder bld = ReleaseShortCircuitAccessResponseProto.newBuilder();	bld.setStatus(status);	if (error != null) {	bld.setError(error);	}	bld.build().writeDelimitedTo(socketOut);	success = true;	} finally {	if (ClientTraceLog.isInfoEnabled()) {	
src dest op release short circuit fds shmid slotidx d srvid s success b 

return;	} catch (IOException e) {	sendShmErrorResponse(ERROR, "Failed to create shared file descriptor: " + e.getMessage());	return;	}	sendShmSuccessResponse(sock, shmInfo);	success = true;	} finally {	if (ClientTraceLog.isInfoEnabled()) {	if (success) {	
cliid s src dest op request short circuit shm shmid srvid s success true 

} catch (IOException e) {	sendShmErrorResponse(ERROR, "Failed to create shared file descriptor: " + e.getMessage());	return;	}	sendShmSuccessResponse(sock, shmInfo);	success = true;	} finally {	if (ClientTraceLog.isInfoEnabled()) {	if (success) {	} else {	
cliid s src dest op request short circuit shm shmid n a srvid s success false 

sendShmSuccessResponse(sock, shmInfo);	success = true;	} finally {	if (ClientTraceLog.isInfoEnabled()) {	if (success) {	} else {	}	}	if ((!success) && (peer == null)) {	try {	
failed to send success response back to the client shutting down socket for 

} finally {	if (ClientTraceLog.isInfoEnabled()) {	if (success) {	} else {	}	}	if ((!success) && (peer == null)) {	try {	sock.shutdown();	} catch (IOException e) {	
failed to shut down socket in error handler 

throw e;	}	writeSuccessWithChecksumInfo(blockSender, new DataOutputStream(getOutputStream()));	long beginRead = Time.monotonicNow();	read = blockSender.sendBlock(out, baseStream, null);	long duration = Time.monotonicNow() - beginRead;	if (blockSender.didSendEntireByteRange()) {	try {	ClientReadStatusProto stat = ClientReadStatusProto.parseFrom( PBHelperClient.vintPrefixed(in));	if (!stat.hasStatus()) {	
client did not send a valid status code after reading will close connection 

long beginRead = Time.monotonicNow();	read = blockSender.sendBlock(out, baseStream, null);	long duration = Time.monotonicNow() - beginRead;	if (blockSender.didSendEntireByteRange()) {	try {	ClientReadStatusProto stat = ClientReadStatusProto.parseFrom( PBHelperClient.vintPrefixed(in));	if (!stat.hasStatus()) {	IOUtils.closeStream(out);	}	} catch (IOException ioe) {	
error reading client status response will close connection 

incrDatanodeNetworkErrors();	}	} else {	IOUtils.closeStream(out);	}	datanode.metrics.incrBytesRead((int) read);	datanode.metrics.incrBlocksRead();	datanode.metrics.incrTotalReadTime(duration);	} catch ( SocketException ignored ) {	if (LOG.isTraceEnabled()) {	
ignoring exception while serving to 

datanode.metrics.incrBytesRead((int) read);	datanode.metrics.incrBlocksRead();	datanode.metrics.incrTotalReadTime(duration);	} catch ( SocketException ignored ) {	if (LOG.isTraceEnabled()) {	}	datanode.metrics.incrBlocksRead();	IOUtils.closeStream(out);	} catch ( IOException ioe ) {	if (!(ioe instanceof SocketTimeoutException)) {	
got exception while serving to 

allowLazyPersist = allowLazyPersist && (dnConf.getAllowNonLocalLazyPersist() || peer.isLocal());	long size = 0;	final DataOutputStream replyOut = getBufferedOutputStream();	checkAccess(replyOut, isClient, block, blockToken, Op.WRITE_BLOCK, BlockTokenIdentifier.AccessMode.WRITE);	if (isTransfer && targets.length > 0) {	throw new IOException(stage + " does not support multiple targets " + Arrays.asList(targets));	}	if (LOG.isDebugEnabled()) {	LOG.debug("opWriteBlock: stage=" + stage + ", clientname=" + clientname + "\n  block  =" + block + ", newGs=" + latestGenerationStamp + ", bytesRcvd=[" + minBytesRcvd + ", " + maxBytesRcvd + "]" + "\n  targets=" + Arrays.asList(targets) + "; pipelineSize=" + pipelineSize + ", srcDataNode=" + srcDataNode + ", pinning=" + pinning);	LOG.debug("isDatanode=" + isDatanode + ", isClient=" + isClient + ", isTransfer=" + isTransfer);	
writeblock receive buf size tcp no delay 

throw new IOException(stage + " does not support multiple targets " + Arrays.asList(targets));	}	if (LOG.isDebugEnabled()) {	LOG.debug("opWriteBlock: stage=" + stage + ", clientname=" + clientname + "\n  block  =" + block + ", newGs=" + latestGenerationStamp + ", bytesRcvd=[" + minBytesRcvd + ", " + maxBytesRcvd + "]" + "\n  targets=" + Arrays.asList(targets) + "; pipelineSize=" + pipelineSize + ", srcDataNode=" + srcDataNode + ", pinning=" + pinning);	LOG.debug("isDatanode=" + isDatanode + ", isClient=" + isClient + ", isTransfer=" + isTransfer);	}	final ExtendedBlock originalBlock = new ExtendedBlock(block);	if (block.getNumBytes() == 0) {	block.setNumBytes(dataXceiverServer.estimateBlockSize);	}	
receiving src dest 

replica = blockReceiver.getReplica();	} else {	replica = datanode.data.recoverClose( block, latestGenerationStamp, minBytesRcvd);	}	storageUuid = replica.getStorageUuid();	isOnTransientStorage = replica.isOnTransientStorage();	if (targets.length > 0) {	InetSocketAddress mirrorTarget = null;	mirrorNode = targets[0].getXferAddr(connectToDnViaHostname);	if (LOG.isDebugEnabled()) {	
connecting to datanode 

} else {	new Sender(mirrorOut).writeBlock(originalBlock, targetStorageTypes[0], blockToken, clientname, targets, targetStorageTypes, srcDataNode, stage, pipelineSize, minBytesRcvd, maxBytesRcvd, latestGenerationStamp, requestedChecksum, cachingStrategy, allowLazyPersist, false, targetPinnings);	}	mirrorOut.flush();	DataNodeFaultInjector.get().writeBlockAfterFlush();	if (isClient) {	BlockOpResponseProto connectAck = BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(mirrorIn));	mirrorInStatus = connectAck.getStatus();	firstBadLink = connectAck.getFirstBadLink();	if (LOG.isDebugEnabled() || mirrorInStatus != SUCCESS) {	
datanode got response for connect ack from downstream datanode with firstbadlink as 

BlockOpResponseProto.newBuilder() .setStatus(ERROR) .setFirstBadLink(targets[0].getXferAddr()) .build() .writeDelimitedTo(replyOut);	replyOut.flush();	}	IOUtils.closeStream(mirrorOut);	mirrorOut = null;	IOUtils.closeStream(mirrorIn);	mirrorIn = null;	IOUtils.closeSocket(mirrorSock);	mirrorSock = null;	if (isClient) {	
exception transfering block to mirror 

}	IOUtils.closeStream(mirrorOut);	mirrorOut = null;	IOUtils.closeStream(mirrorIn);	mirrorIn = null;	IOUtils.closeSocket(mirrorSock);	mirrorSock = null;	if (isClient) {	throw e;	} else {	
exception transfering to mirror continuing without the mirror 

mirrorSock = null;	if (isClient) {	throw e;	} else {	incrDatanodeNetworkErrors();	}	}	}	if (isClient && !isTransfer) {	if (LOG.isDebugEnabled() || mirrorInStatus != SUCCESS) {	
datanode forwarding connect ack to upstream firstbadlink is 

if (LOG.isDebugEnabled() || mirrorInStatus != SUCCESS) {	}	BlockOpResponseProto.newBuilder() .setStatus(mirrorInStatus) .setFirstBadLink(firstBadLink) .build() .writeDelimitedTo(replyOut);	replyOut.flush();	}	if (blockReceiver != null) {	String mirrorAddr = (mirrorSock == null) ? null : mirrorNode;	blockReceiver.receiveBlock(mirrorOut, mirrorIn, replyOut, mirrorAddr, null, targets, false);	if (isTransfer) {	if (LOG.isTraceEnabled()) {	
transfer send close ack 

}	writeResponse(SUCCESS, null, replyOut);	}	}	if (isClient && stage == BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {	block.setGenerationStamp(latestGenerationStamp);	block.setNumBytes(minBytesRcvd);	}	if (isDatanode || stage == BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {	datanode.closeBlock(block, null, storageUuid, isOnTransientStorage);	
received src dest of size 

block.setGenerationStamp(latestGenerationStamp);	block.setNumBytes(minBytesRcvd);	}	if (isDatanode || stage == BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {	datanode.closeBlock(block, null, storageUuid, isOnTransientStorage);	}	if(isClient) {	size = block.getNumBytes();	}	} catch (IOException ioe) {	
opwriteblock received exception 

public void transferBlock(final ExtendedBlock blk, final Token<BlockTokenIdentifier> blockToken, final String clientName, final DatanodeInfo[] targets, final StorageType[] targetStorageTypes) throws IOException {	previousOpClientName = clientName;	updateCurrentThreadName(Op.TRANSFER_BLOCK + " " + blk);	final DataOutputStream out = new DataOutputStream( getOutputStream());	checkAccess(out, true, blk, blockToken, Op.TRANSFER_BLOCK, BlockTokenIdentifier.AccessMode.COPY);	try {	datanode.transferReplicaForPipelineRecovery(blk, targets, targetStorageTypes, clientName);	writeResponse(Status.SUCCESS, null, out);	} catch (IOException ioe) {	
transferblock received exception 

final int csize = checksum.getChecksumSize();	final int bytesPerCRC = checksum.getBytesPerChecksum();	final long crcPerBlock = csize <= 0 ? 0 : (metadataIn.getLength() - BlockMetadataHeader.getHeaderSize()) / csize;	final MD5Hash md5 = partialBlk && crcPerBlock > 0 ? calcPartialBlockChecksum(block, requestLength, checksum, checksumIn) : MD5Hash.digest(checksumIn);	if (LOG.isDebugEnabled()) {	LOG.debug("block=" + block + ", bytesPerCRC=" + bytesPerCRC + ", crcPerBlock=" + crcPerBlock + ", md5=" + md5);	}	BlockOpResponseProto.newBuilder() .setStatus(SUCCESS) .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder() .setBytesPerCrc(bytesPerCRC) .setCrcPerBlock(crcPerBlock) .setMd5(ByteString.copyFrom(md5.getDigest())) .setCrcType(PBHelperClient.convert(checksum.getChecksumType()))) .build() .writeDelimitedTo(out);	out.flush();	} catch (IOException ioe) {	
blockchecksum received exception 

try {	blockSender = new BlockSender(block, 0, -1, false, false, true, datanode, null, CachingStrategy.newDropBehind());	OutputStream baseStream = getOutputStream();	writeSuccessWithChecksumInfo(blockSender, reply);	long beginRead = Time.monotonicNow();	long read = blockSender.sendBlock(reply, baseStream, dataXceiverServer.balanceThrottler);	long duration = Time.monotonicNow() - beginRead;	datanode.metrics.incrBytesRead((int) read);	datanode.metrics.incrBlocksRead();	datanode.metrics.incrTotalReadTime(duration);	
copied to 

OutputStream baseStream = getOutputStream();	writeSuccessWithChecksumInfo(blockSender, reply);	long beginRead = Time.monotonicNow();	long read = blockSender.sendBlock(reply, baseStream, dataXceiverServer.balanceThrottler);	long duration = Time.monotonicNow() - beginRead;	datanode.metrics.incrBytesRead((int) read);	datanode.metrics.incrBlocksRead();	datanode.metrics.incrTotalReadTime(duration);	} catch (IOException ioe) {	isOpSuccess = false;	
opcopyblock received exception 

Socket proxySock = null;	DataOutputStream proxyOut = null;	Status opStatus = SUCCESS;	String errMsg = null;	DataInputStream proxyReply = null;	boolean IoeDuringCopyBlockOperation = false;	try {	if (proxySource.equals(datanode.getDatanodeId())) {	ReplicaInfo oldReplica = datanode.data.moveBlockAcrossStorage(block, storageType);	if (oldReplica != null) {	
moved from storagetype to 

boolean IoeDuringCopyBlockOperation = false;	try {	if (proxySource.equals(datanode.getDatanodeId())) {	ReplicaInfo oldReplica = datanode.data.moveBlockAcrossStorage(block, storageType);	if (oldReplica != null) {	}	} else {	block.setNumBytes(dataXceiverServer.estimateBlockSize);	final String dnAddr = proxySource.getXferAddr(connectToDnViaHostname);	if (LOG.isDebugEnabled()) {	
connecting to datanode 

if (opStatus == SUCCESS && proxyReply != null) {	try {	proxyReply.readChar();	} catch (IOException ignored) {	}	}	dataXceiverServer.balanceThrottler.release();	try {	sendResponse(opStatus, errMsg);	} catch (IOException ioe) {	
error writing reply back to 

private void checkAccess(OutputStream out, final boolean reply, final ExtendedBlock blk, final Token<BlockTokenIdentifier> t, final Op op, final BlockTokenIdentifier.AccessMode mode) throws IOException {	checkAndWaitForBP(blk);	if (datanode.isBlockTokenEnabled) {	if (LOG.isDebugEnabled()) {	
checking block access token for block with mode 

========================= hadoop sample_7867 =========================

public void run() {	boolean success = false;	FileInputStream blockIn = null, metaIn = null;	MappableBlock mappableBlock = null;	ExtendedBlock extBlk = new ExtendedBlock(key.getBlockPoolId(), key.getBlockId(), length, genstamp);	long newUsedBytes = reserve(length);	boolean reservedBytes = false;	try {	if (newUsedBytes < 0) {	
failed to cache could not reserve more bytes in the cache of exceeded 

boolean reservedBytes = false;	try {	if (newUsedBytes < 0) {	return;	}	reservedBytes = true;	try {	blockIn = (FileInputStream)dataset.getBlockInputStream(extBlk, 0);	metaIn = DatanodeUtil.getMetaDataInputStream(extBlk, dataset);	} catch (ClassCastException e) {	
failed to cache underlying blocks are not backed by files 

if (newUsedBytes < 0) {	return;	}	reservedBytes = true;	try {	blockIn = (FileInputStream)dataset.getBlockInputStream(extBlk, 0);	metaIn = DatanodeUtil.getMetaDataInputStream(extBlk, dataset);	} catch (ClassCastException e) {	return;	} catch (FileNotFoundException e) {	
failed to cache failed to find backing files 

}	reservedBytes = true;	try {	blockIn = (FileInputStream)dataset.getBlockInputStream(extBlk, 0);	metaIn = DatanodeUtil.getMetaDataInputStream(extBlk, dataset);	} catch (ClassCastException e) {	return;	} catch (FileNotFoundException e) {	return;	} catch (IOException e) {	
failed to cache failed to open file 

} catch (ClassCastException e) {	return;	} catch (FileNotFoundException e) {	return;	} catch (IOException e) {	return;	}	try {	mappableBlock = MappableBlock. load(length, blockIn, metaIn, blockFileName);	} catch (ChecksumException e) {	
failed to cache checksum verification failed 

} catch (FileNotFoundException e) {	return;	} catch (IOException e) {	return;	}	try {	mappableBlock = MappableBlock. load(length, blockIn, metaIn, blockFileName);	} catch (ChecksumException e) {	return;	} catch (IOException e) {	
failed to cache 

return;	} catch (IOException e) {	return;	}	synchronized (FsDatasetCache.this) {	Value value = mappableBlockMap.get(key);	Preconditions.checkNotNull(value);	Preconditions.checkState(value.state == State.CACHING || value.state == State.CACHING_CANCELLED);	if (value.state == State.CACHING_CANCELLED) {	mappableBlockMap.remove(key);	
caching of was cancelled 

synchronized (FsDatasetCache.this) {	Value value = mappableBlockMap.get(key);	Preconditions.checkNotNull(value);	Preconditions.checkState(value.state == State.CACHING || value.state == State.CACHING_CANCELLED);	if (value.state == State.CACHING_CANCELLED) {	mappableBlockMap.remove(key);	return;	}	mappableBlockMap.put(key, new Value(mappableBlock, State.CACHED));	}	
successfully cached we are now caching bytes in total 

numBlocksCached.addAndGet(1);	dataset.datanode.getMetrics().incrBlocksCached(1);	success = true;	} finally {	IOUtils.closeQuietly(blockIn);	IOUtils.closeQuietly(metaIn);	if (!success) {	if (reservedBytes) {	release(length);	}	
caching of was aborted we are now caching only bytes in total 

private boolean shouldDefer() {	if (revocationTimeMs == 0) {	return false;	}	boolean anchored = !dataset.datanode.getShortCircuitRegistry(). processBlockMunlockRequest(key);	if (!anchored) {	
uncaching now that it is no longer in use by any clients 

private boolean shouldDefer() {	if (revocationTimeMs == 0) {	return false;	}	boolean anchored = !dataset.datanode.getShortCircuitRegistry(). processBlockMunlockRequest(key);	if (!anchored) {	return false;	}	long delta = revocationTimeMs - Time.monotonicNow();	if (delta < 0) {	
forcibly uncaching after because client s refused to stop using it 

return false;	}	boolean anchored = !dataset.datanode.getShortCircuitRegistry(). processBlockMunlockRequest(key);	if (!anchored) {	return false;	}	long delta = revocationTimeMs - Time.monotonicNow();	if (delta < 0) {	return false;	}	
replica still can t be uncached because some clients continue to use it will wait for 

========================= hadoop sample_7922 =========================

private void assertEditFiles(Iterable<URI> dirs, String ... files) throws IOException {	for (URI u : dirs) {	File editDirRoot = new File(u.getPath());	File editDir = new File(editDirRoot, "current");	GenericTestUtils.assertExists(editDir);	if (files.length == 0) {	
checking no edit files exist in 

private void assertEditFiles(Iterable<URI> dirs, String ... files) throws IOException {	for (URI u : dirs) {	File editDirRoot = new File(u.getPath());	File editDir = new File(editDirRoot, "current");	GenericTestUtils.assertExists(editDir);	if (files.length == 0) {	} else {	
checking for following edit files in 

========================= hadoop sample_7471 =========================

public void testModifyAndRead() throws Exception {	
test 

========================= hadoop sample_568 =========================

}	switch (providerString.trim().toLowerCase()) {	case YarnConfiguration.CONFIG_NODE_LABELS_PROVIDER: provider = new ConfigurationNodeLabelsProvider();	break;	case YarnConfiguration.SCRIPT_NODE_LABELS_PROVIDER: provider = new ScriptBasedNodeLabelsProvider();	break;	default: try {	Class<? extends NodeLabelsProvider> labelsProviderClass = conf.getClass(YarnConfiguration.NM_NODE_LABELS_PROVIDER_CONFIG, null, NodeLabelsProvider.class);	provider = labelsProviderClass.newInstance();	} catch (InstantiationException | IllegalAccessException | RuntimeException e) {	
failed to create nodelabelsprovider based on configuration 

case YarnConfiguration.SCRIPT_NODE_LABELS_PROVIDER: provider = new ScriptBasedNodeLabelsProvider();	break;	default: try {	Class<? extends NodeLabelsProvider> labelsProviderClass = conf.getClass(YarnConfiguration.NM_NODE_LABELS_PROVIDER_CONFIG, null, NodeLabelsProvider.class);	provider = labelsProviderClass.newInstance();	} catch (InstantiationException | IllegalAccessException | RuntimeException e) {	throw new IOException( "Failed to create NodeLabelsProvider : " + e.getMessage(), e);	}	}	if (LOG.isDebugEnabled()) {	
distributed node labels is enabled with provider class as 

private void stopRecoveryStore() throws IOException {	if (null != nmStore) {	nmStore.stop();	if (null != context) {	if (context.getDecommissioned() && nmStore.canRecover()) {	
removing state store due to decommission 

private void stopRecoveryStore() throws IOException {	if (null != nmStore) {	nmStore.stop();	if (null != context) {	if (context.getDecommissioned() && nmStore.canRecover()) {	Configuration conf = getConfig();	Path recoveryRoot = new Path(conf.get(YarnConfiguration.NM_RECOVERY_DIR));	
removing state store at due to decommission 

private void stopRecoveryStore() throws IOException {	if (null != nmStore) {	nmStore.stop();	if (null != context) {	if (context.getDecommissioned() && nmStore.canRecover()) {	Configuration conf = getConfig();	Path recoveryRoot = new Path(conf.get(YarnConfiguration.NM_RECOVERY_DIR));	FileSystem recoveryFs = FileSystem.getLocal(conf);	if (!recoveryFs.delete(recoveryRoot, true)) {	
unable to delete 

public static NodeHealthScriptRunner getNodeHealthScriptRunner(Configuration conf) {	String nodeHealthScript = conf.get(YarnConfiguration.NM_HEALTH_CHECK_SCRIPT_PATH);	if(!NodeHealthScriptRunner.shouldRun(nodeHealthScript)) {	
node manager health check script is not available or doesn t have execute permission so not starting the node health script runner 

protected void shutDown(final int exitCode) {	new Thread() {	public void run() {	try {	NodeManager.this.stop();	} catch (Throwable t) {	
error while shutting down nodemanager 

protected void resyncWithRM() {	new Thread() {	public void run() {	try {	if (!rmWorkPreservingRestartEnabled) {	
cleaning up running containers on resync 

protected void resyncWithRM() {	new Thread() {	public void run() {	try {	if (!rmWorkPreservingRestartEnabled) {	containerManager.cleanupContainersOnNMResync();	if (context.getKnownCollectors() != null) {	context.getKnownCollectors().clear();	}	} else {	
preserving containers on resync 

if (!rmWorkPreservingRestartEnabled) {	containerManager.cleanupContainersOnNMResync();	if (context.getKnownCollectors() != null) {	context.getKnownCollectors().clear();	}	} else {	reregisterCollectors();	}	((NodeStatusUpdaterImpl) nodeStatusUpdater) .rebootNodeStatusUpdaterAndRegisterWithRM();	} catch (YarnRuntimeException e) {	
error while rebooting nodestatusupdater 

for (Map.Entry<ApplicationId, AppCollectorData> entry : knownCollectors.entrySet()) {	Application app = context.getApplications().get(entry.getKey());	if ((app != null) && !ApplicationState.FINISHED.equals(app.getApplicationState())) {	registeringCollectors.putIfAbsent(entry.getKey(), entry.getValue());	AppCollectorData data = entry.getValue();	if (LOG.isDebugEnabled()) {	LOG.debug(entry.getKey() + " : " + data.getCollectorAddr() + "@<" + data.getRMIdentifier() + ", " + data.getVersion() + ">");	}	} else {	if (LOG.isDebugEnabled()) {	
remove collector data for done app 

}	if (hasToReboot && null != nodeManagerShutdownHook) {	ShutdownHookManager.get().removeShutdownHook(nodeManagerShutdownHook);	}	nodeManagerShutdownHook = new CompositeServiceShutdownHook(this);	ShutdownHookManager.get().addShutdownHook(nodeManagerShutdownHook, SHUTDOWN_HOOK_PRIORITY);	this.shouldExitOnShutdownEvent = true;	this.init(conf);	this.start();	} catch (Throwable t) {	
error starting nodemanager 

public void handle(NodeManagerEvent event) {	switch (event.getType()) {	case SHUTDOWN: shutDown(NodeManagerStatus.NO_ERROR.getExitCode());	break;	case RESYNC: resyncWithRM();	break;	
invalid shutdown event ignoring 

========================= hadoop sample_1726 =========================

public static void main(String[] args) {	JobConf job = new JobConf(HadoopArchiveLogsRunner.class);	HadoopArchiveLogsRunner halr = new HadoopArchiveLogsRunner(job);	int ret = 0;	try{	ret = ToolRunner.run(halr, args);	} catch(Exception e) {	
Exception 

public int run(String[] args) throws Exception {	handleOpts(args);	Integer exitCode = 1;	UserGroupInformation loginUser = UserGroupInformation.getLoginUser();	if (!proxy || loginUser.getShortUserName().equals(user)) {	
running as 

public int run(String[] args) throws Exception {	handleOpts(args);	Integer exitCode = 1;	UserGroupInformation loginUser = UserGroupInformation.getLoginUser();	if (!proxy || loginUser.getShortUserName().equals(user)) {	exitCode = runInternal();	} else {	
running as but will impersonate 

StringBuilder sb = new StringBuilder("Executing 'hadoop archives'");	for (String haArg : haArgs) {	sb.append("\n\t").append(haArg);	}	LOG.info(sb.toString());	ha.run(haArgs);	FileSystem fs = null;	try {	fs = FileSystem.get(conf);	Path harDest = new Path(remoteAppLogDir, appId + ".har");	
moving har to original location 

for (String haArg : haArgs) {	sb.append("\n\t").append(haArg);	}	LOG.info(sb.toString());	ha.run(haArgs);	FileSystem fs = null;	try {	fs = FileSystem.get(conf);	Path harDest = new Path(remoteAppLogDir, appId + ".har");	fs.rename(new Path(workingDir, appId + ".har"), harDest);	
deleting original logs 

========================= hadoop sample_6519 =========================

this.subClusterId = SubClusterId.newInstance(clusterId);	this.facade = facade.getInstance();	if (configuration instanceof YarnConfiguration) {	this.conf = (YarnConfiguration) configuration;	}	federationFailoverEnabled = conf.getBoolean(YarnConfiguration.FEDERATION_FAILOVER_ENABLED, YarnConfiguration.DEFAULT_FEDERATION_FAILOVER_ENABLED);	conf.setInt( CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_KEY, conf.getInt(YarnConfiguration.CLIENT_FAILOVER_RETRIES, YarnConfiguration.DEFAULT_CLIENT_FAILOVER_RETRIES));	conf.setInt( CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_ON_SOCKET_TIMEOUTS_KEY, conf.getInt( YarnConfiguration.CLIENT_FAILOVER_RETRIES_ON_SOCKET_TIMEOUTS, YarnConfiguration.DEFAULT_CLIENT_FAILOVER_RETRIES_ON_SOCKET_TIMEOUTS));	try {	this.originalUser = UserGroupInformation.getCurrentUser();	
initialized federation proxy for user 

this.facade = facade.getInstance();	if (configuration instanceof YarnConfiguration) {	this.conf = (YarnConfiguration) configuration;	}	federationFailoverEnabled = conf.getBoolean(YarnConfiguration.FEDERATION_FAILOVER_ENABLED, YarnConfiguration.DEFAULT_FEDERATION_FAILOVER_ENABLED);	conf.setInt( CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_KEY, conf.getInt(YarnConfiguration.CLIENT_FAILOVER_RETRIES, YarnConfiguration.DEFAULT_CLIENT_FAILOVER_RETRIES));	conf.setInt( CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_ON_SOCKET_TIMEOUTS_KEY, conf.getInt( YarnConfiguration.CLIENT_FAILOVER_RETRIES_ON_SOCKET_TIMEOUTS, YarnConfiguration.DEFAULT_CLIENT_FAILOVER_RETRIES_ON_SOCKET_TIMEOUTS));	try {	this.originalUser = UserGroupInformation.getCurrentUser();	} catch (IOException e) {	
could not get information of requester ignoring for now 

private T getProxyInternal(boolean isFailover) {	SubClusterInfo subClusterInfo;	T proxy = this.current;	try {	
failing over to the resourcemanager for subclusterid 

private T getProxyInternal(boolean isFailover) {	SubClusterInfo subClusterInfo;	T proxy = this.current;	try {	subClusterInfo = facade.getSubCluster(subClusterId, isFailover);	updateRMAddress(subClusterInfo);	if (this.originalUser == null) {	InetSocketAddress rmAddress = rmProxy.getRMAddress(conf, protocol);	
connecting to subclusterid with protocol without a proxy user 

try {	subClusterInfo = facade.getSubCluster(subClusterId, isFailover);	updateRMAddress(subClusterInfo);	if (this.originalUser == null) {	InetSocketAddress rmAddress = rmProxy.getRMAddress(conf, protocol);	proxy = createRMProxy(rmAddress);	} else {	proxy = this.originalUser.doAs(new PrivilegedExceptionAction<T>() {	public T run() throws IOException {	InetSocketAddress rmAddress = rmProxy.getRMAddress(conf, protocol);	
connecting to subclusterid with protocol as user 

proxy = createRMProxy(rmAddress);	} else {	proxy = this.originalUser.doAs(new PrivilegedExceptionAction<T>() {	public T run() throws IOException {	InetSocketAddress rmAddress = rmProxy.getRMAddress(conf, protocol);	return createRMProxy(rmAddress);	}	});	}	} catch (Exception e) {	
exception while trying to create proxy to the resourcemanager for subclusterid 

private void closeInternal(T currentProxy) {	if (currentProxy != null) {	if (currentProxy instanceof Closeable) {	try {	((Closeable) currentProxy).close();	} catch (IOException e) {	
exception while trying to close proxy 

========================= hadoop sample_1376 =========================

protected JobFactory(JobSubmitter submitter, JobStoryProducer jobProducer, Path scratch, Configuration conf, CountDownLatch startFlag, UserResolver userResolver) {	sequence = new AtomicInteger(0);	this.scratch = scratch;	this.rateFactor = conf.getFloat(Gridmix.GRIDMIX_SUB_MUL, 1.0f);	this.jobProducer = jobProducer;	this.conf = new Configuration(conf);	this.submitter = submitter;	this.startFlag = startFlag;	this.rThread = createReaderThread();	if(LOG.isDebugEnabled()) {	
the submission thread name is 

}	if (job.getSubmissionTime() < 0) {	reason.add("SUBMISSION-TIME (" + job.getSubmissionTime() + ")");	}	if (job.getNumberMaps() == 0) {	reason.add("ZERO-MAPS-JOB");	}	if (reason.size() == 0) {	reason.add("N/A");	}	
ignoring job from the input trace reason 

========================= hadoop sample_6149 =========================

public void testTasksView() {	
HsTasksPage 

public void testTasksViewNaturalSortType() {	
HsTasksPage 

public void testTaskView() {	
HsTaskPage 

public void testTaskViewNaturalSortType() {	
HsTaskPage 

========================= hadoop sample_5335 =========================

public AMRMTokenSecretManager(Configuration conf, RMContext rmContext) {	this.rmContext = rmContext;	this.timer = new Timer();	this.rollingInterval = conf .getLong( YarnConfiguration.RM_AMRM_TOKEN_MASTER_KEY_ROLLING_INTERVAL_SECS, YarnConfiguration.DEFAULT_RM_AMRM_TOKEN_MASTER_KEY_ROLLING_INTERVAL_SECS) * 1000;	this.activationDelay = (long) (conf.getLong(YarnConfiguration.RM_AM_EXPIRY_INTERVAL_MS, YarnConfiguration.DEFAULT_RM_AM_EXPIRY_INTERVAL_MS) * 1.5);	
amrmtokenkeyrollinginterval ms and amrmtokenkeyactivationdelay ms 

public void applicationMasterFinished(ApplicationAttemptId appAttemptId) {	this.writeLock.lock();	try {	
application finished removing password for 

public void activateNextMasterKey() {	this.writeLock.lock();	try {	
activating next master key with id 

public Token<AMRMTokenIdentifier> createAndGetAMRMToken( ApplicationAttemptId appAttemptId) {	this.writeLock.lock();	try {	
create amrmtoken for applicationattempt 

public void addPersistedPassword(Token<AMRMTokenIdentifier> token) throws IOException {	this.writeLock.lock();	try {	AMRMTokenIdentifier identifier = token.decodeIdentifier();	
adding password for 

public byte[] retrievePassword(AMRMTokenIdentifier identifier) throws InvalidToken {	this.readLock.lock();	try {	ApplicationAttemptId applicationAttemptId = identifier.getApplicationAttemptId();	if (LOG.isDebugEnabled()) {	
trying to retrieve password for 

protected byte[] createPassword(AMRMTokenIdentifier identifier) {	this.readLock.lock();	try {	ApplicationAttemptId applicationAttemptId = identifier.getApplicationAttemptId();	
creating password for 

========================= hadoop sample_704 =========================

private static Map<String, Boolean> getSharedCacheUploadPolicies( Configuration conf, boolean areFiles) {	String confParam = areFiles ? MRJobConfig.CACHE_FILES_SHARED_CACHE_UPLOAD_POLICIES : MRJobConfig.CACHE_ARCHIVES_SHARED_CACHE_UPLOAD_POLICIES;	Collection<String> policies = conf.getStringCollection(confParam);	String[] policy;	Map<String, Boolean> policyMap = new LinkedHashMap<String, Boolean>();	for (String s : policies) {	policy = s.split(DELIM);	if (policy.length != 2) {	
is mis formatted returning empty shared cache upload policies error on 

ensureState(JobState.DEFINE);	setUseNewAPI();	connect();	final JobSubmitter submitter = getJobSubmitter(cluster.getFileSystem(), cluster.getClient());	status = ugi.doAs(new PrivilegedExceptionAction<JobStatus>() {	public JobStatus run() throws IOException, InterruptedException, ClassNotFoundException {	return submitter.submitJobInternal(Job.this, cluster);	}	});	state = JobState.RUNNING;	
the url to track the job 

public boolean monitorAndPrintJob() throws IOException, InterruptedException {	String lastReport = null;	Job.TaskStatusFilter filter;	Configuration clientConf = getConfiguration();	filter = Job.getTaskOutputFilter(clientConf);	JobID jobId = getJobID();	
running job 

if (isComplete()) {	reportedAfterCompletion = true;	} else {	Thread.sleep(progMonitorPollIntervalMillis);	}	if (status.getState() == JobStatus.State.PREP) {	continue;	}	if (!reportedUberMode) {	reportedUberMode = true;	
job running in uber mode 

if (!report.equals(lastReport)) {	LOG.info(report);	lastReport = report;	}	TaskCompletionEvent[] events = getTaskCompletionEvents(eventCounter, 10);	eventCounter += events.length;	printTaskEvents(events, filter, profiling, mapRanges, reduceRanges);	}	boolean success = isSuccessful();	if (success) {	
job completed successfully 

LOG.info(report);	lastReport = report;	}	TaskCompletionEvent[] events = getTaskCompletionEvents(eventCounter, 10);	eventCounter += events.length;	printTaskEvents(events, filter, profiling, mapRanges, reduceRanges);	}	boolean success = isSuccessful();	if (success) {	} else {	
job failed with state due to 

public static int getProgressPollInterval(Configuration conf) {	int progMonitorPollIntervalMillis = conf.getInt( PROGRESS_MONITOR_POLL_INTERVAL_KEY, DEFAULT_MONITOR_POLL_INTERVAL);	if (progMonitorPollIntervalMillis < 1) {	
has been set to an invalid value replacing with 

public static int getCompletionPollInterval(Configuration conf) {	int completionPollIntervalMillis = conf.getInt( COMPLETION_POLL_INTERVAL_KEY, DEFAULT_COMPLETION_POLL_INTERVAL);	if (completionPollIntervalMillis < 1) {	
has been set to an invalid value replacing with 

========================= hadoop sample_4898 =========================

public static void setup()  throws IOException {	if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {	
mrappjar not found not running test 

public void testJobSucceed() throws IOException, InterruptedException, ClassNotFoundException {	
starting testjobsucceed 

public void testJobSucceed() throws IOException, InterruptedException, ClassNotFoundException {	if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {	
mrappjar not found not running test 

public void testJobFail() throws IOException, InterruptedException, ClassNotFoundException {	
starting testjobfail 

public void testJobFail() throws IOException, InterruptedException, ClassNotFoundException {	if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {	
mrappjar not found not running test 

========================= hadoop sample_5620 =========================

public boolean remove(LocalizedResource rem, DeletionService delService) {	LocalizedResource r = localrsrc.remove(rem.getRequest());	if (r != null) {	
removed from localized cache 

========================= hadoop sample_1663 =========================

public void testInitNullConf() throws Throwable {	ServiceManager serviceManager = new ServiceManager("testInitNullConf");	CompositeServiceImpl service = new CompositeServiceImpl(0);	serviceManager.addTestService(service);	try {	serviceManager.init(null);	
null configurations are permitted 

========================= hadoop sample_3170 =========================

private static void purgeMatching(File dir, List<Pattern> patterns, long minTxIdToKeep) throws IOException {	for (File f : FileUtil.listFiles(dir)) {	if (!f.isFile()) continue;	for (Pattern p : patterns) {	Matcher matcher = p.matcher(f.getName());	if (matcher.matches()) {	long txid = Long.parseLong(matcher.group(1));	if (txid < minTxIdToKeep) {	
purging no longer needed file 

private static void purgeMatching(File dir, List<Pattern> patterns, long minTxIdToKeep) throws IOException {	for (File f : FileUtil.listFiles(dir)) {	if (!f.isFile()) continue;	for (Pattern p : patterns) {	Matcher matcher = p.matcher(f.getName());	if (matcher.matches()) {	long txid = Long.parseLong(matcher.group(1));	if (txid < minTxIdToKeep) {	if (!f.delete()) {	
unable to delete no longer needed data 

public void close() throws IOException {	
closing journal storage for 

========================= hadoop sample_8383 =========================

private static void waitForCachedBlocks(NameNode nn, final int expectedCachedBlocks, final int expectedCachedReplicas, final String logString) throws Exception {	final FSNamesystem namesystem = nn.getNamesystem();	final CacheManager cacheManager = namesystem.getCacheManager();	
waiting for blocks with replicas 

for (Iterator<CachedBlock> iter = cachedBlocks.iterator();	iter.hasNext(); ) {	CachedBlock cachedBlock = iter.next();	numCachedBlocks++;	numCachedReplicas += cachedBlock.getDatanodes(Type.CACHED).size();	}	}	} finally {	namesystem.readUnlock();	}	
cached blocks have cached replicas have 

iter = dfs.listCacheDirectives(filter);	entry = iter.next();	} catch (IOException e) {	fail("got IOException while calling " + "listCacheDirectives: " + e.getMessage());	}	Assert.assertNotNull(entry);	CacheDirectiveStats stats = entry.getStats();	if ((targetBytesNeeded == stats.getBytesNeeded()) && (targetBytesCached == stats.getBytesCached()) && (targetFilesNeeded == stats.getFilesNeeded()) && (targetFilesCached == stats.getFilesCached())) {	return true;	} else {	
filesneeded filescached bytesneeded bytescached 

private static void waitForCachePoolStats(final DistributedFileSystem dfs, final long targetBytesNeeded, final long targetBytesCached, final long targetFilesNeeded, final long targetFilesCached, final CachePoolInfo pool, final String infoString) throws Exception {	
polling listcachepools for targetbytesneeded targetbytescached targetfilesneeded targetfilescached 

if (entry == null) {	break;	}	if (!entry.getInfo().getPoolName().equals(pool.getPoolName())) {	continue;	}	CachePoolStats stats = entry.getStats();	if ((targetBytesNeeded == stats.getBytesNeeded()) && (targetBytesCached == stats.getBytesCached()) && (targetFilesNeeded == stats.getFilesNeeded()) && (targetFilesCached == stats.getFilesCached())) {	return true;	} else {	
filesneeded filescached bytesneeded bytescached 

final long numBlocks = (len + blockSize - 1) / blockSize;	BlockLocation[] locs = dfs.getFileBlockLocations(p, 0, len);	assertEquals("Unexpected number of block locations for path " + p, numBlocks, locs.length);	for (BlockLocation l: locs) {	if (l.getCachedHosts().length > 0) {	numCachedBlocks++;	}	numCachedReplicas += l.getCachedHosts().length;	}	}	
found of blocks 

final long numBlocks = (len + blockSize - 1) / blockSize;	BlockLocation[] locs = dfs.getFileBlockLocations(p, 0, len);	assertEquals("Unexpected number of block locations for path " + p, numBlocks, locs.length);	for (BlockLocation l: locs) {	if (l.getCachedHosts().length > 0) {	numCachedBlocks++;	}	numCachedReplicas += l.getCachedHosts().length;	}	}	
found of replicas 

========================= hadoop sample_7426 =========================

blacklistToAdd.addAll(blacklistAdditions);	blacklistToRemove.addAll(blacklistRemovals);	ResourceBlacklistRequest blacklistRequest = ResourceBlacklistRequest.newInstance(blacklistToAdd, blacklistToRemove);	allocateRequest = AllocateRequest.newBuilder() .responseId(lastResponseId).progress(progressIndicator) .askList(askList).resourceBlacklistRequest(blacklistRequest) .releaseList(releaseList).updateRequests(updateList).build();	blacklistAdditions.clear();	blacklistRemovals.clear();	}	try {	allocateResponse = rmClient.allocate(allocateRequest);	} catch (ApplicationMasterNotRegisteredException e) {	
applicationmaster is out of sync with resourcemanager hence resyncing 

protected void removePendingChangeRequests( List<UpdatedContainer> changedContainers) {	for (UpdatedContainer changedContainer : changedContainers) {	ContainerId containerId = changedContainer.getContainer().getId();	if (pendingChange.get(containerId) == null) {	continue;	}	if (LOG.isDebugEnabled()) {	
rm has confirmed changed resource allocation for container current resource allocation remove pending change request 

protected void populateNMTokens(List<NMToken> nmTokens) {	for (NMToken token : nmTokens) {	String nodeId = token.getNodeId().toString();	if (LOG.isDebugEnabled()) {	if (getNMTokenCache().containsToken(nodeId)) {	
replacing token for 

protected void populateNMTokens(List<NMToken> nmTokens) {	for (NMToken token : nmTokens) {	String nodeId = token.getNodeId().toString();	if (LOG.isDebugEnabled()) {	if (getNMTokenCache().containsToken(nodeId)) {	} else {	
received new token for 

public void unregisterApplicationMaster(FinalApplicationStatus appStatus, String appMessage, String appTrackingUrl) throws YarnException, IOException {	Preconditions.checkArgument(appStatus != null, "AppStatus should not be null.");	FinishApplicationMasterRequest request = FinishApplicationMasterRequest.newInstance(appStatus, appMessage, appTrackingUrl);	try {	while (true) {	FinishApplicationMasterResponse response = rmClient.finishApplicationMaster(request);	if (response.getIsUnregistered()) {	break;	}	
waiting for application to be successfully unregistered 

FinishApplicationMasterRequest request = FinishApplicationMasterRequest.newInstance(appStatus, appMessage, appTrackingUrl);	try {	while (true) {	FinishApplicationMasterResponse response = rmClient.finishApplicationMaster(request);	if (response.getIsUnregistered()) {	break;	}	Thread.sleep(100);	}	} catch (InterruptedException e) {	
interrupted while waiting for application to be removed from rmstatestore 

try {	while (true) {	FinishApplicationMasterResponse response = rmClient.finishApplicationMaster(request);	if (response.getIsUnregistered()) {	break;	}	Thread.sleep(100);	}	} catch (InterruptedException e) {	} catch (ApplicationMasterNotRegisteredException e) {	
applicationmaster is out of sync with resourcemanager hence resyncing 

public synchronized void addContainerRequest(T req) {	Preconditions.checkArgument(req != null, "Resource request can not be null.");	Set<String> dedupedRacks = new HashSet<String>();	if (req.getRacks() != null) {	dedupedRacks.addAll(req.getRacks());	if(req.getRacks().size() != dedupedRacks.size()) {	Joiner joiner = Joiner.on(',');	
containerrequest has duplicate racks 

Set<String> inferredRacks = resolveRacks(req.getNodes());	inferredRacks.removeAll(dedupedRacks);	checkLocalityRelaxationConflict(req.getAllocationRequestId(), req.getPriority(), ANY_LIST, req.getRelaxLocality());	checkLocalityRelaxationConflict(req.getAllocationRequestId(), req.getPriority(), dedupedRacks, true);	checkLocalityRelaxationConflict(req.getAllocationRequestId(), req.getPriority(), inferredRacks, req.getRelaxLocality());	checkNodeLabelExpression(req);	if (req.getNodes() != null) {	HashSet<String> dedupedNodes = new HashSet<String>(req.getNodes());	if(dedupedNodes.size() != req.getNodes().size()) {	Joiner joiner = Joiner.on(',');	
containerrequest has duplicate nodes 

private Set<String> resolveRacks(List<String> nodes) {	Set<String> racks = new HashSet<String>();	if (nodes != null) {	for (String node : nodes) {	String rack = RackResolver.resolve(node).getNetworkLocation();	if (rack == null) {	
failed to resolve rack for node 

this.blacklistAdditions.addAll(blacklistAdditions);	this.blacklistedNodes.addAll(blacklistAdditions);	this.blacklistRemovals.removeAll(blacklistAdditions);	}	if (blacklistRemovals != null) {	this.blacklistRemovals.addAll(blacklistRemovals);	this.blacklistedNodes.removeAll(blacklistRemovals);	this.blacklistAdditions.removeAll(blacklistRemovals);	}	if (blacklistAdditions != null && blacklistRemovals != null && blacklistAdditions.removeAll(blacklistRemovals)) {	
the same resources appear in both blacklistadditions and blacklistremovals in updateblacklist 

private void updateAMRMToken(Token token) throws IOException {	org.apache.hadoop.security.token.Token<AMRMTokenIdentifier> amrmToken = new org.apache.hadoop.security.token.Token<AMRMTokenIdentifier>(token .getIdentifier().array(), token.getPassword().array(), new Text( token.getKind()), new Text(token.getService()));	UserGroupInformation currentUGI = UserGroupInformation.getCurrentUser();	
updating with new amrmtoken 

========================= hadoop sample_2624 =========================

public static Properties bind(URI fsURI, Configuration conf) throws SwiftConfigurationException {	String host = fsURI.getHost();	if (host == null || host.isEmpty()) {	throw invalidName(host);	}	String container = extractContainerName(host);	String service = extractServiceName(host);	String prefix = buildSwiftInstancePrefix(service);	if (LOG.isDebugEnabled()) {	
filesystem is using configuration keys 

========================= hadoop sample_6237 =========================

application = null;	} catch (YarnException e2) {	throw new IOException(e2);	}	if (application != null) {	trackingUrl = application.getTrackingUrl();	}	InetSocketAddress serviceAddr = null;	while (application == null || YarnApplicationState.RUNNING == application .getYarnApplicationState()) {	if (application == null) {	
could not get job info from rm for job redirecting to job history server 

if (application != null) {	trackingUrl = application.getTrackingUrl();	}	InetSocketAddress serviceAddr = null;	while (application == null || YarnApplicationState.RUNNING == application .getYarnApplicationState()) {	if (application == null) {	return checkAndGetHSProxy(null, JobState.NEW);	}	try {	if (application.getHost() == null || "".equals(application.getHost())) {	
am not assigned to job waiting to get the am 

trackingUrl = application.getTrackingUrl();	}	InetSocketAddress serviceAddr = null;	while (application == null || YarnApplicationState.RUNNING == application .getYarnApplicationState()) {	if (application == null) {	return checkAndGetHSProxy(null, JobState.NEW);	}	try {	if (application.getHost() == null || "".equals(application.getHost())) {	Thread.sleep(2000);	
application state is 

if (application == null) {	return checkAndGetHSProxy(null, JobState.NEW);	}	try {	if (application.getHost() == null || "".equals(application.getHost())) {	Thread.sleep(2000);	application = rm.getApplicationReport(appId);	continue;	} else if (UNAVAILABLE.equals(application.getHost())) {	if (!amAclDisabledStatusLogged) {	
job is running but the host is unknown verify user has view job access 

return getNotRunningJob(application, JobState.RUNNING);	}	if(!conf.getBoolean(MRJobConfig.JOB_AM_ACCESS_DISABLED, false)) {	UserGroupInformation newUgi = UserGroupInformation.createRemoteUser( UserGroupInformation.getCurrentUser().getUserName());	serviceAddr = NetUtils.createSocketAddrForHost( application.getHost(), application.getRpcPort());	if (UserGroupInformation.isSecurityEnabled()) {	org.apache.hadoop.yarn.api.records.Token clientToAMToken = application.getClientToAMToken();	Token<ClientToAMTokenIdentifier> token = ConverterUtils.convertFromYarn(clientToAMToken, serviceAddr);	newUgi.addToken(token);	}	
connecting to 

newUgi.addToken(token);	}	final InetSocketAddress finalServiceAddr = serviceAddr;	realProxy = newUgi.doAs(new PrivilegedExceptionAction<MRClientProtocol>() {	public MRClientProtocol run() throws IOException {	return instantiateAMProxy(finalServiceAddr);	}	});	} else {	if (!amAclDisabledStatusLogged) {	
network acl closed to am for job not going to try to reach the am 

}	});	} else {	if (!amAclDisabledStatusLogged) {	amAclDisabledStatusLogged = true;	}	return getNotRunningJob(null, JobState.RUNNING);	}	return realProxy;	} catch (IOException e) {	
could not connect to waiting for getting the latest am address 

if (!amAclDisabledStatusLogged) {	amAclDisabledStatusLogged = true;	}	return getNotRunningJob(null, JobState.RUNNING);	}	return realProxy;	} catch (IOException e) {	try {	Thread.sleep(2000);	} catch (InterruptedException e1) {	
getproxy call interruped 

Thread.sleep(2000);	} catch (InterruptedException e1) {	throw new YarnRuntimeException(e1);	}	try {	application = rm.getApplicationReport(appId);	} catch (YarnException e1) {	throw new IOException(e1);	}	if (application == null) {	
could not get job info from rm for job redirecting to job history server 

}	try {	application = rm.getApplicationReport(appId);	} catch (YarnException e1) {	throw new IOException(e1);	}	if (application == null) {	return checkAndGetHSProxy(null, JobState.RUNNING);	}	} catch (InterruptedException e) {	
getproxy call interruped 

private MRClientProtocol checkAndGetHSProxy( ApplicationReport applicationReport, JobState state) {	if (null == historyServerProxy) {	
job history server is not configured 

throw new YarnRuntimeException("Method name mismatch", e);	}	maxClientRetry = this.conf.getInt( MRJobConfig.MR_CLIENT_MAX_RETRIES, MRJobConfig.DEFAULT_MR_CLIENT_MAX_RETRIES);	IOException lastException = null;	while (maxClientRetry > 0) {	MRClientProtocol MRClientProxy = null;	try {	MRClientProxy = getProxy();	return methodOb.invoke(MRClientProxy, args);	} catch (InvocationTargetException e) {	
failed to contact am history for job retrying 

throw new IOException(e.getTargetException());	}	if (!usingAMProxy.get()) {	maxClientRetry--;	}	usingAMProxy.set(false);	lastException = new IOException(e.getTargetException());	try {	Thread.sleep(100);	} catch (InterruptedException ie) {	
clientservicedelegate invoke call interrupted 

maxClientRetry--;	}	usingAMProxy.set(false);	lastException = new IOException(e.getTargetException());	try {	Thread.sleep(100);	} catch (InterruptedException ie) {	throw new YarnRuntimeException(ie);	}	} catch (Exception e) {	
failed to contact am history for job will retry 

} catch (InterruptedException ie) {	throw new YarnRuntimeException(ie);	}	} catch (Exception e) {	realProxy = null;	maxClientRetry--;	lastException = new IOException(e.getMessage());	try {	Thread.sleep(100);	} catch (InterruptedException ie) {	
clientservicedelegate invoke call interrupted 

========================= hadoop sample_5698 =========================

private void allocateContainersInternal(long rmIdentifier, AllocationParams appParams, ContainerIdGenerator idCounter, Set<String> blacklist, ApplicationAttemptId id, Map<String, RemoteNode> allNodes, String userName, Map<Resource, List<Allocation>> allocations, EnrichedResourceRequest enrichedAsk) throws YarnException {	if (allNodes.size() == 0) {	
no nodes currently available to allocate opportunistic containers 

int numAllocated = 0;	int loopIndex = OFF_SWITCH_LOOP;	if (enrichedAsk.getNodeLocations().size() > 0) {	loopIndex = NODE_LOCAL_LOOP;	}	while (numAllocated < toAllocate) {	Collection<RemoteNode> nodeCandidates = findNodeCandidates(loopIndex, allNodes, blacklist, enrichedAsk);	for (RemoteNode rNode : nodeCandidates) {	String rNodeHost = rNode.getNodeId().getHost();	if (blacklist.contains(rNodeHost)) {	
nodes for scheduling has a blacklisted node 

location = rNode.getRackName();	} else {	continue;	}	}	Container container = createContainer(rmIdentifier, appParams, idCounter, id, userName, allocations, location, anyAsk, rNode);	numAllocated++;	if (loopIndex != NODE_LOCAL_LOOP) {	blacklist.add(rNode.getNodeId().getHost());	}	
allocated as opportunistic at location 

if (numAllocated >= toAllocate) {	break;	}	}	if (loopIndex == NODE_LOCAL_LOOP && enrichedAsk.getRackLocations().size() > 0) {	loopIndex = RACK_LOCAL_LOOP;	} else {	loopIndex++;	}	if (loopIndex > OFF_SWITCH_LOOP && numAllocated == 0) {	
unable to allocate any opportunistic containers 

========================= hadoop sample_1321 =========================

dispatcher.getEventHandler().handle(new ContainerEvent(containerId, ContainerEventType.RECOVER_PAUSED_CONTAINER));	boolean notInterrupted = true;	try {	File pidFile = locatePidFile(appIdStr, containerIdStr);	if (pidFile != null) {	String pidPathStr = pidFile.getPath();	pidFilePath = new Path(pidPathStr);	exec.activateContainer(containerId, pidFilePath);	retCode = exec.reacquireContainer( new ContainerReacquisitionContext.Builder() .setContainer(container) .setUser(container.getUser()) .setContainerId(containerId) .build());	} else {	
unable to locate pid file for container 

try {	File pidFile = locatePidFile(appIdStr, containerIdStr);	if (pidFile != null) {	String pidPathStr = pidFile.getPath();	pidFilePath = new Path(pidPathStr);	exec.activateContainer(containerId, pidFilePath);	retCode = exec.reacquireContainer( new ContainerReacquisitionContext.Builder() .setContainer(container) .setUser(container.getUser()) .setContainerId(containerId) .build());	} else {	}	} catch (InterruptedException | InterruptedIOException e) {	
interrupted while waiting for exit code from 

if (pidFile != null) {	String pidPathStr = pidFile.getPath();	pidFilePath = new Path(pidPathStr);	exec.activateContainer(containerId, pidFilePath);	retCode = exec.reacquireContainer( new ContainerReacquisitionContext.Builder() .setContainer(container) .setUser(container.getUser()) .setContainerId(containerId) .build());	} else {	}	} catch (InterruptedException | InterruptedIOException e) {	notInterrupted = false;	} catch (IOException e) {	
unable to kill the paused container 

} catch (InterruptedException | InterruptedIOException e) {	notInterrupted = false;	} catch (IOException e) {	} finally {	if (notInterrupted) {	this.completed.set(true);	exec.deactivateContainer(containerId);	try {	getContext().getNMStateStore() .storeContainerCompleted(containerId, retCode);	} catch (IOException e) {	
unable to set exit code for container 

if (notInterrupted) {	this.completed.set(true);	exec.deactivateContainer(containerId);	try {	getContext().getNMStateStore() .storeContainerCompleted(containerId, retCode);	} catch (IOException e) {	}	}	}	if (retCode != 0) {	
recovered container exited with a non zero exit code 

try {	getContext().getNMStateStore() .storeContainerCompleted(containerId, retCode);	} catch (IOException e) {	}	}	}	if (retCode != 0) {	this.dispatcher.getEventHandler().handle(new ContainerExitEvent( containerId, ContainerEventType.CONTAINER_EXITED_WITH_FAILURE, retCode, "Container exited with a non-zero exit code " + retCode));	return retCode;	}	
recovered container succeeded 

========================= hadoop sample_1771 =========================

public void testSplitBasedOnHeadroom() throws Exception {	getPolicyInfo().setHeadroomAlpha(1.0f);	initializePolicy();	List<ResourceRequest> resourceRequests = createSimpleRequest();	prepPolicyWithHeadroom();	Map<SubClusterId, List<ResourceRequest>> response = ((FederationAMRMProxyPolicy) getPolicy()) .splitResourceRequests(resourceRequests);	
initial headroom 

prettyPrintRequests(response);	validateSplit(response, resourceRequests);	checkExpectedAllocation(response, "subcluster0", 1, 60);	checkExpectedAllocation(response, "subcluster1", 1, -1);	checkExpectedAllocation(response, "subcluster2", 1, 15);	checkExpectedAllocation(response, "subcluster5", 1, 25);	checkTotalContainerAllocation(response, 100);	AllocateResponse ar = getAllocateResponseWithTargetHeadroom(40);	((FederationAMRMProxyPolicy) getPolicy()) .notifyOfResponse(SubClusterId.newInstance("subcluster2"), ar);	response = ((FederationAMRMProxyPolicy) getPolicy()) .splitResourceRequests(resourceRequests);	
after headroom update 

int numRR = 1000;	List<ResourceRequest> resourceRequests = createLargeRandomList(numRR);	prepPolicyWithHeadroom();	int numIterations = 1000;	long tstart = System.currentTimeMillis();	for (int i = 0; i < numIterations; i++) {	Map<SubClusterId, List<ResourceRequest>> response = ((FederationAMRMProxyPolicy) getPolicy()) .splitResourceRequests(resourceRequests);	validateSplit(response, resourceRequests);	}	long tend = System.currentTimeMillis();	
performed policy invocations and validations in ms 

========================= hadoop sample_1264 =========================

static String readOutput(Path outDir, Configuration conf) throws IOException {	FileSystem fs = outDir.getFileSystem(conf);	StringBuffer result = new StringBuffer();	Path[] fileList = FileUtil.stat2Paths(fs.listStatus(outDir, new Utils.OutputFileUtils.OutputFilesFilter()));	for (Path outputFile : fileList) {	
Path 

========================= hadoop sample_4436 =========================

private void validateRequest(ServletContext context, Configuration conf, HttpServletRequest request, HttpServletResponse response, FSImage nnImage, String theirStorageInfoString) throws IOException {	if (UserGroupInformation.isSecurityEnabled() && !isValidRequestor(context, request.getUserPrincipal().getName(), conf)) {	String errorMsg = "Only Namenode, Secondary Namenode, and administrators may access " + "this servlet";	response.sendError(HttpServletResponse.SC_FORBIDDEN, errorMsg);	
received non nn snn administrator request for image or edits from at 

private void validateRequest(ServletContext context, Configuration conf, HttpServletRequest request, HttpServletResponse response, FSImage nnImage, String theirStorageInfoString) throws IOException {	if (UserGroupInformation.isSecurityEnabled() && !isValidRequestor(context, request.getUserPrincipal().getName(), conf)) {	String errorMsg = "Only Namenode, Secondary Namenode, and administrators may access " + "this servlet";	response.sendError(HttpServletResponse.SC_FORBIDDEN, errorMsg);	throw new IOException(errorMsg);	}	String myStorageInfoString = nnImage.getStorage().toColonSeparatedString();	if (theirStorageInfoString != null && !myStorageInfoString.equals(theirStorageInfoString)) {	String errorMsg = "This namenode has storage info " + myStorageInfoString + " but the secondary expected " + theirStorageInfoString;	response.sendError(HttpServletResponse.SC_FORBIDDEN, errorMsg);	
received an invalid request file transfer request from a secondary with storage info 

static boolean isValidRequestor(ServletContext context, String remoteUser, Configuration conf) throws IOException {	if (remoteUser == null) {	
received null remoteuser while authorizing access to getimage servlet 

static boolean isValidRequestor(ServletContext context, String remoteUser, Configuration conf) throws IOException {	if (remoteUser == null) {	return false;	}	Set<String> validRequestors = new HashSet<String>();	validRequestors.add(SecurityUtil.getServerPrincipal(conf .get(DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY), DFSUtilClient.getNNAddress(conf).getHostName()));	try {	validRequestors.add( SecurityUtil.getServerPrincipal(conf .get(DFSConfigKeys.DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY), SecondaryNameNode.getHttpAddress(conf).getHostName()));	} catch (Exception e) {	
secondarynamenode principal could not be added 

} catch (Exception e) {	String msg = String.format( "SecondaryNameNode principal not considered, %s = %s, %s = %s", DFSConfigKeys.DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY, conf.get(DFSConfigKeys.DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY), DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY, conf.getTrimmed(DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY, DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_DEFAULT));	LOG.warn(msg);	}	if (HAUtil.isHAEnabled(conf, DFSUtil.getNamenodeNameServiceId(conf))) {	Configuration otherNnConf = HAUtil.getConfForOtherNode(conf);	validRequestors.add(SecurityUtil.getServerPrincipal(otherNnConf .get(DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY), DFSUtilClient.getNNAddress(otherNnConf).getHostName()));	}	for (String v : validRequestors) {	if (v != null && v.equals(remoteUser)) {	
imageservlet allowing checkpointer 

if (HAUtil.isHAEnabled(conf, DFSUtil.getNamenodeNameServiceId(conf))) {	Configuration otherNnConf = HAUtil.getConfForOtherNode(conf);	validRequestors.add(SecurityUtil.getServerPrincipal(otherNnConf .get(DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY), DFSUtilClient.getNNAddress(otherNnConf).getHostName()));	}	for (String v : validRequestors) {	if (v != null && v.equals(remoteUser)) {	return true;	}	}	if (HttpServer2.userHasAdministratorAccess(context, remoteUser)) {	
imageservlet allowing administrator 

validRequestors.add(SecurityUtil.getServerPrincipal(otherNnConf .get(DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY), DFSUtilClient.getNNAddress(otherNnConf).getHostName()));	}	for (String v : validRequestors) {	if (v != null && v.equals(remoteUser)) {	return true;	}	}	if (HttpServer2.userHasAdministratorAccess(context, remoteUser)) {	return true;	}	
imageservlet rejecting 

========================= hadoop sample_8045 =========================

private void deleteTable(DynamoDB db, String tableName) throws InterruptedException {	try {	Table table = db.getTable(tableName);	table.waitForActive();	table.delete();	table.waitForDelete();	} catch (ResourceNotFoundException e) {	
failed to delete as it was not found 

public Exception call() throws Exception {	ContractTestUtils.NanoTimer timer = new ContractTestUtils.NanoTimer();	Exception result = null;	try (DynamoDBMetadataStore store = new DynamoDBMetadataStore()) {	store.initialize(conf);	} catch (Exception e) {	LOG.error(e.getClass() + ": " + e.getMessage());	result = e;	}	timer.end("Parallel DynamoDB client creation %d", index);	
parallel dynamodb client creation ran from to 

========================= hadoop sample_5933 =========================

public Boolean call() throws IOException {	ContractTestUtils.createFile(fs, new Path(srcDir, fileName), false, data);	return fs.exists(new Path(srcDir, fileName));	}	});	}	for (int i = 0; i < count; ++i) {	final Future<Boolean> future = completionService.take();	try {	if (!future.get()) {	
cannot create file 

return fs.exists(new Path(srcDir, fileName));	}	});	}	for (int i = 0; i < count; ++i) {	final Future<Boolean> future = completionService.take();	try {	if (!future.get()) {	}	} catch (ExecutionException e) {	
error while uploading file 

========================= hadoop sample_5899 =========================

final FileSystem fs = createFileSystem(user);	InputStream is = null;	UserGroupInformation ugi = UserGroupInformation .createProxyUser(user.getShortUserName(), UserGroupInformation.getLoginUser());	try {	is = ugi.doAs(new PrivilegedExceptionAction<InputStream>() {	public InputStream run() throws Exception {	return command.execute(fs);	}	});	} catch (InterruptedException ie) {	
open interrupted 

is = ugi.doAs(new PrivilegedExceptionAction<InputStream>() {	public InputStream run() throws Exception {	return command.execute(fs);	}	});	} catch (InterruptedException ie) {	Thread.currentThread().interrupt();	}	Long offset = params.get(OffsetParam.NAME, OffsetParam.class);	Long len = params.get(LenParam.NAME, LenParam.class);	
offset len 

Map json = fsExecute(user, command);	AUDIT_LOG.info("[{}]", path);	response = Response.ok(json).type(MediaType.APPLICATION_JSON).build();	break;	}	case GETFILEBLOCKLOCATIONS: {	long offset = 0;	long len = Long.MAX_VALUE;	Long offsetParam = params.get(OffsetParam.NAME, OffsetParam.class);	Long lenParam = params.get(LenParam.NAME, LenParam.class);	
offset len 

len = lenParam.longValue();	}	FSOperations.FSFileBlockLocations command = new FSOperations.FSFileBlockLocations(path, offset, len);	final String json = JsonUtil.toJsonString("BlockLocations", locations);	response = Response.ok(json).type(MediaType.APPLICATION_JSON).build();	break;	}	case GETACLSTATUS: {	FSOperations.FSAclStatus command = new FSOperations.FSAclStatus(path);	Map json = fsExecute(user, command);	
acl status for 

case GETACLSTATUS: {	FSOperations.FSAclStatus command = new FSOperations.FSAclStatus(path);	Map json = fsExecute(user, command);	response = Response.ok(json).type(MediaType.APPLICATION_JSON).build();	break;	}	case GETXATTRS: {	List<String> xattrNames = params.getValues(XAttrNameParam.NAME, XAttrNameParam.class);	XAttrCodec encoding = params.get(XAttrEncodingParam.NAME, XAttrEncodingParam.class);	FSOperations.FSGetXAttrs command = new FSOperations.FSGetXAttrs(path, xattrNames, encoding);	
xattrs for 

}	case GETXATTRS: {	List<String> xattrNames = params.getValues(XAttrNameParam.NAME, XAttrNameParam.class);	XAttrCodec encoding = params.get(XAttrEncodingParam.NAME, XAttrEncodingParam.class);	FSOperations.FSGetXAttrs command = new FSOperations.FSGetXAttrs(path, xattrNames, encoding);	response = Response.ok(json).type(MediaType.APPLICATION_JSON).build();	break;	}	case LISTXATTRS: {	FSOperations.FSListXAttrs command = new FSOperations.FSListXAttrs(path);	
xattr names for 

response = Response.ok(json).type(MediaType.APPLICATION_JSON).build();	break;	}	case LISTSTATUS_BATCH: {	String startAfter = params.get( HttpFSParametersProvider.StartAfterParam.NAME, HttpFSParametersProvider.StartAfterParam.class);	byte[] token = HttpFSUtils.EMPTY_BYTES;	if (startAfter != null) {	token = startAfter.getBytes(Charsets.UTF_8);	}	FSOperations.FSListStatusBatch command = new FSOperations .FSListStatusBatch(path, token);	
token 

public Response delete(@PathParam("path") String path, throws IOException, FileSystemAccessException {	UserGroupInformation user = HttpUserGroupInformation.get();	Response response;	path = makeAbsolute(path);	MDC.put(HttpFSFileSystem.OP_PARAM, op.value().name());	MDC.put("hostname", request.getRemoteAddr());	switch (op.value()) {	case DELETE: {	Boolean recursive = params.get(RecursiveParam.NAME,  RecursiveParam.class);	
recursive 

Boolean recursive = params.get(RecursiveParam.NAME,  RecursiveParam.class);	FSOperations.FSDelete command = new FSOperations.FSDelete(path, recursive);	JSONObject json = fsExecute(user, command);	response = Response.ok(json).type(MediaType.APPLICATION_JSON).build();	break;	}	case DELETESNAPSHOT: {	String snapshotName = params.get(SnapshotNameParam.NAME, SnapshotNameParam.class);	FSOperations.FSDeleteSnapshot command = new FSOperations.FSDeleteSnapshot(path, snapshotName);	fsExecute(user, command);	
deleted snapshot 

fsExecute(user, command);	AUDIT_LOG.info("[{}]", path);	System.out.println("SENT RESPONSE");	response = Response.ok().build();	break;	}	case TRUNCATE: {	Long newLength = params.get(NewLengthParam.NAME, NewLengthParam.class);	FSOperations.FSTruncate command = new FSOperations.FSTruncate(path, newLength);	JSONObject json = fsExecute(user, command);	
truncate to length 

case TRUNCATE: {	Long newLength = params.get(NewLengthParam.NAME, NewLengthParam.class);	FSOperations.FSTruncate command = new FSOperations.FSTruncate(path, newLength);	JSONObject json = fsExecute(user, command);	response = Response.ok(json).type(MediaType.APPLICATION_JSON).build();	break;	}	case UNSETSTORAGEPOLICY: {	FSOperations.FSUnsetStoragePolicy command = new FSOperations.FSUnsetStoragePolicy(path);	fsExecute(user, command);	
unset storage policy 

Boolean hasData = params.get(DataParam.NAME, DataParam.class);	if (!hasData) {	response = Response.temporaryRedirect( createUploadRedirectionURL(uriInfo, HttpFSFileSystem.Operation.CREATE)).build();	} else {	Short permission = params.get(PermissionParam.NAME, PermissionParam.class);	Boolean override = params.get(OverwriteParam.NAME, OverwriteParam.class);	Short replication = params.get(ReplicationParam.NAME, ReplicationParam.class);	Long blockSize = params.get(BlockSizeParam.NAME, BlockSizeParam.class);	FSOperations.FSCreate command = new FSOperations.FSCreate(is, path, permission, override, replication, blockSize);	fsExecute(user, command);	
permission override replication blocksize 

FSOperations.FSCreate command = new FSOperations.FSCreate(is, path, permission, override, replication, blockSize);	fsExecute(user, command);	response = Response.status(Response.Status.CREATED).build();	}	break;	}	case CREATESNAPSHOT: {	String snapshotName = params.get(SnapshotNameParam.NAME, SnapshotNameParam.class);	FSOperations.FSCreateSnapshot command = new FSOperations.FSCreateSnapshot(path, snapshotName);	String json = fsExecute(user, command);	
snapshot created as 

String json = fsExecute(user, command);	response = Response.ok(json).type(MediaType.APPLICATION_JSON).build();	break;	}	case SETXATTR: {	String xattrName = params.get(XAttrNameParam.NAME, XAttrNameParam.class);	String xattrValue = params.get(XAttrValueParam.NAME, XAttrValueParam.class);	EnumSet<XAttrSetFlag> flag = params.get(XAttrSetFlagParam.NAME, XAttrSetFlagParam.class);	FSOperations.FSSetXAttr command = new FSOperations.FSSetXAttr( path, xattrName, xattrValue, flag);	fsExecute(user, command);	
to xattr 

FSOperations.FSSetXAttr command = new FSOperations.FSSetXAttr( path, xattrName, xattrValue, flag);	fsExecute(user, command);	response = Response.ok().build();	break;	}	case RENAMESNAPSHOT: {	String oldSnapshotName = params.get(OldSnapshotNameParam.NAME, OldSnapshotNameParam.class);	String snapshotName = params.get(SnapshotNameParam.NAME, SnapshotNameParam.class);	FSOperations.FSRenameSnapshot command = new FSOperations.FSRenameSnapshot(path, oldSnapshotName, snapshotName);	fsExecute(user, command);	
renamed snapshot to 

String snapshotName = params.get(SnapshotNameParam.NAME, SnapshotNameParam.class);	FSOperations.FSRenameSnapshot command = new FSOperations.FSRenameSnapshot(path, oldSnapshotName, snapshotName);	fsExecute(user, command);	response = Response.ok().build();	break;	}	case REMOVEXATTR: {	String xattrName = params.get(XAttrNameParam.NAME, XAttrNameParam.class);	FSOperations.FSRemoveXAttr command = new FSOperations.FSRemoveXAttr( path, xattrName);	fsExecute(user, command);	
removed xattr 

String xattrName = params.get(XAttrNameParam.NAME, XAttrNameParam.class);	FSOperations.FSRemoveXAttr command = new FSOperations.FSRemoveXAttr( path, xattrName);	fsExecute(user, command);	response = Response.ok().build();	break;	}	case MKDIRS: {	Short permission = params.get(PermissionParam.NAME, PermissionParam.class);	FSOperations.FSMkdirs command = new FSOperations.FSMkdirs(path, permission);	JSONObject json = fsExecute(user, command);	
permission 

Short permission = params.get(PermissionParam.NAME, PermissionParam.class);	FSOperations.FSMkdirs command = new FSOperations.FSMkdirs(path, permission);	JSONObject json = fsExecute(user, command);	response = Response.ok(json).type(MediaType.APPLICATION_JSON).build();	break;	}	case RENAME: {	String toPath = params.get(DestinationParam.NAME, DestinationParam.class);	FSOperations.FSRename command = new FSOperations.FSRename(path, toPath);	JSONObject json = fsExecute(user, command);	
to 

FSOperations.FSRename command = new FSOperations.FSRename(path, toPath);	JSONObject json = fsExecute(user, command);	response = Response.ok(json).type(MediaType.APPLICATION_JSON).build();	break;	}	case SETOWNER: {	String owner = params.get(OwnerParam.NAME, OwnerParam.class);	String group = params.get(GroupParam.NAME, GroupParam.class);	FSOperations.FSSetOwner command = new FSOperations.FSSetOwner(path, owner, group);	fsExecute(user, command);	
to o g 

String group = params.get(GroupParam.NAME, GroupParam.class);	FSOperations.FSSetOwner command = new FSOperations.FSSetOwner(path, owner, group);	fsExecute(user, command);	response = Response.ok().build();	break;	}	case SETPERMISSION: {	Short permission = params.get(PermissionParam.NAME, PermissionParam.class);	FSOperations.FSSetPermission command = new FSOperations.FSSetPermission(path, permission);	fsExecute(user, command);	
to 

Short permission = params.get(PermissionParam.NAME, PermissionParam.class);	FSOperations.FSSetPermission command = new FSOperations.FSSetPermission(path, permission);	fsExecute(user, command);	response = Response.ok().build();	break;	}	case SETREPLICATION: {	Short replication = params.get(ReplicationParam.NAME, ReplicationParam.class);	FSOperations.FSSetReplication command = new FSOperations.FSSetReplication(path, replication);	JSONObject json = fsExecute(user, command);	
to 

FSOperations.FSSetReplication command = new FSOperations.FSSetReplication(path, replication);	JSONObject json = fsExecute(user, command);	response = Response.ok(json).build();	break;	}	case SETTIMES: {	Long modifiedTime = params.get(ModifiedTimeParam.NAME, ModifiedTimeParam.class);	Long accessTime = params.get(AccessTimeParam.NAME, AccessTimeParam.class);	FSOperations.FSSetTimes command = new FSOperations.FSSetTimes(path, modifiedTime, accessTime);	fsExecute(user, command);	
to m a 

Long accessTime = params.get(AccessTimeParam.NAME, AccessTimeParam.class);	FSOperations.FSSetTimes command = new FSOperations.FSSetTimes(path, modifiedTime, accessTime);	fsExecute(user, command);	response = Response.ok().build();	break;	}	case SETACL: {	String aclSpec = params.get(AclPermissionParam.NAME, AclPermissionParam.class);	FSOperations.FSSetAcl command = new FSOperations.FSSetAcl(path, aclSpec);	fsExecute(user, command);	
to acl 

case SETACL: {	String aclSpec = params.get(AclPermissionParam.NAME, AclPermissionParam.class);	FSOperations.FSSetAcl command = new FSOperations.FSSetAcl(path, aclSpec);	fsExecute(user, command);	response = Response.ok().build();	break;	}	case REMOVEACL: {	FSOperations.FSRemoveAcl command = new FSOperations.FSRemoveAcl(path);	fsExecute(user, command);	
removed acl 

case REMOVEACL: {	FSOperations.FSRemoveAcl command = new FSOperations.FSRemoveAcl(path);	fsExecute(user, command);	response = Response.ok().build();	break;	}	case MODIFYACLENTRIES: {	String aclSpec = params.get(AclPermissionParam.NAME, AclPermissionParam.class);	FSOperations.FSModifyAclEntries command = new FSOperations.FSModifyAclEntries(path, aclSpec);	fsExecute(user, command);	
modify acl entry with 

String aclSpec = params.get(AclPermissionParam.NAME, AclPermissionParam.class);	FSOperations.FSModifyAclEntries command = new FSOperations.FSModifyAclEntries(path, aclSpec);	fsExecute(user, command);	response = Response.ok().build();	break;	}	case REMOVEACLENTRIES: {	String aclSpec = params.get(AclPermissionParam.NAME, AclPermissionParam.class);	FSOperations.FSRemoveAclEntries command = new FSOperations.FSRemoveAclEntries(path, aclSpec);	fsExecute(user, command);	
remove acl entry 

case REMOVEACLENTRIES: {	String aclSpec = params.get(AclPermissionParam.NAME, AclPermissionParam.class);	FSOperations.FSRemoveAclEntries command = new FSOperations.FSRemoveAclEntries(path, aclSpec);	fsExecute(user, command);	response = Response.ok().build();	break;	}	case REMOVEDEFAULTACL: {	FSOperations.FSRemoveDefaultAcl command = new FSOperations.FSRemoveDefaultAcl(path);	fsExecute(user, command);	
remove default acl 

case REMOVEDEFAULTACL: {	FSOperations.FSRemoveDefaultAcl command = new FSOperations.FSRemoveDefaultAcl(path);	fsExecute(user, command);	response = Response.ok().build();	break;	}	case SETSTORAGEPOLICY: {	String policyName = params.get(PolicyNameParam.NAME, PolicyNameParam.class);	FSOperations.FSSetStoragePolicy command = new FSOperations.FSSetStoragePolicy(path, policyName);	fsExecute(user, command);	
to policy 

========================= hadoop sample_6759 =========================

for(int i=0; i < sortSpec.length; ++i) {	int column = Integer.parseInt(sortSpec[i]);	String c1 = logColumns1[column];	String c2 = logColumns2[column];	int comparision = super.compareBytes( c1.getBytes(Charset.forName("UTF-8")), 0, c1.length(), c2.getBytes(Charset.forName("UTF-8")), 0, c2.length() );	if (comparision != 0) {	return comparision;	}	}	} catch (IOException ioe) {	
caught 

========================= hadoop sample_6309 =========================

fullCommand.addAll(prefixCommands);	}	fullCommand.add(containerExecutorExe);	String cliSwitch = operation.getOperationType().getOption();	if (!cliSwitch.isEmpty()) {	fullCommand.add(cliSwitch);	}	fullCommand.addAll(operation.getArguments());	String[] fullCommandArray = fullCommand.toArray(new String[fullCommand.size()]);	if (LOG.isDebugEnabled()) {	
privileged execution command array 

public String executePrivilegedOperation(List<String> prefixCommands, PrivilegedOperation operation, File workingDir, Map<String, String> env, boolean grabOutput, boolean inheritParentEnv) throws PrivilegedOperationException {	String[] fullCommandArray = getPrivilegedOperationExecutionCommand (prefixCommands, operation);	ShellCommandExecutor exec = new ShellCommandExecutor(fullCommandArray, workingDir, env, 0L, inheritParentEnv);	try {	exec.execute();	if (LOG.isDebugEnabled()) {	
command array 

public String executePrivilegedOperation(List<String> prefixCommands, PrivilegedOperation operation, File workingDir, Map<String, String> env, boolean grabOutput, boolean inheritParentEnv) throws PrivilegedOperationException {	String[] fullCommandArray = getPrivilegedOperationExecutionCommand (prefixCommands, operation);	ShellCommandExecutor exec = new ShellCommandExecutor(fullCommandArray, workingDir, env, 0L, inheritParentEnv);	try {	exec.execute();	if (LOG.isDebugEnabled()) {	LOG.debug(Arrays.toString(fullCommandArray));	
privileged execution operation output 

}	} catch (ExitCodeException e) {	if (operation.isFailureLoggingEnabled()) {	StringBuilder logBuilder = new StringBuilder("Shell execution returned " + "exit code: ") .append(exec.getExitCode()) .append(". Privileged Execution Operation Stderr: ") .append(System.lineSeparator()) .append(e.getMessage()) .append(System.lineSeparator()) .append("Stdout: " + exec.getOutput()) .append(System.lineSeparator());	logBuilder.append("Full command array for failed execution: ") .append(System.lineSeparator());	logBuilder.append(Arrays.toString(fullCommandArray));	LOG.warn(logBuilder.toString());	}	throw new PrivilegedOperationException(e, e.getExitCode(), exec.getOutput(), e.getMessage());	} catch (IOException e) {	
ioexception executing command 

========================= hadoop sample_1835 =========================

protected void handleRelaxedException(String action, String expectedException, Exception e) throws Exception {	if (getContract().isSupported(SUPPORTS_STRICT_EXCEPTIONS, false)) {	throw e;	}	
the expected exception was not the exception class raised on 

protected void handleExpectedException(Exception e) {	
expected 

========================= hadoop sample_3222 =========================

public void testGroupShell() throws Exception {	GenericTestUtils.setRootLogLevel(Level.DEBUG);	Configuration conf = new Configuration();	conf.set(CommonConfigurationKeys.HADOOP_SECURITY_GROUP_MAPPING, "org.apache.hadoop.security.ShellBasedUnixGroupsMapping");	Groups groups = new Groups(conf);	String username = System.getProperty("user.name");	List<String> groupList = groups.getGroups(username);	
has groups 

public void testNetgroupShell() throws Exception {	GenericTestUtils.setRootLogLevel(Level.DEBUG);	Configuration conf = new Configuration();	conf.set(CommonConfigurationKeys.HADOOP_SECURITY_GROUP_MAPPING, "org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping");	Groups groups = new Groups(conf);	String username = System.getProperty("user.name");	List<String> groupList = groups.getGroups(username);	
has groups 

public void testGroupWithFallback() throws Exception {	
running mvn pnative dtestgroupfallback clear test will test the normal path and mvn dtestgroupfallback clear test will test the fall back functionality 

public void testGroupWithFallback() throws Exception {	GenericTestUtils.setRootLogLevel(Level.DEBUG);	Configuration conf = new Configuration();	conf.set(CommonConfigurationKeys.HADOOP_SECURITY_GROUP_MAPPING, "org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback");	Groups groups = new Groups(conf);	String username = System.getProperty("user.name");	List<String> groupList = groups.getGroups(username);	
has groups 

public void testNetgroupWithFallback() throws Exception {	
running mvn pnative dtestgroupfallback clear test will test the normal path and mvn dtestgroupfallback clear test will test the fall back functionality 

public void testNetgroupWithFallback() throws Exception {	GenericTestUtils.setRootLogLevel(Level.DEBUG);	Configuration conf = new Configuration();	conf.set(CommonConfigurationKeys.HADOOP_SECURITY_GROUP_MAPPING, "org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback");	Groups groups = new Groups(conf);	String username = System.getProperty("user.name");	List<String> groupList = groups.getGroups(username);	
has groups 

========================= hadoop sample_2960 =========================

public synchronized MetricsSystem init(String prefix) {	if (monitoring && !DefaultMetricsSystem.inMiniClusterMode()) {	
metrics system already initialized 

public synchronized MetricsSystem init(String prefix) {	if (monitoring && !DefaultMetricsSystem.inMiniClusterMode()) {	return this;	}	this.prefix = checkNotNull(prefix, "prefix");	++refCount;	if (monitoring) {	
metrics system started again 

return this;	}	this.prefix = checkNotNull(prefix, "prefix");	++refCount;	if (monitoring) {	return this;	}	switch (initMode()) {	case NORMAL: try { start(); }	catch (MetricsConfigException e) {	
metrics system not started 

return this;	}	this.prefix = checkNotNull(prefix, "prefix");	++refCount;	if (monitoring) {	return this;	}	switch (initMode()) {	case NORMAL: try { start(); }	catch (MetricsConfigException e) {	
stacktrace 

this.prefix = checkNotNull(prefix, "prefix");	++refCount;	if (monitoring) {	return this;	}	switch (initMode()) {	case NORMAL: try { start(); }	catch (MetricsConfigException e) {	}	break;	
metrics system started in standby mode 

public synchronized void start() {	checkNotNull(prefix, "prefix");	if (monitoring) {	
metrics system already started illegal start 

public synchronized void start() {	checkNotNull(prefix, "prefix");	if (monitoring) {	return;	}	for (Callback cb : callbacks) cb.preStart();	for (Callback cb : namedCallbacks.values()) cb.preStart();	configure(prefix);	startTimer();	monitoring = true;	
metrics system started 

public synchronized void stop() {	if (!monitoring && !DefaultMetricsSystem.inMiniClusterMode()) {	
metrics system not yet started illegal stop 

public synchronized void stop() {	if (!monitoring && !DefaultMetricsSystem.inMiniClusterMode()) {	return;	}	if (!monitoring) {	
metrics system stopped again 

public synchronized void stop() {	if (!monitoring && !DefaultMetricsSystem.inMiniClusterMode()) {	return;	}	if (!monitoring) {	return;	}	for (Callback cb : callbacks) cb.preStop();	for (Callback cb : namedCallbacks.values()) cb.preStop();	
stopping metrics system 

if (!monitoring) {	return;	}	for (Callback cb : callbacks) cb.preStop();	for (Callback cb : namedCallbacks.values()) cb.preStop();	stopTimer();	stopSources();	stopSinks();	clearConfigs();	monitoring = false;	
metrics system stopped 

DefaultMetricsSystem.removeSourceName(name);	}	synchronized void registerSource(String name, String desc, MetricsSource source) {	checkNotNull(config, "config");	MetricsConfig conf = sourceConfigs.get(name);	MetricsSourceAdapter sa = new MetricsSourceAdapter(prefix, name, desc, source, injectedTags, period, conf != null ? conf : config.subset(SOURCE_KEY));	sources.put(name, sa);	sa.start();	
registered source 

private Object getProxyForCallback(final Callback callback) {	return Proxy.newProxyInstance(callback.getClass().getClassLoader(), new Class<?>[] { Callback.class }, new InvocationHandler() {	public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {	try {	return method.invoke(callback, args);	}	catch (Exception e) {	
caught exception in callback 

private synchronized void startTimer() {	if (timer != null) {	
metrics system timer already started 

return;	}	logicalTime = 0;	long millis = period;	timer = new Timer("Timer for '"+ prefix +"' metrics system", true);	timer.scheduleAtFixedRate(new TimerTask() {	public void run() {	try {	onTimerEvent();	} catch (Exception e) {	
error invoking metrics timer 

long millis = period;	timer = new Timer("Timer for '"+ prefix +"' metrics system", true);	timer.scheduleAtFixedRate(new TimerTask() {	public void run() {	try {	onTimerEvent();	} catch (Exception e) {	}	}	}, millis, millis);	
scheduled metric snapshot period at second s 

private void snapshotMetrics(MetricsSourceAdapter sa, MetricsBufferBuilder bufferBuilder) {	long startTime = Time.monotonicNow();	bufferBuilder.add(sa.name(), sa.getMetrics(collector, true));	collector.clear();	snapshotStat.add(Time.monotonicNow() - startTime);	
snapshotted source 

private synchronized void stopTimer() {	if (timer == null) {	
metrics system timer already stopped 

confPeriodMillis = confPeriodMillis == 0 ? sinkPeriodMillis : ArithmeticUtils.gcd(confPeriodMillis, sinkPeriodMillis);	String clsName = conf.getClassName("");	if (clsName == null) continue;	String sinkName = entry.getKey();	try {	MetricsSinkAdapter sa = newSink(sinkName, conf.getString(DESC_KEY, sinkName), conf);	sa.start();	sinks.put(sinkName, sa);	}	catch (Exception e) {	
error creating sink 

static String getHostname() {	try {	return InetAddress.getLocalHost().getHostName();	}	catch (Exception e) {	
error getting localhost name using localhost 

public synchronized boolean shutdown() {	LOG.debug("refCount="+ refCount);	if (refCount <= 0) {	
redundant shutdown 

public synchronized boolean shutdown() {	LOG.debug("refCount="+ refCount);	if (refCount <= 0) {	return true;	}	if (--refCount > 0) return false;	if (monitoring) {	try { stop(); }	catch (Exception e) {	
error stopping the metrics system 

}	}	allSources.clear();	allSinks.clear();	callbacks.clear();	namedCallbacks.clear();	if (mbeanName != null) {	MBeans.unregister(mbeanName);	mbeanName = null;	}	
metrics system shutdown complete 

private InitMode initMode() {	
from system property 

private InitMode initMode() {	
from environment variable 

========================= hadoop sample_3388 =========================

public DFSClient(URI nameNodeUri, ClientProtocol rpcNamenode, Configuration conf, FileSystem.Statistics stats) throws IOException {	this.tracer = FsTracer.get(conf);	this.dfsClientConf = new DfsClientConf(conf);	this.conf = conf;	this.stats = stats;	this.socketFactory = NetUtils.getSocketFactory(conf, ClientProtocol.class);	this.dtpReplaceDatanodeOnFailure = ReplaceDatanodeOnFailure.get(conf);	this.smallBufferSize = DFSUtilClient.getSmallBufferSize(conf);	this.dtpReplaceDatanodeOnFailureReplication = (short) conf .getInt(HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure. MIN_REPLICATION, HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure. MIN_REPLICATION_DEFAULT);	if (LOG.isDebugEnabled()) {	
sets to 

this.dtpReplaceDatanodeOnFailureReplication = (short) conf .getInt(HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure. MIN_REPLICATION, HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure. MIN_REPLICATION_DEFAULT);	if (LOG.isDebugEnabled()) {	}	this.ugi = UserGroupInformation.getCurrentUser();	this.namenodeUri = nameNodeUri;	this.clientName = "DFSClient_" + dfsClientConf.getTaskId() + "_" + ThreadLocalRandom.current().nextInt()  + "_" + Thread.currentThread().getId();	int numResponseToDrop = conf.getInt( DFS_CLIENT_TEST_DROP_NAMENODE_RESPONSE_NUM_KEY, DFS_CLIENT_TEST_DROP_NAMENODE_RESPONSE_NUM_DEFAULT);	ProxyAndInfo<ClientProtocol> proxyInfo = null;	AtomicBoolean nnFallbackToSimpleAuth = new AtomicBoolean(false);	if (numResponseToDrop > 0) {	
is set to this hacked client will proactively drop responses 

dtService = null;	} else {	Preconditions.checkArgument(nameNodeUri != null, "null URI");	proxyInfo = NameNodeProxiesClient.createProxyWithClientProtocol(conf, nameNodeUri, nnFallbackToSimpleAuth);	this.dtService = proxyInfo.getDelegationTokenService();	this.namenode = proxyInfo.getProxy();	}	String localInterfaces[] = conf.getTrimmedStrings(DFS_CLIENT_LOCAL_INTERFACES);	localInterfaceAddrs = getLocalInterfaceAddrs(localInterfaces);	if (LOG.isDebugEnabled() && 0 != localInterfaces.length) {	
using local interfaces with addresses 

out = filesBeingWritten.remove(inodeId);	}	if (out != null) {	try {	if (abort) {	out.abort();	} else {	out.close();	}	} catch(IOException ie) {	
failed to abort close file with inode 

public long getBlockSize(String f) throws IOException {	try (TraceScope ignored = newPathTraceScope("getBlockSize", f)) {	return namenode.getPreferredBlockSize(f);	} catch (IOException ie) {	
problem getting block size 

public Token<DelegationTokenIdentifier> getDelegationToken(Text renewer) throws IOException {	assert dtService != null;	try (TraceScope ignored = tracer.newScope("getDelegationToken")) {	Token<DelegationTokenIdentifier> token = namenode.getDelegationToken(renewer);	if (token != null) {	token.setService(this.dtService);	
created 

public Token<DelegationTokenIdentifier> getDelegationToken(Text renewer) throws IOException {	assert dtService != null;	try (TraceScope ignored = tracer.newScope("getDelegationToken")) {	Token<DelegationTokenIdentifier> token = namenode.getDelegationToken(renewer);	if (token != null) {	token.setService(this.dtService);	} else {	
cannot get delegation token from 

public long renewDelegationToken(Token<DelegationTokenIdentifier> token) throws IOException {	
renewing 

public void cancelDelegationToken(Token<DelegationTokenIdentifier> token) throws IOException {	
cancelling 

public void cancel(Token<?> token, Configuration conf) throws IOException {	Token<DelegationTokenIdentifier> delToken = (Token<DelegationTokenIdentifier>) token;	
cancelling 

private DFSOutputStream callAppend(String src, EnumSet<CreateFlag> flag, Progressable progress, String[] favoredNodes) throws IOException {	CreateFlag.validateForAppend(flag);	try {	final LastBlockWithStatus blkWithStatus = callAppend(src, new EnumSetWritable<>(flag, CreateFlag.class));	HdfsFileStatus status = blkWithStatus.getFileStatus();	if (status == null) {	
namenode is on an older version request file info with additional rpc call for file 

public void clearDataEncryptionKey() {	
clearing encryption key 

public DataEncryptionKey newDataEncryptionKey() throws IOException {	if (shouldEncryptData()) {	synchronized (this) {	if (encryptionKey == null || encryptionKey.expiryDate < Time.now()) {	
getting new encryption token from nn 

final long cpb = checksumData.getCrcPerBlock();	if (locatedblocks.size() > 1 && i == 0) {	crcPerBlock = cpb;	}	final MD5Hash md5 = new MD5Hash( checksumData.getMd5().toByteArray());	md5.write(md5out);	final DataChecksum.Type ct;	if (checksumData.hasCrcType()) {	ct = PBHelperClient.convert(checksumData .getCrcType());	} else {	
retrieving checksum from an earlier version datanode inferring checksum by reading first byte 

}	done = true;	if (LOG.isDebugEnabled()) {	if (i == 0) {	LOG.debug("set bytesPerCRC=" + bytesPerCRC + ", crcPerBlock=" + crcPerBlock);	}	LOG.debug("got reply from " + datanodes[j] + ": md5=" + md5);	}	} catch (InvalidBlockTokenException ibte) {	if (i > lastRetriedIndex) {	
got access token error in response to op block checksum for file for block from datanode will retry the block once 

private IOStreamPair connectToDN(DatanodeInfo dn, int timeout, LocatedBlock lb) throws IOException {	boolean success = false;	Socket sock = null;	try {	sock = socketFactory.createSocket();	String dnAddr = dn.getXferAddr(getConf().isConnectToDnViaHostname());	
connecting to datanode 

if (num <= 0 || HEDGED_READ_THREAD_POOL != null) return;	HEDGED_READ_THREAD_POOL = new ThreadPoolExecutor(1, num, 60, TimeUnit.SECONDS, new SynchronousQueue<Runnable>(), new Daemon.DaemonFactory() {	private final AtomicInteger threadIndex = new AtomicInteger(0);	public Thread newThread(Runnable r) {	Thread t = super.newThread(r);	t.setName("hedgedRead-" + threadIndex.getAndIncrement());	return t;	}	}, new ThreadPoolExecutor.CallerRunsPolicy() {	public void rejectedExecution(Runnable runnable, ThreadPoolExecutor e) {	
execution rejected executing in current thread 

========================= hadoop sample_6889 =========================

protected void storeNewMasterKey(DelegationKey key) throws IOException {	if (LOG.isDebugEnabled()) {	
storing master key 

protected void storeNewMasterKey(DelegationKey key) throws IOException {	if (LOG.isDebugEnabled()) {	}	try {	if (stateStore != null) {	stateStore.storeTokenMasterKey(key);	}	} catch (IOException e) {	
unable to store master key 

protected void removeStoredMasterKey(DelegationKey key) {	if (LOG.isDebugEnabled()) {	
removing master key 

protected void removeStoredMasterKey(DelegationKey key) {	if (LOG.isDebugEnabled()) {	}	try {	if (stateStore != null) {	stateStore.removeTokenMasterKey(key);	}	} catch (IOException e) {	
unable to remove master key 

protected void storeNewToken(TimelineDelegationTokenIdentifier tokenId, long renewDate) {	if (LOG.isDebugEnabled()) {	
storing token 

protected void storeNewToken(TimelineDelegationTokenIdentifier tokenId, long renewDate) {	if (LOG.isDebugEnabled()) {	}	try {	if (stateStore != null) {	stateStore.storeToken(tokenId, renewDate);	}	} catch (IOException e) {	
unable to store token 

protected void removeStoredToken(TimelineDelegationTokenIdentifier tokenId) throws IOException {	if (LOG.isDebugEnabled()) {	
storing token 

protected void removeStoredToken(TimelineDelegationTokenIdentifier tokenId) throws IOException {	if (LOG.isDebugEnabled()) {	}	try {	if (stateStore != null) {	stateStore.removeToken(tokenId);	}	} catch (IOException e) {	
unable to remove token 

protected void updateStoredToken(TimelineDelegationTokenIdentifier tokenId, long renewDate) {	if (LOG.isDebugEnabled()) {	
updating token 

protected void updateStoredToken(TimelineDelegationTokenIdentifier tokenId, long renewDate) {	if (LOG.isDebugEnabled()) {	}	try {	if (stateStore != null) {	stateStore.updateToken(tokenId, renewDate);	}	} catch (IOException e) {	
unable to update token 

public void recover(TimelineServiceState state) throws IOException {	
recovering 

========================= hadoop sample_2063 =========================

e.printStackTrace();	debugException = ex;	}	} catch (Exception e) {	exitCode = -1;	debugException = e;	System.err.println(cmd.substring(1) + ": " + e.getLocalizedMessage());	e.printStackTrace();	}	if (debugException != null) {	
exception encountered 

========================= hadoop sample_7774 =========================

};	resourceManager.registerServiceListener(rmStateChangeListener);	resourceManager.init(conf);	new Thread() {	public void run() {	resourceManager.start();	};	}.start();	boolean rmStarted = rmStartedSignal.await(60000L, TimeUnit.MILLISECONDS);	Assert.assertTrue("ResourceManager failed to start up.", rmStarted);	
resourcemanager rmadmin address 

public static void tearDownResourceManager() throws InterruptedException {	if (resourceManager != null) {	
stopping resourcemanager 

========================= hadoop sample_2576 =========================

public void testDumpTree() throws Exception {	final INode root = fsdir.getINode("/");	
original tree 

public void testXAttrMultiSetRemove() throws Exception {	List<XAttr> existingXAttrs = Lists.newArrayListWithCapacity(0);	final Random rand = new Random(0xFEEDA);	int numExpectedXAttrs = 0;	while (numExpectedXAttrs < numGeneratedXAttrs) {	
currently have xattrs 

while (numExpectedXAttrs < numGeneratedXAttrs) {	final int numToAdd = rand.nextInt(5)+1;	List<XAttr> toAdd = Lists.newArrayListWithCapacity(numToAdd);	for (int i = 0; i < numToAdd; i++) {	if (numExpectedXAttrs >= numGeneratedXAttrs) {	break;	}	toAdd.add(generatedXAttrs.get(numExpectedXAttrs));	numExpectedXAttrs++;	}	
attempting to add xattrs 

final int numToAdd = rand.nextInt(5)+1;	List<XAttr> toAdd = Lists.newArrayListWithCapacity(numToAdd);	for (int i = 0; i < numToAdd; i++) {	if (numExpectedXAttrs >= numGeneratedXAttrs) {	break;	}	toAdd.add(generatedXAttrs.get(numExpectedXAttrs));	numExpectedXAttrs++;	}	for (int i = 0; i < toAdd.size(); i++) {	
will add xattr 

toAdd.add(generatedXAttrs.get(numExpectedXAttrs));	numExpectedXAttrs++;	}	for (int i = 0; i < toAdd.size(); i++) {	}	List<XAttr> newXAttrs = FSDirXAttrOp.setINodeXAttrs(fsdir, existingXAttrs, toAdd, EnumSet.of( XAttrSetFlag.CREATE));	verifyXAttrsPresent(newXAttrs, numExpectedXAttrs);	existingXAttrs = newXAttrs;	}	while (numExpectedXAttrs > 0) {	
currently have xattrs 

final int numToRemove = rand.nextInt(5)+1;	List<XAttr> toRemove = Lists.newArrayListWithCapacity(numToRemove);	for (int i = 0; i < numToRemove; i++) {	if (numExpectedXAttrs == 0) {	break;	}	toRemove.add(generatedXAttrs.get(numExpectedXAttrs-1));	numExpectedXAttrs--;	}	final int expectedNumToRemove = toRemove.size();	
attempting to remove xattrs 

========================= hadoop sample_7324 =========================

public void close() throws Exception {	if (readaheadRequest != null) {	readaheadRequest.cancel();	}	if (manageOsCache && getEndOffset() - getStartOffset() > 0) {	try {	NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(identifier, fd, getStartOffset(), getEndOffset() - getStartOffset(), POSIX_FADV_DONTNEED);	} catch (Throwable t) {	
failed to manage os cache for 

========================= hadoop sample_5319 =========================

final Path fileToRead = new Path( TEST_RACE_CONDITION_ON_DELETE_DIR +"/files/many-files/file");	final ExecutorService executorService = Executors.newFixedThreadPool(2);	fs.create(new Path(TEST_RACE_CONDITION_ON_DELETE_DIR +"/file/test/file1"));	fs.create(new Path(TEST_RACE_CONDITION_ON_DELETE_DIR + "/documents/doc1"));	fs.create(new Path( TEST_RACE_CONDITION_ON_DELETE_DIR + "/pictures/picture"));	executorService.execute(new Runnable() {	public void run() {	try {	assertDeleted(new Path(TEST_RACE_CONDITION_ON_DELETE_DIR), true);	} catch (IOException e) {	
deletion thread 

}	}	});	executorService.execute(new Runnable() {	public void run() {	try {	final FSDataOutputStream outputStream = fs.create(fileToRead);	outputStream.write(message.getBytes());	outputStream.close();	} catch (IOException e) {	
writer thread 

});	executorService.awaitTermination(1, TimeUnit.MINUTES);	if (thread1Ex != null) {	throw thread1Ex;	}	if (thread2Ex != null) {	throw thread2Ex;	}	try {	fs.open(fileToRead);	
concurrency test failed to trigger a failure 

========================= hadoop sample_6165 =========================

public List<String> getGroups(String user) throws IOException {	String[] groups = new String[0];	try {	groups = getGroupsForUser(user);	} catch (Exception e) {	if (LOG.isDebugEnabled()) {	
error getting groups for 

public List<String> getGroups(String user) throws IOException {	String[] groups = new String[0];	try {	groups = getGroupsForUser(user);	} catch (Exception e) {	if (LOG.isDebugEnabled()) {	} else {	
error getting groups for 

========================= hadoop sample_3737 =========================

private void add(long start, long end) {	if(end>start) {	Range recRange = new Range(start, end-start);	ranges.add(recRange);	indicesCount+=recRange.getLength();	
added 

private void doNext() {	next++;	
currentindex 

private void skipIfInRange() {	if(next>=range.getStartIndex() && next<range.getEndIndex()) {	
skipping index 

========================= hadoop sample_4676 =========================

private void initDefaultNameService(Configuration conf) {	try {	this.defaultNameService = conf.get( DFS_ROUTER_DEFAULT_NAMESERVICE, DFSUtil.getNamenodeNameServiceId(conf));	} catch (HadoopIllegalArgumentException e) {	
cannot find default name service setting it to the first 

private void initDefaultNameService(Configuration conf) {	try {	this.defaultNameService = conf.get( DFS_ROUTER_DEFAULT_NAMESERVICE, DFSUtil.getNamenodeNameServiceId(conf));	} catch (HadoopIllegalArgumentException e) {	Collection<String> nsIds = DFSUtilClient.getNameServiceIds(conf);	this.defaultNameService = nsIds.iterator().next();	
default name service 

}	Set<String> oldEntries = new TreeSet<>(Collections.reverseOrder());	for (MountTable entry : getTreeValues("/")) {	String srcPath = entry.getSourcePath();	oldEntries.add(srcPath);	}	for (String srcPath : oldEntries) {	if (!newEntries.containsKey(srcPath)) {	this.tree.remove(srcPath);	invalidateLocationCache(srcPath);	
removed stale mount point from resolver 

for (String srcPath : oldEntries) {	if (!newEntries.containsKey(srcPath)) {	this.tree.remove(srcPath);	invalidateLocationCache(srcPath);	}	}	for (MountTable entry : entries) {	String srcPath = entry.getSourcePath();	if (!oldEntries.contains(srcPath)) {	this.tree.put(srcPath, entry);	
added new mount point to resolver 

}	}	for (MountTable entry : entries) {	String srcPath = entry.getSourcePath();	if (!oldEntries.contains(srcPath)) {	this.tree.put(srcPath, entry);	} else {	MountTable existingEntry = this.tree.get(srcPath);	if (existingEntry != null && !existingEntry.equals(entry)) {	invalidateLocationCache(srcPath);	
updated mount point in resolver 

public boolean loadCache(boolean force) {	try {	MountTableStore mountTable = this.getMountTableStore();	mountTable.loadCache(force);	GetMountTableEntriesRequest request = GetMountTableEntriesRequest.newInstance("/");	GetMountTableEntriesResponse response = mountTable.getMountTableEntries(request);	List<MountTable> records = response.getEntries();	refreshEntries(records);	} catch (IOException e) {	
cannot fetch mount table entries from state store 

public void clear() {	
clearing all mount location caches 

private static PathLocation buildLocation( final String path, final MountTable entry) {	String srcPath = entry.getSourcePath();	if (!path.startsWith(srcPath)) {	
cannot build location not a child of 

========================= hadoop sample_8205 =========================

public DockerLinuxContainerRuntime(PrivilegedOperationExecutor privilegedOperationExecutor, CGroupsHandler cGroupsHandler) {	this.privilegedOperationExecutor = privilegedOperationExecutor;	if (cGroupsHandler == null) {	
cgroupshandler is null cgroups not in use 

public void initialize(Configuration conf) throws ContainerExecutionException {	this.conf = conf;	dockerClient = new DockerClient(conf);	allowedNetworks.clear();	allowedNetworks.addAll(Arrays.asList( conf.getTrimmedStrings( YarnConfiguration.NM_DOCKER_ALLOWED_CONTAINER_NETWORKS, YarnConfiguration.DEFAULT_NM_DOCKER_ALLOWED_CONTAINER_NETWORKS)));	defaultNetwork = conf.getTrimmed( YarnConfiguration.NM_DOCKER_DEFAULT_CONTAINER_NETWORK, YarnConfiguration.DEFAULT_NM_DOCKER_DEFAULT_CONTAINER_NETWORK);	if(!allowedNetworks.contains(defaultNetwork)) {	String message = "Default network: " + defaultNetwork + " is not in the set of allowed networks: " + allowedNetworks;	if (LOG.isWarnEnabled()) {	
please check configuration 

private void setHostname(DockerRunCommand runCommand, String containerIdStr, String name) throws ContainerExecutionException {	if (name == null || name.isEmpty()) {	name = RegistryPathUtils.encodeYarnID(containerIdStr);	validateHostname(name);	}	
setting hostname in container to 

protected void addCGroupParentIfRequired(String resourcesOptions, String containerIdStr, DockerRunCommand runCommand) {	if (cGroupsHandler == null) {	if (LOG.isDebugEnabled()) {	
cgroupshandler is null cgroups are not in use nothing to do 

protected void addCGroupParentIfRequired(String resourcesOptions, String containerIdStr, DockerRunCommand runCommand) {	if (cGroupsHandler == null) {	if (LOG.isDebugEnabled()) {	}	return;	}	if (resourcesOptions.equals(PrivilegedOperation.CGROUP_ARG_PREFIX + PrivilegedOperation.CGROUP_ARG_NO_TASKS)) {	if (LOG.isDebugEnabled()) {	
no resource restrictions specified not using docker s cgroup options 

if (cGroupsHandler == null) {	if (LOG.isDebugEnabled()) {	}	return;	}	if (resourcesOptions.equals(PrivilegedOperation.CGROUP_ARG_PREFIX + PrivilegedOperation.CGROUP_ARG_NO_TASKS)) {	if (LOG.isDebugEnabled()) {	}	} else {	if (LOG.isDebugEnabled()) {	
using docker s cgroups options 

return;	}	if (resourcesOptions.equals(PrivilegedOperation.CGROUP_ARG_PREFIX + PrivilegedOperation.CGROUP_ARG_NO_TASKS)) {	if (LOG.isDebugEnabled()) {	}	} else {	if (LOG.isDebugEnabled()) {	}	String cGroupPath = "/" + cGroupsHandler.getRelativePathForCGroup(containerIdStr);	if (LOG.isDebugEnabled()) {	
using cgroup parent 

private boolean allowPrivilegedContainerExecution(Container container) throws ContainerExecutionException {	Map<String, String> environment = container.getLaunchContext() .getEnvironment();	String runPrivilegedContainerEnvVar = environment .get(ENV_DOCKER_CONTAINER_RUN_PRIVILEGED_CONTAINER);	if (runPrivilegedContainerEnvVar == null) {	return false;	}	if (!runPrivilegedContainerEnvVar.equalsIgnoreCase("true")) {	
not running a privileged container value of is invalid 

private boolean allowPrivilegedContainerExecution(Container container) throws ContainerExecutionException {	Map<String, String> environment = container.getLaunchContext() .getEnvironment();	String runPrivilegedContainerEnvVar = environment .get(ENV_DOCKER_CONTAINER_RUN_PRIVILEGED_CONTAINER);	if (runPrivilegedContainerEnvVar == null) {	return false;	}	if (!runPrivilegedContainerEnvVar.equalsIgnoreCase("true")) {	return false;	}	
privileged container requested for 

LOG.warn(message);	throw new ContainerExecutionException(message);	}	String submittingUser = container.getUser();	UserGroupInformation submitterUgi = UserGroupInformation .createRemoteUser(submittingUser);	if (!privilegedContainersAcl.isUserAllowed(submitterUgi)) {	String message = "Cannot launch privileged container. Submitting user (" + submittingUser + ") fails ACL check.";	LOG.warn(message);	throw new ContainerExecutionException(message);	}	
all checks pass launching privileged container for 

}	}	}	if (allowPrivilegedContainerExecution(container)) {	runCommand.setPrivileged();	}	String resourcesOpts = ctx.getExecutionAttribute(RESOURCES_OPTIONS);	addCGroupParentIfRequired(resourcesOpts, containerIdStr, runCommand);	String disableOverride = environment.get( ENV_DOCKER_CONTAINER_RUN_OVERRIDE_DISABLE);	if (disableOverride != null && disableOverride.equals("true")) {	
command override disabled 

runCommand.setOverrideCommandWithArgs(overrideCommands);	}	if(enableUserReMapping) {	runCommand.groupAdd(groups);	}	String commandFile = dockerClient.writeCommandToTempFile(runCommand, containerIdStr);	PrivilegedOperation launchOp = buildLaunchOp(ctx, commandFile, runCommand);	try {	privilegedOperationExecutor.executePrivilegedOperation(null, launchOp, null, null, false, false);	} catch (PrivilegedOperationException e) {	
launch container failed exception 

runCommand.setOverrideCommandWithArgs(overrideCommands);	}	if(enableUserReMapping) {	runCommand.groupAdd(groups);	}	String commandFile = dockerClient.writeCommandToTempFile(runCommand, containerIdStr);	PrivilegedOperation launchOp = buildLaunchOp(ctx, commandFile, runCommand);	try {	privilegedOperationExecutor.executePrivilegedOperation(null, launchOp, null, null, false, false);	} catch (PrivilegedOperationException e) {	
docker command used 

public String[] getIpAndHost(Container container) {	String containerId = container.getContainerId().toString();	DockerInspectCommand inspectCommand = new DockerInspectCommand(containerId).getIpAndHost();	try {	String commandFile = dockerClient.writeCommandToTempFile(inspectCommand, containerId);	PrivilegedOperation privOp = new PrivilegedOperation( PrivilegedOperation.OperationType.RUN_DOCKER_CMD);	privOp.appendArgs(commandFile);	String output = privilegedOperationExecutor .executePrivilegedOperation(null, privOp, null, null, true, false);	
docker inspect output for 

public String[] getIpAndHost(Container container) {	String containerId = container.getContainerId().toString();	DockerInspectCommand inspectCommand = new DockerInspectCommand(containerId).getIpAndHost();	try {	String commandFile = dockerClient.writeCommandToTempFile(inspectCommand, containerId);	PrivilegedOperation privOp = new PrivilegedOperation( PrivilegedOperation.OperationType.RUN_DOCKER_CMD);	privOp.appendArgs(commandFile);	String output = privilegedOperationExecutor .executePrivilegedOperation(null, privOp, null, null, true, false);	int index = output.lastIndexOf(',');	if (index == -1) {	
incorrect format for ip and host 

if (index == -1) {	return null;	}	String ips = output.substring(0, index).trim();	String host = output.substring(index+1).trim();	String[] ipAndHost = new String[2];	ipAndHost[0] = ips;	ipAndHost[1] = host;	return ipAndHost;	} catch (ContainerExecutionException e) {	
error when writing command to temp file 

return null;	}	String ips = output.substring(0, index).trim();	String host = output.substring(index+1).trim();	String[] ipAndHost = new String[2];	ipAndHost[0] = ips;	ipAndHost[1] = host;	return ipAndHost;	} catch (ContainerExecutionException e) {	} catch (PrivilegedOperationException e) {	
error when executing command 

List<String> localDirs = ctx.getExecutionAttribute(LOCAL_DIRS);	List<String> logDirs = ctx.getExecutionAttribute(LOG_DIRS);	String resourcesOpts = ctx.getExecutionAttribute(RESOURCES_OPTIONS);	PrivilegedOperation launchOp = new PrivilegedOperation( PrivilegedOperation.OperationType.LAUNCH_DOCKER_CONTAINER);	launchOp.appendArgs(runAsUser, ctx.getExecutionAttribute(USER), Integer.toString(PrivilegedOperation .RunAsUserCommand.LAUNCH_DOCKER_CONTAINER.getValue()), ctx.getExecutionAttribute(APPID), containerIdStr, containerWorkDir.toString(), nmPrivateContainerScriptPath.toUri().getPath(), ctx.getExecutionAttribute(NM_PRIVATE_TOKENS_PATH).toUri().getPath(), ctx.getExecutionAttribute(PID_FILE_PATH).toString(), StringUtils.join(PrivilegedOperation.LINUX_FILE_PATH_SEPARATOR, localDirs), StringUtils.join(PrivilegedOperation.LINUX_FILE_PATH_SEPARATOR, logDirs), commandFile, resourcesOpts);	String tcCommandFile = ctx.getExecutionAttribute(TC_COMMAND_FILE);	if (tcCommandFile != null) {	launchOp.appendArgs(tcCommandFile);	}	if (LOG.isDebugEnabled()) {	
launching container with cmd 

========================= hadoop sample_1862 =========================

public static Set<String> fetchColumnsFromFilterList( TimelineFilterList filterList) {	Set<String> strSet = new HashSet<String>();	for (TimelineFilter filter : filterList.getFilterList()) {	switch(filter.getFilterType()) {	case LIST: strSet.addAll(fetchColumnsFromFilterList((TimelineFilterList)filter));	break;	case KEY_VALUES: strSet.add(((TimelineKeyValuesFilter)filter).getKey());	break;	case EXISTS: strSet.add(((TimelineExistsFilter)filter).getValue());	break;	
unexpected filter type 

case LIST: list.addFilter(createHBaseFilterList(colPrefix, (TimelineFilterList)filter));	break;	case PREFIX: list.addFilter(createHBaseColQualPrefixFilter(colPrefix, (TimelinePrefixFilter)filter));	break;	case COMPARE: TimelineCompareFilter compareFilter = (TimelineCompareFilter)filter;	list.addFilter( createHBaseSingleColValueFilter( colPrefix.getColumnFamilyBytes(), colPrefix.getColumnPrefixBytes(compareFilter.getKey()), colPrefix.getValueConverter(). encodeValue(compareFilter.getValue()), getHBaseCompareOp(compareFilter.getCompareOp()), compareFilter.getKeyMustExist()));	break;	case KEY_VALUE: TimelineKeyValueFilter kvFilter = (TimelineKeyValueFilter)filter;	list.addFilter( createHBaseSingleColValueFilter( colPrefix.getColumnFamilyBytes(), colPrefix.getColumnPrefixBytes(kvFilter.getKey()), colPrefix.getValueConverter().encodeValue(kvFilter.getValue()), getHBaseCompareOp(kvFilter.getCompareOp()), kvFilter.getKeyMustExist()));	break;	
unexpected filter type 

========================= hadoop sample_1161 =========================

public void testInitializeReplQueuesEarly() throws Exception {	
starting testinitializereplqueuesearly 

public void testInitializeReplQueuesEarly() throws Exception {	BlockManagerTestUtil.setWritingPrefersLocalNode( cluster.getNamesystem().getBlockManager(), false);	cluster.startDataNodes(conf, 2, true, StartupOption.REGULAR, null);	cluster.waitActive();	
creating files 

public void testInitializeReplQueuesEarly() throws Exception {	BlockManagerTestUtil.setWritingPrefersLocalNode( cluster.getNamesystem().getBlockManager(), false);	cluster.startDataNodes(conf, 2, true, StartupOption.REGULAR, null);	cluster.waitActive();	DFSTestUtil.createFile(fs, TEST_PATH, 15*BLOCK_SIZE, (short)1, 1L);	
stopping all datanodes 

public void testInitializeReplQueuesEarly() throws Exception {	BlockManagerTestUtil.setWritingPrefersLocalNode( cluster.getNamesystem().getBlockManager(), false);	cluster.startDataNodes(conf, 2, true, StartupOption.REGULAR, null);	cluster.waitActive();	DFSTestUtil.createFile(fs, TEST_PATH, 15*BLOCK_SIZE, (short)1, 1L);	List<DataNodeProperties> dnprops = Lists.newLinkedList();	dnprops.add(cluster.stopDataNode(0));	dnprops.add(cluster.stopDataNode(0));	dnprops.add(cluster.stopDataNode(0));	cluster.getConfiguration(0).setFloat( DFSConfigKeys.DFS_NAMENODE_REPL_QUEUE_THRESHOLD_PCT_KEY, 1f/15f);	
restarting namenode 

List<DataNodeProperties> dnprops = Lists.newLinkedList();	dnprops.add(cluster.stopDataNode(0));	dnprops.add(cluster.stopDataNode(0));	dnprops.add(cluster.stopDataNode(0));	cluster.getConfiguration(0).setFloat( DFSConfigKeys.DFS_NAMENODE_REPL_QUEUE_THRESHOLD_PCT_KEY, 1f/15f);	cluster.restartNameNode();	final NameNode nn = cluster.getNameNode();	String status = nn.getNamesystem().getSafemode();	assertEquals("Safe mode is ON. The reported blocks 0 needs additional " + "14 blocks to reach the threshold 0.9990 of total blocks 15." + NEWLINE + "The number of live datanodes 0 has reached the minimum number 0. " + "Safe mode will be turned off automatically once the thresholds " + "have been reached.", status);	assertFalse("Mis-replicated block queues should not be initialized " + "until threshold is crossed", NameNodeAdapter.safeModeInitializedReplQueues(nn));	
restarting one datanode 

========================= hadoop sample_7666 =========================

public static void waitForContainerResourceUtilizationChange( ContainersMonitorImpl containersMonitor, int timeoutMsecs) throws InterruptedException {	int timeWaiting = 0;	while (0 == containersMonitor.getContainersUtilization() .compareTo(ResourceUtilization.newInstance(0, 0, 0.0f))) {	if (timeWaiting >= timeoutMsecs) {	break;	}	
monitor thread is waiting for resource utlization change 

========================= hadoop sample_1618 =========================

public void testSlowRegisterCall() throws YarnException, IOException, InterruptedException {	Thread registerAMThread = new Thread(new Runnable() {	public void run() {	try {	createAndRegisterApplicationMaster( RegisterApplicationMasterRequest.newInstance(null, 1001, null), attemptId);	} catch (Exception e) {	
register thread exception 

Thread registerAMThread = new Thread(new Runnable() {	public void run() {	try {	createAndRegisterApplicationMaster( RegisterApplicationMasterRequest.newInstance(null, 1001, null), attemptId);	} catch (Exception e) {	}	}	});	Object syncObj = MockResourceManagerFacade.getSyncObj();	synchronized (syncObj) {	
starting register thread 

try {	createAndRegisterApplicationMaster( RegisterApplicationMasterRequest.newInstance(null, 1001, null), attemptId);	} catch (Exception e) {	}	}	});	Object syncObj = MockResourceManagerFacade.getSyncObj();	synchronized (syncObj) {	registerAMThread.start();	try {	
test main starts waiting 

createAndRegisterApplicationMaster( RegisterApplicationMasterRequest.newInstance(null, 1001, null), attemptId);	} catch (Exception e) {	}	}	});	Object syncObj = MockResourceManagerFacade.getSyncObj();	synchronized (syncObj) {	registerAMThread.start();	try {	syncObj.wait();	
test main wait finished 

} catch (Exception e) {	}	}	});	Object syncObj = MockResourceManagerFacade.getSyncObj();	synchronized (syncObj) {	registerAMThread.start();	try {	syncObj.wait();	} catch (Exception e) {	
test main wait interrupted 

registerAMThread.start();	try {	syncObj.wait();	} catch (Exception e) {	}	}	allocateAsync(AllocateRequest.newInstance(0, 0, null, null, null), callback, attemptId);	synchronized (syncObj) {	syncObj.notifyAll();	}	
test main wait for register thread to finish 

try {	syncObj.wait();	} catch (Exception e) {	}	}	allocateAsync(AllocateRequest.newInstance(0, 0, null, null, null), callback, attemptId);	synchronized (syncObj) {	syncObj.notifyAll();	}	registerAMThread.join();	
register thread finished 

========================= hadoop sample_1248 =========================

public void testInheritedMethodsImplemented() throws Exception {	int errors = 0;	for (Method m : FileSystem.class.getDeclaredMethods()) {	if (Modifier.isStatic(m.getModifiers()) || Modifier.isPrivate(m.getModifiers()) || Modifier.isFinal(m.getModifiers())) {	continue;	}	try {	MustNotImplement.class.getMethod(m.getName(), m.getParameterTypes());	try {	HarFileSystem.class.getDeclaredMethod(m.getName(), m.getParameterTypes());	
harfilesystem must not implement 

MustNotImplement.class.getMethod(m.getName(), m.getParameterTypes());	try {	HarFileSystem.class.getDeclaredMethod(m.getName(), m.getParameterTypes());	errors++;	} catch (NoSuchMethodException ex) {	}	} catch (NoSuchMethodException exc) {	try {	HarFileSystem.class.getDeclaredMethod(m.getName(), m.getParameterTypes());	} catch (NoSuchMethodException exc2) {	
harfilesystem must implement 

========================= hadoop sample_3275 =========================

private int shellRun(String... args) throws Exception {	int exitCode = shell.run(args);	
exit 

========================= hadoop sample_3330 =========================

public void run() {	try {	startFlag.await();	if (Thread.currentThread().isInterrupted()) {	return;	}	final long initTime = TimeUnit.MILLISECONDS.convert( System.nanoTime(), TimeUnit.NANOSECONDS);	
start replay 

try {	final JobStory job = getNextJobFiltered();	if (null == job) {	return;	}	if (first < 0) {	first = job.getSubmissionTime();	}	final long current = job.getSubmissionTime();	if (current < last) {	
job out of order 

========================= hadoop sample_6132 =========================

final AtomicReference<String> errorMessage = new AtomicReference<String>();	final FSDataOutputStream out = fileSystem.create(file);	final Thread writer = new Thread(new Runnable() {	public void run() {	try {	while (!openerDone.get()) {	out.write(DFSTestUtil.generateSequentialBytes(0, writeSize));	out.hflush();	}	} catch (IOException e) {	
error in writer 

try {	while (!openerDone.get()) {	out.write(DFSTestUtil.generateSequentialBytes(0, writeSize));	out.hflush();	}	} catch (IOException e) {	} finally {	try {	out.close();	} catch (IOException e) {	
unable to close file 

for (int i = 0; !error.get() && i < numWrites; i++) {	final byte[] writeBuf = DFSTestUtil.generateSequentialBytes(i * writeSize, writeSize);	outputStream.write(writeBuf);	if (syncType == SyncType.SYNC) {	outputStream.hflush();	}	writerStarted.set(true);	}	} catch (IOException e) {	error.set(true);	
error writing to file 

}	writerStarted.set(true);	}	} catch (IOException e) {	error.set(true);	} finally {	outputStream.close();	}	writerDone.set(true);	} catch (Exception e) {	
error in writer 

});	Thread tailer = new Thread(new Runnable() {	public void run() {	try {	long startPos = 0;	while (!writerDone.get() && !error.get()) {	if (writerStarted.get()) {	try {	startPos = tailFile(file, startPos);	} catch (IOException e) {	
error tailing file s 

} catch (IOException e) {	throw new RuntimeException(e);	}	}	}	} catch (RuntimeException e) {	if (e.getCause() instanceof ChecksumException) {	error.set(true);	}	writer.interrupt();	
error in tailer 

}	}	});	writer.start();	tailer.start();	try {	writer.join();	tailer.join();	assertFalse( "error occurred, see log above", error.get() );	} catch (InterruptedException e) {	
interrupted waiting for writer or tailer to complete 

private boolean validateSequentialBytes(byte[] buf, int startPos, int len) {	for (int i = 0; i < len; i++) {	int expected = (i + startPos) % 127;	if (buf[i] % 127 != expected) {	
at position d got d and expected d 

private long tailFile(Path file, long startPos) throws IOException {	long numRead = 0;	FSDataInputStream inputStream = fileSystem.open(file);	inputStream.seek(startPos);	int len = 4 * 1024;	byte[] buf = new byte[len];	int read;	while ((read = inputStream.read(buf)) > -1) {	
read d bytes 

private long tailFile(Path file, long startPos) throws IOException {	long numRead = 0;	FSDataInputStream inputStream = fileSystem.open(file);	inputStream.seek(startPos);	int len = 4 * 1024;	byte[] buf = new byte[len];	int read;	while ((read = inputStream.read(buf)) > -1) {	if (!validateSequentialBytes(buf, (int) (startPos + numRead), read)) {	
invalid bytes s 

========================= hadoop sample_7689 =========================

public void testRpcBindHostKey() throws IOException {	Configuration conf = new HdfsConfiguration();	MiniDFSCluster cluster = null;	
testing without 

cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0).build();	cluster.waitActive();	String address = getRpcServerAddress(cluster);	assertThat("Bind address not expected to be wildcard by default.", address, not("/" + WILDCARD_ADDRESS));	} finally {	if (cluster != null) {	cluster.shutdown();	cluster = null;	}	}	
testing with 

public void testServiceRpcBindHostKey() throws IOException {	Configuration conf = new HdfsConfiguration();	MiniDFSCluster cluster = null;	
testing without 

cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0).build();	cluster.waitActive();	String address = getServiceRpcServerAddress(cluster);	assertThat("Bind address not expected to be wildcard by default.", address, not("/" + WILDCARD_ADDRESS));	} finally {	if (cluster != null) {	cluster.shutdown();	cluster = null;	}	}	
testing with 

public void testLifelineRpcBindHostKey() throws IOException {	Configuration conf = new HdfsConfiguration();	MiniDFSCluster cluster = null;	
testing without 

cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0).build();	cluster.waitActive();	String address = getLifelineRpcServerAddress(cluster);	assertThat("Bind address not expected to be wildcard by default.", address, not("/" + WILDCARD_ADDRESS));	} finally {	if (cluster != null) {	cluster.shutdown();	cluster = null;	}	}	
testing with 

public void testHttpBindHostKey() throws IOException {	Configuration conf = new HdfsConfiguration();	MiniDFSCluster cluster = null;	
testing without 

cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0).build();	cluster.waitActive();	String address = cluster.getNameNode().getHttpAddress().toString();	assertFalse("HTTP Bind address not expected to be wildcard by default.", address.startsWith(WILDCARD_ADDRESS));	} finally {	if (cluster != null) {	cluster.shutdown();	cluster = null;	}	}	
testing with 

public void testHttpsBindHostKey() throws Exception {	Configuration conf = new HdfsConfiguration();	MiniDFSCluster cluster = null;	
testing behavior without 

cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0).build();	cluster.waitActive();	String address = cluster.getNameNode().getHttpsAddress().toString();	assertFalse("HTTP Bind address not expected to be wildcard by default.", address.startsWith(WILDCARD_ADDRESS));	} finally {	if (cluster != null) {	cluster.shutdown();	cluster = null;	}	}	
testing behavior with 

========================= hadoop sample_7373 =========================

private void setupQueueConfiguration(CapacitySchedulerConfiguration conf) {	conf.setQueues(CapacitySchedulerConfiguration.ROOT, new String[] {A, B});	final String Q_A = CapacitySchedulerConfiguration.ROOT + "." + A;	conf.setCapacity(Q_A, 10);	final String Q_B = CapacitySchedulerConfiguration.ROOT + "." + B;	conf.setCapacity(Q_B, 90);	conf.setUserLimit(CapacitySchedulerConfiguration.ROOT + "." + A, 50);	conf.setUserLimitFactor(CapacitySchedulerConfiguration.ROOT + "." + A, 5.0f);	
setup top level queues a and b 

========================= hadoop sample_505 =========================

assertEquals("Empty file length == 0", 0, splits[0].getLength());	}	BitSet bits = new BitSet(length);	for (int j = 0; j < splits.length; j++) {	LOG.debug("split["+j+"]= " + splits[j]);	RecordReader<LongWritable, Text> reader = format.getRecordReader(splits[j], job, reporter);	try {	int count = 0;	while (reader.next(key, value)) {	int v = Integer.parseInt(value.toString());	
read 

}	BitSet bits = new BitSet(length);	for (int j = 0; j < splits.length; j++) {	LOG.debug("split["+j+"]= " + splits[j]);	RecordReader<LongWritable, Text> reader = format.getRecordReader(splits[j], job, reporter);	try {	int count = 0;	while (reader.next(key, value)) {	int v = Integer.parseInt(value.toString());	if (bits.get(v)) {	
conflict with in split at position 

for (int i = 0; i < length; i++) {	writer.write(Integer.toString(i));	writer.write("\n");	}	} finally {	writer.close();	}	for (long splitpos = 203418; splitpos < 203430; ++splitpos) {	TextInputFormat format = new TextInputFormat();	format.configure(conf);	
setting block size of the input file to 

InputSplit[] splits = format.getSplits(conf, 2);	LOG.info("splitting: got =        " + splits.length);	BitSet bits = new BitSet(length);	for (int j = 0; j < splits.length; j++) {	LOG.debug("split[" + j + "]= " + splits[j]);	RecordReader<LongWritable, Text> reader = format.getRecordReader(splits[j], conf, Reporter.NULL);	try {	int counter = 0;	while (reader.next(key, value)) {	int v = Integer.parseInt(value.toString());	
read 

LOG.info("splitting: got =        " + splits.length);	BitSet bits = new BitSet(length);	for (int j = 0; j < splits.length; j++) {	LOG.debug("split[" + j + "]= " + splits[j]);	RecordReader<LongWritable, Text> reader = format.getRecordReader(splits[j], conf, Reporter.NULL);	try {	int counter = 0;	while (reader.next(key, value)) {	int v = Integer.parseInt(value.toString());	if (bits.get(v)) {	
conflict with in split at position 

InputSplit[] splits = format.getSplits(conf, numSplits);	LOG.info("splitting: got =        " + splits.length);	BitSet bits = new BitSet(length);	for (int j = 0; j < splits.length; j++) {	LOG.debug("split["+j+"]= " + splits[j]);	RecordReader<LongWritable, Text> reader = format.getRecordReader(splits[j], conf, Reporter.NULL);	try {	int counter = 0;	while (reader.next(key, value)) {	int v = Integer.parseInt(value.toString());	
read 

LOG.info("splitting: got =        " + splits.length);	BitSet bits = new BitSet(length);	for (int j = 0; j < splits.length; j++) {	LOG.debug("split["+j+"]= " + splits[j]);	RecordReader<LongWritable, Text> reader = format.getRecordReader(splits[j], conf, Reporter.NULL);	try {	int counter = 0;	while (reader.next(key, value)) {	int v = Integer.parseInt(value.toString());	if (bits.get(v)) {	
conflict with in split at position 

Arrays.fill(b, (byte) 0);	position += b.length;	return b.length;	}	public void reset() {	position=0;	}	};	final LongWritable key = new LongWritable();	final Text val = new Text();	
reading a line from dev null 

========================= hadoop sample_5421 =========================

public MiniYARNCluster( String testName, int numResourceManagers, int numNodeManagers, int numLocalDirs, int numLogDirs, boolean enableAHS) {	super(testName.replace("$", ""));	this.numLocalDirs = numLocalDirs;	this.numLogDirs = numLogDirs;	this.enableAHS = enableAHS;	String testSubDir = testName.replace("$", "");	File targetWorkDir = new File("target", testSubDir);	try {	FileContext.getLocalFSFileContext().delete( new Path(targetWorkDir.getAbsolutePath()), true);	} catch (Exception e) {	
could not cleanup 

private synchronized void startResourceManager(final int index) {	try {	resourceManagers[index].start();	if (resourceManagers[index].getServiceState() != STATE.STARTED) {	throw new IOException( "ResourceManager failed to start. Final state is " + resourceManagers[index].getServiceState());	}	} catch (Throwable t) {	throw new YarnRuntimeException(t);	}	Configuration conf = resourceManagers[index].getConfig();	
miniyarn resourcemanager address 

private synchronized void startResourceManager(final int index) {	try {	resourceManagers[index].start();	if (resourceManagers[index].getServiceState() != STATE.STARTED) {	throw new IOException( "ResourceManager failed to start. Final state is " + resourceManagers[index].getServiceState());	}	} catch (Throwable t) {	throw new YarnRuntimeException(t);	}	Configuration conf = resourceManagers[index].getConfig();	
miniyarn resourcemanager web address 

protected synchronized void serviceStart() throws Exception {	startResourceManager(index);	if(index == 0 && resourceManagers[index].getRMContext().isHAEnabled()) {	resourceManagers[index].getRMContext().getRMAdminService() .transitionToActive(new HAServiceProtocol.StateChangeRequestInfo( HAServiceProtocol.RequestSource.REQUEST_BY_USER_FORCED));	}	Configuration conf = resourceManagers[index].getConfig();	
starting resourcemanager 

protected synchronized void serviceStart() throws Exception {	startResourceManager(index);	if(index == 0 && resourceManagers[index].getRMContext().isHAEnabled()) {	resourceManagers[index].getRMContext().getRMAdminService() .transitionToActive(new HAServiceProtocol.StateChangeRequestInfo( HAServiceProtocol.RequestSource.REQUEST_BY_USER_FORCED));	}	Configuration conf = resourceManagers[index].getConfig();	
miniyarn resourcemanager address 

protected synchronized void serviceStart() throws Exception {	startResourceManager(index);	if(index == 0 && resourceManagers[index].getRMContext().isHAEnabled()) {	resourceManagers[index].getRMContext().getRMAdminService() .transitionToActive(new HAServiceProtocol.StateChangeRequestInfo( HAServiceProtocol.RequestSource.REQUEST_BY_USER_FORCED));	}	Configuration conf = resourceManagers[index].getConfig();	
miniyarn resourcemanager web address 

private void waitForAppMastersToFinish(long timeoutMillis) throws InterruptedException {	long started = System.currentTimeMillis();	synchronized (appMasters) {	while (!appMasters.isEmpty() && System.currentTimeMillis() - started < timeoutMillis) {	appMasters.wait(1000);	}	}	if (!appMasters.isEmpty()) {	
stopping rm while some app masters are still alive 

protected synchronized void serviceStop() throws Exception {	if (resourceManagers[index] != null) {	waitForAppMastersToFinish(5000);	resourceManagers[index].stop();	}	if (Shell.WINDOWS) {	String testWorkDirPath = testWorkDir.getAbsolutePath();	try {	FileContext.getLocalFSFileContext().delete(new Path(testWorkDirPath), true);	} catch (IOException e) {	
could not cleanup symlink 

config.setInt(YarnConfiguration.NM_PMEM_MB, config.getInt( YarnConfiguration.YARN_MINICLUSTER_NM_PMEM_MB, YarnConfiguration.DEFAULT_YARN_MINICLUSTER_NM_PMEM_MB));	config.set(YarnConfiguration.NM_ADDRESS, MiniYARNCluster.getHostname() + ":0");	config.set(YarnConfiguration.NM_LOCALIZER_ADDRESS, MiniYARNCluster.getHostname() + ":0");	config.set(YarnConfiguration.NM_COLLECTOR_SERVICE_ADDRESS, MiniYARNCluster.getHostname() + ":0");	WebAppUtils .setNMWebAppHostNameAndPort(config, MiniYARNCluster.getHostname(), 0);	config.setBoolean( YarnConfiguration.NM_ENABLE_HARDWARE_CAPABILITY_DETECTION, false);	if (!config.getBoolean( YarnConfiguration.YARN_MINICLUSTER_CONTROL_RESOURCE_MONITORING, YarnConfiguration. DEFAULT_YARN_MINICLUSTER_CONTROL_RESOURCE_MONITORING)) {	config.setBoolean( YarnConfiguration.NM_CONTAINER_MONITOR_ENABLED, false);	config.setLong(YarnConfiguration.NM_RESOURCE_MON_INTERVAL_MS, 0);	}	
starting nm 

private String prepareDirs(String dirType, int numDirs) {	File []dirs = new File[numDirs];	String dirsString = "";	for (int i = 0; i < numDirs; i++) {	dirs[i]= new File(testWorkDir, MiniYARNCluster.this.getName() + "-" + dirType + "Dir-nm-" + index + "_" + i);	dirs[i].mkdirs();	
created dir in 

}	protected ResourceTracker getRMClient() {	final ResourceTrackerService rt = getResourceManager().getResourceTrackerService();	final RecordFactory recordFactory = RecordFactoryProvider.getRecordFactory(null);	return new ResourceTracker() {	public NodeHeartbeatResponse nodeHeartbeat( NodeHeartbeatRequest request) throws YarnException, IOException {	NodeHeartbeatResponse response;	try {	response = rt.nodeHeartbeat(request);	} catch (YarnException e) {	
exception in heartbeat from node 

} catch (YarnException e) {	throw e;	}	return response;	}	public RegisterNodeManagerResponse registerNodeManager( RegisterNodeManagerRequest request) throws YarnException, IOException {	RegisterNodeManagerResponse response;	try {	response = rt.registerNodeManager(request);	} catch (YarnException e) {	
exception in node registration from 

public boolean waitForNodeManagersToConnect(long timeout) throws YarnException, InterruptedException {	GetClusterMetricsRequest req = GetClusterMetricsRequest.newInstance();	for (int i = 0; i < timeout / 10; i++) {	ResourceManager rm = getResourceManager();	if (rm == null) {	throw new YarnException("Can not find the active RM.");	}	else if (nodeManagers.length == rm.getClientRMService() .getClusterMetrics(req).getClusterMetrics().getNumNodeManagers()) {	
all node managers connected in miniyarncluster 

for (int i = 0; i < timeout / 10; i++) {	ResourceManager rm = getResourceManager();	if (rm == null) {	throw new YarnException("Can not find the active RM.");	}	else if (nodeManagers.length == rm.getClientRMService() .getClusterMetrics(req).getClusterMetrics().getNumNodeManagers()) {	return true;	}	Thread.sleep(10);	}	
node managers did not connect within 

protected synchronized void serviceStart() throws Exception {	appHistoryServer.start();	if (appHistoryServer.getServiceState() != STATE.STARTED) {	IOException ioe = new IOException( "ApplicationHistoryServer failed to start. Final state is " + appHistoryServer.getServiceState());	ioe.initCause(appHistoryServer.getFailureCause());	throw ioe;	}	
miniyarn applicationhistoryserver address 

protected synchronized void serviceStart() throws Exception {	appHistoryServer.start();	if (appHistoryServer.getServiceState() != STATE.STARTED) {	IOException ioe = new IOException( "ApplicationHistoryServer failed to start. Final state is " + appHistoryServer.getServiceState());	ioe.initCause(appHistoryServer.getFailureCause());	throw ioe;	}	
miniyarn applicationhistoryserver web address 

protected void createAMRMProxyService(Configuration conf) {	this.amrmProxyEnabled = conf.getBoolean(YarnConfiguration.AMRM_PROXY_ENABLED, YarnConfiguration.DEFAULT_AMRM_PROXY_ENABLED) || conf.getBoolean(YarnConfiguration.DIST_SCHEDULING_ENABLED, YarnConfiguration.DEFAULT_DIST_SCHEDULING_ENABLED);	if (this.amrmProxyEnabled) {	
customamrmproxyservice is enabled all the am rm requests will be intercepted by the proxy 

protected void createAMRMProxyService(Configuration conf) {	this.amrmProxyEnabled = conf.getBoolean(YarnConfiguration.AMRM_PROXY_ENABLED, YarnConfiguration.DEFAULT_AMRM_PROXY_ENABLED) || conf.getBoolean(YarnConfiguration.DIST_SCHEDULING_ENABLED, YarnConfiguration.DEFAULT_DIST_SCHEDULING_ENABLED);	if (this.amrmProxyEnabled) {	AMRMProxyService amrmProxyService = useRpc ? new AMRMProxyService(getContext(), dispatcher) : new ShortCircuitedAMRMProxy(getContext(), dispatcher);	this.setAMRMProxyService(amrmProxyService);	addService(this.getAMRMProxyService());	} else {	
customamrmproxyservice is disabled 

protected void createAMRMProxyService(Configuration conf) {	this.amrmProxyEnabled = conf.getBoolean(YarnConfiguration.AMRM_PROXY_ENABLED, YarnConfiguration.DEFAULT_AMRM_PROXY_ENABLED) || conf.getBoolean(YarnConfiguration.DIST_SCHEDULING_ENABLED, YarnConfiguration.DEFAULT_DIST_SCHEDULING_ENABLED);	if (this.amrmProxyEnabled) {	
customamrmproxyservice is enabled all the am rm requests will be intercepted by the proxy 

protected void createAMRMProxyService(Configuration conf) {	this.amrmProxyEnabled = conf.getBoolean(YarnConfiguration.AMRM_PROXY_ENABLED, YarnConfiguration.DEFAULT_AMRM_PROXY_ENABLED) || conf.getBoolean(YarnConfiguration.DIST_SCHEDULING_ENABLED, YarnConfiguration.DEFAULT_DIST_SCHEDULING_ENABLED);	if (this.amrmProxyEnabled) {	AMRMProxyService amrmProxyService = useRpc ? new AMRMProxyService(getContext(), dispatcher) : new ShortCircuitedAMRMProxy(getContext(), dispatcher);	this.setAMRMProxyService(amrmProxyService);	addService(this.getAMRMProxyService());	} else {	
customamrmproxyservice is disabled 

========================= hadoop sample_1931 =========================

private static void warnDeprecation() {	if (!hasWarnedDeprecation.getAndSet(true)) {	
is deprecated and will be removed in future releases use or instead 

========================= hadoop sample_5972 =========================

private void clearRequests() {	schedulerKeys.clear();	schedulerKeyToPlacementSets.clear();	
application requests cleared 

private void updatePendingResources(ResourceRequest lastRequest, ResourceRequest request, SchedulerRequestKey schedulerKey, QueueMetrics metrics) {	int lastRequestContainers = (lastRequest != null) ? lastRequest.getNumContainers() : 0;	if (request.getNumContainers() <= 0) {	if (lastRequestContainers >= 0) {	schedulerKeys.remove(schedulerKey);	schedulerKeyToPlacementSets.remove(schedulerKey);	}	
checking for deactivate of application 

========================= hadoop sample_1032 =========================

public void testEnumContainers() throws Throwable {	describe("Enumerating all the WASB test containers");	int count = 0;	CloudStorageAccount storageAccount = getTestAccount().getRealAccount();	CloudBlobClient blobClient = storageAccount.createCloudBlobClient();	Iterable<CloudBlobContainer> containers = blobClient.listContainers(CONTAINER_PREFIX);	for (CloudBlobContainer container : containers) {	count++;	
container uri 

public void testEnumContainers() throws Throwable {	describe("Enumerating all the WASB test containers");	int count = 0;	CloudStorageAccount storageAccount = getTestAccount().getRealAccount();	CloudBlobClient blobClient = storageAccount.createCloudBlobClient();	Iterable<CloudBlobContainer> containers = blobClient.listContainers(CONTAINER_PREFIX);	for (CloudBlobContainer container : containers) {	count++;	}	
found test containers 

public void testDeleteContainers() throws Throwable {	describe("Delete all the WASB test containers");	int count = 0;	CloudStorageAccount storageAccount = getTestAccount().getRealAccount();	CloudBlobClient blobClient = storageAccount.createCloudBlobClient();	Iterable<CloudBlobContainer> containers = blobClient.listContainers(CONTAINER_PREFIX);	for (CloudBlobContainer container : containers) {	
container uri 

describe("Delete all the WASB test containers");	int count = 0;	CloudStorageAccount storageAccount = getTestAccount().getRealAccount();	CloudBlobClient blobClient = storageAccount.createCloudBlobClient();	Iterable<CloudBlobContainer> containers = blobClient.listContainers(CONTAINER_PREFIX);	for (CloudBlobContainer container : containers) {	if (container.deleteIfExists()) {	count++;	}	}	
deleted test containers 

========================= hadoop sample_6348 =========================

return ContainerAllocation.APP_SKIPPED;	}	}	if (!appInfo.acceptNodePartition(schedulerKey, node.getPartition(), schedulingMode)) {	ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation( activitiesManager, node, application, priority, ActivityDiagnosticConstant. PRIORITY_SKIPPED_BECAUSE_NODE_PARTITION_DOES_NOT_MATCH_REQUEST);	return ContainerAllocation.PRIORITY_SKIPPED;	}	if (!application.getCSLeafQueue().getReservationContinueLooking()) {	if (!shouldAllocOrReserveNewContainer(schedulerKey, required)) {	if (LOG.isDebugEnabled()) {	
doesn t need containers based on reservation algo 

private ContainerAllocation assignContainer(Resource clusterResource, FiCaSchedulerNode node, SchedulerRequestKey schedulerKey, PendingAsk pendingAsk, NodeType type, RMContainer rmContainer, SchedulingMode schedulingMode, ResourceLimits currentResoureLimits) {	Priority priority = schedulerKey.getPriority();	if (LOG.isDebugEnabled()) {	LOG.debug("assignContainers: node=" + node.getNodeName() + " application=" + application.getApplicationId() + " priority=" + schedulerKey.getPriority() + " pendingAsk=" + pendingAsk + " type=" + type);	}	Resource capability = pendingAsk.getPerAllocationResource();	Resource available = node.getUnallocatedResource();	Resource totalResource = node.getTotalResource();	if (!Resources.lessThanOrEqual(rc, clusterResource, capability, totalResource)) {	
node does not have sufficient resource for ask node total capability 

}	ContainerAllocation result = new ContainerAllocation(unreservedContainer, pendingAsk.getPerAllocationResource(), AllocationState.ALLOCATED);	result.containerNodeType = type;	result.setToKillContainers(toKillContainers);	return result;	} else {	if (shouldAllocOrReserveNewContainer || rmContainer != null) {	if (reservationsContinueLooking && rmContainer == null) {	if (needToUnreserve) {	if (LOG.isDebugEnabled()) {	
we needed to unreserve to be able to allocate 

========================= hadoop sample_913 =========================

public MRApp(ApplicationAttemptId appAttemptId, ContainerId amContainerId, int maps, int reduces, boolean autoComplete, String testName, boolean cleanOnStart, int startCount, Clock clock, boolean unregistered, String assignedQueue) {	super(appAttemptId, amContainerId, NM_HOST, NM_PORT, NM_HTTP_PORT, clock, System.currentTimeMillis());	this.testWorkDir = new File("target", testName);	testAbsPath = new Path(testWorkDir.getAbsolutePath());	
pathused 

public MRApp(ApplicationAttemptId appAttemptId, ContainerId amContainerId, int maps, int reduces, boolean autoComplete, String testName, boolean cleanOnStart, int startCount, Clock clock, boolean unregistered, String assignedQueue) {	super(appAttemptId, amContainerId, NM_HOST, NM_PORT, NM_HTTP_PORT, clock, System.currentTimeMillis());	this.testWorkDir = new File("target", testName);	testAbsPath = new Path(testWorkDir.getAbsolutePath());	if (cleanOnStart) {	testAbsPath = new Path(testWorkDir.getAbsolutePath());	try {	FileContext.getLocalFSFileContext().delete(testAbsPath, true);	} catch (Exception e) {	
could not cleanup 

conf.setBoolean(MRJobConfig.MAP_SPECULATIVE, mapSpeculative);	conf.setBoolean(MRJobConfig.REDUCE_SPECULATIVE, reduceSpeculative);	init(conf);	start();	DefaultMetricsSystem.shutdown();	Job job = getContext().getAllJobs().values().iterator().next();	if (assignedQueue != null) {	job.setQueueName(assignedQueue);	}	String jobFile = MRApps.getJobFile(conf, user, TypeConverter.fromYarn(job.getID()));	
writing job conf to 

========================= hadoop sample_5134 =========================

public int run(String[] args) {	ParsedOutput parsedOpts = null;	try {	ArgumentParser argHolder = new ArgumentParser(args);	parsedOpts = argHolder.parse();	if (parsedOpts.shouldOutputHelp()) {	parsedOpts.outputHelp();	return 1;	}	} catch (Exception e) {	
unable to parse arguments due to error 

try {	ArgumentParser argHolder = new ArgumentParser(args);	parsedOpts = argHolder.parse();	if (parsedOpts.shouldOutputHelp()) {	parsedOpts.outputHelp();	return 1;	}	} catch (Exception e) {	return 1;	}	
running with option list 

return 1;	}	ConfigExtractor config = null;	try {	ConfigMerger cfgMerger = new ConfigMerger();	Configuration cfg = cfgMerger.getMerged(parsedOpts, new Configuration(base));	if (cfg != null) {	config = new ConfigExtractor(cfg);	}	} catch (Exception e) {	
unable to merge config due to error 

try {	ConfigMerger cfgMerger = new ConfigMerger();	Configuration cfg = cfgMerger.getMerged(parsedOpts, new Configuration(base));	if (cfg != null) {	config = new ConfigExtractor(cfg);	}	} catch (Exception e) {	return 1;	}	if (config == null) {	
unable to merge config options 

if (cfg != null) {	config = new ConfigExtractor(cfg);	}	} catch (Exception e) {	return 1;	}	if (config == null) {	return 1;	}	try {	
options are 

}	} catch (Exception e) {	return 1;	}	if (config == null) {	return 1;	}	try {	ConfigExtractor.dumpOptions(config);	} catch (Exception e) {	
unable to dump options due to error 

if (config == null) {	return 1;	}	try {	ConfigExtractor.dumpOptions(config);	} catch (Exception e) {	return 1;	}	boolean jobOk = false;	try {	
running job 

try {	ConfigExtractor.dumpOptions(config);	} catch (Exception e) {	return 1;	}	boolean jobOk = false;	try {	runJob(config);	jobOk = true;	} catch (Exception e) {	
unable to run job due to error 

return 1;	}	boolean jobOk = false;	try {	runJob(config);	jobOk = true;	} catch (Exception e) {	}	if (jobOk) {	try {	
reporting on job 

boolean jobOk = false;	try {	runJob(config);	jobOk = true;	} catch (Exception e) {	}	if (jobOk) {	try {	writeReport(config);	} catch (Exception e) {	
unable to report on job due to error 

}	if (jobOk) {	try {	writeReport(config);	} catch (Exception e) {	}	}	boolean cleanUp = getBool(parsedOpts .getValue(ConfigOption.CLEANUP.getOpt()));	if (cleanUp) {	try {	
cleaning up job 

try {	writeReport(config);	} catch (Exception e) {	}	}	boolean cleanUp = getBool(parsedOpts .getValue(ConfigOption.CLEANUP.getOpt()));	if (cleanUp) {	try {	cleanup(config);	} catch (Exception e) {	
unable to cleanup job due to error 

private void writeReport(ConfigExtractor cfg) throws Exception {	Path dn = cfg.getOutputPath();	
writing report using contents of 

}	}	fileReader.close();	fileReader = null;	}	File resFile = null;	if (cfg.getResultFile() != null) {	resFile = new File(cfg.getResultFile());	}	if (resFile != null) {	
report results being placed to logging output and to file 

fileReader.close();	fileReader = null;	}	File resFile = null;	if (cfg.getResultFile() != null) {	resFile = new File(cfg.getResultFile());	}	if (resFile != null) {	reportWriter = new PrintWriter(new FileOutputStream(resFile));	} else {	
report results being placed to logging output 

private void cleanup(ConfigExtractor cfg) throws IOException {	Path base = cfg.getBaseDirectory();	if (base != null) {	
attempting to recursively delete 

========================= hadoop sample_5564 =========================

break;	}	if (saslClient == null && !switchToSimple) {	List<String> serverAuthMethods = new ArrayList<String>();	for (SaslAuth authType : authTypes) {	serverAuthMethods.add(authType.getMethod());	}	throw new AccessControlException( "Client cannot authenticate via:" + serverAuthMethods);	}	if (LOG.isDebugEnabled()) {	
use authentication for protocol 

}	if (LOG.isDebugEnabled()) {	LOG.debug("RPC Server's Kerberos principal name for protocol=" + protocol.getCanonicalName() + " is " + serverPrincipal);	}	break;	}	default: throw new IOException("Unknown authentication method " + method);	}	String mechanism = method.getMechanismName();	if (LOG.isDebugEnabled()) {	
creating sasl client to authenticate to service at 

private Token<?> getServerToken(SaslAuth authType) throws IOException {	TokenInfo tokenInfo = SecurityUtil.getTokenInfo(protocol, conf);	
get token info proto info 

private void sendSaslMessage(OutputStream out, RpcSaslProto message) throws IOException {	if (LOG.isDebugEnabled()) {	
sending sasl message 

private void readNextRpcPacket() throws IOException {	
reading next wrapped rpc packet 

RpcResponseHeaderProto.Builder headerBuilder = RpcResponseHeaderProto.newBuilder();	headerBuilder.mergeDelimitedFrom(bis);	boolean isWrapped = false;	if (headerBuilder.getCallId() == AuthProtocol.SASL.callId) {	RpcSaslProto.Builder saslMessage = RpcSaslProto.newBuilder();	saslMessage.mergeDelimitedFrom(bis);	if (saslMessage.getState() == SaslState.WRAP) {	isWrapped = true;	byte[] token = saslMessage.getToken().toByteArray();	if (LOG.isDebugEnabled()) {	
unwrapping token of length 

public void write(byte[] buf, int off, int len) throws IOException {	if (LOG.isDebugEnabled()) {	
wrapping token of length 

nc = (NameCallback) callback;	} else if (callback instanceof PasswordCallback) {	pc = (PasswordCallback) callback;	} else if (callback instanceof RealmCallback) {	rc = (RealmCallback) callback;	} else {	throw new UnsupportedCallbackException(callback, "Unrecognized SASL client callback");	}	}	if (nc != null) {	
sasl client callback setting username 

} else if (callback instanceof RealmCallback) {	rc = (RealmCallback) callback;	} else {	throw new UnsupportedCallbackException(callback, "Unrecognized SASL client callback");	}	}	if (nc != null) {	nc.setName(userName);	}	if (pc != null) {	
sasl client callback setting userpassword 

throw new UnsupportedCallbackException(callback, "Unrecognized SASL client callback");	}	}	if (nc != null) {	nc.setName(userName);	}	if (pc != null) {	pc.setPassword(userPassword);	}	if (rc != null) {	
sasl client callback setting realm 

========================= hadoop sample_3799 =========================

assertNotNull(mainAttributes);	assertTrue(mainAttributes.containsKey(Attributes.Name.CLASS_PATH));	String classPathAttr = mainAttributes.getValue(Attributes.Name.CLASS_PATH);	assertNotNull(classPathAttr);	assertFalse(classPathAttr.isEmpty());	} finally {	if (jarFile != null) {	try {	jarFile.close();	} catch (IOException e) {	
exception closing jarfile 

========================= hadoop sample_2921 =========================

try {	allRanges = blob.downloadPageRanges(new BlobRequestOptions(), opContext);	} catch (StorageException e) {	throw new IOException(e);	}	if (allRanges.size() > 0) {	if (allRanges.get(0).getStartOffset() != 0) {	throw badStartRangeException(blob, allRanges.get(0));	}	if (allRanges.size() > 1) {	
blob s has d page ranges beyond the first range only reading the first range 

========================= hadoop sample_6431 =========================

public static void main(String[] args) {	
version 

========================= hadoop sample_2188 =========================

static FSDataOutputStream createFile(FileSystem fileSys, Path name, int repl, final long blockSize) throws IOException {	FSDataOutputStream stm = fileSys.create(name, true, fileSys.getConf() .getInt(CommonConfigurationKeys.IO_FILE_BUFFER_SIZE_KEY, 4096), (short) repl, blockSize);	
createfile created with replica 

stm.readFully(b, 0, thisread);	if (verifyData) {	if (thisread == readSize) {	assertTrue("file is corrupted at or after byte " + (fileSize - bytesToRead), Arrays.equals(b, compb));	} else {	for (int k = 0; k < thisread; k++) {	assertTrue("file is corrupted at or after byte " + (fileSize - bytesToRead), b[k] == compb[k]);	}	}	}	
before update to read read already 

if (verifyData) {	if (thisread == readSize) {	assertTrue("file is corrupted at or after byte " + (fileSize - bytesToRead), Arrays.equals(b, compb));	} else {	for (int k = 0; k < thisread; k++) {	assertTrue("file is corrupted at or after byte " + (fileSize - bytesToRead), b[k] == compb[k]);	}	}	}	bytesToRead -= thisread;	
after update to read read already 

public void runTest(final long blockSize) throws IOException {	final long fileSize = blockSize + 1L;	Configuration conf = new Configuration();	MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).build();	FileSystem fs = cluster.getFileSystem();	try {	Path file1 = new Path("/tmp/TestLargeBlock", blockSize + ".dat");	FSDataOutputStream stm = createFile(fs, file1, 1, blockSize);	
file created with file size blocksize 

public void runTest(final long blockSize) throws IOException {	final long fileSize = blockSize + 1L;	Configuration conf = new Configuration();	MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).build();	FileSystem fs = cluster.getFileSystem();	try {	Path file1 = new Path("/tmp/TestLargeBlock", blockSize + ".dat");	FSDataOutputStream stm = createFile(fs, file1, 1, blockSize);	assertTrue(file1 + " should be a file", fs.getFileStatus(file1).isFile());	writeFile(stm, fileSize);	
file written to 

final long fileSize = blockSize + 1L;	Configuration conf = new Configuration();	MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).build();	FileSystem fs = cluster.getFileSystem();	try {	Path file1 = new Path("/tmp/TestLargeBlock", blockSize + ".dat");	FSDataOutputStream stm = createFile(fs, file1, 1, blockSize);	assertTrue(file1 + " should be a file", fs.getFileStatus(file1).isFile());	writeFile(stm, fileSize);	stm.close();	
file closed 

========================= hadoop sample_7670 =========================

public void testTeraSort() throws Exception {	runTeraGen(createJobConf(), SORT_INPUT_PATH);	try {	runTeraGen(createJobConf(), SORT_INPUT_PATH);	fail("Teragen output overwritten!");	} catch (FileAlreadyExistsException fae) {	
expected exception 

try {	runTeraGen(createJobConf(), SORT_INPUT_PATH);	fail("Teragen output overwritten!");	} catch (FileAlreadyExistsException fae) {	}	runTeraSort(createJobConf(), SORT_INPUT_PATH, SORT_OUTPUT_PATH);	try {	runTeraSort(createJobConf(), SORT_INPUT_PATH, SORT_OUTPUT_PATH);	fail("Terasort output overwritten!");	} catch (FileAlreadyExistsException fae) {	
expected exception 

========================= hadoop sample_5708 =========================

LOG.info("hftpfs.getContentSummary = " + cs);	Assert.assertEquals(filesize, cs.getLength());	final FSDataInputStream in = hftpfs.open(hftpfs.makeQualified(filepath));	long bytesRead = 0;	try {	for(int r; (r = in.read(buffer)) != -1; ) {	bytesRead += r;	}	Assert.fail();	} catch(IOException ioe) {	
good get an exception 

========================= hadoop sample_7735 =========================

private int shellRun(String... args) throws Exception {	int exitCode = shell.run(args);	
exit 

========================= hadoop sample_3326 =========================

private int readMoreData() throws IOException {	try {	inStream.readFully(lengthBuf);	int length = unsignedBytesToInt(lengthBuf);	
actual length is 

========================= hadoop sample_3776 =========================

public synchronized void threadFailed(Throwable t) {	if (err == null) err = t;	
failed 

========================= hadoop sample_2864 =========================

public void enableTrash(String bpid) {	if (trashEnabledBpids.add(bpid)) {	getBPStorage(bpid).stopTrashCleaner();	
enabled trash for bpid 

public void clearTrash(String bpid) {	if (trashEnabledBpids.contains(bpid)) {	getBPStorage(bpid).clearTrash();	trashEnabledBpids.remove(bpid);	
cleared trash for bpid 

private StorageDirectory loadStorageDirectory(DataNode datanode, NamespaceInfo nsInfo, File dataDir, StartupOption startOpt, List<Callable<StorageDirectory>> callables) throws IOException {	StorageDirectory sd = new StorageDirectory(dataDir, null, false);	try {	StorageState curState = sd.analyzeStorage(startOpt, this, true);	switch (curState) {	case NORMAL: break;	
storage directory does not exist 

private StorageDirectory loadStorageDirectory(DataNode datanode, NamespaceInfo nsInfo, File dataDir, StartupOption startOpt, List<Callable<StorageDirectory>> callables) throws IOException {	StorageDirectory sd = new StorageDirectory(dataDir, null, false);	try {	StorageState curState = sd.analyzeStorage(startOpt, this, true);	switch (curState) {	case NORMAL: break;	throw new IOException("Storage directory " + dataDir + " does not exist");	
storage directory is not formatted for namespace formatting 

final StorageDirectory sd = loadStorageDirectory( datanode, nsInfo, root, startOpt, callables);	if (callables.isEmpty()) {	addStorageDir(sd);	success.add(dataDir);	} else {	for(Callable<StorageDirectory> c : callables) {	tasks.add(new UpgradeTask(dataDir, executor.submit(c)));	}	}	} catch (IOException e) {	
failed to add storage directory 

addStorageDir(sd);	success.add(dataDir);	} else {	for(Callable<StorageDirectory> c : callables) {	tasks.add(new UpgradeTask(dataDir, executor.submit(c)));	}	}	} catch (IOException e) {	}	} else {	
storage directory has already been used 

tasks.add(new UpgradeTask(dataDir, executor.submit(c)));	}	}	} catch (IOException e) {	}	} else {	success.add(dataDir);	}	}	if (!tasks.isEmpty()) {	
loaddatastorage upgrade tasks 

} else {	success.add(dataDir);	}	}	if (!tasks.isEmpty()) {	for(UpgradeTask t : tasks) {	try {	addStorageDir(t.future.get());	success.add(t.dataDir);	} catch (ExecutionException e) {	
failed to upgrade storage directory 

if (callables.isEmpty()) {	for(StorageDirectory sd : dirs) {	success.add(sd);	}	} else {	for(Callable<StorageDirectory> c : callables) {	tasks.add(new UpgradeTask(dataDir, executor.submit(c)));	}	}	} catch (IOException e) {	
failed to add storage directory for block pool 

}	} else {	for(Callable<StorageDirectory> c : callables) {	tasks.add(new UpgradeTask(dataDir, executor.submit(c)));	}	}	} catch (IOException e) {	}	}	if (!tasks.isEmpty()) {	
loadblockpoolslicestorage upgrade tasks 

}	}	} catch (IOException e) {	}	}	if (!tasks.isEmpty()) {	for(UpgradeTask t : tasks) {	try {	success.add(t.future.get());	} catch (ExecutionException e) {	
failed to upgrade storage directory for block pool 

static void makeBlockPoolDataDir(Collection<File> dataDirs, Configuration conf) throws IOException {	if (conf == null) conf = new HdfsConfiguration();	LocalFileSystem localFS = FileSystem.getLocal(conf);	FsPermission permission = new FsPermission(conf.get( DFSConfigKeys.DFS_DATANODE_DATA_DIR_PERMISSION_KEY, DFSConfigKeys.DFS_DATANODE_DATA_DIR_PERMISSION_DEFAULT));	for (File data : dataDirs) {	try {	DiskChecker.checkDir(localFS, new Path(data.toURI()), permission);	} catch ( IOException e ) {	
invalid directory in 

public boolean isPreUpgradableLayout(StorageDirectory sd) throws IOException {	File oldF = new File(sd.getRoot(), "storage");	if (!oldF.exists()) {	return false;	}	try (RandomAccessFile oldFile = new RandomAccessFile(oldF, "rws");	FileLock oldLock = oldFile.getChannel().tryLock()) {	if (null == oldLock) {	
unable to acquire file lock on path 

private void doUpgrade(final StorageDirectory sd, final NamespaceInfo nsInfo, final File prevDir, final File tmpDir, final File bbwDir, final File toDir, final int oldLV, Configuration conf) throws IOException {	linkAllBlocks(tmpDir, bbwDir, toDir, oldLV, conf);	clusterID = nsInfo.getClusterID();	upgradeProperties(sd);	rename(tmpDir, prevDir);	
upgrade of is complete 

public void run() {	try {	deleteDir(tmpDir);	if (bbwDir.exists()) {	deleteDir(bbwDir);	}	} catch(IOException ex) {	
finalize upgrade for failed 

public void run() {	try {	deleteDir(tmpDir);	if (bbwDir.exists()) {	deleteDir(bbwDir);	}	} catch(IOException ex) {	}	
finalize upgrade for is complete 

HardLink hardLink = new HardLink();	if (DataNodeLayoutVersion.supports( LayoutVersion.Feature.APPEND_RBW_DIR, diskLayoutVersion)) {	linkBlocks(fromDir, toDir, STORAGE_DIR_FINALIZED, diskLayoutVersion, hardLink, conf);	linkBlocks(fromDir, toDir, STORAGE_DIR_RBW, diskLayoutVersion, hardLink, conf);	} else {	linkBlocks(fromDir, new File(toDir, STORAGE_DIR_FINALIZED), diskLayoutVersion, hardLink, conf);	if (fromBbwDir.exists()) {	linkBlocks(fromBbwDir, new File(toDir, STORAGE_DIR_RBW), diskLayoutVersion, hardLink, conf);	}	}	
linked blocks from to 

private static void linkBlocks(File from, File to, int oldLV, HardLink hl, Configuration conf) throws IOException {	
start linking block files from to 

private static void linkBlocks(File from, File to, int oldLV, HardLink hl, Configuration conf) throws IOException {	boolean upgradeToIdBasedLayout = false;	if (oldLV > DataNodeLayoutVersion.Feature.BLOCKID_BASED_LAYOUT_32_by_32 .getInfo().getLayoutVersion() && to.getName().equals(STORAGE_DIR_FINALIZED)) {	upgradeToIdBasedLayout = true;	}	final ArrayList<LinkArgs> idBasedLayoutSingleLinks = Lists.newArrayList();	linkBlocksHelper(from, to, oldLV, hl, upgradeToIdBasedLayout, to, idBasedLayoutSingleLinks);	final ArrayList<LinkArgs> duplicates = findDuplicateEntries(idBasedLayoutSingleLinks);	if (!duplicates.isEmpty()) {	
there are duplicate block entries within the same volume 

List<LinkArgs> highest = highestGenstamps.get(blockId);	if (highest != null) {	boolean found = false;	for (LinkArgs high : highest) {	if (high.src.getParent().equals(duplicate.src.getParent())) {	found = true;	break;	}	}	if (!found) {	
unexpectedly low genstamp on 

}	long blockId = Block.getBlockId(duplicate.src.getName());	LinkArgs prevLongest = longestBlockFiles.get(blockId);	if (prevLongest == null) {	longestBlockFiles.put(blockId, duplicate);	continue;	}	long blockLength = duplicate.src.length();	long prevBlockLength = prevLongest.src.length();	if (blockLength < prevBlockLength) {	
unexpectedly short length on 

if (prevLongest == null) {	longestBlockFiles.put(blockId, duplicate);	continue;	}	long blockLength = duplicate.src.length();	long prevBlockLength = prevLongest.src.length();	if (blockLength < prevBlockLength) {	continue;	}	if (blockLength > prevBlockLength) {	
unexpectedly short length on 

longestBlockFiles.put(blockId, duplicate);	}	for (Iterator<LinkArgs> iter = all.iterator(); iter.hasNext(); ) {	LinkArgs args = iter.next();	long blockId = Block.getBlockId(args.src.getName());	LinkArgs bestDuplicate = longestBlockFiles.get(blockId);	if (bestDuplicate == null) {	continue;	}	if (!bestDuplicate.src.getParent().equals(args.src.getParent())) {	
discarding 

========================= hadoop sample_7918 =========================

public void run() {	String name = Thread.currentThread().getName();	try {	Thread.sleep(TASK_SLEEP_MSEC);	} catch (InterruptedException e) {	
thread interrupted 

private static void ensureCreated() throws Exception {	if (tpe == null) {	
creating thread pool 

private static void ensureDestroyed() throws Exception {	if (tpe == null) {	return;	}	int shutdownTries = SHUTDOWN_WAIT_TRIES;	tpe.shutdown();	if (!tpe.isShutdown()) {	throw new RuntimeException("Shutdown had no effect.");	}	while (!tpe.awaitTermination(SHUTDOWN_WAIT_MSEC, TimeUnit.MILLISECONDS)) {	
waiting for thread pool shutdown 

if (tpe == null) {	return;	}	int shutdownTries = SHUTDOWN_WAIT_TRIES;	tpe.shutdown();	if (!tpe.isShutdown()) {	throw new RuntimeException("Shutdown had no effect.");	}	while (!tpe.awaitTermination(SHUTDOWN_WAIT_MSEC, TimeUnit.MILLISECONDS)) {	if (shutdownTries-- <= 0) {	
failed to terminate thread pool gracefully 

========================= hadoop sample_5925 =========================

protected CuratorService startCuratorServiceInstance(String name, boolean secure) {	Configuration clientConf = new Configuration();	clientConf.set(KEY_REGISTRY_ZK_ROOT, "/");	clientConf.setBoolean(KEY_REGISTRY_SECURE, secure);	describe(LOG, "Starting Curator service");	CuratorService curatorService = new CuratorService(name, secureZK);	curatorService.init(clientConf);	curatorService.start();	
curator binding 

========================= hadoop sample_2649 =========================

static private void handleException(String operation, Throwable e, int singleFileExceptions) {	
exception while 

========================= hadoop sample_5533 =========================

private ConnectionContext getConnection( UserGroupInformation ugi, String nsId, String rpcAddress) throws IOException {	ConnectionContext connection = null;	try {	connection = this.connectionManager.getConnection(ugi, rpcAddress);	
user nn is using connection 

private ConnectionContext getConnection( UserGroupInformation ugi, String nsId, String rpcAddress) throws IOException {	ConnectionContext connection = null;	try {	connection = this.connectionManager.getConnection(ugi, rpcAddress);	} catch (Exception ex) {	
cannot open nn client to address 

private RetryDecision shouldRetry(final IOException ioe, final int retryCount) throws IOException {	try {	final RetryPolicy.RetryAction a = this.retryPolicy.shouldRetry(ioe, retryCount, 0, true);	return a.action;	} catch (Exception ex) {	
re throwing api exception no more retries 

}	String msg = "No namenode available to invoke " + method.getName() + " " + Arrays.toString(params);	LOG.error(msg);	for (Entry<FederationNamenodeContext, IOException> entry : ioes.entrySet()) {	FederationNamenodeContext namenode = entry.getKey();	String nsId = namenode.getNameserviceId();	String nnId = namenode.getNamenodeId();	String addr = namenode.getRpcAddress();	IOException ioe = entry.getValue();	if (ioe instanceof StandbyException) {	
at is in standby 

String msg = "No namenode available to invoke " + method.getName() + " " + Arrays.toString(params);	LOG.error(msg);	for (Entry<FederationNamenodeContext, IOException> entry : ioes.entrySet()) {	FederationNamenodeContext namenode = entry.getKey();	String nsId = namenode.getNameserviceId();	String nnId = namenode.getNamenodeId();	String addr = namenode.getRpcAddress();	IOException ioe = entry.getValue();	if (ioe instanceof StandbyException) {	} else {	
at error 

private Object invoke(int retryCount, final Method method, final Object obj, final Object... params) throws IOException {	try {	return method.invoke(obj, params);	} catch (IllegalAccessException e) {	
unexpected exception while proxying api 

private Object invoke(int retryCount, final Method method, final Object obj, final Object... params) throws IOException {	try {	return method.invoke(obj, params);	} catch (IllegalAccessException e) {	return null;	} catch (IllegalArgumentException e) {	
unexpected exception while proxying api 

}	if (ioe instanceof RemoteException) {	RemoteException re = (RemoteException)ioe;	ret = new RemoteException(re.getClassName(), msg);	} else {	Class<? extends IOException> ioeClass = ioe.getClass();	try {	Constructor<? extends IOException> constructor = ioeClass.getDeclaredConstructor(String.class);	ret = constructor.newInstance(msg);	} catch (ReflectiveOperationException e) {	
could not create exception 

}	if (firstResult == null) {	firstResult = result;	}	} catch (IOException ioe) {	lastThrownException = (IOException) ioe;	if (firstThrownException == null) {	firstThrownException = lastThrownException;	}	} catch (Exception e) {	
unexpected exception proxying to 

Object result = future.get();	results.put(location, result);	} catch (CancellationException ce) {	T loc = orderedLocations.get(i);	String msg = "Invocation to \"" + loc + "\" for \"" + method + "\" timed out";	LOG.error(msg);	IOException ioe = new IOException(msg);	exceptions.put(location, ioe);	} catch (ExecutionException ex) {	Throwable cause = ex.getCause();	
canot execute in 

}	if (results.isEmpty()) {	T location = orderedLocations.get(0);	IOException ioe = exceptions.get(location);	if (ioe != null) {	throw ioe;	}	}	return results;	} catch (InterruptedException ex) {	
unexpected error while invoking api 

========================= hadoop sample_8301 =========================

protected void serviceStart() throws Exception {	
starting router clientrmservice 

Configuration conf = getConfig();	YarnRPC rpc = YarnRPC.create(conf);	UserGroupInformation.setConfiguration(conf);	this.listenerEndpoint = conf.getSocketAddr(YarnConfiguration.ROUTER_BIND_HOST, YarnConfiguration.ROUTER_CLIENTRM_ADDRESS, YarnConfiguration.DEFAULT_ROUTER_CLIENTRM_ADDRESS, YarnConfiguration.DEFAULT_ROUTER_CLIENTRM_PORT);	int maxCacheSize = conf.getInt(YarnConfiguration.ROUTER_PIPELINE_CACHE_MAX_SIZE, YarnConfiguration.DEFAULT_ROUTER_PIPELINE_CACHE_MAX_SIZE);	this.userPipelineMap = Collections.synchronizedMap( new LRUCacheHashMap<String, RequestInterceptorChainWrapper>( maxCacheSize, true));	Configuration serverConf = new Configuration(conf);	int numWorkerThreads = serverConf.getInt(YarnConfiguration.RM_CLIENT_THREAD_COUNT, YarnConfiguration.DEFAULT_RM_CLIENT_THREAD_COUNT);	this.server = rpc.getServer(ApplicationClientProtocol.class, this, listenerEndpoint, serverConf, null, numWorkerThreads);	this.server.start();	
router clientrmservice listening on address 

protected void serviceStop() throws Exception {	
stopping router clientrmservice 

private void initializePipeline(String user) {	RequestInterceptorChainWrapper chainWrapper = null;	synchronized (this.userPipelineMap) {	if (this.userPipelineMap.containsKey(user)) {	
request to start an already existing user was received so ignoring 

private void initializePipeline(String user) {	RequestInterceptorChainWrapper chainWrapper = null;	synchronized (this.userPipelineMap) {	if (this.userPipelineMap.containsKey(user)) {	return;	}	chainWrapper = new RequestInterceptorChainWrapper();	this.userPipelineMap.put(user, chainWrapper);	}	
initializing request processing pipeline for application for the user 

========================= hadoop sample_1989 =========================

public void handle(RMContainerEvent event) {	if (LOG.isDebugEnabled()) {	
processing of type 

public void handle(RMContainerEvent event) {	if (LOG.isDebugEnabled()) {	}	try {	writeLock.lock();	RMContainerState oldState = getState();	try {	stateMachine.doTransition(event.getType(), event);	} catch (InvalidStateTransitionException e) {	
can t handle this event at current state 

public void handle(RMContainerEvent event) {	if (LOG.isDebugEnabled()) {	}	try {	writeLock.lock();	RMContainerState oldState = getState();	try {	stateMachine.doTransition(event.getType(), event);	} catch (InvalidStateTransitionException e) {	
invalid event on container 

if (LOG.isDebugEnabled()) {	}	try {	writeLock.lock();	RMContainerState oldState = getState();	try {	stateMachine.doTransition(event.getType(), event);	} catch (InvalidStateTransitionException e) {	}	if (oldState != getState()) {	
container transitioned from to 

public RMContainerState transition(RMContainerImpl container, RMContainerEvent event) {	NMContainerStatus report = ((RMContainerRecoverEvent) event).getContainerReport();	if (report.getContainerState().equals(ContainerState.COMPLETE)) {	ContainerStatus status = ContainerStatus.newInstance(report.getContainerId(), report.getContainerState(), report.getDiagnostics(), report.getContainerExitStatus());	new FinishedTransition().transition(container, new RMContainerFinishedEvent(container.getContainerId(), status, RMContainerEventType.FINISHED));	return RMContainerState.COMPLETED;	} else if (report.getContainerState().equals(ContainerState.RUNNING)) {	container.eventHandler.handle(new RMAppRunningOnNodeEvent(container .getApplicationAttemptId().getApplicationId(), container.nodeId));	return RMContainerState.RUNNING;	} else {	
rmcontainer received unexpected recover event with container state while recovering 

========================= hadoop sample_1120 =========================

private void handleDeprecation() {	
handling deprecation for all properties in config 

private void handleDeprecation() {	DeprecationContext deprecations = deprecationContext.get();	Set<Object> keys = new HashSet<Object>();	keys.addAll(getProps().keySet());	for (Object item: keys) {	
handling deprecation for 

public static synchronized void reloadExistingConfigurations() {	if (LOG.isDebugEnabled()) {	
reloading existing configurations 

========================= hadoop sample_4288 =========================

public void corruptData() throws IOException {	checkFile(blockFile);	
corrupting block file 

public void corruptData(byte[] newContent) throws IOException {	checkFile(blockFile);	
corrupting block file with new content 

public void truncateData(long newSize) throws IOException {	
truncating block file 

public void deleteData() throws IOException {	
deleting block file 

public void corruptMeta() throws IOException {	checkFile(metaFile);	
corrupting meta file 

public void deleteMeta() throws IOException {	
deleting metadata file 

public void truncateMeta(long newSize) throws IOException {	
truncating metadata file 

public MaterializedReplica getMaterializedReplica(ExtendedBlock block) throws ReplicaNotFoundException {	File blockFile;	try {	blockFile = dataset.getBlockFile( block.getBlockPoolId(), block.getBlockId());	} catch (IOException e) {	
block file for does not existed 

========================= hadoop sample_7277 =========================

}	if (!localFS.exists(ownerDBPath)) {	if (!localFS.mkdirs(ownerDBPath)) {	throw new IOException("Couldn't create directory for leveldb " + "timeline store " + ownerDBPath);	}	localFS.setPermission(ownerDBPath, LEVELDB_DIR_UMASK);	}	}	options.maxOpenFiles(conf.getInt( TIMELINE_SERVICE_LEVELDB_MAX_OPEN_FILES, DEFAULT_TIMELINE_SERVICE_LEVELDB_MAX_OPEN_FILES));	options.writeBufferSize(conf.getInt( TIMELINE_SERVICE_LEVELDB_WRITE_BUFFER_SIZE, DEFAULT_TIMELINE_SERVICE_LEVELDB_WRITE_BUFFER_SIZE));	
using leveldb path 

protected void serviceStop() throws Exception {	if (deletionThread != null) {	deletionThread.interrupt();	
waiting for deletion thread to complete its current action 

protected void serviceStop() throws Exception {	if (deletionThread != null) {	deletionThread.interrupt();	try {	deletionThread.join();	} catch (InterruptedException e) {	
interrupted while waiting for deletion thread to complete closing db now 

public void run() {	Thread.currentThread().setName("Leveldb Timeline Store Retention");	while (true) {	long timestamp = System.currentTimeMillis() - ttl;	try {	discardOldEntities(timestamp);	Thread.sleep(ttlInterval);	} catch (IOException e) {	LOG.error(e.toString());	} catch (InterruptedException e) {	
deletion thread received interrupt exiting 

Object o = null;	String keyStr = parseRemainingKey(key, prefixlen + OTHER_INFO_COLUMN.length);	try {	o = fstConf.asObject(iterator.peekNext().getValue());	entity.addOtherInfo(keyStr, o);	} catch (Exception ignore) {	try {	o = fstConf224.asObject(iterator.peekNext().getValue());	entity.addOtherInfo(keyStr, o);	} catch (Exception e) {	
error while decoding otherinfo 

TimelineEvent event = getEntityEvent(null, key, prefixlen + EVENTS_COLUMN.length, iterator.peekNext().getValue());	if (event != null) {	entity.addEvent(event);	}	}	} else if (key[prefixlen] == DOMAIN_ID_COLUMN[0]) {	byte[] v = iterator.peekNext().getValue();	String domainId = new String(v, UTF_8);	entity.setDomainId(domainId);	} else {	
found unexpected column for entity s of type s 

TimelinePutError error = new TimelinePutError();	error.setEntityId(entity.getEntityId());	error.setEntityType(entity.getEntityType());	error.setErrorCode(TimelinePutError.EXPIRED_ENTITY);	response.addError(error);	return putCount;	}	WriteBatch indexWriteBatch = indexRollingWriteBatch.getWriteBatch();	putCount += writePrimaryFilterEntries(indexWriteBatch, primaryFilters, markerKey, EMPTY_BYTES);	} catch (IOException e) {	
error putting entity of type 

WriteBatch relatedWriteBatch = relatedRollingWriteBatch.getWriteBatch();	byte[] relatedEntityStartTime = writeReverseOrderedLong(relatedEntityStartAndInsertTime);	byte[] key = createDomainIdKey(relatedEntity.getId(), relatedEntity.getType(), relatedEntityStartTime);	relatedWriteBatch.put(key, entity.getDomainId().getBytes(UTF_8));	++putCount;	relatedWriteBatch.put( createRelatedEntityKey(relatedEntity.getId(), relatedEntity.getType(), relatedEntityStartTime, entity.getEntityId(), entity.getEntityType()), EMPTY_BYTES);	++putCount;	relatedWriteBatch.put( createEntityMarkerKey(relatedEntity.getId(), relatedEntity.getType(), relatedEntityStartTime), EMPTY_BYTES);	++putCount;	} catch (IOException e) {	
error putting related entity of type for entity of type 

public TimelinePutResponse put(TimelineEntities entities) {	if (LOG.isDebugEnabled()) {	
starting put 

}	} finally {	for (RollingWriteBatch entityRollingWriteBatch : entityUpdates.values()) {	entityRollingWriteBatch.close();	}	for (RollingWriteBatch indexRollingWriteBatch : indexUpdates.values()) {	indexRollingWriteBatch.close();	}	}	if (LOG.isDebugEnabled()) {	
put new leveldb entity entries and new leveldb index entries from timeline entities 

TimelineEvent event = new TimelineEvent();	event.setTimestamp(ts);	event.setEventType(tstype);	Object o = null;	try {	o = fstConf.asObject(value);	} catch (Exception ignore) {	try {	o = fstConf224.asObject(value);	} catch (Exception e) {	
error while decoding 

byte[] bytes = kp.getRemainingBytes();	Object value = null;	try {	value = fstConf.asObject(bytes);	entity.addPrimaryFilter(name, value);	} catch (Exception ignore) {	try {	value = fstConf224.asObject(bytes);	entity.addPrimaryFilter(name, value);	} catch (Exception e) {	
error while decoding 

private void checkVersion() throws IOException {	Version loadedVersion = loadVersion();	
loaded timeline store version info 

private void checkVersion() throws IOException {	Version loadedVersion = loadVersion();	if (loadedVersion.equals(getCurrentVersion())) {	return;	}	if (loadedVersion.isCompatibleTo(getCurrentVersion())) {	
storing timeline store version info 

} else if (key[prefix.length] == OWNER_COLUMN[0]) {	domain.setOwner(new String(value, UTF_8));	} else if (key[prefix.length] == READER_COLUMN[0]) {	domain.setReaders(new String(value, UTF_8));	} else if (key[prefix.length] == WRITER_COLUMN[0]) {	domain.setWriters(new String(value, UTF_8));	} else if (key[prefix.length] == TIMESTAMP_COLUMN[0]) {	domain.setCreatedTime(readReverseOrderedLong(value, 0));	domain.setModifiedTime(readReverseOrderedLong(value, 8));	} else {	
unrecognized domain column 

========================= hadoop sample_2075 =========================

public void serviceStop() throws Exception {	stopped = true;	if (handler != null) {	
stop 

========================= hadoop sample_694 =========================

public synchronized int read() throws IOException {	int result;	try {	result = in.read();	} catch (IOException e) {	
received ioexception while reading attempting to reopen 

public synchronized int read() throws IOException {	int result;	try {	result = in.read();	} catch (IOException e) {	LOG.debug("{}", e, e);	try {	reopen(pos);	result = in.read();	} catch (EOFException eof) {	
eof on input stream read 

public synchronized int read(byte[] b, int off, int len) throws IOException {	if (in == null) {	throw new EOFException("Cannot read closed stream");	}	int result = -1;	try {	result = in.read(b, off, len);	} catch (EOFException eof) {	throw eof;	} catch (IOException e) {	
received ioexception while reading attempting to reopen 

private synchronized void reopen(long pos) throws IOException {	
reopening key for reading at position 

public NativeS3FsOutputStream(Configuration conf, NativeFileSystemStore store, String key, Progressable progress, int bufferSize) throws IOException {	this.conf = conf;	this.key = key;	this.backupFile = newBackupFile();	
outputstream for key writing to tempfile 

public NativeS3FsOutputStream(Configuration conf, NativeFileSystemStore store, String key, Progressable progress, int bufferSize) throws IOException {	this.conf = conf;	this.key = key;	this.backupFile = newBackupFile();	try {	this.digest = MessageDigest.getInstance("MD5");	this.backupStream = new BufferedOutputStream(new DigestOutputStream( new FileOutputStream(backupFile), this.digest));	} catch (NoSuchAlgorithmException e) {	
cannot load digest algorithm skipping message integrity check 

public synchronized void close() throws IOException {	if (closed) {	return;	}	backupStream.close();	
outputstream for key closed now beginning upload 

public synchronized void close() throws IOException {	if (closed) {	return;	}	backupStream.close();	try {	byte[] md5Hash = digest == null ? null : digest.digest();	store.storeFile(key, backupFile, md5Hash);	} finally {	if (!backupFile.delete()) {	
could not delete temporary file 

backupStream.close();	try {	byte[] md5Hash = digest == null ? null : digest.digest();	store.storeFile(key, backupFile, md5Hash);	} finally {	if (!backupFile.delete()) {	}	super.close();	closed = true;	}	
outputstream for key upload complete 

public FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress) throws IOException {	if (exists(f) && !overwrite) {	throw new FileAlreadyExistsException("File already exists: " + f);	}	if(LOG.isDebugEnabled()) {	
creating new file in 

public boolean delete(Path f, boolean recurse) throws IOException {	FileStatus status;	try {	status = getFileStatus(f);	} catch (FileNotFoundException e) {	if(LOG.isDebugEnabled()) {	
delete called for but file does not exist so returning false 

return false;	}	Path absolutePath = makeAbsolute(f);	String key = pathToKey(absolutePath);	if (status.isDirectory()) {	if (!recurse && listStatus(f).length > 0) {	throw new IOException("Can not delete " + f + " as is a not empty directory and recurse option is false");	}	createParent(f);	if(LOG.isDebugEnabled()) {	
deleting directory 

store.delete(file.getKey());	}	priorLastKey = listing.getPriorLastKey();	} while (priorLastKey != null);	try {	store.delete(key + FOLDER_SUFFIX);	} catch (FileNotFoundException e) {	}	} else {	if(LOG.isDebugEnabled()) {	
deleting file 

public FileStatus getFileStatus(Path f) throws IOException {	Path absolutePath = makeAbsolute(f);	String key = pathToKey(absolutePath);	if (key.length() == 0) {	return newDirectory(absolutePath);	}	if(LOG.isDebugEnabled()) {	
getfilestatus retrieving metadata for key 

Path absolutePath = makeAbsolute(f);	String key = pathToKey(absolutePath);	if (key.length() == 0) {	return newDirectory(absolutePath);	}	if(LOG.isDebugEnabled()) {	}	FileMetadata meta = store.retrieveMetadata(key);	if (meta != null) {	if(LOG.isDebugEnabled()) {	
getfilestatus returning file for key 

if(LOG.isDebugEnabled()) {	}	FileMetadata meta = store.retrieveMetadata(key);	if (meta != null) {	if(LOG.isDebugEnabled()) {	}	return newFile(meta, absolutePath);	}	if (store.retrieveMetadata(key + FOLDER_SUFFIX) != null) {	if(LOG.isDebugEnabled()) {	
getfilestatus returning directory for key as exists 

if(LOG.isDebugEnabled()) {	}	return newFile(meta, absolutePath);	}	if (store.retrieveMetadata(key + FOLDER_SUFFIX) != null) {	if(LOG.isDebugEnabled()) {	}	return newDirectory(absolutePath);	}	if(LOG.isDebugEnabled()) {	
getfilestatus listing key 

if (store.retrieveMetadata(key + FOLDER_SUFFIX) != null) {	if(LOG.isDebugEnabled()) {	}	return newDirectory(absolutePath);	}	if(LOG.isDebugEnabled()) {	}	PartialListing listing = store.list(key, 1);	if (listing.getFiles().length > 0 || listing.getCommonPrefixes().length > 0) {	if(LOG.isDebugEnabled()) {	
getfilestatus returning directory for key as it has contents 

}	if(LOG.isDebugEnabled()) {	}	PartialListing listing = store.list(key, 1);	if (listing.getFiles().length > 0 || listing.getCommonPrefixes().length > 0) {	if(LOG.isDebugEnabled()) {	}	return newDirectory(absolutePath);	}	if(LOG.isDebugEnabled()) {	
getfilestatus could not find key 

private boolean mkdir(Path f) throws IOException {	try {	FileStatus fileStatus = getFileStatus(f);	if (fileStatus.isFile()) {	throw new FileAlreadyExistsException(String.format( "Can't make directory for path '%s' since it is a file.", f));	}	} catch (FileNotFoundException e) {	if(LOG.isDebugEnabled()) {	
making dir in 

public FSDataInputStream open(Path f, int bufferSize) throws IOException {	FileStatus fs = getFileStatus(f);	if (fs.isDirectory()) {	throw new FileNotFoundException("'" + f + "' is a directory");	}	
opening for reading 

String srcKey = pathToKey(makeAbsolute(src));	if (srcKey.length() == 0) {	return false;	}	final String debugPreamble = "Renaming '" + src + "' to '" + dst + "' - ";	String dstKey;	try {	boolean dstIsFile = getFileStatus(dst).isFile();	if (dstIsFile) {	if(LOG.isDebugEnabled()) {	
returning false as dst is an already existing file 

final String debugPreamble = "Renaming '" + src + "' to '" + dst + "' - ";	String dstKey;	try {	boolean dstIsFile = getFileStatus(dst).isFile();	if (dstIsFile) {	if(LOG.isDebugEnabled()) {	}	return false;	} else {	if(LOG.isDebugEnabled()) {	
using dst as output directory 

if(LOG.isDebugEnabled()) {	}	return false;	} else {	if(LOG.isDebugEnabled()) {	}	dstKey = pathToKey(makeAbsolute(new Path(dst, src.getName())));	}	} catch (FileNotFoundException e) {	if(LOG.isDebugEnabled()) {	
using dst as output destination 

}	dstKey = pathToKey(makeAbsolute(new Path(dst, src.getName())));	}	} catch (FileNotFoundException e) {	if(LOG.isDebugEnabled()) {	}	dstKey = pathToKey(makeAbsolute(dst));	try {	if (getFileStatus(dst.getParent()).isFile()) {	if(LOG.isDebugEnabled()) {	
returning false as dst parent exists and is a file 

}	dstKey = pathToKey(makeAbsolute(dst));	try {	if (getFileStatus(dst.getParent()).isFile()) {	if(LOG.isDebugEnabled()) {	}	return false;	}	} catch (FileNotFoundException ex) {	if(LOG.isDebugEnabled()) {	
returning false as dst parent does not exist 

if(LOG.isDebugEnabled()) {	}	return false;	}	}	boolean srcIsFile;	try {	srcIsFile = getFileStatus(src).isFile();	} catch (FileNotFoundException e) {	if(LOG.isDebugEnabled()) {	
returning false as src does not exist 

boolean srcIsFile;	try {	srcIsFile = getFileStatus(src).isFile();	} catch (FileNotFoundException e) {	if(LOG.isDebugEnabled()) {	}	return false;	}	if (srcIsFile) {	if(LOG.isDebugEnabled()) {	
src is file so doing copy then delete in 

}	return false;	}	if (srcIsFile) {	if(LOG.isDebugEnabled()) {	}	store.copy(srcKey, dstKey);	store.delete(srcKey);	} else {	if(LOG.isDebugEnabled()) {	
src is directory so copying contents 

String priorLastKey = null;	do {	PartialListing listing = store.list(srcKey, S3_MAX_LISTING_LENGTH, priorLastKey, true);	for (FileMetadata file : listing.getFiles()) {	keysToDelete.add(file.getKey());	store.copy(file.getKey(), dstKey + file.getKey().substring(srcKey.length()));	}	priorLastKey = listing.getPriorLastKey();	} while (priorLastKey != null);	if(LOG.isDebugEnabled()) {	
all files in src copied now removing src files 

if(LOG.isDebugEnabled()) {	}	for (String key: keysToDelete) {	store.delete(key);	}	try {	store.delete(srcKey + FOLDER_SUFFIX);	} catch (FileNotFoundException e) {	}	if(LOG.isDebugEnabled()) {	
done 

========================= hadoop sample_6036 =========================

public void testRenameNonExistentPath() throws Exception {	if (!renameSupported()) return;	Path src = getTestRootPath(fSys, "test/hadoop/nonExistent");	Path dst = getTestRootPath(fSys, "test/new/newpath");	try {	rename(src, dst, false, false, false, Rename.NONE);	Assert.fail("Should throw FileNotFoundException");	} catch (IOException e) {	
XXX 

========================= hadoop sample_3234 =========================

public void cleanupTestDir() throws Exception {	Path workDirPath = new Path(testWorkDir.getAbsolutePath());	
cleaning test directory 

========================= hadoop sample_2130 =========================

protected void sendBlockReports(DatanodeRegistration dnR, String poolId, StorageBlockReport[] reports) throws IOException {	int i = 0;	for (StorageBlockReport report : reports) {	
sending block report for storage 

========================= hadoop sample_7298 =========================

id.readFields(in);	System.out.println("Token (" + id + ") for " + token.getService());	}	return null;	}	if (renew) {	for (Token<?> token : readTokens(tokenFile, conf)) {	if (token.isManaged()) {	long result = token.renew(conf);	if (LOG.isDebugEnabled()) {	
renewed token for until 

long result = token.renew(conf);	if (LOG.isDebugEnabled()) {	}	}	}	} else if (cancel) {	for(Token<?> token: readTokens(tokenFile, conf)) {	if (token.isManaged()) {	token.cancel(conf);	if (LOG.isDebugEnabled()) {	
cancelled token for 

========================= hadoop sample_7762 =========================


}	}	} finally {	statement.close();	}	} finally {	connection.close();	}	} catch (SQLException e) {	if (LOG.isErrorEnabled()) {	
error during populateresources call 

}	}	} finally {	statement.close();	}	} finally {	connection.close();	}	} catch (SQLException e) {	if (LOG.isErrorEnabled()) {	
error during populateresources call caught exception 

String[] parts = category.split("\\.");	if (parts.length >= 2) {	recordTypeContext = parts[0];	recordTypeName = parts[1];	if (containsArguments(propertyKey) && parts.length > 2) {	tagPairsPattern = StringUtils.join(Arrays.copyOfRange(parts, 2, parts.length), ".");	}	metricsDefinitionSet.add( new MetricDefinition( startTime, endTime, recordTypeContext, recordTypeName, tagPairsPattern, metricName, serviceName != null && serviceName.toLowerCase().equals("hbase") ? serviceName.toLowerCase() : componentName.toLowerCase(), hostName, propertyKey, id, temporalInfo) );	} else {	if (LOG.isWarnEnabled()) {	
can t get metrics for 

========================= ambari sample_480 =========================

for (Map<String, Object> properties : getPropertyMaps(predicate)) {	String artifactName = (String) properties.get(ARTIFACT_NAME_PROPERTY_ID);	if (artifactName == null || artifactName.equals(KERBEROS_DESCRIPTOR_NAME)) {	String stackName = (String) properties.get(STACK_NAME_PROPERTY_ID);	String stackVersion = (String) properties.get(STACK_VERSION_PROPERTY_ID);	String stackService = (String) properties.get(STACK_SERVICE_NAME_PROPERTY_ID);	Map<String, Object> descriptor;	try {	descriptor = getKerberosDescriptor(stackName, stackVersion, stackService);	} catch (IOException e) {	
unable to process kerberos descriptor properties 

descriptor = Collections.singletonMap(componentName, (Object) componentMetrics);	}	} else {	Map<String, Map<String, PropertyInfo>> clusterMetrics = PropertyHelper.getMetricPropertyIds(Resource.Type.Cluster);	Map<String, Map<String, PropertyInfo>> hostMetrics = PropertyHelper.getMetricPropertyIds(Resource.Type.Host);	descriptor = new HashMap<>();	descriptor.put(Resource.Type.Cluster.name(), clusterMetrics);	descriptor.put(Resource.Type.Host.name(), hostMetrics);	}	} catch (IOException e) {	
unable to process metrics descriptor properties 

for (Map<String, Object> properties : getPropertyMaps(predicate)) {	String artifactName = (String) properties.get(ARTIFACT_NAME_PROPERTY_ID);	if (artifactName == null || artifactName.equals(WIDGETS_DESCRIPTOR_NAME)) {	String stackName = (String) properties.get(STACK_NAME_PROPERTY_ID);	String stackVersion = (String) properties.get(STACK_VERSION_PROPERTY_ID);	String stackService = (String) properties.get(STACK_SERVICE_NAME_PROPERTY_ID);	Map<String, Object> descriptor;	try {	descriptor = getWidgetsDescriptor(stackName, stackVersion, stackService);	} catch (IOException e) {	
unable to process widgets descriptor properties 

========================= ambari sample_3535 =========================

try {	fsUri = new URI(defaultFS);	String protocol = fsUri.getScheme();	String ambariSkipCheckValues = viewContext.getAmbariProperty(Constants.AMBARI_SKIP_HOME_DIRECTORY_CHECK_PROTOCOL_LIST);	List<String> protocolSkipList = (ambariSkipCheckValues == null? new LinkedList<String>() : Arrays.asList(ambariSkipCheckValues.split(",")));	if(null != protocol && protocolSkipList.contains(protocol)){	policy.setCheckHomeDirectory(false);	return policy;	}	} catch (URISyntaxException e) {	
error occurred while parsing the defaultfs uri 

========================= ambari sample_896 =========================

public static String dateToString(Date date, String dateFormat) {	if (date == null || dateFormat == null || dateFormat.isEmpty()) {	return "";	}	try {	SimpleDateFormat formatter = new SimpleDateFormat(dateFormat);	return formatter.format(date).toString();	} catch (Exception e) {	
error in coverting datetostring format 

========================= ambari sample_1621 =========================

Element rootNode = doc.getRootElement();	Element record = new Element("RevertRecord");	record.setAttribute(new Attribute("id", iteration));	record.addContent(new Element("datetime").setText(currentDate.toString()));	record.addContent(new Element("dirname").setText(dirname));	record.addContent(new Element("instance").setText(instance));	record.addContent(new Element("query").setText(content));	rootNode.addContent(record);	xmlOutput.output(doc, new FileWriter(ConfigurationCheckImplementation.getHomeDir() + "RevertChangesService.xml"));	} catch (JDOMException e) {	
jdomexception 

doc.setRootElement(revertrecord);	Element record = new Element("RevertRecord");	record.setAttribute(new Attribute("id", iteration));	record.addContent(new Element("datetime").setText(currentDate.toString()));	record.addContent(new Element("dirname").setText(dirname));	record.addContent(new Element("instance").setText(instance));	record.addContent(new Element("query").setText(content));	doc.getRootElement().addContent(record);	xmlOutput.output(doc, new FileWriter(ConfigurationCheckImplementation.getHomeDir() + "RevertChangesService.xml"));	} catch (IOException io) {	
ioexception 

public void updateSequenceno(Connection c, int seqNo, int id, QuerySetAmbariDB ambaridatabase) throws SQLException, IOException {	PreparedStatement prSt;	prSt = ambaridatabase.updateSequenceNoInAmbariSequence(c, seqNo, id);	
the actual insert statement is 

public void updateSequenceno(Connection c, int seqNo, int id, QuerySetAmbariDB ambaridatabase) throws SQLException, IOException {	PreparedStatement prSt;	prSt = ambaridatabase.updateSequenceNoInAmbariSequence(c, seqNo, id);	prSt.executeUpdate();	
adding revert sql hive history 

Date created_data = rs1.getDate("date_created");	pojopig.setUserName(ownerName);	pojopig.setDt(created_data);	pojopig.setScript(script);	pojopig.setTitle(title);	pigArrayList.add(pojopig);	i++;	}	connection.commit();	} catch (SQLException e) {	
SQLException 

pigArrayList.add(pojopig);	i++;	}	connection.commit();	} catch (SQLException e) {	connection.rollback();	} finally {	try {	if (connection != null) connection.close();	} catch (SQLException e) {	
SQLException 

logger.info(homedir + filename2);	File file = new File(homedir + filename2);	if (!file.exists()) {	file.createNewFile();	}	FileWriter fw = new FileWriter(file.getAbsoluteFile());	BufferedWriter bw = new BufferedWriter(fw);	bw.write(script);	bw.close();	} catch (IOException e) {	
IOException 

public void deletePigScriptLocalFile(String homedir, String filename2) {	try {	File file = new File(homedir + filename2);	if (file.delete()) {	
temproray file deleted 

public void deletePigScriptLocalFile(String homedir, String filename2) {	try {	File file = new File(homedir + filename2);	if (file.delete()) {	} else {	
temproray file delete failed 

public void deletePigScriptLocalFile(String homedir, String filename2) {	try {	File file = new File(homedir + filename2);	if (file.delete()) {	} else {	}	} catch (Exception e) {	
file exception 

String[] subDirs = dir.split("/");	String dirPath = USER_DIRECTORY;	for(int i=2;i<subDirs.length;i++) {	dirPath += "/"+subDirs[i];	fs.setOwner(new Path(dirPath), username, username);	}	return b;	}	});	} catch (Exception e) {	
exception in webhdfs 

String[] subDirs = dir.split("/");	String dirPath = USER_DIRECTORY;	for(int i=2;i<subDirs.length;i++) {	dirPath += "/"+subDirs[i];	fs.setOwner(new Path(dirPath), username, username);	}	return b;	}	});	} catch (Exception e) {	
exception in webhdfs 

out.write(b, 0, numBytes);	}	in.close();	out.close();	fileSystem.setOwner(path, username, username);	fileSystem.close();	return null;	}	});	} catch (Exception e) {	
webhdfs exception 

out.write(b, 0, numBytes);	}	in.close();	out.close();	fileSystem.setOwner(path, username, username);	fileSystem.close();	return null;	}	});	} catch (Exception e) {	
webhdfs exception 

========================= ambari sample_1271 =========================

public Collection<String> findLiveCollectorHostsFromZNode() {	Set<String> collectors = new HashSet<>();	RetryPolicy retryPolicy = new BoundedExponentialBackoffRetry(sleepMsBetweenRetries, 10*sleepMsBetweenRetries, tryCount);	final CuratorZookeeperClient client = new CuratorZookeeperClient(zookeeperQuorum, SESSION_TIMEOUT, CONNECTION_TIMEOUT, null, retryPolicy);	List<String> liveInstances = null;	try {	client.start();	Stat stat = client.getZooKeeper().exists(ZNODE, false);	if (stat == null) {	
ambari metrics cluster znode does not exist skipping requesting live instances from zookeeper 

if (stat == null) {	return collectors;	}	liveInstances = RetryLoop.callWithRetry(client, new Callable<List<String>>() {	public List<String> call() throws Exception {	ZooKeeper zookeeper = client.getZooKeeper();	return zookeeper.getChildren(ZK_PATH, false);	}	});	} catch (Exception e) {	
unable to connect to zookeeper 

ZooKeeper zookeeper = client.getZooKeeper();	return zookeeper.getChildren(ZK_PATH, false);	}	});	} catch (Exception e) {	LOG.debug(e);	} finally {	try {	client.close();	} catch (Exception e) {	
caught exception while trying to close zk connection 

========================= ambari sample_253 =========================

protected PDU prepareTrap(Notification notification, SnmpVersion snmpVersion) throws InvalidSnmpConfigurationException {	AlertNotification alertNotification;	PDU pdu = DefaultPDUFactory.createPDU(snmpVersion.getTargetVersion());	if (Notification.Type.ALERT.equals(notification.getType())) {	try {	alertNotification = (AlertNotification) notification;	} catch (ClassCastException e) {	
notification wasn t casted to alertnotification returning empty protocol data unit 

protected PDU prepareTrap(Notification notification, SnmpVersion snmpVersion) throws InvalidSnmpConfigurationException {	AlertNotification alertNotification;	PDU pdu = DefaultPDUFactory.createPDU(snmpVersion.getTargetVersion());	if (Notification.Type.ALERT.equals(notification.getType())) {	try {	alertNotification = (AlertNotification) notification;	} catch (ClassCastException e) {	return pdu;	}	} else {	
notification for ambarisnmpdispatcher should be of type alertnotification but it wasn t returning empty protocol data unit 

========================= ambari sample_4566 =========================

options.createIfMissing(true);	options.cacheSize(conf.getLong( YarnConfiguration.TIMELINE_SERVICE_LEVELDB_READ_CACHE_SIZE, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_LEVELDB_READ_CACHE_SIZE));	JniDBFactory factory = new JniDBFactory();	String path = conf.get(YarnConfiguration.TIMELINE_SERVICE_LEVELDB_PATH);	File p = new File(path);	if (!p.exists()) {	if (!p.mkdirs()) {	throw new IOException("Couldn't create directory for leveldb " + "timeline store " + path);	}	}	
using leveldb path 

protected void serviceStop() throws Exception {	if (deletionThread != null) {	deletionThread.interrupt();	
waiting for deletion thread to complete its current action 

protected void serviceStop() throws Exception {	if (deletionThread != null) {	deletionThread.interrupt();	try {	deletionThread.join();	} catch (InterruptedException e) {	
interrupted while waiting for deletion thread to complete closing db now 

public EntityDeletionThread(Configuration conf) {	ttl  = conf.getLong(YarnConfiguration.TIMELINE_SERVICE_TTL_MS, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_TTL_MS);	ttlInterval = conf.getLong( YarnConfiguration.TIMELINE_SERVICE_LEVELDB_TTL_INTERVAL_MS, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_LEVELDB_TTL_INTERVAL_MS);	
starting deletion thread with ttl and cycle interval 

public void run() {	while (true) {	long timestamp = System.currentTimeMillis() - ttl;	try {	discardOldEntities(timestamp);	Thread.sleep(ttlInterval);	} catch (IOException e) {	LOG.error(e);	} catch (InterruptedException e) {	
deletion thread received interrupt exiting 

}	} else if (key[prefixlen] == EVENTS_COLUMN[0]) {	if (events || (lastEvent && entity.getEvents().size() == 0)) {	TimelineEvent event = getEntityEvent(null, key, prefixlen + EVENTS_COLUMN.length, iterator.peekNext().getValue());	if (event != null) {	entity.addEvent(event);	}	}	} else {	if (key[prefixlen] != INVISIBLE_REVERSE_RELATED_ENTITIES_COLUMN[0]) {	
found unexpected column for entity s of type s 

if (otherInfo != null && !otherInfo.isEmpty()) {	for (Entry<String, Object> i : otherInfo.entrySet()) {	byte[] key = createOtherInfoKey(entity.getEntityId(), entity.getEntityType(), revStartTime, i.getKey());	byte[] value = GenericObjectMapper.write(i.getValue());	writeBatch.put(key, value);	writePrimaryFilterEntries(writeBatch, primaryFilters, key, value);	}	}	db.write(writeBatch);	} catch (IOException e) {	
error putting entity of type 

lock.lock();	try {	StartAndInsertTime relatedEntityStartAndInsertTime = getAndSetStartTime(relatedEntity.getId(), relatedEntity.getType(), readReverseOrderedLong(revStartTime, 0), null);	if (relatedEntityStartAndInsertTime == null) {	throw new IOException("Error setting start time for related entity");	}	byte[] relatedEntityStartTime = writeReverseOrderedLong( relatedEntityStartAndInsertTime.startTime);	db.put(createRelatedEntityKey(relatedEntity.getId(), relatedEntity.getType(), relatedEntityStartTime, entity.getEntityId(), entity.getEntityType()), EMPTY_BYTES);	db.put(createEntityMarkerKey(relatedEntity.getId(), relatedEntity.getType(), relatedEntityStartTime), writeReverseOrderedLong(relatedEntityStartAndInsertTime .insertTime));	} catch (IOException e) {	
error putting related entity of type for entity of type 

========================= ambari sample_438 =========================

if (clusters != null) {	Map<String, Cluster> clusterMap = getCheckedClusterMap(clusters);	for (final Cluster cluster : clusterMap.values()) {	Config hiveEnvConfig = cluster.getDesiredConfigByType("hive-env");	if (hiveEnvConfig != null) {	Map<String, String> hiveEnvProperties = hiveEnvConfig.getProperties();	String webhcatUser = hiveEnvProperties.get("webhcat_user");	String hcatUser = hiveEnvProperties.get("hcat_user");	if (!StringUtils.equals(webhcatUser, hcatUser)) {	System.out.print("WARNING: In hive-env config, webhcat and hcat user are different. In current ambari release (3.0.0), hcat user was removed from stack, so potentially you could have some problems.");	
in hive env config webhcat and hcat user are different in current ambari release hcat user was removed from stack so potentially you could have some problems 

stageDisplayStatuses.add(stageDisplayStatus);	em.merge(stageEntity);	}	HostRoleStatus requestStatus = CalculatedStatus.getOverallStatusForRequest(stageStatuses);	requestEntity.setStatus(requestStatus);	HostRoleStatus requestDisplayStatus = CalculatedStatus.getOverallDisplayStatusForRequest(stageDisplayStatuses);	requestEntity.setDisplayStatus(requestDisplayStatus);	em.merge(requestEntity);	}	} catch (Exception e) {	
setting status for stages and requests threw exception 

protected void upgradeLdapConfiguration() throws AmbariException {	
moving ldap related properties from ambari properties to ambari congiuration db table 

AmbariLdapConfigurationKeys keyToBesaved = AmbariLdapConfigurationKeys.SERVER_HOST == key ? AmbariLdapConfigurationKeys.SERVER_HOST : AmbariLdapConfigurationKeys.SECONDARY_SERVER_HOST;	populateLdapConfigurationToBeUpgraded(propertiesToBeSaved, oldPropertyName, keyToBesaved.key(), hostAndPort.getHostText());	keyToBesaved = AmbariLdapConfigurationKeys.SERVER_HOST == key ? AmbariLdapConfigurationKeys.SERVER_PORT : AmbariLdapConfigurationKeys.SECONDARY_SERVER_PORT;	populateLdapConfigurationToBeUpgraded(propertiesToBeSaved, oldPropertyName, keyToBesaved.key(), String.valueOf(hostAndPort.getPort()));	} else {	populateLdapConfigurationToBeUpgraded(propertiesToBeSaved, oldPropertyName, key.key(), ldapPropertyValue);	}	}	});	if (propertiesToBeSaved.isEmpty()) {	
there was no ldap related properties in ambari properties moved elements 

========================= ambari sample_2723 =========================

};	Collection<String> versions = Collections2.transform(matching, function);	List<RepositoryVersionEntity> used = s_repoVersionDAO.findByServiceDesiredVersion(matching);	if (used.isEmpty()) {	throw new IllegalArgumentException(String.format("Could not determine which version " + "to associate patch %s. Remove one of %s and try again.", entity.getVersion(), StringUtils.join(versions, ", ")));	} else if (used.size() > 1) {	Collection<String> usedVersions = Collections2.transform(used, function);	throw new IllegalArgumentException(String.format("Patch %s was found to match more " + "than one repository in use: %s. Move all services to a common version and try again.", entity.getVersion(), StringUtils.join(usedVersions, ", ")));	} else {	parent = used.get(0);	
patch was found to match more than one repository in repository is in use and will be the parent 

========================= ambari sample_3562 =========================

public void onInitialAlertEvent(InitialAlertEvent event) {	
received event 

public void onAlertStateChangeEvent(AlertStateChangeEvent event) {	
received event 

public void onAlertStateChangeEvent(AggregateAlertRecalculateEvent event) {	
received event 

int warningCount = summary.getWarningCount();	int criticalCount = summary.getCriticalCount();	int unknownCount = summary.getUnknownCount();	int totalCount = okCount + warningCount + criticalCount + unknownCount;	Alert aggregateAlert = new Alert(aggregateDefinition.getName(), null, aggregateDefinition.getServiceName(), null, null, AlertState.UNKNOWN);	aggregateAlert.setLabel(aggregateDefinition.getLabel());	aggregateAlert.setTimestamp(System.currentTimeMillis());	try {	aggregateAlert.setCluster(m_clusters.get().getClusterById(clusterId).getClusterName());	} catch (AmbariException exception) {	
unable to lookup cluster with id 

========================= ambari sample_4531 =========================

public void run() {	String hostString = createHostString(sshHostInfo.getHosts());	long bootstrapTimeout = calculateBSTimeout(sshHostInfo.getHosts().size());	ScheduledExecutorService scheduler = Executors.newScheduledThreadPool(1);	BSStatusCollector statusCollector = new BSStatusCollector();	ScheduledFuture<?> handle = null;	
kicking off the scheduler for polling on logs in 

env.putAll(envVariables);	Process process = pb.start();	try {	String logInfoMessage = "Bootstrap output, log=" + bootStrapErrorFilePath + " " + bootStrapOutputFilePath + " at " + this.ambariHostname;	LOG.info(logInfoMessage);	int exitCode = 1;	boolean timedOut = false;	if (waitForProcessTermination(process, bootstrapTimeout)){	exitCode = process.exitValue();	} else {	
bootstrap process timed out it will be destroyed 

} else {	process.destroy();	timedOut = true;	}	String outMesg = "";	String errMesg = "";	try {	outMesg = FileUtils.readFileToString(new File(bootStrapOutputFilePath));	errMesg = FileUtils.readFileToString(new File(bootStrapErrorFilePath));	} catch(IOException io) {	
error in reading files 

String errMesg = "";	try {	outMesg = FileUtils.readFileToString(new File(bootStrapOutputFilePath));	errMesg = FileUtils.readFileToString(new File(bootStrapErrorFilePath));	} catch(IOException io) {	}	scriptlog = outMesg + "\n\n" + errMesg;	if (timedOut) {	scriptlog += "\n\n Bootstrap process timed out. It was destroyed.";	}	
script log mesg 

if (exitCode != 0) {	stat = BSStat.ERROR;	interuptSetupAgent(99, scriptlog);	} else {	stat = BSStat.SUCCESS;	}	scheduler.schedule(new BSStatusCollector(), 0, TimeUnit.SECONDS);	long startTime = System.currentTimeMillis();	while (true) {	if (LOG.isDebugEnabled()) {	
waiting for hosts status to be updated 

}	if (!pendingHosts) {	break;	}	try {	Thread.sleep(1000);	} catch (InterruptedException e) {	}	long now = System.currentTimeMillis();	if (now >= (startTime+15000)) {	
gave up waiting for hosts status to be updated 

throw new IOException(e);	} finally {	if (handle != null) {	handle.cancel(true);	}	scheduler.schedule(new BSStatusCollector(), 0, TimeUnit.SECONDS);	scheduler.shutdownNow();	try {	scheduler.awaitTermination(10, TimeUnit.SECONDS);	} catch (InterruptedException e) {	
interruped while waiting for scheduler 

}	scheduler.schedule(new BSStatusCollector(), 0, TimeUnit.SECONDS);	scheduler.shutdownNow();	try {	scheduler.awaitTermination(10, TimeUnit.SECONDS);	} catch (InterruptedException e) {	}	process.destroy();	}	} catch(IOException io) {	
error executing bootstrap 

========================= ambari sample_2735 =========================

protected File resolveDirectory(String directoryName, String stack, String service) {	File directory = new File(getAbsolutePath() + File.separator + directoryName);	if (directory.isDirectory()) {	String[] files = directory.list();	int fileCount = files.length;	if (fileCount > 0) {	
service folder for service in has been resolved to 

protected File resolveDirectory(String directoryName, String stack, String service) {	File directory = new File(getAbsolutePath() + File.separator + directoryName);	if (directory.isDirectory()) {	String[] files = directory.list();	int fileCount = files.length;	if (fileCount > 0) {	return directory;	}	else {	
service folder is empty 

if (directory.isDirectory()) {	String[] files = directory.list();	int fileCount = files.length;	if (fileCount > 0) {	return directory;	}	else {	}	}	else {	
service folder does not exist 

private void parseRoleCommandOrder() {	if (rcoFile == null) return;	try {	ObjectMapper mapper = new ObjectMapper();	TypeReference<Map<String, Object>> rcoElementTypeReference = new TypeReference<Map<String, Object>>() {};	HashMap<String, Object> result = mapper.readValue(rcoFile, rcoElementTypeReference);	
role command order info was loaded from file 

private void parseRoleCommandOrder() {	if (rcoFile == null) return;	try {	ObjectMapper mapper = new ObjectMapper();	TypeReference<Map<String, Object>> rcoElementTypeReference = new TypeReference<Map<String, Object>>() {};	HashMap<String, Object> result = mapper.readValue(rcoFile, rcoElementTypeReference);	roleCommandOrder = new StackRoleCommandOrder(result);	if (LOG.isDebugEnabled() && rcoFile != null) {	
role command order for 

if (rcoFile == null) return;	try {	ObjectMapper mapper = new ObjectMapper();	TypeReference<Map<String, Object>> rcoElementTypeReference = new TypeReference<Map<String, Object>>() {};	HashMap<String, Object> result = mapper.readValue(rcoFile, rcoElementTypeReference);	roleCommandOrder = new StackRoleCommandOrder(result);	if (LOG.isDebugEnabled() && rcoFile != null) {	roleCommandOrder.printRoleCommandOrder(LOG);	}	} catch (IOException e) {	
can not read role command order info s 

========================= ambari sample_4462 =========================

private void freeConnector(FreeConnector message) {	ActorRef sender = getSender();	if (message.isForAsync()) {	
about to free connector for job and user 

private void freeConnector(FreeConnector message) {	ActorRef sender = getSender();	if (message.isForAsync()) {	Optional<ActorRef> refOptional = removeFromAsyncBusyPool(message.getUsername(), message.getJobId());	if (refOptional.isPresent()) {	addToAsyncAvailable(message.getUsername(), refOptional.get());	}	return;	}	
about to free sync connector for user 

========================= ambari sample_707 =========================

public void onAmbariEvent(AlertHashInvalidationEvent event) {	
received event 

public void onEvent(ServiceComponentUninstalledEvent event) {	
received event 

public void onAmbariEvent(ClusterEvent event) {	
received event 

========================= ambari sample_4527 =========================

public static String addOffsetToDate(String date, Long utcOffset, String dateFormat) {	if (StringUtils.isBlank(date)) {	
input date is empty or null 

public static String addOffsetToDate(String date, Long utcOffset, String dateFormat) {	if (StringUtils.isBlank(date)) {	return null;	}	if (utcOffset == null) {	
utc offset is null return input date without adding offset 

public static String addOffsetToDate(String date, Long utcOffset, String dateFormat) {	if (StringUtils.isBlank(date)) {	return null;	}	if (utcOffset == null) {	return date;	}	if (StringUtils.isBlank(dateFormat)) {	
dateformat is null or empty return input date without adding offset 

========================= ambari sample_1375 =========================

private boolean hostHasVersionableComponents(Cluster cluster, Set<String> serviceNames, AmbariMetaInfo ami, StackId stackId, Host host) throws SystemException {	List<ServiceComponentHost> components = cluster.getServiceComponentHosts(host.getHostName());	for (ServiceComponentHost component : components) {	if (!serviceNames.isEmpty() && !serviceNames.contains(component.getServiceName())) {	continue;	}	ComponentInfo componentInfo;	try {	componentInfo = ami.getComponent(stackId.getStackName(), stackId.getStackVersion(), component.getServiceName(), component.getServiceComponentName());	} catch (AmbariException e) {	
exception while accessing component s of service s for stack s 

========================= ambari sample_3489 =========================

public boolean init(String inputDesc, String fieldName, String mapClassCode, MapFieldDescriptor mapFieldDescriptor) {	init(inputDesc, fieldName, mapClassCode);	copyName = ((MapFieldCopyDescriptor)mapFieldDescriptor).getCopyName();	if (StringUtils.isEmpty(copyName)) {	
map copy name is empty 

========================= ambari sample_1637 =========================

protected boolean applies(Entry entry, String attribute) {	
checking for object class in entry 

========================= ambari sample_2918 =========================

public void resetUIState() throws AmbariException {	
resetting ui state 

public void cleanUpRCATables() {	
cleaning up rca tables 

public void cleanUpRCATables() {	for (String tableName : rcaTableNames) {	try {	if (dbAccessor.tableExists(tableName)) {	dbAccessor.truncateTable(tableName);	}	} catch (Exception e) {	
error cleaning rca table 

try {	if (dbAccessor.tableExists(tableName)) {	dbAccessor.truncateTable(tableName);	}	} catch (Exception e) {	}	}	try {	cleanUpTablesFromRCADatabase();	} catch (Exception e) {	
error cleaning rca tables from ambarirca db 

String username = configuration.getRcaDatabaseUser();	String password = configuration.getRcaDatabasePassword();	Class.forName(driverName);	try (Connection connection = DriverManager.getConnection(connectionURL, username, password)) {	connection.setAutoCommit(true);	for (String tableName : rcaTableNames) {	String query = "DELETE FROM " + tableName;	try (Statement statement = connection.createStatement()) {	statement.execute(query);	} catch (Exception e) {	
error while executing query 

public static void main(String[] args) throws Exception {	try {	String[] splittedJavaVersion = System.getProperty("java.version").split("\\.");	float javaVersion = Float.parseFloat(splittedJavaVersion[0] + "." + splittedJavaVersion[1]);	if (javaVersion < Configuration.JDK_MIN_VERSION) {	
oracle jdk version is lower than it can cause problems during upgrade process please use ambari server setup command to upgrade jdk 

SchemaUpgradeHelper schemaUpgradeHelper = injector.getInstance(SchemaUpgradeHelper.class);	List<String> myISAMTables = schemaUpgradeHelper.getMyISAMTables();	if (!myISAMTables.isEmpty()) {	String errorMessage = String.format("Unsupported MyISAM table %s detected. " + "For correct upgrade database should be migrated to InnoDB engine.", myISAMTables.get(0));	LOG.error(errorMessage);	throw new AmbariException(errorMessage);	}	String targetVersion = schemaUpgradeHelper.getAmbariServerVersion();	LOG.info("Upgrading schema to target version = " + targetVersion);	UpgradeCatalog targetUpgradeCatalog = AbstractUpgradeCatalog .getUpgradeCatalog(targetVersion);	
target upgrade catalog 

List<UpgradeCatalog> upgradeCatalogs = schemaUpgradeHelper.getUpgradePath(sourceVersion, targetVersion);	String date = new SimpleDateFormat("MM-dd-yyyy_HH:mm:ss").format(new Date());	String ambariUpgradeConfigUpdatesFileName = "ambari_upgrade_config_changes_" + date + ".json";	schemaUpgradeHelper.executeUpgrade(upgradeCatalogs);	schemaUpgradeHelper.startPersistenceService();	schemaUpgradeHelper.executePreDMLUpdates(upgradeCatalogs);	schemaUpgradeHelper.executeDMLUpdates(upgradeCatalogs, ambariUpgradeConfigUpdatesFileName);	schemaUpgradeHelper.executeOnPostUpgrade(upgradeCatalogs);	schemaUpgradeHelper.outputUpgradeJsonOutput(upgradeCatalogs);	schemaUpgradeHelper.resetUIState();	
upgrade successful 

schemaUpgradeHelper.startPersistenceService();	schemaUpgradeHelper.executePreDMLUpdates(upgradeCatalogs);	schemaUpgradeHelper.executeDMLUpdates(upgradeCatalogs, ambariUpgradeConfigUpdatesFileName);	schemaUpgradeHelper.executeOnPostUpgrade(upgradeCatalogs);	schemaUpgradeHelper.outputUpgradeJsonOutput(upgradeCatalogs);	schemaUpgradeHelper.resetUIState();	schemaUpgradeHelper.cleanUpRCATables();	schemaUpgradeHelper.stopPersistenceService();	} catch (Throwable e) {	if (e instanceof AmbariException) {	
exception occurred during upgrade failed 

schemaUpgradeHelper.executeDMLUpdates(upgradeCatalogs, ambariUpgradeConfigUpdatesFileName);	schemaUpgradeHelper.executeOnPostUpgrade(upgradeCatalogs);	schemaUpgradeHelper.outputUpgradeJsonOutput(upgradeCatalogs);	schemaUpgradeHelper.resetUIState();	schemaUpgradeHelper.cleanUpRCATables();	schemaUpgradeHelper.stopPersistenceService();	} catch (Throwable e) {	if (e instanceof AmbariException) {	throw (AmbariException)e;	}else{	
unexpected error upgrade failed 

========================= ambari sample_2719 =========================

public XMLParser(Reader reader, ParseOptions parseOptions) throws IOException {	super(reader, parseOptions);	XMLInputFactory factory = XMLInputFactory.newInstance();	try {	factory.setProperty(XMLInputFactory.SUPPORT_DTD, false);	factory.setProperty(XMLInputFactory.IS_SUPPORTING_EXTERNAL_ENTITIES, false);	this.xmlReader = factory.createXMLEventReader(reader);	} catch (XMLStreamException e) {	
error occurred while creating xml reader 

========================= ambari sample_814 =========================

public XMLParser(Reader reader, ParseOptions parseOptions) throws IOException {	super(reader, parseOptions);	XMLInputFactory factory = XMLInputFactory.newInstance();	try {	factory.setProperty(XMLInputFactory.SUPPORT_DTD, false);	factory.setProperty(XMLInputFactory.IS_SUPPORTING_EXTERNAL_ENTITIES, false);	this.xmlReader = factory.createXMLEventReader(reader);	} catch (XMLStreamException e) {	
error occurred while creating xml reader 

========================= ambari sample_547 =========================

public void testRun() throws Exception {	Properties properties = new Properties();	String bootdir = temp.newFolder("bootdir").toString();	String metadetadir = temp.newFolder("metadetadir").toString();	String serverVersionFilePath = temp.newFolder("serverVersionFilePath").toString();	
bootdir is 

public void testRun() throws Exception {	Properties properties = new Properties();	String bootdir = temp.newFolder("bootdir").toString();	String metadetadir = temp.newFolder("metadetadir").toString();	String serverVersionFilePath = temp.newFolder("serverVersionFilePath").toString();	
metadetadir is 

public void testRun() throws Exception {	Properties properties = new Properties();	String bootdir = temp.newFolder("bootdir").toString();	String metadetadir = temp.newFolder("metadetadir").toString();	String serverVersionFilePath = temp.newFolder("serverVersionFilePath").toString();	
serverversionfilepath is 

SshHostInfo info = new SshHostInfo();	info.setSshKey("xyz");	ArrayList<String> hosts = new ArrayList<>();	hosts.add("host1");	hosts.add("host2");	info.setUserRunAs("root");	info.setHosts(hosts);	info.setUser("user");	info.setPassword("passwd");	BSResponse response = impl.runBootStrap(info);	
response id from bootstrap 

info.setSshKey("xyz");	ArrayList<String> hosts = new ArrayList<>();	hosts.add("host1");	hosts.add("host2");	info.setUserRunAs("root");	info.setHosts(hosts);	info.setUser("user");	info.setPassword("passwd");	BSResponse response = impl.runBootStrap(info);	BootStrapStatus status = impl.getStatus(response.getRequestId());	
status 

public void testHostFailure() throws Exception {	Properties properties = new Properties();	String bootdir = temp.newFolder("bootdir").toString();	String metadetadir = temp.newFolder("metadetadir").toString();	String serverVersionFilePath = temp.newFolder("serverVersionFilePath").toString();	
bootdir is 

public void testHostFailure() throws Exception {	Properties properties = new Properties();	String bootdir = temp.newFolder("bootdir").toString();	String metadetadir = temp.newFolder("metadetadir").toString();	String serverVersionFilePath = temp.newFolder("serverVersionFilePath").toString();	
metadetadir is 

public void testHostFailure() throws Exception {	Properties properties = new Properties();	String bootdir = temp.newFolder("bootdir").toString();	String metadetadir = temp.newFolder("metadetadir").toString();	String serverVersionFilePath = temp.newFolder("serverVersionFilePath").toString();	
serverversionfilepath is 

info.setSshKey("xyz");	ArrayList<String> hosts = new ArrayList<>();	hosts.add("host1");	hosts.add("host2");	info.setHosts(hosts);	info.setUser("user");	info.setUserRunAs("root");	info.setPassword("passwd");	BSResponse response = impl.runBootStrap(info);	long requestId = response.getRequestId();	
response id from bootstrap 

info.setPassword("passwd");	BSResponse response = impl.runBootStrap(info);	long requestId = response.getRequestId();	File requestDir = new File(bootdir, Long.toString(requestId));	int num = 0;	while (!requestDir.exists() && num < 500) {	Thread.sleep(100);	num++;	}	if (!requestDir.exists()) {	
requestdir does not exists 

int num = 0;	while (!requestDir.exists() && num < 500) {	Thread.sleep(100);	num++;	}	if (!requestDir.exists()) {	}	FileUtils.writeStringToFile(new File(requestDir, "host1.done"), "0");	FileUtils.writeStringToFile(new File(requestDir, "host2.done"), "1");	BootStrapStatus status = impl.getStatus(response.getRequestId());	
status 

========================= ambari sample_1861 =========================

public Response getFilePage(@PathParam("filePath") String filePath, @QueryParam("page") Long page) throws IOException, InterruptedException {	
reading file 

public Response deleteFile(@PathParam("filePath") String filePath) throws IOException, InterruptedException {	try {	filePath = sanitizeFilePath(filePath);	
deleting file 

public Response updateFile(FileResourceRequest request, try {	filePath = sanitizeFilePath(filePath);	
rewriting file 

public Response createFile(FileResourceRequest request, throws IOException, InterruptedException {	try {	
creating file 

========================= ambari sample_843 =========================

protected static void dropDatabase() throws ClassNotFoundException, SQLException {	String DROP_DERBY_URL = "jdbc:derby:memory:myDB/ambari;drop=true";	Class.forName(Configuration.JDBC_IN_MEMORY_DRIVER);	try {	DriverManager.getConnection(DROP_DERBY_URL);	} catch (SQLNonTransientConnectionException ignored) {	
database dropped 

========================= ambari sample_49 =========================

public DatabaseMetadataWrapper extract() throws ServiceException {	try {	return new DatabaseMetadataWrapper(databaseMetaData.getDatabaseMajorVersion(), databaseMetaData.getDatabaseMinorVersion());	} catch (SQLException e) {	
error occurred while fetching version from database metadata 

========================= ambari sample_639 =========================

public void start() {	
starting flume metrics sink 

public void stop() {	
stopping flume metrics sink 

public void configure(Context context) {	
context parameters 

public void configure(Context context) {	try {	hostname = InetAddress.getLocalHost().getHostName();	if ((hostname == null) || (!hostname.contains("."))) {	hostname = InetAddress.getLocalHost().getCanonicalHostName();	}	hostname = hostname.toLowerCase();	} catch (UnknownHostException e) {	
could not identify hostname 

public void run() {	
collecting metrics for flume 

public void run() {	try {	Map<String, Map<String, String>> metricsMap = JMXPollUtil.getAllMBeans();	long currentTimeMillis = System.currentTimeMillis();	for (String component : metricsMap.keySet()) {	Map<String, String> attributeMap = metricsMap.get(component);	
attributes for component 

public void run() {	try {	Map<String, Map<String, String>> metricsMap = JMXPollUtil.getAllMBeans();	long currentTimeMillis = System.currentTimeMillis();	for (String component : metricsMap.keySet()) {	Map<String, String> attributeMap = metricsMap.get(component);	processComponentAttributes(currentTimeMillis, component, attributeMap);	}	} catch (UnableToConnectException uce) {	
unable to send metrics to collector by address 

public void run() {	try {	Map<String, Map<String, String>> metricsMap = JMXPollUtil.getAllMBeans();	long currentTimeMillis = System.currentTimeMillis();	for (String component : metricsMap.keySet()) {	Map<String, String> attributeMap = metricsMap.get(component);	processComponentAttributes(currentTimeMillis, component, attributeMap);	}	} catch (UnableToConnectException uce) {	} catch (Exception e) {	
unexpected error 

try {	Map<String, Map<String, String>> metricsMap = JMXPollUtil.getAllMBeans();	long currentTimeMillis = System.currentTimeMillis();	for (String component : metricsMap.keySet()) {	Map<String, String> attributeMap = metricsMap.get(component);	processComponentAttributes(currentTimeMillis, component, attributeMap);	}	} catch (UnableToConnectException uce) {	} catch (Exception e) {	}	
finished collecting metrics for flume 

========================= ambari sample_269 =========================

public static Properties readProperties(String propertiesFile) {	try {	Properties properties = new Properties();	InputStream inputStream = ClassLoader.getSystemResourceAsStream(propertiesFile);	if (inputStream == null) {	inputStream = new FileInputStream(propertiesFile);	}	properties.load(inputStream);	return properties;	} catch (IOException ioEx) {	
error reading properties file for jmeter 

boolean needsTimestamps = line.contains("startTime");	boolean needsHost = line.contains("hostname");	metricList.add(new GetMetricRequestInfo(metrics, needsTimestamps, needsHost));	metrics.clear();	} else {	metrics.add(line);	}	}	return metricList;	} catch (IOException e) {	
cannot read file for appid 

========================= ambari sample_286 =========================

protected String readFromWithDefault(String atsUrl, String defaultResponse) {	String response;	try {	InputStream responseInputStream = context.getURLStreamProvider().readAsCurrent(atsUrl, "GET", (String)null, new HashMap<String, String>());	response = IOUtils.toString(responseInputStream);	} catch (IOException e) {	
error while reading from ats 

========================= ambari sample_855 =========================

public void beforeJob(JobExecution jobExecution) {	
dummy before job execution 

public void afterJob(JobExecution jobExecution) {	
dummy after job execution 

========================= ambari sample_156 =========================

protected boolean applies(Entry entry, String attribute) {	
checking for attribute in entry 

========================= ambari sample_2922 =========================

protected String getResponse(String ... propertyNames) {	if (LOG.isDebugEnabled()) {	
propertyservice getresponse for view 

========================= ambari sample_1123 =========================

when(hbaseService.getDesiredStackId()).thenReturn(new StackId("HDP-2.0.6.1"));	Service zkService = mock(Service.class);	when(zkService.getDesiredStackId()).thenReturn(new StackId("HDP-2.0.6.1"));	when(cluster.getServices()).thenReturn(ImmutableMap.<String, Service>builder() .put("HBASE", hbaseService) .put("ZOOKEEPER", zkService) .build());	RoleCommandOrder rco = roleCommandOrderProvider.getRoleCommandOrder(cluster);	RoleGraph rg = roleGraphFactory.createNew(rco);	long now = System.currentTimeMillis();	Stage stage = StageUtils.getATestStage(1, 1, "host1", "", "");	stage.addHostRoleExecutionCommand("host2", Role.HBASE_MASTER, RoleCommand.START, new ServiceComponentHostStartEvent("HBASE_MASTER", "host2", now), "cluster1", "HBASE", false, false);	stage.addHostRoleExecutionCommand("host3", Role.ZOOKEEPER_SERVER, RoleCommand.START, new ServiceComponentHostStartEvent("ZOOKEEPER_SERVER", "host3", now), "cluster1", "ZOOKEEPER", false, false);	
build and ready to detect circular dependencies short chain 

Service yarnService = mock(Service.class);	when(yarnService.getDesiredStackId()).thenReturn(new StackId("HDP-2.0.6.1"));	when(cluster.getServices()).thenReturn(ImmutableMap.<String, Service>builder() .put("HBASE", hbaseService) .put("ZOOKEEPER", zkService) .put("YARN", yarnService) .build());	RoleCommandOrder rco = roleCommandOrderProvider.getRoleCommandOrder(cluster);	RoleGraph rg = roleGraphFactory.createNew(rco);	long now = System.currentTimeMillis();	Stage stage = StageUtils.getATestStage(1, 1, "host1", "", "");	stage.addHostRoleExecutionCommand("host2", Role.HBASE_MASTER, RoleCommand.STOP, new ServiceComponentHostStartEvent("HBASE_MASTER", "host2", now), "cluster1", "HBASE", false, false);	stage.addHostRoleExecutionCommand("host3", Role.ZOOKEEPER_SERVER, RoleCommand.STOP, new ServiceComponentHostStartEvent("ZOOKEEPER_SERVER", "host3", now), "cluster1", "ZOOKEEPER", false, false);	stage.addHostRoleExecutionCommand("host2", Role.RESOURCEMANAGER, RoleCommand.STOP, new ServiceComponentHostStartEvent("RESOURCEMANAGER", "host4", now), "cluster1", "YARN", false, false);	
build and ready to detect circular dependencies long chain 

Service zkService = mock(Service.class);	when(zkService.getDesiredStackId()).thenReturn(new StackId("HDP-2.0.6.1"));	when(cluster.getServices()).thenReturn(ImmutableMap.<String, Service>builder() .put("HBASE", hbaseService) .put("ZOOKEEPER", zkService) .build());	RoleCommandOrder rco = roleCommandOrderProvider.getRoleCommandOrder(cluster);	RoleGraph rg = roleGraphFactory.createNew(rco);	long now = System.currentTimeMillis();	Stage stage = StageUtils.getATestStage(1, 1, "host1", "", "");	stage.addHostRoleExecutionCommand("host2", Role.HBASE_MASTER, RoleCommand.UPGRADE, new ServiceComponentHostStartEvent("HBASE_MASTER", "host2", now), "cluster1", "HBASE", false, false);	stage.addHostRoleExecutionCommand("host3", Role.ZOOKEEPER_SERVER, RoleCommand.UPGRADE, new ServiceComponentHostStartEvent("ZOOKEEPER_SERVER", "host3", now), "cluster1", "ZOOKEEPER", false, false);	stage.addHostRoleExecutionCommand("host2", Role.HBASE_REGIONSERVER, RoleCommand.UPGRADE, new ServiceComponentHostStartEvent("HBASE_REGIONSERVER", "host4", now), "cluster1", "HBASE", false, false);	
build and ready to detect circular dependencies long chain 

Service zkService = mock(Service.class);	when(zkService.getDesiredStackId()).thenReturn(new StackId("HDP-2.0.6"));	when(cluster.getServices()).thenReturn(ImmutableMap.<String, Service>builder() .put("HBASE", hbaseService) .put("ZOOKEEPER", zkService) .build());	RoleCommandOrder rco = roleCommandOrderProvider.getRoleCommandOrder(cluster);	RoleGraph rg = roleGraphFactory.createNew(rco);	long now = System.currentTimeMillis();	Stage stage = StageUtils.getATestStage(1, 1, "host1", "", "");	stage.addHostRoleExecutionCommand("host2", Role.HBASE_MASTER, RoleCommand.STOP, new ServiceComponentHostStartEvent("HBASE_MASTER", "host2", now), "cluster1", "HBASE", false, false);	stage.addHostRoleExecutionCommand("host3", Role.ZOOKEEPER_SERVER, RoleCommand.STOP, new ServiceComponentHostStartEvent("ZOOKEEPER_SERVER", "host3", now), "cluster1", "ZOOKEEPER", false, false);	stage.addHostRoleExecutionCommand("host2", Role.HBASE_REGIONSERVER, RoleCommand.STOP, new ServiceComponentHostStartEvent("HBASE_REGIONSERVER", "host4", now), "cluster1", "HBASE", false, false);	
build and ready to detect circular dependencies 

========================= ambari sample_1820 =========================

private RESPONSE retry(int tries, AmbariSolrCloudClient solrCloudClient) throws Exception {	try {	return createAndProcessRequest(solrCloudClient);	} catch (Exception ex) {	LOG.error(ex.getMessage(), ex);	tries++;	
command failed tries again tries 

========================= ambari sample_101 =========================

public long readCheckpoint(AGGREGATOR_NAME aggregatorName) {	String path = getCheckpointZKPath(aggregatorName);	
reading checkpoint at 

public boolean writeCheckpoint(AGGREGATOR_NAME aggregatorName, long value) {	String path = getCheckpointZKPath(aggregatorName);	
saving checkpoint at s with value s 

========================= ambari sample_382 =========================

public void setupMap() {	InputStream inputStream = null;	
using hostsmap file 

public void setupMap() {	InputStream inputStream = null;	try {	if (hostsMapFile != null) {	hostsMap = new Properties();	inputStream = new FileInputStream(new File(hostsMapFile));	hostsMap.load(inputStream);	}	} catch (FileNotFoundException fnf) {	
no configuration file found in classpath 

========================= ambari sample_3722 =========================

public static ServiceCalculatedState getServiceStateProvider(String service){	ServiceCalculatedState suggestedServiceProvider;	Service.Type serviceType = null;	try {	serviceType = Service.Type.valueOf(service);	} catch (Exception e){	
could not parse service name will use default state provider 

========================= ambari sample_3688 =========================

Map<String, Set<String>> hgComponentsMap = calculateHostGroupComponentsMap(request);	Map<String, Set<String>> hgHostsMap = calculateHostGroupHostsMap(request);	Map<String, Set<String>> componentHostsMap = calculateComponentHostsMap(hgComponentsMap, hgHostsMap);	Map<String, Map<String, Map<String, String>>> configurations = calculateConfigurations(request);	Map<String, String> userContext = readUserContext(request);	Boolean gplLicenseAccepted = configuration.getGplLicenseAccepted();	List<ChangedConfigInfo> changedConfigurations = requestType == StackAdvisorRequestType.CONFIGURATION_DEPENDENCIES ? calculateChangedConfigurations(request) : Collections.emptyList();	Set<RecommendationResponse.ConfigGroup> configGroups = calculateConfigGroups(request);	return StackAdvisorRequestBuilder. forStack(stackName, stackVersion).ofType(requestType).forHosts(hosts). forServices(services).forHostComponents(hgComponentsMap). forHostsGroupBindings(hgHostsMap). withComponentHostsMap(componentHostsMap). withConfigurations(configurations). withConfigGroups(configGroups). withChangedConfigurations(changedConfigurations). withUserContext(userContext). withGPLLicenseAccepted(gplLicenseAccepted).build();	} catch (Exception e) {	
error occurred during preparation of stack advisor request 

========================= ambari sample_3610 =========================

public List<String> handle(CloudSolrClient solrClient, SolrPropsConfig solrPropsConfig) throws Exception {	try {	CollectionAdminRequest.List colListReq = new CollectionAdminRequest.List();	CollectionAdminResponse response = colListReq.process(solrClient);	if (response.getStatus() != 0) {	LOG.error("Error getting collection list from solr.  response=" + response);	return null;	}	return (List<String>) response.getResponse().get("collections");	} catch (SolrException e) {	
getcollections operation failed 

========================= ambari sample_1338 =========================

for (Resource resource : queryResult.getQueryResponse().getResources()) {	Map<Resource.Type, String> map = getKeyValueMap(resource, queryResult.getKeyValueMap());	Predicate     queryPredicate = subResource.createPredicate(map, subResource.processedPredicate);	Set<Resource> resourceSet    = new LinkedHashSet<>();	try {	Set<Resource> queryResources = subResource.doQuery(resourceType, request, queryPredicate, false).getResources();	providerResourceSet.addAll(queryResources);	resourceSet.addAll(queryResources);	} catch (NoSuchResourceException e) {	} catch (AuthorizationException e) {	
user does not have authorization to get resources the data will not be added to the response 

private QueryResponse doQuery(Resource.Type type, Request request, Predicate predicate, boolean checkEmptyResponse) throws UnsupportedPropertyException, SystemException, NoSuchResourceException, NoSuchParentResourceException {	if (LOG.isDebugEnabled()) {	
executing resource query where 

========================= ambari sample_4090 =========================

LOG.error(msg);	throw new MisconfigurationFormattedException("scripts.dir");	}	int checkId = 0;	boolean fileCreated;	String newFilePath;	do {	String normalizedName = object.getTitle().replaceAll("[^a-zA-Z0-9 ]+", "").replaceAll(" ", "_").toLowerCase();	String timestamp = new SimpleDateFormat("yyyy-MM-dd_hh-mm").format(new Date());	newFilePath = String.format(userScriptsPath + "/%s-%s%s.pig", normalizedName, timestamp, (checkId == 0)?"":"_"+checkId);	
trying to create new file 

boolean fileCreated;	String newFilePath;	do {	String normalizedName = object.getTitle().replaceAll("[^a-zA-Z0-9 ]+", "").replaceAll(" ", "_").toLowerCase();	String timestamp = new SimpleDateFormat("yyyy-MM-dd_hh-mm").format(new Date());	newFilePath = String.format(userScriptsPath + "/%s-%s%s.pig", normalizedName, timestamp, (checkId == 0)?"":"_"+checkId);	try {	FSDataOutputStream stream = UserLocalObjects.getHdfsApi(context).create(newFilePath, false);	stream.close();	fileCreated = true;	
file created successfully 

do {	String normalizedName = object.getTitle().replaceAll("[^a-zA-Z0-9 ]+", "").replaceAll(" ", "_").toLowerCase();	String timestamp = new SimpleDateFormat("yyyy-MM-dd_hh-mm").format(new Date());	newFilePath = String.format(userScriptsPath + "/%s-%s%s.pig", normalizedName, timestamp, (checkId == 0)?"":"_"+checkId);	try {	FSDataOutputStream stream = UserLocalObjects.getHdfsApi(context).create(newFilePath, false);	stream.close();	fileCreated = true;	} catch (FileAlreadyExistsException e) {	fileCreated = false;	
file already exists trying next id 

========================= ambari sample_1011 =========================

public Response updateSetting(@PathParam("id") String id, SettingRequest settingRequest, @Context HttpServletResponse response, @Context UriInfo uriInfo) {	Setting setting = null;	try {	setting = resourceManager.update(id, settingRequest.getSetting());	} catch (ItemNotFound itemNotFound) {	
error occurred while creating settings 

public Response delete(@PathParam("id") String id) {	try {	resourceManager.removeSetting(id);	} catch (ItemNotFound itemNotFound) {	
error occurred while updating setting 

========================= ambari sample_526 =========================

public S3Uploader(S3Properties s3Properties) {	
initializing client with 

========================= ambari sample_176 =========================

public void testUpgrade() throws Exception {	try {	dropDatabase();	} catch (SQLException ignored) {	}	String targetVersion = getLastVersion();	injector = Guice.createInjector(new ControllerModule(properties));	
testing upgrade from version to 

private void dropDatabase() throws ClassNotFoundException, SQLException {	Class.forName(Configuration.JDBC_IN_MEMORY_DRIVER);	try {	DriverManager.getConnection(DROP_DERBY_URL);	} catch (SQLNonTransientConnectionException ignored) {	
database dropped 

Injector injector = Guice.createInjector(new SchemaUpgradeHelper.UpgradeHelperModule(properties) {	protected void configure() {	super.configure();	ViewRegistry viewRegistryMock = EasyMock.createNiceMock(ViewRegistry.class);	bind(ViewRegistry.class).toInstance(viewRegistryMock);	}	});	SchemaUpgradeHelper schemaUpgradeHelper = injector.getInstance(SchemaUpgradeHelper.class);	LOG.info("Upgrading schema to target version = " + targetVersion);	UpgradeCatalog targetUpgradeCatalog = AbstractUpgradeCatalog .getUpgradeCatalog(targetVersion);	
target upgrade catalog 

if (e.getMessage().contains("Column 'T.HOST_NAME' is either not in any table in the FROM list") || e.getMessage().contains("Column 'T.HOSTNAME' is either not in any table in the FROM list")) {	System.out.println("Ignoring on purpose, " + e.getMessage());	} else {	throw e;	}	}	schemaUpgradeHelper.startPersistenceService();	schemaUpgradeHelper.executePreDMLUpdates(upgradeCatalogs);	schemaUpgradeHelper.executeDMLUpdates(upgradeCatalogs, "test");	schemaUpgradeHelper.executeOnPostUpgrade(upgradeCatalogs);	
upgrade successful 

========================= ambari sample_1857 =========================

client.getCredentialsProvider().setCredentials( new AuthScope(/* host */null, /* port */-1, /* realm */null), EMPTY_JAAS_CREDENTIALS);	String hadoopAuthCookie = null;	HttpResponse httpResponse = null;	try {	HttpHost httpHost = new HttpHost(host, port, scheme);	HttpRequest httpRequest = new HttpOptions(path);	httpResponse = client.execute(httpHost, httpRequest);	Header[] headers = httpResponse.getHeaders(SET_COOKIE);	hadoopAuthCookie = getHadoopAuthCookieValue(headers);	if (hadoopAuthCookie == null) {	
spnego authentication failed can not get hadoop auth cookie for url 

if (httpResponse != null) {	HttpEntity entity = httpResponse.getEntity();	if (entity != null) {	entity.getContent().close();	}	}	}	hadoopAuthCookie = HADOOP_AUTH_EQ + quote(hadoopAuthCookie);	setAppCookie(endpoint, hadoopAuthCookie);	if (LOG.isInfoEnabled()) {	
successful spnego authentication to url 

========================= ambari sample_3493 =========================

public void handleMessage(HiveMessage hiveMessage) {	Object message = hiveMessage.getMessage();	if (message instanceof StartLogAggregation) {	start((StartLogAggregation) message);	}	if (message instanceof GetMoreLogs) {	try {	getMoreLogs();	} catch (SQLException e) {	
sql error while getting logs tried writing to exception 

public void handleMessage(HiveMessage hiveMessage) {	Object message = hiveMessage.getMessage();	if (message instanceof StartLogAggregation) {	start((StartLogAggregation) message);	}	if (message instanceof GetMoreLogs) {	try {	getMoreLogs();	} catch (SQLException e) {	} catch (HdfsApiException e) {	
hdfs error while getting writing logs to exception 

========================= ambari sample_930 =========================

public StackManager(@Assisted("stackRoot") File stackRoot, MetainfoDAO metaInfoDAO, ActionMetadata actionMetadata, StackDAO stackDao, ExtensionDAO extensionDao, ExtensionLinkDAO linkDao, AmbariManagementHelper helper) throws AmbariException {	
initializing the stack manager 

parseDirectories(stackRoot, commonServicesRoot, extensionRoot);	for (StackModule module : stackModules.values()) {	StackInfo stack = module.getModuleInfo();	List<ExtensionLinkEntity> entities = linkDao.findByStack(stack.getName(), stack.getVersion());	for (ExtensionLinkEntity entity : entities) {	String name = entity.getExtension().getExtensionName();	String version = entity.getExtension().getExtensionVersion();	String key = name + StackManager.PATH_DELIMITER + version;	ExtensionModule extensionModule = extensionModules.get(key);	if (extensionModule != null) {	
adding extension to stack version extension version 

protected void parseDirectories(File stackRoot, File commonServicesRoot, File extensionRoot) throws AmbariException {	commonServiceModules = parseCommonServicesDirectory(commonServicesRoot);	stackModules = parseStackDirectory(stackRoot);	
about to parse extension directories 

private void populateDB(StackDAO stackDao, ExtensionDAO extensionDao) throws AmbariException {	Collection<StackInfo> stacks = getStacks();	for(StackInfo stack : stacks){	String stackName = stack.getName();	String stackVersion = stack.getVersion();	if (stackDao.find(stackName, stackVersion) == null) {	
adding stack to the database 

stackEntity.setStackName(stackName);	stackEntity.setStackVersion(stackVersion);	stackDao.create(stackEntity);	}	}	Collection<ExtensionInfo> extensions = getExtensions();	for(ExtensionInfo extension : extensions){	String extensionName = extension.getName();	String extensionVersion = extension.getVersion();	if (extensionDao.find(extensionName, extensionVersion) == null) {	
adding extension to the database 

private void createLinks() {	
creating links 

private void validateCommonServicesDirectory(File commonServicesRoot) throws AmbariException {	if(commonServicesRoot != null) {	
validating common services directory 

private void validateStackDirectory(File stackRoot) throws AmbariException {	
validating stack directory 

private void validateExtensionDirectory(File extensionRoot) throws AmbariException {	
validating extension directory 

========================= ambari sample_4478 =========================

public static <T> T executeWithRetry(Callable<T> command) throws AmbariException {	RetryHelper.clearAffectedClusters();	int retryAttempts = RetryHelper.getOperationsRetryAttempts();	do {	try {	return command.call();	} catch (Exception e) {	if (RetryHelper.isDatabaseException(e)) {	RetryHelper.invalidateAffectedClusters();	if (retryAttempts > 0) {	
ignoring database exception to perform operation retry attempts remaining 

========================= ambari sample_2955 =========================

ObjectNode serviceVersion = (ObjectNode) service.get(STACK_SERVICES_PROPERTY);	String serviceName = serviceVersion.get("service_name").getTextValue();	try {	ServiceInfo serviceInfo = metaInfo.getService(stackName, stackVersion, serviceName);	if (serviceInfo.getAdvisorFile() != null) {	serviceVersion.put("advisor_name", serviceInfo.getAdvisorName());	serviceVersion.put("advisor_path", serviceInfo.getAdvisorFile().getAbsolutePath());	}	}	catch (Exception e) {	
error adding service advisor information to services json 

private void cleanupRequestDirectory() throws IOException {	final Date cutoffDate = DateUtils.getDateSpecifiedTimeAgo(recommendationsArtifactsLifetime);	String[] oldDirectories = recommendationsDir.list(new FilenameFilter() {	public boolean accept(File current, String name) {	File file = new File(current, name);	return file.isDirectory() && !FileUtils.isFileNewer(file, cutoffDate);	}	});	if(oldDirectories.length > 0) {	
deleting old directories s from s 

========================= ambari sample_4182 =========================

try {	switch(privilegeEntity.getResource().getResourceType().getName()) {	case "CLUSTER": key = clusterDAO.findByResourceId(privilegeEntity.getResource().getId()).getClusterName();	break;	case "AMBARI": key = "Ambari";	break;	default: key = viewInstanceDAO.findByResourceId(privilegeEntity.getResource().getId()).getLabel();	break;	}	} catch (Throwable ignored) {	
error occurred when cluster or view is searched based on resource id 

========================= ambari sample_2810 =========================

public DDLProxy(ViewContext context, TableMetaParserImpl tableMetaParser, SettingsResourceManager settingsResourceManager) {	this.context = context;	this.tableMetaParser = tableMetaParser;	this.settingsResourceManager = settingsResourceManager;	
creating ddlproxy 

public TableMeta getTableProperties(ViewContext context, ConnectionConfig connectionConfig, String databaseName, String tableName) {	DDLDelegator delegator = new DDLDelegatorImpl(context, ConnectionSystem.getInstance().getActorSystem(), ConnectionSystem.getInstance().getOperationController(context));	List<Row> createTableStatementRows = delegator.getTableCreateStatement(connectionConfig, databaseName, tableName);	List<Row> describeFormattedRows = delegator.getTableDescriptionFormatted(connectionConfig, databaseName, tableName);	DatabaseMetadataWrapper databaseMetadata = new DatabaseMetadataWrapper(1, 0);	try {	databaseMetadata = delegator.getDatabaseMetadata(connectionConfig);	} catch (ServiceException e) {	
exception while fetching hive version 

public String generateCreateTableDDL(String databaseName, TableMeta tableMeta) throws ServiceException {	if (Strings.isNullOrEmpty(tableMeta.getDatabase())) {	tableMeta.setDatabase(databaseName);	}	Optional<String> createTableQuery = new CreateTableQueryGenerator(tableMeta).getQuery();	if (createTableQuery.isPresent()) {	
generated create table query 

public String generateDeleteTableDDL(String databaseName, String tableName) throws ServiceException {	Optional<String> deleteTableQuery = new DeleteTableQueryGenerator(databaseName, tableName).getQuery();	if (deleteTableQuery.isPresent()) {	
deleting table with query 

public Job createJob(String databaseName, String query, String jobTitle, JobResourceManager resourceManager) throws ServiceException {	
creating job for 

public Job createJob(String databaseName, String query, String jobTitle, JobResourceManager resourceManager) throws ServiceException {	Map jobInfo = new HashMap<>();	jobInfo.put("title", jobTitle);	jobInfo.put("forcedContent", query);	jobInfo.put("dataBase", databaseName);	jobInfo.put("referrer", JobImpl.REFERRER.INTERNAL.name());	try {	Job job = new JobImpl(jobInfo);	JobController createdJobController = new JobServiceInternal().createJob(job, resourceManager);	Job returnableJob = createdJobController.getJobPOJO();	
returning job with id for 

jobInfo.put("title", jobTitle);	jobInfo.put("forcedContent", query);	jobInfo.put("dataBase", databaseName);	jobInfo.put("referrer", JobImpl.REFERRER.INTERNAL.name());	try {	Job job = new JobImpl(jobInfo);	JobController createdJobController = new JobServiceInternal().createJob(job, resourceManager);	Job returnableJob = createdJobController.getJobPOJO();	return returnableJob;	} catch (Throwable e) {	
exception occurred while 

} else if (row.length > 1 && row[0].equalsIgnoreCase("COLUMN_STATS_ACCURATE")) {	columnStats.setColumnStatsAccurate(row[1]);	}	}	}	return columnStats;	} else {	throw new ServiceException("Cannot find any result for this jobId: " + jobId);	}	} catch (HiveClientException e) {	
exception occurred while fetching results for column statistics with jobid 

========================= ambari sample_571 =========================

public <T extends Indexed> T load(Class<T> model, int id) throws ItemNotFound {	String modelPropName = getItemPropertyName(model, id);	
loading s 

public <T extends Indexed> T load(Class<T> model, int id) throws ItemNotFound {	String modelPropName = getItemPropertyName(model, id);	if (getConfig().containsKey(modelPropName)) {	String json = read(modelPropName);	
json s 

public synchronized <T extends Indexed> List<T> loadAll(Class<T> model, FilteringStrategy filter) {	ArrayList<T> list = new ArrayList<T>();	String modelIndexingPropName = getIndexPropertyName(model);	
loading all s s 

public synchronized void delete(Class model, int id) {	
deleting s d 

========================= ambari sample_1046 =========================

public void populateResource(Resource resource, InputStream inputStream) throws SystemException {	try {	Map<String, Object> responseMap = GSON.fromJson(IOUtils.toString(inputStream, "UTF-8"), MAP_TYPE);	if (responseMap == null){	
properties map from http response is null 

========================= ambari sample_3581 =========================

protected abstract void doWork(Map<String, Object> properties) throws AmbariException;	protected abstract void finalizeExecution(Map<String, Object> properties) throws AmbariException;	public void execute(JobExecutionContext context) throws JobExecutionException {	JobKey jobKey = context.getJobDetail().getKey();	
executing linear job 

JobKey jobKey = context.getJobDetail().getKey();	JobDataMap jobDataMap = context.getMergedJobDataMap();	if (!executionScheduleManager.continueOnMisfire(context)) {	throw new JobExecutionException("Canceled execution based on misfire" + " toleration threshold, job: " + jobKey + ", scheduleTime = " + context.getScheduledFireTime());	}	Map<String, Object> properties = jobDataMap.getWrappedMap();	boolean finalize = false;	try {	doWork(properties);	} catch (AmbariException e) {	
exception caught on execution of job exiting linear chain 

throw new JobExecutionException("Canceled execution based on misfire" + " toleration threshold, job: " + jobKey + ", scheduleTime = " + context.getScheduledFireTime());	}	Map<String, Object> properties = jobDataMap.getWrappedMap();	boolean finalize = false;	try {	doWork(properties);	} catch (AmbariException e) {	finalize = true;	throw new JobExecutionException(e);	} catch (RuntimeException e) {	
unexpected exception caught on execution of job exiting linear chain 

finalize = true;	throw new JobExecutionException(e);	} catch (RuntimeException e) {	finalize = true;	throw e;	} finally {	if (finalize) {	try {	finalizeExecution(properties);	} catch (AmbariException e) {	
unable to finalize execution for job 

finalize = true;	throw e;	} finally {	if (finalize) {	try {	finalizeExecution(properties);	} catch (AmbariException e) {	}	}	}	
finished linear job 

if (finalize) {	try {	finalizeExecution(properties);	} catch (AmbariException e) {	}	}	}	String nextJobName = jobDataMap.getString(NEXT_EXECUTION_JOB_NAME_KEY);	String nextJobGroup = jobDataMap.getString(NEXT_EXECUTION_JOB_GROUP_KEY);	if (nextJobName == null || nextJobName.isEmpty()) {	
end of linear job chain returning with success 

} catch (AmbariException e) {	}	}	}	String nextJobName = jobDataMap.getString(NEXT_EXECUTION_JOB_NAME_KEY);	String nextJobGroup = jobDataMap.getString(NEXT_EXECUTION_JOB_GROUP_KEY);	if (nextJobName == null || nextJobName.isEmpty()) {	try {	finalizeExecution(properties);	} catch (AmbariException e) {	
unable to finalize execution for job 

========================= ambari sample_2943 =========================

private Authentication doAuth(Authentication authentication, AuthMethod authMethod) {	switch (authMethod) {	case FILE: return fileAuthenticationProvider.authenticate(authentication);	case EXTERNAL_AUTH: return externalServerAuthenticationProvider.authenticate(authentication);	case SIMPLE: return simpleAuthenticationProvider.authenticate(authentication);	
invalid authentication method 

========================= ambari sample_1396 =========================

public ShellCommandUtil.Result runCommand(String[] args) throws IOException, InterruptedException {	
running command 

public ShellCommandUtil.Result runCommand(String[] args, Map<String, String> vars) throws IOException, InterruptedException {	
running command variables 

========================= ambari sample_3344 =========================

name = "file";	}	}	SignerSecretProvider provider;	if ("file".equals(name)) {	provider = new FileSignerSecretProvider();	try {	provider.init(config, ctx, validity);	} catch (Exception e) {	if (!disallowFallbackToRandomSecretProvider) {	
unable to initialize filesignersecretprovider falling back to use random secrets 

public void doFilter(ServletRequest request, ServletResponse response, FilterChain filterChain) throws IOException, ServletException {	HttpServletRequest httpRequest = (HttpServletRequest) request;	
logsearchkrbfilter public filter path 

HttpServletResponse httpResponse = (HttpServletResponse) response;	boolean isHttps = "https".equals(httpRequest.getScheme());	try {	boolean newToken = false;	AuthenticationToken token;	try {	token = getToken(httpRequest);	}	catch (AuthenticationException ex) {	ex.printStackTrace();	
authenticationtoken ignored 

token = getToken(httpRequest);	}	catch (AuthenticationException ex) {	ex.printStackTrace();	authenticationEx = ex;	token = null;	}	if (authHandler.managementOperation(token, httpRequest, httpResponse)) {	if (token == null) {	if (logger.isDebugEnabled()) {	
request triggering authentication 

}	token = authHandler.authenticate(httpRequest, httpResponse);	if (token != null && token.getExpires() != 0 && token != AuthenticationToken.ANONYMOUS) {	token.setExpires(System.currentTimeMillis() + getValidity() * 1000);	}	newToken = true;	}	if (token != null) {	unauthorizedResponse = false;	if (logger.isDebugEnabled()) {	
request user authenticated 

}	doFilter(filterChain, httpRequest, httpResponse);	}	} else {	unauthorizedResponse = false;	}	} catch (AuthenticationException ex) {	ex.printStackTrace();	errCode = HttpServletResponse.SC_FORBIDDEN;	authenticationEx = ex;	
authentication exception 

========================= ambari sample_1379 =========================

public void testSortedCommands() {	injector.getInstance(OrmTestHelper.class).createStageCommands();	HostRoleCommandDAO hostRoleCommandDAO = injector.getInstance(HostRoleCommandDAO.class);	HostDAO hostDAO = injector.getInstance(HostDAO.class);	StageDAO stageDAO = injector.getInstance(StageDAO.class);	List<HostRoleCommandEntity> list = hostRoleCommandDAO.findSortedCommandsByStageAndHost( stageDAO.findByActionId("1-1"), hostDAO.findByName("test_host1"));	
command taskid 

public void testSortedCommands() {	injector.getInstance(OrmTestHelper.class).createStageCommands();	HostRoleCommandDAO hostRoleCommandDAO = injector.getInstance(HostRoleCommandDAO.class);	HostDAO hostDAO = injector.getInstance(HostDAO.class);	StageDAO stageDAO = injector.getInstance(StageDAO.class);	List<HostRoleCommandEntity> list = hostRoleCommandDAO.findSortedCommandsByStageAndHost( stageDAO.findByActionId("1-1"), hostDAO.findByName("test_host1"));	
command taskid 

public void testFindHostsByStage() {	ormTestHelper.createStageCommands();	StageEntity stageEntity = stageDAO.findByActionId("1-1");	
stageentity 

========================= ambari sample_2555 =========================

public Authentication authenticate(Authentication authentication) throws AuthenticationException {	if (!authPropsConfig.isAuthSimpleEnabled()) {	
simple auth is disabled 

========================= ambari sample_1400 =========================

protected void invalidateHostRoleCommandStatusSummaryCache(Long requestId) {	if (!hostRoleCommandStatusSummaryCacheEnabled ) {	return;	}	
invalidating host role command status summary cache for request 

public HostRoleCommandDAO( this.hostRoleCommandStatusSummaryCacheEnabled = hostRoleCommandStatusSummaryCacheEnabled;	
host role command status summary cache enabled disabled 

========================= ambari sample_4252 =========================

try {	fsUri = new URI(defaultFS);	String protocol = fsUri.getScheme();	String ambariSkipCheckValues = viewContext.getAmbariProperty(Constants.AMBARI_SKIP_HOME_DIRECTORY_CHECK_PROTOCOL_LIST);	List<String> protocolSkipList = (ambariSkipCheckValues == null? new LinkedList<String>() : Arrays.asList(ambariSkipCheckValues.split(",")));	if(null != protocol && protocolSkipList.contains(protocol)){	policy.setCheckHomeDirectory(false);	return policy;	}	} catch (URISyntaxException e) {	
error occurred while parsing the defaultfs uri 

========================= ambari sample_567 =========================

private Map<String, Map<String, Collection<String>>> getRequiredPropertiesByService(Blueprint blueprint) {	Map<String, Map<String, Collection<String>>> requiredPropertiesForServiceByType = new HashMap<>();	for (String bpService : blueprint.getServices()) {	
collecting required properties for the service 

private Map<String, Map<String, Collection<String>>> getRequiredPropertiesByService(Blueprint blueprint) {	Map<String, Map<String, Collection<String>>> requiredPropertiesForServiceByType = new HashMap<>();	for (String bpService : blueprint.getServices()) {	Collection<Stack.ConfigProperty> requiredConfigsForService = blueprint.getStack().getRequiredConfigurationProperties(bpService);	Map<String, Collection<String>> requiredPropertiesByConfigType = new HashMap<>();	for (Stack.ConfigProperty configProperty : requiredConfigsForService) {	if (configProperty.getPropertyTypes() != null && configProperty.getPropertyTypes().contains(PropertyInfo.PropertyType.PASSWORD)) {	
skipping required property validation for password type 

for (String bpService : blueprint.getServices()) {	Collection<Stack.ConfigProperty> requiredConfigsForService = blueprint.getStack().getRequiredConfigurationProperties(bpService);	Map<String, Collection<String>> requiredPropertiesByConfigType = new HashMap<>();	for (Stack.ConfigProperty configProperty : requiredConfigsForService) {	if (configProperty.getPropertyTypes() != null && configProperty.getPropertyTypes().contains(PropertyInfo.PropertyType.PASSWORD)) {	continue;	}	if (requiredPropertiesForServiceByType.containsKey(bpService)) {	requiredPropertiesByConfigType = requiredPropertiesForServiceByType.get(bpService);	} else {	
adding required properties entry for service 

}	if (requiredPropertiesForServiceByType.containsKey(bpService)) {	requiredPropertiesByConfigType = requiredPropertiesForServiceByType.get(bpService);	} else {	requiredPropertiesForServiceByType.put(bpService, requiredPropertiesByConfigType);	}	Collection<String> requiredPropsForType = new HashSet<>();	if (requiredPropertiesByConfigType.containsKey(configProperty.getType())) {	requiredPropsForType = requiredPropertiesByConfigType.get(configProperty.getType());	} else {	
adding required properties entry for configuration type 

} else {	requiredPropertiesForServiceByType.put(bpService, requiredPropertiesByConfigType);	}	Collection<String> requiredPropsForType = new HashSet<>();	if (requiredPropertiesByConfigType.containsKey(configProperty.getType())) {	requiredPropsForType = requiredPropertiesByConfigType.get(configProperty.getType());	} else {	requiredPropertiesByConfigType.put(configProperty.getType(), requiredPropsForType);	}	requiredPropsForType.add(configProperty.getName());	
added required property for service configuration type property 

}	Collection<String> requiredPropsForType = new HashSet<>();	if (requiredPropertiesByConfigType.containsKey(configProperty.getType())) {	requiredPropsForType = requiredPropertiesByConfigType.get(configProperty.getType());	} else {	requiredPropertiesByConfigType.put(configProperty.getType(), requiredPropsForType);	}	requiredPropsForType.add(configProperty.getName());	}	}	
identified required properties for blueprint services 

========================= ambari sample_2693 =========================

throw new Exception(th);	}	}	JSONObject jsonJob = jsonObjectFromJob(jobController);	return Response.ok(jsonJob).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (ItemNotFound itemNotFound) {	throw new NotFoundFormattedException(itemNotFound.getMessage(), itemNotFound);	} catch (Exception ex) {	
exception while fetching status of job with id 

csvPrinter.printRecord(resultSet.next().getRow());	writer.flush();	}	} finally {	writer.close();	}	}	};	return Response.ok(stream).build();	} catch (WebApplicationException ex) {	
error occurred while downloading result with filename 

}	} finally {	writer.close();	}	}	};	return Response.ok(stream).build();	} catch (WebApplicationException ex) {	throw ex;	}  catch (Throwable ex) {	
error occurred while downloading result with filename 

public Response fetchJobStatus(@PathParam("jobId") String jobId) throws ItemNotFound, HiveClientException, NoOperationStatusSetException {	JobController jobController = getResourceManager().readController(jobId);	Job job = jobController.getJob();	String jobStatus = job.getStatus();	
jobstatus for jobid 

public Response getList(@QueryParam("startTime") long startTime, @QueryParam("endTime") long endTime) {	try {	
getting all job starttime endtime 

public Response getList(@QueryParam("startTime") long startTime, @QueryParam("endTime") long endTime) {	try {	List<Job> allJobs = getAggregator().readAllForUserByTime(context.getUsername(),startTime, endTime);	for(Job job : allJobs) {	job.setSessionTag(null);	}	JSONObject result = new JSONObject();	result.put("jobs", allJobs);	return Response.ok(result).build();	} catch (WebApplicationException ex) {	
exception occured while fetching all jobs 

List<Job> allJobs = getAggregator().readAllForUserByTime(context.getUsername(),startTime, endTime);	for(Job job : allJobs) {	job.setSessionTag(null);	}	JSONObject result = new JSONObject();	result.put("jobs", allJobs);	return Response.ok(result).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (Exception ex) {	
exception occured while fetching all jobs 

public List<Job> getList(List<JobInfo> jobInfos) {	try {	
fetching jobs with ids 

public List<Job> getList(List<JobInfo> jobInfos) {	try {	List<Job> allJobs = getAggregator().readJobsByIds(jobInfos);	for(Job job : allJobs) {	job.setSessionTag(null);	}	return allJobs;	} catch (WebApplicationException ex) {	
exception occured while fetching all jobs 

public List<Job> getList(List<JobInfo> jobInfos) {	try {	List<Job> allJobs = getAggregator().readJobsByIds(jobInfos);	for(Job job : allJobs) {	job.setSessionTag(null);	}	return allJobs;	} catch (WebApplicationException ex) {	throw ex;	} catch (Exception ex) {	
exception occured while fetching all jobs 

public Response create(JobRequest request, @Context HttpServletResponse response, try {	Map jobInfo = PropertyUtils.describe(request.job);	Job job = new JobImpl(jobInfo);	JobController createdJobController = new JobServiceInternal().createJob(job, getResourceManager());	JSONObject jobObject = jsonObjectFromJob(createdJobController);	response.setHeader("Location", String.format("%s/%s", ui.getAbsolutePath().toString(), job.getId()));	return Response.ok(jobObject).status(201).build();	} catch (WebApplicationException ex) {	
error occurred while creating job 

public Response create(JobRequest request, @Context HttpServletResponse response, try {	Map jobInfo = PropertyUtils.describe(request.job);	Job job = new JobImpl(jobInfo);	JobController createdJobController = new JobServiceInternal().createJob(job, getResourceManager());	JSONObject jobObject = jsonObjectFromJob(createdJobController);	response.setHeader("Location", String.format("%s/%s", ui.getAbsolutePath().toString(), job.getId()));	return Response.ok(jobObject).status(201).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (ItemNotFound itemNotFound) {	
error occurred while creating job 

Job job = new JobImpl(jobInfo);	JobController createdJobController = new JobServiceInternal().createJob(job, getResourceManager());	JSONObject jobObject = jsonObjectFromJob(createdJobController);	response.setHeader("Location", String.format("%s/%s", ui.getAbsolutePath().toString(), job.getId()));	return Response.ok(jobObject).status(201).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (ItemNotFound itemNotFound) {	throw new NotFoundFormattedException(itemNotFound.getMessage(), itemNotFound);	} catch (Throwable ex) {	
error occurred while creating job 

========================= ambari sample_588 =========================

public Set<AlertDefinition> getAlertDefinitions(Reader reader, String serviceName) throws AmbariException {	Map<String, Map<String, List<AlertDefinition>>> serviceDefinitionMap = null;	try {	Type type = new TypeToken<Map<String, Map<String, List<AlertDefinition>>>>() {}.getType();	serviceDefinitionMap = m_gson.fromJson(reader, type);	} catch (Exception e) {	
could not read the alert definitions 

definition.setServiceName(entity.getServiceName());	definition.setLabel(entity.getLabel());	definition.setHelpURL(entity.getHelpURL());	definition.setDescription(entity.getDescription());	definition.setUuid(entity.getHash());	try{	String sourceJson = entity.getSource();	Source source = m_gson.fromJson(sourceJson, Source.class);	definition.setSource(source);	} catch (Exception exception) {	
unable to deserialize the alert definition source during coercion 

public AlertDefinitionEntity mergeSource(Source source, AlertDefinitionEntity entity) {	entity.setSourceType(source.getType());	try {	String sourceJson = m_gson.toJson(source);	entity.setSource(sourceJson);	} catch (Exception e) {	
unable to serialize the alert definition source during merge 

clazz = RecoverySource.class;	break;	}	case SERVER:{	clazz = ServerSource.class;	break;	}	default: break;	}	if (null == clazz) {	
unable to deserialize an alert definition with source type 

========================= ambari sample_3082 =========================

public Authentication authenticate(Authentication authentication) throws AuthenticationException {	if (authentication.getName() == null) {	
authentication failed no username provided 

public Authentication authenticate(Authentication authentication) throws AuthenticationException {	if (authentication.getName() == null) {	throw new InvalidUsernamePasswordCombinationException("");	}	String userName = authentication.getName().trim();	if (authentication.getCredentials() == null) {	
authentication failed no credentials provided 

if (authentication.getName() == null) {	throw new InvalidUsernamePasswordCombinationException("");	}	String userName = authentication.getName().trim();	if (authentication.getCredentials() == null) {	throw new InvalidUsernamePasswordCombinationException(userName);	}	Users users = getUsers();	UserEntity userEntity = users.getUserEntity(userName);	if (userEntity == null) {	
user not found 

Users users = getUsers();	UserEntity userEntity = users.getUserEntity(userName);	if (userEntity == null) {	throw new InvalidUsernamePasswordCombinationException(userName);	}	UserAuthenticationEntity authenticationEntity = getAuthenticationEntity(userEntity, UserAuthenticationType.LOCAL);	if (authenticationEntity != null) {	String password = authenticationEntity.getAuthenticationKey();	String presentedPassword = authentication.getCredentials().toString();	if (passwordEncoder.matches(presentedPassword, password)) {	
authentication succeeded a matching username and password were found 

} else {	throw new InvalidUsernamePasswordCombinationException(userName, false, e);	}	}	User user = new User(userEntity);	Authentication auth = new AmbariUserAuthentication(password, user, users.getUserAuthorities(userEntity));	auth.setAuthenticated(true);	return auth;	}	}	
authentication failed password does not match stored value 

========================= ambari sample_2851 =========================

public void hiveHistoryQueryMigration(String username, String instance, String startDate, String endDate, ViewContext view, MigrationResponse migrationresult, String jobid) throws IOException, ItemNotFound {	InitiateJobMigration migrationservice = new InitiateJobMigration();	long startTime = System.currentTimeMillis();	final Logger logger = Logger.getLogger(HiveHistoryMigrationUtility.class);	Connection connectionHuedb = null;	Connection connectionAmbaridb = null;	
java class path 

public void hiveHistoryQueryMigration(String username, String instance, String startDate, String endDate, ViewContext view, MigrationResponse migrationresult, String jobid) throws IOException, ItemNotFound {	InitiateJobMigration migrationservice = new InitiateJobMigration();	long startTime = System.currentTimeMillis();	final Logger logger = Logger.getLogger(HiveHistoryMigrationUtility.class);	Connection connectionHuedb = null;	Connection connectionAmbaridb = null;	logger.info("--------------------------------------");	
hive history query migration started 

public void hiveHistoryQueryMigration(String username, String instance, String startDate, String endDate, ViewContext view, MigrationResponse migrationresult, String jobid) throws IOException, ItemNotFound {	InitiateJobMigration migrationservice = new InitiateJobMigration();	long startTime = System.currentTimeMillis();	final Logger logger = Logger.getLogger(HiveHistoryMigrationUtility.class);	Connection connectionHuedb = null;	Connection connectionAmbaridb = null;	logger.info("--------------------------------------");	logger.info("--------------------------------------");	
start date 

public void hiveHistoryQueryMigration(String username, String instance, String startDate, String endDate, ViewContext view, MigrationResponse migrationresult, String jobid) throws IOException, ItemNotFound {	InitiateJobMigration migrationservice = new InitiateJobMigration();	long startTime = System.currentTimeMillis();	final Logger logger = Logger.getLogger(HiveHistoryMigrationUtility.class);	Connection connectionHuedb = null;	Connection connectionAmbaridb = null;	logger.info("--------------------------------------");	logger.info("--------------------------------------");	
enddate date 

public void hiveHistoryQueryMigration(String username, String instance, String startDate, String endDate, ViewContext view, MigrationResponse migrationresult, String jobid) throws IOException, ItemNotFound {	InitiateJobMigration migrationservice = new InitiateJobMigration();	long startTime = System.currentTimeMillis();	final Logger logger = Logger.getLogger(HiveHistoryMigrationUtility.class);	Connection connectionHuedb = null;	Connection connectionAmbaridb = null;	logger.info("--------------------------------------");	logger.info("--------------------------------------");	
instance is 

public void hiveHistoryQueryMigration(String username, String instance, String startDate, String endDate, ViewContext view, MigrationResponse migrationresult, String jobid) throws IOException, ItemNotFound {	InitiateJobMigration migrationservice = new InitiateJobMigration();	long startTime = System.currentTimeMillis();	final Logger logger = Logger.getLogger(HiveHistoryMigrationUtility.class);	Connection connectionHuedb = null;	Connection connectionAmbaridb = null;	logger.info("--------------------------------------");	logger.info("--------------------------------------");	
hue username is 

} else if (view.getProperties().get("ambaridrivername").contains("oracle")) {	ambaridatabase = new OracleQuerySetAmbariDB();	}	try {	String[] usernames = username.split(",");	int totalQueries = 0;	for (int k = 0; k < usernames.length; k++) {	connectionHuedb = DataSourceHueDatabase.getInstance(view.getProperties().get("huedrivername"), view.getProperties().get("huejdbcurl"), view.getProperties().get("huedbusername"), view.getProperties().get("huedbpassword")).getConnection();	username = usernames[k];	migrationresult.setProgressPercentage(0);	
migration started for user 

try {	String[] usernames = username.split(",");	int totalQueries = 0;	for (int k = 0; k < usernames.length; k++) {	connectionHuedb = DataSourceHueDatabase.getInstance(view.getProperties().get("huedrivername"), view.getProperties().get("huejdbcurl"), view.getProperties().get("huedbusername"), view.getProperties().get("huedbpassword")).getConnection();	username = usernames[k];	migrationresult.setProgressPercentage(0);	dbpojoHiveHistoryQuery = hiveHistoryQueryImpl.fetchFromHue(username, startDate, endDate, connectionHuedb, huedatabase);	totalQueries += dbpojoHiveHistoryQuery.size();	for (int j = 0; j < dbpojoHiveHistoryQuery.size(); j++) {	
the query fetched from hue 

int totalQueries = 0;	for (int k = 0; k < usernames.length; k++) {	connectionHuedb = DataSourceHueDatabase.getInstance(view.getProperties().get("huedrivername"), view.getProperties().get("huejdbcurl"), view.getProperties().get("huedbusername"), view.getProperties().get("huedbpassword")).getConnection();	username = usernames[k];	migrationresult.setProgressPercentage(0);	dbpojoHiveHistoryQuery = hiveHistoryQueryImpl.fetchFromHue(username, startDate, endDate, connectionHuedb, huedatabase);	totalQueries += dbpojoHiveHistoryQuery.size();	for (int j = 0; j < dbpojoHiveHistoryQuery.size(); j++) {	}	if (dbpojoHiveHistoryQuery.size() == 0) {	
no queries has been selected for the user between dates 

int id = 0;	id = hiveHistoryQueryImpl.fetchInstanceTablename(connectionAmbaridb, instance, ambaridatabase);	sequence = hiveHistoryQueryImpl.fetchSequenceno(connectionAmbaridb, id, ambaridatabase);	for (i = 0; i < dbpojoHiveHistoryQuery.size(); i++) {	float calc = ((float) (i + 1)) / dbpojoHiveHistoryQuery.size() * 100;	int progressPercentage = Math.round(calc);	migrationresult.setProgressPercentage(progressPercentage);	migrationresult.setNumberOfQueryTransfered(i + 1);	getResourceManager(view).update(migrationresult, jobid);	logger.info("_____________________");	
loop no 

id = hiveHistoryQueryImpl.fetchInstanceTablename(connectionAmbaridb, instance, ambaridatabase);	sequence = hiveHistoryQueryImpl.fetchSequenceno(connectionAmbaridb, id, ambaridatabase);	for (i = 0; i < dbpojoHiveHistoryQuery.size(); i++) {	float calc = ((float) (i + 1)) / dbpojoHiveHistoryQuery.size() * 100;	int progressPercentage = Math.round(calc);	migrationresult.setProgressPercentage(progressPercentage);	migrationresult.setNumberOfQueryTransfered(i + 1);	getResourceManager(view).update(migrationresult, jobid);	logger.info("_____________________");	logger.info("_____________________");	
hue query that has been fetched 

id = hiveHistoryQueryImpl.fetchInstanceTablename(connectionAmbaridb, instance, ambaridatabase);	sequence = hiveHistoryQueryImpl.fetchSequenceno(connectionAmbaridb, id, ambaridatabase);	for (i = 0; i < dbpojoHiveHistoryQuery.size(); i++) {	float calc = ((float) (i + 1)) / dbpojoHiveHistoryQuery.size() * 100;	int progressPercentage = Math.round(calc);	migrationresult.setProgressPercentage(progressPercentage);	migrationresult.setNumberOfQueryTransfered(i + 1);	getResourceManager(view).update(migrationresult, jobid);	logger.info("_____________________");	logger.info("_____________________");	
table name has been fetched from intance name 

sequence = hiveHistoryQueryImpl.fetchSequenceno(connectionAmbaridb, id, ambaridatabase);	for (i = 0; i < dbpojoHiveHistoryQuery.size(); i++) {	float calc = ((float) (i + 1)) / dbpojoHiveHistoryQuery.size() * 100;	int progressPercentage = Math.round(calc);	migrationresult.setProgressPercentage(progressPercentage);	migrationresult.setNumberOfQueryTransfered(i + 1);	getResourceManager(view).update(migrationresult, jobid);	logger.info("_____________________");	logger.info("_____________________");	hiveHistoryQueryImpl.writetoFileQueryhql(dbpojoHiveHistoryQuery.get(i).getQuery(), ConfigurationCheckImplementation.getHomeDir());	
hql file created in temp directory 

for (i = 0; i < dbpojoHiveHistoryQuery.size(); i++) {	float calc = ((float) (i + 1)) / dbpojoHiveHistoryQuery.size() * 100;	int progressPercentage = Math.round(calc);	migrationresult.setProgressPercentage(progressPercentage);	migrationresult.setNumberOfQueryTransfered(i + 1);	getResourceManager(view).update(migrationresult, jobid);	logger.info("_____________________");	logger.info("_____________________");	hiveHistoryQueryImpl.writetoFileQueryhql(dbpojoHiveHistoryQuery.get(i).getQuery(), ConfigurationCheckImplementation.getHomeDir());	hiveHistoryQueryImpl.writetoFileLogs(ConfigurationCheckImplementation.getHomeDir());	
log file created in temp directory 

maxCountOfAmbariDb = i + sequence + 1;	time = hiveHistoryQueryImpl.getTime();	epochTime = hiveHistoryQueryImpl.getEpochTime();	if (usernames[k].equals("all")) {	username = dbpojoHiveHistoryQuery.get(i).getOwnerName();	}	dirNameforHiveHistroy = " logger.info("Directory name where .hql will be saved: " + dirNameforHiveHistroy);	String versionName = hiveHistoryQueryImpl.getAllHiveVersionInstance(connectionAmbaridb, ambaridatabase, instance);	hiveHistoryQueryImpl.insertRowinAmbaridb(dirNameforHiveHistroy, maxCountOfAmbariDb, epochTime, connectionAmbaridb, id, instance, i, ambaridatabase, versionName, username, dbpojoHiveHistoryQuery.get(i).getJobStatus());	if (view.getProperties().get("KerberoseEnabled").equals("y")) {	
kerberose enabled 

time = hiveHistoryQueryImpl.getTime();	epochTime = hiveHistoryQueryImpl.getEpochTime();	if (usernames[k].equals("all")) {	username = dbpojoHiveHistoryQuery.get(i).getOwnerName();	}	dirNameforHiveHistroy = " logger.info("Directory name where .hql will be saved: " + dirNameforHiveHistroy);	String versionName = hiveHistoryQueryImpl.getAllHiveVersionInstance(connectionAmbaridb, ambaridatabase, instance);	hiveHistoryQueryImpl.insertRowinAmbaridb(dirNameforHiveHistroy, maxCountOfAmbariDb, epochTime, connectionAmbaridb, id, instance, i, ambaridatabase, versionName, username, dbpojoHiveHistoryQuery.get(i).getJobStatus());	if (view.getProperties().get("KerberoseEnabled").equals("y")) {	hiveHistoryQueryImpl.createDirKerberorisedSecured(dirNameforHiveHistroy, view.getProperties().get("namenode_URI_Ambari"), username, view.getProperties().get("PrincipalUserName"));	
directory created in hdfs 

username = dbpojoHiveHistoryQuery.get(i).getOwnerName();	}	dirNameforHiveHistroy = " logger.info("Directory name where .hql will be saved: " + dirNameforHiveHistroy);	String versionName = hiveHistoryQueryImpl.getAllHiveVersionInstance(connectionAmbaridb, ambaridatabase, instance);	hiveHistoryQueryImpl.insertRowinAmbaridb(dirNameforHiveHistroy, maxCountOfAmbariDb, epochTime, connectionAmbaridb, id, instance, i, ambaridatabase, versionName, username, dbpojoHiveHistoryQuery.get(i).getJobStatus());	if (view.getProperties().get("KerberoseEnabled").equals("y")) {	hiveHistoryQueryImpl.createDirKerberorisedSecured(dirNameforHiveHistroy, view.getProperties().get("namenode_URI_Ambari"), username, view.getProperties().get("PrincipalUserName"));	hiveHistoryQueryImpl.putFileinHdfsKerborizedSecured(ConfigurationCheckImplementation.getHomeDir() + "query.hql", dirNameforHiveHistroy, view.getProperties().get("namenode_URI_Ambari"), username, view.getProperties().get("PrincipalUserName"));	hiveHistoryQueryImpl.putFileinHdfsKerborizedSecured(ConfigurationCheckImplementation.getHomeDir() + "logs", dirNameforHiveHistroy, view.getProperties().get("namenode_URI_Ambari"), username, view.getProperties().get("PrincipalUserName"));	} else {	
kerberose not enabled 

}	dirNameforHiveHistroy = " logger.info("Directory name where .hql will be saved: " + dirNameforHiveHistroy);	String versionName = hiveHistoryQueryImpl.getAllHiveVersionInstance(connectionAmbaridb, ambaridatabase, instance);	hiveHistoryQueryImpl.insertRowinAmbaridb(dirNameforHiveHistroy, maxCountOfAmbariDb, epochTime, connectionAmbaridb, id, instance, i, ambaridatabase, versionName, username, dbpojoHiveHistoryQuery.get(i).getJobStatus());	if (view.getProperties().get("KerberoseEnabled").equals("y")) {	hiveHistoryQueryImpl.createDirKerberorisedSecured(dirNameforHiveHistroy, view.getProperties().get("namenode_URI_Ambari"), username, view.getProperties().get("PrincipalUserName"));	hiveHistoryQueryImpl.putFileinHdfsKerborizedSecured(ConfigurationCheckImplementation.getHomeDir() + "query.hql", dirNameforHiveHistroy, view.getProperties().get("namenode_URI_Ambari"), username, view.getProperties().get("PrincipalUserName"));	hiveHistoryQueryImpl.putFileinHdfsKerborizedSecured(ConfigurationCheckImplementation.getHomeDir() + "logs", dirNameforHiveHistroy, view.getProperties().get("namenode_URI_Ambari"), username, view.getProperties().get("PrincipalUserName"));	} else {	hiveHistoryQueryImpl.createDir(dirNameforHiveHistroy, view.getProperties().get("namenode_URI_Ambari"), username);	
directory created in hdfs 

hiveHistoryQueryImpl.putFileinHdfsKerborizedSecured(ConfigurationCheckImplementation.getHomeDir() + "logs", dirNameforHiveHistroy, view.getProperties().get("namenode_URI_Ambari"), username, view.getProperties().get("PrincipalUserName"));	} else {	hiveHistoryQueryImpl.createDir(dirNameforHiveHistroy, view.getProperties().get("namenode_URI_Ambari"), username);	hiveHistoryQueryImpl.putFileinHdfs(ConfigurationCheckImplementation.getHomeDir() + "query.hql", dirNameforHiveHistroy, view.getProperties().get("namenode_URI_Ambari"), username);	hiveHistoryQueryImpl.putFileinHdfs(ConfigurationCheckImplementation.getHomeDir() + "logs", dirNameforHiveHistroy, view.getProperties().get("namenode_URI_Ambari"), username);	}	}	hiveHistoryQueryImpl.updateSequenceno(connectionAmbaridb, maxCountOfAmbariDb, id, ambaridatabase);	connectionAmbaridb.commit();	}	
migration completed for user 

} else {	hiveHistoryQueryImpl.createDir(dirNameforHiveHistroy, view.getProperties().get("namenode_URI_Ambari"), username);	hiveHistoryQueryImpl.putFileinHdfs(ConfigurationCheckImplementation.getHomeDir() + "query.hql", dirNameforHiveHistroy, view.getProperties().get("namenode_URI_Ambari"), username);	hiveHistoryQueryImpl.putFileinHdfs(ConfigurationCheckImplementation.getHomeDir() + "logs", dirNameforHiveHistroy, view.getProperties().get("namenode_URI_Ambari"), username);	}	}	hiveHistoryQueryImpl.updateSequenceno(connectionAmbaridb, maxCountOfAmbariDb, id, ambaridatabase);	connectionAmbaridb.commit();	}	}	
migration completed 

if (totalQueries == 0) {	migrationresult.setNumberOfQueryTransfered(0);	migrationresult.setTotalNoQuery(0);	} else {	migrationresult.setNumberOfQueryTransfered(totalQueries);	migrationresult.setTotalNoQuery(totalQueries);	migrationresult.setProgressPercentage(100);	}	getResourceManager(view).update(migrationresult, jobid);	} catch (SQLException e) {	
sql exception in ambari database 

migrationresult.setNumberOfQueryTransfered(totalQueries);	migrationresult.setTotalNoQuery(totalQueries);	migrationresult.setProgressPercentage(100);	}	getResourceManager(view).update(migrationresult, jobid);	} catch (SQLException e) {	migrationresult.setError("SQL Exception: " + e.getMessage());	try {	connectionAmbaridb.rollback();	model.setIfSuccess(false);	
sql statement are rolledback 

migrationresult.setTotalNoQuery(totalQueries);	migrationresult.setProgressPercentage(100);	}	getResourceManager(view).update(migrationresult, jobid);	} catch (SQLException e) {	migrationresult.setError("SQL Exception: " + e.getMessage());	try {	connectionAmbaridb.rollback();	model.setIfSuccess(false);	} catch (SQLException e1) {	
sql rollback exception in ambari database 

}	getResourceManager(view).update(migrationresult, jobid);	} catch (SQLException e) {	migrationresult.setError("SQL Exception: " + e.getMessage());	try {	connectionAmbaridb.rollback();	model.setIfSuccess(false);	} catch (SQLException e1) {	}	} catch (ClassNotFoundException e) {	
class not found 

} catch (SQLException e) {	migrationresult.setError("SQL Exception: " + e.getMessage());	try {	connectionAmbaridb.rollback();	model.setIfSuccess(false);	} catch (SQLException e1) {	}	} catch (ClassNotFoundException e) {	migrationresult.setError("Class Not Found: " + e.getMessage());	} catch (ParseException e) {	
parse exception 

try {	connectionAmbaridb.rollback();	model.setIfSuccess(false);	} catch (SQLException e1) {	}	} catch (ClassNotFoundException e) {	migrationresult.setError("Class Not Found: " + e.getMessage());	} catch (ParseException e) {	migrationresult.setError("Parse Exception: " + e.getMessage());	} catch (URISyntaxException e) {	
uri syntax exception 

model.setIfSuccess(false);	} catch (SQLException e1) {	}	} catch (ClassNotFoundException e) {	migrationresult.setError("Class Not Found: " + e.getMessage());	} catch (ParseException e) {	migrationresult.setError("Parse Exception: " + e.getMessage());	} catch (URISyntaxException e) {	migrationresult.setError("URI Syntax Exception: " + e.getMessage());	} catch (PropertyVetoException e) {	
propertyvetoexception 

} catch (ParseException e) {	migrationresult.setError("Parse Exception: " + e.getMessage());	} catch (URISyntaxException e) {	migrationresult.setError("URI Syntax Exception: " + e.getMessage());	} catch (PropertyVetoException e) {	migrationresult.setError("Property Veto Exception: " + e.getMessage());	} catch (ItemNotFound itemNotFound) {	itemNotFound.printStackTrace();	migrationresult.setError("Item Not Found: " + itemNotFound.getMessage());	} catch (Exception e) {	
generic exception 

migrationresult.setError("Property Veto Exception: " + e.getMessage());	} catch (ItemNotFound itemNotFound) {	itemNotFound.printStackTrace();	migrationresult.setError("Item Not Found: " + itemNotFound.getMessage());	} catch (Exception e) {	migrationresult.setError("Exception: " + e.getMessage());	} finally {	if (connectionAmbaridb != null) try {	connectionAmbaridb.close();	} catch (SQLException e) {	
exception in closing the connection 

if (connectionAmbaridb != null) try {	connectionAmbaridb.close();	} catch (SQLException e) {	migrationresult.setError("Exception in closing the connection: " + e.getMessage());	}	getResourceManager(view).update(migrationresult, jobid);	}	hiveHistoryQueryImpl.deleteFileQueryhql(ConfigurationCheckImplementation.getHomeDir());	hiveHistoryQueryImpl.deleteFileQueryLogs(ConfigurationCheckImplementation.getHomeDir());	logger.info("------------------------------");	
hive history query migration ends 

========================= ambari sample_1284 =========================

public Boolean apply(@Nullable Path path) {	
finished processing the view definition for 

public void start() {	try {	Path path = buildWatchService();	Runnable task = startWatching(path);	watchTask = executorService.submit(task);	} catch (Exception e) {	
there were errors in starting the view directory watcher this task will not run 

private Runnable startWatching(final Path path) {	return new Runnable() {	public void run() {	try {	while (true) {	WatchKey key = watchService.take();	
watcher key was signalled 

private Runnable startWatching(final Path path) {	return new Runnable() {	public void run() {	try {	while (true) {	WatchKey key = watchService.take();	for (WatchEvent<?> event : key.pollEvents()) {	
watcher recieved poll event 

private Runnable startWatching(final Path path) {	return new Runnable() {	public void run() {	try {	while (true) {	WatchKey key = watchService.take();	for (WatchEvent<?> event : key.pollEvents()) {	WatchEvent<Path> ev = cast(event);	Path resolvedPath = path.resolve(ev.context());	
event s s 

private Runnable startWatching(final Path path) {	return new Runnable() {	public void run() {	try {	while (true) {	WatchKey key = watchService.take();	for (WatchEvent<?> event : key.pollEvents()) {	WatchEvent<Path> ev = cast(event);	Path resolvedPath = path.resolve(ev.context());	if (!canBlockTillFileAvailable(resolvedPath)) {	
watcher detected that the file was either empty or corrupt 

try {	while (true) {	WatchKey key = watchService.take();	for (WatchEvent<?> event : key.pollEvents()) {	WatchEvent<Path> ev = cast(event);	Path resolvedPath = path.resolve(ev.context());	if (!canBlockTillFileAvailable(resolvedPath)) {	continue;	}	if (!verify(resolvedPath)) {	
the uploaded file was empty not a regular file or not a valid jar archive file 

for (WatchEvent<?> event : key.pollEvents()) {	WatchEvent<Path> ev = cast(event);	Path resolvedPath = path.resolve(ev.context());	if (!canBlockTillFileAvailable(resolvedPath)) {	continue;	}	if (!verify(resolvedPath)) {	continue;	}	try {	
starting view extraction 

}	if (!verify(resolvedPath)) {	continue;	}	try {	viewRegistry.readViewArchive(resolvedPath);	for (Function<Path, Boolean> hook : hooks) {	hook.apply(resolvedPath);	}	} catch (Exception e) {	
cannot read the view archive offending file 

}	try {	viewRegistry.readViewArchive(resolvedPath);	for (Function<Path, Boolean> hook : hooks) {	hook.apply(resolvedPath);	}	} catch (Exception e) {	}	}	if (!key.reset()) {	
the watch key could not be reset directory watcher will not run anymore 

hook.apply(resolvedPath);	}	} catch (Exception e) {	}	}	if (!key.reset()) {	break;	}	}	} catch (InterruptedException x) {	
cancelling the directory watcher 

long emptyCheck = 0;	int fixed = 0;	File file = resolvedPath.toAbsolutePath().toFile();	while (file.length() == 0 && emptyCheck < 5) {	sleep(FILE_CHECK_INTERVAL_MILLIS);	emptyCheck++;	}	if (emptyCheck == 5) return false;	oldLength = file.length();	while (true) {	
waiting for file to be completely copied 

newSize = file.length();	if (newSize > oldLength) {	oldLength = newSize;	continue;	} else if (oldLength == newSize) {	fixed++;	} else {	return false;	}	if (fixed > FIXED_FILE_COUNTER) {	
file has finished copying 

private boolean verify(Path resolvedPath) {	ZipFile zipFile = null;	try {	File file = resolvedPath.toAbsolutePath().toFile();	checkArgument(!file.isDirectory());	checkArgument(file.length() > 0);	zipFile = new ZipFile(file);	} catch (Exception e) {	
verification failed 

========================= ambari sample_3939 =========================

synchronized (CACHED_CIPHER_MAX_KEY_LENGTHS) {	if (CACHED_CIPHER_MAX_KEY_LENGTHS.isEmpty()) {	for (Provider provider : Security.getProviders()) {	String providerName = provider.getName();	for (Provider.Service service : provider.getServices()) {	String algorithmName = service.getAlgorithm();	if ("Cipher".equalsIgnoreCase(service.getType())) {	try {	CACHED_CIPHER_MAX_KEY_LENGTHS.put(String.format("%s.%s", providerName, algorithmName).toLowerCase(), Cipher.getMaxAllowedKeyLength(algorithmName));	} catch (NoSuchAlgorithmException e) {	
failed to get the max key length of cipher s skipping 

========================= ambari sample_3485 =========================

public static void main(String[] args) throws Exception {	Injector injector = Guice.createInjector(new ControllerModule());	injector.getInstance(GuiceJpaInitializer.class);	AmbariServer ambariServer = null;	try {	
getting the controller 

AmbariServer ambariServer = null;	try {	ambariServer = injector.getInstance(AmbariServer.class);	ComponentSSLConfiguration.instance().init(ambariServer.configuration);	SinkConnectionFactory.instance().init(ambariServer.configuration);	ClusterDefinitionProvider.instance().init(ambariServer.configuration);	if (ambariServer != null) {	ambariServer.run();	}	} catch (Throwable t) {	
failed to run the ambari server 

ServletHolder sh = new ServletHolder(ServletContainer.class);	sh.setInitParameter("com.sun.jersey.config.property.resourceConfigClass", "com.sun.jersey.api.core.PackagesResourceConfig");	sh.setInitParameter("com.sun.jersey.config.property.packages", "org.apache.ambari.server.api.rest;" + "org.apache.ambari.server.api.services;" + "org.apache.ambari.eventdb.webservice;" + "org.apache.ambari.server.api");	sh.setInitParameter("com.sun.jersey.api.json.POJOMappingFeature", "true");	root.addServlet(sh, "/api/v1/*");	sh.setInitOrder(2);	server.setThreadPool(new QueuedThreadPool(25));	SelectChannelConnector apiConnector;	if (configuration.getApiSSLAuthentication()) {	String httpsKeystore = configsMap.get(Configuration.CLIENT_API_SSL_KSTR_DIR_NAME_KEY) + File.separator + configsMap.get(Configuration.CLIENT_API_SSL_KSTR_NAME_KEY);	
api ssl authentication is turned on keystore 

apiConnector.setMaxIdleTime(configuration.getConnectionMaxIdleTime());	}	server.addConnector(apiConnector);	server.setStopAtShutdown(true);	springAppContext.start();	String osType = configuration.getServerOsType();	if (osType == null || osType.isEmpty()) {	throw new RuntimeException(Configuration.OS_VERSION_KEY + " is not " + " set in the ambari.properties file");	}	server.start();	
started server 

}	server.addConnector(apiConnector);	server.setStopAtShutdown(true);	springAppContext.start();	String osType = configuration.getServerOsType();	if (osType == null || osType.isEmpty()) {	throw new RuntimeException(Configuration.OS_VERSION_KEY + " is not " + " set in the ambari.properties file");	}	server.start();	server.join();	
joined the server 

server.addConnector(apiConnector);	server.setStopAtShutdown(true);	springAppContext.start();	String osType = configuration.getServerOsType();	if (osType == null || osType.isEmpty()) {	throw new RuntimeException(Configuration.OS_VERSION_KEY + " is not " + " set in the ambari.properties file");	}	server.start();	server.join();	} catch (BadPaddingException bpe) {	
bad keystore or private key password https certificate re importing may be required 

springAppContext.start();	String osType = configuration.getServerOsType();	if (osType == null || osType.isEmpty()) {	throw new RuntimeException(Configuration.OS_VERSION_KEY + " is not " + " set in the ambari.properties file");	}	server.start();	server.join();	} catch (BadPaddingException bpe) {	throw bpe;	} catch (BindException bindException) {	
could not bind to server port instance may already be running terminating this instance 

private void stop() throws Exception {	try {	server.stop();	} catch (Exception e) {	
error stopping the server 

========================= ambari sample_475 =========================

public CommandReport execute( ConcurrentMap<String, Object> requestSharedDataContext) throws AmbariException, InterruptedException {	
create and configure 

RepositoryVersionEntity sourceRepoVersion = upgradeContext.getSourceRepositoryVersion(serviceName);	RepositoryVersionEntity targetRepoVersion = upgradeContext.getTargetRepositoryVersion(serviceName);	StackId sourceStackId = sourceRepoVersion.getStackId();	StackId targetStackId = targetRepoVersion.getStackId();	if (!sourceStackId.equals(targetStackId)){	return createCommandReport(0, HostRoleStatus.FAILED, "{}", "", "Unable to change configuration values across stacks. Use regular config task type instead.");	}	Map<String, DesiredConfig> desiredConfigs = cluster.getDesiredConfigs();	DesiredConfig desiredConfig = desiredConfigs.get(configType);	if (desiredConfig == null) {	
could not find desired config type with name s create it with default values 

========================= ambari sample_3368 =========================

public void commitMetricsFromCache() {	
clearing metrics cache 

public void commitMetrics(Collection<TimelineMetrics> timelineMetricsCollection) {	
committing metrics to store 

metricRecordStmt.setString(7, metric.getUnits());	metricRecordStmt.setDouble(8, aggregates[0]);	metricRecordStmt.setDouble(9, aggregates[1]);	metricRecordStmt.setDouble(10, aggregates[2]);	metricRecordStmt.setLong(11, (long) aggregates[3]);	String json = TimelineUtils.dumpTimelineRecordtoJSON(metric.getMetricValues());	metricRecordStmt.setString(12, json);	try {	metricRecordStmt.executeUpdate();	} catch (SQLException sql) {	
failed on insert records to store 

private Connection getConnectionRetryingOnException() throws SQLException, InterruptedException {	RetryCounter retryCounter = retryCounterFactory.create();	while (true) {	try{	return getConnection();	} catch (SQLException e) {	if(!retryCounter.shouldRetry()){	
hbaseaccessor getconnection failed after attempts 

protected void initMetricSchema() {	Connection conn = null;	Statement stmt = null;	String encoding = metricsConf.get(HBASE_ENCODING_SCHEME, DEFAULT_ENCODING);	String compression = metricsConf.get(HBASE_COMPRESSION_SCHEME, DEFAULT_TABLE_COMPRESSION);	try {	
initializing metrics schema 

String aggregateSql = String.format(CREATE_METRICS_CLUSTER_AGGREGATE_TABLE_SQL, METRICS_CLUSTER_AGGREGATE_TABLE_NAME, encoding, tableTTL.get(METRICS_CLUSTER_AGGREGATE_TABLE_NAME), compression);	splitPoints = metricsConf.get(AGGREGATE_TABLE_SPLIT_POINTS);	if (!StringUtils.isEmpty(splitPoints)) {	aggregateSql += getSplitPointsStr(splitPoints);	}	stmt.executeUpdate(aggregateSql);	stmt.executeUpdate(String.format(CREATE_METRICS_CLUSTER_AGGREGATE_GROUPED_TABLE_SQL, METRICS_CLUSTER_AGGREGATE_MINUTE_TABLE_NAME, encoding, tableTTL.get(METRICS_CLUSTER_AGGREGATE_MINUTE_TABLE_NAME), compression));	stmt.executeUpdate(String.format(CREATE_METRICS_CLUSTER_AGGREGATE_GROUPED_TABLE_SQL, METRICS_CLUSTER_AGGREGATE_HOURLY_TABLE_NAME, encoding, tableTTL.get(METRICS_CLUSTER_AGGREGATE_HOURLY_TABLE_NAME), compression));	stmt.executeUpdate(String.format(CREATE_METRICS_CLUSTER_AGGREGATE_GROUPED_TABLE_SQL, METRICS_CLUSTER_AGGREGATE_DAILY_TABLE_NAME, encoding, tableTTL.get(METRICS_CLUSTER_AGGREGATE_DAILY_TABLE_NAME), compression));	conn.commit();	
metrics schema initialized 

splitPoints = metricsConf.get(AGGREGATE_TABLE_SPLIT_POINTS);	if (!StringUtils.isEmpty(splitPoints)) {	aggregateSql += getSplitPointsStr(splitPoints);	}	stmt.executeUpdate(aggregateSql);	stmt.executeUpdate(String.format(CREATE_METRICS_CLUSTER_AGGREGATE_GROUPED_TABLE_SQL, METRICS_CLUSTER_AGGREGATE_MINUTE_TABLE_NAME, encoding, tableTTL.get(METRICS_CLUSTER_AGGREGATE_MINUTE_TABLE_NAME), compression));	stmt.executeUpdate(String.format(CREATE_METRICS_CLUSTER_AGGREGATE_GROUPED_TABLE_SQL, METRICS_CLUSTER_AGGREGATE_HOURLY_TABLE_NAME, encoding, tableTTL.get(METRICS_CLUSTER_AGGREGATE_HOURLY_TABLE_NAME), compression));	stmt.executeUpdate(String.format(CREATE_METRICS_CLUSTER_AGGREGATE_GROUPED_TABLE_SQL, METRICS_CLUSTER_AGGREGATE_DAILY_TABLE_NAME, encoding, tableTTL.get(METRICS_CLUSTER_AGGREGATE_DAILY_TABLE_NAME), compression));	conn.commit();	} catch (SQLException | InterruptedException sql) {	
error creating metrics schema in hbase using phoenix 

protected void initPoliciesAndTTL() {	HBaseAdmin hBaseAdmin = null;	try {	hBaseAdmin = dataSource.getHBaseAdmin();	} catch (IOException e) {	
unable to initialize hbaseadmin for setting policies 

} catch (IOException e) {	}	if (hBaseAdmin != null) {	for (String tableName : PHOENIX_TABLES) {	try {	boolean modifyTable = false;	HTableDescriptor tableDescriptor = hBaseAdmin.getTableDescriptor(tableName.getBytes());	boolean enableNormalizer = hbaseConf.getBoolean("hbase.normalizer.enabled", false);	if (enableNormalizer ^ tableDescriptor.isNormalizationEnabled()) {	tableDescriptor.setNormalizationEnabled(enableNormalizer);	
normalizer set to for 

modifyTable = modifyTable || durabilitySettingsModified;	boolean compactionPolicyModified = false;	compactionPolicyModified = setCompactionPolicyForTable(tableName, tableDescriptor);	modifyTable = modifyTable || compactionPolicyModified;	HColumnDescriptor[] columnFamilies = tableDescriptor.getColumnFamilies();	if (columnFamilies != null) {	for (HColumnDescriptor family : columnFamilies) {	String ttlValue = family.getValue("TTL");	if (StringUtils.isEmpty(ttlValue) || !ttlValue.trim().equals(tableTTL.get(tableName))) {	family.setValue("TTL", tableTTL.get(tableName));	
setting ttl on table to seconds 

if (StringUtils.isEmpty(ttlValue) || !ttlValue.trim().equals(tableTTL.get(tableName))) {	family.setValue("TTL", tableTTL.get(tableName));	modifyTable = true;	}	}	}	if (modifyTable) {	hBaseAdmin.modifyTable(tableName.getBytes(), tableDescriptor);	}	} catch (IOException e) {	
failed setting policies for 

}	if (modifyTable) {	hBaseAdmin.modifyTable(tableName.getBytes(), tableDescriptor);	}	} catch (IOException e) {	}	}	try {	hBaseAdmin.close();	} catch (IOException e) {	
exception on hbaseadmin close 

private boolean setDurabilityForTable(String tableName, HTableDescriptor tableDescriptor) {	boolean modifyTable = false;	if (METRICS_RECORD_TABLE_NAME.equals(tableName)) {	if (!timelineMetricsPrecisionTableDurability.isEmpty()) {	
setting wal option for table 

boolean validDurability = true;	if ("SKIP_WAL".equals(timelineMetricsPrecisionTableDurability)) {	tableDescriptor.setDurability(Durability.SKIP_WAL);	} else if ("SYNC_WAL".equals(timelineMetricsPrecisionTableDurability)) {	tableDescriptor.setDurability(Durability.SYNC_WAL);	} else if ("ASYNC_WAL".equals(timelineMetricsPrecisionTableDurability)) {	tableDescriptor.setDurability(Durability.ASYNC_WAL);	} else if ("FSYNC_WAL".equals(timelineMetricsPrecisionTableDurability)) {	tableDescriptor.setDurability(Durability.FSYNC_WAL);	} else {	
unknown value for 

tableDescriptor.setDurability(Durability.FSYNC_WAL);	} else {	validDurability = false;	}	if (validDurability) {	modifyTable = true;	}	}	} else {	if (!timelineMetricsTablesDurability.isEmpty()) {	
setting wal option for table 

boolean validDurability = true;	if ("SKIP_WAL".equals(timelineMetricsTablesDurability)) {	tableDescriptor.setDurability(Durability.SKIP_WAL);	} else if ("SYNC_WAL".equals(timelineMetricsTablesDurability)) {	tableDescriptor.setDurability(Durability.SYNC_WAL);	} else if ("ASYNC_WAL".equals(timelineMetricsTablesDurability)) {	tableDescriptor.setDurability(Durability.ASYNC_WAL);	} else if ("FSYNC_WAL".equals(timelineMetricsTablesDurability)) {	tableDescriptor.setDurability(Durability.FSYNC_WAL);	} else {	
unknown value for 

metricRecordStmt.setDouble(16, (double) metric.getPmem50Pct() / 1024);	metricRecordStmt.setDouble(17, (double) metric.getPmem75Pct() / 1024);	metricRecordStmt.setDouble(18, (double) metric.getPmem90Pct() / 1024);	metricRecordStmt.setDouble(19, (double) metric.getPmem95Pct()/ 1024);	metricRecordStmt.setDouble(20, (double) metric.getPmem99Pct() / 1024);	metricRecordStmt.setDouble(21, (double) metric.getPmemLimit() / 1024 - (double) metric.getPmemUsedMax() / 1024);	metricRecordStmt.setDouble(22, ((double) metric.getPmemLimit() / 1024 - (double) metric.getPmemUsedMax() / 1024) * (metric.getFinishTime() - metric.getStartTime()));	try {	metricRecordStmt.executeUpdate();	} catch (SQLException sql) {	
failed on insert records to store 

public void insertMetricRecordsWithMetadata(TimelineMetricMetadataManager metadataManager, TimelineMetrics metrics, boolean skipCache) throws SQLException, IOException {	List<TimelineMetric> timelineMetrics = metrics.getMetrics();	if (timelineMetrics == null || timelineMetrics.isEmpty()) {	
empty metrics insert request 

metadataManager.putIfModifiedHostedAppsMetadata( tm.getHostName(), tm.getAppId());	if (!tm.getAppId().equals("FLUME_HANDLER")) {	metadataManager.putIfModifiedHostedInstanceMetadata(tm.getInstanceId(), tm.getHostName());	}	}	if (!acceptMetric) {	iterator.remove();	}	}	if  (!skipCache && cacheEnabled) {	
adding metrics to cache 

if  (!skipCache && cacheEnabled) {	if (insertCache.size() >= cacheSize) {	commitMetricsFromCache();	}	try {	insertCache.put(metrics);	} catch (InterruptedException e) {	e.printStackTrace();	}	} else {	
skipping metrics cache 

if (condition.isPointInTime()){	getLatestMetricRecords(condition, conn, metrics);	} else {	if (condition.getEndTime() >= condition.getStartTime()) {	stmt = PhoenixTransactSQL.prepareGetMetricsSqlStmt(conn, condition);	rs = stmt.executeQuery();	while (rs.next()) {	appendMetricFromResultSet(metrics, condition, metricFunctions, rs);	}	} else {	
skipping metrics query because endtime starttime 

}	}	} catch (PhoenixIOException pioe) {	Throwable pioe2 = pioe.getCause();	if (pioe2 instanceof PhoenixIOException && pioe2.getCause() instanceof DoNotRetryIOException) {	String className = null;	for (StackTraceElement ste : pioe2.getCause().getStackTrace()) {	className = ste.getClassName();	}	if (className != null && className.equals("HashJoinRegionScanner")) {	
the cache might have expired and have been removed try to increase the cache size by setting bigger value for phoenix coprocessor maxmetadatacachesize in ams hbase site config falling back to sort merge join algorithm 

} catch (SQLException e) {	}	}	if (conn != null) {	try {	conn.close();	} catch (SQLException sql) {	}	}	}	
metrics records size 

} catch (SQLException e) {	}	}	if (conn != null) {	try {	conn.close();	} catch (SQLException sql) {	}	}	}	
aggregate records size 

public void saveHostAggregateRecords(Map<TimelineMetric, MetricHostAggregate> hostAggregateMap, String phoenixTableName) throws SQLException {	if (hostAggregateMap == null || hostAggregateMap.isEmpty()) {	
empty aggregate records 

}	}	long end = System.currentTimeMillis();	if ((end - start) > 60000l) {	LOG.info("Time to save map: " + (end - start) + ", " + "thread = " + Thread.currentThread().getClass());	}	if (aggregatorSink != null) {	try {	aggregatorSink.saveHostAggregateRecords(hostAggregateMap, getTablePrecision(phoenixTableName));	} catch (Exception e) {	
error writing host aggregate records metrics to external sink 

public void saveClusterAggregateRecords(Map<TimelineClusterMetric, MetricClusterAggregate> records) throws SQLException {	if (records == null || records.isEmpty()) {	
empty aggregate records 

}	}	long end = System.currentTimeMillis();	if ((end - start) > 60000l) {	LOG.info("Time to save: " + (end - start) + ", " + "thread = " + Thread.currentThread().getName());	}	if (aggregatorSink != null) {	try {	aggregatorSink.saveClusterAggregateRecords(records);	} catch (Exception e) {	
error writing cluster aggregate records metrics to external sink 

public void saveClusterTimeAggregateRecords(Map<TimelineClusterMetric, MetricHostAggregate> records, String tableName) throws SQLException {	if (records == null || records.isEmpty()) {	
empty aggregate records 

}	}	long end = System.currentTimeMillis();	if ((end - start) > 60000l) {	LOG.info("Time to save: " + (end - start) + ", " + "thread = " + Thread.currentThread().getName());	}	if (aggregatorSink != null) {	try {	aggregatorSink.saveClusterTimeAggregateRecords(records, getTablePrecision(tableName));	} catch (Exception e) {	
error writing cluster time aggregate records metrics to external sink 

public void saveHostAppsMetadata(Map<String, Set<String>> hostedApps) throws SQLException {	Connection conn = getConnection();	PreparedStatement stmt = null;	try {	stmt = conn.prepareStatement(UPSERT_HOSTED_APPS_METADATA_SQL);	int rowCount = 0;	for (Map.Entry<String, Set<String>> hostedAppsEntry : hostedApps.entrySet()) {	if (LOG.isTraceEnabled()) {	
hostedappsmetadata 

for (Map.Entry<String, Set<String>> hostedAppsEntry : hostedApps.entrySet()) {	if (LOG.isTraceEnabled()) {	}	stmt.clearParameters();	stmt.setString(1, hostedAppsEntry.getKey());	stmt.setString(2, StringUtils.join(hostedAppsEntry.getValue(), ","));	try {	stmt.executeUpdate();	rowCount++;	} catch (SQLException sql) {	
error saving hosted apps metadata 

stmt.clearParameters();	stmt.setString(1, hostedAppsEntry.getKey());	stmt.setString(2, StringUtils.join(hostedAppsEntry.getValue(), ","));	try {	stmt.executeUpdate();	rowCount++;	} catch (SQLException sql) {	}	}	conn.commit();	
saved hosted apps metadata records 

public void saveInstanceHostsMetadata(Map<String, Set<String>> instanceHostsMap) throws SQLException {	Connection conn = getConnection();	PreparedStatement stmt = null;	try {	stmt = conn.prepareStatement(UPSERT_INSTANCE_HOST_METADATA_SQL);	int rowCount = 0;	for (Map.Entry<String, Set<String>> hostInstancesEntry : instanceHostsMap.entrySet()) {	if (LOG.isTraceEnabled()) {	
host instances entry 

}	String instanceId = hostInstancesEntry.getKey();	for(String hostname : hostInstancesEntry.getValue()) {	stmt.clearParameters();	stmt.setString(1, instanceId);	stmt.setString(2, hostname);	try {	stmt.executeUpdate();	rowCount++;	} catch (SQLException sql) {	
error saving host instances metadata 

stmt.setString(1, instanceId);	stmt.setString(2, hostname);	try {	stmt.executeUpdate();	rowCount++;	} catch (SQLException sql) {	}	}	}	conn.commit();	
saved host instances metadata records 

public void saveMetricMetadata(Collection<TimelineMetricMetadata> metricMetadata) throws SQLException {	if (metricMetadata.isEmpty()) {	
no metadata records to save 

stmt.setString(2, metadata.getAppId());	stmt.setString(3, metadata.getUnits());	stmt.setString(4, metadata.getType());	stmt.setLong(5, metadata.getSeriesStartTime());	stmt.setBoolean(6, metadata.isSupportsAggregates());	stmt.setBoolean(7, metadata.isWhitelisted());	try {	stmt.executeUpdate();	rowCount++;	} catch (SQLException sql) {	
error saving metadata 

stmt.setLong(5, metadata.getSeriesStartTime());	stmt.setBoolean(6, metadata.isSupportsAggregates());	stmt.setBoolean(7, metadata.isWhitelisted());	try {	stmt.executeUpdate();	rowCount++;	} catch (SQLException sql) {	}	}	conn.commit();	
saved metadata records 

========================= ambari sample_377 =========================

protected void configure() {	Multibinder<Cleanable> multiBinder = Multibinder.newSetBinder(binder(), Cleanable.class);	Set<Class<?>> bindingSet = ClasspathScannerUtils.findOnClassPath(getPackageToScan(), getExclusions(), getSelectors());	for (Class clazz : bindingSet) {	
binding cleaner 

========================= ambari sample_3807 =========================

public static LdapUsernameCollisionHandlingBehavior translate(String value, LdapUsernameCollisionHandlingBehavior defaultValue) {	String processedValue = StringUtils.upperCase(StringUtils.trim(value));	if (StringUtils.isEmpty(processedValue)) {	return defaultValue;	} else {	try {	return valueOf(processedValue);	} catch (IllegalArgumentException e) {	
invalid ldap username collision value using the default value 

configsMap.put(EXTERNAL_SCRIPT_TIMEOUT.getKey(), getProperty(EXTERNAL_SCRIPT_TIMEOUT));	configsMap.put(THREAD_POOL_SIZE_FOR_EXTERNAL_SCRIPT.getKey(), getProperty(THREAD_POOL_SIZE_FOR_EXTERNAL_SCRIPT));	configsMap.put(SHARED_RESOURCES_DIR.getKey(), getProperty(SHARED_RESOURCES_DIR));	configsMap.put(KDC_PORT.getKey(), getProperty(KDC_PORT));	configsMap.put(AGENT_PACKAGE_PARALLEL_COMMANDS_LIMIT.getKey(), getProperty(AGENT_PACKAGE_PARALLEL_COMMANDS_LIMIT));	configsMap.put(PROXY_ALLOWED_HOST_PORTS.getKey(), getProperty(PROXY_ALLOWED_HOST_PORTS));	configsMap.put(TLS_EPHEMERAL_DH_KEY_SIZE.getKey(), getProperty(TLS_EPHEMERAL_DH_KEY_SIZE));	File passFile = new File( configsMap.get(SRVR_KSTR_DIR.getKey()) + File.separator + configsMap.get(SRVR_CRT_PASS_FILE.getKey()));	String password = null;	if (!passFile.exists()) {	
generation of file with password 

if (!passFile.exists()) {	try {	password = RandomStringUtils.randomAlphanumeric(Integer .parseInt(configsMap.get(SRVR_CRT_PASS_LEN.getKey())));	FileUtils.writeStringToFile(passFile, password);	ShellCommandUtil.setUnixFilePermissions( ShellCommandUtil.MASK_OWNER_ONLY_RW, passFile.getAbsolutePath());	} catch (IOException e) {	e.printStackTrace();	throw new RuntimeException( "Error reading certificate password from file");	}	} else {	
reading password from existing file 

} else {	try {	password = FileUtils.readFileToString(passFile);	password = password.replaceAll("\\p{Cntrl}", "");	} catch (IOException e) {	e.printStackTrace();	}	}	configsMap.put(SRVR_CRT_PASS.getKey(), password);	if (getApiSSLAuthentication()) {	
api ssl authentication is turned on 

password = FileUtils.readFileToString(passFile);	password = password.replaceAll("\\p{Cntrl}", "");	} catch (IOException e) {	e.printStackTrace();	}	}	configsMap.put(SRVR_CRT_PASS.getKey(), password);	if (getApiSSLAuthentication()) {	File httpsPassFile = new File(configsMap.get(CLIENT_API_SSL_KSTR_DIR_NAME.getKey()) + File.separator + configsMap.get(CLIENT_API_SSL_CRT_PASS_FILE_NAME.getKey()));	if (httpsPassFile.exists()) {	
reading password from existing file 

File httpsPassFile = new File(configsMap.get(CLIENT_API_SSL_KSTR_DIR_NAME.getKey()) + File.separator + configsMap.get(CLIENT_API_SSL_CRT_PASS_FILE_NAME.getKey()));	if (httpsPassFile.exists()) {	try {	password = FileUtils.readFileToString(httpsPassFile);	password = password.replaceAll("\\p{Cntrl}", "");	} catch (IOException e) {	e.printStackTrace();	throw new RuntimeException("Error reading certificate password from" + " file " + httpsPassFile.getAbsolutePath());	}	} else {	
there is no keystore for https ui connection 

private static Properties readConfigFile() {	Properties properties = new Properties();	InputStream inputStream = Configuration.class.getClassLoader().getResourceAsStream(CONFIG_FILE);	if (inputStream == null) {	throw new RuntimeException(CONFIG_FILE + " not found in classpath");	}	try {	properties.load(new InputStreamReader(inputStream, Charsets.UTF_8));	inputStream.close();	} catch (FileNotFoundException fnf) {	
no configuration file found in classpath 

private void writeConfigFile(Properties propertiesToStore, boolean append) throws AmbariException {	File configFile = null;	try {	configFile = new File(Configuration.class.getClassLoader().getResource(Configuration.CONFIG_FILE).getPath());	propertiesToStore.store(new OutputStreamWriter(new FileOutputStream(configFile, append), Charsets.UTF_8), null);	} catch (Exception e) {	
cannot write properties into configuration file 

public Map<String, String> getDatabaseConnectorNames() {	File file = new File(Configuration.class.getClassLoader().getResource(CONFIG_FILE).getPath());	Long currentConfigLastModifiedDate = file.lastModified();	Properties properties = null;	if (currentConfigLastModifiedDate.longValue() != configLastModifiedDateForCustomJDBC.longValue()) {	
ambari properties config file changed 

public Map<String, String> getPreviousDatabaseConnectorNames() {	File file = new File(Configuration.class.getClassLoader().getResource(CONFIG_FILE).getPath());	Long currentConfigLastModifiedDate = file.lastModified();	Properties properties = null;	if (currentConfigLastModifiedDate.longValue() != configLastModifiedDateForCustomJDBCToRemove.longValue()) {	
ambari properties config file changed 

public String getHostsMapFile() {	
hosts mapping file 

public String getServerVersion() {	try {	return FileUtils.readFileToString(new File(getServerVersionFilePath())).trim();	} catch (IOException e) {	
unable to read server version file 

String passwdProp = properties.getProperty(SERVER_JDBC_USER_PASSWD.getKey());	String dbpasswd = null;	boolean isPasswordAlias = false;	if (CredentialProvider.isAliasString(passwdProp)) {	dbpasswd = PasswordUtils.getInstance().readPasswordFromStore(passwdProp, getMasterKeyLocation(), isMasterKeyPersisted(), getMasterKeyStoreLocation());	isPasswordAlias =true;	}	if (dbpasswd != null) {	return dbpasswd;	} else if (dbpasswd == null && isPasswordAlias) {	
can t read db password from keystore please check master key was set correctly 

public File getMasterKeyLocation() {	File location;	String path = getProperty(MASTER_KEY_LOCATION);	if (StringUtils.isEmpty(path)) {	location = new File(getServerKeyStoreDirectory(), MASTER_KEY_FILENAME_DEFAULT);	
value of is not set using 

public File getMasterKeyLocation() {	File location;	String path = getProperty(MASTER_KEY_LOCATION);	if (StringUtils.isEmpty(path)) {	location = new File(getServerKeyStoreDirectory(), MASTER_KEY_FILENAME_DEFAULT);	} else {	location = new File(path, MASTER_KEY_FILENAME_DEFAULT);	
value of is 

public File getMasterKeyStoreLocation() {	File location;	String path = getProperty(MASTER_KEYSTORE_LOCATION);	if (StringUtils.isEmpty(path)) {	location = new File(getServerKeyStoreDirectory(), MASTER_KEYSTORE_FILENAME_DEFAULT);	
value of is not set using 

public File getMasterKeyStoreLocation() {	File location;	String path = getProperty(MASTER_KEYSTORE_LOCATION);	if (StringUtils.isEmpty(path)) {	location = new File(getServerKeyStoreDirectory(), MASTER_KEYSTORE_FILENAME_DEFAULT);	} else {	location = new File(path, MASTER_KEYSTORE_FILENAME_DEFAULT);	
value of is 

public long getTemporaryKeyStoreRetentionMinutes() {	long minutes;	String value = getProperty(TEMPORARYSTORE_RETENTION_MINUTES);	if(StringUtils.isEmpty(value)) {	
value of is not set using default value 

public long getTemporaryKeyStoreRetentionMinutes() {	long minutes;	String value = getProperty(TEMPORARYSTORE_RETENTION_MINUTES);	if(StringUtils.isEmpty(value)) {	minutes = TEMPORARYSTORE_RETENTION_MINUTES.getDefaultValue();	}	else {	try {	minutes = Long.parseLong(value);	
value of is 

public long getTemporaryKeyStoreRetentionMinutes() {	long minutes;	String value = getProperty(TEMPORARYSTORE_RETENTION_MINUTES);	if(StringUtils.isEmpty(value)) {	minutes = TEMPORARYSTORE_RETENTION_MINUTES.getDefaultValue();	}	else {	try {	minutes = Long.parseLong(value);	} catch (NumberFormatException e) {	
value of should be a number falling back to default value 

public boolean isActivelyPurgeTemporaryKeyStore() {	String value = getProperty(TEMPORARYSTORE_ACTIVELY_PURGE);	if (StringUtils.isEmpty(value)) {	
value of is not set using default value 

public boolean isActivelyPurgeTemporaryKeyStore() {	String value = getProperty(TEMPORARYSTORE_ACTIVELY_PURGE);	if (StringUtils.isEmpty(value)) {	return TEMPORARYSTORE_ACTIVELY_PURGE.getDefaultValue();	} else if ("true".equalsIgnoreCase(value)) {	
value of is 

public boolean isActivelyPurgeTemporaryKeyStore() {	String value = getProperty(TEMPORARYSTORE_ACTIVELY_PURGE);	if (StringUtils.isEmpty(value)) {	return TEMPORARYSTORE_ACTIVELY_PURGE.getDefaultValue();	} else if ("true".equalsIgnoreCase(value)) {	return true;	} else if ("false".equalsIgnoreCase(value)) {	
value of is 

public boolean isActivelyPurgeTemporaryKeyStore() {	String value = getProperty(TEMPORARYSTORE_ACTIVELY_PURGE);	if (StringUtils.isEmpty(value)) {	return TEMPORARYSTORE_ACTIVELY_PURGE.getDefaultValue();	} else if ("true".equalsIgnoreCase(value)) {	return true;	} else if ("false".equalsIgnoreCase(value)) {	return false;	} else {	
value of should be either or but is falling back to default value 

public Long getExecutionSchedulerWait() {	String stringValue = getProperty(EXECUTION_SCHEDULER_WAIT);	Long sleepTime = EXECUTION_SCHEDULER_WAIT.getDefaultValue();	if (stringValue != null) {	try {	sleepTime = Long.valueOf(stringValue);	} catch (NumberFormatException ignored) {	
value of should be a number falling back to default value 

public Long getExecutionSchedulerWait() {	String stringValue = getProperty(EXECUTION_SCHEDULER_WAIT);	Long sleepTime = EXECUTION_SCHEDULER_WAIT.getDefaultValue();	if (stringValue != null) {	try {	sleepTime = Long.valueOf(stringValue);	} catch (NumberFormatException ignored) {	}	}	if (sleepTime > 60) {	
value of should be a number between adn falling back to maximum value 

public String getDefaultAgentTaskTimeout(boolean isPackageInstallationTask) {	ConfigurationProperty<Long> configurationProperty = isPackageInstallationTask ? AGENT_PACKAGE_INSTALL_TASK_TIMEOUT : AGENT_TASK_TIMEOUT;	String key = configurationProperty.getKey();	Long defaultValue = configurationProperty.getDefaultValue();	String value = getProperty(configurationProperty);	if (StringUtils.isNumeric(value)) {	return value;	} else {	
value of s s should be a number falling back to default value s 

public Long getAgentServiceCheckTaskTimeout() {	String value = getProperty(AGENT_SERVICE_CHECK_TASK_TIMEOUT);	if (StringUtils.isNumeric(value)) {	return Long.parseLong(value);	} else {	
value of should be a number falling back to default value 

public Integer getDefaultServerTaskTimeout() {	String value = getProperty(SERVER_TASK_TIMEOUT);	if (StringUtils.isNumeric(value)) {	return Integer.parseInt(value);	} else {	
value of should be a number falling back to default value 

public JwtAuthenticationProperties getJwtProperties() {	boolean enableJwt = Boolean.valueOf(getProperty(JWT_AUTH_ENABLED));	if (enableJwt) {	String providerUrl = getProperty(JWT_AUTH_PROVIDER_URL);	if (providerUrl == null) {	
jwt authentication provider url not specified jwt auth will be disabled 

public JwtAuthenticationProperties getJwtProperties() {	boolean enableJwt = Boolean.valueOf(getProperty(JWT_AUTH_ENABLED));	if (enableJwt) {	String providerUrl = getProperty(JWT_AUTH_PROVIDER_URL);	if (providerUrl == null) {	return null;	}	String publicKeyPath = getProperty(JWT_PUBLIC);	if (publicKeyPath == null) {	
public key pem not specified for jwt auth provider jwt auth will be disabled 

try {	RSAPublicKey publicKey = CertificateUtils.getPublicKeyFromFile(publicKeyPath);	JwtAuthenticationProperties jwtProperties = new JwtAuthenticationProperties();	jwtProperties.setAuthenticationProviderUrl(providerUrl);	jwtProperties.setPublicKey(publicKey);	jwtProperties.setCookieName(getProperty(JWT_COOKIE_NAME));	jwtProperties.setAudiencesString(getProperty(JWT_AUDIENCES));	jwtProperties.setOriginalUrlQueryParam(getProperty(JWT_ORIGINAL_URL_QUERY_PARAM));	return jwtProperties;	} catch (IOException e) {	
unable to read public certificate file jwt auth will be disabled 

JwtAuthenticationProperties jwtProperties = new JwtAuthenticationProperties();	jwtProperties.setAuthenticationProviderUrl(providerUrl);	jwtProperties.setPublicKey(publicKey);	jwtProperties.setCookieName(getProperty(JWT_COOKIE_NAME));	jwtProperties.setAudiencesString(getProperty(JWT_AUDIENCES));	jwtProperties.setOriginalUrlQueryParam(getProperty(JWT_ORIGINAL_URL_QUERY_PARAM));	return jwtProperties;	} catch (IOException e) {	return null;	} catch (CertificateException e) {	
unable to parse public certificate file jwt auth will be disabled 

public int getOperationsRetryAttempts() {	final int RETRY_ATTEMPTS_LIMIT = 10;	String property = getProperty(OPERATIONS_RETRY_ATTEMPTS);	Integer attempts = Integer.valueOf(property);	if (attempts < 0) {	
invalid operations retry attempts number should be value reset to default 

public int getOperationsRetryAttempts() {	final int RETRY_ATTEMPTS_LIMIT = 10;	String property = getProperty(OPERATIONS_RETRY_ATTEMPTS);	Integer attempts = Integer.valueOf(property);	if (attempts < 0) {	attempts = OPERATIONS_RETRY_ATTEMPTS.getDefaultValue();	} else if (attempts > RETRY_ATTEMPTS_LIMIT) {	
invalid operations retry attempts number should be value set to 

public int getOperationsRetryAttempts() {	final int RETRY_ATTEMPTS_LIMIT = 10;	String property = getProperty(OPERATIONS_RETRY_ATTEMPTS);	Integer attempts = Integer.valueOf(property);	if (attempts < 0) {	attempts = OPERATIONS_RETRY_ATTEMPTS.getDefaultValue();	} else if (attempts > RETRY_ATTEMPTS_LIMIT) {	attempts = RETRY_ATTEMPTS_LIMIT;	}	if (attempts > 0) {	
operations retry enabled number of retry attempts 

inputStream = new FileInputStream(System.getProperties().getProperty(AMBARI_CONFIGURATION_MD_TEMPLATE_PROPERTY));	} else {	inputStream = Configuration.class.getResourceAsStream(MARKDOWN_TEMPLATE_FILE);	}	String template = IOUtils.toString(inputStream);	String markdown = template.replace(MARKDOWN_CONFIGURATION_TABLE_KEY, allPropertiesBuffer.toString());	markdown = markdown.replace(MARKDOWN_BASELINE_VALUES_KEY, baselineBuffer.toString());	File file = new File(outputFile);	FileUtils.writeStringToFile(file, markdown);	System.out.println("Successfully created " + outputFile);	
successfully created 

private AmbariKerberosAuthenticationProperties createKerberosAuthenticationProperties() {	AmbariKerberosAuthenticationProperties kerberosAuthProperties = new AmbariKerberosAuthenticationProperties();	kerberosAuthProperties.setKerberosAuthenticationEnabled(Boolean.valueOf(getProperty(KERBEROS_AUTH_ENABLED)));	if (!kerberosAuthProperties.isKerberosAuthenticationEnabled()) {	return kerberosAuthProperties;	}	String spnegoPrincipalName = getProperty(KERBEROS_AUTH_SPNEGO_PRINCIPAL);	if ((spnegoPrincipalName != null) && (spnegoPrincipalName.contains("_HOST"))) {	String hostName = StageUtils.getHostName();	if (StringUtils.isEmpty(hostName)) {	
cannot replace host in the configured spnego principal name with the host name this host since it is not available 

AmbariKerberosAuthenticationProperties kerberosAuthProperties = new AmbariKerberosAuthenticationProperties();	kerberosAuthProperties.setKerberosAuthenticationEnabled(Boolean.valueOf(getProperty(KERBEROS_AUTH_ENABLED)));	if (!kerberosAuthProperties.isKerberosAuthenticationEnabled()) {	return kerberosAuthProperties;	}	String spnegoPrincipalName = getProperty(KERBEROS_AUTH_SPNEGO_PRINCIPAL);	if ((spnegoPrincipalName != null) && (spnegoPrincipalName.contains("_HOST"))) {	String hostName = StageUtils.getHostName();	if (StringUtils.isEmpty(hostName)) {	} else {	
replacing host in the configured spnego principal name with the host name this host 

File keytabFile = new File(kerberosAuthProperties.getSpnegoKeytabFilePath());	if (!keytabFile.exists()) {	String message = String.format("The SPNEGO keytab file path (%s) specified in %s does not exist. " + "This will cause issues authenticating users using Kerberos. . Make sure proper keytab file provided later.", keytabFile.getAbsolutePath(), KERBEROS_AUTH_SPNEGO_KEYTAB_FILE.getKey());	LOG.error(message);	} else if (!keytabFile.canRead()) {	String message = String.format("The SPNEGO keytab file path (%s) specified in %s cannot be read. " + "This will cause issues authenticating users using Kerberos. . Make sure proper keytab file provided later.", keytabFile.getAbsolutePath(), KERBEROS_AUTH_SPNEGO_KEYTAB_FILE.getKey());	LOG.error(message);	}	}	kerberosAuthProperties.setAuthToLocalRules(getProperty(KERBEROS_AUTH_AUTH_TO_LOCAL_RULES));	
kerberos authentication is enabled t t t t 

========================= ambari sample_2715 =========================

Set<PropertyInfo> newStackProperties = m_ambariMetaInfo.get().getServiceProperties( targetStack.getStackName(), targetStack.getStackVersion(), serviceName);	addToMap(newMap, newStackProperties);	}	Set<PropertyInfo> set = m_ambariMetaInfo.get().getStackProperties( oldStack.getStackName(), oldStack.getStackVersion());	addToMap(oldMap, set);	set = m_ambariMetaInfo.get().getStackProperties( targetStack.getStackName(), targetStack.getStackVersion());	addToMap(newMap, set);	Map<String, Map<String, ThreeWayValue>> result = new HashMap<>();	for (Entry<String, Map<String, String>> entry : oldMap.entrySet()) {	if (!newMap.containsKey(entry.getKey())) {	
stack does not have an equivalent config type in 

========================= ambari sample_3041 =========================

for (String propertyId : requestPropertyIds) {	if (propertyId.startsWith(ZERO_PADDING_PARAM)) {	String paddingStrategyStr = propertyId.substring(ZERO_PADDING_PARAM.length() + 1);	metricsPaddingMethod = new MetricsPaddingMethod( MetricsPaddingMethod.PADDING_STRATEGY.valueOf(paddingStrategyStr));	}	}	}	String clusterName = (String) resource.getPropertyValue(clusterNamePropertyId);	if (!hostProvider.isCollectorHostLive(clusterName, TIMELINE_METRICS)) {	if (printSkipPopulateMsgHostCounter.getAndIncrement() == 0) {	
metrics collector host is not live skip populating resources with metrics next message will be logged after attempts 

if (!hostProvider.isCollectorHostLive(clusterName, TIMELINE_METRICS)) {	if (printSkipPopulateMsgHostCounter.getAndIncrement() == 0) {	} else {	printSkipPopulateMsgHostCounter.compareAndSet(1000, 0);	}	return true;	}	printSkipPopulateMsgHostCompCounter.set(0);	if (!hostProvider.isCollectorComponentLive(clusterName, TIMELINE_METRICS)) {	if (printSkipPopulateMsgHostCompCounter.getAndIncrement() == 0) {	
metrics collector is not live skip populating resources with metrics next message will be logged after attempts 

try {	if (metricCache != null && metricCacheKey.getTemporalInfo() != null) {	timelineMetrics = metricCache.getAppTimelineMetricsFromCache(metricCacheKey);	} else {	timelineMetrics = requestHelper.fetchTimelineMetrics(uriBuilder, temporalInfo.getStartTimeMillis(), temporalInfo.getEndTimeMillis());	}	} catch (IOException io) {	timelineMetrics = null;	if (io instanceof SocketTimeoutException || io instanceof ConnectException) {	if (LOG.isDebugEnabled()) {	
skip populating metrics on socket timeout exception 

========================= ambari sample_3411 =========================

metricList.add(cachedMetric);	}	}	}	if (!metricList.isEmpty()) {	TimelineMetrics timelineMetrics = new TimelineMetrics();	timelineMetrics.setMetrics(metricList);	try {	emitMetrics(timelineMetrics);	} catch (UnableToConnectException uce) {	
unable to send metrics to collector by address 

public void cleanup() {	
stopping storm metrics sink 

private List<DataPoint> populateDataPoints(DataPoint dataPoint) {	List<DataPoint> dataPoints = new ArrayList<>();	if (dataPoint.value == null) {	
data point with name is null discarding 

private Double convertValueToDouble(String metricName, Object value) {	if (value instanceof Number) {	return ((Number) value).doubleValue();	} else if (value instanceof String) {	try {	return Double.parseDouble((String) value);	} catch (NumberFormatException e) {	
data point with name doesn t have number format value discarding 

private Double convertValueToDouble(String metricName, Object value) {	if (value instanceof Number) {	return ((Number) value).doubleValue();	} else if (value instanceof String) {	try {	return Double.parseDouble((String) value);	} catch (NumberFormatException e) {	}	return null;	} else {	
data point with name has value which is not supported discarding 

========================= ambari sample_265 =========================

public static BufferedReader getReader(String s3Path, String accessKey, String secretKey) throws IOException {	String s3Bucket = getBucketName(s3Path);	String s3Key = getS3Key(s3Path);	S3Object fileObj = getS3Client(accessKey, secretKey).getObject(new GetObjectRequest(s3Bucket, s3Key));	try {	GZIPInputStream objectInputStream = new GZIPInputStream(fileObj.getObjectContent());	BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(objectInputStream));	return bufferedReader;	} catch (IOException e) {	
error in creating stream reader for file 

try {	in = IOUtils.toInputStream(data, "UTF-8");	} catch (IOException e) {	LOG.error(e);	}	if (in != null) {	TransferManager transferManager = getTransferManager(accessKey, secretKey);	try {	if (transferManager != null) {	transferManager.upload(new PutObjectRequest(bucketName, s3Key, in, new ObjectMetadata())).waitForUploadResult();	
data uploaded to file in bucket 

========================= ambari sample_1617 =========================

public RequestStatus createResources(final Request request) throws SystemException, UnsupportedPropertyException, ResourceAlreadyExistsException, NoSuchParentResourceException {	StackAdvisorRequest recommendationRequest = prepareStackAdvisorRequest(request);	final RecommendationResponse response;	try {	response = saHelper.recommend(recommendationRequest);	} catch (StackAdvisorRequestException e) {	
error occured during recommendation 

public RequestStatus createResources(final Request request) throws SystemException, UnsupportedPropertyException, ResourceAlreadyExistsException, NoSuchParentResourceException {	StackAdvisorRequest recommendationRequest = prepareStackAdvisorRequest(request);	final RecommendationResponse response;	try {	response = saHelper.recommend(recommendationRequest);	} catch (StackAdvisorRequestException e) {	throw new IllegalArgumentException(e.getMessage(), e);	} catch (StackAdvisorException e) {	
error occured during recommendation 

========================= ambari sample_3503 =========================

Cluster cluster = m_clusters.get().getCluster(clusterName);	StackId currentStack = cluster.getCurrentStackVersion();	StackId stackForUpgradePack = targetStackId;	if (direction.isDowngrade()) {	stackForUpgradePack = sourceStackId;	}	Map<String, UpgradePack> packs = m_ambariMetaInfoProvider.get().getUpgradePacks( currentStack.getStackName(), currentStack.getStackVersion());	UpgradePack pack = null;	if (StringUtils.isNotEmpty(preferredUpgradePackName) && packs.containsKey(preferredUpgradePackName)) {	pack = packs.get(preferredUpgradePackName);	
upgrade pack not found for stack 

Cluster cluster = context.getCluster();	MasterHostResolver mhr = context.getResolver();	Map<String, Map<String, ProcessingComponent>> allTasks = upgradePack.getTasks();	List<UpgradeGroupHolder> groups = new ArrayList<>();	UpgradeGroupHolder previousGroupHolder = null;	for (Grouping group : upgradePack.getGroups(context.getDirection())) {	if (!context.isScoped(group.scope)) {	continue;	}	if (null != group.condition && !group.condition.isSatisfied(context)) {	
skipping while building upgrade orchestration due to 

if (functionName == Type.START) {	pc.tasks.add(new StartTask());	}	if (functionName == Type.RESTART) {	pc.tasks.add(new RestartTask());	}	}	}	}	if (pc == null) {	
couldn t create a processing component for service and component 

groups.add(groupHolder);	}	previousGroupHolder = groupHolder;	}	}	if (LOG.isDebugEnabled()) {	for (UpgradeGroupHolder group : groups) {	LOG.debug(group.name);	int i = 0;	for (StageWrapper proxy : group.items) {	
stage 

previousGroupHolder = groupHolder;	}	}	if (LOG.isDebugEnabled()) {	for (UpgradeGroupHolder group : groups) {	LOG.debug(group.name);	int i = 0;	for (StageWrapper proxy : group.items) {	int j = 0;	for (TaskWrapper task : proxy.getTasks()) {	
task 

private void setDisplayNames(UpgradeContext context, String service, String component) {	StackId stackId = context.getCluster().getDesiredStackVersion();	try {	ServiceInfo serviceInfo = m_ambariMetaInfoProvider.get().getService(stackId.getStackName(), stackId.getStackVersion(), service);	context.setServiceDisplay(service, serviceInfo.getDisplayName());	ComponentInfo compInfo = serviceInfo.getComponentByName(component);	context.setComponentDisplay(service, component, compInfo.getDisplayName());	} catch (AmbariException e) {	
could not get service detail 

RepositoryVersionEntity targetRepositoryVersion = upgradeContext.getTargetRepositoryVersion(serviceName);	StackId targetStack = targetRepositoryVersion.getStackId();	service.setDesiredRepositoryVersion(targetRepositoryVersion);	Collection<ServiceComponent> components = service.getServiceComponents().values();	for (ServiceComponent serviceComponent : components) {	boolean versionAdvertised = false;	try {	ComponentInfo ci = m_ambariMetaInfoProvider.get().getComponent(targetStack.getStackName(), targetStack.getStackVersion(), serviceComponent.getServiceName(), serviceComponent.getName());	versionAdvertised = ci.isVersionAdvertised();	} catch (AmbariException e) {	
component doesn t exist for stack setting version to 

Set<String> servicesInUpgrade = upgradeContext.getSupportedServices();	Set<String> clusterConfigTypes = new HashSet<>();	Set<String> processedClusterConfigTypes = new HashSet<>();	for (String serviceName : servicesInUpgrade) {	RepositoryVersionEntity sourceRepositoryVersion = upgradeContext.getSourceRepositoryVersion(serviceName);	RepositoryVersionEntity targetRepositoryVersion = upgradeContext.getTargetRepositoryVersion(serviceName);	StackId sourceStackId = sourceRepositoryVersion.getStackId();	StackId targetStackId = targetRepositoryVersion.getStackId();	if (sourceStackId.equals(targetStackId)) {	RepositoryVersionEntity associatedRepositoryVersion = upgradeContext.getRepositoryVersion();	
the will not change stack configurations for since the source and target are both 

}	} else {	newDefaultConfigurations.put(existingConfigurationKey, existingConfigurationValue);	}	}	for every new configuration which does not exist in the existing configurations, see if it was present in the current stack stack 2.x has foo-site/property (on-ambari-upgrade is false) stack 2.y has foo-site/property the current cluster (on 2.x) does not have it In this case, we should NOT add it back as clearly stack advisor has removed it Iterator<Map.Entry<String, String>> newDefaultConfigurationsIterator = newDefaultConfigurations.entrySet().iterator();	while (newDefaultConfigurationsIterator.hasNext()) {	Map.Entry<String, String> newConfigurationEntry = newDefaultConfigurationsIterator.next();	String newConfigurationPropertyName = newConfigurationEntry.getKey();	if (oldServiceDefaultConfigs.containsKey(newConfigurationPropertyName) && !existingConfigurations.containsKey(newConfigurationPropertyName)) {	
the property exists in both and but is not part of the current set of configurations and will therefore not be included in the configuration merge 

}	if (null != newServiceDefaultConfigsByType) {	for (String clusterConfigType : clusterConfigTypes) {	if (processedClusterConfigTypes.contains(clusterConfigType)) {	newServiceDefaultConfigsByType.remove(clusterConfigType);	} else {	processedClusterConfigTypes.add(clusterConfigType);	}	}	Set<String> configTypes = newServiceDefaultConfigsByType.keySet();	
the upgrade will create the following configurations for stack 

========================= ambari sample_3153 =========================

public void close() {	
calling base close 

========================= ambari sample_1744 =========================

public void close() throws KerberosOperationException {	if (credentialsCacheFile != null) {	if (credentialsCacheFile.delete()) {	
failed to remove the cache file 

throw new KerberosOperationException("Failed to create keytab file, missing principal");	}	if (cachedKeytabs.containsKey(principal)) {	return cachedKeytabs.get(principal);	}	File keytabFile = null;	try {	try {	keytabFile = File.createTempFile("ambari_tmp", ".keytab");	if (!keytabFile.delete()) {	
failed to remove temporary file to hold keytab exporting the keytab file for may fail 

} catch (IOException e) {	throw new KerberosOperationException(String.format("Failed to create the temporary file needed to hold the exported keytab file for %s: %s", principal, e.getLocalizedMessage()), e);	}	exportKeytabFile(principal, keytabFile.getAbsolutePath(), getKeyEncryptionTypes());	Keytab keytab = readKeytabFile(keytabFile);	cachedKeytabs.put(principal, keytab);	return keytab;	} finally {	if ((keytabFile != null) && keytabFile.exists()) {	if (!keytabFile.delete()) {	
failed to remove the temporary keytab file 

protected abstract String[] getKinitCommand(String executableKinit, PrincipalKeyCredential credentials, String credentialsCache);	protected abstract void exportKeytabFile(String principal, String keytabFileDestinationPath, Set<EncryptionType> keyEncryptionTypes) throws KerberosOperationException;	protected boolean init() throws KerberosOperationException {	if (credentialsCacheFile != null) {	if (!credentialsCacheFile.delete()) {	
failed to remove the orphaned cache file 

========================= ambari sample_3329 =========================

CommandLineParser cmdLineParser = new DefaultParser();	HelpFormatter formatter = new HelpFormatter();	MpackContext ctx = null;	try {	CommandLine line = cmdLineParser.parse(getOptions(), args);	String mpackStacksStr = (String) line.getParsedOptionValue(MPACK_STACKS_ARG);	HashSet<String> stacksInMpack = new HashSet<>(Arrays.asList(mpackStacksStr.split(",")));	ctx = new MpackContext(stacksInMpack);	} catch (Exception exp) {	System.err.println("Parsing failed. Reason: " + exp.getMessage());	
parsing failed reason 

String stackVersion = stackInfo.get(stackName);	if(!stacksInMpack.contains(stackName)) {	String errorMsg = String.format("This Ambari instance is already managing the cluster %s that has the " + "%s-%s stack installed on it. The management pack you are attempting to install only contains stack " + "definitions for %s. Since this management pack does not contain a stack that has already being " + "deployed by Ambari, the --purge option would cause your existing Ambari installation to be unusable. " + "Due to that we cannot install this management pack.", clusterName, stackName, stackVersion, stacksInMpack.toString());	LOG.error(errorMsg);	System.err.println(errorMsg);	errorsFound = true;	}	}	} catch (SQLException e) {	System.err.println("SQL Exception occured during check for validating installed clusters. Reason: " + e.getMessage());	
sql exception occured during check for validating installed clusters reason 

}	} catch (SQLException e) {	System.err.println("SQL Exception occured during check for validating installed clusters. Reason: " + e.getMessage());	errorsFound = true;	} finally {	if (rs != null) {	try {	rs.close();	} catch (SQLException e) {	System.err.println("SQL Exception occurred during result set closing procedure. Reason: " +  e.getMessage());	
sql exception occurred during result set closing procedure reason 

} catch (SQLException e) {	System.err.println("SQL Exception occurred during result set closing procedure. Reason: " +  e.getMessage());	errorsFound = true;	}	}	if (statement != null) {	try {	statement.close();	} catch (SQLException e) {	System.err.println("SQL Exception occurred during statement closing procedure. Reason: " + e.getMessage());	
sql exception occurred during statement closing procedure reason 

public static void main(String[] args) throws Exception {	Injector injector = Guice.createInjector(new ControllerModule(), new MpackCheckerAuditModule());	MpackInstallChecker mpackInstallChecker = injector.getInstance(MpackInstallChecker.class);	MpackContext mpackContext = processArguments(args);	mpackInstallChecker.startPersistenceService();	mpackInstallChecker.checkClusters(mpackContext.getStacksInMpack());	mpackInstallChecker.stopPersistenceService();	if(mpackInstallChecker.isErrorsFound()) {	
mpack installation checker failed 

Injector injector = Guice.createInjector(new ControllerModule(), new MpackCheckerAuditModule());	MpackInstallChecker mpackInstallChecker = injector.getInstance(MpackInstallChecker.class);	MpackContext mpackContext = processArguments(args);	mpackInstallChecker.startPersistenceService();	mpackInstallChecker.checkClusters(mpackContext.getStacksInMpack());	mpackInstallChecker.stopPersistenceService();	if(mpackInstallChecker.isErrorsFound()) {	System.err.println("Mpack installation checker failed!");	System.exit(1);	} else {	
no errors found 

========================= ambari sample_2884 =========================

public <T extends Indexed> T load(Class<T> model, int id) throws ItemNotFound {	String modelPropName = getItemPropertyName(model, id);	
loading s 

public <T extends Indexed> T load(Class<T> model, int id) throws ItemNotFound {	String modelPropName = getItemPropertyName(model, id);	if (getConfig().containsKey(modelPropName)) {	String json = read(modelPropName);	
json s 

public synchronized <T extends Indexed> List<T> loadAll(Class<T> model, FilteringStrategy filter) {	ArrayList<T> list = new ArrayList<T>();	String modelIndexingPropName = getIndexPropertyName(model);	
loading all s s 

public synchronized void delete(Class model, int id) {	
deleting s d 

========================= ambari sample_1251 =========================

public RegistrationResponse register(Register message, throws WebApplicationException, InvalidStateTransitionException {	RegistrationResponse response = null;	try {	response = hh.handleRegistration(message);	
sending registration response 

public HeartBeatResponse heartbeat(HeartBeat message) throws WebApplicationException {	if (LOG.isDebugEnabled()) {	
received heartbeat message 

public HeartBeatResponse heartbeat(HeartBeat message) throws WebApplicationException {	if (LOG.isDebugEnabled()) {	}	HeartBeatResponse heartBeatResponse;	try {	heartBeatResponse = hh.handleHeartBeat(message);	if (LOG.isDebugEnabled()) {	
sending heartbeat response with response id 

public HeartBeatResponse heartbeat(HeartBeat message) throws WebApplicationException {	if (LOG.isDebugEnabled()) {	}	HeartBeatResponse heartBeatResponse;	try {	heartBeatResponse = hh.handleHeartBeat(message);	if (LOG.isDebugEnabled()) {	
response details 

public HeartBeatResponse heartbeat(HeartBeat message) throws WebApplicationException {	if (LOG.isDebugEnabled()) {	}	HeartBeatResponse heartBeatResponse;	try {	heartBeatResponse = hh.handleHeartBeat(message);	if (LOG.isDebugEnabled()) {	}	} catch (Exception e) {	
error in heartbeat 

public ComponentsResponse components( if (LOG.isDebugEnabled()) {	
received components request for cluster 

========================= ambari sample_3000 =========================

}	} catch (ObjectNotFoundException e) {	}	}	if ( nonStartedState == null || (isAppTimeLineServerActive && resourceManagerActiveCount > 0)) {	return State.STARTED;	}	return nonStartedState;	}	} catch (AmbariException e) {	
can t determine service state 

========================= ambari sample_3684 =========================

public void start(long period, TimeUnit unit) {	executor.scheduleAtFixedRate(new Runnable() {	public void run() {	try {	report();	} catch (RuntimeException ex) {	
runtimeexception thrown from report exception was suppressed 

========================= ambari sample_224 =========================

try {	String response = ambariApi.readFromAmbari(fetchUrl, "GET", null, null);	if (response != null && !response.isEmpty()) {	JSONObject json = (JSONObject) JSONValue.parse(response);	if (json.containsKey("privileges")) {	JSONArray privileges = (JSONArray) json.get("privileges");	if (privileges.size() > 0) return true;	}	}	} catch (AmbariHttpException e) {	
got error response from url response 

========================= ambari sample_674 =========================

protected void doStart() {	
starting heartbeats processing threads 

protected void doStop() {	
stopping heartbeats processing threads 

public void run() {	while (shouldRun) {	try {	HeartBeat heartbeat = pollHeartbeat();	if (heartbeat == null) {	break;	}	processHeartbeat(heartbeat);	} catch (Exception e) {	
exception received while processing heartbeat 

public void run() {	while (shouldRun) {	try {	HeartBeat heartbeat = pollHeartbeat();	if (heartbeat == null) {	break;	}	processHeartbeat(heartbeat);	} catch (Exception e) {	} catch (Throwable throwable) {	
error 

Map<Long, HostRoleCommand> commands = actionManager.getTasksMap(taskIds);	for (CommandReport report : reports) {	Long clusterId = null;	if (report.getClusterName() != null) {	try {	Cluster cluster = clusterFsm.getCluster(report.getClusterName());	clusterId = cluster.getClusterId();	} catch (AmbariException e) {	}	}	
received command report 

kkpe.setDistributed(true);	kerberosKeytabPrincipalDAO.merge(kkpe);	}	}	}	}	}	} else if (CHECK_KEYTABS.equalsIgnoreCase(customCommand)) {	ListKeytabsStructuredOut structuredOut = gson.fromJson(report.getStructuredOut(), ListKeytabsStructuredOut.class);	for (MissingKeytab each : structuredOut.missingKeytabs) {	
missing principal for keytab on host 

}	if (RoleCommand.ACTIONEXECUTE.toString().equals(report.getRoleCommand()) || (RoleCommand.CUSTOM_COMMAND.toString().equals(report.getRoleCommand()) && !("RESTART".equals(report.getCustomCommand()) || "START".equals(report.getCustomCommand()) || "STOP".equals(report.getCustomCommand())))) {	continue;	}	Cluster cl = clusterFsm.getCluster(report.getClusterName());	String service = report.getServiceName();	if (service == null || service.isEmpty()) {	throw new AmbariException("Invalid command report, service: " + service);	}	if (actionMetadata.getActions(service.toLowerCase()).contains(report.getRole())) {	
is an action skip component lookup 

try {	structuredOutput = gson.fromJson(report.getStructuredOut(), ComponentVersionStructuredOut.class);	} catch (JsonSyntaxException ex) {	}	String newVersion = structuredOutput == null ? null : structuredOutput.version;	Long repoVersionId = structuredOutput == null ? null : structuredOutput.repositoryVersionId;	HostComponentVersionAdvertisedEvent event = new HostComponentVersionAdvertisedEvent( cl, scHost, newVersion, repoVersionId);	versionEventPublisher.publish(event);	}	if ((report.getRoleCommand().equals(RoleCommand.START.toString()) || (report.getRoleCommand().equals(RoleCommand.CUSTOM_COMMAND.toString()) && ("START".equals(report.getCustomCommand()) || "RESTART".equals(report.getCustomCommand())))) && null != report.getConfigurationTags() && !report.getConfigurationTags().isEmpty()) {	
updating applied config on service component host 

scHost.handleEvent(new ServiceComponentHostOpSucceededEvent(schName, hostname, now));	}	} else if (report.getStatus().equals("FAILED")) {	if (StringUtils.isNotBlank(report.getStructuredOut())) {	try {	ComponentVersionStructuredOut structuredOutput = gson.fromJson(report.getStructuredOut(), ComponentVersionStructuredOut.class);	if (null != structuredOutput.upgradeDirection) {	scHost.setUpgradeState(UpgradeState.FAILED);	}	} catch (JsonSyntaxException ex) {	
structured output was found but not parseable 

} else if (report.getStatus().equals("FAILED")) {	if (StringUtils.isNotBlank(report.getStructuredOut())) {	try {	ComponentVersionStructuredOut structuredOutput = gson.fromJson(report.getStructuredOut(), ComponentVersionStructuredOut.class);	if (null != structuredOutput.upgradeDirection) {	scHost.setUpgradeState(UpgradeState.FAILED);	}	} catch (JsonSyntaxException ex) {	}	}	
operation failed may be retried service component host host action id and taskid 

ComponentVersionStructuredOut structuredOutput = gson.fromJson(report.getStructuredOut(), ComponentVersionStructuredOut.class);	if (null != structuredOutput.upgradeDirection) {	scHost.setUpgradeState(UpgradeState.FAILED);	}	} catch (JsonSyntaxException ex) {	}	}	if (actionManager.isInProgressCommand(report)) {	scHost.handleEvent(new ServiceComponentHostOpFailedEvent (schName, hostname, now));	} else {	
received report for a command that is no longer active 

}	}	if (actionManager.isInProgressCommand(report)) {	scHost.handleEvent(new ServiceComponentHostOpFailedEvent (schName, hostname, now));	} else {	}	} else if (report.getStatus().equals("IN_PROGRESS")) {	scHost.handleEvent(new ServiceComponentHostOpInProgressEvent(schName, hostname, now));	}	} catch (ServiceComponentNotFoundException scnex) {	
service component not found 

if (actionManager.isInProgressCommand(report)) {	scHost.handleEvent(new ServiceComponentHostOpFailedEvent (schName, hostname, now));	} else {	}	} else if (report.getStatus().equals("IN_PROGRESS")) {	scHost.handleEvent(new ServiceComponentHostOpInProgressEvent(schName, hostname, now));	}	} catch (ServiceComponentNotFoundException scnex) {	} catch (InvalidStateTransitionException ex) {	if (LOG.isDebugEnabled()) {	
state machine exception 

scHost.handleEvent(new ServiceComponentHostOpFailedEvent (schName, hostname, now));	} else {	}	} else if (report.getStatus().equals("IN_PROGRESS")) {	scHost.handleEvent(new ServiceComponentHostOpInProgressEvent(schName, hostname, now));	}	} catch (ServiceComponentNotFoundException scnex) {	} catch (InvalidStateTransitionException ex) {	if (LOG.isDebugEnabled()) {	} else {	
state machine exception 

Service svc = cl.getService(status.getServiceName());	String componentName = status.getComponentName();	if (svc.getServiceComponents().containsKey(componentName)) {	ServiceComponent svcComp = svc.getServiceComponent( componentName);	ServiceComponentHost scHost = svcComp.getServiceComponentHost( hostname);	org.apache.ambari.server.state.State prevState = scHost.getState();	org.apache.ambari.server.state.State liveState = org.apache.ambari.server.state.State.valueOf(org.apache.ambari.server.state.State.class, status.getStatus());	if (prevState.equals(org.apache.ambari.server.state.State.INSTALLED) || prevState.equals(org.apache.ambari.server.state.State.STARTED) || prevState.equals(org.apache.ambari.server.state.State.UNKNOWN)) {	scHost.setState(liveState);	if (!prevState.equals(liveState)) {	
state of service component of service of cluster has changed from to at host according to status command report 

if (extra.containsKey("processes")) {	List<Map<String, String>> list = (List<Map<String, String>>) extra.get("processes");	scHost.setProcesses(list);	}	if (extra.containsKey("version")) {	String version = extra.get("version").toString();	HostComponentVersionAdvertisedEvent event = new HostComponentVersionAdvertisedEvent(cl, scHost, version);	versionEventPublisher.publish(event);	}	} catch (Exception e) {	
could not access extra json for from 

========================= ambari sample_2979 =========================

public void close() {	
closing file 

while (localFileIterator.hasNext()) {	File localFile = localFileIterator.next();	fileSystem = LogFeederHDFSUtil.buildFileSystem(hdfsHost, hdfsPort);	if (fileSystem != null && localFile.exists()) {	String destFilePath = hdfsOutDir + "/" + localFile.getName();	String localPath = localFile.getAbsolutePath();	boolean overWrite = true;	boolean delSrc = true;	boolean isCopied = LogFeederHDFSUtil.copyFromLocal(localFile.getAbsolutePath(), destFilePath, fileSystem, overWrite, delSrc);	if (isCopied) {	
file copy to hdfs hdfspath and deleted local file 

File localFile = localFileIterator.next();	fileSystem = LogFeederHDFSUtil.buildFileSystem(hdfsHost, hdfsPort);	if (fileSystem != null && localFile.exists()) {	String destFilePath = hdfsOutDir + "/" + localFile.getName();	String localPath = localFile.getAbsolutePath();	boolean overWrite = true;	boolean delSrc = true;	boolean isCopied = LogFeederHDFSUtil.copyFromLocal(localFile.getAbsolutePath(), destFilePath, fileSystem, overWrite, delSrc);	if (isCopied) {	} else {	
hdfs file copy failed for hdfspath and localpath 

synchronized (readyMonitor) {	if (localReadyFiles.isEmpty()) {	readyMonitor.wait();	}	}	} catch (InterruptedException e) {	LOG.error(e.getLocalizedMessage(),e);	}	}	} catch (Exception e) {	
exception in hdfscopythread errormsg 

private void stopHDFSCopyThread() {	if (hdfsCopyThread != null) {	
waiting till copy all local files to hdfs 

private void stopHDFSCopyThread() {	if (hdfsCopyThread != null) {	while (!localReadyFiles.isEmpty()) {	try {	Thread.sleep(1000);	} catch (InterruptedException e) {	LOG.error(e.getLocalizedMessage(), e);	}	
still waiting to copy all local files to hdfs 

private void stopHDFSCopyThread() {	if (hdfsCopyThread != null) {	while (!localReadyFiles.isEmpty()) {	try {	Thread.sleep(1000);	} catch (InterruptedException e) {	LOG.error(e.getLocalizedMessage(), e);	}	}	
calling interrupt method for hdfscopythread to stop it 

while (!localReadyFiles.isEmpty()) {	try {	Thread.sleep(1000);	} catch (InterruptedException e) {	LOG.error(e.getLocalizedMessage(), e);	}	}	try {	hdfsCopyThread.interrupt();	} catch (SecurityException exception) {	
current thread does not have permission to interrupt the thread 

========================= ambari sample_1656 =========================

if (!AuthorizationHelper.isAuthorized(ResourceType.CLUSTER, cluster.getResourceId(), EnumSet.of(RoleAuthorization.CLUSTER_UPGRADE_DOWNGRADE_STACK))) {	throw new AuthorizationException("The authenticated user does not have authorization to " + "manage upgrade and downgrade");	}	Long requestId = (Long) resource.getPropertyValue(UPGRADE_REQUEST_ID);	Long stageId = (Long) resource.getPropertyValue(UPGRADE_ITEM_STAGE_ID);	StageEntityPK primaryKey = new StageEntityPK();	primaryKey.setRequestId(requestId);	primaryKey.setStageId(stageId);	StageEntity stageEntity = s_stageDao.findByPK(primaryKey);	if (null == stageEntity) {	
unable to change the status of request and stage to because it does not exist 

========================= ambari sample_3542 =========================

public TimelineMetricsCacheSizeOfEngine() {	this(new DefaultSizeOfEngine(DEFAULT_MAX_DEPTH, DEFAULT_ABORT_WHEN_MAX_DEPTH_EXCEEDED));	this.sizeOfMapEntry = reflectionSizeOf.sizeOf(new Long(1)) + reflectionSizeOf.sizeOf(new Double(2.0));	TreeMap<Long, Double> map = new TreeMap<>();	long emptyMapSize = reflectionSizeOf.sizeOf(map);	map.put(new Long(1), new Double(2.0));	long sizeOfMapOneEntry = reflectionSizeOf.deepSizeOf(DEFAULT_MAX_DEPTH, DEFAULT_ABORT_WHEN_MAX_DEPTH_EXCEEDED, map).getCalculated();	this.sizeOfMapEntryOverhead =  sizeOfMapOneEntry - (emptyMapSize + this.sizeOfMapEntry);	
creating custom sizeof engine for timelinemetrics 

public Size sizeOf(Object key, Object value, Object container) {	try {	
begin sizeof key value 

try {	long size = 0;	if (key instanceof TimelineAppMetricCacheKey) {	size += getTimelineMetricCacheKeySize((TimelineAppMetricCacheKey) key);	}	if (value instanceof TimelineMetricsCacheValue) {	size += getTimelineMetricCacheValueSize((TimelineMetricsCacheValue) value);	}	return new Size(size, false);	} finally {	
end sizeof key 

timelineMetricPrimitivesApproximation += reflectionSizeOf.sizeOf(metric.getInstanceId());	timelineMetricPrimitivesApproximation += reflectionSizeOf.sizeOf(metric.getTimestamp());	timelineMetricPrimitivesApproximation += reflectionSizeOf.sizeOf(metric.getStartTime());	timelineMetricPrimitivesApproximation += reflectionSizeOf.sizeOf(metric.getType());	timelineMetricPrimitivesApproximation += 8;	LOG.debug("timelineMetricPrimitivesApproximation bytes = {}", timelineMetricPrimitivesApproximation);	}	size += timelineMetricPrimitivesApproximation;	Map<Long, Double> metricValues = metric.getMetricValues();	if (metricValues != null && !metricValues.isEmpty()) {	
size of metric value 

timelineMetricPrimitivesApproximation += reflectionSizeOf.sizeOf(metric.getType());	timelineMetricPrimitivesApproximation += 8;	LOG.debug("timelineMetricPrimitivesApproximation bytes = {}", timelineMetricPrimitivesApproximation);	}	size += timelineMetricPrimitivesApproximation;	Map<Long, Double> metricValues = metric.getMetricValues();	if (metricValues != null && !metricValues.isEmpty()) {	size += (sizeOfMapEntry + sizeOfMapEntryOverhead) * metricValues.size();	}	}	
total size of metric values in cache 

public SizeOfEngine copyWith(int maxDepth, boolean abortWhenMaxDepthExceeded) {	
copying tracing sizeof engine maxdepth abort 

========================= ambari sample_3407 =========================

hostStateEntity.setHealthStatus(gson.toJson(new HostHealthStatus(HealthStatus.UNKNOWN, "")));	} else {	stateMachine.setCurrentState(hostStateEntity.getCurrentState());	}	if (null == hostEntity.getHostId()) {	persistEntities(hostEntity);	for (ClusterEntity clusterEntity : hostEntity.getClusterEntities()) {	try {	clusters.getClusterById(clusterEntity.getClusterId()).refresh();	} catch (AmbariException e) {	
error while looking up the cluster 

agentVersion = e.agentVersion.getVersion();	}	LOG.info("Received host registration, host=" + e.hostInfo + ", registrationTime=" + e.registrationTime + ", agentVersion=" + agentVersion);	host.clusters.updateHostMappings(host);	boolean associatedWithCluster = false;	try {	associatedWithCluster = host.clusters.getClustersForHost(host.getPublicHostName()).size() > 0;	} catch (HostNotFoundException e1) {	associatedWithCluster = false;	} catch (AmbariException e1) {	
unable to determine the clusters for host 

public String getOSFamilyFromHostAttributes(Map<String, String> hostAttributes) {	try {	String majorVersion = hostAttributes.get(OS_RELEASE_VERSION).split("\\.")[0];	return hostAttributes.get(OSFAMILY) + majorVersion;	} catch(Exception e) {	
error while getting os family from host attributes 

========================= ambari sample_3057 =========================

}	public TimelineEntities getEntities( init(res);	TimelineEntities entities = null;	try {	entities = store.getEntities( parseStr(entityType), parseLongStr(limit), parseLongStr(windowStart), parseLongStr(windowEnd), parseStr(fromId), parseLongStr(fromTs), parsePairStr(primaryFilter, ":"), parsePairsStr(secondaryFilter, ",", ":"), parseFieldsStr(fields, ","));	} catch (NumberFormatException e) {	throw new BadRequestException( "windowStart, windowEnd or limit is not a numeric value.");	} catch (IllegalArgumentException e) {	throw new BadRequestException("requested invalid field.");	} catch (IOException e) {	
error getting entities 

public TimelineEntity getEntity( init(res);	TimelineEntity entity = null;	try {	entity = store.getEntity(parseStr(entityId), parseStr(entityType), parseFieldsStr(fields, ","));	} catch (IllegalArgumentException e) {	throw new BadRequestException( "requested invalid field.");	} catch (IOException e) {	
error getting entity 

public TimelineEvents getEvents( init(res);	TimelineEvents events = null;	try {	events = store.getEntityTimelines( parseStr(entityType), parseArrayStr(entityId, ","), parseLongStr(limit), parseLongStr(windowStart), parseLongStr(windowEnd), parseArrayStr(eventType, ","));	} catch (NumberFormatException e) {	throw new BadRequestException( "windowStart, windowEnd or limit is not a numeric value.");	} catch (IOException e) {	
error getting entity timelines 

public TimelinePutResponse postMetrics( TimelineMetrics metrics) {	init(res);	if (metrics == null) {	return new TimelinePutResponse();	}	try {	if (LOG.isDebugEnabled()) {	
storing metrics 

public TimelinePutResponse postMetrics( TimelineMetrics metrics) {	init(res);	if (metrics == null) {	return new TimelinePutResponse();	}	try {	if (LOG.isDebugEnabled()) {	}	return timelineMetricStore.putMetrics(metrics);	} catch (Exception e) {	
error saving metrics 

public TimelinePutResponse postAggregatedMetrics( AggregationResult metrics) {	init(res);	if (metrics == null) {	return new TimelinePutResponse();	}	try {	if (LOG.isDebugEnabled()) {	
storing aggregated metrics 

public TimelinePutResponse postAggregatedMetrics( AggregationResult metrics) {	init(res);	if (metrics == null) {	return new TimelinePutResponse();	}	try {	if (LOG.isDebugEnabled()) {	}	return timelineMetricStore.putHostAggregatedMetrics(metrics);	} catch (Exception e) {	
error saving metrics 

public TimelinePutResponse postContainerMetrics( List<ContainerMetric> metrics) {	init(res);	if (metrics == null || metrics.isEmpty()) {	return new TimelinePutResponse();	}	try {	if (LOG.isDebugEnabled()) {	
storing container metrics 

public TimelinePutResponse postContainerMetrics( List<ContainerMetric> metrics) {	init(res);	if (metrics == null || metrics.isEmpty()) {	return new TimelinePutResponse();	}	try {	if (LOG.isDebugEnabled()) {	}	return timelineMetricStore.putContainerMetrics(metrics);	} catch (Exception e) {	
error saving metrics 

init(res);	if (entities == null) {	return new TimelinePutResponse();	}	try {	List<EntityIdentifier> entityIDs = new ArrayList<EntityIdentifier>();	for (TimelineEntity entity : entities.getEntities()) {	EntityIdentifier entityID = new EntityIdentifier(entity.getEntityId(), entity.getEntityType());	entityIDs.add(entityID);	if (LOG.isDebugEnabled()) {	
storing the entity json style content 

}	try {	List<EntityIdentifier> entityIDs = new ArrayList<EntityIdentifier>();	for (TimelineEntity entity : entities.getEntities()) {	EntityIdentifier entityID = new EntityIdentifier(entity.getEntityId(), entity.getEntityType());	entityIDs.add(entityID);	if (LOG.isDebugEnabled()) {	}	}	if (LOG.isDebugEnabled()) {	
storing entities 

for (TimelineEntity entity : entities.getEntities()) {	EntityIdentifier entityID = new EntityIdentifier(entity.getEntityId(), entity.getEntityType());	entityIDs.add(entityID);	if (LOG.isDebugEnabled()) {	}	}	if (LOG.isDebugEnabled()) {	}	return store.put(entities);	} catch (IOException e) {	
error putting entities 

private static TopNConfig parseTopNConfig(String topN, String topNFunction, String bottomN) {	if (topN == null || topN.isEmpty()) {	return null;	}	Integer topNValue = parseIntStr(topN);	if (topNValue == 0) {	
invalid input for topn query ignoring topn request 

========================= ambari sample_419 =========================

public CleanupResult cleanup(TimeBasedCleanupPolicy cleanupPolicy) {	long affectedRows = 0;	int errorCount = 0;	for (Cleanable cleanable : cleanables) {	
running the purge process for dao with cleanup policy 

public CleanupResult cleanup(TimeBasedCleanupPolicy cleanupPolicy) {	long affectedRows = 0;	int errorCount = 0;	for (Cleanable cleanable : cleanables) {	try {	affectedRows += cleanable.cleanup(cleanupPolicy);	}	catch (Exception ex) {	
running the purge process for dao failed with 

========================= ambari sample_3810 =========================

public void build(Stage stage) {	if (stage == null) {	throw new IllegalArgumentException("Null stage");	}	if (commandExecutionType == CommandExecutionType.DEPENDENCY_ORDERED) {	
build stage with dependency ordered commandexecutiontype 

public List<Stage> getStages() throws AmbariException {	long initialStageId = initialStage.getStageId();	List<Stage> stageList = new ArrayList<>();	List<RoleGraphNode> firstStageNodes = new ArrayList<>();	if(!graph.isEmpty()){	
detecting cycle graphs 

========================= ambari sample_2647 =========================

doIfConfigNotExist(solrPropsConfig, zkConfigManager);	uploadMissingConfigFiles(zkClient, zkConfigManager, solrPropsConfig.getConfigName());	}	} catch (Exception e) {	throw new RuntimeException(String.format("Cannot upload configurations to zk. (collection: %s, config set folder: %s)", solrPropsConfig.getCollection(), solrPropsConfig.getConfigSetFolder()), e);	} finally {	if (tmpDir.exists()) {	try {	FileUtils.deleteDirectory(tmpDir);	} catch (IOException e){	
cannot delete temp directory 

public boolean doIfConfigExists(SolrPropsConfig solrPropsConfig, SolrZkClient zkClient, String separator, String downloadFolderLocation, File tmpDir) throws IOException {	boolean result = false;	
config set exists for collection refreshing it if needed 

public boolean doIfConfigExists(SolrPropsConfig solrPropsConfig, SolrZkClient zkClient, String separator, String downloadFolderLocation, File tmpDir) throws IOException {	boolean result = false;	if (!tmpDir.mkdirs()) {	
cannot create directories for 

========================= ambari sample_1342 =========================

public void init(VerifiableProperties props) {	synchronized (lock) {	if (!initialized) {	
initializing kafka timeline metrics sink 

public void init(VerifiableProperties props) {	synchronized (lock) {	if (!initialized) {	try {	hostname = InetAddress.getLocalHost().getHostName();	if ((hostname == null) || (!hostname.contains("."))) {	hostname = InetAddress.getLocalHost().getCanonicalHostName();	}	} catch (UnknownHostException e) {	
could not identify hostname 

public synchronized void startReporter(long period) {	synchronized (lock) {	if (initialized && !running) {	reporter.start(period, TimeUnit.SECONDS);	running = true;	
started kafka timeline metrics reporter with polling period d seconds 

public synchronized void stopReporter() {	synchronized (lock) {	if (initialized && running) {	reporter.stop();	running = false;	
stopped kafka timeline metrics reporter 

final MetricName metricName = entry.getKey();	final Metric metric = entry.getValue();	Context context = new Context() {	public List<TimelineMetric> getTimelineMetricList() {	return metricsList;	}	};	metric.processWith(this, metricName, context);	}	} catch (Throwable t) {	
exception processing kafka metric 

Context context = new Context() {	public List<TimelineMetric> getTimelineMetricList() {	return metricsList;	}	};	metric.processWith(this, metricName, context);	}	} catch (Throwable t) {	}	if (LOG.isDebugEnabled()) {	
metrics list size 

Context context = new Context() {	public List<TimelineMetric> getTimelineMetricList() {	return metricsList;	}	};	metric.processWith(this, metricName, context);	}	} catch (Throwable t) {	}	if (LOG.isDebugEnabled()) {	
metics set size 

Context context = new Context() {	public List<TimelineMetric> getTimelineMetricList() {	return metricsList;	}	};	metric.processWith(this, metricName, context);	}	} catch (Throwable t) {	}	if (LOG.isDebugEnabled()) {	
excluded metrics set 

} catch (Throwable t) {	}	if (LOG.isDebugEnabled()) {	}	if (!metricsList.isEmpty()) {	TimelineMetrics timelineMetrics = new TimelineMetrics();	timelineMetrics.setMetrics(metricsList);	try {	emitMetrics(timelineMetrics);	} catch (Throwable t) {	
exception emitting metrics 

========================= ambari sample_225 =========================

private void createKafkaRetryThread() {	Thread retryThread = new Thread("kafka-writer-retry,topic=" + topic) {	public void run() {	KafkaCallBack kafkaCallBack = null;	
started thread to monitor failed messsages 

try {	if (failedMessages.size() == 0) {	if (publishMessage(block, inputMarker)) {	break;	}	}	if (isDrain() || inputMarker.getInput().isDrain()) {	break;	}	if (!isKafkaBrokerUp) {	
kafka is down going to sleep for seconds 

public void flush() {	
flush called 

public void close() {	
closing kafka client 

public void close() {	flush();	if (producer != null) {	try {	producer.close();	} catch (Throwable t) {	LOG.error("Error closing Kafka topic. topic=" + topic);	}	}	
closed kafka client 

producer.send(new ProducerRecord<String, String>(topic, block), new KafkaCallBack(this, block, inputMarker, ++messageCount));	return true;	} else {	try {	RecordMetadata metadata = producer.send(new ProducerRecord<String, String>(topic, block)).get();	if (metadata != null) {	statMetric.value++;	writeBytesMetric.value += block.length();	}	if (!isKafkaBrokerUp) {	
started writing to kafka 

statMetric.value++;	writeBytesMetric.value += block.length();	}	if (!isKafkaBrokerUp) {	isKafkaBrokerUp = true;	}	return true;	} catch (InterruptedException e) {	isKafkaBrokerUp = false;	String logKeyMessage = this.getClass().getSimpleName() + "_KAFKA_INTERRUPT";	
interruptedexception error sending message to kafka 

if (!isKafkaBrokerUp) {	isKafkaBrokerUp = true;	}	return true;	} catch (InterruptedException e) {	isKafkaBrokerUp = false;	String logKeyMessage = this.getClass().getSimpleName() + "_KAFKA_INTERRUPT";	} catch (ExecutionException e) {	isKafkaBrokerUp = false;	String logKeyMessage = this.getClass().getSimpleName() + "_KAFKA_EXECUTION";	
executionexception error sending message to kafka 

return true;	} catch (InterruptedException e) {	isKafkaBrokerUp = false;	String logKeyMessage = this.getClass().getSimpleName() + "_KAFKA_INTERRUPT";	} catch (ExecutionException e) {	isKafkaBrokerUp = false;	String logKeyMessage = this.getClass().getSimpleName() + "_KAFKA_EXECUTION";	} catch (Throwable t) {	isKafkaBrokerUp = false;	String logKeyMessage = this.getClass().getSimpleName() + "_KAFKA_WRITE_ERROR";	
genericexception error sending message to kafka 

public void onCompletion(RecordMetadata metadata, Exception exception) {	if (metadata != null) {	if (!output.isKafkaBrokerUp) {	
started writing to kafka 

public void onCompletion(RecordMetadata metadata, Exception exception) {	if (metadata != null) {	if (!output.isKafkaBrokerUp) {	output.isKafkaBrokerUp = true;	}	output.incrementStat(1);	output.writeBytesMetric.value += message.length();	} else {	output.isKafkaBrokerUp = false;	String logKeyMessage = this.getClass().getSimpleName() + "_KAFKA_ASYNC_ERROR";	
error sending message to kafka async callback 

========================= ambari sample_1657 =========================

public MetricsCollectorHAManager() {	clusterCollectorHAState = new HashMap<>();	if (null == eventPublisher && null != AmbariServer.getController()) {	eventPublisher = AmbariServer.getController().getAmbariEventPublisher();	if (eventPublisher != null) {	eventPublisher.register(this);	} else {	
unable to retrieve ambarieventpublisher for metric collector host event listening 

public void addCollectorHost(String clusterName, String collectorHost) {	
adding collector host to cluster 

public void onMetricsCollectorHostDownEvent(MetricsCollectorHostDownEvent event) {	
metricscollectorhostdownevent caught down collector 

========================= ambari sample_3390 =========================

Set<Long> clusterResId = new HashSet<>();	if (clusterNamePropertyId != null) {	try {	AmbariManagementController amc = AmbariServer.getController();	Set<String> clusterNames = getClustersNameFromResources(resources, clusterNamePropertyId);	Iterator<String> clusNameItr = clusterNames.iterator();	while (clusNameItr.hasNext()) {	clusterResId.add(amc.getClusters().getCluster(clusNameItr.next()).getResourceId());	}	} catch (AmbariException e) {	
cluster id couldn t be retrieved 

if (clusterNamePropertyId != null) {	try {	AmbariManagementController amc = AmbariServer.getController();	Set<String> clusterNames = getClustersNameFromResources(resources, clusterNamePropertyId);	Iterator<String> clusNameItr = clusterNames.iterator();	while (clusNameItr.hasNext()) {	clusterResId.add(amc.getClusters().getCluster(clusNameItr.next()).getResourceId());	}	} catch (AmbariException e) {	} catch (Exception e) {	
cluster id couldn t be retrieved 

}	break;	case Component : if (!AuthorizationHelper.isAuthorized(ResourceType.CLUSTER, clusResId, EnumSet.of(RoleAuthorization.SERVICE_VIEW_METRICS))) {	throw new AuthorizationException("The authenticated user does not have authorization to view Service metrics");	}	break;	case HostComponent: if (!AuthorizationHelper.isAuthorized(ResourceType.CLUSTER, clusResId, EnumSet.of(RoleAuthorization.SERVICE_VIEW_METRICS))) {	throw new AuthorizationException("The authenticated user does not have authorization to view Service metrics");	}	break;	
unsuported resource type for metrics 

========================= ambari sample_3514 =========================

public Collection<HostRoleCommandEntity> abortOperation(long requestId) {	long now = System.currentTimeMillis();	List<HostRoleCommandEntity> commands = hostRoleCommandDAO.findByRequestIdAndStatuses(requestId, HostRoleStatus.SCHEDULED_STATES);	for (HostRoleCommandEntity command : commands) {	command.setStatus(HostRoleStatus.ABORTED);	command.setEndTime(now);	
aborting command hostname role requestid taskid stageid 

hostRoleCommandEntity.setHostEntity(hostEntity);	try {	Host hostObject = clusters.getHost(hostEntity.getHostName());	if (!StringUtils.isBlank(hostObject.getPrefix())) {	prefix = hostObject.getPrefix();	if (!prefix.endsWith("/")) {	prefix = prefix + "/";	}	}	} catch (AmbariException e) {	
exception in getting prefix for host and setting output and error log files using no prefix 

long[] requestStageIds = StageUtils.getRequestStage(actionId);	long requestId = requestStageIds[0];	long stageId = requestStageIds[1];	auditLog(commandEntity, requestId);	if (requestDAO.getLastStageId(requestId).equals(stageId)) {	requestsToCheck.add(requestId);	}	}	commandEntitiesToMerge.add(commandEntity);	} else {	
request for invalid transition of host role command status received for task id d from agent s s 

public void updateHostRoleState(String hostname, long requestId, long stageId, String role, CommandReport report) {	boolean checkRequest = false;	if (LOG.isDebugEnabled()) {	
update hostrolestate hostname requestid stageid role report 

public void invalidateCommandCacheOnHostRemove(HostsRemovedEvent event) {	
invalidating hrc cache after receiveing 

========================= ambari sample_2755 =========================

public void setExecutionDetailsRequest(String host, String component, String requestExecutionCmd) {	if (StringUtils.isNotBlank(requestExecutionCmd)) {	Map<String, Boolean> perHostRequiresExecCmdDetails = getPerHostRequiresExecCmdDetails(host);	if (Boolean.TRUE.toString().toUpperCase().equals(requestExecutionCmd.toUpperCase())) {	
setting need for exec command to for 

public Boolean shouldSendExecutionDetails(String host, String component) {	Map<String, Boolean> perHostRequiresExecCmdDetails = getPerHostRequiresExecCmdDetails(host);	if (perHostRequiresExecCmdDetails != null && perHostRequiresExecCmdDetails.containsKey(component)) {	
sending exec command details for 

========================= ambari sample_3001 =========================

public void testMapperFieldValue_replaceValue() {	
testmapperfieldvalue replacevalue 

public void testMapperFieldValue_noPostValue() {	
testmapperfieldvalue nopostvalue 

public void testMapperFieldValue_noPreValueFound() {	
testmapperfieldvalue noprevaluefound 

========================= ambari sample_1600 =========================

public static void closeConnection() {	try {	if (connection != null) {	connection.close();	}	} catch (SQLException e) {	
exception occurred during connection close procedure 

public static DatabaseConsistencyCheckResult runAllDBChecks(boolean fixIssues) throws Throwable {	
check database started 

fixConfigsSelectedMoreThanOnce();	}	checkSchemaName();	checkMySQLEngine();	checkForConfigsNotMappedToService();	checkForConfigsSelectedMoreThanOnce();	checkForHostsWithoutState();	checkHostComponentStates();	checkServiceConfigs();	checkForLargeTables();	
check database completed 

checkMySQLEngine();	checkForConfigsNotMappedToService();	checkForConfigsSelectedMoreThanOnce();	checkForHostsWithoutState();	checkHostComponentStates();	checkServiceConfigs();	checkForLargeTables();	return checkResult;	}	catch (Throwable ex) {	
an error occurred during database consistency check 

public static void checkDBVersionCompatible() throws AmbariException {	
checking db store version 

try (Scanner scanner = new Scanner(versionFile)) {	serverVersion = scanner.useDelimiter("\\Z").next();	} catch (IOException ioe) {	throw new AmbariException("Unable to read server version file.");	}	if (schemaVersionEntity==null || VersionUtils.compareVersions(schemaVersion, serverVersion, 3) != 0) {	String error = "Current database store version is not compatible with " + "current server version" + ", serverVersion=" + serverVersion + ", schemaVersion=" + schemaVersion;	LOG.error(error);	throw new AmbariException(error);	}	
db store version is compatible 

static void checkForLargeTables() {	
checking for tables with large physical size 

while (rs.next()) {	tableSizeInBytes = rs.getLong(1);	if (tableSizeInBytes != null) {	tableSizeInMB = tableSizeInBytes / 1024.0 / 1024.0;	}	}	}	if (tableSizeInMB != null && tableSizeInMB > TABLE_SIZE_LIMIT_MB) {	warning("The database table {} is currently {} MB (limit is {}) and may impact performance. It is recommended " + "that you reduce its size by executing \"ambari-server db-purge-history\".", tableName, tableSizeInMB, TABLE_SIZE_LIMIT_MB);	} else if (tableSizeInMB != null && tableSizeInMB < TABLE_SIZE_LIMIT_MB) {	
the database table s is currently mb and is within normal limits 

}	}	}	if (tableSizeInMB != null && tableSizeInMB > TABLE_SIZE_LIMIT_MB) {	warning("The database table {} is currently {} MB (limit is {}) and may impact performance. It is recommended " + "that you reduce its size by executing \"ambari-server db-purge-history\".", tableName, tableSizeInMB, TABLE_SIZE_LIMIT_MB);	} else if (tableSizeInMB != null && tableSizeInMB < TABLE_SIZE_LIMIT_MB) {	} else {	throw new Exception();	}	} catch (Exception e) {	
failed to get s table size from database will check row count 

try {	rs = statement.executeQuery(String.format(GET_ROW_COUNT_QUERY, tableName));	if (rs != null) {	while (rs.next()) {	tableRowCount = rs.getInt(1);	}	}	if (tableRowCount > TABLE_ROW_COUNT_LIMIT) {	warning("The database table {} currently has {} rows (limit is {}) and may impact performance. It is " + "recommended that you reduce its size by executing \"ambari-server db-purge-history\".", tableName, tableRowCount, TABLE_ROW_COUNT_LIMIT);	} else if (tableRowCount != -1 && tableRowCount < TABLE_ROW_COUNT_LIMIT) {	
the database table s currently has d rows and is within normal limits d 

warning("Unable to get size for table {}!", tableName);	}	} catch (SQLException ex) {	warning(String.format("Failed to get %s row count: ", tableName), e);	}	} finally {	if (rs != null) {	try {	rs.close();	} catch (SQLException e) {	
exception occurred during result set closing procedure 

if (rs != null) {	try {	rs.close();	} catch (SQLException e) {	}	}	if (statement != null) {	try {	statement.close();	} catch (SQLException e) {	
exception occurred during statement closing procedure 

static void checkForConfigsSelectedMoreThanOnce() {	
checking for more than configuration of the same type being enabled 

error("You have config(s), in cluster {}, that is(are) selected more than once in clusterconfig table: {}", clusterName ,StringUtils.join(clusterConfigTypeMap.get(clusterName), ","));	}	}	} catch (SQLException e) {	warning("Exception occurred during check for config selected more than once procedure: ", e);	} finally {	if (rs != null) {	try {	rs.close();	} catch (SQLException e) {	
exception occurred during result set closing procedure 

if (rs != null) {	try {	rs.close();	} catch (SQLException e) {	}	}	if (statement != null) {	try {	statement.close();	} catch (SQLException e) {	
exception occurred during statement closing procedure 

static void checkForHostsWithoutState() {	
checking for hosts without state 

warning("You have host(s) without state (in hoststate table): " + StringUtils.join(hostsWithoutStatus, ","));	}	}	} catch (SQLException e) {	warning("Exception occurred during check for host without state procedure: ", e);	} finally {	if (rs != null) {	try {	rs.close();	} catch (SQLException e) {	
exception occurred during result set closing procedure 

if (rs != null) {	try {	rs.close();	} catch (SQLException e) {	}	}	if (statement != null) {	try {	statement.close();	} catch (SQLException e) {	
exception occurred during statement closing procedure 

result = rs.getInt(1);	}	}	} catch (SQLException e) {	warning("Exception occurred during topology request tables check: ", e);	} finally {	if (rs != null) {	try {	rs.close();	} catch (SQLException e) {	
exception occurred during result set closing procedure 

static void checkHostComponentStates() {	
checking host component states count equals host component desired states count 

for (Map.Entry<String, String> component : hostComponentStateDuplicates.entrySet()) {	warning("Component {} on host with id {}, has more than one host component state (hostcomponentstate table)!", component.getKey(), component.getValue());	}	} catch (SQLException e) {	warning("Exception occurred during check for same count of host component states and host component desired states: ", e);	} finally {	if (rs != null) {	try {	rs.close();	} catch (SQLException e) {	
exception occurred during result set closing procedure 

if (rs != null) {	try {	rs.close();	} catch (SQLException e) {	}	}	if (statement != null) {	try {	statement.close();	} catch (SQLException e) {	
exception occurred during statement closing procedure 

static void fixClusterConfigsNotMappedToAnyService() {	
checking for configs not mapped to any service 

static void fixClusterConfigsNotMappedToAnyService() {	ClusterDAO clusterDAO = injector.getInstance(ClusterDAO.class);	List<ClusterConfigEntity> notMappedClusterConfigs = getNotMappedClusterConfigsToService();	for (ClusterConfigEntity clusterConfigEntity : notMappedClusterConfigs){	if (!clusterConfigEntity.isUnmapped()){	continue;	}	List<String> types = new ArrayList<>();	String type = clusterConfigEntity.getType();	types.add(type);	
removing config that is not mapped to any service 

static void checkForConfigsNotMappedToService() {	
checking for configs that are not mapped to any service 

static void fixHostComponentStatesCountEqualsHostComponentsDesiredStates() {	
checking that there are the same number of actual and desired host components 

ServiceComponentDesiredStateEntity serviceComponentDesiredStateEntity = hostComponentDesiredStateEntity.getServiceComponentDesiredStateEntity();	HostComponentStateEntity stateEntity = new HostComponentStateEntity();	stateEntity.setClusterId(hostComponentDesiredStateEntity.getClusterId());	stateEntity.setComponentName(hostComponentDesiredStateEntity.getComponentName());	stateEntity.setServiceName(hostComponentDesiredStateEntity.getServiceName());	stateEntity.setVersion(State.UNKNOWN.toString());	stateEntity.setHostEntity(hostComponentDesiredStateEntity.getHostEntity());	stateEntity.setCurrentState(State.UNKNOWN);	stateEntity.setUpgradeState(UpgradeState.NONE);	stateEntity.setServiceComponentDesiredStateEntity(hostComponentDesiredStateEntity.getServiceComponentDesiredStateEntity());	
trying to add missing record in hostcomponentstate 

hostComponentStateDAO.create(stateEntity);	}	for (HostComponentStateEntity missedHostComponentState : missedHostComponentStates) {	HostComponentDesiredStateEntity stateEntity = new HostComponentDesiredStateEntity();	stateEntity.setClusterId(missedHostComponentState.getClusterId());	stateEntity.setComponentName(missedHostComponentState.getComponentName());	stateEntity.setServiceName(missedHostComponentState.getServiceName());	stateEntity.setHostEntity(missedHostComponentState.getHostEntity());	stateEntity.setDesiredState(State.UNKNOWN);	stateEntity.setServiceComponentDesiredStateEntity(missedHostComponentState.getServiceComponentDesiredStateEntity());	
trying to add missing record in hostcomponentdesiredstate 

static void checkSchemaName () {	Configuration conf = injector.getInstance(Configuration.class);	if(conf.getDatabaseType() == Configuration.DatabaseType.POSTGRES) {	
ensuring that the schema set for postgres is correct 

static void checkMySQLEngine () {	Configuration conf = injector.getInstance(Configuration.class);	if(conf.getDatabaseType()!=Configuration.DatabaseType.MYSQL) {	return;	}	
checking to ensure that the mysql db engine type is set to innodb 

warning("Found tables with engine type that is not InnoDB : {}", tablesInfo);	}	}	} catch (SQLException e) {	warning("Exception occurred during checking MySQL engine to be innodb: ", e);	} finally {	if (rs != null) {	try {	rs.close();	} catch (SQLException e) {	
exception occurred during result set closing procedure 

static void fixConfigsSelectedMoreThanOnce() {	
fix configs selected more than once 

clusterConfigTypeMap.put(rs.getString("cluster_name"), rs.getString("type_name"));	}	}	} catch (SQLException e) {	warning("Exception occurred during check for config selected more than once procedure: ", e);	} finally {	if (rs != null) {	try {	rs.close();	} catch (SQLException e) {	
exception occurred during result set closing procedure 

if (rs != null) {	try {	rs.close();	} catch (SQLException e) {	}	}	if (statement != null) {	try {	statement.close();	} catch (SQLException e) {	
exception occurred during statement closing procedure 

static void checkServiceConfigs()  {	
checking services and their configs 

String GET_STACK_NAME_VERSION_QUERY = "select c.cluster_name, s.stack_name, s.stack_version from clusters c " + "join stack s on c.desired_stack_id = s.stack_id";	String GET_SERVICES_WITH_CONFIGS_QUERY = "select c.cluster_name, cs.service_name, cc.type_name, sc.version from clusterservices cs " + "join serviceconfig sc on cs.service_name=sc.service_name and cs.cluster_id=sc.cluster_id " + "join serviceconfigmapping scm on sc.service_config_id=scm.service_config_id " + "join clusterconfig cc on scm.config_id=cc.config_id and sc.cluster_id=cc.cluster_id " + "join clusters c on cc.cluster_id=c.cluster_id and sc.stack_id=c.desired_stack_id " + "where sc.group_id is null and sc.service_config_id=(select max(service_config_id) from serviceconfig sc2 where sc2.service_name=sc.service_name and sc2.cluster_id=sc.cluster_id) " + "group by c.cluster_name, cs.service_name, cc.type_name, sc.version";	String GET_NOT_SELECTED_SERVICE_CONFIGS_QUERY = "select c.cluster_name, cs.service_name, cc.type_name from clusterservices cs " + "join serviceconfig sc on cs.service_name=sc.service_name and cs.cluster_id=sc.cluster_id " + "join serviceconfigmapping scm on sc.service_config_id=scm.service_config_id " + "join clusterconfig cc on scm.config_id=cc.config_id and cc.cluster_id=sc.cluster_id " + "join clusters c on cc.cluster_id=c.cluster_id " + "where sc.group_id is null and sc.service_config_id = (select max(service_config_id) from serviceconfig sc2 where sc2.service_name=sc.service_name and sc2.cluster_id=sc.cluster_id) " + "group by c.cluster_name, cs.service_name, cc.type_name " + "having sum(cc.selected) < 1";	Multimap<String, String> clusterServiceMap = HashMultimap.create();	Map<String, Map<String, String>>  clusterStackInfo = new HashMap<>();	Map<String, Multimap<String, String>> clusterServiceVersionMap = new HashMap<>();	Map<String, Multimap<String, String>> clusterServiceConfigType = new HashMap<>();	ResultSet rs = null;	Statement statement = null;	ensureConnection();	
getting ambari metainfo instance 

Map<String, Map<String, String>>  clusterStackInfo = new HashMap<>();	Map<String, Multimap<String, String>> clusterServiceVersionMap = new HashMap<>();	Map<String, Multimap<String, String>> clusterServiceConfigType = new HashMap<>();	ResultSet rs = null;	Statement statement = null;	ensureConnection();	if (ambariMetaInfo == null) {	ambariMetaInfo = injector.getInstance(AmbariMetaInfo.class);	}	try {	
executing query get services without configs 

statement = connection.createStatement(ResultSet.TYPE_SCROLL_SENSITIVE, ResultSet.CONCUR_UPDATABLE);	rs = statement.executeQuery(GET_SERVICES_WITHOUT_CONFIGS_QUERY);	if (rs != null) {	while (rs.next()) {	clusterServiceMap.put(rs.getString("cluster_name"), rs.getString("service_name"));	}	for (String clusterName : clusterServiceMap.keySet()) {	warning("Service(s): {}, from cluster {} has no config(s) in serviceconfig table!", StringUtils.join(clusterServiceMap.get(clusterName), ","), clusterName);	}	}	
executing query get service config without mapping 

clusterServiceVersionMap.put(clusterName, serviceVersion);	}	}	for (String clName : clusterServiceVersionMap.keySet()) {	Multimap<String, String> serviceVersion = clusterServiceVersionMap.get(clName);	for (String servName : serviceVersion.keySet()) {	warning("In cluster {}, service config mapping is unavailable (in table serviceconfigmapping) for service {} with version(s) {}! ", clName, servName, StringUtils.join(serviceVersion.get(servName), ","));	}	}	}	
getting stack info from database 

if (rs != null) {	while (rs.next()) {	Map<String, String> stackInfoMap = new HashMap<>();	stackInfoMap.put(rs.getString("stack_name"), rs.getString("stack_version"));	clusterStackInfo.put(rs.getString("cluster_name"), stackInfoMap);	}	}	Set<String> serviceNames = new HashSet<>();	Map<String, Map<Integer, Multimap<String, String>>> dbClusterServiceVersionConfigs = new HashMap<>();	Multimap<String, String> stackServiceConfigs = HashMultimap.create();	
executing query get services with configs 

}	} else {	Map<Integer, Multimap<String, String>> dbServiceVersionConfigs = new HashMap<>();	Multimap<String, String> dbServiceConfigs = HashMultimap.create();	dbServiceConfigs.put(serviceName, configType);	dbServiceVersionConfigs.put(serviceVersion, dbServiceConfigs);	dbClusterServiceVersionConfigs.put(clusterName, dbServiceVersionConfigs);	}	}	}	
comparing service configs from stack with configs that we got from db 

dbServiceVersionConfigs.put(serviceVersion, dbServiceConfigs);	dbClusterServiceVersionConfigs.put(clusterName, dbServiceVersionConfigs);	}	}	}	for (Map.Entry<String, Map<String, String>> clusterStackInfoEntry : clusterStackInfo.entrySet()) {	String clusterName = clusterStackInfoEntry.getKey();	Map<String, String> stackInfo = clusterStackInfoEntry.getValue();	String stackName = stackInfo.keySet().iterator().next();	String stackVersion = stackInfo.get(stackName);	
getting services from metainfo 

}	}	}	for (Map.Entry<String, Map<String, String>> clusterStackInfoEntry : clusterStackInfo.entrySet()) {	String clusterName = clusterStackInfoEntry.getKey();	Map<String, String> stackInfo = clusterStackInfoEntry.getValue();	String stackName = stackInfo.keySet().iterator().next();	String stackVersion = stackInfo.get(stackName);	Map<String, ServiceInfo> serviceInfoMap = ambariMetaInfo.getServices(stackName, stackVersion);	for (String serviceName : serviceNames) {	
processing 

ServiceInfo serviceInfo = serviceInfoMap.get(serviceName);	if (serviceInfo != null) {	Set<String> configTypes = serviceInfo.getConfigTypeAttributes().keySet();	for (String configType : configTypes) {	stackServiceConfigs.put(serviceName, configType);	}	} else {	warning("Service {} is not available for stack {} in cluster {}", serviceName, stackName + "-" + stackVersion, clusterName);	}	}	
comparing required service configs from stack with mapped service configs from db 

}	if (!serviceConfigsFromStack.isEmpty()) {	warning("Required config(s): {} is(are) not available for service {} with service config version {} in cluster {}", StringUtils.join(serviceConfigsFromStack, ","), serviceName, Integer.toString(serviceVersion), clusterName);	}	}	}	}	}	}	}	
getting services which has mapped configs which are not selected in clusterconfig 

warning("You have non selected configs: {} for service {} from cluster {}!", StringUtils.join(serviceConfig.get(serviceName), ","), serviceName, clusterName);	}	}	} catch (SQLException | AmbariException e) {	warning("Exception occurred during complex service check procedure: ", e);	} finally {	if (rs != null) {	try {	rs.close();	} catch (SQLException e) {	
exception occurred during result set closing procedure 

if (rs != null) {	try {	rs.close();	} catch (SQLException e) {	}	}	if (statement != null) {	try {	statement.close();	} catch (SQLException e) {	
exception occurred during statement closing procedure 

========================= ambari sample_2885 =========================

rows.addAll(result.getRows());	}	if (receive instanceof NoMoreItems) {	if(descriptions.isEmpty()) {	descriptions.addAll(((NoMoreItems)receive).getColumns());	}	endReached = true;	}	if (receive instanceof FetchFailed) {	FetchFailed error = (FetchFailed) receive;	
failed to fetch results 

========================= ambari sample_767 =========================

requestId = stageEntity.getRequestId();	stageId = stageEntity.getStageId();	skippable = stageEntity.isSkippable();	supportsAutoSkipOnFailure = stageEntity.isAutoSkipOnFailureSupported();	logDir = stageEntity.getLogInfo();	clusterId = stageEntity.getClusterId().longValue();	if (-1L != clusterId) {	try {	clusterName = clusters.getClusterById(clusterId).getClusterName();	} catch (Exception e) {	
could not load cluster with id the cluster may have been removed for stage 

for (String host: commandsToSend.keySet()) {	int summaryTaskTimeoutForHost = 0;	for (ExecutionCommandWrapper command : commandsToSend.get(host)) {	Map<String, String> commandParams = command.getExecutionCommand().getCommandParams();	String timeoutKey = ExecutionCommand.KeyNames.COMMAND_TIMEOUT;	if (commandParams != null && commandParams.containsKey(timeoutKey)) {	String timeoutStr = commandParams.get(timeoutKey);	long commandTimeout = Long.parseLong(timeoutStr) * 1000;	summaryTaskTimeoutForHost += commandTimeout;	} else {	
execution command has no timeout parameter 

========================= ambari sample_2743 =========================

InputStreamReader reader = new InputStreamReader(inputStream);	try {	Set<AlertDefinition> allDefinitions = m_factory.getAlertDefinitions( reader, RootService.AMBARI.name());	String componentName = component.name();	for (AlertDefinition definition : allDefinitions) {	if (componentName.equals(definition.getComponentName())) {	definitions.add(definition);	}	}	} catch (Exception exception) {	
unable to load the ambari alerts json file 

========================= ambari sample_4551 =========================

if (postMapValues == null) {	return;	}	for (String fieldName : postMapValues.keySet()) {	List<? extends PostMapValues> values = postMapValues.get(fieldName);	for (PostMapValues pmv : values) {	for (MapFieldDescriptor mapFieldDescriptor : pmv.getMappers()) {	String mapClassCode = mapFieldDescriptor.getJsonName();	Mapper mapper = (Mapper) AliasUtil.getClassInstance(mapClassCode, AliasUtil.AliasType.MAPPER);	if (mapper == null) {	
unknown mapper type 

========================= ambari sample_1753 =========================

public void prepare(Object registrationArgument) {	
preparing storm metrics reporter 

public void prepare(Object registrationArgument) {	try {	try {	hostname = InetAddress.getLocalHost().getHostName();	if ((hostname == null) || (!hostname.contains("."))) {	hostname = InetAddress.getLocalHost().getCanonicalHostName();	}	} catch (UnknownHostException e) {	
could not identify hostname 

instanceId = configuration.getProperty(INSTANCE_ID_PROPERTY);	hostInMemoryAggregationEnabled = Boolean.valueOf(configuration.getProperty(HOST_IN_MEMORY_AGGREGATION_ENABLED_PROPERTY));	hostInMemoryAggregationPort = Integer.valueOf(configuration.getProperty(HOST_IN_MEMORY_AGGREGATION_PORT_PROPERTY));	if (protocol.contains("https")) {	String trustStorePath = configuration.getProperty(SSL_KEYSTORE_PATH_PROPERTY).trim();	String trustStoreType = configuration.getProperty(SSL_KEYSTORE_TYPE_PROPERTY).trim();	String trustStorePwd = configuration.getProperty(SSL_KEYSTORE_PASSWORD_PROPERTY).trim();	loadTruststore(trustStorePath, trustStoreType, trustStorePwd);	}	} catch (Exception e) {	
could not initialize metrics collector please specify protocol host port appid zkquorum under storm home conf storm properties 

for (DataPoint dataPoint : dataPoints) {	LOG.debug(dataPoint.getName() + " = " + dataPoint.getValue());	List<DataPoint> populatedDataPoints = populateDataPoints(dataPoint);	for (DataPoint populatedDataPoint : populatedDataPoints) {	LOG.debug("Populated datapoint: " + dataPoint.getName() + " = " + dataPoint.getValue());	try {	StormAmbariMappedMetric mappedMetric = StormAmbariMappedMetric .valueOf(populatedDataPoint.getName());	TimelineMetric timelineMetric = createTimelineMetric(timestamp * 1000, applicationId, mappedMetric.getAmbariMetricName(), Double.valueOf(populatedDataPoint.getValue().toString()));	totalMetrics.add(timelineMetric);	} catch (IllegalArgumentException e) {	
not interested metrics skip 

}	}	if (totalMetrics.size() <= 0) {	return;	}	TimelineMetrics timelineMetrics = new TimelineMetrics();	timelineMetrics.setMetrics(totalMetrics);	try {	emitMetrics(timelineMetrics);	} catch (UnableToConnectException e) {	
unable to connect to metrics collector 

public void cleanup() {	
stopping storm metrics reporter 

private List<DataPoint> populateDataPoints(DataPoint dataPoint) {	List<DataPoint> dataPoints = new ArrayList<>();	if (dataPoint.getValue() == null) {	
data point with name is null discarding 

private Double convertValueToDouble(String metricName, Object value) {	try {	Double converted = NumberUtil.convertValueToDouble(value);	if (converted == null) {	
data point with name has value which is not supported discarding 

private Double convertValueToDouble(String metricName, Object value) {	try {	Double converted = NumberUtil.convertValueToDouble(value);	if (converted == null) {	}	return converted;	} catch (NumberFormatException e) {	
data point with name doesn t have number format value discarding 

========================= ambari sample_264 =========================

Entry entry = (Entry)obj;	String typeInfo = entry.getValue();	List<String> typePrecisionScale = ParserUtils.parseColumnDataType(typeInfo);	String datatype = typePrecisionScale.get(0);	String precisionString = typePrecisionScale.get(1);	String scaleString = typePrecisionScale.get(2);	Integer precision = !Strings.isNullOrEmpty(precisionString) ? Integer.valueOf(precisionString.trim()): null;	Integer scale = !Strings.isNullOrEmpty(scaleString) ? Integer.valueOf(scaleString.trim()): null;	ColumnInfo columnInfo = new ColumnInfo(entry.getName(), datatype, precision, scale, entry.getComment());	columns.add(columnInfo);	
found column definition 

========================= ambari sample_634 =========================

requestInfo.put("phase", "INITIAL_INSTALL");	requestInfo.put(AmbariManagementControllerImpl.SKIP_INSTALL_FOR_COMPONENTS, StringUtils.join (skipInstallForComponents, ";"));	requestInfo.put(AmbariManagementControllerImpl.DONT_SKIP_INSTALL_FOR_COMPONENTS, StringUtils.join (dontSkipInstallForComponents, ";"));	Request installRequest = PropertyHelper.getUpdateRequest(installProperties, requestInfo);	Predicate statePredicate = new EqualsPredicate<>(HOST_COMPONENT_STATE_PROPERTY_ID, "INIT");	Predicate clusterPredicate = new EqualsPredicate<>(HOST_COMPONENT_CLUSTER_NAME_PROPERTY_ID, cluster);	Predicate hostPredicate = new EqualsPredicate<>(HOST_COMPONENT_HOST_NAME_PROPERTY_ID, hostname);	Predicate hostAndStatePredicate = new AndPredicate(statePredicate, hostPredicate);	Predicate installPredicate = new AndPredicate(hostAndStatePredicate, clusterPredicate);	try {	
installing all components on host 

Map<String, Object> startProperties = new HashMap<>();	startProperties.put(HOST_COMPONENT_DESIRED_STATE_PROPERTY_ID, "STARTED");	Request startRequest = PropertyHelper.getUpdateRequest(startProperties, requestInfo);	Predicate installedStatePredicate = new EqualsPredicate<>(HOST_COMPONENT_DESIRED_STATE_PROPERTY_ID, "INSTALLED");	Predicate notClientPredicate = new NotPredicate(new ClientComponentPredicate());	Predicate clusterAndClientPredicate = new AndPredicate(clusterPredicate, notClientPredicate);	Predicate hostAndStatePredicate = new AndPredicate(installedStatePredicate, hostPredicate);	Predicate startPredicate;	if (installOnlyComponents.isEmpty()) {	startPredicate = new AndPredicate(clusterAndClientPredicate, hostAndStatePredicate);	
starting all non client components on host 

startPredicate = new AndPredicate(clusterAndClientPredicate, hostAndStatePredicate);	} else {	List<Predicate> listOfComponentPredicates = new ArrayList<>();	for (String installOnlyComponent : installOnlyComponents) {	Predicate componentNameEquals = new EqualsPredicate<>(HOST_COMPONENT_COMPONENT_NAME_PROPERTY_ID, installOnlyComponent);	listOfComponentPredicates.add(new NotPredicate(componentNameEquals));	}	Predicate[] arrayOfInstallOnlyPredicates = new Predicate[listOfComponentPredicates.size()];	Predicate installOnlyComponentsPredicate = new AndPredicate(listOfComponentPredicates.toArray(arrayOfInstallOnlyPredicates));	startPredicate = new AndPredicate(clusterAndClientPredicate, hostAndStatePredicate, installOnlyComponentsPredicate);	
starting all non client components on host except for the install only components specified 

if (sc.isClientComponent()) {	throw new IllegalArgumentException("Invalid arguments, cannot set maintenance state on a client component");	} else if (newMaint.equals(MaintenanceState.IMPLIED_FROM_HOST)  || newMaint.equals(MaintenanceState.IMPLIED_FROM_SERVICE)) {	throw new IllegalArgumentException("Invalid arguments, can only set maintenance state to one of " + EnumSet.of(MaintenanceState.OFF, MaintenanceState.ON));	} else {	sch.setMaintenanceState(newMaint);	}	}	}	if (newState == null) {	
nothing to do for new updateservicecomponenthost 

}	}	if (newState == null) {	continue;	}	if(!AuthorizationHelper.isAuthorized(ResourceType.CLUSTER, cluster.getResourceId(), EnumSet.of(RoleAuthorization.SERVICE_START_STOP, RoleAuthorization.SERVICE_ADD_DELETE_SERVICES, RoleAuthorization.HOST_ADD_DELETE_COMPONENTS, RoleAuthorization.HOST_ADD_DELETE_HOSTS))) {	throw new AuthorizationException("The authenticated user is not authorized to change the state of service components");	}	if (sc.isClientComponent() && newState == State.STARTED && !requestProperties.containsKey(sch.getServiceComponentName().toLowerCase())) {	ignoredScHosts.add(sch);	
ignoring servicecomponenthost as started new desired state for client components is not valid 

if (sc.isClientComponent() && newState == State.STARTED && !requestProperties.containsKey(sch.getServiceComponentName().toLowerCase())) {	ignoredScHosts.add(sch);	continue;	}	if (sc.isClientComponent() && !newState.isValidClientComponentState()) {	throw new IllegalArgumentException("Invalid desired state for a client" + " component");	}	State oldSchState = sch.getState();	if (newState == oldSchState && !sc.isClientComponent() && !requestProperties.containsKey(sch.getServiceComponentName().toLowerCase())) {	ignoredScHosts.add(sch);	
ignoring servicecomponenthost as the current state matches the new desired state 

if (sc.isClientComponent() && !newState.isValidClientComponentState()) {	throw new IllegalArgumentException("Invalid desired state for a client" + " component");	}	State oldSchState = sch.getState();	if (newState == oldSchState && !sc.isClientComponent() && !requestProperties.containsKey(sch.getServiceComponentName().toLowerCase())) {	ignoredScHosts.add(sch);	continue;	}	if (! maintenanceStateHelper.isOperationAllowed(reqOpLvl, sch)) {	ignoredScHosts.add(sch);	
ignoring servicecomponenthost as operation is not allowed 

continue;	}	if (! maintenanceStateHelper.isOperationAllowed(reqOpLvl, sch)) {	ignoredScHosts.add(sch);	continue;	}	if (! isValidStateTransition(stages, oldSchState, newState, sch)) {	throw new AmbariException("Invalid state transition for host component" + ", clusterName=" + cluster.getClusterName() + ", clusterId=" + cluster.getClusterId() + ", serviceName=" + sch.getServiceName() + ", componentName=" + sch.getServiceComponentName() + ", hostname=" + sch.getHostName() + ", currentState=" + oldSchState + ", newDesiredState=" + newState);	}	if (isDirectTransition(oldSchState, newState)) {	
handling direct transition update to host component 

}	if (isDirectTransition(oldSchState, newState)) {	directTransitionScHosts.put(sch, newState);	} else {	if (!changedScHosts.containsKey(sc.getName())) {	changedScHosts.put(sc.getName(), new EnumMap<>(State.class));	}	if (!changedScHosts.get(sc.getName()).containsKey(newState)) {	changedScHosts.get(sc.getName()).put(newState, new ArrayList<>());	}	
handling update to host component 

String msg = String.format("Skipping updating hosts: no matching requests for %s", predicate);	LOG.info(msg);	throw new NoSuchResourceException(msg);	}	RequestStageContainer requestStages = modifyResources(new Command<RequestStageContainer>() {	public RequestStageContainer invoke() throws AmbariException {	RequestStageContainer stageContainer = null;	try {	stageContainer = updateHostComponents(stages, requests, request.getRequestInfoProperties(), runSmokeTest);	} catch (Exception e) {	
caught an exception while updating host components will not try again 

========================= ambari sample_3555 =========================

HelpFormatter formatter = new HelpFormatter();	DateFormat df = new SimpleDateFormat(DATE_PATTERN);	CleanupContext ctx = null;	try {	CommandLine line = cmdLineParser.parse(getOptions(), args);	String clusterName = (String) line.getParsedOptionValue(CLUSTER_NAME_ARG);	Date fromDate = df.parse(line.getOptionValue(FROM_DATE_ARG));	ctx = new CleanupContext(clusterName, fromDate.getTime());	} catch (Exception exp) {	System.err.println("Parsing failed.  Reason: " + exp.getMessage());	
parsing failed reason 

public static void main(String... args) throws Exception {	
db purge starting the database purge process 

public static void main(String... args) throws Exception {	CleanupContext cleanupContext = processArguments(args);	Injector injector = Guice.createInjector(new ControllerModule(), new AuditLoggerModule(), new CleanupModule());	injector.getInstance(AmbariJpaPersistService.class).start();	CleanupServiceImpl cleanupService = injector.getInstance(CleanupServiceImpl.class);	CleanupService.CleanupResult result = cleanupService.cleanup(new TimeBasedCleanupPolicy(cleanupContext.getClusterName(), cleanupContext.getFromDayTimestamp()));	injector.getInstance(AmbariJpaPersistService.class).stop();	if (result.getErrorCount() > 0) {	
db purge completed with error check ambari server log for details number of affected records 

public static void main(String... args) throws Exception {	CleanupContext cleanupContext = processArguments(args);	Injector injector = Guice.createInjector(new ControllerModule(), new AuditLoggerModule(), new CleanupModule());	injector.getInstance(AmbariJpaPersistService.class).start();	CleanupServiceImpl cleanupService = injector.getInstance(CleanupServiceImpl.class);	CleanupService.CleanupResult result = cleanupService.cleanup(new TimeBasedCleanupPolicy(cleanupContext.getClusterName(), cleanupContext.getFromDayTimestamp()));	injector.getInstance(AmbariJpaPersistService.class).stop();	if (result.getErrorCount() > 0) {	System.exit(2);	}	
db purge completed number of affected records 

========================= ambari sample_3806 =========================

while (shouldRun) {	try {	if (shouldSendRegistration) {	sendRegistration();	} else if (numberOfHeartbeats > 0 && (currentHeartbeatCount < numberOfHeartbeats)) {	sendHeartBeat();	}	Thread.sleep(sleepTime);	} catch (InterruptedException e) {	} catch (Exception ex) {	
exception received 

reg = new Register();	reg.setTimestamp(System.currentTimeMillis());	reg.setHostname(this.hostname);	reg.setAgentVersion(this.agentVersion);	reg.setPrefix(Configuration.PREFIX_DIR);	}	RegistrationResponse response;	try {	response = handler.handleRegistration(reg);	} catch (AmbariException | InvalidStateTransitionException e) {	
registration failed 

========================= ambari sample_1991 =========================

private void loadServiceConfigTypes() throws AmbariException {	try {	serviceConfigTypes = collectServiceConfigTypesMapping();	} catch (AmbariException e) {	
cannot load stack info 

private void loadServiceConfigTypes() throws AmbariException {	try {	serviceConfigTypes = collectServiceConfigTypesMapping();	} catch (AmbariException e) {	throw e;	}	
service config types loaded 

private Multimap<String, String> collectServiceConfigTypesMapping() throws AmbariException {	Multimap<String, String> serviceConfigTypes = HashMultimap.create();	Map<String, ServiceInfo> serviceInfoMap = null;	try {	serviceInfoMap = ambariMetaInfo.getServices(desiredStackVersion.getStackName(), desiredStackVersion.getStackVersion());	} catch (ParentObjectNotFoundException e) {	
service config versioning disabled due to exception 

public Long getResourceId() {	ClusterEntity clusterEntity = getClusterEntity();	ResourceEntity resourceEntity = clusterEntity.getResource();	if (resourceEntity == null) {	
there is no resource associated with this cluster tcluster name tcluster id 

public void addServiceComponentHost(ServiceComponentHost svcCompHost) throws AmbariException {	if (LOG.isDebugEnabled()) {	
trying to add component of service on to the cache 

public void removeServiceComponentHost(ServiceComponentHost svcCompHost) throws AmbariException {	if (LOG.isDebugEnabled()) {	
trying to remove component of service on from the cache 

clusterStateDAO.create(clusterStateEntity);	clusterStateEntity = clusterStateDAO.merge(clusterStateEntity);	clusterEntity.setClusterStateEntity(clusterStateEntity);	clusterEntity = clusterDAO.merge(clusterEntity);	} else {	clusterStateEntity.setCurrentStack(stackEntity);	clusterStateEntity = clusterStateDAO.merge(clusterStateEntity);	clusterEntity = clusterDAO.merge(clusterEntity);	}	} catch (RollbackException e) {	
unable to set version for cluster 

try {	Map<String, Set<DesiredConfig>> map = new HashMap<>();	Collection<String> types = new HashSet<>();	Collection<ClusterConfigEntity> entities = getClusterEntity().getClusterConfigEntities();	for (ClusterConfigEntity configEntity : entities) {	if (allVersions || configEntity.isSelected()) {	DesiredConfig desiredConfig = new DesiredConfig();	desiredConfig.setServiceName(null);	desiredConfig.setTag(configEntity.getTag());	if (!allConfigs.containsKey(configEntity.getType())) {	
an inconsistency exists for configuration 

for (ClusterConfigEntity configEntity : entities) {	if (allVersions || configEntity.isSelected()) {	DesiredConfig desiredConfig = new DesiredConfig();	desiredConfig.setServiceName(null);	desiredConfig.setTag(configEntity.getTag());	if (!allConfigs.containsKey(configEntity.getType())) {	continue;	}	Map<String, Config> configMap = allConfigs.get(configEntity.getType());	if(!configMap.containsKey(configEntity.getTag())) {	
an inconsistency exists for the configuration with tag 

failedEvents.put(event, message);	} catch (ServiceComponentHostNotFoundException e) {	String message = String.format( "ServiceComponentHost lookup exception. Service Component Host not found for Service: %s, Component: %s, Host: %s. Error: %s", serviceName, serviceComponentName, event.getHostName(), e.getMessage());	LOG.error(message);	failedEvents.put(event, message);	} catch (AmbariException e) {	String message = String.format("ServiceComponentHost lookup exception %s", e.getMessage());	LOG.error(message);	failedEvents.put(event, message);	} catch (InvalidStateTransitionException e) {	
invalid transition 

} catch (AmbariException e) {	String message = String.format("ServiceComponentHost lookup exception %s", e.getMessage());	LOG.error(message);	failedEvents.put(event, message);	} catch (InvalidStateTransitionException e) {	boolean isFailure = true;	Enum<?> currentState = e.getCurrentState();	Enum<?> failedEvent = e.getEvent();	if (currentState == State.STARTED && failedEvent == ServiceComponentHostEventType.HOST_SVCCOMP_START){	isFailure = false;	
the start request for is invalid since the component is already started ignoring the request 

failedEvents.put(event, message);	} catch (InvalidStateTransitionException e) {	boolean isFailure = true;	Enum<?> currentState = e.getCurrentState();	Enum<?> failedEvent = e.getEvent();	if (currentState == State.STARTED && failedEvent == ServiceComponentHostEventType.HOST_SVCCOMP_START){	isFailure = false;	}	if (currentState == State.UNKNOWN && failedEvent == ServiceComponentHostEventType.HOST_SVCCOMP_OP_IN_PROGRESS) {	isFailure = false;	
the host is in an unknown state attempting to put back in progress 

ClusterEntity clusterEntity = getClusterEntity();	Collection<ClusterConfigEntity> configEntities = clusterEntity.getClusterConfigEntities();	ImmutableMap<Object, ClusterConfigEntity> clusterConfigEntityMap = Maps.uniqueIndex( configEntities, Functions.identity());	Set<String> configTypesForService = new HashSet<>();	List<ServiceConfigEntity> latestServiceConfigs = serviceConfigDAO.getLastServiceConfigsForService( getClusterId(), serviceName);	for (ServiceConfigEntity serviceConfig : latestServiceConfigs) {	List<ClusterConfigEntity> latestConfigs = serviceConfig.getClusterConfigEntities();	for( ClusterConfigEntity latestConfig : latestConfigs ){	latestConfig = clusterConfigEntityMap.get(latestConfig);	configTypesForService.add(latestConfig.getType());	
disabling configuration with tag 

latestConfig.setSelected(false);	}	}	Collection<ClusterConfigEntity> latestConfigsByStack = clusterDAO.getLatestConfigurations( clusterId, stackId);	for (ClusterConfigEntity latestConfigByStack : latestConfigsByStack) {	if (!configTypesForService.contains(latestConfigByStack.getType())) {	continue;	}	ClusterConfigEntity entity = clusterConfigEntityMap.get(latestConfigByStack);	entity.setSelected(true);	
setting with version tag created on to selected for stack 

Collection<ClusterConfigEntity> latestConfigsByStack = clusterDAO.getLatestConfigurations( clusterId, stackId);	for (ClusterConfigEntity latestConfigByStack : latestConfigsByStack) {	if (!configTypesForService.contains(latestConfigByStack.getType())) {	continue;	}	ClusterConfigEntity entity = clusterConfigEntityMap.get(latestConfigByStack);	entity.setSelected(true);	}	clusterEntity = clusterDAO.merge(clusterEntity, true);	cacheConfigurations();	
applied latest configurations for on stack the the following types were modified 

public void addSuspendedUpgradeParameters(Map<String, String> commandParams, Map<String, String> roleParams) {	UpgradeEntity suspendedUpgrade = getUpgradeInProgress();	if( null == suspendedUpgrade ){	
an upgrade is not currently suspended the command and role parameters will not be modified 

========================= ambari sample_3040 =========================

public void componentRemoved(ServiceComponentUninstalledEvent event) throws KerberosMissingAdminCredentialsException {	try {	
removing identities after 

public void componentRemoved(ServiceComponentUninstalledEvent event) throws KerberosMissingAdminCredentialsException {	try {	RemovableIdentities .ofComponent(clusters.getCluster(event.getClusterId()), event, kerberosHelper) .remove(kerberosHelper);	} catch (Exception e) {	
error while deleting kerberos identity after an event 

public void serviceRemoved(ServiceRemovedEvent event) {	try {	
removing identities after 

public void serviceRemoved(ServiceRemovedEvent event) {	try {	RemovableIdentities .ofService(clusters.getCluster(event.getClusterId()), event, kerberosHelper) .remove(kerberosHelper);	} catch (Exception e) {	
error while deleting kerberos identity after an event 

========================= ambari sample_3693 =========================

public void close() {	
closing file 

========================= ambari sample_1641 =========================

boolean currentResult = true;	File[] files = file.listFiles();	if (files != null) {	for (File currentFile : files) {	currentResult = visitFile(schema, currentFile, previousResult) && currentResult;	}	}	return previousResult && currentResult;	} else if (file.isFile()) {	if (PATTERN_KERBEROS_DESCRIPTOR_FILENAME.matcher(file.getName()).matches()) {	
validating 

========================= ambari sample_2597 =========================

public void run() {	
checking if metrics cache is empty 

========================= ambari sample_376 =========================

List<String> jobIds = FluentIterable.from(jobInfos).filter(new Predicate<JobInfo>() {	public boolean apply(@Nullable JobInfo input) {	return !Strings.isNullOrEmpty(input.getJobId());	}	}).transform(new Function<JobInfo, String>() {	public String apply(@Nullable JobInfo input) {	return input.getJobId();	}	}).toList();	List<Job> dbJobs = readJobsFromDbByJobId(jobIds);	
readjobsbyids dbjobs 

private List<Job> readJobsFromDbByJobId(final List<String> jobsIds) {	
reading jobs from db with ids 

private List<Job> readJobsFromDbByJobId(final List<String> jobsIds) {	List<Job> jobs = viewJobResourceManager.readAll(new FilteringStrategy() {	public boolean isConform(Indexed item) {	JobImpl job = (JobImpl) item;	return jobsIds.contains(job.getId());	}	public String whereStatement() {	String query = " id in ( " + Joiner.on(",").join(jobsIds) + " ) ";	
where clause for jobsids 

List<Job> jobs = viewJobResourceManager.readAll(new FilteringStrategy() {	public boolean isConform(Indexed item) {	JobImpl job = (JobImpl) item;	return jobsIds.contains(job.getId());	}	public String whereStatement() {	String query = " id in ( " + Joiner.on(",").join(jobsIds) + " ) ";	return query;	}	});	
jobs returned from db 

}	if( null != endTime ){	if(null != startTime){	sb.append(" AND ");	}	sb.append(" dateSubmitted < ").append(endTime);	}	sb.append( " ) " );	}	String where = sb.toString();	
where statement 

sb.append(" AND ");	}	sb.append(" dateSubmitted < ").append(endTime);	}	sb.append( " ) " );	}	String where = sb.toString();	return where;	}	});	
returning jobs 

private List<Job> fetchDagsAndMergeJobs(List<HiveQueryId> queries) {	List<Job> allJobs = new LinkedList<Job>();	for (HiveQueryId atsHiveQuery : queries) {	JobImpl atsJob = null;	if (hasOperationId(atsHiveQuery)) {	try {	Job viewJob = getJobByOperationId(atsHiveQuery.operationId);	TezDagId atsTezDag = getTezDagFromHiveQueryId(atsHiveQuery);	atsJob = mergeHiveAtsTez(atsHiveQuery, atsTezDag, viewJob);	} catch (ItemNotFound itemNotFound) {	
ignore 

protected JobImpl mergeAtsJobWithViewJob(HiveQueryId atsHiveQuery, TezDagId atsTezDag, Job viewJob) {	JobImpl atsJob;	try {	atsJob = new JobImpl(PropertyUtils.describe(viewJob));	} catch (IllegalAccessException e) {	
can t instantiate jobimpl 

protected JobImpl mergeAtsJobWithViewJob(HiveQueryId atsHiveQuery, TezDagId atsTezDag, Job viewJob) {	JobImpl atsJob;	try {	atsJob = new JobImpl(PropertyUtils.describe(viewJob));	} catch (IllegalAccessException e) {	return null;	} catch (InvocationTargetException e) {	
can t instantiate jobimpl 

protected JobImpl mergeAtsJobWithViewJob(HiveQueryId atsHiveQuery, TezDagId atsTezDag, Job viewJob) {	JobImpl atsJob;	try {	atsJob = new JobImpl(PropertyUtils.describe(viewJob));	} catch (IllegalAccessException e) {	return null;	} catch (InvocationTargetException e) {	return null;	} catch (NoSuchMethodException e) {	
can t instantiate jobimpl 

updateDb = true;	}	}	if (tezDagId.status != null && (tezDagId.status.compareToIgnoreCase(Job.JOB_STATE_UNKNOWN) != 0) && !viewJob.getStatus().equalsIgnoreCase(tezDagId.status)) {	dagId = tezDagId.entity;	applicationId = tezDagId.applicationId;	updateDb = true;	}	if(updateDb) {	if (useActorSystem) {	
saving dag information via actor system for job id 

========================= ambari sample_862 =========================

public void collect(Entry entry) {	for (AttributeDetector detector : detectors) {	
collecting information for the detector 

public Map<String, String> detect() {	Map<String, String> detectedAttributes = Maps.newHashMap();	for (AttributeDetector detector : detectors) {	
detecting ldap configuration value using the detector 

========================= ambari sample_2915 =========================

public List<Policy> getPolicies(String database, String table) {	if (context.getCluster() == null) {	return getPoliciesFromNonAmbariCluster(database, table);	} else {	if (!authChecker.isOperator()) {	
user is not authorized to access the table authorization information 

private List<Policy> getPoliciesFromAmbariCluster(String database, String table) {	if (!isHiveRangerPluginEnabled()) {	
ranger authorization is not enabled for hive 

private List<Policy> getPoliciesFromAmbariCluster(String database, String table) {	if (!isHiveRangerPluginEnabled()) {	throw new RangerException("Ranger authorization is not enabled for Hive", "CONFIGURATION_ERROR", 500);	}	String rangerUrl = null;	try {	rangerUrl = getRangerUrlFromAmbari();	} catch (AmbariHttpException e) {	
failed to fetch ranger url from ambari exception 

if (!isHiveRangerPluginEnabled()) {	throw new RangerException("Ranger authorization is not enabled for Hive", "CONFIGURATION_ERROR", 500);	}	String rangerUrl = null;	try {	rangerUrl = getRangerUrlFromAmbari();	} catch (AmbariHttpException e) {	throw new RangerException("Failed to fetch Ranger URL from Ambari", "AMBARI_FETCH_FAILED", 500, e);	}	if (StringUtils.isEmpty(rangerUrl)) {	
ranger url is not configured for the instance 

private List<Policy> getPoliciesFromNonAmbariCluster(String database, String table) {	String rangerUrl = getRangerUrlFromConfig();	if (StringUtils.isEmpty(rangerUrl)) {	
ranger url is not configured for the instance 

private List<Policy> getPoliciesFromRanger(String rangerUrl, String database, String table) {	RangerCred cred = getRangerCredFromConfig();	if (!cred.isValid()) {	
ranger username and password are not configured 

private List<Policy> parseResponse(String rangerResponse) {	Object parsedResult = JSONValue.parse(rangerResponse);	if (parsedResult instanceof JSONObject) {	JSONObject obj = (JSONObject) parsedResult;	
bad response from ranger 

private String fetchResponseFromRanger(String rangerUrl, String username, String password, String database, String table) {	String serviceName = context.getProperties().get("hive.ranger.servicename");	if (StringUtils.isEmpty(serviceName)) {	
bad service name configured 

private String fetchResponseFromRanger(String rangerUrl, String username, String password, String database, String table) {	String serviceName = context.getProperties().get("hive.ranger.servicename");	if (StringUtils.isEmpty(serviceName)) {	throw new RangerException("Ranger service name is not configured in Ambari Instance.", "CONFIGURATION_ERROR", 500);	}	Map<String, String> headers = getRangerHeaders(username, password);	StringBuilder urlBuilder = getRangerUrl(rangerUrl, database, table, serviceName);	try {	InputStream stream = context.getURLStreamProvider().readFrom(urlBuilder.toString(), "GET", (String) null, headers);	if (stream == null) {	
ranger returned an empty stream 

}	Map<String, String> headers = getRangerHeaders(username, password);	StringBuilder urlBuilder = getRangerUrl(rangerUrl, database, table, serviceName);	try {	InputStream stream = context.getURLStreamProvider().readFrom(urlBuilder.toString(), "GET", (String) null, headers);	if (stream == null) {	throw new RangerException("Ranger returned an empty stream.", "RANGER_ERROR", 500);	}	return IOUtils.toString(stream);	} catch (IOException e) {	
bad response from ranger exception 

public String getRangerUrlFromAmbari() throws AmbariHttpException {	AmbariApi ambariApi = new AmbariApi(context);	String url = String.format(RANGER_CONFIG_URL, context.getCluster().getName());	String config = ambariApi.readFromAmbari(url, "GET", null, null);	JSONObject configJson = (JSONObject) JSONValue.parse(config);	JSONArray itemsArray = (JSONArray) configJson.get("items");	if (itemsArray.size() == 0) {	
ranger service is not enabled in ambari 

========================= ambari sample_566 =========================

if (keyObject != null) {	String key = keyObject.toString();	if (key.endsWith("/host_name")) {	if (expression.getRightOperand() != null) {	expression.setRightOperand(expression.getRightOperand().toString().toLowerCase());	}	}	}	}	} catch(Exception e) {	
lowercase host name value in expression failed with error 

========================= ambari sample_3980 =========================

public  synchronized BSResponse runBootStrap(SshHostInfo info) {	BSResponse response = new BSResponse();	
bootstrapping hosts 

========================= ambari sample_2738 =========================

public static String findUnallocatedFileName(HdfsApi hdfs, String fullPathAndFilename, String extension) throws HdfsApiException {	int triesCount = 0;	String newFilePath;	boolean isUnallocatedFilenameFound;	try {	do {	newFilePath = String.format(fullPathAndFilename + "%s" + extension, (triesCount == 0) ? "" : "_" + triesCount);	
trying to find free filename 

public static String findUnallocatedFileName(HdfsApi hdfs, String fullPathAndFilename, String extension) throws HdfsApiException {	int triesCount = 0;	String newFilePath;	boolean isUnallocatedFilenameFound;	try {	do {	newFilePath = String.format(fullPathAndFilename + "%s" + extension, (triesCount == 0) ? "" : "_" + triesCount);	isUnallocatedFilenameFound = !hdfs.exists(newFilePath);	if (isUnallocatedFilenameFound) {	
file created successfully 

private static HdfsApi getHdfsApi(ViewContext context, ConfigurationBuilder configurationBuilder) throws HdfsApiException {	HdfsApi api = null;	AuthConfigurationBuilder authConfigurationBuilder = new AuthConfigurationBuilder(context);	Map<String, String> authParams = authConfigurationBuilder.build();	configurationBuilder.setAuthParams(authParams);	try {	api = new HdfsApi(configurationBuilder, getHdfsUsername(context));	
hdfsapi connected ok 

private static HdfsApi getHdfsApi(ViewContext context, ConfigurationBuilder configurationBuilder) throws HdfsApiException {	HdfsApi api = null;	AuthConfigurationBuilder authConfigurationBuilder = new AuthConfigurationBuilder(context);	Map<String, String> authParams = authConfigurationBuilder.build();	configurationBuilder.setAuthParams(authParams);	try {	api = new HdfsApi(configurationBuilder, getHdfsUsername(context));	} catch (IOException e) {	
exception occurred while creating hdfsapi objcet 

AuthConfigurationBuilder authConfigurationBuilder = new AuthConfigurationBuilder(context);	Map<String, String> authParams = authConfigurationBuilder.build();	configurationBuilder.setAuthParams(authParams);	try {	api = new HdfsApi(configurationBuilder, getHdfsUsername(context));	} catch (IOException e) {	String message = "HDFS040 Couldn't open connection to HDFS";	LOG.error(message);	throw new HdfsApiException(message, e);	} catch (InterruptedException e) {	
exception occurred while creating hdfsapi objcet 

========================= ambari sample_1114 =========================

result  = request.process();	if(ResultStatus.STATUS.OK.equals(result.getStatus().getStatus())) {	requestAuditLogger.log(request, result);	}	}	if(requestBodySet.isEmpty() || !ResultStatus.STATUS.OK.equals(result.getStatus().getStatus())) {	requestAuditLogger.log(request, result);	}	} catch (BodyParseException e) {	result =  new ResultImpl(new ResultStatus(ResultStatus.STATUS.BAD_REQUEST, e.getMessage()));	
bad request received 

========================= ambari sample_4195 =========================

protected synchronized void initializeScheduler() {	StdSchedulerFactory sf = new StdSchedulerFactory();	Properties properties = getQuartzSchedulerProperties();	try {	sf.initialize(properties);	} catch (SchedulerException e) {	
failed to initialize request execution scheduler properties 

protected synchronized void initializeScheduler() {	StdSchedulerFactory sf = new StdSchedulerFactory();	Properties properties = getQuartzSchedulerProperties();	try {	sf.initialize(properties);	} catch (SchedulerException e) {	
scheduler properties 

sf.initialize(properties);	} catch (SchedulerException e) {	e.printStackTrace();	return;	}	try {	scheduler = sf.getScheduler();	scheduler.setJobFactory(guiceJobFactory);	isInitialized = true;	} catch (SchedulerException e) {	
failed to create request execution scheduler 

properties.setProperty("org.quartz.jobStore.useProperties", "false");	properties.setProperty("org.quartz.jobStore.dataSource", "myDS");	properties.setProperty("org.quartz.dataSource.myDS.driver", configuration.getDatabaseDriver());	properties.setProperty("org.quartz.dataSource.myDS.URL", configuration.getDatabaseUrl());	properties.setProperty("org.quartz.dataSource.myDS.user", configuration.getDatabaseUser());	properties.setProperty("org.quartz.dataSource.myDS.password", configuration.getDatabasePassword());	properties.setProperty("org.quartz.dataSource.myDS.maxConnections", configuration.getExecutionSchedulerConnections());	properties.setProperty("org.quartz.dataSource.myDS.maxCachedStatementsPerConnection", configuration.getExecutionSchedulerMaxStatementsPerConnection());	properties.setProperty("org.quartz.dataSource.myDS.validationQuery", subProps[1]);	properties.setProperty("org.quartz.scheduler.skipUpdateCheck", "true");	
using quartz properties 

throw new AmbariException(msg);	}	try {	if (!scheduler.isStarted()) {	if (delay != null) {	scheduler.startDelayed(delay);	} else {	scheduler.start();	}	} else {	
scheduler already started skipping start 

try {	if (!scheduler.isStarted()) {	if (delay != null) {	scheduler.startDelayed(delay);	} else {	scheduler.start();	}	} else {	}	} catch (SchedulerException e) {	
failed to start scheduler 

public synchronized void stopScheduler() throws AmbariException {	if (scheduler == null) {	throw new AmbariException("Scheduler not instantiated !");	}	try {	scheduler.shutdown();	} catch (SchedulerException e) {	
failed to stop scheduler 

========================= ambari sample_2942 =========================

public Authentication authenticate(Authentication authentication) throws AuthenticationException {	if (!authPropsConfig.isAuthExternalEnabled()) {	
external server auth is disabled 

}	if (StringUtils.isBlank(password)) {	throw new BadCredentialsException("Password can't be null or empty.");	}	password = StringEscapeUtils.unescapeHtml(password);	username = StringEscapeUtils.unescapeHtml(username);	try {	String finalLoginUrl = authPropsConfig.getExternalAuthLoginUrl().replace("$USERNAME", username);	String responseObj = (String) externalServerClient.sendGETRequest(finalLoginUrl, String.class, username, password);	if (!isAllowedRole(responseObj)) {	
doesn t have permission 

}	password = StringEscapeUtils.unescapeHtml(password);	username = StringEscapeUtils.unescapeHtml(username);	try {	String finalLoginUrl = authPropsConfig.getExternalAuthLoginUrl().replace("$USERNAME", username);	String responseObj = (String) externalServerClient.sendGETRequest(finalLoginUrl, String.class, username, password);	if (!isAllowedRole(responseObj)) {	throw new BadCredentialsException("Invalid User");	}	} catch (Exception e) {	
login failed for username error 

========================= ambari sample_1397 =========================

public Authentication attemptAuthentication(HttpServletRequest request, HttpServletResponse response) throws AuthenticationException, IOException, ServletException {	if (StringUtils.isEmpty(getProvidedUrl())) {	throw new BadCredentialsException("Authentication provider URL must not be null or empty.");	}	if (StringUtils.isEmpty(getPublicKey())) {	throw new BadCredentialsException("Public key for signature validation must be provisioned.");	}	try {	Claims claims = Jwts .parser() .setSigningKey(parseRSAPublicKey(getPublicKey())) .parseClaimsJws(getJWTFromCookie(request)) .getBody();	String userName  = claims.getSubject();	
username 

LOG.info("URL = " + request.getRequestURL());	if (StringUtils.isNotEmpty(claims.getAudience()) && !getAudiences().contains(claims.getAudience())) {	throw new IllegalArgumentException(String.format("Audience validation failed. (Not found: %s)", claims.getAudience()));	}	Authentication authentication = new JWTAuthenticationToken(userName, getPublicKey(), getAuthorities());	authentication.setAuthenticated(true);	SecurityContextHolder.getContext().setAuthentication(authentication);	return authentication;	} catch (ExpiredJwtException | MalformedJwtException | SignatureException | IllegalArgumentException e) {	LOG.info("URL = " + request.getRequestURL());	
error during jwt authentication 

private String getJWTFromCookie(HttpServletRequest req) {	String serializedJWT = null;	Cookie[] cookies = req.getCookies();	if (cookies != null) {	for (Cookie cookie : cookies) {	if (getCookieName().equals(cookie.getName())) {	
cookie has been found and is being processed 

========================= ambari sample_1378 =========================

private static void exit(int exitCode, String message) {	if (message != null){	LOG.error(message);	}	
return code 

========================= ambari sample_120 =========================

public void removeNoticeByDefinitionId(long definitionId) {	
deleting alertnotice entities by definition id 

EntityManager entityManager = entityManagerProvider.get();	TypedQuery<Integer> historyIdQuery = entityManager.createNamedQuery( "AlertHistoryEntity.findHistoryIdsByDefinitionId", Integer.class);	historyIdQuery.setParameter("definitionId", definitionId);	List<Integer> ids = daoUtils.selectList(historyIdQuery);	int BATCH_SIZE = 999;	TypedQuery<AlertNoticeEntity> noticeQuery = entityManager.createNamedQuery( "AlertNoticeEntity.removeByHistoryIds", AlertNoticeEntity.class);	if (ids != null && !ids.isEmpty()) {	for (int i = 0; i < ids.size(); i += BATCH_SIZE) {	int endIndex = (i + BATCH_SIZE) > ids.size() ? ids.size() : (i + BATCH_SIZE);	List<Integer> idsSubList = ids.subList(i, endIndex);	
deleting alertnotice entity batch with history ids 

========================= ambari sample_4245 =========================

public T call() throws Exception {	long startTime = System.currentTimeMillis();	long timeLeft = timeout;	Future<T> future = executorService.submit(task);	
task execution started at 

public T call() throws Exception {	long startTime = System.currentTimeMillis();	long timeLeft = timeout;	Future<T> future = executorService.submit(task);	while (true) {	Throwable lastError;	try {	
task waiting for result at most ms 

public T call() throws Exception {	long startTime = System.currentTimeMillis();	long timeLeft = timeout;	Future<T> future = executorService.submit(task);	while (true) {	Throwable lastError;	try {	T taskResult = future.get(timeLeft, TimeUnit.MILLISECONDS);	
task successfully completed with result 

public T call() throws Exception {	long startTime = System.currentTimeMillis();	long timeLeft = timeout;	Future<T> future = executorService.submit(task);	while (true) {	Throwable lastError;	try {	T taskResult = future.get(timeLeft, TimeUnit.MILLISECONDS);	return taskResult;	} catch (TimeoutException e) {	
task timeout 

Throwable lastError;	try {	T taskResult = future.get(timeLeft, TimeUnit.MILLISECONDS);	return taskResult;	} catch (TimeoutException e) {	lastError = e;	timeLeft = 0;	} catch (ExecutionException e) {	Throwable cause = Throwables.getRootCause(e);	if (!(cause instanceof RetryTaskSilently)) {	
task s exception during execution 

timeLeft = 0;	} catch (ExecutionException e) {	Throwable cause = Throwables.getRootCause(e);	if (!(cause instanceof RetryTaskSilently)) {	}	lastError = cause;	timeLeft = timeout - (System.currentTimeMillis() - startTime);	}	if (timeLeft < retryDelay) {	attemptToCancel(future);	
task timeout exceeded no more retries 

if (!(cause instanceof RetryTaskSilently)) {	}	lastError = cause;	timeLeft = timeout - (System.currentTimeMillis() - startTime);	}	if (timeLeft < retryDelay) {	attemptToCancel(future);	onError.accept(lastError);	return null;	}	
task retrying execution in milliseconds 

private void attemptToCancel(Future<?> future) {	
task timeout exceeded cancelling 

private void attemptToCancel(Future<?> future) {	if (!future.isDone() && future.cancel(true)) {	
task cancelled 

private void attemptToCancel(Future<?> future) {	if (!future.isDone() && future.cancel(true)) {	} else {	
task already done 

========================= ambari sample_2684 =========================

public void resolve( StackModule parentModule, Map<String, StackModule> allStacks, Map<String, ServiceModule> commonServices, Map<String, ExtensionModule> extensions) throws AmbariException {	moduleState = ModuleState.VISITED;	
resolve s s 

private void mergeServiceWithExplicitParent( ServiceModule service, String parent, Map<String, StackModule> allStacks, Map<String, ServiceModule> commonServices, Map<String, ExtensionModule> extensions) throws AmbariException {	
merge service s with explicit parent s 

for (ServiceModule module : serviceModules.values()) {	File upgradesFolder = module.getModuleInfo().getServiceUpgradesFolder();	if (upgradesFolder != null) {	UpgradePack servicePack = getServiceUpgradePack(pack, upgradesFolder);	if (servicePack != null) {	servicePacks.add(servicePack);	}	}	}	if (servicePacks.size() > 0) {	
merging service specific upgrades for pack 

private UpgradePack getServiceUpgradePack(UpgradePack pack, File upgradesFolder) throws AmbariException {	File stackFolder = new File(upgradesFolder, stackInfo.getName());	File versionFolder = new File(stackFolder, stackInfo.getVersion());	File servicePackFile = new File(versionFolder, pack.getName() + ".xml");	
service folder 

List<Grouping> originalGroups = pack.getAllGroups();	Map<String, List<Grouping>> allGroupMap = new HashMap<>();	for (Grouping group : originalGroups) {	List<Grouping> list = new ArrayList<>();	list.add(group);	allGroupMap.put(group.name, list);	}	for (UpgradePack servicePack : servicePacks) {	for (Grouping group : servicePack.getAllGroups()) {	!!! special case where the service pack is targeted for any version.  When a service UP targets to run after another group, check to make sure that the base UP contains the group. if (servicePack.isAllTarget() && !allGroupMap.keySet().contains(group.addAfterGroup)) {	
service upgrade pack specified after group of but that is not found in 

private boolean addGrouping(List<Grouping> groups, Grouping group) throws AmbariException {	if (group.addAfterGroup == null) {	throw new AmbariException("Group " + group.name + " needs to specify which group it should come after");	}	else {	for (int index = groups.size() - 1; index >= 0; index--) {	String name = groups.get(index).name;	if (name.equals(group.addAfterGroup)) {	groups.add(index + 1, group);	
added group after 

if (0 == upgradeFiles.length) {	return null;	}	for (File f : upgradeFiles) {	try {	UpgradePack upgradePack = unmarshaller.unmarshal(UpgradePack.class, f);	if (upgradePack.isAllTarget() && upgradePack.getType() == base.getType()) {	return upgradePack;	}	} catch (Exception e) {	
file does not appear to be an upgrade pack and will be skipped 

private void processRepositories() throws AmbariException {	List<RepositoryInfo> stackRepos = Collections.emptyList();	RepositoryXml rxml = stackDirectory.getRepoFile();	if (null != rxml) {	stackInfo.setRepositoryXml(rxml);	LOG.debug("Adding repositories to stack, stackName={}, stackVersion={}, repoFolder={}", stackInfo.getName(), stackInfo.getVersion(), stackDirectory.getRepoDir());	stackRepos = rxml.getRepositories();	stackInfo.getRepositories().addAll(stackRepos);	}	
process service custom repositories 

private void registerRepoUpdateTask(RepositoryXml rxml) {	String latest = rxml.getLatestURI();	if (StringUtils.isBlank(latest)) {	return;	}	URI uri = getURI(this, latest);	if (null == uri) {	
could not determine how to load stack latest definition for 

Set<RepositoryInfo> uniqueServiceRepos = new HashSet<>();	for (String osType: serviceReposByOsType.keySet()) {	List<RepositoryInfo> stackReposForOsType = stackReposByOsType.containsKey(osType) ? stackReposByOsType.get(osType) : Collections.emptyList();	List<RepositoryInfo> serviceReposForOsType = serviceReposByOsType.get(osType);	Set<String> stackRepoNames = ImmutableSet.copyOf(Lists.transform(stackReposForOsType, RepositoryInfo.GET_REPO_NAME_FUNCTION));	Set<String> stackRepoUrls = ImmutableSet.copyOf(Lists.transform(stackReposForOsType, RepositoryInfo.SAFE_GET_BASE_URL_FUNCTION));	Set<String> duplicateServiceRepoNames = findDuplicates(serviceReposForOsType, RepositoryInfo.GET_REPO_NAME_FUNCTION);	Set<String> duplicateServiceRepoUrls = findDuplicates(serviceReposForOsType, RepositoryInfo.SAFE_GET_BASE_URL_FUNCTION);	for (RepositoryInfo repo: serviceReposForOsType) {	if (stackRepoUrls.contains(repo.getBaseUrl())) {	
service repo has a base url that is identical to that of a stack repo 

List<RepositoryInfo> stackReposForOsType = stackReposByOsType.containsKey(osType) ? stackReposByOsType.get(osType) : Collections.emptyList();	List<RepositoryInfo> serviceReposForOsType = serviceReposByOsType.get(osType);	Set<String> stackRepoNames = ImmutableSet.copyOf(Lists.transform(stackReposForOsType, RepositoryInfo.GET_REPO_NAME_FUNCTION));	Set<String> stackRepoUrls = ImmutableSet.copyOf(Lists.transform(stackReposForOsType, RepositoryInfo.SAFE_GET_BASE_URL_FUNCTION));	Set<String> duplicateServiceRepoNames = findDuplicates(serviceReposForOsType, RepositoryInfo.GET_REPO_NAME_FUNCTION);	Set<String> duplicateServiceRepoUrls = findDuplicates(serviceReposForOsType, RepositoryInfo.SAFE_GET_BASE_URL_FUNCTION);	for (RepositoryInfo repo: serviceReposForOsType) {	if (stackRepoUrls.contains(repo.getBaseUrl())) {	}	else if (duplicateServiceRepoUrls.contains(repo.getBaseUrl())) {	
service repo has a base url that is identical to that of another service repo 

Set<String> stackRepoNames = ImmutableSet.copyOf(Lists.transform(stackReposForOsType, RepositoryInfo.GET_REPO_NAME_FUNCTION));	Set<String> stackRepoUrls = ImmutableSet.copyOf(Lists.transform(stackReposForOsType, RepositoryInfo.SAFE_GET_BASE_URL_FUNCTION));	Set<String> duplicateServiceRepoNames = findDuplicates(serviceReposForOsType, RepositoryInfo.GET_REPO_NAME_FUNCTION);	Set<String> duplicateServiceRepoUrls = findDuplicates(serviceReposForOsType, RepositoryInfo.SAFE_GET_BASE_URL_FUNCTION);	for (RepositoryInfo repo: serviceReposForOsType) {	if (stackRepoUrls.contains(repo.getBaseUrl())) {	}	else if (duplicateServiceRepoUrls.contains(repo.getBaseUrl())) {	}	if (stackRepoNames.contains(repo.getRepoName())) {	
discarding service repository with the same name as one of the stack repos 

Set<String> duplicateServiceRepoNames = findDuplicates(serviceReposForOsType, RepositoryInfo.GET_REPO_NAME_FUNCTION);	Set<String> duplicateServiceRepoUrls = findDuplicates(serviceReposForOsType, RepositoryInfo.SAFE_GET_BASE_URL_FUNCTION);	for (RepositoryInfo repo: serviceReposForOsType) {	if (stackRepoUrls.contains(repo.getBaseUrl())) {	}	else if (duplicateServiceRepoUrls.contains(repo.getBaseUrl())) {	}	if (stackRepoNames.contains(repo.getRepoName())) {	}	else if (duplicateServiceRepoNames.contains(repo.getRepoName())) {	
discarding service repository with duplicate name and different content 

private void mergeRoleCommandOrder(ServiceModule service) {	if (service.getModuleInfo().getRoleCommandOrder() == null) return;	stackInfo.getRoleCommandOrder().merge(service.getModuleInfo().getRoleCommandOrder(), true);	if (LOG.isDebugEnabled()) {	
role command order for service 

private void validateBulkCommandComponents(Map<String, StackModule> allStacks){	if (null != stackInfo) {	String currentStackId = stackInfo.getName() + StackManager.PATH_DELIMITER + stackInfo.getVersion();	
validate bulk command components for 

if (null != currentStack){	for (ServiceModule serviceModule : currentStack.getServiceModules().values()) {	ServiceInfo service = serviceModule.getModuleInfo();	for(ComponentInfo component: service.getComponents()){	BulkCommandDefinition bcd = component.getBulkCommandDefinition();	if (null != bcd && null != bcd.getMasterComponent()){	String name = bcd.getMasterComponent();	ComponentInfo targetComponent = service.getComponentByName(name);	if (null == targetComponent){	String serviceName = service.getName();	
s bulk command section for service s in stack s references a component s which doesn t exist 

========================= ambari sample_4479 =========================

public static String getPath(String path) {	if(rootDirectory == null) {	
cannot get root enviroment variable installed to custom root directory ambari might not work correctly 

========================= ambari sample_2954 =========================

public Setting create(Setting setting) {	List<Setting> settings = getSettings();	if (checkUniqueViolation(settings, setting)) {	
setting key already exist for the user 

public Setting update(String id, Setting setting) throws ItemNotFound {	Setting current = read(id);	if(!current.getKey().equalsIgnoreCase(setting.getKey())) {	List<Setting> settings = getSettings();	if (checkUniqueViolation(settings, setting)) {	
setting key already exist for the user 

========================= ambari sample_527 =========================

protected void close() {	if (ps != null) try {	ps.close();	} catch (SQLException e) {	
exception while closing statment 

protected void close() {	if (ps != null) try {	ps.close();	} catch (SQLException e) {	}	if (db != null) try {	db.close();	} catch (SQLException e) {	
exception while closing connection 

setInt(resource, JOB_MAPS_PROPERTY_ID, rs, requestedIds);	setInt(resource, JOB_REDUCES_PROPERTY_ID, rs, requestedIds);	setLong(resource, JOB_INPUT_BYTES_PROPERTY_ID, rs, requestedIds);	setLong(resource, JOB_OUTPUT_BYTES_PROPERTY_ID, rs, requestedIds);	setString(resource, JOB_CONF_PATH_PROPERTY_ID, rs, requestedIds);	setString(resource, JOB_WORKFLOW_ID_PROPERTY_ID, rs, requestedIds);	setString(resource, JOB_WORKFLOW_ENTITY_NAME_PROPERTY_ID, rs, requestedIds);	jobs.add(resource);	}	} catch (SQLException e) {	
caught exception getting resource 

setString(resource, JOB_WORKFLOW_ID_PROPERTY_ID, rs, requestedIds);	setString(resource, JOB_WORKFLOW_ENTITY_NAME_PROPERTY_ID, rs, requestedIds);	jobs.add(resource);	}	} catch (SQLException e) {	return Collections.emptySet();	} finally {	if (rs != null) try {	rs.close();	} catch (SQLException e) {	
exception while closing resultset 

========================= ambari sample_3538 =========================

private void cancelJob(CancelJob message) {	if (!executing || connectionDelegate == null) {	
cannot cancel job for user as currently the job is not running or started jobid 

private void cancelJob(CancelJob message) {	if (!executing || connectionDelegate == null) {	return;	}	
cancelling job for user jobid user 

private void cancelJob(CancelJob message) {	if (!executing || connectionDelegate == null) {	return;	}	try {	isCancelCalled = true;	connectionDelegate.cancel();	} catch (SQLException e) {	
failed to cancel job jobid 

private void processCancel() {	executing = false;	if (isAsync() && jobId.isPresent()) {	
job canceled by user for jobid 

private void processResult(Optional<ResultSet> resultSetOptional) {	executing = false;	stopStatementExecutor();	
finished processing sql statements for job id sync job 

private void jobExecutionCompleted() {	
job execution completed for user results are ready to be fetched 

jobId = message.getJobId();	executionType = message.getType();	if (connectable == null) {	connectable = message.getConnectable(authParams);	}	try {	if (!connectable.isOpen()) {	connectable.connect();	}	} catch (ConnectionException e) {	
failed to create a hive connection 

private void updateJobStatus(String jobid, final String status) {	new JobSaver(jobid) {	protected void update(JobImpl job) {	job.setStatus(status);	job.setDuration(getUpdatedDuration(job.getDateSubmitted()));	}	}.save();	
stored job status for job id as 

private void saveGuid(final SaveGuidToDB message) {	new JobSaver(message.getJobId()) {	protected void update(JobImpl job) {	job.setGuid(message.getGuid());	}	}.save();	
stored guid for job id as 

private void saveDagInformation(final SaveDagInformation message) {	if(message.getDagId() == null && message.getDagName() == null && message.getApplicationId() == null) {	
cannot save dag information for job id as all the properties are null 

job.setApplicationId(message.getApplicationId());	}	if (message.getDagId() != null) {	job.setDagId(message.getDagId());	}	if(message.getDagName() != null) {	job.setDagName(message.getDagName());	}	}	}.save();	
store dag information for job job id dagname dagid applicationid 

private void checkInactivity() {	
inactivity check executing status 

private void checkTerminationInactivity() {	
termination check executing status 

private void cleanUp() {	if (jobId.isPresent()) {	
cleaning up resources for inactivity for jobid 

private void cleanUp() {	if (jobId.isPresent()) {	} else {	
cleaning up resources with inactivity for sync execution 

private void cleanUpWithTermination() {	this.executing = false;	
cleaning up resources with inactivity for execution 

========================= ambari sample_931 =========================

ServiceComponent component = service.getServiceComponent(componentName);	StackId stackId = component.getDesiredStackId();	componentInfo = managementController.getAmbariMetaInfo(). getComponent(stackId.getStackName(), stackId.getStackVersion(), serviceName, componentName);	packageFolder = managementController.getAmbariMetaInfo(). getService(stackId.getStackName(), stackId.getStackVersion(), serviceName).getServicePackageFolder();	String commandScript = componentInfo.getCommandScript().getScript();	List<ClientConfigFileDefinition> clientConfigFiles = componentInfo.getClientConfigFiles();	if (clientConfigFiles == null) {	if (componentMap.size() == 1) {	throw new SystemException("No configuration files defined for the component " + componentInfo.getName());	} else {	
no configuration files defined for the component 

private List<CommandLineThreadWrapper> executeCommands(final ExecutorService processExecutor, List<String> commandLines) throws SystemException {	List <CommandLineThreadWrapper> commandLineThreadWrappers = new ArrayList<>();	try {	for (String commandLine : commandLines) {	CommandLineThreadWrapper commandLineThreadWrapper = executeCommand(processExecutor,commandLine);	commandLineThreadWrappers.add(commandLineThreadWrapper);	}	} catch (IOException e) {	
failed to run generate client configs script for components 

for (CommandLineThreadWrapper commandLineThreadWrapper: pythonCmdThreads) {	CommandLineThread commandLineThread = commandLineThreadWrapper.getCommandLineThread();	try {	Integer returnCode = commandLineThread.getReturnCode();	if (returnCode == null) {	throw new TimeoutException();	} else if (returnCode != 0) {	throw new ExecutionException(String.format("Execution of \"%s\" returned %d.", commandLineThreadWrapper.getCommandLine(), returnCode), new Throwable(commandLineThreadWrapper.getLogStream().getOutput()));	}	} catch (TimeoutException e) {	
generate client configs script was killed due to timeout 

} catch (ExecutionException e) {	LOG.error(e.getMessage(), e);	throw new SystemException(e.getMessage() + " " + e.getCause());	} finally {	commandLineThreadWrapper.getProcess().destroy();	}	}	} catch (InterruptedException e) {	Thread.currentThread().interrupt();	processExecutor.shutdownNow();	
failed to run generate client configs script for components 

========================= ambari sample_3492 =========================

public List<Transfer> getTransfers() {	if (null == transfers) {	return Collections.emptyList();	}	List<Transfer> list = new ArrayList<>();	for (Transfer t : transfers) {	switch (t.operation) {	case COPY: case MOVE: if (null != t.fromKey && null != t.toKey) {	list.add(t);	} else {	
transfer s is invalid 

for (Transfer t : transfers) {	switch (t.operation) {	case COPY: case MOVE: if (null != t.fromKey && null != t.toKey) {	list.add(t);	} else {	}	break;	case DELETE: if (null != t.deleteKey) {	list.add(t);	} else {	
transfer s is invalid 

public List<Replace> getReplacements() {	if (null == replacements) {	return Collections.emptyList();	}	List<Replace> list = new ArrayList<>();	for (Replace r : replacements) {	if (null == r.key || null == r.find || null == r.replaceWith) {	
replacement s is invalid 

public List<Replace> getRegexReplacements(Cluster cluster) {	if (null == regexReplacements) {	return Collections.emptyList();	}	List<Replace> list = new ArrayList<>();	for (RegexReplace regexReplaceObj : regexReplacements) {	if (null == regexReplaceObj.key || null == regexReplaceObj.find || null == regexReplaceObj.replaceWith) {	
replacement s is invalid 

========================= ambari sample_3241 =========================

try {	Injector injector = Guice.createInjector(new CheckHelperControllerModule(), new CheckHelperAuditModule());	databaseConsistencyChecker = injector.getInstance(DatabaseConsistencyChecker.class);	databaseConsistencyChecker.startPersistenceService();	DatabaseConsistencyCheckHelper.runAllDBChecks(false);	DatabaseConsistencyCheckHelper.checkHostComponentStates();	DatabaseConsistencyCheckHelper.checkServiceConfigs();	databaseConsistencyChecker.stopPersistenceService();	} catch (Throwable e) {	if (e instanceof AmbariException) {	
exception occurred during database check 

databaseConsistencyChecker = injector.getInstance(DatabaseConsistencyChecker.class);	databaseConsistencyChecker.startPersistenceService();	DatabaseConsistencyCheckHelper.runAllDBChecks(false);	DatabaseConsistencyCheckHelper.checkHostComponentStates();	DatabaseConsistencyCheckHelper.checkServiceConfigs();	databaseConsistencyChecker.stopPersistenceService();	} catch (Throwable e) {	if (e instanceof AmbariException) {	throw (AmbariException)e;	} else {	
unexpected error database check failed 

========================= ambari sample_2903 =========================

public Boolean handle(CloudSolrClient solrClient, SolrPropsConfig solrPropsConfig) throws Exception {	List<ACL> aclsToSetList = solrPropsConfig.getZkAcls();	if (CollectionUtils.isNotEmpty(aclsToSetList)) {	
setting acls for collection 

public Boolean handle(CloudSolrClient solrClient, SolrPropsConfig solrPropsConfig) throws Exception {	List<ACL> aclsToSetList = solrPropsConfig.getZkAcls();	if (CollectionUtils.isNotEmpty(aclsToSetList)) {	SolrZkClient zkClient = solrClient.getZkStateReader().getZkClient();	SolrZooKeeper solrZooKeeper = zkClient.getSolrZooKeeper();	String collectionPath = String.format("/collections/%s", solrPropsConfig.getCollection());	String configsPath = String.format("/configs/%s", solrPropsConfig.getConfigName());	List<ACL> collectionAcls = solrZooKeeper.getACL(collectionPath, new Stat());	if (isRefreshAclsNeeded(aclsToSetList, collectionAcls)) {	
acls differs for update acls 

SolrZkClient zkClient = solrClient.getZkStateReader().getZkClient();	SolrZooKeeper solrZooKeeper = zkClient.getSolrZooKeeper();	String collectionPath = String.format("/collections/%s", solrPropsConfig.getCollection());	String configsPath = String.format("/configs/%s", solrPropsConfig.getConfigName());	List<ACL> collectionAcls = solrZooKeeper.getACL(collectionPath, new Stat());	if (isRefreshAclsNeeded(aclsToSetList, collectionAcls)) {	setRecursivelyOn(solrZooKeeper, collectionPath, aclsToSetList);	}	List<ACL> configsAcls = solrZooKeeper.getACL(configsPath, new Stat());	if (isRefreshAclsNeeded(aclsToSetList, configsAcls)) {	
acls differs for update acls 

private boolean aclDiffers(List<ACL> aclList1, List<ACL> aclList2) {	for (ACL acl : aclList1) {	for (ACL newAcl : aclList2) {	if (acl.getId() != null && acl.getId().getId().equals(newAcl.getId().getId()) && acl.getPerms() != newAcl.getPerms()) {	
acl for differs on znode should be 

========================= ambari sample_1339 =========================

public static void initializeMetricFilter(TimelineMetricConfiguration configuration) {	Configuration metricsConf = null;	try {	metricsConf = configuration.getMetricsConf();	} catch (Exception e) {	
error fetching metrics configuration for getting whitelisting information 

amshbaseWhitelist = new HashSet<>();	if (configuration.isWhitelistingEnabled()) {	String whitelistFile = metricsConf.get(TIMELINE_METRICS_WHITELIST_FILE, TIMELINE_METRICS_WHITELIST_FILE_LOCATION_DEFAULT);	readMetricWhitelistFromFile(whitelistFile);	}	String appsBlacklist = metricsConf.get(TIMELINE_METRICS_APPS_BLACKLIST, "");	if (!StringUtils.isEmpty(appsBlacklist)) {	for (String app : appsBlacklist.split(",")) {	blacklistedApps.add(app);	}	
blacklisted apps 

if (!StringUtils.isEmpty(appsBlacklist)) {	for (String app : appsBlacklist.split(",")) {	blacklistedApps.add(app);	}	}	String appsWhitelist = metricsConf.get(TIMELINE_METRICS_APPS_WHITELIST, "");	if (!StringUtils.isEmpty(appsWhitelist)) {	for (String app : appsWhitelist.split(",")) {	whitelistedApps.add(app);	}	
whitelisted apps 

}	}	String appsWhitelist = metricsConf.get(TIMELINE_METRICS_APPS_WHITELIST, "");	if (!StringUtils.isEmpty(appsWhitelist)) {	for (String app : appsWhitelist.split(",")) {	whitelistedApps.add(app);	}	}	amshbaseWhitelist = configuration.getAmshbaseWhitelist();	if (CollectionUtils.isNotEmpty(amshbaseWhitelist)) {	
whitelisting ams hbase metrics 

if (StringUtils.isEmpty(strLine)) {	continue;	}	if (strLine.startsWith(patternPrefix)) {	whitelistedMetricPatterns.add(Pattern.compile(strLine.substring(patternPrefix.length())));	} else {	whitelistedMetrics.add(strLine);	}	}	} catch (IOException ioEx) {	
unable to parse metric whitelist file 

continue;	}	if (strLine.startsWith(patternPrefix)) {	whitelistedMetricPatterns.add(Pattern.compile(strLine.substring(patternPrefix.length())));	} else {	whitelistedMetrics.add(strLine);	}	}	} catch (IOException ioEx) {	}	
whitelisting metrics 

continue;	}	if (strLine.startsWith(patternPrefix)) {	whitelistedMetricPatterns.add(Pattern.compile(strLine.substring(patternPrefix.length())));	} else {	whitelistedMetrics.add(strLine);	}	}	} catch (IOException ioEx) {	}	
whitelisted metrics 

========================= ambari sample_383 =========================

public ThemeModule(File themeFile, ThemeInfo moduleInfo) {	this.moduleInfo = moduleInfo;	if (!moduleInfo.isDeleted() && themeFile != null) {	
looking for theme in 

public ThemeModule(File themeFile, ThemeInfo moduleInfo) {	this.moduleInfo = moduleInfo;	if (!moduleInfo.isDeleted() && themeFile != null) {	FileReader reader = null;	try {	reader = new FileReader(themeFile);	} catch (FileNotFoundException e) {	
theme file not found 

FileReader reader = null;	try {	reader = new FileReader(themeFile);	} catch (FileNotFoundException e) {	}	try {	Theme theme = mapper.readValue(reader, Theme.class);	Map<String, Theme> map = new HashMap<>();	map.put(THEME_KEY, theme);	moduleInfo.setThemeMap(map);	
loaded theme 

try {	reader = new FileReader(themeFile);	} catch (FileNotFoundException e) {	}	try {	Theme theme = mapper.readValue(reader, Theme.class);	Map<String, Theme> map = new HashMap<>();	map.put(THEME_KEY, theme);	moduleInfo.setThemeMap(map);	} catch (IOException e) {	
unable to parse theme file 

========================= ambari sample_4459 =========================

public String getResponse(String url, Map<String, String> headers, String authType) {	
fetching the result from the url using proxy 

InputStream inputStream = null;	try {	HttpURLConnection connection;	URLConnectionProvider provider = viewContext.getURLConnectionProvider();	if(authType == null || authType.equalsIgnoreCase("simple")) {	connection = provider.getConnection(url, "GET", (String) null, headers);	} else {	connection = provider.getConnectionAsCurrent(url, "GET", (String) null, headers);	}	if (!(connection.getResponseCode() >= 200 && connection.getResponseCode() < 300)) {	
failure in fetching results for the url status 

String trace = "";	inputStream = connection.getErrorStream();	if (inputStream != null) {	trace = IOUtils.toString(inputStream);	}	throw new TezWebAppException("Failed to fetch results by the proxy from url: " + url, connection.getResponseCode(), trace);	}	inputStream = connection.getInputStream();	return IOUtils.toString(inputStream);	} catch (IOException e) {	
cannot access the url 

try {	builder = new URIBuilder(baseUrl + "/" + endPoint);	for(String key: keySet) {	builder.addParameter(key, queryParameters.getFirst(key));	}	if(authType == null || authType.equalsIgnoreCase("simple")) {	builder.addParameter("user.name", viewContext.getUsername());	}	return builder.build().toString();	} catch (URISyntaxException e) {	
failed to build a url from the baseurl and endpoint exception 

========================= ambari sample_1122 =========================

configElement.appendChild(propElement);	Element nameElem = doc.createElement("name");	nameElem.setTextContent(entry.getKey());	Element valueElem = doc.createElement("value");	valueElem.setTextContent(entry.getValue());	propElement.appendChild(nameElem);	propElement.appendChild(valueElem);	}	return utils.generateXml(doc);	} catch (ParserConfigurationException e) {	
error in generating config xml 

killElement.setAttribute("name", "kill");	Element killMessageElement = doc.createElement("message");	killMessageElement.setTextContent("Kill node message");	killElement.appendChild(killMessageElement);	workflowElement.appendChild(killElement);	Element endElement = doc.createElement("end");	endElement.setAttribute("name", "end");	workflowElement.appendChild(endElement);	return utils.generateXml(doc);	} catch (ParserConfigurationException | SAXException | IOException e) {	
error in generating workflow xml 

========================= ambari sample_1088 =========================

if(optHeader != null){	if(optHeader.equals(ParseOptions.HEADER.FIRST_RECORD.toString())) {	format = format.withHeader();	}else if( optHeader.equals(ParseOptions.HEADER.PROVIDED_BY_USER.toString())){	String [] headers = (String[]) parseOptions.getOption(ParseOptions.OPTIONS_HEADERS);	format = format.withHeader(headers);	}	}	Character delimiter = (Character) parseOptions.getOption(ParseOptions.OPTIONS_CSV_DELIMITER);	if(delimiter != null){	
setting delimiter as 

String [] headers = (String[]) parseOptions.getOption(ParseOptions.OPTIONS_HEADERS);	format = format.withHeader(headers);	}	}	Character delimiter = (Character) parseOptions.getOption(ParseOptions.OPTIONS_CSV_DELIMITER);	if(delimiter != null){	format = format.withDelimiter(delimiter);	}	Character quote = (Character) parseOptions.getOption(ParseOptions.OPTIONS_CSV_QUOTE);	if( null != quote ){	
setting quote char 

Character delimiter = (Character) parseOptions.getOption(ParseOptions.OPTIONS_CSV_DELIMITER);	if(delimiter != null){	format = format.withDelimiter(delimiter);	}	Character quote = (Character) parseOptions.getOption(ParseOptions.OPTIONS_CSV_QUOTE);	if( null != quote ){	format = format.withQuote(quote);	}	Character escape = (Character) parseOptions.getOption(ParseOptions.OPTIONS_CSV_ESCAPE_CHAR);	if(escape != null){	
setting escape as 

========================= ambari sample_550 =========================

public Response getList() {	try {	
getting all resources 

========================= ambari sample_803 =========================

boolean isValid = false;	switch (aliasType) {	case FILTER: isValid = Filter.class.isAssignableFrom(instance.getClass());	break;	case INPUT: isValid = Input.class.isAssignableFrom(instance.getClass());	break;	case OUTPUT: isValid = Output.class.isAssignableFrom(instance.getClass());	break;	case MAPPER: isValid = Mapper.class.isAssignableFrom(instance.getClass());	break;	
unhandled aliastype 

break;	case INPUT: isValid = Input.class.isAssignableFrom(instance.getClass());	break;	case OUTPUT: isValid = Output.class.isAssignableFrom(instance.getClass());	break;	case MAPPER: isValid = Mapper.class.isAssignableFrom(instance.getClass());	break;	isValid = true;	}	if (!isValid) {	
not a valid class aliastype 

private static String getClassFullName(String key, AliasType aliastype) {	String className = null;	HashMap<String, String> aliasInfo = getAliasInfo(key, aliastype);	String value = aliasInfo.get("klass");	if (value != null && !value.isEmpty()) {	className = value;	
class name found for key class name aliastype 

private static String getClassFullName(String key, AliasType aliastype) {	String className = null;	HashMap<String, String> aliasInfo = getAliasInfo(key, aliastype);	String value = aliasInfo.get("klass");	if (value != null && !value.isEmpty()) {	className = value;	} else {	
class name not found for key aliastype 

public static HashMap<String, Object> getJsonFileContentFromClassPath(String fileName) {	ObjectMapper mapper = new ObjectMapper();	try (InputStream inputStream = AliasUtil.class.getClassLoader().getResourceAsStream(fileName)) {	return mapper.readValue(inputStream, new TypeReference<HashMap<String, Object>>() {});	} catch (IOException e) {	
error occurred during loading alias json file 

========================= ambari sample_1749 =========================

}	try (FileInputStream in = new FileInputStream(new File(trustStorePath))) {	KeyStore e = KeyStore.getInstance(trustStoreType == null ? KeyStore.getDefaultType() : trustStoreType);	e.load(in, trustStorePassword.toCharArray());	TrustManagerFactory tmf = TrustManagerFactory.getInstance(TrustManagerFactory.getDefaultAlgorithm());	tmf.init(e);	SSLContext context = SSLContext.getInstance("TLS");	context.init((KeyManager[]) null, tmf.getTrustManagers(), (SecureRandom) null);	this.sslSocketFactory = context.getSocketFactory();	} catch (Exception ex) {	
unable to load truststore 

private void setupCredentials(HttpURLConnection httpURLConnection) {	final String logSearchAdminUser = getLogSearchAdminUser();	final String logSearchAdminPassword = getLogSearchAdminPassword();	if ((logSearchAdminUser != null) && (logSearchAdminPassword != null)) {	
credential found in config will be used to connect to logsearch 

private void setupCredentials(HttpURLConnection httpURLConnection) {	final String logSearchAdminUser = getLogSearchAdminUser();	final String logSearchAdminPassword = getLogSearchAdminPassword();	if ((logSearchAdminUser != null) && (logSearchAdminPassword != null)) {	networkConnection.setupBasicAuthentication(httpURLConnection, createEncodedCredentials(logSearchAdminUser, logSearchAdminPassword));	} else {	PrincipalKeyCredential principalKeyCredential = getLogSearchCredentials();	if (principalKeyCredential != null) {	
credential found in credentialstore will be used to connect to logsearch 

private void setupCredentials(HttpURLConnection httpURLConnection) {	final String logSearchAdminUser = getLogSearchAdminUser();	final String logSearchAdminPassword = getLogSearchAdminPassword();	if ((logSearchAdminUser != null) && (logSearchAdminPassword != null)) {	networkConnection.setupBasicAuthentication(httpURLConnection, createEncodedCredentials(logSearchAdminUser, logSearchAdminPassword));	} else {	PrincipalKeyCredential principalKeyCredential = getLogSearchCredentials();	if (principalKeyCredential != null) {	networkConnection.setupBasicAuthentication(httpURLConnection, createEncodedCredentials(principalKeyCredential));	} else {	
no logsearch credential could be found this is probably an error in configuration 

========================= ambari sample_3774 =========================

public HttpURLConnection processURL(String spec, String requestMethod, byte[] body, Map<String, List<String>> headers) throws IOException {	if (LOG.isDebugEnabled()) {	
readfrom spec 

public HttpURLConnection processURL(String spec, String requestMethod, byte[] body, Map<String, List<String>> headers) throws IOException {	if (LOG.isDebugEnabled()) {	}	HttpURLConnection connection = (spec.startsWith("https") && this.setupTruststoreForHttps) ? getSSLConnection(spec) : getConnection(spec);	AppCookieManager appCookieManager = getAppCookieManager();	String appCookie = appCookieManager.getCachedAppCookie(spec);	if (appCookie != null) {	
using cached app cookie for url 

connection.setRequestProperty(entry.getKey(), paramValue.substring(1, paramValue.length() - 1));	}	}	if (body != null) {	connection.getOutputStream().write(body);	}	int statusCode = connection.getResponseCode();	if (statusCode == HttpStatus.SC_UNAUTHORIZED ) {	String wwwAuthHeader = connection.getHeaderField(WWW_AUTHENTICATE);	if (LOG.isInfoEnabled()) {	
received www authentication header for url 

}	if (wwwAuthHeader != null && wwwAuthHeader.trim().startsWith(NEGOTIATE)) {	connection = spec.startsWith("https") ? getSSLConnection(spec) : getConnection(spec);	appCookie = appCookieManager.getAppCookie(spec, true);	connection.setRequestProperty(COOKIE, appCookie);	connection.setConnectTimeout(connTimeout);	connection.setReadTimeout(readTimeout);	connection.setDoOutput(true);	return connection;	} else {	
unsupported www authentication header for url 

connection.setRequestProperty(COOKIE, appCookie);	connection.setConnectTimeout(connTimeout);	connection.setReadTimeout(readTimeout);	connection.setDoOutput(true);	return connection;	} else {	return connection;	}	} else {	if (statusCode == HttpStatus.SC_NOT_FOUND || statusCode == HttpStatus.SC_FORBIDDEN){	
received http s response from url s 

========================= ambari sample_3631 =========================

int page = 1;	while (saved < json.length()) {	int end = Math.min(saved + VALUE_LENGTH_LIMIT, json.length());	String substring = json.substring(saved, end);	getConfig().setProperty(modelPropName + "#" + page, substring);	saved += VALUE_LENGTH_LIMIT;	page += 1;	LOG.debug("Chunk saved: " + modelPropName + "#" + page + "=" + substring);	}	getConfig().setProperty(modelPropName, page - 1);	
write finished pages 

protected String read(String modelPropName) {	StringBuilder result = new StringBuilder();	int pages = getConfig().getInt(modelPropName);	
read started pages 

protected void clear(String modelPropName) {	int pages = getConfig().getInt(modelPropName);	
clean started pages 

protected void clear(String modelPropName) {	int pages = getConfig().getInt(modelPropName);	for(int page = 1; page <= pages; page++) {	getConfig().clearProperty(modelPropName + "#" + page);	
chunk clean 

========================= ambari sample_911 =========================

public RequestStatus createResources(Request request) throws SystemException, UnsupportedPropertyException, ResourceAlreadyExistsException, NoSuchParentResourceException {	for (Map<String, Object> properties : request.getProperties()) {	try {	createResources(getCreateCommand(properties, request.getRequestInfoProperties()));	}catch(IllegalArgumentException e) {	
exception while creating blueprint 

========================= ambari sample_3627 =========================

protected void runOneIteration() throws Exception {	try {	
flushing cached alerts to the database 

protected void runOneIteration() throws Exception {	try {	m_alertsDAO.flushCachedEntitiesToJPA();	} catch (Exception exception) {	
unable to flush cached alerts to the database 

========================= ambari sample_3277 =========================

public void bootStrapGet() throws UniformInterfaceException, JSONException {	WebResource webResource = resource();	BootStrapStatus status = webResource.path("/bootstrap/0").type( MediaType.APPLICATION_JSON) .get(BootStrapStatus.class);	
get response from the api 

========================= ambari sample_1860 =========================

ServiceComponentHostEvent event = eventWrapper.getEvent();	buffer.append(event.getServiceComponentName());	if (null != event.getHostName()) {	buffer.append(" on ");	buffer.append(event.getHostName());	}	buffer.append(": ");	buffer.append(skippedTask.getCommandDetail());	buffer.append("\n");	} catch (Exception exception) {	
unable to extract failure information for 

========================= ambari sample_3365 =========================

public MaintenanceState getEffectiveState(long clusterId, Alert alert) throws AmbariException {	String serviceName = alert.getService();	String componentName = alert.getComponent();	String hostName = alert.getHostName();	if (null == serviceName && null == hostName) {	
unable to determine maintenance state for an alert without a service or host 

========================= ambari sample_3735 =========================

hBaseAccessor.initPoliciesAndTTL();	if (!configuration.isDistributedCollectorModeDisabled()) {	haController = new MetricCollectorHAController(configuration);	try {	haController.initializeHAController();	} catch (Exception e) {	LOG.error(e);	throw new MetricsSystemInitializationException("Unable to " + "initialize HA controller", e);	}	} else {	
distributed collector mode disabled 

haController.initializeHAController();	} catch (Exception e) {	LOG.error(e);	throw new MetricsSystemInitializationException("Unable to " + "initialize HA controller", e);	}	} else {	}	TimelineMetricsFilter.initializeMetricFilter(configuration);	defaultTopNHostsLimit = Integer.parseInt(metricsConf.get(DEFAULT_TOPN_HOSTS_LIMIT, "20"));	if (Boolean.parseBoolean(metricsConf.get(USE_GROUPBY_AGGREGATOR_QUERIES, "true"))) {	
using group by aggregators for aggregating host and cluster metrics 

}	TimelineMetricAggregator secondClusterAggregator = TimelineMetricAggregatorFactory.createTimelineClusterAggregatorSecond( hBaseAccessor, metricsConf, metricMetadataManager, haController);	scheduleAggregatorThread(secondClusterAggregator);	TimelineMetricAggregator minuteClusterAggregator = TimelineMetricAggregatorFactory.createTimelineClusterAggregatorMinute( hBaseAccessor, metricsConf, haController);	scheduleAggregatorThread(minuteClusterAggregator);	TimelineMetricAggregator hourlyClusterAggregator = TimelineMetricAggregatorFactory.createTimelineClusterAggregatorHourly( hBaseAccessor, metricsConf, haController);	scheduleAggregatorThread(hourlyClusterAggregator);	TimelineMetricAggregator dailyClusterAggregator = TimelineMetricAggregatorFactory.createTimelineClusterAggregatorDaily( hBaseAccessor, metricsConf, haController);	scheduleAggregatorThread(dailyClusterAggregator);	if (Boolean.parseBoolean(metricsConf.get(TIMELINE_METRICS_HOST_INMEMORY_AGGREGATION, "true"))) {	
timeline metrics host inmemory aggregation is set to true disabling host minute aggregation on collector 

Multimap<String, List<Function>> metricFunctions = parseMetricNamesToAggregationFunctions(metricNames);	ConditionBuilder conditionBuilder = new ConditionBuilder(new ArrayList<String>(metricFunctions.keySet())) .hostnames(hostnames) .appId(applicationId) .instanceId(instanceId) .startTime(startTime) .endTime(endTime) .precision(precision) .limit(limit) .grouped(groupedByHosts);	if (topNConfig != null) {	if (TopNCondition.isTopNHostCondition(metricNames, hostnames) ^ TopNCondition.isTopNMetricCondition(metricNames, hostnames)) {	conditionBuilder.topN(topNConfig.getTopN());	conditionBuilder.isBottomN(topNConfig.getIsBottomN());	Function.ReadFunction readFunction = Function.ReadFunction.getFunction(topNConfig.getTopNFunction());	Function function = new Function(readFunction, null);	conditionBuilder.topNFunction(function);	} else {	
invalid input for topn query ignoring topn request 

if (topNConfig != null) {	if (TopNCondition.isTopNHostCondition(metricNames, hostnames) ^ TopNCondition.isTopNMetricCondition(metricNames, hostnames)) {	conditionBuilder.topN(topNConfig.getTopN());	conditionBuilder.isBottomN(topNConfig.getIsBottomN());	Function.ReadFunction readFunction = Function.ReadFunction.getFunction(topNConfig.getTopNFunction());	Function function = new Function(readFunction, null);	conditionBuilder.topNFunction(function);	} else {	}	} else if (startTime != null && hostnames != null && hostnames.size() > defaultTopNHostsLimit) {	
requesting data for more than hosts defaulting to top 

public TimelinePutResponse putContainerMetrics(List<ContainerMetric> metrics) throws SQLException, IOException {	if (containerMetricsDisabled) {	
ignoring submitted container metrics according to configuration values will not be stored 

List<String> instances = null;	try {	if (haController == null) {	return Collections.singletonList(configuration.getInstanceHostnameFromEnv());	}	instances = haController.getLiveInstanceHostNames();	if (instances == null || instances.isEmpty()) {	instances = Collections.singletonList(configuration.getInstanceHostnameFromEnv());	}	} catch (UnknownHostException e) {	
exception on getting hostname from env 

private void scheduleAggregatorThread(final TimelineMetricAggregator aggregator) {	if (!aggregator.isDisabled()) {	ScheduledExecutorService executorService = Executors.newSingleThreadScheduledExecutor( new ThreadFactory() {	public Thread newThread(Runnable r) {	return new Thread(r, ACTUAL_AGGREGATOR_NAMES.get(aggregator.getName()));	}	}	);	scheduledExecutors.put(aggregator.getName(), executorService);	executorService.scheduleAtFixedRate(aggregator, 0l, aggregator.getSleepIntervalMillis(), TimeUnit.MILLISECONDS);	
scheduled aggregator thread every milliseconds 

if (!aggregator.isDisabled()) {	ScheduledExecutorService executorService = Executors.newSingleThreadScheduledExecutor( new ThreadFactory() {	public Thread newThread(Runnable r) {	return new Thread(r, ACTUAL_AGGREGATOR_NAMES.get(aggregator.getName()));	}	}	);	scheduledExecutors.put(aggregator.getName(), executorService);	executorService.scheduleAtFixedRate(aggregator, 0l, aggregator.getSleepIntervalMillis(), TimeUnit.MILLISECONDS);	} else {	
skipped scheduling since it is disabled 

========================= ambari sample_395 =========================

public XMLIterator(XMLEventReader reader) throws IOException {	this.reader = reader;	try {	nextObject = readNextObject(this.reader);	} catch (EndOfDocumentException e) {	
error 

public LinkedHashMap<String, String> next() {	LinkedHashMap<String, String> currObject = nextObject;	try {	nextObject = readNextObject(this.reader);	} catch (IOException e) {	
exception occured while reading the next row from xml 

public LinkedHashMap<String, String> next() {	LinkedHashMap<String, String> currObject = nextObject;	try {	nextObject = readNextObject(this.reader);	} catch (IOException e) {	nextObject = null;	} catch (EndOfDocumentException e) {	
end of xml document reached with next character ending the xml 

public LinkedHashMap<String, String> next() {	LinkedHashMap<String, String> currObject = nextObject;	try {	nextObject = readNextObject(this.reader);	} catch (IOException e) {	nextObject = null;	} catch (EndOfDocumentException e) {	nextObject = null;	} catch (XMLStreamException e) {	
exception occured while reading the next row from xml 

public void remove() {	
no operation when remove called 

private LinkedHashMap<String, String> readNextObject(XMLEventReader reader) throws IOException, EndOfDocumentException, XMLStreamException {	LinkedHashMap<String, String> row = new LinkedHashMap<>();	boolean objectStarted = false;	String currentName = null;	while (true) {	XMLEvent event = reader.nextEvent();	switch (event.getEventType()) {	case XMLStreamConstants.START_ELEMENT: StartElement startElement = event.asStartElement();	String qName = startElement.getName().getLocalPart();	
startname 

if( null == nameAttr ){	throw new IllegalArgumentException("Missing name attribute in col tag.");	}	currentName = nameAttr.getValue();	break;	default: throw new IllegalArgumentException("Illegal start tag " + qName + " encountered.");	}	break;	case XMLStreamConstants.END_ELEMENT: EndElement endElement = event.asEndElement();	String name = endElement.getName().getLocalPart();	
endname 

throw new IllegalArgumentException("Stray tag " + name);	}	currentName = null;	break;	default: throw new IllegalArgumentException("Illegal start ending " + name + " encountered.");	}	break;	case XMLStreamConstants.CHARACTERS: Characters characters = event.asCharacters();	if (characters.isWhiteSpace() && currentName == null) break;	String data = characters.getData();	
character data 

========================= ambari sample_546 =========================

Assert.assertNotNull(requestExecution);	executionScheduleManager.scheduleBatch(requestExecution);	String jobName1 = executionScheduleManager.getJobName(requestExecution .getId(), 10L);	String jobName2 = executionScheduleManager.getJobName(requestExecution .getId(), 12L);	JobDetail jobDetail1 = null;	JobDetail jobDetail2 = null;	Trigger trigger1 = null;	Trigger trigger2 = null;	for(String group: scheduler.getJobGroupNames()) {	for(JobKey jobKey : scheduler.getJobKeys(GroupMatcher.jobGroupEquals (ExecutionJob.LINEAR_EXECUTION_JOB_GROUP))) {	
found job identified by 

JobDetail jobDetail2 = null;	Trigger trigger1 = null;	Trigger trigger2 = null;	for(String group: scheduler.getJobGroupNames()) {	for(JobKey jobKey : scheduler.getJobKeys(GroupMatcher.jobGroupEquals (ExecutionJob.LINEAR_EXECUTION_JOB_GROUP))) {	String jobName = jobKey.getName();	String jobGroup = jobKey.getGroup();	List<Trigger> triggers = (List<Trigger>) scheduler.getTriggersOfJob(jobKey);	Trigger trigger = triggers != null && !triggers.isEmpty() ? triggers.get(0) : null;	Date nextFireTime = trigger != null ? trigger.getNextFireTime() : null;	
jobname groupname 

========================= ambari sample_1972 =========================

public boolean init(String inputDesc, String fieldName, String mapClassCode, MapFieldDescriptor mapFieldDescriptor) {	init(inputDesc, fieldName, mapClassCode);	prevValue = ((MapFieldValueDescriptor)mapFieldDescriptor).getPreValue();	newValue = ((MapFieldValueDescriptor)mapFieldDescriptor).getPostValue();;	if (StringUtils.isEmpty(newValue)) {	
map field value is empty 

public Object apply(Map<String, Object> jsonObj, Object value) {	if (newValue != null && prevValue != null) {	if (prevValue.equalsIgnoreCase(value.toString())) {	value = newValue;	jsonObj.put(getFieldName(), value);	}	} else {	
apply new value is null so transformation is not applied 

========================= ambari sample_1636 =========================

public ChainedAttributeDetector userAttributDetector() {	
creating instance with user attribute detectors 

public ChainedAttributeDetector groupAttributeDetector() {	
creating instance with group attribute detectors 

========================= ambari sample_2919 =========================

Element rootNode = doc.getRootElement();	Element record = new Element("RevertRecord");	record.setAttribute(new Attribute("id", iteration));	record.addContent(new Element("datetime").setText(currentDate.toString()));	record.addContent(new Element("dirname").setText(dirname));	record.addContent(new Element("instance").setText(instance));	record.addContent(new Element("query").setText(content));	rootNode.addContent(record);	xmlOutput.output(doc, new FileWriter(ConfigurationCheckImplementation.getHomeDir() + "RevertChangesService.xml"));	} catch (JDOMException e) {	
jdomexception 

public void updateSequenceno(Connection c, int seqNo, String sequenceName, QuerySetAmbariDB ambaridatabase) throws SQLException, IOException {	PreparedStatement prSt;	prSt = ambaridatabase.updateSequenceNoInAmbariSequence(c, seqNo, sequenceName);	
the actual insert statement is 

public void updateSequenceno(Connection c, int seqNo, String sequenceName, QuerySetAmbariDB ambaridatabase) throws SQLException, IOException {	PreparedStatement prSt;	prSt = ambaridatabase.updateSequenceNoInAmbariSequence(c, seqNo, sequenceName);	prSt.executeUpdate();	
adding revert sql hive history 

public void insertRowHiveHistory(String dirname, int maxcount, long epochtime, Connection c, int id, String instance, int i, QuerySetAmbariDB ambaridatabase) throws SQLException, IOException {	String maxcount1 = Integer.toString(maxcount);	String epochtime1 = Long.toString(epochtime);	PreparedStatement prSt = null;	String revsql = null;	prSt = ambaridatabase.insertToHiveHistory(c, id, maxcount1, epochtime, dirname);	System.out.println("the actual query is " + prSt);	
the actual insert statement is 

public void insertRowHiveHistory(String dirname, int maxcount, long epochtime, Connection c, int id, String instance, int i, QuerySetAmbariDB ambaridatabase) throws SQLException, IOException {	String maxcount1 = Integer.toString(maxcount);	String epochtime1 = Long.toString(epochtime);	PreparedStatement prSt = null;	String revsql = null;	prSt = ambaridatabase.insertToHiveHistory(c, id, maxcount1, epochtime, dirname);	System.out.println("the actual query is " + prSt);	prSt.executeUpdate();	revsql = ambaridatabase.revertSqlHistoryQuery(id, maxcount1);	
adding revert sqlsavedquery in hivehistory 

public void insertRowinSavedQuery(int maxcount, String database, String dirname, String query, String name, Connection c, int id, String instance, int i, QuerySetAmbariDB ambaridatabase,String username) throws SQLException, IOException {	String maxcount1 = Integer.toString(maxcount);	String revsql = null;	PreparedStatement prSt = null;	prSt = ambaridatabase.insertToHiveSavedQuery(c, id, maxcount1, database, dirname, query, name,username);	System.out.println("the actual query is " + prSt);	
the actual insert statement is 

public void insertRowinSavedQuery(int maxcount, String database, String dirname, String query, String name, Connection c, int id, String instance, int i, QuerySetAmbariDB ambaridatabase,String username) throws SQLException, IOException {	String maxcount1 = Integer.toString(maxcount);	String revsql = null;	PreparedStatement prSt = null;	prSt = ambaridatabase.insertToHiveSavedQuery(c, id, maxcount1, database, dirname, query, name,username);	System.out.println("the actual query is " + prSt);	prSt.executeUpdate();	revsql = ambaridatabase.revertSqlSavedQuery(id, maxcount1);	
adding revert sqlsavedquery 

} else {	prSt = huedatabase.getQueriesYesStartDateNoEndDate(connection, id, startdate);	}	} else if (!(startdate.equals("")) && !(endtime.equals(""))) {	if (username.equals("all")) {	prSt = huedatabase.getQueriesYesStartDateYesEndDateAllUser(connection, startdate, endtime);	} else {	prSt = huedatabase.getQueriesYesStartDateYesEndDate(connection, id, startdate, endtime);	}	}	
query prepared statement is 

prSt = huedatabase.getQueriesYesStartDateNoEndDate(connection, id, startdate);	}	} else if (!(startdate.equals("")) && !(endtime.equals(""))) {	if (username.equals("all")) {	prSt = huedatabase.getQueriesYesStartDateYesEndDateAllUser(connection, startdate, endtime);	} else {	prSt = huedatabase.getQueriesYesStartDateYesEndDate(connection, id, startdate, endtime);	}	}	rs1 = prSt.executeQuery();	
query executed 

ownerName = resultSet.getString("username");	}	}	String queryTitle = rs1.getString("name");	String temp = rs1.getString("data");	InputStream is = new ByteArrayInputStream(temp.getBytes());	BufferedReader rd = new BufferedReader(new InputStreamReader( is, Charset.forName("UTF-8")));	String jsonText = readAll(rd);	JSONObject json = new JSONObject(jsonText);	String resources = json.get("query").toString();	
query 

ArrayList<String> filePaths = new ArrayList<String>();	ArrayList<String> classNames = new ArrayList<String>();	ArrayList<String> udfNames = new ArrayList<String>();	for(int j=0;j<fileResources.length();j++) {	filePaths.add(fileResources.getJSONObject(j).get("path").toString());	}	for(int j=0;j<functions.length();j++) {	classNames.add(functions.getJSONObject(j).get("class_name").toString());	udfNames.add(functions.getJSONObject(j).get("name").toString());	}	
paths are 

ArrayList<String> filePaths = new ArrayList<String>();	ArrayList<String> classNames = new ArrayList<String>();	ArrayList<String> udfNames = new ArrayList<String>();	for(int j=0;j<fileResources.length();j++) {	filePaths.add(fileResources.getJSONObject(j).get("path").toString());	}	for(int j=0;j<functions.length();j++) {	classNames.add(functions.getJSONObject(j).get("class_name").toString());	udfNames.add(functions.getJSONObject(j).get("name").toString());	}	
class names are 

ArrayList<String> filePaths = new ArrayList<String>();	ArrayList<String> classNames = new ArrayList<String>();	ArrayList<String> udfNames = new ArrayList<String>();	for(int j=0;j<fileResources.length();j++) {	filePaths.add(fileResources.getJSONObject(j).get("path").toString());	}	for(int j=0;j<functions.length();j++) {	classNames.add(functions.getJSONObject(j).get("class_name").toString());	udfNames.add(functions.getJSONObject(j).get("name").toString());	}	
udf names are 

i++;	}	connection.commit();	} catch (SQLException e2) {	e2.printStackTrace();	connection.rollback();	} finally {	try {	if (connection != null) connection.close();	} catch (SQLException e) {	
sql connection exception 

public boolean checkUdfExists(Connection connection, String fileName, String username, int tableId, QuerySetAmbariDB ambaridatabase, HashSet<String> udfSet) throws SQLException{	ResultSet rs = ambaridatabase.getUdfFileNamesAndOwners(connection, tableId).executeQuery();	while(rs.next()){	
filename ds name ds name username ds owner ds owner 

try {	File file = new File(homedir + "query.hql");	if (!file.exists()) {	file.createNewFile();	}	FileWriter fw = new FileWriter(file.getAbsoluteFile());	BufferedWriter bw = new BufferedWriter(fw);	bw.write(content);	bw.close();	} catch (IOException e) {	
ioexception 

public void deleteFileQueryhql(String homedir) {	try {	File file = new File(homedir + "query.hql");	if (file.delete()) {	
temporary hql file deleted 

public void deleteFileQueryhql(String homedir) {	try {	File file = new File(homedir + "query.hql");	if (file.delete()) {	} else {	
temporary hql file delete failed 

public void deleteFileQueryhql(String homedir) {	try {	File file = new File(homedir + "query.hql");	if (file.delete()) {	} else {	}	} catch (Exception e) {	
file exception 

String[] subDirs = dir.split("/");	String dirPath = USER_DIRECTORY;	for(int i=2;i<subDirs.length;i++) {	dirPath += "/"+subDirs[i];	fs.setOwner(new Path(dirPath), username, username);	}	return b;	}	});	} catch (Exception e) {	
exception in webhdfs 

String[] subDirs = dir.split("/");	String dirPath = USER_DIRECTORY;	for(int i=2;i<subDirs.length;i++) {	dirPath += "/"+subDirs[i];	fs.setOwner(new Path(dirPath), username, username);	}	return b;	}	});	} catch (Exception e) {	
exception in webhdfs 

out.write(b, 0, numBytes);	}	in.close();	out.close();	fileSystem.setOwner(path, username, username);	fileSystem.close();	return null;	}	});	} catch (Exception e) {	
webhdfs exception 

out.write(b, 0, numBytes);	}	in.close();	out.close();	fileSystem.setOwner(path, username, username);	fileSystem.close();	return null;	}	});	} catch (Exception e) {	
webhdfs exception 

========================= ambari sample_1289 =========================

public void readViewArchives() {	boolean systemViewsOnly = configuration.extractViewsAfterClusterConfig() && clustersProvider.get().getClusters().isEmpty();	
triggering loading of views SYSTEM ALL 

public void installViewInstance(ViewInstanceEntity instanceEntity) throws ValidationException, IllegalArgumentException, SystemException {	ViewEntity viewEntity = getDefinition(instanceEntity.getViewName());	if (viewEntity != null) {	String instanceName = instanceEntity.getName();	String viewName = viewEntity.getCommonName();	String version = viewEntity.getVersion();	if (getInstanceDefinition(viewName, version, instanceName) == null) {	if (LOG.isDebugEnabled()) {	
creating view instance 

public void uninstallViewInstance(ViewInstanceEntity instanceEntity) throws IllegalStateException {	try {	viewInstanceOperationHandler.uninstallViewInstance(instanceEntity);	updateCaches(instanceEntity);	} catch (IllegalStateException illegalStateExcpetion) {	
exception occurred while uninstalling view 

public void copyPrivileges(ViewInstanceEntity sourceInstanceEntity, ViewInstanceEntity targetInstanceEntity) {	
copy all privileges from to 

public void copyPrivileges(ViewInstanceEntity sourceInstanceEntity, ViewInstanceEntity targetInstanceEntity) {	List<PrivilegeEntity> targetInstancePrivileges = privilegeDAO.findByResourceId(targetInstanceEntity.getResource().getId());	if (targetInstancePrivileges.size() > 0) {	
target instance already has privileges assigned these will not be deleted manual clean up may be needed 

List<PrivilegeEntity> sourceInstancePrivileges = privilegeDAO.findByResourceId(sourceInstanceEntity.getResource().getId());	for (PrivilegeEntity sourcePrivilege : sourceInstancePrivileges) {	PrivilegeEntity targetPrivilege = new PrivilegeEntity();	targetPrivilege.setPrincipal(sourcePrivilege.getPrincipal());	targetPrivilege.setResource(targetInstanceEntity.getResource());	targetPrivilege.setPermission(sourcePrivilege.getPermission());	try {	privilegeDAO.create(targetPrivilege);	targetPrivilege.getPrincipal().getPrivileges().add(sourcePrivilege);	} catch (Exception e) {	
could not migrate privilege 

List<PrivilegeEntity> sourceInstancePrivileges = privilegeDAO.findByResourceId(sourceInstanceEntity.getResource().getId());	for (PrivilegeEntity sourcePrivilege : sourceInstancePrivileges) {	PrivilegeEntity targetPrivilege = new PrivilegeEntity();	targetPrivilege.setPrincipal(sourcePrivilege.getPrincipal());	targetPrivilege.setResource(targetInstanceEntity.getResource());	targetPrivilege.setPermission(sourcePrivilege.getPermission());	try {	privilegeDAO.create(targetPrivilege);	targetPrivilege.getPrincipal().getPrivileges().add(sourcePrivilege);	} catch (Exception e) {	
caught exception 

public Cluster getCluster(ViewInstanceDefinition viewInstance) {	if (viewInstance != null) {	Long clusterId = viewInstance.getClusterHandle();	if (clusterId != null && viewInstance.getClusterType() == ClusterType.LOCAL_AMBARI) {	try {	return new ClusterImpl(clustersProvider.get().getCluster(clusterId));	} catch (AmbariException e) {	
could not find the cluster identified by 

Collection<String> roles = com.google.common.collect.Lists.newArrayList();	if (autoConfig != null && !CollectionUtils.isEmpty(autoConfig.getRoles())) {	roles.addAll(autoConfig.getRoles());	}	for (StackId stackId : stackIds) {	try {	if (checkAutoInstanceConfig(autoConfig, stackId, event.getServiceName(), serviceNames)) {	installAutoInstance(clusterId, clusterName, cluster.getService(event.getServiceName()), viewEntity, viewName, viewConfig, autoConfig, roles);	}	} catch (Exception e) {	
can t auto create instance of view for cluster caught exception 

for (StackId stackId : stackIds) {	try {	if (checkAutoInstanceConfig(autoConfig, stackId, event.getServiceName(), serviceNames)) {	installAutoInstance(clusterId, clusterName, cluster.getService(event.getServiceName()), viewEntity, viewName, viewConfig, autoConfig, roles);	}	} catch (Exception e) {	}	}	}	} catch (AmbariException e) {	
unknown cluster id 

private void installAutoInstance(Long clusterId, String clusterName, Service service, ViewEntity viewEntity, String viewName, ViewConfig viewConfig, AutoInstanceConfig autoConfig, Collection<String> roles) throws SystemException, ValidationException {	
auto creating instance of view for cluster 

private void installAutoInstance(Long clusterId, String clusterName, Service service, ViewEntity viewEntity, String viewName, ViewConfig viewConfig, AutoInstanceConfig autoConfig, Collection<String> roles) throws SystemException, ValidationException {	ViewInstanceEntity viewInstanceEntity = createViewInstanceEntity(viewEntity, viewConfig, autoConfig);	updateHiveLLAPSettingsIfRequired(viewInstanceEntity, service);	viewInstanceEntity.setClusterHandle(clusterId);	installViewInstance(viewInstanceEntity);	setViewInstanceRoleAccess(viewInstanceEntity, roles);	try {	setViewUrl(viewInstanceEntity);	} catch (Exception urlCreateException) {	
error while creating an auto url for the view instance url should be created in view instance settings 

private void installAutoInstance(Long clusterId, String clusterName, Service service, ViewEntity viewEntity, String viewName, ViewConfig viewConfig, AutoInstanceConfig autoConfig, Collection<String> roles) throws SystemException, ValidationException {	ViewInstanceEntity viewInstanceEntity = createViewInstanceEntity(viewEntity, viewConfig, autoConfig);	updateHiveLLAPSettingsIfRequired(viewInstanceEntity, service);	viewInstanceEntity.setClusterHandle(clusterId);	installViewInstance(viewInstanceEntity);	setViewInstanceRoleAccess(viewInstanceEntity, roles);	try {	setViewUrl(viewInstanceEntity);	} catch (Exception urlCreateException) {	
view url creation error 

ViewInstancePropertyEntity propertyEntity = new ViewInstancePropertyEntity();	propertyEntity.setViewInstanceName(viewInstanceEntity.getName());	propertyEntity.setViewName(viewInstanceEntity.getViewName());	propertyEntity.setName(INTERACTIVE_KEY);	propertyEntity.setValue("true");	propertyEntity.setViewInstanceEntity(viewInstanceEntity);	viewInstanceEntity.getProperties().add(propertyEntity);	}	}	} catch (AmbariException e) {	
failed to update parameter for viewname version exception 

private void setViewUrl(ViewInstanceEntity instanceEntity) {	ViewInstanceEntity viewInstanceEntity = instanceDAO.findByName(instanceEntity.getViewName(), instanceEntity.getInstanceName());	Preconditions.checkNotNull(viewInstanceEntity);	ViewURLEntity viewUrl = viewInstanceEntity.getViewUrl();	if (viewUrl != null) {	
url exists for the auto instance new url will not be created 

return;	}	String urlName = getUrlName(viewInstanceEntity);	Optional<ViewURLEntity> existingUrl = viewURLDAO.findByName(urlName);	ViewURLEntity urlEntity = new ViewURLEntity();	urlEntity.setUrlName(urlName);	urlEntity.setUrlSuffix(viewInstanceEntity.getInstanceName().toLowerCase());	ViewURLEntity toSaveOrUpdate = existingUrl.or(urlEntity);	toSaveOrUpdate.setViewInstanceEntity(viewInstanceEntity);	if (existingUrl.isPresent()) {	
url already present for 

String urlName = getUrlName(viewInstanceEntity);	Optional<ViewURLEntity> existingUrl = viewURLDAO.findByName(urlName);	ViewURLEntity urlEntity = new ViewURLEntity();	urlEntity.setUrlName(urlName);	urlEntity.setUrlSuffix(viewInstanceEntity.getInstanceName().toLowerCase());	ViewURLEntity toSaveOrUpdate = existingUrl.or(urlEntity);	toSaveOrUpdate.setViewInstanceEntity(viewInstanceEntity);	if (existingUrl.isPresent()) {	viewURLDAO.update(toSaveOrUpdate);	} else {	
creating a new url for auto instance 

toSaveOrUpdate.setViewInstanceEntity(viewInstanceEntity);	if (existingUrl.isPresent()) {	viewURLDAO.update(toSaveOrUpdate);	} else {	viewURLDAO.save(urlEntity);	}	viewInstanceEntity.setViewUrl(urlEntity);	try {	updateViewInstance(viewInstanceEntity);	} catch (Exception ex) {	
could not update the view instance with new url removing url 

public void onClusterConfigFinishedEvent(ClusterConfigFinishedEvent event) {	if (configuration.extractViewsAfterClusterConfig()) {	
trigger extracting non system views cluster 

public void onClusterConfigFinishedEvent(ClusterConfigFinishedEvent event) {	if (configuration.extractViewsAfterClusterConfig()) {	readNonSystemViewViewArchives();	
trigger extracting non system views cluster done 

private static Masker getMasker(Class<? extends Masker> clazz) {	try {	return clazz.newInstance();	} catch (Exception e) {	
could not create masker instance 

private void removeUndeployedViews() {	for (ViewEntity viewEntity : viewDAO.findAll()) {	String name = viewEntity.getName();	if (!ViewRegistry.getInstance().viewDefinitions.containsKey(name)) {	try {	viewDAO.remove(viewEntity);	} catch (Exception e) {	
caught exception undeploying view 

private void syncView(ViewEntity view, Set<ViewInstanceEntity> instanceDefinitions) throws Exception {	String viewName = view.getName();	ViewEntity persistedView = viewDAO.findByName(viewName);	if (LOG.isDebugEnabled()) {	
syncing view 

private void syncView(ViewEntity view, Set<ViewInstanceEntity> instanceDefinitions) throws Exception {	String viewName = view.getName();	ViewEntity persistedView = viewDAO.findByName(viewName);	if (LOG.isDebugEnabled()) {	}	if (persistedView == null) {	if (LOG.isDebugEnabled()) {	
creating view 

} else {	instanceDAO.merge(persistedInstance);	bindViewInstance(view, persistedInstance);	instanceDefinitions.add(persistedInstance);	}	} else {	syncViewInstance(instance, persistedInstance);	}	}	if (LOG.isDebugEnabled()) {	
syncing view complete 

if (extractor.ensureExtractedArchiveDirectory(extractedArchivesPath)) {	try {	final ViewConfig viewConfig = archiveUtility.getViewConfigFromArchive(archiveFile);	String viewName = ViewEntity.getViewName(viewConfig.getName(), viewConfig.getVersion());	final String extractedArchiveDirPath = extractedArchivesPath + File.separator + viewName;	final File extractedArchiveDirFile = archiveUtility.getFile(extractedArchiveDirPath);	final ViewEntity viewDefinition = new ViewEntity(viewConfig, configuration, extractedArchiveDirPath);	addDefinition(viewDefinition);	readViewArchive(viewDefinition, archiveFile, extractedArchiveDirFile, ambariMetaInfoProvider.get().getServerVersion());	} catch (Exception e) {	
could not process archive at path 

});	}	} catch (Exception e) {	String msg = "Caught exception reading view archive " + archiveFile.getAbsolutePath();	LOG.error(msg, e);	}	}	}	}	} catch (Exception e) {	
caught exception reading view archives 

final ExecutorService executorService = getExecutorService(configuration);	for (Runnable runnable : extractionRunnables) {	executorService.submit(runnable);	}	}	if (configuration.isViewRemoveUndeployedEnabled()) {	removeUndeployedViews();	}	}	} else {	
could not create extracted view archive directory 

executorService.submit(runnable);	}	}	if (configuration.isViewRemoveUndeployedEnabled()) {	removeUndeployedViews();	}	}	} else {	}	} catch (Exception e) {	
caught exception reading view archives 

private synchronized void readViewArchive(ViewEntity viewDefinition, File archiveFile, File extractedArchiveDirFile, String serverVersion) {	setViewStatus(viewDefinition, ViewEntity.ViewStatus.DEPLOYING, "Deploying " + extractedArchiveDirFile + ".");	String extractedArchiveDirPath = extractedArchiveDirFile.getAbsolutePath();	
reading view archive 

for (InstanceConfig instanceConfig : viewConfig.getInstances()) {	ViewInstanceEntity instanceEntity = createViewInstanceDefinition(viewConfig, viewDefinition, instanceConfig);	instanceEntity.setXmlDriven(true);	instanceDefinitions.add(instanceEntity);	}	persistView(viewDefinition, instanceDefinitions);	if (getDefinition(viewDefinition.getViewName(), viewDefinition.getVersion()) != null) {	addAutoInstanceDefinition(viewDefinition);	}	setViewStatus(viewDefinition, ViewEntity.ViewStatus.DEPLOYED, "Deployed " + extractedArchiveDirPath + ".");	
view deployed 

private void migrateDataFromPreviousVersion(ViewEntity viewDefinition, String serverVersion) {	if (!viewDefinitions.containsKey(viewDefinition.getName())) {	
cancel auto migration of not loaded view 

private void migrateDataFromPreviousVersion(ViewEntity viewDefinition, String serverVersion) {	if (!viewDefinitions.containsKey(viewDefinition.getName())) {	return;	}	try {	for (ViewInstanceEntity instance : viewDefinition.getInstances()) {	
try to migrate the data from previous version of 

private void migrateDataFromPreviousVersion(ViewEntity viewDefinition, String serverVersion) {	if (!viewDefinitions.containsKey(viewDefinition.getName())) {	return;	}	try {	for (ViewInstanceEntity instance : viewDefinition.getInstances()) {	ViewInstanceEntity latestUnregisteredView = getLatestUnregisteredInstance(serverVersion, instance);	if (latestUnregisteredView != null) {	String instanceName = instance.getViewEntity().getName() + "/" + instance.getName();	try {	
found previous version of the view instance 

if (!viewDefinitions.containsKey(viewDefinition.getName())) {	return;	}	try {	for (ViewInstanceEntity instance : viewDefinition.getInstances()) {	ViewInstanceEntity latestUnregisteredView = getLatestUnregisteredInstance(serverVersion, instance);	if (latestUnregisteredView != null) {	String instanceName = instance.getViewEntity().getName() + "/" + instance.getName();	try {	getViewDataMigrationUtility().migrateData(instance, latestUnregisteredView, true);	
view data migrated 

return;	}	try {	for (ViewInstanceEntity instance : viewDefinition.getInstances()) {	ViewInstanceEntity latestUnregisteredView = getLatestUnregisteredInstance(serverVersion, instance);	if (latestUnregisteredView != null) {	String instanceName = instance.getViewEntity().getName() + "/" + instance.getName();	try {	getViewDataMigrationUtility().migrateData(instance, latestUnregisteredView, true);	} catch (ViewDataMigrationException e) {	
error occurred during migration 

Long clusterId = cluster.getClusterId();	Set<String> serviceNames = cluster.getServices().keySet();	for (String service : services) {	try {	Service svc = cluster.getService(service);	StackId stackId = svc.getDesiredStackId();	if (checkAutoInstanceConfig(autoInstanceConfig, stackId, service, serviceNames)) {	installAutoInstance(clusterId, clusterName, cluster.getService(service), viewEntity, viewName, viewConfig, autoInstanceConfig, roles);	}	} catch (Exception e) {	
can t auto create instance of view for cluster caught exception 

protected void setViewInstanceRoleAccess(ViewInstanceEntity viewInstanceEntity, Collection<String> roles) {	if ((roles != null) && !roles.isEmpty()) {	PermissionEntity permissionViewUser = permissionDAO.findViewUsePermission();	ResourceEntity resourceEntity = viewInstanceEntity.getResource();	if (null == resourceEntity) {	resourceEntity = instanceDAO.findResourceForViewInstance(viewInstanceEntity.getViewName(), viewInstanceEntity.getInstanceName());	}	if (permissionViewUser == null) {	
missing the role access to view cannot be set 

PermissionEntity permissionViewUser = permissionDAO.findViewUsePermission();	ResourceEntity resourceEntity = viewInstanceEntity.getResource();	if (null == resourceEntity) {	resourceEntity = instanceDAO.findResourceForViewInstance(viewInstanceEntity.getViewName(), viewInstanceEntity.getInstanceName());	}	if (permissionViewUser == null) {	} else {	for (String role : roles) {	PermissionEntity permissionRole = permissionDAO.findByName(role);	if (permissionRole == null) {	
invalid role encountered while setting access to view ignoring 

resourceEntity = instanceDAO.findResourceForViewInstance(viewInstanceEntity.getViewName(), viewInstanceEntity.getInstanceName());	}	if (permissionViewUser == null) {	} else {	for (String role : roles) {	PermissionEntity permissionRole = permissionDAO.findByName(role);	if (permissionRole == null) {	} else {	PrincipalEntity principalRole = permissionRole.getPrincipal();	if (principalRole == null) {	
missing principal id for role encountered while setting access to view ignoring 

for (File archiveDir : extractedArchives) {	if (archiveDir.isDirectory()) {	ViewConfig uViewConfig = archiveUtility.getViewConfigFromExtractedArchive(archiveDir.getPath(), false);	if (!uViewConfig.isSystem()) {	if (!uViewConfig.getName().equals(instance.getViewEntity().getViewName())) {	continue;	}	if (viewDefinitions.containsKey(ViewEntity.getViewName(uViewConfig.getName(), uViewConfig.getVersion()))) {	continue;	}	
unregistered extracted view found 

if (!uViewConfig.isSystem()) {	if (!uViewConfig.getName().equals(instance.getViewEntity().getViewName())) {	continue;	}	if (viewDefinitions.containsKey(ViewEntity.getViewName(uViewConfig.getName(), uViewConfig.getVersion()))) {	continue;	}	ViewEntity uViewDefinition = new ViewEntity(uViewConfig, configuration, archiveDir.getPath());	readViewArchive(uViewDefinition, archiveDir, archiveDir, serverVersion);	for (ViewInstanceEntity instanceEntity : uViewDefinition.getInstances()) {	
instance found 

ViewInstanceEntity latestPrevInstance = null;	for (ViewInstanceEntity unregInstance : unregInstancesTimestamps.keySet()) {	if (unregInstance.getName().equals(instance.getName())) {	if (unregInstancesTimestamps.get(unregInstance) > latestPrevInstanceTimestamp) {	latestPrevInstance = unregInstance;	latestPrevInstanceTimestamp = unregInstancesTimestamps.get(latestPrevInstance);	}	}	}	if (latestPrevInstance != null) {	
previous version of found 

for (ViewInstanceEntity unregInstance : unregInstancesTimestamps.keySet()) {	if (unregInstance.getName().equals(instance.getName())) {	if (unregInstancesTimestamps.get(unregInstance) > latestPrevInstanceTimestamp) {	latestPrevInstance = unregInstance;	latestPrevInstanceTimestamp = unregInstancesTimestamps.get(latestPrevInstance);	}	}	}	if (latestPrevInstance != null) {	} else {	
previous version of not found 

========================= ambari sample_3934 =========================

public SolrDocumentIterator query(String start, String end, Document subIntervalFrom, int rows) {	SolrQuery query = queryProperties.toQueryBuilder() .setInterval(start, end) .setDocument(subIntervalFrom) .build();	query.setRows(rows);	
executing solr query 

========================= ambari sample_182 =========================

this.clusterId = clusterId.longValue();	this.createTime = System.currentTimeMillis();	this.startTime = -1;	this.endTime = -1;	this.exclusive = false;	this.clusterHostInfo = "{}";	if (-1L != this.clusterId) {	try {	this.clusterName = clusters.getClusterById(this.clusterId).getClusterName();	} catch (AmbariException e) {	
could not load cluster with id the cluster may have been removed for request 

public Request(@Assisted RequestEntity entity, final StageFactory stageFactory, Clusters clusters){	if (entity == null) {	throw new RuntimeException("Request entity cannot be null.");	}	this.requestId = entity.getRequestId();	this.clusterId = entity.getClusterId();	if (-1L != this.clusterId) {	try {	this.clusterName = clusters.getClusterById(this.clusterId).getClusterName();	} catch (AmbariException e) {	
could not load cluster with id the cluster may have been removed for request 

========================= ambari sample_2763 =========================

if (upgradePack == null) {	throw new SystemException( String.format("Upgrade pack not found for the target repository version %s", upgradeCheckRequest.getTargetRepositoryVersion()));	}	List<AbstractCheckDescriptor> upgradeChecksToRun = upgradeCheckRegistry.getFilteredUpgradeChecks(upgradePack);	upgradeCheckRequest.setPrerequisiteCheckConfig(upgradePack.getPrerequisiteCheckConfig());	try {	Map<String, ServiceInfo> services = getManagementController().getAmbariMetaInfo().getServices( sourceStackId.getStackName(), sourceStackId.getStackVersion());	List<AbstractCheckDescriptor> serviceLevelUpgradeChecksToRun = upgradeCheckRegistry.getServiceLevelUpgradeChecks(upgradePack, services);	upgradeChecksToRun.addAll(serviceLevelUpgradeChecksToRun);	} catch (ParentObjectNotFoundException parentNotFoundException) {	
invalid stack version 

throw new SystemException( String.format("Upgrade pack not found for the target repository version %s", upgradeCheckRequest.getTargetRepositoryVersion()));	}	List<AbstractCheckDescriptor> upgradeChecksToRun = upgradeCheckRegistry.getFilteredUpgradeChecks(upgradePack);	upgradeCheckRequest.setPrerequisiteCheckConfig(upgradePack.getPrerequisiteCheckConfig());	try {	Map<String, ServiceInfo> services = getManagementController().getAmbariMetaInfo().getServices( sourceStackId.getStackName(), sourceStackId.getStackVersion());	List<AbstractCheckDescriptor> serviceLevelUpgradeChecksToRun = upgradeCheckRegistry.getServiceLevelUpgradeChecks(upgradePack, services);	upgradeChecksToRun.addAll(serviceLevelUpgradeChecksToRun);	} catch (ParentObjectNotFoundException parentNotFoundException) {	} catch (AmbariException ambariException) {	
unable to register all the custom prechecks from the services 

}	List<AbstractCheckDescriptor> upgradeChecksToRun = upgradeCheckRegistry.getFilteredUpgradeChecks(upgradePack);	upgradeCheckRequest.setPrerequisiteCheckConfig(upgradePack.getPrerequisiteCheckConfig());	try {	Map<String, ServiceInfo> services = getManagementController().getAmbariMetaInfo().getServices( sourceStackId.getStackName(), sourceStackId.getStackVersion());	List<AbstractCheckDescriptor> serviceLevelUpgradeChecksToRun = upgradeCheckRegistry.getServiceLevelUpgradeChecks(upgradePack, services);	upgradeChecksToRun.addAll(serviceLevelUpgradeChecksToRun);	} catch (ParentObjectNotFoundException parentNotFoundException) {	} catch (AmbariException ambariException) {	} catch (Exception e) {	
failed to register custom prechecks for the services 

========================= ambari sample_3522 =========================

public void save() {	ClusterConfigEntity entity = clusterDAO.findConfig(configId);	ClusterEntity clusterEntity = clusterDAO.findById(entity.getClusterId());	if (null != entity) {	
updating version with new configurations a new version will not be created 

========================= ambari sample_3172 =========================

protected ResourceProvider createResourceProvider(Resource.Type type) {	
creating resource provider for the type 

case UpgradeItem: return new UpgradeItemResourceProvider(managementController);	case UpgradeSummary: return new UpgradeSummaryResourceProvider(managementController);	case PreUpgradeCheck: return new PreUpgradeCheckResourceProvider(managementController);	case HostStackVersion: return new HostStackVersionResourceProvider(managementController);	case Stage: return new StageResourceProvider(managementController);	case OperatingSystem: return new OperatingSystemResourceProvider(managementController);	case Repository: return new RepositoryResourceProvider(managementController);	case Setting: return new SettingResourceProvider();	case Artifact: return new ArtifactResourceProvider(managementController);	case RemoteCluster: return new RemoteClusterResourceProvider();	
delegating creation of resource provider for to the abstractcontrollerresourceprovider 

========================= ambari sample_3488 =========================

private void cancelJob(CancelJob message) {	if (!executing || connectionDelegate == null) {	
cannot cancel job for user as currently the job is not running or started jobid 

private void cancelJob(CancelJob message) {	if (!executing || connectionDelegate == null) {	return;	}	
cancelling job for user jobid user 

private void cancelJob(CancelJob message) {	if (!executing || connectionDelegate == null) {	return;	}	try {	isCancelCalled = true;	connectionDelegate.cancel();	
cancelled jobid 

private void cancelJob(CancelJob message) {	if (!executing || connectionDelegate == null) {	return;	}	try {	isCancelCalled = true;	connectionDelegate.cancel();	} catch (SQLException e) {	
failed to cancel job jobid 

private void processCancel() {	executing = false;	if (isAsync() && jobId.isPresent()) {	
job canceled by user for jobid 

private void processResult(Optional<ResultSet> resultSetOptional) {	executing = false;	stopStatementExecutor();	
finished processing sql statements for job id sync job 

private void jobExecutionCompleted() {	
job execution completed for user results are ready to be fetched 

jobId = message.getJobId();	executionType = message.getType();	if (connectable == null) {	connectable = message.getConnectable(authParams);	}	try {	if (!connectable.isOpen()) {	connectable.connect();	}	} catch (ConnectionException e) {	
failed to create a hive connection 

private void updateJobStatus(String jobid, final String status) {	new JobSaver(jobid) {	protected void update(JobImpl job) {	job.setStatus(status);	job.setDuration(getUpdatedDuration(job.getDateSubmitted()));	}	}.save();	
stored job status for job id as 

private void saveGuid(final SaveGuidToDB message) {	new JobSaver(message.getJobId()) {	protected void update(JobImpl job) {	job.setGuid(message.getGuid());	}	}.save();	
stored guid for job id as 

private void saveDagInformation(final SaveDagInformation message) {	if(message.getDagId() == null && message.getDagName() == null && message.getApplicationId() == null) {	
cannot save dag information for job id as all the properties are null 

job.setApplicationId(message.getApplicationId());	}	if (message.getDagId() != null) {	job.setDagId(message.getDagId());	}	if(message.getDagName() != null) {	job.setDagName(message.getDagName());	}	}	}.save();	
store dag information for job job id dagname dagid applicationid 

private void checkInactivity() {	
inactivity check executing status 

private void checkTerminationInactivity() {	
termination check executing status 

private void cleanUp() {	if (jobId.isPresent()) {	
cleaning up resources for inactivity for jobid 

private void cleanUp() {	if (jobId.isPresent()) {	} else {	
cleaning up resources with inactivity for sync execution 

private void cleanUpWithTermination() {	this.executing = false;	
cleaning up resources with inactivity for execution 

========================= ambari sample_706 =========================

public TimelineMetricAppAggregator(TimelineMetricMetadataManager metadataManager, Configuration metricsConf) {	appIdsToAggregate = getAppIdsForHostAggregation(metricsConf);	hostedAppsMap = metadataManager.getHostedAppsCache();	metadataManagerInstance = metadataManager;	
appids configured for aggregation 

public void init() {	
initializing aggregation cycle 

public void cleanup() {	
cleanup aggregated data 

private void updateAppAggregatesFromHostMetric(TimelineClusterMetric clusterMetric, String hostname, Double metricValue) {	if (aggregateClusterMetrics == null) {	
aggregation requested without init call 

========================= ambari sample_356 =========================

StackId stackId = cluster.getCurrentStackVersion();	String version = stackId.getStackVersion() + ".1";	StackDAO stackDAO = injector.getInstance(StackDAO.class);	StackEntity stackEntity = stackDAO.find(stackId.getStackName(), stackId.getStackVersion());	assertNotNull(stackEntity);	RepositoryVersionEntity repositoryVersion = repositoryVersionDAO.findByStackAndVersion( stackId, version);	if (repositoryVersion == null) {	try {	repositoryVersion = repositoryVersionDAO.create(stackEntity, version, String.valueOf(System.currentTimeMillis()) + uniqueCounter.incrementAndGet(), "");	} catch (Exception ex) {	
caught exception 

public RepositoryVersionEntity getOrCreateRepositoryVersion(StackId stackId, String version) {	StackEntity stackEntity = null;	try {	stackEntity = createStack(stackId);	} catch (Exception e) {	
expected successful repository 

}	assertNotNull(stackEntity);	RepositoryVersionEntity repositoryVersion = repositoryVersionDAO.findByStackAndVersion( stackId, version);	if (repositoryVersion == null) {	try {	String operatingSystems = "[{\"OperatingSystems/ambari_managed_repositories\":\"true\",\"repositories\":[{\"Repositories/repo_id\":\"HDP\",\"Repositories/base_url\":\"\",\"Repositories/repo_name\":\"HDP\"},{\"Repositories/repo_id\":\"HDP-UTILS\",\"Repositories/base_url\":\"\",\"Repositories/repo_name\":\"HDP-UTILS\"}],\"OperatingSystems/os_type\":\"redhat6\"}]";	repositoryVersion = repositoryVersionDAO.create(stackEntity, version, String.valueOf(System.currentTimeMillis()) + uniqueCounter.incrementAndGet(), operatingSystems);	repositoryVersion.setResolved(true);	repositoryVersion = repositoryVersionDAO.merge(repositoryVersion);	} catch (Exception ex) {	
caught exception 

========================= ambari sample_2558 =========================

Object val = e.getValue();	if (val instanceof List) {	if (!((List) val).contains(paramVal)) continue loopPermissions;	} else if (!Objects.equals(val, paramVal)) continue loopPermissions;	}	}	if (permission.role == null) {	return MatchStatus.PERMITTED;	}	if (principal == null) {	
request has come without principal failed permission 

}	if (principal == null) {	return MatchStatus.USER_REQUIRED;	} else if (permission.role.contains("*")) {	return MatchStatus.PERMITTED;	}	for (String role : permission.role) {	Set<String> userRoles = infraUserRolesLookupStrategy.getUserRolesFromPrincipal(usersVsRoles, principal);	boolean validHostname = infraKerberosDomainValidator.validate(principal, userVsHosts, userVsHostRegex);	if (!validHostname) {	
hostname is not valid for principal 

return MatchStatus.PERMITTED;	}	for (String role : permission.role) {	Set<String> userRoles = infraUserRolesLookupStrategy.getUserRolesFromPrincipal(usersVsRoles, principal);	boolean validHostname = infraKerberosDomainValidator.validate(principal, userVsHosts, userVsHostRegex);	if (!validHostname) {	return MatchStatus.FORBIDDEN;	}	if (userRoles != null && userRoles.contains(role)) return MatchStatus.PERMITTED;	}	
this resource is configured to have a permission the principal does not have the right role 

for (String role : permission.role) {	Set<String> userRoles = infraUserRolesLookupStrategy.getUserRolesFromPrincipal(usersVsRoles, principal);	boolean validHostname = infraKerberosDomainValidator.validate(principal, userVsHosts, userVsHostRegex);	if (!validHostname) {	return MatchStatus.FORBIDDEN;	}	if (userRoles != null && userRoles.contains(role)) return MatchStatus.PERMITTED;	}	return MatchStatus.FORBIDDEN;	}	
no permissions configured for the resource so allowed to access 

Map.Entry e = (Map.Entry) o;	String roleName = (String) e.getKey();	usersVsRoles.put(roleName, readValueAsSet(map, roleName));	}	List<Map> perms = getListValue(initInfo, "permissions");	for (Map o : perms) {	Permission p;	try {	p = Permission.load(o);	} catch (Exception exp) {	
invalid permission 

========================= ambari sample_213 =========================

public void onEvent(AlertDefinitionDisabledEvent event) {	
received event 

========================= ambari sample_4526 =========================

public static File getFileFromClasspath(String filename) {	URL fileCompleteUrl = Thread.currentThread().getContextClassLoader().getResource(filename);	
file complete uri 

public static void createDirectory(String dirPath) {	File dir = new File(dirPath);	if (!dir.exists()) {	
directory does not exist creating 

========================= ambari sample_1374 =========================

public void runScript(ServiceInfo.ServiceAdvisorType serviceAdvisorType, StackAdvisorCommandType saCommandType, File actionDirectory) throws StackAdvisorException {	LOG.info(String.format("StackAdvisorRunner. serviceAdvisorType=%s, actionDirectory=%s, command=%s", serviceAdvisorType.toString(), actionDirectory, saCommandType));	String outputFile = actionDirectory + File.separator + "stackadvisor.out";	String errorFile = actionDirectory + File.separator + "stackadvisor.err";	String hostsFile = actionDirectory + File.separator + "hosts.json";	String servicesFile = actionDirectory + File.separator + "services.json";	LOG.info("StackAdvisorRunner. Expected files: hosts.json={}, services.json={}, output={}, error={}", hostsFile, servicesFile, outputFile, errorFile);	int stackAdvisorReturnCode = -1;	switch (serviceAdvisorType) {	case JAVA: org.apache.ambari.serviceadvisor.ServiceAdvisor serviceAdvisor = new org.apache.ambari.serviceadvisor.ServiceAdvisor();	
stackadvisorrunner runscript calling java serviceadvisor s run method 

LOG.info(String.format("StackAdvisorRunner. serviceAdvisorType=%s, actionDirectory=%s, command=%s", serviceAdvisorType.toString(), actionDirectory, saCommandType));	String outputFile = actionDirectory + File.separator + "stackadvisor.out";	String errorFile = actionDirectory + File.separator + "stackadvisor.err";	String hostsFile = actionDirectory + File.separator + "hosts.json";	String servicesFile = actionDirectory + File.separator + "services.json";	LOG.info("StackAdvisorRunner. Expected files: hosts.json={}, services.json={}, output={}, error={}", hostsFile, servicesFile, outputFile, errorFile);	int stackAdvisorReturnCode = -1;	switch (serviceAdvisorType) {	case JAVA: org.apache.ambari.serviceadvisor.ServiceAdvisor serviceAdvisor = new org.apache.ambari.serviceadvisor.ServiceAdvisor();	stackAdvisorReturnCode = serviceAdvisor.run(saCommandType.toString(), hostsFile, servicesFile, outputFile, errorFile);	
stackadvisorrunner runscript java serviceadvisor s return code d 

String outputFile = actionDirectory + File.separator + "stackadvisor.out";	String errorFile = actionDirectory + File.separator + "stackadvisor.err";	String hostsFile = actionDirectory + File.separator + "hosts.json";	String servicesFile = actionDirectory + File.separator + "services.json";	LOG.info("StackAdvisorRunner. Expected files: hosts.json={}, services.json={}, output={}, error={}", hostsFile, servicesFile, outputFile, errorFile);	int stackAdvisorReturnCode = -1;	switch (serviceAdvisorType) {	case JAVA: org.apache.ambari.serviceadvisor.ServiceAdvisor serviceAdvisor = new org.apache.ambari.serviceadvisor.ServiceAdvisor();	stackAdvisorReturnCode = serviceAdvisor.run(saCommandType.toString(), hostsFile, servicesFile, outputFile, errorFile);	break;	
stackadvisorrunner runscript calling python stack advisor 

private String printMessage(String type, String file) {	String message = null;	try {	message = FileUtils.readFileToString(new File(file)).trim();	
advisor script 

private String printMessage(String type, String file) {	String message = null;	try {	message = FileUtils.readFileToString(new File(file)).trim();	} catch (IOException io) {	
error in reading script log files 

========================= ambari sample_4176 =========================

try {	result = ugi.doAs(action);	succeeded = true;	} catch (IOException ex) {	if (!Strings.isNullOrEmpty(ex.getMessage()) && !ex.getMessage().contains("Cannot obtain block length for")) {	throw ex;	}	if (tryNumber >= 3) {	throw ex;	}	
hdfs threw ioexception cannot obtain block length exception retrying try 

try {	result = ugi.doAs(action);	succeeded = true;	} catch (IOException ex) {	if (!Strings.isNullOrEmpty(ex.getMessage()) && !ex.getMessage().contains("Cannot obtain block length for")) {	throw ex;	}	if (tryNumber >= 3) {	throw ex;	}	
retrying 

========================= ambari sample_1118 =========================

public void onAmbariEvent(HostsAddedEvent event) {	
received event 

ambariServiceDefinitions.addAll(serverDefinitions);	m_hostAlertLock.lock();	try {	for (AlertDefinition agentDefinition : ambariServiceDefinitions) {	AlertDefinitionEntity definition = m_alertDefinitionDao.findByName( clusterId, agentDefinition.getName());	if (null == definition) {	definition = m_alertDefinitionFactory.coerce(clusterId, agentDefinition);	try {	m_alertDefinitionDao.create(definition);	} catch (Exception e) {	
unable to create an alert definition named in cluster 

public void onAmbariEvent(HostsRemovedEvent event) {	
received event 

========================= ambari sample_4530 =========================

public void run() {	for (TopologyHostTask task : tasks) {	try {	LOG.info("Running task for accepted host offer for hostname = {}, task = {}", hostName, task.getType());	task.run();	} catch (Exception e) {	HostRequest hostRequest = task.getHostRequest();	
task for host failed due to 

========================= ambari sample_2667 =========================

public SecurityConfiguration createSecurityConfigurationFromRequest(Map<String, Object> properties, boolean persistEmbeddedDescriptor) {	SecurityConfiguration securityConfiguration = null;	
creating security configuration from properties 

public SecurityConfiguration createSecurityConfigurationFromRequest(Map<String, Object> properties, boolean persistEmbeddedDescriptor) {	SecurityConfiguration securityConfiguration = null;	Map<String, Object> securityProperties = (Map<String, Object>) properties.get(SECURITY_PROPERTY_ID);	if (securityProperties == null) {	
no security information properties provided returning null 

public SecurityConfiguration createSecurityConfigurationFromRequest(Map<String, Object> properties, boolean persistEmbeddedDescriptor) {	SecurityConfiguration securityConfiguration = null;	Map<String, Object> securityProperties = (Map<String, Object>) properties.get(SECURITY_PROPERTY_ID);	if (securityProperties == null) {	return securityConfiguration;	}	String securityTypeString = Strings.emptyToNull((String) securityProperties.get(TYPE_PROPERTY_ID));	if (securityTypeString == null) {	
type is missing from security block 

Map<String, Object> securityProperties = (Map<String, Object>) properties.get(SECURITY_PROPERTY_ID);	if (securityProperties == null) {	return securityConfiguration;	}	String securityTypeString = Strings.emptyToNull((String) securityProperties.get(TYPE_PROPERTY_ID));	if (securityTypeString == null) {	throw new IllegalArgumentException("Type missing from security block.");	}	SecurityType securityType = Enums.getIfPresent(SecurityType.class, securityTypeString).orNull();	if (securityType == null) {	
unsupported security type specified 

throw new IllegalArgumentException("Type missing from security block.");	}	SecurityType securityType = Enums.getIfPresent(SecurityType.class, securityTypeString).orNull();	if (securityType == null) {	throw new IllegalArgumentException("Invalid security type specified: " + securityTypeString);	}	if (securityType == SecurityType.KERBEROS) {	String descriptorReference = Strings.emptyToNull((String) securityProperties.get(KERBEROS_DESCRIPTOR_REFERENCE_PROPERTY_ID));	Object descriptorJsonMap = securityProperties.get(KERBEROS_DESCRIPTOR_PROPERTY_ID);	if (descriptorReference != null && descriptorJsonMap != null) {	
both kerberos descriptor and kerberos descriptor reference are set in the security configuration 

throw new IllegalArgumentException("Invalid security type specified: " + securityTypeString);	}	if (securityType == SecurityType.KERBEROS) {	String descriptorReference = Strings.emptyToNull((String) securityProperties.get(KERBEROS_DESCRIPTOR_REFERENCE_PROPERTY_ID));	Object descriptorJsonMap = securityProperties.get(KERBEROS_DESCRIPTOR_PROPERTY_ID);	if (descriptorReference != null && descriptorJsonMap != null) {	throw new IllegalArgumentException("Usage of properties : " + KERBEROS_DESCRIPTOR_PROPERTY_ID + " and " + KERBEROS_DESCRIPTOR_REFERENCE_PROPERTY_ID + " at the same time, is not allowed.");	}	String descriptorText = null;	if (descriptorJsonMap != null) {	
found embedded descriptor 

throw new IllegalArgumentException("Usage of properties : " + KERBEROS_DESCRIPTOR_PROPERTY_ID + " and " + KERBEROS_DESCRIPTOR_REFERENCE_PROPERTY_ID + " at the same time, is not allowed.");	}	String descriptorText = null;	if (descriptorJsonMap != null) {	descriptorText = jsonSerializer.<Map<String, Object>>toJson(descriptorJsonMap, Map.class);	if (persistEmbeddedDescriptor) {	descriptorReference = persistKerberosDescriptor(descriptorText);	}	securityConfiguration = new SecurityConfiguration(SecurityType.KERBEROS, descriptorReference, descriptorText);	} else if (descriptorReference != null) {	
found descriptor reference 

String descriptorText = null;	if (descriptorJsonMap != null) {	descriptorText = jsonSerializer.<Map<String, Object>>toJson(descriptorJsonMap, Map.class);	if (persistEmbeddedDescriptor) {	descriptorReference = persistKerberosDescriptor(descriptorText);	}	securityConfiguration = new SecurityConfiguration(SecurityType.KERBEROS, descriptorReference, descriptorText);	} else if (descriptorReference != null) {	securityConfiguration = loadSecurityConfigurationByReference(descriptorReference);	} else {	
there is no security descriptor found in the request 

if (persistEmbeddedDescriptor) {	descriptorReference = persistKerberosDescriptor(descriptorText);	}	securityConfiguration = new SecurityConfiguration(SecurityType.KERBEROS, descriptorReference, descriptorText);	} else if (descriptorReference != null) {	securityConfiguration = loadSecurityConfigurationByReference(descriptorReference);	} else {	securityConfiguration = new SecurityConfiguration(SecurityType.KERBEROS);	}	} else {	
there is no security configuration found in the request 

public SecurityConfiguration loadSecurityConfigurationByReference(String reference) {	SecurityConfiguration securityConfiguration = null;	
loading security configuration by reference 

public SecurityConfiguration loadSecurityConfigurationByReference(String reference) {	SecurityConfiguration securityConfiguration = null;	if (reference == null) {	
no security configuration reference provided 

public SecurityConfiguration loadSecurityConfigurationByReference(String reference) {	SecurityConfiguration securityConfiguration = null;	if (reference == null) {	throw new IllegalArgumentException("No security configuration reference provided!");	}	KerberosDescriptorEntity descriptorEntity = kerberosDescriptorDAO.findByName(reference);	if (descriptorEntity == null) {	
no security configuration found for the reference 

private String persistKerberosDescriptor(String descriptor) {	
generating new kerberos descriptor reference 

private String persistKerberosDescriptor(String descriptor) {	String kdReference = generateKerberosDescriptorReference();	KerberosDescriptor kerberosDescriptor = kerberosDescriptorFactory.createKerberosDescriptor(kdReference, descriptor);	
persisting kerberos descriptor 

private String generateKerberosDescriptorReference() {	String kdReference = UUID.randomUUID().toString();	
generated new kerberos descriptor reference 

========================= ambari sample_2688 =========================

protected void updateClusterEnv() throws AmbariException {	AmbariManagementController ambariManagementController = injector.getInstance( AmbariManagementController.class);	AmbariMetaInfo ambariMetaInfo = injector.getInstance(AmbariMetaInfo.class);	
updating stack features and stack tools config properties 

========================= ambari sample_2722 =========================

public void pigScriptMigration(String username, String instance, String startDate, String endDate, ViewContext view, MigrationResponse migrationresult, String jobid) throws IOException, ItemNotFound {	long startTime = System.currentTimeMillis();	final Logger logger = Logger.getLogger(PigScriptMigrationUtility.class);	Connection connectionHuedb = null;	Connection connectionAmbaridb = null;	logger.info("-------------------------------------");	
pig saved script migration started 

public void pigScriptMigration(String username, String instance, String startDate, String endDate, ViewContext view, MigrationResponse migrationresult, String jobid) throws IOException, ItemNotFound {	long startTime = System.currentTimeMillis();	final Logger logger = Logger.getLogger(PigScriptMigrationUtility.class);	Connection connectionHuedb = null;	Connection connectionAmbaridb = null;	logger.info("-------------------------------------");	logger.info("-------------------------------------");	int i = 0;	
start date 

public void pigScriptMigration(String username, String instance, String startDate, String endDate, ViewContext view, MigrationResponse migrationresult, String jobid) throws IOException, ItemNotFound {	long startTime = System.currentTimeMillis();	final Logger logger = Logger.getLogger(PigScriptMigrationUtility.class);	Connection connectionHuedb = null;	Connection connectionAmbaridb = null;	logger.info("-------------------------------------");	logger.info("-------------------------------------");	int i = 0;	
enddate date 

public void pigScriptMigration(String username, String instance, String startDate, String endDate, ViewContext view, MigrationResponse migrationresult, String jobid) throws IOException, ItemNotFound {	long startTime = System.currentTimeMillis();	final Logger logger = Logger.getLogger(PigScriptMigrationUtility.class);	Connection connectionHuedb = null;	Connection connectionAmbaridb = null;	logger.info("-------------------------------------");	logger.info("-------------------------------------");	int i = 0;	
instance is 

public void pigScriptMigration(String username, String instance, String startDate, String endDate, ViewContext view, MigrationResponse migrationresult, String jobid) throws IOException, ItemNotFound {	long startTime = System.currentTimeMillis();	final Logger logger = Logger.getLogger(PigScriptMigrationUtility.class);	Connection connectionHuedb = null;	Connection connectionAmbaridb = null;	logger.info("-------------------------------------");	logger.info("-------------------------------------");	int i = 0;	
hue username is 

String dirNameForPigScript, completeDirandFilePath, pigscriptFilename = "";	int pigInstanceTableName, sequence;	ArrayList<PigModel> dbpojoPigSavedscript = new ArrayList<PigModel>();	try {	String[] usernames = username.split(",");	int totalQueries = 0;	for (int k = 0; k < usernames.length; k++) {	connectionHuedb = DataSourceHueDatabase.getInstance(view.getProperties().get("huedrivername"), view.getProperties().get("huejdbcurl"), view.getProperties().get("huedbusername"), view.getProperties().get("huedbpassword")).getConnection();	username = usernames[k];	migrationresult.setProgressPercentage(0);	
migration started for user 

for (int k = 0; k < usernames.length; k++) {	connectionHuedb = DataSourceHueDatabase.getInstance(view.getProperties().get("huedrivername"), view.getProperties().get("huejdbcurl"), view.getProperties().get("huedbusername"), view.getProperties().get("huedbpassword")).getConnection();	username = usernames[k];	migrationresult.setProgressPercentage(0);	dbpojoPigSavedscript = pigsavedscriptmigration.fetchFromHueDatabase(username, startDate, endDate, connectionHuedb, huedatabase);	totalQueries += dbpojoPigSavedscript.size();	for (int j = 0; j < dbpojoPigSavedscript.size(); j++) {	logger.info("the query fetched from hue=" + dbpojoPigSavedscript.get(j).getScript());	}	if (dbpojoPigSavedscript.size() == 0) {	
no queries has been selected for the user between dates 

migrationresult.setProgressPercentage(0);	dbpojoPigSavedscript = pigsavedscriptmigration.fetchFromHueDatabase(username, startDate, endDate, connectionHuedb, huedatabase);	totalQueries += dbpojoPigSavedscript.size();	for (int j = 0; j < dbpojoPigSavedscript.size(); j++) {	logger.info("the query fetched from hue=" + dbpojoPigSavedscript.get(j).getScript());	}	if (dbpojoPigSavedscript.size() == 0) {	} else {	connectionAmbaridb = DataSourceAmbariDatabase.getInstance(view.getProperties().get("ambaridrivername"), view.getProperties().get("ambarijdbcurl"), view.getProperties().get("ambaridbusername"), view.getProperties().get("ambaridbpassword")).getConnection();	connectionAmbaridb.setAutoCommit(false);	
loop will continue for times 

connectionAmbaridb = DataSourceAmbariDatabase.getInstance(view.getProperties().get("ambaridrivername"), view.getProperties().get("ambarijdbcurl"), view.getProperties().get("ambaridbusername"), view.getProperties().get("ambaridbpassword")).getConnection();	connectionAmbaridb.setAutoCommit(false);	pigInstanceTableName = pigsavedscriptmigration.fetchInstanceTablenamePigScript(connectionAmbaridb, instance, ambaridatabase);	sequence = pigsavedscriptmigration.fetchSequenceno(connectionAmbaridb, pigInstanceTableName, ambaridatabase);	for (i = 0; i < dbpojoPigSavedscript.size(); i++) {	float calc = ((float) (i + 1)) / dbpojoPigSavedscript.size() * 100;	int progressPercentage = Math.round(calc);	migrationresult.setProgressPercentage(progressPercentage);	migrationresult.setNumberOfQueryTransfered(i + 1);	getResourceManager(view).update(migrationresult, jobid);	
loop no 

connectionAmbaridb.setAutoCommit(false);	pigInstanceTableName = pigsavedscriptmigration.fetchInstanceTablenamePigScript(connectionAmbaridb, instance, ambaridatabase);	sequence = pigsavedscriptmigration.fetchSequenceno(connectionAmbaridb, pigInstanceTableName, ambaridatabase);	for (i = 0; i < dbpojoPigSavedscript.size(); i++) {	float calc = ((float) (i + 1)) / dbpojoPigSavedscript.size() * 100;	int progressPercentage = Math.round(calc);	migrationresult.setProgressPercentage(progressPercentage);	migrationresult.setNumberOfQueryTransfered(i + 1);	getResourceManager(view).update(migrationresult, jobid);	logger.info("________________");	
the title of script 

completeDirandFilePath = dirNameForPigScript + pigscriptFilename;	pigsavedscriptmigration.writetPigScripttoLocalFile(dbpojoPigSavedscript.get(i).getScript(), dbpojoPigSavedscript.get(i).getTitle(), dbpojoPigSavedscript.get(i).getDt(), ConfigurationCheckImplementation.getHomeDir(), pigscriptFilename);	pigsavedscriptmigration.insertRowForPigScript(completeDirandFilePath, maxcountforsavequery, maxcountforpigsavedscript, time, timetobeInorder, epochTime, dbpojoPigSavedscript.get(i).getTitle(), connectionAmbaridb, pigInstanceTableName, instance, i, ambaridatabase, username);	if (view.getProperties().get("KerberoseEnabled").equals("y")) {	pigsavedscriptmigration.createDirPigScriptSecured(dirNameForPigScript, view.getProperties().get("namenode_URI_Ambari"), username, view.getProperties().get("PrincipalUserName"));	pigsavedscriptmigration.putFileinHdfsSecured(ConfigurationCheckImplementation.getHomeDir() + pigscriptFilename, dirNameForPigScript, view.getProperties().get("namenode_URI_Ambari"), username, view.getProperties().get("PrincipalUserName"));	} else {	pigsavedscriptmigration.createDirPigScript(dirNameForPigScript, view.getProperties().get("namenode_URI_Ambari"), username);	pigsavedscriptmigration.putFileinHdfs(ConfigurationCheckImplementation.getHomeDir() + pigscriptFilename, dirNameForPigScript, view.getProperties().get("namenode_URI_Ambari"), username);	}	
migrated to ambari 

pigsavedscriptmigration.putFileinHdfsSecured(ConfigurationCheckImplementation.getHomeDir() + pigscriptFilename, dirNameForPigScript, view.getProperties().get("namenode_URI_Ambari"), username, view.getProperties().get("PrincipalUserName"));	} else {	pigsavedscriptmigration.createDirPigScript(dirNameForPigScript, view.getProperties().get("namenode_URI_Ambari"), username);	pigsavedscriptmigration.putFileinHdfs(ConfigurationCheckImplementation.getHomeDir() + pigscriptFilename, dirNameForPigScript, view.getProperties().get("namenode_URI_Ambari"), username);	}	pigsavedscriptmigration.deletePigScriptLocalFile(ConfigurationCheckImplementation.getHomeDir(), pigscriptFilename);	}	pigsavedscriptmigration.updateSequenceno(connectionAmbaridb, maxcountforpigsavedscript, pigInstanceTableName, ambaridatabase);	connectionAmbaridb.commit();	}	
migration completed for user 

} else {	pigsavedscriptmigration.createDirPigScript(dirNameForPigScript, view.getProperties().get("namenode_URI_Ambari"), username);	pigsavedscriptmigration.putFileinHdfs(ConfigurationCheckImplementation.getHomeDir() + pigscriptFilename, dirNameForPigScript, view.getProperties().get("namenode_URI_Ambari"), username);	}	pigsavedscriptmigration.deletePigScriptLocalFile(ConfigurationCheckImplementation.getHomeDir(), pigscriptFilename);	}	pigsavedscriptmigration.updateSequenceno(connectionAmbaridb, maxcountforpigsavedscript, pigInstanceTableName, ambaridatabase);	connectionAmbaridb.commit();	}	}	
migration completed 

if (totalQueries == 0) {	migrationresult.setNumberOfQueryTransfered(0);	migrationresult.setTotalNoQuery(0);	} else {	migrationresult.setNumberOfQueryTransfered(totalQueries);	migrationresult.setTotalNoQuery(totalQueries);	migrationresult.setProgressPercentage(100);	}	getResourceManager(view).update(migrationresult, jobid);	} catch (SQLException e) {	
sql exception in ambari database 

} else {	migrationresult.setNumberOfQueryTransfered(totalQueries);	migrationresult.setTotalNoQuery(totalQueries);	migrationresult.setProgressPercentage(100);	}	getResourceManager(view).update(migrationresult, jobid);	} catch (SQLException e) {	migrationresult.setError("SQL Exception: " + e.getMessage());	try {	connectionAmbaridb.rollback();	
rollback done 

migrationresult.setNumberOfQueryTransfered(totalQueries);	migrationresult.setTotalNoQuery(totalQueries);	migrationresult.setProgressPercentage(100);	}	getResourceManager(view).update(migrationresult, jobid);	} catch (SQLException e) {	migrationresult.setError("SQL Exception: " + e.getMessage());	try {	connectionAmbaridb.rollback();	} catch (SQLException e1) {	
sql exception while doing roll back 

migrationresult.setProgressPercentage(100);	}	getResourceManager(view).update(migrationresult, jobid);	} catch (SQLException e) {	migrationresult.setError("SQL Exception: " + e.getMessage());	try {	connectionAmbaridb.rollback();	} catch (SQLException e1) {	}	} catch (ClassNotFoundException e2) {	
class not found exception 

getResourceManager(view).update(migrationresult, jobid);	} catch (SQLException e) {	migrationresult.setError("SQL Exception: " + e.getMessage());	try {	connectionAmbaridb.rollback();	} catch (SQLException e1) {	}	} catch (ClassNotFoundException e2) {	migrationresult.setError("Class Not Found Exception: " + e2.getMessage());	} catch (ParseException e) {	
parseexception 

migrationresult.setError("SQL Exception: " + e.getMessage());	try {	connectionAmbaridb.rollback();	} catch (SQLException e1) {	}	} catch (ClassNotFoundException e2) {	migrationresult.setError("Class Not Found Exception: " + e2.getMessage());	} catch (ParseException e) {	migrationresult.setError("Parse Exception: " + e.getMessage());	} catch (PropertyVetoException e) {	
propertyvetoexception 

} catch (ClassNotFoundException e2) {	migrationresult.setError("Class Not Found Exception: " + e2.getMessage());	} catch (ParseException e) {	migrationresult.setError("Parse Exception: " + e.getMessage());	} catch (PropertyVetoException e) {	migrationresult.setError("Property Veto Exception: " + e.getMessage());	} catch (URISyntaxException e) {	e.printStackTrace();	migrationresult.setError("URISyntaxException: " + e.getMessage());	} catch (Exception e) {	
generic exception 

migrationresult.setError("Property Veto Exception: " + e.getMessage());	} catch (URISyntaxException e) {	e.printStackTrace();	migrationresult.setError("URISyntaxException: " + e.getMessage());	} catch (Exception e) {	migrationresult.setError("Exception: " + e.getMessage());	} finally {	if (null != connectionAmbaridb) try {	connectionAmbaridb.close();	} catch (SQLException e) {	
connection close exception 

migrationresult.setError("Error Closing Connection: " + e.getMessage());	}	getResourceManager(view).update(migrationresult, jobid);	}	long stopTime = System.currentTimeMillis();	long elapsedTime = stopTime - startTime;	migrationresult.setJobtype("pigsavedscriptmigration");	migrationresult.setTotalTimeTaken(String.valueOf(elapsedTime));	getResourceManager(view).update(migrationresult, jobid);	logger.info("----------------------------------");	
pig saved script migration ends 

========================= ambari sample_1270 =========================

public void dispatch(Notification notification) {	
sending email 

public void dispatch(Notification notification) {	if (null == notification.DispatchProperties) {	
unable to dispatch an email notification that does not contain smtp properties 

Properties properties = new Properties();	for (Entry<String, String> entry : notification.DispatchProperties.entrySet()) {	String key = entry.getKey();	String value = entry.getValue();	properties.put(key, value);	if (key.equals(JAVAMAIL_FROM_PROPERTY)) {	fromAddress = value;	}	}	if (null == notification.Recipients) {	
unable to dispatch an email notification that does not have recipients 

message.addRecipient(RecipientType.TO, address);	}	message.setSentDate(new Date());	message.setSubject(notification.Subject);	message.setText(notification.Body, "UTF-8", "html");	if (null != fromAddress) {	message.setFrom(fromAddress);	}	Transport.send(message);	if (LOG.isDebugEnabled()) {	
successfully dispatched email to 

if (null != fromAddress) {	message.setFrom(fromAddress);	}	Transport.send(message);	if (LOG.isDebugEnabled()) {	}	if (null != notification.Callback) {	notification.Callback.onSuccess(notification.CallbackIds);	}	} catch (Exception exception) {	
unable to dispatch notification via email 

notification.Callback.onSuccess(notification.CallbackIds);	}	} catch (Exception exception) {	if (null != notification.Callback) {	notification.Callback.onFailure(notification.CallbackIds);	}	} finally {	try {	session.getTransport().close();	} catch (MessagingException me) {	
dispatcher unable to close smtp transport 

public TargetConfigurationResult validateTargetConfig(Map<String, Object> properties) {	try {	Transport transport = getMailTransport(properties);	transport.connect();	transport.close();	} catch(AuthenticationFailedException e) {	
invalid credentials authentication failure 

public TargetConfigurationResult validateTargetConfig(Map<String, Object> properties) {	try {	Transport transport = getMailTransport(properties);	transport.connect();	transport.close();	} catch(AuthenticationFailedException e) {	return TargetConfigurationResult.invalid("Invalid credentials. Authentication failure: " + e.getMessage());	} catch(MessagingException e) {	
invalid config 

========================= ambari sample_4567 =========================

if (t != null) {	t.setName(String.format("%s active cleanup timer", InMemoryCredentialStore.class.getSimpleName()));	t.setDaemon(true);	}	return t;	}	};	Runnable runnable = new Runnable() {	public void run() {	if (LOG.isDebugEnabled()) {	
cleaning up cache due to retention timeout of milliseconds 

========================= ambari sample_2775 =========================

public void createJobs() {	if (propertyMap == null || propertyMap.getSolrDataDeleting() == null) return;	propertyMap.getSolrDataDeleting().values().forEach(DocumentDeletingProperties::validate);	propertyMap.getSolrDataDeleting().keySet().forEach(jobName -> {	
registering data deleting job 

========================= ambari sample_194 =========================

if (topology.isClusterKerberosEnabled()) {	Map<String, Object> securityConfigMap = new LinkedHashMap<>();	securityConfigMap.put(SecurityConfigurationFactory.TYPE_PROPERTY_ID, SecurityType.KERBEROS.name());	try {	String clusterName = topology.getAmbariContext().getClusterName(topology.getClusterId());	Map<String, Object> kerberosDescriptor = getKerberosDescriptor(topology.getAmbariContext() .getClusterController(), clusterName);	if (kerberosDescriptor != null) {	securityConfigMap.put(SecurityConfigurationFactory.KERBEROS_DESCRIPTOR_PROPERTY_ID, kerberosDescriptor);	}	} catch (AmbariException e) {	
unable to retrieve kerberos descriptor 

private Collection<Map<String, Object>> getSettings(TreeNode<Resource> clusterNode) {	
clusterblueprintrenderer getsettings 

========================= ambari sample_4079 =========================

public void getQueryWithAlterColumn(){	String origMetaString = "{  " + "  \"database\": \"default\",  " + "  \"table\": \"table2\",  " + "  \"columns\": [{  " + "   \"name\": \"COL1\",  " + "   \"type\": \"TINYINT\",  " + "   \"comment\": \"\",  " + "   \"precision\": null,  " + "   \"scale\": null  " + "  }, {  " + "   \"name\": \"col2\",  " + "   \"type\": \"VARCHAR\",  " + "   \"comment\": \"\",  " + "   \"precision\": \"333\",  " + "   \"scale\": null  " + "  }, {  " + "   \"name\": \"col3\",  " + "   \"type\": \"DECIMAL\",  " + "   \"comment\": \"\",  " + "   \"precision\": \"33\",  " + "   \"scale\": \"3\"  " + "  }],  " + "  \"partitionInfo\": {  " + "   \"columns\": []  " + "  },  " + "  \"detailedInfo\": {  " + "   \"parameters\": {}  " + "  },  " + "  \"storageInfo\": {}  " + " }";	String newMetaString = "{  " + "    \"database\": \"default\",  " + "    \"table\": \"table2\",  " + "    \"columns\": [{  " + "      \"name\": \"col1\",  " + "      \"type\": \"TINYINT\",  " + "      \"comment\": \"\",  " + "      \"precision\": null,  " + "      \"scale\": null  " + "    }, {  " + "      \"name\": \"col3\",  " + "      \"type\": \"STRING\",  " + "      \"comment\": \"\",  " + "      \"precision\": \"333\",  " + "      \"scale\": null  " + "    }, {  " + "      \"name\": \"col4\",  " + "      \"type\": \"TINYINT\",  " + "      \"comment\": \"\",  " + "      \"precision\": null,  " + "      \"scale\": null  " + "    }],  " + "    \"partitionInfo\": {  " + "      \"columns\": []  " + "    },  " + "    \"detailedInfo\": {  " + "      \"parameters\": {}  " + "    },  " + "    \"storageInfo\": {}  " + "  }";	Gson gson = new Gson();	TableMeta origTableMeta = gson.fromJson(origMetaString, TableMeta.class);	TableMeta updatedTableMeta = gson.fromJson(newMetaString, TableMeta.class);	
origtablemeta updatedtablemeta 

public void getQueryWithAlterColumn(){	String origMetaString = "{  " + "  \"database\": \"default\",  " + "  \"table\": \"table2\",  " + "  \"columns\": [{  " + "   \"name\": \"COL1\",  " + "   \"type\": \"TINYINT\",  " + "   \"comment\": \"\",  " + "   \"precision\": null,  " + "   \"scale\": null  " + "  }, {  " + "   \"name\": \"col2\",  " + "   \"type\": \"VARCHAR\",  " + "   \"comment\": \"\",  " + "   \"precision\": \"333\",  " + "   \"scale\": null  " + "  }, {  " + "   \"name\": \"col3\",  " + "   \"type\": \"DECIMAL\",  " + "   \"comment\": \"\",  " + "   \"precision\": \"33\",  " + "   \"scale\": \"3\"  " + "  }],  " + "  \"partitionInfo\": {  " + "   \"columns\": []  " + "  },  " + "  \"detailedInfo\": {  " + "   \"parameters\": {}  " + "  },  " + "  \"storageInfo\": {}  " + " }";	String newMetaString = "{  " + "    \"database\": \"default\",  " + "    \"table\": \"table2\",  " + "    \"columns\": [{  " + "      \"name\": \"col1\",  " + "      \"type\": \"TINYINT\",  " + "      \"comment\": \"\",  " + "      \"precision\": null,  " + "      \"scale\": null  " + "    }, {  " + "      \"name\": \"col3\",  " + "      \"type\": \"STRING\",  " + "      \"comment\": \"\",  " + "      \"precision\": \"333\",  " + "      \"scale\": null  " + "    }, {  " + "      \"name\": \"col4\",  " + "      \"type\": \"TINYINT\",  " + "      \"comment\": \"\",  " + "      \"precision\": null,  " + "      \"scale\": null  " + "    }],  " + "    \"partitionInfo\": {  " + "      \"columns\": []  " + "    },  " + "    \"detailedInfo\": {  " + "      \"parameters\": {}  " + "    },  " + "    \"storageInfo\": {}  " + "  }";	Gson gson = new Gson();	TableMeta origTableMeta = gson.fromJson(origMetaString, TableMeta.class);	TableMeta updatedTableMeta = gson.fromJson(newMetaString, TableMeta.class);	AlterTableQueryGenerator generator = new AlterTableQueryGenerator(origTableMeta, updatedTableMeta);	Optional<String> query = generator.getQuery();	Assert.assertTrue(query.isPresent());	String hqlQuery = query.get();	
hqlquery 

========================= ambari sample_520 =========================

public static String getUnixFilePermissions(String path) {	String result = MASK_EVERYBODY_RWX;	if (LINUX) {	try {	result = runCommand(new String[]{"stat", "-c", "%a", path}).getStdout();	} catch (IOException | InterruptedException e) {	
can not perform stat on s 

public static String getUnixFilePermissions(String path) {	String result = MASK_EVERYBODY_RWX;	if (LINUX) {	try {	result = runCommand(new String[]{"stat", "-c", "%a", path}).getStdout();	} catch (IOException | InterruptedException e) {	}	} else {	
not performing stat s command on file s because current os is not linux returning 

public static void setUnixFilePermissions(String mode, String path) {	if (LINUX) {	try {	runCommand(new String[]{"chmod", mode, path});	} catch (IOException | InterruptedException e) {	
can not perform chmod s s 

public static void setUnixFilePermissions(String mode, String path) {	if (LINUX) {	try {	runCommand(new String[]{"chmod", mode, path});	} catch (IOException | InterruptedException e) {	}	} else {	
not performing chmod s command for file s because current os is not linux 

public static Result setFileOwner(String path, String ownerName) {	if (LINUX) {	if (!StringUtils.isEmpty(ownerName)) {	try {	return runCommand(new String[]{"chown", ownerName, path}, null, null, true);	} catch (IOException | InterruptedException e) {	
can not perform chown s s 

if (!StringUtils.isEmpty(ownerName)) {	try {	return runCommand(new String[]{"chown", ownerName, path}, null, null, true);	} catch (IOException | InterruptedException e) {	return new Result(-1, "", "Cannot perform operation: " + e.getLocalizedMessage());	}	} else {	return new Result(0, "", "");	}	} else {	
not performing chown command for file s because current os is not linux 

public static Result setFileGroup(String path,  String groupName) {	if (LINUX) {	if (!StringUtils.isEmpty(groupName)) {	try {	return runCommand(new String[]{"chgrp", groupName, path}, null, null, true);	} catch (IOException | InterruptedException e) {	
can not perform chgrp s s 

if (!StringUtils.isEmpty(groupName)) {	try {	return runCommand(new String[]{"chgrp", groupName, path}, null, null, true);	} catch (IOException | InterruptedException e) {	return new Result(-1, "", "Cannot perform operation: " + e.getLocalizedMessage());	}	} else {	return new Result(0, "", "");	}	} else {	
not performing chgrp command for file s because current os is not linux 

public static Result setFileMode(String path, boolean ownerReadable, boolean ownerWritable, boolean ownerExecutable, boolean groupReadable, boolean groupWritable, boolean groupExecutable, boolean otherReadable, boolean otherWritable, boolean otherExecutable) {	if (LINUX) {	int modeValue = ((ownerReadable) ? MODE_OWNER_READABLE : 0) + ((ownerWritable) ? MODE_OWNER_WRITABLE : 0) + ((ownerExecutable) ? MODE_OWNER_EXECUTABLE : 0) + ((groupReadable) ? MODE_GROUP_READABLE : 0) + ((groupWritable) ? MODE_GROUP_WRITABLE : 0) + ((groupExecutable) ? MODE_GROUP_EXECUTABLE : 0) + ((otherReadable) ? MODE_OTHER_READABLE : 0) + ((otherWritable) ? MODE_OTHER_WRITABLE : 0) + ((otherExecutable) ? MODE_OTHER_EXECUTABLE : 0);	String mode = String.format("%04d", modeValue);	try {	return runCommand(new String[]{"chmod", mode, path}, null, null, true);	} catch (IOException | InterruptedException e) {	
can not perform chmod s s 

public static Result setFileMode(String path, boolean ownerReadable, boolean ownerWritable, boolean ownerExecutable, boolean groupReadable, boolean groupWritable, boolean groupExecutable, boolean otherReadable, boolean otherWritable, boolean otherExecutable) {	if (LINUX) {	int modeValue = ((ownerReadable) ? MODE_OWNER_READABLE : 0) + ((ownerWritable) ? MODE_OWNER_WRITABLE : 0) + ((ownerExecutable) ? MODE_OWNER_EXECUTABLE : 0) + ((groupReadable) ? MODE_GROUP_READABLE : 0) + ((groupWritable) ? MODE_GROUP_WRITABLE : 0) + ((groupExecutable) ? MODE_GROUP_EXECUTABLE : 0) + ((otherReadable) ? MODE_OTHER_READABLE : 0) + ((otherWritable) ? MODE_OTHER_WRITABLE : 0) + ((otherExecutable) ? MODE_OTHER_EXECUTABLE : 0);	String mode = String.format("%04d", modeValue);	try {	return runCommand(new String[]{"chmod", mode, path}, null, null, true);	} catch (IOException | InterruptedException e) {	return new Result(-1, "", "Cannot perform operation: " + e.getLocalizedMessage());	}	} else {	
not performing chmod command for file s because current os is not linux 

========================= ambari sample_2953 =========================

public void postConstruct() {	try {	isSpnegoEnable();	init(null);	} catch (ServletException e) {	
error while initializing filter 

protected void doFilter(FilterChain filterChain, HttpServletRequest request, HttpServletResponse response) throws IOException, ServletException {	
logsearchkrbauthenticationfilter private filter 

authentication = getGrantedAuthority(authentication);	SecurityContextHolder.getContext().setAuthentication(authentication);	request.getSession(true).setAttribute("SPRING_SECURITY_CONTEXT", SecurityContextHolder.getContext());	request.setAttribute("spnegoEnabled", true);	logger.info("Logged into Logsearch as = " + userName);	filterChain.doFilter(request, response);	} else {	try {	super.doFilter(filterChain, request, response);	} catch (Exception e) {	
error logsearchkrbauthenticationfilter 

public void doFilter(ServletRequest request, ServletResponse response, FilterChain filterChain) throws IOException, ServletException {	HttpServletRequest httpRequest = (HttpServletRequest) request;	
logsearchkrbauthenticationfilter public filter path 

((AbstractAuthenticationToken) finalAuthentication) .setDetails(webDetails);	Authentication authentication = this .authenticate(finalAuthentication);	authentication = getGrantedAuthority(authentication);	SecurityContextHolder.getContext().setAuthentication(authentication);	request.setAttribute("spnegoEnabled", true);	logger.info("Logged into Logsearch as = " + userName);	}else {	try {	super.doFilter(request, response, filterChain);	} catch (Exception e) {	
error logsearchkrbauthenticationfilter 

for (Cookie c : cookie) {	if (c.getName().equalsIgnoreCase(AUTH_COOKIE_NAME)) {	String cookieStr = c.getName() + "=" + c.getValue();	Matcher m = usernamePattern.matcher(cookieStr);	if (m.find()) {	userName = m.group(1);	}	}	}	}	
kerberos username from request 

userName = m.group(1);	}	}	}	if (StringUtils.isNotEmpty(userName)) {	break;	}	}	}	}	
kerberos username from response 

========================= ambari sample_1386 =========================

public void createComponents(Set<ServiceComponentRequest> requests) throws AmbariException, AuthorizationException {	if (requests.isEmpty()) {	
received an empty requests set 

sc.setDesiredRepositoryVersion(s.getDesiredRepositoryVersion());	if (StringUtils.isNotEmpty(request.getDesiredState())) {	State state = State.valueOf(request.getDesiredState());	sc.setDesiredState(state);	} else {	sc.setDesiredState(s.getDesiredState());	}	if (StringUtils.isNotEmpty(request.getRecoveryEnabled())) {	boolean recoveryEnabled = Boolean.parseBoolean(request.getRecoveryEnabled());	sc.setRecoveryEnabled(recoveryEnabled);	
component recovery enabled from request 

if (StringUtils.isNotEmpty(request.getRecoveryEnabled())) {	boolean recoveryEnabled = Boolean.parseBoolean(request.getRecoveryEnabled());	sc.setRecoveryEnabled(recoveryEnabled);	} else {	StackId stackId = s.getDesiredStackId();	ComponentInfo componentInfo = ambariMetaInfo.getComponent(stackId.getStackName(), stackId.getStackVersion(), s.getName(), request.getComponentName());	if (componentInfo == null) {	throw new AmbariException("Could not get component information from stack definition: Stack=" + stackId + ", Service=" + s.getName() + ", Component=" + request.getComponentName());	}	sc.setRecoveryEnabled(componentInfo.isRecoveryEnabled());	
component recovery enabled from stack definition 

protected RequestStatusResponse updateComponents(Set<ServiceComponentRequest> requests, Map<String, String> requestProperties, boolean runSmokeTest) throws AmbariException, AuthorizationException {	if (requests.isEmpty()) {	
received an empty requests set 

Set<String> clusterNames = new HashSet<>();	Map<String, Map<String, Set<String>>> componentNames = new HashMap<>();	Set<State> seenNewStates = new HashSet<>();	Collection<ServiceComponent> recoveryEnabledComponents = new ArrayList<>();	Collection<ServiceComponent> recoveryDisabledComponents = new ArrayList<>();	Resource.Type reqOpLvl;	if (requestProperties.containsKey(RequestOperationLevel.OPERATION_LEVEL_ID)) {	RequestOperationLevel operationLevel = new RequestOperationLevel(requestProperties);	reqOpLvl = operationLevel.getLevel();	} else {	
can not determine request operation level operation level property should be specified for this request 

RequestOperationLevel operationLevel = new RequestOperationLevel(requestProperties);	reqOpLvl = operationLevel.getLevel();	} else {	reqOpLvl = Resource.Type.Cluster;	}	for (ServiceComponentRequest request : requests) {	Validate.notEmpty(request.getComponentName(), "component name should be non-empty");	final Cluster cluster = getClusterForRequest(request, clusters);	final String clusterName = request.getClusterName();	final String componentName = request.getComponentName();	
received a updatecomponent request 

componentNames.get(clusterName).put(serviceName, new HashSet<>());	}	if (componentNames.get(clusterName).get(serviceName).contains(componentName)){	throw new IllegalArgumentException("Invalid request contains duplicate service components");	}	componentNames.get(clusterName).get(serviceName).add(componentName);	Service s = cluster.getService(serviceName);	ServiceComponent sc = s.getServiceComponent(componentName);	State newState = getValidDesiredState(request);	if (! maintenanceStateHelper.isOperationAllowed(reqOpLvl, s)) {	
operations cannot be applied to component because service is in the maintenance state of 

Service s = cluster.getService(serviceName);	ServiceComponent sc = s.getServiceComponent(componentName);	State newState = getValidDesiredState(request);	if (! maintenanceStateHelper.isOperationAllowed(reqOpLvl, s)) {	continue;	}	if (!StringUtils.isEmpty(request.getRecoveryEnabled())) {	AuthorizationHelper.verifyAuthorization(ResourceType.CLUSTER, getClusterResourceId(clusterName), EnumSet.of(RoleAuthorization.CLUSTER_MANAGE_AUTO_START, RoleAuthorization.SERVICE_MANAGE_AUTO_START));	boolean newRecoveryEnabled = Boolean.parseBoolean(request.getRecoveryEnabled());	boolean oldRecoveryEnabled = sc.isRecoveryEnabled();	
component oldrecoveryenabled newrecoveryenabled 

========================= ambari sample_3587 =========================

public synchronized Storage getStorage() {	if (storageInstance == null) {	String fileName = context.getProperties().get("dataworker.storagePath");	if (fileName != null) {	
using local storage in to store data 

public synchronized Storage getStorage() {	if (storageInstance == null) {	String fileName = context.getProperties().get("dataworker.storagePath");	if (fileName != null) {	storageInstance = new LocalKeyValueStorage(context);	} else {	
using persistence api to store data 

========================= ambari sample_1040 =========================

public void initialization() {	if (authPropsConfig.isAuthFileEnabled()) {	try {	String userPassJsonFileName = authPropsConfig.getCredentialsFile();	
user pass json file name 

public void initialization() {	if (authPropsConfig.isAuthFileEnabled()) {	try {	String userPassJsonFileName = authPropsConfig.getCredentialsFile();	File jsonFile = FileUtil.getFileFromClasspath(userPassJsonFileName);	if (jsonFile == null || !jsonFile.exists()) {	
user pass json file not found in classpath 

boolean isUpdated = this.encryptAllPassword();	userInfos.put("users", userList);	if (isUpdated) {	String jsonStr = JSONUtil.mapToJSON(userInfos);	JSONUtil.writeJSONInFile(jsonStr, jsonFile, true);	}	} else {	userList = new ArrayList<HashMap<String, String>>();	}	} catch (Exception exception) {	
error while reading user prop file 

String jsonStr = JSONUtil.mapToJSON(userInfos);	JSONUtil.writeJSONInFile(jsonStr, jsonFile, true);	}	} else {	userList = new ArrayList<HashMap<String, String>>();	}	} catch (Exception exception) {	userList = new ArrayList<HashMap<String, String>>();	}	} else {	
file auth is disabled 

public User loadUserByUsername(String username) {	
loaduserbyusername username 

String encPassword = user.get(ENC_PASSWORD);	String username = user.get(USER_NAME);	String password = user.get(PASSWORD);	if (StringUtils.isNotBlank(password)) {	encPassword = CommonUtil.encryptPassword(username, password);	user.put(PASSWORD, "");	user.put(ENC_PASSWORD, encPassword);	isUpdated = true;	}	if (StringUtils.isBlank(password) && StringUtils.isBlank(encPassword)) {	
password is empty or null for username 

========================= ambari sample_1410 =========================

public Map<String, String> getConfigurationChanges(Cluster cluster, ConfigUpgradePack configUpgradePack) {	Map<String, String> configParameters = new HashMap<>();	if (id == null || id.isEmpty()) {	
config task id is not defined skipping config change 

public Map<String, String> getConfigurationChanges(Cluster cluster, ConfigUpgradePack configUpgradePack) {	Map<String, String> configParameters = new HashMap<>();	if (id == null || id.isEmpty()) {	return configParameters;	}	if (configUpgradePack == null) {	
config upgrade pack is not defined skipping config change 

public Map<String, String> getConfigurationChanges(Cluster cluster, ConfigUpgradePack configUpgradePack) {	Map<String, String> configParameters = new HashMap<>();	if (id == null || id.isEmpty()) {	return configParameters;	}	if (configUpgradePack == null) {	return configParameters;	}	ConfigUpgradeChangeDefinition definition = configUpgradePack.enumerateConfigChangesByID().get(id);	if (definition == null) {	
can not resolve config change definition by id s skipping config change 

========================= ambari sample_3207 =========================

protected void unsuccessfulAuthentication(HttpServletRequest request, HttpServletResponse response, AuthenticationException failed) throws IOException, ServletException {	
login failed 

========================= ambari sample_1390 =========================

public static boolean addServiceReposToOperatingSystemEntities(List<OperatingSystemEntity> operatingSystems, ListMultimap<String, RepositoryInfo> stackReposByOs) {	Set<String> addedRepos = new HashSet<>();	for (OperatingSystemEntity os : operatingSystems) {	List<RepositoryInfo> serviceReposForOs = stackReposByOs.get(os.getOsType());	ImmutableSet<String> repoNames = ImmutableSet.copyOf(Lists.transform(os.getRepositories(), REPO_ENTITY_TO_NAME));	for (RepositoryInfo repoInfo : serviceReposForOs) if (!repoNames.contains(repoInfo.getRepoName())) {	os.getRepositories().add(toRepositoryEntity(repoInfo));	addedRepos.add(String.format("%s (%s)", repoInfo.getRepoId(), os.getOsType()));	}	}	
added service repos 

ListMultimap<String, RepositoryInfo> vdfReposByOs = Multimaps.index(vdfRepos, RepositoryInfo.GET_OSTYPE_FUNCTION);	for(String os: vdfReposByOs.keySet()) {	Set<String> vdfRepoNames = Sets.newHashSet( Lists.transform(vdfReposByOs.get(os), RepositoryInfo.GET_REPO_NAME_FUNCTION));	for (RepositoryInfo repo: stackReposByOs.get(os)) {	if (!vdfRepoNames.contains(repo.getRepoName())) {	serviceRepos.add(repo);	serviceRepoIds.add(repo.getRepoId());	}	}	}	
found service repos 

========================= ambari sample_4468 =========================

public Response update(String keyValues) throws WebApplicationException, InvalidStateTransitionException, JAXBException, IOException {	
received message from ui 

public String store(String values) throws IOException, JAXBException {	
received message from ui 

public String store(String values) throws IOException, JAXBException {	Collection<String> valueCollection = StageUtils.fromJson(values, Collection.class);	Collection<String> keys = new ArrayList<>(valueCollection.size());	for (String s : valueCollection) {	keys.add(persistKeyVal.put(s));	}	String stringRet = StageUtils.jaxbToString(keys);	
returning 

public String getKey( @PathParam("keyName") String keyName) {	
looking for keyname 

public String getAllKeyValues() throws JAXBException, IOException {	Map<String, String> ret = persistKeyVal.getAllKeyValues();	String stringRet = StageUtils.jaxbToString(ret);	
returning 

========================= ambari sample_4192 =========================

public static Properties getPersistenceProperties(Configuration configuration) {	Properties properties = new Properties();	DatabaseType databaseType = configuration.getDatabaseType();	
detected as the database type from the jdbc url 

properties.putAll(customPersistenceProperties);	boolean isConnectionPoolingExternal = false;	ConnectionPoolType connectionPoolType = configuration.getConnectionPoolType();	if (connectionPoolType == ConnectionPoolType.C3P0) {	isConnectionPoolingExternal = true;	}	if (databaseType == DatabaseType.MYSQL) {	isConnectionPoolingExternal = true;	}	if (isConnectionPoolingExternal) {	
using as the eclipslink datasource 

}	if (isConnectionPoolingExternal) {	String testQuery = "SELECT 1";	if (databaseType == DatabaseType.ORACLE) {	testQuery = "SELECT 1 FROM DUAL";	}	ComboPooledDataSource dataSource = new ComboPooledDataSource();	try {	dataSource.setDriverClass(configuration.getDatabaseDriver());	} catch (PropertyVetoException pve) {	
unable to initialize 

protected Set<Class<?>> bindByAnnotation(Set<Class<?>> matchedClasses) {	if (null == matchedClasses) {	List<Class<?>> classes = new ArrayList<>();	classes.add(EagerSingleton.class);	classes.add(StaticallyInject.class);	classes.add(AmbariService.class);	
searching package for annotations matching 

protected Set<Class<?>> bindByAnnotation(Set<Class<?>> matchedClasses) {	if (null == matchedClasses) {	List<Class<?>> classes = new ArrayList<>();	classes.add(EagerSingleton.class);	classes.add(StaticallyInject.class);	classes.add(AmbariService.class);	matchedClasses = ClasspathScannerUtils.findOnClassPath(AMBARI_PACKAGE, new ArrayList<>(), classes);	if (null == matchedClasses || matchedClasses.size() == 0) {	
no instances of found to register 

classes.add(AmbariService.class);	matchedClasses = ClasspathScannerUtils.findOnClassPath(AMBARI_PACKAGE, new ArrayList<>(), classes);	if (null == matchedClasses || matchedClasses.size() == 0) {	return matchedClasses;	}	}	Set<com.google.common.util.concurrent.Service> services = new HashSet<>();	for (Class<?> clazz : matchedClasses) {	if (null != clazz.getAnnotation(EagerSingleton.class)) {	bind(clazz).asEagerSingleton();	
eagerly binding singleton 

return matchedClasses;	}	}	Set<com.google.common.util.concurrent.Service> services = new HashSet<>();	for (Class<?> clazz : matchedClasses) {	if (null != clazz.getAnnotation(EagerSingleton.class)) {	bind(clazz).asEagerSingleton();	}	if (null != clazz.getAnnotation(StaticallyInject.class)) {	requestStaticInjection(clazz);	
statically injecting 

if (!com.google.common.util.concurrent.Service.class.isAssignableFrom(clazz)) {	String message = MessageFormat.format( "Unable to register service {0} because it is not a Service which can be scheduled", clazz);	LOG.error(message);	throw new RuntimeException(message);	}	com.google.common.util.concurrent.Service service = null;	try {	service = (com.google.common.util.concurrent.Service) clazz.newInstance();	bind((Class<com.google.common.util.concurrent.Service>) clazz).toInstance(service);	services.add(service);	
registering service 

String message = MessageFormat.format( "Unable to register service {0} because it is not a Service which can be scheduled", clazz);	LOG.error(message);	throw new RuntimeException(message);	}	com.google.common.util.concurrent.Service service = null;	try {	service = (com.google.common.util.concurrent.Service) clazz.newInstance();	bind((Class<com.google.common.util.concurrent.Service>) clazz).toInstance(service);	services.add(service);	} catch (Exception exception) {	
unable to register as a service 

protected Set<BeanDefinition> bindNotificationDispatchers(Set<BeanDefinition> beanDefinitions) {	DispatchFactory dispatchFactory = DispatchFactory.getInstance();	bind(DispatchFactory.class).toInstance(dispatchFactory);	if (null == beanDefinitions || beanDefinitions.isEmpty()) {	String packageName = AlertScriptDispatcher.class.getPackage().getName();	
searching package for dispatchers matching 

DispatchFactory dispatchFactory = DispatchFactory.getInstance();	bind(DispatchFactory.class).toInstance(dispatchFactory);	if (null == beanDefinitions || beanDefinitions.isEmpty()) {	String packageName = AlertScriptDispatcher.class.getPackage().getName();	ClassPathScanningCandidateComponentProvider scanner = new ClassPathScanningCandidateComponentProvider(false);	AssignableTypeFilter filter = new AssignableTypeFilter( NotificationDispatcher.class);	scanner.addIncludeFilter(filter);	beanDefinitions = scanner.findCandidateComponents(packageName);	}	if (null == beanDefinitions || beanDefinitions.size() == 0) {	
no instances of found to register 

NotificationDispatcher dispatcher;	if (clazz.equals(AmbariSNMPDispatcher.class)) {	dispatcher = (NotificationDispatcher) clazz.getConstructor(Integer.class).newInstance(configuration.getAmbariSNMPUdpBindPort());	} else if (clazz.equals(SNMPDispatcher.class)) {	dispatcher = (NotificationDispatcher) clazz.getConstructor(Integer.class).newInstance(configuration.getSNMPUdpBindPort());	} else {	dispatcher = (NotificationDispatcher) clazz.newInstance();	}	dispatchFactory.register(dispatcher.getType(), dispatcher);	bind((Class<NotificationDispatcher>) clazz).toInstance(dispatcher);	
binding and registering notification dispatcher 

if (clazz.equals(AmbariSNMPDispatcher.class)) {	dispatcher = (NotificationDispatcher) clazz.getConstructor(Integer.class).newInstance(configuration.getAmbariSNMPUdpBindPort());	} else if (clazz.equals(SNMPDispatcher.class)) {	dispatcher = (NotificationDispatcher) clazz.getConstructor(Integer.class).newInstance(configuration.getSNMPUdpBindPort());	} else {	dispatcher = (NotificationDispatcher) clazz.newInstance();	}	dispatchFactory.register(dispatcher.getType(), dispatcher);	bind((Class<NotificationDispatcher>) clazz).toInstance(dispatcher);	} catch (Exception exception) {	
unable to bind and register notification dispatcher 

protected Set<BeanDefinition> registerUpgradeChecks(Set<BeanDefinition> beanDefinitions) {	UpgradeCheckRegistry registry = new UpgradeCheckRegistry();	bind(UpgradeCheckRegistry.class).toInstance(registry);	if (null == beanDefinitions || beanDefinitions.isEmpty()) {	String packageName = AbstractCheckDescriptor.class.getPackage().getName();	
searching package for classes matching 

UpgradeCheckRegistry registry = new UpgradeCheckRegistry();	bind(UpgradeCheckRegistry.class).toInstance(registry);	if (null == beanDefinitions || beanDefinitions.isEmpty()) {	String packageName = AbstractCheckDescriptor.class.getPackage().getName();	ClassPathScanningCandidateComponentProvider scanner = new ClassPathScanningCandidateComponentProvider(false);	AssignableTypeFilter filter = new AssignableTypeFilter(AbstractCheckDescriptor.class);	scanner.addIncludeFilter(filter);	beanDefinitions = scanner.findCandidateComponents(packageName);	}	if (null == beanDefinitions || beanDefinitions.size() == 0) {	
no instances of found to register 

return null;	}	for (BeanDefinition beanDefinition : beanDefinitions) {	String className = beanDefinition.getBeanClassName();	Class<?> clazz = ClassUtils.resolveClassName(className, ClassUtils.getDefaultClassLoader());	try {	AbstractCheckDescriptor upgradeCheck = (AbstractCheckDescriptor) clazz.newInstance();	bind((Class<AbstractCheckDescriptor>) clazz).toInstance(upgradeCheck);	registry.register(upgradeCheck);	} catch (Exception exception) {	
unable to bind and register upgrade check 

Class<?> clazz = ClassUtils.resolveClassName(className, ClassUtils.getDefaultClassLoader());	try {	AbstractCheckDescriptor upgradeCheck = (AbstractCheckDescriptor) clazz.newInstance();	bind((Class<AbstractCheckDescriptor>) clazz).toInstance(upgradeCheck);	registry.register(upgradeCheck);	} catch (Exception exception) {	}	}	List<AbstractCheckDescriptor> upgradeChecks = registry.getUpgradeChecks();	for (AbstractCheckDescriptor upgradeCheck : upgradeChecks) {	
registered pre upgrade check 

========================= ambari sample_3657 =========================

impl.handleEvent(startEvent);	Assert.assertEquals(startTime, impl.getLastOpStartTime());	Assert.assertEquals(-1, impl.getLastOpLastUpdateTime());	Assert.assertEquals(-1, impl.getLastOpEndTime());	Assert.assertEquals(inProgressState, impl.getState());	if (checkStack) {	Assert.assertNotNull(impl.getServiceComponent().getDesiredStackId());	}	ServiceComponentHostEvent installEvent2 = createEvent(impl, ++timestamp, startEventType);	boolean exceptionThrown = false;	
transitioning from 

========================= ambari sample_2046 =========================

public void initDockerContainer() throws Exception {	System.setProperty("HADOOP_USER_NAME", "root");	URL location = AbstractInfraSteps.class.getProtectionDomain().getCodeSource().getLocation();	ambariFolder = new File(location.toURI()).getParentFile().getParentFile().getParentFile().getParent();	
clean local data folder 

public void initDockerContainer() throws Exception {	System.setProperty("HADOOP_USER_NAME", "root");	URL location = AbstractInfraSteps.class.getProtectionDomain().getCodeSource().getLocation();	ambariFolder = new File(location.toURI()).getParentFile().getParentFile().getParentFile().getParent();	FileUtils.cleanDirectory(new File(getLocalDataFolder()));	shellScriptLocation = ambariFolder + "/ambari-infra/ambari-infra-manager/docker/infra-manager-docker-compose.sh";	
create new docker container for testing ambari infra manager 

public void initDockerContainer() throws Exception {	System.setProperty("HADOOP_USER_NAME", "root");	URL location = AbstractInfraSteps.class.getProtectionDomain().getCodeSource().getLocation();	ambariFolder = new File(location.toURI()).getParentFile().getParentFile().getParentFile().getParent();	FileUtils.cleanDirectory(new File(getLocalDataFolder()));	shellScriptLocation = ambariFolder + "/ambari-infra/ambari-infra-manager/docker/infra-manager-docker-compose.sh";	runCommand(new String[]{shellScriptLocation, "start"});	dockerHost = System.getProperty("docker.host") != null ? System.getProperty("docker.host") : "localhost";	waitUntilSolrIsUp();	solrClient = new LBHttpSolrClient.Builder().withBaseSolrUrls(String.format("http: dockerHost, SOLR_PORT, AUDIT_LOGS_COLLECTION)).build();	
creating collection 

System.setProperty("HADOOP_USER_NAME", "root");	URL location = AbstractInfraSteps.class.getProtectionDomain().getCodeSource().getLocation();	ambariFolder = new File(location.toURI()).getParentFile().getParentFile().getParentFile().getParent();	FileUtils.cleanDirectory(new File(getLocalDataFolder()));	shellScriptLocation = ambariFolder + "/ambari-infra/ambari-infra-manager/docker/infra-manager-docker-compose.sh";	runCommand(new String[]{shellScriptLocation, "start"});	dockerHost = System.getProperty("docker.host") != null ? System.getProperty("docker.host") : "localhost";	waitUntilSolrIsUp();	solrClient = new LBHttpSolrClient.Builder().withBaseSolrUrls(String.format("http: dockerHost, SOLR_PORT, AUDIT_LOGS_COLLECTION)).build();	runCommand(new String[]{"docker", "exec", "docker_solr_1", "solr", "create_collection", "-c", AUDIT_LOGS_COLLECTION, "-d", "configsets/"+ AUDIT_LOGS_COLLECTION +"/conf", "-n", AUDIT_LOGS_COLLECTION + "_conf"});	
initializing client 

private void runCommand(String[] command) {	try {	
exec command 

private void runCommand(String[] command) {	try {	Process process = Runtime.getRuntime().exec(command);	String stdout = IOUtils.toString(process.getInputStream(), StandardCharsets.UTF_8);	
exec command result 

runnable.run();	return;	}	catch (Exception e) {	exception = e;	}	if (currentTimeMillis() - start > sec * 1000) {	throw new AssertionError(String.format("Unable to perform action '%s' within %d seconds", actionName, sec), exception);	}	else {	
performing action failed retrying 

private void checkInfraManagerReachable() throws Exception {	try (InfraClient httpClient = getInfraClient()) {	doWithin(30, "Start Ambari Infra Manager", httpClient::getJobs);	
ambari infra manager is up and running 

public void shutdownContainers() throws Exception {	Thread.sleep(2000);	ListObjectsRequest listObjectsRequest = new ListObjectsRequest().withBucketName(S3_BUCKET_NAME);	ObjectListing objectListing = getS3client().listObjects(listObjectsRequest);	
found files on 

public void shutdownContainers() throws Exception {	Thread.sleep(2000);	ListObjectsRequest listObjectsRequest = new ListObjectsRequest().withBucketName(S3_BUCKET_NAME);	ObjectListing objectListing = getS3client().listObjects(listObjectsRequest);	
found file on with key 

public void shutdownContainers() throws Exception {	Thread.sleep(2000);	ListObjectsRequest listObjectsRequest = new ListObjectsRequest().withBucketName(S3_BUCKET_NAME);	ObjectListing objectListing = getS3client().listObjects(listObjectsRequest);	
listing files on hdfs 

public void shutdownContainers() throws Exception {	Thread.sleep(2000);	ListObjectsRequest listObjectsRequest = new ListObjectsRequest().withBucketName(S3_BUCKET_NAME);	ObjectListing objectListing = getS3client().listObjects(listObjectsRequest);	try (FileSystem fileSystem = getHdfs()) {	int count = 0;	RemoteIterator<LocatedFileStatus> it = fileSystem.listFiles(new Path("/test_audit_logs"), true);	while (it.hasNext()) {	
found file on hdfs with name 

public void shutdownContainers() throws Exception {	Thread.sleep(2000);	ListObjectsRequest listObjectsRequest = new ListObjectsRequest().withBucketName(S3_BUCKET_NAME);	ObjectListing objectListing = getS3client().listObjects(listObjectsRequest);	try (FileSystem fileSystem = getHdfs()) {	int count = 0;	RemoteIterator<LocatedFileStatus> it = fileSystem.listFiles(new Path("/test_audit_logs"), true);	while (it.hasNext()) {	++count;	}	
files found on hfds 

Thread.sleep(2000);	ListObjectsRequest listObjectsRequest = new ListObjectsRequest().withBucketName(S3_BUCKET_NAME);	ObjectListing objectListing = getS3client().listObjects(listObjectsRequest);	try (FileSystem fileSystem = getHdfs()) {	int count = 0;	RemoteIterator<LocatedFileStatus> it = fileSystem.listFiles(new Path("/test_audit_logs"), true);	while (it.hasNext()) {	++count;	}	}	
shutdown containers 

========================= ambari sample_218 =========================

public Storage get(ViewContext context) {	
creating storage instance for viewname instance name 

========================= ambari sample_648 =========================

public Response getServiceCheckList(){	ServiceCheck serviceCheck = new ServiceCheck(context);	try {	ServiceCheck.Policy policy = serviceCheck.getServiceCheckPolicy();	JSONObject policyJson = new JSONObject();	policyJson.put("serviceCheckPolicy", policy);	return Response.ok(policyJson).build();	} catch (HdfsApiException e) {	
error occurred while generating service check policy 

========================= ambari sample_1056 =========================

private void parseProperties() throws HdfsApiException {	String defaultFS = getDefaultFS(context);	try {	if (isHAEnabled(defaultFS)) {	copyHAProperties(defaultFS);	
ha hdfs cluster found 

private void copyPropertyIfExists(String type, String key) {	String value;	if (context.getCluster() != null) {	value = context.getCluster().getConfigurationValue(type, key);	if (value != null) {	conf.set(key, value);	LOG.debug("set " + key + " = " + value);	} else {	
no such property 

private void copyPropertyIfExists(String type, String key) {	String value;	if (context.getCluster() != null) {	value = context.getCluster().getConfigurationValue(type, key);	if (value != null) {	conf.set(key, value);	LOG.debug("set " + key + " = " + value);	} else {	}	} else {	
no such property 

private void copyPropertiesBySite(String type) {	if (context.getCluster() != null) {	Map<String, String> configs = context.getCluster().getConfigByType(type);	
configs from core site 

private void copyPropertiesBySite(String type) {	if (context.getCluster() != null) {	Map<String, String> configs = context.getCluster().getConfigByType(type);	copyProperties(configs);	} else {	
cannot find cluster 

private void copyProperties(Map<String, String> configs) {	if (null != configs) {	for(Map.Entry<String, String> entry : configs.entrySet()){	String key = entry.getKey();	String value = entry.getValue();	conf.set(key, value);	}	} else {	
configs were null 

private boolean isHAEnabled(String defaultFS) throws URISyntaxException {	URI uri = new URI(defaultFS);	String nameservice = uri.getHost();	String namenodeIDs = getProperty(HDFS_SITE, String.format(HA_NAMENODES_CLUSTER_PROPERTY, nameservice), HA_NAMENODES_INSTANCE_PROPERTY);	
namenodeids 

copyPropertiesBySite(CORE_SITE);	copyPropertiesBySite(HDFS_SITE);	parseProperties();	setAuthParams(buildAuthenticationConfig());	String umask = context.getProperties().get(UMASK_INSTANCE_PROPERTY);	if (umask != null && !umask.isEmpty()) conf.set(UMASK_CLUSTER_PROPERTY, umask);	if(null != this.customProperties){	copyProperties(this.customProperties);	}	if(LOG.isDebugEnabled()){	
final conf 

private String printConf() {	try {	StringWriter stringWriter = new StringWriter();	conf.writeXml(stringWriter);	stringWriter.close();	return stringWriter.toString().replace("\n", "");	} catch (IOException e) {	
error while converting conf to xml 

========================= ambari sample_1113 =========================

}	if (propertyMaps != null) {	for (Map<String, Object> propertyMap : propertyMaps) {	Object object = propertyMap.get(PERMISSION_ID_PROPERTY_ID);	Collection<RoleAuthorizationEntity> authorizationEntities;	Integer permissionId;	if (object instanceof String) {	try {	permissionId = Integer.valueOf((String) object);	} catch (NumberFormatException e) {	
is not a valid integer value 

========================= ambari sample_3600 =========================

public LdapConnectionConfig createLdapConnectionConfig(AmbariLdapConfiguration ambariLdapConfiguration) throws AmbariLdapException {	
assembling ldap connection config based on 

public LdapConnectionConfig createLdapConnectionConfig(AmbariLdapConfiguration ambariLdapConfiguration) throws AmbariLdapException {	LdapConnectionConfig config = new LdapConnectionConfig();	config.setLdapHost(ambariLdapConfiguration.serverHost());	config.setLdapPort(ambariLdapConfiguration.serverPort());	config.setName(ambariLdapConfiguration.bindDn());	config.setCredentials(ambariLdapConfiguration.bindPassword());	config.setUseSsl(ambariLdapConfiguration.useSSL());	if ("custom".equals(ambariLdapConfiguration.trustStore())) {	
using custom trust manager configuration 

private TrustManager[] trustManagers(AmbariLdapConfiguration ambariLdapConfiguration) throws AmbariLdapException {	try {	TrustManagerFactory tmFactory = TrustManagerFactory.getInstance(getDefaultAlgorithm());	tmFactory.init(keyStore(ambariLdapConfiguration));	return tmFactory.getTrustManagers();	} catch (Exception e) {	
failed to initialize trust managers 

}	if (Strings.isEmpty(ambariLdapConfiguration.trustStorePath())) {	throw new AmbariLdapException("Key Store Path must be specified");	}	try {	KeyStore ks = KeyStore.getInstance(ambariLdapConfiguration.trustStoreType());	FileInputStream fis = new FileInputStream(ambariLdapConfiguration.trustStorePath());	ks.load(fis, ambariLdapConfiguration.trustStorePassword().toCharArray());	return ks;	} catch (Exception e) {	
failed to create keystore 

========================= ambari sample_2912 =========================

public void init(FilterConfig filterConfig) throws ServletException {	
initializing 

public void init(FilterConfig filterConfig) throws ServletException {	if (configuration == null) {	
the ambari configuration object is not available all default options will be assumed 

protected abstract boolean checkPrerequisites(ServletRequest servletRequest);	public void destroy() {	
destroying 

========================= ambari sample_2857 =========================

private void removeKerberosArtifact(Cluster cluster) throws AmbariException {	PredicateBuilder pb = new PredicateBuilder();	Predicate predicate = pb.begin().property("Artifacts/cluster_name").equals(cluster.getClusterName()).and(). property(ArtifactResourceProvider.ARTIFACT_NAME_PROPERTY).equals("kerberos_descriptor"). end().toPredicate();	ClusterController clusterController = ClusterControllerHelper.getClusterController();	ResourceProvider artifactProvider = clusterController.ensureResourceProvider(Resource.Type.Artifact);	try {	artifactProvider.deleteResources(new RequestImpl(null, null, null, null), predicate);	kerberosPrincipalDAO.remove(kerberosPrincipalDAO.findAll());	kerberosKeytabDAO.remove(kerberosKeytabDAO.findAll());	
kerberos descriptor removed successfully 

PredicateBuilder pb = new PredicateBuilder();	Predicate predicate = pb.begin().property("Artifacts/cluster_name").equals(cluster.getClusterName()).and(). property(ArtifactResourceProvider.ARTIFACT_NAME_PROPERTY).equals("kerberos_descriptor"). end().toPredicate();	ClusterController clusterController = ClusterControllerHelper.getClusterController();	ResourceProvider artifactProvider = clusterController.ensureResourceProvider(Resource.Type.Artifact);	try {	artifactProvider.deleteResources(new RequestImpl(null, null, null, null), predicate);	kerberosPrincipalDAO.remove(kerberosPrincipalDAO.findAll());	kerberosKeytabDAO.remove(kerberosKeytabDAO.findAll());	actionLog.writeStdOut("Kerberos descriptor removed successfully.");	} catch (NoSuchResourceException e) {	
the kerberos descriptor was not found in the database while attempting to remove 

========================= ambari sample_3315 =========================

public Response getList() {	try {	
getting all resources 

========================= ambari sample_536 =========================

public static void main(String args[]) {	if (args != null && args.length > 0) {	String action = args[0];	String alias = null;	String masterKey = null;	CredentialProvider credentialProvider = null;	Configuration configuration = new Configuration();	if (args.length > 1 && !args[1].isEmpty()) {	alias = args[1];	} else {	
no valid arguments provided 

String masterKey = null;	CredentialProvider credentialProvider = null;	Configuration configuration = new Configuration();	if (args.length > 1 && !args[1].isEmpty()) {	alias = args[1];	} else {	System.exit(1);	}	if (args.length > 3 && !args[3].isEmpty() && !args[3].equalsIgnoreCase("None")) {	masterKey = args[3];	
master key provided as an argument 

if (args.length > 2 && !args[2].isEmpty()) {	password = args[2];	}	if (alias != null && !alias.isEmpty() && password != null && !password.isEmpty()) {	try {	credentialProvider.addAliasToCredentialStore(alias, password);	} catch (AmbariException e) {	e.printStackTrace();	}	} else {	
alias and password are required arguments 

writeFilePath = args[2];	}	if (alias != null && !alias.isEmpty() && writeFilePath != null && !writeFilePath.isEmpty()) {	String passwd = "";	try {	char[] retPasswd = credentialProvider.getPasswordForAlias(alias);	if (retPasswd != null) {	passwd = new String(retPasswd);	}	} catch (AmbariException e) {	
error retrieving password for alias 

FileOutputStream fo = null;	try {	fo = new FileOutputStream(writeFilePath);	fo.write(passwd.getBytes());	} catch (IOException e) {	e.printStackTrace();	} finally {	Closeables.closeSilently(fo);	}	} else {	
alias and file path are required arguments 

fo.write(passwd.getBytes());	} catch (IOException e) {	e.printStackTrace();	} finally {	Closeables.closeSilently(fo);	}	} else {	}	}	} else {	
no arguments provided to CredentialProvider 

========================= ambari sample_2768 =========================


path download browsing path with download 

ResponseBuilder result = Response.ok(fs);	if (download) {	result.header("Content-Disposition", "attachment; filename=\"" + status.getPath().getName() + "\"").type(MediaType.APPLICATION_OCTET_STREAM);	} else {	FileNameMap fileNameMap = URLConnection.getFileNameMap();	String mimeType = fileNameMap.getContentTypeFor(status.getPath().getName());	result.header("Content-Disposition", "filename=\"" + status.getPath().getName() + "\"").type(mimeType);	}	return result.build();	} catch (WebApplicationException ex) {	
exception while browsing 

result.header("Content-Disposition", "attachment; filename=\"" + status.getPath().getName() + "\"").type(MediaType.APPLICATION_OCTET_STREAM);	} else {	FileNameMap fileNameMap = URLConnection.getFileNameMap();	String mimeType = fileNameMap.getContentTypeFor(status.getPath().getName());	result.header("Content-Disposition", "filename=\"" + status.getPath().getName() + "\"").type(mimeType);	}	return result.build();	} catch (WebApplicationException ex) {	throw ex;	} catch (FileNotFoundException ex) {	
file not found while browsing 

FileNameMap fileNameMap = URLConnection.getFileNameMap();	String mimeType = fileNameMap.getContentTypeFor(status.getPath().getName());	result.header("Content-Disposition", "filename=\"" + status.getPath().getName() + "\"").type(mimeType);	}	return result.build();	} catch (WebApplicationException ex) {	throw ex;	} catch (FileNotFoundException ex) {	throw new NotFoundFormattedException(ex.getMessage(), ex);	} catch (Exception ex) {	
exception while browsing 

try {	FSDataInputStream in = getApi().open(path);	zip.putNextEntry(new ZipEntry(path.substring(1)));	byte[] chunk = new byte[1024];	int readLen = 0;	while(readLen != -1) {	zip.write(chunk, 0, readLen);	readLen = in.read(chunk);	}	} catch (IOException ex) {	
error zipping file file ignored 

FSDataInputStream in = getApi().open(path);	zip.putNextEntry(new ZipEntry(path.substring(1)));	byte[] chunk = new byte[1024];	int readLen = 0;	while(readLen != -1) {	zip.write(chunk, 0, readLen);	readLen = in.read(chunk);	}	} catch (IOException ex) {	} catch (InterruptedException ex) {	
error zipping file file ignored 

while(readLen != -1) {	zip.write(chunk, 0, readLen);	readLen = in.read(chunk);	}	} catch (IOException ex) {	} catch (InterruptedException ex) {	} finally {	try {	zip.closeEntry();	} catch (IOException ex) {	
error closing entry file ignored 

private void zipDirectory(ZipOutputStream zip, String path) {	try {	zip.putNextEntry(new ZipEntry(path.substring(1) + "/"));	} catch (IOException ex) {	
error zipping directory directory ignored 

private void zipDirectory(ZipOutputStream zip, String path) {	try {	zip.putNextEntry(new ZipEntry(path.substring(1) + "/"));	} catch (IOException ex) {	} finally {	try {	zip.closeEntry();	} catch (IOException ex) {	
error zipping directory directory ignored 

public Response downloadGZip(final DownloadRequest request) {	
downloadgzip requested for 

files.add(file);	}	while (!files.isEmpty()) {	String path = files.poll();	FileStatus status = api.getFileStatus(path);	if (status.isDirectory()) {	FileStatus[] subdir;	try {	subdir = api.listdir(path);	} catch (AccessControlException ex) {	
error zipping directory directory ignored 

}	for (FileStatus file : subdir) {	files.add(org.apache.hadoop.fs.Path .getPathWithoutSchemeAndAuthority(file.getPath()) .toString());	}	zipDirectory(zip, path);	} else {	zipFile(zip, path);	}	}	} catch (Exception ex) {	
error occurred 

}	} catch (Exception ex) {	throw new ServiceFormattedException(ex.getMessage(), ex);	} finally {	zip.close();	}	}	};	return Response.ok(result) .header("Content-Disposition", "inline; filename=\"" + name +"\"").build();	} catch (WebApplicationException ex) {	
error occurred 

throw new ServiceFormattedException(ex.getMessage(), ex);	} finally {	zip.close();	}	}	};	return Response.ok(result) .header("Content-Disposition", "inline; filename=\"" + name +"\"").build();	} catch (WebApplicationException ex) {	throw ex;	} catch (Exception ex) {	
error occurred 

public Response concat(final DownloadRequest request) {	
starting concat files 

public Response concat(final DownloadRequest request) {	try {	StreamingOutput result = new StreamingOutput() {	public void write(OutputStream output) throws IOException, ServiceFormattedException {	FSDataInputStream in = null;	for (String path : request.entries) {	try {	try {	in = getApi().open(path);	} catch (AccessControlException ex) {	
error in opening file ignoring concat of this files 

for (String path : request.entries) {	try {	try {	in = getApi().open(path);	} catch (AccessControlException ex) {	continue;	}	long bytesCopied = IOUtils.copyLarge(in, output);	LOG.info("concated file : {}, total bytes added = {}", path, bytesCopied);	} catch (Exception ex) {	
error occurred 

}	};	ResponseBuilder response = Response.ok(result);	if (request.download) {	response.header("Content-Disposition", "attachment; filename=\"concatResult.txt\"").type(MediaType.APPLICATION_OCTET_STREAM);	} else {	response.header("Content-Disposition", "filename=\"concatResult.txt\"").type(MediaType.TEXT_PLAIN);	}	return response.build();	} catch (WebApplicationException ex) {	
error occurred 

ResponseBuilder response = Response.ok(result);	if (request.download) {	response.header("Content-Disposition", "attachment; filename=\"concatResult.txt\"").type(MediaType.APPLICATION_OCTET_STREAM);	} else {	response.header("Content-Disposition", "filename=\"concatResult.txt\"").type(MediaType.TEXT_PLAIN);	}	return response.build();	} catch (WebApplicationException ex) {	throw ex;	} catch (Exception ex) {	
error occurred 

public Response zipByRequestId(@QueryParam("requestId") String requestId) {	
starting zip download requestid 

public Response zipByRequestId(@QueryParam("requestId") String requestId) {	try {	DownloadRequest request = getDownloadRequest(requestId);	return downloadGZip(request);	} catch (WebApplicationException ex) {	
error occurred 

public Response zipByRequestId(@QueryParam("requestId") String requestId) {	try {	DownloadRequest request = getDownloadRequest(requestId);	return downloadGZip(request);	} catch (WebApplicationException ex) {	throw ex;	} catch (Exception ex) {	
error occurred 

public Response zipGenerateLink(final DownloadRequest request) {	
starting generate link 

public Response concatByRequestId(@QueryParam("requestId") String requestId) {	
starting concat for requestid 

public Response concatByRequestId(@QueryParam("requestId") String requestId) {	try {	DownloadRequest request = getDownloadRequest(requestId);	return concat(request);	} catch (WebApplicationException ex) {	
error occurred 

public Response concatByRequestId(@QueryParam("requestId") String requestId) {	try {	DownloadRequest request = getDownloadRequest(requestId);	return concat(request);	} catch (WebApplicationException ex) {	throw ex;	} catch (Exception ex) {	
error occurred 

public Response concatGenerateLink(final DownloadRequest request) {	
starting link generation for concat 

private Response generateLink(DownloadRequest request) {	try {	String requestId = generateUniqueIdentifer(request);	
returning generated requestid 

private void writeToHdfs(String uuid, String json) {	String fileName = getFileNameForRequestData(uuid);	try {	HdfsUtil.putStringToFile(getApi(), fileName, json);	} catch (HdfsApiException e) {	
failed to write request data to hdfs 

private String getFileNameForRequestData(String uuid) {	String tmpPath = context.getProperties().get("tmp.dir");	if (tmpPath == null) {	
tmp dir is not configured 

========================= ambari sample_1144 =========================

public <T extends Indexed> T load(Class<T> model, Object id) throws ItemNotFound {	String modelPropName = getItemPropertyName(model, id);	
loading s 

public <T extends Indexed> T load(Class<T> model, Object id) throws ItemNotFound {	String modelPropName = getItemPropertyName(model, id);	if (getConfig().containsKey(modelPropName)) {	String json = read(modelPropName);	
json s 

public synchronized <T extends Indexed> List<T> loadAll(Class<? extends T> model, FilteringStrategy filter) {	ArrayList<T> list = new ArrayList<T>();	String modelIndexingPropName = getIndexPropertyName(model);	
loading all s s 

public synchronized void delete(Class model, Object id) {	
deleting s s 

========================= ambari sample_920 =========================

AlertDefinition definition = m_definitionFactory.coerce(entity);	ServerSource serverSource = (ServerSource) definition.getSource();	List<AlertParameter> parameters = serverSource.getParameters();	for (AlertParameter parameter : parameters) {	Object value = parameter.getValue();	if (StringUtils.equals(parameter.getName(), STALE_INTERVAL_MULTIPLIER_PARAM_KEY)) {	waitFactor = getThresholdValue(value, INTERVAL_WAIT_FACTOR_DEFAULT);	}	}	if (waitFactor < 2 || waitFactor > 10) {	
the interval multipler of is outside the valid range for and will be set to 

for (AlertParameter parameter : parameters) {	Object value = parameter.getValue();	if (StringUtils.equals(parameter.getName(), STALE_INTERVAL_MULTIPLIER_PARAM_KEY)) {	waitFactor = getThresholdValue(value, INTERVAL_WAIT_FACTOR_DEFAULT);	}	}	if (waitFactor < 2 || waitFactor > 10) {	waitFactor = 2;	}	} catch (Exception exception) {	
unable to read the parameter for 

========================= ambari sample_3801 =========================

public Boolean call() throws Exception {	
Entering 

public Boolean call() throws Exception {	Collection<String> requiredHostGroups = getTopologyRequiredHostGroups();	if (!areHostGroupsResolved(requiredHostGroups)) {	
some host groups require more hosts cluster configuration cannot begin 

public Boolean call() throws Exception {	Collection<String> requiredHostGroups = getTopologyRequiredHostGroups();	if (!areHostGroupsResolved(requiredHostGroups)) {	throw new AsyncCallableService.RetryTaskSilently();	}	
all required host groups are complete cluster configuration can now begin 

public Boolean call() throws Exception {	Collection<String> requiredHostGroups = getTopologyRequiredHostGroups();	if (!areHostGroupsResolved(requiredHostGroups)) {	throw new AsyncCallableService.RetryTaskSilently();	}	configRequest.process();	
cluster configuration finished successfully 

public Boolean call() throws Exception {	Collection<String> requiredHostGroups = getTopologyRequiredHostGroups();	if (!areHostGroupsResolved(requiredHostGroups)) {	throw new AsyncCallableService.RetryTaskSilently();	}	configRequest.process();	notifyListeners();	
Exiting 

public long getTimeout() {	long timeout = DEFAULT_TIMEOUT;	String timeoutStr = topology.getConfiguration().getPropertyValue(ConfigHelper.CLUSTER_ENV, TIMEOUT_PROPERTY_NAME);	if (timeoutStr != null) {	try {	timeout = Long.parseLong(timeoutStr);	
using custom timeout ms 

private boolean areHostGroupsResolved(Collection<String> requiredHostGroups) {	boolean allHostGroupsResolved = true;	Map<String, HostGroupInfo> hostGroupInfo = topology.getHostGroupInfo();	for (String hostGroup : requiredHostGroups) {	HostGroupInfo groupInfo = hostGroupInfo.get(hostGroup);	if (groupInfo == null) {	allHostGroupsResolved = false;	if (missingHostGroups.add(hostGroup)) {	
host group is missing from cluster creation request 

if (missingHostGroups.add(hostGroup)) {	}	} else {	int actualHostCount = groupInfo.getHostNames().size();	int requestedHostCount = groupInfo.getRequestedHostCount();	boolean hostGroupReady = actualHostCount >= requestedHostCount;	allHostGroupsResolved &= hostGroupReady;	Integer previousHostCount = previousHostCounts.put(hostGroup, actualHostCount);	if (previousHostCount == null || previousHostCount != actualHostCount) {	if (hostGroupReady) {	
host group resolved requires hosts and are available 

}	} else {	int actualHostCount = groupInfo.getHostNames().size();	int requestedHostCount = groupInfo.getRequestedHostCount();	boolean hostGroupReady = actualHostCount >= requestedHostCount;	allHostGroupsResolved &= hostGroupReady;	Integer previousHostCount = previousHostCounts.put(hostGroup, actualHostCount);	if (previousHostCount == null || previousHostCount != actualHostCount) {	if (hostGroupReady) {	} else {	
host group pending requires hosts but only are available 

========================= ambari sample_2656 =========================

private void fillShortQueryField(SavedQuery savedQuery) {	if (savedQuery.getQueryFile() != null) {	FilePaginator paginator = new FilePaginator(savedQuery.getQueryFile(), sharedObjectsFactory.getHdfsApi());	String query = null;	try {	query = paginator.readPage(0);	} catch (IOException e) {	
can t read query file 

private void fillShortQueryField(SavedQuery savedQuery) {	if (savedQuery.getQueryFile() != null) {	FilePaginator paginator = new FilePaginator(savedQuery.getQueryFile(), sharedObjectsFactory.getHdfsApi());	String query = null;	try {	query = paginator.readPage(0);	} catch (IOException e) {	return;	} catch (InterruptedException e) {	
can t read query file 

========================= ambari sample_799 =========================

private void getCustomActionDefinitions(File customActionDefinitionRoot) throws JAXBException, AmbariException {	if (customActionDefinitionRoot != null) {	
loading custom action definitions from 

private void getCustomActionDefinitions(File customActionDefinitionRoot) throws JAXBException, AmbariException {	if (customActionDefinitionRoot != null) {	if (customActionDefinitionRoot.exists() && customActionDefinitionRoot.isDirectory()) {	adManager.readCustomActionDefinitions(customActionDefinitionRoot);	} else {	
no action definitions found at 

public Map<String, Map<String, List<MetricDefinition>>> getServiceMetrics(String stackName, String stackVersion, String serviceName) throws AmbariException {	ServiceInfo svc = getService(stackName, stackVersion, serviceName);	if (null == svc.getMetricsFile() || !svc.getMetricsFile().exists()) {	
metrics file for not found 

return null;	}	Map<String, Map<String, List<MetricDefinition>>> map = svc.getMetrics();	if (null == map) {	Type type = new TypeToken<Map<String, Map<String, List<MetricDefinition>>>>(){}.getType();	Gson gson = new Gson();	try {	map = gson.fromJson(new FileReader(svc.getMetricsFile()), type);	svc.setMetrics(processMetricDefinition(map));	} catch (Exception e) {	
could not read the metrics file 

public Set<AlertDefinition> getAlertDefinitions(ServiceInfo service) throws AmbariException {	File alertsFile = service.getAlertsFile();	if (null == alertsFile || !alertsFile.exists()) {	
alerts file for not found 

}	for( AlertDefinition stackDefinition : stackDefinitions ){	AlertDefinitionEntity entity = mappedEntities.get(stackDefinition.getName());	if (null == entity) {	entity = alertDefinitionFactory.coerce(clusterId, stackDefinition);	persist.add(entity);	continue;	}	AlertDefinition databaseDefinition = alertDefinitionFactory.coerce(entity);	if (!stackDefinition.deeplyEquals(databaseDefinition)) {	
the alert named has been modified from the stack definition and will not be merged 

Source stackSource = stackDefinition.getSource();	if (databaseSource.getType() == SourceType.SCRIPT && stackSource.getType() == SourceType.SCRIPT) {	ScriptSource databaseScript = (ScriptSource) databaseSource;	ScriptSource stackScript = (ScriptSource) stackSource;	String oldPath = databaseScript.getPath();	String newPath = stackScript.getPath();	if (!Objects.equals(oldPath, newPath)) {	databaseScript.setPath(newPath);	entity = alertDefinitionFactory.mergeSource(databaseScript, entity);	persist.add(entity);	
updating script path for the alert named from to 

entity = alertDefinitionFactory.mergeSource(databaseScript, entity);	persist.add(entity);	}	}	}	}	}	persist.addAll(getDefinitionsForMerge(ambariServiceAlertDefinitions.getAgentDefinitions(), clusterId, mappedEntities));	persist.addAll(getDefinitionsForMerge(ambariServiceAlertDefinitions.getServerDefinitions(), clusterId, mappedEntities));	for (AlertDefinitionEntity entity : persist) {	
merging alert definition into the database 

}	List<AlertDefinitionEntity> definitions = alertDefinitionDao.findAllEnabled(clusterId);	List<AlertDefinitionEntity> definitionsToDisable = new ArrayList<>();	for (AlertDefinitionEntity definition : definitions) {	String serviceName = definition.getServiceName();	String componentName = definition.getComponentName();	if (RootService.AMBARI.name().equals(serviceName)) {	continue;	}	if (!stackServiceMap.containsKey(serviceName)) {	
the service has been marked as deleted for cluster disabling alert 

for (AlertDefinitionEntity definition : definitions) {	String serviceName = definition.getServiceName();	String componentName = definition.getComponentName();	if (RootService.AMBARI.name().equals(serviceName)) {	continue;	}	if (!stackServiceMap.containsKey(serviceName)) {	definitionsToDisable.add(definition);	} else if (null != componentName && !stackComponentMap.containsKey(componentName)) {	StackId stackId = cluster.getService(serviceName).getDesiredStackId();	
the component has been marked as deleted for stack disabling alert 

public Map<String, UpgradePack> getUpgradePacks(String stackName, String stackVersion) {	try {	StackInfo stack = getStack(stackName, stackVersion);	return stack.getUpgradePacks() == null ? Collections.emptyMap() : stack.getUpgradePacks();	} catch (AmbariException e) {	
cannot load upgrade packs for non existent stack 

public ConfigUpgradePack getConfigUpgradePack(String stackName, String stackVersion) {	try {	StackInfo stack = getStack(stackName, stackVersion);	return stack.getConfigUpgradePack();	} catch (AmbariException e) {	
cannot load config upgrade pack for non existent stack 

public KerberosDescriptor getKerberosDescriptor(String stackName, String stackVersion, boolean includePreconfigureData) throws AmbariException {	StackInfo stackInfo = getStack(stackName, stackVersion);	KerberosDescriptor kerberosDescriptor = readKerberosDescriptorFromFile(getCommonKerberosDescriptorFileLocation());	if (kerberosDescriptor == null) {	
couldn t read common kerberos descriptor with path s 

public KerberosServiceDescriptor[] getKerberosDescriptor(ServiceInfo serviceInfo) throws AmbariException {	KerberosServiceDescriptor[] kerberosServiceDescriptors = null;	File kerberosFile = (serviceInfo == null) ? null : serviceInfo.getKerberosDescriptorFile();	if (kerberosFile != null) {	try {	kerberosServiceDescriptors = kerberosServiceDescriptorFactory.createInstances(kerberosFile);	} catch (Exception e) {	
could not read the kerberos descriptor file 

for (VersionDefinitionXml definition : stack.getVersionDefinitions()) {	versionDefinitions.put(String.format("%s-%s-%s", stack.getName(), stack.getVersion(), definition.release.version), definition);	}	try {	VersionDefinitionXml xml = stack.getLatestVersionDefinition();	if (null == xml) {	xml = VersionDefinitionXml.build(stack);	}	versionDefinitions.put(String.format("%s-%s", stack.getName(), stack.getVersion()), xml);	} catch (Exception e) {	
could not make a stack vdf for 

VersionDefinitionXml xml = stack.getLatestVersionDefinition();	if (null == xml) {	xml = VersionDefinitionXml.build(stack);	}	versionDefinitions.put(String.format("%s-%s", stack.getName(), stack.getVersion()), xml);	} catch (Exception e) {	}	} else {	StackId stackId = new StackId(stack);	if (!stack.isValid()) {	
stack is not valid skipping vdf 

if (null == xml) {	xml = VersionDefinitionXml.build(stack);	}	versionDefinitions.put(String.format("%s-%s", stack.getName(), stack.getVersion()), xml);	} catch (Exception e) {	}	} else {	StackId stackId = new StackId(stack);	if (!stack.isValid()) {	} else if (!stack.isActive()) {	
stack is not active skipping vdf 

========================= ambari sample_4106 =========================

public static void main(String[] args) {	try {	throwException();	} catch (ClassCastException castException) {	
error 

========================= ambari sample_1754 =========================

command.add(adminSeverHost);	}	String defaultRealm = getDefaultRealm();	if (!StringUtils.isEmpty(defaultRealm)) {	command.add("-r");	command.add(defaultRealm);	}	command.add("-q");	command.add(query);	if (LOG.isDebugEnabled()) {	
executing 

}	else if (stdErr.contains("Missing parameters in krb5.conf required for kadmin client")) {	throw new KerberosRealmException(stdErr);	} else if (stdErr.contains("Cannot find KDC for requested realm while initializing kadmin interface")) {	throw new KerberosRealmException(stdErr);	} else {	throw new KerberosOperationException(String.format("Unexpected error condition executing the kadmin command. STDERR: %s", stdErr));	}	} else {	if (LOG.isDebugEnabled()) {	
executed the following command stdout stderr 

========================= ambari sample_3293 =========================

public void init() {	
initializing metricsmanager 

public void init() {	if (amsClient == null) {	amsClient = new LogFeederAMSClient(metricsCollectorConfig, logFeederSecurityConfig);	}	if (amsClient.getCollectorUri(null) != null) {	if (LogFeederUtil.hostName == null) {	isMetricsEnabled = false;	
failed getting hostname for node disabling publishing logfeeder metrics 

amsClient = new LogFeederAMSClient(metricsCollectorConfig, logFeederSecurityConfig);	}	if (amsClient.getCollectorUri(null) != null) {	if (LogFeederUtil.hostName == null) {	isMetricsEnabled = false;	} else {	isMetricsEnabled = true;	LOG.info("LogFeeder Metrics is enabled. Metrics host=" + amsClient.getCollectorUri(null));	}	} else {	
logfeeder metrics publish is disabled 

private void gatherMetrics(List<MetricData> metricsList, long currMS) {	Long currMSLong = new Long(currMS);	for (MetricData metric : metricsList) {	if (metric.metricsName == null) {	
metric metricsname is null 

}	long currCount = metric.value;	if (!metric.isPointInTime && metric.publishCount > 0 && currCount <= metric.prevPublishValue) {	LOG.debug("Nothing changed. " + metric.metricsName + ", currCount=" + currCount + ", prevPublishCount=" + metric.prevPublishValue);	continue;	}	metric.publishCount++;	LOG.debug("Ensuring metrics=" + metric.metricsName);	TimelineMetric timelineMetric = metricsMap.get(metric.metricsName);	if (timelineMetric == null) {	
creating new metric obbject for 

private void publishMetrics(long currMS) {	if (!metricsMap.isEmpty() && currMS - lastPublishTimeMS > publishIntervalMS) {	try {	TimelineMetrics timelineMetrics = new TimelineMetrics();	timelineMetrics.setMetrics(new ArrayList<TimelineMetric>(metricsMap.values()));	amsClient.emitMetrics(timelineMetrics);	
published metrics to ams 

private void publishMetrics(long currMS) {	if (!metricsMap.isEmpty() && currMS - lastPublishTimeMS > publishIntervalMS) {	try {	TimelineMetrics timelineMetrics = new TimelineMetrics();	timelineMetrics.setMetrics(new ArrayList<TimelineMetric>(metricsMap.values()));	amsClient.emitMetrics(timelineMetrics);	metricsMap.clear();	lastPublishTimeMS = currMS;	} catch (Throwable t) {	
error sending metrics to ams 

private void publishMetrics(long currMS) {	if (!metricsMap.isEmpty() && currMS - lastPublishTimeMS > publishIntervalMS) {	try {	TimelineMetrics timelineMetrics = new TimelineMetrics();	timelineMetrics.setMetrics(new ArrayList<TimelineMetric>(metricsMap.values()));	amsClient.emitMetrics(timelineMetrics);	metricsMap.clear();	lastPublishTimeMS = currMS;	} catch (Throwable t) {	if (currMS - lastFailedPublishTimeMS > maxMetricsBuffer) {	
ams was not sent for last seconds purging it and will start rebuilding it again 

========================= ambari sample_1611 =========================

public void testOutputKafka_uploadData() throws Exception {	
testoutputkafka uploaddata 

public void testOutputKafka_noBrokerList() throws Exception {	
testoutputkafka nobrokerlist 

public void testOutputKafka_noTopic() throws Exception {	
testoutputkafka nobrokerlist 

========================= ambari sample_1607 =========================

List<RequestResourceFilter> resourceFilters = actionContext.getResourceFilters();	final RequestResourceFilter resourceFilter;	if (resourceFilters != null && !resourceFilters.isEmpty()) {	resourceFilter = resourceFilters.get(0);	} else {	resourceFilter = new RequestResourceFilter();	}	Set<String> candidateHosts = new HashSet<>();	final String serviceName = actionContext.getExpectedServiceName();	final String componentName = actionContext.getExpectedComponentName();	
called addexecutioncommandstostage for servicename componentname 

final RequestResourceFilter resourceFilter;	if (resourceFilters != null && !resourceFilters.isEmpty()) {	resourceFilter = resourceFilters.get(0);	} else {	resourceFilter = new RequestResourceFilter();	}	Set<String> candidateHosts = new HashSet<>();	final String serviceName = actionContext.getExpectedServiceName();	final String componentName = actionContext.getExpectedComponentName();	if (resourceFilter.getHostNames().isEmpty()) {	
resource filter has no hostnames 

if (resourceFilters != null && !resourceFilters.isEmpty()) {	resourceFilter = resourceFilters.get(0);	} else {	resourceFilter = new RequestResourceFilter();	}	Set<String> candidateHosts = new HashSet<>();	final String serviceName = actionContext.getExpectedServiceName();	final String componentName = actionContext.getExpectedComponentName();	if (resourceFilter.getHostNames().isEmpty()) {	} else {	
resource filter has hosts 

if (serviceName != null && !serviceName.isEmpty()) {	if (componentName != null && !componentName.isEmpty()) {	Service service = cluster.getService(serviceName);	ServiceComponent component = service.getServiceComponent(componentName);	StackId stackId = component.getDesiredStackId();	Map<String, ServiceComponentHost> componentHosts = component.getServiceComponentHosts();	candidateHosts.addAll(componentHosts.keySet());	try {	componentInfo = ambariMetaInfo.getComponent(stackId.getStackName(), stackId.getStackVersion(), serviceName, componentName);	} catch (ObjectNotFoundException e) {	
did not find service and component in stack 

}	} else {	for (String component : cluster.getService(serviceName).getServiceComponents().keySet()) {	Map<String, ServiceComponentHost> componentHosts = cluster.getService(serviceName) .getServiceComponent(component).getServiceComponentHosts();	candidateHosts.addAll(componentHosts.keySet());	}	}	} else {	candidateHosts.addAll(clusters.getHostsForCluster(cluster.getClusterName()).keySet());	}	
request for service and component is set to run on candidate hosts 

} else {	candidateHosts.addAll(clusters.getHostsForCluster(cluster.getClusterName()).keySet());	}	Set<String> ignoredHosts = maintenanceStateHelper.filterHostsInMaintenanceState( candidateHosts, new MaintenanceStateHelper.HostPredicate() {	public boolean shouldHostBeRemoved(final String hostname) throws AmbariException {	return ! maintenanceStateHelper.isOperationAllowed( cluster, actionContext.getOperationLevel(), resourceFilter, serviceName, componentName, hostname);	}	}	);	if (! ignoredHosts.isEmpty()) {	
hosts to ignore 

Map<String, String> actionParameters = actionContext.getParameters();	stage.addHostRoleExecutionCommand(hostName, Role.valueOf(actionContext.getActionName()), RoleCommand.ACTIONEXECUTE, new ServiceComponentHostOpInProgressEvent(actionContext.getActionName(), hostName, System.currentTimeMillis()), clusterName, serviceName, actionContext.isRetryAllowed(), actionContext.isFailureAutoSkipped());	Map<String, String> commandParams = new TreeMap<>();	int taskTimeout = Integer.parseInt(configs.getDefaultAgentTaskTimeout(false));	if (null != actionContext.getTimeout() && actionContext.getTimeout() > taskTimeout) {	commandParams.put(COMMAND_TIMEOUT, actionContext.getTimeout().toString());	} else {	commandParams.put(COMMAND_TIMEOUT, Integer.toString(taskTimeout));	}	if (requestParams != null && requestParams.containsKey(KeyNames.LOG_OUTPUT)) {	
should command log output 

========================= ambari sample_3378 =========================

if (! state.equals(State.STARTED) && ! isInMaintenance && ( otherState == null || state.ordinal() > otherState.ordinal())) {	otherState = state;	}	}	} catch (ObjectNotFoundException e) {	}	}	return hasMaster   ? masterState == null ? State.STARTED : masterState : hasOther    ? otherState == null ? State.STARTED : otherState : hasClient   ? clientState == null ? State.INSTALLED : clientState : hasDisabled ? State.DISABLED : hasMM       ? maxMMState : State.UNKNOWN;	}	} catch (AmbariException e) {	
can t determine service state 

========================= ambari sample_3682 =========================

public RequestStageContainer toggleKerberos(Cluster cluster, SecurityType securityType, RequestStageContainer requestStageContainer, Boolean manageIdentities) throws AmbariException, KerberosOperationException {	KerberosDetails kerberosDetails = getKerberosDetails(cluster, manageIdentities);	kerberosDetails.setSecurityType(securityType);	if (securityType == SecurityType.KERBEROS) {	
configuring kerberos for realm on cluster 

public RequestStageContainer toggleKerberos(Cluster cluster, SecurityType securityType, RequestStageContainer requestStageContainer, Boolean manageIdentities) throws AmbariException, KerberosOperationException {	KerberosDetails kerberosDetails = getKerberosDetails(cluster, manageIdentities);	kerberosDetails.setSecurityType(securityType);	if (securityType == SecurityType.KERBEROS) {	requestStageContainer = handle(cluster, kerberosDetails, null, null, null, null, requestStageContainer, new EnableKerberosHandler());	} else if (securityType == SecurityType.NONE) {	
disabling kerberos from cluster 

public void deleteIdentities(Cluster cluster, List<Component> components, Set<String> identities) throws AmbariException, KerberosOperationException {	if (identities.isEmpty()) {	return;	}	
deleting identities 

}	clusterHostInfoMap.put(key, StringUtils.join(fqdns, ','));	}	}	}	}	}	}	}	} catch (StackAdvisorException e) {	
failed to obtain the recommended host groups for the preconfigured components 

private void addRecommendedPropertiesForConfigType(Map<String, Map<String, String>> kerberosConfigurations, String configType, Map<String, String> recommendedConfigProperties, Map<String, String> existingConfigProperties, Map<String, String> kerberosConfigProperties, Set<String> ignoreProperties) {	for (Map.Entry<String, String> property : recommendedConfigProperties.entrySet()) {	String propertyName = property.getKey();	if ((ignoreProperties == null) || !ignoreProperties.contains(propertyName)) {	String recommendedValue = property.getValue();	if ((kerberosConfigProperties == null) || !kerberosConfigProperties.containsKey(propertyName)) {	if ((existingConfigProperties == null) || !existingConfigProperties.containsKey(propertyName)) {	
adding kerberos configuration based on stackadvisor recommendation tconfigtype tproperty tvalue 

private void removeRecommendedPropertiesForConfigType(String configType, Map<String, ValueAttributesInfo> recommendedConfigPropertyAttributes, Map<String, String> existingConfigProperties, Map<String, Map<String, String>> kerberosConfigurations, Set<String> ignoreProperties, Map<String, Set<String>> propertiesToRemove) {	for (Map.Entry<String, ValueAttributesInfo> property : recommendedConfigPropertyAttributes.entrySet()) {	String propertyName = property.getKey();	if ("true".equalsIgnoreCase(property.getValue().getDelete())) {	Map<String, String> kerberosConfigProperties = kerberosConfigurations.get(configType);	if (((ignoreProperties == null) || !ignoreProperties.contains(propertyName)) && ((kerberosConfigProperties == null) || kerberosConfigProperties.get(propertyName) == null) && (existingConfigProperties != null && existingConfigProperties.containsKey(propertyName))) {	
property to remove from configuration based on stackadvisor recommendation tconfigtype tproperty 

if (identity != null) {	KerberosPrincipalDescriptor principal = identity.getPrincipalDescriptor();	if (principal != null) {	boolean updateJAASFile = AMBARI_SERVER_KERBEROS_IDENTITY_NAME.equals(identity.getName());	Keytab keytab = createIdentity(identity, principal.getType(), kerberosEnvProperties, kerberosOperationHandler, configurations, ambariServerHostname);	installAmbariIdentity(identity, keytab, configurations, ambariServerHostname, kerberosDetails, updateJAASFile);	if (updateJAASFile) {	try {	KerberosChecker.checkJaasConfiguration();	} catch (AmbariException e) {	
error in ambari jaas configuration 

File tmpKeytabFile = createTemporaryFile();	try {	if ((operationHandler != null) && operationHandler.createKeytabFile(keytab, tmpKeytabFile)) {	String ownerName = variableReplacementHelper.replaceVariables(keytabDescriptor.getOwnerName(), configurations);	String ownerAccess = keytabDescriptor.getOwnerAccess();	String groupName = variableReplacementHelper.replaceVariables(keytabDescriptor.getGroupName(), configurations);	String groupAccess = keytabDescriptor.getGroupAccess();	String componentName = principal.contains(KerberosHelper.AMBARI_SERVER_KERBEROS_IDENTITY_NAME) ? "AMBARI_SERVER_SELF" : RootComponent.AMBARI_SERVER.name();	ResolvedKerberosPrincipal resolvedKerberosPrincipal = new ResolvedKerberosPrincipal( null, hostname, principal, false, null, RootService.AMBARI.name(), componentName, destKeytabFilePath );	configureAmbariIdentitiesServerAction.installAmbariServerIdentity(resolvedKerberosPrincipal, tmpKeytabFile.getAbsolutePath(), destKeytabFilePath, ownerName, ownerAccess, groupName, groupAccess, null);	
successfully created keytab file for at 

try {	if ((operationHandler != null) && operationHandler.createKeytabFile(keytab, tmpKeytabFile)) {	String ownerName = variableReplacementHelper.replaceVariables(keytabDescriptor.getOwnerName(), configurations);	String ownerAccess = keytabDescriptor.getOwnerAccess();	String groupName = variableReplacementHelper.replaceVariables(keytabDescriptor.getGroupName(), configurations);	String groupAccess = keytabDescriptor.getGroupAccess();	String componentName = principal.contains(KerberosHelper.AMBARI_SERVER_KERBEROS_IDENTITY_NAME) ? "AMBARI_SERVER_SELF" : RootComponent.AMBARI_SERVER.name();	ResolvedKerberosPrincipal resolvedKerberosPrincipal = new ResolvedKerberosPrincipal( null, hostname, principal, false, null, RootService.AMBARI.name(), componentName, destKeytabFilePath );	configureAmbariIdentitiesServerAction.installAmbariServerIdentity(resolvedKerberosPrincipal, tmpKeytabFile.getAbsolutePath(), destKeytabFilePath, ownerName, ownerAccess, groupName, groupAccess, null);	} else {	
failed to create keytab file for at 

configureAmbariIdentitiesServerAction.installAmbariServerIdentity(resolvedKerberosPrincipal, tmpKeytabFile.getAbsolutePath(), destKeytabFilePath, ownerName, ownerAccess, groupName, groupAccess, null);	} else {	}	} finally {	tmpKeytabFile.delete();	}	} catch (KerberosOperationException e) {	throw new AmbariException(String.format("Failed to create keytab file for %s at %s: %s:", principal, destKeytabFile.getAbsolutePath(), e.getLocalizedMessage()), e);	}	} else {	
no keytab data is available to create the keytab file for at 

if (authToLocalProperties != null) {	authToLocalPropertiesToSet.addAll(authToLocalProperties);	}	Map<String, KerberosServiceDescriptor> serviceDescriptors = kerberosDescriptor.getServices();	if (serviceDescriptors != null) {	for (KerberosServiceDescriptor serviceDescriptor : serviceDescriptors.values()) {	String serviceName = serviceDescriptor.getName();	boolean preconfigure = includePreconfigureData && serviceDescriptor.shouldPreconfigure();	boolean explicitlyAdded = installedServices.containsKey(serviceName);	if (preconfigure || explicitlyAdded) {	
adding identities for service to auth to local mapping explicit preconfigured 

}	Map<String, KerberosComponentDescriptor> componentDescriptors = serviceDescriptor.getComponents();	if (componentDescriptors != null) {	Set<String> installedServiceComponents = installedServices.get(serviceName);	if (installedServiceComponents == null) {	installedServiceComponents = Collections.emptySet();	}	for (KerberosComponentDescriptor componentDescriptor : componentDescriptors.values()) {	String componentName = componentDescriptor.getName();	if (preconfigure || (installedServiceComponents.contains(componentName))) {	
adding identities for component to auth to local mapping 

}	}	if (!authToLocalPropertiesToSet.isEmpty()) {	for (String authToLocalProperty : authToLocalPropertiesToSet) {	Matcher m = KerberosDescriptor.AUTH_TO_LOCAL_PROPERTY_SPECIFICATION_PATTERN.matcher(authToLocalProperty);	if (m.matches()) {	AuthToLocalBuilder builder;	try {	builder = (AuthToLocalBuilder) authToLocalBuilder.clone();	} catch (CloneNotSupportedException e) {	
failed to clone the authtolocalbuilder 

keytabFileGroupAccess = variableReplacementHelper.replaceVariables(keytabDescriptor.getGroupAccess(), configurations);	keytabFileConfiguration = variableReplacementHelper.replaceVariables(keytabDescriptor.getConfiguration(), configurations);	}	String evaluatedPrincipal = principal.replace("_HOST", hostname).replace("_REALM", realm);	ResolvedKerberosKeytab resolvedKeytab = new ResolvedKerberosKeytab( keytabFilePath, keytabFileOwnerName, keytabFileOwnerAccess, keytabFileGroupName, keytabFileGroupAccess, Sets.newHashSet(new ResolvedKerberosPrincipal( hostId, hostname, evaluatedPrincipal, "service".equalsIgnoreCase(principalType), null, serviceName, componentName, keytabFilePath ) ), serviceName.equalsIgnoreCase(RootService.AMBARI.name()), componentName.equalsIgnoreCase("AMBARI_SERVER_SELF") );	if (resolvedKeytabs.containsKey(keytabFilePath)) {	ResolvedKerberosKeytab sameKeytab = resolvedKeytabs.get(keytabFilePath);	boolean differentOwners = false;	String warnTemplate = "Keytab '{}' on host '{}' has different {}, originally set to '{}' and '{}:{}' has '{}', using '{}'";	if (!resolvedKeytab.getOwnerName().equals(sameKeytab.getOwnerName())) {	
owners 

String evaluatedPrincipal = principal.replace("_HOST", hostname).replace("_REALM", realm);	ResolvedKerberosKeytab resolvedKeytab = new ResolvedKerberosKeytab( keytabFilePath, keytabFileOwnerName, keytabFileOwnerAccess, keytabFileGroupName, keytabFileGroupAccess, Sets.newHashSet(new ResolvedKerberosPrincipal( hostId, hostname, evaluatedPrincipal, "service".equalsIgnoreCase(principalType), null, serviceName, componentName, keytabFilePath ) ), serviceName.equalsIgnoreCase(RootService.AMBARI.name()), componentName.equalsIgnoreCase("AMBARI_SERVER_SELF") );	if (resolvedKeytabs.containsKey(keytabFilePath)) {	ResolvedKerberosKeytab sameKeytab = resolvedKeytabs.get(keytabFilePath);	boolean differentOwners = false;	String warnTemplate = "Keytab '{}' on host '{}' has different {}, originally set to '{}' and '{}:{}' has '{}', using '{}'";	if (!resolvedKeytab.getOwnerName().equals(sameKeytab.getOwnerName())) {	differentOwners = true;	}	if (!resolvedKeytab.getOwnerAccess().equals(sameKeytab.getOwnerAccess())) {	
owner access 

ResolvedKerberosKeytab sameKeytab = resolvedKeytabs.get(keytabFilePath);	boolean differentOwners = false;	String warnTemplate = "Keytab '{}' on host '{}' has different {}, originally set to '{}' and '{}:{}' has '{}', using '{}'";	if (!resolvedKeytab.getOwnerName().equals(sameKeytab.getOwnerName())) {	differentOwners = true;	}	if (!resolvedKeytab.getOwnerAccess().equals(sameKeytab.getOwnerAccess())) {	}	if (!resolvedKeytab.getGroupName().equals(sameKeytab.getGroupName())) {	if (differentOwners) {	
groups 

boolean differentOwners = false;	String warnTemplate = "Keytab '{}' on host '{}' has different {}, originally set to '{}' and '{}:{}' has '{}', using '{}'";	if (!resolvedKeytab.getOwnerName().equals(sameKeytab.getOwnerName())) {	differentOwners = true;	}	if (!resolvedKeytab.getOwnerAccess().equals(sameKeytab.getOwnerAccess())) {	}	if (!resolvedKeytab.getGroupName().equals(sameKeytab.getGroupName())) {	if (differentOwners) {	} else {	
groups 

if (!resolvedKeytab.getOwnerAccess().equals(sameKeytab.getOwnerAccess())) {	}	if (!resolvedKeytab.getGroupName().equals(sameKeytab.getGroupName())) {	if (differentOwners) {	} else {	}	}	if (!resolvedKeytab.getGroupAccess().equals(sameKeytab.getGroupAccess())) {	if (differentOwners) {	if (!sameKeytab.getGroupAccess().contains("r")) {	
keytab on host referenced by multiple identities which have different owners but r attribute missing for group make sure all users that need this keytab are in group and keytab can be read by this group 

}	if (!resolvedKeytab.getGroupName().equals(sameKeytab.getGroupName())) {	if (differentOwners) {	} else {	}	}	if (!resolvedKeytab.getGroupAccess().equals(sameKeytab.getGroupAccess())) {	if (differentOwners) {	if (!sameKeytab.getGroupAccess().contains("r")) {	}	
group access 

if (!resolvedKeytab.getGroupName().equals(sameKeytab.getGroupName())) {	if (differentOwners) {	} else {	}	}	if (!resolvedKeytab.getGroupAccess().equals(sameKeytab.getGroupAccess())) {	if (differentOwners) {	if (!sameKeytab.getGroupAccess().contains("r")) {	}	} else {	
group access 

}	sameKeytab.mergePrincipals(resolvedKeytab);	if (sameKeytab.isMustWriteAmbariJaasFile() || resolvedKeytab.isMustWriteAmbariJaasFile()) {	sameKeytab.setMustWriteAmbariJaasFile(true);	}	if (sameKeytab.isAmbariServerKeytab() || resolvedKeytab.isAmbariServerKeytab()) {	sameKeytab.setAmbariServerKeytab(true);	}	} else {	resolvedKeytabs.put(keytabFilePath, resolvedKeytab);	
keytab owner group is defined 

if ((clusterName == null) || clusterName.isEmpty()) {	throw new IllegalArgumentException("Invalid argument, cluster name is required");	}	Cluster cluster = clusters.getCluster(clusterName);	if (cluster == null) {	throw new AmbariException(String.format("The cluster object for the cluster name %s is not available", clusterName));	}	Map<String, Collection<KerberosIdentityDescriptor>> activeIdentities = new HashMap<>();	Config kerberosEnvConfig = cluster.getDesiredConfigByType(KERBEROS_ENV);	if (kerberosEnvConfig == null) {	
calculating the active identities for is being skipped since the kerberos env configuration is not available 

if(kkp.putServiceMapping(sch.getServiceName(), sch.getServiceComponentName())) {	kerberosKeytabPrincipalDAO.merge(kkp);	}	kerberosKeytabDAO.merge(kke);	hostsWithValidKerberosClient.add(hostname);	serviceComponentHostsToProcess.add(sch);	}	}	}	} catch (Exception e) {	
failed 

}	}	}	} catch (Exception e) {	throw e;	}	if (!serviceComponentHostsToProcess.isEmpty()) {	try {	validateKDCCredentials(kerberosDetails, cluster);	} catch (Exception e) {	
cannot validate credentials 

} catch (Exception e) {	throw e;	}	if (!serviceComponentHostsToProcess.isEmpty()) {	try {	validateKDCCredentials(kerberosDetails, cluster);	} catch (Exception e) {	try {	FileUtils.deleteDirectory(dataDirectory);	} catch (Throwable t) {	
the data directory s was not deleted due to an error condition s 

try {	File temporaryDirectory = getConfiguredTemporaryDirectory();	File directory;	int tries = 0;	long now = System.currentTimeMillis();	do {	directory = new File(temporaryDirectory, String.format("%s%d-%d.d", KerberosServerAction.DATA_DIRECTORY_PREFIX, now, tries));	if ((directory.exists()) || !directory.mkdirs()) {	directory = null;	} else {	
created temporary directory 

========================= ambari sample_3728 =========================

public Response getData(@Context UriInfo uriInfo, @PathParam("endpoint") String endpoint) {	String url = getProxyUrl(endpoint, uriInfo.getQueryParameters());	try {	
redirecting to url 

public Response getData(@Context UriInfo uriInfo, @PathParam("endpoint") String endpoint) {	String url = getProxyUrl(endpoint, uriInfo.getQueryParameters());	try {	return Response.temporaryRedirect(new URI(url)).build();	} catch (URISyntaxException e) {	
redirecting to url failed 

========================= ambari sample_1129 =========================

private static Properties readConfigFile() {	Properties properties = new Properties();	InputStream inputStream = MetricsConfiguration.class.getClassLoader().getResourceAsStream(CONFIG_FILE);	if (inputStream == null) {	
not found in classpath 

private static Properties readConfigFile() {	Properties properties = new Properties();	InputStream inputStream = MetricsConfiguration.class.getClassLoader().getResourceAsStream(CONFIG_FILE);	if (inputStream == null) {	return null;	}	try {	properties.load(inputStream);	inputStream.close();	} catch (FileNotFoundException fnf) {	
no configuration file found in classpath 

InputStream inputStream = MetricsConfiguration.class.getClassLoader().getResourceAsStream(CONFIG_FILE);	if (inputStream == null) {	return null;	}	try {	properties.load(inputStream);	inputStream.close();	} catch (FileNotFoundException fnf) {	return null;	} catch (IOException ie) {	
can t read configuration file 

========================= ambari sample_2640 =========================

public void uninstallViewInstance(ViewInstanceEntity instanceEntity) {	
uninstalling viewinstance 

public void uninstallViewInstance(ViewInstanceEntity instanceEntity) {	ViewEntity viewEntity = viewDAO.findByName(instanceEntity.getViewName());	
viewentity received corresponding to the view entity 

public void uninstallViewInstance(ViewInstanceEntity instanceEntity) {	ViewEntity viewEntity = viewDAO.findByName(instanceEntity.getViewName());	if (viewEntity != null) {	String instanceName = instanceEntity.getName();	String viewName = viewEntity.getCommonName();	String version = viewEntity.getVersion();	ViewInstanceEntity instanceDefinition = instanceDAO.findByName(instanceEntity.getViewName(), instanceEntity.getName());	
view instance entity received from database 

if (viewEntity != null) {	String instanceName = instanceEntity.getName();	String viewName = viewEntity.getCommonName();	String version = viewEntity.getVersion();	ViewInstanceEntity instanceDefinition = instanceDAO.findByName(instanceEntity.getViewName(), instanceEntity.getName());	if (instanceDefinition != null) {	if (instanceDefinition.isXmlDriven()) {	throw new IllegalStateException("View instances defined via xml can't be deleted through api requests");	}	List<PrivilegeEntity> instancePrivileges = privilegeDAO.findByResourceId(instanceEntity.getResource().getId());	
removing privilege entities 

String version = viewEntity.getVersion();	ViewInstanceEntity instanceDefinition = instanceDAO.findByName(instanceEntity.getViewName(), instanceEntity.getName());	if (instanceDefinition != null) {	if (instanceDefinition.isXmlDriven()) {	throw new IllegalStateException("View instances defined via xml can't be deleted through api requests");	}	List<PrivilegeEntity> instancePrivileges = privilegeDAO.findByResourceId(instanceEntity.getResource().getId());	for (PrivilegeEntity privilegeEntity : instancePrivileges) {	removePrivilegeEntity(privilegeEntity);	}	
deleting view instance view name version instancename 

========================= ambari sample_3952 =========================

public void testMapperFieldCopy_copyField() {	
testmapperfieldcopy copyfield 

public void testMapperFieldCopy_noNewFieldName() {	
testmapperfieldcopy nonewfieldname 

========================= ambari sample_1596 =========================

public void init(MetricsConfiguration configuration) {	if (ambariManagementController == null) {	return;	}	InternalAuthenticationToken authenticationToken = new InternalAuthenticationToken("admin");	authenticationToken.setAuthenticated(true);	SecurityContextHolder.getContext().setAuthentication(authenticationToken);	Clusters clusters = ambariManagementController.getClusters();	if (clusters == null || clusters.getClusters().isEmpty()) {	
no clusters configured 

instanceId = clusterName;	Cluster c = kv.getValue();	Resource.Type type = Resource.Type.ServiceConfigVersion;	boolean externalHostConfigPresent = false;	boolean externalPortConfigPresent = false;	Config clusterEnv = c.getDesiredConfigByType(ConfigHelper.CLUSTER_ENV);	if (clusterEnv != null) {	Map<String, String> configs = clusterEnv.getProperties();	String metricsCollectorExternalHosts = configs.get("metrics_collector_external_hosts");	if (StringUtils.isNotEmpty(metricsCollectorExternalHosts)) {	
setting metrics collector external host 

if (clusterEnv != null) {	Map<String, String> configs = clusterEnv.getProperties();	String metricsCollectorExternalHosts = configs.get("metrics_collector_external_hosts");	if (StringUtils.isNotEmpty(metricsCollectorExternalHosts)) {	collectorHosts.addAll(Arrays.asList(metricsCollectorExternalHosts.split(",")));	externalHostConfigPresent = true;	setInstanceId = true;	}	String metricsCollectorExternalPort = configs.get("metrics_collector_external_port");	if (StringUtils.isNotEmpty(metricsCollectorExternalPort)) {	
setting metrics collector external port 

port = timelineWebappAddress.split(":")[1];	}	String httpPolicy = (String) properties.get("timeline.metrics.service.http.policy");	protocol = httpPolicy.equals("HTTP_ONLY") ? "http" : "https";	break;	}	}	}	}	} catch (Exception e) {	
exception caught when retrieving collector uri 

protocol = httpPolicy.equals("HTTP_ONLY") ? "http" : "https";	break;	}	}	}	}	} catch (Exception e) {	}	}	hostName = configuration.getProperty("ambariserver.hostname.override", getDefaultLocalHostName());	
hostname used for ambari server metrics 

String trustStorePath = sslConfiguration.getTruststorePath();	String trustStoreType = sslConfiguration.getTruststoreType();	String trustStorePwd = sslConfiguration.getTruststorePassword();	loadTruststore(trustStorePath, trustStoreType, trustStorePwd);	}	collectorUri = getCollectorUri(findPreferredCollectHost());	int maxRowCacheSize = Integer.parseInt(configuration.getProperty(MAX_METRIC_ROW_CACHE_SIZE, String.valueOf(TimelineMetricsCache.MAX_RECS_PER_NAME_DEFAULT)));	int metricsSendInterval = Integer.parseInt(configuration.getProperty(METRICS_SEND_INTERVAL, String.valueOf(TimelineMetricsCache.MAX_EVICTION_TIME_MILLIS)));	timelineMetricsCache = new TimelineMetricsCache(maxRowCacheSize, metricsSendInterval);	if (CollectionUtils.isNotEmpty(collectorHosts)) {	
metric sink initialized with collectorhosts 

private String getDefaultLocalHostName() {	try {	return InetAddress.getLocalHost().getCanonicalHostName();	} catch (UnknownHostException e) {	
error getting host address 

public void publish(List<SingleMetric> metrics) {	if (isInitialized) {	List<TimelineMetric> metricList = getFilteredMetricList(metrics);	if (!metricList.isEmpty()) {	TimelineMetrics timelineMetrics = new TimelineMetrics();	timelineMetrics.setMetrics(metricList);	emitMetrics(timelineMetrics);	}	} else {	
metric sink not yet initialized discarding metrics 

========================= ambari sample_2637 =========================

public MasterKeyServiceImpl(File masterKeyFile) {	if (masterKeyFile == null) {	throw new IllegalArgumentException("Master Key location not provided.");	}	if (masterKeyFile.exists()) {	if (isMasterKeyFile(masterKeyFile)) {	try {	initializeFromFile(masterKeyFile);	} catch (Exception e) {	
cannot initialize master key from s s 

if (masterKeyFile == null) {	throw new IllegalArgumentException("Master Key location not provided.");	}	if (masterKeyFile.exists()) {	if (isMasterKeyFile(masterKeyFile)) {	try {	initializeFromFile(masterKeyFile);	} catch (Exception e) {	}	} else {	
the file at s is not a master ket file 

}	if (masterKeyFile.exists()) {	if (isMasterKeyFile(masterKeyFile)) {	try {	initializeFromFile(masterKeyFile);	} catch (Exception e) {	}	} else {	}	} else {	
cannot open master key file s 

public static boolean initializeMasterKeyFile(File masterKeyFile, String masterKey) {	
persisting master key into 

public static boolean initializeMasterKeyFile(File masterKeyFile, String masterKey) {	EncryptionResult atom = null;	if (masterKey != null) {	try {	atom = aes.encrypt(masterKey);	} catch (Exception e) {	
failed to encrypt master key no changes have been made s 

EncryptionResult atom = null;	if (masterKey != null) {	try {	atom = aes.encrypt(masterKey);	} catch (Exception e) {	return false;	}	}	if (masterKeyFile.exists()) {	if ((masterKeyFile.length() == 0) || isMasterKeyFile(masterKeyFile)) {	
master key file exists at s resetting 

return false;	}	}	if (masterKeyFile.exists()) {	if ((masterKeyFile.length() == 0) || isMasterKeyFile(masterKeyFile)) {	FileChannel fileChannel = null;	try {	fileChannel = new FileOutputStream(masterKeyFile).getChannel();	fileChannel.truncate(0);	} catch (FileNotFoundException e) {	
failed to open key file at s s 

}	}	if (masterKeyFile.exists()) {	if ((masterKeyFile.length() == 0) || isMasterKeyFile(masterKeyFile)) {	FileChannel fileChannel = null;	try {	fileChannel = new FileOutputStream(masterKeyFile).getChannel();	fileChannel.truncate(0);	} catch (FileNotFoundException e) {	} catch (IOException e) {	
failed to reset key file at s s 

} catch (IOException e) {	} finally {	if (fileChannel != null) {	try {	fileChannel.close();	} catch (IOException e) {	}	}	}	} else {	
file exists at s but may not be a master key file it must be manually removed before this file location can be used 

}	if (atom != null) {	try {	ArrayList<String> lines = new ArrayList<>();	lines.add(MASTER_PERSISTENCE_TAG_PREFIX + TimeStamp.getCurrentTime().toDateString());	String line = Base64.encodeBase64String(( Base64.encodeBase64String(atom.salt) + "::" + Base64.encodeBase64String(atom.iv) + "::" + Base64.encodeBase64String(atom.cipher)).getBytes("UTF8"));	lines.add(line);	FileUtils.writeLines(masterKeyFile, "UTF8", lines);	protectAccess(masterKeyFile);	} catch (IOException e) {	
failed to persist master key to s s 

if (keyPath != null && !keyPath.isEmpty()) {	File keyFile = new File(keyPath);	if (keyFile.exists()) {	try {	initializeFromFile(keyFile);	if (master != null) {	key = new String(master);	}	FileUtils.deleteQuietly(keyFile);	} catch (Exception e) {	
cannot read master key from file 

private void initializeFromFile(File masterFile) throws Exception {	try {	List<String> lines = FileUtils.readLines(masterFile, "UTF8");	String tag = lines.get(0);	
loading from persistent master 

========================= ambari sample_2772 =========================

public MetricsService getMetricsServiceType() {	try {	checkInit();	} catch (SystemException e) {	
exception during checkinit 

String configType = "cluster-env";	String currentConfigVersion = getDesiredConfigVersion(clusterName, configType);	String oldConfigVersion = serviceConfigVersions.get(configType);	if (!currentConfigVersion.equals(oldConfigVersion)) {	vipHostConfigPresent = false;	serviceConfigVersions.put(configType, currentConfigVersion);	Map<String, String> configProperties = getDesiredConfigMap (clusterName, currentConfigVersion, configType, Collections.singletonMap("METRICS_COLLECTOR", new String[]{"metrics_collector_vip_host"}));	if (!configProperties.isEmpty()) {	clusterMetricserverVipHost = configProperties.get("METRICS_COLLECTOR");	if (clusterMetricserverVipHost != null) {	
setting metrics collector vip host 

if (clusterMetricserverVipHost != null) {	vipHostConfigPresent = true;	}	}	configProperties = getDesiredConfigMap (clusterName, currentConfigVersion, configType, Collections.singletonMap("METRICS_COLLECTOR", new String[]{"metrics_collector_vip_port"}));	if (!configProperties.isEmpty()) {	clusterMetricServerVipPort = configProperties.get("METRICS_COLLECTOR");	}	}	} catch (NoSuchParentResourceException | UnsupportedPropertyException e) {	
failed to retrieve collector hostname 

if (!configProperties.isEmpty()) {	clusterMetricServerVipPort = configProperties.get("METRICS_COLLECTOR");	}	}	} catch (NoSuchParentResourceException | UnsupportedPropertyException e) {	}	String currentCollectorHost = null;	if (!vipHostConfigPresent) {	currentCollectorHost = metricsCollectorHAManager.getCollectorHost(clusterName);	}	
cluster metrics vip host 

serviceConfigVersions.put(configType, currentConfigVersion);	Map<String, String> configProperties = getDesiredConfigMap(clusterName, currentConfigVersion, configType, Collections.singletonMap("METRICS_COLLECTOR", new String[]{"timeline.metrics.service.webapp.address"}));	if (!configProperties.isEmpty()) {	clusterMetricServerPort = getPortString(configProperties.get("METRICS_COLLECTOR"));	} else {	clusterMetricServerPort = COLLECTOR_DEFAULT_PORT;	}	}	}	} catch (NoSuchParentResourceException | UnsupportedPropertyException e) {	
failed to retrieve collector port 

public Set<String> getHostNames(String clusterName, String componentName) {	Set<String> hosts = null;	try {	Cluster cluster = managementController.getClusters().getCluster(clusterName);	String serviceName = managementController.findServiceName(cluster, componentName);	hosts = cluster.getService(serviceName).getServiceComponent(componentName).getServiceComponentHosts().keySet();	} catch (Exception e) {	
exception in getting host names for jmx metrics 

public Host getHost(String clusterName, String hostName) {	Host host = null;	try {	Cluster cluster = managementController.getClusters().getCluster(clusterName);	if(cluster != null) {	host = cluster.getHost(hostName);	}	} catch (Exception e) {	
exception in getting host info for jmx metrics 

for (Entry<String, String> entry : portMap.entrySet()) {	String portString = getPortString(entry.getValue());	if (null != portString) {	clusterJmxPorts.putIfAbsent(hostName, new ConcurrentHashMap<>());	clusterJmxPorts.get(hostName).put(entry.getKey(), portString);	}	}	initRpcSuffixes(clusterName, componentName, configType, currVersion, hostName, publicHostName);	}	} catch (Exception e) {	
exception initializing jmx port maps 

if (null != portString) {	clusterJmxPorts.putIfAbsent(hostName, new ConcurrentHashMap<>());	clusterJmxPorts.get(hostName).put(entry.getKey(), portString);	}	}	initRpcSuffixes(clusterName, componentName, configType, currVersion, hostName, publicHostName);	}	} catch (Exception e) {	}	}	
jmxportmap 

========================= ambari sample_3499 =========================

public boolean isReady() {	if (!isReady()) {	setLogFiles(getActualFiles(getLogPath()));	if (!ArrayUtils.isEmpty(getLogFiles())) {	if (isTail() && getLogFiles().length > 1) {	
found multiple files for the file filter will use only the first one using 

public boolean isReady() {	if (!isReady()) {	setLogFiles(getActualFiles(getLogPath()));	if (!ArrayUtils.isEmpty(getLogFiles())) {	if (isTail() && getLogFiles().length > 1) {	}	
file filter expanded to 

public boolean isReady() {	if (!isReady()) {	setLogFiles(getActualFiles(getLogPath()));	if (!ArrayUtils.isEmpty(getLogFiles())) {	if (isTail() && getLogFiles().length > 1) {	}	setReady(true);	} else {	
file doesn t exist ignoring for now 

public void start() throws Exception {	if (ArrayUtils.isEmpty(getLogFiles())) {	return;	}	for (int i = getLogFiles().length - 1; i >= 0; i--) {	File file = getLogFiles()[i];	if (i == 0 || !isTail()) {	try {	processFile(file, i == 0);	if (isClosed() || isDrain()) {	
isclosed or isdrain now breaking loop 

========================= ambari sample_1627 =========================

protected String readFromWithDefault(String url, String defaultResponse) {	String response;	try {	InputStream responseInputStream = context.getURLStreamProvider().readFrom(url, "GET", (String)null, new HashMap<String, String>());	response = IOUtils.toString(responseInputStream);	} catch (IOException e) {	
error while reading from rm 

========================= ambari sample_605 =========================

public void runTask() {	
hostrequest executing configure task for host 

public void runTask() {	clusterTopology.getAmbariContext().registerHostWithConfigGroup(hostRequest.getHostName(), clusterTopology, hostRequest.getHostgroupName());	
hostrequest exiting configure task for host 

========================= ambari sample_2660 =========================

public QuickLinksConfigurationModule(File quickLinksConfigurationFile, QuickLinksConfigurationInfo moduleInfo) {	this.moduleInfo = moduleInfo;	if (!moduleInfo.isDeleted() && quickLinksConfigurationFile != null) {	
looking for quicklinks in 

public QuickLinksConfigurationModule(File quickLinksConfigurationFile, QuickLinksConfigurationInfo moduleInfo) {	this.moduleInfo = moduleInfo;	if (!moduleInfo.isDeleted() && quickLinksConfigurationFile != null) {	FileReader reader = null;	try {	reader = new FileReader(quickLinksConfigurationFile);	} catch (FileNotFoundException e) {	
quick links file not found 

FileReader reader = null;	try {	reader = new FileReader(quickLinksConfigurationFile);	} catch (FileNotFoundException e) {	}	try {	QuickLinks quickLinksConfig = mapper.readValue(reader, QuickLinks.class);	Map<String, QuickLinks> map = new HashMap<>();	map.put(QUICKLINKS_CONFIGURATION_KEY, quickLinksConfig);	moduleInfo.setQuickLinksConfigurationMap(map);	
loaded quicklinks configuration 

========================= ambari sample_4472 =========================

protected void aggregate(ResultSet rs, long startTime, long endTime) throws IOException, SQLException {	Map<TimelineMetric, MetricHostAggregate> hostAggregateMap = aggregateMetricsFromResultSet(rs, endTime);	
saving metric aggregates 

========================= ambari sample_347 =========================

valueToInsert = System.lineSeparator() + valueToInsert;	}	if (insert.newlineAfter) {	valueToInsert = valueToInsert + System.lineSeparator();	}	switch (insert.insertType) {	case APPEND: valueToInsertInto = valueToInsertInto + valueToInsert;	break;	case PREPEND: valueToInsertInto = valueToInsert + valueToInsertInto;	break;	
unable to insert with unknown insertion type of 

private boolean isOperationAllowed(Cluster cluster, String configType, String targetPropertyKey, String ifKey, String ifType, String ifValue, PropertyKeyState ifKeyState){	boolean isAllowed = true;	boolean ifKeyIsNotBlank = StringUtils.isNotBlank(ifKey);	boolean ifTypeIsNotBlank = StringUtils.isNotBlank(ifType);	boolean ifValueIsBlank = StringUtils.isBlank(ifValue);	if (ifKeyIsNotBlank && ifTypeIsNotBlank && ifValueIsBlank && ifKeyState == PropertyKeyState.ABSENT) {	boolean keyPresent = getDesiredConfigurationKeyPresence(cluster, ifType, ifKey);	if (keyPresent) {	
skipping property operation for as the key for is present 

boolean ifTypeIsNotBlank = StringUtils.isNotBlank(ifType);	boolean ifValueIsBlank = StringUtils.isBlank(ifValue);	if (ifKeyIsNotBlank && ifTypeIsNotBlank && ifValueIsBlank && ifKeyState == PropertyKeyState.ABSENT) {	boolean keyPresent = getDesiredConfigurationKeyPresence(cluster, ifType, ifKey);	if (keyPresent) {	isAllowed = false;	}	} else if (ifKeyIsNotBlank && ifTypeIsNotBlank && ifValueIsBlank && ifKeyState == PropertyKeyState.PRESENT) {	boolean keyPresent = getDesiredConfigurationKeyPresence(cluster, ifType, ifKey);	if (!keyPresent) {	
skipping property operation for as the key for is not present 

} else if (ifKeyIsNotBlank && ifTypeIsNotBlank && !ifValueIsBlank) {	String ifConfigType = ifType;	String checkValue = getDesiredConfigurationValue(cluster, ifConfigType, ifKey);	if (ifKeyState == PropertyKeyState.ABSENT) {	boolean keyPresent = getDesiredConfigurationKeyPresence(cluster, ifType, ifKey);	if (!keyPresent) {	return true;	}	}	if (!StringUtils.equalsIgnoreCase(ifValue, checkValue)) {	
skipping property operation for as the value for is not equal to 

========================= ambari sample_3358 =========================

public void run() {	
request directory 

public void run() {	hostStatus = new ArrayList<>();	if (hosts == null) {	return;	}	File done;	File log;	
hostlist for polling on 

status.setStatus("RUNNING");	} else {	status.setStatus("FAILED");	try {	String statusCode = FileUtils.readFileToString(done).trim();	if (statusCode.equals("0")) {	status.setStatus("DONE");	}	updateStatus(status, statusCode);	} catch (IOException e) {	
error reading done file 

reader = new BufferedReader(new FileReader(log));	String line = null;	while (null != (line = reader.readLine())) {	if (line.startsWith("tcgetattr:") || line.startsWith("tput:")) continue;	if (0 != sb.length() || 0 == line.length()) sb.append('\n');	if (-1 != line.indexOf ("\\n")) sb.append(line.replace("\\n", "\n"));	else sb.append(line);	}	logString = sb.toString();	} catch (IOException e) {	
error reading log file log file may be have not created yet 

========================= ambari sample_2733 =========================

Thread .setDefaultUncaughtExceptionHandler(new YarnUncaughtExceptionHandler());	StringUtils.startupShutdownMessage(ApplicationHistoryServer.class, args, LOG);	ApplicationHistoryServer appHistoryServer = null;	try {	appHistoryServer = new ApplicationHistoryServer();	ShutdownHookManager.get().addShutdownHook( new CompositeServiceShutdownHook(appHistoryServer), SHUTDOWN_HOOK_PRIORITY);	YarnConfiguration conf = new YarnConfiguration();	appHistoryServer.init(conf);	appHistoryServer.start();	} catch (Throwable t) {	
error starting applicationhistoryserver 

protected TimelineStore createTimelineStore(Configuration conf) {	if (conf.getBoolean(DISABLE_APPLICATION_TIMELINE_STORE, true)) {	
explicitly disabled application timeline store 

protected TimelineMetricStore createTimelineMetricStore(Configuration conf) {	
creating metrics store 

protected void startWebApp() {	String bindAddress = metricConfiguration.getWebappAddress();	
instantiating ahswebapp at 

========================= ambari sample_428 =========================

}	if (!AuthorizationHelper.isAuthorized(ResourceType.CLUSTER, cluster.getResourceId(), EnumSet.of(RoleAuthorization.CLUSTER_UPGRADE_DOWNGRADE_STACK))) {	throw new AuthorizationException("The authenticated user does not have authorization to " + "manage upgrade and downgrade");	}	UpgradeEntity entity = createResources(new Command<UpgradeEntity>() {	public UpgradeEntity invoke() throws AmbariException, AuthorizationException {	final UpgradeContext upgradeContext = s_upgradeContextFactory.create(cluster, requestMap);	try {	return createUpgrade(upgradeContext);	} catch (Exception e) {	
error appears during upgrade task submitting 

final String regexp = "(\\{\\{.*?\\}\\})";	String task = upgradeItem.getTasks();	if (task != null && !task.isEmpty()) {	Matcher m = Pattern.compile(regexp).matcher(task);	while (m.find()) {	String origVar = m.group(1);	String configValue = configHelper.getPlaceholderValueFromDesiredConfigurations(cluster, origVar);	if (null != configValue) {	task = task.replace(origVar, configValue);	} else {	
unable to retrieve value for 

private void makeActionStage(UpgradeContext context, RequestStageContainer request, RepositoryVersionEntity effectiveRepositoryVersion, UpgradeItemEntity entity, StageWrapper wrapper, boolean skippable, boolean supportsAutoSkipOnFailure, boolean allowRetry) throws AmbariException {	if (0 == wrapper.getHosts().size()) {	throw new AmbariException( String.format("Cannot create action for '%s' with no hosts", wrapper.getText()));	}	Cluster cluster = context.getCluster();	
analyzing upgrade item with tasks 

========================= ambari sample_3572 =========================

protected Condition prepareMetricQueryCondition(long startTime, long endTime) {	EmptyCondition condition = new EmptyCondition();	condition.setDoUpdate(true);	condition.setStatement(String.format(GET_AGGREGATED_HOST_METRIC_GROUPBY_SQL, getQueryHint(startTime), outputTableName, endTime, tableName, getDownsampledMetricSkipClause(), startTime, endTime));	if (LOG.isDebugEnabled()) {	
condition 

========================= ambari sample_351 =========================

private void mergeServicesWithParent( ExtensionModule parentExtension, Map<String, StackModule> allStacks, Map<String, ServiceModule> commonServices, Map<String, ExtensionModule> extensions) throws AmbariException {	extensionInfo.getServices().clear();	
merging extension services with parent 

private void mergeServiceWithExplicitParent( ServiceModule service, String parent, Map<String, StackModule> allStacks, Map<String, ServiceModule> commonServices, Map<String, ExtensionModule> extensions) throws AmbariException {	if(isCommonServiceParent(parent)) {	
merging with common service 

private void mergeServiceWithExplicitParent( ServiceModule service, String parent, Map<String, StackModule> allStacks, Map<String, ServiceModule> commonServices, Map<String, ExtensionModule> extensions) throws AmbariException {	if(isCommonServiceParent(parent)) {	mergeServiceWithCommonServiceParent(service, parent, allStacks, commonServices, extensions);	
display name 

========================= ambari sample_4464 =========================

public void onActionFinished(ActionFinalReportReceivedEvent event) {	if (! event.getRole().equals(INSTALL_PACKAGES)) {	return;	}	if (LOG.isDebugEnabled()) {	LOG.debug(event.toString());	}	Long clusterId = event.getClusterId();	if (clusterId == null) {	
distribute repositories expected a cluster id for host 

if (LOG.isDebugEnabled()) {	LOG.debug(event.toString());	}	Long clusterId = event.getClusterId();	if (clusterId == null) {	return;	}	String repositoryVersion = null;	RepositoryVersionState newHostState = RepositoryVersionState.INSTALL_FAILED;	if (event.getCommandReport() == null) {	
command report is null will set all installing versions for host to install failed 

LOG.debug(event.toString());	}	Long clusterId = event.getClusterId();	if (clusterId == null) {	return;	}	String repositoryVersion = null;	RepositoryVersionState newHostState = RepositoryVersionState.INSTALL_FAILED;	if (event.getCommandReport() == null) {	} else if (!event.getCommandReport().getStatus().equals(HostRoleStatus.COMPLETED.toString())) {	
distribute repositories did not complete will set all installing versions for host to install failed 

}	String repositoryVersion = null;	RepositoryVersionState newHostState = RepositoryVersionState.INSTALL_FAILED;	if (event.getCommandReport() == null) {	} else if (!event.getCommandReport().getStatus().equals(HostRoleStatus.COMPLETED.toString())) {	} else {	DistributeRepositoriesStructuredOutput structuredOutput = null;	try {	structuredOutput = gson.fromJson(event.getCommandReport().getStructuredOut(), DistributeRepositoriesStructuredOutput.class);	} catch (JsonSyntaxException e) {	
cannot parse structured output s 

RepositoryVersionState newHostState = RepositoryVersionState.INSTALL_FAILED;	if (event.getCommandReport() == null) {	} else if (!event.getCommandReport().getStatus().equals(HostRoleStatus.COMPLETED.toString())) {	} else {	DistributeRepositoriesStructuredOutput structuredOutput = null;	try {	structuredOutput = gson.fromJson(event.getCommandReport().getStructuredOut(), DistributeRepositoriesStructuredOutput.class);	} catch (JsonSyntaxException e) {	}	if (null == structuredOutput || null == structuredOutput.repositoryVersionId) {	
received an installation reponse but it did not contain a repository version id 

========================= ambari sample_4521 =========================

protected String readFromWithDefault(String url, String defaultResponse) {	String response;	try {	InputStream responseInputStream = context.getURLStreamProvider().readFrom(url, "GET", (String)null, new HashMap<String, String>());	response = IOUtils.toString(responseInputStream);	} catch (IOException e) {	
error while reading from rm 

========================= ambari sample_869 =========================

public static int getResumeFromLineNumber(InputFile inputFile) {	int resumeFromLineNumber = 0;	File checkPointFile = null;	try {	
checking existing checkpoint file 

public static int getResumeFromLineNumber(InputFile inputFile) {	int resumeFromLineNumber = 0;	File checkPointFile = null;	try {	String checkPointFileName = inputFile.getBase64FileKey() + inputFile.getCheckPointExtension();	File checkPointFolder = inputFile.getInputManager().getCheckPointFolderFile();	checkPointFile = new File(checkPointFolder, checkPointFileName);	inputFile.getCheckPointFiles().put(inputFile.getBase64FileKey(), checkPointFile);	Map<String, Object> jsonCheckPoint = null;	if (!checkPointFile.exists()) {	
checkpoint file for log file doesn t exist starting to read it from the beginning 

int readSize = checkPointWriter.read(b, 0, contentSize);	if (readSize != contentSize) {	LOG.error("Couldn't read expected number of bytes from checkpoint file. expected=" + contentSize + ", read=" + readSize + ", checkPointFile=" + checkPointFile + ", input=" + inputFile.getShortDescription());	} else {	String jsonCheckPointStr = new String(b, 0, readSize);	jsonCheckPoint = LogFeederUtil.toJSONObject(jsonCheckPointStr);	resumeFromLineNumber = LogFeederUtil.objectToInt(jsonCheckPoint.get("line_number"), 0, "line_number");	LOG.info("CheckPoint. checkPointFile=" + checkPointFile + ", json=" + jsonCheckPointStr + ", resumeFromLineNumber=" + resumeFromLineNumber);	}	} catch (EOFException eofEx) {	
eofexception will reset checkpoint file for 

========================= ambari sample_1631 =========================

TreeMap<Long, Double> metricsSubSet = new TreeMap<>(metricValues.tailMap(newEldestTimestamp));	if (metricsSubSet.isEmpty()) {	oldestTimestamp = metric.getStartTime();	this.timelineMetric.setStartTime(metric.getStartTime());	} else {	Long newStartTime = metricsSubSet.firstKey();	oldestTimestamp = newStartTime;	this.timelineMetric.setStartTime(newStartTime);	}	this.timelineMetric.setMetricValues(metricsSubSet);	
metrics cache overflow values for metric older than were removed to clean up the cache 

========================= ambari sample_243 =========================

}	DeconstructedPrincipal deconstructedPrincipal = createDeconstructPrincipal(principal);	String normalizedPrincipal = deconstructedPrincipal.getNormalizedPrincipal();	String[] ipaCommand;	if (service) {	ipaCommand = new String[]{	"service-add", normalizedPrincipal };	} else {	String principalName = deconstructedPrincipal.getPrincipalName();	if (!principalName.equals(principal.toLowerCase())) {	
is not in lowercase freeipa does not recognize user principals that are not entirely in lowercase this can lead to issues with kinit and keytabs make sure users are in lowercase 

String stdErr = result.getStderr();	if ((stdErr != null) && ((service && stdErr.contains(String.format("service with name \"%s\" already exists", normalizedPrincipal))) || (!service && stdErr.contains(String.format("user with name \"%s\" already exists", deconstructedPrincipal.getPrimary()))))) {	throw new KerberosPrincipalAlreadyExistsException(principal);	} else {	throw new KerberosOperationException(String.format("Failed to create principal for %s\nSTDOUT: %s\nSTDERR: %s", normalizedPrincipal, result.getStdout(), result.getStderr()));	}	}	if ((!service) && !StringUtils.isEmpty(userPrincipalGroup)) {	result = invokeIpa(new String[]{"group-add-member", userPrincipalGroup, "--users", deconstructedPrincipal.getPrimary()});	if (!result.isSuccessful()) {	
failed to add account for to group stdout stderr 

}	if (StringUtils.isEmpty(executableIpa)) {	throw new KerberosOperationException("No path for ipa is available - this KerberosOperationHandler may not have been opened.");	}	String[] command = new String[query.length + 1];	command[0] = executableIpa;	System.arraycopy(query, 0, command, 1, query.length);	ShellCommandUtil.Result result = executeCommand(command);	if (result.isSuccessful()) {	if (LOG.isDebugEnabled()) {	
executed the following command stdout stderr 

throw new KerberosOperationException("No path for ipa is available - this KerberosOperationHandler may not have been opened.");	}	String[] command = new String[query.length + 1];	command[0] = executableIpa;	System.arraycopy(query, 0, command, 1, query.length);	ShellCommandUtil.Result result = executeCommand(command);	if (result.isSuccessful()) {	if (LOG.isDebugEnabled()) {	}	} else {	
failed to execute the following command stdout stderr 

========================= ambari sample_3319 =========================

public static Optional<Map<String, String>> getViewConfigs(ViewContext context, String viewConfigPropertyName) {	Map<String, String> viewConfigs = new HashMap<>();	String keyValues = context.getProperties().get(viewConfigPropertyName);	LOG.debug("{} : {}", viewConfigPropertyName, keyValues);	if (Strings.isNullOrEmpty(keyValues)) {	
no values found in property 

========================= ambari sample_1099 =========================

rd = new BufferedReader(new InputStreamReader(resultInputStream));	if (rd != null) {	line = rd.readLine();	while (line != null) {	result += line;	line = rd.readLine();	}	rd.close();	}	} catch (Exception e) {	
exception caught processing impersonator request 

========================= ambari sample_3948 =========================

public DirContextOperations authenticate(Authentication authentication) {	if (!(authentication instanceof UsernamePasswordAuthenticationToken)) {	
unexpected authentication token type encountered failing authentication 

throw new BadCredentialsException("Unexpected authentication token type encountered.");	}	DirContextOperations user = authenticate((UsernamePasswordAuthenticationToken) authentication);	LdapServerProperties ldapServerProperties = ldapConfiguration.getLdapServerProperties();	if (StringUtils.isNotEmpty(ldapServerProperties.getAdminGroupMappingRules())) {	setAmbariAdminAttr(user, ldapServerProperties);	}	String ldapUserName = user.getStringAttribute(ldapServerProperties.getUsernameAttribute());	String loginName = authentication.getName();	if (ldapUserName == null) {	
the user data does not contain a value for 

}	DirContextOperations user = authenticate((UsernamePasswordAuthenticationToken) authentication);	LdapServerProperties ldapServerProperties = ldapConfiguration.getLdapServerProperties();	if (StringUtils.isNotEmpty(ldapServerProperties.getAdminGroupMappingRules())) {	setAmbariAdminAttr(user, ldapServerProperties);	}	String ldapUserName = user.getStringAttribute(ldapServerProperties.getUsernameAttribute());	String loginName = authentication.getName();	if (ldapUserName == null) {	} else if (ldapUserName.isEmpty()) {	
the user data contains an empty value for 

private DirContextOperations authenticate(UsernamePasswordAuthenticationToken authentication) {	DirContextOperations user = null;	String username = authentication.getName();	Object credentials = authentication.getCredentials();	String password = (credentials instanceof String) ? (String) credentials : null;	if (StringUtils.isEmpty(username)) {	
empty username encountered failing authentication 

private DirContextOperations authenticate(UsernamePasswordAuthenticationToken authentication) {	DirContextOperations user = null;	String username = authentication.getName();	Object credentials = authentication.getCredentials();	String password = (credentials instanceof String) ? (String) credentials : null;	if (StringUtils.isEmpty(username)) {	throw new BadCredentialsException("Empty username encountered.");	}	
authenticating 

private DirContextOperations authenticate(UsernamePasswordAuthenticationToken authentication) {	DirContextOperations user = null;	String username = authentication.getName();	Object credentials = authentication.getCredentials();	String password = (credentials instanceof String) ? (String) credentials : null;	if (StringUtils.isEmpty(username)) {	throw new BadCredentialsException("Empty username encountered.");	}	if (StringUtils.isEmpty(password)) {	
empty password encountered failing authentication 

Object credentials = authentication.getCredentials();	String password = (credentials instanceof String) ? (String) credentials : null;	if (StringUtils.isEmpty(username)) {	throw new BadCredentialsException("Empty username encountered.");	}	if (StringUtils.isEmpty(password)) {	throw new BadCredentialsException("Empty password encountered.");	}	LdapUserSearch userSearch = getUserSearch();	if (userSearch == null) {	
the user search facility has not been set failing authentication 

throw new BadCredentialsException("Empty username encountered.");	}	if (StringUtils.isEmpty(password)) {	throw new BadCredentialsException("Empty password encountered.");	}	LdapUserSearch userSearch = getUserSearch();	if (userSearch == null) {	throw new BadCredentialsException("The user search facility has not been set.");	} else {	if (LOG.isTraceEnabled()) {	
searching for user with username 

throw new BadCredentialsException("Empty password encountered.");	}	LdapUserSearch userSearch = getUserSearch();	if (userSearch == null) {	throw new BadCredentialsException("The user search facility has not been set.");	} else {	if (LOG.isTraceEnabled()) {	}	DirContextOperations userFromSearch = userSearch.searchForUser(username);	if (userFromSearch == null) {	
ldap user object not found for 

}	LdapUserSearch userSearch = getUserSearch();	if (userSearch == null) {	throw new BadCredentialsException("The user search facility has not been set.");	} else {	if (LOG.isTraceEnabled()) {	}	DirContextOperations userFromSearch = userSearch.searchForUser(username);	if (userFromSearch == null) {	} else {	
found ldap user for 

StringBuilder builder = new StringBuilder();	NamingEnumeration<String> ids = attributes.getIDs();	try {	while (ids.hasMore()) {	String id = ids.next();	builder.append("\n\t");	builder.append(attributes.get(id));	}	} catch (NamingException e) {	}	
user attributes 

NamingEnumeration<String> ids = attributes.getIDs();	try {	while (ids.hasMore()) {	String id = ids.next();	builder.append("\n\t");	builder.append(attributes.get(id));	}	} catch (NamingException e) {	}	} else {	
user attributes not available 

builder.append(attributes.get(id));	}	} catch (NamingException e) {	}	} else {	}	}	}	}	if (user == null) {	
invalid credentials for failing authentication 

} catch (NamingException e) {	}	} else {	}	}	}	}	if (user == null) {	throw new BadCredentialsException("Invalid credentials.");	} else {	
successfully authenticated 

throw new InternalAuthenticationServiceException(message);	}	if (!(contextSource instanceof BaseLdapPathContextSource)) {	String message = String.format("Unexpected ContextSource type (%s) - failing authentication.", contextSource.getClass().getName());	LOG.debug(message);	throw new InternalAuthenticationServiceException(message);	}	BaseLdapPathContextSource baseLdapPathContextSource = (BaseLdapPathContextSource) contextSource;	Name userDistinguishedName = user.getDn();	Name fullDn = AmbariLdapUtils.getFullDn(userDistinguishedName, baseLdapPathContextSource.getBaseLdapName());	
attempting to bind as 

private DirContextOperations setAmbariAdminAttr(DirContextOperations user, LdapServerProperties ldapServerProperties) {	String baseDn = ldapServerProperties.getBaseDN().toLowerCase();	String groupBase = ldapServerProperties.getGroupBase().toLowerCase();	final String groupNamingAttribute = ldapServerProperties.getGroupNamingAttr();	final String adminGroupMappingMemberAttr = ldapServerProperties.getAdminGroupMappingMemberAttr();	int indexOfBaseDn = groupBase.indexOf(baseDn);	groupBase = indexOfBaseDn <= 0 ? "" : groupBase.substring(0, indexOfBaseDn - 1);	String memberValue = StringUtils.isNotEmpty(adminGroupMappingMemberAttr) ? user.getStringAttribute(adminGroupMappingMemberAttr) : user.getNameInNamespace();	
ldap login set as member attribute for admingroupmappingrules 

private DirContextOperations setAmbariAdminAttr(DirContextOperations user, LdapServerProperties ldapServerProperties) {	String baseDn = ldapServerProperties.getBaseDN().toLowerCase();	String groupBase = ldapServerProperties.getGroupBase().toLowerCase();	final String groupNamingAttribute = ldapServerProperties.getGroupNamingAttr();	final String adminGroupMappingMemberAttr = ldapServerProperties.getAdminGroupMappingMemberAttr();	int indexOfBaseDn = groupBase.indexOf(baseDn);	groupBase = indexOfBaseDn <= 0 ? "" : groupBase.substring(0, indexOfBaseDn - 1);	String memberValue = StringUtils.isNotEmpty(adminGroupMappingMemberAttr) ? user.getStringAttribute(adminGroupMappingMemberAttr) : user.getNameInNamespace();	String setAmbariAdminAttrFilter = resolveAmbariAdminAttrFilter(ldapServerProperties, memberValue);	
ldap login set admin attr filter 

========================= ambari sample_2820 =========================

public static void checkDBConsistency() throws AmbariException {	
checking db consistency 

if (!componentCheckFailed && schDesiredStateEntities.size() != schStateEntities.size()) {	checkPassed = false;	LOG.error(String.format("HostComponentStateEntities and HostComponentDesiredStateEntities " + "tables must contain equal number of rows mapped to ServiceComponentDesiredStateEntity, " + "(clusterName=%s, serviceName=%s, componentName=%s) ", clusterEntity.getClusterName(), scDesiredStateEnity.getServiceName(), scDesiredStateEnity.getComponentName()));	}	checkPassed = checkPassed && !componentCheckFailed;	}	}	}	}	if (checkPassed) {	
db consistency check passed 

public static void checkDBConfigsConsistency() throws AmbariException {	
checking db configs consistency 

if (clusterStateEntity != null) {	StackEntity currentStack = clusterStateEntity.getCurrentStack();	StackInfo stack = ambariMetaInfo.getStack(currentStack.getStackName(), currentStack.getStackVersion());	for (ClusterServiceEntity clusterServiceEntity : clusterServiceEntities) {	if (!State.INIT.equals( clusterServiceEntity.getServiceDesiredStateEntity().getDesiredState())) {	String serviceName = clusterServiceEntity.getServiceName();	ServiceInfo serviceInfo = ambariMetaInfo.getService(stack.getName(), stack.getVersion(), serviceName);	for (String configTypeName : serviceInfo.getConfigTypeAttributes().keySet()) {	if (selectedCountForType.get(configTypeName) == null) {	checkPassed = false;	
configuration is missing for service 

for (ClusterServiceEntity clusterServiceEntity : clusterServiceEntities) {	if (!State.INIT.equals( clusterServiceEntity.getServiceDesiredStateEntity().getDesiredState())) {	String serviceName = clusterServiceEntity.getServiceName();	ServiceInfo serviceInfo = ambariMetaInfo.getService(stack.getName(), stack.getVersion(), serviceName);	for (String configTypeName : serviceInfo.getConfigTypeAttributes().keySet()) {	if (selectedCountForType.get(configTypeName) == null) {	checkPassed = false;	} else {	if (selectedCountForType.get(configTypeName) == 0) {	checkPassed = false;	
configuration has no enabled entries for service 

String serviceName = clusterServiceEntity.getServiceName();	ServiceInfo serviceInfo = ambariMetaInfo.getService(stack.getName(), stack.getVersion(), serviceName);	for (String configTypeName : serviceInfo.getConfigTypeAttributes().keySet()) {	if (selectedCountForType.get(configTypeName) == null) {	checkPassed = false;	} else {	if (selectedCountForType.get(configTypeName) == 0) {	checkPassed = false;	} else if (selectedCountForType.get(configTypeName) > 1) {	checkPassed = false;	
configuration has more than enabled entry for service 

checkPassed = false;	}	}	}	}	}	}	}	}	if (checkPassed) {	
db configs consistency check passed 

public static void checkDBVersion() throws AmbariException {	
checking db store version 

try (Scanner scanner = new Scanner(versionFile)) {	serverVersion = scanner.useDelimiter("\\Z").next();	} catch (IOException ioe) {	throw new AmbariException("Unable to read server version file.");	}	if (schemaVersionEntity==null || VersionUtils.compareVersions(schemaVersion, serverVersion, 3) != 0) {	String error = "Current database store version is not compatible with " + "current server version" + ", serverVersion=" + serverVersion + ", schemaVersion=" + schemaVersion;	LOG.warn(error);	throw new AmbariException(error);	}	
db store version is compatible 

========================= ambari sample_3677 =========================

public Storage getStorage() {	String fileName = context.getProperties().get("dataworker.storagePath");	Storage storageInstance;	if (fileName != null) {	
using local storage in to store data 

public Storage getStorage() {	String fileName = context.getProperties().get("dataworker.storagePath");	Storage storageInstance;	if (fileName != null) {	storageInstance = new LocalKeyValueStorage(context);	} else {	
using persistence api to store data 

========================= ambari sample_687 =========================

private Long getClusterResourceId() {	Long clusterResourceId = null;	if(!StringUtils.isEmpty(clusterName)) {	try {	Cluster cluster = controllerFactory.getController().getClusters().getCluster(clusterName);	if(cluster == null) {	
no cluster found with the name assuming null resource id 

Long clusterResourceId = null;	if(!StringUtils.isEmpty(clusterName)) {	try {	Cluster cluster = controllerFactory.getController().getClusters().getCluster(clusterName);	if(cluster == null) {	}	else {	clusterResourceId = cluster.getResourceId();	}	} catch (AmbariException e) {	
an exception occurred looking up the cluster named assuming null resource id 

Cluster cluster = controllerFactory.getController().getClusters().getCluster(clusterName);	if(cluster == null) {	}	else {	clusterResourceId = cluster.getResourceId();	}	} catch (AmbariException e) {	}	}	else {	
the cluster name is not set assuming null resource id 

loggingResource.setProperty("logList", response.getListOfResults());	result.getResultTree().addChild(loggingResource, "logging");	Response.ResponseBuilder builder = Response.status(result.getStatus().getStatusCode()).entity( serializer.serialize(result));	if (mediaType != null) {	builder.type(mediaType);	}	RetryHelper.clearAffectedClusters();	return builder.build();	}	} else {	
logsearch is not currently available an empty response will be returned 

========================= ambari sample_4223 =========================

public Response getJob(@PathParam("jobId") String jobId) {	
fetching job with id 

public Response getJob(@PathParam("jobId") String jobId) {	try {	PigJob job = null;	try {	job = getResourceManager().read(jobId);	} catch (ItemNotFound itemNotFound) {	
exception occurred 

try {	job = getResourceManager().read(jobId);	} catch (ItemNotFound itemNotFound) {	throw new NotFoundFormattedException(itemNotFound.getMessage(), itemNotFound);	}	getResourceManager().retrieveJobStatus(job);	JSONObject object = new JSONObject();	object.put("job", job);	return Response.ok(object).build();	} catch (WebApplicationException ex) {	
exception occurred 

} catch (ItemNotFound itemNotFound) {	throw new NotFoundFormattedException(itemNotFound.getMessage(), itemNotFound);	}	getResourceManager().retrieveJobStatus(job);	JSONObject object = new JSONObject();	object.put("job", job);	return Response.ok(object).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (Exception ex) {	
exception occurred 


jobId killing job remove 

try {	PigJob job = null;	try {	job = getResourceManager().read(jobId);	} catch (ItemNotFound itemNotFound) {	
exception occurred 

job = getResourceManager().read(jobId);	} catch (ItemNotFound itemNotFound) {	throw new NotFoundFormattedException(itemNotFound.getMessage(), itemNotFound);	}	getResourceManager().killJob(job);	if (remove != null && remove.compareTo("true") == 0) {	getResourceManager().delete(jobId);	}	return Response.status(204).build();	} catch (WebApplicationException ex) {	
exception occurred 

throw new NotFoundFormattedException(itemNotFound.getMessage(), itemNotFound);	}	getResourceManager().killJob(job);	if (remove != null && remove.compareTo("true") == 0) {	getResourceManager().delete(jobId);	}	return Response.status(204).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (Exception ex) {	
exception occurred 

public Response jobCompletionNotification(@Context HttpHeaders headers, try {	PigJob job = null;	job = getResourceManager().ignorePermissions(new Callable<PigJob>() {	public PigJob call() throws Exception {	PigJob job = null;	try {	job = getResourceManager().read(jobId);	} catch (ItemNotFound itemNotFound) {	
exception occurred 

} catch (ItemNotFound itemNotFound) {	return null;	}	return job;	}	});	if (job == null) throw new NotFoundFormattedException("Job with id '" + jobId + "' not found", null);	getResourceManager().retrieveJobStatus(job);	return Response.ok().build();	} catch (WebApplicationException ex) {	
exception occurred 

}	return job;	}	});	if (job == null) throw new NotFoundFormattedException("Job with id '" + jobId + "' not found", null);	getResourceManager().retrieveJobStatus(job);	return Response.ok().build();	} catch (WebApplicationException ex) {	throw ex;	} catch (Exception ex) {	
exception occurred 


fetching results in filename 

try {	PigJob job = null;	try {	job = getResourceManager().read(jobId);	} catch (ItemNotFound itemNotFound) {	
exception occurred 

try {	PigJob job = null;	try {	job = getResourceManager().read(jobId);	} catch (ItemNotFound itemNotFound) {	throw new NotFoundFormattedException("Job with id '" + jobId + "' not found", null);	}	String filePath = job.getStatusDir() + "/" + fileName;	
reading file 

FileResource file = new FileResource();	file.setFilePath(filePath);	file.setFileContent(paginator.readPage(page));	file.setHasNext(paginator.pageCount() > page + 1);	file.setPage(page);	file.setPageCount(paginator.pageCount());	JSONObject object = new JSONObject();	object.put("file", file);	return Response.ok(object).status(200).build();	} catch (WebApplicationException ex) {	
exception occurred 

file.setFileContent(paginator.readPage(page));	file.setHasNext(paginator.pageCount() > page + 1);	file.setPage(page);	file.setPageCount(paginator.pageCount());	JSONObject object = new JSONObject();	object.put("file", file);	return Response.ok(object).status(200).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (IOException ex) {	
exception occurred 

file.setPage(page);	file.setPageCount(paginator.pageCount());	JSONObject object = new JSONObject();	object.put("file", file);	return Response.ok(object).status(200).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (IOException ex) {	throw new NotFoundFormattedException(ex.getMessage(), ex);	} catch (InterruptedException ex) {	
exception occurred 

JSONObject object = new JSONObject();	object.put("file", file);	return Response.ok(object).status(200).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (IOException ex) {	throw new NotFoundFormattedException(ex.getMessage(), ex);	} catch (InterruptedException ex) {	throw new NotFoundFormattedException(ex.getMessage(), ex);	} catch (Exception ex) {	
exception occurred 

public Response getJobList(@QueryParam("scriptId") final String scriptId) {	
fechting scriptid 

else {	PigJob job = (PigJob) item;	return (job.getScriptId() != null && scriptId.compareTo(job.getScriptId()) == 0 && super.isConform(item));	}	}	});	JSONObject object = new JSONObject();	object.put("jobs", allJobs);	return Response.ok(object).build();	} catch (WebApplicationException ex) {	
exception occurred 

return (job.getScriptId() != null && scriptId.compareTo(job.getScriptId()) == 0 && super.isConform(item));	}	}	});	JSONObject object = new JSONObject();	object.put("jobs", allJobs);	return Response.ok(object).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (Exception ex) {	
exception occurred 


creating new job 

try {	job = getResourceManager().read(request.job.getId());	} catch (ItemNotFound itemNotFound) {	throw new NotFoundFormattedException("Job not found", null);	}	response.setHeader("Location", String.format("%s/%s", ui.getAbsolutePath().toString(), request.job.getId()));	JSONObject object = new JSONObject();	object.put("job", job);	return Response.ok(object).status(201).build();	} catch (WebApplicationException ex) {	
exception occurred 

} catch (ItemNotFound itemNotFound) {	throw new NotFoundFormattedException("Job not found", null);	}	response.setHeader("Location", String.format("%s/%s", ui.getAbsolutePath().toString(), request.job.getId()));	JSONObject object = new JSONObject();	object.put("job", job);	return Response.ok(object).status(201).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (IllegalArgumentException ex) {	
exception occurred 

}	response.setHeader("Location", String.format("%s/%s", ui.getAbsolutePath().toString(), request.job.getId()));	JSONObject object = new JSONObject();	object.put("job", job);	return Response.ok(object).status(201).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (IllegalArgumentException ex) {	throw new BadRequestFormattedException(ex.getMessage(), ex);	} catch (Exception ex) {	
exception occurred 

========================= ambari sample_1021 =========================

public static int run(String action, String hostsFile, String servicesFile, String outputFile, String errorFile) {	
serviceadvisor received arguments action s hosts file s services file s 

try {	ServiceAdvisorCommandType commandType = ServiceAdvisorCommandType.getEnum(action);	ProcessBuilder builder = preparePythonShellCommand(commandType, hostsFile, servicesFile, outputFile, errorFile);	returnCode = launchProcess(builder);	} catch (IllegalArgumentException e) {	List<ServiceAdvisorCommandType> values = EnumUtils.getEnumList(ServiceAdvisorCommandType.class);	List<String> stringValues = new ArrayList<String>();	for (ServiceAdvisorCommandType value : values) {	stringValues.add(value.toString());	}	
serviceadvisor illegal argument action must be one of 

ProcessBuilder builder = preparePythonShellCommand(commandType, hostsFile, servicesFile, outputFile, errorFile);	returnCode = launchProcess(builder);	} catch (IllegalArgumentException e) {	List<ServiceAdvisorCommandType> values = EnumUtils.getEnumList(ServiceAdvisorCommandType.class);	List<String> stringValues = new ArrayList<String>();	for (ServiceAdvisorCommandType value : values) {	stringValues.add(value.toString());	}	return -1;	}  catch (Exception e) {	
serviceadvisor failed with 

StringBuilder commandString = new StringBuilder();	commandString.append(PYTHON_STACK_ADVISOR_SCRIPT + " ");	commandString.append(commandType.toString()).append(" ");	commandString.append(hostsFile).append(" ");	commandString.append(servicesFile).append(" ");	commandString.append("1> ");	commandString.append(outputFile).append(" ");	commandString.append("2>");	commandString.append(errorFile).append(" ");	builderParameters.add(commandString.toString());	
serviceadvisor python command is 

========================= ambari sample_1308 =========================

}	TimelineMetric tmpMetric = new TimelineMetric(metrics.getMetrics().get(0));	tmpMetric.setMetricValues(new TreeMap<Long, Double>());	metricAggregateMap.add(new TimelineMetricWithAggregatedValues(tmpMetric, new MetricHostAggregate(sum, count, 0d, max, min)));	}	String json = null;	try {	json = mapper.writeValueAsString(new AggregationResult(metricAggregateMap, System.currentTimeMillis()));	LOG.debug(json);	} catch (Exception e) {	
failed to convert result into json 

========================= ambari sample_280 =========================

private String configValue(AmbariLdapConfigurationKeys ambariLdapConfigurationKeys) {	final String value;	if (configurationMap.containsKey(ambariLdapConfigurationKeys.key())) {	value = configurationMap.get(ambariLdapConfigurationKeys.key());	} else {	
ldap configuration property hasn t been set using default value 

========================= ambari sample_2932 =========================

while (!done.get()) {	LoggingEvent event = null;	while ((event = events.poll()) != null) {	Object result = null;	try {	parser.addEventToParse(event);	while ((result = parser.getParseResult()) != null) {	try {	store.persist(event, result);	} catch (IOException e) {	
failed to persist 

Object result = null;	try {	parser.addEventToParse(event);	while ((result = parser.getParseResult()) != null) {	try {	store.persist(event, result);	} catch (IOException e) {	}	}	} catch (IOException ioe) {	
failed to parse log event 

}	}	try {	Thread.sleep(WAIT_EMPTY_QUEUE);	} catch(InterruptedException ie) {	}	}	try {	store.close();	} catch (IOException ioe) {	
failed to close logstore 

========================= ambari sample_500 =========================

int page = 1;	while (saved < json.length()) {	int end = Math.min(saved + VALUE_LENGTH_LIMIT, json.length());	String substring = json.substring(saved, end);	getConfig().setProperty(modelPropName + "#" + page, substring);	saved += VALUE_LENGTH_LIMIT;	page += 1;	LOG.debug("Chunk saved: " + modelPropName + "#" + page + "=" + substring);	}	getConfig().setProperty(modelPropName, page - 1);	
write finished pages 

protected String read(String modelPropName) {	StringBuilder result = new StringBuilder();	int pages = getConfig().getInt(modelPropName);	
read started pages 

protected void clear(String modelPropName) {	int pages = getConfig().getInt(modelPropName);	
clean started pages 

protected void clear(String modelPropName) {	int pages = getConfig().getInt(modelPropName);	for(int page = 1; page <= pages; page++) {	getConfig().clearProperty(modelPropName + "#" + page);	
chunk clean 

========================= ambari sample_1037 =========================

public Response fetchJobStatus(@PathParam("jobId") String jobId) throws ItemNotFound, HiveClientException, NoOperationStatusSetException {	JobController jobController = getResourceManager().readController(jobId);	Job job = jobController.getJob();	String jobStatus = job.getStatus();	
jobstatus for jobid 

public List<Job> getList(@QueryParam("startTime") long startTime, @QueryParam("endTime") long endTime) {	try {	
getting all job starttime endtime 

public List<Job> getList(@QueryParam("startTime") long startTime, @QueryParam("endTime") long endTime) {	try {	List<Job> allJobs = getAggregator().readAllForUserByTime(context.getUsername(),startTime, endTime);	for(Job job : allJobs) {	job.setSessionTag(null);	}	return allJobs;	} catch (WebApplicationException ex) {	
exception occured while fetching all jobs 

public List<Job> getList(@QueryParam("startTime") long startTime, @QueryParam("endTime") long endTime) {	try {	List<Job> allJobs = getAggregator().readAllForUserByTime(context.getUsername(),startTime, endTime);	for(Job job : allJobs) {	job.setSessionTag(null);	}	return allJobs;	} catch (WebApplicationException ex) {	throw ex;	} catch (Exception ex) {	
exception occured while fetching all jobs 

public List<Job> getList(List<JobInfo> jobInfos) {	try {	
fetching jobs with ids 

public List<Job> getList(List<JobInfo> jobInfos) {	try {	List<Job> allJobs = getAggregator().readJobsByIds(jobInfos);	for(Job job : allJobs) {	job.setSessionTag(null);	}	return allJobs;	} catch (WebApplicationException ex) {	
exception occured while fetching all jobs 

public List<Job> getList(List<JobInfo> jobInfos) {	try {	List<Job> allJobs = getAggregator().readJobsByIds(jobInfos);	for(Job job : allJobs) {	job.setSessionTag(null);	}	return allJobs;	} catch (WebApplicationException ex) {	throw ex;	} catch (Exception ex) {	
exception occured while fetching all jobs 

Map jobInfo = PropertyUtils.describe(request.job);	Job job = new JobImpl(jobInfo);	getResourceManager().create(job);	JobController createdJobController = getResourceManager().readController(job.getId());	createdJobController.submit();	getResourceManager().saveIfModified(createdJobController);	response.setHeader("Location", String.format("%s/%s", ui.getAbsolutePath().toString(), job.getId()));	JSONObject jobObject = jsonObjectFromJob(createdJobController);	return Response.ok(jobObject).status(201).build();	} catch (WebApplicationException ex) {	
error occurred while creating job 

getResourceManager().create(job);	JobController createdJobController = getResourceManager().readController(job.getId());	createdJobController.submit();	getResourceManager().saveIfModified(createdJobController);	response.setHeader("Location", String.format("%s/%s", ui.getAbsolutePath().toString(), job.getId()));	JSONObject jobObject = jsonObjectFromJob(createdJobController);	return Response.ok(jobObject).status(201).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (ItemNotFound itemNotFound) {	
error occurred while creating job 

createdJobController.submit();	getResourceManager().saveIfModified(createdJobController);	response.setHeader("Location", String.format("%s/%s", ui.getAbsolutePath().toString(), job.getId()));	JSONObject jobObject = jsonObjectFromJob(createdJobController);	return Response.ok(jobObject).status(201).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (ItemNotFound itemNotFound) {	throw new NotFoundFormattedException(itemNotFound.getMessage(), itemNotFound);	} catch (Throwable ex) {	
error occurred while creating job 

========================= ambari sample_853 =========================

protected void configure() {	collectorProtocol = configuration.get(AMS_SITE_HTTP_POLICY_PROPERTY, "HTTP_ONLY").equalsIgnoreCase("HTTP_ONLY") ? "http" : "https";	collectorPort = configuration.getTrimmed(AMS_SITE_COLLECTOR_WEBAPP_ADDRESS_PROPERTY, "0.0.0.0:6188").split(":")[1];	collectorHosts = parseHostsStringIntoCollection(configuration.getTrimmed(PUBLISHER_COLLECTOR_HOSTS_PROPERTY, ""));	zkQuorum = configuration.get(PUBLISHER_ZOOKEEPER_QUORUM_PROPERTY, "");	hostname = configuration.get(PUBLISHER_HOSTNAME_PROPERTY, "localhost");	collectorHosts = parseHostsStringIntoCollection(configuration.get(PUBLISHER_COLLECTOR_HOSTS_PROPERTY, ""));	if (collectorHosts.isEmpty()) {	
no metric collector configured 

protected void processAndPublishMetrics(Map<String, TimelineMetrics> metricsFromCache) throws Exception {	if (metricsFromCache.size()==0) return;	
preparing s timeline metrics for publishing 

========================= ambari sample_279 =========================

private AmbariLdapConfiguration loadInstance() {	List<AmbariConfigurationEntity> configEntities = null;	
loading ldap configuration 

private AmbariLdapConfiguration loadInstance() {	List<AmbariConfigurationEntity> configEntities = null;	if (jpaInitialized.get()) {	
actually loading ldap configuration from db 

List<AmbariConfigurationEntity> configEntities = null;	if (jpaInitialized.get()) {	configEntities = ambariConfigurationDAOProvider.get().findByCategory(AmbariServerConfigurationCategory.LDAP_CONFIGURATION.getCategoryName());	}	if (configEntities != null) {	Map<String, String> properties = toProperties(configEntities);	instance = new AmbariLdapConfiguration(properties);	}else {	instance = new AmbariLdapConfiguration();	}	
loaded ldap configuration instance 

public void ambariLdapConfigChanged(AmbariConfigurationChangedEvent event) {	
ldap config changed event received 

public void ambariLdapConfigChanged(AmbariConfigurationChangedEvent event) {	loadInstance();	
refreshed ldap config instance 

public void jpaInitialized(JpaInitializedEvent event) {	
jpa initialized event received 

public void jpaInitialized(JpaInitializedEvent event) {	jpaInitialized.getAndSet(true);	loadInstance();	
refreshed ldap config instance 

========================= ambari sample_2924 =========================

private void removeOrphanConfigTypes(Configuration configuration) {	Blueprint blueprint = clusterTopology.getBlueprint();	Collection<String> configTypes = configuration.getAllConfigTypes();	for (String configType : configTypes) {	if (!blueprint.isValidConfigType(configType)) {	configuration.removeConfigType(configType);	
removing config type as related service is not present in either blueprint or cluster creation template 

clusterEnv.put("security_enabled", "true");	for (String configType : updatedConfigs.keySet()) {	if (blueprint.isValidConfigType(configType)) {	Map<String, String> propertyMap = updatedConfigs.get(configType);	Map<String, String> clusterConfigProperties = existingConfigurations.get(configType);	Map<String, String> stackDefaultConfigProperties = stackDefaultProps.get(configType);	for (String property : propertyMap.keySet()) {	String currentValue = clusterConfiguration.getPropertyValue(configType, property);	String newValue = propertyMap.get(property);	if (!propertyHasCustomValue(clusterConfigProperties, stackDefaultConfigProperties, property) && (currentValue == null || !currentValue.equals(newValue))) {	
update kerberos related config property 

String currentValue = clusterConfiguration.getPropertyValue(configType, property);	String newValue = propertyMap.get(property);	if (!propertyHasCustomValue(clusterConfigProperties, stackDefaultConfigProperties, property) && (currentValue == null || !currentValue.equals(newValue))) {	clusterConfiguration.setProperty(configType, property, newValue);	updatedConfigTypes.add(configType);	}	}	}	}	} catch (KerberosInvalidConfigurationException e) {	
an exception occurred while doing kerberos related configuration update 

Map<String, Map<String, String>> updatedConfigs = AmbariContext.getController().getKerberosHelper() .getServiceConfigurationUpdates(cluster, existingConfigurations, createServiceComponentMap(blueprint), null, null, true, false);	for (String configType : updatedConfigs.keySet()) {	Map<String, String> propertyMap = updatedConfigs.get(configType);	for (String property : propertyMap.keySet()) {	String propertyValue = propertyMap.get(property);	Matcher matcher = CLUSTER_HOST_INFO_PATTERN_VARIABLE.matcher(propertyValue);	while (matcher.find()) {	String component = matcher.group(1).toUpperCase();	Collection<String> hostGroups = clusterTopology.getHostGroupsForComponent(component);	if (hostGroups.isEmpty()) {	
no matching hostgroup found for component specified in kerberos config type property 

String component = matcher.group(1).toUpperCase();	Collection<String> hostGroups = clusterTopology.getHostGroupsForComponent(component);	if (hostGroups.isEmpty()) {	} else {	requiredHostGroups.addAll(hostGroups);	}	}	}	}	} catch (KerberosInvalidConfigurationException | AmbariException e) {	
an exception occurred while doing kerberos related configuration update 

LOG.info("Sending cluster config update request for service = " + blueprintConfigRequest.getServiceName());	ambariContext.setConfigurationOnCluster(clusterRequest);	} else {	LOG.error("ClusterRequest should not be null for service = " + blueprintConfigRequest.getServiceName());	}	}	if (tag.equals(TopologyManager.TOPOLOGY_RESOLVED_TAG)) {	try {	ambariContext.waitForConfigurationResolution(clusterName, updatedConfigTypes);	} catch (AmbariException e) {	
error while attempting to wait for the cluster configuration to reach topology resolved state 

========================= ambari sample_2709 =========================

private void trimPropertyValue(Configuration clusterConfig, Stack stack, String configType, Map<String, String> properties, String propertyName) {	if (propertyName != null && properties.get(propertyName) != null) {	TrimmingStrategy trimmingStrategy = PropertyValueTrimmingStrategyDefiner.defineTrimmingStrategy(stack, propertyName, configType);	String oldValue = properties.get(propertyName);	String newValue = trimmingStrategy.trim(oldValue);	if (!newValue.equals(oldValue)){	
changing value for config property from to 

private void doFilterPriorToExport(Configuration configuration) {	Map<String, Map<String, String>> properties = configuration.getFullProperties();	Map<Long, Set<String>> authToLocalPerClusterMap = null;	try {	String clusterName = clusterTopology.getAmbariContext().getClusterName(clusterTopology.getClusterId());	Cluster cluster = clusterTopology.getAmbariContext().getController().getClusters().getCluster(clusterName);	authToLocalPerClusterMap = new HashMap<>();	authToLocalPerClusterMap.put(Long.valueOf(clusterTopology.getClusterId()), clusterTopology.getAmbariContext().getController().getKerberosHelper().getKerberosDescriptor(cluster, false).getAllAuthToLocalProperties());	} catch (AmbariException e) {	
error while getting authtolocal properties 

authToLocalPerClusterMap = new HashMap<>();	authToLocalPerClusterMap.put(Long.valueOf(clusterTopology.getClusterId()), clusterTopology.getAmbariContext().getController().getKerberosHelper().getKerberosDescriptor(cluster, false).getAllAuthToLocalProperties());	} catch (AmbariException e) {	}	PropertyFilter [] exportPropertyFilters = getExportPropertyFilters(authToLocalPerClusterMap);	for (Map.Entry<String, Map<String, String>> configEntry : properties.entrySet()) {	String type = configEntry.getKey();	try {	clusterTopology.getBlueprint().getStack().getServiceForConfigType(type);	} catch (IllegalArgumentException illegalArgumentException) {	
error encountered while trying to obtain the service name for config type s further processing on this config type will be skipped this usually means that a service s definitions have been manually removed from the ambari stack definitions if the stack definitions have not been changed manually this may indicate a stack definition error in ambari 

private void doRecommendConfigurations(Configuration configuration, Set<String> configTypesUpdated) {	ConfigRecommendationStrategy configRecommendationStrategy = clusterTopology.getConfigRecommendationStrategy();	Map<String, AdvisedConfiguration> advisedConfigurations = clusterTopology.getAdvisedConfigurations();	
config recommendation strategy being used is 

private void doRecommendConfigurations(Configuration configuration, Set<String> configTypesUpdated) {	ConfigRecommendationStrategy configRecommendationStrategy = clusterTopology.getConfigRecommendationStrategy();	Map<String, AdvisedConfiguration> advisedConfigurations = clusterTopology.getAdvisedConfigurations();	if (ConfigRecommendationStrategy.ONLY_STACK_DEFAULTS_APPLY.equals(configRecommendationStrategy)) {	
filter out recommended configurations keep only the stack defaults 

private void doRecommendConfigurations(Configuration configuration, Set<String> configTypesUpdated) {	ConfigRecommendationStrategy configRecommendationStrategy = clusterTopology.getConfigRecommendationStrategy();	Map<String, AdvisedConfiguration> advisedConfigurations = clusterTopology.getAdvisedConfigurations();	if (ConfigRecommendationStrategy.ONLY_STACK_DEFAULTS_APPLY.equals(configRecommendationStrategy)) {	doFilterStackDefaults(advisedConfigurations);	}	if (!ConfigRecommendationStrategy.NEVER_APPLY.equals(configRecommendationStrategy)) {	for (Map.Entry<String, AdvisedConfiguration> advConfEntry : advisedConfigurations.entrySet()) {	String configType = advConfEntry.getKey();	AdvisedConfiguration advisedConfig = advConfEntry.getValue();	
update configurations with recommended configurations provided by the stack advisor 

String configType = advConfEntry.getKey();	AdvisedConfiguration advisedConfig = advConfEntry.getValue();	if (advisedConfig.getProperties() != null) {	doReplaceProperties(configuration, configType, advisedConfig, configTypesUpdated);	}	if (advisedConfig.getPropertyValueAttributes() != null) {	doRemovePropertiesIfNeeded(configuration, configType, advisedConfig, configTypesUpdated);	}	}	} else {	
no recommended configurations are applied strategy 

private static boolean shouldPropertyBeExcludedForClusterUpdate(String propertyName, String propertyValue, String propertyType, ClusterTopology topology) {	for(PropertyFilter filter : clusterUpdatePropertyFilters) {	try {	if (!filter.isPropertyIncluded(propertyName, propertyValue, propertyType, topology)) {	if (!shouldPropertyBeStoredWithDefault(propertyName)) {	return true;	}	}	} catch (Throwable throwable) {	
error occurred while attempting to process the property with a filter this may indicate a config error 

if (!result.isEmpty()) {	return result;	} else {	Collection<String> matchingGroups = topology.getHostGroupsForComponent(component);	int matchingGroupCount = matchingGroups.size();	if (matchingGroupCount != 0) {	return new HashSet<>(matchingGroups);	} else {	Cardinality cardinality = topology.getBlueprint().getStack().getCardinality(component);	if (! cardinality.isValidCount(0)) {	
the property is associated with the component which isn t mapped to any host group this may affect configuration topology resolution 

private void addExcludedConfigProperties(Configuration configuration, Set<String> configTypesUpdated, Stack stack) {	Collection<String> blueprintServices = clusterTopology.getBlueprint().getServices();	
handling excluded properties for blueprint services 

private void addExcludedConfigProperties(Configuration configuration, Set<String> configTypesUpdated, Stack stack) {	Collection<String> blueprintServices = clusterTopology.getBlueprint().getServices();	for (String blueprintService : blueprintServices) {	
handling excluded properties for blueprint service 

private void addExcludedConfigProperties(Configuration configuration, Set<String> configTypesUpdated, Stack stack) {	Collection<String> blueprintServices = clusterTopology.getBlueprint().getServices();	for (String blueprintService : blueprintServices) {	Set<String> excludedConfigTypes = stack.getExcludedConfigurationTypes(blueprintService);	if (excludedConfigTypes.isEmpty()) {	
there are no excluded config types for blueprint service 

private void addExcludedConfigProperties(Configuration configuration, Set<String> configTypesUpdated, Stack stack) {	Collection<String> blueprintServices = clusterTopology.getBlueprint().getServices();	for (String blueprintService : blueprintServices) {	Set<String> excludedConfigTypes = stack.getExcludedConfigurationTypes(blueprintService);	if (excludedConfigTypes.isEmpty()) {	continue;	}	for(String configType: excludedConfigTypes) {	
handling excluded config type for blueprint service 

for (String blueprintService : blueprintServices) {	Set<String> excludedConfigTypes = stack.getExcludedConfigurationTypes(blueprintService);	if (excludedConfigTypes.isEmpty()) {	continue;	}	for(String configType: excludedConfigTypes) {	String blueprintServiceForExcludedConfig;	try {	blueprintServiceForExcludedConfig = stack.getServiceForConfigType(configType);	} catch (IllegalArgumentException illegalArgumentException) {	
error encountered while trying to obtain the service name for config type further processing on this excluded config type will be skipped this usually means that a service s definitions have been manually removed from the ambari stack definitions if the stack definitions have not been changed manually this may indicate a stack definition error in ambari 

continue;	}	for(String configType: excludedConfigTypes) {	String blueprintServiceForExcludedConfig;	try {	blueprintServiceForExcludedConfig = stack.getServiceForConfigType(configType);	} catch (IllegalArgumentException illegalArgumentException) {	continue;	}	if (!blueprintServices.contains(blueprintServiceForExcludedConfig)) {	
service for excluded config type is not present in the blueprint ignoring excluded config entries 

try {	blueprintServiceForExcludedConfig = stack.getServiceForConfigType(configType);	} catch (IllegalArgumentException illegalArgumentException) {	continue;	}	if (!blueprintServices.contains(blueprintServiceForExcludedConfig)) {	continue;	}	Map<String, String> configProperties = stack.getConfigurationProperties(blueprintService, configType);	for(Map.Entry<String, String> entry: configProperties.entrySet()) {	
add property 

========================= ambari sample_3548 =========================

protected <T> int cleanTableByIds(Set<Long> ids, String paramName, String entityName, Long beforeDateMillis, String entityQuery, Class<T> type) {	
deleting s entities before date s 

protected <T> int cleanTableByIds(Set<Long> ids, String paramName, String entityName, Long beforeDateMillis, String entityQuery, Class<T> type) {	EntityManager entityManager = entityManagerProvider.get();	int affectedRows = 0;	TypedQuery<T> query = entityManager.createNamedQuery(entityQuery, type);	if (ids != null && !ids.isEmpty()) {	for (int i = 0; i < ids.size(); i += BATCH_SIZE) {	int endRow = (i + BATCH_SIZE) > ids.size() ? ids.size() : (i + BATCH_SIZE);	List<Long> idsSubList = new ArrayList<>(ids).subList(i, endRow);	
deleting entity batch with task ids 

protected <T> int cleanTableByStageEntityPK(List<StageEntityPK> ids, LinkedList<String> paramNames, String entityName, Long beforeDateMillis, String entityQuery, Class<T> type) {	
deleting s entities before date s 

protected <T> int cleanTableByStageEntityPK(List<StageEntityPK> ids, LinkedList<String> paramNames, String entityName, Long beforeDateMillis, String entityQuery, Class<T> type) {	EntityManager entityManager = entityManagerProvider.get();	int affectedRows = 0;	TypedQuery<T> query = entityManager.createNamedQuery(entityQuery, type);	if (ids != null && !ids.isEmpty()) {	for (int i = 0; i < ids.size(); i += BATCH_SIZE) {	int endRow = (i + BATCH_SIZE) > ids.size() ? ids.size() : (i + BATCH_SIZE);	List<StageEntityPK> idsSubList = new ArrayList<>(ids).subList(i, endRow);	
deleting entity batch with task ids 

for (Long topologyRequestId : topologyRequestIds) {	topologyRequestDAO.removeByPK(topologyRequestId);	}	affectedRows += cleanTableByIds(taskIds, "taskIds", "HostRoleCommand", policy.getToDateInMillis(), "HostRoleCommandEntity.removeByTaskIds", HostRoleCommandEntity.class);	affectedRows += cleanTableByStageEntityPK(requestStageIds, params, "RoleSuccessCriteria", policy.getToDateInMillis(), "RoleSuccessCriteriaEntity.removeByRequestStageIds", RoleSuccessCriteriaEntity.class);	affectedRows += cleanTableByStageEntityPK(requestStageIds, params, "Stage", policy.getToDateInMillis(), "StageEntity.removeByRequestStageIds", StageEntity.class);	affectedRows += cleanTableByIds(requestIds, "requestIds", "RequestResourceFilter", policy.getToDateInMillis(), "RequestResourceFilterEntity.removeByRequestIds", RequestResourceFilterEntity.class);	affectedRows += cleanTableByIds(requestIds, "requestIds", "RequestOperationLevel", policy.getToDateInMillis(), "RequestOperationLevelEntity.removeByRequestIds", RequestOperationLevelEntity.class);	affectedRows += cleanTableByIds(requestIds, "requestIds", "Request", policy.getToDateInMillis(), "RequestEntity.removeByRequestIds", RequestEntity.class);	} catch (AmbariException e) {	
error while looking up cluster with name 

========================= ambari sample_4262 =========================

public void start() {	Thread setupThread = new Thread("setup_logsearch_config") {	public void run() {	
started thread to set up log search config 

public void start() {	Thread setupThread = new Thread("setup_logsearch_config") {	public void run() {	while (true) {	try {	logSearchConfig = LogSearchConfigFactory.createLogSearchConfigServer(logSearchConfigMapHolder.getLogsearchProperties(), LogSearchConfigServerZK.class);	logSearchConfigState.setLogSearchConfigAvailable(true);	break;	} catch (Exception e) {	
could not initialize log search config going to sleep for seconds 

========================= ambari sample_1430 =========================

} catch (IOException e) {	String message = String.format("Failed to translate %s to a local username during Kerberos authentication: %s", principal, e.getLocalizedMessage());	LOG.warn(message);	throw new UsernameNotFoundException(message, e);	}	if (username == null) {	String message = String.format("Failed to translate %s to a local username during Kerberos authentication.", principal);	LOG.warn(message);	throw new UsernameNotFoundException(message);	}	
translated to using auth to local rules during kerberos authentication 

throw new UsernameNotFoundException(message, e);	}	if (username == null) {	String message = String.format("Failed to translate %s to a local username during Kerberos authentication.", principal);	LOG.warn(message);	throw new UsernameNotFoundException(message);	}	return createUser(username, principal);	} else if (entities.size() == 1) {	UserEntity userEntity = entities.iterator().next().getUser();	
found kerberos authentication method for using principal 

private UserDetails createUser(String username, String principal) throws AuthenticationException {	UserEntity userEntity = users.getUserEntity(username);	if (userEntity == null) {	
user not found from 

if (userEntity == null) {	throw new UserNotFoundException(username, String.format("Cannot find user using Kerberos ticket (%s).", principal));	} else {	List<UserAuthenticationEntity> authenticationEntities = userEntity.getAuthenticationEntities();	boolean hasKerberos = false;	for (UserAuthenticationEntity entity : authenticationEntities) {	UserAuthenticationType authenticationType = entity.getAuthenticationType();	switch (authenticationType) {	case KERBEROS: String key = entity.getAuthenticationKey();	if (StringUtils.isEmpty(key) || key.equals(username)) {	
found kerberos authentication method for where no principal was set fixing 

boolean hasKerberos = false;	for (UserAuthenticationEntity entity : authenticationEntities) {	UserAuthenticationType authenticationType = entity.getAuthenticationType();	switch (authenticationType) {	case KERBEROS: String key = entity.getAuthenticationKey();	if (StringUtils.isEmpty(key) || key.equals(username)) {	try {	users.addKerberosAuthentication(userEntity, principal);	users.removeAuthentication(userEntity, entity.getUserAuthenticationId());	} catch (AmbariException e) {	
failed to create kerberos authentication method entry for s with principal s s 

switch (authenticationType) {	case KERBEROS: String key = entity.getAuthenticationKey();	if (StringUtils.isEmpty(key) || key.equals(username)) {	try {	users.addKerberosAuthentication(userEntity, principal);	users.removeAuthentication(userEntity, entity.getUserAuthenticationId());	} catch (AmbariException e) {	}	hasKerberos = true;	} else if (principal.equalsIgnoreCase(entity.getAuthenticationKey())) {	
found kerberos authentication method for using principal 

}	break;	}	if (hasKerberos) {	break;	}	}	if (!hasKerberos) {	try {	users.addKerberosAuthentication(userEntity, principal);	
added kerberos authentication method for using principal 

break;	}	if (hasKerberos) {	break;	}	}	if (!hasKerberos) {	try {	users.addKerberosAuthentication(userEntity, principal);	} catch (AmbariException e) {	
failed to add the kerberos authentication method for s s 

========================= ambari sample_2836 =========================

public void run() {	if (checkMetricStore()) {	failures = 0;	if (LOG.isDebugEnabled()) {	
successfully got metrics from timelinemetricstore 

========================= ambari sample_384 =========================

public void start() {	
starting server action executor thread 

while (!Thread.interrupted()) {	try {	synchronized (wakeupSyncObject) {	if (!activeAwakeRequest) {	wakeupSyncObject.wait(sleepTimeMS);	}	activeAwakeRequest = false;	}	doWork();	} catch (InterruptedException e) {	
server action executor thread interrupted starting to shutdown 

if (!activeAwakeRequest) {	wakeupSyncObject.wait(sleepTimeMS);	}	activeAwakeRequest = false;	}	doWork();	} catch (InterruptedException e) {	break;	}	}	
server action executor thread shutting down 

}	doWork();	} catch (InterruptedException e) {	break;	}	}	}	}, "Server Action Executor");	executorThread.start();	if (executorThread.isAlive()) {	
server action executor thread started 

public void stop() {	
stopping server action executor thread 

}	if (!executorThread.isAlive()) {	break;	}	}	if (!executorThread.isAlive()) {	executorThread = null;	}	}	if (executorThread == null) {	
server action executor thread stopped 

if (!executorThread.isAlive()) {	break;	}	}	if (!executorThread.isAlive()) {	executorThread = null;	}	}	if (executorThread == null) {	} else {	
server action executor thread hasn t stopped giving up waiting 

public void doWork() throws InterruptedException {	List<HostRoleCommand> tasks = db.getTasksByRoleAndStatus(Role.AMBARI_SERVER_ACTION.name(), HostRoleStatus.QUEUED);	if ((tasks != null) && !tasks.isEmpty()) {	for (HostRoleCommand task : tasks) {	Long taskId = task.getTaskId();	
processing task 

Long taskId = task.getTaskId();	if (task.getStatus() == HostRoleStatus.QUEUED) {	ExecutionCommandWrapper executionWrapper = task.getExecutionCommandWrapper();	if (executionWrapper != null) {	ExecutionCommand executionCommand = executionWrapper.getExecutionCommand();	if (executionCommand != null) {	Worker worker = new Worker(task, executionCommand);	Thread workerThread = new Thread(worker, String.format("Server Action Executor Worker %s", taskId));	Long timeout = determineTimeout(executionCommand);	updateHostRoleState(task, executionCommand, createInProgressReport());	
starting server action executor worker thread for task 

Long timeout = determineTimeout(executionCommand);	updateHostRoleState(task, executionCommand, createInProgressReport());	workerThread.start();	try {	workerThread.join(timeout);	} catch (InterruptedException e) {	workerThread.interrupt();	throw e;	}	if (workerThread.isAlive()) {	
server action executor worker thread for task timed out it failed to complete within ms 

try {	workerThread.join(timeout);	} catch (InterruptedException e) {	workerThread.interrupt();	throw e;	}	if (workerThread.isAlive()) {	workerThread.interrupt();	updateHostRoleState(task, executionCommand, createTimedOutReport());	} else {	
server action executor worker thread for task exited on its own 

workerThread.interrupt();	throw e;	}	if (workerThread.isAlive()) {	workerThread.interrupt();	updateHostRoleState(task, executionCommand, createTimedOutReport());	} else {	updateHostRoleState(task, executionCommand, worker.getCommandReport());	}	} else {	
task failed to produce an executioncommand skipping 

}	if (workerThread.isAlive()) {	workerThread.interrupt();	updateHostRoleState(task, executionCommand, createTimedOutReport());	} else {	updateHostRoleState(task, executionCommand, worker.getCommandReport());	}	} else {	}	} else {	
task failed to produce an executioncommandwrapper skipping 

workerThread.interrupt();	updateHostRoleState(task, executionCommand, createTimedOutReport());	} else {	updateHostRoleState(task, executionCommand, worker.getCommandReport());	}	} else {	}	} else {	}	} else {	
queued task is expected to have a status of but has a status of skipping 

public void run() {	try {	
executing task 

public void run() {	try {	InternalAuthenticationToken authentication = new InternalAuthenticationToken("server_action_executor");	authentication.setAuthenticated(true);	SecurityContextHolder.getContext().setAuthentication(authentication);	commandReport = execute(hostRoleCommand, executionCommand);	LOG.debug("Task #{} completed execution with status of {}", taskId, (commandReport == null) ? "UNKNOWN" : commandReport.getStatus());	} catch (Throwable t) {	
task failed to complete execution due to thrown exception 

throw new AmbariException("Missing RoleParams data");	} else {	String actionClassname = roleParams.get(ServerAction.ACTION_NAME);	if (actionClassname == null) {	throw new AmbariException("Missing action classname for server action");	} else {	Map<String, ServiceInfo> services = new HashMap<String, ServiceInfo>();	UpgradeSummary upgradeSummary = executionCommand.getUpgradeSummary();	if (upgradeSummary != null) {	Map<String, UpgradeServiceSummary> upgradeServiceSummaries = upgradeSummary.services;	
upgradeservicesummary 

throw new AmbariException("Missing action classname for server action");	} else {	Map<String, ServiceInfo> services = new HashMap<String, ServiceInfo>();	UpgradeSummary upgradeSummary = executionCommand.getUpgradeSummary();	if (upgradeSummary != null) {	Map<String, UpgradeServiceSummary> upgradeServiceSummaries = upgradeSummary.services;	AmbariManagementController ambariManagementController = injector.getInstance(AmbariManagementController.class);	AmbariMetaInfo ambariMetaInfo = ambariManagementController.getAmbariMetaInfo();	String serviceName = executionCommand.getServiceName();	if (serviceName != null && !serviceName.isEmpty()){	
server action s is associated with service s 

UpgradeSummary upgradeSummary = executionCommand.getUpgradeSummary();	if (upgradeSummary != null) {	Map<String, UpgradeServiceSummary> upgradeServiceSummaries = upgradeSummary.services;	AmbariManagementController ambariManagementController = injector.getInstance(AmbariManagementController.class);	AmbariMetaInfo ambariMetaInfo = ambariManagementController.getAmbariMetaInfo();	String serviceName = executionCommand.getServiceName();	if (serviceName != null && !serviceName.isEmpty()){	UpgradeServiceSummary serviceSummary = upgradeServiceSummaries.get(serviceName);	addServiceInfo(services, ambariMetaInfo, serviceSummary.sourceStackId, serviceName);	} else {	
server action s is not associated with a service 

String serviceName = executionCommand.getServiceName();	if (serviceName != null && !serviceName.isEmpty()){	UpgradeServiceSummary serviceSummary = upgradeServiceSummaries.get(serviceName);	addServiceInfo(services, ambariMetaInfo, serviceSummary.sourceStackId, serviceName);	} else {	for(String key: upgradeServiceSummaries.keySet()){	UpgradeServiceSummary serviceSummary = upgradeServiceSummaries.get(key);	addServiceInfo(services, ambariMetaInfo, serviceSummary.sourceStackId, key);	}	}	
attempt to load server action classes from s 

private void addServiceInfo(Map<String, ServiceInfo> services, AmbariMetaInfo ambariMetaInfo, String stackId, String serviceName) {	List<String> stackInfo = getStackInfo(stackId);	
stack info list s 

private void addServiceInfo(Map<String, ServiceInfo> services, AmbariMetaInfo ambariMetaInfo, String stackId, String serviceName) {	List<String> stackInfo = getStackInfo(stackId);	if (stackInfo.size() > 1) {	try {	ServiceInfo service = ambariMetaInfo.getService(stackInfo.get(0), stackInfo.get(1), serviceName);	
adding s to the list of services for loading external jars 

private void addServiceInfo(Map<String, ServiceInfo> services, AmbariMetaInfo ambariMetaInfo, String stackId, String serviceName) {	List<String> stackInfo = getStackInfo(stackId);	if (stackInfo.size() > 1) {	try {	ServiceInfo service = ambariMetaInfo.getService(stackInfo.get(0), stackInfo.get(1), serviceName);	services.put(serviceName, service);	} catch (AmbariException e) {	
failed to obtain service info for stack s service name s 

private List<String> getStackInfo(String stackId) {	
stack id s 

private ServerAction createServerAction(String classname, Map<String, ServiceInfo> services) throws AmbariException {	Class<?> actionClass = null;	actionClass = getServerActionClass(classname);	if (actionClass == null) {	
did not find s in ambari try to load it from external directories 

private ServerAction createServerAction(String classname, Map<String, ServiceInfo> services) throws AmbariException {	Class<?> actionClass = null;	actionClass = getServerActionClass(classname);	if (actionClass == null) {	actionClass = getServiceLevelServerActionClass(classname, services);	}	if (actionClass == null) {	throw new AmbariException("Unable to load server action class: " + classname);	} else {	
ready to init server action s 

private Class<?> getServiceLevelServerActionClass(String classname, Map<String, ServiceInfo> services) {	List<URL> urls = new ArrayList<>();	for (ServiceInfo service : services.values()) {	
checking service s 

private Class<?> getServiceLevelServerActionClass(String classname, Map<String, ServiceInfo> services) {	List<URL> urls = new ArrayList<>();	for (ServiceInfo service : services.values()) {	File dir = service.getServerActionsFolder();	if ( dir != null) {	
service s external dir s 

private Class<?> getServiceLevelServerActionClass(String classname, Map<String, ServiceInfo> services) {	List<URL> urls = new ArrayList<>();	for (ServiceInfo service : services.values()) {	File dir = service.getServerActionsFolder();	if ( dir != null) {	File[] jars = dir.listFiles(new FilenameFilter() {	public boolean accept(File dir, String name) {	
checking folder s 

if ( dir != null) {	File[] jars = dir.listFiles(new FilenameFilter() {	public boolean accept(File dir, String name) {	return name.endsWith(".jar");	}	});	for (File jar : jars) {	try {	URL url = jar.toURI().toURL();	urls.add(url);	
adding server action jar to classpath 

public boolean accept(File dir, String name) {	return name.endsWith(".jar");	}	});	for (File jar : jars) {	try {	URL url = jar.toURI().toURL();	urls.add(url);	}	catch (Exception e) {	
failed to add server action jar to classpath 

});	for (File jar : jars) {	try {	URL url = jar.toURI().toURL();	urls.add(url);	}	catch (Exception e) {	}	}	} else {	
s service server actions folder returned null 

catch (Exception e) {	}	}	} else {	}	}	ClassLoader classLoader = new URLClassLoader(urls.toArray(new URL[urls.size()]), ClassUtils.getDefaultClassLoader());	Class<?> actionClass = null;	try {	actionClass = ClassUtils.resolveClassName(classname, classLoader);	
found external server action s 

}	}	} else {	}	}	ClassLoader classLoader = new URLClassLoader(urls.toArray(new URL[urls.size()]), ClassUtils.getDefaultClassLoader());	Class<?> actionClass = null;	try {	actionClass = ClassUtils.resolveClassName(classname, classLoader);	} catch(IllegalArgumentException illegalArgumentException) {	
unable to find server action s in external server action directories 

private Class<?> getServerActionClass(String classname) throws AmbariException{	Class<?> actionClass = null;	try {	actionClass = Class.forName(classname);	if (actionClass == null) {	
unable to load server action class s from ambari 

private Class<?> getServerActionClass(String classname) throws AmbariException{	Class<?> actionClass = null;	try {	actionClass = Class.forName(classname);	if (actionClass == null) {	}	} catch (ClassNotFoundException e) {	
unable to load server action class s 

}	synchronized (requestSharedDataMap) {	Set<Long> cachedRequestIds = requestSharedDataMap.keySet();	for (long cachedRequestId : cachedRequestIds) {	if (!requestsInProgress.contains(cachedRequestId)) {	requestSharedDataMap.remove(cachedRequestId);	}	}	}	} catch (Exception exception) {	
unable to clear the server side action request cache 

========================= ambari sample_3375 =========================

private void setupSecurity() {	String jaasFile = solrDaoBase.getSolrKerberosConfig().getJaasFile();	boolean securityEnabled = solrDaoBase.getSolrKerberosConfig().isEnabled();	if (securityEnabled) {	System.setProperty("java.security.auth.login.config", jaasFile);	HttpClientUtil.addConfigurer(new Krb5HttpClientConfigurer());	
setupsecurity called for kerberos configuration jaas file 

private void openZkConnectionAndUpdateStatus(final SolrCollectionState state, final SolrPropsConfig solrPropsConfig) throws Exception {	ZooKeeper zkClient = null;	try {	
checking that znode is ready or not 

private void openZkConnectionAndUpdateStatus(final SolrCollectionState state, final SolrPropsConfig solrPropsConfig) throws Exception {	ZooKeeper zkClient = null;	try {	zkClient = openZookeeperConnection(solrPropsConfig);	if (!state.isZnodeReady()) {	
state change zookeeper znode is available for 

private void openZkConnectionAndUpdateStatus(final SolrCollectionState state, final SolrPropsConfig solrPropsConfig) throws Exception {	ZooKeeper zkClient = null;	try {	zkClient = openZookeeperConnection(solrPropsConfig);	if (!state.isZnodeReady()) {	state.setZnodeReady(true);	}	} catch (Exception e) {	
error occurred during the creation of zk client connection string 

state.setZnodeReady(true);	}	} catch (Exception e) {	throw e;	} finally {	try {	if (zkClient != null) {	zkClient.close();	}	} catch (Exception e) {	
could not close zk connection properly 

while (true) {	pingCount++;	try {	List<String> collectionList = new ListCollectionHandler().handle(cloudSolrClient, null);	if (collectionList != null) {	LOG.info("checkSolrStatus(): Solr getCollections() is success. collectionList=" + collectionList);	status = true;	break;	}	} catch (Exception ex) {	
error while doing solr check 

try {	List<String> collectionList = new ListCollectionHandler().handle(cloudSolrClient, null);	if (collectionList != null) {	LOG.info("checkSolrStatus(): Solr getCollections() is success. collectionList=" + collectionList);	status = true;	break;	}	} catch (Exception ex) {	}	if (System.currentTimeMillis() - beginTimeMS > waitDurationMS) {	
solr is not reachable even after ms if you are using alias then you might have to restart logsearch after solr is up and running 

} catch (Exception ex) {	}	if (System.currentTimeMillis() - beginTimeMS > waitDurationMS) {	break;	} else {	LOG.warn("Solr is not not reachable yet. getCollections() attempt count=" + pingCount + ". " + "Will sleep for " + waitIntervalMS + " ms and try again.");	}	Thread.sleep(waitIntervalMS);	}	} catch (Throwable t) {	
seems solr is not up 

========================= ambari sample_1431 =========================

public ConnectionDelegate get(ViewContext context) {	
creating connection delegate instance for viewname instance name 

========================= ambari sample_628 =========================

public void updateRepositoryState(String reportedVersion) throws AmbariException {	ServiceComponentDesiredStateEntity component = serviceComponentDesiredStateDAO.findById( desiredStateEntityId);	List<ServiceComponentVersionEntity> componentVersions = serviceComponentDesiredStateDAO.findVersions( getClusterId(), getServiceName(), getName());	Map<String, ServiceComponentVersionEntity> map = new HashMap<>(Maps.uniqueIndex(componentVersions, new Function<ServiceComponentVersionEntity, String>() {	public String apply(ServiceComponentVersionEntity input) {	return input.getRepositoryVersion().getVersion();	}	}));	if (LOG.isDebugEnabled()) {	
existing versions for 

if (null != repoVersion) {	componentVersion = new ServiceComponentVersionEntity();	componentVersion.setRepositoryVersion(repoVersion);	componentVersion.setState(RepositoryVersionState.INSTALLED);	componentVersion.setUserName("auto-reported");	component.setRepositoryState(RepositoryVersionState.CURRENT);	component.addVersion(componentVersion);	component = serviceComponentDesiredStateDAO.merge(component);	map.put(reportedVersion, componentVersion);	} else {	
there is no repository available for stack version 

========================= ambari sample_3267 =========================

protected void close() {	if (ps != null) try {	ps.close();	} catch (SQLException e) {	
exception while closing statment 

protected void close() {	if (ps != null) try {	ps.close();	} catch (SQLException e) {	}	if (db != null) try {	db.close();	} catch (SQLException e) {	
exception while closing connection 

setLong(resource, WORKFLOW_ELAPSED_TIME_PROPERTY_ID, rs, requestedIds);	setLong(resource, WORKFLOW_INPUT_BYTES_PROPERTY_ID, rs, requestedIds);	setLong(resource, WORKFLOW_OUTPUT_BYTES_PROPERTY_ID, rs, requestedIds);	setInt(resource, WORKFLOW_NUM_JOBS_TOTAL_PROPERTY_ID, rs, requestedIds);	setInt(resource, WORKFLOW_NUM_JOBS_COMPLETED_PROPERTY_ID, rs, requestedIds);	setString(resource, WORKFLOW_PARENT_ID_PROPERTY_ID, rs, requestedIds);	setString(resource, WORKFLOW_CONTEXT_PROPERTY_ID, rs, requestedIds);	workflows.add(resource);	}	} catch (SQLException e) {	
caught exception getting resource 

setString(resource, WORKFLOW_PARENT_ID_PROPERTY_ID, rs, requestedIds);	setString(resource, WORKFLOW_CONTEXT_PROPERTY_ID, rs, requestedIds);	workflows.add(resource);	}	} catch (SQLException e) {	return Collections.emptySet();	} finally {	if (rs != null) try {	rs.close();	} catch (SQLException e) {	
exception while closing resultset 

========================= ambari sample_3637 =========================

protected String processMetrics(Map<String, TimelineMetrics> metricValues) {	TimelineMetrics timelineMetrics = new TimelineMetrics();	for (TimelineMetrics metrics : metricValues.values()) {	for (TimelineMetric timelineMetric : metrics.getMetrics()) timelineMetrics.addOrMergeTimelineMetric(timelineMetric);	}	String json = null;	try {	json = mapper.writeValueAsString(timelineMetrics);	LOG.debug(json);	} catch (Exception e) {	
failed to convert result into json 

========================= ambari sample_281 =========================

public void before() throws Exception {	File stackRoot = new File("src/main/resources/stacks");	File commonServicesRoot = new File("src/main/resources/common-services");	
stacks file 

public void before() throws Exception {	File stackRoot = new File("src/main/resources/stacks");	File commonServicesRoot = new File("src/main/resources/common-services");	
common services file 

properties.setProperty(Configuration.METADATA_DIR_PATH.getKey(), stackRoot.getPath());	properties.setProperty(Configuration.COMMON_SERVICES_DIR_PATH.getKey(), commonServicesRoot.getPath());	properties.setProperty(Configuration.SERVER_VERSION_FILE.getKey(), versionFile.getPath());	Configuration configuration = new Configuration(properties);	TestAmbariMetaInfo metaInfo = new TestAmbariMetaInfo(configuration);	if (replayMocks) {	metaInfo.replayAllMocks();	try {	metaInfo.init();	} catch(Exception e) {	
error in initializing 

========================= ambari sample_2460 =========================

private void setupStatusDir() {	String newDirPrefix = makeStatusDirectoryPrefix();	String newDir = null;	try {	newDir = HdfsUtil.findUnallocatedFileName(hdfsApi, newDirPrefix, "");	} catch (HdfsApiException e) {	throw new ServiceFormattedException(e);	}	job.setStatusDir(newDir);	
status dir for job 

throw new BadRequestFormattedException("queryId or forcedContent should be passed!", null);	}	} catch (IOException e) {	throw new ServiceFormattedException("F040 Error when creating file " + jobQueryFilePath, e);	} catch (InterruptedException e) {	throw new ServiceFormattedException("F040 Error when creating file " + jobQueryFilePath, e);	} catch (HdfsApiException e) {	throw new ServiceFormattedException(e);	}	job.setQueryFile(jobQueryFilePath);	
query file for job 

========================= ambari sample_844 =========================

public Response getUDFList(@Context UriInfo ui) {	try {	
getting all udfs 

========================= ambari sample_1020 =========================

Set<String> candidateHosts = new HashSet<>(resourceFilter.getHostNames());	Set<String> ignoredHosts = maintenanceStateHelper.filterHostsInMaintenanceState( candidateHosts, new MaintenanceStateHelper.HostPredicate() {	public boolean shouldHostBeRemoved(final String hostname) throws AmbariException {	return !maintenanceStateHelper.isOperationAllowed( cluster, actionExecutionContext.getOperationLevel(), resourceFilter, serviceName, componentName, hostname);	}	}	);	Set<String> unhealthyHosts = getUnhealthyHosts(candidateHosts, actionExecutionContext, resourceFilter);	if (!ignoredHosts.isEmpty()) {	if( LOG.isDebugEnabled() ){	
while building the custom command for the following hosts were excluded unhealthy maintenance 

return !maintenanceStateHelper.isOperationAllowed( cluster, actionExecutionContext.getOperationLevel(), resourceFilter, serviceName, componentName, hostname);	}	}	);	Set<String> unhealthyHosts = getUnhealthyHosts(candidateHosts, actionExecutionContext, resourceFilter);	if (!ignoredHosts.isEmpty()) {	if( LOG.isDebugEnabled() ){	}	} else if (!unhealthyHosts.isEmpty()) {	if (LOG.isDebugEnabled()) {	
while building the custom command for the following hosts were excluded unhealthy maintenance 

}	boolean isUpgradeSuspended = cluster.isUpgradeSuspended();	if (isUpgradeSuspended) {	cluster.addSuspendedUpgradeParameters(commandParams, roleParams);	}	StageUtils.useAmbariJdkInCommandParams(commandParams, configs);	roleParams.put(COMPONENT_CATEGORY, componentInfo.getCategory());	if (commandName.equals("RECONFIGURE")) {	String refreshConfigsCommand = configHelper.getRefreshConfigsCommand(cluster, hostName, serviceName, componentName);	if (refreshConfigsCommand != null && !refreshConfigsCommand.equals(RefreshCommandConfiguration.REFRESH_CONFIGS)) {	
refreshing configs for with command 

execCmd.setConfigurationAttributes(configurationAttributes);	execCmd.setConfigurationTags(configTags);	execCmd.setClusterHostInfo( StageUtils.getClusterHostInfo(cluster));	for (ServiceComponentHost sch : cluster.getServiceComponentHosts(hostname)) {	execCmd.getLocalComponents().add(sch.getServiceComponentName());	}	Map<String, String> commandParams = new TreeMap<>();	Map<String, ServiceInfo> serviceInfos = ambariMetaInfo.getServices(stackId.getStackName(), stackId.getStackVersion());	for (ServiceInfo serviceInfoInstance : serviceInfos.values()) {	if (serviceInfoInstance.getServiceType() != null) {	
adding to command parameters for 

boolean alignMtnState = "true".equals(alignMtnStateStr);	List<String> listOfExcludedHosts = new ArrayList<>();	for (ServiceComponentHost sch : svcComponents.get(slaveCompType).getServiceComponentHosts().values()) {	if (filteredExcludedHosts.contains(sch.getHostName())) {	sch.setComponentAdminState(HostComponentAdminState.DECOMMISSIONED);	listOfExcludedHosts.add(sch.getHostName());	if (alignMtnState) {	sch.setMaintenanceState(MaintenanceState.ON);	LOG.info("marking Maintenance=ON on " + sch.getHostName());	}	
decommissioning on 

sch.setMaintenanceState(MaintenanceState.ON);	LOG.info("marking Maintenance=ON on " + sch.getHostName());	}	}	if (filteredIncludedHosts.contains(sch.getHostName())) {	sch.setComponentAdminState(HostComponentAdminState.INSERVICE);	if (alignMtnState) {	sch.setMaintenanceState(MaintenanceState.OFF);	LOG.info("marking Maintenance=OFF on " + sch.getHostName());	}	
recommissioning on 

public void validateAction(ExecuteActionRequest actionRequest) throws AmbariException {	List<RequestResourceFilter> resourceFilters = actionRequest.getResourceFilters();	if (resourceFilters != null && resourceFilters.isEmpty() && actionRequest.getParameters().containsKey(HAS_RESOURCE_FILTERS) && actionRequest.getParameters().get(HAS_RESOURCE_FILTERS).equalsIgnoreCase("true")) {	
couldn t find any resource that satisfies given resource filters 

hostParamsStage.put(CLIENTS_TO_UPDATE_CONFIGS, clientsToUpdateConfigs);	}	clusterHostInfoJson = StageUtils.getGson().toJson(clusterHostInfo);	if (null == stackId && null != cluster) {	stackId = cluster.getDesiredStackVersion();	}	if (null != stackId) {	Map<String, ServiceInfo> serviceInfos = ambariMetaInfo.getServices(stackId.getStackName(), stackId.getStackVersion());	for (ServiceInfo serviceInfoInstance : serviceInfos.values()) {	if (serviceInfoInstance.getServiceType() != null) {	
adding to command parameters for 

private ServiceComponent getServiceComponent ( ActionExecutionContext actionExecutionContext, RequestResourceFilter resourceFilter){	try {	Cluster cluster = clusters.getCluster(actionExecutionContext.getClusterName());	Service service = cluster.getService(resourceFilter.getServiceName());	return service.getServiceComponent(resourceFilter.getComponentName());	} catch (Exception e) {	
unknown error appears during getting service component 

========================= ambari sample_3733 =========================

if (null != svc) {	StackId stackId = svc.getDesiredStackId();	try {	ServiceInfo si = m_metaInfo.getService(stackId.getStackName(), stackId.getStackVersion(), service);	CommandScriptDefinition script = si.getCommandScript();	if (null != script && null != script.getScript() && !script.getScript().isEmpty()) {	ctx.setServiceDisplay(service, si.getDisplayName());	return true;	}	} catch (AmbariException e) {	
could not determine if service can run a service check exception 

========================= ambari sample_3226 =========================

int page = 1;	while (saved < json.length()) {	int end = Math.min(saved + VALUE_LENGTH_LIMIT, json.length());	String substring = json.substring(saved, end);	getConfig().setProperty(modelPropName + "#" + page, substring);	saved += VALUE_LENGTH_LIMIT;	page += 1;	LOG.debug("Chunk saved: " + modelPropName + "#" + page + "=" + substring);	}	getConfig().setProperty(modelPropName, page - 1);	
write finished pages 

protected String read(String modelPropName) {	StringBuilder result = new StringBuilder();	int pages = getConfig().getInt(modelPropName);	
read started pages 

protected void clear(String modelPropName) {	int pages = getConfig().getInt(modelPropName);	
clean started pages 

protected void clear(String modelPropName) {	int pages = getConfig().getInt(modelPropName);	for(int page = 1; page <= pages; page++) {	getConfig().clearProperty(modelPropName + "#" + page);	
chunk clean 

========================= ambari sample_686 =========================

private void setMaxFilesPerPage(ViewContext context) {	String maxFilesPerPageProperty = context.getAmbariProperty(FILES_VIEW_MAX_FILE_PER_PAGE);	LOG.info("maxFilesPerPageProperty = {}", maxFilesPerPageProperty);	if(!Strings.isNullOrEmpty(maxFilesPerPageProperty)){	try {	maxFilesPerPage = Integer.parseInt(maxFilesPerPageProperty);	}catch(Exception e){	
should be integer but it is using default value of 

String finalDestination = getDestination(destination, fileName);	try {	if (api.rename(src, finalDestination)) {	index ++;	} else {	message = "Failed to move '" + src + "' to '" + finalDestination + "'";	break;	}	} catch (IOException exception) {	message = exception.getMessage();	
failed to move to exception 

}	int index = 0;	for (String src : sources) {	String fileName = getFileName(src);	String finalDestination = getDestination(destination, fileName);	try {	api.copy(src, finalDestination);	index ++;	} catch (IOException|HdfsApiException exception) {	message = exception.getMessage();	
failed to copy to exception 

String trashFilePath = api.getTrashDirPath(entry.path);	try {	if (api.rename(entry.path, trashFilePath)) {	index ++;	} else {	message = "Failed to move '" + entry.path + "' to '" + trashFilePath + "'";	break;	}	} catch (IOException exception) {	message = exception.getMessage();	
failed to move to exception 

for (MultiRemoveRequest.PathEntry entry : request.paths) {	try {	if (api.delete(entry.path, entry.recursive)) {	index++;	} else {	message = "Failed to remove '" + entry.path + "'";	break;	}	} catch (IOException exception) {	message = exception.getMessage();	
failed to remove exception 

========================= ambari sample_1098 =========================

String str = object.toString();	if (Strings.isNotEmpty(str)) {	str = str.trim();	if (str.matches(HIVE_DATE_FORMAT_REGEX)) {	try {	SimpleDateFormat sdf = new SimpleDateFormat(HIVE_DATE_FORMAT);	sdf.setLenient(false);	Date date = sdf.parse(str);	return true;	} catch (Exception e) {	
error while parsing as date string format 

public static boolean isTimeStamp(Object object) {	if (object == null) return false;	if (object instanceof Date) return true;	String str = object.toString();	try {	Timestamp ts = Timestamp.valueOf(str);	return true;	} catch (Exception e) {	
error while parsing as timestamp string 

public static boolean checkDatatype( Object object, DataTypes datatype){	switch(datatype){	case BOOLEAN : return isBoolean(object);	case INT : return isInteger(object);	case BIGINT : return isLong(object);	case DOUBLE: return isDouble(object);	case CHAR: return isChar(object);	case DATE: return isDate(object);	case TIMESTAMP: return isTimeStamp(object);	case STRING: return isString(object);	
this datatype detection is not supported 

========================= ambari sample_540 =========================

public SessionManager() {	
sessionmanager created 

========================= ambari sample_1427 =========================

public InputStream readFromUrl(String urlToRead, String method, String body, Map<String, String> newHeaders) {	URLStreamProvider streamProvider = viewContext.getURLStreamProvider();	InputStream stream = null;	try {	if (isSecurityEnabled()) {	stream = streamProvider.readAsCurrent(urlToRead, method, body, newHeaders);	} else {	stream = streamProvider.readFrom(urlToRead, method, body, newHeaders);	}	} catch (IOException e) {	
error talking to oozie 

========================= ambari sample_1077 =========================

public AlertsDAO(Configuration configuration) {	m_configuration = configuration;	if( m_configuration.isAlertCacheEnabled() ){	int maximumSize = m_configuration.getAlertCacheSize();	LOG.info("Alert caching is enabled (size={}, flushInterval={}m)", maximumSize, m_configuration.getAlertCacheFlushInterval());	m_currentAlertCache = CacheBuilder.newBuilder().maximumSize( maximumSize).build(new CacheLoader<AlertCacheKey, AlertCurrentEntity>() {	public AlertCurrentEntity load(AlertCacheKey key) throws Exception {	
cache miss for alert key fetching from jpa 

final AlertCurrentEntity alertCurrentEntity;	long clusterId = key.getClusterId();	String alertDefinitionName = key.getAlertDefinitionName();	String hostName = key.getHostName();	if (StringUtils.isEmpty(hostName)) {	alertCurrentEntity = findCurrentByNameNoHostInternalInJPA(clusterId, alertDefinitionName);	} else {	alertCurrentEntity = findCurrentByHostAndNameInJPA(clusterId, hostName, alertDefinitionName);	}	if (null == alertCurrentEntity) {	
cache lookup failed for because the alert does not yet exist 

public AlertCurrentEntity findCurrentByHostAndName(long clusterId, String hostName, String alertName) {	if( m_configuration.isAlertCacheEnabled() ){	AlertCacheKey key = new AlertCacheKey(clusterId, alertName, hostName);	try {	return m_currentAlertCache.get(key);	} catch (ExecutionException executionException) {	Throwable cause = executionException.getCause();	if (!(cause instanceof AlertNotYetCreatedException)) {	
unable to retrieve alert for key from the cache 

}	if (m_configuration.isAlertCacheEnabled()) {	m_currentAlertCache.invalidateAll();	}	try {	Map<String, Cluster> clusters = m_clusters.get().getClusters();	for (Map.Entry<String, Cluster> entry : clusters.entrySet()) {	m_alertEventPublisher.publish(new AggregateAlertRecalculateEvent( entry.getValue().getClusterId()));	}	} catch (Exception ambariException) {	
unable to recalcuate aggregate alerts after removing host 

public AlertCurrentEntity merge(AlertCurrentEntity alert, boolean updateCacheOnly) {	if (updateCacheOnly) {	AlertCacheKey key = AlertCacheKey.build(alert);	if (!m_configuration.isAlertCacheEnabled()) {	
unable to update a cached alert instance for because cached alerts are not enabled 

public AlertCurrentEntity findCurrentByNameNoHost(long clusterId, String alertName) {	if( m_configuration.isAlertCacheEnabled() ){	AlertCacheKey key = new AlertCacheKey(clusterId, alertName);	try {	return m_currentAlertCache.get(key);	} catch (ExecutionException executionException) {	Throwable cause = executionException.getCause();	if (!(cause instanceof AlertNotYetCreatedException)) {	
unable to retrieve alert for key from the cache 

public void flushCachedEntitiesToJPA() {	if (!m_configuration.isAlertCacheEnabled()) {	
unable to flush cached alerts to jpa because caching is not enabled 

if (!m_configuration.isAlertCacheEnabled()) {	return;	}	long cachedEntityCount = m_currentAlertCache.size();	ConcurrentMap<AlertCacheKey, AlertCurrentEntity> map = m_currentAlertCache.asMap();	Set<Entry<AlertCacheKey, AlertCurrentEntity>> entries = map.entrySet();	for (Entry<AlertCacheKey, AlertCurrentEntity> entry : entries) {	merge(entry.getValue());	}	m_currentAlertCache.invalidateAll();	
flushed cached alerts to the database 

public long cleanup(TimeBasedCleanupPolicy policy) {	long affectedRows = 0;	Long clusterId = null;	try {	clusterId = m_clusters.get().getCluster(policy.getClusterName()).getClusterId();	affectedRows += cleanAlertNoticesForClusterBeforeDate(clusterId, policy.getToDateInMillis());	affectedRows += cleanAlertCurrentsForClusterBeforeDate(clusterId, policy.getToDateInMillis());	affectedRows += cleanAlertHistoriesForClusterBeforeDate(clusterId, policy.getToDateInMillis());	} catch (AmbariException e) {	
error while looking up cluster with name 

private int executeQuery(String namedQuery, Class entityType, long clusterId, long timestamp) {	
starting delete update entries older than for entity 

private int executeQuery(String namedQuery, Class entityType, long clusterId, long timestamp) {	TypedQuery query = m_entityManagerProvider.get().createNamedQuery(namedQuery, entityType);	query.setParameter("clusterId", clusterId);	query.setParameter("beforeDate", timestamp);	int affectedRows = query.executeUpdate();	m_entityManagerProvider.get().flush();	m_entityManagerProvider.get().clear();	
completed delete update entries older than for entity number of entities deleted 

========================= ambari sample_4275 =========================

public XMLIterator(XMLEventReader reader) throws IOException {	this.reader = reader;	try {	nextObject = readNextObject(this.reader);	} catch (EndOfDocumentException e) {	
error 

public LinkedHashMap<String, String> next() {	LinkedHashMap<String, String> currObject = nextObject;	try {	nextObject = readNextObject(this.reader);	} catch (IOException e) {	
exception occured while reading the next row from xml 

public LinkedHashMap<String, String> next() {	LinkedHashMap<String, String> currObject = nextObject;	try {	nextObject = readNextObject(this.reader);	} catch (IOException e) {	nextObject = null;	} catch (EndOfDocumentException e) {	
end of xml document reached with next character ending the xml 

public LinkedHashMap<String, String> next() {	LinkedHashMap<String, String> currObject = nextObject;	try {	nextObject = readNextObject(this.reader);	} catch (IOException e) {	nextObject = null;	} catch (EndOfDocumentException e) {	nextObject = null;	} catch (XMLStreamException e) {	
exception occured while reading the next row from xml 

public void remove() {	
no operation when remove called 

private LinkedHashMap<String, String> readNextObject(XMLEventReader reader) throws IOException, EndOfDocumentException, XMLStreamException {	LinkedHashMap<String, String> row = new LinkedHashMap<>();	boolean objectStarted = false;	String currentName = null;	while (true) {	XMLEvent event = reader.nextEvent();	switch (event.getEventType()) {	case XMLStreamConstants.START_ELEMENT: StartElement startElement = event.asStartElement();	String qName = startElement.getName().getLocalPart();	
startname 

if( null == nameAttr ){	throw new IllegalArgumentException("Missing name attribute in col tag.");	}	currentName = nameAttr.getValue();	break;	default: throw new IllegalArgumentException("Illegal start tag " + qName + " encountered.");	}	break;	case XMLStreamConstants.END_ELEMENT: EndElement endElement = event.asEndElement();	String name = endElement.getName().getLocalPart();	
endname 

throw new IllegalArgumentException("Stray tag " + name);	}	currentName = null;	break;	default: throw new IllegalArgumentException("Illegal start ending " + name + " encountered.");	}	break;	case XMLStreamConstants.CHARACTERS: Characters characters = event.asCharacters();	if (characters.isWhiteSpace() && currentName == null) break;	String data = characters.getData();	
character data 

========================= ambari sample_813 =========================

I.setId(i);	instancelist.add(I);	i++;	}	return instancelist;	} finally {	if (rs1 != null) {	try {	rs1.close();	} catch (SQLException e) {	
sql exception in while closing result set 

if (rs1 != null) {	try {	rs1.close();	} catch (SQLException e) {	}	}	if (prSt != null) {	try {	prSt.close();	} catch (SQLException e) {	
sql exception in while closing preparedstatement 

if (prSt != null) {	try {	prSt.close();	} catch (SQLException e) {	}	}	if (conn != null) {	try {	conn.close();	} catch (SQLException e) {	
sql exception in while closing the connection 

I.setId(i);	instancelist.add(I);	i++;	}	return instancelist;	}  finally {	if (rs1 != null) {	try {	rs1.close();	} catch (SQLException e) {	
sql exception in while closing result set 

if (rs1 != null) {	try {	rs1.close();	} catch (SQLException e) {	}	}	if (prSt != null) {	try {	prSt.close();	} catch (SQLException e) {	
sql exception in while closing preparedstatement 

if (prSt != null) {	try {	prSt.close();	} catch (SQLException e) {	}	}	if (conn != null) {	try {	conn.close();	} catch (SQLException e) {	
sql exception in while closing the connection 

========================= ambari sample_1260 =========================

protected boolean emitMetricsJson(String connectUrl, String jsonData) {	int timeout = getTimeoutSeconds() * 1000;	HttpURLConnection connection = null;	try {	if (connectUrl == null) {	throw new IOException("Unknown URL. Unable to connect to metrics collector.");	}	connection = connectUrl.startsWith("https") ? getSSLConnection(connectUrl) : getConnection(connectUrl);	if (LOG.isDebugEnabled()) {	
emitmetricsjson to 

if (connectUrl == null) {	throw new IOException("Unknown URL. Unable to connect to metrics collector.");	}	connection = connectUrl.startsWith("https") ? getSSLConnection(connectUrl) : getConnection(connectUrl);	if (LOG.isDebugEnabled()) {	}	AppCookieManager appCookieManager = getAppCookieManager();	String appCookie = appCookieManager.getCachedAppCookie(connectUrl);	if (appCookie != null) {	if (LOG.isInfoEnabled()) {	
using cached app cookie for url 

String appCookie = appCookieManager.getCachedAppCookie(connectUrl);	if (appCookie != null) {	if (LOG.isInfoEnabled()) {	}	connection.setRequestProperty(COOKIE, appCookie);	}	int statusCode = emitMetricsJson(connection, timeout, jsonData);	if (statusCode == HttpStatus.SC_UNAUTHORIZED ) {	String wwwAuthHeader = connection.getHeaderField(WWW_AUTHENTICATE);	if (LOG.isInfoEnabled()) {	
received www authentication header for url 

}	if (wwwAuthHeader != null && wwwAuthHeader.trim().startsWith(NEGOTIATE)) {	appCookie = appCookieManager.getAppCookie(connectUrl, true);	if (appCookie != null) {	cleanupInputStream(connection.getInputStream());	connection = connectUrl.startsWith("https") ? getSSLConnection(connectUrl) : getConnection(connectUrl);	connection.setRequestProperty(COOKIE, appCookie);	statusCode = emitMetricsJson(connection, timeout, jsonData);	}	} else {	
unsupported www authentication header for url 

connection.setRequestProperty(COOKIE, appCookie);	statusCode = emitMetricsJson(connection, timeout, jsonData);	}	} else {	}	}	if (statusCode != 200) {	LOG.info("Unable to POST metrics to collector, " + connectUrl + ", " + "statusCode = " + statusCode);	} else {	if (LOG.isDebugEnabled()) {	
metrics posted to collector 

if (failedCollectorConnectionsCounter.getAndIncrement() == 0) {	if (LOG.isDebugEnabled()) {	LOG.debug(errorMessage, ioe);	} else {	LOG.info(errorMessage);	}	throw new UnableToConnectException(ioe).setConnectUrl(connectUrl);	} else {	failedCollectorConnectionsCounter.compareAndSet(NUMBER_OF_SKIPPED_COLLECTOR_EXCEPTIONS, 0);	if (LOG.isDebugEnabled()) {	
ignoring s ams connection exceptions 

protected String getCurrentCollectorHost() {	String collectorHost;	if (targetCollectorHostSupplier != null) {	collectorHost = targetCollectorHostSupplier.get();	if (failedCollectorConnectionsCounter.get() > RETRY_COUNT_BEFORE_COLLECTOR_FAILOVER) {	
removing collector from allknownlivecollectors 

if (failedCollectorConnectionsCounter.get() > RETRY_COUNT_BEFORE_COLLECTOR_FAILOVER) {	allKnownLiveCollectors.remove(collectorHost);	targetCollectorHostSupplier = null;	collectorHost = findPreferredCollectHost();	}	} else {	collectorHost = findPreferredCollectHost();	}	if (collectorHost == null) {	if (nullCollectorCounter.getAndIncrement() == 0) {	
no live collector to send metrics to metrics to be sent will be discarded this message will be skipped for the next times 

validCollectorHost = false;	}	connectUrl = getCollectorUri(collectorHost);	}	if (validCollectorHost) {	String jsonData = null;	LOG.debug("EmitMetrics connectUrl = "  + connectUrl);	try {	jsonData = mapper.writeValueAsString(metrics);	} catch (IOException e) {	
unable to parse metrics 

try {	in = new FileInputStream(new File(trustStorePath));	KeyStore store = KeyStore.getInstance(trustStoreType == null ? KeyStore.getDefaultType() : trustStoreType);	store.load(in, trustStorePassword.toCharArray());	TrustManagerFactory tmf = TrustManagerFactory .getInstance(TrustManagerFactory.getDefaultAlgorithm());	tmf.init(store);	SSLContext context = SSLContext.getInstance("TLS");	context.init(null, tmf.getTrustManagers(), null);	sslSocketFactory = context.getSocketFactory();	} catch (Exception e) {	
unable to load truststore 

tmf.init(store);	SSLContext context = SSLContext.getInstance("TLS");	context.init(null, tmf.getTrustManagers(), null);	sslSocketFactory = context.getSocketFactory();	} catch (Exception e) {	} finally {	if (in != null) {	try {	in.close();	} catch (IOException e) {	
unable to load truststore 

if (targetCollectorHostSupplier != null) {	String targetCollector = targetCollectorHostSupplier.get();	if (targetCollector != null) {	return targetCollector;	}	}	Collection<String> collectorHosts = getConfiguredCollectorHosts();	refreshCollectorsFromConfigured(collectorHosts);	long currentTime = System.currentTimeMillis();	if (allKnownLiveCollectors.size() == 0 && getZookeeperQuorum() != null && (currentTime - lastFailedZkRequestTime) > zookeeperBackoffTimeMillis) {	
no live collectors from configuration requesting zookeeper 

if (allKnownLiveCollectors.size() == 0 && getZookeeperQuorum() != null && (currentTime - lastFailedZkRequestTime) > zookeeperBackoffTimeMillis) {	allKnownLiveCollectors.addAll(collectorHAHelper.findLiveCollectorHostsFromZNode());	boolean noNewCollectorFromZk = true;	for (String collectorHostFromZk : allKnownLiveCollectors) {	if (!collectorHosts.contains(collectorHostFromZk)) {	noNewCollectorFromZk = false;	break;	}	}	if (noNewCollectorFromZk) {	
no new collector was found from zookeeper will not request zookeeper for millis 

if (shardExpired) {	refreshCollectorsFromConfigured(getConfiguredCollectorHosts());	}	return metricSinkWriteShardStrategy.findCollectorShard(new ArrayList<>(allKnownLiveCollectors));	}	}, rand.nextInt(COLLECTOR_HOST_CACHE_MAX_EXPIRATION_MINUTES - COLLECTOR_HOST_CACHE_MIN_EXPIRATION_MINUTES + 1) + COLLECTOR_HOST_CACHE_MIN_EXPIRATION_MINUTES, TimeUnit.MINUTES );	String collectorHost = targetCollectorHostSupplier.get();	shardExpired = true;	return collectorHost;	}	
couldn t find any live collectors returning null 

private void refreshCollectorsFromConfigured(Collection<String> collectorHosts) {	
trying to find live collector host from 

for (String hostStr : collectorHosts) {	hostStr = hostStr.trim();	if (!hostStr.isEmpty()) {	try {	Collection<String> liveHosts = findLiveCollectorHostsFromKnownCollector(hostStr, getCollectorPort());	for (String host : liveHosts) {	allKnownLiveCollectors.add(host);	}	break;	} catch (MetricCollectorUnavailableException e) {	
collector is not longer live removing it from list of know live collector hosts 

public Collection<String> parseHostsStringIntoCollection(String hostsString) {	Set<String> hosts = new HashSet<>();	if (StringUtils.isEmpty(hostsString)) {	
no metric collector configured 

========================= ambari sample_242 =========================

public void migrateEntity(Class originEntityClass, Class currentEntityClass) throws ViewDataMigrationException {	if (isHive15()) {	currentEntityClass = hive1EntitiesMapping.get(originEntityClass.getCanonicalName());	if (currentEntityClass == null) {	
mapping was not found for class 

public void migrateEntity(Class originEntityClass, Class currentEntityClass) throws ViewDataMigrationException {	if (isHive15()) {	currentEntityClass = hive1EntitiesMapping.get(originEntityClass.getCanonicalName());	if (currentEntityClass == null) {	return;	}	migrationContext.copyAllObjects(originEntityClass, currentEntityClass);	} else {	
unknown migration policy for class 

========================= ambari sample_612 =========================

}	try {	KerberosPrincipalEntity principalEntity = kerberosPrincipalDAO.find(resolvedPrincipal.getPrincipal());	if (principalEntity != null) {	String cachedKeytabPath = principalEntity.getCachedKeytabPath();	KerberosKeytabEntity kke = kerberosKeytabDAO.find(resolvedPrincipal.getResolvedKerberosKeytab().getFile());	kerberosKeytabDAO.remove(kke);	kerberosPrincipalDAO.remove(principalEntity);	if (cachedKeytabPath != null) {	if (!new File(cachedKeytabPath).delete()) {	
failed to remove cached keytab for 

}	String hostName = resolvedPrincipal.getHostName();	if (hostName != null && hostName.equalsIgnoreCase(KerberosHelper.AMBARI_SERVER_HOST_NAME)) {	ResolvedKerberosKeytab resolvedKeytab = resolvedPrincipal.getResolvedKerberosKeytab();	if (resolvedKeytab != null) {	String keytabFilePath = resolvedKeytab.getFile();	if (keytabFilePath != null) {	try {	ShellCommandUtil.Result result = ShellCommandUtil.delete(keytabFilePath, true, true);	if (!result.isSuccessful()) {	
failed to remove ambari keytab for due to 

if (hostName != null && hostName.equalsIgnoreCase(KerberosHelper.AMBARI_SERVER_HOST_NAME)) {	ResolvedKerberosKeytab resolvedKeytab = resolvedPrincipal.getResolvedKerberosKeytab();	if (resolvedKeytab != null) {	String keytabFilePath = resolvedKeytab.getFile();	if (keytabFilePath != null) {	try {	ShellCommandUtil.Result result = ShellCommandUtil.delete(keytabFilePath, true, true);	if (!result.isSuccessful()) {	}	} catch (IOException|InterruptedException e) {	
failed to remove ambari keytab for 

========================= ambari sample_3318 =========================

public Double getDAGProgress(String appId, String dagId) {	String dagIdx = parseDagIdIndex(dagId);	JSONObject progresses = delegate.dagProgress(appId, dagIdx);	double dagProgressValue;	if (progresses != null) {	JSONObject dagProgress = (JSONObject) progresses.get("dagProgress");	dagProgressValue = (Double) (dagProgress.get("progress"));	} else {	
error while retrieving progress of assumed 

String vertexIdx = parts[parts.length - 1];	builder.append(vertexIdx).append(",");	vertexIdToEntityMapping.put(vertexId.entity, vertexId.vertexName);	}	builder.setLength(builder.length() - 1);	}	String commaSeparatedVertices = builder.toString();	List<VertexProgress> parsedVertexProgresses = new LinkedList<VertexProgress>();	JSONObject vertexProgressesResponse = delegate.verticesProgress( appId, dagIdx, commaSeparatedVertices);	if (vertexProgressesResponse == null) {	
error while retrieving progress of vertices assumed for all vertices 

========================= ambari sample_868 =========================

public void createJobs() {	if (propertyMap == null || propertyMap.getSolrDataArchiving() == null) return;	propertyMap.getSolrDataArchiving().values().forEach(DocumentArchivingProperties::validate);	propertyMap.getSolrDataArchiving().keySet().forEach(jobName -> {	
registering data archiving job 

break;	case LOCAL: baseDir = new File(properties.getLocalDestinationDirectory());	break;	}	FileNameSuffixFormatter fileNameSuffixFormatter = FileNameSuffixFormatter.from(properties);	LocalItemWriterListener itemWriterListener = new LocalItemWriterListener(fileAction, documentWiper);	File destinationDirectory = new File( baseDir, String.format("%s_%s_%s", properties.getSolr().getCollection(), jobId, isBlank(intervalEnd) ? "" : fileNameSuffixFormatter.format(intervalEnd)));	LOG.info("Destination directory path={}", destinationDirectory);	if (!destinationDirectory.exists()) {	if (!destinationDirectory.mkdirs()) {	
unable to create directory 

private File outFile(String collection, File directoryPath, String suffix) {	File file = new File(directoryPath, String.format("%s_-_%s.json", collection, suffix));	
exporting to temp file 

========================= ambari sample_184 =========================

private void parseMetaInfoFile() throws AmbariException {	File extensionMetaInfoFile = new File(getAbsolutePath() + File.separator + EXTENSION_METAINFO_FILE_NAME);	if (extensionMetaInfoFile.exists()) {	if (LOG.isDebugEnabled()) {	
reading extension version metainfo from file 

if (subDirs.contains(ServiceDirectory.SERVICES_FOLDER_NAME)) {	String servicesDir = getAbsolutePath() + File.separator + ServiceDirectory.SERVICES_FOLDER_NAME;	File baseServiceDir = new File(servicesDir);	File[] serviceFolders = baseServiceDir.listFiles(StackDirectory.FILENAME_FILTER);	if (serviceFolders != null) {	for (File d : serviceFolders) {	if (d.isDirectory()) {	try {	dirs.add(new StackServiceDirectory(d.getAbsolutePath()));	} catch (AmbariException e) {	
unable to parse extension definition service at s ignoring service s 

if (d.isDirectory()) {	try {	dirs.add(new StackServiceDirectory(d.getAbsolutePath()));	} catch (AmbariException e) {	}	}	}	}	}	if (dirs.isEmpty()) {	
the extension defined at contains no services 

========================= ambari sample_4465 =========================

public void onAmbariEvent(HostComponentVersionAdvertisedEvent event) {	
received event 

}	}	}	}	try {	StackId desiredStackId = sch.getDesiredStackId();	AmbariMetaInfo ambariMetaInfo = ambariMetaInfoProvider.get();	ComponentInfo componentInfo = ambariMetaInfo.getComponent(desiredStackId.getStackName(), desiredStackId.getStackVersion(), sch.getServiceName(), sch.getServiceComponentName());	if (!componentInfo.isVersionAdvertised()) {	if (!StringUtils.equalsIgnoreCase(UNKNOWN_VERSION, newVersion)) {	
servicecomponent doesn t advertise version however servicehostcomponent on host advertised version as skipping version update 

}	return;	}	ServiceComponent sc = cluster.getService(sch.getServiceName()).getServiceComponent( sch.getServiceComponentName());	if (StringUtils.equalsIgnoreCase(UNKNOWN_VERSION, newVersion)) {	processUnknownDesiredVersion(cluster, sc, sch, newVersion);	return;	}	processComponentAdvertisedVersion(cluster, sc, sch, newVersion);	} catch (Exception e) {	
unable to propagate version for servicehostcomponent on component host error 

========================= ambari sample_4520 =========================

public Optional<HdfsApi> get(ViewContext context) {	try {	if(!hdfsApiMap.containsKey(getKey(context))) {	synchronized (lock) {	if(!hdfsApiMap.containsKey(getKey(context))) {	
creating hdfsapi instance for viewname instance name 

}else{	api = HdfsUtil.connectToHDFSApi(context);	}	hdfsApiMap.put(getKey(context), api);	return Optional.of(api);	}	}	}	return Optional.of(hdfsApiMap.get(getKey(context)));	} catch (HdfsApiException e) {	
cannot get the hdfs api 

========================= ambari sample_645 =========================

private String execute(HttpRequestBase post) {	try (CloseableHttpResponse response = httpClient.execute(post)) {	String responseBodyText = IOUtils.toString(response.getEntity().getContent(), Charset.defaultCharset());	
response code body 

========================= ambari sample_215 =========================

public String call() throws Exception {	AppMetrics hostMetrics = hmg.createMetrics();	try {	String request = new Json().serialize(hostMetrics);	String response = sender.pushMetrics(request);	return response;	} catch (IOException e) {	
error while pushing metrics 

========================= ambari sample_346 =========================

protected void close() {	if (ps != null) try {	ps.close();	} catch (SQLException e) {	
exception while closing statment 

protected void close() {	if (ps != null) try {	ps.close();	} catch (SQLException e) {	}	if (db != null) try {	db.close();	} catch (SQLException e) {	
exception while closing connection 

setLong(resource, TASK_ATTEMPT_MAP_FINISH_TIME_PROPERTY_ID, rs, requestedIds);	setLong(resource, TASK_ATTEMPT_SHUFFLE_FINISH_TIME_PROPERTY_ID, rs, requestedIds);	setLong(resource, TASK_ATTEMPT_SORT_FINISH_TIME_PROPERTY_ID, rs, requestedIds);	setLong(resource, TASK_ATTEMPT_INPUT_BYTES_PROPERTY_ID, rs, requestedIds);	setLong(resource, TASK_ATTEMPT_OUTPUT_BYTES_PROPERTY_ID, rs, requestedIds);	setString(resource, TASK_ATTEMPT_STATUS_PROPERTY_ID, rs, requestedIds);	setString(resource, TASK_ATTEMPT_LOCALITY_PROPERTY_ID, rs, requestedIds);	taskAttempts.add(resource);	}	} catch (SQLException e) {	
caught exception getting resource 

setString(resource, TASK_ATTEMPT_STATUS_PROPERTY_ID, rs, requestedIds);	setString(resource, TASK_ATTEMPT_LOCALITY_PROPERTY_ID, rs, requestedIds);	taskAttempts.add(resource);	}	} catch (SQLException e) {	return Collections.emptySet();	} finally {	if (rs != null) try {	rs.close();	} catch (SQLException e) {	
exception while closing resultset 

========================= ambari sample_3565 =========================

private String getRMUrlFromClusterConfig() {	String url;	String haEnabled = getYarnConfig(YARN_RESOURCEMANAGER_HA_ENABLED);	String httpPolicy = getYarnConfig(YARN_HTTP_POLICY);	if (!(HTTP_ONLY.equals(httpPolicy) || HTTPS_ONLY.equals(httpPolicy))) {	
unknown value s of yarn site yarn http policy http only assumed 

private String getActiveRMUrl(String[] urls) {	if (urls.length == 1) return urls[0].trim();	else {	for (String url : urls) {	url = url.trim();	if (isActiveUrl(url)) return url;	}	}	
all resourcemanagers are not accessible or none seem to be active 

private boolean isActiveUrl(String url) {	InputStream inputStream = null;	try {	inputStream = context.getURLStreamProvider() .readFrom(url + RM_INFO_API_ENDPOINT, "GET", (String) null, new HashMap<String, String>());	String response = IOUtils.toString(inputStream);	String haState = getHAStateFromRMResponse(response);	if (StringUtils.isNotEmpty(haState) && "ACTIVE".equals(haState)) return true;	} catch (IOException e) {	
resource manager s is not accessible this cannot be a active rm returning false 

public String getWebHCatURL() {	String host = null;	if (context.getCluster() != null) {	List<String> hiveServerHosts = context.getCluster().getHostsForServiceComponent("HIVE","WEBHCAT_SERVER");	if (!hiveServerHosts.isEmpty()) {	host = hiveServerHosts.get(0);	
webhcat server component was found on host 

public String getWebHCatURL() {	String host = null;	if (context.getCluster() != null) {	List<String> hiveServerHosts = context.getCluster().getHostsForServiceComponent("HIVE","WEBHCAT_SERVER");	if (!hiveServerHosts.isEmpty()) {	host = hiveServerHosts.get(0);	} else {	
no host was found with webhcat server component using hive host property to get hostname 

public String getYARNProtocol() {	String httpPolicy = getYarnConfig(YARN_HTTP_POLICY);	if (!(HTTP_ONLY.equals(httpPolicy) || HTTPS_ONLY.equals(httpPolicy))) {	
unknown value s of yarn site yarn http policy http only assumed 

private String getATSUrlFromCluster() {	String url;	String httpPolicy = getYarnConfig(YARN_HTTP_POLICY);	if (!(HTTP_ONLY.equals(httpPolicy) || HTTPS_ONLY.equals(httpPolicy))) {	
unknown value s of yarn site yarn http policy http only assumed 

========================= ambari sample_1108 =========================

try {	Matcher m = serviceNamePattern.matcher(inputConfigFile.getName());	m.find();	String serviceName = m.group(1);	String inputConfig = Files.toString(inputConfigFile, Charset.defaultCharset());	if (!config.inputConfigExists(serviceName)) {	config.createInputConfig(logFeederProps.getClusterName(), serviceName, inputConfig);	}	filesHandled.add(inputConfigFile.getAbsolutePath());	} catch (Exception e) {	
error handling file 

String inputConfig = Files.toString(inputConfigFile, Charset.defaultCharset());	if (!config.inputConfigExists(serviceName)) {	config.createInputConfig(logFeederProps.getClusterName(), serviceName, inputConfig);	}	filesHandled.add(inputConfigFile.getAbsolutePath());	} catch (Exception e) {	}	}	}	} else {	
cannot find input config files in config dir 

filesHandled.add(inputConfigFile.getAbsolutePath());	} catch (Exception e) {	}	}	}	} else {	}	try {	Thread.sleep(SLEEP_BETWEEN_CHECK);	} catch (InterruptedException e) {	
interrupted during sleep 

========================= ambari sample_1626 =========================

public void pigJobMigration(String username, String instance, String startDate, String endDate, ViewContext view, MigrationResponse migrationresult, String jobid) throws IOException, ItemNotFound {	long startTime = System.currentTimeMillis();	final Logger logger = Logger.getLogger(PigJobMigrationUtility.class);	Connection connectionHuedb = null;	Connection connectionAmbaridb = null;	logger.info("------------------------------");	
pig jobs migration started 

public void pigJobMigration(String username, String instance, String startDate, String endDate, ViewContext view, MigrationResponse migrationresult, String jobid) throws IOException, ItemNotFound {	long startTime = System.currentTimeMillis();	final Logger logger = Logger.getLogger(PigJobMigrationUtility.class);	Connection connectionHuedb = null;	Connection connectionAmbaridb = null;	logger.info("------------------------------");	logger.info("------------------------------");	
start date 

public void pigJobMigration(String username, String instance, String startDate, String endDate, ViewContext view, MigrationResponse migrationresult, String jobid) throws IOException, ItemNotFound {	long startTime = System.currentTimeMillis();	final Logger logger = Logger.getLogger(PigJobMigrationUtility.class);	Connection connectionHuedb = null;	Connection connectionAmbaridb = null;	logger.info("------------------------------");	logger.info("------------------------------");	
enddate date 

public void pigJobMigration(String username, String instance, String startDate, String endDate, ViewContext view, MigrationResponse migrationresult, String jobid) throws IOException, ItemNotFound {	long startTime = System.currentTimeMillis();	final Logger logger = Logger.getLogger(PigJobMigrationUtility.class);	Connection connectionHuedb = null;	Connection connectionAmbaridb = null;	logger.info("------------------------------");	logger.info("------------------------------");	
instance is 

public void pigJobMigration(String username, String instance, String startDate, String endDate, ViewContext view, MigrationResponse migrationresult, String jobid) throws IOException, ItemNotFound {	long startTime = System.currentTimeMillis();	final Logger logger = Logger.getLogger(PigJobMigrationUtility.class);	Connection connectionHuedb = null;	Connection connectionAmbaridb = null;	logger.info("------------------------------");	logger.info("------------------------------");	
hue username is 

Long epochtime = null;	String pigJobDirName;	ArrayList<PigModel> pigJobDbPojo = new ArrayList<PigModel>();	try {	String[] usernames = username.split(",");	int totalQueries = 0;	for (int k = 0; k < usernames.length; k++) {	connectionHuedb = DataSourceHueDatabase.getInstance(view.getProperties().get("huedrivername"), view.getProperties().get("huejdbcurl"), view.getProperties().get("huedbusername"), view.getProperties().get("huedbpassword")).getConnection();	username = usernames[k];	migrationresult.setProgressPercentage(0);	
migration started for user 

for (int k = 0; k < usernames.length; k++) {	connectionHuedb = DataSourceHueDatabase.getInstance(view.getProperties().get("huedrivername"), view.getProperties().get("huejdbcurl"), view.getProperties().get("huedbusername"), view.getProperties().get("huedbpassword")).getConnection();	username = usernames[k];	migrationresult.setProgressPercentage(0);	pigJobDbPojo = pigjobimpl.fetchFromHueDB(username, startDate, endDate, connectionHuedb, huedatabase);	totalQueries += pigJobDbPojo.size();	for (int j = 0; j < pigJobDbPojo.size(); j++) {	logger.info("the query fetched from hue=" + pigJobDbPojo.get(i).getScript());	}	if (pigJobDbPojo.size() == 0) {	
no queries has been selected for the user between dates 

connectionAmbaridb = DataSourceAmbariDatabase.getInstance(view.getProperties().get("ambaridrivername"), view.getProperties().get("ambarijdbcurl"), view.getProperties().get("ambaridbusername"), view.getProperties().get("ambaridbpassword")).getConnection();	connectionAmbaridb.setAutoCommit(false);	int fetchPigTablenameInstance = pigjobimpl.fetchInstanceTablename(connectionAmbaridb, instance, ambaridatabase);	int sequence = pigjobimpl.fetchSequenceno(connectionAmbaridb, fetchPigTablenameInstance, ambaridatabase);	for (i = 0; i < pigJobDbPojo.size(); i++) {	float calc = ((float) (i + 1)) / pigJobDbPojo.size() * 100;	int progressPercentage = Math.round(calc);	migrationresult.setProgressPercentage(progressPercentage);	migrationresult.setNumberOfQueryTransfered(i + 1);	getResourceManager(view).update(migrationresult, jobid);	
loop no 

connectionAmbaridb.setAutoCommit(false);	int fetchPigTablenameInstance = pigjobimpl.fetchInstanceTablename(connectionAmbaridb, instance, ambaridatabase);	int sequence = pigjobimpl.fetchSequenceno(connectionAmbaridb, fetchPigTablenameInstance, ambaridatabase);	for (i = 0; i < pigJobDbPojo.size(); i++) {	float calc = ((float) (i + 1)) / pigJobDbPojo.size() * 100;	int progressPercentage = Math.round(calc);	migrationresult.setProgressPercentage(progressPercentage);	migrationresult.setNumberOfQueryTransfered(i + 1);	getResourceManager(view).update(migrationresult, jobid);	logger.info("________________");	
the title of script 

pigjobimpl.createDirPigJobSecured(pigJobDirName, view.getProperties().get("namenode_URI_Ambari"), username, view.getProperties().get("PrincipalUserName"));	pigjobimpl.copyFileBetweenHdfsSecured(pigJobDbPojo.get(i).getDir() + "/script.pig", pigJobDirName, view.getProperties().get("namenode_URI_Ambari"), view.getProperties().get("namenode_URI_Hue"), username, view.getProperties().get("PrincipalUserName"));	pigjobimpl.copyFileBetweenHdfsSecured(pigJobDbPojo.get(i).getDir() + "/stderr", pigJobDirName, view.getProperties().get("namenode_URI_Ambari"), view.getProperties().get("namenode_URI_Hue"), username, view.getProperties().get("PrincipalUserName"));	pigjobimpl.copyFileBetweenHdfsSecured(pigJobDbPojo.get(i).getDir() + "/stdout", pigJobDirName, view.getProperties().get("namenode_URI_Ambari"), view.getProperties().get("namenode_URI_Hue"), username, view.getProperties().get("PrincipalUserName"));	} else {	pigjobimpl.createDirPigJob(pigJobDirName, view.getProperties().get("namenode_URI_Ambari"), username);	pigjobimpl.copyFileBetweenHdfs(pigJobDbPojo.get(i).getDir() + "/script.pig", pigJobDirName, view.getProperties().get("namenode_URI_Ambari"), view.getProperties().get("namenode_URI_Hue"), username);	pigjobimpl.copyFileBetweenHdfs(pigJobDbPojo.get(i).getDir() + "/stderr", pigJobDirName, view.getProperties().get("namenode_URI_Ambari"), view.getProperties().get("namenode_URI_Hue"), username);	pigjobimpl.copyFileBetweenHdfs(pigJobDbPojo.get(i).getDir() + "/stdout", pigJobDirName, view.getProperties().get("namenode_URI_Ambari"), view.getProperties().get("namenode_URI_Hue"), username);	}	
has been migrated to ambari 

} else {	pigjobimpl.createDirPigJob(pigJobDirName, view.getProperties().get("namenode_URI_Ambari"), username);	pigjobimpl.copyFileBetweenHdfs(pigJobDbPojo.get(i).getDir() + "/script.pig", pigJobDirName, view.getProperties().get("namenode_URI_Ambari"), view.getProperties().get("namenode_URI_Hue"), username);	pigjobimpl.copyFileBetweenHdfs(pigJobDbPojo.get(i).getDir() + "/stderr", pigJobDirName, view.getProperties().get("namenode_URI_Ambari"), view.getProperties().get("namenode_URI_Hue"), username);	pigjobimpl.copyFileBetweenHdfs(pigJobDbPojo.get(i).getDir() + "/stdout", pigJobDirName, view.getProperties().get("namenode_URI_Ambari"), view.getProperties().get("namenode_URI_Hue"), username);	}	}	pigjobimpl.updateSequenceno(connectionAmbaridb, maxCountforPigScript, fetchPigTablenameInstance, ambaridatabase);	connectionAmbaridb.commit();	}	
migration completed for user 

if (totalQueries == 0) {	migrationresult.setNumberOfQueryTransfered(0);	migrationresult.setTotalNoQuery(0);	} else {	migrationresult.setNumberOfQueryTransfered(totalQueries);	migrationresult.setTotalNoQuery(totalQueries);	migrationresult.setProgressPercentage(100);	}	getResourceManager(view).update(migrationresult, jobid);	} catch (SQLException e) {	
sql exception in ambari database 

} else {	migrationresult.setNumberOfQueryTransfered(totalQueries);	migrationresult.setTotalNoQuery(totalQueries);	migrationresult.setProgressPercentage(100);	}	getResourceManager(view).update(migrationresult, jobid);	} catch (SQLException e) {	migrationresult.setError("SQL Exception: " + e.getMessage());	try {	connectionAmbaridb.rollback();	
roll back done 

migrationresult.setNumberOfQueryTransfered(totalQueries);	migrationresult.setTotalNoQuery(totalQueries);	migrationresult.setProgressPercentage(100);	}	getResourceManager(view).update(migrationresult, jobid);	} catch (SQLException e) {	migrationresult.setError("SQL Exception: " + e.getMessage());	try {	connectionAmbaridb.rollback();	} catch (SQLException e1) {	
roll back exception 

migrationresult.setProgressPercentage(100);	}	getResourceManager(view).update(migrationresult, jobid);	} catch (SQLException e) {	migrationresult.setError("SQL Exception: " + e.getMessage());	try {	connectionAmbaridb.rollback();	} catch (SQLException e1) {	}	} catch (ClassNotFoundException e2) {	
class not found exception 

getResourceManager(view).update(migrationresult, jobid);	} catch (SQLException e) {	migrationresult.setError("SQL Exception: " + e.getMessage());	try {	connectionAmbaridb.rollback();	} catch (SQLException e1) {	}	} catch (ClassNotFoundException e2) {	migrationresult.setError("Class Not Found Exception: " + e2.getMessage());	} catch (ParseException e) {	
parseexception 

migrationresult.setError("SQL Exception: " + e.getMessage());	try {	connectionAmbaridb.rollback();	} catch (SQLException e1) {	}	} catch (ClassNotFoundException e2) {	migrationresult.setError("Class Not Found Exception: " + e2.getMessage());	} catch (ParseException e) {	migrationresult.setError("ParseException: " + e.getMessage());	} catch (URISyntaxException e) {	
URISyntaxException 

connectionAmbaridb.rollback();	} catch (SQLException e1) {	}	} catch (ClassNotFoundException e2) {	migrationresult.setError("Class Not Found Exception: " + e2.getMessage());	} catch (ParseException e) {	migrationresult.setError("ParseException: " + e.getMessage());	} catch (URISyntaxException e) {	migrationresult.setError("URI Syntax Exception: " + e.getMessage());	} catch (PropertyVetoException e) {	
PropertyVetoException 

}	} catch (ClassNotFoundException e2) {	migrationresult.setError("Class Not Found Exception: " + e2.getMessage());	} catch (ParseException e) {	migrationresult.setError("ParseException: " + e.getMessage());	} catch (URISyntaxException e) {	migrationresult.setError("URI Syntax Exception: " + e.getMessage());	} catch (PropertyVetoException e) {	migrationresult.setError("Property Veto Exception: " + e.getMessage());	} catch (Exception e) {	
generic exception 

} catch (URISyntaxException e) {	migrationresult.setError("URI Syntax Exception: " + e.getMessage());	} catch (PropertyVetoException e) {	migrationresult.setError("Property Veto Exception: " + e.getMessage());	} catch (Exception e) {	migrationresult.setError("Exception: " + e.getMessage());	} finally {	if (null != connectionAmbaridb) try {	connectionAmbaridb.close();	} catch (SQLException e) {	
connection closing exception 

migrationresult.setError("Exception: " + e.getMessage());	} finally {	if (null != connectionAmbaridb) try {	connectionAmbaridb.close();	} catch (SQLException e) {	migrationresult.setError("Error closing connection: " + e.getMessage());	}	getResourceManager(view).update(migrationresult, jobid);	}	logger.info("------------------------------");	
pig job migration end 

========================= ambari sample_1276 =========================

public LockFactory(Configuration config) {	profiling = config.isServerLocksProfilingEnabled();	profiledLocks = profiling ? new CopyOnWriteArraySet<>() : null;	
lock profiling is enabled disabled 

========================= ambari sample_4561 =========================

public Double getDAGProgress(String appId, String dagId) {	String dagIdx = parseDagIdIndex(dagId);	JSONObject progresses = delegate.dagProgress(appId, dagIdx);	double dagProgressValue;	if (progresses != null) {	JSONObject dagProgress = (JSONObject) progresses.get("dagProgress");	dagProgressValue = (Double) (dagProgress.get("progress"));	} else {	
error while retrieving progress of assumed 

String vertexIdx = parts[parts.length - 1];	builder.append(vertexIdx).append(",");	vertexIdToEntityMapping.put(vertexId.entity, vertexId.vertexName);	}	builder.setLength(builder.length() - 1);	}	String commaSeparatedVertices = builder.toString();	List<VertexProgress> parsedVertexProgresses = new LinkedList<VertexProgress>();	JSONObject vertexProgressesResponse = delegate.verticesProgress( appId, dagIdx, commaSeparatedVertices);	if (vertexProgressesResponse == null) {	
error while retrieving progress of vertices assumed for all vertices 

========================= ambari sample_604 =========================

public Result process() {	if (LOG.isDebugEnabled()) {	
handling api request 

========================= ambari sample_4218 =========================

public void init(SubsetConfiguration conf) {	String nameNodeUrl;	String blockSizeString;	
entering init 

if (matcher.find()) {	clusterName = matcher.group(1);	}	}	blockSizeString = hadoopConfig.get(DFS_BLOCK_SIZE_KEY);	if (blockSizeString != null) {	try {	blockSize = Integer.parseInt(blockSizeString);	logger.info("blockSize = " + blockSize);	} catch (NumberFormatException nfe) {	
exception on init 

public String getLocalNodeName() {	if (nodeName == null) {	try {	nodeName = InetAddress.getLocalHost().getCanonicalHostName();	} catch (Exception e) {	
error during getlocalhostname 

public String getClusterNodeName() {	if (clusterName.equalsIgnoreCase("localhost")) return getLocalNodeName();	try {	return InetAddress.getByName(clusterName).getCanonicalHostName();	} catch (Exception e) {	
error during getclusternodename 

public String getLocalNodeIPAddress() {	if (nodeIPAddress == null) {	try {	nodeIPAddress = InetAddress.getLocalHost().getHostAddress();	} catch (Exception e) {	
error during getlocalnodeipaddress 

public boolean ensureConnection() {	if (conn == null) {	try {	if (databaseUrl != null) {	conn = DriverManager.getConnection(databaseUrl);	}	} catch (Exception e) {	
error during getconnection 

cstmt.setNString(colid++, serviceName);	cstmt.setNString(colid++, tagPairs);	cstmt.setLong(colid++, recordTimestamp);	cstmt.registerOutParameter(colid, java.sql.Types.BIGINT);	cstmt.execute();	result = cstmt.getLong(colid);	if (cstmt.wasNull()) return -1;	return result;	}	} catch (Exception e) {	
error during getmetricrecordid call sproc 

if (cstmt.wasNull()) return -1;	return result;	}	} catch (Exception e) {	flush();	} finally {	if (cstmt != null) {	try {	cstmt.close();	} catch (SQLException se) {	
error during getmetricrecordid close cstmt 

logger.trace("Insert metricRecordId : " + metricRecordID + ", " + "metricName : " + metricName + ", metricValue : " + metricValue + ", " + "procedure = " + getInsertMetricsProcedureName());	if (ensureConnection()) {	String procedureCall = String.format("{call %s(?, ?, ?)}", getInsertMetricsProcedureName());	cstmt = conn.prepareCall(procedureCall);	cstmt.setLong(1, metricRecordID);	cstmt.setNString(2, metricName);	cstmt.setNString(3, metricValue);	cstmt.execute();	}	} catch (Exception e) {	
error during insertmetricvalue call sproc 

cstmt.setNString(3, metricValue);	cstmt.execute();	}	} catch (Exception e) {	flush();	} finally {	if (cstmt != null) {	try {	cstmt.close();	} catch (SQLException se) {	
error during insertmetricvalue close cstmt 

========================= ambari sample_487 =========================

Collection<HostRoleCommand> commands = new ArrayList<>(100);	if (!entities.isEmpty()) {	for (HostRoleCommandEntity entity : entities) {	commands.add(s_hostRoleCommandFactory.createExisting(entity));	}	} else {	if (requestId != null) {	commands.addAll(s_topologyManager.getTasks(requestId));	}	}	
retrieved commands for request 

========================= ambari sample_3545 =========================

public TimelineMetrics fetchTimelineMetrics(URIBuilder uriBuilder, Long startTime, Long endTime) throws IOException {	LOG.debug("Metrics request url = {}", uriBuilder);	BufferedReader reader = null;	TimelineMetrics timelineMetrics = null;	try {	HttpURLConnection connection = streamProvider.processURL(uriBuilder.toString(), HttpMethod.GET, (String) null, Collections.emptyMap());	if (!checkConnectionForPrecisionException(connection)) {	String higherPrecision = getHigherPrecision(uriBuilder, startTime, endTime);	if (higherPrecision != null) {	
requesting metrics with higher precision 

private boolean checkConnectionForPrecisionException(HttpURLConnection connection) throws IOException, URISyntaxException {	if (connection != null && connection.getResponseCode() == HttpStatus.SC_BAD_REQUEST) {	InputStream errorStream = connection.getErrorStream();	BufferedReader reader = new BufferedReader(new InputStreamReader(errorStream));	String errorMessage = reader.readLine();	if (errorMessage != null && errorMessage.contains("PrecisionLimitExceededException")) {	
encountered precision exception while requesting metrics 

========================= ambari sample_3400 =========================

return missingDependencies;	}	GsonBuilder gsonBuilder = new GsonBuilder();	gsonBuilder.registerTypeAdapter(UpgradeDependencies.class, new UpgradeDependencyDeserializer());	Gson gson = gsonBuilder.create();	Type type = new TypeToken<Map<String, StackPackage>>(){}.getType();	final Map<String, StackPackage> stackPackages;	try {	stackPackages = gson.fromJson(stackPackagesJson, type);	} catch( Exception exception ) {	
unable to deserialize the stack packages json assuming no service dependencies 

========================= ambari sample_3050 =========================

public void testMapperDate_epoch() {	
testmapperdate epoch 

public void testMapperDate_pattern() throws Exception {	
testmapperdate pattern 

public void testMapperDate_noDatePattern() {	
testmapperdate nodatepattern 

public void testMapperDate_notParsableDatePattern() {	
testmapperdate notparsabledatepattern 

public void testMapperDate_invalidEpochValue() {	
testmapperdate invalidepochvalue 

public void testMapperDate_invalidDateStringValue() {	
testmapperdate invaliddatestringvalue 

========================= ambari sample_1597 =========================

public Connection getConnection() throws SQLException {	
metric store connection url 

public Connection getConnection() throws SQLException {	try {	return DriverManager.getConnection(url);	} catch (SQLException e) {	
unable to connect to hbase store using phoenix 

========================= ambari sample_389 =========================

State state = State.UNKNOWN;	for (ServiceComponentHostResponse schr : hostComponentResponses) {	State schState = getHostComponentState(schr);	if (schState.ordinal() < state.ordinal()) {	state = schState;	}	}	return state;	}	} catch (AmbariException e) {	
can t determine service state 

========================= ambari sample_3683 =========================

private void initParams() throws Exception {	type = getStringValue("type");	while (true) {	OutputSolrProperties outputSolrProperties = getLogSearchConfig().getOutputSolrProperties(type);	if (outputSolrProperties == null) {	
output solr properties for type is not available yet 

}	zkConnectString = getStringValue("zk_connect_string");	if (StringUtils.isEmpty(zkConnectString)) {	throw new Exception("For solr output the zk_connect_string property need to be set");	}	skipLogtime = getBooleanValue("skip_logtime", DEFAULT_SKIP_LOGTIME);	maxIntervalMS = getIntValue("idle_flush_time_ms", DEFAULT_MAX_INTERVAL_MS);	workers = getIntValue("workers", DEFAULT_NUMBER_OF_WORKERS);	maxBufferSize = getIntValue("flush_size", DEFAULT_MAX_BUFFER_SIZE);	if (maxBufferSize < 1) {	
maxbuffersize is less than making it 

private void setupSecurity() {	String jaasFile = logFeederProps.getLogFeederSecurityConfig().getSolrJaasFile();	boolean securityEnabled = logFeederProps.getLogFeederSecurityConfig().isSolrKerberosEnabled();	if (securityEnabled) {	System.setProperty("java.security.auth.login.config", jaasFile);	HttpClientUtil.addConfigurer(new Krb5HttpClientConfigurer());	
setupsecurity called for kerberos configuration jaas file 

private void createSolrStateWatcher() throws Exception {	if ("none".equals(splitMode)) {	return;	}	CloudSolrClient stateWatcherClient = createSolrClient();	stateWatcherClient.registerCollectionStateWatcher(collection, this);	while (true) {	if (shards == null) {	
shards are not available yet waiting 

public void flush() {	
flush called 

try {	solrWorkerThread.interrupt();	} catch (Throwable t) {	}	} else {	isPending = true;	}	}	if (isPending) {	try {	
will give seconds to wrap up 

public void close() {	
closing solr client 

public void close() {	flush();	
closed solr client 

public void run() {	
solrworker thread started 

LOG.warn("In drain mode and sending to Solr failed. So exiting. output=" + getShortDescription());	break;	}	}	if (localBuffer.isEmpty()) {	lastDispatchTime = currTimeMS;	}	} catch (InterruptedException e) {	} catch (Throwable t) {	String logMessageKey = this.getClass().getSimpleName() + "_SOLR_MAINLOOP_EXCEPTION";	
caught exception in main loop 

result = true;	break;	} catch (IOException | SolrException exception) {	try {	LOG.warn("Solr is not reachable. Going to retry after " + RETRY_INTERVAL + " seconds. " + "output=" + getShortDescription(), exception);	Thread.sleep(RETRY_INTERVAL * 1000);	} catch (Throwable t) {	}	} catch (Throwable serverException) {	String logMessageKey = this.getClass().getSimpleName() + "_SOLR_UPDATE_EXCEPTION";	
error sending log message to server dropping logs 

========================= ambari sample_1651 =========================

}	if (null == stackId) {	if (propertyMaps.size() == 1) {	Map<String, Object> propertyMap = propertyMaps.iterator().next();	stackId = getStackInformationFromUrl(propertyMap);	} else {	LOG.error("Property Maps size is NOT equal to 1. Current 'propertyMaps' size = {}", propertyMaps.size());	}	}	if (null == stackId) {	
could not determine stack to process returning empty set 

LOG.error("Property Maps size is NOT equal to 1. Current 'propertyMaps' size = {}", propertyMaps.size());	}	}	if (null == stackId) {	return resources;	}	for (RepositoryVersionEntity repositoryVersionEntity : s_repositoryVersionDAO.findByStack(stackId)) {	currentStackUniqueId = repositoryVersionEntity.getId();	compatibleRepositoryVersionsMap.put(repositoryVersionEntity.getId(), new CompatibleRepositoryVersion(repositoryVersionEntity));	if (LOG.isDebugEnabled()) {	
added current stack id to map 

}	Map<String, UpgradePack> packs = s_ambariMetaInfo.get().getUpgradePacks( stackId.getStackName(), stackId.getStackVersion());	for (UpgradePack up : packs.values()) {	if (null != up.getTargetStack()) {	StackId targetStackId = new StackId(up.getTargetStack());	List<RepositoryVersionEntity> repositoryVersionEntities = s_repositoryVersionDAO.findByStack(targetStackId);	for (RepositoryVersionEntity repositoryVersionEntity : repositoryVersionEntities) {	if (compatibleRepositoryVersionsMap.containsKey(repositoryVersionEntity.getId())) {	compatibleRepositoryVersionsMap.get(repositoryVersionEntity.getId()).addUpgradePackType(up.getType());	if (LOG.isDebugEnabled()) {	
stack id exists in map appended new upgrade type 

for (RepositoryVersionEntity repositoryVersionEntity : repositoryVersionEntities) {	if (compatibleRepositoryVersionsMap.containsKey(repositoryVersionEntity.getId())) {	compatibleRepositoryVersionsMap.get(repositoryVersionEntity.getId()).addUpgradePackType(up.getType());	if (LOG.isDebugEnabled()) {	}	} else {	CompatibleRepositoryVersion compatibleRepositoryVersionEntity = new CompatibleRepositoryVersion(repositoryVersionEntity);	compatibleRepositoryVersionEntity.addUpgradePackType(up.getType());	compatibleRepositoryVersionsMap.put(repositoryVersionEntity.getId(), compatibleRepositoryVersionEntity);	if (LOG.isDebugEnabled()) {	
added stack id to map with upgrade type 

compatibleRepositoryVersionEntity.addUpgradePackType(up.getType());	compatibleRepositoryVersionsMap.put(repositoryVersionEntity.getId(), compatibleRepositoryVersionEntity);	if (LOG.isDebugEnabled()) {	}	}	}	} else {	if (currentStackUniqueId != null) {	compatibleRepositoryVersionsMap.get(currentStackUniqueId).addUpgradePackType(up.getType());	if (LOG.isDebugEnabled()) {	
current stack id retrieved from map added upgrade type 

if (LOG.isDebugEnabled()) {	}	}	}	} else {	if (currentStackUniqueId != null) {	compatibleRepositoryVersionsMap.get(currentStackUniqueId).addUpgradePackType(up.getType());	if (LOG.isDebugEnabled()) {	}	} else {	
couldn t retrieve current stack entry from map 

========================= ambari sample_3512 =========================

public void publish(List<SingleMetric> metrics) {	
published metrics 

========================= ambari sample_2616 =========================

if (customActionDefinitionRoot == null || !customActionDefinitionRoot.exists() || !customActionDefinitionRoot.canRead()) {	LOG.warn("Cannot read custom action definitions. " + customActionDefinitionRoot == null ? "" : "Check path " + customActionDefinitionRoot.getAbsolutePath());	}	File[] customActionDefinitionFiles = customActionDefinitionRoot.listFiles(StackDirectory.FILENAME_FILTER);	if (customActionDefinitionFiles != null) {	for (File definitionFile : customActionDefinitionFiles) {	ActionDefinitionXml adx = null;	try {	adx = unmarshal(ActionDefinitionXml.class, definitionFile);	} catch (UnmarshalException uex) {	
encountered badly formed action definition file 

StringBuilder errorReason = new StringBuilder("Error while parsing action definition. ").append(ad).append(" --- ");	TargetHostType targetType = safeValueOf(TargetHostType.class, ad.getTargetType(), errorReason);	ActionType actionType = safeValueOf(ActionType.class, ad.getActionType(), errorReason);	Short defaultTimeout = MIN_TIMEOUT;	if (ad.getDefaultTimeout() != null && !ad.getDefaultTimeout().isEmpty()) {	defaultTimeout = Short.parseShort(ad.getDefaultTimeout());	}	if (isValidActionDefinition(ad, actionType, defaultTimeout, errorReason)) {	String actionName = ad.getActionName();	if (actionDefinitionMap.containsKey(actionName)) {	
ignoring action definition as a different definition by that name already exists 

Short defaultTimeout = MIN_TIMEOUT;	if (ad.getDefaultTimeout() != null && !ad.getDefaultTimeout().isEmpty()) {	defaultTimeout = Short.parseShort(ad.getDefaultTimeout());	}	if (isValidActionDefinition(ad, actionType, defaultTimeout, errorReason)) {	String actionName = ad.getActionName();	if (actionDefinitionMap.containsKey(actionName)) {	continue;	}	actionDefinitionMap.put(ad.getActionName(), new ActionDefinition(ad.getActionName(), actionType, ad.getInputs(), ad.getTargetService(), ad.getTargetComponent(), ad.getDescription(), targetType, defaultTimeout, translatePermissions(ad.getPermissions())));	
added custom action definition for 

========================= ambari sample_3904 =========================

public void run() {	try {	
started to monitor 

public void run() {	try {	start();	} catch (Exception e) {	
error writing to output 

public void run() {	try {	start();	} catch (Exception e) {	}	
exiting thread 

public abstract void start() throws Exception;	public void outputLine(String line, INPUT_MARKER marker) {	statMetric.value++;	readBytesMetric.value += (line.length());	if (firstFilter != null) {	try {	firstFilter.apply(line, marker);	} catch (Exception e) {	
error during filter apply 

public void close() {	
close called 

========================= ambari sample_1743 =========================

public void runOnce() throws InterruptedException {	List<Future<String>> futures = workersPool.invokeAll(workers, sendIntervalMillis / 2, TimeUnit.MILLISECONDS);	int done = 0;	for (Future<String> future : futures) {	done += future.isDone() ? 1 : 0;	}	
finished successfully tasks 

========================= ambari sample_331 =========================

instancelist.add(I);	i++;	}	return instancelist;	}	finally {	if (rs1 != null) {	try {	rs1.close();	} catch (SQLException e) {	
sql exception in while closing result set 

if (rs1 != null) {	try {	rs1.close();	} catch (SQLException e) {	}	}	if (prSt != null) {	try {	prSt.close();	} catch (SQLException e) {	
sql exception in while closing preparedstatement 

if (prSt != null) {	try {	prSt.close();	} catch (SQLException e) {	}	}	if (conn != null) {	try {	conn.close();	} catch (SQLException e) {	
sql exception in while closing the connection 

========================= ambari sample_1256 =========================

File dir = service.getChecksFolder();	File[] jars = dir.listFiles(new FilenameFilter() {	public boolean accept(File dir, String name) {	return name.endsWith(".jar");	}	});	for (File jar : jars) {	try {	URL url = jar.toURI().toURL();	urls.add(url);	
adding service check jar to classpath 

public boolean accept(File dir, String name) {	return name.endsWith(".jar");	}	});	for (File jar : jars) {	try {	URL url = jar.toURI().toURL();	urls.add(url);	}	catch (Exception e) {	
failed to add service check jar to classpath 

}	}	}	ClassLoader classLoader = new URLClassLoader(urls.toArray(new URL[urls.size()]), ClassUtils.getDefaultClassLoader());	for (String prerequisiteCheck : missingChecks) {	Class<?> clazz = null;	try {	clazz = ClassUtils.resolveClassName(prerequisiteCheck, classLoader);	}	catch (IllegalArgumentException illegalArgumentException) {	
unable to find upgrade check 

clazz = ClassUtils.resolveClassName(prerequisiteCheck, classLoader);	}	catch (IllegalArgumentException illegalArgumentException) {	}	try {	if (clazz != null) {	AbstractCheckDescriptor upgradeCheck = (AbstractCheckDescriptor) clazz.newInstance();	checks.add(upgradeCheck);	}	} catch (Exception exception) {	
unable to create upgrade check 

========================= ambari sample_2883 =========================

throw new AmbariException(String.format( "The only upgrade which is currently allowed to be reverted for cluster %s is upgrade ID %s which was an upgrade to %s", cluster.getClusterName(), revertableUpgrade.getId(), revertableUpgrade.getRepositoryVersion().getVersion()));	}	m_type = calculateUpgradeType(upgradeRequestMap, revertUpgrade);	Set<RepositoryVersionEntity> priors = new HashSet<>();	Map<String, Service> clusterServices = cluster.getServices();	for (UpgradeHistoryEntity history : revertUpgrade.getHistory()) {	String serviceName = history.getServiceName();	String componentName = history.getComponentName();	priors.add(history.getFromReposistoryVersion());	if (!clusterServices.containsKey(serviceName)) {	
will not be reverted since it is no longer installed in the cluster 

UpgradeSummary summary = new UpgradeSummary();	summary.direction = m_direction;	summary.type = m_type;	summary.orchestration = m_orchestration;	summary.isRevert = m_isRevert;	summary.services = new HashMap<>();	for (String serviceName : m_services) {	RepositoryVersionEntity sourceRepositoryVersion = m_sourceRepositoryMap.get(serviceName);	RepositoryVersionEntity targetRepositoryVersion = m_targetRepositoryMap.get(serviceName);	if (null == sourceRepositoryVersion || null == targetRepositoryVersion) {	
unable to get the source target repositories for for the upgrade summary 

Iterator<String> iterator = servicesForUpgrade.iterator();	while (iterator.hasNext()) {	String serviceName = null;	try {	serviceName = iterator.next();	Service service = cluster.getService(serviceName);	m_sourceRepositoryMap.put(serviceName, service.getDesiredRepositoryVersion());	m_targetRepositoryMap.put(serviceName, repositoryVersion);	} catch (ServiceNotFoundException e) {	iterator.remove();	
skipping orchestration for service as it was defined to upgrade but is not installed in cluster 

========================= ambari sample_3154 =========================

String patternStr = "Order\\s*\\(\\s*col\\s*:\\s*([^,]+)\\s*,\\s*order\\s*:\\s*(\\d)\\s*\\)";	Pattern pattern = Pattern.compile(patternStr);	Matcher matcher = pattern.matcher(str);	LinkedList<ColumnOrder> list = new LinkedList<>();	while(matcher.find()){	String colName = matcher.group(1);	String orderString = matcher.group(2);	Order order = Order.fromOrdinal(Integer.valueOf(orderString));	ColumnOrder co = new ColumnOrder(colName, order);	list.add(co);	
columnorder 

========================= ambari sample_632 =========================

public ResourceInstance createResource(Resource.Type type, Map<Resource.Type, String> mapIds) {	try {	if (mapIds.containsKey(Resource.Type.Host)) {	String hostName = mapIds.get(Resource.Type.Host);	if (hostName != null) {	mapIds.put(Resource.Type.Host, hostName.toLowerCase());	}	}	} catch(Exception e) {	
lowercase host name value in resource failed with error 

========================= ambari sample_4042 =========================

public void testFilterKeyValue_extraction() throws Exception {	
testfilterkeyvalue extraction 

public void testFilterKeyValue_extractionWithBorders() throws Exception {	
testfilterkeyvalue extractionwithborders 

public void testFilterKeyValue_missingSourceField() throws Exception {	
testfilterkeyvalue missingsourcefield 

public void testFilterKeyValue_noSourceFieldPresent() throws Exception {	
testfilterkeyvalue nosourcefieldpresent 

========================= ambari sample_1610 =========================

public void initHomePage() {	home = new Home(driverProvider);	
init home page 

public void deleteCookies() {	
delete all cookies 

public void beforeStories() throws Exception {	initDockerContainer();	
initialize web driver 

public void beforeStories() throws Exception {	initDockerContainer();	StoryDataRegistry.INSTANCE.getWebDriverProvider().initialize();	
web driver details 

public void closePage() throws Exception {	
closing web driver 

public void initBrowser() {	
delete all cookies 

public void initBrowser() {	home.manage().deleteAllCookies();	
open home page 

public void contains(@Named("text") String text) {	
check page contains text 

public void notContains(@Named("text") String text) {	
check page does not contain text 

public void waitSeconds(@Named("second") String second) {	
wait seconds 

public void clickOnElementByXPath(@Named("xpath") String xPath) {	
click on element by xpath 

public void clickOnElementById(@Named("id") String id) {	
click on element by id 

public void clickOnElementByCssSelector(@Named("css") String cssSelector) {	
click on element by css selector 

public void findByXPath(@Named("xpath") String xPath) {	
find element by xpath 

public void findById(@Named("id") String id) {	
find element by id 

public void findByCssSelector(@Named("css") String cssSelector) {	
find element by css selector 

public void equalsByXPath(@Named("text") String text, @Named("xpath") String xPath) {	
check text of the element xpath equals with 

public void equalsyId(@Named("text") String text, @Named("id") String id) {	
check text of the element id equals with 

public void equalsCssSelector(@Named("text") String text, @Named("css") String cssSelector) {	
check text of the element css selector equals with 

public void doNotFindByXPath(@Named("xpath") String xPath) {	try {	
check that element does not exist with xpath 

public void doNotFindById(@Named("id") String id) {	try {	
check that element does not exist with id 

public void doNotFindByCssSelector(@Named("css") String cssSelector) {	try {	
check that element does not exist with css selector 

private void closeTourPopup() {	
close tour popup if needed 

========================= ambari sample_1581 =========================

public void runTask() {	
hostrequest executing start task for host 

public void runTask() {	RequestStatusResponse response = clusterTopology.startHost(hostRequest.getHostName(), skipFailure);	if (response != null) {	List<ShortTaskStatus> underlyingTasks = response.getTasks();	for (ShortTaskStatus task : underlyingTasks) {	String component = task.getRole();	Long logicalStartTaskId = hostRequest.getLogicalTasksForTopologyTask(this).get(component);	if (logicalStartTaskId == null) {	
skipping physical start task registering because component cannot be found 

List<ShortTaskStatus> underlyingTasks = response.getTasks();	for (ShortTaskStatus task : underlyingTasks) {	String component = task.getRole();	Long logicalStartTaskId = hostRequest.getLogicalTasksForTopologyTask(this).get(component);	if (logicalStartTaskId == null) {	continue;	}	hostRequest.registerPhysicalTaskId(logicalStartTaskId, task.getTaskId());	}	}	
hostrequest exiting start task for host 

========================= ambari sample_2657 =========================

HttpUriRequest outboundRequest = new HttpGet(endpoint);	URI uri = outboundRequest.getURI();	String scheme = uri.getScheme();	String host = uri.getHost();	int port = uri.getPort();	String path = uri.getPath();	if (!refresh) {	String appCookie = endpointCookieMap.get(endpoint);	if (appCookie != null) {	if (LOG.isDebugEnabled()) {	
got cached cookie 

if (httpResponse != null) {	HttpEntity entity = httpResponse.getEntity();	if (entity != null) {	entity.getContent().close();	}	}	}	hadoopAuthCookie = HADOOP_AUTH_EQ + quote(hadoopAuthCookie);	setAppCookie(endpoint, hadoopAuthCookie);	if (LOG.isInfoEnabled()) {	
successful spnego authentication to url 

========================= ambari sample_235 =========================

private void initializeSpoolDirectory() {	File spoolDir = new File(spoolDirectory);	if (!spoolDir.exists()) {	
creating spool directory 

private void initializeSpoolState() {	initializeSpoolDirectory();	currentSpoolFile = initializeSpoolFile();	try {	currentSpoolBufferedWriter = initializeSpoolWriter(currentSpoolFile);	} catch (IOException e) {	throw new LogSpoolerException("Could not create buffered writer for spool file: " + currentSpoolFile + ", error message: " + e.getLocalizedMessage(), e);	}	currentSpoolerContext = new LogSpoolerContext(currentSpoolFile);	
initialized spool file at path 

public void rollover() {	
rollover condition detected rolling over file 

public void rollover() {	currentSpoolBufferedWriter.flush();	if (currentSpoolFile.length()==0) {	
no data in file not doing rollover 

public void rollover() {	currentSpoolBufferedWriter.flush();	if (currentSpoolFile.length()==0) {	} else {	currentSpoolBufferedWriter.close();	rolloverHandler.handleRollover(currentSpoolFile);	
invoked rollover handler with file 

public void rollover() {	currentSpoolBufferedWriter.flush();	if (currentSpoolFile.length()==0) {	} else {	currentSpoolBufferedWriter.close();	rolloverHandler.handleRollover(currentSpoolFile);	initializeSpoolState();	}	boolean status = rolloverInProgress.compareAndSet(true, false);	if (!status) {	
should have reset rollover flag 

private synchronized void tryRollover() {	if (rolloverInProgress.compareAndSet(false, true)) {	rollover();	} else {	
ignoring rollover call as rollover already in progress for file 

public void run() {	
trying rollover based on time 

========================= ambari sample_1647 =========================

String            clusterName       = clusterDefinition.getClusterName();	Set<String>       hosts             = clusterDefinition.getHosts();	for (String hostName : hosts) {	Resource host = new ResourceImpl(Resource.Type.Host);	host.setProperty(HOST_CLUSTER_NAME_PROPERTY_ID, clusterName);	host.setProperty(HOST_NAME_PROPERTY_ID, hostName);	try {	host.setProperty(HOST_IP_PROPERTY_ID, clusterDefinition.getHostInfoProvider().getHostAddress(hostName));	} catch (SystemException e) {	if (LOG.isErrorEnabled()) {	
can t set host ip address caught exception 

========================= ambari sample_472 =========================

public Response previewFile(@QueryParam("path") String path, @QueryParam("start") int start, @QueryParam("end") int end) {	
previewing file from start till end 

if (start != 0) IOUtils.skip(stream, start);	int readBytes = IOUtils.read(stream, bytes);	boolean isFileEnd = false;	if (readBytes < length) isFileEnd = true;	JSONObject response = new JSONObject();	response.put("data", new String(bytes));	response.put("readbytes", readBytes);	response.put("isFileEnd", isFileEnd);	return Response.ok(response).build();	} catch (WebApplicationException ex) {	
error occurred while previewing 

boolean isFileEnd = false;	if (readBytes < length) isFileEnd = true;	JSONObject response = new JSONObject();	response.put("data", new String(bytes));	response.put("readbytes", readBytes);	response.put("isFileEnd", isFileEnd);	return Response.ok(response).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (FileNotFoundException ex) {	
error occurred while previewing 

JSONObject response = new JSONObject();	response.put("data", new String(bytes));	response.put("readbytes", readBytes);	response.put("isFileEnd", isFileEnd);	return Response.ok(response).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (FileNotFoundException ex) {	throw new NotFoundFormattedException(ex.getMessage(), ex);	} catch (Exception ex) {	
error occurred while previewing 

========================= ambari sample_1145 =========================

public void checkConnection(AmbariLdapConfiguration ambariLdapConfiguration) throws AmbariLdapException {	
trying to connect to the ldap server using provided configuration 

public void checkConnection(AmbariLdapConfiguration ambariLdapConfiguration) throws AmbariLdapException {	LdapConnectionTemplate ldapConnectionTemplate = ldapConnectionTemplateFactory.create(ambariLdapConfiguration);	Boolean isConnected = ldapConnectionTemplate.execute(new ConnectionCallback<Boolean>() {	public Boolean doWithConnection(LdapConnection connection) throws LdapException {	return connection.isConnected() && connection.isAuthenticated();	}	});	if (!isConnected) {	
could not connect to the ldap server 

public void checkConnection(AmbariLdapConfiguration ambariLdapConfiguration) throws AmbariLdapException {	LdapConnectionTemplate ldapConnectionTemplate = ldapConnectionTemplateFactory.create(ambariLdapConfiguration);	Boolean isConnected = ldapConnectionTemplate.execute(new ConnectionCallback<Boolean>() {	public Boolean doWithConnection(LdapConnection connection) throws LdapException {	return connection.isConnected() && connection.isAuthenticated();	}	});	if (!isConnected) {	throw new AmbariLdapException("Could not connect to the LDAP server. Configuration: " + ambariLdapConfiguration);	}	
successfully conencted to the ldap 

public String checkUserAttributes(String testUserName, String testPassword, AmbariLdapConfiguration ambariLdapConfiguration) throws AmbariLdapException {	String userDn;	try {	
checking user attributes for user 

public String checkUserAttributes(String testUserName, String testPassword, AmbariLdapConfiguration ambariLdapConfiguration) throws AmbariLdapException {	String userDn;	try {	String filter = FilterBuilder.and( FilterBuilder.equal(SchemaConstants.OBJECT_CLASS_AT, ambariLdapConfiguration.userObjectClass()), FilterBuilder.equal(ambariLdapConfiguration.userNameAttribute(), testUserName)) .toString();	
searching for the user using the search filter 

public String checkUserAttributes(String testUserName, String testPassword, AmbariLdapConfiguration ambariLdapConfiguration) throws AmbariLdapException {	String userDn;	try {	String filter = FilterBuilder.and( FilterBuilder.equal(SchemaConstants.OBJECT_CLASS_AT, ambariLdapConfiguration.userObjectClass()), FilterBuilder.equal(ambariLdapConfiguration.userNameAttribute(), testUserName)) .toString();	userDn = ldapConnectionTemplateFactory.create(ambariLdapConfiguration).searchFirst(new Dn(ambariLdapConfiguration.userSearchBase()), filter, SearchScope.SUBTREE, getUserDnNameEntryMapper(ambariLdapConfiguration));	if (null == userDn) {	
could not find test user based on the provided configuration user attributes may not be complete or the user may not exist 

public String checkUserAttributes(String testUserName, String testPassword, AmbariLdapConfiguration ambariLdapConfiguration) throws AmbariLdapException {	String userDn;	try {	String filter = FilterBuilder.and( FilterBuilder.equal(SchemaConstants.OBJECT_CLASS_AT, ambariLdapConfiguration.userObjectClass()), FilterBuilder.equal(ambariLdapConfiguration.userNameAttribute(), testUserName)) .toString();	userDn = ldapConnectionTemplateFactory.create(ambariLdapConfiguration).searchFirst(new Dn(ambariLdapConfiguration.userSearchBase()), filter, SearchScope.SUBTREE, getUserDnNameEntryMapper(ambariLdapConfiguration));	if (null == userDn) {	throw new AmbariLdapException("Could not find test user based on the provided configuration. User attributes may not be complete or the user may not exist.");	}	
attribute validation succeeded filter 

public String checkUserAttributes(String testUserName, String testPassword, AmbariLdapConfiguration ambariLdapConfiguration) throws AmbariLdapException {	String userDn;	try {	String filter = FilterBuilder.and( FilterBuilder.equal(SchemaConstants.OBJECT_CLASS_AT, ambariLdapConfiguration.userObjectClass()), FilterBuilder.equal(ambariLdapConfiguration.userNameAttribute(), testUserName)) .toString();	userDn = ldapConnectionTemplateFactory.create(ambariLdapConfiguration).searchFirst(new Dn(ambariLdapConfiguration.userSearchBase()), filter, SearchScope.SUBTREE, getUserDnNameEntryMapper(ambariLdapConfiguration));	if (null == userDn) {	throw new AmbariLdapException("Could not find test user based on the provided configuration. User attributes may not be complete or the user may not exist.");	}	} catch (Exception e) {	
user attributes validation failed 

public Set<String> checkGroupAttributes(String userDn, AmbariLdapConfiguration ambariLdapConfiguration) throws AmbariLdapException {	List<String> groups = Lists.newArrayList();	try {	
checking group attributes for user dn 

public Set<String> checkGroupAttributes(String userDn, AmbariLdapConfiguration ambariLdapConfiguration) throws AmbariLdapException {	List<String> groups = Lists.newArrayList();	try {	String filter = FilterBuilder.and( FilterBuilder.equal(SchemaConstants.OBJECT_CLASS_AT, ambariLdapConfiguration.groupObjectClass()), FilterBuilder.equal(ambariLdapConfiguration.groupMemberAttribute(), userDn) ).toString();	
searching for the groups the user dn is member of using the search filter 

public Set<String> checkGroupAttributes(String userDn, AmbariLdapConfiguration ambariLdapConfiguration) throws AmbariLdapException {	List<String> groups = Lists.newArrayList();	try {	String filter = FilterBuilder.and( FilterBuilder.equal(SchemaConstants.OBJECT_CLASS_AT, ambariLdapConfiguration.groupObjectClass()), FilterBuilder.equal(ambariLdapConfiguration.groupMemberAttribute(), userDn) ).toString();	LdapConnectionTemplate ldapConnectionTemplate = ldapConnectionTemplateFactory.create(ambariLdapConfiguration);	SearchRequest searchRequest = ldapConnectionTemplate.newSearchRequest(new Dn(ambariLdapConfiguration.groupSearchBase()), filter, SearchScope.SUBTREE);	searchRequest.addAttributes(ambariLdapConfiguration.groupMemberAttribute(), ambariLdapConfiguration.groupNameAttribute());	groups = ldapConnectionTemplate.search(searchRequest, getGroupNameEntryMapper(ambariLdapConfiguration));	if (groups == null || groups.isEmpty()) {	
no groups found for the user dn group attributes configuration is incomplete 

List<String> groups = Lists.newArrayList();	try {	String filter = FilterBuilder.and( FilterBuilder.equal(SchemaConstants.OBJECT_CLASS_AT, ambariLdapConfiguration.groupObjectClass()), FilterBuilder.equal(ambariLdapConfiguration.groupMemberAttribute(), userDn) ).toString();	LdapConnectionTemplate ldapConnectionTemplate = ldapConnectionTemplateFactory.create(ambariLdapConfiguration);	SearchRequest searchRequest = ldapConnectionTemplate.newSearchRequest(new Dn(ambariLdapConfiguration.groupSearchBase()), filter, SearchScope.SUBTREE);	searchRequest.addAttributes(ambariLdapConfiguration.groupMemberAttribute(), ambariLdapConfiguration.groupNameAttribute());	groups = ldapConnectionTemplate.search(searchRequest, getGroupNameEntryMapper(ambariLdapConfiguration));	if (groups == null || groups.isEmpty()) {	throw new AmbariLdapException("Group attribute ldap configuration is incomplete");	}	
group attribute configuration check succeeded 

try {	String filter = FilterBuilder.and( FilterBuilder.equal(SchemaConstants.OBJECT_CLASS_AT, ambariLdapConfiguration.groupObjectClass()), FilterBuilder.equal(ambariLdapConfiguration.groupMemberAttribute(), userDn) ).toString();	LdapConnectionTemplate ldapConnectionTemplate = ldapConnectionTemplateFactory.create(ambariLdapConfiguration);	SearchRequest searchRequest = ldapConnectionTemplate.newSearchRequest(new Dn(ambariLdapConfiguration.groupSearchBase()), filter, SearchScope.SUBTREE);	searchRequest.addAttributes(ambariLdapConfiguration.groupMemberAttribute(), ambariLdapConfiguration.groupNameAttribute());	groups = ldapConnectionTemplate.search(searchRequest, getGroupNameEntryMapper(ambariLdapConfiguration));	if (groups == null || groups.isEmpty()) {	throw new AmbariLdapException("Group attribute ldap configuration is incomplete");	}	} catch (Exception e) {	
user attributes validation failed 

========================= ambari sample_2913 =========================

ids.addAll(Collections2.transform(m_stacks, new Function<StackModule, String>() {	public String apply(StackModule input) {	return new StackId(input.getModuleInfo()).toString();	}	}));	String stackIds = StringUtils.join(ids, ',');	Long time = System.nanoTime();	try {	if (m_uri.getScheme().startsWith("http")) {	URLStreamProvider streamProvider = new URLStreamProvider( LOOKUP_CONNECTION_TIMEOUT, LOOKUP_READ_TIMEOUT, null, null, null);	
loading latest url info from for stacks 

}));	String stackIds = StringUtils.join(ids, ',');	Long time = System.nanoTime();	try {	if (m_uri.getScheme().startsWith("http")) {	URLStreamProvider streamProvider = new URLStreamProvider( LOOKUP_CONNECTION_TIMEOUT, LOOKUP_READ_TIMEOUT, null, null, null);	latestUrlMap = gson.fromJson(new InputStreamReader( streamProvider.readFrom(m_uri.toString())), type);	} else {	File jsonFile = new File(m_uri);	if (jsonFile.exists()) {	
loading latest url info from file for stacks 

if (m_uri.getScheme().startsWith("http")) {	URLStreamProvider streamProvider = new URLStreamProvider( LOOKUP_CONNECTION_TIMEOUT, LOOKUP_READ_TIMEOUT, null, null, null);	latestUrlMap = gson.fromJson(new InputStreamReader( streamProvider.readFrom(m_uri.toString())), type);	} else {	File jsonFile = new File(m_uri);	if (jsonFile.exists()) {	latestUrlMap = gson.fromJson(new FileReader(jsonFile), type);	}	}	} catch (Exception e) {	
could not load the uri from stack defaults will be used 

latestUrlMap = gson.fromJson(new InputStreamReader( streamProvider.readFrom(m_uri.toString())), type);	} else {	File jsonFile = new File(m_uri);	if (jsonFile.exists()) {	latestUrlMap = gson.fromJson(new FileReader(jsonFile), type);	}	}	} catch (Exception e) {	throw e;	} finally {	
loaded uri for stacks in ms 

if (jsonFile.exists()) {	latestUrlMap = gson.fromJson(new FileReader(jsonFile), type);	}	}	} catch (Exception e) {	throw e;	} finally {	}	Map<StackModule, RepoUrlInfoResult> result = new HashMap<>();	if (null == latestUrlMap) {	
could not load latest data for uri and stacks 

private Map<String, URI> resolveOsMap(StackModule stackModule, Map<String, String> osMap) {	Map<String, URI> resolved = new HashMap<>();	for (Entry<String, String> osEntry : osMap.entrySet()) {	String uriString = osEntry.getValue();	URI uri = StackModule.getURI(stackModule, uriString);	if (null == uri) {	
could not resolve uri 

========================= ambari sample_3251 =========================

public AmbariDelegatingAuthenticationFilter(Collection<AmbariAuthenticationFilter> filters) {	this.filters = (filters == null) ? Collections.emptyList() : filters;	if (this.filters.isEmpty()) {	
the delegated filters list is empty no authentication tests will be performed by this authentication filter 

public AmbariDelegatingAuthenticationFilter(Collection<AmbariAuthenticationFilter> filters) {	this.filters = (filters == null) ? Collections.emptyList() : filters;	if (this.filters.isEmpty()) {	} else if (LOG.isDebugEnabled()) {	StringBuffer filterNames = new StringBuffer();	for (AmbariAuthenticationFilter filter : this.filters) {	filterNames.append("\n\t");	filterNames.append(filter.getClass().getName());	}	
this authentication filter will attempt to authenticate a user using one of the following delegated authentication filters 

public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain chain) throws IOException, ServletException {	boolean handled = false;	HttpServletRequest httpServletRequest = (HttpServletRequest) servletRequest;	for (AmbariAuthenticationFilter filter : filters) {	if (LOG.isTraceEnabled()) {	
attempting to apply authentication filter 

public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain chain) throws IOException, ServletException {	boolean handled = false;	HttpServletRequest httpServletRequest = (HttpServletRequest) servletRequest;	for (AmbariAuthenticationFilter filter : filters) {	if (LOG.isTraceEnabled()) {	}	if (filter.shouldApply(httpServletRequest)) {	if (LOG.isDebugEnabled()) {	
using authentication filter since it applies 

if (LOG.isTraceEnabled()) {	}	if (filter.shouldApply(httpServletRequest)) {	if (LOG.isDebugEnabled()) {	}	filter.doFilter(servletRequest, servletResponse, chain);	handled = true;	break;	} else {	if (LOG.isDebugEnabled()) {	
filter does not apply skipping 

}	filter.doFilter(servletRequest, servletResponse, chain);	handled = true;	break;	} else {	if (LOG.isDebugEnabled()) {	}	}	}	if (!handled) {	
no delegated filters applied while attempting to authenticate a user continuing with the filter chain 

========================= ambari sample_2856 =========================

protected File createKeytabFile(String keytabData) throws KerberosOperationException {	boolean success = false;	File tempFile = null;	try {	tempFile = File.createTempFile("temp", ".dat");	} catch (IOException e) {	
failed to create temporary keytab file s 

protected Set<EncryptionType> translateEncryptionType(String name) {	Set<EncryptionType> encryptionTypes = null;	if (!StringUtils.isEmpty(name)) {	encryptionTypes = ENCRYPTION_TYPE_TRANSLATION_MAP.get(name.toLowerCase());	}	if (encryptionTypes == null) {	
the given encryption type name is not supported 

========================= ambari sample_3314 =========================

private void runStatement(RunStatement message) {	try {	HiveStatement statement = connectionDelegate.createStatement(connection);	if (message.shouldStartLogAggregation()) {	startLogAggregation(statement, message.getStatement(), message.getLogFile().get());	}	if (message.shouldStartGUIDFetch() && message.getJobId().isPresent()) {	startGUIDFetch(message.getId(), statement, message.getJobId().get());	}	
statement executor is executing statement statement id jobid sync job 

private void runStatement(RunStatement message) {	try {	HiveStatement statement = connectionDelegate.createStatement(connection);	if (message.shouldStartLogAggregation()) {	startLogAggregation(statement, message.getStatement(), message.getLogFile().get());	}	if (message.shouldStartGUIDFetch() && message.getJobId().isPresent()) {	startGUIDFetch(message.getId(), statement, message.getJobId().get());	}	Optional<ResultSet> resultSetOptional = connectionDelegate.execute(message.getStatement());	
finished executing statement statement id jobid sync job 

if (message.shouldStartGUIDFetch() && message.getJobId().isPresent()) {	startGUIDFetch(message.getId(), statement, message.getJobId().get());	}	Optional<ResultSet> resultSetOptional = connectionDelegate.execute(message.getStatement());	if (resultSetOptional.isPresent()) {	sender().tell(new ResultInformation(message.getId(), resultSetOptional.get()), self());	} else {	sender().tell(new ResultInformation(message.getId()), self());	}	} catch (SQLException e) {	
failed to execute statement 

private void startGUIDFetch(int statementId, HiveStatement statement, String jobId) {	if (guidFetcher == null) {	guidFetcher = getContext().actorOf(Props.create(YarnAtsGUIDFetcher.class, sender()) .withDispatcher("akka.actor.misc-dispatcher"), "YarnAtsGUIDFetcher:" + UUID.randomUUID().toString());	}	
fetching guid for job id 

private void getColumnMetaData(GetColumnMetadataJob message) {	try {	ResultSet resultSet = connectionDelegate.getColumnMetadata(connection, message);	sender().tell(new ResultInformation(-1, resultSet), self());	} catch (SQLException e) {	
failed to get column metadata for databasepattern tablepattern columnpattern 

========================= ambari sample_976 =========================

collection.add(new TaskWrapper(service, component, Collections.singleton(ambariServerHostname), params, t));	continue;	}	if (t.getType().equals(Task.Type.EXECUTE)) {	ExecuteTask et = (ExecuteTask) t;	if (et.hosts == ExecuteHostType.MASTER) {	if (hostsType.master != null) {	collection.add(new TaskWrapper(service, component, Collections.singleton(hostsType.master), params, t));	continue;	} else {	
found an execute task for and meant to run on a master but could not find any masters to run on skipping this task 

continue;	} else {	continue;	}	}	if (et.hosts == ExecuteHostType.ANY) {	if (hostsType.hosts != null && !hostsType.hosts.isEmpty()) {	collection.add(new TaskWrapper(service, component, Collections.singleton(hostsType.hosts.iterator().next()), params, t));	continue;	} else {	
found an execute task for and meant to run on any host but could not find host to run on skipping this task 

continue;	}	}	if (et.hosts == ExecuteHostType.FIRST) {	if (hostsType.hosts != null && !hostsType.hosts.isEmpty()) {	List<String> sortedHosts = new ArrayList<>(hostsType.hosts);	Collections.sort(sortedHosts, String.CASE_INSENSITIVE_ORDER);	collection.add(new TaskWrapper(service, component, Collections.singleton(sortedHosts.get(0)), params, t));	continue;	} else {	
found an execute task for and meant to run on the first host sorted alphabetically but could not find host to run on skipping this task 

========================= ambari sample_3206 =========================

public void process() throws AmbariException {	
updating existing repo versions with service repos 

========================= ambari sample_4469 =========================

boolean retVal = false;	ResultSet rs = null;	try {	rs = statement.executeQuery(query);	if (rs != null) {	if (rs.next()) {	return rs.getInt(1) > 0;	}	}	} catch (Exception e) {	
unable to check if table has any data exception 

try {	while (rs.next()) {	if (StringUtils.equalsIgnoreCase(fkName, rs.getString("FK_NAME"))) {	return true;	}	}	} finally {	rs.close();	}	}	
fk not found for table 

try {	while (rs.next()) {	if (StringUtils.equalsIgnoreCase(fkName, rs.getString("FK_NAME"))) {	return rs.getString("FK_NAME");	}	}	} finally {	rs.close();	}	}	
fk not found for table 

}	if (rs != null) {	try {	while (rs.next()) {	String pkColumn = rs.getString("PKCOLUMN_NAME");	String fkColumn = rs.getString("FKCOLUMN_NAME");	int pkIndex = pkColumns.indexOf(pkColumn);	int fkIndex = fkColumns.indexOf(fkColumn);	if (pkIndex != -1 && fkIndex != -1) {	if (pkIndex != fkIndex) {	
columns for fk constraint should be provided in exact order 

String query = dbmsHelper.getAddForeignKeyStatement(tableName, constraintName, Arrays.asList(keyColumns), referenceTableName, Arrays.asList(referenceColumns), shouldCascadeOnDelete);	try {	executeQuery(query, ignoreFailure);	} catch (SQLException e) {	LOG.warn("Add FK constraint failed" + ", constraintName = " + constraintName + ", tableName = " + tableName, e.getMessage());	if (!ignoreFailure) {	throw e;	}	}	} else {	
foreign key constraint already exists skipping 

public void addUniqueConstraint(String tableName, String constraintName, String... columnNames) throws SQLException {	if (!tableHasConstraint(tableName, constraintName) && tableHasColumn(tableName, columnNames)) {	String query = dbmsHelper.getAddUniqueConstraintStatement(tableName, constraintName, columnNames);	try {	executeQuery(query);	} catch (SQLException e) {	LOG.warn("Add unique constraint failed, constraintName={},tableName={}", constraintName, tableName);	throw e;	}	} else {	
unique constraint already exists or columns not found skipping 

public void addPKConstraint(String tableName, String constraintName, boolean ignoreErrors, String... columnName) throws SQLException {	if (!tableHasPrimaryKey(tableName, null) && tableHasColumn(tableName, columnName)) {	String query = dbmsHelper.getAddPrimaryKeyConstraintStatement(tableName, constraintName, columnName);	executeQuery(query, ignoreErrors);	} else {	
primary constraint not altered to table as column not present or constraint already exists 

builder.append(",");	}	}	builder.append(")");	Statement statement = getConnection().createStatement();	int rowsUpdated = 0;	String query = builder.toString();	try {	rowsUpdated = statement.executeUpdate(query);	} catch (SQLException e) {	
unable to execute query 

Statement statement = getConnection().createStatement();	ResultSet resultSet = null;	int count = -1;	String query = builder.toString();	try {	resultSet = statement.executeQuery(query);	if ((resultSet != null) && (resultSet.next())) {	count = resultSet.getInt(1);	}	} catch (SQLException e) {	
unable to execute query 

public void executeQuery(String query, boolean ignoreFailure) throws SQLException {	
executing query 

public void executeQuery(String query, boolean ignoreFailure) throws SQLException {	Statement statement = getConnection().createStatement();	try {	statement.execute(query);	} catch (SQLException e) {	if (!ignoreFailure) {	
error executing query 

public void executePreparedQuery(String query, boolean ignoreFailure, Object...arguments) throws SQLException{	
executing prepared query 

public void executePreparedQuery(String query, boolean ignoreFailure, Object...arguments) throws SQLException{	PreparedStatement preparedStatement = getConnection().prepareStatement(query);	setArgumentsForPreparedStatement(preparedStatement, arguments);	try {	preparedStatement.execute();	} catch (SQLException e) {	if (!ignoreFailure){	
error executing prepared query 

public void executePreparedUpdate(String query, boolean ignoreFailure, Object...arguments) throws SQLException{	
executing prepared query 

public void executePreparedUpdate(String query, boolean ignoreFailure, Object...arguments) throws SQLException{	PreparedStatement preparedStatement = getConnection().prepareStatement(query);	setArgumentsForPreparedStatement(preparedStatement, arguments);	try {	preparedStatement.executeUpdate();	} catch (SQLException e) {	if (!ignoreFailure){	
error executing prepared query 

public void dropTable(String tableName) throws SQLException {	if (tableExists(tableName)){	String query = dbmsHelper.getDropTableStatement(tableName);	executeQuery(query);	} else {	
table doesn t exists skipping 

public void dropFKConstraint(String tableName, String constraintName, boolean ignoreFailure) throws SQLException {	String checkedConstraintName = getCheckedForeignKey(convertObjectName(tableName), constraintName);	if (checkedConstraintName != null) {	String query = dbmsHelper.getDropFKConstraintStatement(tableName, checkedConstraintName);	executeQuery(query, ignoreFailure);	} else {	
foreign key from table does not exist and will not be dropped 

public void dropUniqueConstraint(String tableName, String constraintName, boolean ignoreFailure) throws SQLException {	if (tableHasConstraint(convertObjectName(tableName), convertObjectName(constraintName))) {	String query = dbmsHelper.getDropUniqueConstraintStatement(tableName, constraintName);	executeQuery(query, ignoreFailure);	} else {	
unique constraint from table not found nothing to drop 

public void dropPKConstraint(String tableName, String constraintName, String columnName, boolean cascade) throws SQLException {	if (tableHasPrimaryKey(tableName, columnName)) {	String query = dbmsHelper.getDropPrimaryKeyStatement(convertObjectName(tableName), constraintName, cascade);	executeQuery(query, false);	} else {	
primary key doesn t exists for table skipping 

public void dropPKConstraint(String tableName, String constraintName, boolean ignoreFailure, boolean cascade) throws SQLException {	if (tableHasPrimaryKey(tableName, null)) {	String query = dbmsHelper.getDropPrimaryKeyStatement(convertObjectName(tableName), constraintName, cascade);	executeQuery(query, ignoreFailure);	} else {	
primary key doesn t exists for table skipping 

public void setColumnNullable(String tableName, DBAccessor.DBColumnInfo columnInfo, boolean nullable) throws SQLException {	String columnName = columnInfo.getName();	if (isColumnNullable(tableName, columnName) != nullable) {	String query = dbmsHelper.getSetNullableStatement(tableName, columnInfo, nullable);	executeQuery(query);	} else {	
column nullability property is not changed due to column from table is already in state skipping nullable not nullable 

public void dropPKConstraint(String tableName, String defaultConstraintName) throws SQLException {	Configuration.DatabaseType databaseType = configuration.getDatabaseType();	if (databaseType == DatabaseType.MYSQL) {	String mysqlDropQuery = String.format("ALTER TABLE %s DROP PRIMARY KEY", tableName);	executeQuery(mysqlDropQuery, true);	return;	}	String primaryKeyConstraintName = getPrimaryKeyConstraintName(tableName);	if (null == primaryKeyConstraintName) {	primaryKeyConstraintName = defaultConstraintName;	
unable to dynamically determine the pk constraint name for defaulting to 

if (databaseType == DatabaseType.MYSQL) {	String mysqlDropQuery = String.format("ALTER TABLE %s DROP PRIMARY KEY", tableName);	executeQuery(mysqlDropQuery, true);	return;	}	String primaryKeyConstraintName = getPrimaryKeyConstraintName(tableName);	if (null == primaryKeyConstraintName) {	primaryKeyConstraintName = defaultConstraintName;	}	if (null == primaryKeyConstraintName) {	
unable to determine the primary key constraint name for 

ResultSet resultSet = null;	String query = builder.toString();	try {	resultSet = statement.executeQuery(query);	if (resultSet != null) {	while (resultSet.next()) {	result.add(resultSet.getInt(1));	}	}	} catch (SQLException e) {	
unable to execute query 

public void clearTable(String tableName) throws SQLException {	if (tableExists(tableName)){	String sqlQuery = "DELETE FROM " + convertObjectName(tableName);	executeQuery(sqlQuery);	} else {	
table doesn t exists skipping 

public void clearTableColumn(String tableName, String columnName, Object value) throws SQLException {	if (tableExists(tableName)){	String sqlQuery = String.format("UPDATE %s SET %s = ?", convertObjectName(tableName), convertObjectName(columnName));	executePreparedUpdate(sqlQuery, value);	} else {	
table doesn t exists skipping 

========================= ambari sample_4243 =========================

public QueryResponse process(SolrQuery solrQuery, String event) {	SolrUtil.removeDoubleOrTripleEscapeFromFilters(solrQuery);	
solr query will be processed 

solrQuery.remove("event");	try {	QueryResponse queryResponse = getSolrClient().query(solrQuery, METHOD.POST);	logSolrEvent(event, solrQuery, queryResponse);	return queryResponse;	} catch (Exception e){	LOG.error("Error during solrQuery=" + e);	throw RESTErrorUtil.createRESTException(MessageEnums.SOLR_ERROR.getMessage().getMessage(), MessageEnums.ERROR_SYSTEM);	}	} else {	
solr configuration improper for logs 

public UpdateResponse deleteByQuery(SolrQuery solrQuery, String event) {	SolrUtil.removeDoubleOrTripleEscapeFromFilters(solrQuery);	
solr delete query will be processed 

if (getSolrClient() != null) {	try {	UpdateResponse updateResponse = getSolrClient().deleteByQuery(solrQuery.getQuery());	logSolrEvent(event, solrQuery, updateResponse);	return updateResponse;	} catch (Exception e) {	LOG.error("Error during delete solrQuery=" + e);	throw RESTErrorUtil.createRESTException(MessageEnums.SOLR_ERROR.getMessage().getMessage(), MessageEnums.ERROR_SYSTEM);	}	} else {	
solr configuration improper for logs 

public long count(final SolrDataQuery solrDataQuery) {	return getSolrTemplate().execute(new SolrCallback<Long>() {	public Long doInSolr(SolrClient solrClient) throws SolrServerException, IOException {	SolrQuery solrQuery = new DefaultQueryParser().doConstructSolrQuery(solrDataQuery);	solrQuery.setStart(0);	solrQuery.setRows(0);	QueryResponse queryResponse = solrClient.query(solrQuery);	long count = solrClient.query(solrQuery).getResults().getNumFound();	
username count solrquery query time execution total time elapsed is count result 

========================= ambari sample_1411 =========================

public Response getList() {	try {	
getting all udf 

========================= ambari sample_576 =========================

protected <T> T createResources(Command<T> command) throws SystemException, ResourceAlreadyExistsException, NoSuchParentResourceException {	try {	return invokeWithRetry(command);	} catch (ParentObjectNotFoundException e) {	throw new NoSuchParentResourceException(e.getMessage(), e);	} catch (DuplicateResourceException e) {	throw new ResourceAlreadyExistsException(e.getMessage());	} catch (AmbariException e) {	if (LOG.isErrorEnabled()) {	
caught ambariexception when creating a resource 

protected <T> T getResources (Command<T> command) throws SystemException, NoSuchResourceException, NoSuchParentResourceException {	try {	return command.invoke();	} catch (ParentObjectNotFoundException e) {	throw new NoSuchParentResourceException(e.getMessage(), e);	} catch (ObjectNotFoundException e) {	throw new NoSuchResourceException("The requested resource doesn't exist: " + e.getMessage(), e);	}  catch (AmbariException e) {	if (LOG.isErrorEnabled()) {	
caught ambariexception when getting a resource 

protected <T> T modifyResources (Command<T> command) throws SystemException, NoSuchResourceException, NoSuchParentResourceException {	try {	return invokeWithRetry(command);	} catch (ParentObjectNotFoundException e) {	throw new NoSuchParentResourceException(e.getMessage(), e);	} catch (ObjectNotFoundException e) {	throw new NoSuchResourceException("The specified resource doesn't exist: " + e.getMessage(), e);	}  catch (AmbariException e) {	if (LOG.isErrorEnabled()) {	
caught ambariexception when modifying a resource 

private <T> T invokeWithRetry(Command<T> command) throws AmbariException, AuthorizationException {	RetryHelper.clearAffectedClusters();	int retryAttempts = RetryHelper.getOperationsRetryAttempts();	do {	try {	return command.invoke();	} catch (Exception e) {	if (RetryHelper.isDatabaseException(e)) {	RetryHelper.invalidateAffectedClusters();	if (retryAttempts > 0) {	
ignoring database exception to perform operation retry attempts remaining 

========================= ambari sample_3569 =========================

public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException {	HttpServletRequest request = (HttpServletRequest) servletRequest;	if (requestMatcher.matches(request)) {	VResponse errorResponse = getErrorResponse();	if (errorResponse != null) {	
request is filtered out 

========================= ambari sample_1387 =========================

public void testGetQuery() throws Exception {	String createTableQuery = "CREATE TABLE `d1`.`t2` (`col_name1` string COMMENT 'col_name1 comment'," + "`col_name2` decimal(10,2) COMMENT 'col_name2 comment')  PARTITIONED BY ( `col_name4` char(1) COMMENT 'col_name4 comment'," + "`col_name3` string COMMENT 'col_name3 comment') CLUSTERED BY (col_name1, col_name2) SORTED BY (col_name1 ASC,col_name2 DESC)" + " INTO 5 BUCKETS  ROW FORMAT DELIMITED  FIELDS TERMINATED BY ',' ESCAPED BY '\\\\' STORED AS  INPUTFORMAT " + "'org.apache.hadoop.mapred.SequenceFileInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat' " + "LOCATION 'hdfs: "'orc.compress'='SNAPPY','transient_lastDdlTime'='1481520077','NO_AUTO_COMPACTION'='true','comment'='table t1 comment'," + "'SORTBUCKETCOLSPREFIX'='TRUE')";	String json = "{\n" + "\t\"id\": \"d1/t2\",\n" + "\t\"database\": \"d1\",\n" + "\t\"table\": \"t2\",\n" + "\t\"columns\": [{\n" + "\t\t\"name\": \"col_name1\",\n" + "\t\t\"type\": \"string\",\n" + "\t\t\"comment\": \"col_name1 comment\"\n" + "\t}, {\n" + "\t\t\"name\": \"col_name2\",\n" + "\t\t\"type\": \"decimal(10,2)\",\n" + "\t\t\"comment\": \"col_name2 comment\"\n" + "\t}],\n" + "\t\"partitionInfo\": {\n" + "\t\t\"columns\": [{\n" + "\t\t\t\"name\": \"col_name4\",\n" + "\t\t\t\"type\": \"char(1)\",\n" + "\t\t\t\"comment\": \"col_name4 comment\"\n" + "\t\t}, {\n" + "\t\t\t\"name\": \"col_name3\",\n" + "\t\t\t\"type\": \"string\",\n" + "\t\t\t\"comment\": \"col_name3 comment\"\n" + "\t\t}]\n" + "\t},\n" + "\t\"detailedInfo\": {\n" + "\t\t\"dbName\": \"d1\",\n" + "\t\t\"owner\": \"admin\",\n" + "\t\t\"createTime\": \"Mon Dec 12 05:21:17 UTC 2016\",\n" + "\t\t\"lastAccessTime\": \"UNKNOWN\",\n" + "\t\t\"retention\": \"0\",\n" + "\t\t\"tableType\": \"MANAGED_TABLE\",\n" + "\t\t\"location\": \"hdfs: "\t\t\"parameters\": {\n" + "\t\t\t\"immutable\": \"false\",\n" + "\t\t\t\"orc.compress\": \"SNAPPY\",\n" + "\t\t\t\"transient_lastDdlTime\": \"1481520077\",\n" + "\t\t\t\"NO_AUTO_COMPACTION\": \"true\",\n" + "\t\t\t\"comment\": \"table t1 comment\",\n" + "\t\t\t\"SORTBUCKETCOLSPREFIX\": \"TRUE\"\n" + "\t\t}\n" + "\t},\n" + "\t\"storageInfo\": {\n" + "\t\t\"serdeLibrary\": \"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\",\n" + "\t\t\"inputFormat\": \"org.apache.hadoop.mapred.SequenceFileInputFormat\",\n" + "\t\t\"outputFormat\": \"org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\",\n" + "\t\t\"compressed\": \"No\",\n" + "\t\t\"numBuckets\": \"5\",\n" + "\t\t\"bucketCols\": [\"col_name1\", \" col_name2\"],\n" + "\t\t\"sortCols\": [{\n" + "\t\t\t\"columnName\": \"col_name1\",\n" + "\t\t\t\"order\": \"ASC\"\n" + "\t\t}, {\n" + "\t\t\t\"columnName\": \"col_name2\",\n" + "\t\t\t\"order\": \"DESC\"\n" + "\t\t}],\n" + "\t\t\"parameters\": {\n" + "\t\t\t\"escape.delim\": \"\\\\\\\\\",\n" + "\t\t\t\"field.delim\": \",\",\n" + "\t\t\t\"serialization.format\": \",\"\n" + "\t\t}\n" + "\t}\n" + "}";	TableMeta tableMeta = new Gson().fromJson(json, TableMeta.class);	Optional<String> createQuery = new CreateTableQueryGenerator(tableMeta).getQuery();	
createquery 

========================= ambari sample_521 =========================

}	try {	final LdapTemplate ldapTemplate = loadLdapTemplate();	ldapTemplate.search(ldapServerProperties.getBaseDN(), "uid=dummy_search", new AttributesMapper() {	public Object mapFromAttributes(Attributes arg0) throws NamingException {	return null;	}	});	return true;	} catch (Exception ex) {	
could not connect to ldap server 

public LdapBatchDto synchronizeAllLdapGroups(LdapBatchDto batchInfo) throws AmbariException {	
synchronize all ldap groups 

public LdapBatchDto synchronizeAllLdapUsers(LdapBatchDto batchInfo) throws AmbariException {	
synchronize all ldap users 

public LdapBatchDto synchronizeAllLdapUsers(LdapBatchDto batchInfo) throws AmbariException {	Set<LdapUserDto> externalLdapUserInfo = getExternalLdapUserInfo();	Map<String, User> internalUsersMap = getInternalUsers();	for (LdapUserDto userDto : externalLdapUserInfo) {	String userName = userDto.getUserName();	if (internalUsersMap.containsKey(userName)) {	final User user = internalUsersMap.get(userName);	if (user != null && !user.isLdapUser()) {	if (LdapUsernameCollisionHandlingBehavior.SKIP == configuration.syncCollisionHandlingBehavior()) {	
user skipped because it is local user 

Map<String, User> internalUsersMap = getInternalUsers();	for (LdapUserDto userDto : externalLdapUserInfo) {	String userName = userDto.getUserName();	if (internalUsersMap.containsKey(userName)) {	final User user = internalUsersMap.get(userName);	if (user != null && !user.isLdapUser()) {	if (LdapUsernameCollisionHandlingBehavior.SKIP == configuration.syncCollisionHandlingBehavior()) {	batchInfo.getUsersSkipped().add(userDto);	} else {	batchInfo.getUsersToBecomeLdap().add(userDto);	
convert user to ldap user 

public LdapBatchDto synchronizeLdapGroups(Set<String> groups, LdapBatchDto batchInfo) throws AmbariException {	
synchronize ldap groups 

public LdapBatchDto synchronizeLdapUsers(Set<String> users, LdapBatchDto batchInfo) throws AmbariException {	
synchronize ldap users 

}	specifiedUsers.addAll(userDtos);	}	final Map<String, User> internalUsersMap = getInternalUsers();	for (LdapUserDto userDto : specifiedUsers) {	String userName = userDto.getUserName();	if (internalUsersMap.containsKey(userName)) {	final User user = internalUsersMap.get(userName);	if (user != null && !user.isLdapUser()) {	if (LdapUsernameCollisionHandlingBehavior.SKIP == configuration.syncCollisionHandlingBehavior()) {	
user skipped because it is local user 

public LdapBatchDto synchronizeExistingLdapGroups(LdapBatchDto batchInfo) throws AmbariException {	
synchronize existing ldap groups 

public LdapBatchDto synchronizeExistingLdapUsers(LdapBatchDto batchInfo) throws AmbariException {	
synchronize existing ldap users 

if (internalUsers.containsKey(userName)) {	final User user = internalUsers.get(userName);	if (user == null) {	if (!internalMembers.containsKey(userName)) {	batchInfo.getMembershipToAdd().add(new LdapUserGroupMemberDto(groupName, externalMember.getUserName()));	}	continue;	}	if (!user.isLdapUser()) {	if (LdapUsernameCollisionHandlingBehavior.SKIP == configuration.syncCollisionHandlingBehavior()) {	
user skipped because it is local user 

protected LdapUserDto getLdapUserByMemberAttr(String memberAttributeValue) {	Set<LdapUserDto> filteredLdapUsers;	memberAttributeValue = getUniqueIdByMemberPattern(memberAttributeValue, ldapServerProperties.getSyncUserMemberReplacePattern());	Filter syncMemberFilter = createCustomMemberFilter(memberAttributeValue, ldapServerProperties.getSyncUserMemberFilter());	if (memberAttributeValue != null && syncMemberFilter != null) {	
use custom filter for getting member user with default basedn 

protected LdapUserDto getLdapUserByMemberAttr(String memberAttributeValue) {	Set<LdapUserDto> filteredLdapUsers;	memberAttributeValue = getUniqueIdByMemberPattern(memberAttributeValue, ldapServerProperties.getSyncUserMemberReplacePattern());	Filter syncMemberFilter = createCustomMemberFilter(memberAttributeValue, ldapServerProperties.getSyncUserMemberFilter());	if (memberAttributeValue != null && syncMemberFilter != null) {	filteredLdapUsers = getFilteredLdapUsers(ldapServerProperties.getBaseDN(), syncMemberFilter);	} else if (memberAttributeValue != null && isMemberAttributeBaseDn(memberAttributeValue)) {	
member can be used as basedn 

protected LdapUserDto getLdapUserByMemberAttr(String memberAttributeValue) {	Set<LdapUserDto> filteredLdapUsers;	memberAttributeValue = getUniqueIdByMemberPattern(memberAttributeValue, ldapServerProperties.getSyncUserMemberReplacePattern());	Filter syncMemberFilter = createCustomMemberFilter(memberAttributeValue, ldapServerProperties.getSyncUserMemberFilter());	if (memberAttributeValue != null && syncMemberFilter != null) {	filteredLdapUsers = getFilteredLdapUsers(ldapServerProperties.getBaseDN(), syncMemberFilter);	} else if (memberAttributeValue != null && isMemberAttributeBaseDn(memberAttributeValue)) {	Filter filter = new EqualsFilter(OBJECT_CLASS_ATTRIBUTE, ldapServerProperties.getUserObjectClass());	filteredLdapUsers = getFilteredLdapUsers(memberAttributeValue, filter);	} else {	
member cannot be used as basedn 

protected LdapGroupDto getLdapGroupByMemberAttr(String memberAttributeValue) {	Set<LdapGroupDto> filteredLdapGroups;	memberAttributeValue = getUniqueIdByMemberPattern(memberAttributeValue, ldapServerProperties.getSyncGroupMemberReplacePattern());	Filter syncMemberFilter = createCustomMemberFilter(memberAttributeValue, ldapServerProperties.getSyncGroupMemberFilter());	if (memberAttributeValue != null && syncMemberFilter != null) {	
use custom filter for getting member group with default basedn 

protected LdapGroupDto getLdapGroupByMemberAttr(String memberAttributeValue) {	Set<LdapGroupDto> filteredLdapGroups;	memberAttributeValue = getUniqueIdByMemberPattern(memberAttributeValue, ldapServerProperties.getSyncGroupMemberReplacePattern());	Filter syncMemberFilter = createCustomMemberFilter(memberAttributeValue, ldapServerProperties.getSyncGroupMemberFilter());	if (memberAttributeValue != null && syncMemberFilter != null) {	filteredLdapGroups = getFilteredLdapGroups(ldapServerProperties.getBaseDN(), syncMemberFilter);	} else if (memberAttributeValue != null && isMemberAttributeBaseDn(memberAttributeValue)) {	
member can be used as basedn 

protected LdapGroupDto getLdapGroupByMemberAttr(String memberAttributeValue) {	Set<LdapGroupDto> filteredLdapGroups;	memberAttributeValue = getUniqueIdByMemberPattern(memberAttributeValue, ldapServerProperties.getSyncGroupMemberReplacePattern());	Filter syncMemberFilter = createCustomMemberFilter(memberAttributeValue, ldapServerProperties.getSyncGroupMemberFilter());	if (memberAttributeValue != null && syncMemberFilter != null) {	filteredLdapGroups = getFilteredLdapGroups(ldapServerProperties.getBaseDN(), syncMemberFilter);	} else if (memberAttributeValue != null && isMemberAttributeBaseDn(memberAttributeValue)) {	Filter filter = new EqualsFilter(OBJECT_CLASS_ATTRIBUTE, ldapServerProperties.getGroupObjectClass());	filteredLdapGroups = getFilteredLdapGroups(memberAttributeValue, filter);	} else {	
member cannot be used as basedn 

protected String getUniqueIdByMemberPattern(String memberAttributeValue, String pattern) {	if (StringUtils.isNotEmpty(memberAttributeValue) && StringUtils.isNotEmpty(pattern)) {	try {	Pattern p = Pattern.compile(pattern);	Matcher m = p.matcher(memberAttributeValue);	
apply replace pattern on membership attribbute value 

protected String getUniqueIdByMemberPattern(String memberAttributeValue, String pattern) {	if (StringUtils.isNotEmpty(memberAttributeValue) && StringUtils.isNotEmpty(pattern)) {	try {	Pattern p = Pattern.compile(pattern);	Matcher m = p.matcher(memberAttributeValue);	if (m.matches()) {	memberAttributeValue = m.replaceAll(MEMBER_ATTRIBUTE_REPLACE_STRING);	
membership attribute value after replace pattern applied 

protected String getUniqueIdByMemberPattern(String memberAttributeValue, String pattern) {	if (StringUtils.isNotEmpty(memberAttributeValue) && StringUtils.isNotEmpty(pattern)) {	try {	Pattern p = Pattern.compile(pattern);	Matcher m = p.matcher(memberAttributeValue);	if (m.matches()) {	memberAttributeValue = m.replaceAll(MEMBER_ATTRIBUTE_REPLACE_STRING);	} else {	
membership attribute value pattern is not matched on 

protected String getUniqueIdByMemberPattern(String memberAttributeValue, String pattern) {	if (StringUtils.isNotEmpty(memberAttributeValue) && StringUtils.isNotEmpty(pattern)) {	try {	Pattern p = Pattern.compile(pattern);	Matcher m = p.matcher(memberAttributeValue);	if (m.matches()) {	memberAttributeValue = m.replaceAll(MEMBER_ATTRIBUTE_REPLACE_STRING);	} else {	}	} catch (Exception e) {	
error during replace memberattribute with pattern 

protected void addLdapGroup(LdapBatchDto batchInfo, Map<String, Group> internalGroupsMap, LdapGroupDto groupDto) {	String groupName = groupDto.getGroupName();	if (internalGroupsMap.containsKey(groupName)) {	final Group group = internalGroupsMap.get(groupName);	if (!group.isLdapGroup()) {	batchInfo.getGroupsToBecomeLdap().add(groupDto);	
convert group to ldap group 

private Set<LdapGroupDto> getFilteredLdapGroups(String baseDn, Filter filter) {	final Set<LdapGroupDto> groups = new HashSet<>();	final LdapTemplate ldapTemplate = loadLdapTemplate();	
ldap group query base dn filter 

private Set<LdapUserDto> getFilteredLdapUsers(String baseDn, Filter filter) {	final Set<LdapUserDto> users = new HashSet<>();	final LdapTemplate ldapTemplate = loadLdapTemplate();	PagedResultsDirContextProcessor processor = createPagingProcessor();	SearchControls searchControls = new SearchControls();	searchControls.setReturningObjFlag(true);	searchControls.setSearchScope(SearchControls.SUBTREE_SCOPE);	LdapUserContextMapper ldapUserContextMapper = new LdapUserContextMapper(ldapServerProperties);	String encodedFilter = filter.encode();	do {	
ldap user query base dn filter 

protected Map<String, User> getInternalUsers() {	final List<User> internalUsers = users.getAllUsers();	final Map<String, User> internalUsersMap = new HashMap<>();	
get all users from ambari server 

protected LdapTemplate loadLdapTemplate() {	final LdapServerProperties properties = configuration .getLdapServerProperties();	if (ldapTemplate == null || !properties.equals(ldapServerProperties)) {	
reloading properties 

ldapContextSource.setPooled(true);	final List<String> ldapUrls = ldapServerProperties.getLdapUrls();	ldapContextSource.setUrls(ldapUrls.toArray(new String[ldapUrls.size()]));	if (!ldapServerProperties.isAnonymousBind()) {	ldapContextSource.setUserDn(ldapServerProperties.getManagerDn());	ldapContextSource.setPassword(ldapServerProperties.getManagerPassword());	}	try {	ldapContextSource.afterPropertiesSet();	} catch (Exception e) {	
ldap context source not loaded 

public Object mapFromContext(Object ctx) {	final DirContextAdapter adapter = (DirContextAdapter) ctx;	final String groupNameAttribute = adapter.getStringAttribute(ldapServerProperties.getGroupNamingAttr());	boolean outOfScope = AmbariLdapUtils.isLdapObjectOutOfScopeFromBaseDn(adapter, ldapServerProperties.getBaseDN());	if (outOfScope) {	
group is out of scope of the base dn it will be skipped 

public Object mapFromContext(Object ctx) {	final DirContextAdapter adapter = (DirContextAdapter) ctx;	final String usernameAttribute = adapter.getStringAttribute(ldapServerProperties.getUsernameAttribute());	final String uidAttribute = adapter.getStringAttribute(UID_ATTRIBUTE);	boolean outOfScope = AmbariLdapUtils.isLdapObjectOutOfScopeFromBaseDn(adapter, ldapServerProperties.getBaseDN());	if (outOfScope) {	
user is out of scope of the base dn it will be skipped 

if (outOfScope) {	return null;	}	if (usernameAttribute != null || uidAttribute != null) {	final LdapUserDto user = new LdapUserDto();	user.setUserName(usernameAttribute != null ? usernameAttribute.toLowerCase() : null);	user.setUid(uidAttribute != null ? uidAttribute.toLowerCase() : null);	user.setDn(adapter.getNameInNamespace().toLowerCase());	return user;	} else {	
ignoring ldap user as it doesn t have required attributes uid and 

========================= ambari sample_2790 =========================

public void init(FilterConfig filterConfig) throws ServletException {	m_timeout = m_configuration.getViewRequestThreadPoolTimeout();	int clientThreadPoolSize = m_configuration.getClientThreadPoolSize();	int viewThreadPoolSize = m_configuration.getViewRequestThreadPoolMaxSize();	int viewSemaphoreCount = clientThreadPoolSize / 2;	if (viewThreadPoolSize > 0) {	viewSemaphoreCount = viewThreadPoolSize;	if (viewThreadPoolSize > clientThreadPoolSize) {	
the number of view processing threads cannot be greater than the rest api client threads 

m_timeout = m_configuration.getViewRequestThreadPoolTimeout();	int clientThreadPoolSize = m_configuration.getClientThreadPoolSize();	int viewThreadPoolSize = m_configuration.getViewRequestThreadPoolMaxSize();	int viewSemaphoreCount = clientThreadPoolSize / 2;	if (viewThreadPoolSize > 0) {	viewSemaphoreCount = viewThreadPoolSize;	if (viewThreadPoolSize > clientThreadPoolSize) {	viewSemaphoreCount = clientThreadPoolSize;	}	}	
ambari views will be able to utilize concurrent rest api threads 

public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException {	if (!(request instanceof HttpServletRequest)) {	chain.doFilter(request, response);	return;	}	HttpServletResponse httpResponse = (HttpServletResponse) response;	boolean acquired = false;	try {	acquired = m_semaphore.tryAcquire(m_timeout, TimeUnit.MILLISECONDS);	} catch (InterruptedException interruptedException) {	
while waiting for an available thread the view request was interrupted 

========================= ambari sample_3928 =========================

public static String getHadoopServiceConfigJSON() {	String fileContent = null;	try {	ClassLoader classLoader = HadoopServiceConfigHelper.class.getClassLoader();	File file = new File(classLoader.getResource("HadoopServiceConfig.json").getFile());	fileContent = FileUtils.readFileToString(file);	} catch (IOException e) {	
unable to read hadoopserviceconfig json 

========================= ambari sample_1446 =========================

public Response getFile(@PathParam("filePath") String filePath, try {	filePath = sanitizeFilePath(filePath);	if (action != null && action.equals("ls")) {	
list directory 

filePath = sanitizeFilePath(filePath);	if (action != null && action.equals("ls")) {	List<String> ls = new LinkedList<String>();	for (FileStatus fs : getHdfsApi().listdir(filePath)) {	ls.add(fs.getPath().toString());	}	JSONObject object = new JSONObject();	object.put("ls", ls);	return Response.ok(object).status(200).build();	}	
reading file 

FileResource file = new FileResource();	file.setFilePath(filePath);	file.setFileContent(paginator.readPage(page));	file.setHasNext(paginator.pageCount() > page + 1);	file.setPage(page);	file.setPageCount(paginator.pageCount());	JSONObject object = new JSONObject();	object.put("file", file);	return Response.ok(object).status(200).build();	} catch (WebApplicationException ex) {	
error occurred 

file.setFileContent(paginator.readPage(page));	file.setHasNext(paginator.pageCount() > page + 1);	file.setPage(page);	file.setPageCount(paginator.pageCount());	JSONObject object = new JSONObject();	object.put("file", file);	return Response.ok(object).status(200).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (FileNotFoundException ex) {	
error occurred 

file.setPage(page);	file.setPageCount(paginator.pageCount());	JSONObject object = new JSONObject();	object.put("file", file);	return Response.ok(object).status(200).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (FileNotFoundException ex) {	throw new NotFoundFormattedException(ex.getMessage(), ex);	} catch (IllegalArgumentException ex) {	
error occurred 

JSONObject object = new JSONObject();	object.put("file", file);	return Response.ok(object).status(200).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (FileNotFoundException ex) {	throw new NotFoundFormattedException(ex.getMessage(), ex);	} catch (IllegalArgumentException ex) {	throw new BadRequestFormattedException(ex.getMessage(), ex);	} catch (Exception ex) {	
error occurred 

public Response deleteFile(@PathParam("filePath") String filePath) throws IOException, InterruptedException {	try {	filePath = sanitizeFilePath(filePath);	
deleting file 

public Response deleteFile(@PathParam("filePath") String filePath) throws IOException, InterruptedException {	try {	filePath = sanitizeFilePath(filePath);	if (getHdfsApi().delete(filePath, false)) {	return Response.status(204).build();	}	throw new NotFoundFormattedException("FileSystem.delete returned false", null);	} catch (WebApplicationException ex) {	
error occurred 

public Response deleteFile(@PathParam("filePath") String filePath) throws IOException, InterruptedException {	try {	filePath = sanitizeFilePath(filePath);	if (getHdfsApi().delete(filePath, false)) {	return Response.status(204).build();	}	throw new NotFoundFormattedException("FileSystem.delete returned false", null);	} catch (WebApplicationException ex) {	throw ex;	} catch (Exception ex) {	
error occurred 

public Response updateFile(FileResourceRequest request, try {	filePath = sanitizeFilePath(filePath);	
rewriting file 

public Response updateFile(FileResourceRequest request, try {	filePath = sanitizeFilePath(filePath);	FSDataOutputStream output = getHdfsApi().create(filePath, true);	output.write(request.file.getFileContent().getBytes("UTF-8"));	output.close();	return Response.status(204).build();	} catch (WebApplicationException ex) {	
error occurred 

public Response updateFile(FileResourceRequest request, try {	filePath = sanitizeFilePath(filePath);	FSDataOutputStream output = getHdfsApi().create(filePath, true);	output.write(request.file.getFileContent().getBytes("UTF-8"));	output.close();	return Response.status(204).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (Exception ex) {	
error occurred 

public Response createFile(FileResourceRequest request, throws IOException, InterruptedException {	try {	
creating file 

if (request.file.getFileContent() != null) {	output.write(request.file.getFileContent().getBytes("UTF-8"));	}	output.close();	} catch (FileAlreadyExistsException ex) {	throw new ServiceFormattedException(ex.getMessage(), ex, 400);	}	response.setHeader("Location", String.format("%s/%s", ui.getAbsolutePath().toString(), request.file.getFilePath()));	return Response.status(204).build();	} catch (WebApplicationException ex) {	
error occurred 

}	output.close();	} catch (FileAlreadyExistsException ex) {	throw new ServiceFormattedException(ex.getMessage(), ex, 400);	}	response.setHeader("Location", String.format("%s/%s", ui.getAbsolutePath().toString(), request.file.getFilePath()));	return Response.status(204).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (Exception ex) {	
error occurred 

try {	Optional<Map<String, String>> props = ViewPropertyHelper.getViewConfigs(context, VIEW_CONF_KEYVALUES);	HdfsApi api;	if(props.isPresent()){	api = HdfsUtil.connectToHDFSApi(context, props.get());	}else{	api = HdfsUtil.connectToHDFSApi(context);	}	api.getStatus();	} catch (WebApplicationException ex) {	
error occurred 

HdfsApi api;	if(props.isPresent()){	api = HdfsUtil.connectToHDFSApi(context, props.get());	}else{	api = HdfsUtil.connectToHDFSApi(context);	}	api.getStatus();	} catch (WebApplicationException ex) {	throw ex;	} catch (Exception ex) {	
error occurred 

========================= ambari sample_1016 =========================

newAclList.add(new ACL(ZooDefs.Perms.READ, new Id("world", "anyone")));	String configsPath = String.format("%s/%s", zNode, "configs");	String collectionsPath = String.format("%s/%s", zNode, "collections");	String aliasesPath = String.format("%s List<String> excludePaths = Arrays.asList(configsPath, collectionsPath, aliasesPath);	createZnodeIfNeeded(configsPath, client.getSolrZkClient());	createZnodeIfNeeded(collectionsPath, client.getSolrZkClient());	AclUtils.setRecursivelyOn(client.getSolrZkClient().getSolrZooKeeper(), zNode, newAclList, excludePaths);	List<ACL> commonConfigAcls = new ArrayList<>();	commonConfigAcls.addAll(saslUserList);	commonConfigAcls.add(new ACL(ZooDefs.Perms.READ | ZooDefs.Perms.CREATE, new Id("world", "anyone")));	
set sasl users for znode 

newAclList.add(new ACL(ZooDefs.Perms.READ, new Id("world", "anyone")));	String configsPath = String.format("%s/%s", zNode, "configs");	String collectionsPath = String.format("%s/%s", zNode, "collections");	String aliasesPath = String.format("%s List<String> excludePaths = Arrays.asList(configsPath, collectionsPath, aliasesPath);	createZnodeIfNeeded(configsPath, client.getSolrZkClient());	createZnodeIfNeeded(collectionsPath, client.getSolrZkClient());	AclUtils.setRecursivelyOn(client.getSolrZkClient().getSolrZooKeeper(), zNode, newAclList, excludePaths);	List<ACL> commonConfigAcls = new ArrayList<>();	commonConfigAcls.addAll(saslUserList);	commonConfigAcls.add(new ACL(ZooDefs.Perms.READ | ZooDefs.Perms.CREATE, new Id("world", "anyone")));	
skip configs and collections 

String collectionsPath = String.format("%s/%s", zNode, "collections");	String aliasesPath = String.format("%s List<String> excludePaths = Arrays.asList(configsPath, collectionsPath, aliasesPath);	createZnodeIfNeeded(configsPath, client.getSolrZkClient());	createZnodeIfNeeded(collectionsPath, client.getSolrZkClient());	AclUtils.setRecursivelyOn(client.getSolrZkClient().getSolrZooKeeper(), zNode, newAclList, excludePaths);	List<ACL> commonConfigAcls = new ArrayList<>();	commonConfigAcls.addAll(saslUserList);	commonConfigAcls.add(new ACL(ZooDefs.Perms.READ | ZooDefs.Perms.CREATE, new Id("world", "anyone")));	solrZooKeeper.setACL(configsPath, AclUtils.mergeAcls(solrZooKeeper.getACL(configsPath, new Stat()), commonConfigAcls), -1);	solrZooKeeper.setACL(collectionsPath, AclUtils.mergeAcls(solrZooKeeper.getACL(collectionsPath, new Stat()), commonConfigAcls), -1);	
set world anyone to cr on configs and collections 

private void createZnodeIfNeeded(String configsPath, SolrZkClient zkClient) throws KeeperException, InterruptedException {	if (!zkClient.exists(configsPath, true)) {	
does not exist creating it 

========================= ambari sample_97 =========================

} else {	result.setResultStatus(new ResultStatus(ResultStatus.STATUS.OK));	}	} else {	result.setResultStatus(new ResultStatus(ResultStatus.STATUS.ACCEPTED));	}	} catch (AuthorizationException e) {	result = new ResultImpl(new ResultStatus(ResultStatus.STATUS.FORBIDDEN, e.getMessage()));	} catch (UnsupportedPropertyException e) {	result = new ResultImpl(new ResultStatus(ResultStatus.STATUS.BAD_REQUEST, e.getMessage()));	
bad request received 

result.setResultStatus(new ResultStatus(ResultStatus.STATUS.ACCEPTED));	}	} catch (AuthorizationException e) {	result = new ResultImpl(new ResultStatus(ResultStatus.STATUS.FORBIDDEN, e.getMessage()));	} catch (UnsupportedPropertyException e) {	result = new ResultImpl(new ResultStatus(ResultStatus.STATUS.BAD_REQUEST, e.getMessage()));	} catch (NoSuchParentResourceException e) {	result = new ResultImpl(new ResultStatus(ResultStatus.STATUS.NOT_FOUND, e.getMessage()));	} catch (SystemException e) {	if (LOG.isErrorEnabled()) {	
caught a system exception while attempting to create a resource 

result = new ResultImpl(new ResultStatus(ResultStatus.STATUS.BAD_REQUEST, e.getMessage()));	} catch (NoSuchParentResourceException e) {	result = new ResultImpl(new ResultStatus(ResultStatus.STATUS.NOT_FOUND, e.getMessage()));	} catch (SystemException e) {	if (LOG.isErrorEnabled()) {	}	result = new ResultImpl(new ResultStatus(ResultStatus.STATUS.SERVER_ERROR, e.getMessage()));	} catch (ResourceAlreadyExistsException e) {	result = new ResultImpl(new ResultStatus(ResultStatus.STATUS.CONFLICT, e.getMessage()));	} catch(IllegalArgumentException e) {	
bad request received 

} catch (SystemException e) {	if (LOG.isErrorEnabled()) {	}	result = new ResultImpl(new ResultStatus(ResultStatus.STATUS.SERVER_ERROR, e.getMessage()));	} catch (ResourceAlreadyExistsException e) {	result = new ResultImpl(new ResultStatus(ResultStatus.STATUS.CONFLICT, e.getMessage()));	} catch(IllegalArgumentException e) {	result = new ResultImpl(new ResultStatus(ResultStatus.STATUS.BAD_REQUEST, e.getMessage()));	} catch (RuntimeException e) {	if (LOG.isErrorEnabled()) {	
caught a runtime exception while attempting to create a resource 

========================= ambari sample_4097 =========================

public Object createEntry(Object key) throws Exception {	LOG.debug("Creating cache entry since none exists, key = {}", key);	TimelineAppMetricCacheKey metricCacheKey = (TimelineAppMetricCacheKey) key;	TimelineMetrics timelineMetrics = null;	try {	URIBuilder uriBuilder = new URIBuilder(metricCacheKey.getSpec());	timelineMetrics = requestHelperForGets.fetchTimelineMetrics(uriBuilder, metricCacheKey.getTemporalInfo().getStartTimeMillis(), metricCacheKey.getTemporalInfo().getEndTimeMillis());	} catch (IOException io) {	
caught ioexception on fetching metrics 

TimelineMetrics timelineMetrics = null;	try {	URIBuilder uriBuilder = new URIBuilder(metricCacheKey.getSpec());	timelineMetrics = requestHelperForGets.fetchTimelineMetrics(uriBuilder, metricCacheKey.getTemporalInfo().getStartTimeMillis(), metricCacheKey.getTemporalInfo().getEndTimeMillis());	} catch (IOException io) {	throw io;	}	TimelineMetricsCacheValue value = null;	if (timelineMetrics != null && !timelineMetrics.getMetrics().isEmpty()) {	value = new TimelineMetricsCacheValue( metricCacheKey.getTemporalInfo().getStartTime(), metricCacheKey.getTemporalInfo().getEndTime(), timelineMetrics, Precision.getPrecision(metricCacheKey.getTemporalInfo().getStartTimeMillis(), metricCacheKey.getTemporalInfo().getEndTimeMillis()) );	
created cache entry 

Long existingSeriesEndTime = existingMetrics.getEndTime();	TemporalInfo newTemporalInfo = metricCacheKey.getTemporalInfo();	Long requestedStartTime = newTemporalInfo.getStartTimeMillis();	Long requestedEndTime = newTemporalInfo.getEndTimeMillis();	URIBuilder uriBuilder = new URIBuilder(metricCacheKey.getSpec());	Precision requestedPrecision = Precision.getPrecision(requestedStartTime, requestedEndTime);	Precision currentPrecision = existingMetrics.getPrecision();	Long newStartTime = null;	Long newEndTime = null;	if(!requestedPrecision.equals(currentPrecision)) {	
precision changed from to 

Long requestedEndTime = newTemporalInfo.getEndTimeMillis();	URIBuilder uriBuilder = new URIBuilder(metricCacheKey.getSpec());	Precision requestedPrecision = Precision.getPrecision(requestedStartTime, requestedEndTime);	Precision currentPrecision = existingMetrics.getPrecision();	Long newStartTime = null;	Long newEndTime = null;	if(!requestedPrecision.equals(currentPrecision)) {	newStartTime = requestedStartTime;	newEndTime = requestedEndTime;	} else {	
no change in precision 

uriBuilder.setParameter("endTime", String.valueOf(newEndTime));	uriBuilder.setParameter("precision",requestedPrecision.toString());	try {	TimelineMetrics newTimeSeries = requestHelperForUpdates.fetchTimelineMetrics(uriBuilder, newStartTime, newEndTime);	updateTimelineMetricsInCache(newTimeSeries, existingMetrics, getMillisecondsTime(requestedStartTime), getMillisecondsTime(requestedEndTime), !currentPrecision.equals(requestedPrecision));	existingMetrics.setStartTime(requestedStartTime);	existingMetrics.setEndTime(requestedEndTime);	existingMetrics.setPrecision(requestedPrecision);	} catch (IOException io) {	if (LOG.isDebugEnabled()) {	
exception retrieving metrics 

private void updateExistingMetricValues(TimelineMetrics existingMetrics, Long requestedStartTime, Long requestedEndTime, boolean removeAll) {	for (TimelineMetric existingMetric : existingMetrics.getMetrics()) {	if (removeAll) {	existingMetric.setMetricValues(new TreeMap<>());	} else {	TreeMap<Long, Double> existingMetricValues = existingMetric.getMetricValues();	
existing metric 

========================= ambari sample_3403 =========================

public void init(MetricsConfiguration configuration, MetricsSink sink) {	super.init(configuration, sink);	registerAll(JVM_PREFIX + ".gc", new GarbageCollectorMetricSet(), registry);	registerAll(JVM_PREFIX + ".buffers", new BufferPoolMetricSet(ManagementFactory.getPlatformMBeanServer()), registry);	registerAll(JVM_PREFIX + ".memory", new MemoryUsageGaugeSet(), registry);	registerAll(JVM_PREFIX + ".threads", new ThreadStatesGaugeSet(), registry);	registry.register(JVM_PREFIX + ".file.open.descriptor.ratio", new FileDescriptorRatioGauge());	interval = Integer.parseInt(configuration.getProperty("interval", "10"));	
initialized jvm metrics source 

public void start() {	try {	executor.scheduleWithFixedDelay(new Runnable() {	public void run() {	try {	
publishing jvm metrics to sink 

public void start() {	try {	executor.scheduleWithFixedDelay(new Runnable() {	public void run() {	try {	sink.publish(getMetrics());	} catch (Exception e) {	
error in publishing jvm metrics to sink 

public void start() {	try {	executor.scheduleWithFixedDelay(new Runnable() {	public void run() {	try {	sink.publish(getMetrics());	} catch (Exception e) {	}	}	}, interval, interval, TimeUnit.SECONDS);	
started jvm metrics source 

try {	executor.scheduleWithFixedDelay(new Runnable() {	public void run() {	try {	sink.publish(getMetrics());	} catch (Exception e) {	}	}	}, interval, interval, TimeUnit.SECONDS);	} catch (Exception e) {	
throwing exception when starting metric source 

========================= ambari sample_2642 =========================

public void write(String block, InputMarker inputMarker){	
ignore log block 

========================= ambari sample_1652 =========================

}	} catch (ObjectNotFoundException e) {	}	}	if (nonStartedState == null || (hiveServerComponentStarted && webHcatComponentStarted && activeHiveMetastoreComponentCount > 0 && (embeddedMysqlComponentExists ? mysqlComponentStarted : true))) {	return State.STARTED;	}	return nonStartedState == null ? State.INSTALLED : nonStartedState;	}	} catch (AmbariException e) {	
can t determine service state 

========================= ambari sample_3680 =========================

private Credential createCredential(Map<String, Object> properties) throws IllegalArgumentException {	String principal;	String key;	if (properties.get(CREDENTIAL_PRINCIPAL_PROPERTY_ID) == null) {	throw new IllegalArgumentException("Property " + CREDENTIAL_PRINCIPAL_PROPERTY_ID + " must be provided");	} else {	principal = String.valueOf(properties.get(CREDENTIAL_PRINCIPAL_PROPERTY_ID));	}	if (properties.get(CREDENTIAL_KEY_PROPERTY_ID) == null) {	
the credential is being added without a key 

========================= ambari sample_3511 =========================

configuration.set("timeline.metrics.collector.hosts", collectorHosts);	configuration.set("timeline.metrics.hostname", hostname);	configuration.set("timeline.metrics.zk.quorum", getZkQuorumFromConfiguration());	this.aggregationInterval = configuration.getInt("timeline.metrics.host.aggregator.minute.interval", 300);	this.rawPublishingInterval = configuration.getInt("timeline.metrics.sink.report.interval", 60);	this.webApplicationPort = configuration.getInt("timeline.metrics.host.inmemory.aggregation.port", 61888);	this.timelineMetricsHolder = TimelineMetricsHolder.getInstance(rawPublishingInterval, aggregationInterval);	try {	this.httpServer = createHttpServer();	} catch (IOException e) {	
exception while starting http server exiting 

protected void initConfiguration() {	ClassLoader classLoader = Thread.currentThread().getContextClassLoader();	if (classLoader == null) {	classLoader = getClass().getClassLoader();	}	URL amsResUrl = classLoader.getResource(METRICS_SITE_CONFIGURATION_FILE);	
found metric service configuration 

if (classLoader == null) {	classLoader = getClass().getClassLoader();	}	URL amsResUrl = classLoader.getResource(METRICS_SITE_CONFIGURATION_FILE);	if (amsResUrl == null) {	throw new IllegalStateException("Unable to initialize the metrics " + "subsystem. No ams-site present in the classpath.");	}	try {	configuration.addResource(amsResUrl.toURI().toURL());	} catch (Exception e) {	
couldn t init configuration 

private void startWebServer() {	
starting web server 

private void startAggregatePublisherThread() {	
starting aggregated metrics publisher 

private void startRawPublisherThread() {	
starting raw metrics publisher 

private void stop() {	
stopping aggregator application 

private void stop() {	aggregatePublisherThread.interrupt();	rawPublisherThread.interrupt();	httpServer.stop(STOP_SECONDS_DELAY);	
stopped web server 

private void stop() {	aggregatePublisherThread.interrupt();	rawPublisherThread.interrupt();	httpServer.stop(STOP_SECONDS_DELAY);	try {	
waiting for threads to join 

private void stop() {	aggregatePublisherThread.interrupt();	rawPublisherThread.interrupt();	httpServer.stop(STOP_SECONDS_DELAY);	try {	aggregatePublisherThread.join(JOIN_SECONDS_TIMEOUT * 1000);	rawPublisherThread.join(JOIN_SECONDS_TIMEOUT * 1000);	
gracefully stopped aggregator application 

private void stop() {	aggregatePublisherThread.interrupt();	rawPublisherThread.interrupt();	httpServer.stop(STOP_SECONDS_DELAY);	try {	aggregatePublisherThread.join(JOIN_SECONDS_TIMEOUT * 1000);	rawPublisherThread.join(JOIN_SECONDS_TIMEOUT * 1000);	} catch (InterruptedException e) {	
received exception during stop 

private void startWebServerAndPublishersThreads() {	
starting aggregator application 

========================= ambari sample_277 =========================

configGroupId = configGroupEntity.getGroupId();	configGroupName = configGroupEntity.getGroupName();	serviceName = configGroupEntity.getServiceName();	m_configurations = new ConcurrentHashMap<>();	m_hosts = new ConcurrentHashMap<>();	for (ConfigGroupConfigMappingEntity configMappingEntity : configGroupEntity.getConfigGroupConfigMappingEntities()) {	Config config = cluster.getConfig(configMappingEntity.getConfigType(), configMappingEntity.getVersionTag());	if (config != null) {	m_configurations.put(config.getType(), config);	} else {	
unable to find config mapping for config group in cluster 

}	}	for (ConfigGroupHostMappingEntity hostMappingEntity : configGroupEntity.getConfigGroupHostMappingEntities()) {	try {	Host host = clusters.getHost(hostMappingEntity.getHostname());	HostEntity hostEntity = hostMappingEntity.getHostEntity();	if (host != null && hostEntity != null) {	m_hosts.put(hostEntity.getHostId(), host);	}	} catch (Exception e) {	
host seems to be deleted but config group mapping still exists 

}	}	for (ConfigGroupHostMappingEntity hostMappingEntity : configGroupEntity.getConfigGroupHostMappingEntities()) {	try {	Host host = clusters.getHost(hostMappingEntity.getHostname());	HostEntity hostEntity = hostMappingEntity.getHostEntity();	if (host != null && hostEntity != null) {	m_hosts.put(hostEntity.getHostId(), host);	}	} catch (Exception e) {	
host seems to be deleted but config group mapping still exists 

Host host = m_hosts.get(hostId);	if (null == host) {	return;	}	String hostName = host.getHostName();	LOG.info("Removing host (id={}, name={}) from config group", host.getHostId(), hostName);	try {	removeConfigGroupHostEntity(host);	m_hosts.remove(hostId);	} catch (Exception e) {	
failed to delete config group host mapping for cluster and host 

========================= ambari sample_3287 =========================

public void createExtensionLinks(StackManager stackManager, List<ExtensionInfo> extensions) throws AmbariException {	Map<String, List<StackInfo>> stackMap = stackManager.getStacksByName();	for (List<StackInfo> stacks : stackMap.values()) {	Collections.sort(stacks);	Collections.reverse(stacks);	}	Collections.sort(extensions);	Collections.reverse(extensions);	for (ExtensionInfo extension : extensions) {	if (extension.isActive() && extension.isAutoLink()) {	
autolink looking for matching stack versions for extension 

Collections.reverse(stacks);	}	Collections.sort(extensions);	Collections.reverse(extensions);	for (ExtensionInfo extension : extensions) {	if (extension.isActive() && extension.isAutoLink()) {	for (ExtensionMetainfoXml.Stack supportedStack : extension.getStacks()) {	List<StackInfo> stacks = stackMap.get(supportedStack.getName());	for (StackInfo stack : stacks) {	if (stack.getExtension(extension.getName()) == null && VersionUtils.compareVersions(stack.getVersion(), supportedStack.getVersion()) > -1) {	
autolink extension stack 

Collections.reverse(extensions);	for (ExtensionInfo extension : extensions) {	if (extension.isActive() && extension.isAutoLink()) {	for (ExtensionMetainfoXml.Stack supportedStack : extension.getStacks()) {	List<StackInfo> stacks = stackMap.get(supportedStack.getName());	for (StackInfo stack : stacks) {	if (stack.getExtension(extension.getName()) == null && VersionUtils.compareVersions(stack.getVersion(), supportedStack.getVersion()) > -1) {	createExtensionLink(stackManager, stack, extension);	}	else {	
autolink not a match extension stack 

for (StackInfo stack : stacks) {	if (stack.getExtension(extension.getName()) == null && VersionUtils.compareVersions(stack.getVersion(), supportedStack.getVersion()) > -1) {	createExtensionLink(stackManager, stack, extension);	}	else {	}	}	}	}	else {	
autolink skipping extension it is either not active or set to autolink 

========================= ambari sample_3442 =========================

public void run() {	while (true) {	try {	Thread.sleep(30 * 1000);	} catch (Throwable t) {	}	try {	logStats();	} catch (Throwable t) {	
logstats caught exception while logging stats 

========================= ambari sample_1613 =========================

public Swagger read(Set<Class<?>> classes) {	
looking for nested api s 

public Swagger read(Set<Class<?>> classes) {	for (Class<?> cls: classes) {	
examining api 

for (Class<?> cls: classes) {	for (Method method: cls.getMethods()) {	Path methodPath = AnnotationUtils.findAnnotation(method, Path.class);	if (null != methodPath) {	Class<?> returnType = method.getReturnType();	Api nestedApi = AnnotationUtils.findAnnotation(returnType, Api.class);	Path nestedApiPath = AnnotationUtils.findAnnotation(returnType, Path.class);	logger.debug("Examinig API method {}#{}, path={}, returnType={}", cls.getSimpleName(), method.getName(), nestedApiPath != null ? nestedApiPath.value() : null, returnType.getSimpleName());	if (null != nestedApi) {	if (null != nestedApiPath) {	
this class exists both as top level and nested api treating it as top level api 

Api nestedApi = AnnotationUtils.findAnnotation(returnType, Api.class);	Path nestedApiPath = AnnotationUtils.findAnnotation(returnType, Path.class);	logger.debug("Examinig API method {}#{}, path={}, returnType={}", cls.getSimpleName(), method.getName(), nestedApiPath != null ? nestedApiPath.value() : null, returnType.getSimpleName());	if (null != nestedApi) {	if (null != nestedApiPath) {	}	else {	Path apiPath = AnnotationUtils.findAnnotation(cls, Path.class);	String apiPathValue;	if (null == apiPath) {	
parent api also seems to be a nested api the current version does not support multi level nesting 

Path apiPath = AnnotationUtils.findAnnotation(cls, Path.class);	String apiPathValue;	if (null == apiPath) {	apiPathValue = "";	}	else {	apiPathValue = apiPath.value();	}	NestedApiRecord nar = new NestedApiRecord(returnType, cls, apiPathValue, method, methodPath.value());	if (nestedAPIs.containsKey(returnType)) {	
is a nested api of multiple top level api s ignoring top level api 

if (null == apiPath) {	apiPathValue = "";	}	else {	apiPathValue = apiPath.value();	}	NestedApiRecord nar = new NestedApiRecord(returnType, cls, apiPathValue, method, methodPath.value());	if (nestedAPIs.containsKey(returnType)) {	}	else {	
registering nested api 

if (nestedAPIs.containsKey(returnType)) {	}	else {	nestedAPIs.put(returnType, nar);	}	}	}	}	}	}	
found nested api s 

protected Swagger read(Class<?> cls, String parentPath, String parentMethod, boolean readHidden, String[] parentConsumes, String[] parentProduces, Map<String, Tag> parentTags, List<Parameter> parentParameters) {	NestedApiRecord nestedApiRecord = nestedAPIs.get(cls);	if (null != nestedApiRecord) {	
processing nested api 

protected Swagger read(Class<?> cls, String parentPath, String parentMethod, boolean readHidden, String[] parentConsumes, String[] parentProduces, Map<String, Tag> parentTags, List<Parameter> parentParameters) {	NestedApiRecord nestedApiRecord = nestedAPIs.get(cls);	if (null != nestedApiRecord) {	Operation operation = parseMethod(nestedApiRecord.parentMethod);	List<Parameter> pathParameters = ImmutableList.copyOf( Collections2.filter(operation.getParameters(), Predicates.instanceOf(PathParameter.class)));	
will copy path params from parent method 

protected Swagger read(Class<?> cls, String parentPath, String parentMethod, boolean readHidden, String[] parentConsumes, String[] parentProduces, Map<String, Tag> parentTags, List<Parameter> parentParameters) {	NestedApiRecord nestedApiRecord = nestedAPIs.get(cls);	if (null != nestedApiRecord) {	Operation operation = parseMethod(nestedApiRecord.parentMethod);	List<Parameter> pathParameters = ImmutableList.copyOf( Collections2.filter(operation.getParameters(), Predicates.instanceOf(PathParameter.class)));	return super.read(cls, joinPaths(nestedApiRecord.parentApiPath, nestedApiRecord.parentMethodPath, parentPath), parentMethod, readHidden, parentConsumes, parentProduces, parentTags, pathParameters);	}	else {	
processing top level api 

========================= ambari sample_1306 =========================

metricList.add(cachedMetric);	}	}	}	if (!metricList.isEmpty()) {	TimelineMetrics timelineMetrics = new TimelineMetrics();	timelineMetrics.setMetrics(metricList);	try {	emitMetrics(timelineMetrics);	} catch (UnableToConnectException uce) {	
unable to send metrics to collector by address 

public void cleanup() {	
stopping storm metrics sink 

private List<DataPoint> populateDataPoints(DataPoint dataPoint) {	List<DataPoint> dataPoints = new ArrayList<>();	if (dataPoint.value == null) {	
data point with name is null discarding 

private Double convertValueToDouble(String metricName, Object value) {	if (value instanceof Number) {	return ((Number) value).doubleValue();	} else if (value instanceof String) {	try {	return Double.parseDouble((String) value);	} catch (NumberFormatException e) {	
data point with name doesn t have number format value discarding 

private Double convertValueToDouble(String metricName, Object value) {	if (value instanceof Number) {	return ((Number) value).doubleValue();	} else if (value instanceof String) {	try {	return Double.parseDouble((String) value);	} catch (NumberFormatException e) {	}	return null;	} else {	
data point with name has value which is not supported discarding 

========================= ambari sample_261 =========================

Entry entry = (Entry)obj;	String typeInfo = entry.getValue();	List<String> typePrecisionScale = ParserUtils.parseColumnDataType(typeInfo);	String datatype = typePrecisionScale.get(0);	String precisionString = typePrecisionScale.get(1);	String scaleString = typePrecisionScale.get(2);	Integer precision = !Strings.isNullOrEmpty(precisionString) ? Integer.valueOf(precisionString.trim()): null;	Integer scale = !Strings.isNullOrEmpty(scaleString) ? Integer.valueOf(scaleString.trim()): null;	ColumnInfo columnInfo = new ColumnInfo(entry.getName(), datatype, precision, scale, entry.getComment());	columns.add(columnInfo);	
found partition column definition 

========================= ambari sample_631 =========================

if (!metaInfDir.mkdir()) {	msg = "Could not create archive META-INF directory.";	view.setStatusDetail(msg);	LOG.error(msg);	throw new ExtractionException(msg);	}	JarEntry jarEntry;	while ((jarEntry = jarInputStream.getNextJarEntry())!= null){	try {	String   entryPath = archivePath + File.separator + jarEntry.getName();	
extracting 

view.setStatusDetail(msg);	LOG.error(msg);	throw new ExtractionException(msg);	}	JarEntry jarEntry;	while ((jarEntry = jarInputStream.getNextJarEntry())!= null){	try {	String   entryPath = archivePath + File.separator + jarEntry.getName();	File entryFile = archiveUtility.getFile(entryPath);	if (jarEntry.isDirectory()) {	
making directory 

if (jarEntry.isDirectory()) {	if (!entryFile.mkdir()) {	msg = "Could not create archive entry directory " + entryPath + ".";	view.setStatusDetail(msg);	LOG.error(msg);	throw new ExtractionException(msg);	}	} else {	FileOutputStream fos = archiveUtility.getFileOutputStream(entryFile);	try {	
begin copying from to 

throw new ExtractionException(msg);	}	} else {	FileOutputStream fos = archiveUtility.getFileOutputStream(entryFile);	try {	byte[] buffer = new byte[BUFFER_SIZE];	int n;	while((n = jarInputStream.read(buffer)) > -1) {	fos.write(buffer, 0, n);	}	
finish copying from to 

========================= ambari sample_3946 =========================

protected void initDockerContainer() throws Exception{	boolean logsearchStarted = StoryDataRegistry.INSTANCE.isLogsearchContainerStarted();	if (!logsearchStarted) {	
create new docker container for log search 

protected void initDockerContainer() throws Exception{	boolean logsearchStarted = StoryDataRegistry.INSTANCE.isLogsearchContainerStarted();	if (!logsearchStarted) {	URL location = LogSearchDockerSteps.class.getProtectionDomain().getCodeSource().getLocation();	String ambariFolder = new File(location.toURI()).getParentFile().getParentFile().getParentFile().getParent();	StoryDataRegistry.INSTANCE.setAmbariFolder(ambariFolder);	String shellScriptLocation = ambariFolder + "/ambari-logsearch/docker/logsearch-docker.sh";	StoryDataRegistry.INSTANCE.setShellScriptLocation(shellScriptLocation);	String output = runCommand(new String[]{StoryDataRegistry.INSTANCE.getShellScriptLocation(), "start"});	
command output 

StoryDataRegistry.INSTANCE.setAmbariFolder(ambariFolder);	String shellScriptLocation = ambariFolder + "/ambari-logsearch/docker/logsearch-docker.sh";	StoryDataRegistry.INSTANCE.setShellScriptLocation(shellScriptLocation);	String output = runCommand(new String[]{StoryDataRegistry.INSTANCE.getShellScriptLocation(), "start"});	StoryDataRegistry.INSTANCE.setLogsearchContainerStarted(true);	String dockerHostFromUri = System.getProperty("docker.host") != null ? System.getProperty("docker.host") : "localhost";;	StoryDataRegistry.INSTANCE.setDockerHost(dockerHostFromUri);	checkHostAndPortReachable(dockerHostFromUri, StoryDataRegistry.INSTANCE.getLogsearchPort(), "LogSearch");	waitUntilSolrIsUp();	waitUntilSolrHasAnyData();	
waiting for logfeeder to finish the test log parsings sec 

private void waitUntilSolrIsUp() throws Exception {	int maxTries = 30;	boolean solrIsUp = false;	String lastExceptionMessage = null;	for (int tries = 1; tries < maxTries; tries++) {	try {	SolrClient solrClient = new LBHttpSolrClient(String.format("http: StoryDataRegistry.INSTANCE.getDockerHost(), StoryDataRegistry.INSTANCE.getSolrPort(), StoryDataRegistry.INSTANCE.getServiceLogsCollection()));	StoryDataRegistry.INSTANCE.setSolrClient(solrClient);	SolrPingResponse pingResponse = solrClient.ping();	if (pingResponse.getStatus() != 0) {	
solr is not up yet retrying tries 

String lastExceptionMessage = null;	for (int tries = 1; tries < maxTries; tries++) {	try {	SolrClient solrClient = new LBHttpSolrClient(String.format("http: StoryDataRegistry.INSTANCE.getDockerHost(), StoryDataRegistry.INSTANCE.getSolrPort(), StoryDataRegistry.INSTANCE.getServiceLogsCollection()));	StoryDataRegistry.INSTANCE.setSolrClient(solrClient);	SolrPingResponse pingResponse = solrClient.ping();	if (pingResponse.getStatus() != 0) {	Thread.sleep(2000);	} else {	solrIsUp = true;	
solr is up and running 

SolrClient solrClient = new LBHttpSolrClient(String.format("http: StoryDataRegistry.INSTANCE.getDockerHost(), StoryDataRegistry.INSTANCE.getSolrPort(), StoryDataRegistry.INSTANCE.getServiceLogsCollection()));	StoryDataRegistry.INSTANCE.setSolrClient(solrClient);	SolrPingResponse pingResponse = solrClient.ping();	if (pingResponse.getStatus() != 0) {	Thread.sleep(2000);	} else {	solrIsUp = true;	break;	}	} catch (Exception e) {	
error occurred during pinging solr retrying tries 

SolrClient solrClient = StoryDataRegistry.INSTANCE.getSolrClient();	SolrQuery solrQuery = new SolrQuery();	solrQuery.setQuery("*:*");	QueryResponse queryResponse = solrClient.query(solrQuery);	SolrDocumentList list = queryResponse.getResults();	if (list.size() > 0) {	solrHasData = true;	break;	} else {	Thread.sleep(2000);	
solr has no data yet retrying tries 

solrQuery.setQuery("*:*");	QueryResponse queryResponse = solrClient.query(solrQuery);	SolrDocumentList list = queryResponse.getResults();	if (list.size() > 0) {	solrHasData = true;	break;	} else {	Thread.sleep(2000);	}	} catch (Exception e) {	
error occurred during checking solr retrying tries 

protected void checkHostAndPortReachable(String host, int port, String serviceName) throws InterruptedException {	boolean reachable = false;	int maxTries = 60;	for (int tries = 1; tries < maxTries; tries++ ) {	try (Socket socket = new Socket()) {	socket.connect(new InetSocketAddress(host, port), 1000);	reachable = true;	break;	} catch (IOException e) {	Thread.sleep(2000);	
is not reachable yet retrying tries 

protected String runCommand(String[] command) {	try {	
exec command 

========================= ambari sample_1582 =========================

solrQuery.setQuery("*:*");	solrQuery.addFilterQuery(FILTER_NAME + ":" + filterName);	solrQuery.addFilterQuery(USER_NAME + ":" + LogSearchContext.getCurrentUsername());	SolrUtil.setRowCount(solrQuery, 0);	try {	Long numFound = eventHistorySolrDao.process(solrQuery).getResults().getNumFound();	if (numFound > 0) {	return true;	}	} catch (SolrException e) {	
error while checking if event history data is unique 

QueryResponse queryResponse = eventHistorySolrDao.process(userListQuery);	if (queryResponse == null) {	return userList;	}	List<Count> counList = queryResponse.getFacetField(USER_NAME).getValues();	for (Count cnt : counList) {	String userName = cnt.getName();	userList.add(userName);	}	} catch (SolrException e) {	
error getting all users 

========================= ambari sample_1428 =========================

public HdfsApi getApi() {	if (_api == null) {	try {	if(this.customProperties != null){	_api = HdfsUtil.connectToHDFSApi(context, customProperties);	}else{	_api = HdfsUtil.connectToHDFSApi(context);	}	} catch (Exception ex) {	
exception while connecting to hdfs 

private static Map<String, String> getHdfsAuthParams(ViewContext context) {	String auth = context.getProperties().get("webhdfs.auth");	Map<String, String> params = new HashMap<String, String>();	if (auth == null || auth.isEmpty()) {	auth = "auth=SIMPLE";	}	for(String param : auth.split(";")) {	String[] keyvalue = param.split("=");	if (keyvalue.length != 2) {	
can not parse authentication param in 

========================= ambari sample_1097 =========================

hi.setHostName(DummyHostname1);	hi.setOS(DummyOsType);	reg.setHostname(DummyHostname1);	reg.setHardwareProfile(hi);	reg.setAgentVersion("");	reg.setPrefix(Configuration.PREFIX_DIR);	try {	handler.handleRegistration(reg);	fail ("Expected failure for non compatible agent version");	} catch (AmbariException e) {	
error 

handler.handleRegistration(reg);	fail ("Expected failure for non compatible agent version");	} catch (AmbariException e) {	Assert.assertTrue(e.getMessage().contains( "Cannot register host with non compatible agent version"));	}	reg.setAgentVersion(null);	try {	handler.handleRegistration(reg);	fail ("Expected failure for non compatible agent version");	} catch (AmbariException e) {	
error 

========================= ambari sample_1994 =========================

if (addMessageMD5) {	jsonObj.put("message_md5", "" + MurmurHash.hash64A(logMessage.getBytes(), 31174077));	}	}	if (logLevelFilterHandler.isAllowed(jsonObj, inputMarker) && !outputLineFilter.apply(jsonObj, inputMarker.getInput())) {	List<? extends Output> outputList = input.getOutputList();	for (Output output : outputList) {	try {	output.write(jsonObj, inputMarker);	} catch (Exception e) {	
error writing to 

public void write(String jsonBlock, InputMarker inputMarker) {	if (logLevelFilterHandler.isAllowed(jsonBlock, inputMarker)) {	List<? extends Output> outputList = inputMarker.getInput().getOutputList();	for (Output output : outputList) {	try {	output.write(jsonBlock, inputMarker);	} catch (Exception e) {	
error writing to 

public void copyFile(File inputFile, InputMarker inputMarker) {	Input input = inputMarker.getInput();	List<? extends Output> outputList = input.getOutputList();	for (Output output : outputList) {	try {	output.copyFile(inputFile, inputMarker);	}catch (Exception e) {	
error coyping file to 

public void close() {	
close called for outputs 

}	}	int iterations = 30;	int waitTimeMS = 1000;	for (int i = 0; i < iterations; i++) {	boolean allClosed = true;	for (Output output : outputs) {	if (!output.isClosed()) {	try {	allClosed = false;	
waiting for output to close more seconds 

Thread.sleep(waitTimeMS);	} catch (Throwable t) {	}	}	}	if (allClosed) {	LOG.info("All outputs are closed. Iterations=" + i);	return;	}	}	
some outpus were not closed after iterations 

========================= ambari sample_1644 =========================

public Configuration(String configFile) {	properties = new Properties();	InputStream inputStream = Configuration.class.getResourceAsStream(configFile);	if (inputStream == null) {	throw new IllegalArgumentException(configFile + " not found in classpath");	}	try {	properties.load(inputStream);	inputStream.close();	} catch (FileNotFoundException fnf) {	
no configuration file found in classpath 

========================= ambari sample_236 =========================

public HeartBeatResponse handleHeartBeat(HeartBeat heartbeat) throws AmbariException {	long now = System.currentTimeMillis();	if (heartbeat.getAgentEnv() != null && heartbeat.getAgentEnv().getHostHealth() != null) {	heartbeat.getAgentEnv().getHostHealth().setServerTimeStampAtReporting(now);	}	String hostname = heartbeat.getHostname();	Long currentResponseId = hostResponseIds.get(hostname);	HeartBeatResponse response;	if (currentResponseId == null) {	
currentresponseid unknown for send register command 

} else if (heartbeat.getResponseId() != currentResponseId) {	LOG.error("Error in responseId sequence - received responseId={} from host {} - sending agent restart command with responseId={}", heartbeat.getResponseId(), hostname, currentResponseId);	return createRestartCommand(currentResponseId);	}	response = new HeartBeatResponse();	response.setResponseId(++currentResponseId);	Host hostObject;	try {	hostObject = clusterFsm.getHost(hostname);	} catch (HostNotFoundException e) {	
host not found agent is still heartbeating 

LOG.error("Error in responseId sequence - received responseId={} from host {} - sending agent restart command with responseId={}", heartbeat.getResponseId(), hostname, currentResponseId);	return createRestartCommand(currentResponseId);	}	response = new HeartBeatResponse();	response.setResponseId(++currentResponseId);	Host hostObject;	try {	hostObject = clusterFsm.getHost(hostname);	} catch (HostNotFoundException e) {	if (LOG.isDebugEnabled()) {	
host associated with the agent heratbeat might have been deleted 

response.setResponseId(++currentResponseId);	Host hostObject;	try {	hostObject = clusterFsm.getHost(hostname);	} catch (HostNotFoundException e) {	if (LOG.isDebugEnabled()) {	}	return response;	}	if (hostObject.getState().equals(HostState.HEARTBEAT_LOST)) {	
host is in heartbeat lost state sending register command 

}	return response;	}	if (hostObject.getState().equals(HostState.HEARTBEAT_LOST)) {	return createRegisterCommand();	}	hostResponseIds.put(hostname, currentResponseId);	hostResponses.put(hostname, response);	if (heartbeat.componentStatus.size() > 0 && hostObject.getState().equals(HostState.WAITING_FOR_HOST_STATUS_UPDATES)) {	try {	
got component status updates 

}	if (hostObject.getState().equals(HostState.HEARTBEAT_LOST)) {	return createRegisterCommand();	}	hostResponseIds.put(hostname, currentResponseId);	hostResponses.put(hostname, response);	if (heartbeat.componentStatus.size() > 0 && hostObject.getState().equals(HostState.WAITING_FOR_HOST_STATUS_UPDATES)) {	try {	hostObject.handleEvent(new HostStatusUpdatesReceivedEvent(hostname, now));	} catch (InvalidStateTransitionException e) {	
failed to notify the host about component status updates 

RecoveryReport rr = heartbeat.getRecoveryReport();	processRecoveryReport(rr, hostname);	}	try {	if (heartbeat.getNodeStatus().getStatus().equals(HostStatus.Status.HEALTHY)) {	hostObject.handleEvent(new HostHealthyHeartbeatEvent(hostname, now, heartbeat.getAgentEnv(), heartbeat.getMounts()));	} else {	hostObject.handleEvent(new HostUnhealthyHeartbeatEvent(hostname, now, null));	}	} catch (InvalidStateTransitionException ex) {	
asking agent to re register due to 

hostObject.setState(HostState.INIT);	return createRegisterCommand();	}	Set<Cluster> clusters = clusterFsm.getClustersForHost(hostname);	if (clusters.size() > 0) {	String clusterName = clusters.iterator().next().getClusterName();	if (recoveryConfigHelper.isConfigStale(clusterName, hostname, heartbeat.getRecoveryTimestamp())) {	RecoveryConfig rc = recoveryConfigHelper.getRecoveryConfig(clusterName, hostname);	response.setRecoveryConfig(rc);	if (response.getRecoveryConfig() != null) {	
recovery configuration set to 

protected void processRecoveryReport(RecoveryReport recoveryReport, String hostname) throws AmbariException {	
received recovery report 

try {	if (LOG.isDebugEnabled()) {	LOG.debug("Sending command string = {}", StageUtils.jaxbToString(ac));	}	} catch (Exception e) {	throw new AmbariException("Could not get jaxb string for command", e);	}	switch (ac.getCommandType()) {	case BACKGROUND_EXECUTION_COMMAND: case EXECUTION_COMMAND: {	ExecutionCommand ec = (ExecutionCommand)ac;	
heartbeathandler sendcommands sending executioncommand for host role rolecommand and command id task id 

} catch (Exception e) {	throw new AmbariException("Could not get jaxb string for command", e);	}	switch (ac.getCommandType()) {	case BACKGROUND_EXECUTION_COMMAND: case EXECUTION_COMMAND: {	ExecutionCommand ec = (ExecutionCommand)ac;	Map<String, String> hlp = ec.getHostLevelParams();	if (hlp != null) {	String customCommand = hlp.get("custom_command");	if (SET_KEYTAB.equalsIgnoreCase(customCommand) || REMOVE_KEYTAB.equalsIgnoreCase(customCommand) || CHECK_KEYTABS.equalsIgnoreCase(customCommand)) {	
s called 

RegistrationResponse response = new RegistrationResponse();	if (cmds.isEmpty()) {	hostObject.handleEvent(new HostStatusUpdatesReceivedEvent(hostname, now));	}	response.setStatusCommands(cmds);	response.setResponseStatus(RegistrationStatus.OK);	List<AlertDefinitionCommand> alertDefinitionCommands = getRegistrationAlertDefinitionCommands(hostname);	response.setAlertDefinitionCommands(alertDefinitionCommands);	response.setAgentConfig(config.getAgentConfigsMap());	if(response.getAgentConfig() != null) {	
agent configuration map set to 

response.setAlertDefinitionCommands(alertDefinitionCommands);	response.setAgentConfig(config.getAgentConfigsMap());	if(response.getAgentConfig() != null) {	}	Set<Cluster> clusters = clusterFsm.getClustersForHost(hostname);	if (clusters.size() > 0) {	String clusterName = clusters.iterator().next().getClusterName();	RecoveryConfig rc = recoveryConfigHelper.getRecoveryConfig(clusterName, hostname);	response.setRecoveryConfig(rc);	if(response.getRecoveryConfig() != null) {	
recovery configuration set to 

private void annotateResponse(String hostname, HeartBeatResponse response) throws AmbariException {	for (Cluster cl : clusterFsm.getClustersForHost(hostname)) {	response.setClusterSize(cl.getClusterSize());	List<ServiceComponentHost> scHosts = cl.getServiceComponentHosts(hostname);	if (scHosts != null && scHosts.size() > 0) {	response.setHasMappedComponents(true);	break;	}	}	if(actionQueue.hasPendingTask(hostname)) {	
host has pending tasks 

========================= ambari sample_2984 =========================

}	}	List<ShortTaskStatus> taskStatuses = trackAction.getTasks();	Assert.assertEquals(5, taskStatuses.size());	boolean foundH1NN = false;	boolean foundH1DN = false;	boolean foundH2DN = false;	boolean foundH1CLT = false;	boolean foundH2CLT = false;	for (ShortTaskStatus taskStatus : taskStatuses) {	
task dump 

Assert.assertEquals(State.INSTALLED, sch.getDesiredState());	} else {	Assert.assertEquals(State.STARTED, sch.getDesiredState());	}	}	}	stages = actionDB.getAllStages(trackAction.getRequestId());	Assert.assertEquals(2, stages.size());	StringBuilder sb = new StringBuilder();	clusters.debugDump(sb);	
cluster dump 

Assert.assertEquals(State.INSTALLED, sch6.getDesiredState());	Assert.assertEquals(State.INSTALLED, sch1.getState());	Assert.assertEquals(State.INSTALLED, sch2.getState());	Assert.assertEquals(State.INSTALLED, sch3.getState());	Assert.assertEquals(State.STARTED, sch4.getState());	Assert.assertEquals(State.INSTALLED, sch5.getState());	Assert.assertEquals(State.INSTALLED, sch6.getState());	long requestId = trackAction.getRequestId();	List<Stage> stages = actionDB.getAllStages(requestId);	for (Stage stage : stages) {	
stage dump 

Assert.assertEquals(State.INSTALLED, sch5.getDesiredState());	Assert.assertEquals(State.STARTED, sch1.getState());	Assert.assertEquals(State.INIT, sch2.getState());	Assert.assertEquals(State.INSTALLED, sch3.getState());	Assert.assertEquals(State.STARTED, sch4.getState());	Assert.assertEquals(State.INIT, sch5.getState());	long requestId = trackAction.getRequestId();	List<Stage> stages = actionDB.getAllStages(requestId);	Assert.assertTrue(!stages.isEmpty());	for (Stage stage : stages) {	
stage dump 

reqs.add(req2);	reqs.add(req3);	reqs.add(req4);	reqs.add(req5);	RequestStatusResponse trackAction = updateHostComponents(reqs, Collections.emptyMap(), true);	Assert.assertNotNull(trackAction);	long requestId = trackAction.getRequestId();	Assert.assertFalse(actionDB.getAllStages(requestId).isEmpty());	List<Stage> stages = actionDB.getAllStages(requestId);	for (Stage stage : stages) {	
stage dump 

stages = new ArrayList<>();	stages.addAll(actionDB.getAllStages(requestId2));	stages.addAll(actionDB.getAllStages(requestId3));	HostRoleCommand hdfsCmdHost3 = null;	HostRoleCommand hdfsCmdHost2 = null;	HostRoleCommand mapRedCmdHost2 = null;	HostRoleCommand mapRedCmdHost3 = null;	for (Stage stage : stages) {	List<HostRoleCommand> hrcs = stage.getOrderedHostRoleCommands();	for (HostRoleCommand hrc : hrcs) {	
role 

for (ServiceComponent sc : clusters.getCluster(cluster1).getService(serviceName) .getServiceComponents().values()) {	Assert.assertEquals(State.INSTALLED, sc.getDesiredState());	Assert.assertTrue(sc.isRecoveryEnabled());	for (ServiceComponentHost sch : sc.getServiceComponentHosts().values()) {	Assert.assertEquals(State.INSTALLED, sch.getDesiredState());	Assert.assertEquals(State.INSTALLED, sch.getState());	}	}	stages = actionDB.getAllStages(trackAction.getRequestId());	for (Stage s : stages) {	
stage dump 

RequestResourceFilter resourceFilter1 = new RequestResourceFilter("HDFS", "DATANODE", hosts);	hosts = new ArrayList<String>() {{ add(host1); }};	RequestResourceFilter resourceFilter2 = new RequestResourceFilter("HDFS", "NAMENODE", hosts);	resourceFilters.add(resourceFilter1);	resourceFilters.add(resourceFilter2);	ExecuteActionRequest actionRequest = new ExecuteActionRequest(cluster1, null, action1, resourceFilters, null, params, false);	RequestStatusResponse response = null;	try {	response = controller.createAction(actionRequest, requestProperties);	} catch (Exception ae) {	
expected exception 

}	Set<HostRequest> requests = new HashSet<>();	requests.clear();	requests.add(new HostRequest(host1, cluster1));	Service s = cluster.getService(serviceName);	s.getServiceComponent("DATANODE").getServiceComponentHost(host1).setState(State.STARTED);	try {	HostResourceProviderTest.deleteHosts(controller, requests, false);	fail("Expect failure deleting hosts when components exist and have not been stopped.");	} catch (Exception e) {	
exception is 

requests.add(new HostRequest(host1, cluster1));	Service s = cluster.getService(serviceName);	s.getServiceComponent("DATANODE").getServiceComponentHost(host1).setState(State.STARTED);	try {	HostResourceProviderTest.deleteHosts(controller, requests, false);	fail("Expect failure deleting hosts when components exist and have not been stopped.");	} catch (Exception e) {	Assert.assertTrue(e.getMessage().contains("these components are not in the removable state:"));	}	DeleteStatusMetaData data = null;	
test dry run of delete with all host components 

fail("Expect failure deleting hosts when components exist and have not been stopped.");	} catch (Exception e) {	Assert.assertTrue(e.getMessage().contains("these components are not in the removable state:"));	}	DeleteStatusMetaData data = null;	s.getServiceComponent("DATANODE").getServiceComponentHost(host1).setState(State.INSTALLED);	try {	data = HostResourceProviderTest.deleteHosts(controller, requests, true);	Assert.assertTrue(data.getDeletedKeys().size() == 1);	} catch (Exception e) {	
exception is 

Assert.assertTrue(e.getMessage().contains("these components are not in the removable state:"));	}	DeleteStatusMetaData data = null;	s.getServiceComponent("DATANODE").getServiceComponentHost(host1).setState(State.INSTALLED);	try {	data = HostResourceProviderTest.deleteHosts(controller, requests, true);	Assert.assertTrue(data.getDeletedKeys().size() == 1);	} catch (Exception e) {	fail("Do not expect failure deleting hosts when components exist and are stopped.");	}	
test successful delete with all host components 

} catch (Exception e) {	fail("Do not expect failure deleting hosts when components exist and are stopped.");	}	s.getServiceComponent("DATANODE").getServiceComponentHost(host1).setState(State.INSTALLED);	try {	data = HostResourceProviderTest.deleteHosts(controller, requests, false);	Assert.assertNotNull(data);	Assert.assertTrue(4 == data.getDeletedKeys().size());	Assert.assertTrue(0 == data.getExceptionForKeys().size());	} catch (Exception e) {	
exception is 

========================= ambari sample_2290 =========================

public void revert() {	IOUtils.closeQuietly(bufferedWriter);	
file was not deleted exists 

========================= ambari sample_164 =========================

public Response getList() {	try {	
getting all savedquery 

========================= ambari sample_534 =========================

public boolean isEnabled(Configuration configuration) {	if (null != m_enabled) {	return m_enabled.booleanValue();	}	m_enabled = Boolean.TRUE;	String property = configuration.getProperty(m_configurationProperty);	if (null != property) {	try {	m_enabled = Boolean.valueOf(property);	} catch (Exception exception) {	
unable to determine if the lock area is enabled defaulting to true 

return m_enabled.booleanValue();	}	m_enabled = Boolean.TRUE;	String property = configuration.getProperty(m_configurationProperty);	if (null != property) {	try {	m_enabled = Boolean.valueOf(property);	} catch (Exception exception) {	}	}	
lockarea is enabled disabled 

========================= ambari sample_4578 =========================

public MigrationModel revertChangeUtility(String instance, String revertDate,String jobid,ViewContext view,MigrationResponse migrationresult) throws IOException, ItemNotFound {	long startTime = System.currentTimeMillis();	final Logger logger = Logger.getLogger(RevertChangeUtility.class);	logger.info("------------------------------");	
reverting the changes start 

public MigrationModel revertChangeUtility(String instance, String revertDate,String jobid,ViewContext view,MigrationResponse migrationresult) throws IOException, ItemNotFound {	long startTime = System.currentTimeMillis();	final Logger logger = Logger.getLogger(RevertChangeUtility.class);	logger.info("------------------------------");	logger.info("------------------------------");	
revert date 

public MigrationModel revertChangeUtility(String instance, String revertDate,String jobid,ViewContext view,MigrationResponse migrationresult) throws IOException, ItemNotFound {	long startTime = System.currentTimeMillis();	final Logger logger = Logger.getLogger(RevertChangeUtility.class);	logger.info("------------------------------");	logger.info("------------------------------");	
instance name 

for (i = 0; i < list.size(); i++) {	float calc = ((float) (i + 1)) / list.size() * 100;	int progressPercentage = Math.round(calc);	migrationresult.setIsNoQuerySelected("yes");	migrationresult.setProgressPercentage(progressPercentage);	migrationresult.setNumberOfQueryTransfered(i+1);	migrationresult.setTotalNoQuery(list.size());	getResourceManager(view).update(migrationresult, jobid);	Element node = (Element) list.get(i);	if (node.getChildText("instance").equals(instance)) {	
instance matched 

float calc = ((float) (i + 1)) / list.size() * 100;	int progressPercentage = Math.round(calc);	migrationresult.setIsNoQuerySelected("yes");	migrationresult.setProgressPercentage(progressPercentage);	migrationresult.setNumberOfQueryTransfered(i+1);	migrationresult.setTotalNoQuery(list.size());	getResourceManager(view).update(migrationresult, jobid);	Element node = (Element) list.get(i);	if (node.getChildText("instance").equals(instance)) {	if (stringtoDatecompare(revertDate, node.getChildText("datetime").toString())) {	
date is less query is sucess 

migrationresult.setNumberOfQueryTransfered(i+1);	migrationresult.setTotalNoQuery(list.size());	getResourceManager(view).update(migrationresult, jobid);	Element node = (Element) list.get(i);	if (node.getChildText("instance").equals(instance)) {	if (stringtoDatecompare(revertDate, node.getChildText("datetime").toString())) {	String sql = node.getChildText("query");	logger.info(sql);	stmt.executeUpdate(sql);	removedir(node.getChildText("dirname").toString(), view.getProperties().get("namenode_URI_Ambari"));	
dirname deleted 

getResourceManager(view).update(migrationresult, jobid);	Element node = (Element) list.get(i);	if (node.getChildText("instance").equals(instance)) {	if (stringtoDatecompare(revertDate, node.getChildText("datetime").toString())) {	String sql = node.getChildText("query");	logger.info(sql);	stmt.executeUpdate(sql);	removedir(node.getChildText("dirname").toString(), view.getProperties().get("namenode_URI_Ambari"));	}	else {	
date is big query is failed 

logger.info(sql);	stmt.executeUpdate(sql);	removedir(node.getChildText("dirname").toString(), view.getProperties().get("namenode_URI_Ambari"));	}	else {	}	}	}	connectionAmbariDatabase.commit();	logger.info("------------------------------");	
reverting the changes end 

removedir(node.getChildText("dirname").toString(), view.getProperties().get("namenode_URI_Ambari"));	}	else {	}	}	}	connectionAmbariDatabase.commit();	logger.info("------------------------------");	logger.info("------------------------------");	} catch (IOException e) {	
ioexception 

}	else {	}	}	}	connectionAmbariDatabase.commit();	logger.info("------------------------------");	logger.info("------------------------------");	} catch (IOException e) {	} catch (ParseException e) {	
parseexception 

else {	}	}	}	connectionAmbariDatabase.commit();	logger.info("------------------------------");	logger.info("------------------------------");	} catch (IOException e) {	} catch (ParseException e) {	} catch (JDOMException e) {	
jdomexception 

}	}	}	connectionAmbariDatabase.commit();	logger.info("------------------------------");	logger.info("------------------------------");	} catch (IOException e) {	} catch (ParseException e) {	} catch (JDOMException e) {	} catch (URISyntaxException e) {	
urisyntaxexception 

}	connectionAmbariDatabase.commit();	logger.info("------------------------------");	logger.info("------------------------------");	} catch (IOException e) {	} catch (ParseException e) {	} catch (JDOMException e) {	} catch (URISyntaxException e) {	}	} catch (SQLException e1) {	
sqlexception 

logger.info("------------------------------");	logger.info("------------------------------");	} catch (IOException e) {	} catch (ParseException e) {	} catch (JDOMException e) {	} catch (URISyntaxException e) {	}	} catch (SQLException e1) {	try {	connectionAmbariDatabase.rollback();	
rollback done 

logger.info("------------------------------");	} catch (IOException e) {	} catch (ParseException e) {	} catch (JDOMException e) {	} catch (URISyntaxException e) {	}	} catch (SQLException e1) {	try {	connectionAmbariDatabase.rollback();	} catch (SQLException e2) {	
sqlexception in rollback 

} catch (ParseException e) {	} catch (JDOMException e) {	} catch (URISyntaxException e) {	}	} catch (SQLException e1) {	try {	connectionAmbariDatabase.rollback();	} catch (SQLException e2) {	}	} catch (PropertyVetoException e) {	
propertyvetoexception 

========================= ambari sample_1280 =========================

if (eventHandler != null) {	AmbariAuthenticationException cause;	if (authException instanceof AmbariAuthenticationException) {	cause = (AmbariAuthenticationException) authException;	} else {	String header = servletRequest.getHeader("Authorization");	String username = null;	try {	username = getUsernameFromAuth(header, getCredentialsCharset(servletRequest));	} catch (Exception e) {	
error occurred during decoding authorization header 

========================= ambari sample_2855 =========================

principalPasswordMap.put(resolvedPrincipal.getPrincipal(), result.getPassword());	principalKeyNumberMap.put(resolvedPrincipal.getPrincipal(), result.getKeyNumber());	for (KerberosKeytabPrincipalEntity kkpe: kerberosKeytabPrincipalDAO.findByPrincipal(resolvedPrincipal.getPrincipal())) {	kkpe.setDistributed(false);	kerberosKeytabPrincipalDAO.merge(kkpe);	}	KerberosPrincipalEntity principalEntity = kerberosPrincipalDAO.find(resolvedPrincipal.getPrincipal());	try {	new File(principalEntity.getCachedKeytabPath()).delete();	} catch (Exception e) {	
failed to delete cache file 

========================= ambari sample_3317 =========================

public LdapConnectionTemplate create(AmbariLdapConfiguration ambariLdapConfiguration) throws AmbariLdapException {	
constructing new instance based on the provided ambari ldap configuration 

public LdapConnectionTemplate create(AmbariLdapConfiguration ambariLdapConfiguration) throws AmbariLdapException {	LdapConnectionConfig ldapConnectionConfig = ldapConnectionConfigService.createLdapConnectionConfig(ambariLdapConfiguration);	LdapConnectionFactory ldapConnectionFactory = new DefaultLdapConnectionFactory(ldapConnectionConfig);	LdapConnectionPool ldapConnectionPool = new LdapConnectionPool(new ValidatingPoolableLdapConnectionFactory(ldapConnectionFactory));	LdapConnectionTemplate template = new LdapConnectionTemplate(ldapConnectionPool);	
ldap connection template instance 

========================= ambari sample_2910 =========================

static Resource toResource( Map<Long, Map<Long, HostRoleCommandStatusSummaryDTO>> cache, StageEntity entity, Set<String> requestedIds) {	Resource resource = new ResourceImpl(Resource.Type.Stage);	Long clusterId = entity.getClusterId();	if (clusterId != null && !clusterId.equals(Long.valueOf(-1L))) {	try {	Cluster cluster = clustersProvider.get().getClusterById(clusterId);	setResourceProperty(resource, STAGE_CLUSTER_NAME, cluster.getClusterName(), requestedIds);	} catch (Exception e) {	
can not get information for cluster 

========================= ambari sample_3602 =========================

protected void serviceInit(Configuration conf) throws Exception {	
applicationhistory init 

protected void serviceStart() throws Exception {	
starting applicationhistory 

protected void serviceStop() throws Exception {	
stopping applicationhistory 

protected ApplicationHistoryStore createApplicationHistoryStore( Configuration conf) {	if (conf.getBoolean(DISABLE_APPLICATION_TIMELINE_STORE, true)) {	
explicitly disabled application timeline store 

========================= ambari sample_426 =========================

private void updateGuid(UpdateYarnAtsGuid message) {	HiveStatement statement = message.getStatement();	String jobId = message.getJobId();	String yarnAtsGuid = statement.getYarnATSGuid();	
fetched guid for job id 

private void updateGuid(UpdateYarnAtsGuid message) {	HiveStatement statement = message.getStatement();	String jobId = message.getJobId();	String yarnAtsGuid = statement.getYarnATSGuid();	if(yarnAtsGuid == null) {	
retrying to fetch guid 

private void updateGuid(UpdateYarnAtsGuid message) {	HiveStatement statement = message.getStatement();	String jobId = message.getJobId();	String yarnAtsGuid = statement.getYarnATSGuid();	if(yarnAtsGuid == null) {	getContext().system().scheduler() .scheduleOnce(Duration.create(1, TimeUnit.SECONDS), getSelf(), message, getContext().dispatcher(), null);	} else {	jdbcConnectorActor.tell(new SaveGuidToDB(message.getStatementId(), yarnAtsGuid, jobId), self());	
message send to save guid for statement id job id guid 

========================= ambari sample_975 =========================

public ValidationResult validateProperty(String property, ViewInstanceDefinition viewInstanceDefinition, ValidationContext validationContext) {	ClusterType clusterType = viewInstanceDefinition.getClusterType();	if (clusterType == ClusterType.LOCAL_AMBARI || clusterType == ClusterType.REMOTE_AMBARI) {	return ValidationResult.SUCCESS;	}	if (property.equals(WEBHDFS_URL)) {	String webhdfsUrl = viewInstanceDefinition.getPropertyMap().get(WEBHDFS_URL);	if (!ValidatorUtils.validateHdfsURL(webhdfsUrl)) {	
illegal webhdfsurl 

if (!ValidatorUtils.validateHdfsURL(webhdfsUrl)) {	return new InvalidPropertyValidationResult(false, "Must be valid URL");	}	}	if (property.equals(WEBHCAT_PORT)) {	String webhcatPort = viewInstanceDefinition.getPropertyMap().get(WEBHCAT_PORT);	if (webhcatPort != null) {	try {	int port = Integer.valueOf(webhcatPort);	if (port < 1 || port > 65535) {	
illegal port 

========================= ambari sample_1054 =========================

public CommandReport execute(ConcurrentMap<String, Object> requestSharedDataContext) throws AmbariException, InterruptedException {	
executing custom script server action context 

CommandReport cmdReport = null;	try {	Map<String, String> commandParams = getCommandParameters();	validateCommandParams(commandParams);	CollectionPersisterService<String, List<String>> csvPersisterService = collectionPersisterServiceFactory.createCsvFilePersisterService(commandParams.get(UserHookParams.CMD_INPUT_FILE.param()));	csvPersisterService.persistMap(getPayload(commandParams));	String[] cmd = assembleCommand(commandParams);	result = shellCommandUtilityWrapper.runCommand(cmd);	logCommandResult(Arrays.asList(cmd).toString(), result);	cmdReport = createCommandReport(result.getExitCode(), result.isSuccessful() ? HostRoleStatus.COMPLETED : HostRoleStatus.FAILED, "{}", result.getStdout(), result.getStderr());	
command report 

try {	Map<String, String> commandParams = getCommandParameters();	validateCommandParams(commandParams);	CollectionPersisterService<String, List<String>> csvPersisterService = collectionPersisterServiceFactory.createCsvFilePersisterService(commandParams.get(UserHookParams.CMD_INPUT_FILE.param()));	csvPersisterService.persistMap(getPayload(commandParams));	String[] cmd = assembleCommand(commandParams);	result = shellCommandUtilityWrapper.runCommand(cmd);	logCommandResult(Arrays.asList(cmd).toString(), result);	cmdReport = createCommandReport(result.getExitCode(), result.isSuccessful() ? HostRoleStatus.COMPLETED : HostRoleStatus.FAILED, "{}", result.getStdout(), result.getStderr());	} catch (InterruptedException e) {	
the server action thread has been interrupted 

validateCommandParams(commandParams);	CollectionPersisterService<String, List<String>> csvPersisterService = collectionPersisterServiceFactory.createCsvFilePersisterService(commandParams.get(UserHookParams.CMD_INPUT_FILE.param()));	csvPersisterService.persistMap(getPayload(commandParams));	String[] cmd = assembleCommand(commandParams);	result = shellCommandUtilityWrapper.runCommand(cmd);	logCommandResult(Arrays.asList(cmd).toString(), result);	cmdReport = createCommandReport(result.getExitCode(), result.isSuccessful() ? HostRoleStatus.COMPLETED : HostRoleStatus.FAILED, "{}", result.getStdout(), result.getStderr());	} catch (InterruptedException e) {	throw e;	} catch (Exception e) {	
server action is about to quit due to an exception 

private String[] assembleCommand(Map<String, String> params) {	String[] cmdArray = new String[]{	params.get(UserHookParams.SCRIPT.param()), params.get(UserHookParams.CMD_INPUT_FILE.param()), params.get(UserHookParams.CLUSTER_SECURITY_TYPE.param()), params.get(UserHookParams.CMD_HDFS_PRINCIPAL.param()), params.get(UserHookParams.CMD_HDFS_KEYTAB.param()), params.get(UserHookParams.CMD_HDFS_USER.param()) };	
server action command to be executed 

private void validateCommandParams(Map<String, String> commandParams) {	
validating command parameters 

private void validateCommandParams(Map<String, String> commandParams) {	if (!commandParams.containsKey(UserHookParams.PAYLOAD.param())) {	
missing command parameter failing the server action 

private void validateCommandParams(Map<String, String> commandParams) {	if (!commandParams.containsKey(UserHookParams.PAYLOAD.param())) {	throw new IllegalArgumentException("Missing command parameter: [" + UserHookParams.PAYLOAD.param() + "]");	}	if (!commandParams.containsKey(UserHookParams.SCRIPT.param())) {	
missing command parameter failing the server action 

private void validateCommandParams(Map<String, String> commandParams) {	if (!commandParams.containsKey(UserHookParams.PAYLOAD.param())) {	throw new IllegalArgumentException("Missing command parameter: [" + UserHookParams.PAYLOAD.param() + "]");	}	if (!commandParams.containsKey(UserHookParams.SCRIPT.param())) {	throw new IllegalArgumentException("Missing command parameter: [" + UserHookParams.SCRIPT.param() + "]");	}	if (!commandParams.containsKey(UserHookParams.CMD_INPUT_FILE.param())) {	
missing command parameter failing the server action 

if (!commandParams.containsKey(UserHookParams.PAYLOAD.param())) {	throw new IllegalArgumentException("Missing command parameter: [" + UserHookParams.PAYLOAD.param() + "]");	}	if (!commandParams.containsKey(UserHookParams.SCRIPT.param())) {	throw new IllegalArgumentException("Missing command parameter: [" + UserHookParams.SCRIPT.param() + "]");	}	if (!commandParams.containsKey(UserHookParams.CMD_INPUT_FILE.param())) {	throw new IllegalArgumentException("Missing command parameter: [" + UserHookParams.CMD_INPUT_FILE.param() + "]");	}	if (!commandParams.containsKey(UserHookParams.CLUSTER_SECURITY_TYPE.param())) {	
missing command parameter failing the server action 

if (!commandParams.containsKey(UserHookParams.SCRIPT.param())) {	throw new IllegalArgumentException("Missing command parameter: [" + UserHookParams.SCRIPT.param() + "]");	}	if (!commandParams.containsKey(UserHookParams.CMD_INPUT_FILE.param())) {	throw new IllegalArgumentException("Missing command parameter: [" + UserHookParams.CMD_INPUT_FILE.param() + "]");	}	if (!commandParams.containsKey(UserHookParams.CLUSTER_SECURITY_TYPE.param())) {	throw new IllegalArgumentException("Missing command parameter: [" + UserHookParams.CLUSTER_SECURITY_TYPE.param() + "]");	}	if (!commandParams.containsKey(UserHookParams.CMD_HDFS_USER.param())) {	
missing command parameter failing the server action 

}	if (!commandParams.containsKey(UserHookParams.CMD_INPUT_FILE.param())) {	throw new IllegalArgumentException("Missing command parameter: [" + UserHookParams.CMD_INPUT_FILE.param() + "]");	}	if (!commandParams.containsKey(UserHookParams.CLUSTER_SECURITY_TYPE.param())) {	throw new IllegalArgumentException("Missing command parameter: [" + UserHookParams.CLUSTER_SECURITY_TYPE.param() + "]");	}	if (!commandParams.containsKey(UserHookParams.CMD_HDFS_USER.param())) {	throw new IllegalArgumentException("Missing command parameter: [" + UserHookParams.CMD_HDFS_USER.param() + "]");	}	
command parameter validation passed 

========================= ambari sample_3340 =========================

private RepositoryVersionEntity getRepositoryVersionEntity(Cluster cluster, ServiceComponent component) throws SystemException {	RepositoryVersionEntity repositoryEntity = null;	if (null != component) {	repositoryEntity = component.getDesiredRepositoryVersion();	} else {	
service component not passed in attempt to resolve the repository for cluster 

}	if (null == repositoryEntity && null != component) {	try {	Service service = cluster.getService(component.getServiceName());	repositoryEntity = service.getDesiredRepositoryVersion();	} catch (AmbariException e) {	throw new SystemException("Unhandled exception", e);	}	}	if (null == repositoryEntity) {	
cluster has no specific repository versions using stack defined values 

public String getUpgradePackageName(String stackName, String stackVersion, String repositoryVersion, UpgradeType upgradeType) throws AmbariException {	final Map<String, UpgradePack> upgradePacks = ami.get().getUpgradePacks(stackName, stackVersion);	for (UpgradePack upgradePack : upgradePacks.values()) {	final String upgradePackName = upgradePack.getName();	if (null != upgradeType && upgradePack.getType() != upgradeType) {	continue;	}	if (StringUtils.isBlank(upgradePack.getTarget())) {	
upgrade pack is corrupted it should contain target node 

public Map<String, String> buildRoleParams(AmbariManagementController amc, RepositoryVersionEntity repoVersion, String osFamily, Set<String> servicesOnHost) throws SystemException {	StackId stackId = repoVersion.getStackId();	List<ServiceOsSpecific.Package> packages = new ArrayList<>();	for (String serviceName : servicesOnHost) {	ServiceInfo info;	try {	if (ami.get().isServiceRemovedInStack(stackId.getStackName(), stackId.getStackVersion(), serviceName)) {	
s has been removed from stack s s skip calculating its installation packages 

if (repoVersion.isLegacy()){	commandRepo.setLegacyRepoFileName(repoVersion.getStackName(), repoVersion.getVersion());	commandRepo.setLegacyRepoId(repoVersion.getVersion());	commandRepo.getFeature().setIsScoped(false);	} else {	commandRepo.setRepoFileName(repoVersion.getStackName(), repoVersion.getId());	commandRepo.setUniqueSuffix(String.format("-repo-%s", repoVersion.getId()));	}	}	if (configuration.get().arePackagesLegacyOverridden()) {	
legacy override option is turned on disabling commandrepositoryfeature scoped feature 

public void addCommandRepositoryToContext(ActionExecutionContext context, OperatingSystemEntity osEntity) throws SystemException {	final RepositoryVersionEntity repoVersion = context.getRepositoryVersion();	final CommandRepository commandRepo = getCommandRepository(repoVersion, osEntity);	ClusterVersionSummary summary = null;	if (RepositoryType.STANDARD != repoVersion.getType()) {	try {	final Cluster cluster = clusters.get().getCluster(context.getClusterName());	VersionDefinitionXml xml = repoVersion.getRepositoryXml();	summary = xml.getClusterSummary(cluster);	} catch (Exception e) {	
could not determine repository from s s will not pass cluster version 

========================= ambari sample_3205 =========================

public void sessionCreated(HttpSessionEvent event) {	synchronized (this) {	numberOfSessions++;	}	
new session is created id s number of sessions d 

public void sessionDestroyed(HttpSessionEvent event) {	synchronized (this) {	numberOfSessions--;	}	
session destroyed id s number of sessions d 

========================= ambari sample_1392 =========================

private String getPasswordFromCredentialStore(String propertyName) {	try {	if (StringUtils.isEmpty(credentialStoreProviderPath)) {	return null;	}	org.apache.hadoop.conf.Configuration config = new org.apache.hadoop.conf.Configuration();	config.set(CREDENTIAL_STORE_PROVIDER_PATH_PROPERTY, credentialStoreProviderPath);	char[] passwordChars = config.getPassword(propertyName);	return (ArrayUtils.isNotEmpty(passwordChars)) ? new String(passwordChars) : null;	} catch (Exception e) {	
could not load password s from credential store using default password 

private String getPasswordFromFile(String fileName) {	try {	File pwdFile = new File(LOGFEEDER_CERT_DEFAULT_FOLDER, fileName);	if (!pwdFile.exists()) {	FileUtils.writeStringToFile(pwdFile, LOGFEEDER_STORE_DEFAULT_PASSWORD, Charset.defaultCharset());	return LOGFEEDER_STORE_DEFAULT_PASSWORD;	} else {	return FileUtils.readFileToString(pwdFile, Charset.defaultCharset());	}	} catch (Exception e) {	
exception occurred during read write password file for keystore truststore 

========================= ambari sample_1668 =========================

public CommandReport execute(ConcurrentMap<String, Object> requestSharedDataContext) throws AmbariException, InterruptedException {	String clusterName = getExecutionCommand().getClusterName();	Cluster cluster = getClusters().getCluster(clusterName);	UpgradeContext upgradeContext = getUpgradeContext(cluster);	Map<String, String> roleParams = getExecutionCommand().getRoleParams();	String userName;	if (roleParams != null && roleParams.containsKey(ServerAction.ACTION_USER_NAME)) {	userName = roleParams.get(ServerAction.ACTION_USER_NAME);	} else {	userName = m_configuration.getAnonymousAuditName();	
did not receive role parameter s will save configs using anonymous username s 

========================= ambari sample_3356 =========================

public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException {	HttpServletRequest request = (HttpServletRequest) servletRequest;	if (requestMatcher.matches(request)) {	VResponse errorResponse = getErrorResponse(solrCollectionState, solrPropsConfig, request);	if (errorResponse != null) {	
request is filtered out 

========================= ambari sample_1384 =========================

for (String key : keys) {	if (key.startsWith(downSamplerConfigPrefix) && key.endsWith(downSamplerMetricPatternsConfig)) {	String type = key.split("\\.")[3];	CustomDownSampler downSampler = getDownSamplerByType(type, conf);	if (downSampler != null) {	downSamplers.add(downSampler);	}	}	}	} catch (Exception e) {	
exception caught while parsing downsampler configs from ams site 

public static CustomDownSampler getDownSamplerByType(String type, Map<String, String> conf) {	if (type == null) {	return null;	}	if (StringUtils.isNotEmpty(type) && type.equalsIgnoreCase(topNDownSampler)) {	return TopNDownSampler.fromConfig(conf);	}	
unknown downsampler requested 

========================= ambari sample_349 =========================

private RequestStatusResponse processBlueprintCreate(Map<String, Object> properties, Map<String, String> requestInfoProperties) throws ResourceAlreadyExistsException, SystemException, UnsupportedPropertyException, NoSuchParentResourceException {	
creating cluster based on blueprint 

========================= ambari sample_3497 =========================

KerberosIdentityDescriptor identityToAdd;	if (resolveReferences) {	identityToAdd = dereferenceIdentity(identity);	} else {	identityToAdd = identity;	}	if ((identityToAdd != null) && ((contextForFilter == null) || identityToAdd.shouldInclude(contextForFilter))) {	if (isReferredServiceInstalled(identity, contextForFilter)) {	list.add(identityToAdd);	} else {	
skipping identity because referred service is not installed 

private KerberosIdentityDescriptor dereferenceIdentity(KerberosIdentityDescriptor identity) throws AmbariException {	KerberosIdentityDescriptor dereferencedIdentity = null;	if (identity != null) {	KerberosIdentityDescriptor referencedIdentity;	try {	if (identity.getReference() != null) {	referencedIdentity = getReferencedIdentityDescriptor(identity.getReference());	} else {	referencedIdentity = getReferencedIdentityDescriptor(identity.getName());	if(referencedIdentity != null) {	
referenced identities should be declared using the identity s attribute not the identity s attribute this is a deprecated feature problems may occur in the future unless this is corrected 

========================= ambari sample_3028 =========================

public void start() {	
initializing ambariserver metrics service 

if (configuration == null) {	return;	}	sink = new AmbariMetricSinkImpl(amc);	initializeMetricsSink();	initializeMetricSources();	if (!sink.isInitialized()) {	Executors.newScheduledThreadPool(1).scheduleWithFixedDelay(new Runnable() {	public void run() {	if (!sink.isInitialized()) {	
attempting to initialize metrics sink 

}	sink = new AmbariMetricSinkImpl(amc);	initializeMetricsSink();	initializeMetricSources();	if (!sink.isInitialized()) {	Executors.newScheduledThreadPool(1).scheduleWithFixedDelay(new Runnable() {	public void run() {	if (!sink.isInitialized()) {	initializeMetricsSink();	if (sink.isInitialized()) {	
metric sink initialization successful 

public void run() {	if (!sink.isInitialized()) {	initializeMetricsSink();	if (sink.isInitialized()) {	}	}	}	}, 5, 5, TimeUnit.MINUTES);	}	} catch (Exception e) {	
unable to initialize metricsservice 

private void initializeMetricsSink() {	
configuring metric sink 

private void initializeMetricSources() {	try {	
configuring metric sources 

private void initializeMetricSources() {	try {	String commaSeparatedSources = configuration.getProperty("metric.sources");	if (StringUtils.isEmpty(commaSeparatedSources)) {	
no metric sources configured 

for (String sourceName : sourceNames) {	if (StringUtils.isEmpty(sourceName)) {	continue;	}	sourceName = sourceName.trim();	String className = configuration.getProperty("source." + sourceName + ".class");	Class sourceClass;	try {	sourceClass = Class.forName(className);	} catch (ClassNotFoundException ex) {	
source class not found for source name 

sourceClass = Class.forName(className);	} catch (ClassNotFoundException ex) {	continue;	}	AbstractMetricsSource src = (AbstractMetricsSource) sourceClass.newInstance();	src.init(MetricsConfiguration.getSubsetConfiguration(configuration, "source." + sourceName + "."), sink);	sources.put(sourceName, src);	src.start();	}	} catch (Exception e) {	
error when configuring metric sink and source 

========================= ambari sample_2639 =========================

hostnamesCondition.append(")");	appendConjunction = append(sb, appendConjunction, getHostnames(), hostnamesCondition.toString());	} else if (isTopNMetricCondition(metricNames, hostnames)) {	StringBuilder metricNamesCondition = new StringBuilder();	metricNamesCondition.append(" METRIC_NAME IN (");	metricNamesCondition.append(getTopNInnerQuery());	metricNamesCondition.append(")");	appendConjunction = append(sb, appendConjunction, getMetricNames(), metricNamesCondition.toString());	appendConjunction = appendHostnameClause(sb, appendConjunction);	} else {	
unsupported topn operation requested query can have either multiple hosts or multiple metric names but not both 

========================= ambari sample_386 =========================

continue;	}	switch (hostVersionEntity.getState()) {	case INSTALLED: case NOT_REQUIRED: hostVersionEntity.setState(RepositoryVersionState.OUT_OF_SYNC);	hostVersionDAO.get().merge(hostVersionEntity);	break;	default: break;	}	}	} catch (AmbariException e) {	
can not update hosts about out of sync 

public void onServiceComponentHostEvent(ServiceComponentUninstalledEvent event) {	m_lock.lock();	try {	Cluster cluster = clusters.get().getClusterById(event.getClusterId());	List<HostVersionEntity> hostVersionEntities = hostVersionDAO.get().findByClusterAndHost(cluster.getClusterName(), event.getHostName());	for (HostVersionEntity hostVersionEntity : hostVersionEntities) {	HostEntity hostEntity = hostVersionEntity.getHostEntity();	RepositoryVersionEntity repoVersionEntity = hostVersionEntity.getRepositoryVersion();	StackId stackId = repoVersionEntity.getStackId();	if (null == stackId) {	
stack id could not be loaded for host version repo 

if (null == stackId) {	continue;	}	RepositoryVersionState repoState = checkAllHostComponents(stackId, hostEntity);	if (null != repoState) {	hostVersionEntity.setState(repoState);	hostVersionDAO.get().merge(hostVersionEntity);	}	}	} catch (AmbariException e) {	
cannot update states after a component was uninstalled 

if (! hasChangedComponentsWithVersions) {	continue;	}	if (hostVersionEntity.getState().equals(RepositoryVersionState.INSTALLED)) {	hostVersionEntity.setState(RepositoryVersionState.OUT_OF_SYNC);	hostVersionDAO.get().merge(hostVersionEntity);	}	}	}	} catch (AmbariException e) {	
can not update hosts about out of sync 

========================= ambari sample_4519 =========================

String userName = jwtToken.getJWTClaimsSet().getSubject();	Authentication authentication = authenticationProvider.authenticate(new JwtAuthenticationToken(userName, serializedJWT, null));	SecurityContextHolder.getContext().setAuthentication(authentication);	if (eventHandler != null) {	eventHandler.onSuccessfulAuthentication(this, httpServletRequest, httpServletResponse, authentication);	}	} else {	throw new BadCredentialsException("Invalid JWT token");	}	} catch (ParseException e) {	
unable to parse the jwt token 

if (eventHandler != null) {	eventHandler.onSuccessfulAuthentication(this, httpServletRequest, httpServletResponse, authentication);	}	} else {	throw new BadCredentialsException("Invalid JWT token");	}	} catch (ParseException e) {	throw new BadCredentialsException("Unable to parse the JWT token - " + e.getLocalizedMessage());	}	} else {	
no jwt cookie found do nothing 

} else {	throw new BadCredentialsException("Invalid JWT token");	}	} catch (ParseException e) {	throw new BadCredentialsException("Unable to parse the JWT token - " + e.getLocalizedMessage());	}	} else {	}	chain.doFilter(servletRequest, servletResponse);	} catch (AuthenticationException e) {	
jwt authentication failed 

private boolean validateToken(SignedJWT jwtToken) {	boolean sigValid = validateSignature(jwtToken);	if (!sigValid) {	
signature could not be verified 

private boolean validateToken(SignedJWT jwtToken) {	boolean sigValid = validateSignature(jwtToken);	if (!sigValid) {	}	boolean audValid = validateAudiences(jwtToken);	if (!audValid) {	
audience validation failed 

private boolean validateToken(SignedJWT jwtToken) {	boolean sigValid = validateSignature(jwtToken);	if (!sigValid) {	}	boolean audValid = validateAudiences(jwtToken);	if (!audValid) {	}	boolean expValid = validateExpiration(jwtToken);	if (!expValid) {	
expiration validation failed 

========================= ambari sample_2834 =========================

protected AmbariSolrState executeZkCommand(AmbariSolrCloudClient client, SolrZkClient zkClient, SolrZooKeeper solrZooKeeper) throws Exception {	boolean secure = client.isSecure();	String stateFile = String.format("%s/%s", unsecureZnode, AbstractStateFileZkCommand.STATE_FILE);	AmbariSolrState result = null;	if (secure) {	
update state file in secure mode 

protected AmbariSolrState executeZkCommand(AmbariSolrCloudClient client, SolrZkClient zkClient, SolrZooKeeper solrZooKeeper) throws Exception {	boolean secure = client.isSecure();	String stateFile = String.format("%s/%s", unsecureZnode, AbstractStateFileZkCommand.STATE_FILE);	AmbariSolrState result = null;	if (secure) {	updateStateFile(client, zkClient, AmbariSolrState.SECURE, stateFile);	result = AmbariSolrState.SECURE;	} else {	
update state file in unsecure mode 

private void updateStateFile(AmbariSolrCloudClient client, SolrZkClient zkClient, AmbariSolrState stateToUpdate, String stateFile) throws Exception {	if (!zkClient.exists(stateFile, true)) {	
state file does not exits initializing it as 

private void updateStateFile(AmbariSolrCloudClient client, SolrZkClient zkClient, AmbariSolrState stateToUpdate, String stateFile) throws Exception {	if (!zkClient.exists(stateFile, true)) {	zkClient.create(stateFile, createStateJson(stateToUpdate).getBytes(StandardCharsets.UTF_8), CreateMode.PERSISTENT, true);	} else {	AmbariSolrState stateOnSecure = getStateFromJson(client, stateFile);	if (stateToUpdate.equals(stateOnSecure)) {	
state file is in mode no update 

private void updateStateFile(AmbariSolrCloudClient client, SolrZkClient zkClient, AmbariSolrState stateToUpdate, String stateFile) throws Exception {	if (!zkClient.exists(stateFile, true)) {	zkClient.create(stateFile, createStateJson(stateToUpdate).getBytes(StandardCharsets.UTF_8), CreateMode.PERSISTENT, true);	} else {	AmbariSolrState stateOnSecure = getStateFromJson(client, stateFile);	if (stateToUpdate.equals(stateOnSecure)) {	} else {	
state file is in mode updating it to 

========================= ambari sample_105 =========================

LOG.error(getErrorMessage(e), e);	throw e;	} catch (Exception e) {	LOG.error(e.getMessage(), e);	throw new ServiceFormattedException(e);	} finally {	if (null != uploadedInputStream) {	try {	uploadedInputStream.close();	} catch (IOException e) {	
exception occured while closing the hdfs file stream for path 

public Response createTable(TableInput tableInput) {	try {	tableInput.validate();	String databaseName = tableInput.getDatabaseName();	String tableCreationQuery = generateCreateQuery(tableInput);	
tablecreationquery 

public Response createTable(TableInput tableInput) {	try {	tableInput.validate();	String databaseName = tableInput.getDatabaseName();	String tableCreationQuery = generateCreateQuery(tableInput);	Job job = createJob(tableCreationQuery, databaseName);	
job created for table creation 

} catch (WebApplicationException e) {	LOG.error(getErrorMessage(e), e);	throw e;	} catch (Exception e) {	LOG.error(e.getMessage(), e);	throw new ServiceFormattedException(e);	} finally {	if (null != hdfsStream) try {	hdfsStream.close();	} catch (IOException e) {	
exception occured while closing the hdfs stream for path 

public Response insertFromTempTable(InsertFromQueryInput input) {	try {	String insertQuery = generateInsertFromQuery(input);	
insertquery 

public Response insertFromTempTable(InsertFromQueryInput input) {	try {	String insertQuery = generateInsertFromQuery(input);	Job job = createJob(insertQuery, input.getFromDatabase());	
job created for insert from temp table 

public Response deleteTable(DeleteQueryInput input) {	try {	String deleteQuery = generateDeleteQuery(input);	
deletequery 

public Response deleteTable(DeleteQueryInput input) {	try {	String deleteQuery = generateDeleteQuery(input);	Job job = createJob(deleteQuery, input.getDatabase());	
job created for delete temp table 

private String uploadIntoTable(Reader reader, String databaseName, String tempTableName) {	try {	String fullPath = getHiveMetaStoreLocation(databaseName, tempTableName);	
uploading file into 

private Job createJob(String query, String databaseName) throws Throwable{	Map jobInfo = new HashMap<>();	jobInfo.put("title", "Internal Job");	jobInfo.put("forcedContent", query);	jobInfo.put("dataBase", databaseName);	Job job = new JobImpl(jobInfo);	
creating job 

urlString = row.getRow()[1] == null ? null : row.getRow()[1].toString();	break;	}	}	String tablePath = null;	if (null != urlString) {	try {	URI uri = new URI(urlString);	tablePath = uri.getPath();	} catch (URISyntaxException e) {	
error occurred while parsing as url 

private String getHiveMetaStoreLocation() {	String dir = context.getProperties().get(HIVE_METASTORE_LOCATION_KEY_VIEW_PROPERTY);	if (dir != null && !dir.trim().isEmpty()) {	return dir;	} else {	
neither found associated cluster nor found the view property returning default location 

ParseOptions parseOptions = new ParseOptions();	parseOptions.setOption(ParseOptions.OPTIONS_FILE_TYPE, inputFileType);	if (inputFileType.equals(ParseOptions.InputFileType.CSV.toString())){	if(isFirstRowHeader) parseOptions.setOption(ParseOptions.OPTIONS_HEADER, ParseOptions.HEADER.FIRST_RECORD.toString());	else parseOptions.setOption(ParseOptions.OPTIONS_HEADER, ParseOptions.HEADER.NONE.toString());	parseOptions.setOption(ParseOptions.OPTIONS_CSV_DELIMITER, csvParams.getCsvDelimiter());	parseOptions.setOption(ParseOptions.OPTIONS_CSV_ESCAPE_CHAR, csvParams.getCsvEscape());	parseOptions.setOption(ParseOptions.OPTIONS_CSV_QUOTE, csvParams.getCsvQuote());	}	else parseOptions.setOption(ParseOptions.OPTIONS_HEADER, ParseOptions.HEADER.EMBEDDED.toString());	
isfirstrowheader inputfiletype 

private String uploadFileFromStream( InputStream uploadedInputStream, Boolean isFirstRowHeader, String inputFileType, String tableName, String databaseName, List<ColumnDescriptionImpl> header, boolean containsEndlines, CSVParams csvParams ) throws Exception {	
uploading file into databasename tablename 

========================= ambari sample_806 =========================

public void testMapperFieldName_replaceField() {	
testmapperfieldname replacefield 

public void testMapperFieldName_noNewFieldName() {	
testmapperfieldname nonewfieldname 

========================= ambari sample_1598 =========================

public SSLContext getSSLContext() {	SslContextFactory sslContextFactory = getSslContextFactory();	try {	sslContextFactory.start();	return sslContextFactory.getSslContext();	} catch (Exception e) {	
could not create ssl context 

SslContextFactory sslContextFactory = getSslContextFactory();	try {	sslContextFactory.start();	return sslContextFactory.getSslContext();	} catch (Exception e) {	return null;	} finally {	try {	sslContextFactory.stop();	} catch (Exception e) {	
could not stop sslcontextfactory 

private String getPasswordFromFile(String fileName) {	try {	File pwdFile = new File(LOGSEARCH_CERT_DEFAULT_FOLDER, fileName);	if (!pwdFile.exists()) {	FileUtils.writeStringToFile(pwdFile, LOGSEARCH_KEYSTORE_DEFAULT_PASSWORD);	return LOGSEARCH_KEYSTORE_DEFAULT_PASSWORD;	} else {	return FileUtils.readFileToString(pwdFile);	}	} catch (Exception e) {	
exception occurred during read write password file for keystore truststore 

try {	String providerPath = logSearchSslConfig.getCredentialStoreProviderPath();	if (StringUtils.isEmpty(providerPath)) {	return null;	}	Configuration config = new Configuration();	config.set(CREDENTIAL_STORE_PROVIDER_PATH, providerPath);	char[] passwordChars = config.getPassword(propertyName);	return (ArrayUtils.isNotEmpty(passwordChars)) ? new String(passwordChars) : null;	} catch (Exception e) {	
could not load password s from credential store using default password 

private void setKeyAndCertInKeystore(X509Certificate cert, KeyPair keyPair, KeyStore keyStore, String keyStoreLocation, char[] password) throws Exception {	Certificate[] certChain = new Certificate[1];	certChain[0] = cert;	try (FileOutputStream fos = new FileOutputStream(keyStoreLocation)) {	keyStore.setKeyEntry("logsearch.alias", keyPair.getPrivate(), password, certChain);	keyStore.store(fos, password);	} catch (Exception e) {	
could not write certificate to keystore 

private X509Certificate generateCertificate(String certificateLocation, KeyPair keyPair, String algorithm) throws Exception {	try {	File certFile = new File(certificateLocation);	if (certFile.exists()) {	
certificate file exists skip the generation 

File certFile = new File(certificateLocation);	if (certFile.exists()) {	return getCertFile(certificateLocation);	} else {	Security.addProvider(new BouncyCastleProvider());	X509Certificate cert = createCert(keyPair, algorithm, InetAddress.getLocalHost().getCanonicalHostName());	FileUtils.writeByteArrayToFile(certFile, cert.getEncoded());	return cert;	}	} catch (Exception e) {	
could not create certificate 

private X509Certificate getCertFile(String location) throws Exception {	try (FileInputStream fos = new FileInputStream(location)) {	CertificateFactory factory = CertificateFactory.getInstance("X.509");	return (X509Certificate) factory.generateCertificate(fos);	} catch (Exception e) {	
cannot read cert file 

public void loadKeystore() {	try {	String certFolder = logSearchSslConfig.getCertFolder();	String certAlgorithm = logSearchSslConfig.getCertAlgorithm();	String certLocation = String.format("%s/%s", LOGSEARCH_CERT_DEFAULT_FOLDER, LOGSEARCH_CERT_FILENAME);	String keyStoreLocation = StringUtils.isNotEmpty(getKeyStoreLocation()) ? getKeyStoreLocation() : String.format("%s/%s", LOGSEARCH_CERT_DEFAULT_FOLDER, LOGSEARCH_KEYSTORE_FILENAME);	char[] password = StringUtils.isNotEmpty(getKeyStorePassword()) ? getKeyStorePassword().toCharArray() : LOGSEARCH_KEYSTORE_DEFAULT_PASSWORD.toCharArray();	boolean keyStoreFileExists = new File(keyStoreLocation).exists();	if (!keyStoreFileExists) {	FileUtil.createDirectory(certFolder);	
keystore file does not exist creating new one if the file exists make sure you have proper permissions on that 

String certAlgorithm = logSearchSslConfig.getCertAlgorithm();	String certLocation = String.format("%s/%s", LOGSEARCH_CERT_DEFAULT_FOLDER, LOGSEARCH_CERT_FILENAME);	String keyStoreLocation = StringUtils.isNotEmpty(getKeyStoreLocation()) ? getKeyStoreLocation() : String.format("%s/%s", LOGSEARCH_CERT_DEFAULT_FOLDER, LOGSEARCH_KEYSTORE_FILENAME);	char[] password = StringUtils.isNotEmpty(getKeyStorePassword()) ? getKeyStorePassword().toCharArray() : LOGSEARCH_KEYSTORE_DEFAULT_PASSWORD.toCharArray();	boolean keyStoreFileExists = new File(keyStoreLocation).exists();	if (!keyStoreFileExists) {	FileUtil.createDirectory(certFolder);	if (isKeyStoreSpecified() && !"JKS".equalsIgnoreCase(getKeyStoreType())) {	throw new RuntimeException(String.format("Keystore does not exist. Only JKS keystore can be auto generated. (%s)", keyStoreLocation));	}	
ssl keystore is not specified generating it with certificate using default format jks 

========================= ambari sample_1432 =========================

public void testJSONFilterCode_convertFields() throws Exception {	
testjsonfiltercode convertfields 

public void testJSONFilterCode_lineNumberOnly() throws Exception {	
testjsonfiltercode linenumberonly 

public void testJSONFilterCode_invalidJson() throws Exception {	
testjsonfiltercode invalidjson 

========================= ambari sample_1609 =========================

public Set<Resource> populateResources(Set<Resource> resources, Request request, Predicate predicate) throws SystemException {	Map<String, Boolean> isLogSearchRunning = new HashMap<>();	for (Resource resource : resources) {	final String componentName = (String)resource.getPropertyValue(PropertyHelper.getPropertyId("HostRoles", "component_name"));	final String hostName = (String) resource.getPropertyValue(PropertyHelper.getPropertyId("HostRoles", "host_name"));	final String clusterName = (String) resource.getPropertyValue(PropertyHelper.getPropertyId("HostRoles", "cluster_name"));	if(!AuthorizationHelper.isAuthorized(ResourceType.CLUSTER, getClusterResourceID(clusterName), REQUIRED_AUTHORIZATIONS)) {	if(LOG.isDebugEnabled()) {	
the authenticated user is not authorized to access logsearch data for the cluster named 

private Long getClusterResourceID(String clusterName) {	Long clusterResourceId = null;	if(!StringUtils.isEmpty(clusterName)) {	try {	Cluster cluster = ambariManagementController.getClusters().getCluster(clusterName);	if(cluster == null) {	
no cluster found with the name s assuming null resource id 

Long clusterResourceId = null;	if(!StringUtils.isEmpty(clusterName)) {	try {	Cluster cluster = ambariManagementController.getClusters().getCluster(clusterName);	if(cluster == null) {	}	else {	clusterResourceId = cluster.getResourceId();	}	} catch (AmbariException e) {	
an exception occurred looking up the cluster named s assuming null resource id s 

Cluster cluster = ambariManagementController.getClusters().getCluster(clusterName);	if(cluster == null) {	}	else {	clusterResourceId = cluster.getResourceId();	}	} catch (AmbariException e) {	}	}	else {	
the cluster name is not set assuming null resource id 

========================= ambari sample_3771 =========================

public void activateOptions() {	synchronized (this) {	if (database.equals("none")) {	logStore = nullStore;	
database set to none 

public void activateOptions() {	synchronized (this) {	if (database.equals("none")) {	logStore = nullStore;	} else {	try {	logStore = new DatabaseStore(driver, database, user, password, new MapReduceJobHistoryUpdater());	} catch (IOException ioe) {	
failed to connect to db 

synchronized (this) {	if (database.equals("none")) {	logStore = nullStore;	} else {	try {	logStore = new DatabaseStore(driver, database, user, password, new MapReduceJobHistoryUpdater());	} catch (IOException ioe) {	System.err.println("Failed to connect to db " + database + " as user " + user + " password " + password + " and driver " + driver + " with " + StringUtils.stringifyException(ioe));	throw new RuntimeException( "Failed to create database store for " + database, ioe);	} catch (Exception e) {	
failed to connect to db 

public void close() {	try {	logThreadRunnable.close();	} catch (IOException ioe) {	
failed to close logthreadrunnable 

public void close() {	try {	logThreadRunnable.close();	} catch (IOException ioe) {	}	try {	logThread.join(1000);	} catch (InterruptedException ie) {	
logthread interrupted 

========================= ambari sample_502 =========================

public void beforeStep(StepExecution stepExecution) {	
dummy step before step execution 

public ExitStatus afterStep(StepExecution stepExecution) {	
dummy step after step execution 

========================= ambari sample_152 =========================

public void checkConnection(AmbariLdapConfiguration ambariLdapConfiguration) throws AmbariLdapException {	try {	ldapConfigurationService.checkConnection(ambariLdapConfiguration);	
validating ldap connection related configuration success 

public void checkConnection(AmbariLdapConfiguration ambariLdapConfiguration) throws AmbariLdapException {	try {	ldapConfigurationService.checkConnection(ambariLdapConfiguration);	} catch (Exception e) {	
validating ldap connection configuration failed 

public AmbariLdapConfiguration detectAttributes(AmbariLdapConfiguration ambariLdapConfiguration) throws AmbariLdapException {	
detecting ldap configuration attributes 

public AmbariLdapConfiguration detectAttributes(AmbariLdapConfiguration ambariLdapConfiguration) throws AmbariLdapException {	try {	
detecting user attributes 

public AmbariLdapConfiguration detectAttributes(AmbariLdapConfiguration ambariLdapConfiguration) throws AmbariLdapException {	try {	ambariLdapConfiguration = ldapAttributeDetectionService.detectLdapUserAttributes(ambariLdapConfiguration);	
detecting group attributes 

public AmbariLdapConfiguration detectAttributes(AmbariLdapConfiguration ambariLdapConfiguration) throws AmbariLdapException {	try {	ambariLdapConfiguration = ldapAttributeDetectionService.detectLdapUserAttributes(ambariLdapConfiguration);	ambariLdapConfiguration = ldapAttributeDetectionService.detectLdapGroupAttributes(ambariLdapConfiguration);	
attribute detection finished 

public AmbariLdapConfiguration detectAttributes(AmbariLdapConfiguration ambariLdapConfiguration) throws AmbariLdapException {	try {	ambariLdapConfiguration = ldapAttributeDetectionService.detectLdapUserAttributes(ambariLdapConfiguration);	ambariLdapConfiguration = ldapAttributeDetectionService.detectLdapGroupAttributes(ambariLdapConfiguration);	return ambariLdapConfiguration;	} catch (Exception e) {	
error during ldap attribute detection 

public Set<String> checkLdapAttributes(Map<String, Object> parameters, AmbariLdapConfiguration ldapConfiguration) throws AmbariLdapException {	String userName = getTestUserNameFromParameters(parameters);	String testUserPass = getTestUserPasswordFromParameters(parameters);	if (null == userName) {	throw new IllegalArgumentException("No test user available for testing LDAP attributes");	}	
testing ldap user attributes with test user 

public Set<String> checkLdapAttributes(Map<String, Object> parameters, AmbariLdapConfiguration ldapConfiguration) throws AmbariLdapException {	String userName = getTestUserNameFromParameters(parameters);	String testUserPass = getTestUserPasswordFromParameters(parameters);	if (null == userName) {	throw new IllegalArgumentException("No test user available for testing LDAP attributes");	}	String userDn = ldapConfigurationService.checkUserAttributes(userName, testUserPass, ldapConfiguration);	
testing ldap group attributes with test user dn 

private Object parameterValue(Map<String, Object> parameters, Parameters parameter) {	Object value = null;	if (parameters.containsKey(parameter.getParameterKey())) {	value = parameters.get(parameter.getParameterKey());	} else {	
parameter is missing from parameters 

========================= ambari sample_2930 =========================

continue;	}	boolean oldEnabled = entity.getEnabled();	try {	populateEntity(entity, propertyMap);	alertDefinitionDAO.merge(entity);	Set<String> invalidatedHosts = alertDefinitionHash.invalidateHosts(entity);	AlertHashInvalidationEvent event = new AlertHashInvalidationEvent( entity.getClusterId(), invalidatedHosts);	eventPublisher.publish(event);	} catch (AmbariException ae) {	
unable to find cluster when updating alert definition 

public RequestStatus deleteResources(Request request, Predicate predicate) throws SystemException, UnsupportedPropertyException, NoSuchResourceException, NoSuchParentResourceException {	Set<Resource> resources = getResources( new RequestImpl(null, null, null, null), predicate);	Set<Long> definitionIds = new HashSet<>();	for (final Resource resource : resources) {	Long id = (Long) resource.getPropertyValue(ALERT_DEF_ID);	definitionIds.add(id);	}	for (Long definitionId : definitionIds) {	
deleting alert definition 

setResourceProperty(resource, ALERT_DEF_REPEAT_TOLERANCE_ENABLED, Boolean.valueOf(entity.isRepeatToleranceEnabled()), requestedIds);	boolean sourceTypeRequested = setResourceProperty(resource, ALERT_DEF_SOURCE_TYPE, entity.getSourceType(), requestedIds);	if (sourceTypeRequested && null != resource.getPropertyValue(ALERT_DEF_SOURCE_TYPE)) {	try {	Map<String, String> map = gson.<Map<String, String>> fromJson( entity.getSource(), Map.class);	for (Entry<String, String> entry : map.entrySet()) {	String subProp = PropertyHelper.getPropertyId(ALERT_DEF_SOURCE, entry.getKey());	resource.setProperty(subProp, entry.getValue());	}	} catch (Exception e) {	
could not coerce alert json into a type 

private void scheduleImmediateAlert(Map<String, Object> propertyMap) throws AuthorizationException {	Clusters clusters = getManagementController().getClusters();	String stringId = (String) propertyMap.get(ALERT_DEF_ID);	long id = Long.parseLong(stringId);	AlertDefinitionEntity entity = alertDefinitionDAO.findById(id);	if (null == entity) {	
unable to lookup alert definition with id 

String stringId = (String) propertyMap.get(ALERT_DEF_ID);	long id = Long.parseLong(stringId);	AlertDefinitionEntity entity = alertDefinitionDAO.findById(id);	if (null == entity) {	return;	}	Cluster cluster = null;	try {	cluster = clusters.getClusterById(entity.getClusterId());	} catch (AmbariException ambariException) {	
unable to lookup cluster with id 

========================= ambari sample_3622 =========================

case 200: {	break;	}	default: {	String reason = getFailReason(KEY_RANGER_UNKNOWN_RESPONSE, check, request);	warnReasons.add(String.format(reason, adminUsername, response, rangerAuthUrl));	break;	}	}	} catch (IOException e) {	
could not access the url message 

case 200: {	break;	}	default: {	String reason = getFailReason(KEY_RANGER_UNKNOWN_RESPONSE, check, request);	warnReasons.add(String.format(reason, adminUsername, response, rangerAuthUrl));	break;	}	}	} catch (IOException e) {	
could not access the url message 

case 200: {	break;	}	default: {	String reason = getFailReason(KEY_RANGER_UNKNOWN_RESPONSE, check, request);	warnReasons.add(String.format(reason, rangerAdminUsername, response, rangerAuthUrl));	break;	}	}	} catch (IOException e) {	
could not access the url message 

case 200: {	break;	}	default: {	String reason = getFailReason(KEY_RANGER_UNKNOWN_RESPONSE, check, request);	warnReasons.add(String.format(reason, rangerAdminUsername, response, rangerAuthUrl));	break;	}	}	} catch (IOException e) {	
could not access the url message 

return false;	}	List<Map<?, ?>> list = (List<Map<?, ?>>) map.get("vXUsers");	for (Map<?, ?> listMap : list) {	if (listMap.containsKey("name") && listMap.get("name").equals(userToSearch)) {	return true;	}	}	}	} catch (IOException e) {	
could not determine user error is 

return false;	}	List<Map<?, ?>> list = (List<Map<?, ?>>) map.get("vXUsers");	for (Map<?, ?> listMap : list) {	if (listMap.containsKey("name") && listMap.get("name").equals(userToSearch)) {	return true;	}	}	}	} catch (IOException e) {	
could not determine user error is 

for (Map<?, ?> listMap : list) {	if (listMap.containsKey("name") && listMap.get("name").equals(userToSearch)) {	return true;	}	}	}	} catch (IOException e) {	String reason = getFailReason(KEY_RANGER_COULD_NOT_ACCESS, check, request);	warnReasons.add(String.format(reason, username, url, e.getMessage()));	} catch (Exception e) {	
could not determine user error is 

for (Map<?, ?> listMap : list) {	if (listMap.containsKey("name") && listMap.get("name").equals(userToSearch)) {	return true;	}	}	}	} catch (IOException e) {	String reason = getFailReason(KEY_RANGER_COULD_NOT_ACCESS, check, request);	warnReasons.add(String.format(reason, username, url, e.getMessage()));	} catch (Exception e) {	
could not determine user error is 

========================= ambari sample_2875 =========================

Map<String, Map<String, HostRoleCommand>> restartCommandsForHost = new HashMap<>();	Map<String, HostRoleCommand> restartCommandsByRole = new HashMap<>();	restartCommandsForHost.put(hostName, restartCommandsByRole);	for (ServiceComponentHost sch : cluster.getServiceComponentHosts(hostName)) {	if (!isVersionAdvertised(upgradeContext, sch)) {	continue;	}	HostsType hostsType = upgradeContext.getResolver().getMasterAndHosts( sch.getServiceName(), sch.getServiceComponentName());	if (null != hostsType && !hostsType.hosts.contains(hostName)) {	RepositoryVersionEntity targetRepositoryVersion = upgradeContext.getTargetRepositoryVersion( sch.getServiceName());	
host could not be orchestrated either there are no components for or the target version is already current 

continue;	}	if (!sch.isClientComponent()) {	stopTasks.add(new TaskWrapper(sch.getServiceName(), sch.getServiceComponentName(), Collections.singleton(hostName), new StopTask()));	}	Role role = Role.valueOf(sch.getServiceComponentName());	HostRoleCommand hostRoleCommand = hrcFactory.create(hostName, role, null, RoleCommand.START);	restartCommandsByRole.put(role.name(), hostRoleCommand);	}	if (stopTasks.isEmpty() && restartCommandsByRole.isEmpty()) {	
there were no commands generated for 

String stageTitle = String.format("Starting components on %s (phase %d)", hostName, phaseCounter++);	List<TaskWrapper> taskWrappers = new ArrayList<>();	for (HostRoleCommand command : stageCommandsForHost) {	StackId stackId = upgradeContext.getRepositoryVersion().getStackId();	String componentName = command.getRole().name();	String serviceName = null;	try {	AmbariMetaInfo ambariMetaInfo = upgradeContext.getAmbariMetaInfo();	serviceName = ambariMetaInfo.getComponentToService(stackId.getStackName(), stackId.getStackVersion(), componentName);	} catch (AmbariException ambariException) {	
unable to lookup service by component for stack 

return Collections.emptyList();	}	List<StageWrapper> wrappers = new ArrayList<>();	Cluster cluster = upgradeContext.getCluster();	for (String serviceName : serviceChecks) {	boolean hasService = false;	try {	cluster.getService(serviceName);	hasService = true;	} catch (Exception e) {	
service not found to orchestrate 

private boolean isVersionAdvertised(UpgradeContext upgradeContext, ServiceComponentHost sch) {	RepositoryVersionEntity targetRepositoryVersion = upgradeContext.getTargetRepositoryVersion( sch.getServiceName());	StackId targetStack = targetRepositoryVersion.getStackId();	try {	ComponentInfo component = upgradeContext.getAmbariMetaInfo().getComponent( targetStack.getStackName(), targetStack.getStackVersion(), sch.getServiceName(), sch.getServiceComponentName());	return component.isVersionAdvertised();	} catch (AmbariException e) {	
could not determine if could be upgraded returning false 

private RoleCommandOrder getRoleCommandOrderForUpgrade(Cluster cluster) {	RoleCommandOrder roleCommandOrder = cluster.getRoleCommandOrder();	try {	roleCommandOrder = (RoleCommandOrder) roleCommandOrder.clone();	} catch (CloneNotSupportedException cloneNotSupportedException) {	
unable to clone role command order and apply overrides for this upgrade 

========================= ambari sample_3225 =========================

public String readPasswordFromFile(String filePath, String defaultPassword) {	if (StringUtils.isBlank(filePath) || !fileExistsAndCanBeRead(filePath)) {	
db password file not specified or does not exist can not be read using default 

public String readPasswordFromFile(String filePath, String defaultPassword) {	if (StringUtils.isBlank(filePath) || !fileExistsAndCanBeRead(filePath)) {	return defaultPassword;	} else {	
reading password from file 

public String readPasswordFromStore(String aliasStr, File masterKeyLocation, boolean isMasterKeyPersisted, File masterKeyStoreLocation) {	String password = null;	loadCredentialProvider(masterKeyLocation, isMasterKeyPersisted, masterKeyStoreLocation);	if (credentialProvider != null) {	char[] result = null;	try {	result = credentialProvider.getPasswordForAlias(aliasStr);	} catch (AmbariException e) {	
error reading from credential store 

try {	result = credentialProvider.getPasswordForAlias(aliasStr);	} catch (AmbariException e) {	}	if (result != null) {	password = new String(result);	} else {	if (CredentialProvider.isAliasString(aliasStr)) {	LOG.error("Cannot read password for alias = " + aliasStr);	} else {	
raw password provided not an alias it cannot be read from credential store 

private void loadCredentialProvider(File masterKeyLocation, boolean isMasterKeyPersisted, File masterKeyStoreLocation) {	if (credentialProvider == null) {	try {	LOCK.lock();	credentialProvider = new CredentialProvider(null, masterKeyLocation, isMasterKeyPersisted, masterKeyStoreLocation);	} catch (Exception e) {	
credential provider creation failed 

========================= ambari sample_2967 =========================

public CommandReport execute(ConcurrentMap<String, Object> requestSharedDataContext) throws AmbariException, InterruptedException {	String clusterName = getExecutionCommand().getClusterName();	Cluster cluster = getClusters().getCluster(clusterName);	String ranger_admin_process = "ranger_admin_process";	try {	AlertDefinitionEntity rangerAlertDefinitionEntity = alertDefinitionDAO.findByName(cluster.getClusterId(), ranger_admin_process);	if (rangerAlertDefinitionEntity != null) {	
updating the alert definition for ranger admin process 

rangerAlertDefinitionEntity.setSource(rangerAlertsCurrentDefinitionJSON.toString());	alertDefinitionDAO.merge(rangerAlertDefinitionEntity);	Set<String> invalidatedHosts = alertDefinitionHash.invalidateHosts(rangerAlertDefinitionEntity);	AlertHashInvalidationEvent alertInvalidationEvent = new AlertHashInvalidationEvent( rangerAlertDefinitionEntity.getClusterId(), invalidatedHosts);	eventPublisher.publish(alertInvalidationEvent);	return createCommandReport(0, HostRoleStatus.COMPLETED, "{}", "Ranger service alert check configuration has been updated successfully.", "");	} else {	return createCommandReport(0, HostRoleStatus.COMPLETED, "{}", String.format( "The %s configuration type was not found; unable to update Ranger Alert properties.", ranger_admin_process), "");	}	} catch (Exception e) {	
rangerwebalertconfigaction execute there was an error in updating ranger alerts 

========================= ambari sample_3360 =========================

public void run() {	while (shouldRun) {	try {	doWork();	
putting monitor to sleep for milliseconds 

public void run() {	while (shouldRun) {	try {	doWork();	Thread.sleep(threadWakeupInterval);	} catch (InterruptedException ex) {	
scheduler thread is interrupted going to stop 

public void run() {	while (shouldRun) {	try {	doWork();	Thread.sleep(threadWakeupInterval);	} catch (InterruptedException ex) {	shouldRun = false;	} catch (Exception ex) {	
exception received 

public void run() {	while (shouldRun) {	try {	doWork();	Thread.sleep(threadWakeupInterval);	} catch (InterruptedException ex) {	shouldRun = false;	} catch (Exception ex) {	} catch (Throwable t) {	
ERROR 

if (hostObj.getState() == HostState.HEARTBEAT_LOST) {	continue;	}	String host = hostObj.getHostName();	HostState hostState = hostObj.getState();	String hostname = hostObj.getHostName();	long lastHeartbeat = 0;	try {	lastHeartbeat = clusters.getHost(host).getLastHeartbeatTime();	} catch (AmbariException e) {	
exception in getting host object is it fatal 

}	String host = hostObj.getHostName();	HostState hostState = hostObj.getState();	String hostname = hostObj.getHostName();	long lastHeartbeat = 0;	try {	lastHeartbeat = clusters.getHost(host).getLastHeartbeatTime();	} catch (AmbariException e) {	}	if (lastHeartbeat + 2 * threadWakeupInterval < now) {	
heartbeat lost from host 

lastHeartbeat = clusters.getHost(host).getLastHeartbeatTime();	} catch (AmbariException e) {	}	if (lastHeartbeat + 2 * threadWakeupInterval < now) {	hostObj.handleEvent(new HostHeartbeatLostEvent(host));	for (Cluster cluster : clusters.getClustersForHost(hostObj.getHostName())) {	for (ServiceComponentHost sch : cluster.getServiceComponentHosts(hostObj.getHostName())) {	Service s = cluster.getService(sch.getServiceName());	ServiceComponent sc = s.getServiceComponent(sch.getServiceComponentName());	if (!sc.isClientComponent() && !sch.getState().equals(State.INIT) && !sch.getState().equals(State.INSTALLING) && !sch.getState().equals(State.INSTALL_FAILED) && !sch.getState().equals(State.UNINSTALLED) && !sch.getState().equals(State.DISABLED)) {	
setting component state to unknown for component on 

sch.setState(State.UNKNOWN);	}	}	}	actionQueue.dequeueAll(host);	actionManager.handleLostHost(host);	}	if (hostState == HostState.WAITING_FOR_HOST_STATUS_UPDATES) {	long timeSpentInState = hostObj.getTimeInState();	if (timeSpentInState + 5 * threadWakeupInterval < now) {	
timespentinstate threadwakeupinterval now go back to init 

actionQueue.dequeueAll(host);	actionManager.handleLostHost(host);	}	if (hostState == HostState.WAITING_FOR_HOST_STATUS_UPDATES) {	long timeSpentInState = hostObj.getTimeInState();	if (timeSpentInState + 5 * threadWakeupInterval < now) {	hostObj.setState(HostState.INIT);	}	}	List<StatusCommand> cmds = generateStatusCommands(hostname);	
generated status commands for host 

StatusCommand statusCmd = new StatusCommand();	statusCmd.setClusterName(cluster.getClusterName());	statusCmd.setServiceName(serviceName);	statusCmd.setComponentName(componentName);	statusCmd.setConfigurations(configurations);	statusCmd.setConfigurationAttributes(configurationAttributes);	statusCmd.setHostname(hostname);	statusCmd.setDesiredState(sch.getDesiredState());	statusCmd.setHasStaleConfigs(configHelper.isStaleConfigs(sch, desiredConfigs));	if (getAgentRequests().shouldSendExecutionDetails(hostname, componentName)) {	
is at adding more payload per agent ask 

commandParams.put(COMMAND_TIMEOUT, commandTimeout);	commandParams.put(SERVICE_PACKAGE_FOLDER, serviceInfo.getServicePackageFolder());	commandParams.put(HOOKS_FOLDER, configuration.getProperty(Configuration.HOOKS_FOLDER));	Map<String, String> hostLevelParams = statusCmd.getHostLevelParams();	hostLevelParams.put(JDK_LOCATION, ambariManagementController.getJdkResourceUrl());	hostLevelParams.put(STACK_NAME, stackId.getStackName());	hostLevelParams.put(STACK_VERSION, stackId.getStackVersion());	if (statusCmd.getPayloadLevel() == StatusCommand.StatusCommandPayload.EXECUTION_COMMAND) {	ExecutionCommand ec = ambariManagementController.getExecutionCommand(cluster, sch, RoleCommand.START);	statusCmd.setExecutionCommand(ec);	
has more payload for execution command 

========================= ambari sample_2988 =========================

public Collection<User> getGroupMembers(String groupName) {	final GroupEntity groupEntity = groupDAO.findGroupByName(groupName);	if (groupEntity == null) {	return null;	} else {	final Set<User> users = new HashSet<>();	for (MemberEntity memberEntity : groupEntity.getMemberEntities()) {	if (memberEntity.getUser() != null) {	users.add(new User(memberEntity.getUser()));	} else {	
wrong state not found user for member group 

final GroupEntity groupEntity = groupDAO.findGroupByName(group.getGroupName());	allGroups.remove(groupEntity.getGroupName());	groupsToRemove.add(groupEntity);	}	groupDAO.remove(groupsToRemove);	final Set<UserEntity> userEntitiesToUpdate = new HashSet<>();	for (LdapUserDto user : batchInfo.getUsersToBecomeLdap()) {	String userName = user.getUserName();	UserEntity userEntity = userDAO.findUserByName(userName);	if (userEntity != null) {	
enabling ldap authentication for the user account with the username 

removeAuthentication(userEntity, userAuthenticationEntity.getUserAuthenticationId());	iterator.remove();	}	}	}	}	try {	addLdapAuthentication(userEntity, user.getDn(), false);	userEntitiesToUpdate.add(userEntity);	} catch (AmbariException e) {	
failed to enable ldap authentication for the user account with the username s s 

}	}	}	}	try {	addLdapAuthentication(userEntity, user.getDn(), false);	userEntitiesToUpdate.add(userEntity);	} catch (AmbariException e) {	}	} else {	
failed to find user account for while enabling ldap authentication for the user 

groupsToBecomeLdap.add(groupEntity);	}	groupDAO.merge(groupsToBecomeLdap);	final List<PrincipalEntity> principalsToCreate = new ArrayList<>();	for (LdapUserDto user : batchInfo.getUsersToBeCreated()) {	String userName = user.getUserName();	UserEntity userEntity;	try {	userEntity = createUser(userName, userName, userName, true);	} catch (AmbariException e) {	
failed to create new user s 

final List<PrincipalEntity> principalsToCreate = new ArrayList<>();	for (LdapUserDto user : batchInfo.getUsersToBeCreated()) {	String userName = user.getUserName();	UserEntity userEntity;	try {	userEntity = createUser(userName, userName, userName, true);	} catch (AmbariException e) {	userEntity = null;	}	if (userEntity != null) {	
enabling ldap authentication for the user account with the username 

UserEntity userEntity;	try {	userEntity = createUser(userName, userName, userName, true);	} catch (AmbariException e) {	userEntity = null;	}	if (userEntity != null) {	try {	addLdapAuthentication(userEntity, user.getDn(), false);	} catch (AmbariException e) {	
failed to enable ldap authentication for the user account with the username s s 

private void processLdapAdminGroupMappingRules(Set<MemberEntity> membershipsToCreate) {	String adminGroupMappings = ldapConfiguration.groupMappingRules();	if (Strings.isNullOrEmpty(adminGroupMappings) || membershipsToCreate.isEmpty()) {	
nothing to do ldap admin group mappings memberships to handle 

private void processLdapAdminGroupMappingRules(Set<MemberEntity> membershipsToCreate) {	String adminGroupMappings = ldapConfiguration.groupMappingRules();	if (Strings.isNullOrEmpty(adminGroupMappings) || membershipsToCreate.isEmpty()) {	return;	}	
processing admin group mapping rules membership entry count 

private void processLdapAdminGroupMappingRules(Set<MemberEntity> membershipsToCreate) {	String adminGroupMappings = ldapConfiguration.groupMappingRules();	if (Strings.isNullOrEmpty(adminGroupMappings) || membershipsToCreate.isEmpty()) {	return;	}	Set<String> ldapAdminGroups = Sets.newHashSet(adminGroupMappings.split(","));	Set<UserEntity> ambariAdminProspects = Sets.newHashSet();	for (MemberEntity memberEntity : membershipsToCreate) {	if (ldapAdminGroups.contains(memberEntity.getGroup().getGroupName())) {	
ambari admin user prospect 

return;	}	Set<String> ldapAdminGroups = Sets.newHashSet(adminGroupMappings.split(","));	Set<UserEntity> ambariAdminProspects = Sets.newHashSet();	for (MemberEntity memberEntity : membershipsToCreate) {	if (ldapAdminGroups.contains(memberEntity.getGroup().getGroupName())) {	ambariAdminProspects.add(memberEntity.getUser());	}	}	for (UserEntity userEntity : ambariAdminProspects) {	
granting ambari admin roles to the user 

userDAO.merge(userEntity);	return userEntity;	} catch (Throwable t) {	Throwable cause = t;	int failSafe = 50;	do {	if (cause instanceof OptimisticLockException) {	Integer userID = userEntity.getUserId();	userEntity = userDAO.findByPK(userID);	if (userEntity == null) {	
failed to find user with user id of the user may have been removed aborting 

int failSafe = 50;	do {	if (cause instanceof OptimisticLockException) {	Integer userID = userEntity.getUserId();	userEntity = userDAO.findByPK(userID);	if (userEntity == null) {	return null;	}	retriesLeft--;	if (retriesLeft == 0) {	
failed to update the user s consecutive failures value due to an optimisticlockexception aborting 

if (cause instanceof OptimisticLockException) {	Integer userID = userEntity.getUserId();	userEntity = userDAO.findByPK(userID);	if (userEntity == null) {	return null;	}	retriesLeft--;	if (retriesLeft == 0) {	throw t;	} else {	
failed to update the user s consecutive failures value due to an optimisticlockexception retries left retrying 

========================= ambari sample_2822 =========================

public Response createInputConfig(String clusterName, String serviceName, LSServerInputConfig inputConfig) {	try {	if (logSearchConfigConfigurer.getConfig().inputConfigExists(clusterName, serviceName)) {	return Response.serverError() .type(MediaType.APPLICATION_JSON) .entity(ImmutableMap.of("errorMessage", "Input config already exists for service " + serviceName)) .build();	}	logSearchConfigConfigurer.getConfig().createInputConfig(clusterName, serviceName, new ObjectMapper().writeValueAsString(inputConfig));	return Response.ok().build();	} catch (Exception e) {	
could not create input config 

public Response setInputConfig(String clusterName, String serviceName, LSServerInputConfig inputConfig) {	try {	if (!logSearchConfigConfigurer.getConfig().inputConfigExists(clusterName, serviceName)) {	return Response.serverError() .type(MediaType.APPLICATION_JSON) .entity(ImmutableMap.of("errorMessage", "Input config doesn't exist for service " + serviceName)) .build();	}	logSearchConfigConfigurer.getConfig().setInputConfig(clusterName, serviceName, new ObjectMapper().writeValueAsString(inputConfig));	return Response.ok().build();	} catch (Exception e) {	
could not update input config 

========================= ambari sample_1424 =========================

public PreviewData parsePreview() {	
generating preview for 

public PreviewData parsePreview() {	ArrayList<Row> previewRows;	List<ColumnDescription> header;	try {	numberOfPreviewRows = (Integer) parseOptions.getOption(ParseOptions.OPTIONS_NUMBER_OF_PREVIEW_ROWS);	} catch (Exception e) {	
illegal number of preview columns supplied 

headerRow = extractHeader();	numOfCols = headerRow.getRow().length;	}	Row r;	if (iterator().hasNext()) {	r = iterator().next();	if( null == numOfCols ) {	numOfCols = r.getRow().length;	}	} else {	
no rows found in the file returning error 

}else{	newValues[colNum] = null;	}	}	previewRows.add(new Row(newValues));	numberOfRows--;	if (numberOfRows <= 0 || !iterator().hasNext()) break;	r = iterator().next();	}	if (previewRows.size() <= 0) {	
no rows found in the file returning error 

private List<ColumnDescription> generateHeader(Row headerRow,List<Row> previewRows, int numOfCols) {	List<ColumnDescription> header = new ArrayList<>();	for (int colNum = 0; colNum < numOfCols; colNum++) {	ColumnDescription.DataTypes type = getLikelyDataType(previewRows,colNum);	
datatype detected for column 

private List<ColumnDescription> generateHeader(Row headerRow,List<Row> previewRows, int numOfCols) {	List<ColumnDescription> header = new ArrayList<>();	for (int colNum = 0; colNum < numOfCols; colNum++) {	ColumnDescription.DataTypes type = getLikelyDataType(previewRows,colNum);	String colName = COLUMN_PREFIX + (colNum + 1);	if (null != headerRow) colName = (String) headerRow.getRow()[colNum];	ColumnDescription cd = new ColumnDescriptionImpl(colName, type.toString(), colNum);	header.add(cd);	}	
return headers 

========================= ambari sample_810 =========================

public PreviewData parsePreview() {	
generating preview for 

public PreviewData parsePreview() {	ArrayList<Row> previewRows;	List<ColumnInfo> header;	try {	numberOfPreviewRows = (Integer) parseOptions.getOption(ParseOptions.OPTIONS_NUMBER_OF_PREVIEW_ROWS);	} catch (Exception e) {	
illegal number of preview columns supplied 

headerRow = extractHeader();	numOfCols = headerRow.getRow().length;	}	Row r;	if (iterator().hasNext()) {	r = iterator().next();	if( null == numOfCols ) {	numOfCols = r.getRow().length;	}	} else {	
no rows found in the file returning error 

}else{	newValues[colNum] = null;	}	}	previewRows.add(new Row(newValues));	numberOfRows--;	if (numberOfRows <= 0 || !iterator().hasNext()) break;	r = iterator().next();	}	if (previewRows.size() <= 0) {	
no rows found in the file returning error 

private List<ColumnInfo> generateHeader(Row headerRow, List<Row> previewRows, int numOfCols) {	List<ColumnInfo> header = new ArrayList<>();	for (int colNum = 0; colNum < numOfCols; colNum++) {	ColumnDescription.DataTypes type = getLikelyDataType(previewRows,colNum);	
datatype detected for column 

private List<ColumnInfo> generateHeader(Row headerRow, List<Row> previewRows, int numOfCols) {	List<ColumnInfo> header = new ArrayList<>();	for (int colNum = 0; colNum < numOfCols; colNum++) {	ColumnDescription.DataTypes type = getLikelyDataType(previewRows,colNum);	String colName = COLUMN_PREFIX + (colNum + 1);	if (null != headerRow) colName = (String) headerRow.getRow()[colNum];	ColumnInfo cd = new ColumnInfo(colName, type.toString());	header.add(cd);	}	
return headers 

========================= ambari sample_543 =========================

public void runTask() {	
hostrequest executing resource creation task for host 

public void runTask() {	HostGroup group = hostRequest.getHostGroup();	Map<String, Collection<String>> serviceComponents = new HashMap<>();	for (String service : group.getServices()) {	serviceComponents.put(service, new HashSet<>(group.getComponents(service)));	}	clusterTopology.getAmbariContext().createAmbariHostResources(hostRequest.getClusterId(), hostRequest.getHostName(), serviceComponents);	
hostrequest exiting resource creation task for host 

========================= ambari sample_2658 =========================

public void addOutput(Output output) {	try {	Class<? extends Output> clazz = output.getClass();	Output outputCopy = clazz.newInstance();	outputCopy.loadConfig(output.getConfigs());	outputCopy.setDestination(output.getDestination());	simulateOutputs.add(outputCopy);	super.addOutput(outputCopy);	} catch (Exception e) {	
could not copy output class using original output 

========================= ambari sample_1633 =========================

public CredentialStoreServiceImpl(Configuration configuration, SecurePasswordHelper securePasswordHelper) {	this.securePasswordHelper = securePasswordHelper;	if (configuration != null) {	File masterKeyLocation = configuration.getMasterKeyLocation();	try {	initializeTemporaryCredentialStore(configuration.getTemporaryKeyStoreRetentionMinutes(), TimeUnit.MINUTES, configuration.isActivelyPurgeTemporaryKeyStore());	
initialized the temporary credential store keystore entries will be retained for minutes and be actively purged will will not 

public CredentialStoreServiceImpl(Configuration configuration, SecurePasswordHelper securePasswordHelper) {	this.securePasswordHelper = securePasswordHelper;	if (configuration != null) {	File masterKeyLocation = configuration.getMasterKeyLocation();	try {	initializeTemporaryCredentialStore(configuration.getTemporaryKeyStoreRetentionMinutes(), TimeUnit.MINUTES, configuration.isActivelyPurgeTemporaryKeyStore());	} catch (AmbariException e) {	
failed to initialize the temporary credential store storage of temporary credentials will fail 

}	MasterKeyService masterKeyService = null;	if(masterKeyLocation.exists()) {	masterKeyService = new MasterKeyServiceImpl(masterKeyLocation);	} else {	masterKeyService = new MasterKeyServiceImpl();	}	if (masterKeyService.isMasterKeyInitialized()) {	try {	initializePersistedCredentialStore(configuration.getMasterKeyStoreLocation(), masterKeyService);	
initialized the persistent credential store using keystore file at 

MasterKeyService masterKeyService = null;	if(masterKeyLocation.exists()) {	masterKeyService = new MasterKeyServiceImpl(masterKeyLocation);	} else {	masterKeyService = new MasterKeyServiceImpl();	}	if (masterKeyService.isMasterKeyInitialized()) {	try {	initializePersistedCredentialStore(configuration.getMasterKeyStoreLocation(), masterKeyService);	} catch (AmbariException e) {	
failed to initialize the persistent credential store storage of persisted credentials will fail 

========================= ambari sample_2766 =========================

stateMachine = daemonStateMachineFactory.make(this);	}	HostEntity hostEntity = null;	try {	host = clusters.getHost(hostName);	hostEntity = hostDAO.findByName(hostName);	if (hostEntity == null) {	throw new AmbariException("Could not find host " + hostName);	}	} catch (AmbariException e) {	
host was not found 

hostComponentStateId = stateEntity.getId();	if (serviceComponent.isClientComponent()) {	stateMachine = clientStateMachineFactory.make(this);	} else {	stateMachine = daemonStateMachineFactory.make(this);	}	stateMachine.setCurrentState(stateEntity.getCurrentState());	try {	host = clusters.getHost(stateEntity.getHostName());	} catch (AmbariException e) {	
host was not found 

RepositoryVersionEntity repositoryVersion = serviceComponent.getDesiredRepositoryVersion();	if (null != repositoryVersion) {	desiredRepositoryVersion = repositoryVersion.getVersion();	}	ServiceComponentHostResponse r = new ServiceComponentHostResponse(clusterName, serviceName, serviceComponentName, displayName, hostName, publicHostName, state, getVersion(), desiredState, desiredStackId, desiredRepositoryVersion, componentAdminState);	r.setActualConfigs(actualConfigs);	r.setUpgradeState(upgradeState);	try {	r.setStaleConfig(helper.isStaleConfigs(this, desiredConfigs, hostComponentDesiredStateEntity));	} catch (Exception e) {	
could not determine stale config 

r.setStaleConfig(helper.isStaleConfigs(this, desiredConfigs, hostComponentDesiredStateEntity));	} catch (Exception e) {	}	try {	Cluster cluster = clusters.getCluster(clusterName);	ServiceComponent serviceComponent = cluster.getService(serviceName).getServiceComponent(serviceComponentName);	ServiceComponentHost sch = serviceComponent.getServiceComponentHost(hostName);	String refreshConfigsCommand = helper.getRefreshConfigsCommand(cluster,sch);	r.setReloadConfig(refreshConfigsCommand != null);	} catch (Exception e) {	
could not determine reload config flag 

public void delete() {	boolean fireRemovalEvent = false;	writeLock.lock();	try {	removeEntities();	fireRemovalEvent = true;	clusters.getCluster(getClusterName()).removeServiceComponentHost(this);	} catch (AmbariException ex) {	
unable to remove a service component from a host 

public void updateActualConfigs(Map<String, Map<String, String>> configTags) {	Map<Long, ConfigGroup> configGroupMap;	String clusterName = getClusterName();	try {	Cluster cluster = clusters.getCluster(clusterName);	configGroupMap = cluster.getConfigGroups();	} catch (AmbariException e) {	
unable to find cluster 

public void updateActualConfigs(Map<String, Map<String, String>> configTags) {	Map<Long, ConfigGroup> configGroupMap;	String clusterName = getClusterName();	try {	Cluster cluster = clusters.getCluster(clusterName);	configGroupMap = cluster.getConfigGroups();	} catch (AmbariException e) {	return;	}	
updating configuration tags for 

========================= ambari sample_3198 =========================

try {	String authToLocalRules = getAuthToLocalRules();	String defaultRealm = KerberosUtil.getDefaultRealm();	if(Strings.isNotEmpty(authToLocalRules) && Strings.isNotEmpty(defaultRealm)){	synchronized (KerberosName.class){	KerberosName.setRules(authToLocalRules);	shortName = new KerberosName(shortName+"@"+defaultRealm).getShortName();	}	}	} catch (InvocationTargetException e) {	
failed to get default realm 

String authToLocalRules = getAuthToLocalRules();	String defaultRealm = KerberosUtil.getDefaultRealm();	if(Strings.isNotEmpty(authToLocalRules) && Strings.isNotEmpty(defaultRealm)){	synchronized (KerberosName.class){	KerberosName.setRules(authToLocalRules);	shortName = new KerberosName(shortName+"@"+defaultRealm).getShortName();	}	}	} catch (InvocationTargetException e) {	}catch (Exception e){	
failed to apply auth to local rules 

String authToLocalRules = getAuthToLocalRules();	String defaultRealm = KerberosUtil.getDefaultRealm();	if(Strings.isNotEmpty(authToLocalRules) && Strings.isNotEmpty(defaultRealm)){	synchronized (KerberosName.class){	KerberosName.setRules(authToLocalRules);	shortName = new KerberosName(shortName+"@"+defaultRealm).getShortName();	}	}	} catch (InvocationTargetException e) {	}catch (Exception e){	
failed to apply auth to local rules 

ParameterConfig parameterConfig = parameters.get(propertyName);	if (parameterConfig != null) {	String clusterConfig = parameterConfig.getClusterConfig();	if (clusterConfig != null && cluster != null) {	propertyValue = getClusterConfigurationValue(cluster, clusterConfig);	} else {	if (parameterConfig.isMasked()) {	try {	propertyValue = masker.unmask(propertyValue);	} catch (MaskException e) {	
failed to unmask view property 

private String evaluatePropertyTemplates(String rawValue) {	if (rawValue != null) {	try {	Writer templateWriter = new StringWriter();	Velocity.evaluate(velocityContext, templateWriter, rawValue, rawValue);	return templateWriter.toString();	} catch (ParseErrorException e) {	
error during parsing s parameter leaving original value 

========================= ambari sample_3931 =========================

public File perform(File inputFile) {	File outputFile =  onPerform(inputFile);	
file was not deleted exists 

========================= ambari sample_168 =========================

public void onTaskUpdateEvent(TaskUpdateEvent event) {	
received task update event 

public void onTaskUpdateEvent(TaskUpdateEvent event) {	List<HostRoleCommand> hostRoleCommandListAll = event.getHostRoleCommands();	List<HostRoleCommand>  hostRoleCommandWithReceivedStatus =  new ArrayList<>();	Set<StageEntityPK> stagesWithReceivedTaskStatus = new HashSet<>();	Set<Long> requestIdsWithReceivedTaskStatus =  new HashSet<>();	for (HostRoleCommand hostRoleCommand : hostRoleCommandListAll) {	Long reportedTaskId = hostRoleCommand.getTaskId();	HostRoleCommand activeTask =  activeTasksMap.get(reportedTaskId);	if (activeTask == null) {	
received update for a task d which is not being tracked as running task 

public void onTaskCreateEvent(TaskCreateEvent event) {	
received task create event 

Boolean didAnyStageStatusUpdated = Boolean.FALSE;	for (StageEntityPK reportedStagePK : stagesWithReceivedTaskStatus) {	if (activeStageMap.containsKey(reportedStagePK)) {	Boolean didStatusChange = updateStageStatus(reportedStagePK, hostRoleCommandListAll);	if (didStatusChange) {	ActiveStage reportedStage = activeStageMap.get(reportedStagePK);	stageDAO.updateStatus(reportedStagePK, reportedStage.getStatus(), reportedStage.getDisplayStatus());	didAnyStageStatusUpdated = Boolean.TRUE;	}	} else {	
received update for a task whose stage is not being tracked as running stage s 

if (activeRequestMap.containsKey(reportedRequestId)) {	ActiveRequest request =  activeRequestMap.get(reportedRequestId);	Boolean didStatusChange = updateRequestStatus(reportedRequestId, stagesWithChangedTaskStatus);	if (didStatusChange) {	requestDAO.updateStatus(reportedRequestId, request.getStatus(), request.getDisplayStatus());	}	if (request.isCompleted() && isAllTasksCompleted(reportedRequestId)) {	removeRequestStageAndTasks(reportedRequestId);	}	} else {	
received update for a task whose request d is not being tracked as running request 

private void removeTasks(Long requestId) {	Iterator<Map.Entry<Long, HostRoleCommand>> iter = activeTasksMap.entrySet().iterator();	while (iter.hasNext()) {	Map.Entry<Long, HostRoleCommand> entry = iter.next();	HostRoleCommand hrc = entry.getValue();	if (hrc.getRequestId() == requestId) {	if (!hrc.getStatus().isCompletedState()) {	
task d should have been completed before being removed from running task cache activetasksmap 

========================= ambari sample_4523 =========================

public void execute() {	long currentTime = System.nanoTime();	List<Future<Map<StackModule, RepoUrlInfoResult>>> results = new ArrayList<>();	try {	results = executor.invokeAll(tasks.values(), 2, TimeUnit.MINUTES);	} catch (InterruptedException e) {	
could not load urlinfo as the executor was interrupted 

public void execute() {	long currentTime = System.nanoTime();	List<Future<Map<StackModule, RepoUrlInfoResult>>> results = new ArrayList<>();	try {	results = executor.invokeAll(tasks.values(), 2, TimeUnit.MINUTES);	} catch (InterruptedException e) {	return;	} finally {	
loaded urlinfo in ms 

results = executor.invokeAll(tasks.values(), 2, TimeUnit.MINUTES);	} catch (InterruptedException e) {	return;	} finally {	}	List<Map<StackModule, RepoUrlInfoResult>> urlInfoResults = new ArrayList<>();	for (Future<Map<StackModule, RepoUrlInfoResult>> future : results) {	try {	urlInfoResults.add(future.get());	} catch (Exception e) {	
could not load repo results 

if (MapUtils.isNotEmpty(result.getLatestVdf())) {	futures.add(executor.submit( new RepoVdfCallable(stackModule, result.getLatestVdf(), m_family)));	}	}	}	}	executor.shutdown();	try {	executor.awaitTermination(2,  TimeUnit.MINUTES);	} catch (InterruptedException e) {	
loading all vdf was interrupted 

futures.add(executor.submit( new RepoVdfCallable(stackModule, result.getLatestVdf(), m_family)));	}	}	}	}	executor.shutdown();	try {	executor.awaitTermination(2,  TimeUnit.MINUTES);	} catch (InterruptedException e) {	} finally {	
loaded all vdf in ms 

========================= ambari sample_4466 =========================

private Map<String, Collection<DependencyInfo>> validateHostGroup(HostGroup group) {	
validating hostgroup 

private Map<String, Collection<DependencyInfo>> validateHostGroup(HostGroup group) {	Map<String, Collection<DependencyInfo>> missingDependencies = new HashMap<>();	for (String component : new HashSet<>(group.getComponentNames())) {	
processing component 

private Map<String, Collection<DependencyInfo>> validateHostGroup(HostGroup group) {	Map<String, Collection<DependencyInfo>> missingDependencies = new HashMap<>();	for (String component : new HashSet<>(group.getComponentNames())) {	for (DependencyInfo dependency : stack.getDependenciesForComponent(component)) {	
processing dependency for component 

private Map<String, Collection<DependencyInfo>> validateHostGroup(HostGroup group) {	Map<String, Collection<DependencyInfo>> missingDependencies = new HashMap<>();	for (String component : new HashSet<>(group.getComponentNames())) {	for (DependencyInfo dependency : stack.getDependenciesForComponent(component)) {	String conditionalService = stack.getConditionalServiceForDependency(dependency);	if (conditionalService != null && !blueprint.getServices().contains(conditionalService)) {	
conditional service is missing from the blueprint skipping dependency 

private Map<String, Collection<DependencyInfo>> validateHostGroup(HostGroup group) {	Map<String, Collection<DependencyInfo>> missingDependencies = new HashMap<>();	for (String component : new HashSet<>(group.getComponentNames())) {	for (DependencyInfo dependency : stack.getDependenciesForComponent(component)) {	String conditionalService = stack.getConditionalServiceForDependency(dependency);	if (conditionalService != null && !blueprint.getServices().contains(conditionalService)) {	continue;	}	boolean isClientDependency = stack.getComponentInfo(dependency.getComponentName()).isClient();	if (isClientDependency && !blueprint.getServices().contains(dependency.getServiceName())) {	
the service for component is missing from the blueprint skipping dependency 

========================= ambari sample_2674 =========================

Collection<Workflow> workflows = this.dataStore.findAll(Workflow.class, "workflowDefinitionPath='" + path + "'");	if (workflows == null || workflows.isEmpty()) {	return null;	} else {	List<Workflow> myWorkflows = filterWorkflows(workflows, userName, true);	if (myWorkflows.isEmpty()) {	return null;	} else if (myWorkflows.size() == 1) {	return myWorkflows.get(0);	} else {	
duplicate workflows found having same path 

========================= ambari sample_1058 =========================

private void parseMetaInfoFile() throws AmbariException {	File stackMetaInfoFile = new File(getAbsolutePath() + File.separator + STACK_METAINFO_FILE_NAME);	if (stackMetaInfoFile.exists()) {	if (LOG.isDebugEnabled()) {	
reading stack version metainfo from file 

if (subDirs.contains(ServiceDirectory.SERVICES_FOLDER_NAME)) {	String servicesDir = getAbsolutePath() + File.separator + ServiceDirectory.SERVICES_FOLDER_NAME;	File baseServiceDir = new File(servicesDir);	File[] serviceFolders = baseServiceDir.listFiles(FILENAME_FILTER);	if (serviceFolders != null) {	for (File d : serviceFolders) {	if (d.isDirectory()) {	try {	dirs.add(new StackServiceDirectory(d.getAbsolutePath()));	} catch (AmbariException e) {	
unable to parse stack definition service at s ignoring service s 

if (d.isDirectory()) {	try {	dirs.add(new StackServiceDirectory(d.getAbsolutePath()));	} catch (AmbariException e) {	}	}	}	}	}	if (dirs.isEmpty()) {	
the stack defined at contains no services 

} else {	String upgradePackName = FilenameUtils.removeExtension(upgradeFile.getName());	UpgradePack pack = parseUpgradePack(upgradePackName, upgradeFile);	pack.setName(upgradePackName);	upgradeMap.put(upgradePackName, pack);	}	}	}	}	if (upgradesDir == null) {	
stack doesn t contain an upgrade directory 

}	if (!upgradeMap.isEmpty()) {	upgradePacks = upgradeMap;	}	if (configUpgradePack != null) {	this.configUpgradePack = configUpgradePack;	} else {	ConfigUpgradePack emptyConfigUpgradePack = new ConfigUpgradePack();	emptyConfigUpgradePack.services = new ArrayList<>();	this.configUpgradePack = emptyConfigUpgradePack;	
stack doesn t contain config upgrade pack file 

private void parseRoleCommandOrder() {	HashMap<String, Object> result = null;	ObjectMapper mapper = new ObjectMapper();	try {	TypeReference<Map<String, Object>> rcoElementTypeReference = new TypeReference<Map<String, Object>>() {	};	if (rcoFilePath != null) {	File file = new File(rcoFilePath);	result = mapper.readValue(file, rcoElementTypeReference);	
role command order info was loaded from file 

private void parseRoleCommandOrder() {	HashMap<String, Object> result = null;	ObjectMapper mapper = new ObjectMapper();	try {	TypeReference<Map<String, Object>> rcoElementTypeReference = new TypeReference<Map<String, Object>>() {	};	if (rcoFilePath != null) {	File file = new File(rcoFilePath);	result = mapper.readValue(file, rcoElementTypeReference);	} else {	
stack doesn t contain role command order file 

TypeReference<Map<String, Object>> rcoElementTypeReference = new TypeReference<Map<String, Object>>() {	};	if (rcoFilePath != null) {	File file = new File(rcoFilePath);	result = mapper.readValue(file, rcoElementTypeReference);	} else {	result = new HashMap<>();	}	roleCommandOrder = new StackRoleCommandOrder(result);	if (LOG.isDebugEnabled()) {	
role command order for 

File file = new File(rcoFilePath);	result = mapper.readValue(file, rcoElementTypeReference);	} else {	result = new HashMap<>();	}	roleCommandOrder = new StackRoleCommandOrder(result);	if (LOG.isDebugEnabled()) {	roleCommandOrder.printRoleCommandOrder(LOG);	}	} catch (IOException e) {	
can not read role command order info s 

========================= ambari sample_4470 =========================

public void init(Map<String, String> properties, String clusterName) throws Exception {	super.init(properties);	while (client.checkExists().forPath("/") == null) {	
root node is not present yet going to sleep for seconds 

Type eventType = event.getType();	String configPathStab = String.format("/%s/", clusterName);	if (event.getData().getPath().startsWith(configPathStab + "input/")) {	handleInputConfigChange(eventType, nodeName, nodeData);	} else if (event.getData().getPath().startsWith(configPathStab + "loglevelfilter/")) {	handleLogLevelFilterChange(eventType, nodeName, nodeData);	}	}	private void handleInputConfigChange(Type eventType, String nodeName, String nodeData) {	switch (eventType) {	
node added under input zk node 

if (event.getData().getPath().startsWith(configPathStab + "input/")) {	handleInputConfigChange(eventType, nodeName, nodeData);	} else if (event.getData().getPath().startsWith(configPathStab + "loglevelfilter/")) {	handleLogLevelFilterChange(eventType, nodeName, nodeData);	}	}	private void handleInputConfigChange(Type eventType, String nodeName, String nodeData) {	switch (eventType) {	addInputs(nodeName, nodeData);	break;	
node updated under input zk node 

handleLogLevelFilterChange(eventType, nodeName, nodeData);	}	}	private void handleInputConfigChange(Type eventType, String nodeName, String nodeData) {	switch (eventType) {	addInputs(nodeName, nodeData);	break;	removeInputs(nodeName);	addInputs(nodeName, nodeData);	break;	
node removed from input zk node 

JsonElement inputConfigJson = parser.parse(inputConfig);	for (Map.Entry<String, JsonElement> typeEntry : inputConfigJson.getAsJsonObject().entrySet()) {	for (JsonElement e : typeEntry.getValue().getAsJsonArray()) {	for (JsonElement globalConfig : globalConfigNode) {	merge(globalConfig.getAsJsonObject(), e.getAsJsonObject());	}	}	}	inputConfigMonitor.loadInputConfigs(serviceName, InputConfigGson.gson.fromJson(inputConfigJson, InputConfigImpl.class));	} catch (Exception e) {	
could not load input configuration for service 

merge(globalConfig.getAsJsonObject(), e.getAsJsonObject());	}	}	}	inputConfigMonitor.loadInputConfigs(serviceName, InputConfigGson.gson.fromJson(inputConfigJson, InputConfigImpl.class));	} catch (Exception e) {	}	}	private void handleLogLevelFilterChange(Type eventType, String nodeName, String nodeData) {	switch (eventType) {	
node added updated under loglevelfilter zk node 

}	inputConfigMonitor.loadInputConfigs(serviceName, InputConfigGson.gson.fromJson(inputConfigJson, InputConfigImpl.class));	} catch (Exception e) {	}	}	private void handleLogLevelFilterChange(Type eventType, String nodeName, String nodeData) {	switch (eventType) {	LogLevelFilter logLevelFilter = gson.fromJson(nodeData, LogLevelFilter.class);	logLevelFilterMonitor.setLogLevelFilter(nodeName, logLevelFilter);	break;	
node removed loglevelfilter input zk node 

private void createGlobalConfigNode(JsonArray globalConfigNode, String clusterName) {	String globalConfigNodePath = String.format("/%s/global", clusterName);	String data = InputConfigGson.gson.toJson(globalConfigNode);	try {	if (logFeederClusterCache.getCurrentData(globalConfigNodePath) != null) {	client.setData().forPath(globalConfigNodePath, data.getBytes());	} else {	client.create().creatingParentContainersIfNeeded().withACL(getAcls()).forPath(globalConfigNodePath, data.getBytes());	}	} catch (Exception e) {	
exception during global config node creation update 

public void monitorOutputProperties(final List<? extends OutputConfigMonitor> outputConfigMonitors) throws Exception {	TreeCacheListener listener = new TreeCacheListener() {	public void childEvent(CuratorFramework client, TreeCacheEvent event) throws Exception {	if (event.getType() != Type.NODE_UPDATED) {	return;	}	
output config updated 

========================= ambari sample_1672 =========================

protected void fixYarnHsiKerberosDescriptorAndSiteConfig(Cluster cluster, KerberosDescriptor kerberosDescriptor) {	
updating yarn s hsi kerberos descriptor 

AmbariManagementController ambariManagementController = injector.getInstance(AmbariManagementController.class);	Clusters clusters = ambariManagementController.getClusters();	if (clusters != null) {	Map<String, Cluster> clusterMap = getCheckedClusterMap(clusters);	if (clusterMap != null && !clusterMap.isEmpty()) {	for (final Cluster cluster : clusterMap.values()) {	Config amsSslClient = cluster.getDesiredConfigByType(AMS_SSL_CLIENT);	if (amsSslClient != null) {	Map<String, String> amsSslClientProperties = amsSslClient.getProperties();	if (amsSslClientProperties.containsKey(METRIC_TRUSTSTORE_ALIAS)) {	
removing from 

protected void updateHDFSWidgetDefinition() throws AmbariException {	
updating hdfs widget definition 

private Cluster getCluster(ArtifactEntity artifactEntity) {	if (artifactEntity != null) {	Map<String, String> keys = artifactEntity.getForeignKeys();	if (keys != null) {	String clusterId = keys.get("cluster");	if (StringUtils.isNumeric(clusterId)) {	Clusters clusters = injector.getInstance(Clusters.class);	try {	return clusters.getCluster(Long.valueOf(clusterId));	} catch (AmbariException e) {	
failed to obtain cluster using cluster id s s 

Map<String, String> keys = artifactEntity.getForeignKeys();	if (keys != null) {	String clusterId = keys.get("cluster");	if (StringUtils.isNumeric(clusterId)) {	Clusters clusters = injector.getInstance(Clusters.class);	try {	return clusters.getCluster(Long.valueOf(clusterId));	} catch (AmbariException e) {	}	} else {	
failed to obtain cluster id from artifact entity with foreign keys s 

========================= ambari sample_2727 =========================

throw new IllegalArgumentException("Components list contains a component with no 'name' property");	}	String serviceName = null;	try {	serviceName = ami.getComponentToService(stackName, stackVersion, componentName.trim().toUpperCase());	if (serviceName == null) {	throw new IllegalArgumentException("Service not found for component : " + componentName);	}	servicesOnHost.add(serviceName);	} catch (AmbariException e) {	
service not found for component 

========================= ambari sample_3566 =========================

private void setupStatusDir() {	String newDirPrefix = makeStatusDirectoryPrefix();	String newDir = null;	try {	newDir = HdfsUtil.findUnallocatedFileName(hdfsApi, newDirPrefix, "");	} catch (HdfsApiException e) {	throw new ServiceFormattedException(e);	}	job.setStatusDir(newDir);	
status dir for job 

throw new BadRequestFormattedException("queryId or forcedContent should be passed!", null);	}	} catch (IOException e) {	throw new ServiceFormattedException("F040 Error when creating file " + jobQueryFilePath, e);	} catch (InterruptedException e) {	throw new ServiceFormattedException("F040 Error when creating file " + jobQueryFilePath, e);	} catch (HdfsApiException e) {	throw new ServiceFormattedException(e);	}	job.setQueryFile(jobQueryFilePath);	
query file for job 

========================= ambari sample_579 =========================

result = new ResultImpl(new ResultStatus(ResultStatus.STATUS.BAD_REQUEST, e.getMessage()));	} catch (NoSuchResourceException e) {	if (p == null) {	result = new ResultImpl(new ResultStatus(ResultStatus.STATUS.NOT_FOUND, e.getMessage()));	} else {	result = new ResultImpl(new ResultStatus(ResultStatus.STATUS.OK, e));	result.getResultTree().setProperty("isCollection", "true");	}	} catch (IllegalArgumentException e) {	result = new ResultImpl(new ResultStatus(ResultStatus.STATUS.BAD_REQUEST, "Invalid Request: " + e.getMessage()));	
bad request 

if (p == null) {	result = new ResultImpl(new ResultStatus(ResultStatus.STATUS.NOT_FOUND, e.getMessage()));	} else {	result = new ResultImpl(new ResultStatus(ResultStatus.STATUS.OK, e));	result.getResultTree().setProperty("isCollection", "true");	}	} catch (IllegalArgumentException e) {	result = new ResultImpl(new ResultStatus(ResultStatus.STATUS.BAD_REQUEST, "Invalid Request: " + e.getMessage()));	}  catch (RuntimeException e) {	if (LOG.isErrorEnabled()) {	
caught a runtime exception executing a query 

========================= ambari sample_4098 =========================

public void createServices(Set<ServiceRequest> requests) throws AmbariException, AuthorizationException {	if (requests.isEmpty()) {	
received an empty requests set 

protected RequestStageContainer updateServices(RequestStageContainer requestStages, Set<ServiceRequest> requests, Map<String, String> requestProperties, boolean runSmokeTest, boolean reconfigureClients, boolean startDependencies) throws AmbariException, AuthorizationException {	AmbariManagementController controller = getManagementController();	if (requests.isEmpty()) {	
received an empty requests set 

}	if (StringUtils.isNotEmpty(request.getCredentialStoreEnabled())) {	boolean credentialStoreEnabled = Boolean.parseBoolean(request.getCredentialStoreEnabled());	if (!s.isCredentialStoreSupported() && credentialStoreEnabled) {	throw new IllegalArgumentException("Invalid arguments, cannot enable credential store " + "as it is not supported by the service. Service=" + s.getName());	}	if (s.isCredentialStoreRequired() && !credentialStoreEnabled) {	throw new IllegalArgumentException("Invalid arguments, cannot disable credential store " + "as it is required by the service. Service=" + s.getName());	}	serviceCredentialStoreEnabledMap.put(s, credentialStoreEnabled);	
service credential store enabled from request 

if (StringUtils.isNotEmpty(request.getCredentialStoreSupported())) {	throw new IllegalArgumentException("Invalid arguments, cannot update credential_store_supported " + "as it is set only via service definition. Service=" + s.getName());	}	if (newState == null) {	if (LOG.isDebugEnabled()) {	LOG.debug("Nothing to do for new updateService request, clusterName={}, serviceName={}, newDesiredState=null", request.getClusterName(), request.getServiceName());	}	continue;	}	if (! maintenanceStateHelper.isOperationAllowed(reqOpLvl, s)) {	
operations cannot be applied to service in the maintenance state of 

}	if (sc.isClientComponent() && !newState.isValidClientComponentState()) {	continue;	}	if (! isValidStateTransition(requestStages, oldSchState, newState, sch)) {	String error = "Invalid transition for" + " servicecomponenthost" + ", clusterName=" + cluster.getClusterName() + ", clusterId=" + cluster.getClusterId() + ", serviceName=" + sch.getServiceName() + ", componentName=" + sch.getServiceComponentName() + ", hostname=" + sch.getHostName() + ", currentState=" + oldSchState + ", newDesiredState=" + newState;	StackId sid = service.getDesiredStackId();	if ( ambariMetaInfo.getComponent( sid.getStackName(), sid.getStackVersion(), sc.getServiceName(), sch.getServiceComponentName()).isMaster()) {	throw new IllegalArgumentException(error);	} else {	
ignoring 

if (null == desiredRepositoryVersion) {	Set<Long> repoIds = new HashSet<>();	for (Service service : cluster.getServices().values()) {	RepositoryVersionEntity serviceRepo = service.getDesiredRepositoryVersion();	if (null != serviceRepo.getParentId()) {	repoIds.add(serviceRepo.getParentId());	} else {	repoIds.add(serviceRepo.getId());	}	}	
was not specified the following repository ids were found 

========================= ambari sample_3618 =========================

public Reader getReader(File file) throws FileNotFoundException {	
inside reader factory for file 

public Reader getReader(File file) throws FileNotFoundException {	if (GZIPReader.isValidFile(file.getAbsolutePath())) {	
reading file as gzip file 

========================= ambari sample_1625 =========================

protected void delete(String deleteQueryText) {	try (CloudSolrClient client = createClient()) {	try {	
executing solr delete by query 

protected void delete(String deleteQueryText) {	try (CloudSolrClient client = createClient()) {	try {	client.deleteByQuery(deleteQueryText);	client.commit();	} catch (Exception e) {	try {	client.rollback();	} catch (SolrServerException e1) {	
unable to rollback after solr delete operation failure 

========================= ambari sample_191 =========================

private InputStream getInputStream(String spec, String requestMethod, Map<String, String> headers, byte[] info) throws IOException {	if (!isProxyCallAllowed(spec)) {	
call to is not allowed see ambari properties proxy allowed hostports 

private HttpURLConnection getHttpURLConnection(String spec, String requestMethod, Map<String, String> headers, byte[] info) throws IOException {	if (!isProxyCallAllowed(spec)) {	
call to is not allowed see ambari properties proxy allowed hostports 

public HostPortRestrictionHandler(String allowedHostPortsValue) {	this.allowedHostPortsValue = allowedHostPortsValue;	
proxy restriction will be derived from 

public boolean allowProxy(String host, String port) {	
checking host port against allowed list 

if (!allowedStr.equals(Configuration.PROXY_ALLOWED_HOST_PORTS.getDefaultValue())) {	proxyCallRestricted = true;	String[] hostPorts = allowedStr.trim().split(",");	for (String hostPortStr : hostPorts) {	String[] hostAndPort = hostPortStr.trim().split(":");	if (hostAndPort.length == 1) {	if (!allowed.containsKey(hostAndPort[0])) {	allowed.put(hostAndPort[0], new HashSet<>());	}	allowed.get(hostAndPort[0]).add("*");	
allow proxy to host and all ports 

if (hostAndPort.length == 1) {	if (!allowed.containsKey(hostAndPort[0])) {	allowed.put(hostAndPort[0], new HashSet<>());	}	allowed.get(hostAndPort[0]).add("*");	} else {	if (!allowed.containsKey(hostAndPort[0])) {	allowed.put(hostAndPort[0], new HashSet<>());	}	allowed.get(hostAndPort[0]).add(hostAndPort[1]);	
allow proxy to host and port 

========================= ambari sample_3927 =========================

batchRequests.add(batchRequest);	}	}	}	}	}	}	batch.getBatchRequests().addAll(batchRequests);	batch.setBatchSettings(batchSettings);	} catch (Exception e) {	
request schedule batch json is unparseable 

========================= ambari sample_3530 =========================

public FileBasedCredentialStore(File keyStoreLocation) {	if (keyStoreLocation == null) {	
writing key store to the current working directory of the running process 

public FileBasedCredentialStore(File keyStoreLocation) {	if (keyStoreLocation == null) {	keyStoreLocation = new File(Configuration.MASTER_KEYSTORE_FILENAME_DEFAULT);	} else if (keyStoreLocation.isDirectory()) {	keyStoreLocation = new File(keyStoreLocation, Configuration.MASTER_KEYSTORE_FILENAME_DEFAULT);	}	if (keyStoreLocation.exists()) {	if (!keyStoreLocation.canWrite()) {	
the destination file is not writable failures may occur when writing the key store to disk 

keyStoreLocation = new File(Configuration.MASTER_KEYSTORE_FILENAME_DEFAULT);	} else if (keyStoreLocation.isDirectory()) {	keyStoreLocation = new File(keyStoreLocation, Configuration.MASTER_KEYSTORE_FILENAME_DEFAULT);	}	if (keyStoreLocation.exists()) {	if (!keyStoreLocation.canWrite()) {	}	} else {	File directory = keyStoreLocation.getParentFile();	if ((directory != null) && !directory.canWrite()) {	
the destination directory is not writable failures may occur when writing the key store to disk 

private KeyStore getKeyStore(final File keyStoreFile, String keyStoreType) throws AmbariException {	KeyStore keyStore;	FileInputStream inputStream;	if (keyStoreFile.exists()) {	if (keyStoreFile.length() > 0) {	
reading key store from 

KeyStore keyStore;	FileInputStream inputStream;	if (keyStoreFile.exists()) {	if (keyStoreFile.length() > 0) {	try {	inputStream = new FileInputStream(keyStoreFile);	} catch (FileNotFoundException e) {	throw new AmbariException(String.format("Failed to open the key store file: %s", e.getLocalizedMessage()), e);	}	} else {	
the key store file found in is empty returning new non persisted keystore 

if (keyStoreFile.length() > 0) {	try {	inputStream = new FileInputStream(keyStoreFile);	} catch (FileNotFoundException e) {	throw new AmbariException(String.format("Failed to open the key store file: %s", e.getLocalizedMessage()), e);	}	} else {	inputStream = null;	}	} else {	
key store file not found in returning new non persisted keystore 

private void putKeyStore(KeyStore keyStore, File keyStoreFile) throws AmbariException {	
writing key store to 

========================= ambari sample_2771 =========================

private InputStream getInputStream(HttpURLConnection connection) throws IOException, AmbariHttpException {	int responseCode = connection.getResponseCode();	if (responseCode >= ProxyService.HTTP_ERROR_RANGE_START) {	String message = connection.getResponseMessage();	if (connection.getErrorStream() != null) {	message = IOUtils.toString(connection.getErrorStream());	}	
got error response for url response code 

========================= ambari sample_3926 =========================

public String findCollectorShard(List<String> collectorHosts) {	long index = hostnameHash % collectorHosts.size();	index = index < 0 ? index + collectorHosts.size() : index;	String collectorHost = collectorHosts.get((int) index);	
calculated collector shard s based on hostname s 

========================= ambari sample_256 =========================

public void pigUdfMigration(String username, String instance, ViewContext view, MigrationResponse migrationresult, String jobid) throws IOException, ItemNotFound {	long startTime = System.currentTimeMillis();	final Logger logger = Logger.getLogger(PigUdfMigrationUtility.class);	Connection connectionHuedb = null;	Connection connectionAmbaridb = null;	logger.info("-------------------------------------");	
pig udf migration started 

public void pigUdfMigration(String username, String instance, ViewContext view, MigrationResponse migrationresult, String jobid) throws IOException, ItemNotFound {	long startTime = System.currentTimeMillis();	final Logger logger = Logger.getLogger(PigUdfMigrationUtility.class);	Connection connectionHuedb = null;	Connection connectionAmbaridb = null;	logger.info("-------------------------------------");	logger.info("-------------------------------------");	int i = 0;	
instance is 

public void pigUdfMigration(String username, String instance, ViewContext view, MigrationResponse migrationresult, String jobid) throws IOException, ItemNotFound {	long startTime = System.currentTimeMillis();	final Logger logger = Logger.getLogger(PigUdfMigrationUtility.class);	Connection connectionHuedb = null;	Connection connectionAmbaridb = null;	logger.info("-------------------------------------");	logger.info("-------------------------------------");	int i = 0;	
hue username is 

String dirNameForPigUdf = "";	int pigInstanceTableId, sequence;	ArrayList<PigModel> dbpojoPigUdf = new ArrayList<PigModel>();	try {	String[] usernames = username.split(",");	int totalQueries = 0;	for (int k = 0; k < usernames.length; k++) {	connectionHuedb = DataSourceHueDatabase.getInstance(view.getProperties().get("huedrivername"), view.getProperties().get("huejdbcurl"), view.getProperties().get("huedbusername"), view.getProperties().get("huedbpassword")).getConnection();	username = usernames[k];	migrationresult.setProgressPercentage(0);	
migration started for user 

for (int k = 0; k < usernames.length; k++) {	connectionHuedb = DataSourceHueDatabase.getInstance(view.getProperties().get("huedrivername"), view.getProperties().get("huejdbcurl"), view.getProperties().get("huedbusername"), view.getProperties().get("huedbpassword")).getConnection();	username = usernames[k];	migrationresult.setProgressPercentage(0);	dbpojoPigUdf = pigudfmigration.fetchFromHueDatabase(username, connectionHuedb, huedatabase);	totalQueries += dbpojoPigUdf.size();	for (int j = 0; j < dbpojoPigUdf.size(); j++) {	logger.info("jar fetched from hue=" + dbpojoPigUdf.get(j).getFileName());	}	if (dbpojoPigUdf.size() == 0) {	
no queries has been selected for the user 

migrationresult.setProgressPercentage(0);	dbpojoPigUdf = pigudfmigration.fetchFromHueDatabase(username, connectionHuedb, huedatabase);	totalQueries += dbpojoPigUdf.size();	for (int j = 0; j < dbpojoPigUdf.size(); j++) {	logger.info("jar fetched from hue=" + dbpojoPigUdf.get(j).getFileName());	}	if (dbpojoPigUdf.size() == 0) {	} else {	connectionAmbaridb = DataSourceAmbariDatabase.getInstance(view.getProperties().get("ambaridrivername"), view.getProperties().get("ambarijdbcurl"), view.getProperties().get("ambaridbusername"), view.getProperties().get("ambaridbpassword")).getConnection();	connectionAmbaridb.setAutoCommit(false);	
loop will continue for times 

connectionAmbaridb = DataSourceAmbariDatabase.getInstance(view.getProperties().get("ambaridrivername"), view.getProperties().get("ambarijdbcurl"), view.getProperties().get("ambaridbusername"), view.getProperties().get("ambaridbpassword")).getConnection();	connectionAmbaridb.setAutoCommit(false);	pigInstanceTableId = pigudfmigration.fetchInstanceTablenamePigUdf(connectionAmbaridb, instance, ambaridatabase);	sequence = pigudfmigration.fetchSequenceno(connectionAmbaridb, pigInstanceTableId, ambaridatabase);	for (i = 0; i < dbpojoPigUdf.size(); i++) {	float calc = ((float) (i + 1)) / dbpojoPigUdf.size() * 100;	int progressPercentage = Math.round(calc);	migrationresult.setProgressPercentage(progressPercentage);	migrationresult.setNumberOfQueryTransfered(i + 1);	getResourceManager(view).update(migrationresult, jobid);	
loop no 

connectionAmbaridb.setAutoCommit(false);	pigInstanceTableId = pigudfmigration.fetchInstanceTablenamePigUdf(connectionAmbaridb, instance, ambaridatabase);	sequence = pigudfmigration.fetchSequenceno(connectionAmbaridb, pigInstanceTableId, ambaridatabase);	for (i = 0; i < dbpojoPigUdf.size(); i++) {	float calc = ((float) (i + 1)) / dbpojoPigUdf.size() * 100;	int progressPercentage = Math.round(calc);	migrationresult.setProgressPercentage(progressPercentage);	migrationresult.setNumberOfQueryTransfered(i + 1);	getResourceManager(view).update(migrationresult, jobid);	logger.info("________________");	
jar name 

String ambariNameNodeUri = view.getProperties().get("namenode_URI_Ambari");	String dirAndFileName = ambariNameNodeUri + dirNameForPigUdf + fileName;	if (view.getProperties().get("KerberoseEnabled").equals("y")) {	pigudfmigration.createDirPigUdfSecured(dirNameForPigUdf, ambariNameNodeUri, ownerName, view.getProperties().get("PrincipalUserName"));	pigudfmigration.copyFileBetweenHdfsSecured(filePath, dirNameForPigUdf, ambariNameNodeUri, ownerName, view.getProperties().get("PrincipalUserName"));	} else {	pigudfmigration.createDirPigUdf(dirNameForPigUdf, ambariNameNodeUri, ownerName);	pigudfmigration.copyFileBetweenHdfs(filePath, dirNameForPigUdf, ambariNameNodeUri, ownerName);	}	pigudfmigration.insertRowForPigUdf(maxcountforpigudf, dirAndFileName, fileName, connectionAmbaridb, pigInstanceTableId, ambaridatabase, ownerName);	
migrated to ambari 

pigudfmigration.copyFileBetweenHdfsSecured(filePath, dirNameForPigUdf, ambariNameNodeUri, ownerName, view.getProperties().get("PrincipalUserName"));	} else {	pigudfmigration.createDirPigUdf(dirNameForPigUdf, ambariNameNodeUri, ownerName);	pigudfmigration.copyFileBetweenHdfs(filePath, dirNameForPigUdf, ambariNameNodeUri, ownerName);	}	pigudfmigration.insertRowForPigUdf(maxcountforpigudf, dirAndFileName, fileName, connectionAmbaridb, pigInstanceTableId, ambaridatabase, ownerName);	}	pigudfmigration.updateSequenceno(connectionAmbaridb, maxcountforpigudf, pigInstanceTableId, ambaridatabase);	connectionAmbaridb.commit();	}	
migration completed for user 

if (totalQueries == 0) {	migrationresult.setNumberOfQueryTransfered(0);	migrationresult.setTotalNoQuery(0);	} else {	migrationresult.setNumberOfQueryTransfered(totalQueries);	migrationresult.setTotalNoQuery(totalQueries);	migrationresult.setProgressPercentage(100);	}	getResourceManager(view).update(migrationresult, jobid);	} catch (SQLException e) {	
sql exception in ambari database 

} else {	migrationresult.setNumberOfQueryTransfered(totalQueries);	migrationresult.setTotalNoQuery(totalQueries);	migrationresult.setProgressPercentage(100);	}	getResourceManager(view).update(migrationresult, jobid);	} catch (SQLException e) {	migrationresult.setError("SQL Exception: " + e.getMessage());	try {	connectionAmbaridb.rollback();	
rollback done 

migrationresult.setNumberOfQueryTransfered(totalQueries);	migrationresult.setTotalNoQuery(totalQueries);	migrationresult.setProgressPercentage(100);	}	getResourceManager(view).update(migrationresult, jobid);	} catch (SQLException e) {	migrationresult.setError("SQL Exception: " + e.getMessage());	try {	connectionAmbaridb.rollback();	} catch (SQLException e1) {	
sql exception while doing roll back 

migrationresult.setProgressPercentage(100);	}	getResourceManager(view).update(migrationresult, jobid);	} catch (SQLException e) {	migrationresult.setError("SQL Exception: " + e.getMessage());	try {	connectionAmbaridb.rollback();	} catch (SQLException e1) {	}	} catch (ClassNotFoundException e2) {	
class not found exception 

getResourceManager(view).update(migrationresult, jobid);	} catch (SQLException e) {	migrationresult.setError("SQL Exception: " + e.getMessage());	try {	connectionAmbaridb.rollback();	} catch (SQLException e1) {	}	} catch (ClassNotFoundException e2) {	migrationresult.setError("Class Not Found Exception: " + e2.getMessage());	} catch (PropertyVetoException e) {	
propertyvetoexception 

} catch (SQLException e1) {	}	} catch (ClassNotFoundException e2) {	migrationresult.setError("Class Not Found Exception: " + e2.getMessage());	} catch (PropertyVetoException e) {	migrationresult.setError("Property Veto Exception: " + e.getMessage());	} catch (URISyntaxException e) {	e.printStackTrace();	migrationresult.setError("URI Syntax Exception: " + e.getMessage());	} catch (Exception e) {	
generic exception 

migrationresult.setError("Property Veto Exception: " + e.getMessage());	} catch (URISyntaxException e) {	e.printStackTrace();	migrationresult.setError("URI Syntax Exception: " + e.getMessage());	} catch (Exception e) {	migrationresult.setError("Exception: " + e.getMessage());	} finally {	if (null != connectionAmbaridb) try {	connectionAmbaridb.close();	} catch (SQLException e) {	
connection close exception 

migrationresult.setError("Error in closing connection: " + e.getMessage());	}	getResourceManager(view).update(migrationresult, jobid);	}	long stopTime = System.currentTimeMillis();	long elapsedTime = stopTime - startTime;	migrationresult.setJobtype("pigudfmigration");	migrationresult.setTotalTimeTaken(String.valueOf(elapsedTime));	getResourceManager(view).update(migrationresult, jobid);	logger.info("----------------------------------");	
pig udf migration ends 

========================= ambari sample_1274 =========================

private List<Job> readJobsFromDbByJobId(List<String> jobsIds) {	List<Job> jobs = new LinkedList<>();	for (final String jid : jobsIds) {	try {	Job job = getJobFromDbByJobId(jid);	jobs.add(job);	} catch (ItemNotFound itemNotFound) {	
error while finding job with id 

public List<Job> readAll(String username) {	List<HiveQueryId> queries = ats.getHiveQueryIdsForUser(username);	
hivequeryids fetched 

public List<Job> readAll(String username) {	List<HiveQueryId> queries = ats.getHiveQueryIdsForUser(username);	List<Job> allJobs = fetchDagsAndMergeJobs(queries);	List<Job> dbOnlyJobs = readDBOnlyJobs(username, queries, null, null);	
jobs only present in db 

private List<Job> readDBOnlyJobs(String username, List<HiveQueryId> queries, Long startTime, Long endTime) {	List<Job> dbOnlyJobs = new LinkedList<>();	HashMap<String, String> operationIdVsHiveId = new HashMap<>();	for (HiveQueryId hqid : queries) {	operationIdVsHiveId.put(hqid.operationId, hqid.entity);	}	
operationidvshiveid 

private List<Job> fetchDagsAndMergeJobs(List<HiveQueryId> queries) {	List<Job> allJobs = new LinkedList<Job>();	for (HiveQueryId atsHiveQuery : queries) {	JobImpl atsJob = null;	if (hasOperationId(atsHiveQuery)) {	try {	Job viewJob = getJobByOperationId(atsHiveQuery.operationId);	TezDagId atsTezDag = getTezDagFromHiveQueryId(atsHiveQuery);	atsJob = mergeHiveAtsTez(atsHiveQuery, atsTezDag, viewJob);	} catch (ItemNotFound itemNotFound) {	
ignore 

protected JobImpl mergeAtsJobWithViewJob(HiveQueryId atsHiveQuery, TezDagId atsTezDag, Job viewJob) {	JobImpl atsJob;	try {	atsJob = new JobImpl(PropertyUtils.describe(viewJob));	} catch (IllegalAccessException e) {	
can t instantiate jobimpl 

protected JobImpl mergeAtsJobWithViewJob(HiveQueryId atsHiveQuery, TezDagId atsTezDag, Job viewJob) {	JobImpl atsJob;	try {	atsJob = new JobImpl(PropertyUtils.describe(viewJob));	} catch (IllegalAccessException e) {	return null;	} catch (InvocationTargetException e) {	
can t instantiate jobimpl 

protected JobImpl mergeAtsJobWithViewJob(HiveQueryId atsHiveQuery, TezDagId atsTezDag, Job viewJob) {	JobImpl atsJob;	try {	atsJob = new JobImpl(PropertyUtils.describe(viewJob));	} catch (IllegalAccessException e) {	return null;	} catch (InvocationTargetException e) {	return null;	} catch (NoSuchMethodException e) {	
can t instantiate jobimpl 

updateDb = true;	}	}	if (tezDagId.status != null && (tezDagId.status.compareToIgnoreCase(Job.JOB_STATE_UNKNOWN) != 0) && !viewJob.getStatus().equalsIgnoreCase(tezDagId.status)) {	dagId = tezDagId.entity;	applicationId = tezDagId.applicationId;	updateDb = true;	}	if(updateDb) {	if (useActorSystem) {	
saving dag information via actor system for job id 

========================= ambari sample_597 =========================

public Response getFilePage(@PathParam("filePath") String filePath, @QueryParam("page") Long page) throws IOException, InterruptedException {	
reading file 

public Response deleteFile(@PathParam("filePath") String filePath) throws IOException, InterruptedException {	try {	filePath = sanitizeFilePath(filePath);	
deleting file 

public Response updateFile(FileResourceRequest request, try {	filePath = sanitizeFilePath(filePath);	
rewriting file 

public Response createFile(FileResourceRequest request, throws IOException, InterruptedException {	try {	
creating file 

========================= ambari sample_578 =========================

public void runTask() {	
hostrequest executing install task for host 

public void runTask() {	boolean skipInstallTaskCreate = clusterTopology.getProvisionAction().equals(ProvisionAction.START_ONLY);	RequestStatusResponse response = clusterTopology.installHost(hostRequest.getHostName(), skipInstallTaskCreate, skipFailure);	if(response != null) {	List<ShortTaskStatus> underlyingTasks = response.getTasks();	for (ShortTaskStatus task : underlyingTasks) {	String component = task.getRole();	Long logicalInstallTaskId = hostRequest.getLogicalTasksForTopologyTask(this).get(component);	if (logicalInstallTaskId == null) {	
skipping physical install task registering because component cannot be found 

for (ShortTaskStatus task : underlyingTasks) {	String component = task.getRole();	Long logicalInstallTaskId = hostRequest.getLogicalTasksForTopologyTask(this).get(component);	if (logicalInstallTaskId == null) {	continue;	}	long taskId = task.getTaskId();	hostRequest.registerPhysicalTaskId(logicalInstallTaskId, taskId);	}	}	
hostrequest exiting install task for host 

========================= ambari sample_2663 =========================

public void setInputConfig(String clusterName, String serviceName, String inputConfig) throws Exception {	String nodePath = String.format("/%s/input/%s", clusterName, serviceName);	client.setData().forPath(nodePath, inputConfig.getBytes());	
set input config for the service for cluster 

========================= ambari sample_1673 =========================

private void resetResultSet() {	try {	resultSet.beforeFirst();	sender().tell(new CursorReset(), self());	} catch (SQLException e) {	
failed to reset the cursor 

private void sendKeepAlive() {	
sending a keep alive to 

private void getNext() {	List<Row> rows = Lists.newArrayList();	if (!metaDataFetched) {	try {	initialize();	} catch (SQLException ex) {	
failed to fetch metadata for the resultset 

if (index == 0) {	sender().tell(new NoMoreItems(columnDescriptions), self());	if(!async) {	cleanUpResources();	}	} else {	Result result = new Result(rows, columnDescriptions);	sender().tell(result, self());	}	} catch (SQLException ex) {	
failed to fetch next batch for the resultset 

========================= ambari sample_978 =========================

public void saveWorkflow(String projectId, String path, JobType jobType, String descripton, String userName, String name) {	
save workflow called 

========================= ambari sample_1059 =========================

public UpdateResponse addDocs(SolrInputDocument doc) throws SolrServerException, IOException, SolrException {	UpdateResponse updateResoponse = getSolrClient().add(doc);	
username update time execution total time elapsed is 

public UpdateResponse removeDoc(String query) throws SolrServerException, IOException, SolrException {	UpdateResponse updateResoponse = getSolrClient().deleteByQuery(query);	getSolrClient().commit();	
username remove time execution total time elapsed is 

========================= ambari sample_1409 =========================

public void startJob(String jobName, String parameters) throws Exception {	try (InfraClient httpClient = getInfraClient()) {	String jobId = httpClient.startJob(jobName, parameters);	
job started jobid 

public void checkNumberOfFilesOnLocalFilesystem(long count, String text, String path) {	File destinationDirectory = new File(getLocalDataFolder(), path);	
destination directory path 

========================= ambari sample_217 =========================

public void onAmbariEvent(StackUpgradeFinishEvent event) {	
received event 

public void onAmbariEvent(StackUpgradeFinishEvent event) {	Cluster cluster = event.getCluster();	for (Service service : cluster.getServices().values()) {	try {	service.updateServiceInfo();	for (ServiceComponent sc : service.getServiceComponents().values()) {	sc.updateComponentInfo();	}	} catch (AmbariException e) {	if (LOG.isErrorEnabled()) {	
caught ambariexception when update component info 

service.updateServiceInfo();	for (ServiceComponent sc : service.getServiceComponents().values()) {	sc.updateComponentInfo();	}	} catch (AmbariException e) {	if (LOG.isErrorEnabled()) {	}	}	}	if (roleCommandOrderProvider.get() instanceof CachedRoleCommandOrderProvider) {	
clearing rco cache 

========================= ambari sample_4522 =========================

public static HashMap<String, Object> jsonToMapObject(String jsonStr) {	if (StringUtils.isBlank(jsonStr)) {	
jsonstring is empty cannot conver to map 

public static synchronized void writeJSONInFile(String jsonStr, File outputFile, boolean beautify) {	FileWriter fileWriter = null;	if (outputFile == null) {	
user pass json file can t be null 

if (beautify) {	ObjectMapper mapper = new ObjectMapper();	Object json = mapper.readValue(jsonStr, Object.class);	jsonStr = mapper.writerWithDefaultPrettyPrinter().writeValueAsString(json);	}	fileWriter.write(jsonStr);	} else {	logger.error("Applcation does not have permission to update file to write enc_password. file="+ outputFile.getAbsolutePath());	}	} catch (IOException e) {	
error writing to password file 

========================= ambari sample_1370 =========================

return filterHosts(hostsType, serviceName, componentName);	}	Map<Status, String> pair = getNameNodePair(componentHosts);	if (pair != null) {	hostsType.master = pair.containsKey(Status.ACTIVE) ? pair.get(Status.ACTIVE) :  null;	hostsType.secondary = pair.containsKey(Status.STANDBY) ? pair.get(Status.STANDBY) :  null;	} else {	Iterator<String> iterator = componentHosts.iterator();	hostsType.master = iterator.next();	hostsType.secondary = iterator.next();	
could not determine the active standby states from namenodes using as active and as standby 

resolveResourceManagers(getCluster(), hostsType);	}	break;	case HBASE: if (componentName.equalsIgnoreCase("HBASE_MASTER")) {	resolveHBaseMasters(getCluster(), hostsType);	}	break;	default: break;	}	} catch (Exception err) {	
unable to get master and hosts for component error 

String downgradeToVersion = downgradeToRepositoryVersion.getVersion();	if (!StringUtils.equals(downgradeToVersion, sch.getVersion())) {	upgradeHosts.add(hostName);	continue;	}	}	hostsType.unhealthy = unhealthyHosts;	hostsType.hosts = upgradeHosts;	return hostsType;	} catch (AmbariException e) {	
could not determine host components to upgrade defaulting to saved hosts 

String namenodeFragment = "dfs.namenode." + (encrypted ? "https-address" : "http-address") + ".{0}.{1}";	for (String nnUniqueID : nnUniqueIDs) {	String key = MessageFormat.format(namenodeFragment, nameService, nnUniqueID);	String value = m_configHelper.getValueFromDesiredConfigurations(cluster, ConfigHelper.HDFS_SITE, key);	try {	HostAndPort hp = HTTPUtils.getHostAndPortFromProperty(value);	if (hp == null) {	throw new MalformedURLException("Could not parse host and port from " + value);	}	if (!componentHosts.contains(hp.host)){	
hadoop namenode ha configuration contains host that does not exist in the namenode hosts list 

if (hp == null) {	throw new MalformedURLException("Could not parse host and port from " + value);	}	if (!componentHosts.contains(hp.host)){	}	String state = queryJmxBeanValue(hp.host, hp.port, "Hadoop:service=NameNode,name=NameNodeStatus", "State", true, encrypted);	if (null != state && (state.equalsIgnoreCase(Status.ACTIVE.toString()) || state.equalsIgnoreCase(Status.STANDBY.toString()))) {	Status status = Status.valueOf(state.toUpperCase());	stateToHost.put(status, hp.host.toLowerCase());	} else {	
could not retrieve state for namenode s from property s by querying jmx 

String response = HTTPUtils.requestURL(endPoint);	if (null == response || response.isEmpty()) {	return null;	}	Type type = new TypeToken<Map<String, ArrayList<HashMap<String, String>>>>() {}.getType();	try {	Map<String, ArrayList<HashMap<String, String>>> jmxBeans = StageUtils.getGson().fromJson(response, type);	return jmxBeans.get("beans").get(0).get(attributeName);	} catch (Exception e) {	if (LOG.isDebugEnabled()) {	
could not load jmx from from 

if (null == response || response.isEmpty()) {	return null;	}	Type type = new TypeToken<Map<String, ArrayList<HashMap<String, String>>>>() {}.getType();	try {	Map<String, ArrayList<HashMap<String, String>>> jmxBeans = StageUtils.getGson().fromJson(response, type);	return jmxBeans.get("beans").get(0).get(attributeName);	} catch (Exception e) {	if (LOG.isDebugEnabled()) {	} else {	
could not load jmx from from 

========================= ambari sample_4467 =========================

protected Condition prepareMetricQueryCondition(long startTime, long endTime) {	EmptyCondition condition = new EmptyCondition();	condition.setDoUpdate(true);	UPSERT INTO METRIC_AGGREGATE_HOURLY (METRIC_NAME, APP_ID, INSTANCE_ID, SERVER_TIME, UNITS, METRIC_SUM, METRIC_COUNT, METRIC_MAX, METRIC_MIN) SELECT METRIC_NAME, APP_ID, INSTANCE_ID, MAX(SERVER_TIME), UNITS, SUM(METRIC_SUM), SUM(HOSTS_COUNT), MAX(METRIC_MAX), MIN(METRIC_MIN) FROM METRIC_AGGREGATE WHERE SERVER_TIME >= 1441155600000 AND SERVER_TIME < 1441159200000 GROUP BY METRIC_NAME, APP_ID, INSTANCE_ID, UNITS;	condition.setStatement(String.format(GET_AGGREGATED_APP_METRIC_GROUPBY_SQL, getQueryHint(startTime), outputTableName, endTime, aggregateColumnName, tableName, getDownsampledMetricSkipClause(), startTime, endTime));	if (LOG.isDebugEnabled()) {	
condition 

========================= ambari sample_352 =========================

public Storage get(ViewContext context) {	
creating storage instance for viewname instance name 

========================= ambari sample_885 =========================

public List<DescriptorPreCheck> getApplicablePrerequisiteChecks(PrereqCheckRequest request, List<AbstractCheckDescriptor> checksRegistry) {	List<DescriptorPreCheck> applicablePreChecks = new LinkedList<>();	final String clusterName = request.getClusterName();	for (AbstractCheckDescriptor checkDescriptor : checksRegistry) {	final PrerequisiteCheck prerequisiteCheck = new PrerequisiteCheck(checkDescriptor.getDescription(), clusterName);	try {	if (checkDescriptor.isApplicable(request)) {	applicablePreChecks.add(new DescriptorPreCheck(checkDescriptor, prerequisiteCheck));	}	} catch (Exception ex) {	
unable to determine whether the pre upgrade check is applicable to this upgrade 

List<DescriptorPreCheck> applicablePreChecks = getApplicablePrerequisiteChecks(request, checksRegistry);	for (DescriptorPreCheck descriptorPreCheck : applicablePreChecks) {	AbstractCheckDescriptor checkDescriptor = descriptorPreCheck.descriptor;	PrerequisiteCheck prerequisiteCheck = descriptorPreCheck.check;	try {	checkDescriptor.perform(prerequisiteCheck, request);	} catch (ClusterNotFoundException ex) {	prerequisiteCheck.setFailReason("Cluster with name " + clusterName + " doesn't exists");	prerequisiteCheck.setStatus(PrereqCheckStatus.FAIL);	} catch (Exception ex) {	
check failed 

try {	checkDescriptor.perform(prerequisiteCheck, request);	} catch (ClusterNotFoundException ex) {	prerequisiteCheck.setFailReason("Cluster with name " + clusterName + " doesn't exists");	prerequisiteCheck.setStatus(PrereqCheckStatus.FAIL);	} catch (Exception ex) {	prerequisiteCheck.setFailReason("Unexpected server error happened");	prerequisiteCheck.setStatus(PrereqCheckStatus.FAIL);	}	if (prerequisiteCheck.getStatus() == PrereqCheckStatus.FAIL && canBypassPreChecks) {	
check failed but stack upgrade is allowed to bypass failures error to bypass failed on 

========================= ambari sample_3176 =========================

LOG.error(getErrorMessage(e), e);	throw e;	} catch (Exception e) {	LOG.error(e.getMessage(), e);	throw new ServiceFormattedException(e);	} finally {	if (null != uploadedInputStream) {	try {	uploadedInputStream.close();	} catch (IOException e) {	
exception occured while closing the hdfs file stream for path 

} catch (WebApplicationException e) {	LOG.error(getErrorMessage(e), e);	throw e;	} catch (Exception e) {	LOG.error(e.getMessage(), e);	throw new ServiceFormattedException(e);	} finally {	if (null != hdfsStream) try {	hdfsStream.close();	} catch (IOException e) {	
exception occured while closing the hdfs stream for path 

public Response insertFromTempTable(InsertFromQueryInput input) {	try {	String insertQuery = generateInsertFromQuery(input);	
insertquery 

String insertQuery = generateInsertFromQuery(input);	if( null != input.getGlobalSettings() && !Strings.isNullOrEmpty(input.getGlobalSettings().trim())){	String globalSettings = input.getGlobalSettings().trim();	if(!globalSettings.endsWith(";")){	globalSettings += ";\n";	}else{	globalSettings += "\n";	}	insertQuery = globalSettings + insertQuery;	}	
creating job for query 

if( null != input.getGlobalSettings() && !Strings.isNullOrEmpty(input.getGlobalSettings().trim())){	String globalSettings = input.getGlobalSettings().trim();	if(!globalSettings.endsWith(";")){	globalSettings += ";\n";	}else{	globalSettings += "\n";	}	insertQuery = globalSettings + insertQuery;	}	Job job = createJob(insertQuery, input.getFromDatabase(), "Insert from " + input.getFromDatabase() + "." + input.getFromTable() + " to " + input.getToDatabase() + "." + input.getToTable());	
job created for insert from temp table 

private String uploadIntoTable(Reader reader, String databaseName, String tempTableName) {	try {	String fullPath = getHiveMetaStoreLocation(databaseName, tempTableName);	
uploading file into 

private Job createJob(String query, String databaseName, String jobTitle) throws Throwable{	Map jobInfo = new HashMap<>();	jobInfo.put("title", jobTitle);	jobInfo.put("forcedContent", query);	jobInfo.put("dataBase", databaseName);	jobInfo.put("referrer", JobImpl.REFERRER.INTERNAL.name());	Job job = new JobImpl(jobInfo);	
creating job 

urlString = row.getRow()[1] == null ? null : row.getRow()[1].toString();	break;	}	}	String tablePath = null;	if (null != urlString) {	try {	URI uri = new URI(urlString);	tablePath = uri.getPath();	} catch (URISyntaxException e) {	
error occurred while parsing as url 

private String getHiveMetaStoreLocation() {	String dir = context.getProperties().get(HIVE_METASTORE_LOCATION_KEY_VIEW_PROPERTY);	if (dir != null && !dir.trim().isEmpty()) {	return dir;	} else {	
neither found associated cluster nor found the view property returning default location 

ParseOptions parseOptions = new ParseOptions();	parseOptions.setOption(ParseOptions.OPTIONS_FILE_TYPE, inputFileType);	if (inputFileType.equals(ParseOptions.InputFileType.CSV.toString())){	if(isFirstRowHeader) parseOptions.setOption(ParseOptions.OPTIONS_HEADER, ParseOptions.HEADER.FIRST_RECORD.toString());	else parseOptions.setOption(ParseOptions.OPTIONS_HEADER, ParseOptions.HEADER.NONE.toString());	parseOptions.setOption(ParseOptions.OPTIONS_CSV_DELIMITER, csvParams.getCsvDelimiter());	parseOptions.setOption(ParseOptions.OPTIONS_CSV_ESCAPE_CHAR, csvParams.getCsvEscape());	parseOptions.setOption(ParseOptions.OPTIONS_CSV_QUOTE, csvParams.getCsvQuote());	}	else parseOptions.setOption(ParseOptions.OPTIONS_HEADER, ParseOptions.HEADER.EMBEDDED.toString());	
isfirstrowheader inputfiletype 

private String uploadFileFromStream( InputStream uploadedInputStream, Boolean isFirstRowHeader, String inputFileType, String tableName, String databaseName, List<ColumnInfo> header, boolean containsEndlines, CSVParams csvParams ) throws Exception {	
uploading file into databasename tablename 

========================= ambari sample_539 =========================

String instanceName = (String) propertyMap.get(INSTANCE_NAME_PROPERTY_ID);	try {	getResourceProvider(instanceName).createResource(resourceId, propertyMap);	} catch (org.apache.ambari.view.NoSuchResourceException e) {	throw new NoSuchParentResourceException(e.getMessage(), e);	} catch (org.apache.ambari.view.UnsupportedPropertyException e) {	throw new UnsupportedPropertyException(getResourceType(e), e.getPropertyIds());	} catch (org.apache.ambari.view.ResourceAlreadyExistsException e) {	throw new ResourceAlreadyExistsException(e.getMessage());	} catch (Exception e) {	
caught exception creating view sub resources 

results.add(resource);	}	}	}	return results;	} catch (org.apache.ambari.view.NoSuchResourceException e) {	throw new NoSuchParentResourceException(e.getMessage(), e);	} catch (org.apache.ambari.view.UnsupportedPropertyException e) {	throw new UnsupportedPropertyException(getResourceType(e), e.getPropertyIds());	} catch (Exception e) {	
caught exception getting view sub resources 

for (Resource resource : resources) {	String resourceId   = (String) resource.getPropertyValue(pkField);	String instanceName = (String) resource.getPropertyValue(INSTANCE_NAME_PROPERTY_ID);	try {	getResourceProvider(instanceName).updateResource(resourceId, propertyMap);	} catch (org.apache.ambari.view.NoSuchResourceException e) {	throw new NoSuchParentResourceException(e.getMessage(), e);	} catch (org.apache.ambari.view.UnsupportedPropertyException e) {	throw new UnsupportedPropertyException(getResourceType(e), e.getPropertyIds());	} catch (Exception e) {	
caught exception updating view sub resources 

for (Resource resource : resources) {	String resourceId   = (String) resource.getPropertyValue(pkField);	String instanceName = (String) resource.getPropertyValue(INSTANCE_NAME_PROPERTY_ID);	try {	getResourceProvider(instanceName).deleteResource(resourceId);	} catch (org.apache.ambari.view.NoSuchResourceException e) {	throw new NoSuchParentResourceException(e.getMessage(), e);	} catch (org.apache.ambari.view.UnsupportedPropertyException e) {	throw new UnsupportedPropertyException(getResourceType(e), e.getPropertyIds());	} catch (Exception e) {	
caught exception deleting view sub resources 

========================= ambari sample_3930 =========================

private void resetResultSet() {	try {	resultSet.beforeFirst();	sender().tell(new CursorReset(), self());	} catch (SQLException e) {	
failed to reset the cursor 

private void sendKeepAlive() {	
sending a keep alive to 

private void getNext() {	List<Row> rows = Lists.newArrayList();	if (!metaDataFetched) {	try {	initialize();	} catch (SQLException ex) {	
failed to fetch metadata for the resultset 

if (index == 0) {	sender().tell(new NoMoreItems(columnDescriptions), self());	if(!async) {	cleanUpResources();	}	} else {	Result result = new Result(rows, columnDescriptions);	sender().tell(result, self());	}	} catch (SQLException ex) {	
failed to fetch next batch for the resultset 

========================= ambari sample_755 =========================

try {	Optional<Map<String, String>> props = ViewPropertyHelper.getViewConfigs(viewContext, VIEW_CONF_KEYVALUES);	HdfsApi api;	if(props.isPresent()){	api = HdfsUtil.connectToHDFSApi(viewContext, props.get());	}else{	api = HdfsUtil.connectToHDFSApi(viewContext);	}	return api;	} catch (Exception ex) {	
error in getting hdfs api 

String defaultFS = configurations.get(DEFAULT_FS);	try {	URI fsUri = new URI(defaultFS);	String protocol = fsUri.getScheme();	String ambariSkipCheckValues = viewContext.getAmbariProperty(AMBARI_SKIP_HOME_DIRECTORY_CHECK_PROTOCOL_LIST);	List<String> protocolSkipList = (ambariSkipCheckValues == null? new LinkedList<String>() : Arrays.asList(ambariSkipCheckValues.split(",")));	if(null != protocol && protocolSkipList.contains(protocol)){	return Boolean.FALSE;	}	} catch (URISyntaxException e) {	
error occurred while parsing the defaultfs uri 

========================= ambari sample_1075 =========================

public Response getData(@Context UriInfo uriInfo, @PathParam("endpoint") String endpoint) {	String url = getProxyUrl(endpoint, uriInfo.getQueryParameters());	
proxying to url 

public Response getData(@Context UriInfo uriInfo, @PathParam("endpoint") String endpoint) {	String url = getProxyUrl(endpoint, uriInfo.getQueryParameters());	String response = proxyHelper.getResponse(url, new HashMap<String, String>(), getAuthType());	JSONObject jsonObject = (JSONObject) JSONValue.parse(response);	if (jsonObject == null) {	
response received from url 

public Response getData(@Context UriInfo uriInfo, @PathParam("endpoint") String endpoint) {	String url = getProxyUrl(endpoint, uriInfo.getQueryParameters());	String response = proxyHelper.getResponse(url, new HashMap<String, String>(), getAuthType());	JSONObject jsonObject = (JSONObject) JSONValue.parse(response);	if (jsonObject == null) {	
failed to parse json from url 

public Response getData(@Context UriInfo uriInfo, @PathParam("endpoint") String endpoint) {	String url = getProxyUrl(endpoint, uriInfo.getQueryParameters());	String response = proxyHelper.getResponse(url, new HashMap<String, String>(), getAuthType());	JSONObject jsonObject = (JSONObject) JSONValue.parse(response);	if (jsonObject == null) {	throw new TezWebAppException("Failed to parse JSON from URL : " + url + ". Internal Error.", Response.Status.INTERNAL_SERVER_ERROR.getStatusCode(), response);	}	try {	
response received from url 

String url = getProxyUrl(endpoint, uriInfo.getQueryParameters());	String response = proxyHelper.getResponse(url, new HashMap<String, String>(), getAuthType());	JSONObject jsonObject = (JSONObject) JSONValue.parse(response);	if (jsonObject == null) {	throw new TezWebAppException("Failed to parse JSON from URL : " + url + ". Internal Error.", Response.Status.INTERNAL_SERVER_ERROR.getStatusCode(), response);	}	try {	return Response.ok(jsonObject).type(MediaType.APPLICATION_JSON).build();	}	catch(WebApplicationException e) {	
response received from url 

String url = getProxyUrl(endpoint, uriInfo.getQueryParameters());	String response = proxyHelper.getResponse(url, new HashMap<String, String>(), getAuthType());	JSONObject jsonObject = (JSONObject) JSONValue.parse(response);	if (jsonObject == null) {	throw new TezWebAppException("Failed to parse JSON from URL : " + url + ". Internal Error.", Response.Status.INTERNAL_SERVER_ERROR.getStatusCode(), response);	}	try {	return Response.ok(jsonObject).type(MediaType.APPLICATION_JSON).build();	}	catch(WebApplicationException e) {	
proxying to url failed 

========================= ambari sample_1130 =========================

public Authentication authenticate(Authentication authentication) throws AuthenticationException {	if (isLdapEnabled()) {	if (authentication.getName() == null) {	
authentication failed no username provided 

public Authentication authenticate(Authentication authentication) throws AuthenticationException {	if (isLdapEnabled()) {	if (authentication.getName() == null) {	throw new InvalidUsernamePasswordCombinationException("");	}	String username = authentication.getName().trim();	if (authentication.getCredentials() == null) {	
authentication failed no credentials provided 

throw new InvalidUsernamePasswordCombinationException("");	}	String username = authentication.getName().trim();	if (authentication.getCredentials() == null) {	throw new InvalidUsernamePasswordCombinationException(username);	}	try {	Authentication auth = loadLdapAuthenticationProvider(username).authenticate(authentication);	UserEntity userEntity = getUserEntity(auth);	if (userEntity == null) {	
user not found 

throw e;	} else {	throw new InvalidUsernamePasswordCombinationException(username, false, e);	}	}	Authentication authToken = new AmbariUserAuthentication(null, users.getUser(userEntity), users.getUserAuthorities(userEntity));	authToken.setAuthenticated(true);	return authToken;	}	} catch (AuthenticationException e) {	
got exception during ldap authentication attempt 

}	Authentication authToken = new AmbariUserAuthentication(null, users.getUser(userEntity), users.getUserAuthorities(userEntity));	authToken.setAuthenticated(true);	return authToken;	}	} catch (AuthenticationException e) {	Throwable cause = e.getCause();	if ((cause != null) && (cause != e)) {	if (cause instanceof org.springframework.ldap.CommunicationException) {	if (LOG.isDebugEnabled()) {	
failed to communicate with the ldap server 

Authentication authToken = new AmbariUserAuthentication(null, users.getUser(userEntity), users.getUserAuthorities(userEntity));	authToken.setAuthenticated(true);	return authToken;	}	} catch (AuthenticationException e) {	Throwable cause = e.getCause();	if ((cause != null) && (cause != e)) {	if (cause instanceof org.springframework.ldap.CommunicationException) {	if (LOG.isDebugEnabled()) {	} else {	
failed to communicate with the ldap server 

return authToken;	}	} catch (AuthenticationException e) {	Throwable cause = e.getCause();	if ((cause != null) && (cause != e)) {	if (cause instanceof org.springframework.ldap.CommunicationException) {	if (LOG.isDebugEnabled()) {	} else {	}	} else if (cause instanceof org.springframework.ldap.AuthenticationException) {	
looks like ldap manager credentials that are used for connecting to ldap server are invalid 

private boolean reloadLdapServerProperties() {	LdapServerProperties properties = ldapConfiguration.getLdapServerProperties();	if (!properties.equals(ldapServerProperties.get())) {	
reloading properties 

========================= ambari sample_2823 =========================

if (orderByClause != null) {	sb.append(orderByClause);	} else {	sb.append(" ORDER BY METRIC_NAME, SERVER_TIME ");	}	}	if (condition.getLimit() != null) {	sb.append(" LIMIT ").append(condition.getLimit());	}	if (LOG.isDebugEnabled()) {	
sql condition 

if (condition.getMetricNames() == null || condition.getMetricNames().isEmpty()) {	throw new IllegalArgumentException("Point in time query without " + "metric names not supported ");	}	String stmtStr;	if (condition.getStatement() != null) {	stmtStr = condition.getStatement();	} else {	stmtStr = String.format(GET_LATEST_METRIC_SQL, getLatestMetricsHints(), METRICS_RECORD_TABLE_NAME, METRICS_RECORD_TABLE_NAME, condition.getConditionClause());	}	if (LOG.isDebugEnabled()) {	
sql condition 

for (String metricName : condition.getMetricNames()) {	if (LOG.isDebugEnabled()) {	LOG.debug("Setting pos: " + pos + ", value = " + metricName);	}	stmt.setString(pos++, metricName);	}	}	if (condition.getHostnames() != null) {	for (String hostname : condition.getHostnames()) {	if (LOG.isDebugEnabled()) {	
setting pos value 

}	if (condition.getHostnames() != null) {	for (String hostname : condition.getHostnames()) {	if (LOG.isDebugEnabled()) {	}	stmt.setString(pos++, hostname);	}	}	if (condition.getAppId() != null) {	if (LOG.isDebugEnabled()) {	
setting pos value 

stmt.setString(pos++, hostname);	}	}	if (condition.getAppId() != null) {	if (LOG.isDebugEnabled()) {	}	stmt.setString(pos++, condition.getAppId());	}	if (condition.getInstanceId() != null) {	if (LOG.isDebugEnabled()) {	
setting pos value 

sb.append(condition.getConditionClause());	String orderByClause = condition.getOrderByClause(false);	if (orderByClause != null) {	sb.append(orderByClause);	} else {	sb.append(" ORDER BY METRIC_NAME DESC, SERVER_TIME DESC  ");	}	sb.append(" LIMIT ").append(condition.getMetricNames().size());	String query = sb.toString();	if (LOG.isDebugEnabled()) {	
sql condition 

try {	stmt = connection.prepareStatement(query);	int pos = 1;	if (condition.getMetricNames() != null) {	for (; pos <= condition.getMetricNames().size(); pos++) {	stmt.setString(pos, condition.getMetricNames().get(pos - 1));	}	}	if (condition.getAppId() != null) {	if (LOG.isDebugEnabled()) {	
setting pos value 

private static int addHostNames(Condition condition, int pos, PreparedStatement stmt) throws SQLException {	int i = pos;	if (condition.getHostnames() != null) {	for (String hostname : condition.getHostnames()) {	if (LOG.isDebugEnabled()) {	
setting pos value 

private static int addAppId(Condition condition, int pos, PreparedStatement stmt) throws SQLException {	if (condition.getAppId() != null) {	if (LOG.isDebugEnabled()) {	
setting pos value 

private static int addInstanceId(Condition condition, int pos, PreparedStatement stmt) throws SQLException {	if (condition.getInstanceId() != null) {	if (LOG.isDebugEnabled()) {	
setting pos value 

private static int addStartTime(Condition condition, int pos, PreparedStatement stmt) throws SQLException {	if (condition.getStartTime() != null) {	if (LOG.isDebugEnabled()) {	
setting pos value 

private static int addEndTime(Condition condition, int pos, PreparedStatement stmt) throws SQLException {	if (condition.getEndTime() != null) {	if (LOG.isDebugEnabled()) {	
setting pos value 

========================= ambari sample_391 =========================

public PigJob create(PigJob object) {	object.setStatus(PigJob.PIG_JOB_STATE_SUBMITTING);	PigJob job = super.create(object);	
submitting job 

public PigJob create(PigJob object) {	object.setStatus(PigJob.PIG_JOB_STATE_SUBMITTING);	PigJob job = super.create(object);	try {	submitJob(object);	} catch (RuntimeException e) {	object.setStatus(PigJob.PIG_JOB_STATE_SUBMIT_FAILED);	save(object);	
job submit failed 

public PigJob create(PigJob object) {	object.setStatus(PigJob.PIG_JOB_STATE_SUBMITTING);	PigJob job = super.create(object);	try {	submitJob(object);	} catch (RuntimeException e) {	object.setStatus(PigJob.PIG_JOB_STATE_SUBMIT_FAILED);	save(object);	throw e;	}	
job submit ok 

public void killJob(PigJob object) throws IOException {	
killing job 

public void killJob(PigJob object) throws IOException {	if (object.getJobId() != null) {	try {	UserLocalObjects.getTempletonApi(context).killJob(object.getJobId());	} catch (IOException e) {	
job kill failed 

public void killJob(PigJob object) throws IOException {	if (object.getJobId() != null) {	try {	UserLocalObjects.getTempletonApi(context).killJob(object.getJobId());	} catch (IOException e) {	throw e;	}	
job kill ok 

public void killJob(PigJob object) throws IOException {	if (object.getJobId() != null) {	try {	UserLocalObjects.getTempletonApi(context).killJob(object.getJobId());	} catch (IOException e) {	throw e;	}	} else {	
job was not submitted ignoring kill request 

public void retrieveJobStatus(PigJob job) {	TempletonApi.JobInfo info;	try {	info = UserLocalObjects.getTempletonApi(context).checkJob(job.getJobId());	} catch (IOException e) {	
io exception s 

info = UserLocalObjects.getTempletonApi(context).checkJob(job.getJobId());	} catch (IOException e) {	return;	}	if (info.status != null && (info.status.containsKey("runState"))) {	Long time = System.currentTimeMillis() / 1000L;	Long currentDuration = time - job.getDateStarted();	int runState = ((Double) info.status.get("runState")).intValue();	boolean isStatusChanged = false;	switch (runState) {	
job killed s 

}	if (info.status != null && (info.status.containsKey("runState"))) {	Long time = System.currentTimeMillis() / 1000L;	Long currentDuration = time - job.getDateStarted();	int runState = ((Double) info.status.get("runState")).intValue();	boolean isStatusChanged = false;	switch (runState) {	isStatusChanged = !job.getStatus().equals(PigJob.PIG_JOB_STATE_KILLED);	job.setStatus(PigJob.PIG_JOB_STATE_KILLED);	break;	
job failed s 

switch (runState) {	isStatusChanged = !job.getStatus().equals(PigJob.PIG_JOB_STATE_KILLED);	job.setStatus(PigJob.PIG_JOB_STATE_KILLED);	break;	isStatusChanged = !job.getStatus().equals(PigJob.PIG_JOB_STATE_FAILED);	job.setStatus(PigJob.PIG_JOB_STATE_FAILED);	break;	case RUN_STATE_PREP: case RUN_STATE_RUNNING: isStatusChanged = !job.getStatus().equals(PigJob.PIG_JOB_STATE_RUNNING);	job.setStatus(PigJob.PIG_JOB_STATE_RUNNING);	break;	
job completed s 

break;	isStatusChanged = !job.getStatus().equals(PigJob.PIG_JOB_STATE_FAILED);	job.setStatus(PigJob.PIG_JOB_STATE_FAILED);	break;	case RUN_STATE_PREP: case RUN_STATE_RUNNING: isStatusChanged = !job.getStatus().equals(PigJob.PIG_JOB_STATE_RUNNING);	job.setStatus(PigJob.PIG_JOB_STATE_RUNNING);	break;	isStatusChanged = !job.getStatus().equals(PigJob.PIG_JOB_STATE_COMPLETED);	job.setStatus(PigJob.PIG_JOB_STATE_COMPLETED);	break;	
job in unknown state s 

========================= ambari sample_1022 =========================

public void testPersist() throws UniformInterfaceException, JSONException, IOException {	ClientConfig clientConfig = new DefaultClientConfig();	clientConfig.getFeatures().put(JSONConfiguration.FEATURE_POJO_MAPPING, Boolean.TRUE);	client = Client.create(clientConfig);	WebResource webResource = client.resource(String.format("http: webResource.post("{\"xyx\" : \"t\"}");	
done posting to the server 

public void testPersist() throws UniformInterfaceException, JSONException, IOException {	ClientConfig clientConfig = new DefaultClientConfig();	clientConfig.getFeatures().put(JSONConfiguration.FEATURE_POJO_MAPPING, Boolean.TRUE);	client = Client.create(clientConfig);	WebResource webResource = client.resource(String.format("http: webResource.post("{\"xyx\" : \"t\"}");	String output = webResource.get(String.class);	
all key values 

ClientConfig clientConfig = new DefaultClientConfig();	clientConfig.getFeatures().put(JSONConfiguration.FEATURE_POJO_MAPPING, Boolean.TRUE);	client = Client.create(clientConfig);	WebResource webResource = client.resource(String.format("http: webResource.post("{\"xyx\" : \"t\"}");	String output = webResource.get(String.class);	Map<String, String> jsonOutput = StageUtils.fromJson(output, Map.class);	String value = jsonOutput.get("xyx");	Assert.assertEquals("t", value);	webResource = client.resource(String.format("http: output = webResource.get(String.class);	Assert.assertEquals("t", output);	
value for xyx 

========================= ambari sample_2462 =========================

protected void doStart() {	
initializing caches 

protected void doStart() {	final int maxTimeoutForCacheInHours = ambariServerConfiguration.getLogSearchMetadataCacheExpireTimeout();	
caches configured with a max expire timeout of hours 

protected void doStop() {	
invalidating logsearch caches 

public void run() {	
logsearchfilenamerequestrunnable starting 

public void run() {	try {	LoggingRequestHelper helper = loggingRequestHelperFactory.getHelper(controller, cluster);	if (helper != null) {	Set<String> logFileNamesResult = helper.sendGetLogFileNamesRequest(component, host);	if (CollectionUtils.isNotEmpty(logFileNamesResult)) {	
logsearchfilenamerequestrunnable request was successful updating cache 

final String key = generateKey(component, host);	logFileNameCache.put(key, logFileNamesResult);	} else {	LOG.debug("LogSearchFileNameRequestRunnable: remote request was not successful for component = {} on host ={}", component, host);	if (!componentRequestFailureCounts.containsKey(component)) {	componentRequestFailureCounts.put(component, new AtomicInteger());	}	componentRequestFailureCounts.get(component).incrementAndGet();	}	} else {	
logsearchfilenamerequestrunnable request helper was null this may mean that logsearch is not available or could be a potential connection problem 

========================= ambari sample_3766 =========================

public void write(List<? extends String> values) throws Exception {	
dummyitem writer called values wait seconds 

public void write(List<? extends String> values) throws Exception {	Thread.sleep(1000);	String outputDirectoryLocation = String.format("%s%s%s%s", infraManagerDataConfig.getDataFolder(), File.separator, "dummyOutput-", new Date().getTime());	Path pathToDirectory = Paths.get(outputDirectoryLocation);	Path pathToFile = Paths.get(String.format("%s%s%s", outputDirectoryLocation, File.separator, "dummyOutput.txt"));	Files.createDirectories(pathToDirectory);	
write location to step execution context 

public void write(List<? extends String> values) throws Exception {	Thread.sleep(1000);	String outputDirectoryLocation = String.format("%s%s%s%s", infraManagerDataConfig.getDataFolder(), File.separator, "dummyOutput-", new Date().getTime());	Path pathToDirectory = Paths.get(outputDirectoryLocation);	Path pathToFile = Paths.get(String.format("%s%s%s", outputDirectoryLocation, File.separator, "dummyOutput.txt"));	Files.createDirectories(pathToDirectory);	stepExecution.getExecutionContext().put("stepOutputLocation", pathToFile.toAbsolutePath().toString());	
write location to job execution context 

public void write(List<? extends String> values) throws Exception {	Thread.sleep(1000);	String outputDirectoryLocation = String.format("%s%s%s%s", infraManagerDataConfig.getDataFolder(), File.separator, "dummyOutput-", new Date().getTime());	Path pathToDirectory = Paths.get(outputDirectoryLocation);	Path pathToFile = Paths.get(String.format("%s%s%s", outputDirectoryLocation, File.separator, "dummyOutput.txt"));	Files.createDirectories(pathToDirectory);	stepExecution.getExecutionContext().put("stepOutputLocation", pathToFile.toAbsolutePath().toString());	stepExecution.getJobExecution().getExecutionContext().put("jobOutputLocation", pathToFile.toAbsolutePath().toString());	
write to file 

========================= ambari sample_153 =========================

public void setUp() throws Exception {	
org apache hadoop yarn server applicationhistoryservice metrics timeline 

private void deleteTableIgnoringExceptions(Statement stmt, String tableName) {	try {	stmt.execute("delete from " + tableName);	} catch (Exception e) {	
exception on delete table 

deleteTableIgnoringExceptions(stmt, "METRIC_AGGREGATE_HOURLY");	deleteTableIgnoringExceptions(stmt, "METRIC_AGGREGATE_DAILY");	deleteTableIgnoringExceptions(stmt, "METRIC_RECORD");	deleteTableIgnoringExceptions(stmt, "METRIC_RECORD_MINUTE");	deleteTableIgnoringExceptions(stmt, "METRIC_RECORD_HOURLY");	deleteTableIgnoringExceptions(stmt, "METRIC_RECORD_DAILY");	deleteTableIgnoringExceptions(stmt, "METRICS_METADATA");	deleteTableIgnoringExceptions(stmt, "HOSTED_APPS_METADATA");	conn.commit();	} catch (Exception e) {	
error on deleting hbase schema 

} catch (SQLException e) {	LOG.error(e);	}	return null;	}	public Connection getConnection() {	Connection connection = null;	try {	connection = DriverManager.getConnection(getUrl());	} catch (SQLException e) {	
unable to connect to hbase store using phoenix 

protected void insertMetricRecords(Connection conn, TimelineMetrics metrics, long currentTime) throws SQLException, IOException {	List<TimelineMetric> timelineMetrics = metrics.getMetrics();	if (timelineMetrics == null || timelineMetrics.isEmpty()) {	
empty metrics insert request 

========================= ambari sample_295 =========================

public void testFilterGrok_parseMessage() throws Exception {	
testfiltergrok parsemessage 

public void testFilterGrok_parseMultiLineMessage() throws Exception {	
testfiltergrok parsemultilinemessage 

public void testFilterGrok_notMatchingMesagePattern() throws Exception {	
testfiltergrok notmatchingmesagepattern 

public void testFilterGrok_noMesagePattern() throws Exception {	
testfiltergrok nomesagepattern 

========================= ambari sample_1608 =========================

public void testMapperAnonymize_anonymize() {	
testmapperanonymize anonymize 

public void testMapperAnonymize_anonymize2() {	
testmapperanonymize 

public void testMapperAnonymize_noPattern() {	
testmapperanonymize nopattern 

========================= ambari sample_1599 =========================

String str = object.toString();	if (Strings.isNotEmpty(str)) {	str = str.trim();	if (str.matches(HIVE_DATE_FORMAT_REGEX)) {	try {	SimpleDateFormat sdf = new SimpleDateFormat(HIVE_DATE_FORMAT);	sdf.setLenient(false);	Date date = sdf.parse(str);	return true;	} catch (Exception e) {	
error while parsing as date string format 

public static boolean isTimeStamp(Object object) {	if (object == null) return false;	if (object instanceof Date) return true;	String str = object.toString();	try {	Timestamp ts = Timestamp.valueOf(str);	return true;	} catch (Exception e) {	
error while parsing as timestamp string 

public static boolean checkDatatype( Object object, DataTypes datatype){	switch(datatype){	case BOOLEAN : return isBoolean(object);	case INT : return isInteger(object);	case BIGINT : return isLong(object);	case DOUBLE: return isDouble(object);	case CHAR: return isChar(object);	case DATE: return isDate(object);	case TIMESTAMP: return isTimeStamp(object);	case STRING: return isString(object);	
this datatype detection is not supported 

========================= ambari sample_807 =========================

public String submitWorkflowJobToOozie(HttpHeaders headers, String filePath, MultivaluedMap<String, String> queryParams, JobType jobType) {	String nameNode = viewContext.getProperties().get("webhdfs.url");	if (nameNode == null) {	
name node couldn t be determined automatically 

public Response consumeService(HttpHeaders headers, String path, MultivaluedMap<String, String> queryParameters, String method, String body) throws Exception {	return consumeService(headers, this.buildUri(path, queryParameters), method, body, null);	}	private Response consumeService(HttpHeaders headers, String urlToRead, String method, String body, Map<String, String> customHeaders) {	Response response = null;	InputStream stream = readFromOozie(headers, urlToRead, method, body, customHeaders);	String stringResponse = null;	try {	stringResponse = IOUtils.toString(stream);	} catch (IOException e) {	
error while converting stream to string 

return response;	}	public InputStream readFromOozie(HttpHeaders headers, String urlToRead, String method, String body, Map<String, String> customHeaders) {	Map<String, String> newHeaders = utils.getHeaders(headers);	newHeaders.put(USER_NAME_HEADER, USER_OOZIE_SUPER);	newHeaders.put(DO_AS_HEADER, viewContext.getUsername());	newHeaders.put("Accept", MediaType.APPLICATION_JSON);	if (customHeaders != null) {	newHeaders.putAll(customHeaders);	}	
proxy request for url s s 

========================= ambari sample_1063 =========================

Element rootNode = doc.getRootElement();	Element record = new Element("RevertRecord");	record.setAttribute(new Attribute("id", iteration));	record.addContent(new Element("datetime").setText(currentDate.toString()));	record.addContent(new Element("dirname").setText(dirname));	record.addContent(new Element("instance").setText(instance));	record.addContent(new Element("query").setText(content));	rootNode.addContent(record);	xmlOutput.output(doc, new FileWriter(ConfigurationCheckImplementation.getHomeDir() + "RevertChangesService.xml"));	} catch (JDOMException e) {	
jdom exception 

doc.setRootElement(revertrecord);	Element record = new Element("RevertRecord");	record.setAttribute(new Attribute("id", iteration));	record.addContent(new Element("datetime").setText(currentDate.toString()));	record.addContent(new Element("dirname").setText(dirname));	record.addContent(new Element("instance").setText(instance));	record.addContent(new Element("query").setText(content));	doc.getRootElement().addContent(record);	xmlOutput.output(doc, new FileWriter(ConfigurationCheckImplementation.getHomeDir() + "RevertChangesService.xml"));	} catch (IOException io) {	
jdom exception 

pigjjobobject.setTitle(title);	String dir = rs1.getString("statusdir");	pigjjobobject.setDir(dir);	Date created_data = rs1.getDate("start_time");	pigjjobobject.setDt(created_data);	pigjobarraylist.add(pigjjobobject);	i++;	}	connection.commit();	} catch (SQLException e) {	
sqlexception 

pigjobarraylist.add(pigjjobobject);	i++;	}	connection.commit();	} catch (SQLException e) {	connection.rollback();	} finally {	try {	if (connection != null) connection.close();	} catch (SQLException e) {	
sqlexception in closing the connection 

public void updateSequenceno(Connection c, int seqNo, int id, QuerySetAmbariDB ambaridatabase) throws SQLException, IOException {	PreparedStatement prSt;	prSt = ambaridatabase.updateSequenceNoInAmbariSequence(c, seqNo, id);	
the actual insert statement is 

public void updateSequenceno(Connection c, int seqNo, int id, QuerySetAmbariDB ambaridatabase) throws SQLException, IOException {	PreparedStatement prSt;	prSt = ambaridatabase.updateSequenceNoInAmbariSequence(c, seqNo, id);	prSt.executeUpdate();	
adding revert sql hive history 

String[] subDirs = dir.split("/");	String dirPath = USER_DIRECTORY;	for(int i=2;i<subDirs.length;i++) {	dirPath += "/"+subDirs[i];	fs.setOwner(new Path(dirPath), username, username);	}	return null;	}	});	} catch (Exception e) {	
webhdfs exception 

String[] subDirs = dir.split("/");	String dirPath = USER_DIRECTORY;	for(int i=2;i<subDirs.length;i++) {	dirPath += "/"+subDirs[i];	fs.setOwner(new Path(dirPath), username, username);	}	return b;	}	});	} catch (Exception e) {	
exception in webhdfs 

}	in1.close();	out.close();	fileSystemAmbari.setOwner(path, username, username);	fileSystemHue.close();	fileSystemAmbari.close();	return null;	}	});	} catch (Exception e) {	
webhdfs exception 

}	in1.close();	out.close();	fileSystemAmbari.setOwner(path, username, username);	fileSystemHue.close();	fileSystemAmbari.close();	return null;	}	});	} catch (Exception e) {	
webhdfs exception 

========================= ambari sample_1278 =========================

Set<Resource> resources = getResources(new RequestImpl(null, null, null, null), predicate);	Map<Long, AlertGroupEntity> entities = new HashMap<>();	for (final Resource resource : resources) {	Long id = (Long) resource.getPropertyValue(ALERT_GROUP_ID);	if (!entities.containsKey(id)) {	AlertGroupEntity entity = s_dao.findGroupById(id);	try {	AlertResourceProviderUtils.verifyManageAuthorization(entity, getClusterResourceId(entity.getClusterId()));	entities.put(id, entity);	} catch (AmbariException e) {	
the default alert group for cannot be removed 

if (!entities.containsKey(id)) {	AlertGroupEntity entity = s_dao.findGroupById(id);	try {	AlertResourceProviderUtils.verifyManageAuthorization(entity, getClusterResourceId(entity.getClusterId()));	entities.put(id, entity);	} catch (AmbariException e) {	}	}	}	for (final AlertGroupEntity entity : entities.values()) {	
deleting alert group 

AlertGroupEntity entity = s_dao.findGroupById(id);	try {	AlertResourceProviderUtils.verifyManageAuthorization(entity, getClusterResourceId(entity.getClusterId()));	entities.put(id, entity);	} catch (AmbariException e) {	}	}	}	for (final AlertGroupEntity entity : entities.values()) {	if (entity.isDefault()) {	
the default alert group for cannot be removed 

========================= ambari sample_3615 =========================

public void removeInput(Input input) {	
trying to remove from inputlist 

public void removeInput(Input input) {	for (List<Input> inputList : inputs.values()) {	Iterator<Input> iter = inputList.iterator();	while (iter.hasNext()) {	Input iterInput = iter.next();	if (iterInput.equals(input)) {	
removing input from inputlist 

private void initCheckPointSettings() {	checkPointExtension = logFeederProps.getCheckPointExtension();	
determining valid checkpoint folder 

String checkPointFolder = logFeederProps.getCheckpointFolder();	if (!StringUtils.isEmpty(checkPointFolder)) {	checkPointFolderFile = new File(checkPointFolder);	isCheckPointFolderValid = verifyCheckPointFolder(checkPointFolderFile);	}	if (!isCheckPointFolderValid) {	checkPointFolderFile = new File(logFeederProps.getTmpDir(), CHECKPOINT_SUBFOLDER_NAME);	LOG.info("Checking if tmp folder can be used for checkpoints. Folder=" + checkPointFolderFile);	isCheckPointFolderValid = verifyCheckPointFolder(checkPointFolderFile);	if (isCheckPointFolderValid) {	
using tmp folder to store check points this is not recommended please set logfeeder checkpoint folder property 

isCheckPointFolderValid = verifyCheckPointFolder(checkPointFolderFile);	}	if (!isCheckPointFolderValid) {	checkPointFolderFile = new File(logFeederProps.getTmpDir(), CHECKPOINT_SUBFOLDER_NAME);	LOG.info("Checking if tmp folder can be used for checkpoints. Folder=" + checkPointFolderFile);	isCheckPointFolderValid = verifyCheckPointFolder(checkPointFolderFile);	if (isCheckPointFolderValid) {	}	}	if (isCheckPointFolderValid) {	
using folder for storing checkpoints 

private void startMonitorThread() {	inputIsReadyMonitor = new Thread("InputIsReadyMonitor") {	public void run() {	
going to monitor for these missing files 

private void startMonitorThread() {	inputIsReadyMonitor = new Thread("InputIsReadyMonitor") {	public void run() {	while (true) {	if (isDrain) {	
exiting missing file monitor 

try {	Iterator<Input> iter = notReadyList.iterator();	while (iter.hasNext()) {	Input input = iter.next();	try {	if (input.isReady()) {	input.monitor();	iter.remove();	}	} catch (Throwable t) {	
error while enabling monitoring for input 

public void startInputs(String serviceName) {	for (Input input : inputs.get(serviceName)) {	try {	input.init(logFeederProps);	if (input.isReady()) {	input.monitor();	} else {	
adding input to not ready list note it is possible this component is not run on this host so it might not be an issue 

public void startInputs(String serviceName) {	for (Input input : inputs.get(serviceName)) {	try {	input.init(logFeederProps);	if (input.isReady()) {	input.monitor();	} else {	notReadyList.add(input);	}	} catch (Exception e) {	
error initializing input 

} catch (Throwable t) {	LOG.warn("Error creating folder for check point. folder=" + folderPathFile, t);	}	}	if (folderPathFile.exists() && folderPathFile.isDirectory()) {	File testFile = new File(folderPathFile, UUID.randomUUID().toString());	try {	testFile.createNewFile();	return testFile.delete();	} catch (IOException e) {	
couldn t create test file in for checkpoint 

FileFilter fileFilter = new WildcardFileFilter(searchPath);	File[] checkPointFiles = checkPointFolderFile.listFiles(fileFilter);	int totalCheckFilesDeleted = 0;	for (File checkPointFile : checkPointFiles) {	if (checkCheckPointFile(checkPointFile)) {	totalCheckFilesDeleted++;	}	}	LOG.info("Deleted " + totalCheckFilesDeleted + " checkPoint file(s). checkPointFolderFile=" + checkPointFolderFile.getAbsolutePath());	} catch (Throwable t) {	
error while cleaning checkpointfiles 

deleteCheckPointFile = !wasFileRenamed(logFile.getParentFile(), logFileKey);	}	if (deleteCheckPointFile) {	LOG.info("Deleting CheckPoint file=" + checkPointFile.getAbsolutePath() + ", logFile=" + logFilePath);	checkPointFile.delete();	deleted = true;	}	}	}	} catch (EOFException eof) {	
caught eofexception ignoring reading existing checkpoint file 

}	if (deleteCheckPointFile) {	LOG.info("Deleting CheckPoint file=" + checkPointFile.getAbsolutePath() + ", logFile=" + logFilePath);	checkPointFile.delete();	deleted = true;	}	}	}	} catch (EOFException eof) {	} catch (Throwable t) {	
error while checking checkpoint file 

private boolean wasFileRenamed(File folder, String searchFileBase64) {	for (File file : folder.listFiles()) {	Object fileKeyObj = FileUtil.getFileKey(file);	String fileBase64 = Base64.byteArrayToBase64(fileKeyObj.toString().getBytes());	if (searchFileBase64.equals(fileBase64)) {	
checkpoint clean file key matches file it must have been renamed 

isDrain = true;	int iterations = 30;	int waitTimeMS = 1000;	for (int i = 0; i < iterations; i++) {	boolean allClosed = true;	for (List<Input> inputList : inputs.values()) {	for (Input input : inputList) {	if (!input.isClosed()) {	try {	allClosed = false;	
waiting for input to close more seconds 

} catch (Throwable t) {	}	}	}	}	if (allClosed) {	LOG.info("All inputs are closed. Iterations=" + i);	return;	}	}	
some inputs were not closed after iterations 

}	}	if (allClosed) {	LOG.info("All inputs are closed. Iterations=" + i);	return;	}	}	for (List<Input> inputList : inputs.values()) {	for (Input input : inputList) {	if (!input.isClosed()) {	
input not closed will ignore it 

========================= ambari sample_1628 =========================

private String processRackInfo(Map<String, Object> properties) {	String rackInfo = null;	if (properties.containsKey(HostResourceProvider.HOST_RACK_INFO_PROPERTY_ID)) {	rackInfo = (String) properties.get(HostResourceProvider.HOST_RACK_INFO_PROPERTY_ID);	} else if (properties.containsKey(HostResourceProvider.RACK_INFO_PROPERTY_ID)) {	rackInfo = (String) properties.get(HostResourceProvider.RACK_INFO_PROPERTY_ID);	} else {	
no rack info provided 

========================= ambari sample_3496 =========================

public void init(MetricsConfiguration metricsConfig, MetricsSink sink) {	super.init(metricsConfig, sink);	configuration = metricsConfig;	initializeFilterSets();	
initialized ambari db metrics source 

public void start() {	ThreadFactory threadFactory = new ThreadFactoryBuilder() .setNameFormat("DatabaseMetricsSource-%d") .build();	executor = Executors.newSingleThreadExecutor(threadFactory);	
started ambari db metrics source 

if (value != 0.0) {	metrics.add(new SingleMetric(baseMetricName, timerValue / value , currentTime));	}	}	}	}	sink.publish(metrics);	}	});	} catch (Exception e) {	
exception when publishing database metrics to sink 

========================= ambari sample_2638 =========================

public void commence(HttpServletRequest request, HttpServletResponse response, AuthenticationException authException) throws IOException, ServletException {	
got from request 

========================= ambari sample_1385 =========================

public void testConfigurationDefinitionsExist() throws Exception {	Collection<StackInfo> stacks = ambariMetaInfo.getStacks();	Assert.assertFalse(stacks.isEmpty());	Cluster cluster = EasyMock.createNiceMock(Cluster.class);	for( StackInfo stack : stacks ){	if (!stack.isActive()) {	
skipping configuration validity test for 

private void assertIdDefinitionExists(String id, ConfigUpgradePack configUpgradePack, UpgradePack upgradePack, StackId sourceStackId) {	Assert.assertNotNull(id);	if (configUpgradePack.enumerateConfigChangesByID().containsKey(id)) {	validatedConfigCount++;	
validated from upgrade pack for 

========================= ambari sample_2053 =========================

protected void aggregate(ResultSet rs, long startTime, long endTime) throws IOException, SQLException {	Map<TimelineClusterMetric, MetricHostAggregate> hostAggregateMap = aggregateMetricsFromResultSet(rs, endTime);	
saving metric aggregates 

========================= ambari sample_348 =========================

SolrRequest<SchemaResponse> schemaRequest = new SchemaRequest();	schemaRequest.setMethod(SolrRequest.METHOD.GET);	schemaRequest.setPath("/schema");	schemaResponse = schemaRequest.process(solrClient);	LOG.debug("populateSchemaFields() collection=" + solrClient.getDefaultCollection() + ", luke=" + lukeResponses + ", schema= " + schemaResponse);	} catch (SolrException | SolrServerException | IOException e) {	LOG.error("Error occured while popuplating field. collection=" + solrClient.getDefaultCollection(), e);	}	if (schemaResponse != null) {	extractSchemaFieldsName(lukeResponses, schemaResponse, schemaFieldNameMap, schemaFieldTypeMap);	
populate fields for collection was successful next update it after minutes 

for (Slice slice : activeSlices) {	for (Replica replica : slice.getReplicas()) {	try (CloseableHttpClient httpClient = HttpClientUtil.createClient(null)) {	HttpGet request = new HttpGet(replica.getCoreUrl() + LUKE_REQUEST_URL_SUFFIX);	HttpResponse response = httpClient.execute(request);	NamedList<Object> lukeData = (NamedList<Object>) new JavaBinCodec().unmarshal(response.getEntity().getContent());	LukeResponse lukeResponse = new LukeResponse();	lukeResponse.setResponse(lukeData);	lukeResponses.add(lukeResponse);	} catch (IOException e) {	
exception during getting luke responses 

if (_schemaFieldNameMap.isEmpty() || _schemaFieldTypeMap.isEmpty()) {	return;	}	synchronized (this) {	schemaFieldNameMap.clear();	schemaFieldNameMap.putAll(_schemaFieldNameMap);	schemaFieldTypeMap.clear();	schemaFieldTypeMap.putAll(_schemaFieldTypeMap);	}	} catch (Exception e) {	
credentials not specified in logsearch properties 

========================= ambari sample_1413 =========================

public boolean persist(Collection<List<String>> collectionData) {	try {	
persisting collection to csv file 

public boolean persist(Collection<List<String>> collectionData) {	try {	csvPrinter.printRecords(collectionData);	
collection successfully persisted to csv file 

public boolean persist(Collection<List<String>> collectionData) {	try {	csvPrinter.printRecords(collectionData);	return true;	} catch (IOException e) {	
failed to persist the collection to csv file 

csvPrinter.printRecords(collectionData);	return true;	} catch (IOException e) {	return false;	} finally {	try {	fileWriter.flush();	fileWriter.close();	csvPrinter.close();	} catch (IOException e) {	
error while flushing closing filewriter csvprinter 

public boolean persistMap(Map<String, List<String>> mapData) {	
persisting map data to csv file 

========================= ambari sample_3341 =========================

public void serviceInit(Configuration conf) throws Exception {	Path fsWorkingPath = new Path(conf.get(YarnConfiguration.FS_APPLICATION_HISTORY_STORE_URI));	rootDirPath = new Path(fsWorkingPath, ROOT_DIR_NAME);	try {	fs = fsWorkingPath.getFileSystem(conf);	fs.mkdirs(rootDirPath);	fs.setPermission(rootDirPath, ROOT_DIR_UMASK);	} catch (IOException e) {	
error when initializing filesystemhistorystorage 

ApplicationFinishData finishData = parseApplicationFinishData(entry.value);	mergeApplicationHistoryData(historyData, finishData);	readFinishData = true;	}	}	}	if (!readStartData && !readFinishData) {	return null;	}	if (!readStartData) {	
start information is missing for application 

readFinishData = true;	}	}	}	if (!readStartData && !readFinishData) {	return null;	}	if (!readStartData) {	}	if (!readFinishData) {	
finish information is missing for application 

}	}	}	if (!readStartData && !readFinishData) {	return null;	}	if (!readStartData) {	}	if (!readFinishData) {	}	
completed reading history information of application 

}	if (!readStartData && !readFinishData) {	return null;	}	if (!readStartData) {	}	if (!readFinishData) {	}	return historyData;	} catch (IOException e) {	
error when reading history file of application 

Map<ApplicationId, ApplicationHistoryData> historyDataMap = new HashMap<ApplicationId, ApplicationHistoryData>();	FileStatus[] files = fs.listStatus(rootDirPath);	for (FileStatus file : files) {	ApplicationId appId = ConverterUtils.toApplicationId(file.getPath().getName());	try {	ApplicationHistoryData historyData = getApplication(appId);	if (historyData != null) {	historyDataMap.put(appId, historyData);	}	} catch (IOException e) {	
history information of application is not included into the result due to the exception 

ApplicationAttemptFinishData finishData = parseApplicationAttemptFinishData(entry.value);	mergeApplicationAttemptHistoryData(historyData, finishData);	readFinishData = true;	}	}	}	if (!readStartData && !readFinishData) {	return null;	}	if (!readStartData) {	
start information is missing for application attempt 

readFinishData = true;	}	}	}	if (!readStartData && !readFinishData) {	return null;	}	if (!readStartData) {	}	if (!readFinishData) {	
finish information is missing for application attempt 

}	}	}	if (!readStartData && !readFinishData) {	return null;	}	if (!readStartData) {	}	if (!readFinishData) {	}	
completed reading history information of application attempt 

}	if (!readStartData && !readFinishData) {	return null;	}	if (!readStartData) {	}	if (!readFinishData) {	}	return historyData;	} catch (IOException e) {	
error when reading history file of application attempt 

ContainerFinishData finishData = parseContainerFinishData(entry.value);	mergeContainerHistoryData(historyData, finishData);	readFinishData = true;	}	}	}	if (!readStartData && !readFinishData) {	return null;	}	if (!readStartData) {	
start information is missing for container 

readFinishData = true;	}	}	}	if (!readStartData && !readFinishData) {	return null;	}	if (!readStartData) {	}	if (!readFinishData) {	
finish information is missing for container 

}	}	}	if (!readStartData && !readFinishData) {	return null;	}	if (!readStartData) {	}	if (!readFinishData) {	}	
completed reading history information of container 

}	if (!readStartData && !readFinishData) {	return null;	}	if (!readStartData) {	}	if (!readFinishData) {	}	return historyData;	} catch (IOException e) {	
error when reading history file of container 

historyDataMap.put(containerId, historyData);	}	if (entry.key.suffix.equals(START_DATA_SUFFIX)) {	mergeContainerHistoryData(historyData, parseContainerStartData(entry.value));	} else if (entry.key.suffix.equals(FINISH_DATA_SUFFIX)) {	mergeContainerHistoryData(historyData, parseContainerFinishData(entry.value));	}	}	}	}	
completed reading history information of all conatiners of application attempt 

}	if (entry.key.suffix.equals(START_DATA_SUFFIX)) {	mergeContainerHistoryData(historyData, parseContainerStartData(entry.value));	} else if (entry.key.suffix.equals(FINISH_DATA_SUFFIX)) {	mergeContainerHistoryData(historyData, parseContainerFinishData(entry.value));	}	}	}	}	} catch (IOException e) {	
error when reading history information of some containers of application attempt 

public void applicationStarted(ApplicationStartData appStart) throws IOException {	HistoryFileWriter hfWriter = outstandingWriters.get(appStart.getApplicationId());	if (hfWriter == null) {	Path applicationHistoryFile = new Path(rootDirPath, appStart.getApplicationId().toString());	try {	hfWriter = new HistoryFileWriter(applicationHistoryFile);	
opened history file of application 

public void applicationStarted(ApplicationStartData appStart) throws IOException {	HistoryFileWriter hfWriter = outstandingWriters.get(appStart.getApplicationId());	if (hfWriter == null) {	Path applicationHistoryFile = new Path(rootDirPath, appStart.getApplicationId().toString());	try {	hfWriter = new HistoryFileWriter(applicationHistoryFile);	} catch (IOException e) {	
error when openning history file of application 

} catch (IOException e) {	throw e;	}	outstandingWriters.put(appStart.getApplicationId(), hfWriter);	} else {	throw new IOException("History file of application " + appStart.getApplicationId() + " is already opened");	}	assert appStart instanceof ApplicationStartDataPBImpl;	try {	hfWriter.writeHistoryData(new HistoryDataKey(appStart.getApplicationId() .toString(), START_DATA_SUFFIX), ((ApplicationStartDataPBImpl) appStart).getProto().toByteArray());	
start information of application is written 

throw e;	}	outstandingWriters.put(appStart.getApplicationId(), hfWriter);	} else {	throw new IOException("History file of application " + appStart.getApplicationId() + " is already opened");	}	assert appStart instanceof ApplicationStartDataPBImpl;	try {	hfWriter.writeHistoryData(new HistoryDataKey(appStart.getApplicationId() .toString(), START_DATA_SUFFIX), ((ApplicationStartDataPBImpl) appStart).getProto().toByteArray());	} catch (IOException e) {	
error when writing start information of application 

public void applicationFinished(ApplicationFinishData appFinish) throws IOException {	HistoryFileWriter hfWriter = getHistoryFileWriter(appFinish.getApplicationId());	assert appFinish instanceof ApplicationFinishDataPBImpl;	try {	hfWriter.writeHistoryData(new HistoryDataKey(appFinish.getApplicationId() .toString(), FINISH_DATA_SUFFIX), ((ApplicationFinishDataPBImpl) appFinish).getProto().toByteArray());	
finish information of application is written 

public void applicationFinished(ApplicationFinishData appFinish) throws IOException {	HistoryFileWriter hfWriter = getHistoryFileWriter(appFinish.getApplicationId());	assert appFinish instanceof ApplicationFinishDataPBImpl;	try {	hfWriter.writeHistoryData(new HistoryDataKey(appFinish.getApplicationId() .toString(), FINISH_DATA_SUFFIX), ((ApplicationFinishDataPBImpl) appFinish).getProto().toByteArray());	} catch (IOException e) {	
error when writing finish information of application 

public void applicationAttemptStarted( ApplicationAttemptStartData appAttemptStart) throws IOException {	HistoryFileWriter hfWriter = getHistoryFileWriter(appAttemptStart.getApplicationAttemptId() .getApplicationId());	assert appAttemptStart instanceof ApplicationAttemptStartDataPBImpl;	try {	hfWriter.writeHistoryData(new HistoryDataKey(appAttemptStart .getApplicationAttemptId().toString(), START_DATA_SUFFIX), ((ApplicationAttemptStartDataPBImpl) appAttemptStart).getProto() .toByteArray());	
start information of application attempt is written 

public void applicationAttemptStarted( ApplicationAttemptStartData appAttemptStart) throws IOException {	HistoryFileWriter hfWriter = getHistoryFileWriter(appAttemptStart.getApplicationAttemptId() .getApplicationId());	assert appAttemptStart instanceof ApplicationAttemptStartDataPBImpl;	try {	hfWriter.writeHistoryData(new HistoryDataKey(appAttemptStart .getApplicationAttemptId().toString(), START_DATA_SUFFIX), ((ApplicationAttemptStartDataPBImpl) appAttemptStart).getProto() .toByteArray());	} catch (IOException e) {	
error when writing start information of application attempt 

public void applicationAttemptFinished( ApplicationAttemptFinishData appAttemptFinish) throws IOException {	HistoryFileWriter hfWriter = getHistoryFileWriter(appAttemptFinish.getApplicationAttemptId() .getApplicationId());	assert appAttemptFinish instanceof ApplicationAttemptFinishDataPBImpl;	try {	hfWriter.writeHistoryData(new HistoryDataKey(appAttemptFinish .getApplicationAttemptId().toString(), FINISH_DATA_SUFFIX), ((ApplicationAttemptFinishDataPBImpl) appAttemptFinish).getProto() .toByteArray());	
finish information of application attempt is written 

public void applicationAttemptFinished( ApplicationAttemptFinishData appAttemptFinish) throws IOException {	HistoryFileWriter hfWriter = getHistoryFileWriter(appAttemptFinish.getApplicationAttemptId() .getApplicationId());	assert appAttemptFinish instanceof ApplicationAttemptFinishDataPBImpl;	try {	hfWriter.writeHistoryData(new HistoryDataKey(appAttemptFinish .getApplicationAttemptId().toString(), FINISH_DATA_SUFFIX), ((ApplicationAttemptFinishDataPBImpl) appAttemptFinish).getProto() .toByteArray());	} catch (IOException e) {	
error when writing finish information of application attempt 

public void containerStarted(ContainerStartData containerStart) throws IOException {	HistoryFileWriter hfWriter = getHistoryFileWriter(containerStart.getContainerId() .getApplicationAttemptId().getApplicationId());	assert containerStart instanceof ContainerStartDataPBImpl;	try {	hfWriter.writeHistoryData(new HistoryDataKey(containerStart .getContainerId().toString(), START_DATA_SUFFIX), ((ContainerStartDataPBImpl) containerStart).getProto().toByteArray());	
start information of container is written 

public void containerStarted(ContainerStartData containerStart) throws IOException {	HistoryFileWriter hfWriter = getHistoryFileWriter(containerStart.getContainerId() .getApplicationAttemptId().getApplicationId());	assert containerStart instanceof ContainerStartDataPBImpl;	try {	hfWriter.writeHistoryData(new HistoryDataKey(containerStart .getContainerId().toString(), START_DATA_SUFFIX), ((ContainerStartDataPBImpl) containerStart).getProto().toByteArray());	} catch (IOException e) {	
error when writing start information of container 

public void containerFinished(ContainerFinishData containerFinish) throws IOException {	HistoryFileWriter hfWriter = getHistoryFileWriter(containerFinish.getContainerId() .getApplicationAttemptId().getApplicationId());	assert containerFinish instanceof ContainerFinishDataPBImpl;	try {	hfWriter.writeHistoryData(new HistoryDataKey(containerFinish .getContainerId().toString(), FINISH_DATA_SUFFIX), ((ContainerFinishDataPBImpl) containerFinish).getProto().toByteArray());	
finish information of container is written 

public void containerFinished(ContainerFinishData containerFinish) throws IOException {	HistoryFileWriter hfWriter = getHistoryFileWriter(containerFinish.getContainerId() .getApplicationAttemptId().getApplicationId());	assert containerFinish instanceof ContainerFinishDataPBImpl;	try {	hfWriter.writeHistoryData(new HistoryDataKey(containerFinish .getContainerId().toString(), FINISH_DATA_SUFFIX), ((ContainerFinishDataPBImpl) containerFinish).getProto().toByteArray());	} catch (IOException e) {	
error when writing finish information of container 

========================= ambari sample_427 =========================

private static ReentrantLock getLockFor(String key){	
finding lock for 

private static ReentrantLock getLockFor(String key){	if( null == locks.get(key)){	
lock not found for 

private static ReentrantLock getLockFor(String key){	if( null == locks.get(key)){	synchronized (locks){	if(null == locks.get(key)){	
creating lock for 

public T get(ViewContext context) {	if (!viewSingletonObjects.containsKey(tClass)) {	synchronized (viewSingletonObjects) {	if (!viewSingletonObjects.containsKey(tClass)) {	viewSingletonObjects.put(tClass, new ConcurrentHashMap<String, Object>());	}	}	}	Map<String, Object> instances = viewSingletonObjects.get(tClass);	String key = getTagName(context);	
looking for key 

synchronized (viewSingletonObjects) {	if (!viewSingletonObjects.containsKey(tClass)) {	viewSingletonObjects.put(tClass, new ConcurrentHashMap<String, Object>());	}	}	}	Map<String, Object> instances = viewSingletonObjects.get(tClass);	String key = getTagName(context);	if (!instances.containsKey(key)) {	String lockKey = tClass.getName() + "_" + key;	
key not found getting lock for 

}	}	}	Map<String, Object> instances = viewSingletonObjects.get(tClass);	String key = getTagName(context);	if (!instances.containsKey(key)) {	String lockKey = tClass.getName() + "_" + key;	ReentrantLock lock = getLockFor(lockKey);	boolean gotLock = lock.tryLock();	if( !gotLock ){	
lock could not be obtained for throwing exception 

String lockKey = tClass.getName() + "_" + key;	ReentrantLock lock = getLockFor(lockKey);	boolean gotLock = lock.tryLock();	if( !gotLock ){	throw new RuntimeException(String.format("Failed to initialize %s for %s. Try Again.", tClass.getName(), key));	}	else {	try {	if (!instances.containsKey(key)) {	T initValue = initialValue(context);	
obtained initial value for key 

public void set(T obj, ViewContext context) {	if (!viewSingletonObjects.containsKey(tClass)) {	synchronized (viewSingletonObjects) {	if (!viewSingletonObjects.containsKey(tClass)) {	viewSingletonObjects.put(tClass, new ConcurrentHashMap<String, Object>());	}	}	}	String key = getTagName(context);	
setting key value 

public void remove(ViewContext context) {	String key = getTagName(context);	
removing key 

public static void dropAllConnections(Class tClass) {	
removing all 

public static void dropAllConnections() {	
clearing all viewsingletonobjects 

public static void dropInstanceCache(String instanceName){	
removing all the keys for instancename 

public static void dropInstanceCache(String instanceName){	for(Map<String,Object> cache : viewSingletonObjects.values()){	for(Iterator<Map.Entry<String, Object>> it = cache.entrySet().iterator();it.hasNext();){	Map.Entry<String, Object> entry = it.next();	if(entry.getKey().startsWith(instanceName+":")){	
removing key 

========================= ambari sample_1110 =========================

}	query.append(")");	}	}	query.append(")");	if(tableInfo.getHiveFileType().equals(HiveFileType.TEXTFILE)) {	query.append(getRowFormatQuery(tableInfo.getRowFormat()));	}	query.append(" STORED AS ").append(tableInfo.getHiveFileType().toString());	String queryString = query.append(";").toString();	
query 

insertQuery.append("UNHEX(");	}	insertQuery.append(column.getName());	if(unhex) {	insertQuery.append(")");	}	first = false;	}	insertQuery.append(" FROM ").append(ifqi.getFromDatabase()).append(".").append(ifqi.getFromTable()).append(";");	String query = insertQuery.toString();	
insert query 

public String generateDropTableQuery(DeleteQueryInput deleteQueryInput) {	String dropQuery = new StringBuilder("DROP TABLE ").append(deleteQueryInput.getDatabase()) .append(".").append(deleteQueryInput.getTable()).append(";").toString();	
drop query 

========================= ambari sample_831 =========================

public JobExecutionInfoResponse launchJob(String jobName, String params) throws JobParametersInvalidException, JobInstanceAlreadyExistsException, NoSuchJobException, JobExecutionAlreadyRunningException, JobRestartException, JobInstanceAlreadyCompleteException {	JobParametersBuilder jobParametersBuilder = new JobParametersBuilder();	if (params != null) {	
parsing parameters of job 

========================= ambari sample_132 =========================

public void onSuccessfulAuthentication(AmbariAuthenticationFilter filter, HttpServletRequest servletRequest, HttpServletResponse servletResponse, Authentication result) {	String username = (result == null) ? null : result.getName();	if (auditLogger.isEnabled()) {	AuditEvent loginSucceededAuditEvent = LoginAuditEvent.builder() .withRemoteIp(RequestUtils.getRemoteAddress(servletRequest)) .withUserName(username) .withTimestamp(System.currentTimeMillis()) .withRoles(permissionHelper.getPermissionLabels(result)) .build();	auditLogger.log(loginSucceededAuditEvent);	}	if (!StringUtils.isEmpty(username)) {	
successfully authenticated 

public void onSuccessfulAuthentication(AmbariAuthenticationFilter filter, HttpServletRequest servletRequest, HttpServletResponse servletResponse, Authentication result) {	String username = (result == null) ? null : result.getName();	if (auditLogger.isEnabled()) {	AuditEvent loginSucceededAuditEvent = LoginAuditEvent.builder() .withRemoteIp(RequestUtils.getRemoteAddress(servletRequest)) .withUserName(username) .withTimestamp(System.currentTimeMillis()) .withRoles(permissionHelper.getPermissionLabels(result)) .build();	auditLogger.log(loginSucceededAuditEvent);	}	if (!StringUtils.isEmpty(username)) {	users.clearConsecutiveAuthenticationFailures(username);	} else {	
successfully authenticated an unknown user 

========================= ambari sample_2854 =========================

}	visitedPrincipalKeys.add(visitationKey);	}	} else {	message = String.format("Failed to create keytab file for %s, the container directory does not exist: %s", resolvedPrincipal.getPrincipal(), hostDirectory.getAbsolutePath());	actionLog.writeStdErr(message);	LOG.error(message);	commandReport = createCommandReport(1, HostRoleStatus.FAILED, "{}", actionLog.getStdOut(), actionLog.getStdErr());	}	} else {	
skipping previously processed keytab for on host 

public Keytab createKeytab(String principal, String password, Integer keyNumber, KerberosOperationHandler operationHandler, boolean checkCache, boolean canCache, ActionLog actionLog) throws AmbariException {	
creating keytab for with kvno 

KerberosPrincipalEntity principalEntity = kerberosPrincipalDAO.find(principal);	if (principalEntity != null) {	if (canCache) {	File cachedKeytabFile = cacheKeytab(principal, keytab);	String previousCachedFilePath = principalEntity.getCachedKeytabPath();	String cachedKeytabFilePath = ((cachedKeytabFile == null) || !cachedKeytabFile.exists()) ? null : cachedKeytabFile.getAbsolutePath();	principalEntity.setCachedKeytabPath(cachedKeytabFilePath);	kerberosPrincipalDAO.merge(principalEntity);	if (previousCachedFilePath != null) {	if (!new File(previousCachedFilePath).delete()) {	
failed to remove orphaned cache file 

========================= ambari sample_3316 =========================

public AmbariLdapConfiguration detectLdapUserAttributes(AmbariLdapConfiguration ambariLdapConfiguration) throws AmbariLdapException {	
detecting ldap user attributes 

public AmbariLdapConfiguration detectLdapUserAttributes(AmbariLdapConfiguration ambariLdapConfiguration) throws AmbariLdapException {	if (Strings.isEmpty(ambariLdapConfiguration.userSearchBase())) {	
no user search base provided 

public AmbariLdapConfiguration detectLdapUserAttributes(AmbariLdapConfiguration ambariLdapConfiguration) throws AmbariLdapException {	if (Strings.isEmpty(ambariLdapConfiguration.userSearchBase())) {	return ambariLdapConfiguration;	}	try {	LdapConnectionTemplate ldapConnectionTemplate = ldapConnectionTemplateFactory.create(ambariLdapConfiguration);	AttributeDetector<Entry> userAttributeDetector = attributeDetectorFactory.userAttributDetector();	SearchRequest searchRequest = assembleUserSearchRequest(ldapConnectionTemplate, ambariLdapConfiguration);	List<Entry> entries = ldapConnectionTemplate.search(searchRequest, getEntryMapper());	for (Entry entry : entries) {	
collecting user attribute information from the sample entry with dn 

try {	LdapConnectionTemplate ldapConnectionTemplate = ldapConnectionTemplateFactory.create(ambariLdapConfiguration);	AttributeDetector<Entry> userAttributeDetector = attributeDetectorFactory.userAttributDetector();	SearchRequest searchRequest = assembleUserSearchRequest(ldapConnectionTemplate, ambariLdapConfiguration);	List<Entry> entries = ldapConnectionTemplate.search(searchRequest, getEntryMapper());	for (Entry entry : entries) {	userAttributeDetector.collect(entry);	}	Map<String, String> detectedUserAttributes = userAttributeDetector.detect();	setDetectedAttributes(ambariLdapConfiguration, detectedUserAttributes);	
decorated ambari ldap config 

LdapConnectionTemplate ldapConnectionTemplate = ldapConnectionTemplateFactory.create(ambariLdapConfiguration);	AttributeDetector<Entry> userAttributeDetector = attributeDetectorFactory.userAttributDetector();	SearchRequest searchRequest = assembleUserSearchRequest(ldapConnectionTemplate, ambariLdapConfiguration);	List<Entry> entries = ldapConnectionTemplate.search(searchRequest, getEntryMapper());	for (Entry entry : entries) {	userAttributeDetector.collect(entry);	}	Map<String, String> detectedUserAttributes = userAttributeDetector.detect();	setDetectedAttributes(ambariLdapConfiguration, detectedUserAttributes);	} catch (Exception e) {	
ldap operation failed while detecting user attributes 

public AmbariLdapConfiguration detectLdapGroupAttributes(AmbariLdapConfiguration ambariLdapConfiguration) throws AmbariLdapException {	
detecting ldap group attributes 

public AmbariLdapConfiguration detectLdapGroupAttributes(AmbariLdapConfiguration ambariLdapConfiguration) throws AmbariLdapException {	if (Strings.isEmpty(ambariLdapConfiguration.groupSearchBase())) {	
no group search base provided 

public AmbariLdapConfiguration detectLdapGroupAttributes(AmbariLdapConfiguration ambariLdapConfiguration) throws AmbariLdapException {	if (Strings.isEmpty(ambariLdapConfiguration.groupSearchBase())) {	return ambariLdapConfiguration;	}	try {	LdapConnectionTemplate ldapConnectionTemplate = ldapConnectionTemplateFactory.create(ambariLdapConfiguration);	AttributeDetector<Entry> groupAttributeDetector = attributeDetectorFactory.groupAttributeDetector();	SearchRequest searchRequest = assembleGroupSearchRequest(ldapConnectionTemplate, ambariLdapConfiguration);	List<Entry> groupEntries = ldapConnectionTemplate.search(searchRequest, getEntryMapper());	for (Entry groupEntry : groupEntries) {	
collecting group attribute information from the sample entry with dn 

try {	LdapConnectionTemplate ldapConnectionTemplate = ldapConnectionTemplateFactory.create(ambariLdapConfiguration);	AttributeDetector<Entry> groupAttributeDetector = attributeDetectorFactory.groupAttributeDetector();	SearchRequest searchRequest = assembleGroupSearchRequest(ldapConnectionTemplate, ambariLdapConfiguration);	List<Entry> groupEntries = ldapConnectionTemplate.search(searchRequest, getEntryMapper());	for (Entry groupEntry : groupEntries) {	groupAttributeDetector.collect(groupEntry);	}	Map<String, String> detectedGroupAttributes = groupAttributeDetector.detect();	setDetectedAttributes(ambariLdapConfiguration, detectedGroupAttributes);	
decorated ambari ldap config 

LdapConnectionTemplate ldapConnectionTemplate = ldapConnectionTemplateFactory.create(ambariLdapConfiguration);	AttributeDetector<Entry> groupAttributeDetector = attributeDetectorFactory.groupAttributeDetector();	SearchRequest searchRequest = assembleGroupSearchRequest(ldapConnectionTemplate, ambariLdapConfiguration);	List<Entry> groupEntries = ldapConnectionTemplate.search(searchRequest, getEntryMapper());	for (Entry groupEntry : groupEntries) {	groupAttributeDetector.collect(groupEntry);	}	Map<String, String> detectedGroupAttributes = groupAttributeDetector.detect();	setDetectedAttributes(ambariLdapConfiguration, detectedGroupAttributes);	} catch (Exception e) {	
ldap operation failed while detecting group attributes 

private void setDetectedAttributes(AmbariLdapConfiguration ambariLdapConfiguration, Map<String, String> detectedAttributes) {	for (Map.Entry<String, String> detecteMapEntry : detectedAttributes.entrySet()) {	
setting detected configuration value 

private SearchRequest assembleUserSearchRequest(LdapConnectionTemplate ldapConnectionTemplate, AmbariLdapConfiguration ambariLdapConfiguration) throws AmbariLdapException {	try {	SearchRequest req = ldapConnectionTemplate.newSearchRequest(ambariLdapConfiguration.userSearchBase(), FilterBuilder.present("objectClass").toString(), SearchScope.SUBTREE);	req.setSizeLimit(SAMPLE_RESULT_SIZE);	return req;	} catch (Exception e) {	
could not assemble ldap search request 

private SearchRequest assembleGroupSearchRequest(LdapConnectionTemplate ldapConnectionTemplate, AmbariLdapConfiguration ambariLdapConfiguration) throws AmbariLdapException {	try {	SearchRequest req = ldapConnectionTemplate.newSearchRequest(ambariLdapConfiguration.groupSearchBase(), FilterBuilder.present("objectClass").toString(), SearchScope.SUBTREE);	req.setSizeLimit(SAMPLE_RESULT_SIZE);	return req;	} catch (Exception e) {	
could not assemble ldap search request 

========================= ambari sample_2911 =========================

for (ProcessingService svc : processing) {	Map<String, ProcessingComponent> componentMap = m_process.get(svc.name);	if (null == componentMap) {	componentMap = new LinkedHashMap<>();	m_process.put(svc.name, componentMap);	}	for (ProcessingComponent pc : svc.components) {	if (pc != null) {	componentMap.put(pc.name, pc);	} else {	
processingservice has null amongst it s values total components 

========================= ambari sample_3253 =========================

public synchronized <T extends Indexed> T load(Class<T> model, Object id) throws ItemNotFound {	
loading s s 

public synchronized <T extends Indexed> List<T> loadAll(Class<? extends T> model, FilteringStrategy filter) {	LinkedList<T> list = new LinkedList<T>();	
loading all s s 

public synchronized void delete(Class model, Object id) throws ItemNotFound {	
deleting s s 

========================= ambari sample_923 =========================

public void onClusterConfigFinishedEvent(ClusterConfigFinishedEvent event) {	ManagedThreadPoolExecutor taskExecutor = topologyTaskExecutorServiceMap.get(event.getClusterId());	if (taskExecutor == null) {	
can t find executor service taskqueue not found for cluster 

public void onClusterConfigFinishedEvent(ClusterConfigFinishedEvent event) {	ManagedThreadPoolExecutor taskExecutor = topologyTaskExecutorServiceMap.get(event.getClusterId());	if (taskExecutor == null) {	} else {	
starting topology task executorservice for cluster 

private SecurityConfiguration processSecurityConfiguration(ProvisionClusterRequest request) {	
getting security configuration from the request 

private SecurityConfiguration processSecurityConfiguration(ProvisionClusterRequest request) {	SecurityConfiguration securityConfiguration = request.getSecurityConfiguration();	if (securityConfiguration == null) {	
there s no security configuration in the request retrieving it from the associated blueprint 

Map<String, Object> properties = new HashMap<>();	properties.put(ArtifactResourceProvider.ARTIFACT_NAME_PROPERTY, "kerberos_descriptor");	properties.put("Artifacts/cluster_name", clusterName);	Map<String, String> requestInfoProps = new HashMap<>();	requestInfoProps.put(org.apache.ambari.server.controller.spi.Request.REQUEST_INFO_BODY_PROPERTY, "{\"" + ArtifactResourceProvider.ARTIFACT_DATA_PROPERTY + "\": " + descriptor + "}");	org.apache.ambari.server.controller.spi.Request request = new RequestImpl(Collections.emptySet(), Collections.singleton(properties), requestInfoProps, null);	try {	RequestStatus status = artifactProvider.createResources(request);	try {	while (status.getStatus() != RequestStatus.Status.Complete) {	
waiting for kerberos descriptor artifact creation 

Map<String, String> requestInfoProps = new HashMap<>();	requestInfoProps.put(org.apache.ambari.server.controller.spi.Request.REQUEST_INFO_BODY_PROPERTY, "{\"" + ArtifactResourceProvider.ARTIFACT_DATA_PROPERTY + "\": " + descriptor + "}");	org.apache.ambari.server.controller.spi.Request request = new RequestImpl(Collections.emptySet(), Collections.singleton(properties), requestInfoProps, null);	try {	RequestStatus status = artifactProvider.createResources(request);	try {	while (status.getStatus() != RequestStatus.Status.Complete) {	Thread.sleep(100);	}	} catch (InterruptedException e) {	
wait for resource creation interrupted 

public RequestStatusResponse scaleHosts(final ScaleClusterRequest request) throws InvalidTopologyException, AmbariException {	ensureInitialized();	
topologymanager scalehosts entering 

public void removePendingHostRequests(String clusterName, long requestId) {	ensureInitialized();	
topologymanager removependinghostrequests entering 

public void removePendingHostRequests(String clusterName, long requestId) {	ensureInitialized();	long clusterId = 0;	try {	clusterId = ambariContext.getClusterId(clusterName);	} catch (AmbariException e) {	
unable to retrieve clusterid 

if (!logicalRequest.hasPendingHostRequests()) {	outstandingRequests.remove(logicalRequest);	}	if (logicalRequest.getHostRequests().isEmpty()) {	allRequests.remove(requestId);	}	persistedState.removeHostRequests(requestId, pendingHostRequests);	for (HostGroupInfo currentHostGroupInfo : topology.getHostGroupInfo().values()) {	currentHostGroupInfo.setRequestedCount(currentHostGroupInfo.getHostNames().size());	}	
topologymanager removependinghostrequests exit 

public void onHostRegistered(HostImpl host, boolean associatedWithCluster) {	ensureInitialized();	
topologymanager onhostregistered entering 

case DECLINED_PREDICATE: LOG.info("TopologyManager.onHostRegistered: DECLINED_PREDICATE received for host = {}", hostName);	break;	}	}	}	}	if (!matchedToRequest) {	boolean addToAvailableList = true;	for (HostImpl registered : availableHosts) {	if (registered.getHostId() == host.getHostId()) {	
host re registered will not be added to the available hosts list 

}	if (!matchedToRequest) {	boolean addToAvailableList = true;	for (HostImpl registered : availableHosts) {	if (registered.getHostId() == host.getHostId()) {	addToAvailableList = false;	break;	}	}	if (addToAvailableList) {	
topologymanager queueing available host 

public void onHostHeartBeatLost(Host host) {	if (AmbariServer.getController() == null) {	return;	}	ensureInitialized();	synchronized (availableHosts) {	
hearbeat for host lost thus removing it from available hosts 

public void processHostRemovedEvent(HostsRemovedEvent hostsRemovedEvent) {	if (hostsRemovedEvent.getHostNames().isEmpty()) {	
missing host name from host removed event 

public void processHostRemovedEvent(HostsRemovedEvent hostsRemovedEvent) {	if (hostsRemovedEvent.getHostNames().isEmpty()) {	return;	}	
removing hosts from available hosts on hosts removed event 

for (String hostName : hostsRemovedEvent.getHostNames()) {	if (hostName.equals(hostImpl.getHostName())) {	toBeRemoved.add(hostImpl);	break;	}	}	}	if (!toBeRemoved.isEmpty()) {	for (HostImpl host : toBeRemoved) {	availableHosts.remove(host);	
removed host from available hosts 

toBeRemoved.add(hostImpl);	break;	}	}	}	if (!toBeRemoved.isEmpty()) {	for (HostImpl host : toBeRemoved) {	availableHosts.remove(host);	}	} else {	
no any host found in available hosts 

========================= ambari sample_2655 =========================

public Collection<String> getHostAssignmentsForComponent(String component) {	Collection<String> hosts = new ArrayList<>();	Collection<String> hostGroups = getHostGroupsForComponent(component);	for (String group : hostGroups) {	HostGroupInfo hostGroupInfo = getHostGroupInfo().get(group);	if (hostGroupInfo != null) {	hosts.addAll(hostGroupInfo.getHostNames());	} else {	
hostgroup not found when checking for hosts for component 

========================= ambari sample_2710 =========================

if (null == definition || !definition.getEnabled()) {	continue;	}	List<Alert> alerts = execute(cluster, definition);	for (Alert alert : alerts) {	AlertReceivedEvent event = new AlertReceivedEvent(cluster.getClusterId(), alert);	m_alertEventPublisher.publish(event);	}	}	} catch (Exception exception) {	
unable to run the alert 

========================= ambari sample_3802 =========================

public void onUpdate(ViewInstanceDefinition definition) {	
settings updated for instance 

========================= ambari sample_775 =========================

int page = 1;	while (saved < json.length()) {	int end = Math.min(saved + VALUE_LENGTH_LIMIT, json.length());	String substring = json.substring(saved, end);	getConfig().setProperty(modelPropName + "#" + page, substring);	saved += VALUE_LENGTH_LIMIT;	page += 1;	LOG.debug("Chunk saved: " + modelPropName + "#" + page + "=" + substring);	}	getConfig().setProperty(modelPropName, page - 1);	
write finished pages 

protected String read(String modelPropName) {	StringBuilder result = new StringBuilder();	int pages = getConfig().getInt(modelPropName);	
read started pages 

protected void clear(String modelPropName) {	int pages = getConfig().getInt(modelPropName);	
clean started pages 

protected void clear(String modelPropName) {	int pages = getConfig().getInt(modelPropName);	for(int page = 1; page <= pages; page++) {	getConfig().clearProperty(modelPropName + "#" + page);	
chunk clean 

========================= ambari sample_1242 =========================

public void perform(PrerequisiteCheck prerequisiteCheck, PrereqCheckRequest request) throws AmbariException {	String isRangerHTTPEnabled = getProperty(request, "ranger-admin-site", "ranger.service.http.enabled");	String isRangerSSLEnabled = getProperty(request, "ranger-admin-site", "ranger.service.https.attrib.ssl.enabled");	String rangerSSLKeystoreFile = getProperty(request, "ranger-admin-site", "ranger.https.attrib.keystore.file");	if (("false").equalsIgnoreCase(isRangerHTTPEnabled) && ("true").equalsIgnoreCase(isRangerSSLEnabled) && rangerSSLKeystoreFile.contains("/etc/ranger/admin/conf") ) {	
ranger is ssl enabled need to show configuration changes warning before upragade proceeds 

public void perform(PrerequisiteCheck prerequisiteCheck, PrereqCheckRequest request) throws AmbariException {	String isRangerHTTPEnabled = getProperty(request, "ranger-admin-site", "ranger.service.http.enabled");	String isRangerSSLEnabled = getProperty(request, "ranger-admin-site", "ranger.service.https.attrib.ssl.enabled");	String rangerSSLKeystoreFile = getProperty(request, "ranger-admin-site", "ranger.https.attrib.keystore.file");	if (("false").equalsIgnoreCase(isRangerHTTPEnabled) && ("true").equalsIgnoreCase(isRangerSSLEnabled) && rangerSSLKeystoreFile.contains("/etc/ranger/admin/conf") ) {	prerequisiteCheck.getFailedOn().add(serviceName);	prerequisiteCheck.setStatus(PrereqCheckStatus.WARNING);	prerequisiteCheck.setFailReason(getFailReason(prerequisiteCheck, request));	} else {	
ranger is not ssl enabled no need to show configuration changes warning before upragade proceeds 

========================= ambari sample_2878 =========================

public void prepare(Map conf) {	
preparing storm metrics reporter 

public void prepare(Map conf) {	try {	try {	hostname = InetAddress.getLocalHost().getHostName();	if ((hostname == null) || (!hostname.contains("."))) {	hostname = InetAddress.getLocalHost().getCanonicalHostName();	}	} catch (UnknownHostException e) {	
could not identify hostname 

hostInMemoryAggregationEnabled = Boolean.valueOf(cf.get(HOST_IN_MEMORY_AGGREGATION_ENABLED_PROPERTY).toString());	hostInMemoryAggregationPort = Integer.valueOf(cf.get(HOST_IN_MEMORY_AGGREGATION_PORT_PROPERTY).toString());	collectorUri = constructTimelineMetricUri(protocol, findPreferredCollectHost(), port);	if (protocol.contains("https")) {	String trustStorePath = cf.get(SSL_KEYSTORE_PATH_PROPERTY).toString().trim();	String trustStoreType = cf.get(SSL_KEYSTORE_TYPE_PROPERTY).toString().trim();	String trustStorePwd = cf.get(SSL_KEYSTORE_PASSWORD_PROPERTY).toString().trim();	loadTruststore(trustStorePath, trustStoreType, trustStorePwd);	}	} catch (Exception e) {	
could not initialize metrics collector please specify protocol host port under storm home conf config yaml 

totalExecutors += topo.get_num_executors();	totalTasks += topo.get_num_tasks();	}	totalMetrics.add(createTimelineMetric(currentTimeMillis, applicationId, "Total Executors", String.valueOf(totalExecutors)));	totalMetrics.add(createTimelineMetric(currentTimeMillis, applicationId, "Total Tasks", String.valueOf(totalTasks)));	TimelineMetrics timelineMetrics = new TimelineMetrics();	timelineMetrics.setMetrics(totalMetrics);	try {	emitMetrics(timelineMetrics);	} catch (UnableToConnectException e) {	
unable to connect to metrics collector 

========================= ambari sample_260 =========================

public boolean execute(HookContext hookContext) {	
executing user hook for 

public boolean execute(HookContext hookContext) {	PostUserCreationHookContext hookCtx = validateHookInput(hookContext);	if (!checkUserHookPrerequisites()) {	
prerequisites for user hook are not satisfied hook not triggered 

public boolean execute(HookContext hookContext) {	PostUserCreationHookContext hookCtx = validateHookInput(hookContext);	if (!checkUserHookPrerequisites()) {	return false;	}	if (hookCtx.getUserGroups().isEmpty()) {	
no users found for executing user hook for 

public boolean execute(HookContext hookContext) {	PostUserCreationHookContext hookCtx = validateHookInput(hookContext);	if (!checkUserHookPrerequisites()) {	return false;	}	if (hookCtx.getUserGroups().isEmpty()) {	return false;	}	UserCreatedEvent userCreatedEvent = (UserCreatedEvent) eventFactory.newUserCreatedEvent(hookCtx);	
triggering user hook for user 

public void onUserCreatedEvent(UserCreatedEvent event) throws AmbariException {	
preparing hook execution for event 

String stageContextText = String.format(POST_USER_CREATION_REQUEST_CONTEXT, ctx.getUserGroups().size());	Stage stage = stageFactory.createNew(requestStageContainer.getId(), configuration.getServerTempDir() + File.pathSeparatorChar + requestStageContainer.getId(), clsData.getClusterName(), clsData.getClusterId(), stageContextText, "{}", "{}");	stage.setStageId(requestStageContainer.getLastStageId());	ServiceComponentHostServerActionEvent serverActionEvent = new ServiceComponentHostServerActionEvent("ambari-server-host", System.currentTimeMillis());	Map<String, String> commandParams = prepareCommandParams(ctx, clsData);	stage.addServerActionCommand(PostUserCreationHookServerAction.class.getName(), "ambari", Role.AMBARI_SERVER_ACTION, RoleCommand.EXECUTE, clsData.getClusterName(), serverActionEvent, commandParams, stageContextText, null, null, false, false);	requestStageContainer.setClusterHostInfo("{}");	requestStageContainer.addStages(Collections.singletonList(stage));	requestStageContainer.persist();	} catch (IOException e) {	
failed to assemble stage for server action event 

private String generateInputFileName() {	String inputFileName = String.format(INPUT_FILE_PREFIX, Calendar.getInstance().getTimeInMillis());	
command input file name 

private boolean checkUserHookPrerequisites() {	if (!configuration.isUserHookEnabled()) {	
post user creation hook disabled 

private boolean checkUserHookPrerequisites() {	if (!configuration.isUserHookEnabled()) {	return false;	}	if (clusters.getClusters().isEmpty()) {	
there s no cluster found post user creation hook won t be executed 

private ClusterData getClusterData() {	String keyTab = "NA";	String principal = "NA";	Map.Entry<String, Cluster> clustersMapEntry = clusters.getClusters().entrySet().iterator().next();	Cluster cluster = clustersMapEntry.getValue();	switch (cluster.getSecurityType()) {	case KERBEROS: Map<String, String> hadoopEnv = cluster.getDesiredConfigByType(HADOOP_ENV).getProperties();	keyTab = hadoopEnv.get(HDFS_USER_KEYTAB);	principal = hadoopEnv.get(HDFS_PRINCIPAL_NAME);	break;	
the cluster security is not set security type 

========================= ambari sample_3910 =========================

public void start() {	
starting scheduler thread 

public void sendActions(Request request, ExecuteActionRequest executeActionRequest) throws AmbariException {	if (LOG.isDebugEnabled()) {	
persisting request into db 

public void sendActions(Request request, ExecuteActionRequest executeActionRequest) throws AmbariException {	if (LOG.isDebugEnabled()) {	if (executeActionRequest != null) {	
in response to request 

}	Collections.sort(reports, new Comparator<CommandReport>() {	public int compare(CommandReport o1, CommandReport o2) {	return (int) (o1.getTaskId()-o2.getTaskId());	}	});	List<CommandReport> reportsToProcess = new ArrayList<>();	for (CommandReport report : reports) {	HostRoleCommand command = commands.get(report.getTaskId());	if (LOG.isDebugEnabled()) {	
processing command report 

public int compare(CommandReport o1, CommandReport o2) {	return (int) (o1.getTaskId()-o2.getTaskId());	}	});	List<CommandReport> reportsToProcess = new ArrayList<>();	for (CommandReport report : reports) {	HostRoleCommand command = commands.get(report.getTaskId());	if (LOG.isDebugEnabled()) {	}	if (command == null) {	
the task is invalid 

});	List<CommandReport> reportsToProcess = new ArrayList<>();	for (CommandReport report : reports) {	HostRoleCommand command = commands.get(report.getTaskId());	if (LOG.isDebugEnabled()) {	}	if (command == null) {	continue;	}	if (! command.getStatus().equals(HostRoleStatus.IN_PROGRESS) && ! command.getStatus().equals(HostRoleStatus.QUEUED) && ! command.getStatus().equals(HostRoleStatus.ABORTED)) {	
the task is not in progress ignoring update 

public boolean isInProgressCommand(CommandReport report) {	HostRoleCommand command = db.getTask(report.getTaskId());	if (command == null) {	
the task is invalid 

========================= ambari sample_2747 =========================

}	if (StringUtils.isEmpty(configGroup.getServiceName())) {	if (!AuthorizationHelper.isAuthorized(ResourceType.CLUSTER, cluster.getResourceId(), RoleAuthorization.CLUSTER_MANAGE_CONFIG_GROUPS)) {	throw new AuthorizationException("The authenticated user is not authorized to delete config groups");	}	} else {	if (!AuthorizationHelper.isAuthorized(ResourceType.CLUSTER, cluster.getResourceId(), RoleAuthorization.SERVICE_MANAGE_CONFIG_GROUPS)) {	throw new AuthorizationException("The authenticated user is not authorized to delete config groups");	}	}	
configchange deleting configuration group cluster changed by config group config group id 

private synchronized void updateConfigGroups (Set<ConfigGroupRequest> requests) throws AmbariException, AuthorizationException {	if (requests.isEmpty()) {	
received an empty requests set 

throw new AuthorizationException("The authenticated user is not authorized to update config groups");	}	}	if (serviceName != null && requestServiceName != null && !StringUtils.equals(serviceName, requestServiceName)) {	throw new IllegalArgumentException("Config group " + configGroup.getId() + " is mapped to service " + serviceName + ", " + "but request contain configs from service " + requestServiceName);	} else if (serviceName == null && requestServiceName != null) {	configGroup.setServiceName(requestServiceName);	serviceName = requestServiceName;	}	int numHosts = (null != configGroup.getHosts()) ? configGroup.getHosts().size() : 0;	
configchange updating configuration group host membership or config value cluster changed by service name config group tag num hosts in config group note 

throw new IllegalArgumentException("Config group " + configGroup.getId() + " is mapped to service " + serviceName + ", " + "but request contain configs from service " + requestServiceName);	} else if (serviceName == null && requestServiceName != null) {	configGroup.setServiceName(requestServiceName);	serviceName = requestServiceName;	}	int numHosts = (null != configGroup.getHosts()) ? configGroup.getHosts().size() : 0;	if (!request.getConfigs().isEmpty()) {	List<String> affectedConfigTypeList = new ArrayList(request.getConfigs().keySet());	Collections.sort(affectedConfigTypeList);	String affectedConfigTypesString = "(" + StringUtils.join(affectedConfigTypeList, ", ") + ")";	
configchange affected configs 

}	int numHosts = (null != configGroup.getHosts()) ? configGroup.getHosts().size() : 0;	if (!request.getConfigs().isEmpty()) {	List<String> affectedConfigTypeList = new ArrayList(request.getConfigs().keySet());	Collections.sort(affectedConfigTypeList);	String affectedConfigTypesString = "(" + StringUtils.join(affectedConfigTypeList, ", ") + ")";	for (Config config : request.getConfigs().values()) {	List<String> sortedConfigKeys = new ArrayList(config.getProperties().keySet());	Collections.sort(sortedConfigKeys);	String sortedConfigKeysString = StringUtils.join(sortedConfigKeys, ", ");	
configchange config type was modified with the following keys 

ConfigGroupResponse configGroupResponse = new ConfigGroupResponse(configGroup.getId(), cluster.getClusterName(), configGroup.getName(), request.getTag(), "", new HashSet<>(), new HashSet<>());	Set<Map<String, Object>> versionTags = new HashSet<>();	Map<String, Object> tagsMap = new HashMap<>();	for (Config config : configGroup.getConfigurations().values()) {	tagsMap.put(config.getType(), config.getTag());	}	versionTags.add(tagsMap);	configGroupResponse.setVersionTags(versionTags);	getManagementController().saveConfigGroupUpdate(request, configGroupResponse);	} else {	
could not determine service name for config group service config version not created 

========================= ambari sample_3506 =========================

public  <T extends Indexed> T load(Class<T> model, int id) throws ItemNotFound {	
loading s d 

public synchronized <T extends Indexed> List<T> loadAll(Class<T> model, FilteringStrategy filter) {	LinkedList<T> list = new LinkedList<T>();	
loading all s s 

public synchronized void delete(Class model, int id) throws ItemNotFound {	
deleting s d 

========================= ambari sample_1254 =========================

private Optional<Result> getRowsFromDB(ConnectionConfig config, String[] statements) {	Connect connect = config.createConnectMessage();	HiveJob job = new SQLStatementJob(HiveJob.Type.SYNC, statements, config.getUsername());	ExecuteJob execute = new ExecuteJob(connect, job);	
executing query for user 

private Optional<Result> getTableDescription(ConnectionConfig config, String databasePattern, String tablePattern, String columnPattern) {	Connect connect = config.createConnectMessage();	HiveJob job = new GetColumnMetadataJob(config.getUsername(), databasePattern, tablePattern, columnPattern);	ExecuteJob execute = new ExecuteJob(connect, job);	
executing query to fetch the column description for dbpattern tablepattern columnpattern for user 

public DatabaseMetadataWrapper getDatabaseMetadata(ConnectionConfig config) throws ServiceException {	Connect connect = config.createConnectMessage();	HiveJob job = new GetDatabaseMetadataJob(config.getUsername());	ExecuteJob execute = new ExecuteJob(connect, job);	
fetching databasemetadata 

inbox.send(controller, job);	Object submitResult;	try {	submitResult = inbox.receive(Duration.create(actorConfiguration.getSyncQueryTimeout(60 * 1000), TimeUnit.MILLISECONDS));	} catch (Throwable ex) {	String errorMessage = "Query timed out to fetch table description for user: " + job.getConnect().getUsername();	LOG.error(errorMessage, ex);	throw new ServiceFormattedException(errorMessage, ex);	}	if (submitResult instanceof NoResult) {	
query returned with no result 

submitResult = inbox.receive(Duration.create(actorConfiguration.getSyncQueryTimeout(60 * 1000), TimeUnit.MILLISECONDS));	} catch (Throwable ex) {	String errorMessage = "Query timed out to fetch table description for user: " + job.getConnect().getUsername();	LOG.error(errorMessage, ex);	throw new ServiceFormattedException(errorMessage, ex);	}	if (submitResult instanceof NoResult) {	return Optional.absent();	}	if (submitResult instanceof DatabaseMetadataWrapper) {	
query returned with no result 

throw new ServiceFormattedException(errorMessage, ex);	}	if (submitResult instanceof NoResult) {	return Optional.absent();	}	if (submitResult instanceof DatabaseMetadataWrapper) {	return Optional.of(new Result((DatabaseMetadataWrapper)submitResult));	}	if (submitResult instanceof ExecutionFailed) {	ExecutionFailed error = (ExecutionFailed) submitResult;	
failed to get the table description 

return Optional.absent();	}	if (submitResult instanceof DatabaseMetadataWrapper) {	return Optional.of(new Result((DatabaseMetadataWrapper)submitResult));	}	if (submitResult instanceof ExecutionFailed) {	ExecutionFailed error = (ExecutionFailed) submitResult;	throw new ServiceFormattedException(error.getMessage(), error.getError());	} else if (submitResult instanceof AuthenticationFailed) {	AuthenticationFailed exception = (AuthenticationFailed) submitResult;	
failed to connect to hive 

if (descriptions == null) {	descriptions = result.getColumns();	}	rows.addAll(result.getRows());	}	if (receive instanceof NoMoreItems) {	break;	}	if (receive instanceof FetchFailed) {	FetchFailed error = (FetchFailed) receive;	
failed to fetch results 

========================= ambari sample_763 =========================

public String pushMetrics(String payload) {	String responseString = "";	UrlService svc = null;	Stopwatch timer = new Stopwatch().start();	try {	
server 

public String pushMetrics(String payload) {	String responseString = "";	UrlService svc = null;	Stopwatch timer = new Stopwatch().start();	try {	svc = getConnectedUrlService();	responseString = svc.send(payload);	timer.stop();	
http response time ms 

public String pushMetrics(String payload) {	String responseString = "";	UrlService svc = null;	Stopwatch timer = new Stopwatch().start();	try {	svc = getConnectedUrlService();	responseString = svc.send(payload);	timer.stop();	if (responseString.length() > 0) {	
post response from server 

========================= ambari sample_333 =========================

requestBodySet.add(requestBody);	} else {	if (!propertySet.getProperties().isEmpty()) {	rootBody.addPropertySet(propertySet);	}	requestBodySet.add(rootBody);	}	}	} catch (IOException e) {	if (LOG.isDebugEnabled()) {	
caught exception parsing msg body 

requestBodySet.add(requestBody);	} else {	if (!propertySet.getProperties().isEmpty()) {	rootBody.addPropertySet(propertySet);	}	requestBodySet.add(rootBody);	}	}	} catch (IOException e) {	if (LOG.isDebugEnabled()) {	
message body 

========================= ambari sample_4113 =========================

public static QuickLinkVisibilityController get(@Nullable String quickLinkProfileJson) {	if (null == quickLinkProfileJson) {	
no quick link profile is set will display all quicklinks 

public static QuickLinkVisibilityController get(@Nullable String quickLinkProfileJson) {	if (null == quickLinkProfileJson) {	return new ShowAllLinksVisibilityController();	}	try {	QuickLinksProfile profile = new QuickLinksProfileParser().parse(quickLinkProfileJson.getBytes());	return new DefaultQuickLinkVisibilityController(profile);	}	catch (IOException | QuickLinksProfileEvaluationException ex) {	
unable to parse quick link profile json 

========================= ambari sample_3005 =========================

if (null == executionCommand.getConfigurations()) {	executionCommand.setConfigurations(new TreeMap<>());	}	Map<String, Map<String, String>> configurations = executionCommand.getConfigurations();	Long clusterId = hostRoleCommandDAO.findByPK( executionCommand.getTaskId()).getStage().getClusterId();	Cluster cluster = clusters.getClusterById(clusterId);	boolean refreshConfigTagsBeforeExecution = executionCommand.getForceRefreshConfigTagsBeforeExecution();	if (refreshConfigTagsBeforeExecution) {	Map<String, DesiredConfig> desiredConfigs = cluster.getDesiredConfigs();	Map<String, Map<String, String>> configurationTags = configHelper.getEffectiveDesiredTags( cluster, executionCommand.getHostname(), desiredConfigs);	
while scheduling task on cluster configurations are being refreshed using desired configurations of 

RepositoryVersionEntity repoVersion = service.getDesiredRepositoryVersion();	OperatingSystemEntity osEntity = repoVersionHelper.getOSEntityForHost(host, repoVersion);	commandRepository = repoVersionHelper.getCommandRepository(repoVersion, osEntity);	}	executionCommand.setRepositoryFile(commandRepository);	} catch (SystemException e) {	throw new RuntimeException(e);	}	}	} catch (ClusterNotFoundException cnfe) {	
unable to lookup the cluster by id assuming that there is no cluster and therefore no configs for this execution command 

}	if (!commandParams.containsKey(SERVICE_PACKAGE_FOLDER)) {	if (!StringUtils.isEmpty(serviceName)) {	ServiceInfo serviceInfo = ambariMetaInfo.getService(stackId.getStackName(), stackId.getStackVersion(), serviceName);	commandParams.put(SERVICE_PACKAGE_FOLDER, serviceInfo.getServicePackageFolder());	}	}	}	executionCommand.setComponentVersions(cluster);	} catch (ServiceNotFoundException serviceNotFoundException) {	
the service is not installed in the cluster no repository version will be sent for this command 

========================= ambari sample_2751 =========================

public void run() {	
persisting metric metadata 

public void run() {	persistMetricMetadata();	
persisting hosted apps metadata 

public void run() {	persistMetricMetadata();	persistHostAppsMetadata();	
persisting hosted instance metadata 

public void run() {	persistMetricMetadata();	persistHostAppsMetadata();	persistHostInstancesMetadata();	if (cacheManager.isDistributedModeEnabled()) {	
refreshing metric metadata 

public void run() {	persistMetricMetadata();	persistHostAppsMetadata();	persistHostInstancesMetadata();	if (cacheManager.isDistributedModeEnabled()) {	refreshMetricMetadata();	
refreshing hosted apps metadata 

public void run() {	persistMetricMetadata();	persistHostAppsMetadata();	persistHostInstancesMetadata();	if (cacheManager.isDistributedModeEnabled()) {	refreshMetricMetadata();	refreshHostAppsMetadata();	
refreshing hosted instances metadata 

if (!metadata.isPersisted()) {	metadataToPersist.add(metadata);	}	}	boolean markSuccess = false;	if (!metadataToPersist.isEmpty()) {	try {	cacheManager.persistMetadata(metadataToPersist);	markSuccess = true;	} catch (SQLException e) {	
error persisting metadata 

private void refreshMetricMetadata() {	Map<TimelineMetricMetadataKey, TimelineMetricMetadata> metadataFromStore = null;	try {	metadataFromStore = cacheManager.getMetadataFromStore();	} catch (SQLException e) {	
error refreshing metadata from store 

private void persistHostAppsMetadata() {	if (cacheManager.syncHostedAppsMetadata()) {	Map<String, Set<String>> persistedData = null;	try {	persistedData = cacheManager.getHostedAppsFromStore();	} catch (SQLException e) {	
failed on fetching hosted apps data from store 

if (cachedData != null && !cachedData.isEmpty()) {	for (Map.Entry<String, Set<String>> cacheEntry : cachedData.entrySet()) {	if (persistedData == null || persistedData.isEmpty() || !persistedData.containsKey(cacheEntry.getKey()) || !persistedData.get(cacheEntry.getKey()).containsAll(cacheEntry.getValue())) {	dataToSync.put(cacheEntry.getKey(), cacheEntry.getValue());	}	}	try {	cacheManager.persistHostedAppsMetadata(dataToSync);	cacheManager.markSuccessOnSyncHostedAppsMetadata();	} catch (SQLException e) {	
error persisting hosted apps metadata 

private void persistHostInstancesMetadata() {	if (cacheManager.syncHostedInstanceMetadata()) {	Map<String, Set<String>> persistedData = null;	try {	persistedData = cacheManager.getHostedInstancesFromStore();	} catch (SQLException e) {	
failed on fetching hosted instances data from store 

if (cachedData != null && !cachedData.isEmpty()) {	for (Map.Entry<String, Set<String>> cacheEntry : cachedData.entrySet()) {	if (persistedData == null || persistedData.isEmpty() || !persistedData.containsKey(cacheEntry.getKey()) || !persistedData.get(cacheEntry.getKey()).containsAll(cacheEntry.getValue())) {	dataToSync.put(cacheEntry.getKey(), cacheEntry.getValue());	}	}	try {	cacheManager.persistHostedInstanceMetadata(dataToSync);	cacheManager.markSuccessOnSyncHostedInstanceMetadata();	} catch (SQLException e) {	
error persisting hosted apps metadata 

private void refreshHostAppsMetadata() {	Map<String, Set<String>> hostedAppsDataFromStore = null;	try {	hostedAppsDataFromStore = cacheManager.getHostedAppsFromStore();	} catch (SQLException e) {	
error refreshing metadata from store 

private void refreshHostedInstancesMetadata() {	Map<String, Set<String>> hostedInstancesFromStore = null;	try {	hostedInstancesFromStore = cacheManager.getHostedInstancesFromStore();	} catch (SQLException e) {	
error refreshing metadata from store 

========================= ambari sample_364 =========================

String logMessage = (String) lineMap.get(input.getCacheKeyField());	Long timestamp = null;	if (lineMap.containsKey((LogFeederConstants.IN_MEMORY_TIMESTAMP))) {	timestamp = (Long) lineMap.get(LogFeederConstants.IN_MEMORY_TIMESTAMP);	}	if (logMessage != null && timestamp != null) {	isLogFilteredOut = !inputLruCache.isEntryReplaceable(logMessage, timestamp);	if (!isLogFilteredOut) {	inputLruCache.put(logMessage, timestamp);	} else {	
log line filtered out file dedupinterval lastdedupenabled 

========================= ambari sample_1653 =========================

}	} catch (ObjectNotFoundException e) {	}	}	if (hBaseMasterActiveCount > 0) {	return State.STARTED;	}	return nonStartedState == null ? State.INSTALLED : nonStartedState;	}	} catch (AmbariException e) {	
can t determine service state 

========================= ambari sample_3681 =========================

public void testInputFile_process3Rows() throws Exception {	
testinputfile 

========================= ambari sample_1595 =========================

public AmbariPerformanceMonitor() {	super();	
ambariperformancemonitor instantiated 

private void init() {	if (metricsSource == null) {	metricsSource = (DatabaseMetricsSource) MetricsServiceImpl.getSource("database");	}	if (metricsSource != null) {	
ambariperformancemonitor initialized 

if (metricsSource == null) {	metricsSource = (DatabaseMetricsSource) MetricsServiceImpl.getSource("database");	}	if (metricsSource != null) {	long interval = Long.parseLong(metricsSource.getConfigurationValue("dumptime", "60000"));	this.setDumpTime(interval);	String profileWeight = metricsSource.getConfigurationValue("query.weight", "HEAVY");	this.setProfileWeight(getWeight(profileWeight));	isInitialized = true;	} else {	
ambariperformancemonitor not yet initialized 

}	}	metrics.put(StringUtils.join(list, "."), (Long)value);	}	}	if (!metrics.isEmpty()) {	if (!isInitialized) {	init();	}	if (isInitialized) {	
publishing metrics to sink 

========================= ambari sample_2643 =========================

public <T extends Indexed> T load(Class<T> model, Object id) throws ItemNotFound {	String modelPropName = getItemPropertyName(model, id);	
loading s 

public <T extends Indexed> T load(Class<T> model, Object id) throws ItemNotFound {	String modelPropName = getItemPropertyName(model, id);	if (getConfig().containsKey(modelPropName)) {	String json = read(modelPropName);	
json s 

public synchronized <T extends Indexed> List<T> loadAll(Class<? extends T> model, FilteringStrategy filter) {	ArrayList<T> list = new ArrayList<T>();	String modelIndexingPropName = getIndexPropertyName(model);	
loading all s s 

public synchronized void delete(Class model, Object id) {	
deleting s s 

========================= ambari sample_695 =========================

private void loadConfigFiles() throws Exception {	List<String> configFiles = getConfigFiles();	for (String configFileName : configFiles) {	
going to load config file 

private void loadConfigFiles() throws Exception {	List<String> configFiles = getConfigFiles();	for (String configFileName : configFiles) {	configFileName = configFileName.replace("\\ ", "%20");	File configFile = new File(configFileName);	if (configFile.exists() && configFile.isFile()) {	
config file exists in path 

private void loadConfigFiles() throws Exception {	List<String> configFiles = getConfigFiles();	for (String configFileName : configFiles) {	configFileName = configFileName.replace("\\ ", "%20");	File configFile = new File(configFileName);	if (configFile.exists() && configFile.isFile()) {	loadConfigsUsingFile(configFile);	} else {	
trying to load config file from classloader 

private void loadConfigFiles() throws Exception {	List<String> configFiles = getConfigFiles();	for (String configFileName : configFiles) {	configFileName = configFileName.replace("\\ ", "%20");	File configFile = new File(configFileName);	if (configFile.exists() && configFile.isFile()) {	loadConfigsUsingFile(configFile);	} else {	loadConfigsUsingClassLoader(configFileName);	
loaded config file from classloader 

Type type = new TypeToken<Map<String, Object>>() {}.getType();	Map<String, Object> configMap = LogFeederUtil.getGson().fromJson(configData, type);	for (String key : configMap.keySet()) {	switch (key) {	case "global" : globalConfigs.putAll((Map<String, Object>) configMap.get(key));	globalConfigJsons.add(configData);	break;	case "output" : List<Map<String, Object>> outputConfig = (List<Map<String, Object>>) configMap.get(key);	outputConfigList.addAll(outputConfig);	break;	
unknown config key 

private void loadOutputs() {	for (Map<String, Object> map : outputConfigList) {	if (map == null) {	continue;	}	mergeBlocks(globalConfigs, map);	String value = (String) map.get("destination");	if (StringUtils.isEmpty(value)) {	
output block doesn t have destination element 

if (map == null) {	continue;	}	mergeBlocks(globalConfigs, map);	String value = (String) map.get("destination");	if (StringUtils.isEmpty(value)) {	continue;	}	Output output = (Output) AliasUtil.getClassInstance(value, AliasUtil.AliasType.OUTPUT);	if (output == null) {	
output object could not be found 

if (output == null) {	continue;	}	output.setDestination(value);	output.loadConfig(map);	output.setLogSearchConfig(logSearchConfig);	if (output.isEnabled()) {	output.logConfigs();	outputManager.add(output);	} else {	
output is disabled so ignoring it 

private void loadInputs(String serviceName) {	for (InputDescriptor inputDescriptor : inputConfigList) {	if (inputDescriptor == null) {	continue;	}	String source = (String) inputDescriptor.getSource();	if (StringUtils.isEmpty(source)) {	
input block doesn t have source element 

for (InputDescriptor inputDescriptor : inputConfigList) {	if (inputDescriptor == null) {	continue;	}	String source = (String) inputDescriptor.getSource();	if (StringUtils.isEmpty(source)) {	continue;	}	Input input = (Input) AliasUtil.getClassInstance(source, AliasUtil.AliasType.INPUT);	if (input == null) {	
input object could not be found 

continue;	}	input.setType(source);	input.loadConfig(inputDescriptor);	if (input.isEnabled()) {	input.setOutputManager(outputManager);	input.setInputManager(inputManager);	inputManager.add(serviceName, input);	input.logConfigs();	} else {	
input is disabled so ignoring it 

private void loadFilters(String serviceName) {	sortFilters();	List<Input> toRemoveInputList = new ArrayList<Input>();	for (Input input : inputManager.getInputList(serviceName)) {	for (FilterDescriptor filterDescriptor : filterConfigList) {	if (filterDescriptor == null) {	continue;	}	if (BooleanUtils.isFalse(filterDescriptor.isEnabled())) {	
ignoring filter because it is disabled 

List<Input> toRemoveInputList = new ArrayList<Input>();	for (Input input : inputManager.getInputList(serviceName)) {	for (FilterDescriptor filterDescriptor : filterConfigList) {	if (filterDescriptor == null) {	continue;	}	if (BooleanUtils.isFalse(filterDescriptor.isEnabled())) {	continue;	}	if (!input.isFilterRequired(filterDescriptor)) {	
ignoring filter for input 

continue;	}	if (BooleanUtils.isFalse(filterDescriptor.isEnabled())) {	continue;	}	if (!input.isFilterRequired(filterDescriptor)) {	continue;	}	String value = filterDescriptor.getFilter();	if (StringUtils.isEmpty(value)) {	
filter block doesn t have filter element 

}	if (!input.isFilterRequired(filterDescriptor)) {	continue;	}	String value = filterDescriptor.getFilter();	if (StringUtils.isEmpty(value)) {	continue;	}	Filter filter = (Filter) AliasUtil.getClassInstance(value, AliasUtil.AliasType.FILTER);	if (filter == null) {	
filter object could not be found 

filter.setInput(input);	filter.setOutputManager(outputManager);	input.addFilter(filter);	filter.logConfigs();	}	if (input.getFirstFilter() == null) {	toRemoveInputList.add(input);	}	}	for (Input toRemoveInput : toRemoveInputList) {	
there are no filters we will ignore this input 

========================= ambari sample_1658 =========================

String propertyId = PropertyHelper.getPropertyId(metaData.getTableName(i), metaData.getColumnName(i));	if (propertyIds.contains(propertyId)) {	resource.setProperty(propertyId, rs.getString(i));	}	}	resources.add(resource);	}	statement.close();	} catch (SQLException e) {	if (LOG.isDebugEnabled()) {	
caught exception getting resource 

}	statement.close();	} catch (SQLException e) {	if (LOG.isDebugEnabled()) {	}	return Collections.emptySet();	} finally {	try {	if (rs != null) rs.close();	} catch (SQLException e) {	
exception while closing resultset 

}	return Collections.emptySet();	} finally {	try {	if (rs != null) rs.close();	} catch (SQLException e) {	}	try {	if (statement != null) statement.close();	} catch (SQLException e) {	
exception while closing statment 

if (rs != null) rs.close();	} catch (SQLException e) {	}	try {	if (statement != null) statement.close();	} catch (SQLException e) {	}	try {	if (connection != null) connection.close();	} catch (SQLException e) {	
exception while closing statment 

========================= ambari sample_3665 =========================

Short timeout = null;	for (String key : timeoutKeys) {	String configValue = configuration.getProperty(key);	if (StringUtils.isNotBlank(configValue)) {	try {	Short configTimeout = Short.valueOf(configValue);	if (null == timeout || configTimeout > timeout) {	timeout = configTimeout;	}	} catch (Exception e) {	
could not parse to a timeout value 

String configValue = configuration.getProperty(key);	if (StringUtils.isNotBlank(configValue)) {	try {	Short configTimeout = Short.valueOf(configValue);	if (null == timeout || configTimeout > timeout) {	timeout = configTimeout;	}	} catch (Exception e) {	}	} else {	
configuration not found to compute timeout 

========================= ambari sample_3235 =========================

public Authentication authenticate(Authentication authentication) throws AuthenticationException {	if (authentication.getName() == null) {	
authentication failed no username provided 

public Authentication authenticate(Authentication authentication) throws AuthenticationException {	if (authentication.getName() == null) {	throw new AmbariAuthenticationException(null, "Unexpected error due to missing username", false);	}	String userName = authentication.getName().trim();	if (authentication.getCredentials() == null) {	
authentication failed no credentials provided 

if (authentication.getName() == null) {	throw new AmbariAuthenticationException(null, "Unexpected error due to missing username", false);	}	String userName = authentication.getName().trim();	if (authentication.getCredentials() == null) {	throw new AmbariAuthenticationException(userName, "Unexpected error due to missing JWT token", false);	}	Users users = getUsers();	UserEntity userEntity = users.getUserEntity(userName);	if (userEntity == null) {	
user not found 

UserAuthenticationEntity authenticationEntity = getAuthenticationEntity(userEntity, UserAuthenticationType.JWT);	if (authenticationEntity != null) {	authOK = true;	} else {	authenticationEntity = getAuthenticationEntity(userEntity, UserAuthenticationType.LDAP);	if (authenticationEntity != null) {	try {	users.addJWTAuthentication(userEntity, userName);	authOK = true;	} catch (AmbariException e) {	
failed to add the jwt authentication method for s s 

if (authenticationEntity != null) {	try {	users.addJWTAuthentication(userEntity, userName);	authOK = true;	} catch (AmbariException e) {	throw new AmbariAuthenticationException(userName, "Unexpected error has occurred", false, e);	}	}	}	if (authOK) {	
authentication succeeded a matching user was found 

throw e;	} else {	throw new AmbariAuthenticationException(userName, "Unexpected error due to missing JWT token", false);	}	}	User user = new User(userEntity);	Authentication auth = new AmbariUserAuthentication(authentication.getCredentials().toString(), user, users.getUserAuthorities(userEntity));	auth.setAuthenticated(true);	return auth;	} else {	
authentication failed password does not match stored value 

========================= ambari sample_2832 =========================

public MetricCollectorHAController(TimelineMetricConfiguration configuration) {	this.configuration = configuration;	String instancePort;	try {	instanceHostname = configuration.getInstanceHostnameFromEnv();	instancePort = configuration.getInstancePort();	} catch (Exception e) {	
error reading configs from classpath will resort to defaults 

throw new MetricsSystemInitializationException(e.getMessage());	}	try {	String zkClientPort = configuration.getClusterZKClientPort();	String zkQuorum = configuration.getClusterZKQuorum();	if (StringUtils.isEmpty(zkClientPort) || StringUtils.isEmpty(zkQuorum)) {	throw new Exception("Unable to parse zookeeper quorum. clientPort = " + zkClientPort +", quorum = " + zkQuorum);	}	zkConnectUrl = getZkConnectionUrl(zkClientPort, zkQuorum);	} catch (Exception e) {	
unable to load hbase site from classpath 

public void initializeHAController() throws Exception {	String clusterName = getClusterName();	admin = new ZKHelixAdmin(zkConnectUrl);	
creating zookeeper cluster node 

public void initializeHAController() throws Exception {	String clusterName = getClusterName();	admin = new ZKHelixAdmin(zkConnectUrl);	boolean clusterAdded = admin.addCluster(clusterName, false);	
was cluster added successfully 

String clusterName = getClusterName();	admin = new ZKHelixAdmin(zkConnectUrl);	boolean clusterAdded = admin.addCluster(clusterName, false);	boolean success = false;	int tries = 5;	int sleepTimeInSeconds = 5;	for (int i = 0; i < tries && !success; i++) {	try {	List<String> nodes = admin.getInstancesInCluster(clusterName);	if (CollectionUtils.isEmpty(nodes) || !nodes.contains(instanceConfig.getInstanceName())) {	
adding participant instance 

int tries = 5;	int sleepTimeInSeconds = 5;	for (int i = 0; i < tries && !success; i++) {	try {	List<String> nodes = admin.getInstancesInCluster(clusterName);	if (CollectionUtils.isEmpty(nodes) || !nodes.contains(instanceConfig.getInstanceName())) {	admin.addInstance(clusterName, instanceConfig);	}	success = true;	} catch (HelixException | ZkNoNodeException ex) {	
helix cluster not yet setup fully 

int sleepTimeInSeconds = 5;	for (int i = 0; i < tries && !success; i++) {	try {	List<String> nodes = admin.getInstancesInCluster(clusterName);	if (CollectionUtils.isEmpty(nodes) || !nodes.contains(instanceConfig.getInstanceName())) {	admin.addInstance(clusterName, instanceConfig);	}	success = true;	} catch (HelixException | ZkNoNodeException ex) {	if (i < tries - 1) {	
waiting for seconds and retrying 

success = true;	} catch (HelixException | ZkNoNodeException ex) {	if (i < tries - 1) {	TimeUnit.SECONDS.sleep(sleepTimeInSeconds);	} else {	LOG.error(ex);	}	}	}	if (!success) {	
trying to create again since waiting for the creation did not help 

TimeUnit.SECONDS.sleep(sleepTimeInSeconds);	} else {	LOG.error(ex);	}	}	}	if (!success) {	admin.addCluster(clusterName, true);	List<String> nodes = admin.getInstancesInCluster(clusterName);	if (CollectionUtils.isEmpty(nodes) || !nodes.contains(instanceConfig.getInstanceName())) {	
adding participant instance 

}	}	if (!success) {	admin.addCluster(clusterName, true);	List<String> nodes = admin.getInstancesInCluster(clusterName);	if (CollectionUtils.isEmpty(nodes) || !nodes.contains(instanceConfig.getInstanceName())) {	admin.addInstance(clusterName, instanceConfig);	}	}	if (admin.getStateModelDef(clusterName, DEFAULT_STATE_MODEL) == null) {	
adding online offline state model to the cluster 

List<String> nodes = admin.getInstancesInCluster(clusterName);	if (CollectionUtils.isEmpty(nodes) || !nodes.contains(instanceConfig.getInstanceName())) {	admin.addInstance(clusterName, instanceConfig);	}	}	if (admin.getStateModelDef(clusterName, DEFAULT_STATE_MODEL) == null) {	admin.addStateModelDef(clusterName, DEFAULT_STATE_MODEL, new StateModelDefinition( StateModelConfigGenerator.generateConfigForOnlineOffline()));	}	List<String> resources = admin.getResourcesInCluster(clusterName);	if (!resources.contains(METRIC_AGGREGATORS)) {	
adding resource with partitions and replicas 

private void startAggregators() {	try {	aggregationTaskRunner.initialize();	} catch (Exception e) {	
unable to start aggregators 

========================= ambari sample_380 =========================

uqs.add(Constraint.uq("<default>", toColumns(unnamedUqMatcher.group(1))));	} else if (colMatcher.matches()) {	String colName = stripPrefixQuotationAndBrackets(colMatcher.group(1));	columns.add(colName);	if (pkColumnMatcher.matches()) {	pk = Optional.of(Constraint.pk("<default>", Collections.singleton(colName)));	} else if (uqColumnMatcher.matches()) {	uqs.add(Constraint.uq("<default>", Collections.singleton(colName)));	}	} else {	
unexpected definition context 

private static void assertNounnamedConstraint(String tableName, String definition) {	if (tableName.contains("qrtz")) {	
skipp checking quartz table 

if (tableName.contains("qrtz")) {	}	else {	for (Pattern unnamedConstraint: CheckedUnnamedConstraints) {	if (unnamedConstraint.matcher(definition).matches()) {	throw new IllegalStateException( String.format("Found invalid (unnamed) constraint in table %s: %s", tableName, definition));	}	}	for (Pattern unnamedConstraint: UncheckedUnnamedConstraints) {	if (unnamedConstraint.matcher(definition).matches()) {	
found unnamed constraint in table 

========================= ambari sample_2561 =========================

public void start() {	final SolrAuditLogPropsConfig solrPropsConfig = (SolrAuditLogPropsConfig) auditSolrDao.getSolrPropsConfig();	final SolrAuditLogsState state = (SolrAuditLogsState) auditSolrDao.getSolrCollectionState();	final Collection<String> collectionListIn = Arrays.asList(solrPropsConfig.getCollection(), solrPropsConfig.getRangerCollection().trim());	if (solrPropsConfig.getAliasNameIn() == null || collectionListIn.size() == 0) {	
will not create alias for 

public void start() {	final SolrAuditLogPropsConfig solrPropsConfig = (SolrAuditLogPropsConfig) auditSolrDao.getSolrPropsConfig();	final SolrAuditLogsState state = (SolrAuditLogsState) auditSolrDao.getSolrCollectionState();	final Collection<String> collectionListIn = Arrays.asList(solrPropsConfig.getCollection(), solrPropsConfig.getRangerCollection().trim());	if (solrPropsConfig.getAliasNameIn() == null || collectionListIn.size() == 0) {	return;	}	
setupalias for 

} else {	LOG.warn("Not able to create alias=" + solrPropsConfig.getAliasNameIn() + ", retryCount=" + retryCount);	}	} catch (Exception e) {	LOG.error("Error setting up alias=" + solrPropsConfig.getAliasNameIn(), e);	}	}	try {	Thread.sleep(ALIAS_SETUP_RETRY_SECOND * 1000);	} catch (InterruptedException sleepInterrupted) {	
sleep interrupted while setting up alias 

private int createAlias(final CloudSolrClient solrClient, String aliasNameIn, Collection<String> collectionListIn) throws SolrServerException, IOException {	List<String> collectionToAdd = new ArrayList<>();	try {	collectionToAdd = new ListCollectionHandler().handle(solrClient, null);	} catch (Exception e) {	
invalid state during getting collections for creating alias 

========================= ambari sample_1434 =========================

public OpenCSVParser(Reader reader, ParseOptions parseOptions) throws IOException {	super(reader, parseOptions);	CSVParserBuilder csvParserBuilder = new CSVParserBuilder();	CSVReaderBuilder builder =  new CSVReaderBuilder(reader);	Character delimiter = (Character) parseOptions.getOption(ParseOptions.OPTIONS_CSV_DELIMITER);	if(delimiter != null){	
setting delimiter as 

public OpenCSVParser(Reader reader, ParseOptions parseOptions) throws IOException {	super(reader, parseOptions);	CSVParserBuilder csvParserBuilder = new CSVParserBuilder();	CSVReaderBuilder builder =  new CSVReaderBuilder(reader);	Character delimiter = (Character) parseOptions.getOption(ParseOptions.OPTIONS_CSV_DELIMITER);	if(delimiter != null){	csvParserBuilder = csvParserBuilder.withSeparator(delimiter);	}	Character quote = (Character) parseOptions.getOption(ParseOptions.OPTIONS_CSV_QUOTE);	if( null != quote ){	
setting quote char 

Character delimiter = (Character) parseOptions.getOption(ParseOptions.OPTIONS_CSV_DELIMITER);	if(delimiter != null){	csvParserBuilder = csvParserBuilder.withSeparator(delimiter);	}	Character quote = (Character) parseOptions.getOption(ParseOptions.OPTIONS_CSV_QUOTE);	if( null != quote ){	csvParserBuilder = csvParserBuilder.withQuoteChar(quote);	}	Character escapeChar = (Character) parseOptions.getOption(ParseOptions.OPTIONS_CSV_ESCAPE_CHAR);	if( null != escapeChar ){	
setting escapechar 

========================= ambari sample_553 =========================

HostInfo hi = new HostInfo();	hi.setOS("Centos5");	reg.setHardwareProfile(hi);	handler.handleRegistration(reg);	HeartBeat hb = new HeartBeat();	hb.setHostname(hostname1);	hb.setNodeStatus(new HostStatus(HostStatus.Status.HEALTHY, "cool"));	hb.setTimestamp(System.currentTimeMillis());	hb.setResponseId(13);	handler.handleHeartBeat(hb);	
YYY 

========================= ambari sample_1988 =========================

private void updateJobStatsAtFinish(String jobId) {	try {	jobEndUpdate.setString(1, "MAP");	jobEndUpdate.setString(2, "REDUCE");	jobEndUpdate.setString(3, jobId);	jobEndUpdate.executeUpdate();	} catch (SQLException sqle) {	
failed to update mapsruntime reducesruntime for 

try {	workflowSelectPS.setString(1, workflowContext.getWorkflowId());	workflowSelectPS.execute();	rs = workflowSelectPS.getResultSet();	if (rs.next()) {	existingContextString = rs.getString(1);	} else {	insertWorkflow = true;	}	} catch (SQLException sqle) {	
workflow select failed with 

existingContextString = rs.getString(1);	} else {	insertWorkflow = true;	}	} catch (SQLException sqle) {	insertWorkflow = false;	} finally {	try {	if (rs != null) rs.close();	} catch (SQLException e) {	
exception while closing resultset 

workflowPS.setLong(6, historyEvent.getSubmitTime());	workflowPS.setLong(7, workflowContext.getWorkflowDag().size());	workflowPS.executeUpdate();	LOG.debug("Successfully inserted workflowId = " + workflowContext.getWorkflowId());	} else {	ObjectMapper om = new ObjectMapper();	WorkflowContext existingWorkflowContext = null;	try {	if (existingContextString != null) existingWorkflowContext = om.readValue(existingContextString.getBytes(), WorkflowContext.class);	} catch (IOException e) {	
couldn t read existing workflow context for 

workflowUpdateTimePS.setLong(4, historyEvent.getSubmitTime());	workflowUpdateTimePS.setString(5, workflowContext.getWorkflowId());	workflowUpdateTimePS.executeUpdate();	LOG.debug("Successfully updated workflowId = " + workflowContext.getWorkflowId());	}	jobPS.setString(7, workflowContext.getWorkflowId());	jobPS.setString(8, workflowContext.getWorkflowEntityName());	jobPS.executeUpdate();	LOG.debug("Successfully inserted job = " + jobId + " and workflowId = " + workflowContext.getWorkflowId());	} catch (SQLException sqle) {	
failed to store for job into 

workflowUpdateTimePS.setString(5, workflowContext.getWorkflowId());	workflowUpdateTimePS.executeUpdate();	LOG.debug("Successfully updated workflowId = " + workflowContext.getWorkflowId());	}	jobPS.setString(7, workflowContext.getWorkflowId());	jobPS.setString(8, workflowContext.getWorkflowEntityName());	jobPS.executeUpdate();	LOG.debug("Successfully inserted job = " + jobId + " and workflowId = " + workflowContext.getWorkflowId());	} catch (SQLException sqle) {	} catch (Exception e) {	
failed to store for job into 

entityPS.setInt(5, historyEvent.getFailedReduces());	entityPS.setLong(6, inputBytes);	entityPS.setLong(7, outputBytes);	entityPS.setString(8, historyEvent.getJobid().toString());	entityPS.executeUpdate();	workflowUpdateNumCompletedPS.setLong(1, historyEvent.getFinishTime());	workflowUpdateNumCompletedPS.setLong(2, historyEvent.getFinishTime());	workflowUpdateNumCompletedPS.setString(3, historyEvent.getJobid().toString());	workflowUpdateNumCompletedPS.executeUpdate();	} catch (SQLException sqle) {	
failed to store for job into 

private void processJobInitedEvent( PreparedStatement entityPS, LoggingEvent logEvent, JobInitedEvent historyEvent) {	try {	entityPS.setLong(1, historyEvent.getLaunchTime());	entityPS.setInt(2, historyEvent.getTotalMaps());	entityPS.setInt(3, historyEvent.getTotalReduces());	entityPS.setString(4, historyEvent.getStatus());	entityPS.setString(5, historyEvent.getJobId().toString());	entityPS.executeUpdate();	} catch (SQLException sqle) {	
failed to store for job into 

private void processJobStatusChangedEvent( PreparedStatement entityPS, LoggingEvent logEvent, JobStatusChangedEvent historyEvent) {	try {	entityPS.setString(1, historyEvent.getStatus());	entityPS.setString(2, historyEvent.getJobId().toString());	entityPS.executeUpdate();	} catch (SQLException sqle) {	
failed to store for job into 

private void processJobInfoChangeEvent( PreparedStatement entityPS, LoggingEvent logEvent, JobInfoChangeEvent historyEvent) {	try {	entityPS.setLong(1, historyEvent.getSubmitTime());	entityPS.setLong(2, historyEvent.getLaunchTime());	entityPS.setString(3, historyEvent.getJobId().toString());	entityPS.executeUpdate();	} catch (SQLException sqle) {	
failed to store for job into 

private void processJobUnsuccessfulEvent( PreparedStatement entityPS, LoggingEvent logEvent, JobUnsuccessfulCompletionEvent historyEvent) {	try {	entityPS.setLong(1, historyEvent.getFinishTime());	entityPS.setLong(2, historyEvent.getFinishedMaps());	entityPS.setLong(3, historyEvent.getFinishedReduces());	entityPS.setString(4, historyEvent.getStatus());	entityPS.setString(5, historyEvent.getJobId().toString());	entityPS.executeUpdate();	} catch (SQLException sqle) {	
failed to store for job into 

private void processTaskStartedEvent(PreparedStatement entityPS, LoggingEvent logEvent, TaskStartedEvent historyEvent) {	try {	entityPS.setString(1, historyEvent.getTaskId().getJobID().toString());	entityPS.setString(2, historyEvent.getTaskType().toString());	entityPS.setString(3, historyEvent.getSplitLocations());	entityPS.setLong(4, historyEvent.getStartTime());	entityPS.setString(5, historyEvent.getTaskId().toString());	entityPS.executeUpdate();	} catch (SQLException sqle) {	
failed to store for task into 

private void processTaskFinishedEvent( PreparedStatement entityPS, LoggingEvent logEvent, TaskFinishedEvent historyEvent) {	try {	entityPS.setString(1, historyEvent.getTaskId().getJobID().toString());	entityPS.setString(2, historyEvent.getTaskType().toString());	entityPS.setString(3, historyEvent.getTaskStatus());	entityPS.setLong(4, historyEvent.getFinishTime());	entityPS.setString(5, historyEvent.getTaskId().toString());	entityPS.executeUpdate();	} catch (SQLException sqle) {	
failed to store for task into 

entityPS.setLong(4, historyEvent.getFinishTime());	entityPS.setString(5, historyEvent.getError());	if (historyEvent.getFailedAttemptID() != null) {	entityPS.setString(6, historyEvent.getFailedAttemptID().toString());	} else {	entityPS.setString(6, "task_na");	}	entityPS.setString(7, historyEvent.getTaskId().toString());	entityPS.executeUpdate();	} catch (SQLException sqle) {	
failed to store for task into 

entityPS.setString(1, historyEvent.getTaskId().getJobID().toString());	entityPS.setString(2, historyEvent.getTaskId().toString());	entityPS.setString(3, historyEvent.getTaskType().toString());	entityPS.setLong(4, historyEvent.getStartTime());	entityPS.setString(5, historyEvent.getTrackerName());	entityPS.setString(6, historyEvent.getLocality().toString());	entityPS.setString(7, historyEvent.getAvataar().toString());	entityPS.setString(8, historyEvent.getTaskAttemptId().toString());	entityPS.executeUpdate();	} catch (SQLException sqle) {	
failed to store for taskattempt into 

private void processTaskAttemptFinishedEvent( PreparedStatement entityPS, LoggingEvent logEvent, TaskAttemptFinishedEvent historyEvent) {	if (historyEvent.getTaskType() == TaskType.MAP || historyEvent.getTaskType() == TaskType.REDUCE) {	
ignoring taskattemptfinishedevent for 

try {	entityPS.setString(1, historyEvent.getTaskId().getJobID().toString());	entityPS.setString(2, historyEvent.getTaskId().toString());	entityPS.setString(3, historyEvent.getTaskType().toString());	entityPS.setLong(4, historyEvent.getFinishTime());	entityPS.setString(5, historyEvent.getTaskStatus());	entityPS.setString(6, historyEvent.getHostname());	entityPS.setString(7, historyEvent.getAttemptId().toString());	entityPS.executeUpdate();	} catch (SQLException sqle) {	
failed to store for taskattempt into 

entityPS.setString(1, historyEvent.getTaskId().getJobID().toString());	entityPS.setString(2, historyEvent.getTaskId().toString());	entityPS.setString(3, historyEvent.getTaskType().toString());	entityPS.setLong(4, historyEvent.getFinishTime());	entityPS.setString(5, historyEvent.getTaskStatus());	entityPS.setString(6, historyEvent.getHostname());	entityPS.setString(7, historyEvent.getError());	entityPS.setString(8, historyEvent.getTaskAttemptId().toString());	entityPS.executeUpdate();	} catch (SQLException sqle) {	
failed to store for taskattempt into 

private void processMapAttemptFinishedEvent( PreparedStatement entityPS, LoggingEvent logEvent, MapAttemptFinishedEvent historyEvent) {	if (historyEvent.getTaskType() != TaskType.MAP) {	
ignoring mapattemptfinishedevent for 

entityPS.setString(3, historyEvent.getTaskType().toString());	entityPS.setLong(4, historyEvent.getMapFinishTime());	entityPS.setLong(5, historyEvent.getFinishTime());	entityPS.setLong(6, ioBytes[0]);	entityPS.setLong(7, ioBytes[1]);	entityPS.setString(8, historyEvent.getTaskStatus());	entityPS.setString(9, historyEvent.getHostname());	entityPS.setString(10, historyEvent.getAttemptId().toString());	entityPS.executeUpdate();	} catch (SQLException sqle) {	
failed to store for taskattempt into 

private void processReduceAttemptFinishedEvent( PreparedStatement entityPS, LoggingEvent logEvent, ReduceAttemptFinishedEvent historyEvent) {	if (historyEvent.getTaskType() != TaskType.REDUCE) {	
ignoring reduceattemptfinishedevent for 

entityPS.setLong(4, historyEvent.getShuffleFinishTime());	entityPS.setLong(5, historyEvent.getSortFinishTime());	entityPS.setLong(6, historyEvent.getFinishTime());	entityPS.setLong(7, ioBytes[0]);	entityPS.setLong(8, ioBytes[1]);	entityPS.setString(9, historyEvent.getTaskStatus());	entityPS.setString(10, historyEvent.getHostname());	entityPS.setString(11, historyEvent.getAttemptId().toString());	entityPS.executeUpdate();	} catch (SQLException sqle) {	
failed to store for taskattempt into 

========================= ambari sample_504 =========================

public String process(DummyObject input) throws Exception {	
dummy processing wait seconds 

========================= ambari sample_154 =========================

private void normalizeHosts(SshHostInfo info) {	List<String> validHosts = new ArrayList<>();	List<String> newHosts = new ArrayList<>();	for (String host: info.getHosts()) {	try {	InetAddress addr = InetAddress.getByName(host);	if (!validHosts.contains(addr.getHostAddress())) {	validHosts.add(addr.getHostAddress());	newHosts.add(host);	} else {	
host has already been targeted to be bootstrapped 

List<String> newHosts = new ArrayList<>();	for (String host: info.getHosts()) {	try {	InetAddress addr = InetAddress.getByName(host);	if (!validHosts.contains(addr.getHostAddress())) {	validHosts.add(addr.getHostAddress());	newHosts.add(host);	} else {	}	} catch (UnknownHostException e) {	
host cannot be determined 

========================= ambari sample_4231 =========================

public Storage getStorage() {	String fileName = context.getProperties().get("dataworker.storagePath");	Storage storageInstance;	if (fileName != null) {	
using local storage in to store data 

public Storage getStorage() {	String fileName = context.getProperties().get("dataworker.storagePath");	Storage storageInstance;	if (fileName != null) {	storageInstance = new LocalKeyValueStorage(context);	} else {	
using persistence api to store data 

========================= ambari sample_912 =========================

protected void setUp() throws Exception {	tmpFolder.create();	fileDir = tmpFolder.newFolder("keys").getAbsolutePath();	
setting temp folder to 

========================= ambari sample_1875 =========================

public void getServices() throws AmbariException {	Map<String, ServiceInfo> services = metaInfo.getServices(STACK_NAME_HDP, STACK_VERSION_HDP);	
getting all the services 

public void getServices() throws AmbariException {	Map<String, ServiceInfo> services = metaInfo.getServices(STACK_NAME_HDP, STACK_VERSION_HDP);	for (Map.Entry<String, ServiceInfo> entry : services.entrySet()) {	
service name values 

public void testGetRepos() throws Exception {	Map<String, List<RepositoryInfo>> repos = metaInfo.getRepository( STACK_NAME_HDP, STACK_VERSION_HDP);	Set<String> centos5Cnt = new HashSet<>();	Set<String> centos6Cnt = new HashSet<>();	Set<String> redhat6cnt = new HashSet<>();	Set<String> redhat5cnt = new HashSet<>();	for (List<RepositoryInfo> vals : repos.values()) {	for (RepositoryInfo repo : vals) {	
dumping repo info 

public void testBadStack() throws Exception {	File stackRoot = new File("src/test/resources/bad-stacks");	File version = new File("src/test/resources/version");	if (System.getProperty("os.name").contains("Windows")) {	stackRoot = new File(ClassLoader.getSystemClassLoader().getResource("bad-stacks").getPath());	version = new File(new File(ClassLoader.getSystemClassLoader().getResource("").getPath()).getParent(), "version");	}	
stacks file 

Properties properties = new Properties();	properties.setProperty(Configuration.METADATA_DIR_PATH.getKey(), stacks.getPath());	properties.setProperty(Configuration.COMMON_SERVICES_DIR_PATH.getKey(), commonServicesRoot.getPath());	properties.setProperty(Configuration.SERVER_VERSION_FILE.getKey(), version.getPath());	Configuration configuration = new Configuration(properties);	TestAmbariMetaInfo ambariMetaInfo = new TestAmbariMetaInfo(configuration);	ambariMetaInfo.replayAllMocks();	try {	ambariMetaInfo.init();	} catch(Exception e) {	
error in metainfo initializing 

waitForAllReposToBeResolved(ambariMetaInfo);	String[] metricsTypes = {	Resource.Type.Component.name(), Resource.Type.HostComponent.name() };	for (StackInfo stackInfo: ambariMetaInfo.getStacks(STACK_NAME_HDP)) {	for (ServiceInfo serviceInfo: stackInfo.getServices()) {	for (ComponentInfo componentInfo: serviceInfo.getComponents()) {	for (String metricType: metricsTypes) {	List<MetricDefinition> list = ambariMetaInfo.getMetrics(stackInfo.getName(), stackInfo.getVersion(), serviceInfo.getName(), componentInfo.getName(), metricType);	String currentComponentInfo =  stackInfo.getName() + "-" + stackInfo.getVersion() + ", " + serviceInfo.getName() + ", " + componentInfo.getName()+ ", " + metricType;	if (list == null) {	
no metrics found for 

for (ServiceInfo serviceInfo: stackInfo.getServices()) {	for (ComponentInfo componentInfo: serviceInfo.getComponents()) {	for (String metricType: metricsTypes) {	List<MetricDefinition> list = ambariMetaInfo.getMetrics(stackInfo.getName(), stackInfo.getVersion(), serviceInfo.getName(), componentInfo.getName(), metricType);	String currentComponentInfo =  stackInfo.getName() + "-" + stackInfo.getVersion() + ", " + serviceInfo.getName() + ", " + componentInfo.getName()+ ", " + metricType;	if (list == null) {	continue;	} else {	checkNoAggregatedFunctionsForJmx(list);	}	
cross checking jmx to ganglia metrics for 

Properties properties = new Properties();	properties.setProperty(Configuration.METADATA_DIR_PATH.getKey(), stackRoot.getPath());	properties.setProperty(Configuration.SERVER_VERSION_FILE.getKey(), versionFile.getPath());	properties.setProperty(Configuration.RESOURCES_DIR.getKey(), resourcesRoot.getPath());	Configuration configuration = new Configuration(properties);	TestAmbariMetaInfo metaInfo = new TestAmbariMetaInfo(configuration);	metaInfo.replayAllMocks();	try {	metaInfo.init();	} catch(Exception e) {	
error in initializing 

========================= ambari sample_2465 =========================

if (kdcHost == null || kdcHost.isEmpty()) {	throw new IllegalArgumentException("Invalid hostname for KDC server");	}	String[] kdcDetails = kdcHost.split(":");	if (kdcDetails.length == 1) {	return isKdcReachable(kdcDetails[0], parsePort(config.getDefaultKdcPort()));	} else {	return isKdcReachable(kdcDetails[0], parsePort(kdcDetails[1]));	}	} catch (Exception e) {	
exception while checking kdc reachability 

public boolean isKdcReachable(String server, int port) {	boolean success = isKdcReachable(server, port, ConnectionProtocol.TCP) || isKdcReachable(server, port, ConnectionProtocol.UDP);	if (!success) {	
failed to connect to the kdc at using either tcp or udp 

config.setHostName(server);	config.setKdcPort(port);	config.setUseUdp(ConnectionProtocol.UDP == connectionProtocol);	config.setTimeout(timeoutMillis);	FutureTask<Boolean> future = new FutureTask<>(new Callable<Boolean>() {	public Boolean call() {	Boolean success;	try {	KdcConnection connection = getKdcConnection(config);	connection.getTgt("noUser@noRealm", "noPassword");	
encountered no exceptions while testing connectivity to the kdc host s d s 

messageTypeCode = -1;	}	String message = String.format("Received KerberosException while testing connectivity to the KDC: %s\n" + "**** Host:    %s:%d (%s)\n" + "**** Error:   %s\n" + "**** Code:    %d (%s)\n" + "**** Message: %d (%s)", e.getLocalizedMessage(), server, port, connectionProtocol.name(), error.getEText(), errorCodeCode, errorCodeMessage, messageTypeCode, messageTypeMessage);	if (LOG.isDebugEnabled()) {	LOG.info(message, e);	} else {	LOG.info(message);	}	}	} catch (Throwable e) {	
received exception while testing connectivity to the kdc s host s d s 

throw new RuntimeException(e);	}	return success;	}	});	new Thread(future, "ambari-kdc-verify").start();	Boolean result;	try {	result = future.get(timeoutMillis, TimeUnit.MILLISECONDS);	if (result) {	
successfully connected to the kdc server at s d over s 

}	return success;	}	});	new Thread(future, "ambari-kdc-verify").start();	Boolean result;	try {	result = future.get(timeoutMillis, TimeUnit.MILLISECONDS);	if (result) {	} else {	
failed to connect to the kdc server at s d over s 

========================= ambari sample_2936 =========================

perHostSummaryMap.put(clusterId, perHostCounts);	}	break;	default: break;	}	}	for (Resource res : resources) {	populateResource(res, propertyIds, perHostSummaryMap, hostsSummaryMap);	}	} catch (AmbariException e) {	
could not load built in alerts executor exception 

========================= ambari sample_3624 =========================

public Set<String> filterInvalidPropertyValues(Map<PropertyInfo, String> properties, String filteredListName) {	Set<String> resultSet = new HashSet<>();	for (Iterator<Entry<PropertyInfo, String>> iterator = properties.entrySet().iterator(); iterator.hasNext();) {	Entry<PropertyInfo, String> property = iterator.next();	PropertyInfo propertyInfo = property.getKey();	String propertyValue = property.getValue();	if (property == null || propertyValue == null || propertyValue.toLowerCase().equals("null") || propertyValue.isEmpty()) {	
excluding property s from s because of invalid or empty value 

public boolean isStaleConfigs(ServiceComponentHost sch, Map<String, DesiredConfig> requestDesiredConfigs, HostComponentDesiredStateEntity hostComponentDesiredStateEntity) throws AmbariException {	boolean stale = calculateIsStaleConfigs(sch, requestDesiredConfigs, hostComponentDesiredStateEntity);	if (LOG.isDebugEnabled()) {	
cache configuration staleness for host and component as 

actualConfigs.put(configType, cluster.getConfig(configType, desiredConfig.getTag()));	}	for (Service service : cluster.getServices().values()) {	Set<PropertyInfo> serviceProperties = new HashSet<>(servicesMap.get(service.getName()).getProperties());	for (PropertyInfo serviceProperty : serviceProperties) {	if (serviceProperty.getPropertyTypes().contains(propertyType)) {	String stackPropertyConfigType = fileNameToConfigType(serviceProperty.getFilename());	try {	String property = actualConfigs.get(stackPropertyConfigType).getProperties().get(serviceProperty.getName());	if (null == property){	
unable to obtain property values for s with property attribute s the property does not exist in version s of s configuration 

actualConfigs.put(configType, cluster.getConfig(configType, desiredConfig.getTag()));	}	for (Service service : cluster.getServices().values()) {	Set<PropertyInfo> serviceProperties = new HashSet<>(servicesMap.get(service.getName()).getProperties());	for (PropertyInfo serviceProperty : serviceProperties) {	if (serviceProperty.getPropertyTypes().contains(propertyType)) {	String stackPropertyConfigType = fileNameToConfigType(serviceProperty.getFilename());	try {	String property = actualConfigs.get(stackPropertyConfigType).getProperties().get(serviceProperty.getName());	if (null == property){	
unable to obtain property values for s with property attribute s the property does not exist in version s of s configuration 

Set<String> stackConfigTypes = ambariMetaInfo.getStack(stackId.getStackName(), stackId.getStackVersion()).getConfigTypeAttributes().keySet();	Map<String, Config> actualConfigs = new HashMap<>();	Map<String, DesiredConfig> desiredConfigs = cluster.getDesiredConfigs();	for (Map.Entry<String, DesiredConfig> desiredConfigEntry : desiredConfigs.entrySet()) {	String configType = desiredConfigEntry.getKey();	DesiredConfig desiredConfig = desiredConfigEntry.getValue();	actualConfigs.put(configType, cluster.getConfig(configType, desiredConfig.getTag()));	}	for (String stackConfigType : stackConfigTypes) {	if (!actualConfigs.containsKey(stackConfigType)) {	
unable to find stack configuration s in ambari configs 

stale = stale | staleEntry;	}	String refreshCommand = calculateRefreshCommand(stackInfo.getRefreshCommandConfiguration(), sch, changedProperties);	if (STALE_CONFIGS_CACHE_ENABLED) {	staleConfigsCache.put(staleHash, stale);	if (refreshCommand != null) {	refreshConfigCommandCache.put(staleHash, refreshCommand);	}	}	if (LOG.isDebugEnabled()) {	
changed properties command 

Map<Long, ConfigGroup> configGroups = cluster.getConfigGroupsByHostname(hostname);	if (configGroups != null && !configGroups.isEmpty()) {	for (ConfigGroup configGroup : configGroups.values()) {	Config config = configGroup.getConfigurations().get(type);	if (config != null) {	return true;	}	}	}	} catch (AmbariException ambariException) {	
could not determine group configuration for host details 

========================= ambari sample_3274 =========================

template = createTemplate;	}	try {	if (Velocity.evaluate(new VelocityContext(context), stringWriter, "Active Directory principal create template", template)) {	String json = stringWriter.toString();	Type type = new TypeToken<Map<String, Object>>() {	}.getType();	data = gson.fromJson(json, type);	}	} catch (ParseErrorException e) {	
failed to parse active directory create principal template 

try {	if (Velocity.evaluate(new VelocityContext(context), stringWriter, "Active Directory principal create template", template)) {	String json = stringWriter.toString();	Type type = new TypeToken<Map<String, Object>>() {	}.getType();	data = gson.fromJson(json, type);	}	} catch (ParseErrorException e) {	throw new KerberosOperationException("Failed to parse Active Directory create principal template", e);	} catch (MethodInvocationException | ResourceNotFoundException e) {	
failed to process active directory create principal template 

========================= ambari sample_3331 =========================

public Response getList() {	try {	
getting all udf 

========================= ambari sample_841 =========================

if (null == executionStages) {	return stageWrappers;	}	List<StageWrapper> results = new ArrayList<>(stageWrappers);	if (executionStages != null) {	for (ExecuteStage execution : executionStages) {	if (null != execution.intendedDirection && execution.intendedDirection != upgradeContext.getDirection()) {	continue;	}	if (null != execution.condition && !execution.condition.isSatisfied(upgradeContext)) {	
skipping while building upgrade orchestration due to 

}	if (null != execution.condition && !execution.condition.isSatisfied(upgradeContext)) {	continue;	}	if (StringUtils.isNotBlank(execution.service)) {	if (!upgradeContext.isServiceSupported(execution.service)) {	continue;	}	}	if (null != execution.task.condition && !execution.task.condition.isSatisfied(upgradeContext)) {	
skipping while building upgrade orchestration due to 

========================= ambari sample_3223 =========================

private void refreshCollectorHost(String currentHost) {	
refreshing collector host current collector host 

if (!deadCollectorHosts.contains(currentHost)) {	deadCollectorHosts.add(currentHost);	}	}	if (!liveCollectorHosts.isEmpty()) {	currentCollectorHost = getRandom(liveCollectorHosts);	}	if (currentCollectorHost == null && !deadCollectorHosts.isEmpty()) {	currentCollectorHost = getRandom(deadCollectorHosts);	}	
after refresh new collector host 

private void testAndAddDeadCollectorsToLiveList() {	Set<String> liveHosts = new HashSet<>();	for (String deadHost : deadCollectorHosts) {	if (isValidAliveCollectorHost(clusterName, deadHost)) {	liveHosts.add(deadHost);	}	}	for (String liveHost : liveHosts) {	
removing collector from dead list to live list 

========================= ambari sample_3389 =========================

public void onBecomeOnlineFromOffline(Message message, NotificationContext context) {	String partitionName = message.getPartitionName();	
received transition to online from offline for partition 

public void onBecomeOfflineFromOnline(Message message, NotificationContext context) {	String partitionName = message.getPartitionName();	
received transition to offline from online for partition 

public void onBecomeDroppedFromOffline(Message message, NotificationContext context) {	String partitionName = message.getPartitionName();	
received transition to dropped from offline for partition 

========================= ambari sample_379 =========================

} else {	data = "{" + convertObjToString(vBarUserDataList) + "," + convertObjToString(vBarResourceDataList) + "}";	dataFormat = "json";	}	String fileName = "Users_Resource" + startTime + endTime + ".";	File file = File.createTempFile(fileName, dataFormat);	fis = new FileOutputStream(file);	fis.write(data.getBytes());	return Response .ok(file, MediaType.APPLICATION_OCTET_STREAM) .header("Content-Disposition", "attachment;filename=" + fileName + dataFormat) .build();	} catch (IOException e) {	
error during download file audit log 

========================= ambari sample_1422 =========================

String serviceName = alertDefinition.getServiceName();	group = dispatchDao.createDefaultGroup(alertDefinition.getClusterId(), serviceName);	}	group.addAlertDefinition(alertDefinition);	dispatchDao.merge(group);	AlertDefinition coerced = alertDefinitionFactory.coerce(alertDefinition);	if (null != coerced) {	AlertDefinitionRegistrationEvent event = new AlertDefinitionRegistrationEvent( alertDefinition.getClusterId(), coerced);	eventPublisher.publish(event);	} else {	
unable to broadcast alert registration event for 

alertsDao.removeByDefinitionId(alertDefinition.getDefinitionId());	EntityManager entityManager = entityManagerProvider.get();	alertDefinition = findById(alertDefinition.getDefinitionId());	if (null != alertDefinition) {	entityManager.remove(alertDefinition);	AlertDefinition coerced = alertDefinitionFactory.coerce(alertDefinition);	if (null != coerced) {	AlertDefinitionDeleteEvent event = new AlertDefinitionDeleteEvent( alertDefinition.getClusterId(), coerced);	eventPublisher.publish(event);	} else {	
unable to broadcast alert removal event for 

========================= ambari sample_4270 =========================

public LinkedHashMap<String, String> next() {	LinkedHashMap<String, String> currObject = nextObject;	try {	nextObject = readNextObject(this.reader);	} catch (EndOfDocumentException e) {	
end of json document reached with next character ending the json array 

public void remove() {	
no operation when remove called on jsoniterator 

========================= ambari sample_816 =========================

public Map<String, ConfigUpgradeChangeDefinition> enumerateConfigChangesByID() {	if (changesById == null) {	changesById = new HashMap<>();	for(AffectedService service : services) {	for(AffectedComponent component: service.components) {	for (ConfigUpgradeChangeDefinition changeDefinition : component.changes) {	if (changeDefinition.id == null) {	
config upgrade change definition for service s component s has no id 

public Map<String, ConfigUpgradeChangeDefinition> enumerateConfigChangesByID() {	if (changesById == null) {	changesById = new HashMap<>();	for(AffectedService service : services) {	for(AffectedComponent component: service.components) {	for (ConfigUpgradeChangeDefinition changeDefinition : component.changes) {	if (changeDefinition.id == null) {	} else if (changesById.containsKey(changeDefinition.id)) {	
duplicate config upgrade change definition with id 

========================= ambari sample_3258 =========================

public boolean init(String inputDesc, String fieldName, String mapClassCode, MapFieldDescriptor mapFieldDescriptor) {	init(inputDesc, fieldName, mapClassCode);	pattern = ((MapAnonymizeDescriptor)mapFieldDescriptor).getPattern();	if (StringUtils.isEmpty(pattern)) {	
pattern is empty 

========================= ambari sample_1635 =========================

private void updateGuid(UpdateYarnAtsGuid message) {	HiveStatement statement = message.getStatement();	String jobId = message.getJobId();	String yarnAtsGuid = statement.getYarnATSGuid();	
fetched guid for job id 

private void updateGuid(UpdateYarnAtsGuid message) {	HiveStatement statement = message.getStatement();	String jobId = message.getJobId();	String yarnAtsGuid = statement.getYarnATSGuid();	if(yarnAtsGuid == null) {	
retrying to fetch guid 

private void updateGuid(UpdateYarnAtsGuid message) {	HiveStatement statement = message.getStatement();	String jobId = message.getJobId();	String yarnAtsGuid = statement.getYarnATSGuid();	if(yarnAtsGuid == null) {	getContext().system().scheduler() .scheduleOnce(Duration.create(1, TimeUnit.SECONDS), getSelf(), message, getContext().dispatcher(), null);	} else {	jdbcConnectorActor.tell(new SaveGuidToDB(message.getStatementId(), yarnAtsGuid, jobId), self());	
message send to save guid for statement id job id guid 

========================= ambari sample_752 =========================

private void populateResource(HttpPropertyRequest httpPropertyRequest, Resource resource, Cluster cluster, String hostName, String publicHostName) throws SystemException {	String url = httpPropertyRequest.getUrl(cluster, hostName);	try {	InputStream inputStream = streamProvider.readFrom(url);	try {	httpPropertyRequest.populateResource(resource, inputStream);	} finally {	try {	inputStream.close();	} catch (IOException ioe) {	
error closing http response stream s 

InputStream inputStream = streamProvider.readFrom(url);	try {	httpPropertyRequest.populateResource(resource, inputStream);	} finally {	try {	inputStream.close();	} catch (IOException ioe) {	}	}	} catch (Exception e) {	
error reading http response from s 

httpPropertyRequest.populateResource(resource, inputStream);	} finally {	try {	inputStream.close();	} catch (IOException ioe) {	}	}	} catch (Exception e) {	if(publicHostName != null && !publicHostName.equalsIgnoreCase(hostName)) {	String publicUrl = httpPropertyRequest.getUrl(cluster, publicHostName);	
retry using public host name url s 

if(publicHostName != null && !publicHostName.equalsIgnoreCase(hostName)) {	String publicUrl = httpPropertyRequest.getUrl(cluster, publicHostName);	try {	InputStream inputStream = streamProvider.readFrom(publicUrl);	try {	httpPropertyRequest.populateResource(resource, inputStream);	} finally {	try {	inputStream.close();	} catch (IOException ioe) {	
error closing http response stream s 

InputStream inputStream = streamProvider.readFrom(publicUrl);	try {	httpPropertyRequest.populateResource(resource, inputStream);	} finally {	try {	inputStream.close();	} catch (IOException ioe) {	}	}	} catch (Exception ex) {	
error reading http response from public host name url s 

========================= ambari sample_3521 =========================

public Optional<HdfsApi> get(ViewContext context) {	try {	if(!hdfsApiMap.containsKey(getKey(context))) {	synchronized (lock) {	if(!hdfsApiMap.containsKey(getKey(context))) {	
creating hdfsapi instance for viewname instance name 

}else{	api = HdfsUtil.connectToHDFSApi(context);	}	hdfsApiMap.put(getKey(context), api);	return Optional.of(api);	}	}	}	return Optional.of(hdfsApiMap.get(getKey(context)));	} catch (HdfsApiException e) {	
cannot get the hdfs api 

========================= ambari sample_882 =========================

while (shouldRun) {	try {	synchronized (wakeupSyncObject) {	if (!activeAwakeRequest) {	wakeupSyncObject.wait(sleepTime);	}	activeAwakeRequest = false;	}	doWork();	} catch (InterruptedException ex) {	
scheduler thread is interrupted going to stop 

synchronized (wakeupSyncObject) {	if (!activeAwakeRequest) {	wakeupSyncObject.wait(sleepTime);	}	activeAwakeRequest = false;	}	doWork();	} catch (InterruptedException ex) {	shouldRun = false;	} catch (Exception ex) {	
exception received 

wakeupSyncObject.wait(sleepTime);	}	activeAwakeRequest = false;	}	doWork();	} catch (InterruptedException ex) {	shouldRun = false;	} catch (Exception ex) {	requestsInProgress.clear();	} catch (Throwable t) {	
ERROR 

public void doWork() throws AmbariException {	try {	unitOfWork.begin();	threadEntityManager = entityManagerProvider.get();	processCancelledRequestsList();	if (db.getCommandsInProgressCount() == 0) {	if (LOG.isDebugEnabled()) {	
there are no stages currently in progress 

processCancelledRequestsList();	if (db.getCommandsInProgressCount() == 0) {	if (LOG.isDebugEnabled()) {	}	actionQueue.updateListOfHostsWithPendingTask(null);	return;	}	Set<Long> runningRequestIds = new HashSet<>();	List<Stage> firstStageInProgressPerRequest = db.getFirstStageInProgressPerRequest();	if (LOG.isDebugEnabled()) {	
scheduler wakes up 

processCancelledRequestsList();	if (db.getCommandsInProgressCount() == 0) {	if (LOG.isDebugEnabled()) {	}	actionQueue.updateListOfHostsWithPendingTask(null);	return;	}	Set<Long> runningRequestIds = new HashSet<>();	List<Stage> firstStageInProgressPerRequest = db.getFirstStageInProgressPerRequest();	if (LOG.isDebugEnabled()) {	
processing in progress stages 

actionQueue.updateListOfHostsWithPendingTask(null);	return;	}	Set<Long> runningRequestIds = new HashSet<>();	List<Stage> firstStageInProgressPerRequest = db.getFirstStageInProgressPerRequest();	if (LOG.isDebugEnabled()) {	}	publishInProgressTasks(firstStageInProgressPerRequest);	if (firstStageInProgressPerRequest.isEmpty()) {	if (LOG.isDebugEnabled()) {	
there are no stages currently in progress 

actionQueue.updateListOfHostsWithPendingTask(new HashSet<>(hostsWithPendingTasks));	List<Stage> stages = filterParallelPerHostStages(firstStageInProgressPerRequest);	boolean exclusiveRequestIsGoing = false;	for (Stage stage : stages) {	i_stage++;	long requestId = stage.getRequestId();	LOG.debug("==> STAGE_i = {}(requestId={},StageId={})", i_stage, requestId, stage.getStageId());	RequestEntity request = db.getRequestEntity(requestId);	if (request.isExclusive()) {	if (runningRequestIds.size() > 0) {	
stage requires exclusive execution but other requests are already executing stopping for now 

Multimap<String, AgentCommand> commandsToEnqueue = ArrayListMultimap.create();	Map<String, RoleStats> roleStats = processInProgressStage(stage, commandsToSchedule, commandsToEnqueue);	boolean failed = false;	for (Map.Entry<String, RoleStats> entry : roleStats.entrySet()) {	String role = entry.getKey();	RoleStats stats = entry.getValue();	if (LOG.isDebugEnabled()) {	LOG.debug("Stats for role: {}, stats={}", role, stats);	}	if (stats.isRoleFailed() && !stage.isSkippable()) {	
failed request will be aborted 

}	if (stats.isRoleFailed() && !stage.isSkippable()) {	failed = true;	break;	}	}	if (!failed) {	failed = hasPreviousStageFailed(stage);	}	if (failed) {	
operation completely failed aborting request id 

ExecutionCommand cmd = iterator.next();	for (ServiceComponentHostEvent event : failedEvents.keySet()) {	if (StringUtils.equals(event.getHostName(), cmd.getHostname()) && StringUtils.equals(event.getServiceComponentName(), cmd.getRole())) {	iterator.remove();	commandsToAbort.put(cmd, failedEvents.get(event));	break;	}	}	}	} else {	
there was events to process but cluster not found 

} else {	commandsToEnqueue.put(cmd.getHostname(), cmd);	}	}	actionQueue.enqueueAll(commandsToEnqueue.asMap());	LOG.debug("==> Finished.");	if (!configuration.getParallelStageExecution()) {	return;	}	if (exclusiveRequestIsGoing) {	
stage requires exclusive execution skipping all executing any further stages 

LOG.debug("==> Finished.");	if (!configuration.getParallelStageExecution()) {	return;	}	if (exclusiveRequestIsGoing) {	break;	}	}	requestsInProgress.retainAll(runningRequestIds);	} finally {	
scheduler finished work 

Cluster cluster = null;	if (null != s.getClusterName()) {	cluster = clusters.getCluster(s.getClusterName());	}	for (String host : s.getHosts()) {	List<ExecutionCommandWrapper> commandWrappers = s.getExecutionCommands(host);	Host hostObj = null;	try {	hostObj = clusters.getHost(host);	} catch (AmbariException e) {	
host not found stage is likely a server side action 

}	}	long commandTimeout = actionTimeout;	if (taskTimeoutAdjustment) {	Map<String, String> commandParams = c.getCommandParams();	String timeoutKey = ExecutionCommand.KeyNames.COMMAND_TIMEOUT;	if (commandParams != null && commandParams.containsKey(timeoutKey)) {	String timeoutStr = commandParams.get(timeoutKey);	commandTimeout += Long.parseLong(timeoutStr) * 1000;	} else {	
execution command has no timeout parameter 

String timeoutKey = ExecutionCommand.KeyNames.COMMAND_TIMEOUT;	if (commandParams != null && commandParams.containsKey(timeoutKey)) {	String timeoutStr = commandParams.get(timeoutKey);	commandTimeout += Long.parseLong(timeoutStr) * 1000;	} else {	}	}	boolean isHostStateUnknown = false;	if (hostDeleted) {	String message = String.format( "Host not found when trying to schedule an execution command. " + "The most probable reason for that is that host or host component " + "has been deleted recently. The command has been aborted and dequeued." + "Execution command details: " + "cmdId: %s; taskId: %s; roleCommand: %s", c.getCommandId(), c.getTaskId(), c.getRoleCommand());	
host has been detected as non available 

boolean isHostStateUnknown = false;	if (hostDeleted) {	String message = String.format( "Host not found when trying to schedule an execution command. " + "The most probable reason for that is that host or host component " + "has been deleted recently. The command has been aborted and dequeued." + "Execution command details: " + "cmdId: %s; taskId: %s; roleCommand: %s", c.getCommandId(), c.getTaskId(), c.getRoleCommand());	db.abortHostRole(host, s.getRequestId(), s.getStageId(), c.getRole(), message);	if (c.getRoleCommand().equals(RoleCommand.ACTIONEXECUTE)) {	processActionDeath(cluster.getClusterName(), c.getHostname(), roleStr);	}	status = HostRoleStatus.ABORTED;	} else if (timeOutActionNeeded(status, s, hostObj, roleStr, now, commandTimeout) || (isHostStateUnknown = isHostStateUnknown(s, hostObj, roleStr))) {	if (s.getAttemptCount(host, roleStr) >= maxAttempts || isHostStateUnknown) {	
host role actionid expired and will be failed 

transitionToFailedState(cluster.getClusterName(), c.getServiceName(), roleStr, host, now, false);	}	if (c.getRoleCommand().equals(RoleCommand.ACTIONEXECUTE)) {	processActionDeath(cluster.getClusterName(), c.getHostname(), roleStr);	}	}	LOG.info("Removing command from queue, host={}, commandId={} ", host, c.getCommandId());	actionQueue.dequeue(host, c.getCommandId());	} else {	cancelCommandOnTimeout(Collections.singletonList(s.getHostRoleCommand(host, roleStr)), commandsToEnqueue);	
host role actionid timed out and will be rescheduled 

}	} else if (status.equals(HostRoleStatus.PENDING)) {	if (CommandExecutionType.STAGE == s.getCommandExecutionType() || (CommandExecutionType.DEPENDENCY_ORDERED == s.getCommandExecutionType() && CommandExecutionType.DEPENDENCY_ORDERED == configuration.getStageExecutionType() && areCommandDependenciesFinished(c, s, rolesCommandsInProgress))) {	commandsToSchedule.add(c);	LOG.trace("===>commandsToSchedule(first_time)={}", commandsToSchedule.size());	}	}	updateRoleStats(status, roleStats.get(roleStr));	}	}	
collected commands to schedule in this wakeup 

private void transitionToFailedState(String clusterName, String serviceName, String componentName, String hostname, long timestamp, boolean ignoreTransitionException) {	try {	Cluster cluster = clusters.getCluster(clusterName);	ServiceComponentHostOpFailedEvent failedEvent = new ServiceComponentHostOpFailedEvent(componentName, hostname, timestamp);	if (serviceName != null && ! serviceName.isEmpty() && componentName != null && ! componentName.isEmpty()) {	Service svc = cluster.getService(serviceName);	ServiceComponent svcComp = svc.getServiceComponent(componentName);	ServiceComponentHost svcCompHost = svcComp.getServiceComponentHost(hostname);	svcCompHost.handleEvent(failedEvent);	} else {	
service name is component name is skipping sending servicecomponenthostopfailedevent for 

Cluster cluster = clusters.getCluster(clusterName);	ServiceComponentHostOpFailedEvent failedEvent = new ServiceComponentHostOpFailedEvent(componentName, hostname, timestamp);	if (serviceName != null && ! serviceName.isEmpty() && componentName != null && ! componentName.isEmpty()) {	Service svc = cluster.getService(serviceName);	ServiceComponent svcComp = svc.getServiceComponent(componentName);	ServiceComponentHost svcCompHost = svcComp.getServiceComponentHost(hostname);	svcCompHost.handleEvent(failedEvent);	} else {	}	} catch (ServiceComponentNotFoundException scnex) {	
associated with service is not a service component assuming it s an action 

ServiceComponentHost svcCompHost = svcComp.getServiceComponentHost(hostname);	svcCompHost.handleEvent(failedEvent);	} else {	}	} catch (ServiceComponentNotFoundException scnex) {	} catch (ServiceComponentHostNotFoundException e) {	String msg = String.format("Service component host %s not found, " + "unable to transition to failed state.", componentName);	LOG.warn(msg, e);	} catch (InvalidStateTransitionException e) {	if (ignoreTransitionException) {	
unable to transition to failed state 

svcCompHost.handleEvent(failedEvent);	} else {	}	} catch (ServiceComponentNotFoundException scnex) {	} catch (ServiceComponentHostNotFoundException e) {	String msg = String.format("Service component host %s not found, " + "unable to transition to failed state.", componentName);	LOG.warn(msg, e);	} catch (InvalidStateTransitionException e) {	if (ignoreTransitionException) {	} else {	
unable to transition to failed state 

}	} catch (ServiceComponentNotFoundException scnex) {	} catch (ServiceComponentHostNotFoundException e) {	String msg = String.format("Service component host %s not found, " + "unable to transition to failed state.", componentName);	LOG.warn(msg, e);	} catch (InvalidStateTransitionException e) {	if (ignoreTransitionException) {	} else {	}	} catch (AmbariException e) {	
unable to transition to failed state 

private boolean isHostStateUnknown(Stage stage, Host host, String role) {	if (null != host && (host.getState().equals(HostState.HEARTBEAT_LOST) || wasAgentRestartedDuringOperation(host, stage, role))) {	
abort action since agent is not heartbeating or agent was restarted 

CommandReport report = new CommandReport();	report.setRole(role);	report.setStdOut("Action is dead");	report.setStdErr("Action is dead");	report.setStructuredOut("{}");	report.setExitCode(1);	report.setStatus(HostRoleStatus.ABORTED.toString());	ActionFinalReportReceivedEvent event = new ActionFinalReportReceivedEvent( clusterId, hostname, report, true);	ambariEventPublisher.publish(event);	} catch (AmbariException e) {	
can not get cluster s 

case TIMEDOUT: rs.numTimedOut++;	break;	case ABORTED: rs.numAborted++;	break;	case IN_PROGRESS: rs.numInProgress++;	break;	case HOLDING: case HOLDING_FAILED: case HOLDING_TIMEDOUT: rs.numHolding++;	break;	case SKIPPED_FAILED: rs.numSkipped++;	break;	
unknown status 

public void onEvent(EntityManagerCacheInvalidationEvent event) {	try {	if (null != threadEntityManager && threadEntityManager.isOpen()) {	threadEntityManager.clear();	}	} catch (Throwable throwable) {	
unable to clear the entitymanager for the scheduler thread 

========================= ambari sample_2760 =========================

public Collection<? extends GrantedAuthority> getGrantedAuthorities(DirContextOperations userData, String username) {	username = AuthorizationHelper.resolveLoginAliasToUserName(username);	
get authorities for user from local db 

public Collection<? extends GrantedAuthority> getGrantedAuthorities(DirContextOperations userData, String username) {	username = AuthorizationHelper.resolveLoginAliasToUserName(username);	UserEntity user;	user = userDAO.findUserByName(username);	if (user == null) {	
can t get authorities for user he is not present in local db 

========================= ambari sample_2808 =========================

public void onAlertEvent(AlertStateChangeEvent event) {	
received event 

if (!alertStates.contains(history.getAlertState())) {	return false;	}	}	Long clusterId = history.getClusterId();	try {	Cluster cluster = m_clusters.get().getClusterById(clusterId);	if (null != cluster.getUpgradeInProgress()) {	String serviceName = definition.getServiceName();	if (!StringUtils.equals(serviceName, RootService.AMBARI.name())) {	
skipping alert notifications for because the cluster is upgrading 

Long clusterId = history.getClusterId();	try {	Cluster cluster = m_clusters.get().getClusterById(clusterId);	if (null != cluster.getUpgradeInProgress()) {	String serviceName = definition.getServiceName();	if (!StringUtils.equals(serviceName, RootService.AMBARI.name())) {	return false;	}	}	} catch (AmbariException ambariException) {	
unable to process an alert state change for cluster with id because it does not exist 

========================= ambari sample_4525 =========================

public void handleMessage(HiveMessage hiveMessage) {	Object message = hiveMessage.getMessage();	if (message instanceof StartLogAggregation) {	start((StartLogAggregation) message);	}	if (message instanceof GetMoreLogs) {	try {	getMoreLogs();	} catch (SQLException e) {	
sql error while getting logs tried writing to exception 

public void handleMessage(HiveMessage hiveMessage) {	Object message = hiveMessage.getMessage();	if (message instanceof StartLogAggregation) {	start((StartLogAggregation) message);	}	if (message instanceof GetMoreLogs) {	try {	getMoreLogs();	} catch (SQLException e) {	} catch (HdfsApiException e) {	
hdfs error while writing logs to exception 

========================= ambari sample_705 =========================

public boolean isAllowed(Map<String, Object> jsonObj, InputMarker inputMarker) {	if ("audit".equals(inputMarker.getInput().getInputDescriptor().getRowtype())) return true;	boolean isAllowed = applyFilter(jsonObj);	if (!isAllowed) {	
filter block the content 

public boolean applyFilter(Map<String, Object> jsonObj) {	if (MapUtils.isEmpty(jsonObj)) {	
output jsonobj is empty 

========================= ambari sample_1662 =========================

public Set<Resource> getResources(Request request, Predicate predicate) throws SystemException, UnsupportedPropertyException, NoSuchResourceException, NoSuchParentResourceException {	final Set<ActionRequest> requests = new HashSet<>();	if (predicate != null) {	for (Map<String, Object> propertyMap : getPropertyMaps(predicate)) {	ActionRequest actionReq = getRequest(propertyMap);	LOG.debug("Received a get request for Action with, actionName = {}", actionReq.getActionName());	requests.add(actionReq);	}	} else {	
received a get request for all actions 

========================= ambari sample_3599 =========================

rows.addAll(result.getRows());	}	if (receive instanceof NoMoreItems) {	if(descriptions.isEmpty()) {	descriptions.addAll(((NoMoreItems)receive).getColumns());	}	endReached = true;	}	if (receive instanceof FetchFailed) {	FetchFailed error = (FetchFailed) receive;	
failed to fetch results 

========================= ambari sample_990 =========================

Map<String, String> errors = new HashMap<>();	errors.put("message", "Authentication Exception");	entity.put("errors", errors);	return Response.status(Response.Status.UNAUTHORIZED).entity(entity).build();	} else throw new ServiceFormattedException(e.getMessage(), e);	} finally {	try {	hiveConnectionWrapper.disconnect();	}	catch(ConnectionException e){	
cannot close the connection 

========================= ambari sample_569 =========================

public Response toResponse(Throwable throwable) {	
rest exception occurred 

public Response toResponse(Throwable throwable) {	Response.Status status = Response.Status.INTERNAL_SERVER_ERROR;	for (Map.Entry<Class, Response.Status> entry : exceptionStatusCodeMap.entrySet()) {	if (throwable.getClass().isAssignableFrom(entry.getKey())) {	status = entry.getValue();	
exception mapped to with status code 

========================= ambari sample_206 =========================

}	insertQuery.append("tempTable.");	insertQuery.append('`').append(column.getName()).append('`');	if(unhex) {	insertQuery.append(")");	}	first = false;	}	insertQuery.append(";");	String query = insertQuery.toString();	
insert from query 

========================= ambari sample_656 =========================

public void onEvent(MaintenanceModeEvent event) {	
received event 

String hostName = serviceComponentHost.getHostName();	String serviceName = serviceComponentHost.getServiceName();	String componentName = serviceComponentHost.getServiceComponentName();	if (hostName.equals(alertHostName) && serviceName.equals(alertServiceName) && componentName.equals(alertComponentName)) {	if (updateMaintenanceStateAndRecalculateAggregateAlert(history, currentAlert, newMaintenanceState)) recalculateAggregateAlert = true;	continue;	}	}	} catch (Exception exception) {	AlertDefinitionEntity definition = history.getAlertDefinition();	
unable to put alert for host into maintenance mode 

private boolean updateMaintenanceStateAndRecalculateAggregateAlert (AlertHistoryEntity historyAlert, AlertCurrentEntity currentAlert, MaintenanceState maintenanceState) {	if (maintenanceState != MaintenanceState.OFF && maintenanceState != MaintenanceState.ON) {	
unable to set invalid maintenance state of on the alert 

========================= ambari sample_4533 =========================

public void init(Map<String, String> properties) throws Exception {	this.properties = properties;	String root = MapUtils.getString(properties, ZK_ROOT_NODE_PROPERTY, DEFAULT_ZK_ROOT);	
connecting to zookeeper at 

public void createInputConfig(String clusterName, String serviceName, String inputConfig) throws Exception {	String nodePath = String.format("/%s/input/%s", clusterName, serviceName);	try {	client.create().creatingParentContainersIfNeeded().withACL(getAcls()).forPath(nodePath, inputConfig.getBytes());	
uploaded input config for the service for cluster 

public void createInputConfig(String clusterName, String serviceName, String inputConfig) throws Exception {	String nodePath = String.format("/%s/input/%s", clusterName, serviceName);	try {	client.create().creatingParentContainersIfNeeded().withACL(getAcls()).forPath(nodePath, inputConfig.getBytes());	} catch (NodeExistsException e) {	
did not upload input config for service as it was already uploaded by another log feeder 

public void close() {	
closing zookeeper connection 

========================= ambari sample_1674 =========================

public void resolveInternal( ServiceModule parentModule, Map<String, StackModule> allStacks, Map<String, ServiceModule> commonServices, Map<String, ExtensionModule> extensions, boolean resolveExplicit) throws AmbariException {	if (!serviceInfo.isValid() || !parentModule.isValid()) {	return;	}	
resolve service 

if (!serviceInfo.isValid() || !parentModule.isValid()) {	return;	}	if(!StringUtils.isBlank(serviceInfo.getParent()) && !resolveExplicit) {	return;	}	ServiceInfo parent = parentModule.getModuleInfo();	if (serviceInfo.getComment() == null) {	serviceInfo.setComment(parent.getComment());	}	
display name service parent s s 

if (serviceInfo.getThemes() != null) {	List<ThemeInfo> themes = new ArrayList<>(serviceInfo.getThemes().size());	for (ThemeInfo themeInfo : serviceInfo.getThemes()) {	File themeFile = new File(themesDir + File.separator + themeInfo.getFileName());	ThemeModule module = new ThemeModule(themeFile, themeInfo);	if (module.isValid()) {	themeModules.put(module.getId(), module);	themes.add(themeInfo);	}	else {	
invalid theme for service 

private void finalizeConfiguration() {	
finalize config number of configuration modules 

private void finalizeConfiguration() {	hasConfigs = !(configurationModules.isEmpty());	
finalize config hasconfigs 

========================= ambari sample_4476 =========================

private static Map<String, RandomMetricsProvider> readMetrics(AppID type) {	InputStream input = null;	Map<String, RandomMetricsProvider> metrics = new HashMap<String, RandomMetricsProvider>();	String fileName = "metrics_def/" + type.toString() + ".dat";	try {	
loading 

Map<String, RandomMetricsProvider> metrics = new HashMap<String, RandomMetricsProvider>();	String fileName = "metrics_def/" + type.toString() + ".dat";	try {	input = MetricsGeneratorConfigurer.class.getClassLoader() .getResourceAsStream(fileName);	BufferedReader reader = new BufferedReader(new InputStreamReader(input));	String line;	while ((line = reader.readLine()) != null) {	metrics.put(line.trim(), new RandomMetricsProvider(100, 200));	}	} catch (IOException e) {	
cannot read file for appid 

========================= ambari sample_343 =========================

protected String readFromWithDefault(String atsUrl, String defaultResponse) {	String response;	try {	InputStream responseInputStream = context.getURLStreamProvider().readAsCurrent(atsUrl, "GET", (String)null, new HashMap<String, String>());	response = IOUtils.toString(responseInputStream);	} catch (IOException e) {	
error while reading from ats 

========================= ambari sample_590 =========================

if (!MapUtils.isEmpty(configurations)) {	for (Map.Entry<String, Collection<String>> entry : configurations.entrySet()) {	String configType = entry.getKey();	for (String propertyName : entry.getValue()) {	Map<String, KerberosConfigurationDescriptor> configurationDescriptors = kerberosDescriptorContainer.getConfigurations(false);	KerberosConfigurationDescriptor configurationDescriptor = (configurationDescriptors == null) ? null : configurationDescriptors.get(configType);	if (configurationDescriptor != null) {	Map<String, String> properties = configurationDescriptor.getProperties();	if ((properties != null) && properties.containsKey(propertyName)) {	properties.remove(propertyName);	
removed from the descriptor named 

========================= ambari sample_2721 =========================

private Optional<Result> getRowsFromDB(ConnectionConfig config, String[] statements) {	Connect connect = config.createConnectMessage();	HiveJob job = new SQLStatementJob(HiveJob.Type.SYNC, statements, config.getUsername());	ExecuteJob execute = new ExecuteJob(connect, job);	
executing query for user 

private Optional<Result> getTableDescription(ConnectionConfig config, String databasePattern, String tablePattern, String columnPattern) {	Connect connect = config.createConnectMessage();	HiveJob job = new GetColumnMetadataJob(config.getUsername(), databasePattern, tablePattern, columnPattern);	ExecuteJob execute = new ExecuteJob(connect, job);	
executing query to fetch the column description for dbpattern tablepattern columnpattern for user 

inbox.send(controller, job);	Object submitResult;	try {	submitResult = inbox.receive(Duration.create(actorConfiguration.getSyncQueryTimeout(60 * 1000), TimeUnit.MILLISECONDS));	} catch (Throwable ex) {	String errorMessage = "Query timed out to fetch table description for user: " + job.getConnect().getUsername();	LOG.error(errorMessage, ex);	throw new ServiceFormattedException(errorMessage, ex);	}	if (submitResult instanceof NoResult) {	
query returned with no result 

} catch (Throwable ex) {	String errorMessage = "Query timed out to fetch table description for user: " + job.getConnect().getUsername();	LOG.error(errorMessage, ex);	throw new ServiceFormattedException(errorMessage, ex);	}	if (submitResult instanceof NoResult) {	return Optional.absent();	}	if (submitResult instanceof ExecutionFailed) {	ExecutionFailed error = (ExecutionFailed) submitResult;	
failed to get the table description 

if (descriptions == null) {	descriptions = result.getColumns();	}	rows.addAll(result.getRows());	}	if (receive instanceof NoMoreItems) {	break;	}	if (receive instanceof FetchFailed) {	FetchFailed error = (FetchFailed) receive;	
failed to fetch results 

========================= ambari sample_986 =========================

private Set<String> invalidateHosts(long clusterId, SourceType definitionSourceType, String definitionName, String definitionServiceName, String definitionComponentName) {	Cluster cluster = null;	String clusterName = null;	try {	cluster = m_clusters.get().getClusterById(clusterId);	if (null != cluster) {	clusterName = cluster.getClusterName();	}	if (null == cluster) {	
unable to lookup cluster with id 

Cluster cluster = null;	String clusterName = null;	try {	cluster = m_clusters.get().getClusterById(clusterId);	if (null != cluster) {	clusterName = cluster.getClusterName();	}	if (null == cluster) {	}	} catch (Exception exception) {	
unable to lookup cluster with id 

public Set<String> getAssociatedHosts(Cluster cluster, SourceType definitionSourceType, String definitionName, String definitionServiceName, String definitionComponentName) {	if (definitionSourceType == SourceType.AGGREGATE) {	return Collections.emptySet();	}	Map<String, Host> hosts = null;	String clusterName = cluster.getClusterName();	Set<String> affectedHosts = new HashSet<>();	try {	hosts = m_clusters.get().getHostsForCluster(clusterName);	} catch (AmbariException ambariException) {	
unable to lookup hosts for cluster named 

String serviceName = component.getServiceName();	String componentName = component.getServiceComponentName();	if (serviceName.equals(definitionServiceName) && componentName.equals(definitionComponentName)) {	affectedHosts.add(hostName);	}	}	}	Map<String, Service> services = cluster.getServices();	Service service = services.get(definitionServiceName);	if (null == service) {	
the alert definition has an unknown service of 

try {	Cluster cluster = m_clusters.get().getClusterById(clusterId);	clusterName = cluster.getClusterName();	Collection<Host> hosts = cluster.getHosts();	hostNames = new ArrayList<>(hosts.size());	for (Host host : hosts) {	hostNames.add(host.getHostName());	}	enqueueAgentCommands(cluster, clusterName, hostNames);	} catch (AmbariException ae) {	
unable to lookup cluster for alert definition commands 

public void enqueueAgentCommands(long clusterId, Collection<String> hosts) {	String clusterName = null;	Cluster cluster = null;	try {	cluster = m_clusters.get().getClusterById(clusterId);	clusterName = cluster.getClusterName();	} catch (AmbariException ae) {	
unable to lookup cluster for alert definition commands 

private void enqueueAgentCommands(Cluster cluster, String clusterName, Collection<String> hosts) {	if (null == clusterName) {	
unable to create alert definition agent commands because of a null cluster name 

m_actionQueueLock.lock();	for (String hostName : hosts) {	List<AlertDefinition> definitions = getAlertDefinitions(clusterName, hostName);	String hash = getHash(clusterName, hostName);	Host host = cluster.getHost(hostName);	String publicHostName = host == null? hostName : host.getPublicHostName();	AlertDefinitionCommand command = new AlertDefinitionCommand( clusterName, hostName, publicHostName, hash, definitions);	try {	command.addConfigs(m_configHelper.get(), cluster);	} catch (AmbariException ae) {	
unable to add configurations to alert definition command 

}	Collections.sort(uuids);	try {	MessageDigest digest = MessageDigest.getInstance("MD5");	for (String uuid : uuids) {	digest.update(uuid.getBytes());	}	byte[] hashBytes = digest.digest();	return Hex.encodeHexString(hashBytes);	} catch (NoSuchAlgorithmException nsae) {	
unable to calculate alert definition hash 

}	}	}	if (services.size() > 0) {	definitions.addAll(m_definitionDao.findByServiceMaster(clusterId, services));	}	}	definitions.addAll(m_definitionDao.findAgentScoped(clusterId));	}	catch (ClusterNotFoundException clusterNotFound) {	
unable to get alert definitions for the missing cluster 

if (services.size() > 0) {	definitions.addAll(m_definitionDao.findByServiceMaster(clusterId, services));	}	}	definitions.addAll(m_definitionDao.findAgentScoped(clusterId));	}	catch (ClusterNotFoundException clusterNotFound) {	return Collections.emptySet();	}	catch (AmbariException ambariException) {	
unable to get alert definitions 

========================= ambari sample_3075 =========================

collectionCreateRequest.setRouterField(ROUTER_FIELD);	collectionCreateRequest.setMaxShardsPerNode(solrPropsConfig.getReplicationFactor() * solrPropsConfig.getNumberOfShards());	CollectionAdminResponse createResponse = collectionCreateRequest.process(solrClient);	if (createResponse.getStatus() != 0) {	returnValue = false;	LOG.error("Error creating collection. collectionName=" + solrPropsConfig.getCollection() + ", shardsList=" + shardsList +", response=" + createResponse);	} else {	LOG.info("Created collection " + solrPropsConfig.getCollection() + ", shardsList=" + shardsList);	}	} else {	
collection is already there will check whether it has the required shards 

LOG.info("Created collection " + solrPropsConfig.getCollection() + ", shardsList=" + shardsList);	}	} else {	Collection<Slice> slices = getSlices(solrClient, solrPropsConfig);	Collection<String> existingShards = getShards(slices, solrPropsConfig);	if (existingShards.size() < shardsList.size()) {	try {	updateMaximumNumberOfShardsPerCore(slices, solrPropsConfig);	} catch (Throwable t) {	returnValue = false;	
exception during updating collection s 

if (existingShards.size() < shardsList.size()) {	try {	updateMaximumNumberOfShardsPerCore(slices, solrPropsConfig);	} catch (Throwable t) {	returnValue = false;	}	}	for (String shard : shardsList) {	if (!existingShards.contains(shard)) {	try {	
going to add shard to collection 

for (String shard : shardsList) {	if (!existingShards.contains(shard)) {	try {	CollectionAdminRequest.CreateShard createShardRequest = CollectionAdminRequest.createShard(solrPropsConfig.getCollection(), shard);	CollectionAdminResponse response = createShardRequest.process(solrClient);	if (response.getStatus() != 0) {	LOG.error("Error creating shard " + shard + " in collection " + solrPropsConfig.getCollection() + ", response=" + response);	returnValue = false;	break;	} else {	
successfully created shard in collection 

try {	CollectionAdminRequest.CreateShard createShardRequest = CollectionAdminRequest.createShard(solrPropsConfig.getCollection(), shard);	CollectionAdminResponse response = createShardRequest.process(solrClient);	if (response.getStatus() != 0) {	LOG.error("Error creating shard " + shard + " in collection " + solrPropsConfig.getCollection() + ", response=" + response);	returnValue = false;	break;	} else {	}	} catch (Throwable t) {	
error creating shard in collection 

private boolean createCollection(CloudSolrClient solrClient, SolrPropsConfig solrPropsConfig, List<String> allCollectionList) throws SolrServerException, IOException {	if (allCollectionList.contains(solrPropsConfig.getCollection())) {	
collection is already there won t create it 

========================= ambari sample_1336 =========================

public void updateSequenceno(Connection c, int seqNo, int id, QuerySetAmbariDB ambaridatabase) throws SQLException, IOException {	PreparedStatement prSt;	prSt = ambaridatabase.updateSequenceNoInAmbariSequence(c, seqNo, id);	
the actual insert statement is 

public void updateSequenceno(Connection c, int seqNo, int id, QuerySetAmbariDB ambaridatabase) throws SQLException, IOException {	PreparedStatement prSt;	prSt = ambaridatabase.updateSequenceNoInAmbariSequence(c, seqNo, id);	prSt.executeUpdate();	
adding revert sql hive history 

String url = rs1.getString("url");	String fileName = rs1.getString("file_name");	int ownerId = rs1.getInt("owner_id");	String ownerName = username;	if(username.equals("all")){	ResultSet rs2 = huedatabase.getUserNamefromUserId(connection, ownerId).executeQuery();	while (rs2.next()) {	ownerName = rs2.getString("username");	}	}	
udf ownwer name is 

}	}	pojopig.setUrl(url);	pojopig.setFileName(fileName);	pojopig.setUserName(ownerName);	pigArrayList.add(pojopig);	i++;	}	connection.commit();	} catch (SQLException e) {	
SQLException 

pigArrayList.add(pojopig);	i++;	}	connection.commit();	} catch (SQLException e) {	connection.rollback();	} finally {	try {	if (connection != null) connection.close();	} catch (SQLException e) {	
SQLException 

String[] subDirs = dir.split("/");	String dirPath = USER_DIRECTORY;	for(int i=2;i<subDirs.length;i++) {	dirPath += "/"+subDirs[i];	fs.setOwner(new Path(dirPath), username, username);	}	return b;	}	});	} catch (Exception e) {	
exception in webhdfs 

String[] subDirs = dir.split("/");	String dirPath = USER_DIRECTORY;	for(int i=2;i<subDirs.length;i++) {	dirPath += "/"+subDirs[i];	fs.setOwner(new Path(dirPath), username, username);	}	return null;	}	});	} catch (Exception e) {	
webhdfs 

}	in1.close();	out.close();	fileSystemAmbari.setOwner(path, username, username);	fileSystemHue.close();	fileSystemAmbari.close();	return null;	}	});	} catch (Exception e) {	
webhdfs exception 

}	in1.close();	out.close();	fileSystemAmbari.setOwner(path, username, username);	fileSystemHue.close();	fileSystemAmbari.close();	return null;	}	});	} catch (Exception e) {	
webhdfs exception 

========================= ambari sample_1273 =========================

public TimelineMetrics getAppTimelineMetricsFromCache(TimelineAppMetricCacheKey key) throws IllegalArgumentException, IOException {	if (LOG.isDebugEnabled()) {	
fetching metrics with key 

}	if (t instanceof ConnectException) {	throw new ConnectException(t.getMessage());	}	}	}	TimelineMetrics timelineMetrics = new TimelineMetrics();	if (element != null && element.getObjectValue() != null) {	TimelineMetricsCacheValue value = (TimelineMetricsCacheValue) element.getObjectValue();	if (LOG.isDebugEnabled()) {	
returning value from cache 

public Element get(Object key) throws LockTimeoutException {	Element element = this.getQuiet(key);	if (element != null) {	if (LOG.isTraceEnabled()) {	
key 

public Element get(Object key) throws LockTimeoutException {	Element element = this.getQuiet(key);	if (element != null) {	if (LOG.isTraceEnabled()) {	
value 

public Element get(Object key) throws LockTimeoutException {	Element element = this.getQuiet(key);	if (element != null) {	if (LOG.isTraceEnabled()) {	}	TimelineAppMetricCacheKey existingKey = (TimelineAppMetricCacheKey) element.getObjectKey();	
existing temporal info for 

public Element get(Object key) throws LockTimeoutException {	Element element = this.getQuiet(key);	if (element != null) {	if (LOG.isTraceEnabled()) {	}	TimelineAppMetricCacheKey existingKey = (TimelineAppMetricCacheKey) element.getObjectKey();	TimelineAppMetricCacheKey newKey = (TimelineAppMetricCacheKey) key;	existingKey.setTemporalInfo(newKey.getTemporalInfo());	
new temporal info for 

public Element get(Object key) throws LockTimeoutException {	Element element = this.getQuiet(key);	if (element != null) {	if (LOG.isTraceEnabled()) {	}	TimelineAppMetricCacheKey existingKey = (TimelineAppMetricCacheKey) element.getObjectKey();	TimelineAppMetricCacheKey newKey = (TimelineAppMetricCacheKey) key;	existingKey.setTemporalInfo(newKey.getTemporalInfo());	if (existingKey.getSpec() == null || !existingKey.getSpec().equals(newKey.getSpec())) {	existingKey.setSpec(newKey.getSpec());	
new spec for 

========================= ambari sample_3405 =========================

private Command<Void> getCreateCommand(final Map<String, Object> properties, final Map<String, String> requestInfoProps) {	return new Command<Void>() {	public Void invoke() throws AmbariException {	validateParent(properties);	String artifactName = String.valueOf(properties.get(ARTIFACT_NAME_PROPERTY));	TreeMap<String, String> foreignKeyMap = createForeignKeyMap(properties);	if (artifactDAO.findByNameAndForeignKeys(artifactName, foreignKeyMap) != null) {	throw new DuplicateResourceException(String.format( "Attempted to create an artifact which already exists, artifact_name='%s', foreign_keys='%s'", artifactName, getRequestForeignKeys(properties)));	}	
creating artifact resource with name parent information 

========================= ambari sample_3540 =========================

public void run() {	while (!stopRunningThread.get()) {	try {	String fileNameToUpload = fileContextsToUpload.take();	if (POISON_PILL.equals(fileNameToUpload)) {	
found poison pill while waiting for files to upload exiting 

public void run() {	while (!stopRunningThread.get()) {	try {	String fileNameToUpload = fileContextsToUpload.take();	if (POISON_PILL.equals(fileNameToUpload)) {	return;	}	uploadFile(new File(fileNameToUpload), logType);	} catch (InterruptedException e) {	
interrupted while waiting for elements from filecontextstoupload 

protected void uploadFileToS3(String bucketName, String s3Key, File localFile, String accessKey, String secretKey) {	TransferManager transferManager = S3Util.getTransferManager(accessKey, secretKey);	try {	Upload upload = transferManager.upload(bucketName, s3Key, localFile);	upload.waitForUploadResult();	} catch (AmazonClientException | InterruptedException e) {	
uploading failed for file 

========================= ambari sample_1654 =========================

String[] tokens = questionMarkPattern.split(specWithParams, 2);	try {	spec = tokens[0];	params = tokens[1];	} catch (ArrayIndexOutOfBoundsException e) {	LOG.info(e.toString());	}	BufferedReader reader = null;	try {	if (!hostProvider.isCollectorHostLive(clusterName, GANGLIA)) {	
ganglia host is not live 

params = tokens[1];	} catch (ArrayIndexOutOfBoundsException e) {	LOG.info(e.toString());	}	BufferedReader reader = null;	try {	if (!hostProvider.isCollectorHostLive(clusterName, GANGLIA)) {	return Collections.emptySet();	}	if (!hostProvider.isCollectorComponentLive(clusterName, GANGLIA)) {	
ganglia server component is not live 

return Collections.emptySet();	}	}	String feedEnd = reader.readLine();	if (feedEnd == null || feedEnd.isEmpty()) {	LOG.info("Error reading end of feed while getting ganglia metrics " + "for spec => " + spec);	} else {	int endTime = convertToNumber(feedEnd).intValue();	int totalTime = endTime - startTime;	if (LOG.isInfoEnabled() && totalTime > POPULATION_TIME_UPPER_LIMIT) {	
ganglia resource population time 

========================= ambari sample_3393 =========================

public void initializeMetadata() {	if (metricsConf.getBoolean(DISABLE_METRIC_METADATA_MGMT, false)) {	isDisabled = true;	} else {	metricMetadataSync = new TimelineMetricMetadataSync(this);	executorService.scheduleWithFixedDelay(metricMetadataSync, metricsConf.getInt(METRICS_METADATA_SYNC_INIT_DELAY, 120), metricsConf.getInt(METRICS_METADATA_SYNC_SCHEDULE_DELAY, 300), TimeUnit.SECONDS);	try {	Map<TimelineMetricMetadataKey, TimelineMetricMetadata> metadata = getMetadataFromStore();	
retrieved metadata objects from store 

public void initializeMetadata() {	if (metricsConf.getBoolean(DISABLE_METRIC_METADATA_MGMT, false)) {	isDisabled = true;	} else {	metricMetadataSync = new TimelineMetricMetadataSync(this);	executorService.scheduleWithFixedDelay(metricMetadataSync, metricsConf.getInt(METRICS_METADATA_SYNC_INIT_DELAY, 120), metricsConf.getInt(METRICS_METADATA_SYNC_SCHEDULE_DELAY, 300), TimeUnit.SECONDS);	try {	Map<TimelineMetricMetadataKey, TimelineMetricMetadata> metadata = getMetadataFromStore();	METADATA_CACHE.putAll(metadata);	Map<String, Set<String>> hostedAppData = getHostedAppsFromStore();	
retrieved host objects from store 

isDisabled = true;	} else {	metricMetadataSync = new TimelineMetricMetadataSync(this);	executorService.scheduleWithFixedDelay(metricMetadataSync, metricsConf.getInt(METRICS_METADATA_SYNC_INIT_DELAY, 120), metricsConf.getInt(METRICS_METADATA_SYNC_SCHEDULE_DELAY, 300), TimeUnit.SECONDS);	try {	Map<TimelineMetricMetadataKey, TimelineMetricMetadata> metadata = getMetadataFromStore();	METADATA_CACHE.putAll(metadata);	Map<String, Set<String>> hostedAppData = getHostedAppsFromStore();	HOSTED_APPS_MAP.putAll(hostedAppData);	} catch (SQLException e) {	
exception loading metric metadata 

}	TimelineMetricMetadataKey key = new TimelineMetricMetadataKey( metadata.getMetricName(), metadata.getAppId());	TimelineMetricMetadata metadataFromCache = METADATA_CACHE.get(key);	if (metadataFromCache != null) {	try {	if (metadataFromCache.needsToBeSynced(metadata)) {	metadata.setIsPersisted(false);	METADATA_CACHE.put(key, metadata);	}	} catch (MetadataException e) {	
error inserting metadata in cache 

========================= ambari sample_363 =========================

}	} catch (ObjectNotFoundException e) {	}	}	if (oozieServerActiveCount > 0) {	return State.STARTED;	}	return nonStartedState == null ? State.INSTALLED : nonStartedState;	}	} catch (AmbariException e) {	
can t determine service state 

========================= ambari sample_3686 =========================

public boolean updateConfigIfNeeded(SolrPropsConfig solrPropsConfig, SolrZkClient zkClient, File file, String separator, String downloadFolderLocation) throws IOException {	boolean result = false;	if (localSchemaFileHasMoreFields(file, new File(String.format("%s%s%s", downloadFolderLocation, separator, file.getName())))) {	
solr schema file differs update config schema 

List<Node> children = localFileXml.getRoot().getChildren(DYNAMIC_FIELD);	for (Node dynamicFieldNode : children) {	List<Node> attributes = dynamicFieldNode.getAttributes();	Map<String, Object> attributesMap = new HashMap<>();	for (Node attribute : attributes) {	attributesMap.put(attribute.getName(), attribute.getValue());	}	if (attributesMap.get("name") != null && localDynamicFields.contains(attributesMap.get("name").toString())) {	SchemaRequest.AddDynamicField addDynamicFieldRequest = new SchemaRequest.AddDynamicField(attributesMap);	addDynamicFieldRequest.process(cloudSolrClient);	
added dynamic field request sent field name name 

if (fields == null) {	result = true;	} else {	localFields.removeAll(fields);	if (!localFields.isEmpty()) {	result = true;	}	}	}	if (result) {	
found new fields or field types in local schema file 

========================= ambari sample_1341 =========================

public Set<String> checkPropertyIds(Set<String> propertyIds) {	
skipping property id validation for kerberos descriptor resources 

public RequestStatus deleteResources(Request request, Predicate predicate) throws SystemException, UnsupportedPropertyException, NoSuchResourceException, NoSuchParentResourceException {	Set<Resource> setResources = getResources(new RequestImpl(null, null, null, null), predicate);	for (Resource resource : setResources) {	final String kerberosDescriptorName = (String) resource.getPropertyValue(KERBEROS_DESCRIPTOR_NAME_PROPERTY_ID);	
deleting resource with name 

private String getRawKerberosDescriptorFromRequest(Request request) throws UnsupportedPropertyException {	if (request.getRequestInfoProperties() == null || !request.getRequestInfoProperties().containsKey(Request.REQUEST_INFO_BODY_PROPERTY)) {	
could not find the raw request body in the request 

private String getNameFromRequest(Request request) throws UnsupportedPropertyException {	if (request.getProperties() == null || !request.getProperties().iterator().hasNext()) {	
there is no property id in the request 

========================= ambari sample_3517 =========================

int maxPoolSize = m_configuration.getMetricsServiceThreadPoolMaxSize();	m_queueMaximumSize = m_configuration.getMetricsServiceWorkerQueueSize();	int threadPriority = m_configuration.getMetricsServiceThreadPriority();	m_threadPoolExecutor = new ScalingThreadPoolExecutor(corePoolSize, maxPoolSize, 30, TimeUnit.SECONDS, m_queueMaximumSize);	m_threadPoolExecutor.allowCoreThreadTimeOut(true);	m_threadPoolExecutor.setRejectedExecutionHandler(new ThreadPoolExecutor.DiscardOldestPolicy());	ThreadFactory threadFactory = new ThreadFactoryBuilder().setDaemon(true).setNameFormat( "ambari-metrics-retrieval-service-thread-%d").setPriority( threadPriority).setUncaughtExceptionHandler( new MetricRunnableExceptionHandler()).build();	m_threadPoolExecutor.setThreadFactory(threadFactory);	LOG.info( "Initializing the Metrics Retrieval Service with core={}, max={}, workerQueue={}, threadPriority={}", corePoolSize, maxPoolSize, m_queueMaximumSize, threadPriority);	if (ttlCacheEnabled) {	
metrics retrieval service request ttl cache is enabled and set to seconds 

public void submitRequest(MetricSourceType type, StreamProvider streamProvider, String url) {	if (m_queuedUrls.contains(url)) {	return;	}	if (null != m_ttlUrlCache && null != m_ttlUrlCache.getIfPresent(url)) {	return;	}	BlockingQueue<Runnable> queue = m_threadPoolExecutor.getQueue();	int queueSize = queue.size();	if (queueSize > Math.floor(0.9f * m_queueMaximumSize)) {	
the worker queue contains work items and is at of capacity 

int queueSize = queue.size();	if (queueSize > Math.floor(0.9f * m_queueMaximumSize)) {	}	m_queuedUrls.add(url);	Runnable runnable = null;	switch (type) {	case JMX: runnable = new JMXRunnable(m_jmxCache, m_queuedUrls, m_ttlUrlCache, m_jmxObjectReader, streamProvider, url);	break;	case REST: runnable = new RESTRunnable(m_restCache, m_queuedUrls, m_ttlUrlCache, m_gson, streamProvider, url);	break;	
unable to retrieve metrics for the unknown type 

long startTime = 0;	long endTime = 0;	boolean isDebugEnabled = LOG.isDebugEnabled();	if (isDebugEnabled) {	startTime = System.currentTimeMillis();	}	InputStream inputStream = null;	try {	if (isDebugEnabled) {	endTime = System.currentTimeMillis();	
loading metric json from took ms 

try {	if (isDebugEnabled) {	endTime = System.currentTimeMillis();	}	inputStream = m_streamProvider.readFrom(m_url);	processInputStreamAndCacheResult(inputStream);	if (null != m_ttlUrlCache) {	m_ttlUrlCache.put(m_url, m_url);	}	} catch (IOException exception) {	
removing cached values for url 

protected abstract void removeCachedMetricsForCurrentURL();	protected abstract void processInputStreamAndCacheResult(InputStream inputStream) throws Exception;	final void logException(Throwable throwable, String url) {	String cacheKey = buildCacheKey(throwable, url);	if (null == s_exceptionCache.getIfPresent(cacheKey)) {	s_exceptionCache.put(cacheKey, throwable);	
unable to retrieve metrics from subsequent failures will be suppressed from the log for minutes 

public void uncaughtException(Thread t, Throwable e) {	
asynchronous metric retrieval encountered an exception with thread 

========================= ambari sample_3281 =========================

public synchronized Storage getStorage() {	if (storageInstance == null) {	String fileName = context.getProperties().get("dataworker.storagePath");	if (fileName != null) {	
using local storage in to store data 

public synchronized Storage getStorage() {	if (storageInstance == null) {	String fileName = context.getProperties().get("dataworker.storagePath");	if (fileName != null) {	storageInstance = new LocalKeyValueStorage(context);	} else {	
using persistence api to store data 

========================= ambari sample_1245 =========================

private void unschedule(String definitionName, ScheduledFuture<?> scheduledFuture) {	m_futureMap.remove(definitionName);	if (null != scheduledFuture) {	scheduledFuture.cancel(true);	
unscheduled server alert 

if (!entity.getEnabled()) {	return;	}	AlertDefinition definition = m_alertDefinitionFactory.coerce(entity);	ServerSource serverSource = (ServerSource) definition.getSource();	String sourceClass = serverSource.getSourceClass();	int interval = definition.getInterval();	try {	Class<?> clazz = Class.forName(sourceClass);	if (!AlertRunnable.class.isAssignableFrom(clazz)) {	
unable to schedule a server side alert for because it is not an 

if (!AlertRunnable.class.isAssignableFrom(clazz)) {	return;	}	Constructor<? extends AlertRunnable> constructor = clazz.asSubclass( AlertRunnable.class).getConstructor(String.class);	AlertRunnable alertRunnable = constructor.newInstance(entity.getDefinitionName());	m_injector.injectMembers(alertRunnable);	ScheduledFuture<?> scheduledFuture = m_scheduledExecutorService.scheduleWithFixedDelay( alertRunnable, interval, interval, TimeUnit.MINUTES);	String definitionName = entity.getDefinitionName();	ScheduledAlert scheduledAlert = new ScheduledAlert(scheduledFuture, interval);	m_futureMap.put(definitionName, scheduledAlert);	
scheduled server alert to run every minutes 

return;	}	Constructor<? extends AlertRunnable> constructor = clazz.asSubclass( AlertRunnable.class).getConstructor(String.class);	AlertRunnable alertRunnable = constructor.newInstance(entity.getDefinitionName());	m_injector.injectMembers(alertRunnable);	ScheduledFuture<?> scheduledFuture = m_scheduledExecutorService.scheduleWithFixedDelay( alertRunnable, interval, interval, TimeUnit.MINUTES);	String definitionName = entity.getDefinitionName();	ScheduledAlert scheduledAlert = new ScheduledAlert(scheduledFuture, interval);	m_futureMap.put(definitionName, scheduledAlert);	} catch (ClassNotFoundException cnfe) {	
unable to schedule a server side alert for because it could not be found in the classpath 

}	Constructor<? extends AlertRunnable> constructor = clazz.asSubclass( AlertRunnable.class).getConstructor(String.class);	AlertRunnable alertRunnable = constructor.newInstance(entity.getDefinitionName());	m_injector.injectMembers(alertRunnable);	ScheduledFuture<?> scheduledFuture = m_scheduledExecutorService.scheduleWithFixedDelay( alertRunnable, interval, interval, TimeUnit.MINUTES);	String definitionName = entity.getDefinitionName();	ScheduledAlert scheduledAlert = new ScheduledAlert(scheduledFuture, interval);	m_futureMap.put(definitionName, scheduledAlert);	} catch (ClassNotFoundException cnfe) {	} catch (NoSuchMethodException nsme) {	
unable to schedule a server side alert for because it does not have a constructor which takes the proper arguments 

Constructor<? extends AlertRunnable> constructor = clazz.asSubclass( AlertRunnable.class).getConstructor(String.class);	AlertRunnable alertRunnable = constructor.newInstance(entity.getDefinitionName());	m_injector.injectMembers(alertRunnable);	ScheduledFuture<?> scheduledFuture = m_scheduledExecutorService.scheduleWithFixedDelay( alertRunnable, interval, interval, TimeUnit.MINUTES);	String definitionName = entity.getDefinitionName();	ScheduledAlert scheduledAlert = new ScheduledAlert(scheduledFuture, interval);	m_futureMap.put(definitionName, scheduledAlert);	} catch (ClassNotFoundException cnfe) {	} catch (NoSuchMethodException nsme) {	} catch (InvocationTargetException ite) {	
unable to schedule a server side alert for because an exception occurred while constructing the instance 

========================= ambari sample_3278 =========================

final Cluster cluster = clustersProvider.get().getCluster(clusterName);	long clusterId = cluster.getClusterId();	Map<String, Long> lastServiceConfigUpdates = new HashMap<>();	for (Service service : cluster.getServices().values()) {	if (service.getMaintenanceState() != MaintenanceState.OFF || !hasAtLeastOneComponentVersionAdvertised(service)) {	continue;	}	StackId stackId = service.getDesiredStackId();	boolean isServiceWitNoConfigs = ambariMetaInfo.get().isServiceWithNoConfigs(stackId.getStackName(), stackId.getStackVersion(), service.getName());	if (isServiceWitNoConfigs){	
s in s version s does not have customizable configurations skip checking service configuration history 

long clusterId = cluster.getClusterId();	Map<String, Long> lastServiceConfigUpdates = new HashMap<>();	for (Service service : cluster.getServices().values()) {	if (service.getMaintenanceState() != MaintenanceState.OFF || !hasAtLeastOneComponentVersionAdvertised(service)) {	continue;	}	StackId stackId = service.getDesiredStackId();	boolean isServiceWitNoConfigs = ambariMetaInfo.get().isServiceWithNoConfigs(stackId.getStackName(), stackId.getStackVersion(), service.getName());	if (isServiceWitNoConfigs){	} else {	
s in s version s has customizable configurations check service configuration history 

Map<String, Long> lastServiceChecksByRole = new HashMap<>();	for( LastServiceCheckDTO lastServiceCheck : lastServiceChecks ) {	lastServiceChecksByRole.put(lastServiceCheck.role, lastServiceCheck.endTime);	}	LinkedHashSet<String> failedServiceNames = new LinkedHashSet<>();	for( Entry<String, Long> entry : lastServiceConfigUpdates.entrySet() ) {	String serviceName = entry.getKey();	long configCreationTime = entry.getValue();	String role = actionMetadataProvider.get().getServiceCheckAction(serviceName);	if(!lastServiceChecksByRole.containsKey(role) ) {	
there was no service check found for service matching role 

String serviceName = entry.getKey();	long configCreationTime = entry.getValue();	String role = actionMetadataProvider.get().getServiceCheckAction(serviceName);	if(!lastServiceChecksByRole.containsKey(role) ) {	failedServiceNames.add(serviceName);	continue;	}	long lastServiceCheckTime = lastServiceChecksByRole.get(role);	if (lastServiceCheckTime < configCreationTime) {	failedServiceNames.add(serviceName);	
the service role had its configurations updated on but the last service check was 

========================= ambari sample_2890 =========================

public void handle(String target, Request baseRequest, HttpServletRequest request, HttpServletResponse response) throws IOException, ServletException {	ViewEntity viewEntity = getTargetView(target);	if (viewEntity == null) {	processHandlers(target, baseRequest, request, response);	} else {	ClassLoader contextClassLoader = Thread.currentThread().getContextClassLoader();	try {	ClassLoader viewClassLoader = viewEntity.getClassLoader();	if (viewClassLoader == null) {	
no class loader associated with view 

========================= ambari sample_3782 =========================

public static boolean copyFromLocal(String sourceFilepath, String destFilePath, FileSystem fileSystem, boolean overwrite, boolean delSrc) {	Path src = new Path(sourceFilepath);	Path dst = new Path(destFilePath);	boolean isCopied = false;	try {	LOG.info("copying localfile := " + sourceFilepath + " to hdfsPath := " + destFilePath);	fileSystem.copyFromLocalFile(delSrc, overwrite, src, dst);	isCopied = true;	} catch (Exception e) {	
error copying local file to hdfs location 

public static FileSystem buildFileSystem(String hdfsHost, String hdfsPort) {	try {	Configuration configuration = buildHdfsConfiguration(hdfsHost, hdfsPort);	FileSystem fs = FileSystem.get(configuration);	return fs;	} catch (Exception e) {	
exception is buildfilesystem 

========================= ambari sample_1615 =========================

public void init(SubsetConfiguration conf) {	this.conf = conf;	
initializing timeline metrics sink 

instanceId = conf.getString(INSTANCE_ID_PROPERTY, null);	setInstanceId = conf.getBoolean(SET_INSTANCE_ID_PROPERTY, false);	LOG.info("Identified hostname = " + hostName + ", serviceName = " + serviceName);	super.init();	protocol = conf.getString(COLLECTOR_PROTOCOL, "http");	collectorHosts = parseHostsStringArrayIntoCollection(conf.getStringArray(COLLECTOR_HOSTS_PROPERTY));	port = conf.getString(COLLECTOR_PORT, "6188");	hostInMemoryAggregationEnabled = conf.getBoolean(HOST_IN_MEMORY_AGGREGATION_ENABLED_PROPERTY);	hostInMemoryAggregationPort = conf.getInt(HOST_IN_MEMORY_AGGREGATION_PORT_PROPERTY);	if (collectorHosts.isEmpty()) {	
no metric collector configured 

if (protocol.contains("https")) {	String trustStorePath = conf.getString(SSL_KEYSTORE_PATH_PROPERTY).trim();	String trustStoreType = conf.getString(SSL_KEYSTORE_TYPE_PROPERTY).trim();	String trustStorePwd = conf.getString(SSL_KEYSTORE_PASSWORD_PROPERTY).trim();	loadTruststore(trustStorePath, trustStoreType, trustStorePwd);	}	String preferredCollectorHost = findPreferredCollectHost();	collectorUri = constructTimelineMetricUri(protocol, preferredCollectorHost, port);	containerMetricsUri = constructContainerMetricUri(protocol, preferredCollectorHost, port);	if (StringUtils.isNotEmpty(preferredCollectorHost)) {	
collector uri 

if (protocol.contains("https")) {	String trustStorePath = conf.getString(SSL_KEYSTORE_PATH_PROPERTY).trim();	String trustStoreType = conf.getString(SSL_KEYSTORE_TYPE_PROPERTY).trim();	String trustStorePwd = conf.getString(SSL_KEYSTORE_PASSWORD_PROPERTY).trim();	loadTruststore(trustStorePath, trustStoreType, trustStorePwd);	}	String preferredCollectorHost = findPreferredCollectHost();	collectorUri = constructTimelineMetricUri(protocol, preferredCollectorHost, port);	containerMetricsUri = constructContainerMetricUri(protocol, preferredCollectorHost, port);	if (StringUtils.isNotEmpty(preferredCollectorHost)) {	
container metrics uri 

String trustStorePath = conf.getString(SSL_KEYSTORE_PATH_PROPERTY).trim();	String trustStoreType = conf.getString(SSL_KEYSTORE_TYPE_PROPERTY).trim();	String trustStorePwd = conf.getString(SSL_KEYSTORE_PASSWORD_PROPERTY).trim();	loadTruststore(trustStorePath, trustStoreType, trustStorePwd);	}	String preferredCollectorHost = findPreferredCollectHost();	collectorUri = constructTimelineMetricUri(protocol, preferredCollectorHost, port);	containerMetricsUri = constructContainerMetricUri(protocol, preferredCollectorHost, port);	if (StringUtils.isNotEmpty(preferredCollectorHost)) {	} else {	
no suitable collector found 

}	if (propertyName.startsWith(RPC_METRIC_PREFIX)) {	int beginIdx = RPC_METRIC_PREFIX.length() + 1;	String suffixStr = propertyName.substring(beginIdx);	String configPrefix = suffixStr.substring(0, suffixStr.indexOf("."));	rpcPortSuffixes.put(conf.getString(propertyName).trim(), configPrefix.trim());	}	}	}	if (!rpcPortSuffixes.isEmpty()) {	
rpc port properties configured 

metricList.add(cachedMetric);	}	sb.setLength(sbBaseLen);	}	TimelineMetrics timelineMetrics = new TimelineMetrics();	timelineMetrics.setMetrics(metricList);	if (!metricList.isEmpty()) {	emitMetrics(timelineMetrics);	}	} catch (UnableToConnectException uce) {	
unable to send metrics to collector by address 

containerMetric.setContainerId(tag.value());	}	}	parseContainerMetrics(record, containerMetric);	List<ContainerMetric> list = new ArrayList<>();	list.add(containerMetric);	String jsonData = null;	try {	jsonData = mapper.writeValueAsString(list);	} catch (IOException e) {	
unable to parse container metrics 

public void close() throws IOException {	executorService.submit(new Runnable() {	public void run() {	
closing hadooptimelinemetricsink flushing metrics to collector 

========================= ambari sample_267 =========================

public LocalAmbariServer() {}	public void run(){	try {	startServer();	}	catch (Exception ex) {	
exception received 

private void startServer() throws Exception {	try {	
attempting to start ambari server 

private void startServer() throws Exception {	try {	AmbariServer.setupProxyAuth();	ambariServer = injector.getInstance(AmbariServer.class);	ambariServer.initViewRegistry();	ambariServer.run();	} catch (InterruptedException ex) {	LOG.info(ex);	} catch (Throwable t) {	
failed to run the ambari server 

public void stopServer() throws Exception {	
stopping ambari server 

========================= ambari sample_47 =========================

public int compareDeps(RoleCommandOrder rco) {	Set<RoleCommandPair> v1;	Set<RoleCommandPair> v2;	if (this == rco) {	return 0;	}	if (!dependencies.keySet().equals(rco.dependencies.keySet())){	
dependency keysets differ 

public int compareDeps(RoleCommandOrder rco) {	Set<RoleCommandPair> v1;	Set<RoleCommandPair> v2;	if (this == rco) {	return 0;	}	if (!dependencies.keySet().equals(rco.dependencies.keySet())){	return 1;	}	
dependency keysets match 

if (this == rco) {	return 0;	}	if (!dependencies.keySet().equals(rco.dependencies.keySet())){	return 1;	}	for (Map.Entry<RoleCommandPair, Set<RoleCommandPair>> roleCommandPairSetEntry : dependencies.entrySet()) {	v1 = dependencies.get(roleCommandPairSetEntry.getKey());	v2 = rco.dependencies.get(roleCommandPairSetEntry.getKey());	if (!v1.equals(v2)) {	
different entry found for key 

if (!dependencies.keySet().equals(rco.dependencies.keySet())){	return 1;	}	for (Map.Entry<RoleCommandPair, Set<RoleCommandPair>> roleCommandPairSetEntry : dependencies.entrySet()) {	v1 = dependencies.get(roleCommandPairSetEntry.getKey());	v2 = rco.dependencies.get(roleCommandPairSetEntry.getKey());	if (!v1.equals(v2)) {	return 1;	}	}	
dependency entries match 

========================= ambari sample_4552 =========================

public void sendApiQuery(@Named("apiQuery") String apiQuery) {	JerseyClient jerseyClient = JerseyClientBuilder.createClient();	HttpAuthenticationFeature authFeature = HttpAuthenticationFeature.basicBuilder() .credentials("admin", "admin") .build();	jerseyClient.register(authFeature);	String logsearchUrl = String.format("http: StoryDataRegistry.INSTANCE.getDockerHost(), StoryDataRegistry.INSTANCE.getLogsearchPort(), apiQuery);	
url 

========================= ambari sample_1584 =========================

logicalTaskMap.put(startTask, new HashMap<>());	} else {	LOG.info("Skipping Start task creation since provision action = " + topology.getProvisionAction());	}	HostGroup hostGroup = getHostGroup();	Collection<String> startOnlyComponents = hostGroup.getComponentNames(START_ONLY);	Collection<String> installOnlyComponents = hostGroup.getComponentNames(INSTALL_ONLY);	Collection<String> installAndStartComponents = hostGroup.getComponentNames(INSTALL_AND_START);	for (String component : hostGroup.getComponentNames()) {	if (component == null || component.equals("AMBARI_SERVER")) {	
skipping component when creating request 

Collection<String> installOnlyComponents = hostGroup.getComponentNames(INSTALL_ONLY);	Collection<String> installAndStartComponents = hostGroup.getComponentNames(INSTALL_AND_START);	for (String component : hostGroup.getComponentNames()) {	if (component == null || component.equals("AMBARI_SERVER")) {	continue;	}	String hostName = getHostName() != null ? getHostName() : "PENDING HOST ASSIGNMENT : HOSTGROUP=" + getHostgroupName();	AmbariContext context = topology.getAmbariContext();	Stack stack = hostGroup.getStack();	if (startOnlyComponents.contains(component) || (skipInstallTaskCreate && !installOnlyComponents.contains(component) && !installAndStartComponents.contains(component)) && stack != null && !stack.getComponentInfo(component).isClient()) {	
skipping create of install task for on 

String hostName = getHostName() != null ? getHostName() : "PENDING HOST ASSIGNMENT : HOSTGROUP=" + getHostgroupName();	AmbariContext context = topology.getAmbariContext();	Stack stack = hostGroup.getStack();	if (startOnlyComponents.contains(component) || (skipInstallTaskCreate && !installOnlyComponents.contains(component) && !installAndStartComponents.contains(component)) && stack != null && !stack.getComponentInfo(component).isClient()) {	} else {	HostRoleCommand logicalInstallTask = context.createAmbariTask( getRequestId(), id, component, hostName, AmbariContext.TaskType.INSTALL, skipFailure);	logicalTasks.put(logicalInstallTask.getTaskId(), logicalInstallTask);	logicalTaskMap.get(installTask).put(component, logicalInstallTask.getTaskId());	}	if (installOnlyComponents.contains(component) || skipStartTaskCreate || (stack != null && stack.getComponentInfo(component).isClient())) {	
skipping create of start task for on 

private Predicate toPredicate(String predicate) {	Predicate compiledPredicate = null;	try {	if (predicate != null && ! predicate.isEmpty()) {	compiledPredicate = predicateCompiler.compile(predicate);	}	} catch (InvalidQueryException e) {	
unable to compile predicate for host request 

========================= ambari sample_2652 =========================

try {	DynamicEntity dynamicEntity = persistEntity(entity, em, new HashSet<>());	em.getTransaction().commit();	Map<String, Object> props = getEntityProperties(entity);	List<String> keys = new ArrayList<>(props.keySet());	for( String key : keys){	String attribute = getAttributeName(key);	try {	props.put(key, dynamicEntity.get(attribute));	}catch(DynamicException de){	
error occurred while copying entity property 

========================= ambari sample_3940 =========================

public LinkedHashMap<String, String> next() {	LinkedHashMap<String, String> currObject = nextObject;	try {	nextObject = readNextObject(this.reader);	} catch (EndOfDocumentException e) {	
end of json document reached with next character ending the json array 

public void remove() {	
no operation when remove called on jsoniterator 

========================= ambari sample_549 =========================

public static Set<Class<?>> findOnClassPath(String packageName, List<Class<?>> exclusions, List<Class<?>> selectors) {	Set<Class<?>> bindingSet = new LinkedHashSet<>();	try {	ClassPath classpath = ClassPath.from(ClasspathScannerUtils.class.getClassLoader());	
checking package for binding candidates 

public static Set<Class<?>> findOnClassPath(String packageName, List<Class<?>> exclusions, List<Class<?>> selectors) {	Set<Class<?>> bindingSet = new LinkedHashSet<>();	try {	ClassPath classpath = ClassPath.from(ClasspathScannerUtils.class.getClassLoader());	for (ClassPath.ClassInfo classInfo : classpath.getTopLevelClassesRecursive(packageName)) {	Class<?> candidate = classInfo.load();	if (exclusions.contains(candidate)) {	
candidate is excluded excluded 

public static Set<Class<?>> findOnClassPath(String packageName, List<Class<?>> exclusions, List<Class<?>> selectors) {	Set<Class<?>> bindingSet = new LinkedHashSet<>();	try {	ClassPath classpath = ClassPath.from(ClasspathScannerUtils.class.getClassLoader());	for (ClassPath.ClassInfo classInfo : classpath.getTopLevelClassesRecursive(packageName)) {	Class<?> candidate = classInfo.load();	if (exclusions.contains(candidate)) {	continue;	}	if (isEligible(candidate, selectors)) {	
found class 

try {	ClassPath classpath = ClassPath.from(ClasspathScannerUtils.class.getClassLoader());	for (ClassPath.ClassInfo classInfo : classpath.getTopLevelClassesRecursive(packageName)) {	Class<?> candidate = classInfo.load();	if (exclusions.contains(candidate)) {	continue;	}	if (isEligible(candidate, selectors)) {	bindingSet.add(candidate);	} else {	
candidate doesn t match 

Class<?> candidate = classInfo.load();	if (exclusions.contains(candidate)) {	continue;	}	if (isEligible(candidate, selectors)) {	bindingSet.add(candidate);	} else {	}	}	} catch (IOException e) {	
failure during configuring juice bindings 

private static boolean checkAnnotations(Class<?> candidate, List<Class<?>> selectors) {	
checking annotations for 

private static boolean checkSubClasses(Class<?> candidate, List<Class<?>> selectors) {	boolean ret = false;	
checking interfaces for 

private static boolean checkSubClasses(Class<?> candidate, List<Class<?>> selectors) {	boolean ret = false;	List interfaces = ClassUtils.getAllInterfaces(candidate);	for (Class selectorItf : selectors) {	if (interfaces.contains(selectorItf)) {	
checking candidate for subclassing interface 

========================= ambari sample_3805 =========================

public void updateStatus(Status status) {	setStatus(status);	if (isPersisted) {	requestScheduleEntity.setUpdateTimestamp(System.currentTimeMillis());	requestScheduleDAO.merge(requestScheduleEntity);	} else {	
updated status in memory since request schedule is not persisted 

========================= ambari sample_3106 =========================

public String getPassword() {	try {	return masker.unmask(password);	} catch (MaskException e) {	
unable to unmask password for remote cluster 

========================= ambari sample_4440 =========================

public String toString() {	try {	return StageUtils.jaxbToString(this);	} catch (Exception ex) {	
exception in json conversion 

========================= ambari sample_2982 =========================

public static boolean isHostComponentLive(AmbariManagementController managementController, String clusterName, String hostName, String serviceName, String componentName) {	if (clusterName == null) {	return false;	}	ServiceComponentHostResponse componentHostResponse;	try {	ServiceComponentHostRequest componentRequest = new ServiceComponentHostRequest(clusterName, serviceName, componentName, hostName, null);	Set<ServiceComponentHostResponse> hostComponents = managementController.getHostComponents(Collections.singleton(componentRequest));	componentHostResponse = hostComponents.size() == 1 ? hostComponents.iterator().next() : null;	} catch (AmbariException e) {	
error checking server host component state 

public static boolean isHostLive(AmbariManagementController managementController, String clusterName, String hostName) {	if (clusterName == null) {	return false;	}	HostResponse hostResponse;	try {	HostRequest hostRequest = new HostRequest(hostName, clusterName);	Set<HostResponse> hosts = HostResourceProvider.getHosts(managementController, hostRequest, null);	hostResponse = hosts.size() == 1 ? hosts.iterator().next() : null;	} catch (AmbariException e) {	
error while checking host live status 

========================= ambari sample_3487 =========================

public Response getServiceCheckList(){	ServiceCheck serviceCheck = new ServiceCheck(context);	try {	ServiceCheck.Policy policy = serviceCheck.getServiceCheckPolicy();	JSONObject policyJson = new JSONObject();	policyJson.put("serviceCheckPolicy", policy);	return Response.ok(policyJson).build();	} catch (HdfsApiException e) {	
error occurred while generating service check policy 

========================= ambari sample_924 =========================

public Response readLatestConfiguration() {	
reading all configurations 

public Response readLatestConfiguration() {	Response response = null;	try {	String versionTag = getVersionTag();	JSONObject configurations = getConfigurationFromAmbari(versionTag);	response = Response.ok(configurations).build();	} catch (WebApplicationException ex) {	
error occurred 

public Response readLatestConfiguration() {	Response response = null;	try {	String versionTag = getVersionTag();	JSONObject configurations = getConfigurationFromAmbari(versionTag);	response = Response.ok(configurations).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (Exception ex) {	
error occurred 

public Response readClusterInfo() {	
reading cluster info 

public Response readClusterInfo() {	Response response = null;	try {	JSONObject configurations = readFromCluster("?fields=Clusters/version");	response = Response.ok(configurations).build();	} catch (AmbariHttpException ex) {	
error occurred 

try {	JSONObject configurations = readFromCluster("?fields=Clusters/version");	response = Response.ok(configurations).build();	} catch (AmbariHttpException ex) {	if (ex.getResponseCode() == 403) {	throw new ServiceFormattedException("You do not have permission to view Capacity Scheduler configuration. Contact your Cluster administrator", ex);	} else {	throw new ServiceFormattedException(ex.getMessage(), ex);	}	} catch (WebApplicationException ex) {	
error occurred 

response = Response.ok(configurations).build();	} catch (AmbariHttpException ex) {	if (ex.getResponseCode() == 403) {	throw new ServiceFormattedException("You do not have permission to view Capacity Scheduler configuration. Contact your Cluster administrator", ex);	} else {	throw new ServiceFormattedException(ex.getMessage(), ex);	}	} catch (WebApplicationException ex) {	throw ex;	} catch (Exception ex) {	
error occurred 

public Response readAllConfigurations() {	
reading all configurations 

public Response readAllConfigurations() {	Response response = null;	try {	JSONObject responseJSON = readFromCluster(CONFIGURATION_URL);	response = Response.ok( responseJSON ).build();	} catch (WebApplicationException ex) {	
error occurred 

public Response readAllConfigurations() {	Response response = null;	try {	JSONObject responseJSON = readFromCluster(CONFIGURATION_URL);	response = Response.ok( responseJSON ).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (Exception ex) {	
error occurred 

public Response readConfigurationByTag(@PathParam("tag") String tag) {	
reading configurations for tag 

public Response readConfigurationByTag(@PathParam("tag") String tag) {	Response response = null;	try {	JSONObject configurations = getConfigurationFromAmbari(tag);	response = Response.ok(configurations).build();	} catch (WebApplicationException ex) {	
exception occurred 

public Response readConfigurationByTag(@PathParam("tag") String tag) {	Response response = null;	try {	JSONObject configurations = getConfigurationFromAmbari(tag);	response = Response.ok(configurations).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (Exception ex) {	
exception occurred 

public Response getPrivilege() {	
reading privilege 

public Response getPrivilege() {	Response response = null;	try {	boolean   operator = isOperator();	response = Response.ok(operator).build();	} catch (WebApplicationException ex) {	
exception occurred 

public Response getPrivilege() {	Response response = null;	try {	boolean   operator = isOperator();	response = Response.ok(operator).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (Exception ex) {	
exception occurred 

public Response getNodeLabels() {	
reading nodelables 

public Response getNodeLabels() {	Response response;	try {	String url = String.format(RM_GET_NODE_LABEL_URL, getRMUrl());	InputStream rmResponse = context.getURLStreamProvider().readFrom( url, "GET", (String) null, new HashMap<String, String>());	String nodeLabels = IOUtils.toString(rmResponse);	response = Response.ok(nodeLabels).build();	} catch (ConnectException ex) {	
exception occurred 

public Response getNodeLabels() {	Response response;	try {	String url = String.format(RM_GET_NODE_LABEL_URL, getRMUrl());	InputStream rmResponse = context.getURLStreamProvider().readFrom( url, "GET", (String) null, new HashMap<String, String>());	String nodeLabels = IOUtils.toString(rmResponse);	response = Response.ok(nodeLabels).build();	} catch (ConnectException ex) {	throw new ServiceFormattedException("Connection to Resource Manager refused", ex);	} catch (WebApplicationException ex) {	
exception occurred 

try {	String url = String.format(RM_GET_NODE_LABEL_URL, getRMUrl());	InputStream rmResponse = context.getURLStreamProvider().readFrom( url, "GET", (String) null, new HashMap<String, String>());	String nodeLabels = IOUtils.toString(rmResponse);	response = Response.ok(nodeLabels).build();	} catch (ConnectException ex) {	throw new ServiceFormattedException("Connection to Resource Manager refused", ex);	} catch (WebApplicationException ex) {	throw ex;	} catch (Exception ex) {	
exception occurred 

try {	String response = ambariApi.readFromAmbari(url, "GET", null, null);	if(response != null && !response.isEmpty()){	JSONObject json = (JSONObject) JSONValue.parse(response);	if (json.containsKey("privileges")) {	JSONArray privileges = (JSONArray) json.get("privileges");	if(privileges.size() > 0) return true;	}	}	} catch (AmbariHttpException e) {	
got error response from url response 

public Response writeConfiguration(String requestBody) {	
writeconfiguration for request 

public Response writeConfiguration(String requestBody) {	JSONObject response;	try {	if (isOperator() == false) {	
returning as not an operator 

JSONObject response;	try {	if (isOperator() == false) {	return Response.status(401).build();	}	Map<String, String> headers = new HashMap<String, String>();	headers.put("Content-Type", "application/x-www-form-urlencoded");	String responseString = ambariApi.requestClusterAPI("", "PUT",requestBody, headers);	response = getJsonObject(responseString);	} catch (WebApplicationException ex) {	
exception occurred 

if (isOperator() == false) {	return Response.status(401).build();	}	Map<String, String> headers = new HashMap<String, String>();	headers.put("Content-Type", "application/x-www-form-urlencoded");	String responseString = ambariApi.requestClusterAPI("", "PUT",requestBody, headers);	response = getJsonObject(responseString);	} catch (WebApplicationException ex) {	throw ex;	} catch (Exception ex) {	
exception occurred 

public Response writeAndRefreshConfiguration(JSONObject request) {	
writeandrefreshconfiguration for request 

public Response writeAndRefreshConfiguration(JSONObject request) {	try {	if (isOperator() == false) {	
throwing error as not an operator 

try {	if (isOperator() == false) {	return Response.status(401).build();	}	String rmHosts = getRMHosts();	JSONObject data = getJsonObject(String.format(REFRESH_RM_REQUEST_DATA, rmHosts));	Map<String, String> headers = new HashMap<String, String>();	headers.put("Content-Type", "application/x-www-form-urlencoded");	ambariApi.requestClusterAPI("requests/", "POST", data.toJSONString(), headers);	} catch (WebApplicationException ex) {	
exception occurred 

return Response.status(401).build();	}	String rmHosts = getRMHosts();	JSONObject data = getJsonObject(String.format(REFRESH_RM_REQUEST_DATA, rmHosts));	Map<String, String> headers = new HashMap<String, String>();	headers.put("Content-Type", "application/x-www-form-urlencoded");	ambariApi.requestClusterAPI("requests/", "POST", data.toJSONString(), headers);	} catch (WebApplicationException ex) {	throw ex;	} catch (Exception ex) {	
exception occurred 

public Response writeAndRestartConfiguration(JSONObject request) {	
writeandrestartconfiguration for request 

public Response writeAndRestartConfiguration(JSONObject request) {	try {	if (isOperator() == false) {	
throwing error as not an operator 

try {	if (isOperator() == false) {	return Response.status(401).build();	}	String rmHosts = getRMHosts();	JSONObject data = getJsonObject(String.format(RESTART_RM_REQUEST_DATA, context.getCluster().getName(), rmHosts, rmHosts));	Map<String, String> headers = new HashMap<String, String>();	headers.put("Content-Type", "application/x-www-form-urlencoded");	ambariApi.requestClusterAPI("requests/", "POST", data.toJSONString(), headers);	} catch (WebApplicationException ex) {	
exception occured 

return Response.status(401).build();	}	String rmHosts = getRMHosts();	JSONObject data = getJsonObject(String.format(RESTART_RM_REQUEST_DATA, context.getCluster().getName(), rmHosts, rmHosts));	Map<String, String> headers = new HashMap<String, String>();	headers.put("Content-Type", "application/x-www-form-urlencoded");	ambariApi.requestClusterAPI("requests/", "POST", data.toJSONString(), headers);	} catch (WebApplicationException ex) {	throw ex;	} catch (Exception ex) {	
exception occured 

public Response getConfigurationValue(@QueryParam("siteName") String siteName,@QueryParam("configName") String configName){	
get configuration value for sitename configname 

JSONObject res = new JSONObject();	JSONArray arr = new JSONArray();	JSONObject conf = new JSONObject();	conf.put("siteName",siteName);	conf.put("configName", configName);	conf.put("configValue", configValue);	arr.add(conf);	res.put("configs" ,arr);	return Response.ok(res).build();	} catch (WebApplicationException ex) {	
exception occurred 

JSONObject conf = new JSONObject();	conf.put("siteName",siteName);	conf.put("configName", configName);	conf.put("configValue", configValue);	arr.add(conf);	res.put("configs" ,arr);	return Response.ok(res).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (Exception ex) {	
exception occurred 

========================= ambari sample_1140 =========================

}	}	String clusterName = (String) resource.getPropertyValue(clusterNamePropertyId);	String protocol = jmxHostProvider.getJMXProtocol(clusterName, componentName);	boolean httpsEnabled = false;	if (protocol.equals("https")){	httpsEnabled = true;	}	Set<String> hostNames = getHosts(resource, clusterName, componentName);	if (hostNames == null || hostNames.isEmpty()) {	
unable to get jmx metrics no host name for 

Set<String> hostNames = getHosts(resource, clusterName, componentName);	if (hostNames == null || hostNames.isEmpty()) {	return resource;	}	String spec = null;	for (String hostName : hostNames) {	try {	String port = getPort(clusterName, componentName, hostName, httpsEnabled);	String publicHostName = jmxHostProvider.getPublicHostName(clusterName, hostName);	if (port == null) {	
unable to get jmx metrics no port value for 

========================= ambari sample_3468 =========================

Element rootNode = doc.getRootElement();	Element record = new Element("RevertRecord");	record.setAttribute(new Attribute("id", iteration));	record.addContent(new Element("datetime").setText(currentDate.toString()));	record.addContent(new Element("dirname").setText(dirname));	record.addContent(new Element("instance").setText(instance));	record.addContent(new Element("query").setText(content));	rootNode.addContent(record);	xmlOutput.output(doc, new FileWriter(ConfigurationCheckImplementation.getHomeDir() + "RevertChangesService.xml"));	} catch (JDOMException e) {	
JDOMException 

doc.setRootElement(revertrecord);	Element record = new Element("RevertRecord");	record.setAttribute(new Attribute("id", iteration));	record.addContent(new Element("datetime").setText(currentDate.toString()));	record.addContent(new Element("dirname").setText(dirname));	record.addContent(new Element("instance").setText(instance));	record.addContent(new Element("query").setText(content));	doc.getRootElement().addContent(record);	xmlOutput.output(doc, new FileWriter(ConfigurationCheckImplementation.getHomeDir() + "RevertChangesService.xml"));	} catch (IOException io) {	
JDOMException 

public void insertRowinAmbaridb(String dirname, int maxcount, long epochtime, Connection c, int id, String instance, int i, QuerySetAmbariDB ambaridatabase, String versionName, String username, String jobStatus) throws SQLException, IOException {	String maxcount1 = Integer.toString(maxcount);	String epochtime1 = Long.toString(epochtime);	PreparedStatement prSt = null;	String revsql = null;	if (versionName.contains("1.0")) {	prSt = ambaridatabase.insertToHiveHistoryForHive(c, id, maxcount1, epochtime, dirname, username, jobStatus);	} else {	prSt = ambaridatabase.insertToHiveHistoryForHiveNext(c, id, maxcount1, epochtime, dirname, username, jobStatus);	}	
the actual insert statement is 

String epochtime1 = Long.toString(epochtime);	PreparedStatement prSt = null;	String revsql = null;	if (versionName.contains("1.0")) {	prSt = ambaridatabase.insertToHiveHistoryForHive(c, id, maxcount1, epochtime, dirname, username, jobStatus);	} else {	prSt = ambaridatabase.insertToHiveHistoryForHiveNext(c, id, maxcount1, epochtime, dirname, username, jobStatus);	}	prSt.executeUpdate();	revsql = ambaridatabase.revertSql(id, maxcount1);	
adding revert sql hive history 

public void updateSequenceno(Connection c, int seqNo, int id, QuerySetAmbariDB ambaridatabase) throws SQLException, IOException {	PreparedStatement prSt;	prSt = ambaridatabase.updateSequenceNoInAmbariSequence(c, seqNo, id);	
the actual insert statement is 

public void updateSequenceno(Connection c, int seqNo, int id, QuerySetAmbariDB ambaridatabase) throws SQLException, IOException {	PreparedStatement prSt;	prSt = ambaridatabase.updateSequenceNoInAmbariSequence(c, seqNo, id);	prSt.executeUpdate();	
adding revert sql hive history 

hiveArrayList.add(hivepojo);	i++;	}	connection.commit();	} catch (SQLException e) {	connection.rollback();	} finally {	try {	if (connection != null) connection.close();	} catch (SQLException e) {	
sql exception error 

try {	File file = new File(homedir + "query.hql");	if (!file.exists()) {	file.createNewFile();	}	FileWriter fw = new FileWriter(file.getAbsoluteFile());	BufferedWriter bw = new BufferedWriter(fw);	bw.write(content);	bw.close();	} catch (IOException e) {	
IOException 

public void deleteFileQueryhql(String homedir) {	try {	File file = new File(homedir + "query.hql");	if (file.delete()) {	
temporary hql file deleted 

public void deleteFileQueryhql(String homedir) {	try {	File file = new File(homedir + "query.hql");	if (file.delete()) {	} else {	
temporary hql file delete failed 

public void deleteFileQueryhql(String homedir) {	try {	File file = new File(homedir + "query.hql");	if (file.delete()) {	} else {	}	} catch (Exception e) {	
file exception 

String[] subDirs = dir.split("/");	String dirPath = USER_DIRECTORY;	for(int i=2;i<subDirs.length;i++) {	dirPath += "/"+subDirs[i];	fs.setOwner(new Path(dirPath), username, username);	}	return b;	}	});	} catch (Exception e) {	
exception in webhdfs 

String[] subDirs = dir.split("/");	String dirPath = USER_DIRECTORY;	for(int i=2;i<subDirs.length;i++) {	dirPath += "/"+subDirs[i];	fs.setOwner(new Path(dirPath), username, username);	}	return b;	}	});	} catch (Exception e) {	
exception in webhdfs 

out.write(b, 0, numBytes);	}	in.close();	out.close();	fileSystem.setOwner(path, username, username);	fileSystem.close();	return null;	}	});	} catch (Exception e) {	
webhdfs exception 

out.write(b, 0, numBytes);	}	in.close();	out.close();	fileSystem.setOwner(path, username, username);	fileSystem.close();	return null;	}	});	} catch (Exception e) {	
webhdfs exception 

========================= ambari sample_1286 =========================

protected abstract boolean applies(Entry entry, String attribute);	public abstract String detectedProperty();	public Map<String, String> detect() {	
calculating the most probable attribute value 

protected abstract boolean applies(Entry entry, String attribute);	public abstract String detectedProperty();	public Map<String, String> detect() {	Map<String, String> detectedMap = Maps.newHashMap();	Map.Entry<String, Integer> selectedEntry = null;	for (Map.Entry<String, Integer> entry : occurrenceMap().entrySet()) {	if (selectedEntry == null) {	selectedEntry = entry;	
initial attribute value entry 

public abstract String detectedProperty();	public Map<String, String> detect() {	Map<String, String> detectedMap = Maps.newHashMap();	Map.Entry<String, Integer> selectedEntry = null;	for (Map.Entry<String, Integer> entry : occurrenceMap().entrySet()) {	if (selectedEntry == null) {	selectedEntry = entry;	continue;	}	if (selectedEntry.getValue() < entry.getValue()) {	
changing potential attribute value entry from to 

continue;	}	if (selectedEntry.getValue() < entry.getValue()) {	selectedEntry = entry;	}	}	String detectedVal = "N/A";	if (selectedEntry.getValue() > 0) {	detectedVal = selectedEntry.getKey();	} else {	
unable to detect attribute or attribute value 

}	if (selectedEntry.getValue() < entry.getValue()) {	selectedEntry = entry;	}	}	String detectedVal = "N/A";	if (selectedEntry.getValue() > 0) {	detectedVal = selectedEntry.getKey();	} else {	}	
detected attribute or value 

public void collect(Entry entry) {	
collecting ldap attributes values form entry with dn 

public void collect(Entry entry) {	for (String attributeValue : occurrenceMap().keySet()) {	if (applies(entry, attributeValue)) {	Integer cnt = occurrenceMap().get(attributeValue).intValue();	if (weightsMap().containsKey(attributeValue)) {	cnt = cnt + weightsMap().get(attributeValue);	} else {	cnt = cnt + 1;	}	occurrenceMap().put(attributeValue, cnt);	
collected potential name attr count 

for (String attributeValue : occurrenceMap().keySet()) {	if (applies(entry, attributeValue)) {	Integer cnt = occurrenceMap().get(attributeValue).intValue();	if (weightsMap().containsKey(attributeValue)) {	cnt = cnt + weightsMap().get(attributeValue);	} else {	cnt = cnt + 1;	}	occurrenceMap().put(attributeValue, cnt);	} else {	
the result entry doesn t contain the attribute 

========================= ambari sample_2916 =========================

public boolean init(String inputDesc, String fieldName, String mapClassCode, MapFieldDescriptor mapFieldDescriptor) {	init(inputDesc, fieldName, mapClassCode);	String targetDateFormat = ((MapDateDescriptor)mapFieldDescriptor).getTargetDatePattern();	String srcDateFormat = ((MapDateDescriptor)mapFieldDescriptor).getSourceDatePattern();	if (StringUtils.isEmpty(targetDateFormat)) {	
date format for map is empty 

public boolean init(String inputDesc, String fieldName, String mapClassCode, MapFieldDescriptor mapFieldDescriptor) {	init(inputDesc, fieldName, mapClassCode);	String targetDateFormat = ((MapDateDescriptor)mapFieldDescriptor).getTargetDatePattern();	String srcDateFormat = ((MapDateDescriptor)mapFieldDescriptor).getSourceDatePattern();	if (StringUtils.isEmpty(targetDateFormat)) {	} else {	
date mapper format is 

========================= ambari sample_1639 =========================

Set<Map<String, Object>> resourceFilters;	Map<String, String> params = new HashMap<>();	Object resourceFilterObj = propertyMap.get(REQUEST_RESOURCE_FILTER_ID);	if (resourceFilterObj != null && resourceFilterObj instanceof HashSet) {	resourceFilters = (HashSet<Map<String, Object>>) resourceFilterObj;	resourceFilterList = new ArrayList<>();	for (Map<String, Object> resourceMap : resourceFilters) {	params.put(HAS_RESOURCE_FILTERS, "true");	resourceFilterList.addAll(parseRequestResourceFilter(resourceMap, (String) propertyMap.get(REQUEST_CLUSTER_NAME_PROPERTY_ID)));	}	
requestresourcefilters 

========================= ambari sample_3604 =========================

public User loadUserByUsername(final String username) throws UsernameNotFoundException {	
loaduserbyusername 

========================= ambari sample_1547 =========================

private void parseProperties() throws HdfsApiException {	String auth;	auth = context.getProperties().get("webhdfs.auth");	if (Strings.isNullOrEmpty(auth)) {	if (context.getCluster() != null) {	auth = getConfigurationFromAmbari();	} else {	auth = "auth=SIMPLE";	
authentication parameters could not be determined s assumed 

private void parseProperties() throws HdfsApiException {	String auth;	auth = context.getProperties().get("webhdfs.auth");	if (Strings.isNullOrEmpty(auth)) {	if (context.getCluster() != null) {	auth = getConfigurationFromAmbari();	} else {	auth = "auth=SIMPLE";	}	}	
hdfs auth params 

private void parseAuthString(String auth) {	for (String param : auth.split(";")) {	String[] keyvalue = param.split("=");	if (keyvalue.length != 2) {	
can not parse authentication param in 

========================= ambari sample_1117 =========================

public RequestStatus createResources(final Request request) throws SystemException, UnsupportedPropertyException, ResourceAlreadyExistsException, NoSuchParentResourceException {	StackAdvisorRequest validationRequest = prepareStackAdvisorRequest(request);	final ValidationResponse response;	try {	response = saHelper.validate(validationRequest);	} catch (StackAdvisorRequestException e) {	
error occurred during validation 

public RequestStatus createResources(final Request request) throws SystemException, UnsupportedPropertyException, ResourceAlreadyExistsException, NoSuchParentResourceException {	StackAdvisorRequest validationRequest = prepareStackAdvisorRequest(request);	final ValidationResponse response;	try {	response = saHelper.validate(validationRequest);	} catch (StackAdvisorRequestException e) {	throw new IllegalArgumentException(e.getMessage(), e);	} catch (StackAdvisorException e) {	
error occurred during validation 

========================= ambari sample_3595 =========================

try {	hiveConnectionWrapper.connect();	} catch (ConnectionException e) {	if(isLoginError(e) && ldapEnabled) return Response.status(Response.Status.UNAUTHORIZED).build();	else throw new ServiceFormattedException(e.getMessage(), e);	} finally {	try {	hiveConnectionWrapper.disconnect();	}	catch(ConnectionException e){	
cannot close the connection 

========================= ambari sample_836 =========================

public void killJob(String jobId) throws IOException {	JSONRequest<JobInfo> request = new JSONRequest<JobInfo>(service.path("jobs").path(jobId), JobInfo.class, doAs, doAs, context);	try {	request.delete();	} catch (IOException e) {	
ignoring response from webhcat see hive 

========================= ambari sample_1052 =========================

public void onAmbariEvent(AlertDefinitionRegistrationEvent event) {	
received event 

public void onAmbariEvent(AlertDefinitionRegistrationEvent event) {	AlertDefinition definition = event.getDefinition();	
registering alert definition 

public void onAmbariEvent(AlertDefinitionChangedEvent event) {	
received event 

public void onAmbariEvent(AlertDefinitionChangedEvent event) {	AlertDefinition definition = event.getDefinition();	
updating alert definition 

public void onAmbariEvent(AlertDefinitionDeleteEvent event) {	
received event 

public void onAmbariEvent(AlertDefinitionDeleteEvent event) {	AlertDefinition definition = event.getDefinition();	
removing alert definition 

========================= ambari sample_4529 =========================

enableLog4jMonitor(configsMap);	handlerList.addHandler(root);	server.setHandler(handlerList);	ServletHolder agent = new ServletHolder(ServletContainer.class);	agent.setInitParameter("com.sun.jersey.config.property.resourceConfigClass", "com.sun.jersey.api.core.PackagesResourceConfig");	agent.setInitParameter("com.sun.jersey.config.property.packages", "org.apache.ambari.server.agent.rest;" + "org.apache.ambari.server.api");	agent.setInitParameter("com.sun.jersey.api.json.POJOMappingFeature", "true");	agentroot.addServlet(agent, "/agent/v1/*");	agent.setInitOrder(3);	AgentResource.statHeartBeatHandler();	
started heartbeat handler 

apiConnector.setAcceptors(acceptors);	configureJettyThreadPool(server, acceptors, CLIENT_THREAD_POOL_NAME, configs.getClientThreadPoolSize());	server.addConnector(apiConnector);	server.setStopAtShutdown(true);	serverForAgent.setStopAtShutdown(true);	springAppContext.start();	String osType = getServerOsType();	if (osType == null || osType.isEmpty()) {	throw new RuntimeException(Configuration.OS_VERSION.getKey() + " is not " + " set in the ambari.properties file");	}	
initializing clusters 

server.setStopAtShutdown(true);	serverForAgent.setStopAtShutdown(true);	springAppContext.start();	String osType = getServerOsType();	if (osType == null || osType.isEmpty()) {	throw new RuntimeException(Configuration.OS_VERSION.getKey() + " is not " + " set in the ambari.properties file");	}	Clusters clusters = injector.getInstance(Clusters.class);	StringBuilder clusterDump = new StringBuilder();	clusters.debugDump(clusterDump);	
current clusters state 

serverForAgent.setStopAtShutdown(true);	springAppContext.start();	String osType = getServerOsType();	if (osType == null || osType.isEmpty()) {	throw new RuntimeException(Configuration.OS_VERSION.getKey() + " is not " + " set in the ambari.properties file");	}	Clusters clusters = injector.getInstance(Clusters.class);	StringBuilder clusterDump = new StringBuilder();	clusters.debugDump(clusterDump);	LOG.info(clusterDump.toString());	
reconciling alert definitions 

springAppContext.start();	String osType = getServerOsType();	if (osType == null || osType.isEmpty()) {	throw new RuntimeException(Configuration.OS_VERSION.getKey() + " is not " + " set in the ambari.properties file");	}	Clusters clusters = injector.getInstance(Clusters.class);	StringBuilder clusterDump = new StringBuilder();	clusters.debugDump(clusterDump);	LOG.info(clusterDump.toString());	ambariMetaInfo.reconcileAlertDefinitions(clusters, false);	
initializing actionmanager 

String osType = getServerOsType();	if (osType == null || osType.isEmpty()) {	throw new RuntimeException(Configuration.OS_VERSION.getKey() + " is not " + " set in the ambari.properties file");	}	Clusters clusters = injector.getInstance(Clusters.class);	StringBuilder clusterDump = new StringBuilder();	clusters.debugDump(clusterDump);	LOG.info(clusterDump.toString());	ambariMetaInfo.reconcileAlertDefinitions(clusters, false);	ActionManager manager = injector.getInstance(ActionManager.class);	
initializing controller 

if (osType == null || osType.isEmpty()) {	throw new RuntimeException(Configuration.OS_VERSION.getKey() + " is not " + " set in the ambari.properties file");	}	Clusters clusters = injector.getInstance(Clusters.class);	StringBuilder clusterDump = new StringBuilder();	clusters.debugDump(clusterDump);	LOG.info(clusterDump.toString());	ambariMetaInfo.reconcileAlertDefinitions(clusters, false);	ActionManager manager = injector.getInstance(ActionManager.class);	AmbariManagementController controller = injector.getInstance( AmbariManagementController.class);	
initializing scheduled request manager 

ActionManager manager = injector.getInstance(ActionManager.class);	AmbariManagementController controller = injector.getInstance( AmbariManagementController.class);	ExecutionScheduleManager executionScheduleManager = injector .getInstance(ExecutionScheduleManager.class);	MetricsService metricsService = injector.getInstance( MetricsService.class);	clusterController = controller;	StateRecoveryManager recoveryManager = injector.getInstance( StateRecoveryManager.class);	recoveryManager.doWork();	server.start();	configureMaxInactiveInterval();	serverForAgent.start();	
started server 

AmbariManagementController controller = injector.getInstance( AmbariManagementController.class);	ExecutionScheduleManager executionScheduleManager = injector .getInstance(ExecutionScheduleManager.class);	MetricsService metricsService = injector.getInstance( MetricsService.class);	clusterController = controller;	StateRecoveryManager recoveryManager = injector.getInstance( StateRecoveryManager.class);	recoveryManager.doWork();	server.start();	configureMaxInactiveInterval();	serverForAgent.start();	if( !configs.isViewDirectoryWatcherServiceDisabled()) {	
starting view directory watcher 

clusterController = controller;	StateRecoveryManager recoveryManager = injector.getInstance( StateRecoveryManager.class);	recoveryManager.doWork();	server.start();	configureMaxInactiveInterval();	serverForAgent.start();	if( !configs.isViewDirectoryWatcherServiceDisabled()) {	viewDirectoryWatcher.start();	}	manager.start();	
started actionmanager 

StateRecoveryManager recoveryManager = injector.getInstance( StateRecoveryManager.class);	recoveryManager.doWork();	server.start();	configureMaxInactiveInterval();	serverForAgent.start();	if( !configs.isViewDirectoryWatcherServiceDisabled()) {	viewDirectoryWatcher.start();	}	manager.start();	executionScheduleManager.start();	
started scheduled request manager 

recoveryManager.doWork();	server.start();	configureMaxInactiveInterval();	serverForAgent.start();	if( !configs.isViewDirectoryWatcherServiceDisabled()) {	viewDirectoryWatcher.start();	}	manager.start();	executionScheduleManager.start();	serviceManager.startAsync();	
started services 

serverForAgent.start();	if( !configs.isViewDirectoryWatcherServiceDisabled()) {	viewDirectoryWatcher.start();	}	manager.start();	executionScheduleManager.start();	serviceManager.startAsync();	if (!configs.isMetricsServiceDisabled()) {	metricsService.start();	} else {	
ambariserver metrics disabled 

viewDirectoryWatcher.start();	}	manager.start();	executionScheduleManager.start();	serviceManager.startAsync();	if (!configs.isMetricsServiceDisabled()) {	metricsService.start();	} else {	}	server.join();	
joined the server 

}	manager.start();	executionScheduleManager.start();	serviceManager.startAsync();	if (!configs.isMetricsServiceDisabled()) {	metricsService.start();	} else {	}	server.join();	} catch (BadPaddingException bpe) {	
bad keystore or private key password https certificate re importing may be required 

executionScheduleManager.start();	serviceManager.startAsync();	if (!configs.isMetricsServiceDisabled()) {	metricsService.start();	} else {	}	server.join();	} catch (BadPaddingException bpe) {	throw bpe;	} catch (BindException bindException) {	
could not bind to server port instance may already be running terminating this instance 

private SelectChannelConnector createSelectChannelConnectorForClient() {	Map<String, String> configsMap = configs.getConfigsMap();	SelectChannelConnector apiConnector;	if (configs.getApiSSLAuthentication()) {	String httpsKeystore = configsMap.get(Configuration.CLIENT_API_SSL_KSTR_DIR_NAME.getKey()) + File.separator + configsMap.get(Configuration.CLIENT_API_SSL_KSTR_NAME.getKey());	String httpsTruststore = configsMap.get(Configuration.CLIENT_API_SSL_KSTR_DIR_NAME.getKey()) + File.separator + configsMap.get(Configuration.CLIENT_API_SSL_TSTR_NAME.getKey());	
api ssl authentication is turned on keystore 

protected void configureJettyThreadPool(Server server, int acceptorThreads, String threadPoolName, int configuredThreadPoolSize) {	int minumumAvailableThreads = 20;	int reservedJettyThreads = acceptorThreads * 2;	if (configuredThreadPoolSize < reservedJettyThreads + minumumAvailableThreads) {	int newThreadPoolSize = reservedJettyThreads + minumumAvailableThreads;	
the configured jetty thread pool value of is not sufficient on a host with processors increasing the value to 

protected void configureJettyThreadPool(Server server, int acceptorThreads, String threadPoolName, int configuredThreadPoolSize) {	int minumumAvailableThreads = 20;	int reservedJettyThreads = acceptorThreads * 2;	if (configuredThreadPoolSize < reservedJettyThreads + minumumAvailableThreads) {	int newThreadPoolSize = reservedJettyThreads + minumumAvailableThreads;	configuredThreadPoolSize = newThreadPoolSize;	}	
jetty is configuring with reserved acceptors selectors and a total pool size of for processors 

protected void initDB() throws AmbariException {	if (configs.getPersistenceType() == PersistenceType.IN_MEMORY || dbInitNeeded) {	
database init needed creating default data 

public void stop() throws Exception {	if (server == null) {	throw new AmbariException("Error stopping the server");	} else {	try {	server.stop();	} catch (Exception e) {	
error stopping the server 

public static void setupProxyAuth() {	final String proxyUser = System.getProperty("http.proxyUser");	final String proxyPass = System.getProperty("http.proxyPassword");	if (null != proxyUser && null != proxyPass) {	
proxy authentication enabled 

public static void setupProxyAuth() {	final String proxyUser = System.getProperty("http.proxyUser");	final String proxyPass = System.getProperty("http.proxyPassword");	if (null != proxyUser && null != proxyPass) {	Authenticator.setDefault(new Authenticator() {	protected PasswordAuthentication getPasswordAuthentication() {	return new PasswordAuthentication(proxyUser, proxyPass.toCharArray());	}	});	} else {	
proxy authentication not specified 

public static void main(String[] args) throws Exception {	logStartup();	Injector injector = Guice.createInjector(new ControllerModule(), new AuditLoggerModule(), new LdapModule());	AmbariServer server = null;	try {	
getting the controller 

injector.getInstance(UpdateActiveRepoVersionOnStartup.class).process();	CertificateManager certMan = injector.getInstance(CertificateManager.class);	certMan.initRootCert();	KerberosChecker.checkJaasConfiguration();	ViewRegistry.initInstance(server.viewRegistry);	ComponentSSLConfiguration.instance().init(server.configs);	server.run();	} catch (Throwable t) {	System.err.println("An unexpected error occured during starting Ambari Server.");	t.printStackTrace();	
failed to run the ambari server 

========================= ambari sample_3653 =========================

List<String> metricPatternList = Arrays.asList(metricPatterns.split(","));	for (String metricPattern : metricPatternList) {	String metricPatternClause = "'" + metricPattern + "'";	if (tableName.contains("RECORD")) {	stmts.add(String.format(TOPN_DOWNSAMPLER_HOST_METRIC_SELECT_SQL, endTime, columnSelect, columnSelect, columnSelect, tableName, metricPatternClause, startTime, endTime, columnSelect, topNConfig.getTopN()));	} else {	stmts.add(String.format(TOPN_DOWNSAMPLER_CLUSTER_METRIC_SELECT_SQL, endTime, columnSelect, columnSelect, columnSelect, tableName, metricPatternClause, startTime, endTime, columnSelect, topNConfig.getTopN()));	}	}	if (LOG.isDebugEnabled()) {	
downsampling stmt 

========================= ambari sample_359 =========================

public OpenCSVParser(Reader reader, ParseOptions parseOptions) throws IOException {	super(reader, parseOptions);	CSVParserBuilder csvParserBuilder = new CSVParserBuilder();	CSVReaderBuilder builder =  new CSVReaderBuilder(reader);	Character delimiter = (Character) parseOptions.getOption(ParseOptions.OPTIONS_CSV_DELIMITER);	if(delimiter != null){	
setting delimiter as 

public OpenCSVParser(Reader reader, ParseOptions parseOptions) throws IOException {	super(reader, parseOptions);	CSVParserBuilder csvParserBuilder = new CSVParserBuilder();	CSVReaderBuilder builder =  new CSVReaderBuilder(reader);	Character delimiter = (Character) parseOptions.getOption(ParseOptions.OPTIONS_CSV_DELIMITER);	if(delimiter != null){	csvParserBuilder = csvParserBuilder.withSeparator(delimiter);	}	Character quote = (Character) parseOptions.getOption(ParseOptions.OPTIONS_CSV_QUOTE);	if( null != quote ){	
setting quote char 

Character delimiter = (Character) parseOptions.getOption(ParseOptions.OPTIONS_CSV_DELIMITER);	if(delimiter != null){	csvParserBuilder = csvParserBuilder.withSeparator(delimiter);	}	Character quote = (Character) parseOptions.getOption(ParseOptions.OPTIONS_CSV_QUOTE);	if( null != quote ){	csvParserBuilder = csvParserBuilder.withQuoteChar(quote);	}	Character escapeChar = (Character) parseOptions.getOption(ParseOptions.OPTIONS_CSV_ESCAPE_CHAR);	if( null != escapeChar ){	
setting escapechar 

========================= ambari sample_820 =========================

public void postConstructor() {	
postconstructor called 

========================= ambari sample_1414 =========================

public Set<QueryResource> getResources(ReadRequest request) throws SystemException, NoSuchResourceException, UnsupportedPropertyException {	Set<QueryResource> resources;	try {	LOG.debug("Get queries: View Context Properties = {}", viewContext.getProperties());	resources = getQueries();	
retrieved queries from hawq master 

private Set<QueryResource> getQueries() throws SystemException, SQLException {	Set<QueryResource> result = new HashSet<>();	Connection conn = null;	Statement st = null;	ResultSet rs = null;	try {	conn = dataSource.getConnection(getViewContextProperty(HOSTNAME_PROP, HOSTNAME_DESC), getViewContextProperty(HOSTPORT_PROP, HOSTPORT_DESC), getViewContextProperty(USER_PROP, USER_DESC), getViewContextProperty(PASSWORD_PROP, PASSWORD_DESC));	st = conn.createStatement();	
executing query 

========================= ambari sample_1156 =========================

private void freeConnector(FreeConnector message) {	ActorRef sender = getSender();	if (message.isForAsync()) {	
about to free connector for job and user 

private void freeConnector(FreeConnector message) {	ActorRef sender = getSender();	if (message.isForAsync()) {	Optional<ActorRef> refOptional = removeFromAsyncBusyPool(message.getUsername(), message.getJobId());	if (refOptional.isPresent()) {	addToAsyncAvailable(message.getUsername(), refOptional.get());	}	return;	}	
about to free sync connector for user 

========================= ambari sample_932 =========================

public ConnectionDelegate get(ViewContext context) {	
creating connection delegate instance for viewname instance name 

========================= ambari sample_877 =========================

public void dispatch(Notification notification) {	String scriptKey = null;	String script = getScriptLocation(notification);	if( null == script){	scriptKey = getScriptConfigurationKey(notification);	script = m_configuration.getProperty(scriptKey);	}	if (null == script) {	
unable to dispatch notification because the configuration property was not found 

scriptKey = getScriptConfigurationKey(notification);	script = m_configuration.getProperty(scriptKey);	}	if (null == script) {	if (null != notification.Callback) {	notification.Callback.onFailure(notification.CallbackIds);	}	return;	}	if (notification.getType() != Notification.Type.ALERT) {	
the dispatcher is not able to dispatch notifications of type 

public void run() {	boolean isDispatchSuccessful = true;	try {	Process process = m_processBuilder.start();	int exitCode = execute(process, m_timeout, TimeUnit.MILLISECONDS);	if (exitCode != 0) {	
unable to dispatch notification because terminated with exit code 

public void run() {	boolean isDispatchSuccessful = true;	try {	Process process = m_processBuilder.start();	int exitCode = execute(process, m_timeout, TimeUnit.MILLISECONDS);	if (exitCode != 0) {	isDispatchSuccessful = false;	}	} catch (TimeoutException timeoutException) {	isDispatchSuccessful = false;	
unable to dispatch notification with in under ms 

try {	Process process = m_processBuilder.start();	int exitCode = execute(process, m_timeout, TimeUnit.MILLISECONDS);	if (exitCode != 0) {	isDispatchSuccessful = false;	}	} catch (TimeoutException timeoutException) {	isDispatchSuccessful = false;	} catch (Exception exception) {	isDispatchSuccessful = false;	
unable to dispatch notification with 

========================= ambari sample_4568 =========================

for (Entry<String, URI> versionEntry : osMap.entrySet()) {	String osFamily = m_family.find(versionEntry.getKey());	URI uri = versionEntry.getValue();	if (null == osFamily) {	String alias = m_family.getAliases().get(versionEntry.getKey());	if (null != alias) {	osFamily = m_family.find(alias);	}	}	if (null == osFamily || !oses.contains(osFamily)) {	
stack cannot resolve os to the supported ones family 

}	}	if (null == osFamily || !oses.contains(osFamily)) {	continue;	}	try {	VersionDefinitionXml xml = timedVDFLoad(uri);	version = (null == version) ? xml.release.version : version;	merger.add(version, xml);	} catch (Exception e) {	
could not load version definition for identified by 

private VersionDefinitionXml timedVDFLoad(URI uri) throws Exception {	long time = System.currentTimeMillis();	try {	return VersionDefinitionXml.load(uri.toURL());	} finally {	
loaded vdf in ms 

========================= ambari sample_3242 =========================

public String createCollection() throws Exception {	List<String> collections = listCollections();	if (!collections.contains(getCollection())) {	String collection = new CreateCollectionCommand(getRetryTimes(), getInterval()).run(this);	
collection creation request sent 

public String createCollection() throws Exception {	List<String> collections = listCollections();	if (!collections.contains(getCollection())) {	String collection = new CreateCollectionCommand(getRetryTimes(), getInterval()).run(this);	} else {	
collection already exits 

public void setClusterProp() throws Exception {	
set cluster prop 

public void setClusterProp() throws Exception {	String newPropValue = new SetClusterPropertyZkCommand(getRetryTimes(), getInterval()).run(this);	
set cluster prop successfully to 

public void createZnode() throws Exception {	boolean znodeExists = isZnodeExists(this.znode);	if (znodeExists) {	
znode already exists 

public void createZnode() throws Exception {	boolean znodeExists = isZnodeExists(this.znode);	if (znodeExists) {	} else {	
znode does not exist creating 

public void createZnode() throws Exception {	boolean znodeExists = isZnodeExists(this.znode);	if (znodeExists) {	} else {	String newZnode = new CreateSolrZnodeZkCommand(getRetryTimes(), getInterval()).run(this);	
znode is created successfully 

public boolean isZnodeExists(String znode) throws Exception {	
check znode exists or not 

public boolean isZnodeExists(String znode) throws Exception {	boolean result = new CheckZnodeZkCommand(getRetryTimes(), getInterval(), znode).run(this);	if (result) {	
znode exists 

public boolean isZnodeExists(String znode) throws Exception {	boolean result = new CheckZnodeZkCommand(getRetryTimes(), getInterval(), znode).run(this);	if (result) {	} else {	
znode does not exist 

public void setupKerberosPlugin() throws Exception {	
setup kerberos plugin in security json 

public void setupKerberosPlugin() throws Exception {	new EnableKerberosPluginSolrZkCommand(getRetryTimes(), getInterval()).run(this);	
kerberosplugin is set in security json 

public void unsecureZnode() throws Exception {	
disable security for znode 

public String uploadConfiguration() throws Exception {	String configSet = new UploadConfigZkCommand(getRetryTimes(), getInterval()).run(this);	
is uploaded to zookeeper 

public String downloadConfiguration() throws Exception {	String configDir = new DownloadConfigZkCommand(getRetryTimes(), getInterval()).run(this);	
config set is download from zookeeper 

public boolean configurationExists() throws Exception {	boolean configExits = new CheckConfigZkCommand(getRetryTimes(), getInterval()).run(this);	if (configExits) {	
config exits 

public boolean configurationExists() throws Exception {	boolean configExits = new CheckConfigZkCommand(getRetryTimes(), getInterval()).run(this);	if (configExits) {	} else {	
configuration does not exist 

public Collection<String> createShard(String shard) throws Exception {	Collection<String> existingShards = getShardNames();	if (shard != null) {	new CreateShardCommand(shard, getRetryTimes(), getInterval()).run(this);	existingShards.add(shard);	} else {	List<String> shardList = ShardUtils.generateShardList(getMaxShardsPerNode());	for (String shardName : shardList) {	if (!existingShards.contains(shardName)) {	new CreateShardCommand(shardName, getRetryTimes(), getInterval()).run(this);	
adding new shard to collection request sent 

========================= ambari sample_92 =========================

public static void main(String[] args) {	try {	Injector injector = Guice.createInjector(new ControllerModule(), new AuditLoggerModule());	LdapToPamMigrationHelper migrationHelper = injector.getInstance(LdapToPamMigrationHelper.class);	migrationHelper.migrateLdapUsersGroups();	} catch (Throwable t) {	
caught exception on migration exiting 

========================= ambari sample_2844 =========================

public synchronized void createHosts(Request request) throws AmbariException, AuthorizationException {	Set<Map<String, Object>> propertySet = request.getProperties();	if (propertySet == null || propertySet.isEmpty()) {	
received a create host request with no associated property sets 

protected synchronized void updateHosts(Set<HostRequest> requests) throws AmbariException, AuthorizationException {	if (requests.isEmpty()) {	
received an empty requests set 

throw new IllegalArgumentException("Invalid arguments, can only set " + "maintenance state to one of " + EnumSet.of(MaintenanceState.OFF, MaintenanceState.ON));	} else {	host.setMaintenanceState(clusterId, newState);	}	}	}	if (null != clusterName && null != request.getDesiredConfigs()) {	if (clusters.getHostsForCluster(clusterName).containsKey(host.getHostName())) {	for (ConfigurationRequest cr : request.getDesiredConfigs()) {	if (null != cr.getProperties() && cr.getProperties().size() > 0) {	
applying configuration with tag to host in cluster 

private void processDeleteHostRequests(List<HostRequest> requests,  Clusters clusters, DeleteStatusMetaData deleteStatusMetaData) throws AmbariException {	Set<String> hostsClusters = new HashSet<>();	Set<String> hostNames = new HashSet<>();	Set<Cluster> allClustersWithHosts = new HashSet<>();	for (HostRequest hostRequest : requests) {	String hostname = hostRequest.getHostname();	hostNames.add(hostname);	if (hostRequest.getClusterName() != null) {	hostsClusters.add(hostRequest.getClusterName());	}	
received delete request for host from cluster 

========================= ambari sample_3613 =========================

Map<String, String> overriddenHosts = new HashMap<>();	try {	for (Resource r : resources) {	String clusterName = r.getPropertyValue(clusterNamePropertyId).toString();	String componentName = r.getPropertyValue(componentNamePropertyId).toString();	Cluster cluster = clusters.getCluster(clusterName);	Service service = null;	try {	service = cluster.getServiceByComponentName(componentName);	} catch (ServiceNotFoundException e) {	
could not load component 

} else {	defaultJmx.populateResources(resources, request, predicate);	}	for (PropertyProvider pp : additional) {	pp.populateResources(resources, request, predicate);	}	} catch (AuthorizationException e) {	throw e;	} catch (Exception e) {	e.printStackTrace();	
error loading deferred resources 

private PropertyProvider getDelegate(MetricDefinition definition) {	try {	Class<?> clz = Class.forName(definition.getType());	try {	Method m = clz.getMethod("getInstance", Map.class, Map.class);	Object o = m.invoke(null, definition.getProperties(), definition.getMetrics());	return PropertyProvider.class.cast(o);	} catch (Exception e) {	
could not load singleton or factory method for type 

Method m = clz.getMethod("getInstance", Map.class, Map.class);	Object o = m.invoke(null, definition.getProperties(), definition.getMetrics());	return PropertyProvider.class.cast(o);	} catch (Exception e) {	}	try {	Constructor<?> ct = clz.getConstructor(Map.class, Map.class);	Object o = ct.newInstance(definition.getProperties(), definition.getMetrics());	return PropertyProvider.class.cast(o);	} catch (Exception e) {	
could not find contructor for type 

} catch (Exception e) {	}	try {	Constructor<?> ct = clz.getConstructor(Map.class, Map.class);	Object o = ct.newInstance(definition.getProperties(), definition.getMetrics());	return PropertyProvider.class.cast(o);	} catch (Exception e) {	}	return PropertyProvider.class.cast(clz.newInstance());	} catch (Exception e) {	
could not load class 

try {	Class<?> clz = Class.forName(definition.getType());	if (clz.equals(RestMetricsPropertyProvider.class)) {	return metricPropertyProviderFactory.createRESTMetricsPropertyProvider( definition.getProperties(), componentMetrics, streamProvider, metricsHostProvider, clusterNamePropertyId, hostNamePropertyId, componentNamePropertyId, statePropertyId, componentName);	}	try {	Constructor<?> ct = clz.getConstructor(Map.class, Map.class, StreamProvider.class, MetricHostProvider.class, String.class, String.class, String.class, String.class, String.class);	Object o = ct.newInstance( injector, definition.getProperties(), componentMetrics, streamProvider, metricsHostProvider, clusterNamePropertyId, hostNamePropertyId, componentNamePropertyId, statePropertyId, componentName);	return PropertyProvider.class.cast(o);	} catch (Exception e) {	
could not find contructor for type 

return metricPropertyProviderFactory.createRESTMetricsPropertyProvider( definition.getProperties(), componentMetrics, streamProvider, metricsHostProvider, clusterNamePropertyId, hostNamePropertyId, componentNamePropertyId, statePropertyId, componentName);	}	try {	Constructor<?> ct = clz.getConstructor(Map.class, Map.class, StreamProvider.class, MetricHostProvider.class, String.class, String.class, String.class, String.class, String.class);	Object o = ct.newInstance( injector, definition.getProperties(), componentMetrics, streamProvider, metricsHostProvider, clusterNamePropertyId, hostNamePropertyId, componentNamePropertyId, statePropertyId, componentName);	return PropertyProvider.class.cast(o);	} catch (Exception e) {	}	return PropertyProvider.class.cast(clz.newInstance());	} catch (Exception e) {	
could not load class 

========================= ambari sample_3490 =========================

public void handleMessage(HiveMessage hiveMessage) {	Object message = hiveMessage.getMessage();	if(message instanceof RegisterActor){	RegisterActor registerActor = (RegisterActor) message;	ActorRef actorRef = registerActor.getActorRef();	this.getContext().watch(actorRef);	
registered new actor 

public void handleMessage(HiveMessage hiveMessage) {	Object message = hiveMessage.getMessage();	if(message instanceof RegisterActor){	RegisterActor registerActor = (RegisterActor) message;	ActorRef actorRef = registerActor.getActorRef();	this.getContext().watch(actorRef);	
registration for at 

public void handleMessage(HiveMessage hiveMessage) {	Object message = hiveMessage.getMessage();	if(message instanceof RegisterActor){	RegisterActor registerActor = (RegisterActor) message;	ActorRef actorRef = registerActor.getActorRef();	this.getContext().watch(actorRef);	}else if(message instanceof Terminated){	Terminated terminated = (Terminated) message;	ActorRef actor = terminated.actor();	
received terminate for actor 

public void handleMessage(HiveMessage hiveMessage) {	Object message = hiveMessage.getMessage();	if(message instanceof RegisterActor){	RegisterActor registerActor = (RegisterActor) message;	ActorRef actorRef = registerActor.getActorRef();	this.getContext().watch(actorRef);	}else if(message instanceof Terminated){	Terminated terminated = (Terminated) message;	ActorRef actor = terminated.actor();	
termination for at 

public void handleMessage(HiveMessage hiveMessage) {	Object message = hiveMessage.getMessage();	if(message instanceof RegisterActor){	RegisterActor registerActor = (RegisterActor) message;	ActorRef actorRef = registerActor.getActorRef();	this.getContext().watch(actorRef);	}else if(message instanceof Terminated){	Terminated terminated = (Terminated) message;	ActorRef actor = terminated.actor();	}else{	
received unknown message 

========================= ambari sample_933 =========================

public void doFilter(ServletRequest serReq, ServletResponse serResp, FilterChain filtCh) throws IOException, ServletException {	HttpServletRequest req = (HttpServletRequest) serReq;	String reqUrl = req.getRequestURL().toString();	
filtering for security purposes 

public void doFilter(ServletRequest serReq, ServletResponse serResp, FilterChain filtCh) throws IOException, ServletException {	HttpServletRequest req = (HttpServletRequest) serReq;	String reqUrl = req.getRequestURL().toString();	if (serReq.getLocalPort() != config.getTwoWayAuthPort()) {	if (isRequestAllowed(reqUrl)) {	filtCh.doFilter(serReq, serResp);	}	else {	
this request is not allowed on this port 

HttpServletRequest req = (HttpServletRequest) serReq;	String reqUrl = req.getRequestURL().toString();	if (serReq.getLocalPort() != config.getTwoWayAuthPort()) {	if (isRequestAllowed(reqUrl)) {	filtCh.doFilter(serReq, serResp);	}	else {	}	}	else {	
request can continue on secure port 

private boolean isRequestAllowed(String reqUrl) {	try {	URL url = new URL(reqUrl);	if (!"https".equals(url.getProtocol())) {	
request s is not using https 

if (Pattern.matches("/connection_info", url.getPath())) {	return true;	}	if (Pattern.matches("/certs/[^/0-9][^/]*", url.getPath())) {	return true;	}	if (Pattern.matches("/resources/.*", url.getPath())) {	return true;	}	} catch (Exception e) {	
exception while validating if request is secure 

return true;	}	if (Pattern.matches("/certs/[^/0-9][^/]*", url.getPath())) {	return true;	}	if (Pattern.matches("/resources/.*", url.getPath())) {	return true;	}	} catch (Exception e) {	}	
request doesn t match any pattern 

========================= ambari sample_2794 =========================

public void migrateEntity(Class originEntityClass, Class currentEntityClass) throws ViewDataMigrationException {	if (isHive1()) {	currentEntityClass = hive1EntitiesMapping.get(originEntityClass.getCanonicalName());	if (currentEntityClass == null) {	
mapping was not found for class 

public void migrateEntity(Class originEntityClass, Class currentEntityClass) throws ViewDataMigrationException {	if (isHive1()) {	currentEntityClass = hive1EntitiesMapping.get(originEntityClass.getCanonicalName());	if (currentEntityClass == null) {	return;	}	migrationContext.copyAllObjects(originEntityClass, currentEntityClass);	} else {	
unknown migration policy for class 

========================= ambari sample_876 =========================

public Response getScript(@PathParam("scriptId") String scriptId) {	
fetching scriptid 

public Response getScript(@PathParam("scriptId") String scriptId) {	try {	PigScript script = null;	script = getResourceManager().read(scriptId);	JSONObject object = new JSONObject();	object.put("script", script);	return Response.ok(object).build();	} catch (WebApplicationException ex) {	
exception occurred 

public Response getScript(@PathParam("scriptId") String scriptId) {	try {	PigScript script = null;	script = getResourceManager().read(scriptId);	JSONObject object = new JSONObject();	object.put("script", script);	return Response.ok(object).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (ItemNotFound itemNotFound) {	
exception occurred 

PigScript script = null;	script = getResourceManager().read(scriptId);	JSONObject object = new JSONObject();	object.put("script", script);	return Response.ok(object).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (ItemNotFound itemNotFound) {	throw new NotFoundFormattedException(itemNotFound.getMessage(), itemNotFound);	} catch (Exception ex) {	
exception occurred 

public Response deleteScript(@PathParam("scriptId") String scriptId) {	
deleting scriptid 

public Response deleteScript(@PathParam("scriptId") String scriptId) {	try {	getResourceManager().delete(scriptId);	return Response.status(204).build();	} catch (WebApplicationException ex) {	
exception occurred 

public Response deleteScript(@PathParam("scriptId") String scriptId) {	try {	getResourceManager().delete(scriptId);	return Response.status(204).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (ItemNotFound itemNotFound) {	
exception occurred 

public Response deleteScript(@PathParam("scriptId") String scriptId) {	try {	getResourceManager().delete(scriptId);	return Response.status(204).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (ItemNotFound itemNotFound) {	throw new NotFoundFormattedException(itemNotFound.getMessage(), itemNotFound);	} catch (Exception ex) {	
exception occurred 

public Response getScriptList() {	try {	
getting all scripts 

public Response getScriptList() {	try {	List allScripts = getResourceManager().readAll( new OnlyOwnersFilteringStrategy(this.context.getUsername()));	JSONObject object = new JSONObject();	object.put("scripts", allScripts);	return Response.ok(object).build();	} catch (WebApplicationException ex) {	
exception occurred 

public Response getScriptList() {	try {	List allScripts = getResourceManager().readAll( new OnlyOwnersFilteringStrategy(this.context.getUsername()));	JSONObject object = new JSONObject();	object.put("scripts", allScripts);	return Response.ok(object).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (Exception ex) {	
exception occurred 


updating scriptid 

try {	getResourceManager().update(request.script, scriptId);	return Response.status(204).build();	} catch (WebApplicationException ex) {	
exception occurred 

try {	getResourceManager().update(request.script, scriptId);	return Response.status(204).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (ItemNotFound itemNotFound) {	
exception occurred 

try {	getResourceManager().update(request.script, scriptId);	return Response.status(204).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (ItemNotFound itemNotFound) {	throw new NotFoundFormattedException(itemNotFound.getMessage(), itemNotFound);	} catch (Exception ex) {	
exception occurred 


creating new script 

try {	getResourceManager().create(request.script);	PigScript script = null;	script = getResourceManager().read(request.script.getId());	response.setHeader("Location", String.format("%s/%s", ui.getAbsolutePath().toString(), request.script.getId()));	JSONObject object = new JSONObject();	object.put("script", script);	return Response.ok(object).status(201).build();	} catch (WebApplicationException ex) {	
exception occurred 

getResourceManager().create(request.script);	PigScript script = null;	script = getResourceManager().read(request.script.getId());	response.setHeader("Location", String.format("%s/%s", ui.getAbsolutePath().toString(), request.script.getId()));	JSONObject object = new JSONObject();	object.put("script", script);	return Response.ok(object).status(201).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (ItemNotFound itemNotFound) {	
exception occurred 

script = getResourceManager().read(request.script.getId());	response.setHeader("Location", String.format("%s/%s", ui.getAbsolutePath().toString(), request.script.getId()));	JSONObject object = new JSONObject();	object.put("script", script);	return Response.ok(object).status(201).build();	} catch (WebApplicationException ex) {	throw ex;	} catch (ItemNotFound itemNotFound) {	throw new NotFoundFormattedException(itemNotFound.getMessage(), itemNotFound);	} catch (Exception ex) {	
exception occurred 

========================= ambari sample_1012 =========================

futures.add(future);	}	boolean completed = true;	R[] result = (R[]) new Object[futures.size()];	for (int i = 0; i < futures.size(); i++) {	try {	Future<ResultWrapper<R>> futureResult = null;	try {	futureResult = completionService.poll(POLL_DURATION_MILLISECONDS, TimeUnit.MILLISECONDS);	} catch (InterruptedException e) {	
caught interruptedexception in parallel forloop 

boolean completed = true;	R[] result = (R[]) new Object[futures.size()];	for (int i = 0; i < futures.size(); i++) {	try {	Future<ResultWrapper<R>> futureResult = null;	try {	futureResult = completionService.poll(POLL_DURATION_MILLISECONDS, TimeUnit.MILLISECONDS);	} catch (InterruptedException e) {	}	if (futureResult == null) {	
completion service in parallel forloop timed out 

Future<ResultWrapper<R>> futureResult = null;	try {	futureResult = completionService.poll(POLL_DURATION_MILLISECONDS, TimeUnit.MILLISECONDS);	} catch (InterruptedException e) {	}	if (futureResult == null) {	completed = false;	for(int fIndex = 0; fIndex < futures.size(); fIndex++) {	Future<ResultWrapper<R>> future = futures.get(fIndex);	if(future.isDone()) {	
task has already completed 

try {	futureResult = completionService.poll(POLL_DURATION_MILLISECONDS, TimeUnit.MILLISECONDS);	} catch (InterruptedException e) {	}	if (futureResult == null) {	completed = false;	for(int fIndex = 0; fIndex < futures.size(); fIndex++) {	Future<ResultWrapper<R>> future = futures.get(fIndex);	if(future.isDone()) {	} else if(future.isCancelled()) {	
task has already been cancelled 

futureResult = completionService.poll(POLL_DURATION_MILLISECONDS, TimeUnit.MILLISECONDS);	} catch (InterruptedException e) {	}	if (futureResult == null) {	completed = false;	for(int fIndex = 0; fIndex < futures.size(); fIndex++) {	Future<ResultWrapper<R>> future = futures.get(fIndex);	if(future.isDone()) {	} else if(future.isCancelled()) {	} else if(!future.cancel(true)) {	
task could not be cancelled 

} catch (InterruptedException e) {	}	if (futureResult == null) {	completed = false;	for(int fIndex = 0; fIndex < futures.size(); fIndex++) {	Future<ResultWrapper<R>> future = futures.get(fIndex);	if(future.isDone()) {	} else if(future.isCancelled()) {	} else if(!future.cancel(true)) {	} else {	
task successfully cancelled 

} else if(!future.cancel(true)) {	} else {	}	}	break;	} else {	ResultWrapper<R> res = futureResult.get();	if(res.result != null) {	result[res.index] = res.result;	} else {	
result is null for 

break;	} else {	ResultWrapper<R> res = futureResult.get();	if(res.result != null) {	result[res.index] = res.result;	} else {	completed = false;	}	}	} catch (InterruptedException e) {	
caught interruptedexception in parallel forloop 

ResultWrapper<R> res = futureResult.get();	if(res.result != null) {	result[res.index] = res.result;	} else {	completed = false;	}	}	} catch (InterruptedException e) {	completed = false;	} catch (ExecutionException e) {	
caught executionexception in parallel forloop 

result[res.index] = res.result;	} else {	completed = false;	}	}	} catch (InterruptedException e) {	completed = false;	} catch (ExecutionException e) {	completed = false;	} catch (CancellationException e) {	
caught cancellationexception in parallel forloop 

========================= ambari sample_2956 =========================

String stackInfo = String.format("%s-%s", stackName, stackVersion);	final ClusterRequest clusterRequest = new ClusterRequest(null, clusterName, null, securityType, stackInfo, null);	try {	RetryHelper.executeWithRetry(new Callable<Object>() {	public Object call() throws Exception {	getController().createCluster(clusterRequest);	return null;	}	});	} catch (AmbariException e) {	
failed to create cluster resource 

throw new RuntimeException(e);	}	String clusterName = cluster.getClusterName();	Map<String, Object> properties = new HashMap<>();	properties.put(HostResourceProvider.HOST_CLUSTER_NAME_PROPERTY_ID, clusterName);	properties.put(HostResourceProvider.HOST_HOST_NAME_PROPERTY_ID, hostName);	properties.put(HostResourceProvider.HOST_RACK_INFO_PROPERTY_ID, host.getRackInfo());	try {	getHostResourceProvider().createHosts(new RequestImpl(null, Collections.singleton(properties), null, null));	} catch (AmbariException | AuthorizationException e) {	
unable to create host component resource for host 

}	final Set<ServiceComponentHostRequest> requests = new HashSet<>();	for (Map.Entry<String, Collection<String>> entry : components.entrySet()) {	String service = entry.getKey();	for (String component : entry.getValue()) {	try {	if (cluster.getService(service) != null && !component.equals(RootComponent.AMBARI_SERVER.name())) {	requests.add(new ServiceComponentHostRequest(clusterName, service, component, hostName, null));	}	} catch(AmbariException se) {	
service already deleted from cluster 

}	}	try {	RetryHelper.executeWithRetry(new Callable<Object>() {	public Object call() throws Exception {	getController().createHostComponents(requests);	return null;	}	});	} catch (AmbariException e) {	
unable to create host component resource for host 

public RequestStatusResponse installHost(String hostName, String clusterName, Collection<String> skipInstallForComponents, Collection<String> dontSkipInstallForComponents, boolean skipFailure) {	try {	return getHostResourceProvider().install(clusterName, hostName, skipInstallForComponents, dontSkipInstallForComponents, skipFailure);	} catch (Exception e) {	
install host request submission failed 

public RequestStatusResponse startHost(String hostName, String clusterName, Collection<String> installOnlyComponents, boolean skipFailure) {	try {	return getHostComponentResourceProvider().start(clusterName, hostName, installOnlyComponents, skipFailure);	} catch (Exception e) {	
start host request submission failed 

String stackInfo = String.format("%s-%s", stackName, stackVersion);	final ClusterRequest clusterRequest = new ClusterRequest(null, clusterName, "INSTALLED", null, stackInfo, null);	try {	RetryHelper.executeWithRetry(new Callable<Object>() {	public Object call() throws Exception {	getController().updateClusters(Collections.singleton(clusterRequest), null);	return null;	}	});	} catch (AmbariException e) {	
unable to set install state for ui 

public void setConfigurationOnCluster(final ClusterRequest clusterRequest) {	try {	RetryHelper.executeWithRetry(new Callable<Object>() {	public Object call() throws Exception {	getController().updateClusters(Collections.singleton(clusterRequest), null);	return null;	}	});	} catch (AmbariException e) {	
failed to set configurations on cluster 

Cluster cluster = getController().getClusters().getCluster(clusterName);	boolean shouldWaitForResolution = true;	while (shouldWaitForResolution) {	int numOfRequestsStillRequiringResolution = 0;	for (String actualConfigType : updatedConfigTypes) {	DesiredConfig actualConfig = cluster.getDesiredConfigs().get(actualConfigType);	if (actualConfig == null && actualConfigType.equals("core-site")) {	continue;	}	if (!actualConfig.getTag().equals(TopologyManager.TOPOLOGY_RESOLVED_TAG)) {	
config type not resolved yet blueprint deployment will wait until configuration update is completed 

while (shouldWaitForResolution) {	int numOfRequestsStillRequiringResolution = 0;	for (String actualConfigType : updatedConfigTypes) {	DesiredConfig actualConfig = cluster.getDesiredConfigs().get(actualConfigType);	if (actualConfig == null && actualConfigType.equals("core-site")) {	continue;	}	if (!actualConfig.getTag().equals(TopologyManager.TOPOLOGY_RESOLVED_TAG)) {	numOfRequestsStillRequiringResolution++;	} else {	
config type is resolved in the cluster config 

DesiredConfig actualConfig = cluster.getDesiredConfigs().get(actualConfigType);	if (actualConfig == null && actualConfigType.equals("core-site")) {	continue;	}	if (!actualConfig.getTag().equals(TopologyManager.TOPOLOGY_RESOLVED_TAG)) {	numOfRequestsStillRequiringResolution++;	} else {	}	}	if (numOfRequestsStillRequiringResolution == 0) {	
all required configuration types are in the state blueprint deployment can now continue 

continue;	}	if (!actualConfig.getTag().equals(TopologyManager.TOPOLOGY_RESOLVED_TAG)) {	numOfRequestsStillRequiringResolution++;	} else {	}	}	if (numOfRequestsStillRequiringResolution == 0) {	shouldWaitForResolution = false;	} else {	
waiting for configuration types to be resolved before blueprint deployment can continue 

numOfRequestsStillRequiringResolution++;	} else {	}	}	if (numOfRequestsStillRequiringResolution == 0) {	shouldWaitForResolution = false;	} else {	try {	Thread.sleep(100);	} catch (InterruptedException e) {	
sleep interrupted 

========================= ambari sample_2669 =========================

public Response deleteDatabase(@PathParam("database_id") String databaseId) {	Job job = null;	try {	job = proxy.deleteDatabase(databaseId, getResourceManager());	JSONObject response = new JSONObject();	response.put("job", job);	return Response.status(Response.Status.ACCEPTED).entity(response).build();	} catch (ServiceException e) {	
exception occurred while delete database 

public Response createDatabase(CreateDatabaseRequestWrapper wrapper) {	String databaseId = wrapper.database.name;	Job job = null;	try {	job = proxy.createDatabase(databaseId, getResourceManager());	JSONObject response = new JSONObject();	response.put("job", job);	return Response.status(Response.Status.ACCEPTED).entity(response).build();	} catch (ServiceException e) {	
exception occurred while delete database 

public Response createTable(@PathParam("database_id") String databaseName, TableMetaRequest request) {	try {	Job job = proxy.createTable(databaseName, request.tableInfo, getResourceManager());	JSONObject response = new JSONObject();	response.put("job", job);	return Response.status(Response.Status.ACCEPTED).entity(response).build();	} catch (ServiceException e) {	
exception occurred while creatint table for db with details 

public Response renameTable(@PathParam("database_id") String oldDatabaseName, @PathParam("table_id") String oldTableName, TableRenameRequest request) {	try {	Job job = proxy.renameTable(oldDatabaseName, oldTableName, request.newDatabase, request.newTable, getResourceManager());	JSONObject response = new JSONObject();	response.put("job", job);	return Response.status(Response.Status.ACCEPTED).entity(response).build();	} catch (ServiceException e) {	
exception occurred while renaming table for olddatabasename oldtablename newdatabasename newtablename 

query = proxy.generateCreateTableDDL(request.tableInfo.getDatabase(), request.tableInfo);	}else if(queryType.equals(ALTER_TABLE)){	query = proxy.generateAlterTableQuery(context, getHiveConnectionConfig(), request.tableInfo.getDatabase(), request.tableInfo.getTable(), request.tableInfo);	}else{	throw new ServiceException("query_type = '" + queryType + "' is not supported");	}	JSONObject response = new JSONObject();	response.put("ddl", new DDL(query));	return Response.status(Response.Status.ACCEPTED).entity(response).build();	} catch (ServiceException e) {	
exception occurred while generating ddl for 

public Response alterTable(@PathParam("database_id") String databaseName, @PathParam("table_id") String oldTableName, TableMetaRequest tableMetaRequest) {	try {	ConnectionConfig hiveConnectionConfig = getHiveConnectionConfig();	Job job = proxy.alterTable(context, hiveConnectionConfig, databaseName, oldTableName, tableMetaRequest.tableInfo, getResourceManager());	JSONObject response = new JSONObject();	response.put("job", job);	return Response.status(Response.Status.ACCEPTED).entity(response).build();	} catch (ServiceException e) {	
exception occurred while creatint table for db with details 

public Response deleteTable(@PathParam("database_id") String databaseName, @PathParam("table_id") String tableName) {	try {	Job job = proxy.deleteTable(databaseName, tableName, getResourceManager());	JSONObject response = new JSONObject();	response.put("job", job);	return Response.status(Response.Status.ACCEPTED).entity(response).build();	} catch (ServiceException e) {	
exception occurred while deleting table for db tablename 

public Response getColumnStats(@PathParam("database_id") String databaseName, @PathParam("table_id") String tableName, try {	Job job = proxy.getColumnStatsJob(databaseName, tableName, columnName, getResourceManager());	JSONObject response = new JSONObject();	response.put("job", job);	return Response.status(Response.Status.ACCEPTED).entity(response).build();	} catch (ServiceException e) {	
exception occurred while fetching column stats 

public Response fetchColumnStats(@PathParam("database_id") String databaseName, @PathParam("table_id") String tablename, @PathParam("column_id") String columnName, @QueryParam("job_id") String jobId) {	try {	ColumnStats columnStats = proxy.fetchColumnStats(columnName, jobId, context);	columnStats.setTableName(tablename);	columnStats.setDatabaseName(databaseName);	JSONObject response = new JSONObject();	response.put("columnStats", columnStats);	return Response.status(Response.Status.ACCEPTED).entity(response).build();	} catch (ServiceException e) {	
exception occurred while fetching column stats for column and jobid 

========================= ambari sample_572 =========================

public void onAmbariEvent(ServiceInstalledEvent event) {	
received event 

String stackName = event.getStackName();	String stackVersion = event.getStackVersion();	String serviceName = event.getServiceName();	Lock lock = m_locksByService.get(serviceName);	lock.lock();	try {	if (null == m_alertDispatchDao.findDefaultServiceGroup(clusterId, serviceName)) {	try {	m_alertDispatchDao.createDefaultGroup(clusterId, serviceName);	} catch (AmbariException ambariException) {	
unable to create a default alert group for 

public void onAmbariEvent(ServiceRemovedEvent event) {	
received event 

public void onAmbariEvent(ServiceRemovedEvent event) {	try {	m_clusters.get().getClusterById(event.getClusterId());	} catch (AmbariException e) {	
unable to retrieve cluster with id 

}	String serviceName = event.getServiceName();	Lock lock = m_locksByService.get(serviceName);	lock.lock();	try {	List<AlertDefinitionEntity> definitions = m_definitionDao.findByService(event.getClusterId(), event.getServiceName());	for (AlertDefinitionEntity definition : definitions) {	try {	m_definitionDao.remove(definition);	} catch (Exception exception) {	
unable to remove alert definition 

try {	m_definitionDao.remove(definition);	} catch (Exception exception) {	}	}	AlertGroupEntity group = m_alertDispatchDao.findGroupByName(event.getClusterId(), event.getServiceName());	if (null != group && group.isDefault()) {	try {	m_alertDispatchDao.remove(group);	} catch (Exception exception) {	
unable to remove default alert group 

========================= ambari sample_4528 =========================

return false;	}	if (input.getType().isServerAction()) {	visitedServerSideTasks.add(input);	}	return true;	};	};	List<StageWrapper> results = new ArrayList<>(stageWrappers);	if (LOG.isDebugEnabled()) {	
ru initial 

return false;	}	if (input.getType().isServerAction()) {	visitedServerSideTasks.add(input);	}	return true;	};	};	List<StageWrapper> results = new ArrayList<>(stageWrappers);	if (LOG.isDebugEnabled()) {	
ru final 

========================= ambari sample_3202 =========================

public void start() {	
starting scheduler 

public void start() {	try {	executionScheduler.startScheduler(configuration.getExecutionSchedulerStartDelay());	schedulerAvailable = true;	} catch (AmbariException e) {	
unable to start scheduler no recurring tasks will be scheduled 

public void stop() {	
stopping scheduler 

public void stop() {	schedulerAvailable = false;	try {	executionScheduler.stopScheduler();	} catch (AmbariException e) {	
unable to stop scheduler no new recurring tasks will be scheduled 

public void scheduleJob(Trigger trigger) {	
scheduling job 

public void scheduleJob(Trigger trigger) {	if (isSchedulerAvailable()) {	try {	executionScheduler.scheduleJob(trigger);	} catch (SchedulerException e) {	
unable to add trigger for execution job 

public void scheduleJob(Trigger trigger) {	if (isSchedulerAvailable()) {	try {	executionScheduler.scheduleJob(trigger);	} catch (SchedulerException e) {	}	} else {	
scheduler unavailable cannot schedule jobs 

public void scheduleBatch(RequestExecution requestExecution) throws AmbariException {	if (!isSchedulerAvailable()) {	throw new AmbariException("Scheduler unavailable.");	}	try {	if (!executionScheduler.isSchedulerStarted()) {	executionScheduler.startScheduler(null);	}	} catch (SchedulerException e) {	
unable to determine scheduler state 

if (schedule != null) {	String triggerExpression = schedule.getScheduleExpression();	Date startDate = null;	Date endDate = null;	try {	String startTime = schedule.getStartTime();	String endTime = schedule.getEndTime();	startDate = startTime != null && !startTime.isEmpty() ? DateUtils.convertToDate(startTime) : new Date();	endDate = endTime != null && !endTime.isEmpty() ? DateUtils.convertToDate(endTime) : null;	} catch (ParseException e) {	
unable to parse starttime endtime 

try {	String startTime = schedule.getStartTime();	String endTime = schedule.getEndTime();	startDate = startTime != null && !startTime.isEmpty() ? DateUtils.convertToDate(startTime) : new Date();	endDate = endTime != null && !endTime.isEmpty() ? DateUtils.convertToDate(endTime) : null;	} catch (ParseException e) {	}	Trigger trigger = newTrigger() .withIdentity(REQUEST_EXECUTION_TRIGGER_PREFIX + "-" + requestExecution.getId(), ExecutionJob.LINEAR_EXECUTION_TRIGGER_GROUP) .withSchedule(cronSchedule(triggerExpression) .withMisfireHandlingInstructionFireAndProceed()) .forJob(firstJobDetail) .startAt(startDate) .endAt(endDate) .build();	try {	executionScheduler.scheduleJob(trigger);	
scheduled trigger next fire time 

String startTime = schedule.getStartTime();	String endTime = schedule.getEndTime();	startDate = startTime != null && !startTime.isEmpty() ? DateUtils.convertToDate(startTime) : new Date();	endDate = endTime != null && !endTime.isEmpty() ? DateUtils.convertToDate(endTime) : null;	} catch (ParseException e) {	}	Trigger trigger = newTrigger() .withIdentity(REQUEST_EXECUTION_TRIGGER_PREFIX + "-" + requestExecution.getId(), ExecutionJob.LINEAR_EXECUTION_TRIGGER_GROUP) .withSchedule(cronSchedule(triggerExpression) .withMisfireHandlingInstructionFireAndProceed()) .forJob(firstJobDetail) .startAt(startDate) .endAt(endDate) .build();	try {	executionScheduler.scheduleJob(trigger);	} catch (SchedulerException e) {	
unable to schedule request execution 

Trigger trigger = newTrigger() .withIdentity(REQUEST_EXECUTION_TRIGGER_PREFIX + "-" + requestExecution.getId(), ExecutionJob.LINEAR_EXECUTION_TRIGGER_GROUP) .withSchedule(cronSchedule(triggerExpression) .withMisfireHandlingInstructionFireAndProceed()) .forJob(firstJobDetail) .startAt(startDate) .endAt(endDate) .build();	try {	executionScheduler.scheduleJob(trigger);	} catch (SchedulerException e) {	throw new AmbariException(e.getMessage());	}	} else {	Trigger trigger = newTrigger() .forJob(firstJobDetail) .withIdentity(REQUEST_EXECUTION_TRIGGER_PREFIX + "-" + requestExecution.getId(), ExecutionJob.LINEAR_EXECUTION_TRIGGER_GROUP) .withSchedule(simpleSchedule().withMisfireHandlingInstructionFireNow()) .startNow() .build();	try {	executionScheduler.scheduleJob(trigger);	
scheduled trigger next fire time 

try {	executionScheduler.scheduleJob(trigger);	} catch (SchedulerException e) {	throw new AmbariException(e.getMessage());	}	} else {	Trigger trigger = newTrigger() .forJob(firstJobDetail) .withIdentity(REQUEST_EXECUTION_TRIGGER_PREFIX + "-" + requestExecution.getId(), ExecutionJob.LINEAR_EXECUTION_TRIGGER_GROUP) .withSchedule(simpleSchedule().withMisfireHandlingInstructionFireNow()) .startNow() .build();	try {	executionScheduler.scheduleJob(trigger);	} catch (SchedulerException e) {	
unable to schedule request execution 

ListIterator<BatchRequest> iterator = batchRequests.listIterator(batchRequests.size());	String nextJobName = null;	while (iterator.hasPrevious()) {	BatchRequest batchRequest = iterator.previous();	String jobName = getJobName(requestExecution.getId(), batchRequest.getOrderId());	Integer separationSeconds = requestExecution.getBatch() .getBatchSettings().getBatchSeparationInSeconds();	jobDetail = newJob(BatchRequestJob.class) .withIdentity(jobName, ExecutionJob.LINEAR_EXECUTION_JOB_GROUP) .usingJobData(ExecutionJob.NEXT_EXECUTION_JOB_NAME_KEY, nextJobName) .usingJobData(ExecutionJob.NEXT_EXECUTION_JOB_GROUP_KEY, ExecutionJob.LINEAR_EXECUTION_JOB_GROUP) .usingJobData(BatchRequestJob.BATCH_REQUEST_EXECUTION_ID_KEY, requestExecution.getId()) .usingJobData(BatchRequestJob.BATCH_REQUEST_BATCH_ID_KEY, batchRequest.getOrderId()) .usingJobData(BatchRequestJob.BATCH_REQUEST_CLUSTER_NAME_KEY, requestExecution.getClusterName()) .usingJobData(BatchRequestJob.NEXT_EXECUTION_SEPARATION_SECONDS, separationSeconds != null ? separationSeconds : 0) .storeDurably() .build();	try {	executionScheduler.addJob(jobDetail);	} catch (SchedulerException e) {	
failed to add job detail 

Batch batch = requestExecution.getBatch();	if (batch != null) {	List<BatchRequest> batchRequests = batch.getBatchRequests();	if (batchRequests != null) {	for (BatchRequest batchRequest : batchRequests) {	String jobName = getJobName(requestExecution.getId(), batchRequest.getOrderId());	LOG.debug("Deleting Job, jobName = {}", jobName);	try {	executionScheduler.deleteJob(JobKey.jobKey(jobName, ExecutionJob.LINEAR_EXECUTION_JOB_GROUP));	} catch (SchedulerException e) {	
unable to delete job 

private BatchRequestResponse convertToBatchRequestResponse(ClientResponse clientResponse) {	BatchRequestResponse batchRequestResponse = new BatchRequestResponse();	int retCode = clientResponse.getStatus();	batchRequestResponse.setReturnCode(retCode);	String responseString = clientResponse.getEntity(String.class);	LOG.debug("Processing API response: status={}, body={}", retCode, responseString);	Map<String, Object> httpResponseMap;	try {	httpResponseMap = gson.<Map<String, Object>>fromJson(responseString, Map.class);	
processing responce as json 

private BatchRequestResponse convertToBatchRequestResponse(ClientResponse clientResponse) {	BatchRequestResponse batchRequestResponse = new BatchRequestResponse();	int retCode = clientResponse.getStatus();	batchRequestResponse.setReturnCode(retCode);	String responseString = clientResponse.getEntity(String.class);	LOG.debug("Processing API response: status={}, body={}", retCode, responseString);	Map<String, Object> httpResponseMap;	try {	httpResponseMap = gson.<Map<String, Object>>fromJson(responseString, Map.class);	} catch (JsonSyntaxException e) {	
response is not valid json object recording as is 

}	}	boolean markCompleted = false;	if (firstBatchRequest != null) {	String jobName = getJobName(executionId, firstBatchRequest.getOrderId());	JobKey jobKey = JobKey.jobKey(jobName, ExecutionJob.LINEAR_EXECUTION_JOB_GROUP);	JobDetail jobDetail;	try {	jobDetail = executionScheduler.getJobDetail(jobKey);	} catch (SchedulerException e) {	
unable to retrieve job details from scheduler job 

throw new AmbariException("Too many triggers defined for job. " + "job: " + jobKey);	}	Trigger trigger = triggers.get(0);	if (!trigger.mayFireAgain() || (trigger.getFinalFireTime() != null && !DateUtils.isFutureTime(trigger.getFinalFireTime()))) {	markCompleted = true;	}	} else {	markCompleted = true;	}	} catch (SchedulerException e) {	
unable to retrieve triggers for job 

========================= ambari sample_2940 =========================

public void onAuthenticationFailure(HttpServletRequest request, HttpServletResponse response, AuthenticationException exception) throws IOException, ServletException {	
authfailurehandler onauthenticationfailure 

========================= ambari sample_1395 =========================

public void handleMessage(HiveMessage hiveMessage) {	Object message = hiveMessage.getMessage();	if(message instanceof RegisterActor){	RegisterActor registerActor = (RegisterActor) message;	ActorRef actorRef = registerActor.getActorRef();	this.getContext().watch(actorRef);	
registered new actor 

public void handleMessage(HiveMessage hiveMessage) {	Object message = hiveMessage.getMessage();	if(message instanceof RegisterActor){	RegisterActor registerActor = (RegisterActor) message;	ActorRef actorRef = registerActor.getActorRef();	this.getContext().watch(actorRef);	
registration for at 

public void handleMessage(HiveMessage hiveMessage) {	Object message = hiveMessage.getMessage();	if(message instanceof RegisterActor){	RegisterActor registerActor = (RegisterActor) message;	ActorRef actorRef = registerActor.getActorRef();	this.getContext().watch(actorRef);	}else if(message instanceof Terminated){	Terminated terminated = (Terminated) message;	ActorRef actor = terminated.actor();	
received terminate for actor with message 

public void handleMessage(HiveMessage hiveMessage) {	Object message = hiveMessage.getMessage();	if(message instanceof RegisterActor){	RegisterActor registerActor = (RegisterActor) message;	ActorRef actorRef = registerActor.getActorRef();	this.getContext().watch(actorRef);	}else if(message instanceof Terminated){	Terminated terminated = (Terminated) message;	ActorRef actor = terminated.actor();	
termination for at 

public void handleMessage(HiveMessage hiveMessage) {	Object message = hiveMessage.getMessage();	if(message instanceof RegisterActor){	RegisterActor registerActor = (RegisterActor) message;	ActorRef actorRef = registerActor.getActorRef();	this.getContext().watch(actorRef);	}else if(message instanceof Terminated){	Terminated terminated = (Terminated) message;	ActorRef actor = terminated.actor();	}else{	
received unknown message 

========================= ambari sample_708 =========================

public RESPONSE get(WebResource resource) throws IOException {	
get 

public RESPONSE get(WebResource resource) throws IOException {	InputStream inputStream = readFrom(resource, "GET", null, new HashMap<String, String>());	recordLastCurlCommand(String.format("curl \"" + resource.toString() + "\""));	String responseJson = IOUtils.toString(inputStream);	
response 

public RESPONSE post(WebResource resource, MultivaluedMapImpl data) throws IOException {	
post 

public RESPONSE post(WebResource resource, MultivaluedMapImpl data) throws IOException {	
data 

public RESPONSE put(WebResource resource, MultivaluedMapImpl data) throws IOException {	
put 

public RESPONSE delete(WebResource resource, MultivaluedMapImpl data) throws IOException {	
delete 

private InputStream getInputStream(HttpURLConnection connection) throws IOException {	int responseCode = connection.getResponseCode();	if (responseCode >= Response.Status.BAD_REQUEST.getStatusCode()) {	String message = connection.getResponseMessage();	if (connection.getErrorStream() != null) {	message = IOUtils.toString(connection.getErrorStream());	}	
got error response for url response code 

========================= ambari sample_1053 =========================

public AmbariManagementControllerImpl(ActionManager actionManager, Clusters clusters, Injector injector) throws Exception {	this.clusters = clusters;	this.actionManager = actionManager;	this.injector = injector;	injector.injectMembers(this);	gson = injector.getInstance(Gson.class);	
initializing the ambarimanagementcontrollerimpl 

public synchronized void createHostComponents(Set<ServiceComponentHostRequest> requests) throws AmbariException, AuthorizationException {	if (requests.isEmpty()) {	
received an empty requests set 

}	if (configs.containsKey(request.getVersionTag())) {	throw new AmbariException(MessageFormat.format("Configuration with tag ''{0}'' exists for ''{1}''", request.getVersionTag(), request.getType()));	}	StackId stackId = null;	if (null != service) {	try {	Service svc = cluster.getService(service);	stackId = svc.getDesiredStackId();	} catch (AmbariException ambariException) {	
adding configurations for even though its parent service is not installed 

try {	Service svc = cluster.getService(service);	stackId = svc.getDesiredStackId();	} catch (AmbariException ambariException) {	}	}	if (null == stackId) {	stackId = cluster.getDesiredStackVersion();	}	Config config = createConfig(cluster, stackId, request.getType(), requestProperties, request.getVersionTag(), propertiesAttributes);	
creating configuration with tag to cluster for configuration type 

public Set<MemberResponse> getMembers(Set<MemberRequest> requests) throws AmbariException {	final Set<MemberResponse> responses = new HashSet<>();	for (MemberRequest request: requests) {	
received a getmembers request 

for (Cluster c : allClusters.values()) {	ClusterResponse cr = c.convertToResponse();	cr.setDesiredConfigs(c.getDesiredConfigs());	cr.setDesiredServiceConfigVersions(c.getActiveServiceConfigVersions());	cr.setCredentialStoreServiceProperties(getCredentialStoreServiceProperties());	response.add(cr);	}	StringBuilder builder = new StringBuilder();	if (LOG.isDebugEnabled()) {	clusters.debugDump(builder);	
cluster state for cluster 

private Set<ServiceComponentHostResponse> getHostComponents( ServiceComponentHostRequest request) throws AmbariException {	
processing request 

private Set<ServiceComponentHostResponse> getHostComponents( ServiceComponentHostRequest request) throws AmbariException {	if (request.getClusterName() == null || request.getClusterName().isEmpty()) {	IllegalArgumentException e = new IllegalArgumentException("Invalid arguments, cluster name should not be null");	
cluster not specified in request 

private Set<ServiceComponentHostResponse> getHostComponents( ServiceComponentHostRequest request) throws AmbariException {	if (request.getClusterName() == null || request.getClusterName().isEmpty()) {	IllegalArgumentException e = new IllegalArgumentException("Invalid arguments, cluster name should not be null");	throw e;	}	final Cluster cluster;	try {	cluster = clusters.getCluster(request.getClusterName());	} catch (ClusterNotFoundException e) {	
cluster not found 

}	final Cluster cluster;	try {	cluster = clusters.getCluster(request.getClusterName());	} catch (ClusterNotFoundException e) {	throw new ParentObjectNotFoundException("Parent Cluster resource doesn't exist", e);	}	if (request.getHostname() != null) {	try {	if (!clusters.getClustersForHost(request.getHostname()).contains(cluster)) {	
host doesn t belong to cluster 

cluster = clusters.getCluster(request.getClusterName());	} catch (ClusterNotFoundException e) {	throw new ParentObjectNotFoundException("Parent Cluster resource doesn't exist", e);	}	if (request.getHostname() != null) {	try {	if (!clusters.getClustersForHost(request.getHostname()).contains(cluster)) {	throw new ParentObjectNotFoundException("Parent Host resource doesn't exist", new HostNotFoundException(request.getClusterName(), request.getHostname()));	}	} catch (HostNotFoundException e) {	
host not found 

throw new ParentObjectNotFoundException("Parent Host resource doesn't exist", new HostNotFoundException(request.getClusterName(), request.getHostname()));	}	} catch (HostNotFoundException e) {	throw new ParentObjectNotFoundException("Parent Host resource doesn't exist", new HostNotFoundException(request.getClusterName(), request.getHostname()));	}	}	if (request.getComponentName() != null) {	if (StringUtils.isBlank(request.getServiceName())) {	String serviceName = findServiceName(cluster, request.getComponentName());	if (StringUtils.isBlank(serviceName)) {	
unable to find service for component 

throw new HostNotFoundException(cluster.getClusterName(), sch.getHostName());	}	MaintenanceState effectiveMaintenanceState = maintenanceStateHelper.getEffectiveState(sch, host);	if(filterByMaintenanceState(request, effectiveMaintenanceState)) {	continue;	}	r.setMaintenanceState(effectiveMaintenanceState.name());	response.add(r);	} catch (ServiceComponentHostNotFoundException e) {	if (request.getServiceName() == null || request.getComponentName() == null) {	
ignoring not specified host component 

}	MaintenanceState effectiveMaintenanceState = maintenanceStateHelper.getEffectiveState(sch, host);	if(filterByMaintenanceState(request, effectiveMaintenanceState)) {	continue;	}	r.setMaintenanceState(effectiveMaintenanceState.name());	response.add(r);	} catch (ServiceComponentHostNotFoundException e) {	if (request.getServiceName() == null || request.getComponentName() == null) {	} else {	
servicecomponenthost not found 

AuthorizationHelper.verifyAuthorization(ResourceType.CLUSTER, cluster.getResourceId(), RoleAuthorization.AUTHORIZATIONS_UPDATE_CLUSTER);	List<ConfigurationResponse> configurationResponses = new LinkedList<>();	ServiceConfigVersionResponse serviceConfigVersionResponse = null;	if (desiredConfigs != null && request.getServiceConfigVersionRequest() != null) {	String msg = "Unable to set desired configs and rollback at same time, request = " + request;	LOG.error(msg);	throw new IllegalArgumentException(msg);	}	if (!cluster.getClusterName().equals(request.getClusterName())) {	if (LOG.isDebugEnabled()) {	
received cluster name change request from to 

if (!desiredConfigs.isEmpty()) {	Set<Config> configs = new HashSet<>();	String note = null;	for (ConfigurationRequest cr : desiredConfigs) {	String configType = cr.getType();	if (null != cr.getProperties()) {	Map<String, Config> all = cluster.getConfigsByType(configType);	if (null == all || !all.containsKey(cr.getVersionTag()) || cr.getProperties().size() > 0) {	cr.setClusterName(cluster.getClusterName());	configurationResponses.add(createConfiguration(cr));	
applying configuration with tag to cluster for configuration type 

Map<String, Config> existingConfigTypeToConfig = new HashMap();	for (Config config : configs) {	Config existingConfig = cluster.getDesiredConfigByType(config.getType());	existingConfigTypeToConfig.put(config.getType(), existingConfig);	}	String authName = getAuthName();	serviceConfigVersionResponse = cluster.addDesiredConfig(authName, configs, note);	if (serviceConfigVersionResponse != null) {	List<String> hosts = serviceConfigVersionResponse.getHosts();	int numAffectedHosts = null != hosts ? hosts.size() : 0;	
configchange changing default config cluster changed by service name config group num affected hosts during creation note 

existingConfigTypeToConfig.put(config.getType(), existingConfig);	}	String authName = getAuthName();	serviceConfigVersionResponse = cluster.addDesiredConfig(authName, configs, note);	if (serviceConfigVersionResponse != null) {	List<String> hosts = serviceConfigVersionResponse.getHosts();	int numAffectedHosts = null != hosts ? hosts.size() : 0;	for (Config config : configs) {	config.getVersion();	serviceConfigVersionResponse.getNote();	
configchange type tag version 

if (serviceConfigVersionResponse != null) {	List<String> hosts = serviceConfigVersionResponse.getHosts();	int numAffectedHosts = null != hosts ? hosts.size() : 0;	for (Config config : configs) {	config.getVersion();	serviceConfigVersionResponse.getNote();	Map<String, String> configKeyToAction = getConfigKeyDeltaToAction(existingConfigTypeToConfig.get(config.getType()), config.getProperties());	Map<String, List<String>> actionToListConfigKeys = inverseMapByValue(configKeyToAction);	if (!actionToListConfigKeys.isEmpty()) {	String configOutput = getActionToConfigListAsString(actionToListConfigKeys);	
configchange config type was modified with the following keys 

cluster.setCurrentStackVersion(desiredVersion);	}	boolean requiresHostListUpdate = request.getHostNames() != null && !request.getHostNames().isEmpty();	if (requiresHostListUpdate) {	clusters.mapAndPublishHostsToCluster( request.getHostNames(), request.getClusterName());	}	if (null != request.getProvisioningState()) {	State oldProvisioningState = cluster.getProvisioningState();	State provisioningState = State.valueOf(request.getProvisioningState());	if (provisioningState != State.INIT && provisioningState != State.INSTALLED) {	
invalid cluster provisioning state cannot be set on the cluster 

}	if (null != request.getProvisioningState()) {	State oldProvisioningState = cluster.getProvisioningState();	State provisioningState = State.valueOf(request.getProvisioningState());	if (provisioningState != State.INIT && provisioningState != State.INSTALLED) {	throw new IllegalArgumentException( "Invalid cluster provisioning state " + provisioningState + " cannot be set on cluster " + request.getClusterName());	}	if (provisioningState != oldProvisioningState) {	boolean isStateTransitionValid = State.isValidDesiredStateTransition( oldProvisioningState, provisioningState);	if (!isStateTransitionValid) {	
invalid cluster provisioning state cannot be set on the cluster because the current state is 

throw new AuthorizationException("The authenticated user does not have authorization to perform Kerberos-specific operations");	}	try {	requestStageContainer = kerberosHelper.executeCustomOperations(cluster, requestProperties, requestStageContainer, kerberosHelper.getManageIdentitiesDirective(requestProperties));	} catch (KerberosOperationException e) {	throw new IllegalArgumentException(e.getMessage(), e);	}	} else {	boolean forceToggleKerberos = kerberosHelper.getForceToggleKerberosDirective(requestProperties);	if (forceToggleKerberos || (cluster.getSecurityType() != securityType)) {	
received cluster security type change request from to forced 

if (!potentialSch.getHostState().equals(HostState.HEARTBEAT_LOST) && potentialSch.getMaintenanceState() != MaintenanceState.ON && host.getMaintenanceState(cluster.getClusterId()) == MaintenanceState.OFF) {	potentialHosts.add(potentialSch);	}	}	}	if (!potentialHosts.isEmpty()) {	clientSchs.put(sc.getName(), potentialHosts);	}	}	}	
client hosts for reinstall 

configCredentials = configHelper.getCredentialStoreEnabledProperties(stackId, clusterService);	configCredentialsForService.put(clusterService.getName(), configCredentials);	}	execCmd.setConfigurationCredentials(configCredentials);	Map<String, String> commandParams = new TreeMap<>();	if (commandParamsInp != null) {	commandParams.putAll(commandParamsInp);	}	for (Service service : cluster.getServices().values()) {	ServiceInfo serviceInfoInstance = servicesMap.get(service.getName());	
iterating service type instance in createhostaction 

}	execCmd.setConfigurationCredentials(configCredentials);	Map<String, String> commandParams = new TreeMap<>();	if (commandParamsInp != null) {	commandParams.putAll(commandParamsInp);	}	for (Service service : cluster.getServices().values()) {	ServiceInfo serviceInfoInstance = servicesMap.get(service.getName());	String serviceType = serviceInfoInstance.getServiceType();	if (serviceType != null) {	
adding service type info in createhostaction 

hostParams.put(UNLIMITED_KEY_JCE_REQUIRED, (unlimitedKeyJCEPolicyRequired) ? "true" : "false");	execCmd.setHostLevelParams(hostParams);	Map<String, String> roleParams = new TreeMap<>();	if (isUpgradeSuspended) {	cluster.addSuspendedUpgradeParameters(commandParams, roleParams);	}	execCmd.setRoleParams(roleParams);	execCmd.setCommandParams(commandParams);	execCmdWrapper.setVersions(cluster);	if (execCmd.getConfigurationTags().containsKey("cluster-env")) {	
ambarimanagementcontrollerimpl createhostaction created executioncommand for host role rolecommand and command id with cluster env tags cluster env tag 

protected RequestStageContainer doStageCreation(RequestStageContainer requestStages, Cluster cluster, Map<State, List<Service>> changedServices, Map<State, List<ServiceComponent>> changedComps, Map<String, Map<State, List<ServiceComponentHost>>> changedScHosts, Map<String, String> requestParameters, Map<String, String> requestProperties, boolean runSmokeTest, boolean reconfigureClients) throws AmbariException {	if ((changedServices == null || changedServices.isEmpty()) && (changedComps == null || changedComps.isEmpty()) && (changedScHosts == null || changedScHosts.isEmpty())) {	
created stages 

case STARTED: StackId stackId = serviceComponent.getDesiredStackId();	ComponentInfo compInfo = ambariMetaInfo.getComponent( stackId.getStackName(), stackId.getStackVersion(), scHost.getServiceName(), scHost.getServiceComponentName());	if (oldSchState == State.INSTALLED || oldSchState == State.STARTING || true) {	roleCommand = RoleCommand.START;	event = new ServiceComponentHostStartEvent( scHost.getServiceComponentName(), scHost.getHostName(), nowTimestamp);	} else {	String error = "Invalid transition for" + " servicecomponenthost" + ", clusterName=" + cluster.getClusterName() + ", clusterId=" + cluster.getClusterId() + ", serviceName=" + scHost.getServiceName() + ", componentName=" + scHost.getServiceComponentName() + ", hostname=" + scHost.getHostName() + ", currentState=" + oldSchState + ", newDesiredState=" + newState;	if (compInfo.isMaster()) {	throw new AmbariException(error);	} else {	
ignoring 

requestParameters = new HashMap<>();	}	requestParameters.put(CLUSTER_PHASE_PROPERTY, requestProperties.get(CLUSTER_PHASE_PROPERTY));	}	Map<String, Map<String, String>> configurations = new TreeMap<>();	Map<String, Map<String, Map<String, String>>> configurationAttributes = new TreeMap<>();	Host host = clusters.getHost(scHost.getHostName());	Map<String, DesiredConfig> clusterDesiredConfigs = cluster.getDesiredConfigs();	Map<String, Map<String, String>> configTags = configHelper.getEffectiveDesiredTags(cluster, host.getHostName(), clusterDesiredConfigs);	if (newState == State.INSTALLED && skipInstallTaskForComponent(requestProperties, cluster, scHost)) {	
skipping create of install task for on 

Map<String, Map<String, Map<String, String>>> configurationAttributes = new TreeMap<>();	Host host = clusters.getHost(scHost.getHostName());	Map<String, DesiredConfig> clusterDesiredConfigs = cluster.getDesiredConfigs();	Map<String, Map<String, String>> configTags = configHelper.getEffectiveDesiredTags(cluster, host.getHostName(), clusterDesiredConfigs);	if (newState == State.INSTALLED && skipInstallTaskForComponent(requestProperties, cluster, scHost)) {	scHost.setState(State.INSTALLING);	long now = System.currentTimeMillis();	try {	scHost.handleEvent(new ServiceComponentHostOpSucceededEvent(scHost.getServiceComponentName(), scHost.getHostName(), now));	} catch (InvalidStateTransitionException e) {	
error transitioning servicecomponenthost state to installed 

}	if (StringUtils.isBlank(stage.getHostParamsStage())) {	RepositoryVersionEntity repositoryVersion = component.getDesiredRepositoryVersion();	stage.setHostParamsStage(StageUtils.getGson().toJson( customCommandExecutionHelper.createDefaultHostParams(cluster, repositoryVersion.getStackId())));	}	customCommandExecutionHelper.addServiceCheckAction(stage, clientHost, smokeTestRole, nowTimestamp, serviceName, componentName, null, false, false);	}	RoleCommandOrder rco = getRoleCommandOrder(cluster);	RoleGraph rg = roleGraphFactory.createNew(rco);	if (CommandExecutionType.DEPENDENCY_ORDERED == configs.getStageExecutionType() && "INITIAL_START".equals (requestProperties.get("phase"))) {	
set dependency ordered commandexecutiontype on stage 

}	try {	kerberosHelper.ensureIdentities(cluster, serviceFilter, hostFilter, null, hostsToForceKerberosOperations, requestStages, kerberosHelper.getManageIdentitiesDirective(requestProperties));	} catch (KerberosOperationException e) {	throw new IllegalArgumentException(e.getMessage(), e);	}	}	List<Stage> stages = requestStages.getStages();	LOG.debug("Created {} stages", ((stages != null) ? stages.size() : 0));	} else {	
created stages 

private void createAndExecuteRefreshIncludeExcludeFilesActionForMasters(Map<String, String> serviceMasterMap, Map<String, Set<String>> masterSlaveHostsMap, String clusterName, boolean isDecommission) throws AmbariException {	serviceMasterMap.remove(Service.Type.HBASE.toString());	if (serviceMasterMap.isEmpty()) {	return;	}	
refresh include exclude files action will be executed for 

public void deleteMembers(java.util.Set<MemberRequest> requests) throws AmbariException {	for (MemberRequest request : requests) {	
received a delete member request 

public Set<ServiceComponentHostResponse> getHostComponents( Set<ServiceComponentHostRequest> requests) throws AmbariException {	
processing requests 

public Set<ServiceComponentHostResponse> getHostComponents( Set<ServiceComponentHostRequest> requests) throws AmbariException {	Set<ServiceComponentHostResponse> response = new HashSet<>();	for (ServiceComponentHostRequest request : requests) {	try {	response.addAll(getHostComponents(request));	} catch (ServiceComponentHostNotFoundException | ServiceComponentNotFoundException | ServiceNotFoundException e) {	if (requests.size() == 1) {	throw e;	} else {	
ignoring not found exception due to other requests 

if (requests.size() == 1) {	throw e;	} else {	}	} catch (ParentObjectNotFoundException e) {	boolean throwException = true;	if (requests.size() > 1 && HostNotFoundException.class.isInstance(e.getCause())) {	for (ServiceComponentHostRequest r : requests) {	if (r.getHostname() == null) {	throwException = false;	
hostnotfoundexception ignored 

ServiceInfo serviceInfo = ambariMetaInfo.getService(service);	File widgetDescriptorFile = serviceInfo.getWidgetsDescriptorFile();	if (widgetDescriptorFile != null && widgetDescriptorFile.exists()) {	widgetDescriptorFiles.add(widgetDescriptorFile);	}	} else {	File commonWidgetsFile = ambariMetaInfo.getCommonWidgetsDescriptorFile();	if (commonWidgetsFile != null && commonWidgetsFile.exists()) {	widgetDescriptorFiles.add(commonWidgetsFile);	} else {	
common widgets file with path s doesn t exist no cluster widgets will be created 

========================= ambari sample_3740 =========================

public MigrationModel hiveSavedQueryMigration(String username, String instance, String startDate, String endDate, ViewContext view, MigrationResponse migrationresult, String jobid) throws IOException, ItemNotFound {	long startTime = System.currentTimeMillis();	final Logger logger = Logger.getLogger(HiveSavedQueryMigrationUtility.class);	Connection connectionAmbaridb = null;	Connection connectionHuedb = null;	int i = 0, j = 0;	String sequenceName = "";	logger.info("-------------------------------------");	
hive saved query migration started 

public MigrationModel hiveSavedQueryMigration(String username, String instance, String startDate, String endDate, ViewContext view, MigrationResponse migrationresult, String jobid) throws IOException, ItemNotFound {	long startTime = System.currentTimeMillis();	final Logger logger = Logger.getLogger(HiveSavedQueryMigrationUtility.class);	Connection connectionAmbaridb = null;	Connection connectionHuedb = null;	int i = 0, j = 0;	String sequenceName = "";	logger.info("-------------------------------------");	logger.info("-------------------------------------");	
start date 

public MigrationModel hiveSavedQueryMigration(String username, String instance, String startDate, String endDate, ViewContext view, MigrationResponse migrationresult, String jobid) throws IOException, ItemNotFound {	long startTime = System.currentTimeMillis();	final Logger logger = Logger.getLogger(HiveSavedQueryMigrationUtility.class);	Connection connectionAmbaridb = null;	Connection connectionHuedb = null;	int i = 0, j = 0;	String sequenceName = "";	logger.info("-------------------------------------");	logger.info("-------------------------------------");	
enddate date 

public MigrationModel hiveSavedQueryMigration(String username, String instance, String startDate, String endDate, ViewContext view, MigrationResponse migrationresult, String jobid) throws IOException, ItemNotFound {	long startTime = System.currentTimeMillis();	final Logger logger = Logger.getLogger(HiveSavedQueryMigrationUtility.class);	Connection connectionAmbaridb = null;	Connection connectionHuedb = null;	int i = 0, j = 0;	String sequenceName = "";	logger.info("-------------------------------------");	logger.info("-------------------------------------");	
instance is 

public MigrationModel hiveSavedQueryMigration(String username, String instance, String startDate, String endDate, ViewContext view, MigrationResponse migrationresult, String jobid) throws IOException, ItemNotFound {	long startTime = System.currentTimeMillis();	final Logger logger = Logger.getLogger(HiveSavedQueryMigrationUtility.class);	Connection connectionAmbaridb = null;	Connection connectionHuedb = null;	int i = 0, j = 0;	String sequenceName = "";	logger.info("-------------------------------------");	logger.info("-------------------------------------");	
hue username is 

final Logger logger = Logger.getLogger(HiveSavedQueryMigrationUtility.class);	Connection connectionAmbaridb = null;	Connection connectionHuedb = null;	int i = 0, j = 0;	String sequenceName = "";	logger.info("-------------------------------------");	logger.info("-------------------------------------");	HiveSavedQueryMigrationImplementation hivesavedqueryimpl = new HiveSavedQueryMigrationImplementation();/* creating Implementation object  */ QuerySetHueDb huedatabase = null;	if (view.getProperties().get("huedrivername").contains("mysql")) {	huedatabase = new MysqlQuerySetHueDb();	
hue database is mysql 

Connection connectionHuedb = null;	int i = 0, j = 0;	String sequenceName = "";	logger.info("-------------------------------------");	logger.info("-------------------------------------");	HiveSavedQueryMigrationImplementation hivesavedqueryimpl = new HiveSavedQueryMigrationImplementation();/* creating Implementation object  */ QuerySetHueDb huedatabase = null;	if (view.getProperties().get("huedrivername").contains("mysql")) {	huedatabase = new MysqlQuerySetHueDb();	} else if (view.getProperties().get("huedrivername").contains("postgresql")) {	huedatabase = new PostgressQuerySetHueDb();	
hue database is postgres 

String sequenceName = "";	logger.info("-------------------------------------");	logger.info("-------------------------------------");	HiveSavedQueryMigrationImplementation hivesavedqueryimpl = new HiveSavedQueryMigrationImplementation();/* creating Implementation object  */ QuerySetHueDb huedatabase = null;	if (view.getProperties().get("huedrivername").contains("mysql")) {	huedatabase = new MysqlQuerySetHueDb();	} else if (view.getProperties().get("huedrivername").contains("postgresql")) {	huedatabase = new PostgressQuerySetHueDb();	} else if (view.getProperties().get("huedrivername").contains("sqlite")) {	huedatabase = new SqliteQuerySetHueDb();	
hue database is sqlite 

logger.info("-------------------------------------");	HiveSavedQueryMigrationImplementation hivesavedqueryimpl = new HiveSavedQueryMigrationImplementation();/* creating Implementation object  */ QuerySetHueDb huedatabase = null;	if (view.getProperties().get("huedrivername").contains("mysql")) {	huedatabase = new MysqlQuerySetHueDb();	} else if (view.getProperties().get("huedrivername").contains("postgresql")) {	huedatabase = new PostgressQuerySetHueDb();	} else if (view.getProperties().get("huedrivername").contains("sqlite")) {	huedatabase = new SqliteQuerySetHueDb();	} else if (view.getProperties().get("huedrivername").contains("oracle")) {	huedatabase = new OracleQuerySetHueDb();	
hue database is oracle 

} else if (view.getProperties().get("huedrivername").contains("postgresql")) {	huedatabase = new PostgressQuerySetHueDb();	} else if (view.getProperties().get("huedrivername").contains("sqlite")) {	huedatabase = new SqliteQuerySetHueDb();	} else if (view.getProperties().get("huedrivername").contains("oracle")) {	huedatabase = new OracleQuerySetHueDb();	}	QuerySetAmbariDB ambaridatabase = null;	if (view.getProperties().get("ambaridrivername").contains("mysql")) {	ambaridatabase = new MysqlQuerySetAmbariDB();	
ambari database is mysql 

} else if (view.getProperties().get("huedrivername").contains("sqlite")) {	huedatabase = new SqliteQuerySetHueDb();	} else if (view.getProperties().get("huedrivername").contains("oracle")) {	huedatabase = new OracleQuerySetHueDb();	}	QuerySetAmbariDB ambaridatabase = null;	if (view.getProperties().get("ambaridrivername").contains("mysql")) {	ambaridatabase = new MysqlQuerySetAmbariDB();	} else if (view.getProperties().get("ambaridrivername").contains("postgresql")) {	ambaridatabase = new PostgressQuerySetAmbariDB();	
ambari database is postgres 

} else if (view.getProperties().get("huedrivername").contains("oracle")) {	huedatabase = new OracleQuerySetHueDb();	}	QuerySetAmbariDB ambaridatabase = null;	if (view.getProperties().get("ambaridrivername").contains("mysql")) {	ambaridatabase = new MysqlQuerySetAmbariDB();	} else if (view.getProperties().get("ambaridrivername").contains("postgresql")) {	ambaridatabase = new PostgressQuerySetAmbariDB();	} else if (view.getProperties().get("ambaridrivername").contains("oracle")) {	ambaridatabase = new OracleQuerySetAmbariDB();	
ambari database is oracle 

ArrayList<HiveModel> dbpojoHiveSavedQuery = new ArrayList<HiveModel>();	HashSet<String> udfSet = new HashSet<>();	try {	String[] usernames = username.split(",");	int totalQueries = 0;	for (int l = 0; l < usernames.length; l++) {	connectionHuedb = DataSourceHueDatabase.getInstance(view.getProperties().get("huedrivername"), view.getProperties().get("huejdbcurl"), view.getProperties().get("huedbusername"), view.getProperties().get("huedbpassword")).getConnection(); /* fetching connection to hue DB */ logger.info("Hue database connection successful");	username = usernames[l];	migrationresult.setProgressPercentage(0);	dbpojoHiveSavedQuery = hivesavedqueryimpl.fetchFromHuedb(username, startDate, endDate, connectionHuedb, huedatabase); /* fetching data from hue db and storing it in to a model */ totalQueries += dbpojoHiveSavedQuery.size();	
migration started for user 

ArrayList<HiveModel> dbpojoHiveSavedQuery = new ArrayList<HiveModel>();	HashSet<String> udfSet = new HashSet<>();	try {	String[] usernames = username.split(",");	int totalQueries = 0;	for (int l = 0; l < usernames.length; l++) {	connectionHuedb = DataSourceHueDatabase.getInstance(view.getProperties().get("huedrivername"), view.getProperties().get("huejdbcurl"), view.getProperties().get("huedbusername"), view.getProperties().get("huedbpassword")).getConnection(); /* fetching connection to hue DB */ logger.info("Hue database connection successful");	username = usernames[l];	migrationresult.setProgressPercentage(0);	dbpojoHiveSavedQuery = hivesavedqueryimpl.fetchFromHuedb(username, startDate, endDate, connectionHuedb, huedatabase); /* fetching data from hue db and storing it in to a model */ totalQueries += dbpojoHiveSavedQuery.size();	
queries fetched from hue 

HashSet<String> udfSet = new HashSet<>();	try {	String[] usernames = username.split(",");	int totalQueries = 0;	for (int l = 0; l < usernames.length; l++) {	connectionHuedb = DataSourceHueDatabase.getInstance(view.getProperties().get("huedrivername"), view.getProperties().get("huejdbcurl"), view.getProperties().get("huedbusername"), view.getProperties().get("huedbpassword")).getConnection(); /* fetching connection to hue DB */ logger.info("Hue database connection successful");	username = usernames[l];	migrationresult.setProgressPercentage(0);	dbpojoHiveSavedQuery = hivesavedqueryimpl.fetchFromHuedb(username, startDate, endDate, connectionHuedb, huedatabase); /* fetching data from hue db and storing it in to a model */ totalQueries += dbpojoHiveSavedQuery.size();	for (i = 0; i < dbpojoHiveSavedQuery.size(); i++) {	
the query fetched from hue 

String[] usernames = username.split(",");	int totalQueries = 0;	for (int l = 0; l < usernames.length; l++) {	connectionHuedb = DataSourceHueDatabase.getInstance(view.getProperties().get("huedrivername"), view.getProperties().get("huejdbcurl"), view.getProperties().get("huedbusername"), view.getProperties().get("huedbpassword")).getConnection(); /* fetching connection to hue DB */ logger.info("Hue database connection successful");	username = usernames[l];	migrationresult.setProgressPercentage(0);	dbpojoHiveSavedQuery = hivesavedqueryimpl.fetchFromHuedb(username, startDate, endDate, connectionHuedb, huedatabase); /* fetching data from hue db and storing it in to a model */ totalQueries += dbpojoHiveSavedQuery.size();	for (i = 0; i < dbpojoHiveSavedQuery.size(); i++) {	}	if (dbpojoHiveSavedQuery.size() == 0) /* if no data has been fetched from hue db according to search criteria */ {	
no queries has been selected for the user between dates 

int tableIdSavedQuery = hivesavedqueryimpl.fetchInstancetablename(connectionAmbaridb, instance, ambaridatabase, SAVEDQUERYSEQUENCE); /* fetching the instance table name for migration saved query  from the given instance name */ int tableIdFileResource = hivesavedqueryimpl.fetchInstancetablename(connectionAmbaridb, instance, ambaridatabase, FILERESOURCESEQUENCE);	int tableIdUdf = hivesavedqueryimpl.fetchInstancetablename(connectionAmbaridb, instance, ambaridatabase, UDFSEQUENCE);	sequenceName = SAVEDQUERYTABLE + "_" + tableIdSavedQuery + "_" + SEQ;	int savedQuerySequence = hivesavedqueryimpl.fetchSequenceno(connectionAmbaridb, ambaridatabase, sequenceName);	sequenceName = FILETABLE + "_" + tableIdFileResource + "_" + SEQ;	int fileResourceSequence = hivesavedqueryimpl.fetchSequenceno(connectionAmbaridb, ambaridatabase, sequenceName);	sequenceName = UDFTABLE + "_" + tableIdUdf + "_" + SEQ;	int udfSequence = hivesavedqueryimpl.fetchSequenceno(connectionAmbaridb, ambaridatabase, sequenceName);	for (i = 0; i < dbpojoHiveSavedQuery.size(); i++) {	logger.info("_____________________");	
loop no 

sequenceName = UDFTABLE + "_" + tableIdUdf + "_" + SEQ;	int udfSequence = hivesavedqueryimpl.fetchSequenceno(connectionAmbaridb, ambaridatabase, sequenceName);	for (i = 0; i < dbpojoHiveSavedQuery.size(); i++) {	logger.info("_____________________");	logger.info("_____________________");	float calc = ((float) (i + 1)) / dbpojoHiveSavedQuery.size() * 100;	int progressPercentage = Math.round(calc);	migrationresult.setProgressPercentage(progressPercentage);	migrationresult.setNumberOfQueryTransfered(i + 1);	getResourceManager(view).update(migrationresult, jobid);	
query fetched from hue 

sequenceName = UDFTABLE + "_" + tableIdUdf + "_" + SEQ;	int udfSequence = hivesavedqueryimpl.fetchSequenceno(connectionAmbaridb, ambaridatabase, sequenceName);	for (i = 0; i < dbpojoHiveSavedQuery.size(); i++) {	logger.info("_____________________");	logger.info("_____________________");	float calc = ((float) (i + 1)) / dbpojoHiveSavedQuery.size() * 100;	int progressPercentage = Math.round(calc);	migrationresult.setProgressPercentage(progressPercentage);	migrationresult.setNumberOfQueryTransfered(i + 1);	getResourceManager(view).update(migrationresult, jobid);	
table name are fetched from instance name 

sequenceName = UDFTABLE + "_" + tableIdUdf + "_" + SEQ;	int udfSequence = hivesavedqueryimpl.fetchSequenceno(connectionAmbaridb, ambaridatabase, sequenceName);	for (i = 0; i < dbpojoHiveSavedQuery.size(); i++) {	logger.info("_____________________");	logger.info("_____________________");	float calc = ((float) (i + 1)) / dbpojoHiveSavedQuery.size() * 100;	int progressPercentage = Math.round(calc);	migrationresult.setProgressPercentage(progressPercentage);	migrationresult.setNumberOfQueryTransfered(i + 1);	getResourceManager(view).update(migrationresult, jobid);	
hql and logs file are saved in temporary directory 

float calc = ((float) (i + 1)) / dbpojoHiveSavedQuery.size() * 100;	int progressPercentage = Math.round(calc);	migrationresult.setProgressPercentage(progressPercentage);	migrationresult.setNumberOfQueryTransfered(i + 1);	getResourceManager(view).update(migrationresult, jobid);	maxCountforSavequeryAmbaridb = i + savedQuerySequence + 1;	time = hivesavedqueryimpl.getTime();/* getting system time */ if (usernames[l].equals("all")) {	username = dbpojoHiveSavedQuery.get(i).getOwnerName();	}	dirNameforHiveSavedquery = "/user/" + username + "/hive/scripts/hive-query-" + maxCountforSavequeryAmbaridb + "-" + time + " logger.info("Directory will be creted in HDFS" + dirNameforHiveSavedquery);	
row inserted in hive history table 

int progressPercentage = Math.round(calc);	migrationresult.setProgressPercentage(progressPercentage);	migrationresult.setNumberOfQueryTransfered(i + 1);	getResourceManager(view).update(migrationresult, jobid);	maxCountforSavequeryAmbaridb = i + savedQuerySequence + 1;	time = hivesavedqueryimpl.getTime();/* getting system time */ if (usernames[l].equals("all")) {	username = dbpojoHiveSavedQuery.get(i).getOwnerName();	}	dirNameforHiveSavedquery = "/user/" + username + "/hive/scripts/hive-query-" + maxCountforSavequeryAmbaridb + "-" + time + " logger.info("Directory will be creted in HDFS" + dirNameforHiveSavedquery);	if (view.getProperties().get("KerberoseEnabled").equals("y")) {	
kerberose enabled 

maxCountforSavequeryAmbaridb = i + savedQuerySequence + 1;	time = hivesavedqueryimpl.getTime();/* getting system time */ if (usernames[l].equals("all")) {	username = dbpojoHiveSavedQuery.get(i).getOwnerName();	}	dirNameforHiveSavedquery = "/user/" + username + "/hive/scripts/hive-query-" + maxCountforSavequeryAmbaridb + "-" + time + " logger.info("Directory will be creted in HDFS" + dirNameforHiveSavedquery);	if (view.getProperties().get("KerberoseEnabled").equals("y")) {	hivesavedqueryimpl.createDirHiveSecured(dirNameforHiveSavedquery, view.getProperties().get("namenode_URI_Ambari"), username, view.getProperties().get("PrincipalUserName"));	hivesavedqueryimpl.putFileinHdfsSecured(ConfigurationCheckImplementation.getHomeDir() + "query.hql", dirNameforHiveSavedquery, view.getProperties().get("namenode_URI_Ambari"), username, view.getProperties().get("PrincipalUserName"));	hivesavedqueryimpl.putFileinHdfsSecured(ConfigurationCheckImplementation.getHomeDir() + "logs", dirNameforHiveSavedquery, view.getProperties().get("namenode_URI_Ambari"), username, view.getProperties().get("PrincipalUserName"));	} else {	
kerberose not enabled 

}	}	sequenceName = SAVEDQUERYTABLE + "_" + tableIdSavedQuery + "_" + SEQ;	hivesavedqueryimpl.updateSequenceno(connectionAmbaridb, maxCountforSavequeryAmbaridb, sequenceName, ambaridatabase);	sequenceName = FILETABLE + "_" + tableIdFileResource + "_" + SEQ;	hivesavedqueryimpl.updateSequenceno(connectionAmbaridb, maxCountforFileResourceAmbaridb, sequenceName, ambaridatabase);	sequenceName = UDFTABLE + "_" + tableIdUdf + "_" + SEQ;	hivesavedqueryimpl.updateSequenceno(connectionAmbaridb, maxCountforUdfAmbaridb, sequenceName, ambaridatabase);	connectionAmbaridb.commit();	}	
migration completed for user 

}	sequenceName = SAVEDQUERYTABLE + "_" + tableIdSavedQuery + "_" + SEQ;	hivesavedqueryimpl.updateSequenceno(connectionAmbaridb, maxCountforSavequeryAmbaridb, sequenceName, ambaridatabase);	sequenceName = FILETABLE + "_" + tableIdFileResource + "_" + SEQ;	hivesavedqueryimpl.updateSequenceno(connectionAmbaridb, maxCountforFileResourceAmbaridb, sequenceName, ambaridatabase);	sequenceName = UDFTABLE + "_" + tableIdUdf + "_" + SEQ;	hivesavedqueryimpl.updateSequenceno(connectionAmbaridb, maxCountforUdfAmbaridb, sequenceName, ambaridatabase);	connectionAmbaridb.commit();	}	}	
migration completed 

if (totalQueries == 0) {	migrationresult.setNumberOfQueryTransfered(0);	migrationresult.setTotalNoQuery(0);	} else {	migrationresult.setNumberOfQueryTransfered(totalQueries);	migrationresult.setTotalNoQuery(totalQueries);	migrationresult.setProgressPercentage(100);	}	getResourceManager(view).update(migrationresult, jobid);	} catch (SQLException e) {	
sql exception 

} else {	migrationresult.setNumberOfQueryTransfered(totalQueries);	migrationresult.setTotalNoQuery(totalQueries);	migrationresult.setProgressPercentage(100);	}	getResourceManager(view).update(migrationresult, jobid);	} catch (SQLException e) {	migrationresult.setError("SQL Exception: " + e.getMessage());	try {	connectionAmbaridb.rollback();	
roll back done 

migrationresult.setNumberOfQueryTransfered(totalQueries);	migrationresult.setTotalNoQuery(totalQueries);	migrationresult.setProgressPercentage(100);	}	getResourceManager(view).update(migrationresult, jobid);	} catch (SQLException e) {	migrationresult.setError("SQL Exception: " + e.getMessage());	try {	connectionAmbaridb.rollback();	} catch (SQLException e1) {	
rollback error 

migrationresult.setProgressPercentage(100);	}	getResourceManager(view).update(migrationresult, jobid);	} catch (SQLException e) {	migrationresult.setError("SQL Exception: " + e.getMessage());	try {	connectionAmbaridb.rollback();	} catch (SQLException e1) {	}	} catch (ClassNotFoundException e1) {	
class not found 

getResourceManager(view).update(migrationresult, jobid);	} catch (SQLException e) {	migrationresult.setError("SQL Exception: " + e.getMessage());	try {	connectionAmbaridb.rollback();	} catch (SQLException e1) {	}	} catch (ClassNotFoundException e1) {	migrationresult.setError("Class not found Exception: " + e1.getMessage());	} catch (ParseException e) {	
parseexception 

migrationresult.setError("SQL Exception: " + e.getMessage());	try {	connectionAmbaridb.rollback();	} catch (SQLException e1) {	}	} catch (ClassNotFoundException e1) {	migrationresult.setError("Class not found Exception: " + e1.getMessage());	} catch (ParseException e) {	migrationresult.setError("Parse Exception: " + e.getMessage());	} catch (URISyntaxException e) {	
urisyntaxexception 

connectionAmbaridb.rollback();	} catch (SQLException e1) {	}	} catch (ClassNotFoundException e1) {	migrationresult.setError("Class not found Exception: " + e1.getMessage());	} catch (ParseException e) {	migrationresult.setError("Parse Exception: " + e.getMessage());	} catch (URISyntaxException e) {	migrationresult.setError("URI Syntax Exception: " + e.getMessage());	} catch (PropertyVetoException e) {	
propertyvetoexception 

}	} catch (ClassNotFoundException e1) {	migrationresult.setError("Class not found Exception: " + e1.getMessage());	} catch (ParseException e) {	migrationresult.setError("Parse Exception: " + e.getMessage());	} catch (URISyntaxException e) {	migrationresult.setError("URI Syntax Exception: " + e.getMessage());	} catch (PropertyVetoException e) {	migrationresult.setError("Property Veto Exception: " + e.getMessage());	} catch (Exception e) {	
generic exception 

} catch (URISyntaxException e) {	migrationresult.setError("URI Syntax Exception: " + e.getMessage());	} catch (PropertyVetoException e) {	migrationresult.setError("Property Veto Exception: " + e.getMessage());	} catch (Exception e) {	migrationresult.setError("Exception: " + e.getMessage());	} finally {	if (null != connectionAmbaridb) try {	connectionAmbaridb.close();	} catch (SQLException e) {	
error in connection close 

}	hivesavedqueryimpl.deleteFileQueryhql(ConfigurationCheckImplementation.getHomeDir());	hivesavedqueryimpl.deleteFileQueryLogs(ConfigurationCheckImplementation.getHomeDir());	long stopTime = System.currentTimeMillis();	long elapsedTime = stopTime - startTime;	MigrationModel model = new MigrationModel();	migrationresult.setJobtype("hivesavedquerymigration");	migrationresult.setTotalTimeTaken(String.valueOf(elapsedTime));	getResourceManager(view).update(migrationresult, jobid);	logger.info("-------------------------------");	
hive saved query migration end 

========================= ambari sample_1287 =========================

public boolean init(String inputDesc, String fieldName, String mapClassCode, MapFieldDescriptor mapFieldDescriptor) {	init(inputDesc, fieldName, mapClassCode);	newValue = ((MapFieldNameDescriptor)mapFieldDescriptor).getNewFieldName();	if (StringUtils.isEmpty(newValue)) {	
map field value is empty 

public Object apply(Map<String, Object> jsonObj, Object value) {	if (newValue != null) {	jsonObj.remove(getFieldName());	jsonObj.put(newValue, value);	} else {	
apply new fieldname is null so transformation is not applied 

========================= ambari sample_1638 =========================

public Void call() throws Exception {	Type type = new TypeToken<Map<String, Map<String, Object>>>(){}.getType();	Gson gson = new Gson();	Map<String, Map<String, Object>> latestUrlMap = null;	Long time = System.currentTimeMillis();	try {	if (sourceUri.startsWith("http")) {	URLStreamProvider streamProvider = new URLStreamProvider( LOOKUP_CONNECTION_TIMEOUT, LOOKUP_READ_TIMEOUT, null, null, null);	
loading latest url info for stack from 

URLStreamProvider streamProvider = new URLStreamProvider( LOOKUP_CONNECTION_TIMEOUT, LOOKUP_READ_TIMEOUT, null, null, null);	latestUrlMap = gson.fromJson(new InputStreamReader( streamProvider.readFrom(sourceUri)), type);	} else {	File jsonFile = null;	if (sourceUri.charAt(0) == '.') {	jsonFile = new File(stackRepoFolder, sourceUri);	} else {	jsonFile = new File(sourceUri);	}	if (jsonFile.exists()) {	
loading latest url info for stack from 

if (sourceUri.charAt(0) == '.') {	jsonFile = new File(stackRepoFolder, sourceUri);	} else {	jsonFile = new File(sourceUri);	}	if (jsonFile.exists()) {	latestUrlMap = gson.fromJson(new FileReader(jsonFile), type);	}	}	} catch (Exception e) {	
could not load the uri for stack from using default repository values 

} else {	jsonFile = new File(sourceUri);	}	if (jsonFile.exists()) {	latestUrlMap = gson.fromJson(new FileReader(jsonFile), type);	}	}	} catch (Exception e) {	throw e;	} finally {	
loaded uri in ms 

VersionDefinitionXml.Merger merger = new VersionDefinitionXml.Merger();	for (Entry<String, String> versionEntry : osMap.entrySet()) {	String osFamily = os_family.find(versionEntry.getKey());	if (null == osFamily) {	String alias = os_family.getAliases().get(versionEntry.getKey());	if (null != alias) {	osFamily = os_family.find(alias);	}	}	if (null == osFamily || !oses.contains(osFamily)) {	
stack cannot resolve os to the supported ones family 

}	try {	URI uri = new URI(uriString);	VersionDefinitionXml xml = parsedMap.containsKey(uri) ? parsedMap.get(uri) : timedVDFLoad(uri);	version = (null == version) ? xml.release.version : version;	merger.add(version, xml);	if (!parsedMap.containsKey(uri)) {	parsedMap.put(uri, xml);	}	} catch (Exception e) {	
could not load version definition for identified by 

private VersionDefinitionXml timedVDFLoad(URI uri) throws Exception {	long time = System.currentTimeMillis();	try {	return VersionDefinitionXml.load(uri.toURL());	} finally {	
loaded vdf in ms 

========================= ambari sample_3255 =========================

ClusterEntity clusterEntity = new ClusterEntity();	clusterEntity.setClusterName(clusterName);	clusterEntity.setDesiredStack(stackEntity);	clusterEntity.setResource(resourceEntity);	if (securityType != null) {	clusterEntity.setSecurityType(securityType);	}	try {	clusterDAO.create(clusterEntity);	} catch (RollbackException e) {	
unable to create cluster 

public void deleteCluster(String clusterName) throws AmbariException {	Cluster cluster = getCluster(clusterName);	if (!cluster.canBeRemoved()) {	throw new AmbariException("Could not delete cluster" + ", clusterName=" + clusterName);	}	
deleting cluster 

========================= ambari sample_3039 =========================

public void testGoodManagerCredentials() throws Exception {	AmbariLdapAuthoritiesPopulator authoritiesPopulator = createMock(AmbariLdapAuthoritiesPopulator.class);	AmbariLdapAuthenticationProvider provider = createMockBuilder(AmbariLdapAuthenticationProvider.class) .addMockedMethod("loadLdapAuthenticationProvider") .addMockedMethod("isLdapEnabled") .withConstructor(users, configuration, ldapConfiguration, authoritiesPopulator).createMock();	org.springframework.security.core.AuthenticationException exception = createNiceMock(org.springframework.security.core.AuthenticationException.class);	expect(exception.getCause()).andReturn(exception).atLeastOnce();	expect(provider.isLdapEnabled()).andReturn(true);	expect(provider.loadLdapAuthenticationProvider("notFound")).andThrow(exception);	Logger log = createNiceMock(Logger.class);	provider.LOG = log;	
ldap manager credentials 

public void testBadManagerCredentials() throws Exception {	AmbariLdapAuthoritiesPopulator authoritiesPopulator = createMock(AmbariLdapAuthoritiesPopulator.class);	AmbariLdapAuthenticationProvider provider = createMockBuilder(AmbariLdapAuthenticationProvider.class) .addMockedMethod("loadLdapAuthenticationProvider") .addMockedMethod("isLdapEnabled") .withConstructor(users, configuration, ldapConfiguration, authoritiesPopulator).createMock();	org.springframework.ldap.AuthenticationException cause = createNiceMock(org.springframework.ldap.AuthenticationException.class);	org.springframework.security.core.AuthenticationException exception = createNiceMock(org.springframework.security.core.AuthenticationException.class);	expect(exception.getCause()).andReturn(cause).atLeastOnce();	expect(provider.isLdapEnabled()).andReturn(true);	expect(provider.loadLdapAuthenticationProvider("notFound")).andThrow(exception);	Logger log = createNiceMock(Logger.class);	provider.LOG = log;	
ldap manager credentials 

========================= ambari sample_1907 =========================

public void initialize() throws URISyntaxException, MalformedURLException {	ClassLoader classLoader = Thread.currentThread().getContextClassLoader();	if (classLoader == null) {	classLoader = getClass().getClassLoader();	}	URL hbaseResUrl = classLoader.getResource(HBASE_SITE_CONFIGURATION_FILE);	URL amsResUrl = classLoader.getResource(METRICS_SITE_CONFIGURATION_FILE);	
found hbase site configuration 

public void initialize() throws URISyntaxException, MalformedURLException {	ClassLoader classLoader = Thread.currentThread().getContextClassLoader();	if (classLoader == null) {	classLoader = getClass().getClassLoader();	}	URL hbaseResUrl = classLoader.getResource(HBASE_SITE_CONFIGURATION_FILE);	URL amsResUrl = classLoader.getResource(METRICS_SITE_CONFIGURATION_FILE);	
found metric service configuration 

public Set<String> getAmshbaseWhitelist() {	ClassLoader classLoader = Thread.currentThread().getContextClassLoader();	if (classLoader == null) {	classLoader = getClass().getClassLoader();	}	BufferedReader br = null;	String strLine;	Set<String> whitelist = new HashSet<>();	try(InputStream inputStream = classLoader.getResourceAsStream(AMSHBASE_METRICS_WHITESLIST_FILE)) {	if (inputStream == null) {	
ams hbase metrics whitelist file not present 

}	br = new BufferedReader(new InputStreamReader(inputStream));	while ((strLine = br.readLine()) != null)   {	strLine = strLine.trim();	if (StringUtils.isEmpty(strLine)) {	continue;	}	whitelist.add(strLine);	}	} catch (Exception ex) {	
unable to read ams hbase metric whitelist file 

========================= ambari sample_374 =========================

public void write(String block, InputFileMarker inputMarker) throws Exception {	if (logSpooler == null) {	if (inputMarker.getInput().getClass().isAssignableFrom(InputFile.class)) {	InputFile input = (InputFile) inputMarker.getInput();	logSpooler = createSpooler(input.getFilePath());	s3Uploader = createUploader(input.getInputDescriptor().getType());	logSpooler.add(block);	} else {	
cannot write from non local file 

========================= ambari sample_1643 =========================

public static void checkJaasConfiguration() throws AmbariException {	if (config.isKerberosJaasConfigurationCheckEnabled()) {	
checking ambari server kerberos credentials 

public static void checkJaasConfiguration() throws AmbariException {	if (config.isKerberosJaasConfigurationCheckEnabled()) {	String jaasConfPath = System.getProperty(JAVA_SECURITY_AUTH_LOGIN_CONFIG);	javax.security.auth.login.Configuration jaasConf = javax.security.auth.login.Configuration.getConfiguration();	AppConfigurationEntry[] jaasConfEntries = jaasConf.getAppConfigurationEntry(HTTP_SPNEGO_STANDARD_ENTRY);	if (jaasConfEntries == null) {	
can t find entry in 

boolean krb5LoginModulePresent = false;	for (AppConfigurationEntry ace : jaasConfEntries) {	if (KRB5_LOGIN_MODULE.equals(ace.getLoginModuleName())) {	krb5LoginModulePresent = true;	Map<String, ?> options = ace.getOptions();	if ((options != null)) {	if (options.containsKey("keyTab")) {	String keytabPath = (String) options.get("keyTab");	File keytabFile = new File(keytabPath);	if (!keytabFile.exists()) {	
doesn t exist 

for (AppConfigurationEntry ace : jaasConfEntries) {	if (KRB5_LOGIN_MODULE.equals(ace.getLoginModuleName())) {	krb5LoginModulePresent = true;	Map<String, ?> options = ace.getOptions();	if ((options != null)) {	if (options.containsKey("keyTab")) {	String keytabPath = (String) options.get("keyTab");	File keytabFile = new File(keytabPath);	if (!keytabFile.exists()) {	} else if (!keytabFile.canRead()) {	
unable to read please check the file access permissions for user user name 

krb5LoginModulePresent = true;	Map<String, ?> options = ace.getOptions();	if ((options != null)) {	if (options.containsKey("keyTab")) {	String keytabPath = (String) options.get("keyTab");	File keytabFile = new File(keytabPath);	if (!keytabFile.exists()) {	} else if (!keytabFile.canRead()) {	}	} else {	
can t find keytab option in module of entry in 

if ((options != null)) {	if (options.containsKey("keyTab")) {	String keytabPath = (String) options.get("keyTab");	File keytabFile = new File(keytabPath);	if (!keytabFile.exists()) {	} else if (!keytabFile.canRead()) {	}	} else {	}	if (!options.containsKey("principal")) {	
can t find principal option in module of entry in 

} else if (!keytabFile.canRead()) {	}	} else {	}	if (!options.containsKey("principal")) {	}	}	}	}	if (!krb5LoginModulePresent) {	
can t find module in entry in 

}	}	try {	LoginContext loginContext = loginContextHelper.createLoginContext(HTTP_SPNEGO_STANDARD_ENTRY);	loginContext.login();	loginContext.logout();	} catch (LoginException le) {	LOG.error(le.getMessage());	throw new AmbariException( "Ambari Server Kerberos credentials check failed. \n" + "Check KDC availability and JAAS configuration in " + jaasConfPath);	}	
ambari server kerberos credentials check passed 

}	try {	LoginContext loginContext = loginContextHelper.createLoginContext(HTTP_SPNEGO_STANDARD_ENTRY);	loginContext.login();	loginContext.logout();	} catch (LoginException le) {	LOG.error(le.getMessage());	throw new AmbariException( "Ambari Server Kerberos credentials check failed. \n" + "Check KDC availability and JAAS configuration in " + jaasConfPath);	}	} else {	
skipping ambari server kerberos credentials check 

========================= ambari sample_3691 =========================

static Optional<List<String>> createColumnQueries(List<ColumnInfo> oldColumns, List<ColumnInfo> newColumns, boolean cascade) {	if (isNullOrEmpty(oldColumns) || isNullOrEmpty(newColumns)) {	LOG.error("oldColumns = {} or newColumns = {} was null.", oldColumns, newColumns);	throw new IllegalArgumentException("Old or new columns cannot be empty.");	}	if (oldColumns.size() > newColumns.size()) {	
removing columns from hive table is not supported yet 

static Optional<String> createStoragePropertyQuery(List<String> oldBucketCols, List<ColumnOrder> oldSortCols, String oldNumBuckets, List<String> newBucketCols, List<ColumnOrder> newSortCols, String newNumBuckets) {	StringBuilder queryBuilder = new StringBuilder();	boolean foundDiff = false;	if (isNullOrEmpty(newBucketCols)) {	if (!isNullOrEmpty(oldBucketCols)) {	
cannot handle removal of all the columns from buckets 

queryBuilder.append(" CLUSTERED BY ( ").append(Joiner.on(",").join(newBucketCols)).append(" ) ");	}	if (!isNullOrEmpty(newSortCols)) {	queryBuilder.append(" SORTED BY ( ") .append(Joiner.on(",").join(FluentIterable.from(newSortCols).transform(new Function<ColumnOrder, String>() {	public String apply(@Nullable ColumnOrder input) {	return input.getColumnName() + " " + input.getOrder().name();	}	}))) .append(" ) ");	}	if (Strings.isNullOrEmpty(newNumBuckets)) {	
number of buckets cannot be empty if clustered by is mentioned 

static Optional<String> createTableRenameQuery(String oldDatabaseName, String oldTableName, String newDatabaseName, String newTableName) {	if (Strings.isNullOrEmpty(oldTableName) || Strings.isNullOrEmpty(newTableName)) {	
oldtablename or newtablename is empty 

========================= ambari sample_661 =========================

public OozieProxyImpersonator(ViewContext viewContext) {	this.viewContext = viewContext;	hdfsFileUtils = new HDFSFileUtils(viewContext);	workflowFilesService = new WorkflowFilesService(hdfsFileUtils);	this.oozieDelegate = new OozieDelegate(viewContext);	assetResource = new AssetResource(viewContext);	if (PROJ_MANAGER_ENABLED) {	workflowManagerService = new WorkflowManagerService(viewContext);	}	
oozieproxyimpersonator initialized for instance s 


submit workflow job called 


save workflow called 


publish asset called 

public Response handleGet(@Context HttpHeaders headers, @Context UriInfo ui) {	try {	return oozieDelegate.consumeService(headers, ui.getAbsolutePath() .getPath(), ui.getQueryParameters(), HttpMethod.GET, null);	} catch (Exception ex) {	
error in get proxy 

public Response handlePost(String xml, @Context HttpHeaders headers, try {	return oozieDelegate.consumeService(headers, ui.getAbsolutePath() .getPath(), ui.getQueryParameters(), HttpMethod.POST, xml);	} catch (Exception ex) {	
error in post proxy 

public Response handleDelete(@Context HttpHeaders headers, try {	return oozieDelegate.consumeService(headers, ui.getAbsolutePath() .getPath(), ui.getQueryParameters(), HttpMethod.POST, null);	} catch (Exception ex) {	
error in delete proxy 

public Response handlePut(String body, @Context HttpHeaders headers, try {	return oozieDelegate.consumeService(headers, ui.getAbsolutePath() .getPath(), ui.getQueryParameters(), HttpMethod.PUT, body);	} catch (Exception ex) {	
error in put proxy 

========================= ambari sample_1090 =========================

super.startUp();	InputStream inputStream = null;	String alertTemplatesFile = null;	try {	alertTemplatesFile = m_configuration.getAlertTemplateFile();	if (null != alertTemplatesFile) {	File file = new File(alertTemplatesFile);	inputStream = new FileInputStream(file);	}	} catch (Exception exception) {	
unable to load alert template file 

} catch (Exception exception) {	}	try {	JAXBContext context = JAXBContext.newInstance(AlertTemplates.class);	Unmarshaller unmarshaller = context.createUnmarshaller();	if (null == inputStream) {	inputStream = ClassLoader.getSystemResourceAsStream(AMBARI_ALERT_TEMPLATES);	}	m_alertTemplates = (AlertTemplates) unmarshaller.unmarshal(inputStream);	} catch (Exception exception) {	
unable to load alert template file outbound notifications will not be formatted 

protected void runOneIteration() throws Exception {	List<AlertNoticeEntity> pending = m_dao.findPendingNotices();	if (pending.size() == 0) {	return;	}	
there are pending alert notices about to be dispatched 

for (AlertNoticeEntity notice : notices) {	AlertHistoryEntity history = notice.getAlertHistory();	histories.add(history);	notification.CallbackIds.add(notice.getUuid());	}	try {	renderDigestNotificationContent(dispatcher, notification, histories, target);	DispatchRunnable runnable = new DispatchRunnable(dispatcher, notification);	m_executor.execute(runnable);	} catch (Exception exception) {	
unable to create notification for alerts 

} else {	for (AlertNoticeEntity notice : notices) {	AlertNotification notification = buildNotificationFromTarget(target);	AlertHistoryEntity history = notice.getAlertHistory();	notification.CallbackIds = Collections.singletonList(notice.getUuid());	try {	renderNotificationContent(dispatcher, notification, history, target);	DispatchRunnable runnable = new DispatchRunnable(dispatcher, notification);	m_executor.execute(runnable);	} catch (Exception exception) {	
unable to create notification for alert 

try {	renderNotificationContent(dispatcher, notification, history, target);	DispatchRunnable runnable = new DispatchRunnable(dispatcher, notification);	m_executor.execute(runnable);	} catch (Exception exception) {	notification.Callback.onFailure(notification.CallbackIds);	}	}	}	} catch (Exception e) {	
caught exception during alert notice dispatching 

private void updateAlertNotice(String uuid, NotificationState state) {	try {	AlertNoticeEntity entity = m_dao.findNoticeByUuid(uuid);	if (null == entity) {	
unable to find an alert notice with uuid 

private void updateAlertNotice(String uuid, NotificationState state) {	try {	AlertNoticeEntity entity = m_dao.findNoticeByUuid(uuid);	if (null == entity) {	return;	}	entity.setNotifyState(state);	m_dao.merge(entity);	} catch (Exception exception) {	
unable to update the alert notice with uuid to notifications will continue to be sent 

========================= ambari sample_3279 =========================

protected RequestStatus deleteResourcesAuthorized(Request request, Predicate predicate) throws SystemException, UnsupportedPropertyException, NoSuchResourceException, NoSuchParentResourceException {	Set<Resource> resources = getResources(new RequestImpl(null, null, null, null), predicate);	Set<Long> targetIds = new HashSet<>();	for (final Resource resource : resources) {	Long id = (Long) resource.getPropertyValue(ALERT_TARGET_ID);	targetIds.add(id);	}	for (Long targetId : targetIds) {	
deleting alert target 

========================= ambari sample_3629 =========================

String summariserName = JMeterUtils.getPropDefault("summariser.name", "summary");	if (summariserName.length() > 0) {	summariser = new Summariser(summariserName);	}	String jmeterLogFile = tmpDir + "/amsJmeterTestResults.jtl";	ResultCollector resultCollector = new ResultCollector(summariser);	resultCollector.setFilename(jmeterLogFile);	amsTestPlanTree.add(amsTestPlanTree.getArray()[0], resultCollector);	jmeterEngine.configure(amsTestPlanTree);	jmeterEngine.run();	
ams jmeter test started up successfully 

ResultCollector resultCollector = new ResultCollector(summariser);	resultCollector.setFilename(jmeterLogFile);	amsTestPlanTree.add(amsTestPlanTree.getArray()[0], resultCollector);	jmeterEngine.configure(amsTestPlanTree);	jmeterEngine.run();	} catch (Exception ioEx) {	amsTestPlanTree.clear();	jmeterEngine.askThreadsToStop();	jmeterEngine.stopTest();	JMeterContextService.endTest();	
error occurred while running ams load test 

========================= ambari sample_289 =========================

public void setup() {	
org apache hadoop sink timeline 

========================= ambari sample_266 =========================

public void migrateData(ViewInstanceEntity targetInstanceDefinition, ViewInstanceEntity sourceInstanceDefinition, boolean migrateOnce) throws ViewDataMigrationException {	ViewDataMigrationContextImpl migrationContext = getViewDataMigrationContext(targetInstanceDefinition, sourceInstanceDefinition);	if (migrateOnce) {	if (!isTargetEmpty(migrationContext)) {	
migration canceled because target instance is not empty 

public void migrateData(ViewInstanceEntity targetInstanceDefinition, ViewInstanceEntity sourceInstanceDefinition, boolean migrateOnce) throws ViewDataMigrationException {	ViewDataMigrationContextImpl migrationContext = getViewDataMigrationContext(targetInstanceDefinition, sourceInstanceDefinition);	if (migrateOnce) {	if (!isTargetEmpty(migrationContext)) {	return;	}	}	ViewDataMigrator dataMigrator = getViewDataMigrator(targetInstanceDefinition, migrationContext);	
running before migration hook 

}	ViewDataMigrator dataMigrator = getViewDataMigrator(targetInstanceDefinition, migrationContext);	if (!dataMigrator.beforeMigration()) {	String msg = "View " + targetInstanceDefinition.getInstanceName() + " canceled the migration process";	LOG.error(msg);	throw new ViewDataMigrationException(msg);	}	Map<String, Class> originClasses = migrationContext.getOriginEntityClasses();	Map<String, Class> currentClasses = migrationContext.getCurrentEntityClasses();	for (Map.Entry<String, Class> originEntity : originClasses.entrySet()) {	
migrating persistence entity 

LOG.error(msg);	throw new ViewDataMigrationException(msg);	}	Map<String, Class> originClasses = migrationContext.getOriginEntityClasses();	Map<String, Class> currentClasses = migrationContext.getCurrentEntityClasses();	for (Map.Entry<String, Class> originEntity : originClasses.entrySet()) {	if (currentClasses.containsKey(originEntity.getKey())) {	Class entity = currentClasses.get(originEntity.getKey());	dataMigrator.migrateEntity(originEntity.getValue(), entity);	} else {	
entity not found in target view 

Map<String, Class> originClasses = migrationContext.getOriginEntityClasses();	Map<String, Class> currentClasses = migrationContext.getCurrentEntityClasses();	for (Map.Entry<String, Class> originEntity : originClasses.entrySet()) {	if (currentClasses.containsKey(originEntity.getKey())) {	Class entity = currentClasses.get(originEntity.getKey());	dataMigrator.migrateEntity(originEntity.getValue(), entity);	} else {	dataMigrator.migrateEntity(originEntity.getValue(), null);	}	}	
migrating instance data 

Map<String, Class> currentClasses = migrationContext.getCurrentEntityClasses();	for (Map.Entry<String, Class> originEntity : originClasses.entrySet()) {	if (currentClasses.containsKey(originEntity.getKey())) {	Class entity = currentClasses.get(originEntity.getKey());	dataMigrator.migrateEntity(originEntity.getValue(), entity);	} else {	dataMigrator.migrateEntity(originEntity.getValue(), null);	}	}	dataMigrator.migrateInstanceData();	
running after migration hook 

for (Map.Entry<String, Class> originEntity : originClasses.entrySet()) {	if (currentClasses.containsKey(originEntity.getKey())) {	Class entity = currentClasses.get(originEntity.getKey());	dataMigrator.migrateEntity(originEntity.getValue(), entity);	} else {	dataMigrator.migrateEntity(originEntity.getValue(), null);	}	}	dataMigrator.migrateInstanceData();	dataMigrator.afterMigration();	
copying user permissions 

return false;	}	try {	for (Class entity : migrationContext.getCurrentEntityClasses().values()) {	if (migrationContext.getCurrentDataStore().findAll(entity, null).size() > 0) {	return false;	}	}	} catch (PersistenceException e) {	ViewInstanceDefinition current = migrationContext.getCurrentInstanceDefinition();	
persistence exception while check if instance is empty 

protected ViewDataMigrator getViewDataMigrator(ViewInstanceEntity currentInstanceDefinition, ViewDataMigrationContextImpl migrationContext) throws ViewDataMigrationException {	ViewDataMigrator dataMigrator;	
migrating data from to data version 

protected ViewDataMigrator getViewDataMigrator(ViewInstanceEntity currentInstanceDefinition, ViewDataMigrationContextImpl migrationContext) throws ViewDataMigrationException {	ViewDataMigrator dataMigrator;	if (migrationContext.getOriginDataVersion() == migrationContext.getCurrentDataVersion()) {	
instances of same version copying all data 

protected ViewDataMigrator getViewDataMigrator(ViewInstanceEntity currentInstanceDefinition, ViewDataMigrationContextImpl migrationContext) throws ViewDataMigrationException {	ViewDataMigrator dataMigrator;	if (migrationContext.getOriginDataVersion() == migrationContext.getCurrentDataVersion()) {	dataMigrator = new CopyAllDataMigrator(migrationContext);	} else {	try {	dataMigrator = currentInstanceDefinition.getDataMigrator(migrationContext);	if (dataMigrator == null) {	throw new ViewDataMigrationException("A view instance " + currentInstanceDefinition.getInstanceName() + " does not support migration.");	}	
data migrator loaded 

========================= ambari sample_3916 =========================

try {	fsUri = new URI(defaultFS);	String protocol = fsUri.getScheme();	String ambariSkipCheckValues = viewContext.getAmbariProperty(Constants.AMBARI_SKIP_HOME_DIRECTORY_CHECK_PROTOCOL_LIST);	List<String> protocolSkipList = (ambariSkipCheckValues == null? new LinkedList<String>() : Arrays.asList(ambariSkipCheckValues.split(",")));	if(null != protocol && protocolSkipList.contains(protocol)){	policy.setCheckHomeDirectory(false);	return policy;	}	} catch (URISyntaxException e) {	
error occurred while parsing the defaultfs uri 

========================= ambari sample_1028 =========================

public static void main(String[] args) throws Exception {	try {	
host names update started 

HostUpdateHelper hostUpdateHelper = injector.getInstance(HostUpdateHelper.class);	hostUpdateHelper.setHostChangesFile(hostChangesFile);	hostUpdateHelper.initHostChangesFileMap();	hostUpdateHelper.startPersistenceService();	hostUpdateHelper.validateHostChanges();	hostUpdateHelper.checkForSecurity();	hostUpdateHelper.updateHostsInDB();	hostUpdateHelper.updateHostsForTopologyRequests();	hostUpdateHelper.updateHostsForAlertsInDB();	hostUpdateHelper.updateHostsInConfigurations();	
host names update completed successfully 

hostUpdateHelper.startPersistenceService();	hostUpdateHelper.validateHostChanges();	hostUpdateHelper.checkForSecurity();	hostUpdateHelper.updateHostsInDB();	hostUpdateHelper.updateHostsForTopologyRequests();	hostUpdateHelper.updateHostsForAlertsInDB();	hostUpdateHelper.updateHostsInConfigurations();	hostUpdateHelper.stopPersistenceService();	} catch (Throwable e) {	if (e instanceof AmbariException) {	
exception occurred during host names update failed 

hostUpdateHelper.checkForSecurity();	hostUpdateHelper.updateHostsInDB();	hostUpdateHelper.updateHostsForTopologyRequests();	hostUpdateHelper.updateHostsForAlertsInDB();	hostUpdateHelper.updateHostsInConfigurations();	hostUpdateHelper.stopPersistenceService();	} catch (Throwable e) {	if (e instanceof AmbariException) {	throw (AmbariException)e;	}else{	
unexpected error host names update failed 

========================= ambari sample_2741 =========================

protected void startUp() throws Exception {	MAX_TIMEOUT_MINS = m_configuration.getStackUpgradeAutoRetryTimeoutMins();	MAX_TIMEOUT_MS = MAX_TIMEOUT_MINS * 60000L;	if (MAX_TIMEOUT_MINS < 1) {	
will not start service used to auto retry failed actions during stack upgrade since since the property is either invalid missing or set to 

protected void runOneIteration() throws Exception {	Map<String, Cluster> clusterMap = m_clustersProvider.get().getClusters();	for (Cluster cluster : clusterMap.values()) {	try {	
analyzing tasks for cluster that can be retried during stack upgrade 

protected void runOneIteration() throws Exception {	Map<String, Cluster> clusterMap = m_clustersProvider.get().getClusters();	for (Cluster cluster : clusterMap.values()) {	try {	Long effectiveRequestId = getActiveUpgradeRequestId(cluster);	if (effectiveRequestId != null) {	
upgrade is in progress with request id 

protected void runOneIteration() throws Exception {	Map<String, Cluster> clusterMap = m_clustersProvider.get().getClusters();	for (Cluster cluster : clusterMap.values()) {	try {	Long effectiveRequestId = getActiveUpgradeRequestId(cluster);	if (effectiveRequestId != null) {	retryHoldingCommandsInRequest(effectiveRequestId);	}	} catch (Exception e) {	
unable to analyze commands that may be retried for cluster with id exception 

private Long getActiveUpgradeRequestId(Cluster cluster) {	UpgradeEntity currentUpgrade = cluster.getUpgradeInProgress();	if (currentUpgrade == null) {	
there is no active upgrade in progress skip retrying failed tasks 

private Long getActiveUpgradeRequestId(Cluster cluster) {	UpgradeEntity currentUpgrade = cluster.getUpgradeInProgress();	if (currentUpgrade == null) {	return null;	}	Direction direction = currentUpgrade.getDirection();	RepositoryVersionEntity repositoryVersion = currentUpgrade.getRepositoryVersion();	
found an active upgrade with id direction 

========================= ambari sample_3280 =========================

public QueryResponse getResources(Type type, Request request, Predicate predicate) throws UnsupportedPropertyException, NoSuchResourceException, NoSuchParentResourceException, SystemException {	QueryResponse queryResponse = null;	ExtendedResourceProviderWrapper provider = ensureResourceProviderWrapper(type);	ensurePropertyProviders(type);	if (provider != null) {	if (LOG.isDebugEnabled()) {	
using resource provider for request type 

========================= ambari sample_3516 =========================

final CompletionService<Resource> completionService = new BufferedThreadPoolExecutorCompletionService<>(EXECUTOR_SERVICE);	for (Resource resource : resources) {	completionService.submit( getPopulateResourceCallable(resource, request, predicate, ticket));	}	Set<Resource> keepers = new HashSet<>();	try {	for (int i = 0; i < resources.size(); ++i) {	Future<Resource> resourceFuture = completionService.poll(COMPLETION_SERVICE_POLL_TIMEOUT, TimeUnit.MILLISECONDS);	if (resourceFuture == null) {	ticket.invalidate();	
timed out after waiting ms waiting for request 

public Throwable call() {	
skipping same exceptions for next minutes 

========================= ambari sample_3392 =========================

public synchronized <T extends Indexed> T load(Class<T> model, Object id) throws ItemNotFound {	
loading s s 

public synchronized <T extends Indexed> List<T> loadAll(Class<? extends T> model, FilteringStrategy filter) {	LinkedList<T> list = new LinkedList<T>();	
loading all s s 

public synchronized void delete(Class model, Object id) throws ItemNotFound {	
deleting s s 

========================= ambari sample_698 =========================

protected void aggregate(ResultSet rs, long startTime, long endTime) throws SQLException, IOException {	List<Long[]> timeSlices = getTimeSlices(startTime - serverTimeShiftAdjustment, endTime - serverTimeShiftAdjustment);	appAggregator.init();	Map<TimelineClusterMetric, MetricClusterAggregate> aggregateClusterMetrics = aggregateMetricsFromResultSet(rs, timeSlices);	
saving metric aggregates 

protected Condition prepareMetricQueryCondition(long startTime, long endTime) {	List<String> metricNames = new ArrayList<>();	boolean metricNamesNotCondition = false;	if (!StringUtils.isEmpty(skipAggrPatternStrings)) {	
skipping aggregation for metric patterns 

protected int processAggregateClusterMetrics(Map<TimelineClusterMetric, MetricClusterAggregate> aggregateClusterMetrics, TimelineMetric metric, List<Long[]> timeSlices) {	TimelineMetricMetadataKey appKey =  new TimelineMetricMetadataKey(metric.getMetricName(), metric.getAppId());	TimelineMetricMetadata metricMetadata = metadataManagerInstance.getMetadataCacheValue(appKey);	if (metricMetadata != null && !metricMetadata.isSupportsAggregates()) {	
skipping cluster aggregation for 

}	}	Map<Long, Double> interpolatedValuesMap = PostProcessingUtil.interpolate(timelineMetric.getMetricValues(), requiredTimestamps);	if (interpolatedValuesMap != null) {	for (Map.Entry<Long, Double> entry : interpolatedValuesMap.entrySet()) {	Double interpolatedValue = entry.getValue();	if (interpolatedValue != null) {	TimelineClusterMetric clusterMetric = new TimelineClusterMetric( timelineMetric.getMetricName(), timelineMetric.getAppId(), timelineMetric.getInstanceId(), entry.getKey(), timelineMetric.getType());	timelineClusterMetricMap.put(clusterMetric, interpolatedValue);	} else {	
cannot compute interpolated value hence skipping 

if (interpolatedValue != null) {	TimelineClusterMetric clusterMetric = new TimelineClusterMetric( timelineMetric.getMetricName(), timelineMetric.getAppId(), timelineMetric.getInstanceId(), entry.getKey(), timelineMetric.getType());	timelineClusterMetricMap.put(clusterMetric, interpolatedValue);	} else {	}	}	}	} else {	Double defaultNextSeenValue = null;	if (MapUtils.isEmpty(timeSliceValueMap) && MapUtils.isNotEmpty(timelineMetric.getMetricValues())) {	
no value found within range for metric 

timelineClusterMetricMap.put(clusterMetric, interpolatedValue);	} else {	}	}	}	} else {	Double defaultNextSeenValue = null;	if (MapUtils.isEmpty(timeSliceValueMap) && MapUtils.isNotEmpty(timelineMetric.getMetricValues())) {	Map.Entry<Long,Double> firstEntry  = timelineMetric.getMetricValues().firstEntry();	defaultNextSeenValue = firstEntry.getValue();	
found a data point outside timeslice range 

}	} else {	Double defaultNextSeenValue = null;	if (MapUtils.isEmpty(timeSliceValueMap) && MapUtils.isNotEmpty(timelineMetric.getMetricValues())) {	Map.Entry<Long,Double> firstEntry  = timelineMetric.getMetricValues().firstEntry();	defaultNextSeenValue = firstEntry.getValue();	}	for (int sliceNum = 0; sliceNum < timeSlices.size(); sliceNum++) {	Long[] timeSlice = timeSlices.get(sliceNum);	if (!timeSliceValueMap.containsKey(timeSlice[1])) {	
found an empty slice 

while (nextSeenValue == null && index < timeSlices.size()) {	nextTimeSlice = timeSlices.get(index++);	nextSeenValue = timeSliceValueMap.get(nextTimeSlice[1]);	}	if (nextSeenValue == null) {	nextSeenValue = defaultNextSeenValue;	}	Double interpolatedValue = PostProcessingUtil.interpolate(timeSlice[1], (prevTimeSlice != null ? prevTimeSlice[1] : null), lastSeenValue, (nextTimeSlice != null ? nextTimeSlice[1] : null), nextSeenValue);	if (interpolatedValue != null) {	TimelineClusterMetric clusterMetric = new TimelineClusterMetric( timelineMetric.getMetricName(), timelineMetric.getAppId(), timelineMetric.getInstanceId(), timeSlice[1], timelineMetric.getType());	
interpolated value 

nextSeenValue = timeSliceValueMap.get(nextTimeSlice[1]);	}	if (nextSeenValue == null) {	nextSeenValue = defaultNextSeenValue;	}	Double interpolatedValue = PostProcessingUtil.interpolate(timeSlice[1], (prevTimeSlice != null ? prevTimeSlice[1] : null), lastSeenValue, (nextTimeSlice != null ? nextTimeSlice[1] : null), nextSeenValue);	if (interpolatedValue != null) {	TimelineClusterMetric clusterMetric = new TimelineClusterMetric( timelineMetric.getMetricName(), timelineMetric.getAppId(), timelineMetric.getInstanceId(), timeSlice[1], timelineMetric.getType());	timelineClusterMetricMap.put(clusterMetric, interpolatedValue);	} else {	
cannot compute interpolated value hence skipping 

========================= ambari sample_362 =========================

}	} catch (ObjectNotFoundException e) {	}	}	if ( nonStartedState == null || ((nameNodeCount > 0 && !hasSecondary || hasJournal) && nameNodeActiveCount > 0)) {	return State.STARTED;	}	return nonStartedState;	}	} catch (AmbariException e) {	
can t determine service state 

========================= ambari sample_3687 =========================

public boolean updateConfigIfNeeded(SolrPropsConfig solrPropsConfig, SolrZkClient zkClient, File file, String separator, String downloadFolderLocation) throws IOException {	boolean result = false;	if (!FileUtils.contentEquals(file, new File(String.format("%s%s%s", downloadFolderLocation, separator, file.getName())))) {	
solr config file differs upload config set to zookeeper 

public void doIfConfigNotExist(SolrPropsConfig solrPropsConfig, ZkConfigManager zkConfigManager) throws IOException {	
config set does not exist for collection uploading it to zookeeper 

public void uploadMissingConfigFiles(SolrZkClient zkClient, ZkConfigManager zkConfigManager, String configName) throws IOException {	
check any of the configs files are missing for config 

public void uploadMissingConfigFiles(SolrZkClient zkClient, ZkConfigManager zkConfigManager, String configName) throws IOException {	for (String configFile : configFiles) {	if ("enumsConfig.xml".equals(configFile) && !hasEnumConfig) {	
config file is not needed for 

public void uploadMissingConfigFiles(SolrZkClient zkClient, ZkConfigManager zkConfigManager, String configName) throws IOException {	for (String configFile : configFiles) {	if ("enumsConfig.xml".equals(configFile) && !hasEnumConfig) {	continue;	}	String zkPath = String.format("%s/%s", configName, configFile);	if (zkConfigManager.configExists(zkPath)) {	
config file has already uploaded properly 

public void uploadMissingConfigFiles(SolrZkClient zkClient, ZkConfigManager zkConfigManager, String configName) throws IOException {	for (String configFile : configFiles) {	if ("enumsConfig.xml".equals(configFile) && !hasEnumConfig) {	continue;	}	String zkPath = String.format("%s/%s", configName, configFile);	if (zkConfigManager.configExists(zkPath)) {	} else {	
config file is missing reupload 

========================= ambari sample_1340 =========================

protected void configure() {	bind(AuditLogger.class).to(AsyncAuditLogger.class);	bind(AuditLogger.class).annotatedWith(Names.named(AsyncAuditLogger.InnerLogger)).to(AuditLoggerDefaultImpl.class);	Multibinder<RequestAuditEventCreator> multiBinder = Multibinder.newSetBinder(binder(), RequestAuditEventCreator.class);	Set<Class<?>> bindingSet = ClasspathScannerUtils.findOnClassPath(getPackageToScan(), getExclusions(), getSelectors());	for (Class clazz : bindingSet) {	
binding audit event creator 

========================= ambari sample_3812 =========================

private void postConstruct() {	Thread loadApiDocThread = new Thread("load_swagger_api_doc") {	public void run() {	
start thread to scan rest api doc from endpoints 

String[] parts = yaml.split("\n");	for (String part : parts) {	b.append(part);	b.append("\n");	}	setSwaggerYaml(b.toString());	}	} catch (Exception e) {	e.printStackTrace();	}	
scanning rest api endpoints and generating docs has been successful 

========================= ambari sample_196 =========================

public void initRootCert() {	
initialization of root certificate 

public void initRootCert() {	boolean certExists = isCertExists();	
certificate exists 

private void generateServerCertificate() {	
generation of server certificate 

public synchronized SignCertResponse signAgentCrt(String agentHostname, String agentCrtReqContent, String passphraseAgent) {	SignCertResponse response = new SignCertResponse();	
signing agent certificate 

public synchronized SignCertResponse signAgentCrt(String agentHostname, String agentCrtReqContent, String passphraseAgent) {	SignCertResponse response = new SignCertResponse();	agentHostname = StringUtils.trim(agentHostname);	if(StringUtils.isEmpty(agentHostname)) {	
the agent hostname is missing 

public synchronized SignCertResponse signAgentCrt(String agentHostname, String agentCrtReqContent, String passphraseAgent) {	SignCertResponse response = new SignCertResponse();	agentHostname = StringUtils.trim(agentHostname);	if(StringUtils.isEmpty(agentHostname)) {	response.setResult(SignCertResponse.ERROR_STATUS);	response.setMessage("The agent hostname is missing");	return response;	}	if(configs.validateAgentHostnames()) {	
validating agent hostname 

public synchronized SignCertResponse signAgentCrt(String agentHostname, String agentCrtReqContent, String passphraseAgent) {	SignCertResponse response = new SignCertResponse();	agentHostname = StringUtils.trim(agentHostname);	if(StringUtils.isEmpty(agentHostname)) {	response.setResult(SignCertResponse.ERROR_STATUS);	response.setMessage("The agent hostname is missing");	return response;	}	if(configs.validateAgentHostnames()) {	if(!HostUtils.isValidHostname(agentHostname)) {	
the agent hostname is not a valid hostname 

return response;	}	if(configs.validateAgentHostnames()) {	if(!HostUtils.isValidHostname(agentHostname)) {	response.setResult(SignCertResponse.ERROR_STATUS);	response.setMessage("The agent hostname is not a valid hostname");	return response;	}	}	else {	
skipping validation of agent hostname 

}	if(configs.validateAgentHostnames()) {	if(!HostUtils.isValidHostname(agentHostname)) {	response.setResult(SignCertResponse.ERROR_STATUS);	response.setMessage("The agent hostname is not a valid hostname");	return response;	}	}	else {	}	
verifying passphrase 

if(!HostUtils.isValidHostname(agentHostname)) {	response.setResult(SignCertResponse.ERROR_STATUS);	response.setMessage("The agent hostname is not a valid hostname");	return response;	}	}	else {	}	String passphraseSrvr = configs.getConfigsMap().get(Configuration. PASSPHRASE.getKey()).trim();	if (!passphraseSrvr.equals(passphraseAgent.trim())) {	
incorrect passphrase from the agent 

Map<String, String> configsMap = configs.getConfigsMap();	String srvrKstrDir = configsMap.get(Configuration.SRVR_KSTR_DIR.getKey());	String srvrCrtPass = configsMap.get(Configuration.SRVR_CRT_PASS.getKey());	String srvrCrtName = configsMap.get(Configuration.SRVR_CRT_NAME.getKey());	String srvrKeyName = configsMap.get(Configuration.SRVR_KEY_NAME.getKey());	String agentCrtReqName = agentHostname + ".csr";	String agentCrtName = agentHostname + ".crt";	Object[] scriptArgs = {srvrKstrDir, agentCrtReqName, agentCrtName, srvrCrtPass, srvrKeyName, srvrCrtName};	File agentCrtFile = new File(srvrKstrDir + File.separator + agentCrtName);	if (agentCrtFile.exists()) {	
revoking of certificate 

if (commandExitCode != 0) {	response.setResult(SignCertResponse.ERROR_STATUS);	response.setMessage(ShellCommandUtil.getOpenSslCommandResult(command, commandExitCode));	return response;	}	String agentCrtContent = "";	try {	agentCrtContent = FileUtils.readFileToString(agentCrtFile);	} catch (IOException e) {	e.printStackTrace();	
error reading signed agent certificate 

========================= ambari sample_2798 =========================

public Authentication authenticate(Authentication authentication) throws AuthenticationException {	if (isPamEnabled()) {	if (authentication.getName() == null) {	
authentication failed no username provided 

public Authentication authenticate(Authentication authentication) throws AuthenticationException {	if (isPamEnabled()) {	if (authentication.getName() == null) {	throw new InvalidUsernamePasswordCombinationException("");	}	String userName = authentication.getName().trim();	if (authentication.getCredentials() == null) {	
authentication failed no credentials provided 

if (StringUtils.isEmpty(localUsername)) {	localUsername = ambariUsername;	}	}	UnixUser unixUser = performPAMAuthentication(ambariUsername, localUsername, password);	if (unixUser != null) {	if (userEntity == null) {	try {	userEntity = users.createUser(ambariUsername, unixUser.getUserName(), ambariUsername, true);	} catch (AmbariException e) {	
failed to add the user s s 

} else {	throw new InvalidUsernamePasswordCombinationException(userName, false, e);	}	}	}	UserAuthenticationEntity authenticationEntity = getAuthenticationEntity(userEntity, UserAuthenticationType.PAM);	if (authenticationEntity == null) {	try {	users.addPamAuthentication(userEntity, unixUser.getUserName());	} catch (AmbariException e) {	
failed to add the pam authentication method for s s 

throw new AmbariAuthenticationException(ambariUsername, "Unexpected error has occurred", false, e);	}	}	if (isAutoGroupCreationAllowed()) {	synchronizeGroups(unixUser, userEntity);	}	Authentication authToken = new AmbariUserAuthentication(password, users.getUser(userEntity), users.getUserAuthorities(userEntity));	authToken.setAuthenticated(true);	return authToken;	}	
authentication failed password does not match stored value s 

private UnixUser performPAMAuthentication(String ambariUsername, String localUsername, String password) {	PAM pam = pamAuthenticationFactory.createInstance(getConfiguration());	if (pam == null) {	String message = "Failed to authenticate the user using the PAM authentication method: unexpected error";	LOG.error(message);	throw new AmbariAuthenticationException(ambariUsername, message, false);	} else {	if (LOG.isDebugEnabled() && !ambariUsername.equals(localUsername)) {	
authenticating ambari user using the local username 

if (pam == null) {	String message = "Failed to authenticate the user using the PAM authentication method: unexpected error";	LOG.error(message);	throw new AmbariAuthenticationException(ambariUsername, message, false);	} else {	if (LOG.isDebugEnabled() && !ambariUsername.equals(localUsername)) {	}	try {	return pam.authenticate(localUsername, password);	} catch (PAMException e) {	
authentication failed password does not match stored value s 

private void synchronizeGroups(UnixUser unixUser, UserEntity userEntity) {	
synchronizing groups for pam user 

private void synchronizeGroups(UnixUser unixUser, UserEntity userEntity) {	Users users = getUsers();	try {	Set<String> unixUserGroups = convertToLowercase(unixUser.getGroups());	for (String group : unixUserGroups) {	GroupEntity groupEntity = users.getGroupEntity(group, GroupType.PAM);	if (groupEntity == null) {	
synchronizing groups for adding new pam group 

private void synchronizeGroups(UnixUser unixUser, UserEntity userEntity) {	Users users = getUsers();	try {	Set<String> unixUserGroups = convertToLowercase(unixUser.getGroups());	for (String group : unixUserGroups) {	GroupEntity groupEntity = users.getGroupEntity(group, GroupType.PAM);	if (groupEntity == null) {	groupEntity = users.createGroup(group, GroupType.PAM);	}	if (!users.isUserInGroup(userEntity, groupEntity)) {	
synchronizing groups for adding user to pam group 

Set<MemberEntity> memberEntities = userEntity.getMemberEntities();	if (memberEntities != null) {	Collection<GroupEntity> groupsToRemove = new ArrayList<>();	for (MemberEntity memberEntity : memberEntities) {	GroupEntity groupEntity = memberEntity.getGroup();	if ((groupEntity.getGroupType() == GroupType.PAM) && !unixUserGroups.contains(groupEntity.getGroupName())) {	groupsToRemove.add(groupEntity);	}	}	for(GroupEntity groupEntity :groupsToRemove) {	
synchronizing groups for removing user from pam group 

========================= ambari sample_2848 =========================

private void createTransportMapping() throws IOException {	if (transportMapping == null) {	synchronized (this) {	if (transportMapping == null) {	if (port != null) {	
setting snmp dispatch port 

public void dispatch(Notification notification) {	
sending snmp trap 

public void dispatch(Notification notification) {	try {	createTransportMapping();	snmp = new Snmp(transportMapping);	SnmpVersion snmpVersion = getSnmpVersion(notification.DispatchProperties);	sendTraps(notification, snmpVersion);	successCallback(notification);	} catch (InvalidSnmpConfigurationException ex) {	
unable to dispatch snmp trap with invalid configuration 

public void dispatch(Notification notification) {	try {	createTransportMapping();	snmp = new Snmp(transportMapping);	SnmpVersion snmpVersion = getSnmpVersion(notification.DispatchProperties);	sendTraps(notification, snmpVersion);	successCallback(notification);	} catch (InvalidSnmpConfigurationException ex) {	failureCallback(notification);	} catch (Exception ex) {	
error occurred during snmp trap dispatching 

========================= ambari sample_4565 =========================

public Boolean handle(CloudSolrClient solrClient, SolrPropsConfig solrPropsConfig) throws Exception {	boolean result = false;	try {	
reload collection 

public Boolean handle(CloudSolrClient solrClient, SolrPropsConfig solrPropsConfig) throws Exception {	boolean result = false;	try {	CollectionAdminRequest.Reload request = CollectionAdminRequest.reloadCollection(solrPropsConfig.getCollection());	request.process(solrClient);	result = true;	} catch (Exception e) {	
reload collection s failed 

========================= ambari sample_1337 =========================

public static AWSCredentials createAWSCredentials(String accessKey, String secretKey) {	if (accessKey != null && secretKey != null) {	
creating aws client as per new accesskey and secretkey 

========================= ambari sample_1622 =========================

Statement statement = null;	ResultSet rs = null;	try {	statement = dbAccessor.getConnection().createStatement();	if (statement != null) {	rs = statement.executeQuery(String.format("SELECT COUNT(*) from %s where sequence_name='%s'", ambariSequencesTable, seqName));	if (rs != null) {	if (rs.next() && rs.getInt(1) == 0) {	dbAccessor.executeQuery(String.format("INSERT INTO %s(sequence_name, sequence_value) VALUES('%s', %d)", ambariSequencesTable, seqName, seqDefaultValue), ignoreFailure);	} else {	
sequence already exists skipping 

protected static Document convertStringToDocument(String xmlStr) {	DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();	DocumentBuilder builder;	Document doc = null;	try {	builder = factory.newDocumentBuilder();	doc = builder.parse( new InputSource( new StringReader( xmlStr ) ) );	} catch (Exception e) {	
error during convertation from string xmlstr to xml 

public void addConnectionTimeoutParamForWebAndMetricAlerts() {	
updating alert definitions 

EntityManager entityManager = getEntityManagerProvider().get();	if (entityManager.getTransaction().isActive()) {	func.run();	} else {	entityManager.getTransaction().begin();	try {	func.run();	entityManager.getTransaction().commit();	entityManager.getEntityManagerFactory().getCache().evictAll();	} catch (Exception e) {	
error in transaction 

if (clusters == null) {	return;	}	Map<String, Cluster> clusterMap = clusters.getClusters();	if (clusterMap != null && !clusterMap.isEmpty()) {	for (Cluster cluster : clusterMap.values()) {	Map<String, String> properties = new HashMap<>();	for(String propertyName:propertyNames) {	String propertyValue = configHelper.getPropertyValueFromStackDefinitions(cluster, configType, propertyName);	if(propertyValue == null) {	
config from is not found in xml definitions skipping configuration property update 

protected void updateConfigurationPropertiesForCluster(Cluster cluster, String configType, Map<String, String> properties, Set<String> removePropertiesList, boolean updateIfExists, boolean createNewConfigType) throws AmbariException {	AmbariManagementController controller = injector.getInstance(AmbariManagementController.class);	String newTag = "version" + System.currentTimeMillis();	if (properties != null) {	Map<String, Config> all = cluster.getConfigsByType(configType);	if (all == null || !all.containsKey(newTag) || properties.size() > 0) {	Map<String, String> oldConfigProperties;	Config oldConfig = cluster.getDesiredConfigByType(configType);	if (oldConfig == null && !createNewConfigType) {	
config not found assuming service not installed skipping configuration properties update 

Multimap<ConfigUpdateType, Entry<String, String>> propertiesToLog = ArrayListMultimap.create();	String serviceName = cluster.getServiceByConfigType(configType);	Map<String, String> mergedProperties = mergeProperties(oldConfigProperties, properties, updateIfExists, propertiesToLog);	if (removePropertiesList != null) {	mergedProperties = removeProperties(mergedProperties, removePropertiesList, propertiesToLog);	}	if (propertiesToLog.size() > 0) {	try {	configuration.writeToAmbariUpgradeConfigUpdatesFile(propertiesToLog, configType, serviceName, ambariUpgradeConfigUpdatesFileName);	} catch(Exception e) {	
write to config updates file failed 

if (removePropertiesList != null) {	mergedProperties = removeProperties(mergedProperties, removePropertiesList, propertiesToLog);	}	if (propertiesToLog.size() > 0) {	try {	configuration.writeToAmbariUpgradeConfigUpdatesFile(propertiesToLog, configType, serviceName, ambariUpgradeConfigUpdatesFileName);	} catch(Exception e) {	}	}	if (!Maps.difference(oldConfigProperties, mergedProperties).areEqual()) {	
applying configuration with tag and configtype to cluster 

Config baseConfig = cluster.getConfig(configType, newTag);	if (baseConfig != null) {	String authName = AUTHENTICATED_USER_NAME;	String configVersionNote = String.format("Updated %s during Ambari Upgrade from %s to %s.", configType, getSourceVersion(), getTargetVersion());	if (cluster.addDesiredConfig(authName, Collections.singleton(baseConfig), configVersionNote) != null) {	String oldConfigString = (oldConfig != null) ? " from='" + oldConfig.getTag() + "'" : "";	LOG.info("cluster '" + cluster.getClusterName() + "' " + "changed by: '" + authName + "'; " + "type='" + baseConfig.getType() + "' " + "tag='" + baseConfig.getTag() + "'" + oldConfigString);	}	}	} else {	
no changes detected to config skipping configuration properties update 

protected boolean isQueueNameValid(Cluster cluster, Set<String> validLeafQueues, String queueNameProperty, String configType) {	Config site = cluster.getDesiredConfigByType(configType);	Map<String, String> properties = site.getProperties();	boolean result = properties.containsKey(queueNameProperty) && validLeafQueues.contains(properties.get(queueNameProperty));	if (!result){	
queue name in not defined or not corresponds to valid capacity scheduler queue 

protected void updateQueueName(Cluster cluster, Set<String> validLeafQueues, String queueNameProperty, String configType) throws AmbariException {	String recommendQueue = validLeafQueues.iterator().next();	
update in set to 

if (capacitySchedulerMap.containsKey(queueKey)){	StringTokenizer queueTokenizer = new StringTokenizer(capacitySchedulerMap.get(queueKey), ",");	while (queueTokenizer.hasMoreTokens()){	toProcessQueues.push(queue + "." + queueTokenizer.nextToken());	}	} else {	if (!queue.endsWith(".")){	String queueName = queue.substring(queue.lastIndexOf('.')+1);	leafQueues.add(queueName);	} else {	
queue is not valid 

for (String widgetName : widgets) {	List<WidgetEntity> widgetEntities = widgetDAO.findByName(clusterID, widgetName, "ambari", section);	if (widgetEntities != null && widgetEntities.size() > 0) {	WidgetEntity entityToUpdate = null;	if (widgetEntities.size() > 1) {	LOG.info("Found more that 1 entity with name = "+ widgetName + " for cluster = " + cluster.getClusterName() + ", skipping update.");	} else {	entityToUpdate = widgetEntities.iterator().next();	}	if (entityToUpdate != null) {	
updating widget 

if (widgetDescriptorFile != null && widgetDescriptorFile.exists()) {	try {	widgetDescriptor = gson.fromJson(new FileReader(widgetDescriptorFile), widgetLayoutType);	} catch (Exception ex) {	String msg = "Error loading widgets from file: " + widgetDescriptorFile;	LOG.error(msg, ex);	widgetDescriptor = null;	}	}	if (widgetDescriptor != null) {	
loaded widget descriptor 

}	}	}	}	if (targetWidgetLayoutInfo != null) {	entityToUpdate.setMetrics(gson.toJson(targetWidgetLayoutInfo.getMetricsInfo()));	entityToUpdate.setWidgetValues(gson.toJson(targetWidgetLayoutInfo.getValues()));	entityToUpdate.setDescription(targetWidgetLayoutInfo.getDescription());	widgetDAO.merge(entityToUpdate);	} else {	
unable to find widget layout info for in the stack 

========================= ambari sample_2720 =========================

private List<HiveQueryId> getHqidListFromJsonArray(JSONArray jobs) {	List<HiveQueryId> parsedJobs = new LinkedList<>();	for (Object job : jobs) {	try {	HiveQueryId parsedJob = parseAtsHiveJob((JSONObject) job);	parsedJobs.add(parsedJob);	} catch (Exception ex) {	
error while parsing ats job 

public List<TezVertexId> getVerticesForDAGId(String dagId) {	JSONObject entities = delegate.tezVerticesListForDAG(dagId);	JSONArray vertices = (JSONArray) entities.get("entities");	List<TezVertexId> parsedVertices = new LinkedList<TezVertexId>();	for(Object vertex : vertices) {	try {	TezVertexId parsedVertex = parseVertex((JSONObject) vertex);	parsedVertices.add(parsedVertex);	} catch (Exception ex) {	
error while parsing the vertex 

========================= ambari sample_591 =========================

public void addHost(String hostName) {	synchronized(hostNames) {	String lowerHostName = hostName.toLowerCase();	if (!hostName.equals(lowerHostName)) {	
host name contains upper case letters will be converted to lowercase 

========================= ambari sample_2665 =========================

public synchronized <T extends Indexed> T load(Class<T> model, int id) throws ItemNotFound {	
loading s d 

public synchronized <T extends Indexed> List<T> loadAll(Class<T> model, FilteringStrategy filter) {	LinkedList<T> list = new LinkedList<T>();	
loading all s s 

public synchronized void delete(Class model, int id) throws ItemNotFound {	
deleting s d 

========================= ambari sample_1049 =========================

public void onEvent(ServiceComponentUninstalledEvent event) {	
received event 

========================= ambari sample_4532 =========================

private static void validateServiceDuplication(StackManager stackManager, StackInfo stack, ExtensionInfo extension, Collection<ServiceInfo> services) throws AmbariException {	
looking for duplicate services 

private static void validateServiceDuplication(StackManager stackManager, StackInfo stack, ExtensionInfo extension, Collection<ServiceInfo> services) throws AmbariException {	for (ServiceInfo service : services) {	
looking for duplicate service 

private static void validateServiceDuplication(StackManager stackManager, StackInfo stack, ExtensionInfo extension, Collection<ServiceInfo> services) throws AmbariException {	for (ServiceInfo service : services) {	if (service != null) {	ServiceInfo stackService = null;	try {	stackService = stack.getService(service.getName());	if (stackService != null) {	
found service 

ServiceInfo stackService = null;	try {	stackService = stack.getService(service.getName());	if (stackService != null) {	if (isInheritedExtensionService(stackManager, stack, service.getName(), extension.getName())) {	stackService = null;	}	}	}	catch (Exception e) {	
error validating service duplication 

private static boolean isInheritedExtensionService(StackManager stackManager, StackInfo stack, String serviceName, String extensionName) {	if (isExtensionService(stack, serviceName, extensionName)) {	
service is at requested stack version level 

private static boolean isExtensionService(StackManager stackManager, String stackName, String stackVersion, String serviceName, String extensionName) {	
checking at stack version 

private static boolean isExtensionService(StackManager stackManager, String stackName, String stackVersion, String serviceName, String extensionName) {	StackInfo stack = stackManager.getStack(stackName, stackVersion);	if (stack == null) {	
stack version not found 

private static boolean isExtensionService(StackManager stackManager, String stackName, String stackVersion, String serviceName, String extensionName) {	StackInfo stack = stackManager.getStack(stackName, stackVersion);	if (stack == null) {	return false;	}	if (isExtensionService(stack, serviceName, extensionName)) {	
stack version contains service 

private static boolean isExtensionService(StackInfo stack, String serviceName, String extensionName) {	ExtensionInfo extension = stack.getExtension(extensionName);	if (extension == null) {	
extension not found 

========================= ambari sample_4477 =========================

}	if (properties != null) {	ldapConfigurationProperties.putAll(properties);	}	AmbariLdapConfiguration ambariLdapConfiguration = new AmbariLdapConfiguration(ldapConfigurationProperties);	boolean success = true;	String message = null;	Object resultData = null;	try {	switch (operationType) {	
testing connection to the ldap server 

ldapConfigurationProperties.putAll(properties);	}	AmbariLdapConfiguration ambariLdapConfiguration = new AmbariLdapConfiguration(ldapConfigurationProperties);	boolean success = true;	String message = null;	Object resultData = null;	try {	switch (operationType) {	ldapFacade.checkConnection(ambariLdapConfiguration);	break;	
testing ldap attributes 

boolean success = true;	String message = null;	Object resultData = null;	try {	switch (operationType) {	ldapFacade.checkConnection(ambariLdapConfiguration);	break;	Set<String> groups = ldapFacade.checkLdapAttributes(operationParameters, ambariLdapConfiguration);	resultData = Collections.singletonMap("groups", groups);	break;	
detecting ldap attributes 

try {	switch (operationType) {	ldapFacade.checkConnection(ambariLdapConfiguration);	break;	Set<String> groups = ldapFacade.checkLdapAttributes(operationParameters, ambariLdapConfiguration);	resultData = Collections.singletonMap("groups", groups);	break;	ambariLdapConfiguration = ldapFacade.detectAttributes(ambariLdapConfiguration);	resultData = Collections.singletonMap("attributes", ambariLdapConfiguration.toMap());	break;	
no action provided 

resultData = Collections.singletonMap("attributes", ambariLdapConfiguration.toMap());	break;	throw new IllegalArgumentException("No request action provided");	}	} catch (AmbariLdapException e) {	success = false;	message = determineCause(e);	if (StringUtils.isEmpty(message)) {	message = "An unexpected error has occurred.";	}	
failed to perform s s 

========================= ambari sample_3598 =========================

return Response.ok(response).build();	}	public Response getServiceCheckList(){	ServiceCheck serviceCheck = new ServiceCheck(context);	try {	ServiceCheck.Policy policy = serviceCheck.getServiceCheckPolicy();	JSONObject policyJson = new JSONObject();	policyJson.put("serviceCheckPolicy", policy);	return Response.ok(policyJson).build();	} catch (HdfsApiException e) {	
error occurred while generating service check policy 

========================= ambari sample_568 =========================

if (!isValid(alert)) {	continue;	}	String clusterName = alert.getCluster();	Long clusterId = getClusterIdByName(clusterName);	if (clusterId == null) {	clusterId = event.getClusterId();	}	AlertDefinitionEntity definition = m_definitionDao.findByName(clusterId, alert.getName());	if (null == definition) {	
received an alert for which is a definition that does not exist anymore 

String clusterName = alert.getCluster();	Long clusterId = getClusterIdByName(clusterName);	if (clusterId == null) {	clusterId = event.getClusterId();	}	AlertDefinitionEntity definition = m_definitionDao.findByName(clusterId, alert.getName());	if (null == definition) {	continue;	}	if (!definition.getEnabled()) {	
received an alert for which is disabled no more alerts should be received for this definition 

try {	current = getCurrentEntity(clusterId, alert, definition);	if( null != current ) {	continue;	}	AlertHistoryEntity history = createHistory(clusterId, definition, alert);	MaintenanceState maintenanceState = MaintenanceState.OFF;	try {	maintenanceState = m_maintenanceStateHelper.get().getEffectiveState(clusterId, alert);	} catch (Exception exception) {	
unable to determine the maintenance mode state for defaulting to off 

}	if (alertState == AlertState.SKIPPED) {	String alertText = alert.getText();	if (StringUtils.isNotBlank(alertText)) {	current.setLatestText(alertText);	}	}	toMerge.add(current);	} else {	if (LOG.isDebugEnabled()) {	
alert state changed currentid currenttimestamp historyid historystate 

private Long getClusterIdByName(String clusterName) {	try {	return m_clusters.get().getCluster(clusterName).getClusterId();	} catch (AmbariException e) {	
cluster lookup failed for cluster named 

String ambariServerComponentName = RootComponent.AMBARI_SERVER.name();	String ambariAgentComponentName = RootComponent.AMBARI_AGENT.name();	if (ambariServiceName.equals(serviceName) && ambariServerComponentName.equals(componentName)) {	return true;	}	if (StringUtils.isBlank(clusterName)) {	if (StringUtils.isBlank(hostName)) {	return true;	}	if (!m_clusters.get().hostExists(hostName)) {	
unable to process alert for an invalid host named 

}	if (!m_clusters.get().hostExists(hostName)) {	return false;	}	return true;	}	final Cluster cluster;	try {	cluster = m_clusters.get().getCluster(clusterName);	if (null == cluster) {	
unable to process alert for an invalid cluster named 

return true;	}	final Cluster cluster;	try {	cluster = m_clusters.get().getCluster(clusterName);	if (null == cluster) {	return false;	}	} catch (AmbariException ambariException) {	if (LOG.isDebugEnabled()) {	
unable to process alert for an invalid cluster named 

}	final Cluster cluster;	try {	cluster = m_clusters.get().getCluster(clusterName);	if (null == cluster) {	return false;	}	} catch (AmbariException ambariException) {	if (LOG.isDebugEnabled()) {	} else {	
unable to process alert for an invalid cluster named 

return false;	}	} catch (AmbariException ambariException) {	if (LOG.isDebugEnabled()) {	} else {	}	return false;	}	if (ambariServiceName.equals(serviceName) && ambariAgentComponentName.equals(componentName)) {	if (StringUtils.isBlank(hostName) || !m_clusters.get().hostExists(hostName) || !m_clusters.get().isHostMappedToCluster(clusterName, hostName)) {	
unable to process alert for cluster and host because the host is not a part of the cluster 

return false;	}	if (ambariServiceName.equals(serviceName) && ambariAgentComponentName.equals(componentName)) {	if (StringUtils.isBlank(hostName) || !m_clusters.get().hostExists(hostName) || !m_clusters.get().isHostMappedToCluster(clusterName, hostName)) {	return false;	}	return true;	}	if (StringUtils.isNotBlank(hostName)) {	if (!m_clusters.get().hostExists(hostName)) {	
unable to process alert for an invalid host named 

if (StringUtils.isBlank(hostName) || !m_clusters.get().hostExists(hostName) || !m_clusters.get().isHostMappedToCluster(clusterName, hostName)) {	return false;	}	return true;	}	if (StringUtils.isNotBlank(hostName)) {	if (!m_clusters.get().hostExists(hostName)) {	return false;	}	if (!cluster.getServices().containsKey(serviceName)) {	
unable to process alert for an invalid service named 

return true;	}	if (StringUtils.isNotBlank(hostName)) {	if (!m_clusters.get().hostExists(hostName)) {	return false;	}	if (!cluster.getServices().containsKey(serviceName)) {	return false;	}	if (null != componentName && !cluster.getHosts(serviceName, componentName).contains(hostName)) {	
unable to process alert for an invalid service and component on host 

private int getRepeatTolerance(AlertDefinitionEntity definition, String clusterName) {	if (definition.isRepeatToleranceEnabled()) {	return definition.getRepeatTolerance();	}	int repeatTolerance = 1;	try {	Cluster cluster = m_clusters.get().getCluster(clusterName);	String value = cluster.getClusterProperty(ConfigHelper.CLUSTER_ENV_ALERT_REPEAT_TOLERANCE, "1");	repeatTolerance = NumberUtils.toInt(value, 1);	} catch (AmbariException ambariException) {	
unable to read from cluster defaulting to 

========================= ambari sample_4524 =========================

public void run() {	
started timeline aggregator thread 

switch (getAggregatorType()) {	case HOST: performAggregationFunction = taskRunner.performsHostAggregation();	break;	case CLUSTER: performAggregationFunction = taskRunner.performsClusterAggregation();	}	}	if (performAggregationFunction) {	long currentTime = System.currentTimeMillis();	long lastCheckPointTime = readLastCheckpointSavingOnFirstRun(currentTime);	if (lastCheckPointTime != -1) {	
last check point time lagby seconds 

}	if (performAggregationFunction) {	long currentTime = System.currentTimeMillis();	long lastCheckPointTime = readLastCheckpointSavingOnFirstRun(currentTime);	if (lastCheckPointTime != -1) {	boolean success = doWork(lastCheckPointTime, lastCheckPointTime + SLEEP_INTERVAL);	if (success) {	try {	saveCheckPoint(lastCheckPointTime + SLEEP_INTERVAL);	} catch (IOException io) {	
error saving checkpoint restarting aggregation at previous checkpoint 

if (lastCheckPointTime != -1) {	boolean success = doWork(lastCheckPointTime, lastCheckPointTime + SLEEP_INTERVAL);	if (success) {	try {	saveCheckPoint(lastCheckPointTime + SLEEP_INTERVAL);	} catch (IOException io) {	}	}	}	} else {	
skipping aggregation function not owned by this instance 

private long readLastCheckpointSavingOnFirstRun(long currentTime) {	long lastCheckPointTime = -1;	try {	lastCheckPointTime = readCheckPoint();	if (lastCheckPointTime != -1) {	
last checkpoint read 

private long readLastCheckpointSavingOnFirstRun(long currentTime) {	long lastCheckPointTime = -1;	try {	lastCheckPointTime = readCheckPoint();	if (lastCheckPointTime != -1) {	if (isLastCheckPointTooOld(currentTime, lastCheckPointTime)) {	LOG.warn("Last Checkpoint is too old, discarding last checkpoint. " + "lastCheckPointTime = " + new Date(lastCheckPointTime));	lastCheckPointTime = getRoundedAggregateTimeMillis(getSleepIntervalMillis()) - getSleepIntervalMillis();	
saving checkpoint time 

try {	lastCheckPointTime = readCheckPoint();	if (lastCheckPointTime != -1) {	if (isLastCheckPointTooOld(currentTime, lastCheckPointTime)) {	LOG.warn("Last Checkpoint is too old, discarding last checkpoint. " + "lastCheckPointTime = " + new Date(lastCheckPointTime));	lastCheckPointTime = getRoundedAggregateTimeMillis(getSleepIntervalMillis()) - getSleepIntervalMillis();	saveCheckPoint(lastCheckPointTime);	} else {	if (lastCheckPointTime > 0) {	lastCheckPointTime = getRoundedCheckPointTimeMillis(lastCheckPointTime, getSleepIntervalMillis());	
rounded off checkpoint 

if (lastCheckPointTime != -1) {	if (isLastCheckPointTooOld(currentTime, lastCheckPointTime)) {	LOG.warn("Last Checkpoint is too old, discarding last checkpoint. " + "lastCheckPointTime = " + new Date(lastCheckPointTime));	lastCheckPointTime = getRoundedAggregateTimeMillis(getSleepIntervalMillis()) - getSleepIntervalMillis();	saveCheckPoint(lastCheckPointTime);	} else {	if (lastCheckPointTime > 0) {	lastCheckPointTime = getRoundedCheckPointTimeMillis(lastCheckPointTime, getSleepIntervalMillis());	}	if (isLastCheckPointTooYoung(lastCheckPointTime)) {	
last checkpoint too recent for aggregation sleeping for cycle 

saveCheckPoint(lastCheckPointTime);	} else {	if (lastCheckPointTime > 0) {	lastCheckPointTime = getRoundedCheckPointTimeMillis(lastCheckPointTime, getSleepIntervalMillis());	}	if (isLastCheckPointTooYoung(lastCheckPointTime)) {	return -1;	}	}	} else {	
no checkpoint found 

} else {	if (lastCheckPointTime > 0) {	lastCheckPointTime = getRoundedCheckPointTimeMillis(lastCheckPointTime, getSleepIntervalMillis());	}	if (isLastCheckPointTooYoung(lastCheckPointTime)) {	return -1;	}	}	} else {	long firstCheckPoint = getRoundedAggregateTimeMillis(getSleepIntervalMillis());	
saving checkpoint time 

}	if (isLastCheckPointTooYoung(lastCheckPointTime)) {	return -1;	}	}	} else {	long firstCheckPoint = getRoundedAggregateTimeMillis(getSleepIntervalMillis());	saveCheckPoint(firstCheckPoint);	}	} catch (IOException io) {	
unable to write last checkpoint time resuming sleep 

public boolean doWork(long startTime, long endTime) {	LOG.info("Start aggregation cycle @ " + new Date() + ", " + "startTime = " + new Date(startTime) + ", endTime = " + new Date(endTime));	boolean success = true;	Condition condition = prepareMetricQueryCondition(startTime, endTime);	Connection conn = null;	PreparedStatement stmt = null;	ResultSet rs = null;	try {	conn = hBaseAccessor.getConnection();	stmt = PhoenixTransactSQL.prepareGetMetricsSqlStmt(conn, condition);	
query issued 

Condition condition = prepareMetricQueryCondition(startTime, endTime);	Connection conn = null;	PreparedStatement stmt = null;	ResultSet rs = null;	try {	conn = hBaseAccessor.getConnection();	stmt = PhoenixTransactSQL.prepareGetMetricsSqlStmt(conn, condition);	if (condition.doUpdate()) {	int rows = stmt.executeUpdate();	conn.commit();	
row s updated in aggregation 

try {	conn = hBaseAccessor.getConnection();	stmt = PhoenixTransactSQL.prepareGetMetricsSqlStmt(conn, condition);	if (condition.doUpdate()) {	int rows = stmt.executeUpdate();	conn.commit();	downsample(conn, startTime, endTime);	} else {	rs = stmt.executeQuery();	}	
query returned 

stmt = PhoenixTransactSQL.prepareGetMetricsSqlStmt(conn, condition);	if (condition.doUpdate()) {	int rows = stmt.executeUpdate();	conn.commit();	downsample(conn, startTime, endTime);	} else {	rs = stmt.executeQuery();	}	aggregate(rs, startTime, endTime);	} catch (SQLException | IOException e) {	
exception during aggregating metrics 

} catch (SQLException e) {	}	}	if (conn != null) {	try {	conn.close();	} catch (SQLException sql) {	}	}	}	
end aggregation cycle 

protected abstract Condition prepareMetricQueryCondition(long startTime, long endTime);	protected abstract void aggregate(ResultSet rs, long startTime, long endTime) throws IOException, SQLException;	protected void downsample(Connection conn, Long startTime, Long endTime) {	
checking for downsampling requests 

protected abstract Condition prepareMetricQueryCondition(long startTime, long endTime);	protected abstract void aggregate(ResultSet rs, long startTime, long endTime) throws IOException, SQLException;	protected void downsample(Connection conn, Long startTime, Long endTime) {	if (CollectionUtils.isEmpty(configuredDownSamplers)) {	
no downsamplers configured 

CustomDownSampler downSampler = iterator.next();	if (downSampler.validateConfigs()) {	EmptyCondition downSamplingCondition = new EmptyCondition();	downSamplingCondition.setDoUpdate(true);	List<String> stmts = downSampler.prepareDownSamplingStatement(startTime, endTime, tableName);	for (String stmt : stmts) {	downSamplingCondition.setStatement(queryPrefix + stmt);	runDownSamplerQuery(conn, downSamplingCondition);	}	} else {	
the following downsampler failed config validation removing it from downsamplers list 

private void runDownSamplerQuery(Connection conn, Condition condition) {	PreparedStatement stmt = null;	ResultSet rs = null;	
downsampling query 

private void runDownSamplerQuery(Connection conn, Condition condition) {	PreparedStatement stmt = null;	ResultSet rs = null;	try {	stmt = PhoenixTransactSQL.prepareGetMetricsSqlStmt(conn, condition);	
downsampler query issued 

private void runDownSamplerQuery(Connection conn, Condition condition) {	PreparedStatement stmt = null;	ResultSet rs = null;	try {	stmt = PhoenixTransactSQL.prepareGetMetricsSqlStmt(conn, condition);	if (condition.doUpdate()) {	int rows = stmt.executeUpdate();	conn.commit();	
row s updated in downsampling 

PreparedStatement stmt = null;	ResultSet rs = null;	try {	stmt = PhoenixTransactSQL.prepareGetMetricsSqlStmt(conn, condition);	if (condition.doUpdate()) {	int rows = stmt.executeUpdate();	conn.commit();	} else {	rs = stmt.executeQuery();	}	
downsampler query returned 

PreparedStatement stmt = null;	ResultSet rs = null;	try {	stmt = PhoenixTransactSQL.prepareGetMetricsSqlStmt(conn, condition);	if (condition.doUpdate()) {	int rows = stmt.executeUpdate();	conn.commit();	} else {	rs = stmt.executeQuery();	}	
end downsampling cycle 

ResultSet rs = null;	try {	stmt = PhoenixTransactSQL.prepareGetMetricsSqlStmt(conn, condition);	if (condition.doUpdate()) {	int rows = stmt.executeUpdate();	conn.commit();	} else {	rs = stmt.executeQuery();	}	} catch (SQLException e) {	
exception during downsampling metrics 

========================= ambari sample_354 =========================

String propertyFileType = pi.getPropertyValueAttributes().getPropertyFileType();	String propertyFilePath = propertiesDirFile.getAbsolutePath() + File.separator + propertyFileName;	File propertyFile = new File(propertyFilePath);	if (propertyFile.exists() && propertyFile.isFile()) {	try {	String propertyValue = FileUtils.readFileToString(propertyFile);	boolean valid = true;	switch (propertyFileType.toLowerCase()) {	case "xml" : if (!XmlUtils.isValidXml(propertyValue)) {	valid = false;	
failed to load value from property file property file is not a valid xml file 

try {	String propertyValue = FileUtils.readFileToString(propertyFile);	boolean valid = true;	switch (propertyFileType.toLowerCase()) {	case "xml" : if (!XmlUtils.isValidXml(propertyValue)) {	valid = false;	}	break;	case "json": if (!JsonUtils.isValidJson(propertyValue)) {	valid = false;	
failed to load value from property file property file is not a valid json file 

case "json": if (!JsonUtils.isValidJson(propertyValue)) {	valid = false;	}	break;	case "text": default: break;	}	if (valid) {	pi.setValue(propertyValue);	}	} catch (IOException e) {	
failed to load value from property file error message 

}	break;	case "text": default: break;	}	if (valid) {	pi.setValue(propertyValue);	}	} catch (IOException e) {	}	} else {	
failed to load value from property file properties file does not exist 

case "text": default: break;	}	if (valid) {	pi.setValue(propertyValue);	}	} catch (IOException e) {	}	} else {	}	} else {	
failed to load value from property file properties directory does not exist 

========================= ambari sample_4461 =========================

public Authentication authenticate(Authentication authentication) throws AuthenticationException {	if (!authPropsConfig.isAuthFileEnabled()) {	
file auth is disabled 

if (StringUtils.isBlank(username)) {	throw new BadCredentialsException("Username can't be null or empty.");	}	if (StringUtils.isBlank(password)) {	throw new BadCredentialsException("Password can't be null or empty.");	}	password = StringEscapeUtils.unescapeHtml(password);	username = StringEscapeUtils.unescapeHtml(username);	UserDetails user = userDetailsService.loadUserByUsername(username);	if (user == null) {	
username not found 

if (StringUtils.isBlank(password)) {	throw new BadCredentialsException("Password can't be null or empty.");	}	password = StringEscapeUtils.unescapeHtml(password);	username = StringEscapeUtils.unescapeHtml(username);	UserDetails user = userDetailsService.loadUserByUsername(username);	if (user == null) {	throw new BadCredentialsException("User not found.");	}	if (StringUtils.isEmpty(user.getPassword())) {	
password can t be null or empty 

========================= ambari sample_1399 =========================

private boolean loadPatterns(Grok grok) {	InputStreamReader grokPatternsReader = null;	
loading pattern file 

private boolean loadPatterns(Grok grok) {	InputStreamReader grokPatternsReader = null;	try {	InputStream fileInputStream = getClass().getClassLoader().getResourceAsStream(GROK_PATTERN_FILE);	if (fileInputStream == null) {	
couldn t load grok patterns file things will not work 

private boolean loadPatterns(Grok grok) {	InputStreamReader grokPatternsReader = null;	try {	InputStream fileInputStream = getClass().getClassLoader().getResourceAsStream(GROK_PATTERN_FILE);	if (fileInputStream == null) {	return false;	}	grokPatternsReader = new InputStreamReader(fileInputStream);	} catch (Throwable t) {	
error reading grok patterns file from classpath grok filtering will not work 

if (fileInputStream == null) {	return false;	}	grokPatternsReader = new InputStreamReader(fileInputStream);	} catch (Throwable t) {	return false;	}	try {	grok.addPatternFromReader(grokPatternsReader);	} catch (GrokException e) {	
error loading patterns from grok patterns reader for file 

========================= ambari sample_1663 =========================

public void removeConfiguration(String categoryName) throws NoSuchResourceException {	if (null == categoryName) {	
no resource id provided in the request 

public void removeConfiguration(String categoryName) throws NoSuchResourceException {	if (null == categoryName) {	} else {	
deleting ambari configuration with id 

========================= ambari sample_3520 =========================

public String getKeys(@PathParam("number") int number) throws IOException, JAXBException {	Collection<String> keys = persistKeyVal.generateKeys(number);	String result = StageUtils.jaxbToString(keys);	
returning keys 

========================= ambari sample_4123 =========================

public boolean isReady() {	if (!isReady) {	logFiles = getActualFiles(logPath);	if (!ArrayUtils.isEmpty(logFiles) && logFiles[0].isFile()) {	if (tail && logFiles.length > 1) {	
found multiple files for the file filter will follow only the first one using 

public boolean isReady() {	if (!isReady) {	logFiles = getActualFiles(logPath);	if (!ArrayUtils.isEmpty(logFiles) && logFiles[0].isFile()) {	if (tail && logFiles.length > 1) {	}	
file filter expanded to 

public boolean isReady() {	if (!isReady) {	logFiles = getActualFiles(logPath);	if (!ArrayUtils.isEmpty(logFiles) && logFiles[0].isFile()) {	if (tail && logFiles.length > 1) {	}	isReady = true;	} else {	
file doesn t exist ignoring for now 

public boolean monitor() {	if (isReady()) {	
starting thread 

public void start() throws Exception {	boolean isProcessFile = BooleanUtils.toBooleanDefaultIfNull(((InputFileDescriptor)getInputDescriptor()).getProcessFile(), true);	if (isProcessFile) {	for (int i = logFiles.length - 1; i >= 0; i--) {	File file = logFiles[i];	if (i == 0 || !tail) {	try {	processFile(file, i == 0);	if (isClosed() || isDrain()) {	
isclosed or isdrain now breaking loop 

private void copyFiles(File[] files) {	boolean isCopyFile = BooleanUtils.toBooleanDefaultIfNull(((InputFileDescriptor)getInputDescriptor()).getCopyFile(), false);	if (isCopyFile && files != null) {	for (File file : files) {	try {	InputFileMarker marker = new InputFileMarker(this, null, 0);	getOutputManager().copyFile(file, marker);	if (isClosed() || isDrain()) {	
isclosed or isdrain now breaking loop 

public void close() {	super.close();	
close calling checkpoint checkin 

========================= ambari sample_1634 =========================

f.set(hdb, precisionValues);	Field f2 = PhoenixHBaseAccessor.class.getDeclaredField("timelineMetricsTablesDurability");	f2.setAccessible(true);	f2.set(hdb, "ASYNC_WAL");	hdb.initPoliciesAndTTL();	boolean normalizerEnabled = false;	String precisionTableCompactionPolicy = null;	String aggregateTableCompactionPolicy = null;	boolean tableDurabilitySet  = false;	for (int i = 0; i < 10; i++) {	
policy check retry 

========================= ambari sample_303 =========================

private void runStatement(RunStatement message) {	try {	HiveStatement statement = connectionDelegate.createStatement(connection);	if (message.shouldStartLogAggregation()) {	startLogAggregation(statement, message.getStatement(), message.getLogFile().get());	}	if (message.shouldStartGUIDFetch() && message.getJobId().isPresent()) {	startGUIDFetch(message.getId(), statement, message.getJobId().get());	}	
statement executor is executing statement statement id jobid sync job 

private void runStatement(RunStatement message) {	try {	HiveStatement statement = connectionDelegate.createStatement(connection);	if (message.shouldStartLogAggregation()) {	startLogAggregation(statement, message.getStatement(), message.getLogFile().get());	}	if (message.shouldStartGUIDFetch() && message.getJobId().isPresent()) {	startGUIDFetch(message.getId(), statement, message.getJobId().get());	}	Optional<ResultSet> resultSetOptional = connectionDelegate.execute(message.getStatement());	
finished executing statement statement id jobid sync job 

if (message.shouldStartGUIDFetch() && message.getJobId().isPresent()) {	startGUIDFetch(message.getId(), statement, message.getJobId().get());	}	Optional<ResultSet> resultSetOptional = connectionDelegate.execute(message.getStatement());	if (resultSetOptional.isPresent()) {	sender().tell(new ResultInformation(message.getId(), resultSetOptional.get()), self());	} else {	sender().tell(new ResultInformation(message.getId()), self());	}	} catch (SQLException e) {	
failed to execute statement 

private void startGUIDFetch(int statementId, HiveStatement statement, String jobId) {	if (guidFetcher == null) {	guidFetcher = getContext().actorOf(Props.create(YarnAtsGUIDFetcher.class, sender()) .withDispatcher("akka.actor.misc-dispatcher"), "YarnAtsGUIDFetcher:" + UUID.randomUUID().toString());	}	
fetching guid for job id 

private void getColumnMetaData(GetColumnMetadataJob message) {	try {	ResultSet resultSet = connectionDelegate.getColumnMetadata(connection, message);	sender().tell(new ResultInformation(-1, resultSet), self());	} catch (SQLException e) {	
failed to get column metadata for databasepattern tablepattern columnpattern 

private void getDatabaseMetaData(GetDatabaseMetadataJob message) {	try {	DatabaseMetaData metaData = connectionDelegate.getDatabaseMetadata(connection);	sender().tell(new ResultInformation(-1, metaData), self());	} catch (SQLException e) {	
failed to get database metadata 

========================= ambari sample_753 =========================

if(optHeader != null){	if(optHeader.equals(ParseOptions.HEADER.FIRST_RECORD.toString())) {	format = format.withHeader();	}else if( optHeader.equals(ParseOptions.HEADER.PROVIDED_BY_USER.toString())){	String [] headers = (String[]) parseOptions.getOption(ParseOptions.OPTIONS_HEADERS);	format = format.withHeader(headers);	}	}	Character delimiter = (Character) parseOptions.getOption(ParseOptions.OPTIONS_CSV_DELIMITER);	if(delimiter != null){	
setting delimiter as 

String [] headers = (String[]) parseOptions.getOption(ParseOptions.OPTIONS_HEADERS);	format = format.withHeader(headers);	}	}	Character delimiter = (Character) parseOptions.getOption(ParseOptions.OPTIONS_CSV_DELIMITER);	if(delimiter != null){	format = format.withDelimiter(delimiter);	}	Character quote = (Character) parseOptions.getOption(ParseOptions.OPTIONS_CSV_QUOTE);	if( null != quote ){	
setting quote char 

Character delimiter = (Character) parseOptions.getOption(ParseOptions.OPTIONS_CSV_DELIMITER);	if(delimiter != null){	format = format.withDelimiter(delimiter);	}	Character quote = (Character) parseOptions.getOption(ParseOptions.OPTIONS_CSV_QUOTE);	if( null != quote ){	format = format.withQuote(quote);	}	Character escape = (Character) parseOptions.getOption(ParseOptions.OPTIONS_CSV_ESCAPE_CHAR);	if(escape != null){	
setting escape as 

========================= ambari sample_817 =========================

if (resourceOK) {	PermissionEntity permission = privilegeEntity.getPermission();	Collection<RoleAuthorizationEntity> userAuthorizations = (permission == null) ? null : permission.getAuthorizations();	if (userAuthorizations != null) {	for (RoleAuthorizationEntity userAuthorization : userAuthorizations) {	try {	if (requiredAuthorizations.contains(RoleAuthorization.translate(userAuthorization.getAuthorizationId()))) {	return true;	}	} catch (IllegalArgumentException e) {	
invalid authorization name ignoring 

========================= ambari sample_2825 =========================

if (newStackServices == null) {	newStackServices = Collections.emptyMap();	}	for (Map.Entry<String, KerberosServiceDescriptor> entry : userServices.entrySet()) {	String name = entry.getKey();	KerberosServiceDescriptor userValue = entry.getValue();	if (userValue != null) {	if (newStackServices.containsKey(name)) {	KerberosServiceDescriptor oldValue = previousStackServices.get(name);	KerberosServiceDescriptor newValue = newStackServices.get(name);	
processing service for modifications 

}	for (Map.Entry<String, KerberosServiceDescriptor> entry : userServices.entrySet()) {	String name = entry.getKey();	KerberosServiceDescriptor userValue = entry.getValue();	if (userValue != null) {	if (newStackServices.containsKey(name)) {	KerberosServiceDescriptor oldValue = previousStackServices.get(name);	KerberosServiceDescriptor newValue = newStackServices.get(name);	updatedServices.put(name, processService(oldValue, newValue, userValue));	} else if (previousStackServices.containsKey(name)) {	
removing service from user specified kerberos descriptor 

for (Map.Entry<String, KerberosServiceDescriptor> entry : userServices.entrySet()) {	String name = entry.getKey();	KerberosServiceDescriptor userValue = entry.getValue();	if (userValue != null) {	if (newStackServices.containsKey(name)) {	KerberosServiceDescriptor oldValue = previousStackServices.get(name);	KerberosServiceDescriptor newValue = newStackServices.get(name);	updatedServices.put(name, processService(oldValue, newValue, userValue));	} else if (previousStackServices.containsKey(name)) {	} else {	
leaving service in user specified kerberos descriptor unchanged since it was user defined 

Iterator<Map.Entry<String, KerberosComponentDescriptor>> iterator = userServiceComponents.entrySet().iterator();	while (iterator.hasNext()) {	Map.Entry<String, KerberosComponentDescriptor> entry = iterator.next();	String name = entry.getKey();	KerberosComponentDescriptor userValue = entry.getValue();	if (userValue == null) {	iterator.remove();	} else if (newServiceComponents.containsKey(name)) {	KerberosComponentDescriptor oldValue = oldServiceComponents.get(name);	KerberosComponentDescriptor newValue = newServiceComponents.get(name);	
processing component for modifications 

Map.Entry<String, KerberosComponentDescriptor> entry = iterator.next();	String name = entry.getKey();	KerberosComponentDescriptor userValue = entry.getValue();	if (userValue == null) {	iterator.remove();	} else if (newServiceComponents.containsKey(name)) {	KerberosComponentDescriptor oldValue = oldServiceComponents.get(name);	KerberosComponentDescriptor newValue = newServiceComponents.get(name);	processComponent(oldValue, newValue, userValue);	} else {	
removing component from user specified kerberos descriptor 

}	for (Map.Entry<String, KerberosIdentityDescriptor> entry : userStackIdentityMap.entrySet()) {	String name = entry.getKey();	KerberosIdentityDescriptor userValue = entry.getValue();	if (userValue != null) {	if (newStackIdentityMap.containsKey(name)) {	KerberosIdentityDescriptor newValue = newStackIdentityMap.get(name);	KerberosIdentityDescriptor previousValue = previousStackIdentityMap.get(name);	updatedIdentities.put(name, processIdentity(previousValue, newValue, userValue));	} else if (previousStackIdentityMap.containsKey(name)) {	
removing identity named from user specified kerberos descriptor 

for (Map.Entry<String, KerberosIdentityDescriptor> entry : userStackIdentityMap.entrySet()) {	String name = entry.getKey();	KerberosIdentityDescriptor userValue = entry.getValue();	if (userValue != null) {	if (newStackIdentityMap.containsKey(name)) {	KerberosIdentityDescriptor newValue = newStackIdentityMap.get(name);	KerberosIdentityDescriptor previousValue = previousStackIdentityMap.get(name);	updatedIdentities.put(name, processIdentity(previousValue, newValue, userValue));	} else if (previousStackIdentityMap.containsKey(name)) {	} else {	
leaving identity named in user specified kerberos descriptor unchanged since it was user defined 

if (newStackConfigurations == null) {	newStackConfigurations = Collections.emptyMap();	}	for (Map.Entry<String, KerberosConfigurationDescriptor> entry : userConfigurations.entrySet()) {	String name = entry.getKey();	KerberosConfigurationDescriptor userValue = entry.getValue();	if (userValue != null) {	if (newStackConfigurations.containsKey(name)) {	KerberosConfigurationDescriptor oldValue = previousStackConfigurations.get(name);	KerberosConfigurationDescriptor newValue = newStackConfigurations.get(name);	
processing configuration type for modifications 

}	for (Map.Entry<String, KerberosConfigurationDescriptor> entry : userConfigurations.entrySet()) {	String name = entry.getKey();	KerberosConfigurationDescriptor userValue = entry.getValue();	if (userValue != null) {	if (newStackConfigurations.containsKey(name)) {	KerberosConfigurationDescriptor oldValue = previousStackConfigurations.get(name);	KerberosConfigurationDescriptor newValue = newStackConfigurations.get(name);	updatedConfigurations.put(name, processConfiguration(oldValue, newValue, userValue));	} else if (previousStackConfigurations.containsKey(name)) {	
removing configuration type from user specified kerberos descriptor 

for (Map.Entry<String, KerberosConfigurationDescriptor> entry : userConfigurations.entrySet()) {	String name = entry.getKey();	KerberosConfigurationDescriptor userValue = entry.getValue();	if (userValue != null) {	if (newStackConfigurations.containsKey(name)) {	KerberosConfigurationDescriptor oldValue = previousStackConfigurations.get(name);	KerberosConfigurationDescriptor newValue = newStackConfigurations.get(name);	updatedConfigurations.put(name, processConfiguration(oldValue, newValue, userValue));	} else if (previousStackConfigurations.containsKey(name)) {	} else {	
leaving configuration type in user specified kerberos descriptor unchanged since it was user defined 

if (newStackProperties == null) {	newStackProperties = Collections.emptyMap();	}	for (Map.Entry<String, String> entry : previousStackProperties.entrySet()) {	String name = entry.getKey();	if (newStackProperties.containsKey(name)) {	String previousValue = entry.getValue();	String newValue = newStackProperties.get(name);	String userValue = updatedProperties.get(name);	if (((previousValue == null) ? (newValue != null) : !previousValue.equals(newValue)) && ((previousValue == null) ? (userValue == null) : previousValue.equals(userValue))) {	
modifying property named from user specified kerberos descriptor 

for (Map.Entry<String, String> entry : previousStackProperties.entrySet()) {	String name = entry.getKey();	if (newStackProperties.containsKey(name)) {	String previousValue = entry.getValue();	String newValue = newStackProperties.get(name);	String userValue = updatedProperties.get(name);	if (((previousValue == null) ? (newValue != null) : !previousValue.equals(newValue)) && ((previousValue == null) ? (userValue == null) : previousValue.equals(userValue))) {	updatedProperties.put(name, newValue);	}	} else {	
removing property named from user specified kerberos descriptor 

if (((previousValue == null) ? (newValue != null) : !previousValue.equals(newValue)) && ((previousValue == null) ? (userValue == null) : previousValue.equals(userValue))) {	updatedProperties.put(name, newValue);	}	} else {	updatedProperties.remove(name);	}	}	for (Map.Entry<String, String> entry : newStackProperties.entrySet()) {	String name = entry.getKey();	if (!previousStackProperties.containsKey(name) && !updatedProperties.containsKey(name)) {	
adding property named to user specified kerberos descriptor 

========================= ambari sample_3019 =========================

public void run() {	try {	resourceManager.ignorePermissions(new Callable<Void>() {	public Void call() throws Exception {	
polling job status 

public void run() {	try {	resourceManager.ignorePermissions(new Callable<Void>() {	public Void call() throws Exception {	try {	job = resourceManager.read(job.getId());	} catch (ItemNotFound itemNotFound) {	
job does not exist polling canceled 

public Void call() throws Exception {	try {	job = resourceManager.read(job.getId());	} catch (ItemNotFound itemNotFound) {	thisFuture.cancel(false);	return null;	}	resourceManager.retrieveJobStatus(job);	Long time = System.currentTimeMillis() / 1000L;	if (time - job.getDateStarted() > LONG_JOB_THRESHOLD) {	
job becomes long rescheduling polling to longer period 

thisFuture.cancel(false);	return null;	}	resourceManager.retrieveJobStatus(job);	Long time = System.currentTimeMillis() / 1000L;	if (time - job.getDateStarted() > LONG_JOB_THRESHOLD) {	thisFuture.cancel(false);	scheduleJobPolling(true);	}	if (job.getStatus().equals(PigJob.PIG_JOB_STATE_SUBMIT_FAILED) || job.getStatus().equals(PigJob.PIG_JOB_STATE_COMPLETED) || job.getStatus().equals(PigJob.PIG_JOB_STATE_FAILED) || job.getStatus().equals(PigJob.PIG_JOB_STATE_KILLED)) {	
job finished polling canceled 

scheduleJobPolling(true);	}	if (job.getStatus().equals(PigJob.PIG_JOB_STATE_SUBMIT_FAILED) || job.getStatus().equals(PigJob.PIG_JOB_STATE_COMPLETED) || job.getStatus().equals(PigJob.PIG_JOB_STATE_FAILED) || job.getStatus().equals(PigJob.PIG_JOB_STATE_KILLED)) {	thisFuture.cancel(false);	} else {	}	return null;	}	});	} catch (Exception e) {	
exception during handling job polling 

public static boolean pollJob(JobResourceManager resourceManager, PigJob job) {	if (jobPollers.get(job.getJobId()) == null) {	
setting up polling for 

========================= ambari sample_1024 =========================

private int getRepeatTolerance(AlertDefinitionEntity definition, String clusterName ){	if( definition.isRepeatToleranceEnabled() ){	return definition.getRepeatTolerance();	}	int repeatTolerance = 1;	try {	Cluster cluster = clusters.get().getCluster(clusterName);	String value = cluster.getClusterProperty(ConfigHelper.CLUSTER_ENV_ALERT_REPEAT_TOLERANCE, "1");	repeatTolerance = NumberUtils.toInt(value, 1);	} catch (AmbariException ambariException) {	
unable to read from cluster defaulting to 

========================= ambari sample_3625 =========================

protected PreconfigureServiceType getCommandPreconfigureType() {	String preconfigureServices = getCommandParameterValue(getCommandParameters(), PRECONFIGURE_SERVICES);	PreconfigureServiceType type = null;	if (!StringUtils.isEmpty(preconfigureServices)) {	try {	type = PreconfigureServiceType.valueOf(preconfigureServices.toUpperCase());	} catch (Throwable t) {	
invalid preconfigure services value assuming default 

protected CommandReport processIdentities(Map<String, Object> requestSharedDataContext) throws AmbariException {	CommandReport commandReport = null;	Map<String, String> commandParameters = getCommandParameters();	actionLog.writeStdOut("Processing identities...");	
processing identities 

}	}	} finally {	try {	handler.close();	} catch (KerberosOperationException e) {	}	}	}	actionLog.writeStdOut("Processing identities completed.");	
processing identities completed 

========================= ambari sample_3330 =========================

private List<HiveQueryId> getHqidListFromJsonArray(JSONArray jobs) {	List<HiveQueryId> parsedJobs = new LinkedList<>();	for (Object job : jobs) {	try {	HiveQueryId parsedJob = parseAtsHiveJob((JSONObject) job);	parsedJobs.add(parsedJob);	} catch (Exception ex) {	
error while parsing ats job 

public List<TezVertexId> getVerticesForDAGId(String dagId) {	JSONObject entities = delegate.tezVerticesListForDAG(dagId);	JSONArray vertices = (JSONArray) entities.get("entities");	List<TezVertexId> parsedVertices = new LinkedList<TezVertexId>();	for(Object vertex : vertices) {	try {	TezVertexId parsedVertex = parseVertex((JSONObject) vertex);	parsedVertices.add(parsedVertex);	} catch (Exception ex) {	
error while parsing the vertex 

========================= ambari sample_856 =========================

protected void serviceStart() throws Exception {	Configuration conf = getConfig();	YarnRPC rpc = YarnRPC.create(conf);	InetSocketAddress address = NetUtils.createSocketAddr(metricConfiguration.getTimelineServiceRpcAddress(), YarnConfiguration.DEFAULT_TIMELINE_SERVICE_PORT);	server = rpc.getServer(ApplicationHistoryProtocol.class, this, address, conf, null, metricConfiguration.getTimelineMetricsServiceHandlerThreadCount());	server.start();	this.bindAddress = conf.updateConnectAddr(YarnConfiguration.TIMELINE_SERVICE_ADDRESS, server.getListenerAddress());	
instantiated applicationhistoryclientservice at 

========================= ambari sample_440 =========================

private void processHostCountAndPredicate(Map<String, Object> hostGroupProperties, HostGroupInfo hostGroupInfo) throws InvalidTopologyTemplateException {	if (hostGroupProperties.containsKey(HOSTGROUP_HOST_COUNT_PROPERTY)) {	hostGroupInfo.setRequestedCount(Integer.valueOf(String.valueOf( hostGroupProperties.get(HOSTGROUP_HOST_COUNT_PROPERTY))));	
stored expected hosts count for group 

hostGroupInfo.setRequestedCount(Integer.valueOf(String.valueOf( hostGroupProperties.get(HOSTGROUP_HOST_COUNT_PROPERTY))));	}	if (hostGroupProperties.containsKey(HOSTGROUP_HOST_PREDICATE_PROPERTY)) {	if (hostGroupInfo.getRequestedHostCount() == 0) {	throw new InvalidTopologyTemplateException(String.format( "Host group '%s' must not specify 'host_predicate' without 'host_count'", hostGroupInfo.getHostGroupName()));	}	String hostPredicate = String.valueOf(hostGroupProperties.get(HOSTGROUP_HOST_PREDICATE_PROPERTY));	validateHostPredicateProperties(hostPredicate);	try {	hostGroupInfo.setPredicate(hostPredicate);	
compiled host predicate for group 

========================= ambari sample_3633 =========================

DocumentBuilder db = dbf.newDocumentBuilder();	StreamResult result = new StreamResult(new StringWriter());	Document document = db .parse(new InputSource(new StringReader(xml)));	Transformer transformer = TransformerFactory.newInstance() .newTransformer();	transformer.setOutputProperty(OutputKeys.INDENT, "yes");	transformer.setOutputProperty(XML_INDENT_AMT_PROP_NAME, XML_INDENT_SPACES);	DOMSource source = new DOMSource(document);	transformer.transform(source, result);	return result.getWriter().toString();	} catch (ParserConfigurationException | SAXException | IOException | TransformerFactoryConfigurationError | TransformerException e) {	
error in formatting xml 

========================= ambari sample_1065 =========================

public Response getList() {	try {	
getting all savedquery 

========================= ambari sample_801 =========================

private void postConstruct() {	Thread loadApiDocThread = new Thread("load_swagger_api_doc") {	public void run() {	
start thread to scan rest api doc from endpoints 

String[] parts = yaml.split("\n");	for (String part : parts) {	b.append(part);	b.append("\n");	}	setSwaggerYaml(b.toString());	}	} catch (Exception e) {	e.printStackTrace();	}	
scanning rest api endpoints and generating docs has been successful 

========================= ambari sample_1435 =========================

public Set<Resource> populateResourcesWithProperties(Set<Resource> resources, Request request, Set<String> propertyIds) throws SystemException {	Map<String, Map<TemporalInfo, MetricsRequest>> requestMap = getMetricsRequests(resources, request, propertyIds);	for (Map.Entry<String, Map<TemporalInfo, MetricsRequest>> clusterEntry : requestMap.entrySet()) {	for (MetricsRequest metricsRequest : clusterEntry.getValue().values()) {	try {	metricsRequest.populateResources();	} catch (IOException io) {	if (io instanceof SocketTimeoutException) {	if (LOG.isDebugEnabled()) {	
skip populating resources on socket timeout 

}	boolean clusterCollectorHostLive;	if (clusterCollectorHostLiveMap.containsKey(clusterName)) {	clusterCollectorHostLive = clusterCollectorHostLiveMap.get(clusterName);	} else {	clusterCollectorHostLive = hostProvider.isCollectorComponentLive(clusterName, TIMELINE_METRICS);	clusterCollectorHostLiveMap.put(clusterName, clusterCollectorHostLive);	}	if (!clusterCollectorHostLive) {	if (printSkipPopulateMsgHostCounter.getAndIncrement() == 0) {	
metrics collector host is not live skip populating resources with metrics next message will be logged after attempts 

printSkipPopulateMsgHostCounter.set(0);	boolean clusterCollectorComponentLive;	if (clusterCollectorComponentLiveMap.containsKey(clusterName)) {	clusterCollectorComponentLive = clusterCollectorComponentLiveMap.get(clusterName);	} else {	clusterCollectorComponentLive = hostProvider.isCollectorComponentLive(clusterName, TIMELINE_METRICS);	clusterCollectorComponentLiveMap.put(clusterName, clusterCollectorComponentLive);	}	if (!clusterCollectorComponentLive) {	if (printSkipPopulateMsgHostCompCounter.getAndIncrement() == 0) {	
metrics collector is not live skip populating resources with metrics next message will be logged after attempts 

private String preprocessPropertyId(String propertyId, String componentName) {	if (propertyId.startsWith("jvm") && JVM_PROCESS_NAMES.keySet().contains(componentName)) {	String newPropertyId = propertyId.replace("jvm.", "jvm." + JVM_PROCESS_NAMES.get(componentName));	
pre process to 

========================= ambari sample_3408 =========================

case DEQUEUE: dequeueOp();	break;	case DEQUEUEALL: dequeueAllOp();	break;	case CHECKPENDING: checkPending();	break;	case UPDATEHOSTLIST: updateHostList();	break;	}	} catch (Exception ex) {	
Failure 

for (int h = 0; h<hosts.length; h++) {	long opsEnqueued = 0;	long opsDequeued = 0;	for (int i = 0; i < threadCount; i++) {	opsEnqueued += enqueOperators[i].getOpCounts()[h];	opsDequeued += dequeOperators[i].getOpCounts()[h];	opsDequeued += dequeAllOperators[i].getOpCounts()[h];	}	assertTrue(opsEnqueued != 0);	assertEquals(0, aq.size(hosts[h]));	
host opsenqueued opsdequeued 

consumer.join();	}	int totalChecks = 0;	int totalUpdates = 0;	for (int i = 0; i < threadCount; i++) {	totalChecks += hostUpdaters[i].getOpCounts()[0];	for (int h = 0; h<hosts.length; h++) {	totalUpdates += hostCheckers[i].getOpCounts()[h];	}	}	
report totalchecks totalupdates 

========================= ambari sample_1989 =========================

public void setPartitionAggregationFunction(AGGREGATOR_TYPE type) {	switch (type) {	case HOST: performsHostAggregation.set(true);	
set host aggregator function for 

public void setPartitionAggregationFunction(AGGREGATOR_TYPE type) {	switch (type) {	case HOST: performsHostAggregation.set(true);	break;	case CLUSTER: performsClusterAggregation.set(true);	
set cluster aggregator function for 

public void unsetPartitionAggregationFunction(AGGREGATOR_TYPE type) {	switch (type) {	case HOST: performsHostAggregation.set(false);	
unset host aggregator function for 

public void unsetPartitionAggregationFunction(AGGREGATOR_TYPE type) {	switch (type) {	case HOST: performsHostAggregation.set(false);	break;	case CLUSTER: performsClusterAggregation.set(false);	
unset cluster aggregator function for 

========================= ambari sample_381 =========================

private void verifyDDL(String dbType) throws Exception {	
checking ddl for 

static void compareAgainstPostgres(String dbType) throws Exception {	
comparing against postgres 

static void compareAgainstPostgres(String dbType) throws Exception {	DDL postgres = DDLTestUtils.getDdl("Postgres");	DDL other = DDLTestUtils.getDdl(dbType);	List<String> diffs = compareDdls(postgres, other);	if (diffs.isEmpty()) {	
compare ok 

static void compareAgainstPostgres(String dbType) throws Exception {	DDL postgres = DDLTestUtils.getDdl("Postgres");	DDL other = DDLTestUtils.getDdl(dbType);	List<String> diffs = compareDdls(postgres, other);	if (diffs.isEmpty()) {	}	else {	
differences found 

static void printDDLMetrics(DDL ddl) {	
ddl metrics for 

int fkCount = 0;	int uqCount = 0;	for (Table t: ddl.tables.values()) {	colCount += t.columns.size();	if (t.primaryKey.isPresent()) {	pkCount ++;	}	fkCount += t.foreignKeys.size();	uqCount += t.uniqueConstraints.size();	}	
found tables 

colCount += t.columns.size();	if (t.primaryKey.isPresent()) {	pkCount ++;	}	fkCount += t.foreignKeys.size();	uqCount += t.uniqueConstraints.size();	}	List<String> tableNames = new ArrayList<>();	tableNames.addAll(ddl.tableNames());	Collections.sort(tableNames);	
table names 

colCount += t.columns.size();	if (t.primaryKey.isPresent()) {	pkCount ++;	}	fkCount += t.foreignKeys.size();	uqCount += t.uniqueConstraints.size();	}	List<String> tableNames = new ArrayList<>();	tableNames.addAll(ddl.tableNames());	Collections.sort(tableNames);	
total number of columns 

colCount += t.columns.size();	if (t.primaryKey.isPresent()) {	pkCount ++;	}	fkCount += t.foreignKeys.size();	uqCount += t.uniqueConstraints.size();	}	List<String> tableNames = new ArrayList<>();	tableNames.addAll(ddl.tableNames());	Collections.sort(tableNames);	
total number of pk s 

colCount += t.columns.size();	if (t.primaryKey.isPresent()) {	pkCount ++;	}	fkCount += t.foreignKeys.size();	uqCount += t.uniqueConstraints.size();	}	List<String> tableNames = new ArrayList<>();	tableNames.addAll(ddl.tableNames());	Collections.sort(tableNames);	
total number of fk s 

colCount += t.columns.size();	if (t.primaryKey.isPresent()) {	pkCount ++;	}	fkCount += t.foreignKeys.size();	uqCount += t.uniqueConstraints.size();	}	List<String> tableNames = new ArrayList<>();	tableNames.addAll(ddl.tableNames());	Collections.sort(tableNames);	
total number of uq s 

colCount += t.columns.size();	if (t.primaryKey.isPresent()) {	pkCount ++;	}	fkCount += t.foreignKeys.size();	uqCount += t.uniqueConstraints.size();	}	List<String> tableNames = new ArrayList<>();	tableNames.addAll(ddl.tableNames());	Collections.sort(tableNames);	
number of alter table statements 

diffs.addAll(compareSets(String.format("Comparing %ss of table %s.", constraintType, tableName), baseByContent.keySet(), otherByContent.keySet()));	Set<ContentType> common = Sets.intersection(baseByContent.keySet(), otherByContent.keySet());	for (ContentType constrContent : common) {	Constraint b = baseByContent.get(constrContent);	Constraint o = otherByContent.get(constrContent);	if (!b.name().equals(o.name())) {	if (compareConstraintNames) {	diffs.add(String.format("Constraint name mismatch for table %s: %s vs. %s", tableName, b, o));	}	else {	
ignoring constraint name mismatch for table vs 

========================= ambari sample_2560 =========================

